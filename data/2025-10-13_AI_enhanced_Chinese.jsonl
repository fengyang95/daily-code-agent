{"id": "2510.08610", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08610", "abs": "https://arxiv.org/abs/2510.08610", "authors": ["Imranur Rahman", "Md Rayhanur Rahman"], "title": "Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model", "comment": "Accepted to Context Collection Workshop co-located with ASE 2025", "summary": "Code completion can help developers improve efficiency and ease the\ndevelopment lifecycle. Although code completion is available in modern\nintegrated development environments (IDEs), research lacks in determining what\nmakes a good context for code completion based on the information available to\nthe IDEs for the large language models (LLMs) to perform better. In this paper,\nwe describe an effective context collection strategy to assist the LLMs in\nperforming better at code completion tasks. The key idea of our strategy is to\npreprocess the repository into smaller code chunks and later use syntactic and\nsemantic similarity-based code chunk retrieval with relative positioning. We\nfound that code chunking and relative positioning of the chunks in the final\ncontext improve the performance of code completion tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u7b56\u7565\uff0c\u901a\u8fc7\u4ee3\u7801\u5206\u5757\u548c\u76f8\u5bf9\u5b9a\u4f4d\u6765\u63d0\u5347LLM\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u786e\u5b9a\u4ec0\u4e48\u6784\u6210\u826f\u597d\u7684\u4ee3\u7801\u8865\u5168\u4e0a\u4e0b\u6587\uff0c\u4ee5\u5e2e\u52a9LLM\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d", "method": "\u5c06\u4ee3\u7801\u5e93\u9884\u5904\u7406\u4e3a\u66f4\u5c0f\u7684\u4ee3\u7801\u5757\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u53e5\u6cd5\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u4ee3\u7801\u5757\u68c0\u7d22\u4e0e\u76f8\u5bf9\u5b9a\u4f4d", "result": "\u4ee3\u7801\u5206\u5757\u548c\u4ee3\u7801\u5757\u5728\u6700\u7ec8\u4e0a\u4e0b\u6587\u4e2d\u7684\u76f8\u5bf9\u5b9a\u4f4d\u63d0\u9ad8\u4e86\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u8868\u73b0", "topic": "code agent"}}
{"id": "2510.08619", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08619", "abs": "https://arxiv.org/abs/2510.08619", "authors": ["Tennison Liu", "Silas Ruhrberg Est\u00e9vez", "David L. Bentley", "Mihaela van der Schaar"], "title": "Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents", "comment": null, "summary": "Large-scale scientific datasets -- spanning health biobanks, cell atlases,\nEarth reanalyses, and more -- create opportunities for exploratory discovery\nunconstrained by specific research questions. We term this process hypothesis\nhunting: the cumulative search for insight through sustained exploration across\nvast and complex hypothesis spaces. To support it, we introduce AScience, a\nframework modeling discovery as the interaction of agents, networks, and\nevaluation norms, and implement it as ASCollab, a distributed system of\nLLM-based research agents with heterogeneous behaviors. These agents\nself-organize into evolving networks, continually producing and peer-reviewing\nfindings under shared standards of evaluation. Experiments show that such\nsocial dynamics enable the accumulation of expert-rated results along the\ndiversity-quality-novelty frontier, including rediscoveries of established\nbiomarkers, extensions of known pathways, and proposals of new therapeutic\ntargets. While wet-lab validation remains indispensable, our experiments on\ncancer cohorts demonstrate that socially structured, agentic networks can\nsustain exploratory hypothesis hunting at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86AScience\u6846\u67b6\u548cASCollab\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u7814\u7a76\u4ee3\u7406\u7684\u81ea\u7ec4\u7ec7\u7f51\u7edc\u8fdb\u884c\u5927\u89c4\u6a21\u79d1\u5b66\u5047\u8bbe\u63a2\u7d22\uff0c\u5728\u764c\u75c7\u961f\u5217\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u80fd\u591f\u6301\u7eed\u4ea7\u751f\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u3001\u65b0\u9896\u7684\u53d1\u73b0", "motivation": "\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u96c6\u4e3a\u65e0\u7279\u5b9a\u7814\u7a76\u95ee\u9898\u7684\u63a2\u7d22\u6027\u53d1\u73b0\u521b\u9020\u4e86\u673a\u4f1a\uff0c\u9700\u8981\u652f\u6301\u5728\u590d\u6742\u5047\u8bbe\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7d2f\u79ef\u641c\u7d22\u7684\u7cfb\u7edf", "method": "\u5f15\u5165AScience\u6846\u67b6\uff0c\u5c06\u53d1\u73b0\u5efa\u6a21\u4e3a\u4ee3\u7406\u3001\u7f51\u7edc\u548c\u8bc4\u4f30\u89c4\u8303\u7684\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e3aASCollab\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5305\u542b\u5177\u6709\u5f02\u8d28\u884c\u4e3a\u7684LLM\u7814\u7a76\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u81ea\u7ec4\u7ec7\u6210\u6f14\u5316\u7f51\u7edc\uff0c\u6301\u7eed\u4ea7\u751f\u548c\u540c\u884c\u8bc4\u5ba1\u53d1\u73b0", "result": "\u5b9e\u9a8c\u663e\u793a\u8fd9\u79cd\u793e\u4f1a\u52a8\u6001\u80fd\u591f\u6cbf\u591a\u6837\u6027-\u8d28\u91cf-\u65b0\u9896\u6027\u524d\u6cbf\u79ef\u7d2f\u4e13\u5bb6\u8bc4\u7ea7\u7ed3\u679c\uff0c\u5305\u62ec\u91cd\u65b0\u53d1\u73b0\u5df2\u5efa\u7acb\u7684\u751f\u7269\u6807\u5fd7\u7269\u3001\u6269\u5c55\u5df2\u77e5\u901a\u8def\u548c\u63d0\u51fa\u65b0\u7684\u6cbb\u7597\u9776\u70b9", "conclusion": "\u867d\u7136\u6e7f\u5b9e\u9a8c\u5ba4\u9a8c\u8bc1\u4ecd\u7136\u4e0d\u53ef\u6216\u7f3a\uff0c\u4f46\u793e\u4f1a\u7ed3\u6784\u5316\u7684\u4ee3\u7406\u7f51\u7edc\u53ef\u4ee5\u5728\u5927\u89c4\u6a21\u4e0a\u6301\u7eed\u8fdb\u884c\u63a2\u7d22\u6027\u5047\u8bbe\u641c\u7d22", "topic": "agent analysis"}}
{"id": "2510.08640", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08640", "abs": "https://arxiv.org/abs/2510.08640", "authors": ["Ha Min Son", "Huan Ren", "Xin Liu", "Zhe Zhao"], "title": "Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools", "comment": null, "summary": "Android is the largest mobile platform, yet automatically building\napplications remains a practical challenge. While Large Language Models (LLMs)\nshow promise for code repair, their use for fixing Android build errors remains\nunderexplored. To address this gap, we first introduce AndroidBuildBench, a\nbenchmark of 1,019 build failures curated from the commit histories of 43\nopen-source Android projects. Each problem is paired with a verified solution\nfrom a subsequent commit, ensuring that fixes are feasible. Second, we propose\nGradleFixer, an LLM agent with domain-specific tools for inspecting and\nmanipulating the Gradle build environment. GradleFixer achieves a resolve rate\nof 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent\nthat relies on a general-purpose shell. GradleFixer's success suggests that\nwhile LLMs possess the high-level knowledge to solve these failures, they\nstruggle to translate this knowledge into effective low-level actions using a\ngeneral-purpose shell. We demonstrate the effectiveness of a strategy we term\nTool Bridging, which replaces general-purpose shell commands with domain-aware\nabstractions. We hypothesize this approach works through two mechanisms: 1) it\nprovides tools in an API-like format that LLMs use more reliably, and 2) it\nconstrains the action space to relevant operations. This approach bridges the\ngap between the model's high-level reasoning and effective low-level execution.", "AI": {"tldr": "\u63d0\u51fa\u4e86AndroidBuildBench\u57fa\u51c6\u6d4b\u8bd5\u548cGradleFixer\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u4fee\u590dAndroid\u6784\u5efa\u9519\u8bef\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6784\u5efa\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "Android\u4f5c\u4e3a\u6700\u5927\u7684\u79fb\u52a8\u5e73\u53f0\uff0c\u81ea\u52a8\u6784\u5efa\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\u3002\u867d\u7136LLM\u5728\u4ee3\u7801\u4fee\u590d\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u4fee\u590dAndroid\u6784\u5efa\u9519\u8bef\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u9996\u5148\u521b\u5efaAndroidBuildBench\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b1,019\u4e2a\u6784\u5efa\u5931\u8d25\u6848\u4f8b\uff09\uff0c\u7136\u540e\u63d0\u51faGradleFixer\u2014\u2014\u4e00\u4e2a\u914d\u5907\u9886\u57df\u7279\u5b9a\u5de5\u5177\u7684LLM\u4ee3\u7406\uff0c\u7528\u4e8e\u68c0\u67e5\u548c\u64cd\u4f5cGradle\u6784\u5efa\u73af\u5883\u3002", "result": "GradleFixer\u5b9e\u73b0\u4e8681.4%\u7684\u89e3\u51b3\u7387\uff08pass@1\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f9d\u8d56\u901a\u7528shell\u7684\u6700\u5148\u8fdb\u7f16\u7801\u4ee3\u7406\u3002", "conclusion": "LLM\u5177\u5907\u89e3\u51b3\u6784\u5efa\u5931\u8d25\u7684\u9ad8\u5c42\u77e5\u8bc6\uff0c\u4f46\u96be\u4ee5\u901a\u8fc7\u901a\u7528shell\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u5e95\u5c42\u64cd\u4f5c\u3002\u5de5\u5177\u6865\u63a5\u7b56\u7565\u901a\u8fc7\u63d0\u4f9bAPI\u5f0f\u5de5\u5177\u548c\u7ea6\u675f\u64cd\u4f5c\u7a7a\u95f4\uff0c\u5f25\u5408\u4e86\u9ad8\u5c42\u63a8\u7406\u4e0e\u5e95\u5c42\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "swe application"}}
{"id": "2510.08592", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08592", "abs": "https://arxiv.org/abs/2510.08592", "authors": ["Shahriar Kabir Nahin", "Hadi Askari", "Muhao Chen", "Anshuman Chhabra"], "title": "Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models", "comment": null, "summary": "Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple\ncandidate responses and then operating over this set to find the best output. A\ntacit premise behind TTS is that sufficiently diverse candidate pools enhance\nreliability. In this work, we show that this assumption in TTS introduces a\npreviously unrecognized failure mode. When candidate diversity is curtailed,\neven by a modest amount, TTS becomes much more likely to produce unsafe\noutputs. We present a reference-guided diversity reduction protocol (RefDiv)\nthat serves as a diagnostic attack to stress test TTS pipelines. Through\nextensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,\nGemma3) and two widely used TTS strategies (Monte Carlo Tree Search and\nBest-of-N), constraining diversity consistently signifies the rate at which TTS\nproduces unsafe results. The effect is often stronger than that produced by\nprompts directly with high adversarial intent scores. This observed phenomenon\nalso transfers across TTS strategies and to closed-source models (e.g. OpenAI\no3 and Gemini-2.5-Pro), thus indicating that this is a general and extant\nproperty of TTS rather than a model-specific artifact. Additionally, we find\nthat numerous widely used safety guardrail classifiers (e.g. Llama-Guard and\nOpenAI Moderation API), are unable to flag the adversarial input prompts\ngenerated by RefDiv, demonstrating that existing defenses offer limited\nprotection against this diversity-driven failure mode. Through this work, we\nhope to motivate future research on designing robust TTS strategies that are\nboth effective and secure against diversity-targeted stress tests as\nillustrated by RefDiv.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6d4b\u8bd5\u65f6\u7f29\u653e(TTS)\u6280\u672f\u4e2d\u4e00\u4e2a\u672a\u88ab\u8bc6\u522b\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u5f53\u5019\u9009\u54cd\u5e94\u591a\u6837\u6027\u53d7\u9650\u65f6\uff0c\u5373\u4f7f\u53ea\u662f\u9002\u5ea6\u51cf\u5c11\uff0cTTS\u4e5f\u66f4\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\u3002", "motivation": "TTS\u6280\u672f\u5047\u8bbe\u5019\u9009\u6c60\u7684\u591a\u6837\u6027\u80fd\u591f\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u5047\u8bbe\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u7814\u7a76TTS\u5728\u591a\u6837\u6027\u53d7\u9650\u65f6\u7684\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86RefDiv\u53c2\u8003\u5f15\u5bfc\u591a\u6837\u6027\u51cf\u5c11\u534f\u8bae\u4f5c\u4e3a\u8bca\u65ad\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u5019\u9009\u54cd\u5e94\u7684\u591a\u6837\u6027\u6765\u538b\u529b\u6d4b\u8bd5TTS\u7ba1\u9053\uff0c\u5728\u56db\u4e2a\u5f00\u6e90\u6a21\u578b\u548c\u4e24\u79cdTTS\u7b56\u7565\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u591a\u6837\u6027\u53d7\u9650\u663e\u8457\u589e\u52a0\u4e86TTS\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\u7684\u6982\u7387\uff0c\u8fd9\u79cd\u6548\u5e94\u901a\u5e38\u6bd4\u9ad8\u5bf9\u6297\u6027\u610f\u56fe\u63d0\u793a\u66f4\u5f3a\uff0c\u4e14\u5728\u4e0d\u540cTTS\u7b56\u7565\u548c\u95ed\u6e90\u6a21\u578b\u4e2d\u90fd\u5b58\u5728\u3002\u73b0\u6709\u5b89\u5168\u9632\u62a4\u5206\u7c7b\u5668\u65e0\u6cd5\u68c0\u6d4bRefDiv\u751f\u6210\u7684\u5bf9\u6297\u6027\u8f93\u5165\u63d0\u793a\u3002", "conclusion": "TTS\u5b58\u5728\u57fa\u4e8e\u591a\u6837\u6027\u7684\u56fa\u6709\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u8bbe\u8ba1\u65e2\u80fd\u4fdd\u6301\u6709\u6548\u6027\u53c8\u80fd\u62b5\u5fa1\u591a\u6837\u6027\u9488\u5bf9\u6027\u653b\u51fb\u7684\u9c81\u68d2TTS\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2510.08664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08664", "abs": "https://arxiv.org/abs/2510.08664", "authors": ["Jianan Mu", "Mingyu Shi", "Yining Wang", "Tianmeng Yang", "Bin Sun", "Xing Hu", "Jing Ye", "Huawei Li"], "title": "Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware", "comment": null, "summary": "LLM-based RTL generation is an interesting research direction, as it holds\nthe potential to liberate the least automated stage in the current chip design.\nHowever, due to the substantial semantic gap between high-level specifications\nand RTL, coupled with limited training data, existing models struggle with\ngeneration accuracy. Drawing on human experience, design with verification\nhelps improving accuracy. However, as the RTL testbench data are even more\nscarce, it is not friendly for LLMs. Although LLMs excel at higher-level\nlanguages like Python/C, they have a huge semantic gap from RTL. When\nimplementing the same functionality, Python/C code and hardware code differ\nsignificantly in the spatiotemporal granularity, requiring the LLM not only to\nconsider high-level functional semantics but also to ensure the low-level\ndetails align with the circuit code. It is not an easy task. In this paper, we\npropose a function abstracted verifiable middleware (Faver) that streamlines\nRTL verification in LLM-based workflows. By mixing LLM-friendly code structures\nwith a rule-based template, Faver decouples the details of circuit\nverification, allowing the LLM to focus on the functionality itself. In our\nexperiments on the SFT model and open-source models, Faver improved the model's\ngeneration accuracy by up to 14%.", "AI": {"tldr": "\u63d0\u51faFaver\u65b9\u6cd5\uff0c\u901a\u8fc7\u51fd\u6570\u62bd\u8c61\u7684\u53ef\u9a8c\u8bc1\u4e2d\u95f4\u4ef6\u7b80\u5316\u57fa\u4e8eLLM\u7684RTL\u751f\u6210\u6d41\u7a0b\uff0c\u5c06\u7535\u8def\u9a8c\u8bc1\u7ec6\u8282\u4e0e\u529f\u80fd\u5b9e\u73b0\u89e3\u8026\uff0c\u63d0\u9ad8\u751f\u6210\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728RTL\u751f\u6210\u4e2d\u9762\u4e34\u8bed\u4e49\u9e3f\u6c9f\u5927\u3001\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u51c6\u786e\u6027\u4e0d\u8db3\u3002\u501f\u9274\u4eba\u7c7b\u8bbe\u8ba1\u7ecf\u9a8c\uff0c\u9700\u8981\u7ed3\u5408\u9a8c\u8bc1\u6765\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46RTL\u6d4b\u8bd5\u6570\u636e\u7a00\u7f3a\u4e14LLM\u5bf9\u9ad8\u7ea7\u8bed\u8a00\u66f4\u64c5\u957f\u3002", "method": "\u63d0\u51faFaver\u65b9\u6cd5\uff0c\u6df7\u5408LLM\u53cb\u597d\u7684\u4ee3\u7801\u7ed3\u6784\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u677f\uff0c\u5c06\u7535\u8def\u9a8c\u8bc1\u7ec6\u8282\u4e0e\u529f\u80fd\u5b9e\u73b0\u89e3\u8026\uff0c\u8ba9LLM\u4e13\u6ce8\u4e8e\u529f\u80fd\u672c\u8eab\u3002", "result": "\u5728SFT\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaver\u5c06\u6a21\u578b\u751f\u6210\u51c6\u786e\u6027\u63d0\u5347\u4e86\u6700\u9ad814%\u3002", "conclusion": "Faver\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728RTL\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u901a\u8fc7\u9a8c\u8bc1\u4e2d\u95f4\u4ef6\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2510.08665", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08665", "abs": "https://arxiv.org/abs/2510.08665", "authors": ["Aofan Liu", "Haoxuan Li", "Bin Wang", "Ao Yang", "Hui Li"], "title": "RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution", "comment": null, "summary": "Code generation models based on large language models (LLMs) have gained wide\nadoption, but challenges remain in ensuring safety, accuracy, and\ncontrollability, especially for complex tasks. Existing methods often lack\ndynamic integration of external tools, transparent reasoning, and user control\nover safety. To address these issues, we propose a controllable code generation\nframework utilizing the ReAct paradigm for multi-agent task execution. This\nframework is a multi-agent system designed to enable efficient, precise, and\ninterpretable code generation through dynamic interactions between LLMs and\nexternal resources. The framework adopts a collaborative architecture\ncomprising four specialized agents: a Planner for task decomposition, a\nSearcher that leverages the ReAct framework for reasoning and tool integration,\na CodeGen agent for accurate code generation, and an Extractor for structured\ndata retrieval. The ReAct-based Searcher alternates between generating\nreasoning traces and executing actions, facilitating seamless integration of\ninternal knowledge with external tools (such as search engines) to enhance\naccuracy and user control. Experimental results show the framework's\neffectiveness across multiple languages, achieving a 94.8% security rate on the\nSVEN dataset with CodeQL, outperforming existing approaches. Its transparent\nreasoning process fosters user trust and improves controllability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eReAct\u8303\u5f0f\u7684\u53ef\u63a7\u4ee3\u7801\u751f\u6210\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u5b9e\u73b0\u9ad8\u6548\u3001\u7cbe\u786e\u548c\u53ef\u89e3\u91ca\u7684\u4ee3\u7801\u751f\u6210\uff0c\u5728\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5728\u5b89\u5168\u6027\u3001\u51c6\u786e\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u52a8\u6001\u5916\u90e8\u5de5\u5177\u96c6\u6210\u3001\u900f\u660e\u63a8\u7406\u548c\u7528\u6237\u5b89\u5168\u63a7\u5236\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u4efb\u52a1\u5206\u89e3\u89c4\u5212\u5668\u3001\u57fa\u4e8eReAct\u7684\u641c\u7d22\u5668\u3001\u4ee3\u7801\u751f\u6210\u5668\u548c\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\u3002", "result": "\u5728\u591a\u8bed\u8a00\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728SVEN\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.8%\u7684\u5b89\u5168\u7387\uff08\u4f7f\u7528CodeQL\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u900f\u660e\u63a8\u7406\u8fc7\u7a0b\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\uff0c\u63d0\u9ad8\u53ef\u63a7\u6027\uff0c\u4e3a\u590d\u6742\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2510.08667", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08667", "abs": "https://arxiv.org/abs/2510.08667", "authors": ["Mohammad Baqar"], "title": "RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data", "comment": "13 Pages", "summary": "Modern software teams frequently encounter delays in resolving recurring or\nrelated issues due to fragmented knowledge scattered across JIRA tickets,\ndeveloper discussions, and GitHub pull requests (PRs). To address this\nchallenge, we propose a Retrieval-Augmented Generation (RAG) framework that\nintegrates Sentence-Transformers for semantic embeddings with FAISS-based\nvector search to deliver context-aware ticket resolution recommendations. The\napproach embeds historical JIRA tickets, user comments, and linked PR metadata\nto retrieve semantically similar past cases, which are then synthesized by a\nLarge Language Model (LLM) into grounded and explainable resolution\nsuggestions. The framework contributes a unified pipeline linking JIRA and\nGitHub data, an embedding and FAISS indexing strategy for heterogeneous\nsoftware artifacts, and a resolution generation module guided by retrieved\nevidence. Experimental evaluation using precision, recall, resolution time\nreduction, and developer acceptance metrics shows that the proposed system\nsignificantly improves resolution accuracy, fix quality, and knowledge reuse in\nmodern DevOps environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408JIRA\u5de5\u5355\u3001\u5f00\u53d1\u8005\u8ba8\u8bba\u548cGitHub PR\u4e2d\u7684\u788e\u7247\u5316\u77e5\u8bc6\uff0c\u4e3a\u8f6f\u4ef6\u56e2\u961f\u63d0\u4f9b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u95ee\u9898\u89e3\u51b3\u5efa\u8bae\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u56e2\u961f\u5728\u5904\u7406\u91cd\u590d\u6216\u76f8\u5173\u95ee\u9898\u65f6\u5e38\u56e0\u77e5\u8bc6\u5206\u6563\u5728JIRA\u5de5\u5355\u3001\u5f00\u53d1\u8005\u8ba8\u8bba\u548cGitHub PR\u4e2d\u800c\u9047\u5230\u5ef6\u8fdf\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u77e5\u8bc6\u6574\u5408\u548c\u68c0\u7d22\u7cfb\u7edf\u6765\u52a0\u901f\u95ee\u9898\u89e3\u51b3\u3002", "method": "\u4f7f\u7528Sentence-Transformers\u751f\u6210\u8bed\u4e49\u5d4c\u5165\uff0c\u7ed3\u5408FAISS\u5411\u91cf\u641c\u7d22\u6784\u5efaRAG\u6846\u67b6\uff0c\u5d4c\u5165\u5386\u53f2JIRA\u5de5\u5355\u3001\u7528\u6237\u8bc4\u8bba\u548c\u5173\u8054PR\u5143\u6570\u636e\uff0c\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u7684\u8fc7\u5f80\u6848\u4f8b\uff0c\u7136\u540e\u901a\u8fc7LLM\u5408\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u89e3\u51b3\u65b9\u6848\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u89e3\u51b3\u65f6\u95f4\u51cf\u5c11\u548c\u5f00\u53d1\u8005\u63a5\u53d7\u5ea6\u7b49\u6307\u6807\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u51c6\u786e\u6027\u3001\u4fee\u590d\u8d28\u91cf\u548c\u77e5\u8bc6\u91cd\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684JIRA\u548cGitHub\u6570\u636e\u7ba1\u9053\u3001\u5f02\u6784\u8f6f\u4ef6\u5de5\u4ef6\u7684\u5d4c\u5165\u548c\u7d22\u5f15\u7b56\u7565\uff0c\u4ee5\u53ca\u57fa\u4e8e\u68c0\u7d22\u8bc1\u636e\u7684\u89e3\u51b3\u65b9\u6848\u751f\u6210\u6a21\u5757\uff0c\u6709\u6548\u6539\u5584\u4e86\u73b0\u4ee3DevOps\u73af\u5883\u4e2d\u7684\u95ee\u9898\u89e3\u51b3\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2510.08790", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08790", "abs": "https://arxiv.org/abs/2510.08790", "authors": ["Guangya Wan", "Mingyang Ling", "Xiaoqi Ren", "Rujun Han", "Sheng Li", "Zizhao Zhang"], "title": "COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context", "comment": "Under Review for ACL", "summary": "Long-horizon tasks that require sustained reasoning and multiple tool\ninteractions remain challenging for LLM agents: small errors compound across\nsteps, and even state-of-the-art models often hallucinate or lose coherence. We\nidentify context management as the central bottleneck -- extended histories\ncause agents to overlook critical evidence or become distracted by irrelevant\ninformation, thus failing to replan or reflect from previous mistakes. To\naddress this, we propose COMPASS (Context-Organized Multi-Agent Planning and\nStrategy System), a lightweight hierarchical framework that separates tactical\nexecution, strategic oversight, and context organization into three specialized\ncomponents: (1) a Main Agent that performs reasoning and tool use, (2) a\nMeta-Thinker that monitors progress and issues strategic interventions, and (3)\na Context Manager that maintains concise, relevant progress briefs for\ndifferent reasoning stages. Across three challenging benchmarks -- GAIA,\nBrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20%\nrelative to both single- and multi-agent baselines. We further introduce a\ntest-time scaling extension that elevates performance to match established\nDeepResearch agents, and a post-training pipeline that delegates context\nmanagement to smaller models for enhanced efficiency.", "AI": {"tldr": "COMPASS\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u6218\u672f\u6267\u884c\u3001\u6218\u7565\u76d1\u7763\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6765\u89e3\u51b3LLM\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u957f\u65f6\u7a0b\u4efb\u52a1\u9700\u8981\u6301\u7eed\u63a8\u7406\u548c\u591a\u6b21\u5de5\u5177\u4ea4\u4e92\uff0c\u4f46LLM\u4ee3\u7406\u5bb9\u6613\u56e0\u5c0f\u9519\u8bef\u7d2f\u79ef\u3001\u5e7b\u89c9\u6216\u5931\u53bb\u8fde\u8d2f\u6027\u800c\u5931\u8d25\uff0c\u4e0a\u4e0b\u6587\u7ba1\u7406\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faCOMPASS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1a\u6267\u884c\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7684\u4e3b\u4ee3\u7406\u3001\u76d1\u63a7\u8fdb\u5ea6\u5e76\u53d1\u51fa\u6218\u7565\u5e72\u9884\u7684\u5143\u601d\u8003\u8005\u3001\u7ef4\u62a4\u7b80\u6d01\u76f8\u5173\u8fdb\u5ea6\u6458\u8981\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002", "result": "\u5728GAIA\u3001BrowseComp\u548cHumanity's Last Exam\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOMPASS\u76f8\u6bd4\u5355\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe20%\u3002", "conclusion": "COMPASS\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u8bad\u7ec3\u540e\u6d41\u6c34\u7ebf\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.08697", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08697", "abs": "https://arxiv.org/abs/2510.08697", "authors": ["Terry Yue Zhuo", "Xiaolong Jin", "Hange Liu", "Juyong Jiang", "Tianyang Liu", "Chen Gong", "Bhupesh Bishnoi", "Vaisakhi Mishra", "Marek Suppa", "Noah Ziems", "Saiteja Utpala", "Ming Xu", "Guangyu Song", "Kaixin Li", "Yuhan Cao", "Bo Liu", "Zheng Liu", "Sabina Abdurakhmanova", "Wenhao Yu", "Mengzhao Jia", "Jihan Yao", "Kenneth Hamilton", "Kumar Shridhar", "Minh Chien Vu", "Dingmin Wang", "Jiawei Liu", "Zijian Wang", "Qian Liu", "Binyuan Hui", "Meg Risdal", "Ahsen Khaliq", "Atin Sood", "Zhenchang Xing", "Wasi Uddin Ahmad", "John Grundy", "David Lo", "Banghua Zhu", "Xiaoning Du", "Torsten Scholak", "Leandro von Werra"], "title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution", "comment": "Built with love by the BigCode community :)", "summary": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable\nreal-time evaluation from human perspectives to assess the quality of model\nresponses. In the coding domain, manually examining the quality of\nLLM-generated content is extremely challenging, as it requires understanding\nlong chunks of raw code and deliberately simulating code execution. To this\nend, we introduce BigCodeArena, an open human evaluation platform for code\ngeneration backed by a comprehensive and on-the-fly execution environment.\nBuilt on top of Chatbot Arena, BigCodeArena enables the execution of\nLLM-generated code and allows humans to interact with the execution process and\noutcomes. We collected over 14,000 raw code-centric conversation sessions\nacross 10 widely used LLMs, spanning 10 languages and 8 types of execution\nenvironments. Among these conversations, we identified more than 4,700\nmulti-turn samples with pairwise human preferences. Further analysis uncovers\nunderexplored preferences of LLMs in fine-grained domains characterized by\ntasks, languages, and frameworks. To systematically examine code understanding\nand generation capabilities of frontier LLMs, we curated two benchmarks based\non the collected data, namely BigCodeReward and AutoCodeArena. For\nBigCodeReward, we post-processed the 4,700 conversations and evaluated the\nconsistency between reward models and human preferences. The evaluation shows\nthat most LLMs have superior performance in judging coding preferences when the\nexecution results are available. Inspired by these findings, we propose\nAutoCodeArena, an automatic Elo rating benchmark designed to assess the coding\nquality of LLMs without human involvement. We find that proprietary LLMs like\nGPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation\nperformance among recent emerging models.", "AI": {"tldr": "BigCodeArena\u662f\u4e00\u4e2a\u57fa\u4e8eChatbot Arena\u6784\u5efa\u7684\u4ee3\u7801\u751f\u6210\u4eba\u7c7b\u8bc4\u4f30\u5e73\u53f0\uff0c\u63d0\u4f9b\u5b9e\u65f6\u6267\u884c\u73af\u5883\u6765\u8bc4\u4f30LLM\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u6536\u96c6\u4e86\u8d85\u8fc714,000\u4e2a\u4ee3\u7801\u5bf9\u8bdd\u4f1a\u8bdd\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u5f00\u53d1\u4e86BigCodeReward\u548cAutoCodeArena\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5728\u4ee3\u7801\u9886\u57df\uff0c\u624b\u52a8\u8bc4\u4f30LLM\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u6781\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u7406\u89e3\u539f\u59cb\u4ee3\u7801\u5e76\u6a21\u62df\u6267\u884c\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u6784\u5efaBigCodeArena\u5e73\u53f0\uff0c\u652f\u6301LLM\u751f\u6210\u4ee3\u7801\u7684\u6267\u884c\u548c\u4eba\u7c7b\u4ea4\u4e92\uff0c\u6536\u96c6\u591a\u8f6e\u5bf9\u8bdd\u6837\u672c\uff0c\u5f00\u53d1BigCodeReward\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u53caAutoCodeArena\u81ea\u52a8Elo\u8bc4\u7ea7\u57fa\u51c6\u3002", "result": "\u6536\u96c6\u4e8614,000+\u4ee3\u7801\u5bf9\u8bdd\uff0c\u8bc6\u522b\u4e864,700+\u5e26\u6709\u4eba\u7c7b\u504f\u597d\u7684\u591a\u8f6e\u6837\u672c\uff0c\u53d1\u73b0\u5f53\u6267\u884c\u7ed3\u679c\u53ef\u7528\u65f6\uff0c\u5927\u591a\u6570LLM\u5728\u5224\u65ad\u7f16\u7801\u504f\u597d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cGPT-5\u3001Claude-Sonnet-4\u548cClaude-Opus-4\u5728\u4ee3\u7801\u751f\u6210\u6027\u80fd\u4e0a\u9886\u5148\u3002", "conclusion": "BigCodeArena\u4e3a\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e73\u53f0\uff0c\u57fa\u4e8e\u6267\u884c\u7ed3\u679c\u7684\u8bc4\u4f30\u80fd\u66f4\u597d\u5730\u53cd\u6620LLM\u7684\u4ee3\u7801\u80fd\u529b\uff0c\u4e13\u6709LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u4fdd\u6301\u9886\u5148\u5730\u4f4d\u3002", "topic": "swe benchmark"}}
{"id": "2510.08847", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08847", "abs": "https://arxiv.org/abs/2510.08847", "authors": ["Allison Sihan Jia", "Daniel Huang", "Nikhil Vytla", "Nirvika Choudhury", "John C Mitchell", "Anupam Datta"], "title": "What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment", "comment": null, "summary": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation\nparadigm based on an agent's operational loop of setting goals, devising plans,\nand executing actions. The framework includes five evaluation metrics: Goal\nFulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan\nAdherence. Logical Consistency checks that an agent's actions are consistent\nwith its prior actions. Execution Efficiency checks whether the agent executes\nin the most efficient way to achieve its goal. Plan Quality checks whether an\nagent's plans are aligned with its goals; Plan Adherence checks if an agent's\nactions are aligned with its plan; and Goal Fulfillment checks that agent's\nfinal outcomes match the stated goals. Our experimental results on two\nbenchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for\na production-grade data agent - show that this framework (a) provides a\nsystematic way to cover a broad range of agent failures, including all agent\nerrors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that\nexhibit strong agreement with human annotation, covering 80% to over 95%\nerrors; and (c) localizes errors with 86% agreement to enable targeted\nimprovement of agent performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent GPA\u6846\u67b6\uff0c\u57fa\u4e8e\u76ee\u6807-\u8ba1\u5212-\u52a8\u4f5c\u5faa\u73af\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u5305\u542b\u4e94\u4e2a\u8bc4\u4ef7\u6307\u6807\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u9700\u8981\u7cfb\u7edf\u5316\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\u6765\u5168\u9762\u8986\u76d6\u5404\u79cd\u667a\u80fd\u4f53\u5931\u8d25\u60c5\u51b5\uff0c\u652f\u6301\u9519\u8bef\u5b9a\u4f4d\u548c\u6027\u80fd\u6539\u8fdb\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u76ee\u6807-\u8ba1\u5212-\u52a8\u4f5c\u5faa\u73af\u7684Agent GPA\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u8bc4\u4ef7\u6307\u6807\uff1a\u76ee\u6807\u8fbe\u6210\u5ea6\u3001\u903b\u8f91\u4e00\u81f4\u6027\u3001\u6267\u884c\u6548\u7387\u3001\u8ba1\u5212\u8d28\u91cf\u548c\u8ba1\u5212\u9075\u5faa\u5ea6\u3002", "result": "\u5728TRAIL/GAIA\u548c\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u8986\u76d6\u6240\u6709\u667a\u80fd\u4f53\u9519\u8bef\uff0c\u652f\u6301LLM\u8bc4\u5224\u5668\u4e0e\u4eba\u5de5\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\uff0880%-95%\uff09\uff0c\u9519\u8bef\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe86%\u3002", "conclusion": "Agent GPA\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u548c\u5b9a\u4f4d\u9519\u8bef\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u6027\u80fd\u7684\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2510.08810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08810", "abs": "https://arxiv.org/abs/2510.08810", "authors": ["Mohayeminul Islam", "Ajay Kumar Jha", "May Mahmoud", "Sarah Nadi"], "title": "PyMigTool: a tool for end-to-end Python library migration", "comment": "arXiv admin note: text overlap with arXiv:2504.13272", "summary": "Library migration is the process of replacing a library with a similar one in\na software project. Manual library migration is time consuming and error prone,\nas it requires developers to understand the Application Programming Interfaces\n(API) of both libraries, map equivalent APIs, and perform the necessary code\ntransformations. Due to the difficulty of the library migration process, most\nof the existing automated techniques and tooling stop at the API mapping stage\nor support a limited set of libraries and code transformations. In this paper,\nwe develop an end-to-end solution that can automatically migrate code between\nany arbitrary pair of Python libraries that provide similar functionality. Due\nto the promising capabilities of Large Language Models (LLMs) in code\ngeneration and transformation, we use LLMs as the primary engine for migration.\nBefore building the tool, we first study the capabilities of LLMs for library\nmigration on a benchmark of 321 real-world library migrations. We find that\nLLMs can effectively perform library migration, but some post-processing steps\ncan further improve the performance. Based on this, we develop PyMigTool, a\ncommand line application that combines the power of LLMs, static analysis, and\ndynamic analysis to provide accurate library migration. We evaluate PyMigTool\non 717 real-world Python applications that are not from our benchmark. We find\nthat PyMigTool can migrate 32% of the migrations with complete correctness. Of\nthe remaining migrations, only 14% of the migration-related changes are left\nfor developers to fix for more than half of the projects.", "AI": {"tldr": "\u5f00\u53d1\u4e86PyMigTool\uff0c\u4e00\u4e2a\u7ed3\u5408LLM\u3001\u9759\u6001\u5206\u6790\u548c\u52a8\u6001\u5206\u6790\u7684\u7aef\u5230\u7aefPython\u5e93\u8fc1\u79fb\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u52a8\u8fc1\u79fb\u4efb\u610f\u529f\u80fd\u76f8\u4f3c\u7684Python\u5e93\u4e4b\u95f4\u7684\u4ee3\u7801", "motivation": "\u624b\u52a8\u5e93\u8fc1\u79fb\u8017\u65f6\u4e14\u6613\u9519\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u6280\u672f\u5927\u591a\u53ea\u505c\u7559\u5728API\u6620\u5c04\u9636\u6bb5\u6216\u652f\u6301\u6709\u9650\u7684\u5e93\u548c\u4ee3\u7801\u8f6c\u6362", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u4e3b\u8981\u8fc1\u79fb\u5f15\u64ce\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u548c\u52a8\u6001\u5206\u6790\uff0c\u5f00\u53d1\u4e86PyMigTool\u547d\u4ee4\u884c\u5e94\u7528", "result": "\u5728717\u4e2a\u771f\u5b9ePython\u5e94\u7528\u4e0a\u8bc4\u4f30\uff0cPyMigTool\u80fd\u5b8c\u5168\u6b63\u786e\u8fc1\u79fb32%\u7684\u6848\u4f8b\uff0c\u5269\u4f59\u6848\u4f8b\u4e2d\u8d85\u8fc7\u4e00\u534a\u9879\u76ee\u53ea\u670914%\u7684\u8fc1\u79fb\u76f8\u5173\u53d8\u66f4\u9700\u8981\u5f00\u53d1\u8005\u4fee\u590d", "conclusion": "LLM\u80fd\u6709\u6548\u6267\u884c\u5e93\u8fc1\u79fb\uff0c\u7ed3\u5408\u540e\u5904\u7406\u6b65\u9aa4\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0cPyMigTool\u4e3aPython\u5e93\u8fc1\u79fb\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "2510.08872", "categories": ["cs.AI", "cs.GT", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08872", "abs": "https://arxiv.org/abs/2510.08872", "authors": ["Siqi Zhu", "David Zhang", "Pedro Cisneros-Velarde", "Jiaxuan You"], "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare", "comment": "31 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a mutual welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and mutual welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .", "AI": {"tldr": "\u63d0\u51fa\u4e86GTAlign\u6846\u67b6\uff0c\u5c06\u535a\u5f08\u8bba\u51b3\u7b56\u6574\u5408\u5230LLM\u7684\u63a8\u7406\u548c\u8bad\u7ec3\u4e2d\uff0c\u901a\u8fc7\u6784\u5efa\u6536\u76ca\u77e9\u9635\u548c\u76f8\u4e92\u798f\u5229\u5956\u52b1\u6765\u4f18\u5316\u7528\u6237-LLM\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e92\u5229\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5047\u8bbe\u6700\u5927\u5316\u6a21\u578b\u5956\u52b1\u5c31\u80fd\u6700\u5927\u5316\u7528\u6237\u798f\u5229\uff0c\u4f46\u5b9e\u8df5\u4e2dLLM\u53ef\u80fd\u4ea7\u751f\u8fc7\u5ea6\u6f84\u6e05\u6216\u5197\u957f\u63a8\u7406\uff0c\u5bfc\u81f4\u7528\u6237\u504f\u597d\u4e0e\u6a21\u578b\u884c\u4e3a\u4e0d\u5339\u914d\uff0c\u7c7b\u4f3c\u4e8e\u56da\u5f92\u56f0\u5883\u95ee\u9898\u3002", "method": "\u5728\u63a8\u7406\u9636\u6bb5\u5c06\u7528\u6237-LLM\u4ea4\u4e92\u89c6\u4e3a\u7b56\u7565\u535a\u5f08\uff0c\u6784\u5efa\u6536\u76ca\u77e9\u9635\u8bc4\u4f30\u53cc\u65b9\u798f\u5229\uff1b\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u76f8\u4e92\u798f\u5229\u5956\u52b1\u5f3a\u5316\u5408\u4f5c\u54cd\u5e94\uff1b\u8fd8\u5305\u542b\u57fa\u4e8e\u535a\u5f08\u8bba\u63a8\u7406\u7684\u52a8\u6001\u9002\u5e94\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGTAlign\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3001\u7b54\u6848\u8d28\u91cf\u548c\u76f8\u4e92\u798f\u5229\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GTAlign\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5bf9\u9f50\u4e2d\u7684\u793e\u4f1a\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7528\u6237\u4e0e\u6a21\u578b\u7684\u4e92\u5229\u5171\u8d62\u3002", "topic": "agent analysis"}}
{"id": "2510.08928", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08928", "abs": "https://arxiv.org/abs/2510.08928", "authors": ["Yushuo Zheng", "Zicheng Zhang", "Xiongkuo Min", "Huiyu Duan", "Guangtao Zhai"], "title": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition", "comment": null, "summary": "Existing benchmarks for large multimodal models (LMMs) often fail to capture\ntheir performance in real-time, adversarial environments. We introduce LM Fight\nArena (Large Model Fight Arena), a novel framework that evaluates LMMs by\npitting them against each other in the classic fighting game Mortal Kombat II,\na task requiring rapid visual understanding and tactical, sequential\ndecision-making. In a controlled tournament, we test six leading open- and\nclosed-source models, where each agent operates controlling the same character\nto ensure a fair comparison. The models are prompted to interpret game frames\nand state data to select their next actions. Unlike static evaluations, LM\nFight Arena provides a fully automated, reproducible, and objective assessment\nof an LMM's strategic reasoning capabilities in a dynamic setting. This work\nintroduces a challenging and engaging benchmark that bridges the gap between AI\nevaluation and interactive entertainment.", "AI": {"tldr": "LM Fight Arena\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u5728\u683c\u6597\u6e38\u620f\u300a\u771f\u4eba\u5feb\u6253II\u300b\u4e2d\u5bf9\u6218\u6765\u6d4b\u8bd5\u5176\u89c6\u89c9\u7406\u89e3\u548c\u6218\u7565\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u65e0\u6cd5\u6355\u6349\u5176\u5728\u5b9e\u65f6\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u9700\u8981\u521b\u5efa\u66f4\u52a8\u6001\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5728\u53d7\u63a7\u7684\u9526\u6807\u8d5b\u4e2d\u6d4b\u8bd5\u516d\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u6bcf\u4e2a\u6a21\u578b\u63a7\u5236\u76f8\u540c\u89d2\u8272\u4ee5\u786e\u4fdd\u516c\u5e73\u6bd4\u8f83\uff0c\u901a\u8fc7\u89e3\u6790\u6e38\u620f\u753b\u9762\u548c\u72b6\u6001\u6570\u636e\u6765\u9009\u62e9\u884c\u52a8\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u73b0\u4e14\u5ba2\u89c2\u7684\u8bc4\u4f30\uff0c\u80fd\u591f\u8861\u91cf\u591a\u6a21\u6001\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "LM Fight Arena\u5f15\u5165\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5438\u5f15\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f25\u5408\u4e86AI\u8bc4\u4f30\u4e0e\u4ea4\u4e92\u5a31\u4e50\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2510.08850", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08850", "abs": "https://arxiv.org/abs/2510.08850", "authors": ["Vasudha Yanuganti", "Ishaan Puri", "Swapnil Chhatre", "Mantinder Singh", "Ashok Jallepalli", "Hritvik Shrivastava", "Pradeep Kumar Sharma"], "title": "Repository-Aware File Path Retrieval via Fine-Tuned LLMs", "comment": null, "summary": "Modern codebases make it hard for developers and AI coding assistants to find\nthe right source files when answering questions like \"How does this feature\nwork?\" or \"Where was the bug introduced?\" Traditional code search (keyword or\nIR based) often misses semantic context and cross file links, while large\nlanguage models (LLMs) understand natural language but lack repository specific\ndetail. We present a method for file path retrieval that fine tunes a strong\nLLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file\npaths directly from a natural language query. To build training data, we\nintroduce six code aware strategies that use abstract syntax tree (AST)\nstructure and repository content to generate realistic question-answer pairs,\nwhere answers are sets of file paths. The strategies range from single file\nprompts to hierarchical repository summaries, providing broad coverage. We fine\ntune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,\nand obtain high retrieval accuracy: up to 91\\% exact match and 93\\% recall on\nheld out queries, clearly beating single strategy training. On a large codebase\nlike PyTorch (about 4,000 Python files), the model reaches 59\\% recall, showing\nscalability. We analyze how multi level code signals help the LLM reason over\ncross file context and discuss dataset design, limits (for example, context\nlength in very large repos), and future integration of retrieval with LLM based\ncode intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eQLoRA\u548cUnsloth\u4f18\u5316\u7684\u6587\u4ef6\u8def\u5f84\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03Qwen3-8B\u6a21\u578b\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u9884\u6d4b\u76f8\u5173\u6587\u4ef6\u8def\u5f84\uff0c\u5728\u591a\u4e2aPython\u9879\u76ee\u4e2d\u8fbe\u5230\u9ad8\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u8de8\u6587\u4ef6\u94fe\u63a5\u7406\u89e3\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u4f46\u7f3a\u4e4f\u4ed3\u5e93\u7279\u5b9a\u7ec6\u8282\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u6587\u4ef6\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u516d\u79cd\u57fa\u4e8e\u62bd\u8c61\u8bed\u6cd5\u6811\u548c\u4ed3\u5e93\u5185\u5bb9\u7684\u4ee3\u7801\u611f\u77e5\u7b56\u7565\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7QLoRA\u548cUnsloth\u4f18\u5316\u5fae\u8c03Qwen3-8B\u6a21\u578b\u8fdb\u884c\u6587\u4ef6\u8def\u5f84\u9884\u6d4b\u3002", "result": "\u5728Flask\u3001Click\u7b49\u9879\u76ee\u4e2d\u8fbe\u523091%\u7cbe\u786e\u5339\u914d\u548c93%\u53ec\u56de\u7387\uff0c\u5728PyTorch\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u8fbe\u523059%\u53ec\u56de\u7387\uff0c\u660e\u663e\u4f18\u4e8e\u5355\u7b56\u7565\u8bad\u7ec3\u3002", "conclusion": "\u591a\u7ea7\u4ee3\u7801\u4fe1\u53f7\u5e2e\u52a9LLM\u63a8\u7406\u8de8\u6587\u4ef6\u4e0a\u4e0b\u6587\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u672a\u6765\u53ef\u4e0e\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u667a\u80fd\u7cfb\u7edf\u96c6\u6210\u3002", "topic": "code agent"}}
{"id": "2510.08981", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08981", "abs": "https://arxiv.org/abs/2510.08981", "authors": ["Mandira Roy", "Novarun Deb", "Nabendu Chaki", "Agostino Cortesi"], "title": "SEER: Sustainability Enhanced Engineering of Software Requirements", "comment": "Main Paper: 32 pages, References: 3 pages, Appendix: 13 pages.\n  Submitted to the Journal of Systems and Software, Elsevier", "summary": "The rapid expansion of software development has significant environmental,\ntechnical, social, and economic impacts. Achieving the United Nations\nSustainable Development Goals by 2030 compels developers to adopt sustainable\npractices. Existing methods mostly offer high-level guidelines, which are\ntime-consuming to implement and rely on team adaptability. Moreover, they focus\non design or implementation, while sustainability assessment should start at\nthe requirements engineering phase. In this paper, we introduce SEER, a\nframework which addresses sustainability concerns in the early software\ndevelopment phase. The framework operates in three stages: (i) it identifies\nsustainability requirements (SRs) relevant to a specific software product from\na general taxonomy; (ii) it evaluates how sustainable system requirements are\nbased on the identified SRs; and (iii) it optimizes system requirements that\nfail to satisfy any SR. The framework is implemented using the reasoning\ncapabilities of large language models and the agentic RAG (Retrieval Augmented\nGeneration) approach. SEER has been experimented on four software projects from\ndifferent domains. Results generated using Gemini 2.5 reasoning model\ndemonstrate the effectiveness of the proposed approach in accurately\nidentifying a broad range of sustainability concerns across diverse domains.", "AI": {"tldr": "SEER\u662f\u4e00\u4e2a\u5728\u8f6f\u4ef6\u5f00\u53d1\u65e9\u671f\u9636\u6bb5\u89e3\u51b3\u53ef\u6301\u7eed\u6027\u95ee\u9898\u7684\u6846\u67b6\uff0c\u4f7f\u7528LLM\u63a8\u7406\u80fd\u529b\u548c\u4ee3\u7406RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u53ef\u6301\u7eed\u6027\u9700\u6c42\u6765\u63d0\u5347\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u73b0\u6709\u53ef\u6301\u7eed\u6027\u65b9\u6cd5\u591a\u4e3a\u9ad8\u5c42\u6307\u5357\uff0c\u5b9e\u65bd\u8017\u65f6\u4e14\u4f9d\u8d56\u56e2\u961f\u9002\u5e94\u6027\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u8bbe\u8ba1\u6216\u5b9e\u73b0\u9636\u6bb5\uff0c\u800c\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u5e94\u4ece\u9700\u6c42\u5de5\u7a0b\u9636\u6bb5\u5f00\u59cb\u3002", "method": "SEER\u6846\u67b6\u5206\u4e09\u4e2a\u9636\u6bb5\uff1a\u4ece\u901a\u7528\u5206\u7c7b\u6cd5\u4e2d\u8bc6\u522b\u7279\u5b9a\u8f6f\u4ef6\u4ea7\u54c1\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff1b\u57fa\u4e8e\u8bc6\u522b\u7684SR\u8bc4\u4f30\u7cfb\u7edf\u9700\u6c42\u7684\u53ef\u6301\u7eed\u6027\uff1b\u4f18\u5316\u672a\u80fd\u6ee1\u8db3\u4efb\u4f55SR\u7684\u7cfb\u7edf\u9700\u6c42\u3002\u4f7f\u7528LLM\u63a8\u7406\u80fd\u529b\u548c\u4ee3\u7406RAG\u65b9\u6cd5\u5b9e\u73b0\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u8f6f\u4ef6\u9879\u76ee\u4e0a\u5b9e\u9a8c\uff0c\u4f7f\u7528Gemini 2.5\u63a8\u7406\u6a21\u578b\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51c6\u786e\u8bc6\u522b\u8de8\u9886\u57df\u7684\u5e7f\u6cdb\u53ef\u6301\u7eed\u6027\u95ee\u9898\u3002", "conclusion": "SEER\u6846\u67b6\u5728\u8f6f\u4ef6\u5f00\u53d1\u65e9\u671f\u9636\u6bb5\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u6301\u7eed\u6027\u95ee\u9898\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u548c\u4f18\u5316\u53ef\u6301\u7eed\u6027\u9700\u6c42\u3002", "topic": "swe application"}}
{"id": "2510.08996", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08996", "abs": "https://arxiv.org/abs/2510.08996", "authors": ["Spandan Garg", "Ben Steenhoek", "Yufan Huang"], "title": "Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation", "comment": null, "summary": "Current benchmarks for evaluating software engineering agents, such as\nSWE-Bench Verified, are predominantly derived from GitHub issues and fail to\naccurately reflect how developers interact with chat-based coding assistants in\nintegrated development environments (IDEs). We posit that this mismatch leads\nto a systematic overestimation of agent's capabilities in real-world scenarios,\nespecially bug fixing. We introduce a novel benchmarking framework that\ntransforms existing formal benchmarks into realistic user queries through\nsystematic analysis of developer interaction patterns with chat-based agents.\nOur methodology is flexible and can be easily extended to existing benchmarks.\nIn this paper, we apply our testing framework to SWE-Bench Verified, the\nTypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and\ntransform formal GitHub issue descriptions into realistic user-style queries\nbased on telemetry analysis of a popular chat-based agent interactions. Our\nfindings reveal that existing benchmarks significantly overestimate agent\ncapabilities for some models by >50% over baseline performance for public\nbenchmarks and ~10-16% for our internal benchmark. This work establishes a new\nparadigm for evaluating interactive chat-based software engineering agents\nthrough benchmark mutation techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5c06\u73b0\u6709\u7684\u6b63\u5f0f\u57fa\u51c6\u8f6c\u6362\u4e3a\u66f4\u771f\u5b9e\u7684\u7528\u6237\u67e5\u8be2\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u663e\u8457\u9ad8\u4f30\u4e86\u804a\u5929\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eGitHub\u95ee\u9898\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5f00\u53d1\u8005\u5728IDE\u4e2d\u4e0e\u804a\u5929\u5f0f\u7f16\u7801\u52a9\u624b\u7684\u5b9e\u9645\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5bfc\u81f4\u5bf9\u4ee3\u7406\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u9ad8\u4f30\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f00\u53d1\u8005\u4e0e\u804a\u5929\u5f0f\u4ee3\u7406\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u65b9\u6cd5\u8bba\uff0c\u5c06\u73b0\u6709\u6b63\u5f0f\u57fa\u51c6\u8f6c\u6362\u4e3a\u771f\u5b9e\u7528\u6237\u98ce\u683c\u7684\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u663e\u8457\u9ad8\u4f30\u4e86\u4ee3\u7406\u80fd\u529b\uff0c\u516c\u5f00\u57fa\u51c6\u4e2d\u67d0\u4e9b\u6a21\u578b\u7684\u6027\u80fd\u88ab\u9ad8\u4f30\u8d85\u8fc750%\uff0c\u5185\u90e8\u57fa\u51c6\u4e2d\u9ad8\u4f30\u7ea610-16%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u57fa\u51c6\u53d8\u5f02\u6280\u672f\u4e3a\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u804a\u5929\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "swe benchmark"}}
{"id": "2510.09045", "categories": ["cs.SE", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09045", "abs": "https://arxiv.org/abs/2510.09045", "authors": ["Manojit Chakraborty", "Madhusudan Ghosh", "Rishabh Gupta"], "title": "Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements", "comment": null, "summary": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u957f\u6807\u8bc6\u7b26\u66ff\u6362\u4e3a\u901a\u7528\u5360\u4f4d\u7b26\u6765\u51cf\u5c11token\u6570\u91cf\uff0c\u63d0\u9ad8\u957f\u4ee3\u7801\u7ffb\u8bd1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "LLMs\u5728\u5904\u7406\u8d85\u51fa\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u957f\u6e90\u4ee3\u7801\u65f6\u7ffb\u8bd1\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u89e3\u51b3\u957f\u4ee3\u7801\u7ffb\u8bd1\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6807\u8bc6\u7b26\u66ff\u6362\u6280\u672f\uff0c\u5c06\u7528\u6237\u7ed9\u5b9a\u7684\u957f\u6807\u8bc6\u7b26\u66ff\u6362\u4e3a\u901a\u7528\u5360\u4f4d\u7b26\uff0c\u8ba9LLM\u4e13\u6ce8\u4e8e\u4ee3\u7801\u903b\u8f91\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u8bed\u6cd5\u548c\u5c42\u6b21\u7ed3\u6784\u4fe1\u606f\uff0c\u51cf\u5c11\u4e86\u7ffb\u8bd1\u7ed3\u679c\u4e2d\u7684token\u6570\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6807\u8bc6\u7b26\u66ff\u6362\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u957f\u4ee3\u7801\u7ffb\u8bd1\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "topic": "swe application"}}
{"id": "2510.09058", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09058", "abs": "https://arxiv.org/abs/2510.09058", "authors": ["Italo Santos", "Cleyton Magalhaes", "Ronnie de Souza Santos"], "title": "Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding", "comment": null, "summary": "Large Language Models have quickly become a central component of modern\nsoftware development workflows, and software practitioners are increasingly\nintegrating LLMs into various stages of the software development lifecycle.\nDespite the growing presence of LLMs, there is still a limited understanding of\nhow these tools are actually used in practice and how professionals perceive\ntheir benefits and limitations. This paper presents preliminary findings from a\nglobal survey of 131 software practitioners. Our results reveal how LLMs are\nutilized for various coding-specific tasks. Software professionals report\nbenefits such as increased productivity, reduced cognitive load, and faster\nlearning, but also raise concerns about LLMs' inaccurate outputs, limited\ncontext awareness, and associated ethical risks. Most developers treat LLMs as\nassistive tools rather than standalone solutions, reflecting a cautious yet\npractical approach to their integration. Our findings provide an early,\npractitioner-focused perspective on LLM adoption, highlighting key\nconsiderations for future research and responsible use in software engineering.", "AI": {"tldr": "\u5bf9131\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u7684\u5168\u7403\u8c03\u67e5\u663e\u793a\uff0cLLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u88ab\u5e7f\u6cdb\u7528\u4e8e\u7f16\u7801\u4efb\u52a1\uff0c\u5e26\u6765\u751f\u4ea7\u529b\u63d0\u5347\u548c\u8ba4\u77e5\u8d1f\u8377\u51cf\u5c11\uff0c\u4f46\u4e5f\u5b58\u5728\u8f93\u51fa\u4e0d\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u7b49\u62c5\u5fe7\u3002", "motivation": "\u4e86\u89e3LLM\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u4ee5\u53ca\u4e13\u4e1a\u4eba\u58eb\u5bf9\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u770b\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u5168\u7403131\u540d\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c6\u5173\u4e8eLLM\u4f7f\u7528\u60c5\u51b5\u7684\u5b9e\u8bc1\u6570\u636e\u3002", "result": "\u5f00\u53d1\u8005\u4e3b\u8981\u5c06LLM\u7528\u4f5c\u8f85\u52a9\u5de5\u5177\u800c\u975e\u72ec\u7acb\u89e3\u51b3\u65b9\u6848\uff0c\u62a5\u544a\u4e86\u751f\u4ea7\u529b\u63d0\u5347\u3001\u8ba4\u77e5\u8d1f\u8377\u51cf\u5c11\u7b49\u597d\u5904\uff0c\u4f46\u4e5f\u62c5\u5fe7\u8f93\u51fa\u4e0d\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u7406\u89e3\u6709\u9650\u548c\u4f26\u7406\u98ce\u9669\u3002", "conclusion": "LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u9700\u8981\u8c28\u614e\u800c\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u8d1f\u8d23\u4efb\u7684\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2510.09073", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09073", "abs": "https://arxiv.org/abs/2510.09073", "authors": ["Matthew Sotoudeh"], "title": "Literate Tracing", "comment": "examples at https://lair.masot.net/trex . SPLASH Onward 2025", "summary": "As computer systems grow ever larger and more complex, a crucial task in\nsoftware development is for one person (the system expert) to communicate to\nanother (the system novice) how a certain program works. This paper reports on\nthe author's experiences with a paradigm for program documentation that we call\nliterate tracing. A literate trace explains a software system using annotated,\nconcrete execution traces of the system. Literate traces complement both\nin-code comments (which often lack global context) and out-of-band design docs\n(which often lack a concrete connection to the code). We also describe TReX,\nour tool for making literate traces that are interactive, visual, and\nguaranteed by construction to be faithful to the program semantics. We have\nused TReX to write literate traces explaining components of large systems\nsoftware including the Linux kernel, Git source control system, and GCC\ncompiler.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a'\u6587\u5b66\u5316\u8ffd\u8e2a'\u7684\u7a0b\u5e8f\u6587\u6863\u8303\u5f0f\uff0c\u901a\u8fc7\u5e26\u6ce8\u91ca\u7684\u5177\u4f53\u6267\u884c\u8ffd\u8e2a\u6765\u89e3\u91ca\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u5e76\u5f00\u53d1\u4e86TReX\u5de5\u5177\u6765\u521b\u5efa\u4ea4\u4e92\u5f0f\u3001\u53ef\u89c6\u5316\u7684\u6587\u5b66\u5316\u8ffd\u8e2a\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u590d\u6742\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u7cfb\u7edf\u4e13\u5bb6\u5411\u65b0\u624b\u89e3\u91ca\u7a0b\u5e8f\u5de5\u4f5c\u539f\u7406\u65f6\uff0c\u73b0\u6709\u6587\u6863\u65b9\u6cd5\uff08\u4ee3\u7801\u6ce8\u91ca\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u8bbe\u8ba1\u6587\u6863\u7f3a\u4e4f\u4e0e\u4ee3\u7801\u7684\u5177\u4f53\u8fde\u63a5\uff09\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86TReX\u5de5\u5177\uff0c\u901a\u8fc7\u751f\u6210\u4ea4\u4e92\u5f0f\u3001\u53ef\u89c6\u5316\u7684\u6267\u884c\u8ffd\u8e2a\u6765\u521b\u5efa\u6587\u5b66\u5316\u8ffd\u8e2a\u6587\u6863\uff0c\u8fd9\u4e9b\u8ffd\u8e2a\u5728\u6784\u9020\u4e0a\u4fdd\u8bc1\u4e0e\u7a0b\u5e8f\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002", "result": "\u4f7f\u7528TReX\u5de5\u5177\u6210\u529f\u4e3aLinux\u5185\u6838\u3001Git\u6e90\u4ee3\u7801\u63a7\u5236\u7cfb\u7edf\u548cGCC\u7f16\u8bd1\u5668\u7b49\u5927\u578b\u7cfb\u7edf\u8f6f\u4ef6\u7ec4\u4ef6\u7f16\u5199\u4e86\u6587\u5b66\u5316\u8ffd\u8e2a\u6587\u6863\u3002", "conclusion": "\u6587\u5b66\u5316\u8ffd\u8e2a\u4f5c\u4e3a\u4e00\u79cd\u7a0b\u5e8f\u6587\u6863\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u8865\u5145\u73b0\u6709\u6587\u6863\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u66f4\u76f4\u89c2\u548c\u5177\u4f53\u7684\u7a0b\u5e8f\u7406\u89e3\u65b9\u5f0f\u3002", "topic": "swe application"}}
{"id": "2510.09021", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09021", "abs": "https://arxiv.org/abs/2510.09021", "authors": ["Hamed Mahdavi", "Pouria Mahdavinia", "Samira Malek", "Pegah Mohammadipour", "Alireza Hashemi", "Majid Daliri", "Alireza Farhadi", "Amir Khasahmadi", "Niloofar Mireshghallah", "Vasant Honavar"], "title": "RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows", "comment": null, "summary": "State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based\nOlympiad problems to solving most of the IMO 2025 problems, with leading\nsystems reportedly handling 5 of 6 problems. Given this progress, we assess how\nwell these models can grade proofs: detecting errors, judging their severity,\nand assigning fair scores beyond binary correctness. We study proof-analysis\ncapabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we\ngrade on a 1-4 scale with detailed error annotations, and on MathArena solution\nsets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models\ncan reliably flag incorrect (including subtly incorrect) solutions but exhibit\ncalibration gaps in how partial credit is assigned. To address this, we\nintroduce agentic workflows that extract and analyze reference solutions and\nautomatically derive problem-specific rubrics for a multi-step grading process.\nWe instantiate and compare different design choices for the grading workflows,\nand evaluate their trade-offs. Across our annotated corpus and MathArena, our\nproposed workflows achieve higher agreement with human grades and more\nconsistent handling of partial credit across metrics. We release all code,\ndata, and prompts/logs to facilitate future research.", "AI": {"tldr": "\u8bc4\u4f30LLMs\u5728\u6570\u5b66\u8bc1\u660e\u8bc4\u5206\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u62ec\u9519\u8bef\u68c0\u6d4b\u3001\u4e25\u91cd\u6027\u5224\u65ad\u548c\u5206\u6570\u5206\u914d\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u7684\u8bc4\u5206\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u4e0e\u4eba\u5de5\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728\u89e3\u51b3\u5965\u6797\u5339\u514b\u6570\u5b66\u95ee\u9898\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u5728\u8bc1\u660e\u8bc4\u5206\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u68c0\u6d4b\u9519\u8bef\u3001\u5224\u65ad\u4e25\u91cd\u6027\u548c\u5206\u914d\u516c\u5e73\u5206\u6570\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e8c\u5143\u6b63\u786e\u6027\u5224\u65ad\u3002", "method": "\u4f7f\u752890\u4e2aGemini 2.5 Pro\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u548cMathArena\u7684IMO/USAMO 2025\u89e3\u51b3\u65b9\u6848\u96c6\uff0c\u5f15\u5165\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u6765\u63d0\u53d6\u548c\u5206\u6790\u53c2\u8003\u89e3\u51b3\u65b9\u6848\uff0c\u81ea\u52a8\u63a8\u5bfc\u95ee\u9898\u7279\u5b9a\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u5e76\u8fdb\u884c\u591a\u6b65\u9aa4\u8bc4\u5206\u8fc7\u7a0b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u53ef\u9760\u5730\u6807\u8bb0\u9519\u8bef\u89e3\u51b3\u65b9\u6848\uff08\u5305\u62ec\u5fae\u5999\u9519\u8bef\uff09\uff0c\u4f46\u5728\u5206\u914d\u90e8\u5206\u5206\u6570\u65b9\u9762\u5b58\u5728\u6821\u51c6\u5dee\u8ddd\u3002\u63d0\u51fa\u7684\u5de5\u4f5c\u6d41\u7a0b\u5728\u6ce8\u91ca\u8bed\u6599\u5e93\u548cMathArena\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u4eba\u5de5\u8bc4\u5206\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5904\u7406\u90e8\u5206\u5206\u6570\u65b9\u9762\u66f4\u52a0\u4e00\u81f4\u3002", "conclusion": "\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8LLMs\u5728\u6570\u5b66\u8bc1\u660e\u8bc4\u5206\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u90e8\u5206\u5206\u6570\u548c\u4e0e\u4eba\u5de5\u8bc4\u5206\u4e00\u81f4\u6027\u65b9\u9762\u3002", "topic": "agent analysis"}}
{"id": "2510.09108", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09108", "abs": "https://arxiv.org/abs/2510.09108", "authors": ["Lukas Krodinger", "Altin Hajdari", "Stephan Lukasczyk", "Gordon Fraser"], "title": "Constraint-Guided Unit Test Generation for Machine Learning Libraries", "comment": "Accepted for SSBSE 2025", "summary": "Machine learning (ML) libraries such as PyTorch and TensorFlow are essential\nfor a wide range of modern applications. Ensuring the correctness of ML\nlibraries through testing is crucial. However, ML APIs often impose strict\ninput constraints involving complex data structures such as tensors. Automated\ntest generation tools such as Pynguin are not aware of these constraints and\noften create non-compliant inputs. This leads to early test failures and\nlimited code coverage. Prior work has investigated extracting constraints from\nofficial API documentation. In this paper, we present PynguinML, an approach\nthat improves the Pynguin test generator to leverage these constraints to\ngenerate compliant inputs for ML APIs, enabling more thorough testing and\nhigher code coverage. Our evaluation is based on 165 modules from PyTorch and\nTensorFlow, comparing PynguinML against Pynguin. The results show that\nPynguinML significantly improves test effectiveness, achieving up to 63.9 %\nhigher code coverage.", "AI": {"tldr": "PynguinML\u901a\u8fc7\u4eceAPI\u6587\u6863\u4e2d\u63d0\u53d6\u7ea6\u675f\u6761\u4ef6\uff0c\u6539\u8fdbPynguin\u6d4b\u8bd5\u751f\u6210\u5668\uff0c\u4e3a\u673a\u5668\u5b66\u4e60API\u751f\u6210\u5408\u89c4\u8f93\u5165\uff0c\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387", "motivation": "\u673a\u5668\u5b66\u4e60\u5e93\u7684API\u901a\u5e38\u6709\u4e25\u683c\u7684\u8f93\u5165\u7ea6\u675f\uff0c\u6d89\u53ca\u5f20\u91cf\u7b49\u590d\u6742\u6570\u636e\u7ed3\u6784\u3002\u73b0\u6709\u6d4b\u8bd5\u751f\u6210\u5de5\u5177\u4e0d\u4e86\u89e3\u8fd9\u4e9b\u7ea6\u675f\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u5931\u8d25\u548c\u4ee3\u7801\u8986\u76d6\u7387\u6709\u9650", "method": "\u57fa\u4e8ePynguin\u6d4b\u8bd5\u751f\u6210\u5668\uff0c\u4ece\u5b98\u65b9API\u6587\u6863\u4e2d\u63d0\u53d6\u7ea6\u675f\u6761\u4ef6\uff0c\u5229\u7528\u8fd9\u4e9b\u7ea6\u675f\u751f\u6210\u7b26\u5408\u8981\u6c42\u7684\u8f93\u5165\u6570\u636e", "result": "\u5728PyTorch\u548cTensorFlow\u7684165\u4e2a\u6a21\u5757\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4Pynguin\uff0cPynguinML\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u6548\u679c\uff0c\u4ee3\u7801\u8986\u76d6\u7387\u6700\u9ad8\u63d0\u534763.9%", "conclusion": "\u901a\u8fc7\u5229\u7528API\u6587\u6863\u4e2d\u7684\u7ea6\u675f\u6761\u4ef6\uff0cPynguinML\u80fd\u591f\u66f4\u6709\u6548\u5730\u6d4b\u8bd5\u673a\u5668\u5b66\u4e60\u5e93\uff0c\u751f\u6210\u5408\u89c4\u8f93\u5165\u5e76\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387", "topic": "swe application"}}
{"id": "2510.08621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08621", "abs": "https://arxiv.org/abs/2510.08621", "authors": ["Wen-Yu Chang", "Tzu-Hung Huang", "Chih-Ho Chen", "Yun-Nung Chen"], "title": "From Simulation to Strategy: Automating Personalized Interaction Planning for Conversational Agents", "comment": null, "summary": "Amid the rapid rise of agentic dialogue models, realistic user-simulator\nstudies are essential for tuning effective conversation strategies. This work\ninvestigates a sales-oriented agent that adapts its dialogue based on user\nprofiles spanning age, gender, and occupation. While age and gender influence\noverall performance, occupation produces the most pronounced differences in\nconversational intent. Leveraging this insight, we introduce a lightweight,\noccupation-conditioned strategy that guides the agent to prioritize intents\naligned with user preferences, resulting in shorter and more successful\ndialogues. Our findings highlight the importance of rich simulator profiles and\ndemonstrate how simple persona-informed strategies can enhance the\neffectiveness of sales-oriented dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u804c\u4e1a\u4fe1\u606f\u7684\u8f7b\u91cf\u7ea7\u5bf9\u8bdd\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5148\u8003\u8651\u4e0e\u7528\u6237\u504f\u597d\u4e00\u81f4\u7684\u5bf9\u8bdd\u610f\u56fe\uff0c\u5b9e\u73b0\u4e86\u66f4\u77ed\u4e14\u66f4\u6210\u529f\u7684\u9500\u552e\u5bf9\u8bdd\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u5bf9\u8bdd\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u771f\u5b9e\u7528\u6237\u6a21\u62df\u7814\u7a76\u6765\u4f18\u5316\u6709\u6548\u7684\u5bf9\u8bdd\u7b56\u7565\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u4e8e\u7528\u6237\u753b\u50cf\uff08\u5e74\u9f84\u3001\u6027\u522b\u3001\u804c\u4e1a\uff09\u7684\u9500\u552e\u5bfc\u5411\u5bf9\u8bdd\u4ee3\u7406\u3002", "method": "\u7814\u7a76\u9500\u552e\u5bfc\u5411\u4ee3\u7406\u6839\u636e\u7528\u6237\u5e74\u9f84\u3001\u6027\u522b\u548c\u804c\u4e1a\u8c03\u6574\u5bf9\u8bdd\u7b56\u7565\u3002\u5f15\u5165\u8f7b\u91cf\u7ea7\u804c\u4e1a\u6761\u4ef6\u7b56\u7565\uff0c\u5f15\u5bfc\u4ee3\u7406\u4f18\u5148\u8003\u8651\u4e0e\u7528\u6237\u504f\u597d\u4e00\u81f4\u7684\u5bf9\u8bdd\u610f\u56fe\u3002", "result": "\u5e74\u9f84\u548c\u6027\u522b\u5f71\u54cd\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u804c\u4e1a\u5728\u5bf9\u8bdd\u610f\u56fe\u4e0a\u4ea7\u751f\u6700\u663e\u8457\u5dee\u5f02\u3002\u4f7f\u7528\u804c\u4e1a\u6761\u4ef6\u7b56\u7565\u540e\uff0c\u5bf9\u8bdd\u66f4\u77ed\u4e14\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e30\u5bcc\u6a21\u62df\u5668\u914d\u7f6e\u6587\u4ef6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u7b80\u5355\u57fa\u4e8e\u4eba\u7269\u753b\u50cf\u7684\u7b56\u7565\u53ef\u4ee5\u63d0\u5347\u9500\u552e\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.09038", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09038", "abs": "https://arxiv.org/abs/2510.09038", "authors": ["Wenyi Wu", "Kun Zhou", "Ruoxin Yuan", "Vivian Yu", "Stephen Wang", "Zhiting Hu", "Biwei Huang"], "title": "Auto-scaling Continuous Memory for GUI Agent", "comment": null, "summary": "We study how to endow GUI agents with scalable memory that help generalize\nacross unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress\npast trajectories into text tokens, which balloons context length and misses\ndecisive visual cues (e.g., exact widget size and position). We propose a\ncontinuous memory that encodes each GUI trajectory into a fixed-length sequence\nof continuous embeddings using the VLM itself as an encoder; these embeddings\nare plugged directly into the backbone's input layer, sharply reducing context\ncost while preserving fine-grained visual information. As memory size and\nretrieval depth increase, performance improves monotonically, unlike text\nmemories that degrade with long prompts. To grow memory at low cost, we\nintroduce an auto-scaling data flywheel that (i) discovers new environments via\nsearch, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out\ntrajectories with the agent, and (iv) verifies success with the same VLM. Using\nthis pipeline, we collect 100k+ trajectories for about \\$4000 and fine-tune\nonly the memory encoder (LoRA on a Q-Former, 1.2\\% parameters) with 1,500\nsamples. On real-world GUI benchmarks, our memory-augmented agent consistently\nimproves success rates under long horizons and distribution shifts. Notably,\nQwen-2.5-VL-7B + continuous memory achieves performance comparable to\nstate-of-the-art closed-source models (e.g., GPT-4o, Claude-4).", "AI": {"tldr": "\u63d0\u51fa\u8fde\u7eed\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06GUI\u8f68\u8ff9\u7f16\u7801\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684\u8fde\u7eed\u5d4c\u5165\uff0c\u663e\u8457\u51cf\u5c11\u4e0a\u4e0b\u6587\u6210\u672c\u5e76\u4fdd\u7559\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u3002\u901a\u8fc7\u81ea\u52a8\u6269\u5c55\u6570\u636e\u98de\u8f6e\u6536\u96c610\u4e07+\u8f68\u8ff9\uff0c\u4ec5\u5fae\u8c031.2%\u53c2\u6570\uff0c\u5728\u771f\u5b9eGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u957f\u89c6\u91ce\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u5c06\u5386\u53f2\u8f68\u8ff9\u538b\u7f29\u4e3a\u6587\u672c\u6807\u8bb0\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u957f\u5ea6\u81a8\u80c0\u4e14\u4e22\u5931\u5173\u952e\u89c6\u89c9\u7ebf\u7d22\uff08\u5982\u7cbe\u786e\u7684\u63a7\u4ef6\u5927\u5c0f\u548c\u4f4d\u7f6e\uff09\u3002\u9700\u8981\u53ef\u6269\u5c55\u7684\u8bb0\u5fc6\u673a\u5236\u6765\u5e2e\u52a9\u4ee3\u7406\u5728\u964c\u751f\u754c\u9762\u548c\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u6cdb\u5316\u3002", "method": "\u4f7f\u7528VLM\u4f5c\u4e3a\u7f16\u7801\u5668\u5c06GUI\u8f68\u8ff9\u7f16\u7801\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684\u8fde\u7eed\u5d4c\u5165\uff0c\u76f4\u63a5\u63d2\u5165\u4e3b\u5e72\u7f51\u7edc\u8f93\u5165\u5c42\u3002\u5f15\u5165\u81ea\u52a8\u6269\u5c55\u6570\u636e\u98de\u8f6e\uff1a\u641c\u7d22\u53d1\u73b0\u65b0\u73af\u5883\u3001\u4f7f\u7528\u5f00\u6e90VLM\u5408\u6210\u4efb\u52a1\u3001\u4ee3\u7406\u6267\u884c\u8f68\u8ff9\u3001\u76f8\u540cVLM\u9a8c\u8bc1\u6210\u529f\u3002\u4ec5\u5fae\u8c03\u8bb0\u5fc6\u7f16\u7801\u5668\uff08LoRA on Q-Former\uff0c1.2%\u53c2\u6570\uff09\u3002", "result": "\u968f\u7740\u8bb0\u5fc6\u5927\u5c0f\u548c\u68c0\u7d22\u6df1\u5ea6\u589e\u52a0\uff0c\u6027\u80fd\u5355\u8c03\u63d0\u5347\uff0c\u800c\u6587\u672c\u8bb0\u5fc6\u5728\u957f\u63d0\u793a\u4e0b\u4f1a\u9000\u5316\u3002\u5728\u771f\u5b9eGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\u5728\u957f\u89c6\u91ce\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u6301\u7eed\u63d0\u5347\u6210\u529f\u7387\u3002Qwen-2.5-VL-7B + \u8fde\u7eed\u8bb0\u5fc6\u8fbe\u5230\u4e0eGPT-4o\u3001Claude-4\u7b49\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8fde\u7eed\u8bb0\u5fc6\u673a\u5236\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4e0a\u4e0b\u6587\u6210\u672c\uff0c\u4fdd\u7559\u89c6\u89c9\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u98de\u8f6e\u5b9e\u73b0\u4f4e\u6210\u672c\u6269\u5c55\u3002\u8be5\u65b9\u6cd5\u4f7f\u5f00\u6e90\u6a21\u578b\u5728GUI\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u9876\u7ea7\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.09400", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09400", "abs": "https://arxiv.org/abs/2510.09400", "authors": ["He Jiang", "Yufu Wang", "Hao Lin", "Peiyu Zou", "Zhide Zhou", "Ang Jia", "Xiaochen Li", "Zhilei Ren"], "title": "TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance in automated\nsource-to-target code translation through pretraining on extensive code\ncorpora. However, mainstream LLM-based code translation methods suffer from two\ncritical limitations. First, they are highly sensitive to language-specific\nfeatures, which often introduce source-language syntax or lexicon into the\noutput, leading to syntactic confusion. Second, they lack fine-grained semantic\nalignment due to an over-reliance on function-level parallel datasets,\nresulting in semantic misalignment between the translated code and the original\nsource. To overcome these limitations, we propose TIT, a Tree-structured\nInstruction Tuning paradigm for LLM-based code translation. Specifically, TIT\nconsists of three modules. First, to mitigate syntactic confusion, the\nsyntactic information representation module integrates language-agnostic\nsyntactic features via structured parsing. Then, to generate high-quality\nfine-grained parallel data, the fine-grained parallel dataset augmentation\nmodule aligns nodes with code segments through statement-level segmentation and\ncontrastive matching. Finally, we leverage the dual-stage tree instruction\ntuning module to alleviate the contextual processing burden on the LLM caused\nby the introduction of syntactic information. The first stage employs\nsyntax-aware fine-tuning to enable the LLM to autonomously comprehend\nstructured syntactic information, while the second stage utilizes code\ngeneration fine-tuning to guide the model in generating accurate target code\nbased on function-level syntactic dependencies. The experimental results\ndemonstrate that the proposed method significantly outperforms existing\napproaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in\ncode translation while markedly reducing syntactic confusion.", "AI": {"tldr": "\u63d0\u51faTIT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u6307\u4ee4\u8c03\u4f18\u89e3\u51b3LLM\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u8bed\u6cd5\u6df7\u6dc6\u548c\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6210\u529f\u7387", "motivation": "\u73b0\u6709LLM\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\u5bf9\u8bed\u8a00\u7279\u5b9a\u7279\u5f81\u654f\u611f\uff0c\u5bb9\u6613\u5f15\u5165\u6e90\u8bed\u8a00\u8bed\u6cd5\uff0c\u4e14\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u8bed\u6cd5\u6df7\u6dc6\u548c\u8bed\u4e49\u504f\u5dee", "method": "TIT\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u8bed\u6cd5\u4fe1\u606f\u8868\u793a\u6a21\u5757\u96c6\u6210\u8bed\u8a00\u65e0\u5173\u8bed\u6cd5\u7279\u5f81\uff1b\u7ec6\u7c92\u5ea6\u5e76\u884c\u6570\u636e\u589e\u5f3a\u6a21\u5757\u901a\u8fc7\u8bed\u53e5\u7ea7\u5206\u5272\u548c\u5bf9\u9f50\u5339\u914d\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff1b\u53cc\u9636\u6bb5\u6811\u6307\u4ee4\u8c03\u4f18\u6a21\u5757\u51cf\u8f7bLLM\u4e0a\u4e0b\u6587\u5904\u7406\u8d1f\u62c5", "result": "\u5728\u591a\u4e2aLLM\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u7ffb\u8bd1\u6210\u529f\u7387\u63d0\u9ad81.22-1.75\u500d\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bed\u6cd5\u6df7\u6dc6", "conclusion": "TIT\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bed\u6cd5\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf\u548c\u51c6\u786e\u6027", "topic": "code agent"}}
{"id": "2510.09049", "categories": ["cs.AI", "cs.SE", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09049", "abs": "https://arxiv.org/abs/2510.09049", "authors": ["Joonghyuk Hahn", "Soohan Lim", "Yo-Sub Han"], "title": "MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction", "comment": "24 pages, 11 figures, 10 tables", "summary": "Predicting the complexity of source code is essential for software\ndevelopment and algorithm analysis. Recently, Baik et al. (2025) introduced\nCodeComplex for code time complexity prediction. The paper shows that LLMs\nwithout fine-tuning struggle with certain complexity classes. This suggests\nthat no single LLM excels at every class, but rather each model shows\nadvantages in certain classes. We propose MEC$^3$O, a multi-expert consensus\nsystem, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs\nto complexity classes based on their performance and provides them with\nclass-specialized instructions, turning them into experts. These experts engage\nin structured debates, and their predictions are integrated through a weighted\nconsensus mechanism. Our expertise assignments to LLMs effectively handle\nDegeneration-of-Thought, reducing reliance on a separate judge model, and\npreventing convergence to incorrect majority opinions. Experiments on\nCodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving\nat least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini\nin macro-F1 scores on average and demonstrates competitive on-par F1 scores to\nGPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of\nmulti-expert debates and weight consensus strategy to generate the final\npredictions. Our code and data is available at\nhttps://github.com/suhanmen/MECO.", "AI": {"tldr": "\u63d0\u51fa\u4e86MEC^3O\u591a\u4e13\u5bb6\u5171\u8bc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06LLM\u5206\u914d\u5230\u4e0d\u540c\u590d\u6742\u5ea6\u7c7b\u522b\u5e76\u8ba9\u4e13\u5bb6\u8fdb\u884c\u7ed3\u6784\u5316\u8fa9\u8bba\uff0c\u4f7f\u7528\u52a0\u6743\u5171\u8bc6\u673a\u5236\u6574\u5408\u9884\u6d4b\uff0c\u5728\u4ee3\u7801\u590d\u6742\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u5728\u4ee3\u7801\u65f6\u95f4\u590d\u6742\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u7c7b\u522b\u4e0a\u5404\u6709\u4f18\u52bf\uff0c\u4f46\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u7c7b\u522b\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\u3002", "method": "MEC^3O\u591a\u4e13\u5bb6\u5171\u8bc6\u7cfb\u7edf\uff1a\u57fa\u4e8e\u6027\u80fd\u5c06LLM\u5206\u914d\u5230\u7279\u5b9a\u590d\u6742\u5ea6\u7c7b\u522b\uff0c\u63d0\u4f9b\u7c7b\u522b\u4e13\u4e1a\u5316\u6307\u4ee4\uff0c\u8ba9\u4e13\u5bb6\u8fdb\u884c\u7ed3\u6784\u5316\u8fa9\u8bba\uff0c\u901a\u8fc7\u52a0\u6743\u5171\u8bc6\u673a\u5236\u6574\u5408\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5728CodeComplex\u6570\u636e\u96c6\u4e0a\uff0cMEC^3O\u6bd4\u5f00\u6e90\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u548cmacro-F1\u5206\u6570\u81f3\u5c11\u63d0\u9ad810%\uff0c\u5728macro-F1\u5206\u6570\u4e0a\u5e73\u5747\u8d85\u8fc7GPT-4o-mini\uff0c\u4e0eGPT-4o\u548cGPT-o4-mini\u7684F1\u5206\u6570\u76f8\u5f53\u3002", "conclusion": "\u591a\u4e13\u5bb6\u8fa9\u8bba\u548c\u52a0\u6743\u5171\u8bc6\u7b56\u7565\u80fd\u6709\u6548\u751f\u6210\u6700\u7ec8\u9884\u6d4b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4ee3\u7801\u590d\u6742\u5ea6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "code agent"}}
{"id": "2510.08768", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08768", "abs": "https://arxiv.org/abs/2510.08768", "authors": ["Francisco Pascoa", "Ian Lalonde", "Alexandre Girard"], "title": "Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem", "comment": null, "summary": "Reinforcement learning (RL) policies often fail to generalize to new robots,\ntasks, or environments with different physical parameters, a challenge that\nlimits their real-world applicability. This paper presents a simple, zero-shot\ntransfer method based on Buckingham's Pi Theorem to address this limitation.\nThe method adapts a pre-trained policy to new system contexts by scaling its\ninputs (observations) and outputs (actions) through a dimensionless space,\nrequiring no retraining. The approach is evaluated against a naive transfer\nbaseline across three environments of increasing complexity: a simulated\npendulum, a physical pendulum for sim-to-real validation, and the\nhigh-dimensional HalfCheetah. Results demonstrate that the scaled transfer\nexhibits no loss of performance on dynamically similar contexts. Furthermore,\non non-similar contexts, the scaled policy consistently outperforms the naive\ntransfer, significantly expanding the volume of contexts where the original\npolicy remains effective. These findings demonstrate that dimensional analysis\nprovides a powerful and practical tool to enhance the robustness and\ngeneralization of RL policies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u767d\u91d1\u6c49\u03c0\u5b9a\u7406\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u91cf\u7eb2\u7a7a\u95f4\u7f29\u653e\u9884\u8bad\u7ec3RL\u7b56\u7565\u7684\u8f93\u5165\u8f93\u51fa\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u65b0\u7cfb\u7edf\u73af\u5883\u3002", "motivation": "\u89e3\u51b3RL\u7b56\u7565\u5728\u9762\u4e34\u4e0d\u540c\u7269\u7406\u53c2\u6570\u7684\u65b0\u673a\u5668\u4eba\u3001\u4efb\u52a1\u6216\u73af\u5883\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u57fa\u4e8e\u767d\u91d1\u6c49\u03c0\u5b9a\u7406\uff0c\u5c06\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u89c2\u6d4b\u503c\u548c\u52a8\u4f5c\u6620\u5c04\u5230\u65e0\u91cf\u7eb2\u7a7a\u95f4\u8fdb\u884c\u7f29\u653e\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684\u73af\u5883\u4e2d\u9a8c\u8bc1\uff1a\u6a21\u62df\u949f\u6446\u3001\u7269\u7406\u949f\u6446\uff08sim-to-real\u9a8c\u8bc1\uff09\u548c\u9ad8\u7ef4HalfCheetah\u3002\u7f29\u653e\u8fc1\u79fb\u5728\u52a8\u6001\u76f8\u4f3c\u73af\u5883\u4e0b\u65e0\u6027\u80fd\u635f\u5931\uff0c\u5728\u975e\u76f8\u4f3c\u73af\u5883\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6734\u7d20\u8fc1\u79fb\u3002", "conclusion": "\u91cf\u7eb2\u5206\u6790\u4e3a\u589e\u5f3aRL\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5f3a\u5927\u5b9e\u7528\u7684\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09087", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09087", "abs": "https://arxiv.org/abs/2510.09087", "authors": ["Zhang Zheng", "Deheng Ye", "Peilin Zhao", "Hao Wang"], "title": "Leading the Follower: Learning Persuasive Agents in Social Deduction Games", "comment": null, "summary": "Large language model (LLM) agents have shown remarkable progress in social\ndeduction games (SDGs). However, existing approaches primarily focus on\ninformation processing and strategy selection, overlooking the significance of\npersuasive communication in influencing other players' beliefs and responses.\nIn SDGs, success depends not only on making correct deductions but on\nconvincing others to response in alignment with one's intent. To address this\nlimitation, we formalize turn-based dialogue in SDGs as a Stackelberg\ncompetition, where the current player acts as the leader who strategically\ninfluences the follower's response. Building on this theoretical foundation, we\npropose a reinforcement learning framework that trains agents to optimize\nutterances for persuasive impact. Through comprehensive experiments across\nthree diverse SDGs, we demonstrate that our agents significantly outperform\nbaselines. This work represents a significant step toward developing AI agents\ncapable of strategic social influence, with implications extending to scenarios\nrequiring persuasive communication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStackelberg\u7ade\u4e89\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u4ee3\u7406\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u4f18\u5316\u8bf4\u670d\u6027\u6c9f\u901a\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u591a\u79cd\u6e38\u620f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u4e3b\u8981\u5173\u6ce8\u4fe1\u606f\u5904\u7406\u548c\u7b56\u7565\u9009\u62e9\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bf4\u670d\u6027\u6c9f\u901a\u5bf9\u5f71\u54cd\u5176\u4ed6\u73a9\u5bb6\u4fe1\u5ff5\u548c\u53cd\u5e94\u7684\u91cd\u8981\u6027\u3002\u6e38\u620f\u6210\u529f\u4e0d\u4ec5\u4f9d\u8d56\u6b63\u786e\u63a8\u7406\uff0c\u66f4\u9700\u8981\u8bf4\u670d\u4ed6\u4eba\u6309\u7167\u81ea\u5df1\u7684\u610f\u56fe\u884c\u52a8\u3002", "method": "\u5c06\u56de\u5408\u5236\u5bf9\u8bdd\u5f62\u5f0f\u5316\u4e3aStackelberg\u7ade\u4e89\uff0c\u5f53\u524d\u73a9\u5bb6\u4f5c\u4e3a\u9886\u5bfc\u8005\u6218\u7565\u6027\u5730\u5f71\u54cd\u8ddf\u968f\u8005\u7684\u54cd\u5e94\u3002\u57fa\u4e8e\u6b64\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u8bad\u7ec3\u4ee3\u7406\u4f18\u5316\u6709\u8bf4\u670d\u529b\u7684\u8868\u8fbe\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u4e2d\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u5f00\u53d1\u5177\u6709\u6218\u7565\u793e\u4ea4\u5f71\u54cd\u529bAI\u4ee3\u7406\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5bf9\u9700\u8981\u8bf4\u670d\u6027\u6c9f\u901a\u7684\u573a\u666f\u5177\u6709\u5e7f\u6cdb\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2510.08647", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08647", "abs": "https://arxiv.org/abs/2510.08647", "authors": ["Chengzhengxu Li", "Xiaoming Liu", "Zhaohan Zhang", "Shaochu Zhang", "Shengchao Liu", "Guoxin Ma", "Yu Lan", "Chao Shen"], "title": "Upfront Chain-of-Thought: A Cooperative Framework for Chain-of-Thought Compression", "comment": "ACL2026 Under Review", "summary": "Recent developments have enabled advanced reasoning in Large Language Models\n(LLMs) via long Chain-of-Thought (CoT), while long CoT suffers from high\ncomputational costs and significant latency losses owing to the autoregressive\nnature of generative LLMs. CoT compression aims to improve efficiency in the\nreasoning process by reducing output length. Previous works trade reasoning\nefficiency by either laborious discrete prompt designing or the construction of\nexternal compressed CoT datasets that sacrifice key reasoning details. In this\nwork, we propose Upfront CoT (UCoT): an efficient reasoning framework with\nupfront thought embedding to automate CoT compression. UCoT is a cooperative\nworkflow involving a small model (compressor) and a large model (executor). The\nfirst stage of UCoT trains compressor to generate upfront thought embeddings\nrich in reasoning information for the executor, avoiding the drawbacks of\nmanually designed prompts. The second stage optimizes executor to utilize\nupfront thought embeddings to derive the correct answer with short reasoning,\nusing a reward mechanism. Extensive experiments show that UCoT maintains the\npowerful reasoning ability of executor while significantly reducing the length\nof CoT. It is worth mentioning that when applying UCoT to the\nQwen2.5-7B-Instruct model, the usage of tokens on GSM8K dataset is reduced by\n50\\%, while the performance is 3.08\\% higher than that of the state-of-the-art\n(SOTA) method. The code and dataset are in supplementary material.", "AI": {"tldr": "\u63d0\u51fa\u4e86UCoT\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u671f\u601d\u60f3\u5d4c\u5165\u5b9e\u73b0CoT\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u957f\u5ea6\uff0c\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u51cf\u5c1150%token\u4f7f\u7528\u4e14\u6027\u80fd\u63d0\u53473.08%\u3002", "motivation": "\u957f\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u73b0\u6709CoT\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u7e41\u7410\u7684\u79bb\u6563\u63d0\u793a\u8bbe\u8ba1\uff0c\u8981\u4e48\u727a\u7272\u5173\u952e\u63a8\u7406\u7ec6\u8282\u3002", "method": "UCoT\u662f\u4e00\u4e2a\u5c0f\u6a21\u578b(\u538b\u7f29\u5668)\u548c\u5927\u6a21\u578b(\u6267\u884c\u5668)\u534f\u4f5c\u7684\u5de5\u4f5c\u6d41\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u538b\u7f29\u5668\u751f\u6210\u5bcc\u542b\u63a8\u7406\u4fe1\u606f\u7684\u524d\u671f\u601d\u60f3\u5d4c\u5165\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f18\u5316\u6267\u884c\u5668\u5229\u7528\u8fd9\u4e9b\u5d4c\u5165\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u3002", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\uff0cUCoT\u5c06token\u4f7f\u7528\u51cf\u5c1150%\uff0c\u6027\u80fd\u6bd4SOTA\u65b9\u6cd5\u63d0\u9ad83.08%\u3002", "conclusion": "UCoT\u6846\u67b6\u80fd\u6709\u6548\u538b\u7f29\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.08779", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08779", "abs": "https://arxiv.org/abs/2510.08779", "authors": ["Vaibhav Jain", "Gerrit Grossmann"], "title": "Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations", "comment": "Accepted to LM4Plan Workshop @ ICAPS 2025 (withdrawn before\n  presentation due to lack of travel funding)", "summary": "Reinforcement Learning (RL) agents often struggle in sparse-reward\nenvironments where traditional exploration strategies fail to discover\neffective action sequences. Large Language Models (LLMs) possess procedural\nknowledge and reasoning capabilities from text pretraining that could guide RL\nexploration, but existing approaches create rigid dependencies where RL\npolicies must follow LLM suggestions or incorporate them directly into reward\nfunctions. We propose a framework that provides LLM-generated action\nrecommendations through augmented observation spaces, allowing RL agents to\nlearn when to follow or ignore this guidance. Our method leverages LLMs' world\nknowledge and reasoning abilities while maintaining flexibility through soft\nconstraints. We evaluate our approach on three BabyAI environments of\nincreasing complexity and show that the benefits of LLM guidance scale with\ntask difficulty. In the most challenging environment, we achieve 71% relative\nimprovement in final success rates over baseline. The approach provides\nsubstantial sample efficiency gains, with agents reaching performance\nthresholds up to 9 times faster, and requires no modifications to existing RL\nalgorithms. Our results demonstrate an effective method for leveraging LLM\nplanning capabilities to accelerate RL training in challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u89c2\u5bdf\u7a7a\u95f4\u63d0\u4f9bLLM\u751f\u6210\u7684\u52a8\u4f5c\u63a8\u8350\uff0c\u8ba9RL\u667a\u80fd\u4f53\u5b66\u4e60\u4f55\u65f6\u9075\u5faa\u6216\u5ffd\u7565\u8fd9\u4e9b\u6307\u5bfc\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3RL\u667a\u80fd\u4f53\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u63a2\u7d22\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5229\u7528LLM\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u6307\u5bfcRL\u63a2\u7d22\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u4e2dLLM\u5efa\u8bae\u4e0eRL\u7b56\u7565\u4e4b\u95f4\u7684\u521a\u6027\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u589e\u5f3a\u89c2\u5bdf\u7a7a\u95f4\u63d0\u4f9bLLM\u751f\u6210\u7684\u52a8\u4f5c\u63a8\u8350\uff0c\u8ba9RL\u667a\u80fd\u4f53\u5b66\u4e60\u4f55\u65f6\u9075\u5faa\u6216\u5ffd\u7565\u8fd9\u4e9b\u6307\u5bfc\uff0c\u4fdd\u6301\u8f6f\u7ea6\u675f\u7684\u7075\u6d3b\u6027\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709RL\u7b97\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684BabyAI\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cLLM\u6307\u5bfc\u7684\u76ca\u5904\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u589e\u5f3a\u3002\u5728\u6700\u6311\u6218\u7684\u73af\u5883\u4e2d\uff0c\u6700\u7ec8\u6210\u529f\u7387\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534771%\uff0c\u6837\u672c\u6548\u7387\u63d0\u5347\u9ad8\u8fbe9\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528LLM\u7684\u89c4\u5212\u80fd\u529b\u6765\u52a0\u901fRL\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301RL\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u81ea\u4e3b\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08794", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08794", "abs": "https://arxiv.org/abs/2510.08794", "authors": ["I. Arda Vurankaya", "Mustafa O. Karabag", "Wesley A. Suttle", "Jesse Milzman", "David Fridovich-Keil", "Ufuk Topcu"], "title": "Deceptive Exploration in Multi-armed Bandits", "comment": null, "summary": "We consider a multi-armed bandit setting in which each arm has a public and a\nprivate reward distribution. An observer expects an agent to follow Thompson\nSampling according to the public rewards, however, the deceptive agent aims to\nquickly identify the best private arm without being noticed. The observer can\nobserve the public rewards and the pulled arms, but not the private rewards.\nThe agent, on the other hand, observes both the public and private rewards. We\nformalize detectability as a stepwise Kullback-Leibler (KL) divergence\nconstraint between the actual pull probabilities used by the agent and the\nanticipated pull probabilities by the observer. We model successful pulling of\npublic suboptimal arms as a % Bernoulli process where the success probability\ndecreases with each successful pull, and show these pulls can happen at most at\na $\\Theta(\\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin\nproblem based on public and private means, whose solution characterizes the\noptimal error exponent for best private arm identification. We finally propose\nan algorithm inspired by top-two algorithms. This algorithm naturally adapts\nits exploration according to the hardness of pulling arms based on the public\nsuboptimality gaps. We provide numerical examples illustrating the\n$\\Theta(\\sqrt{T}) $ rate and the behavior of the proposed algorithm.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u81c2\u6709\u516c\u5f00\u548c\u79c1\u6709\u5956\u52b1\u5206\u5e03\u3002\u4e00\u4e2a\u6b3a\u9a97\u6027\u667a\u80fd\u4f53\u8bd5\u56fe\u5728\u89c2\u5bdf\u8005\u4e0d\u77e5\u60c5\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u8bc6\u522b\u6700\u4f73\u79c1\u6709\u81c2\uff0c\u540c\u65f6\u6ee1\u8db3KL\u6563\u5ea6\u7ea6\u675f\u3002", "motivation": "\u7814\u7a76\u5728\u5b58\u5728\u89c2\u5bdf\u8005\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u667a\u80fd\u4f53\u5982\u4f55\u5728\u4e0d\u88ab\u53d1\u73b0\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316\u79c1\u6709\u5956\u52b1\u7684\u83b7\u53d6\uff0c\u8fd9\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u5b89\u5168\u7cfb\u7edf\u7b49\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u5c06\u53ef\u68c0\u6d4b\u6027\u5f62\u5f0f\u5316\u4e3a\u667a\u80fd\u4f53\u5b9e\u9645\u62c9\u53d6\u6982\u7387\u4e0e\u89c2\u5bdf\u8005\u9884\u671f\u62c9\u53d6\u6982\u7387\u4e4b\u95f4\u7684KL\u6563\u5ea6\u7ea6\u675f\uff0c\u5efa\u6a21\u516c\u5171\u6b21\u4f18\u81c2\u7684\u62c9\u53d6\u4e3a\u4f2f\u52aa\u5229\u8fc7\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8etop-two\u7b97\u6cd5\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u5728KL\u7ea6\u675f\u4e0b\uff0c\u516c\u5171\u6b21\u4f18\u81c2\u7684\u62c9\u53d6\u6700\u591a\u4ee5\u0398(\u221aT)\u7684\u901f\u7387\u53d1\u751f\uff0c\u5e76\u7ed9\u51fa\u4e86\u6700\u4f73\u79c1\u6709\u81c2\u8bc6\u522b\u7684\u6700\u4f18\u9519\u8bef\u6307\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6839\u636e\u516c\u5171\u6b21\u4f18\u6027\u5dee\u8ddd\u81ea\u9002\u5e94\u8c03\u6574\u63a2\u7d22\u7b56\u7565\uff0c\u5728\u6ee1\u8db3\u68c0\u6d4b\u7ea6\u675f\u7684\u540c\u65f6\u6709\u6548\u8bc6\u522b\u6700\u4f73\u79c1\u6709\u81c2\u3002", "topic": "agent analysis"}}
{"id": "2510.09227", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.09227", "abs": "https://arxiv.org/abs/2510.09227", "authors": ["Hyundong Jin", "Joonghyuk Hahn", "Yo-Sub Han"], "title": "RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems", "comment": null, "summary": "Large language models (LLMs) show strong performance across natural language\nprocessing (NLP), mathematical reasoning, and programming, and recent large\nreasoning models (LRMs) further emphasize explicit reasoning. Yet their\ncomputational limits, particularly spatial complexity constrained by finite\ncontext windows, remain poorly understood. While recent works often focus on\nproblems within the NP complexity class, we push the boundary by introducing a\nnovel benchmark grounded in two PSPACE-complete regular expression (regex)\nproblems: equivalence decision (RegexEQ) and minimization (RegexMin).\nPSPACE-complete problems serve as a more rigorous standard for assessing\ncomputational capacity, as their solutions require massive search space\nexploration. We perform a double-exponential space exploration to construct a\nlabeled dataset of over a million regex instances with a sound filtering\nprocess to build the benchmark. We conduct extensive evaluations on 6 LLMs and\n5 LRMs of varying scales, revealing common failure patterns such as verbosity\nand repetition. With its well-defined structure and quantitative evaluation\nmetrics, this work presents the first empirical investigation into the spatial\ncomputational limitations of LLMs and LRMs, offering a new framework for\nevaluating their advanced reasoning capabilities. Our code is available at\nhttps://github.com/hyundong98/RegexPSPACE .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8ePSPACE\u5b8c\u5168\u6b63\u5219\u8868\u8fbe\u5f0f\u95ee\u9898\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u548cLRM\u7684\u7a7a\u95f4\u8ba1\u7b97\u9650\u5236\uff0c\u63ed\u793a\u4e86\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u5982\u5197\u957f\u548c\u91cd\u590d\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\u5728\u7a7a\u95f4\u590d\u6742\u5ea6\u65b9\u9762\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u7279\u522b\u662f\u53d7\u9650\u4e8e\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7100\u4e07\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u4e24\u4e2aPSPACE\u5b8c\u5168\u95ee\u9898\uff08\u6b63\u5219\u8868\u8fbe\u5f0f\u7b49\u4ef7\u6027\u5224\u5b9a\u548c\u6700\u5c0f\u5316\uff09\uff0c\u5e76\u8fdb\u884c\u53cc\u91cd\u6307\u6570\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u5bf96\u4e2aLLM\u548c5\u4e2aLRM\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u53d1\u73b0\u5b83\u4eec\u5b58\u5728\u5197\u957f\u548c\u91cd\u590d\u7b49\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5bf9LLM\u548cLRM\u7a7a\u95f4\u8ba1\u7b97\u9650\u5236\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u4e3a\u8bc4\u4f30\u5176\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.08702", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08702", "abs": "https://arxiv.org/abs/2510.08702", "authors": ["Xianzhen Luo", "Wenzhen Zheng", "Qingfu Zhu", "Rongyi Zhang", "Houyi Li", "Siming Huang", "YuanTao Fan", "Wanxiang Che"], "title": "Scaling Laws for Code: A More Data-Hungry Regime", "comment": "Under Review", "summary": "Code Large Language Models (LLMs) are revolutionizing software engineering.\nHowever, scaling laws that guide the efficient training are predominantly\nanalyzed on Natural Language (NL). Given the fundamental differences like\nstrict syntax between code and NL, it is unclear whether these laws are\ndirectly applicable to code. To address this gap, we conduct the first\nlarge-scale empirical study of scaling laws for code, comprising 117\nexperimental runs with model sizes from 0.2B to 3.8B and training tokens from\n2B to 128B. We fit the Chinchilla law and the Farsser law. First, the results\nshow that the more expressive Farseer law offers greater accuracy. Second, the\nanalysis reveals that Code LLMs scale effectively with model size. Crucially,\ncode represents a more data-hungry regime, requiring a substantially higher\ndata-to-parameter ratio than NL. Finally, two additional sets of experiments on\ncode-NL mixtures show that NL benefits resource-constrained scenarios, but\nbecomes a detriment at higher compute budgets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u4ee3\u7801\u7684\u7f29\u653e\u5b9a\u5f8b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u4ee3\u7801LLM\u5728\u6a21\u578b\u5927\u5c0f\u4e0a\u6709\u6548\u6269\u5c55\uff0c\u4f46\u4ee3\u7801\u6bd4\u81ea\u7136\u8bed\u8a00\u9700\u8981\u66f4\u9ad8\u7684\u6570\u636e\u53c2\u6570\u6bd4\u3002", "motivation": "\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u4e3b\u8981\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5206\u6790\uff0c\u4f46\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff08\u5982\u4e25\u683c\u8bed\u6cd5\uff09\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u5b9a\u5f8b\u662f\u5426\u9002\u7528\u4e8e\u4ee3\u7801\u3002", "method": "\u8fdb\u884c\u4e86117\u6b21\u5b9e\u9a8c\u8fd0\u884c\uff0c\u6a21\u578b\u5927\u5c0f\u4ece0.2B\u52303.8B\uff0c\u8bad\u7ec3token\u4ece2B\u5230128B\uff0c\u62df\u5408\u4e86Chinchilla\u5b9a\u5f8b\u548cFarsser\u5b9a\u5f8b\u3002", "result": "1) Farsser\u5b9a\u5f8b\u66f4\u51c6\u786e\uff1b2) \u4ee3\u7801LLM\u968f\u6a21\u578b\u5927\u5c0f\u6709\u6548\u6269\u5c55\uff1b3) \u4ee3\u7801\u9700\u8981\u6bd4\u81ea\u7136\u8bed\u8a00\u66f4\u9ad8\u7684\u6570\u636e\u53c2\u6570\u6bd4\uff1b4) \u4ee3\u7801-\u81ea\u7136\u8bed\u8a00\u6df7\u5408\u5b9e\u9a8c\u4e2d\uff0c\u81ea\u7136\u8bed\u8a00\u5728\u8d44\u6e90\u53d7\u9650\u65f6\u6709\u76ca\uff0c\u4f46\u5728\u9ad8\u8ba1\u7b97\u9884\u7b97\u65f6\u6210\u4e3a\u4e0d\u5229\u56e0\u7d20\u3002", "conclusion": "\u4ee3\u7801\u7f29\u653e\u5b9a\u5f8b\u4e0e\u81ea\u7136\u8bed\u8a00\u4e0d\u540c\uff0c\u9700\u8981\u66f4\u9ad8\u7684\u6570\u636e\u53c2\u6570\u6bd4\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u4ee3\u7801LLM\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "topic": "code agent"}}
{"id": "2510.09244", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09244", "abs": "https://arxiv.org/abs/2510.09244", "authors": ["Victor de Lamo Castrillo", "Habtom Kahsay Gidey", "Alexander Lenz", "Alois Knoll"], "title": "Fundamentals of Building Autonomous LLM Agents", "comment": null, "summary": "This paper reviews the architecture and implementation methods of agents\npowered by large language models (LLMs). Motivated by the limitations of\ntraditional LLMs in real-world tasks, the research aims to explore patterns to\ndevelop \"agentic\" LLMs that can automate complex tasks and bridge the\nperformance gap with human capabilities. Key components include a perception\nsystem that converts environmental percepts into meaningful representations; a\nreasoning system that formulates plans, adapts to feedback, and evaluates\nactions through different techniques like Chain-of-Thought and Tree-of-Thought;\na memory system that retains knowledge through both short-term and long-term\nmechanisms; and an execution system that translates internal decisions into\nconcrete actions. This paper shows how integrating these systems leads to more\ncapable and generalized software bots that mimic human cognitive processes for\nautonomous and intelligent behavior.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u67b6\u6784\u4e0e\u5b9e\u73b0\u65b9\u6cd5\uff0c\u63a2\u8ba8\u5982\u4f55\u6784\u5efa\u80fd\u591f\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u3001\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u80fd\u529b\u5dee\u8ddd\u7684\"\u667a\u80fd\"LLM\u3002", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u3001\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u80fd\u529b\u5dee\u8ddd\u7684\u667a\u80fdLLM\u6a21\u5f0f\u3002", "method": "\u6784\u5efa\u5305\u542b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a\u611f\u77e5\u7cfb\u7edf\u5c06\u73af\u5883\u611f\u77e5\u8f6c\u6362\u4e3a\u6709\u610f\u4e49\u7684\u8868\u793a\uff1b\u63a8\u7406\u7cfb\u7edf\u901a\u8fc7\u601d\u7ef4\u94fe\u3001\u601d\u7ef4\u6811\u7b49\u6280\u672f\u5236\u5b9a\u8ba1\u5212\u3001\u9002\u5e94\u53cd\u9988\u548c\u8bc4\u4f30\u884c\u52a8\uff1b\u8bb0\u5fc6\u7cfb\u7edf\u901a\u8fc7\u77ed\u671f\u548c\u957f\u671f\u673a\u5236\u4fdd\u7559\u77e5\u8bc6\uff1b\u6267\u884c\u7cfb\u7edf\u5c06\u5185\u90e8\u51b3\u7b56\u8f6c\u5316\u4e3a\u5177\u4f53\u884c\u52a8\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6574\u5408\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u4ea7\u751f\u66f4\u5f3a\u5927\u548c\u901a\u7528\u7684\u8f6f\u4ef6\u673a\u5668\u4eba\uff0c\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u5b9e\u73b0\u81ea\u4e3b\u667a\u80fd\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u611f\u77e5\u3001\u63a8\u7406\u3001\u8bb0\u5fc6\u548c\u6267\u884c\u7cfb\u7edf\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u667a\u80fdLLM\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u81ea\u4e3b\u548c\u667a\u80fd\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2510.08720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08720", "abs": "https://arxiv.org/abs/2510.08720", "authors": ["Xianzhen Luo", "Jinyang Huang", "Wenzhen Zheng", "Qingfu Zhu", "Mingzheng Xu", "Yiheng Xu", "Yuantao Fan", "Libo Qin", "Wanxiang Che"], "title": "How Many Code and Test Cases Are Enough? Evaluating Test Cases Generation from a Binary-Matrix Perspective", "comment": "Work in Progress", "summary": "Evaluating test cases automatically generated by Large Language Models (LLMs)\nis a critical yet challenging task. Existing benchmarks suffer from high\ncomputational costs, score inflation, and a bias towards trivial bugs over\nrare, critical faults. In this work, we ask two fundamental questions: (1) What\nis the minimal set of wrong codes sufficient to represent the entire error\nspace? and (2) What is the minimal set of test cases needed to distinguish\nthem? We introduce a framework that formalizes benchmark construction as\nfinding an optimal diagnostic basis in a binary code-test matrix. The rank of\nthis matrix specifies the minimal number of independent error patterns (wrong\ncodes) and provides a tight upper bound on the number of test cases required\nfor complete fault coverage. Our objective is to identify a basis of size equal\nto the matrix rank that maximizes internal diversity. To tackle this NP-hard\nproblem, we propose WrongSelect, an efficient approximation algorithm to select\nmaximally diverse wrong codes. Applying this framework to millions of\ncompetitive programming submissions, we construct TC-Bench, a compact, diverse,\nand inflation-resistant benchmark. Extensive experiments show that even the\nmost advanced test case generation methods achieve only ~60% exclusion rates on\nTC-Bench, exposing a significant gap in their diagnostic power. Our dataset is\navailable at: https://huggingface.co/datasets/Luoberta/TC-Bench and our code is\nat: https://github.com/Luowaterbi/TC-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86TC-Bench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u5bfb\u627e\u4e8c\u8fdb\u5236\u4ee3\u7801-\u6d4b\u8bd5\u77e9\u9635\u7684\u6700\u4f18\u8bca\u65ad\u57fa\u7840\uff0c\u6784\u5efa\u7d27\u51d1\u3001\u591a\u6837\u5316\u4e14\u6297\u5206\u6570\u81a8\u80c0\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5206\u6570\u81a8\u80c0\u4ee5\u53ca\u5bf9\u7b80\u5355\u9519\u8bef\u504f\u89c1\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6846\u67b6\u5c06\u57fa\u51c6\u6784\u5efa\u5f62\u5f0f\u5316\u4e3a\u5728\u4e8c\u8fdb\u5236\u4ee3\u7801-\u6d4b\u8bd5\u77e9\u9635\u4e2d\u5bfb\u627e\u6700\u4f18\u8bca\u65ad\u57fa\u7840\uff0c\u63d0\u51faWrongSelect\u7b97\u6cd5\u9009\u62e9\u6700\u5927\u591a\u6837\u6027\u7684\u9519\u8bef\u4ee3\u7801\u3002", "result": "\u6784\u5efa\u7684TC-Bench\u57fa\u51c6\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u6cd5\u4e5f\u53ea\u80fd\u8fbe\u5230\u7ea660%\u7684\u6392\u9664\u7387\uff0c\u66b4\u9732\u4e86\u5176\u8bca\u65ad\u80fd\u529b\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6784\u5efa\u7d27\u51d1\u4e14\u591a\u6837\u5316\u7684\u57fa\u51c6\uff0c\u6709\u6548\u8bc4\u4f30LLM\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2510.08839", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.GR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.08839", "abs": "https://arxiv.org/abs/2510.08839", "authors": ["Motahare Mounesan", "Sourya Saha", "Houchao Gan", "Md. Nurul Absur", "Saptarshi Debroy"], "title": "Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction", "comment": null, "summary": "Real-time multi-view 3D reconstruction is a mission-critical application for\nkey edge-native use cases, such as fire rescue, where timely and accurate 3D\nscene modeling enables situational awareness and informed decision-making.\nHowever, the dynamic and unpredictable nature of edge resource availability\nintroduces disruptions, such as degraded image quality, unstable network links,\nand fluctuating server loads, which challenge the reliability of the\nreconstruction pipeline. In this work, we present a reinforcement learning\n(RL)-based edge resource management framework for reliable 3D reconstruction to\nensure high quality reconstruction within a reasonable amount of time, despite\nthe system operating under a resource-constrained and disruption-prone\nenvironment. In particular, the framework adopts two cooperative Q-learning\nagents, one for camera selection and one for server selection, both of which\noperate entirely online, learning policies through interactions with the edge\nenvironment. To support learning under realistic constraints and evaluate\nsystem performance, we implement a distributed testbed comprising lab-hosted\nend devices and FABRIC infrastructure-hosted edge servers to emulate smart city\nedge infrastructure under realistic disruption scenarios. Results show that the\nproposed framework improves application reliability by effectively balancing\nend-to-end latency and reconstruction quality in dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fb9\u7f18\u8d44\u6e90\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u548c\u6613\u53d7\u5e72\u6270\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u89c6\u89d23D\u91cd\u5efa\u3002", "motivation": "\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u8d44\u6e90\u53ef\u7528\u6027\uff08\u5982\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3001\u7f51\u7edc\u4e0d\u7a33\u5b9a\u3001\u670d\u52a1\u5668\u8d1f\u8f7d\u6ce2\u52a8\uff09\u5bf93D\u91cd\u5efa\u7ba1\u9053\u7684\u53ef\u9760\u6027\u6784\u6210\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6d88\u9632\u6551\u63f4\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u534f\u4f5c\u7684Q\u5b66\u4e60\u4ee3\u7406\uff1a\u4e00\u4e2a\u7528\u4e8e\u76f8\u673a\u9009\u62e9\uff0c\u4e00\u4e2a\u7528\u4e8e\u670d\u52a1\u5668\u9009\u62e9\uff0c\u4e24\u8005\u5b8c\u5168\u5728\u7ebf\u8fd0\u884c\uff0c\u901a\u8fc7\u4e0e\u8fb9\u7f18\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u5305\u542b\u5b9e\u9a8c\u5ba4\u8bbe\u5907\u548cFABRIC\u57fa\u7840\u8bbe\u65bd\u7684\u5206\u5e03\u5f0f\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u63d0\u9ad8\u5e94\u7528\u53ef\u9760\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u76843D\u91cd\u5efa\uff0c\u5e73\u8861\u5ef6\u8fdf\u548c\u8d28\u91cf\u8981\u6c42\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09404", "abs": "https://arxiv.org/abs/2510.09404", "authors": ["Christian Bluethgen", "Dave Van Veen", "Daniel Truhn", "Jakob Nikolas Kather", "Michael Moor", "Malgorzata Polacin", "Akshay Chaudhari", "Thomas Frauenfelder", "Curtis P. Langlotz", "Michael Krauthammer", "Farhad Nooralahzadeh"], "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges", "comment": null, "summary": "Building agents, systems that perceive and act upon their environment with a\ndegree of autonomy, has long been a focus of AI research. This pursuit has\nrecently become vastly more practical with the emergence of large language\nmodels (LLMs) capable of using natural language to integrate information,\nfollow instructions, and perform forms of \"reasoning\" and planning across a\nwide range of tasks. With its multimodal data streams and orchestrated\nworkflows spanning multiple systems, radiology is uniquely suited to benefit\nfrom agents that can adapt to context and automate repetitive yet complex\ntasks. In radiology, LLMs and their multimodal variants have already\ndemonstrated promising performance for individual tasks such as information\nextraction and report summarization. However, using LLMs in isolation\nunderutilizes their potential to support complex, multi-step workflows where\ndecisions depend on evolving context from multiple information sources.\nEquipping LLMs with external tools and feedback mechanisms enables them to\ndrive systems that exhibit a spectrum of autonomy, ranging from semi-automated\nworkflows to more adaptive agents capable of managing complex processes. This\nreview examines the design of such LLM-driven agentic systems, highlights key\napplications, discusses evaluation methods for planning and tool use, and\noutlines challenges such as error cascades, tool-use efficiency, and health IT\nintegration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u653e\u5c04\u5b66\u9886\u57df\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u548c\u53cd\u9988\u673a\u5236\u589e\u5f3aLLMs\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u653e\u5c04\u5b66\u5177\u6709\u591a\u6a21\u6001\u6570\u636e\u6d41\u548c\u8de8\u7cfb\u7edf\u534f\u8c03\u5de5\u4f5c\u6d41\u7a0b\u7684\u7279\u70b9\uff0c\u975e\u5e38\u9002\u5408\u5e94\u7528\u80fd\u591f\u9002\u5e94\u4e0a\u4e0b\u6587\u5e76\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u867d\u7136LLMs\u5728\u5355\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5355\u72ec\u4f7f\u7528\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5176\u5728\u590d\u6742\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u4e3aLLMs\u914d\u5907\u5916\u90e8\u5de5\u5177\u548c\u53cd\u9988\u673a\u5236\uff0c\u6784\u5efa\u80fd\u591f\u5c55\u73b0\u4e0d\u540c\u7a0b\u5ea6\u81ea\u4e3b\u6027\u7684\u7cfb\u7edf\uff0c\u4ece\u534a\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u5230\u80fd\u591f\u7ba1\u7406\u590d\u6742\u6d41\u7a0b\u7684\u81ea\u9002\u5e94\u667a\u80fd\u4f53\u3002", "result": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u653e\u5c04\u5b66\u9886\u57df\u663e\u793a\u51fa\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u5904\u7406\u4fe1\u606f\u63d0\u53d6\u3001\u62a5\u544a\u603b\u7ed3\u7b49\u4efb\u52a1\uff0c\u5e76\u652f\u6301\u66f4\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u5206\u6790\u4e86LLM\u9a71\u52a8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u5173\u952e\u5e94\u7528\u3001\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u9519\u8bef\u7ea7\u8054\u3001\u5de5\u5177\u4f7f\u7528\u6548\u7387\u548c\u5065\u5eb7IT\u96c6\u6210\u7b49\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2510.08776", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08776", "abs": "https://arxiv.org/abs/2510.08776", "authors": ["Kimaya Basu", "Savi Kolari", "Allison Yu"], "title": "Measuring Moral LLM Responses in Multilingual Capacities", "comment": "10 pages, 5 figures; referenced articles: arXiv:2303.08774,\n  arXiv:2303.12528, arXiv:2308.14132, arXiv:2505.12201, arXiv:2406.04428,\n  arXiv:2407.02273, arXiv:2404.01268, arXiv:2502.09747, arXiv:2507.13474,\n  arXiv:2505.21479, arXiv:2306.05685", "summary": "With LLM usage becoming widespread across countries, languages, and humanity\nmore broadly, the need to understand and guardrail their multilingual responses\nincreases. Large-scale datasets for testing and benchmarking have been created\nto evaluate and facilitate LLM responses across multiple dimensions. In this\nstudy, we evaluate the responses of frontier and leading open-source models in\nfive dimensions across low and high-resource languages to measure LLM accuracy\nand consistency across multilingual contexts. We evaluate the responses using a\nfive-point grading rubric and a judge LLM. Our study shows that GPT-5 performed\nthe best on average in each category, while other models displayed more\ninconsistency across language and category. Most notably, in the Consent &\nAutonomy and Harm Prevention & Safety categories, GPT scored the highest with\naverages of 3.56 and 4.73, while Gemini 2.5 Pro scored the lowest with averages\nof 1.39 and 1.98, respectively. These findings emphasize the need for further\ntesting on how linguistic shifts impact LLM responses across various categories\nand improvement in these areas.", "AI": {"tldr": "\u8bc4\u4f30\u524d\u6cbf\u548c\u9886\u5148\u5f00\u6e90\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u4e94\u4e2a\u7ef4\u5ea6\u8868\u73b0\uff0c\u53d1\u73b0GPT-5\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u7c7b\u522b\u95f4\u5b58\u5728\u8f83\u5927\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740LLM\u5728\u5168\u7403\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7406\u89e3\u548c\u9632\u62a4\u5176\u591a\u8bed\u8a00\u54cd\u5e94\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bed\u8a00\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u4e94\u70b9\u8bc4\u5206\u6807\u51c6\u548c\u6cd5\u5b98LLM\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e94\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u591a\u8bed\u8a00\u54cd\u5e94\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "result": "GPT-5\u5728\u6240\u6709\u7c7b\u522b\u4e2d\u5e73\u5747\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u540c\u610f\u4e0e\u81ea\u4e3b\u6743\uff083.56\uff09\u548c\u4f24\u5bb3\u9884\u9632\u4e0e\u5b89\u5168\uff084.73\uff09\u7c7b\u522b\u5f97\u5206\u6700\u9ad8\uff1bGemini 2.5 Pro\u8868\u73b0\u6700\u5dee\uff0c\u76f8\u5e94\u5f97\u5206\u4e3a1.39\u548c1.98\u3002", "conclusion": "\u8bed\u8a00\u53d8\u5316\u5bf9LLM\u5728\u4e0d\u540c\u7c7b\u522b\u4e2d\u7684\u54cd\u5e94\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u548c\u6539\u8fdb\u8fd9\u4e9b\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2510.09567", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.09567", "abs": "https://arxiv.org/abs/2510.09567", "authors": ["Jacopo Tagliabue", "Ciro Greco"], "title": "Safe, Untrusted, \"Proof-Carrying\" AI Agents: toward the agentic lakehouse", "comment": "IEEE Big Data, Workshop on Secure and Safe AI Agents for Big Data\n  Infrastructures", "summary": "Data lakehouses run sensitive workloads, where AI-driven automation raises\nconcerns about trust, correctness, and governance. We argue that API-first,\nprogrammable lakehouses provide the right abstractions for safe-by-design,\nagentic workflows. Using Bauplan as a case study, we show how data branching\nand declarative environments extend naturally to agents, enabling\nreproducibility and observability while reducing the attack surface. We present\na proof-of-concept in which agents repair data pipelines using correctness\nchecks inspired by proof-carrying code. Our prototype demonstrates that\nuntrusted AI agents can operate safely on production data and outlines a path\ntoward a fully agentic lakehouse.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528API\u4f18\u5148\u3001\u53ef\u7f16\u7a0b\u7684\u6570\u636e\u6e56\u4ed3\u4f5c\u4e3a\u5b89\u5168\u8bbe\u8ba1\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301AI\u4ee3\u7406\u5de5\u4f5c\u6d41\u3002\u901a\u8fc7Bauplan\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u6570\u636e\u5206\u652f\u548c\u58f0\u660e\u5f0f\u73af\u5883\u5982\u4f55\u81ea\u7136\u6269\u5c55\u5230\u4ee3\u7406\uff0c\u5b9e\u73b0\u53ef\u91cd\u73b0\u6027\u548c\u53ef\u89c2\u6d4b\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u653b\u51fb\u9762\u3002", "motivation": "\u6570\u636e\u6e56\u4ed3\u8fd0\u884c\u654f\u611f\u5de5\u4f5c\u8d1f\u8f7d\uff0cAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5f15\u53d1\u4e86\u5173\u4e8e\u4fe1\u4efb\u3001\u6b63\u786e\u6027\u548c\u6cbb\u7406\u7684\u62c5\u5fe7\u3002\u9700\u8981\u627e\u5230\u65e2\u80fd\u5229\u7528AI\u81ea\u52a8\u5316\u4f18\u52bf\u53c8\u80fd\u786e\u4fdd\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Bauplan\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u91c7\u7528\u6570\u636e\u5206\u652f\u548c\u58f0\u660e\u5f0f\u73af\u5883\u6765\u6269\u5c55\u4ee3\u7406\u80fd\u529b\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8ba9\u4ee3\u7406\u4f7f\u7528\u53d7\u8bc1\u660e\u643a\u5e26\u4ee3\u7801\u542f\u53d1\u7684\u6b63\u786e\u6027\u68c0\u67e5\u6765\u4fee\u590d\u6570\u636e\u7ba1\u9053\u3002", "result": "\u539f\u578b\u6f14\u793a\u8868\u660e\uff0c\u4e0d\u53d7\u4fe1\u4efb\u7684AI\u4ee3\u7406\u53ef\u4ee5\u5728\u751f\u4ea7\u6570\u636e\u4e0a\u5b89\u5168\u64cd\u4f5c\uff0c\u5e76\u4e3a\u5b9e\u73b0\u5b8c\u5168\u4ee3\u7406\u5316\u6e56\u4ed3\u6307\u660e\u4e86\u8def\u5f84\u3002", "conclusion": "API\u4f18\u5148\u3001\u53ef\u7f16\u7a0b\u7684\u6e56\u4ed3\u4e3a\u5b89\u5168\u8bbe\u8ba1\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5408\u9002\u7684\u62bd\u8c61\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u91cd\u73b0\u6027\u3001\u53ef\u89c2\u6d4b\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u3002", "topic": "swe application"}}
{"id": "2510.08804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08804", "abs": "https://arxiv.org/abs/2510.08804", "authors": ["Siddeshwar Raghavan", "Tanwi Mallick"], "title": "MOSAIC: Multi-agent Orchestration for Task-Intelligent Scientific Coding", "comment": null, "summary": "We present MOSAIC, a multi-agent Large Language Model (LLM) framework for\nsolving challenging scientific coding tasks. Unlike general-purpose coding,\nscientific workflows require algorithms that are rigorous, interconnected with\ndeep domain knowledge, and incorporate domain-specific reasoning, as well as\nalgorithm iteration without requiring I/O test cases. Many scientific problems\nalso require a sequence of subproblems to be solved, leading to the final\ndesired result. MOSAIC is designed as a training-free framework with specially\ndesigned agents to self-reflect, create the rationale, code, and debug within a\nstudent-teacher paradigm to address the challenges of scientific code\ngeneration. This design facilitates stepwise problem decomposition, targeted\nerror correction, and, when combined with our Consolidated Context Window\n(CCW), mitigates LLM hallucinations when solving complex scientific tasks\ninvolving chained subproblems. We evaluate MOSAIC on scientific coding\nbenchmarks and demonstrate that our specialized agentic framework outperforms\nexisting approaches in terms of accuracy, robustness, and interpretability.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u7f16\u7801\u4efb\u52a1\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u751f-\u6559\u5e08\u8303\u5f0f\u548c\u6574\u5408\u4e0a\u4e0b\u6587\u7a97\u53e3\u6765\u63d0\u5347\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u79d1\u5b66\u5de5\u4f5c\u6d41\u9700\u8981\u4e25\u8c28\u7684\u7b97\u6cd5\u3001\u6df1\u5ea6\u9886\u57df\u77e5\u8bc6\u548c\u7279\u5b9a\u63a8\u7406\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u94fe\u5f0f\u5b50\u95ee\u9898\u548c\u65e0I/O\u6d4b\u8bd5\u7528\u4f8b\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u65e0\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u81ea\u53cd\u601d\u3001\u521b\u5efa\u539f\u7406\u3001\u7f16\u7801\u548c\u8c03\u8bd5\u7684\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u5b66\u751f-\u6559\u5e08\u8303\u5f0f\u548c\u6574\u5408\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "result": "\u5728\u79d1\u5b66\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOSAIC\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MOSAIC\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u7684\u6311\u6218\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6d89\u53ca\u94fe\u5f0f\u5b50\u95ee\u9898\u7684\u590d\u6742\u4efb\u52a1\u3002", "topic": "code agent"}}
{"id": "2510.08859", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08859", "abs": "https://arxiv.org/abs/2510.08859", "authors": ["Ragib Amin Nihal", "Rui Wen", "Kazuhiro Nakadai", "Jun Sakuma"], "title": "Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models", "comment": null, "summary": "Large language models (LLMs) remain vulnerable to multi-turn jailbreaking\nattacks that exploit conversational context to bypass safety constraints\ngradually. These attacks target different harm categories (like malware\ngeneration, harassment, or fraud) through distinct conversational approaches\n(educational discussions, personal experiences, hypothetical scenarios).\nExisting multi-turn jailbreaking methods often rely on heuristic or ad hoc\nexploration strategies, providing limited insight into underlying model\nweaknesses. The relationship between conversation patterns and model\nvulnerabilities across harm categories remains poorly understood. We propose\nPattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation\npatterns to construct effective multi-turn jailbreaks through natural dialogue.\nEvaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve\nstate-of-the-art performance, uncovering pattern-specific vulnerabilities and\nLLM behavioral characteristics: models exhibit distinct weakness profiles where\nrobustness to one conversational pattern does not generalize to others, and\nmodel families share similar failure modes. These findings highlight\nlimitations of safety training and indicate the need for pattern-aware\ndefenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA", "AI": {"tldr": "\u63d0\u51fa\u4e86PE-CoA\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u79cd\u5bf9\u8bdd\u6a21\u5f0f\u6784\u5efa\u6709\u6548\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u572812\u4e2aLLM\u548c10\u4e2a\u5371\u5bb3\u7c7b\u522b\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u6a21\u5f0f\u7279\u5b9a\u7684\u6f0f\u6d1e\u548cLLM\u884c\u4e3a\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u4e34\u65f6\u63a2\u7d22\u7b56\u7565\uff0c\u5bf9\u6a21\u578b\u5e95\u5c42\u5f31\u70b9\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\uff0c\u4e14\u5bf9\u8bdd\u6a21\u5f0f\u4e0e\u8de8\u5371\u5bb3\u7c7b\u522b\u6a21\u578b\u6f0f\u6d1e\u4e4b\u95f4\u7684\u5173\u7cfb\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPE-CoA\u6846\u67b6\uff0c\u5305\u542b\u4e94\u79cd\u5bf9\u8bdd\u6a21\u5f0f\u6765\u901a\u8fc7\u81ea\u7136\u5bf9\u8bdd\u6784\u5efa\u6709\u6548\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u3002", "result": "\u572812\u4e2aLLM\u548c10\u4e2a\u5371\u5bb3\u7c7b\u522b\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5f31\u70b9\u5206\u5e03\uff0c\u5bf9\u4e00\u79cd\u5bf9\u8bdd\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u4e0d\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u6a21\u5f0f\uff0c\u4e14\u6a21\u578b\u5bb6\u65cf\u5171\u4eab\u76f8\u4f3c\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u51fa\u4e86\u5b89\u5168\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u8868\u660e\u9700\u8981\u6a21\u5f0f\u611f\u77e5\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.08892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08892", "abs": "https://arxiv.org/abs/2510.08892", "authors": ["Haomin Zhuang", "Yujun Zhou", "Taicheng Guo", "Yue Huang", "Fangxu Liu", "Kai Song", "Xiangliang Zhang"], "title": "Exploring Multi-Temperature Strategies for Token- and Rollout-Level Control in RLVR", "comment": null, "summary": "Reinforcement Learning has demonstrated substantial improvements in the\nreasoning abilities of Large Language Models (LLMs), exhibiting significant\napplicability across various domains. Recent research has identified that\ntokens within LLMs play distinct roles during reasoning tasks, categorizing\nthem into high-entropy reasoning tokens and low-entropy knowledge tokens. Prior\napproaches have typically focused on restricting updates to indirectly\nencourage exploration, yet they do not explicitly facilitate exploratory\nbehavior during the token generation stage itself. In this work, we introduce a\ncomplementary approach that explicitly promotes exploration during sampling by\napplying distinct temperature settings for different token types. Specifically,\nour method employs higher temperatures for reasoning tokens to actively\nencourage exploration, while retaining lower temperatures for knowledge tokens\nto maintain factual correctness. Furthermore, we systematically investigate\nvarious multi-temperature scheduling strategies and their impacts within\nreinforcement learning contexts. Empirical evaluations on several reasoning\nbenchmarks demonstrate that our approach significantly enhances the reasoning\nperformance of LLMs. The code is available at\nhttps://github.com/zhmzm/Multi_Temperature_Verl.git.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6e29\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684token\u5e94\u7528\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\uff1a\u5bf9\u63a8\u7406token\u4f7f\u7528\u9ad8\u6e29\u9f13\u52b1\u63a2\u7d22\uff0c\u5bf9\u77e5\u8bc6token\u4f7f\u7528\u4f4e\u6e29\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9650\u5236\u66f4\u65b0\u6765\u95f4\u63a5\u9f13\u52b1\u63a2\u7d22\uff0c\u4f46\u6ca1\u6709\u5728token\u751f\u6210\u9636\u6bb5\u660e\u786e\u4fc3\u8fdb\u63a2\u7d22\u884c\u4e3a\u3002\u7814\u7a76\u53d1\u73b0LLM\u4e2d\u7684token\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u626e\u6f14\u4e0d\u540c\u89d2\u8272\uff0c\u53ef\u5206\u4e3a\u9ad8\u71b5\u63a8\u7406token\u548c\u4f4e\u71b5\u77e5\u8bc6token", "method": "\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5e94\u7528\u4e0d\u540c\u7684\u6e29\u5ea6\u8bbe\u7f6e\uff1a\u5bf9\u63a8\u7406token\u4f7f\u7528\u8f83\u9ad8\u6e29\u5ea6\u4ee5\u79ef\u6781\u9f13\u52b1\u63a2\u7d22\uff0c\u5bf9\u77e5\u8bc6token\u4f7f\u7528\u8f83\u4f4e\u6e29\u5ea6\u4ee5\u4fdd\u6301\u4e8b\u5b9e\u6b63\u786e\u6027\u3002\u7cfb\u7edf\u7814\u7a76\u591a\u79cd\u591a\u6e29\u5ea6\u8c03\u5ea6\u7b56\u7565\u53ca\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5f71\u54cd", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u6027\u80fd", "conclusion": "\u901a\u8fc7\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684token\u5e94\u7528\u4e0d\u540c\u7684\u6e29\u5ea6\u8bbe\u7f6e\uff0c\u660e\u786e\u4fc3\u8fdb\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u63a2\u7d22\u884c\u4e3a\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2510.08915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08915", "abs": "https://arxiv.org/abs/2510.08915", "authors": ["Nicholas Deas", "Kathleen McKeown"], "title": "Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions", "comment": "EMNLP 2025 Camera Ready", "summary": "We introduce and study artificial impressions--patterns in LLMs' internal\nrepresentations of prompts that resemble human impressions and stereotypes\nbased on language. We fit linear probes on generated prompts to predict\nimpressions according to the two-dimensional Stereotype Content Model (SCM).\nUsing these probes, we study the relationship between impressions and\ndownstream model behavior as well as prompt features that may inform such\nimpressions. We find that LLMs inconsistently report impressions when prompted,\nbut also that impressions are more consistently linearly decodable from their\nhidden representations. Additionally, we show that artificial impressions of\nprompts are predictive of the quality and use of hedging in model responses. We\nalso investigate how particular content, stylistic, and dialectal features in\nprompts impact LLM impressions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u5185\u90e8\u8868\u793a\u4e2d\u7c7b\u4f3c\u4eba\u7c7b\u5370\u8c61\u548c\u523b\u677f\u5370\u8c61\u7684\u4eba\u5de5\u5370\u8c61\u6a21\u5f0f\uff0c\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u9884\u6d4b\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\u7684\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u5370\u8c61\u4e0e\u4e0b\u6e38\u6a21\u578b\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76LLM\u5185\u90e8\u8868\u793a\u4e2d\u662f\u5426\u5b58\u5728\u7c7b\u4f3c\u4eba\u7c7b\u5370\u8c61\u548c\u523b\u677f\u5370\u8c61\u7684\u6a21\u5f0f\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u548c\u54cd\u5e94\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u5728\u751f\u6210\u7684\u63d0\u793a\u4e0a\u9884\u6d4b\u523b\u677f\u5370\u8c61\u5185\u5bb9\u6a21\u578b\u7684\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u5206\u6790\u5370\u8c61\u4e0e\u4e0b\u6e38\u6a21\u578b\u884c\u4e3a\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u63d0\u793a\u7279\u5f81\u5bf9\u5370\u8c61\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0LLM\u5728\u63d0\u793a\u65f6\u4e0d\u4e00\u81f4\u5730\u62a5\u544a\u5370\u8c61\uff0c\u4f46\u4ece\u9690\u85cf\u8868\u793a\u4e2d\u7ebf\u6027\u89e3\u7801\u5370\u8c61\u66f4\u4e00\u81f4\uff1b\u4eba\u5de5\u5370\u8c61\u80fd\u9884\u6d4b\u6a21\u578b\u54cd\u5e94\u7684\u8d28\u91cf\u548c\u4f7f\u7528\u5bf9\u51b2\u7684\u7a0b\u5ea6\uff1b\u7279\u5b9a\u5185\u5bb9\u3001\u98ce\u683c\u548c\u65b9\u8a00\u7279\u5f81\u4f1a\u5f71\u54cdLLM\u5370\u8c61\u3002", "conclusion": "LLM\u5185\u90e8\u5b58\u5728\u53ef\u89e3\u7801\u7684\u4eba\u5de5\u5370\u8c61\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u4e0e\u6a21\u578b\u884c\u4e3a\u76f8\u5173\uff0c\u63d0\u793a\u7279\u5f81\u4f1a\u5f71\u54cd\u5370\u8c61\u5f62\u6210\u3002", "topic": "agent analysis"}}
{"id": "2510.08942", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08942", "abs": "https://arxiv.org/abs/2510.08942", "authors": ["Jiaming Wang", "Zhe Tang", "Yilin Jin", "Peng Ding", "Xiaoyu Li", "Xuezhi Cao"], "title": "SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures", "comment": null, "summary": "As large language models (LLMs) are widely deployed as domain-specific\nagents, many benchmarks have been proposed to evaluate their ability to follow\ninstructions and make decisions in real-world scenarios. However, business\nscenarios often involve complex standard operating procedures (SOPs), and the\nevaluation of LLM capabilities in such contexts has not been fully explored. To\nbridge this gap, we propose SOP-Maze, a benchmark constructed from real-world\nbusiness data and adapted into a collection of 397 tasks from 23 complex SOP\nscenarios. We further categorize SOP tasks into two broad classes: Lateral Root\nSystem (LRS), representing wide-option tasks that demand precise selection; and\nHeart Root System (HRS), which emphasizes deep logical reasoning with complex\nbranches. Extensive experiments reveal that nearly all state-of-the-art models\nstruggle with SOP-Maze. We conduct a comprehensive analysis and identify three\nkey error categories: (i) route blindness: difficulty following procedures;\n(ii) conversational fragility: inability to handle real dialogue nuances; and\n(iii) calculation errors: mistakes in time or arithmetic reasoning under\ncomplex contexts. The systematic study explores LLM performance across SOP\ntasks that challenge both breadth and depth, offering new insights for\nimproving model capabilities. We have open-sourced our work on\nhttps://github.com/ADoublLEN/SOP-Maze.", "AI": {"tldr": "\u63d0\u51fa\u4e86SOP-Maze\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b397\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e1a\u52a1SOP\u7684\u4efb\u52a1\uff0c\u8bc4\u4f30LLM\u5728\u590d\u6742\u6807\u51c6\u64cd\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6d41\u7a0b\u9075\u5faa\u3001\u5bf9\u8bdd\u5904\u7406\u548c\u8ba1\u7b97\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30LLM\u7684\u6307\u4ee4\u9075\u5faa\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u4f46\u5728\u6d89\u53ca\u590d\u6742\u6807\u51c6\u64cd\u4f5c\u6d41\u7a0b(SOP)\u7684\u4e1a\u52a1\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4ece\u771f\u5b9e\u4e1a\u52a1\u6570\u636e\u6784\u5efaSOP-Maze\u57fa\u51c6\uff0c\u5305\u542b397\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a\u6a2a\u5411\u6839\u7cfb\u7edf(LRS)\u548c\u6838\u5fc3\u6839\u7cfb\u7edf(HRS)\u4e24\u7c7b\uff0c\u5206\u522b\u6d4b\u8bd5\u9009\u62e9\u80fd\u529b\u548c\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u51e0\u4e4e\u6240\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728SOP-Maze\u4e0a\u90fd\u8868\u73b0\u4e0d\u4f73\uff0c\u8bc6\u522b\u51fa\u4e09\u4e2a\u4e3b\u8981\u9519\u8bef\u7c7b\u578b\uff1a\u6d41\u7a0b\u76f2\u533a\u3001\u5bf9\u8bdd\u8106\u5f31\u6027\u548c\u8ba1\u7b97\u9519\u8bef\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6539\u8fdb\u6a21\u578b\u5728\u590d\u6742SOP\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u57fa\u51c6\u6d4b\u8bd5\u5df2\u5f00\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2510.08952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08952", "abs": "https://arxiv.org/abs/2510.08952", "authors": ["Zhihan Zhang", "Xunkai Li", "Yilong Zuo", "Zhenjun Li", "Bing Zhou", "Rong-Hua Li", "Guoren Wang"], "title": "When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach", "comment": "12 pages, 7figures", "summary": "Text-attributed graphs (TAGs) have emerged as a powerful representation that\ncombines structural connections with fine-grained semantics, supporting a wide\nrange of data-centric applications. However, the performance of graph neural\nnetworks (GNNs) on TAGs is highly sensitive to input quality. Our empirical\nstudy shows that both traditional GNNs and LLM-enhanced GNNs suffer significant\ndegradation across nine representative scenarios of sparsity, noise, and\nimbalance, highlighting graph quality as a critical bottleneck. Existing\napproaches mainly focus on improving model architectures, while neglecting\nsystematic optimization of TAG data itself, leading to limited effectiveness in\npractice. To address this gap, we propose LAGA (Large Language and Graph\nAgent), a unified multi-agent framework that treats graph quality control as a\nfirst-class, data-centric problem. LAGA integrates four collaborative\nagents-detection, planning, action, and evaluation-into an automated closed\nloop. At its core, the action agent employs a dual-encoder and tri-objective\ndesign to capture complementary information across modalities and perform\nholistic graph quality enhancement. Experiments across nine scenarios show that\nLAGA improves graph quality and achieves state-of-the-art performance across\nvarious tasks and backbones, validating data-centric quality optimization as\nkey to reliable TAGs and robust graph learning.", "AI": {"tldr": "\u63d0\u51faLAGA\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u4f18\u5316\u6587\u672c\u5c5e\u6027\u56fe\u7684\u8d28\u91cf\uff0c\u89e3\u51b3GNN\u5bf9\u8f93\u5165\u8d28\u91cf\u654f\u611f\u7684\u95ee\u9898", "motivation": "\u4f20\u7edfGNN\u548cLLM\u589e\u5f3a\u7684GNN\u5728\u7a00\u758f\u3001\u566a\u58f0\u548c\u4e0d\u5e73\u8861\u7b49\u4e5d\u79cd\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u67b6\u6784\u800c\u5ffd\u89c6\u56fe\u6570\u636e\u672c\u8eab\u7684\u7cfb\u7edf\u6027\u4f18\u5316", "method": "LAGA\u6846\u67b6\u5305\u542b\u56db\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1a\u68c0\u6d4b\u3001\u89c4\u5212\u3001\u884c\u52a8\u548c\u8bc4\u4f30\uff0c\u5f62\u6210\u81ea\u52a8\u5316\u95ed\u73af\u3002\u6838\u5fc3\u884c\u52a8\u667a\u80fd\u4f53\u91c7\u7528\u53cc\u7f16\u7801\u5668\u548c\u4e09\u76ee\u6807\u8bbe\u8ba1\uff0c\u6355\u83b7\u8de8\u6a21\u6001\u4e92\u8865\u4fe1\u606f\u8fdb\u884c\u6574\u4f53\u56fe\u8d28\u91cf\u589e\u5f3a", "result": "\u5728\u4e5d\u79cd\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAGA\u63d0\u9ad8\u4e86\u56fe\u8d28\u91cf\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8d28\u91cf\u4f18\u5316\u662f\u5b9e\u73b0\u53ef\u9760\u6587\u672c\u5c5e\u6027\u56fe\u548c\u9c81\u68d2\u56fe\u5b66\u4e60\u7684\u5173\u952e", "topic": "agent analysis"}}
{"id": "2510.08992", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08992", "abs": "https://arxiv.org/abs/2510.08992", "authors": ["Kamel Alrashedy", "Vriksha Srihari", "Zulfiqar Zaidi", "Ridam Srivastava", "Pradyumna Tambwekar", "Matthew Gombolay"], "title": "Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search", "comment": null, "summary": "While researchers have made significant progress in enabling large language\nmodels (LLMs) to perform multi-step planning, LLMs struggle to ensure that\nthose plans align with high-level user intent and satisfy symbolic constraints,\nespecially in complex, multi-step domains. Existing reasoning approaches such\nas Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented\nmethods, expand the search space but often yield infeasible actions or\nhallucinated steps. To overcome these limitations, we propose\nConstraints-of-Thought (Const-o-T), a framework that provides a structured\nprior that enables Monte Carlo Tree Search (MCTS) focus search on semantically\nmeaningful paths. Each reasoning step is represented as an (intent, constraint)\npair, which serves both to compress the search space and enforce validity.\nUnlike prior methods that merely generate reasoning traces or validate outputs\npost hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search\ntoward feasible and meaningful plans. We integrate Const-o-T into MCTS using a\nstructured representation of intent-constraint pairs constraints prune\ninfeasible branches and guide exploration toward semantically valid actions,\nimproving planning efficiency and verifiable decision-making. We demonstrate\nacross three domains Risk game, CAD code generation, and arithmetic reasoning\nthat our approach outperforms baselines, yielding higher accuracy and stronger\nstructural alignment. Our contribution is to demonstrate that Const-of-T offers\na generalizable foundation for constraint-guided reasoning, enabling more\nefficient, constraint-aligned, and domain-adaptable planning with LLMs.", "AI": {"tldr": "\u63d0\u51faConstraints-of-Thought (Const-o-T)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u6b65\u9aa4\u8868\u793a\u4e3a(\u610f\u56fe,\u7ea6\u675f)\u5bf9\uff0c\u5f15\u5bfc\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e13\u6ce8\u4e8e\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u8def\u5f84\uff0c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u548c\u7ea6\u675f\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u5982CoT\u3001ToT\u7b49\u5728\u590d\u6742\u591a\u6b65\u9886\u57df\u4e2d\u96be\u4ee5\u786e\u4fdd\u8ba1\u5212\u7b26\u5408\u7528\u6237\u610f\u56fe\u548c\u7b26\u53f7\u7ea6\u675f\uff0c\u5e38\u4ea7\u751f\u4e0d\u53ef\u884c\u52a8\u4f5c\u6216\u5e7b\u89c9\u6b65\u9aa4\u3002", "method": "\u4f7f\u7528(\u610f\u56fe,\u7ea6\u675f)\u5bf9\u7ed3\u6784\u5316\u8868\u793a\uff0c\u96c6\u6210\u5230MCTS\u4e2d\uff0c\u7ea6\u675f\u526a\u679d\u4e0d\u53ef\u884c\u5206\u652f\uff0c\u5f15\u5bfc\u641c\u7d22\u671d\u5411\u8bed\u4e49\u6709\u6548\u52a8\u4f5c\u3002", "result": "\u5728Risk\u6e38\u620f\u3001CAD\u4ee3\u7801\u751f\u6210\u548c\u7b97\u672f\u63a8\u7406\u4e09\u4e2a\u9886\u57df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0c\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u548c\u66f4\u5f3a\u7684\u7ed3\u6784\u5bf9\u9f50\u3002", "conclusion": "Const-of-T\u4e3a\u7ea6\u675f\u5f15\u5bfc\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6cdb\u5316\u7684\u57fa\u7840\uff0c\u4f7fLLM\u89c4\u5212\u66f4\u9ad8\u6548\u3001\u7ea6\u675f\u5bf9\u9f50\u4e14\u9886\u57df\u9002\u5e94\u6027\u5f3a\u3002", "topic": "agent analysis"}}
{"id": "2510.09158", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09158", "abs": "https://arxiv.org/abs/2510.09158", "authors": ["Seiya Ishikura", "Hiroaki Yamada", "Tatsuya Hiraoka", "Hiroaki Yamada", "Takenobu Tokunaga"], "title": "Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM", "comment": "8 pages, 1 figure", "summary": "This study proposes augmenting dialog data with think-aloud utterances (TAUs)\nfor modeling individual personalities in text chat by LLM. TAU is a\nverbalization of a speaker's thought before articulating the utterance. We\nexpect \"persona LLMs\" trained with TAU-augmented data can mimic the speaker's\npersonality trait better. We tested whether the trained persona LLMs obtain the\nhuman personality with respect to Big Five, a framework characterizing human\npersonality traits from five aspects. The results showed that LLMs trained with\nTAU-augmented data more closely align to the speakers' Agreeableness and\nNeuroticism of Big Five than those trained with original dialog data. We also\nfound that the quality of TAU-augmentation impacts persona LLM's performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "2510.09041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09041", "abs": "https://arxiv.org/abs/2510.09041", "authors": ["Junchao Fan", "Xiaolin Chang"], "title": "Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach", "comment": null, "summary": "Deep reinforcement learning (DRL) has demonstrated remarkable success in\ndeveloping autonomous driving policies. However, its vulnerability to\nadversarial attacks remains a critical barrier to real-world deployment.\nAlthough existing robust methods have achieved success, they still suffer from\nthree key issues: (i) these methods are trained against myopic adversarial\nattacks, limiting their abilities to respond to more strategic threats, (ii)\nthey have trouble causing truly safety-critical events (e.g., collisions), but\ninstead often result in minor consequences, and (iii) these methods can\nintroduce learning instability and policy drift during training due to the lack\nof robust constraints. To address these issues, we propose Intelligent\nGeneral-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel\nrobust autonomous driving approach that consists of a strategic targeted\nadversary and a robust driving agent. The strategic targeted adversary is\ndesigned to leverage the temporal decision-making capabilities of DRL to\nexecute strategically coordinated multi-step attacks. In addition, it\nexplicitly focuses on inducing safety-critical events by adopting a general-sum\nobjective. The robust driving agent learns by interacting with the adversary to\ndevelop a robust autonomous driving policy against adversarial attacks. To\nensure stable learning in adversarial environments and to mitigate policy drift\ncaused by attacks, the agent is optimized under a constrained formulation.\nExtensive experiments show that IGCARL improves the success rate by at least\n27.9\\% over state-of-the-art methods, demonstrating superior robustness to\nadversarial attacks and enhancing the safety and reliability of DRL-based\nautonomous driving.", "AI": {"tldr": "\u63d0\u51faIGCARL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6218\u7565\u76ee\u6807\u5bf9\u6297\u8005\u548c\u9c81\u68d2\u9a7e\u9a76\u4ee3\u7406\u6765\u589e\u5f3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9762\u5bf9\u6218\u7565\u6027\u653b\u51fb\u3001\u96be\u4ee5\u5f15\u53d1\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u4ee5\u53ca\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709DRL\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5728\u9762\u5bf9\u5bf9\u6297\u653b\u51fb\u65f6\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u53ea\u80fd\u5e94\u5bf9\u77ed\u89c6\u653b\u51fb\u3001\u96be\u4ee5\u5f15\u53d1\u771f\u6b63\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u3001\u7f3a\u4e4f\u9c81\u68d2\u7ea6\u675f\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u653f\u7b56\u6f02\u79fb\u3002", "method": "IGCARL\u5305\u542b\u6218\u7565\u76ee\u6807\u5bf9\u6297\u8005\u548c\u9c81\u68d2\u9a7e\u9a76\u4ee3\u7406\u3002\u5bf9\u6297\u8005\u5229\u7528DRL\u7684\u65f6\u5e8f\u51b3\u7b56\u80fd\u529b\u6267\u884c\u6218\u7565\u6027\u591a\u6b65\u653b\u51fb\uff0c\u91c7\u7528\u5e7f\u4e49\u548c\u535a\u5f08\u76ee\u6807\u8bf1\u5bfc\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff1b\u9a7e\u9a76\u4ee3\u7406\u901a\u8fc7\u4e0e\u5bf9\u6297\u8005\u4ea4\u4e92\u5b66\u4e60\u9c81\u68d2\u7b56\u7565\uff0c\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u4f18\u5316\u4ee5\u786e\u4fdd\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIGCARL\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u81f3\u5c11\u63d0\u9ad827.9%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u4e86DRL\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "IGCARL\u901a\u8fc7\u6218\u7565\u6027\u5bf9\u6297\u8bad\u7ec3\u548c\u7ea6\u675f\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86DRL\u81ea\u52a8\u9a7e\u9a76\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09255", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09255", "abs": "https://arxiv.org/abs/2510.09255", "authors": ["Chenyang Gu", "Yewen Pu", "Bruce Yang", "Xiaofan Li", "Huan Gao"], "title": "DSPO: Stable and Efficient Policy Optimization for Agentic Search and Reasoning", "comment": null, "summary": "Enhancing LLMs with the ability to actively search external knowledge is\ncrucial for complex and real-world tasks. Current approaches either rely on\nprompting to elicit the model's innate agent capabilities, or suffer from\nperformance ceilings and collapse when applying RL to complex interactive\ntasks, leaving their true agentic potential untapped. To address this, we\nintroduce \\textbf{D}ynamic-filter \\textbf{S}equence-level \\textbf{P}olicy\n\\textbf{O}ptimization (DSPO), an improved RL algorithm designed for robust\nagent training through sequence-level optimization and dynamic sample\nfiltering. We train our model purely through RL to interleave multi-turn search\nand reasoning, obviating the need for supervised demonstration data. Across\nmultiple QA benchmarks, our DSPO-trained 7B model improves over a comparable\nprevious work by \\textbf{34.1\\%}, and even outperforms the 14B model from\nprevious work in complex multihop QA such as HotpotQA by nearly \\textbf{9\\%\nrelative}, maintaining exceptional training stability.", "AI": {"tldr": "\u63d0\u51faDSPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u548c\u52a8\u6001\u6837\u672c\u8fc7\u6ee4\u589e\u5f3aLLM\u7684\u5916\u90e8\u77e5\u8bc6\u641c\u7d22\u80fd\u529b\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u63d0\u793a\u6fc0\u53d1\u6a21\u578b\u5185\u5728\u4ee3\u7406\u80fd\u529b\uff0c\u8981\u4e48\u5728\u590d\u6742\u4ea4\u4e92\u4efb\u52a1\u4e2d\u5e94\u7528RL\u65f6\u5b58\u5728\u6027\u80fd\u4e0a\u9650\u548c\u5d29\u6e83\u95ee\u9898\uff0c\u672a\u80fd\u5145\u5206\u6316\u6398\u6a21\u578b\u7684\u4ee3\u7406\u6f5c\u529b", "method": "DSPO\uff08\u52a8\u6001\u8fc7\u6ee4\u5e8f\u5217\u7ea7\u7b56\u7565\u4f18\u5316\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u548c\u52a8\u6001\u6837\u672c\u8fc7\u6ee4\u8fdb\u884c\u9c81\u68d2\u4ee3\u7406\u8bad\u7ec3\uff0c\u7eafRL\u8bad\u7ec3\u5b9e\u73b0\u591a\u8f6e\u641c\u7d22\u548c\u63a8\u7406\u4ea4\u9519", "result": "\u5728\u591a\u4e2aQA\u57fa\u51c6\u4e0a\uff0c7B\u6a21\u578b\u6bd4\u540c\u7c7b\u5de5\u4f5c\u63d0\u534734.1%\uff0c\u5728\u590d\u6742\u591a\u8df3QA\u5982HotpotQA\u4e0a\u751a\u81f3\u8d85\u8fc7\u4e4b\u524d\u5de5\u4f5c\u768414B\u6a21\u578b\u8fd19%\u76f8\u5bf9\u63d0\u5347\uff0c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4f18\u5f02", "conclusion": "DSPO\u7b97\u6cd5\u80fd\u6709\u6548\u589e\u5f3aLLM\u7684\u5916\u90e8\u77e5\u8bc6\u641c\u7d22\u80fd\u529b\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u8bad\u7ec3\u7a33\u5b9a", "topic": "agentic reinforcement learning"}}
{"id": "2510.09259", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09259", "abs": "https://arxiv.org/abs/2510.09259", "authors": ["Yongding Tao", "Tian Wang", "Yihong Dong", "Huanyu Liu", "Kechi Zhang", "Xiaolong Hu", "Ge Li"], "title": "Detecting Data Contamination from Reinforcement Learning Post-training for Large Language Models", "comment": null, "summary": "Data contamination poses a significant threat to the reliable evaluation of\nLarge Language Models (LLMs). This issue arises when benchmark samples may\ninadvertently appear in training sets, compromising the validity of reported\nperformance. While detection methods have been developed for the pre-training\nand Supervised Fine-Tuning stages, a critical research gap exists for the\nincreasingly significant phase of Reinforcement Learning (RL) post-training. As\nRL post-training becomes pivotal for advancing LLM reasoning, the absence of\nspecialized contamination detection methods in this paradigm presents a\ncritical vulnerability. To address this, we conduct the first systematic study\nof data detection within RL post-training scenario and propose Self-Critique.\nOur method is motivated by a key observation: after RL phase, the output\nentropy distribution of LLMs tends to collapse into highly specific and sparse\nmodes. Self-Critique probes for the underlying policy collapse, i.e., the\nmodel's convergence to a narrow reasoning path, which causes this entropy\nreduction. To facilitate this research, we also introduce RL-MIA, a benchmark\nconstructed to simulate this specific contamination scenario. Extensive\nexperiments show that Self-Critique significantly outperforms baseline methods\nacross multiple models and contamination tasks, achieving an AUC improvement of\nup to 30%. Whereas existing methods are close to a random guess for RL-phase\ncontamination, our method makes detection possible.", "AI": {"tldr": "\u63d0\u51fa\u4e86Self-Critique\u65b9\u6cd5\uff0c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76RL\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6570\u636e\u6c61\u67d3\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b\u6a21\u578b\u8f93\u51fa\u71b5\u5206\u5e03\u5d29\u6e83\u6765\u8bc6\u522b\u6c61\u67d3\u6570\u636e\u3002", "motivation": "RL\u540e\u8bad\u7ec3\u9636\u6bb5\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u6c61\u67d3\u68c0\u6d4b\u65b9\u6cd5\uff0c\u800c\u8be5\u9636\u6bb5\u5bf9\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u68c0\u6d4bRL\u9636\u6bb5\u7684\u6c61\u67d3\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u71b5\u5206\u5e03\u5d29\u6e83\u7684\u89c2\u5bdf\uff0c\u63d0\u51faSelf-Critique\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u6d4b\u5e95\u5c42\u7b56\u7565\u5d29\u6e83\uff08\u6a21\u578b\u6536\u655b\u5230\u72ed\u7a84\u63a8\u7406\u8def\u5f84\uff09\u6765\u68c0\u6d4b\u6c61\u67d3\u3002", "result": "\u5728RL-MIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSelf-Critique\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cAUC\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u3002", "conclusion": "Self-Critique\u4f7f\u5f97RL\u9636\u6bb5\u6c61\u67d3\u68c0\u6d4b\u6210\u4e3a\u53ef\u80fd\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u91cd\u8981\u7a7a\u767d\u3002", "topic": "agent analysis"}}
{"id": "2510.09278", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09278", "abs": "https://arxiv.org/abs/2510.09278", "authors": ["Jiuheng Lin", "Cong Jiang", "Zirui Wu", "Jiarui Sun", "Yansong Feng"], "title": "CLARity: Reasoning Consistency Alone Can Teach Reinforced Experts", "comment": null, "summary": "Training expert LLMs in domains with scarce data is difficult, often relying\non multiple-choice questions (MCQs). However, standard outcome-based\nreinforcement learning (RL) on MCQs is risky. While it may improve accuracy, we\nobserve it often degrades reasoning quality such as logical consistency.\nExisting solutions to supervise reasoning, such as large-scale Process Reward\nModels (PRMs), are prohibitively expensive. To address this, we propose\nCLARity, a cost-effective RL framework that enhances reasoning quality using\nonly a small, general-purpose LLM. CLARity integrates a consistency-aware\nreward mechanism with a 2-stage refine-then-monitor training pipeline to\nenhance reasoning consistency, and a dynamic data reformulation strategy to to\nbetter exploit limited data. Experiments demonstrate that CLARity improves\nresponse consistency by 16.5% and accuracy by 7.5% over baselines. Human\nevaluations further confirm holistic improvements in coherence and\nprofessionalism. Thus, CLARity offers a generalizable solution that enables\nsmaller models to effectively guide expert models by reasoning consistency.Our\ncode is open sourced at: https://github.com/Infinite-set/CLARity", "AI": {"tldr": "CLARity\u662f\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u578b\u901a\u7528LLM\u6765\u63d0\u5347\u4e13\u5bb6\u6a21\u578b\u5728\u7a00\u7f3a\u6570\u636e\u9886\u57df\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u611f\u77e5\u5956\u52b1\u673a\u5236\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u6539\u5584\u63a8\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u8bad\u7ec3\u4e13\u5bb6LLM\u5f88\u56f0\u96be\uff0c\u901a\u5e38\u4f9d\u8d56\u9009\u62e9\u9898\u3002\u4f46\u6807\u51c6\u7684\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u53ef\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5374\u5e38\u5e38\u635f\u5bb3\u63a8\u7406\u8d28\u91cf\u5982\u903b\u8f91\u4e00\u81f4\u6027\u3002\u73b0\u6709\u76d1\u7763\u63a8\u7406\u7684\u89e3\u51b3\u65b9\u6848\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faCLARity\u6846\u67b6\uff0c\u96c6\u6210\u4e00\u81f4\u6027\u611f\u77e5\u5956\u52b1\u673a\u5236\u4e0e\u4e24\u9636\u6bb5\u7cbe\u70bc-\u76d1\u63a7\u8bad\u7ec3\u6d41\u7a0b\u6765\u589e\u5f3a\u63a8\u7406\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u6570\u636e\u91cd\u6784\u7b56\u7565\u66f4\u597d\u5730\u5229\u7528\u6709\u9650\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCLARity\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c06\u54cd\u5e94\u4e00\u81f4\u6027\u63d0\u9ad8\u4e8616.5%\uff0c\u51c6\u786e\u6027\u63d0\u9ad8\u4e867.5%\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u786e\u8ba4\u4e86\u5728\u8fde\u8d2f\u6027\u548c\u4e13\u4e1a\u6027\u65b9\u9762\u7684\u6574\u4f53\u6539\u8fdb\u3002", "conclusion": "CLARity\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u8f83\u5c0f\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u63a8\u7406\u4e00\u81f4\u6027\u6709\u6548\u6307\u5bfc\u4e13\u5bb6\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09295", "abs": "https://arxiv.org/abs/2510.09295", "authors": ["Jiapeng Wang", "Changxin Tian", "Kunlong Chen", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "title": "MaP: A Unified Framework for Reliable Evaluation of Pre-training Dynamics", "comment": null, "summary": "Reliable evaluation is fundamental to the progress of Large Language Models\n(LLMs), yet the evaluation process during pre-training is plagued by\nsignificant instability that obscures true learning dynamics. In this work, we\nsystematically diagnose this instability, attributing it to two distinct\nsources: \\textit{Parameter Instability} from training stochasticity and\n\\textit{Evaluation Instability} from noisy measurement protocols. To counteract\nboth sources of noise, we introduce \\textbf{MaP}, a dual-pronged framework that\nsynergistically integrates checkpoint \\underline{M}erging \\underline{a}nd the\n\\underline{P}ass@k metric. Checkpoint merging smooths the parameter space by\naveraging recent model weights, while Pass@k provides a robust, low-variance\nstatistical estimate of model capability. Extensive experiments show that MaP\nyields significantly smoother performance curves, reduces inter-run variance,\nand ensures more consistent model rankings. Ultimately, MaP provides a more\nreliable and faithful lens for observing LLM training dynamics, laying a\ncrucial empirical foundation for LLM research.", "AI": {"tldr": "\u63d0\u51faMaP\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u67e5\u70b9\u5408\u5e76\u548cPass@k\u6307\u6807\u6765\u89e3\u51b3LLM\u9884\u8bad\u7ec3\u8bc4\u4f30\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u4e0d\u7a33\u5b9a\u6027\uff0c\u8fd9\u63a9\u76d6\u4e86\u771f\u5b9e\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u963b\u788d\u4e86LLM\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "MaP\u6846\u67b6\u7ed3\u5408\u68c0\u67e5\u70b9\u5408\u5e76\uff08\u5e73\u6ed1\u53c2\u6570\u7a7a\u95f4\uff09\u548cPass@k\u6307\u6807\uff08\u4f4e\u65b9\u5dee\u7edf\u8ba1\u4f30\u8ba1\uff09\uff0c\u4ece\u53c2\u6570\u4e0d\u7a33\u5b9a\u6027\u548c\u8bc4\u4f30\u4e0d\u7a33\u5b9a\u6027\u4e24\u4e2a\u6e90\u5934\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMaP\u80fd\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u6027\u80fd\u66f2\u7ebf\uff0c\u51cf\u5c11\u8fd0\u884c\u95f4\u65b9\u5dee\uff0c\u786e\u4fdd\u66f4\u4e00\u81f4\u7684\u6a21\u578b\u6392\u540d\u3002", "conclusion": "MaP\u4e3a\u89c2\u5bdfLLM\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5fe0\u5b9e\u7684\u89c6\u89d2\uff0c\u4e3aLLM\u7814\u7a76\u5960\u5b9a\u4e86\u91cd\u8981\u7684\u5b9e\u8bc1\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.09156", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09156", "abs": "https://arxiv.org/abs/2510.09156", "authors": ["Jing Li", "Zhijie Sun", "Zhicheng Zhou", "Suming Qiu", "Junjie Huang", "Haijia Sun", "Linyuan Qiu"], "title": "Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning", "comment": null, "summary": "Current knowledge-enhanced large language models (LLMs) rely on static,\npre-constructed knowledge bases that suffer from coverage gaps and temporal\nobsolescence, limiting their effectiveness in dynamic information environments.\nWe present Agentic-KGR, a novel framework enabling co-evolution between LLMs\nand knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our\napproach introduces three key innovations: (1) a dynamic schema expansion\nmechanism that systematically extends graph ontologies beyond pre-defined\nboundaries during training; (2) a retrieval-augmented memory system enabling\nsynergistic co-evolution between model parameters and knowledge structures\nthrough continuous optimization; (3) a learnable multi-scale prompt compression\napproach that preserves critical information while reducing computational\ncomplexity through adaptive sequence optimization. Experimental results\ndemonstrate substantial improvements over supervised baselines and single-round\nRL approaches in knowledge extraction tasks. When integrated with GraphRAG, our\nmethod achieves superior performance in downstream QA tasks, with significant\ngains in both accuracy and knowledge coverage compared to existing methods.", "AI": {"tldr": "Agentic-KGR\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7684\u534f\u540c\u8fdb\u5316\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u77e5\u8bc6\u5e93\u7684\u8986\u76d6\u4e0d\u8db3\u548c\u65f6\u6548\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u77e5\u8bc6\u7684LLMs\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u5e93\uff0c\u5b58\u5728\u8986\u76d6\u4e0d\u8db3\u548c\u65f6\u6548\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u6a21\u5f0f\u6269\u5c55\u673a\u5236\u3001\u68c0\u7d22\u589e\u5f3a\u8bb0\u5fc6\u7cfb\u7edf\u548c\u53ef\u5b66\u4e60\u591a\u5c3a\u5ea6\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u5728\u77e5\u8bc6\u62bd\u53d6\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\u548c\u5355\u8f6eRL\u65b9\u6cd5\uff0c\u4e0eGraphRAG\u96c6\u6210\u540e\u5728QA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u77e5\u8bc6\u8986\u76d6\u5ea6\u3002", "conclusion": "Agentic-KGR\u901a\u8fc7LLM\u4e0eKG\u7684\u534f\u540c\u8fdb\u5316\u6709\u6548\u89e3\u51b3\u4e86\u9759\u6001\u77e5\u8bc6\u5e93\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u52a8\u6001\u77e5\u8bc6\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09312", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09312", "abs": "https://arxiv.org/abs/2510.09312", "authors": ["Zheng Zhao", "Yeskendir Koishekenov", "Xianjun Yang", "Naila Murray", "Nicola Cancedda"], "title": "Verifying Chain-of-Thought Reasoning via Its Computational Graph", "comment": null, "summary": "Current Chain-of-Thought (CoT) verification methods predict reasoning\ncorrectness based on outputs (black-box) or activations (gray-box), but offer\nlimited insight into why a computation fails. We introduce a white-box method:\nCircuit-based Reasoning Verification (CRV). We hypothesize that attribution\ngraphs of correct CoT steps, viewed as execution traces of the model's latent\nreasoning circuits, possess distinct structural fingerprints from those of\nincorrect steps. By training a classifier on structural features of these\ngraphs, we show that these traces contain a powerful signal of reasoning\nerrors. Our white-box approach yields novel scientific insights unattainable by\nother methods. (1) We demonstrate that structural signatures of error are\nhighly predictive, establishing the viability of verifying reasoning directly\nvia its computational graph. (2) We find these signatures to be highly\ndomain-specific, revealing that failures in different reasoning tasks manifest\nas distinct computational patterns. (3) We provide evidence that these\nsignatures are not merely correlational; by using our analysis to guide\ntargeted interventions on individual transcoder features, we successfully\ncorrect the model's faulty reasoning. Our work shows that, by scrutinizing a\nmodel's computational process, we can move from simple error detection to a\ndeeper, causal understanding of LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7535\u8def\u63a8\u7406\u9a8c\u8bc1\uff08CRV\uff09\u7684\u767d\u76d2\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6b63\u786e\u548c\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\u7684\u5f52\u56e0\u56fe\u7ed3\u6784\u7279\u5f81\u6765\u9a8c\u8bc1\u63a8\u7406\u6b63\u786e\u6027\uff0c\u76f8\u6bd4\u9ed1\u76d2\u548c\u7070\u76d2\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u9519\u8bef\u539f\u56e0\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684CoT\u9a8c\u8bc1\u65b9\u6cd5\u57fa\u4e8e\u8f93\u51fa\uff08\u9ed1\u76d2\uff09\u6216\u6fc0\u6d3b\uff08\u7070\u76d2\uff09\u9884\u6d4b\u63a8\u7406\u6b63\u786e\u6027\uff0c\u4f46\u5bf9\u8ba1\u7b97\u5931\u8d25\u7684\u539f\u56e0\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6df1\u5165\u7406\u89e3\u63a8\u7406\u9519\u8bef\u539f\u56e0\u7684\u767d\u76d2\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7535\u8def\u63a8\u7406\u9a8c\u8bc1\uff08CRV\uff09\u7684\u767d\u76d2\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u5206\u6790\u6b63\u786e\u548c\u9519\u8befCoT\u6b65\u9aa4\u5f52\u56e0\u56fe\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u8fd9\u4e9b\u56fe\u88ab\u89c6\u4e3a\u6a21\u578b\u6f5c\u5728\u63a8\u7406\u7535\u8def\u7684\u6267\u884c\u8f68\u8ff9\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u793a\u63a8\u7406\u9519\u8bef\u5177\u6709\u9ad8\u5ea6\u9884\u6d4b\u6027\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5177\u6709\u9886\u57df\u7279\u5f02\u6027\uff0c\u4e14\u901a\u8fc7\u9488\u5bf9\u6027\u5e72\u9884\u53ef\u4ee5\u6210\u529f\u7ea0\u6b63\u6a21\u578b\u7684\u9519\u8bef\u63a8\u7406\u3002", "conclusion": "\u901a\u8fc7\u5ba1\u67e5\u6a21\u578b\u7684\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u4ece\u7b80\u5355\u7684\u9519\u8bef\u68c0\u6d4b\u8f6c\u5411\u5bf9LLM\u63a8\u7406\u7684\u66f4\u6df1\u5c42\u6b21\u56e0\u679c\u7406\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.09359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09359", "abs": "https://arxiv.org/abs/2510.09359", "authors": ["Eshaan Tanwar", "Deepak Nathani", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Understanding the Effects of Domain Finetuning on LLMs", "comment": null, "summary": "Large Language Models (LLMs) fine-tuned for specific domains exhibit strong\nperformance; however, the underlying mechanisms by which this fine-tuning\nreshapes their parametric space are not well understood. Prior works primarily\nfocus on auto-regressive or general-purpose instruct models, leaving\ndomain-specialised LLMs under-explored. We present the first systematic study\nof domain-specific fine-tuning in large medical language models. Our analysis\nreveals that fine-tuning modifies only a small subset of the representational\nsubspace, essentially preserving the pre-trained model's representation. To\ninterpret these changes in subspaces, we propose tuning vectors, a novel\nframework inspired by task vectors, which explicitly capture the directional\nparameter shifts induced by fine-tuning. We demonstrate that these vectors are\ncritical for enhancing both instruction-following and generation quality.\nFurthermore, combining tuning vectors across different domains yields improved\ngeneralisation. Upon closer inspection of directional alignment, we find these\nvectors primarily write new directional information into the MLP layers of the\nmodel, while amplifying existing directions in attention heads. Our findings\noffer new insights into LLM adaptation and provide a general, interpretable\nframework for analysing specialisation in large language models.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u533b\u7597\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u673a\u5236\uff0c\u53d1\u73b0\u5fae\u8c03\u4ec5\u6539\u53d8\u8868\u793a\u5b50\u7a7a\u95f4\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u63d0\u51fa\u4e86tuning vectors\u6846\u67b6\u6765\u6355\u6349\u53c2\u6570\u65b9\u5411\u6027\u53d8\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u8fd9\u4e9b\u5411\u91cf\u4e3b\u8981\u5728MLP\u5c42\u5199\u5165\u65b0\u65b9\u5411\u4fe1\u606f\u800c\u5728\u6ce8\u610f\u529b\u5c42\u653e\u5927\u5df2\u6709\u65b9\u5411\u3002", "motivation": "\u7406\u89e3\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5982\u4f55\u91cd\u5851\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u7a7a\u95f4\u673a\u5236\uff0c\u6b64\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u81ea\u56de\u5f52\u6216\u901a\u7528\u6307\u4ee4\u6a21\u578b\uff0c\u9886\u57df\u4e13\u4e1a\u5316\u6a21\u578b\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fatuning vectors\u6846\u67b6\uff08\u53d7task vectors\u542f\u53d1\uff09\u6765\u663e\u5f0f\u6355\u6349\u5fae\u8c03\u5f15\u8d77\u7684\u53c2\u6570\u65b9\u5411\u6027\u53d8\u5316\uff0c\u5206\u6790\u4e0d\u540c\u5c42\u7684\u53d8\u5316\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u5fae\u8c03\u4ec5\u4fee\u6539\u8868\u793a\u5b50\u7a7a\u95f4\u7684\u5c0f\u90e8\u5206\uff0ctuning vectors\u80fd\u63d0\u5347\u6307\u4ee4\u9075\u5faa\u548c\u751f\u6210\u8d28\u91cf\uff0c\u8de8\u9886\u57df\u7ec4\u5408\u53ef\u6539\u5584\u6cdb\u5316\u80fd\u529b\uff0cMLP\u5c42\u5199\u5165\u65b0\u65b9\u5411\u4fe1\u606f\u800c\u6ce8\u610f\u529b\u5c42\u653e\u5927\u5df2\u6709\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u9002\u5e94\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6790\u4e13\u4e1a\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2510.09222", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09222", "abs": "https://arxiv.org/abs/2510.09222", "authors": ["Zhenglin Wan", "Jingxuan Wu", "Xingrui Yu", "Chubin Zhang", "Mingcong Lei", "Bo An", "Ivor Tsang"], "title": "FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning", "comment": "20 pages", "summary": "Flow Matching (FM) has shown remarkable ability in modeling complex\ndistributions and achieves strong performance in offline imitation learning for\ncloning expert behaviors. However, despite its behavioral cloning\nexpressiveness, FM-based policies are inherently limited by their lack of\nenvironmental interaction and exploration. This leads to poor generalization in\nunseen scenarios beyond the expert demonstrations, underscoring the necessity\nof online interaction with environment. Unfortunately, optimizing FM policies\nvia online interaction is challenging and inefficient due to instability in\ngradient computation and high inference costs. To address these issues, we\npropose to let a student policy with simple MLP structure explore the\nenvironment and be online updated via RL algorithm with a reward model. This\nreward model is associated with a teacher FM model, containing rich information\nof expert data distribution. Furthermore, the same teacher FM model is utilized\nto regularize the student policy's behavior to stabilize policy learning. Due\nto the student's simple architecture, we avoid the gradient instability of FM\npolicies and enable efficient online exploration, while still leveraging the\nexpressiveness of the teacher FM model. Extensive experiments show that our\napproach significantly enhances learning efficiency, generalization, and\nrobustness, especially when learning from suboptimal expert data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Flow Matching\u6559\u5e08\u6a21\u578b\u548cMLP\u5b66\u751f\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b66\u4e60\u6548\u7387", "motivation": "Flow Matching\u5728\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u73af\u5883\u4ea4\u4e92\u548c\u63a2\u7d22\uff0c\u5bfc\u81f4\u5728\u672a\u89c1\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u76f4\u63a5\u5728\u7ebf\u4f18\u5316FM\u7b56\u7565\u5b58\u5728\u68af\u5ea6\u4e0d\u7a33\u5b9a\u548c\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898", "method": "\u4f7f\u7528MLP\u7ed3\u6784\u7684\u5b66\u751f\u7b56\u7565\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u548c\u5728\u7ebf\u66f4\u65b0\uff0c\u901a\u8fc7\u57fa\u4e8e\u6559\u5e08FM\u6a21\u578b\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u6307\u5bfc\uff0c\u5e76\u7528\u6559\u5e08FM\u6a21\u578b\u6b63\u5219\u5316\u5b66\u751f\u7b56\u7565\u884c\u4e3a\u4ee5\u7a33\u5b9a\u5b66\u4e60", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u4ece\u6b21\u4f18\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\u8868\u73b0\u7a81\u51fa", "conclusion": "\u901a\u8fc7\u7b80\u5355MLP\u5b66\u751f\u7b56\u7565\u4e0e\u590d\u6742FM\u6559\u5e08\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86FM\u7b56\u7565\u5728\u7ebf\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a2\u7d22\u548c\u7a33\u5b9a\u5b66\u4e60", "topic": "agentic reinforcement learning"}}
{"id": "2510.09418", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09418", "abs": "https://arxiv.org/abs/2510.09418", "authors": ["Yavuz Durmazkeser", "Patrik Okanovic", "Andreas Kirsch", "Torsten Hoefler", "Nezihe Merve G\u00fcrel"], "title": "Active Model Selection for Large Language Models", "comment": null, "summary": "We introduce LLM SELECTOR, the first framework for active model selection of\nLarge Language Models (LLMs). Unlike prior evaluation and benchmarking\napproaches that rely on fully annotated datasets, LLM SELECTOR efficiently\nidentifies the best LLM with limited annotations. In particular, for any given\ntask, LLM SELECTOR adaptively selects a small set of queries to annotate that\nare most informative about the best model for the task. To further reduce\nannotation cost, we leverage a judge-based oracle annotation model. Through\nextensive experiments on 6 benchmarks with 151 LLMs, we show that LLM SELECTOR\nreduces annotation costs by up to 59.62% when selecting the best and near-best\nLLM for the task.", "AI": {"tldr": "LLM SELECTOR\u662f\u4e00\u4e2a\u4e3b\u52a8\u9009\u62e9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u5c11\u91cf\u67e5\u8be2\u8fdb\u884c\u6807\u6ce8\u6765\u9ad8\u6548\u8bc6\u522b\u6700\u4f73LLM\uff0c\u76f8\u6bd4\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "motivation": "\u4f20\u7edfLLM\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7528\u6709\u9650\u6807\u6ce8\u5c31\u80fd\u8bc6\u522b\u6700\u4f73LLM\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u6a21\u578b\u9009\u62e9\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u67e5\u8be2\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u8bc4\u5224\u8005\u7684\u6807\u6ce8\u6a21\u578b\u8fdb\u4e00\u6b65\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c151\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLM SELECTOR\u5728\u9009\u62e9\u6700\u4f73\u548c\u63a5\u8fd1\u6700\u4f73LLM\u65f6\uff0c\u6807\u6ce8\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe59.62%\u3002", "conclusion": "LLM SELECTOR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684LLM\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bc4\u4f30\u6210\u672c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.09421", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09421", "abs": "https://arxiv.org/abs/2510.09421", "authors": ["Victor Morand", "Josiane Mothe", "Benjamin Piwowarski"], "title": "On the Representations of Entities in Auto-regressive Large Language Models", "comment": "Accepted at BlackBoxNLP@EMNLP2025", "summary": "Named entities are fundamental building blocks of knowledge in text,\ngrounding factual information and structuring relationships within language.\nDespite their importance, it remains unclear how Large Language Models (LLMs)\ninternally represent entities. Prior research has primarily examined explicit\nrelationships, but little is known about entity representations themselves. We\nintroduce entity mention reconstruction as a novel framework for studying how\nLLMs encode and manipulate entities. We investigate whether entity mentions can\nbe generated from internal representations, how multi-token entities are\nencoded beyond last-token embeddings, and whether these representations capture\nrelational knowledge. Our proposed method, leveraging _task vectors_, allows to\nconsistently generate multi-token mentions from various entity representations\nderived from the LLMs hidden states. We thus introduce the _Entity Lens_,\nextending the _logit-lens_ to predict multi-token mentions. Our results bring\nnew evidence that LLMs develop entity-specific mechanisms to represent and\nmanipulate any multi-token entities, including those unseen during training.\nOur code is avalable at https://github.com/VictorMorand/EntityRepresentations .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5b9e\u4f53\u63d0\u53ca\u91cd\u6784\u6846\u67b6\u6765\u7814\u7a76LLM\u5982\u4f55\u7f16\u7801\u548c\u64cd\u4f5c\u5b9e\u4f53\uff0c\u901a\u8fc7\u4efb\u52a1\u5411\u91cf\u65b9\u6cd5\u4eceLLM\u9690\u85cf\u72b6\u6001\u751f\u6210\u591atoken\u5b9e\u4f53\u63d0\u53ca\uff0c\u63ed\u793a\u4e86LLM\u5177\u6709\u5b9e\u4f53\u7279\u5b9a\u673a\u5236\u6765\u8868\u793a\u548c\u64cd\u4f5c\u591atoken\u5b9e\u4f53\u3002", "motivation": "\u7814\u7a76LLM\u5185\u90e8\u5982\u4f55\u8868\u793a\u5b9e\u4f53\uff0c\u76ee\u524d\u5bf9\u5b9e\u4f53\u8868\u793a\u672c\u8eab\u4e86\u89e3\u751a\u5c11\uff0c\u9700\u8981\u63a2\u7d22LLM\u5982\u4f55\u7f16\u7801\u548c\u64cd\u4f5c\u5b9e\u4f53\uff0c\u7279\u522b\u662f\u591atoken\u5b9e\u4f53\u7684\u8868\u793a\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u5b9e\u4f53\u63d0\u53ca\u91cd\u6784\u6846\u67b6\uff0c\u5229\u7528\u4efb\u52a1\u5411\u91cf\u65b9\u6cd5\u4eceLLM\u9690\u85cf\u72b6\u6001\u4e00\u81f4\u751f\u6210\u591atoken\u5b9e\u4f53\u63d0\u53ca\uff0c\u5f15\u5165Entity Lens\u6269\u5c55logit-lens\u6765\u9884\u6d4b\u591atoken\u63d0\u53ca\u3002", "result": "\u53d1\u73b0LLM\u5f00\u53d1\u4e86\u5b9e\u4f53\u7279\u5b9a\u673a\u5236\u6765\u8868\u793a\u548c\u64cd\u4f5c\u4efb\u4f55\u591atoken\u5b9e\u4f53\uff0c\u5305\u62ec\u8bad\u7ec3\u671f\u95f4\u672a\u89c1\u8fc7\u7684\u5b9e\u4f53\uff0c\u5b9e\u4f53\u8868\u793a\u6355\u83b7\u4e86\u5173\u7cfb\u77e5\u8bc6\u3002", "conclusion": "LLM\u5177\u6709\u4e13\u95e8\u673a\u5236\u6765\u7f16\u7801\u548c\u64cd\u4f5c\u5b9e\u4f53\u8868\u793a\uff0c\u8fd9\u4e9b\u8868\u793a\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u548c\u5173\u7cfb\u4fe1\u606f\uff0cEntity Lens\u4e3a\u7814\u7a76\u5b9e\u4f53\u8868\u793a\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2510.09330", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09330", "abs": "https://arxiv.org/abs/2510.09330", "authors": ["Tuan Nguyen", "Long Tran-Thanh"], "title": "Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers", "comment": null, "summary": "Ensuring that large language models (LLMs) comply with safety requirements is\na central challenge in AI deployment. Existing alignment approaches primarily\noperate during training, such as through fine-tuning or reinforcement learning\nfrom human feedback, but these methods are costly and inflexible, requiring\nretraining whenever new requirements arise. Recent efforts toward\ninference-time alignment mitigate some of these limitations but still assume\naccess to model internals, which is impractical, and not suitable for third\nparty stakeholders who do not have access to the models. In this work, we\npropose a model-independent, black-box framework for safety alignment that does\nnot require retraining or access to the underlying LLM architecture. As a proof\nof concept, we address the problem of trading off between generating safe but\nuninformative answers versus helpful yet potentially risky ones. We formulate\nthis dilemma as a two-player zero-sum game whose minimax equilibrium captures\nthe optimal balance between safety and helpfulness. LLM agents operationalize\nthis framework by leveraging a linear programming solver at inference time to\ncompute equilibrium strategies. Our results demonstrate the feasibility of\nblack-box safety alignment, offering a scalable and accessible pathway for\nstakeholders, including smaller organizations and entities in\nresource-constrained settings, to enforce safety across rapidly evolving LLM\necosystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u9ed1\u76d2\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u5e73\u8861\u5b89\u5168\u6027\u548c\u5e2e\u52a9\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e0d\u7075\u6d3b\uff0c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\uff0c\u4e0d\u9002\u5408\u7b2c\u4e09\u65b9\u5229\u76ca\u76f8\u5173\u8005\u3002", "method": "\u5c06\u5b89\u5168\u4e0e\u5e2e\u52a9\u6027\u7684\u6743\u8861\u5efa\u6a21\u4e3a\u4e24\u4eba\u96f6\u548c\u535a\u5f08\uff0c\u4f7f\u7528\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u5668\u5728\u63a8\u7406\u65f6\u8ba1\u7b97\u5747\u8861\u7b56\u7565\u3002", "result": "\u8bc1\u660e\u4e86\u9ed1\u76d2\u5b89\u5168\u5bf9\u9f50\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b89\u5168\u6267\u884c\u9014\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5feb\u901f\u53d1\u5c55\u7684LLM\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u7684\u5b89\u5168\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.09517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09517", "abs": "https://arxiv.org/abs/2510.09517", "authors": ["Yuchen Lu", "Run Yang", "Yichen Zhang", "Shuguang Yu", "Runpeng Dai", "Ziwei Wang", "Jiayi Xiang", "Wenxin E", "Siran Gao", "Xinyao Ruan", "Yirui Huang", "Chenjing Xi", "Haibo Hu", "Yueming Fu", "Qinglan Yu", "Xiaobing Wei", "Jiani Gu", "Rui Sun", "Jiaxuan Jia", "Fan Zhou"], "title": "StatEval: A Comprehensive Benchmark for Large Language Models in Statistics", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable advances in\nmathematical and logical reasoning, yet statistics, as a distinct and\nintegrative discipline, remains underexplored in benchmarking efforts. To\naddress this gap, we introduce \\textbf{StatEval}, the first comprehensive\nbenchmark dedicated to statistics, spanning both breadth and depth across\ndifficulty levels. StatEval consists of 13,817 foundational problems covering\nundergraduate and graduate curricula, together with 2374 research-level proof\ntasks extracted from leading journals. To construct the benchmark, we design a\nscalable multi-agent pipeline with human-in-the-loop validation that automates\nlarge-scale problem extraction, rewriting, and quality control, while ensuring\nacademic rigor. We further propose a robust evaluation framework tailored to\nboth computational and proof-based tasks, enabling fine-grained assessment of\nreasoning ability. Experimental results reveal that while closed-source models\nsuch as GPT5-mini achieve below 57\\% on research-level problems, with\nopen-source models performing significantly lower. These findings highlight the\nunique challenges of statistical reasoning and the limitations of current LLMs.\nWe expect StatEval to serve as a rigorous benchmark for advancing statistical\nintelligence in large language models. All data and code are available on our\nweb platform: https://stateval.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86StatEval\uff0c\u9996\u4e2a\u5168\u9762\u7684\u7edf\u8ba1\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b13,817\u4e2a\u57fa\u7840\u95ee\u9898\u548c2,374\u4e2a\u7814\u7a76\u7ea7\u8bc1\u660e\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u7edf\u8ba1\u5b66\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u5df2\u6709\u8f83\u597d\u8986\u76d6\uff0c\u4f46\u7edf\u8ba1\u5b66\u4f5c\u4e3a\u4e00\u95e8\u72ec\u7279\u4e14\u7efc\u5408\u7684\u5b66\u79d1\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bbe\u8ba1\u4e86\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\uff0c\u81ea\u52a8\u5316\u5927\u89c4\u6a21\u95ee\u9898\u63d0\u53d6\u3001\u91cd\u5199\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u786e\u4fdd\u5b66\u672f\u4e25\u8c28\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u95ed\u6e90\u6a21\u578b\u5982GPT5-mini\u5728\u7814\u7a76\u7ea7\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u4f4e\u4e8e57%\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff0c\u51f8\u663e\u4e86\u7edf\u8ba1\u63a8\u7406\u7684\u72ec\u7279\u6311\u6218\u3002", "conclusion": "StatEval\u53ef\u4f5c\u4e3a\u63a8\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7edf\u8ba1\u667a\u80fd\u7684\u4e25\u683c\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u7edf\u8ba1\u5b66\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2510.09388", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09388", "abs": "https://arxiv.org/abs/2510.09388", "authors": ["Xinyi Wang", "Jinyi Han", "Zishang Jiang", "Tingyun Li", "Jiaqing Liang", "Sihang Jiang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness", "comment": null, "summary": "Reinforcement Learning (RL) has become a key driver for enhancing the long\nchain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs).\nHowever, prevalent methods like GRPO often fail when task difficulty exceeds\nthe model's capacity, leading to reward sparsity and inefficient training.\nWhile prior work attempts to mitigate this using off-policy data, such as\nmixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide\npolicy updates In this work, we identify a core issue underlying these\nfailures, which we term low training affinity. This condition arises from a\nlarge distributional mismatch between external guidance and the model's policy.\nTo diagnose this, we introduce Affinity, the first quantitative metric for\nmonitoring exploration efficiency and training stability. To improve Affinity,\nwe propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness,\nan adaptive hinting framework. Instead of providing direct answers, HINT\nsupplies heuristic hints that guide the model to discover solutions on its own,\npreserving its autonomous reasoning capabilities. Extensive experiments on\nmathematical reasoning tasks show that HINT consistently outperforms existing\nmethods, achieving state-of-the-art results with models of various scales,\nwhile also demonstrating significantly more stable learning and greater data\nefficiency.Code is available on Github.", "AI": {"tldr": "\u63d0\u51faHINT\u6846\u67b6\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u4eb2\u548c\u529b\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u4f9b\u542f\u53d1\u5f0f\u63d0\u793a\u800c\u975e\u76f4\u63a5\u7b54\u6848\uff0c\u5f15\u5bfc\u6a21\u578b\u81ea\u4e3b\u53d1\u73b0\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5728\u4efb\u52a1\u96be\u5ea6\u8d85\u8fc7\u6a21\u578b\u80fd\u529b\u65f6\u4f1a\u51fa\u73b0\u5956\u52b1\u7a00\u758f\u548c\u8bad\u7ec3\u4f4e\u6548\u95ee\u9898\uff0c\u800c\u6df7\u5408SFT\u6216\u4f7f\u7528\u63d0\u793a\u7684\u65b9\u6cd5\u5f80\u5f80\u8bef\u5bfc\u7b56\u7565\u66f4\u65b0\uff0c\u6838\u5fc3\u95ee\u9898\u662f\u8bad\u7ec3\u4eb2\u548c\u529b\u4f4e\u3002", "method": "\u5f15\u5165Affinity\u6307\u6807\u76d1\u63a7\u63a2\u7d22\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u63d0\u51faHINT\u81ea\u9002\u5e94\u63d0\u793a\u6846\u67b6\uff0c\u63d0\u4f9b\u542f\u53d1\u5f0f\u63d0\u793a\u800c\u975e\u76f4\u63a5\u7b54\u6848\uff0c\u4fdd\u6301\u6a21\u578b\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHINT\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u5b66\u4e60\u66f4\u7a33\u5b9a\u4e14\u6570\u636e\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "HINT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2d\u7684\u4eb2\u548c\u529b\u95ee\u9898\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u63d0\u793a\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09535", "abs": "https://arxiv.org/abs/2510.09535", "authors": ["Feifan Song", "Shaohang Wei", "Bofei Gao", "Yejie Wang", "Wen Luo", "Wei Li", "Linli Yao", "Weimin Xiong", "Liang Chen", "Tianyu Liu", "Houfeng Wang"], "title": "Mitigating Overthinking through Reasoning Shaping", "comment": null, "summary": "Large reasoning models (LRMs) boosted by Reinforcement Learning from Verifier\nReward (RLVR) have shown great power in problem solving, yet they often cause\noverthinking: excessive, meandering reasoning that inflates computational cost.\nPrior designs of penalization in RLVR manage to reduce token consumption while\noften harming model performance, which arises from the oversimplicity of\ntoken-level supervision. In this paper, we argue that the granularity of\nsupervision plays a crucial role in balancing efficiency and accuracy, and\npropose Group Relative Segment Penalization (GRSP), a step-level method to\nregularize reasoning. Since preliminary analyses show that reasoning segments\nare strongly correlated with token consumption and model performance, we design\na length-aware weighting mechanism across segment clusters. Extensive\nexperiments demonstrate that GRSP achieves superior token efficiency without\nheavily compromising accuracy, especially the advantages with harder problems.\nMoreover, GRSP stabilizes RL training and scales effectively across model\nsizes.", "AI": {"tldr": "\u63d0\u51faGRSP\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u76d1\u7763\u6765\u5e73\u8861\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u8fc7\u5ea6\u601d\u8003\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9a8c\u8bc1\u5668\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u51cf\u5c11token\u6d88\u8017\u65f6\u5f80\u5f80\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u76d1\u7763\u6765\u5e73\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027", "method": "\u63d0\u51faGroup Relative Segment Penalization (GRSP)\uff0c\u57fa\u4e8e\u63a8\u7406\u6b65\u9aa4\u7684\u6bb5\u7ea7\u60e9\u7f5a\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u8de8\u6bb5\u7c07\u7684\u957f\u5ea6\u611f\u77e5\u6743\u91cd\u673a\u5236", "result": "GRSP\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347token\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u8fd8\u80fd\u7a33\u5b9aRL\u8bad\u7ec3\u5e76\u6709\u6548\u6269\u5c55\u5230\u4e0d\u540c\u6a21\u578b\u89c4\u6a21", "conclusion": "\u76d1\u7763\u7c92\u5ea6\u5bf9\u5e73\u8861\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0cGRSP\u901a\u8fc7\u6b65\u9aa4\u7ea7\u76d1\u7763\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861", "topic": "agentic reinforcement learning"}}
{"id": "2510.09536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09536", "abs": "https://arxiv.org/abs/2510.09536", "authors": ["Yihong Liu", "Raoyuan Zhao", "Lena Altinger", "Hinrich Sch\u00fctze", "Michael A. Hedderich"], "title": "Evaluating Robustness of Large Language Models Against Multilingual Typographical Errors", "comment": "preprint", "summary": "Large language models (LLMs) are increasingly deployed in multilingual,\nreal-world applications with user inputs -- naturally introducing typographical\nerrors (typos). Yet most benchmarks assume clean input, leaving the robustness\nof LLMs to typos across languages largely underexplored. To address this gap,\nwe introduce MulTypo, a multilingual typo generation algorithm that simulates\nhuman-like errors based on language-specific keyboard layouts and typing\nbehavior. We evaluate 18 open-source LLMs across three model families and five\ndownstream tasks spanning language inference, multi-choice question answering,\nmathematical reasoning, and machine translation tasks. Our results show that\ntypos consistently degrade performance, particularly in generative tasks and\nthose requiring reasoning -- while the natural language inference task is\ncomparatively more robust. Instruction tuning improves clean-input performance\nbut may increase brittleness under noise. We also observe language-dependent\nrobustness: high-resource languages are generally more robust than low-resource\nones, and translation from English is more robust than translation into\nEnglish. Our findings underscore the need for noise-aware training and\nmultilingual robustness evaluation. We make our code and data publicly\navailable.", "AI": {"tldr": "MulTypo\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u62fc\u5199\u9519\u8bef\u751f\u6210\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5305\u542b\u62fc\u5199\u9519\u8bef\u7684\u8f93\u5165\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u62fc\u5199\u9519\u8bef\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u6027\u4efb\u52a1\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4e14\u4e0d\u540c\u8bed\u8a00\u7684\u9c81\u68d2\u6027\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u5047\u8bbe\u8f93\u5165\u662f\u5e72\u51c0\u7684\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u7528\u6237\u8f93\u5165\u5f80\u5f80\u5305\u542b\u62fc\u5199\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30LLM\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5bf9\u62fc\u5199\u9519\u8bef\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faMulTypo\u7b97\u6cd5\uff0c\u57fa\u4e8e\u8bed\u8a00\u7279\u5b9a\u7684\u952e\u76d8\u5e03\u5c40\u548c\u6253\u5b57\u884c\u4e3a\u6a21\u62df\u4eba\u7c7b\u62fc\u5199\u9519\u8bef\uff0c\u8bc4\u4f30\u4e8618\u4e2a\u5f00\u6e90LLM\u57285\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u62fc\u5199\u9519\u8bef\u6301\u7eed\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u751f\u6210\u6027\u4efb\u52a1\u548c\u63a8\u7406\u4efb\u52a1\u53d7\u5f71\u54cd\u6700\u4e25\u91cd\uff1b\u6307\u4ee4\u8c03\u4f18\u63d0\u9ad8\u4e86\u5e72\u51c0\u8f93\u5165\u7684\u6027\u80fd\u4f46\u53ef\u80fd\u589e\u52a0\u5bf9\u566a\u58f0\u7684\u8106\u5f31\u6027\uff1b\u9ad8\u8d44\u6e90\u8bed\u8a00\u6bd4\u4f4e\u8d44\u6e90\u8bed\u8a00\u66f4\u9c81\u68d2\u3002", "conclusion": "\u9700\u8981\u8fdb\u884c\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u548c\u591a\u8bed\u8a00\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2510.09541", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09541", "abs": "https://arxiv.org/abs/2510.09541", "authors": ["Chengyu Wang", "Paria Rashidinejad", "DiJia Su", "Song Jiang", "Sid Wang", "Siyan Zhao", "Cai Zhou", "Shannon Zejiang Shen", "Feiyu Chen", "Tommi Jaakkola", "Yuandong Tian", "Bo Liu"], "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models", "comment": null, "summary": "Diffusion large language models (dLLMs) are emerging as an efficient\nalternative to autoregressive models due to their ability to decode multiple\ntokens in parallel. However, aligning dLLMs with human preferences or\ntask-specific rewards via reinforcement learning (RL) is challenging because\ntheir intractable log-likelihood precludes the direct application of standard\npolicy gradient methods. While prior work uses surrogates like the evidence\nlower bound (ELBO), these one-sided approximations can introduce significant\npolicy gradient bias. To address this, we propose the Sandwiched Policy\nGradient (SPG) that leverages both an upper and a lower bound of the true\nlog-likelihood. Experiments show that SPG significantly outperforms baselines\nbased on ELBO or one-step estimation. Specifically, SPG improves the accuracy\nover state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500,\n18.4% in Countdown and 27.0% in Sudoku.", "AI": {"tldr": "\u63d0\u51fa\u4e86Sandwiched Policy Gradient (SPG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5bf9\u6570\u4f3c\u7136\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u6765\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u6570\u4f3c\u7136\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6570\u5b66\u63a8\u7406\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u56e0\u5176\u5e76\u884c\u89e3\u7801\u80fd\u529b\u6210\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u9ad8\u6548\u66ff\u4ee3\u54c1\uff0c\u4f46\u7531\u4e8e\u5176\u96be\u4ee5\u5904\u7406\u7684\u5bf9\u6570\u4f3c\u7136\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u6807\u51c6\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u8fdb\u884c\u4eba\u7c7b\u504f\u597d\u6216\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u7684\u5bf9\u9f50\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528ELBO\u7b49\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u5355\u8fb9\u8fd1\u4f3c\u4f1a\u5f15\u5165\u663e\u8457\u7684\u7b56\u7565\u68af\u5ea6\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86Sandwiched Policy Gradient (SPG)\u65b9\u6cd5\uff0c\u5229\u7528\u771f\u5b9e\u5bf9\u6570\u4f3c\u7136\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u6765\u51cf\u5c11\u7b56\u7565\u68af\u5ea6\u504f\u5dee\uff0c\u76f8\u6bd4\u57fa\u4e8eELBO\u6216\u4e00\u6b65\u4f30\u8ba1\u7684\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002", "result": "SPG\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1a\u5728GSM8K\u4e0a\u51c6\u786e\u7387\u63d0\u53473.6%\uff0c\u5728MATH500\u4e0a\u63d0\u53472.6%\uff0c\u5728Countdown\u4e0a\u63d0\u534718.4%\uff0c\u5728Sudoku\u4e0a\u63d0\u534727.0%\u3002", "conclusion": "SPG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86dLLMs\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u6570\u4f3c\u7136\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0a\u4e0b\u754c\u7ea6\u675f\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u68af\u5ea6\u7684\u51c6\u786e\u6027\uff0c\u5728\u5404\u79cd\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09555", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09555", "abs": "https://arxiv.org/abs/2510.09555", "authors": ["Raoyuan Zhao", "Yihong Liu", "Hinrich Sch\u00fctze", "Michael A. Hedderich"], "title": "A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages", "comment": "preprint", "summary": "Large reasoning models (LRMs) increasingly rely on step-by-step\nChain-of-Thought (CoT) reasoning to improve task performance, particularly in\nhigh-resource languages such as English. While recent work has examined\nfinal-answer accuracy in multilingual settings, the thinking traces themselves,\ni.e., the intermediate steps that lead to the final answer, remain\nunderexplored. In this paper, we present the first comprehensive study of\nmultilingual CoT reasoning, evaluating three key dimensions: performance,\nconsistency, and faithfulness. We begin by measuring language compliance,\nanswer accuracy, and answer consistency when LRMs are explicitly instructed or\nprompt-hacked to think in a target language, revealing strong language\npreferences and divergent performance across languages. Next, we assess\ncrosslingual consistency of thinking traces by interchanging them between\nlanguages. We find that the quality and effectiveness of thinking traces vary\nsubstantially depending on the prompt language. Finally, we adapt\nperturbation-based techniques -- i.e., truncation and error injection -- to\nprobe the faithfulness of thinking traces across languages, showing that models\nrely on traces to varying degrees. We release our code and data to support\nfuture research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u591a\u8bed\u8a00\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u6027\u80fd\u3001\u4e00\u81f4\u6027\u548c\u5fe0\u5b9e\u6027\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u4e0b\u601d\u7ef4\u8f68\u8ff9\u7684\u8d28\u91cf\u548c\u6709\u6548\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u867d\u7136\u5927\u578b\u63a8\u7406\u6a21\u578b\u4f9d\u8d56\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u6765\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u5bfc\u81f4\u6700\u7ec8\u7b54\u6848\u7684\u4e2d\u95f4\u601d\u7ef4\u8f68\u8ff9\u672c\u8eab\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u8bed\u8a00\u5408\u89c4\u6027\u3001\u7b54\u6848\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff1b\u901a\u8fc7\u8de8\u8bed\u8a00\u4ea4\u6362\u601d\u7ef4\u8f68\u8ff9\u8bc4\u4f30\u4e00\u81f4\u6027\uff1b\u91c7\u7528\u622a\u65ad\u548c\u9519\u8bef\u6ce8\u5165\u7b49\u6270\u52a8\u6280\u672f\u6765\u63a2\u6d4b\u601d\u7ef4\u8f68\u8ff9\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e0b\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u8bed\u8a00\u504f\u597d\u548c\u4e0d\u540c\u7684\u6027\u80fd\u8868\u73b0\uff1b\u601d\u7ef4\u8f68\u8ff9\u7684\u8d28\u91cf\u548c\u6709\u6548\u6027\u56e0\u63d0\u793a\u8bed\u8a00\u800c\u5f02\uff1b\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e0b\u5bf9\u601d\u7ef4\u8f68\u8ff9\u7684\u4f9d\u8d56\u7a0b\u5ea6\u5404\u4e0d\u76f8\u540c\u3002", "conclusion": "\u591a\u8bed\u8a00\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5b58\u5728\u663e\u8457\u7684\u8bed\u8a00\u4f9d\u8d56\u6027\uff0c\u601d\u7ef4\u8f68\u8ff9\u7684\u8d28\u91cf\u548c\u6709\u6548\u6027\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5dee\u5f02\u660e\u663e\uff0c\u8fd9\u5bf9\u591a\u8bed\u8a00\u63a8\u7406\u7cfb\u7edf\u7684\u5f00\u53d1\u5177\u6709\u91cd\u8981\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2510.09462", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.09462", "abs": "https://arxiv.org/abs/2510.09462", "authors": ["Mikhail Terekhov", "Alexander Panfilov", "Daniil Dzenhaliou", "Caglar Gulcehre", "Maksym Andriushchenko", "Ameya Prabhu", "Jonas Geiping"], "title": "Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols", "comment": null, "summary": "AI control protocols serve as a defense mechanism to stop untrusted LLM\nagents from causing harm in autonomous settings. Prior work treats this as a\nsecurity problem, stress testing with exploits that use the deployment context\nto subtly complete harmful side tasks, such as backdoor insertion. In practice,\nmost AI control protocols are fundamentally based on LLM monitors, which can\nbecome a central point of failure. We study adaptive attacks by an untrusted\nmodel that knows the protocol and the monitor model, which is plausible if the\nuntrusted model was trained with a later knowledge cutoff or can search for\nthis information autonomously. We instantiate a simple adaptive attack vector\nby which the attacker embeds publicly known or zero-shot prompt injections in\nthe model outputs. Using this tactic, frontier models consistently evade\ndiverse monitors and complete malicious tasks on two main AI control\nbenchmarks. The attack works universally against current protocols that rely on\na monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,\nas its resampling amplifies the prompt injection and effectively reframes it as\na best-of-$n$ attack. In general, adaptive attacks on monitor models represent\na major blind spot in current control protocols and should become a standard\ncomponent of evaluations for future AI control mechanisms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9488\u5bf9AI\u63a7\u5236\u534f\u8bae\u7684\u81ea\u9002\u5e94\u653b\u51fb\uff0c\u53d1\u73b0\u5f53\u524d\u57fa\u4e8eLLM\u76d1\u63a7\u5668\u7684\u63a7\u5236\u534f\u8bae\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u5d4c\u5165\u516c\u5f00\u5df2\u77e5\u6216\u96f6\u6837\u672c\u63d0\u793a\u6ce8\u5165\u6765\u9003\u907f\u76d1\u63a7\u5e76\u5b8c\u6210\u6076\u610f\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709AI\u63a7\u5236\u534f\u8bae\u4e3b\u8981\u4f9d\u8d56LLM\u76d1\u63a7\u5668\u4f5c\u4e3a\u9632\u5fa1\u673a\u5236\uff0c\u4f46\u8fd9\u4e9b\u76d1\u63a7\u5668\u53ef\u80fd\u6210\u4e3a\u5355\u70b9\u6545\u969c\u3002\u8bba\u6587\u65e8\u5728\u7814\u7a76\u5f53\u653b\u51fb\u8005\u4e86\u89e3\u534f\u8bae\u548c\u76d1\u63a7\u6a21\u578b\u65f6\u7684\u81ea\u9002\u5e94\u653b\u51fb\u5a01\u80c1\u3002", "method": "\u5b9e\u4f8b\u5316\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u81ea\u9002\u5e94\u653b\u51fb\u5411\u91cf\uff0c\u653b\u51fb\u8005\u5728\u6a21\u578b\u8f93\u51fa\u4e2d\u5d4c\u5165\u516c\u5f00\u5df2\u77e5\u6216\u96f6\u6837\u672c\u63d0\u793a\u6ce8\u5165\uff0c\u5e76\u5728\u4e24\u4e2a\u4e3b\u8981AI\u63a7\u5236\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u8fd9\u79cd\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u80fd\u591f\u6301\u7eed\u9003\u907f\u5404\u79cd\u76d1\u63a7\u5668\u5e76\u5b8c\u6210\u6076\u610f\u4efb\u52a1\uff0c\u653b\u51fb\u5bf9\u5f53\u524d\u4f9d\u8d56\u76d1\u63a7\u5668\u7684\u534f\u8bae\u5177\u6709\u666e\u904d\u6709\u6548\u6027\uff0c\u751a\u81f3Defer-to-Resample\u534f\u8bae\u4f1a\u9002\u5f97\u5176\u53cd\u3002", "conclusion": "\u5bf9\u76d1\u63a7\u6a21\u578b\u7684\u81ea\u9002\u5e94\u653b\u51fb\u662f\u5f53\u524d\u63a7\u5236\u534f\u8bae\u7684\u4e3b\u8981\u76f2\u70b9\uff0c\u5e94\u8be5\u6210\u4e3a\u672a\u6765AI\u63a7\u5236\u673a\u5236\u8bc4\u4f30\u7684\u6807\u51c6\u7ec4\u6210\u90e8\u5206\u3002", "topic": "agent analysis"}}
{"id": "2510.09577", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09577", "abs": "https://arxiv.org/abs/2510.09577", "authors": ["Xiao Yu", "Baolin Peng", "Michel Galley", "Hao Cheng", "Qianhui Wu", "Janardhan Kulkarni", "Suman Nath", "Zhou Yu", "Jianfeng Gao"], "title": "Dyna-Mind: Learning to Simulate from Experience for Better AI Agents", "comment": null, "summary": "Reasoning models have recently shown remarkable progress in domains such as\nmath and coding. However, their expert-level abilities in math and coding\ncontrast sharply with their performance in long-horizon, interactive tasks such\nas web navigation and computer/phone-use. Inspired by literature on human\ncognition, we argue that current AI agents need ''vicarious trial and error'' -\nthe capacity to mentally simulate alternative futures before acting - in order\nto enhance their understanding and performance in complex interactive\nenvironments. We introduce Dyna-Mind, a two-stage training framework that\nexplicitly teaches (V)LM agents to integrate such simulation into their\nreasoning. In stage 1, we introduce Reasoning with Simulations (ReSim), which\ntrains the agent to generate structured reasoning traces from expanded search\ntrees built from real experience gathered through environment interactions.\nReSim thus grounds the agent's reasoning in faithful world dynamics and equips\nit with the ability to anticipate future states in its reasoning. In stage 2,\nwe propose Dyna-GRPO, an online reinforcement learning method to further\nstrengthen the agent's simulation and decision-making ability by using both\noutcome rewards and intermediate states as feedback from real rollouts.\nExperiments on two synthetic benchmarks (Sokoban and ALFWorld) and one\nrealistic benchmark (AndroidWorld) demonstrate that (1) ReSim effectively\ninfuses simulation ability into AI agents, and (2) Dyna-GRPO leverages outcome\nand interaction-level signals to learn better policies for long-horizon,\nplanning-intensive tasks. Together, these results highlight the central role of\nsimulation in enabling AI agents to reason, plan, and act more effectively in\nthe ever more challenging environments.", "AI": {"tldr": "\u63d0\u51faDyna-Mind\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u589e\u5f3aAI\u4ee3\u7406\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u957f\u65f6\u7a0b\u4ea4\u4e92\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6a21\u62df\u672a\u6765\u72b6\u6001\u7684\u80fd\u529b", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5ReSim\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u751f\u6210\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5Dyna-GRPO\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u51b3\u7b56", "result": "\u5728Sokoban\u3001ALFWorld\u548cAndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u6a21\u62df\u548c\u51b3\u7b56\u80fd\u529b", "conclusion": "\u6a21\u62df\u80fd\u529b\u5bf9\u4e8eAI\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u6709\u6548\u63a8\u7406\u3001\u89c4\u5212\u548c\u884c\u52a8\u81f3\u5173\u91cd\u8981", "topic": "agentic reinforcement learning"}}
{"id": "2510.09599", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09599", "abs": "https://arxiv.org/abs/2510.09599", "authors": ["Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "title": "Prompting Test-Time Scaling Is A Strong LLM Reasoning Data Augmentation", "comment": "Our code and data are available at https://github.com/VILA-Lab/PTTS", "summary": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities when provided with chain-of-thought exemplars, but curating large\nreasoning datasets remains laborious and resource-intensive. In this work, we\nintroduce Prompting Test-Time Scaling (P-TTS), a simple yet effective\ninference-time data augmentation strategy for enhancing LLM reasoning through\nfinetuning. Rather than collecting thousands or even millions of examples,\nP-TTS leverages a small pool of only 90 manually selected reasoning instances\nand systematically varies exemplar augmentation through principled instruction\nprompting intensities at test time to synthesize diverse reasoning trajectory\ncontexts. Then we finetune the various sizes of Qwen-2.5 models on P-TTS data.\nAcross a suite of mathematical reasoning AIME2024 & 25, MATH500, and\nGPQA-Diamond, our P-TTS-7B and 32B models outperform the prior competitive\nbaselines like S1 and S1.1 (1K-shot), achieving absolute accuracy gains of\n+26.66% and +30.00% on AIME'24 (7B), and +13.34% and +6.67% on AIME'25 (7B);\nP-TTS-32B yields gains of +23.33% and +16.63% on AIME'24, and +26.63% and\n+3.33% on AIME'25 (vs. S1 and S1.1, respectively), with comparable or better\nperformance on MATH500 and GPQA-Diamond. We further show that P-TTS enhances\nzero-shot generalization accuracy on out-of-domain reasoning benchmarks of\nGaokao, Kaoyan, OlympiadBench, AMC23, GradeSchoolMath, and Minerva. Our\nanalysis suggests that test-time scaling effectively explores the latent space\nof reasoning patterns, amplifying LLM problem-solving with minimal annotation\noverhead, and further unlocking the reasoning potential and capabilities of\nLLMs. Prompting Test-Time Scaling offers a practical, low-cost way to elicit\nLLM reasoning in resource-constrained or rapidly evolving domains.", "AI": {"tldr": "P-TTS\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4ec5\u4f7f\u752890\u4e2a\u624b\u52a8\u9009\u62e9\u7684\u4e8b\u4f8b\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6539\u53d8\u63d0\u793a\u5f3a\u5ea6\u6765\u5408\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u4ece\u800c\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u63a8\u7406\u6570\u636e\u96c6\u7684\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684LLM\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4ec590\u4e2a\u624b\u52a8\u9009\u62e9\u7684\u63a8\u7406\u5b9e\u4f8b\uff0c\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u539f\u5219\u6027\u6307\u4ee4\u63d0\u793a\u5f3a\u5ea6\u7cfb\u7edf\u5316\u53d8\u5316\u6765\u5408\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u5bf9Qwen-2.5\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c7B\u548c32B\u6a21\u578b\u5728AIME'24\u548cAIME'25\u4e0a\u5206\u522b\u83b7\u5f97+26.66%\u5230+30.00%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u6709\u6548\u63a2\u7d22\u4e86\u63a8\u7406\u6a21\u5f0f\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u6700\u5c0f\u6807\u6ce8\u5f00\u9500\u653e\u5927\u4e86LLM\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u6216\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f4e\u6210\u672cLLM\u63a8\u7406\u6fc0\u53d1\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.09487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09487", "abs": "https://arxiv.org/abs/2510.09487", "authors": ["Shangzhe Li", "Dongruo Zhou", "Weitong Zhang"], "title": "Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning", "comment": "48 pages, 3 figures, 4 tables", "summary": "We study online adversarial imitation learning (AIL), where an agent learns\nfrom offline expert demonstrations and interacts with the environment online\nwithout access to rewards. Despite strong empirical results, the benefits of\nonline interaction and the impact of stochasticity remain poorly understood. We\naddress these gaps by introducing a model-based AIL algorithm (MB-AIL) and\nestablish its horizon-free, second-order sample-complexity guarantees under\ngeneral function approximations for both expert data and reward-free\ninteractions. These second-order bounds provide an instance-dependent result\nthat can scale with the variance of returns under the relevant policies and\ntherefore tighten as the system approaches determinism. Together with\nsecond-order, information-theoretic lower bounds on a newly constructed\nhard-instance family, we show that MB-AIL attains minimax-optimal sample\ncomplexity for online interaction (up to logarithmic factors) with limited\nexpert demonstrations and matches the lower bound for expert demonstrations in\nterms of the dependence on horizon $H$, precision $\\epsilon$ and the policy\nvariance $\\sigma^2$. Experiments further validate our theoretical findings and\ndemonstrate that a practical implementation of MB-AIL matches or surpasses the\nsample efficiency of existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5(MB-AIL)\uff0c\u5728\u65e0\u5956\u52b1\u7684\u5728\u7ebf\u73af\u5883\u4e2d\u4ece\u79bb\u7ebf\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\uff0c\u5efa\u7acb\u4e86\u65e0\u89c6\u91ce\u9650\u5236\u7684\u4e8c\u9636\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6700\u4f18\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u4e2d\u5728\u7ebf\u4ea4\u4e92\u7684\u76ca\u5904\u548c\u968f\u673a\u6027\u5f71\u54cd\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u65e0\u5956\u52b1\u73af\u5883\u4e0b\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6a21\u578b\u7684\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5(MB-AIL)\uff0c\u4f7f\u7528\u4e00\u822c\u51fd\u6570\u903c\u8fd1\u5904\u7406\u4e13\u5bb6\u6570\u636e\u548c\u5956\u52b1\u81ea\u7531\u4ea4\u4e92\uff0c\u5efa\u7acb\u4e8c\u9636\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\u6846\u67b6\u3002", "result": "MB-AIL\u5728\u5728\u7ebf\u4ea4\u4e92\u4e2d\u8fbe\u5230\u4e86\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6(\u5bf9\u6570\u56e0\u5b50\u5185)\uff0c\u4e0e\u4e13\u5bb6\u6f14\u793a\u7684\u4e0b\u754c\u5728\u89c6\u91ceH\u3001\u7cbe\u5ea6\u03b5\u548c\u7b56\u7565\u65b9\u5dee\u03c3\u00b2\u7684\u4f9d\u8d56\u4e0a\u5339\u914d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6837\u672c\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MB-AIL\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5728\u7ebf\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u7279\u522b\u5728\u7cfb\u7edf\u63a5\u8fd1\u786e\u5b9a\u6027\u65f6\u8868\u73b0\u66f4\u597d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.f19934ba", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2F6-winning-figma-makes-and-what-you-can-learn-from-them%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/ooycE_XoUJHDDYjhZz2lK1lTHRfqwSm11U-mMcSoL9c=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2F6-winning-figma-makes-and-what-you-can-learn-from-them%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/ooycE_XoUJHDDYjhZz2lK1lTHRfqwSm11U-mMcSoL9c=426", "authors": ["TLDR Newsletter"], "title": "Six Winning Figma Makes\u2014and What You Can Learn from Them", "comment": "Source: TLDR Newsletter, Date: 2025-10-09, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.figma.com%2Fblog%2F6-winning-figma-makes-and-what-you-can-learn-from-them%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/ooycE_XoUJHDDYjhZz2lK1lTHRfqwSm11U-mMcSoL9c=426", "summary": "Six Winning Figma Makes\u2014and What You Can Learn from Them (7 minute read) Figma's first global Make-a-thon attracted over 10,000 creators competing for $100,000 in prizes. Showcasing innovative projects, winners emphasized strategies such as defining clear structural requirements, refining designs before building, utilizing modular code organization, and leveraging shorter prompts for easier iteration. The competition demonstrated Figma Make's capability to bridge design and development, with ...", "source": "tldr", "AI": {"tldr": "Figma\u9996\u5c4a\u5168\u7403Make-a-thon\u5438\u5f15\u4e86\u8d85\u8fc710,000\u540d\u521b\u4f5c\u8005\u53c2\u4e0e\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u660e\u786e\u7ed3\u6784\u8981\u6c42\u3001\u8bbe\u8ba1\u4f18\u5148\u3001\u6a21\u5757\u5316\u4ee3\u7801\u7ec4\u7ec7\u548c\u7b80\u77ed\u63d0\u793a\u7b49\u7b56\u7565\u6765\u8fde\u63a5\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u7684\u80fd\u529b\u3002", "motivation": "\u5c55\u793aFigma Make\u5e73\u53f0\u5982\u4f55\u901a\u8fc7\u7ade\u8d5b\u5f62\u5f0f\u4fc3\u8fdb\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u7684\u878d\u5408\uff0c\u5e76\u5206\u4eab\u83b7\u80dc\u8005\u7684\u6210\u529f\u7b56\u7565\u3002", "method": "\u5206\u6790Figma\u9996\u5c4a\u5168\u7403Make-a-thon\u7ade\u8d5b\u4e2d\u83b7\u80dc\u9879\u76ee\u7684\u7b56\u7565\u548c\u65b9\u6cd5\uff0c\u5305\u62ec\u7ed3\u6784\u8981\u6c42\u5b9a\u4e49\u3001\u8bbe\u8ba1\u4f18\u5316\u3001\u4ee3\u7801\u7ec4\u7ec7\u548c\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u7ade\u8d5b\u5c55\u793a\u4e86Figma Make\u80fd\u591f\u6709\u6548\u8fde\u63a5\u8bbe\u8ba1\u548c\u5f00\u53d1\uff0c\u83b7\u80dc\u8005\u901a\u8fc7\u6e05\u6670\u7684\u7ed3\u6784\u89c4\u5212\u3001\u8bbe\u8ba1\u4f18\u5148\u65b9\u6cd5\u3001\u6a21\u5757\u5316\u4ee3\u7801\u548c\u7b80\u77ed\u63d0\u793a\u5b9e\u73b0\u4e86\u521b\u65b0\u9879\u76ee\u3002", "conclusion": "Figma Make-a-thon\u8bc1\u660e\u4e86\u8be5\u5e73\u53f0\u5728\u5f25\u5408\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u9e3f\u6c9f\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u83b7\u80dc\u7b56\u7565\u4e3a\u5176\u4ed6\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "topic": "swe application"}}
{"id": "tldr.2510.9698301b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/KQqYANJ-gWk893JJjpxiDaMfijWiD9muakaSXUhbRtY=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/KQqYANJ-gWk893JJjpxiDaMfijWiD9muakaSXUhbRtY=426", "authors": ["TLDR Newsletter"], "title": "Designing Agentic Loops", "comment": "Source: TLDR Newsletter, Date: 2025-10-09, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrdesign/1/01000199c8e6cbec-3225094e-4f5f-4637-9009-3def68d11f13-000000/KQqYANJ-gWk893JJjpxiDaMfijWiD9muakaSXUhbRtY=426", "summary": "Designing Agentic Loops (8 minute read) Coding agents like Claude Code can brute force solutions by running tools in loops, but require carefully designed \"agentic loops\" with appropriate tools and credentials to be effective. Running agents in YOLO mode (auto-approving commands) within sandboxed environments like GitHub Codespaces balances productivity with security risks, including data loss and exfiltration attacks. This approach works best for problems with clear success criteria involvin...", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u667a\u80fd\u4f53\u5faa\u73af\uff0c\u8ba9\u7f16\u7801\u667a\u80fd\u4f53\u5982Claude Code\u80fd\u591f\u901a\u8fc7\u5faa\u73af\u8fd0\u884c\u5de5\u5177\u6765\u66b4\u529b\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u5305\u542b\u9002\u5f53\u5de5\u5177\u548c\u51ed\u8bc1\u7684\u667a\u80fd\u4f53\u5faa\u73af\u624d\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "motivation": "\u7f16\u7801\u667a\u80fd\u4f53\u867d\u7136\u80fd\u591f\u901a\u8fc7\u5faa\u73af\u8fd0\u884c\u5de5\u5177\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u9700\u8981\u8bbe\u8ba1\u6709\u6548\u7684\u667a\u80fd\u4f53\u5faa\u73af\u673a\u5236\u6765\u5e73\u8861\u751f\u4ea7\u529b\u548c\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u660e\u786e\u6210\u529f\u6807\u51c6\u7684\u95ee\u9898\u65f6\u3002", "method": "\u5728\u6c99\u76d2\u73af\u5883\uff08\u5982GitHub Codespaces\uff09\u4e2d\u8fd0\u884c\u667a\u80fd\u4f53\u7684YOLO\u6a21\u5f0f\uff08\u81ea\u52a8\u6279\u51c6\u547d\u4ee4\uff09\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u9002\u5f53\u7684\u5b89\u5168\u63aa\u65bd\u6765\u7ba1\u7406\u6570\u636e\u4e22\u5931\u548c\u6570\u636e\u6cc4\u9732\u7b49\u98ce\u9669\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u660e\u786e\u6210\u529f\u6807\u51c6\u7684\u95ee\u9898\u65f6\u6548\u679c\u6700\u4f73\uff0c\u80fd\u591f\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u63d0\u9ad8\u751f\u4ea7\u529b\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u5faa\u73af\u7ed3\u5408\u6c99\u76d2\u73af\u5883\u4e2d\u7684YOLO\u6a21\u5f0f\u8fd0\u884c\uff0c\u662f\u5e73\u8861\u7f16\u7801\u667a\u80fd\u4f53\u751f\u4ea7\u529b\u548c\u5b89\u5168\u98ce\u9669\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.05204de5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffin.ai%2Fsolutions%2Ffinancial-services%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=f_tldr_newsletter_bmkt_ba_awar_pros_3p_fintech-vertical_namer_us_en_newsletter/2/01000199c9206c6b-58735486-11c8-4a20-b8da-e9982b0c67cb-000000/xecy3Comw-CMJA08f6QLSNxD2ICyDfF4f7YerIfUQL8=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffin.ai%2Fsolutions%2Ffinancial-services%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=f_tldr_newsletter_bmkt_ba_awar_pros_3p_fintech-vertical_namer_us_en_newsletter/2/01000199c9206c6b-58735486-11c8-4a20-b8da-e9982b0c67cb-000000/xecy3Comw-CMJA08f6QLSNxD2ICyDfF4f7YerIfUQL8=426", "authors": ["TLDR Newsletter"], "title": "Intercom's Fin AI agent achieves 70% resolution rate for finserv support tickets", "comment": "Source: TLDR Newsletter, Date: 2025-10-09, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffin.ai%2Fsolutions%2Ffinancial-services%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=f_tldr_newsletter_bmkt_ba_awar_pros_3p_fintech-vertical_namer_us_en_newsletter/2/01000199c9206c6b-58735486-11c8-4a20-b8da-e9982b0c67cb-000000/xecy3Comw-CMJA08f6QLSNxD2ICyDfF4f7YerIfUQL8=426", "summary": "Intercom's Fin AI agent achieves 70% resolution rate for finserv support tickets (Sponsor) Moneybox, Checkout .com, and Polymarket all use the best-performing AI agent for financial services \u2014 Fin. Made by Intercom, Fin: >> follows your business rules, >> maintains full audit trails, and >> integrates with any helpdesk. Unlike generic chatbots, Fin was built for regulated financial services companies. It's the only complete, fully configurable AI agent system in the industry. Try the #1 agent...", "source": "tldr", "AI": {"tldr": "Intercom\u7684Fin AI\u4ee3\u7406\u5728\u91d1\u878d\u670d\u52a1\u652f\u6301\u5de5\u5355\u4e2d\u8fbe\u523070%\u89e3\u51b3\u7387\uff0c\u4e13\u4e3a\u53d7\u76d1\u7ba1\u7684\u91d1\u878d\u670d\u52a1\u516c\u53f8\u8bbe\u8ba1", "motivation": "\u4e3a\u91d1\u878d\u670d\u52a1\u884c\u4e1a\u63d0\u4f9b\u4e13\u95e8\u5b9a\u5236\u7684AI\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u901a\u7528\u804a\u5929\u673a\u5668\u4eba\u5728\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\u7684\u5c40\u9650\u6027", "method": "\u5f00\u53d1\u4e13\u4e3a\u91d1\u878d\u670d\u52a1\u8bbe\u8ba1\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u4e1a\u52a1\u89c4\u5219\u9075\u5faa\u3001\u5b8c\u6574\u5ba1\u8ba1\u8ffd\u8e2a\u548c\u4e0e\u4efb\u4f55\u5e2e\u52a9\u53f0\u96c6\u6210", "result": "\u5728\u91d1\u878d\u670d\u52a1\u652f\u6301\u5de5\u5355\u4e2d\u5b9e\u73b070%\u7684\u89e3\u51b3\u7387\uff0c\u88abMoneybox\u3001Checkout.com\u548cPolymarket\u7b49\u516c\u53f8\u91c7\u7528", "conclusion": "Fin\u662f\u884c\u4e1a\u4e2d\u552f\u4e00\u5b8c\u6574\u3001\u5b8c\u5168\u53ef\u914d\u7f6e\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u4e13\u4e3a\u53d7\u76d1\u7ba1\u7684\u91d1\u878d\u670d\u52a1\u516c\u53f8\u91cf\u8eab\u5b9a\u5236", "topic": "swe application"}}
{"id": "tldr.2510.27cd0bf0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fwhat-makes-browser-use-hard-for-ai-agents%3Futm_campaign=TLDR_labs_blog%26utm_medium=employer-brand%26utm_source=TLDR%26utm_browser_use_hard_for_AI_agents%26utm_term=2025-october/1/01000199c920b810-e2958e8a-de32-4c7b-b730-7c28c78fa479-000000/0vEsuaEpmZj55DQaxyenTg3TTrV7Y0g1ah4WQqSKZbI=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fwhat-makes-browser-use-hard-for-ai-agents%3Futm_campaign=TLDR_labs_blog%26utm_medium=employer-brand%26utm_source=TLDR%26utm_browser_use_hard_for_AI_agents%26utm_term=2025-october/1/01000199c920b810-e2958e8a-de32-4c7b-b730-7c28c78fa479-000000/0vEsuaEpmZj55DQaxyenTg3TTrV7Y0g1ah4WQqSKZbI=426", "authors": ["TLDR Newsletter"], "title": "Cracking the Code of Browser Automation for AI", "comment": "Source: TLDR Newsletter, Date: 2025-10-09, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fwhat-makes-browser-use-hard-for-ai-agents%3Futm_campaign=TLDR_labs_blog%26utm_medium=employer-brand%26utm_source=TLDR%26utm_browser_use_hard_for_AI_agents%26utm_term=2025-october/1/01000199c920b810-e2958e8a-de32-4c7b-b730-7c28c78fa479-000000/0vEsuaEpmZj55DQaxyenTg3TTrV7Y0g1ah4WQqSKZbI=426", "summary": "Cracking the Code of Browser Automation for AI (Sponsor) Discover how Amazon's AGI Lab is revolutionizing browser automation for AI agents, achieving 90%+ reliability in early enterprise use cases. Our approach combines technical excellence with human-centric design, setting new standards for AI interaction with web interfaces.What makes browser use hard for AI Agents? \u2192 Impact the future of AI, today. Join our team \u2192", "source": "tldr", "AI": {"tldr": "\u4e9a\u9a6c\u900aAGI\u5b9e\u9a8c\u5ba4\u5f00\u53d1\u4e86\u9769\u547d\u6027\u7684\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u6280\u672f\uff0c\u4f7fAI\u4ee3\u7406\u5728\u65e9\u671f\u4f01\u4e1a\u7528\u4f8b\u4e2d\u8fbe\u523090%\u4ee5\u4e0a\u7684\u53ef\u9760\u6027\uff0c\u7ed3\u5408\u6280\u672f\u5353\u8d8a\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u5728\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u63a8\u52a8AI\u4e0e\u7f51\u7edc\u754c\u9762\u7684\u4ea4\u4e92\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u6280\u672f\u5353\u8d8a\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f00\u53d1\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u65e9\u671f\u4f01\u4e1a\u7528\u4f8b\u4e2d\u5b9e\u73b0\u4e8690%\u4ee5\u4e0a\u7684\u53ef\u9760\u6027\uff0c\u4e3aAI\u4e0e\u7f51\u7edc\u754c\u9762\u4ea4\u4e92\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u8be5\u6280\u672f\u5c06\u5f71\u54cdAI\u7684\u672a\u6765\u53d1\u5c55\uff0c\u56e2\u961f\u6b63\u5728\u62db\u52df\u4eba\u624d\u3002", "topic": "swe application"}}
{"id": "tldr.2510.469f3a2e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sweep.dev%2Fposts%2Fnext-edit-jetbrains%3Futm_source=tldr%26utm_medium=email/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/udwc2Xy1-pBQLbr5LnVkP-RB0iwuuCFh7dLGAex8ddI=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sweep.dev%2Fposts%2Fnext-edit-jetbrains%3Futm_source=tldr%26utm_medium=email/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/udwc2Xy1-pBQLbr5LnVkP-RB0iwuuCFh7dLGAex8ddI=426", "authors": ["TLDR Newsletter"], "title": "Sweep's autocomplete for Jetbrains runs in <100ms. With full codebase context", "comment": "Source: TLDR Newsletter, Date: 2025-10-10, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sweep.dev%2Fposts%2Fnext-edit-jetbrains%3Futm_source=tldr%26utm_medium=email/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/udwc2Xy1-pBQLbr5LnVkP-RB0iwuuCFh7dLGAex8ddI=426", "summary": "Sweep's autocomplete for Jetbrains runs in <100ms. With full codebase context (Sponsor) Sweep's AI autocomplete is the fastest autocomplete for JetBrains. Sweep's next-edit suggestions learn from your codebase to suggest changes anywhere in your code - while staying in your favorite IDEs like IntelliJ or PyCharm. Built from scratch with syntax-aware FIM on a custom inference stack. Get the Jetbrains plugin.", "source": "tldr", "AI": {"tldr": "Sweep\u4e3aJetBrains IDE\u5f00\u53d1\u4e86\u5feb\u901fAI\u81ea\u52a8\u8865\u5168\u529f\u80fd\uff0c\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e100\u6beb\u79d2\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u4ee3\u7801\u5e93\u7684\u667a\u80fd\u7f16\u8f91\u5efa\u8bae", "motivation": "\u89e3\u51b3\u4f20\u7edfIDE\u81ea\u52a8\u8865\u5168\u529f\u80fd\u54cd\u5e94\u6162\u3001\u7f3a\u4e4f\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u667a\u80fd\u9ad8\u6548\u7684\u5f00\u53d1\u4f53\u9a8c", "method": "\u4ece\u5934\u6784\u5efa\u8bed\u6cd5\u611f\u77e5\u7684FIM\uff08\u586b\u5145\u4e2d\u95f4\u4ee3\u7801\uff09\u6280\u672f\uff0c\u91c7\u7528\u81ea\u5b9a\u4e49\u63a8\u7406\u6808\uff0c\u652f\u6301\u5168\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587", "result": "\u5b9e\u73b0\u4e86JetBrains IDE\u4e2d\u6700\u5feb\u7684\u81ea\u52a8\u8865\u5168\u529f\u80fd\uff0c\u54cd\u5e94\u65f6\u95f4<100ms\uff0c\u5e76\u80fd\u57fa\u4e8e\u4ee3\u7801\u5e93\u5b66\u4e60\u63d0\u4f9b\u667a\u80fd\u7f16\u8f91\u5efa\u8bae", "conclusion": "Sweep\u7684AI\u81ea\u52a8\u8865\u5168\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u4e3aJetBrains\u7528\u6237\u63d0\u4f9b\u4e86\u5feb\u901f\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7f16\u7801\u8f85\u52a9\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2510.4e18c624", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrnewsletter/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/35ussuSyB16-5Iaxgf_h6I58cdikwVMMgVrAkmcX8QA=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrnewsletter/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/35ussuSyB16-5Iaxgf_h6I58cdikwVMMgVrAkmcX8QA=426", "authors": ["TLDR Newsletter"], "title": "Embracing the parallel coding agent lifestyle", "comment": "Source: TLDR Newsletter, Date: 2025-10-10, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrnewsletter/1/01000199cda59753-8622b292-4000-41fd-aefd-a78bc488e52d-000000/35ussuSyB16-5Iaxgf_h6I58cdikwVMMgVrAkmcX8QA=426", "summary": "Embracing the parallel coding agent lifestyle (7 minute read) Coding agent software is still really new. Models have only really become good enough to drive them in the past few months. One of the ways to use this software is to run multiple coding agents at once, sometimes in the same repo and against multiple checkouts or git work trees. This approach increases the number of tasks that can be fired off in parallel without adding too much cognitive overhead. This post discusses patterns for ...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u7f16\u7801\u4ee3\u7406\u7684\u6a21\u5f0f\uff0c\u8fd9\u79cd\u65b0\u5174\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u592a\u591a\u8ba4\u77e5\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5e76\u884c\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u7f16\u7801\u4ee3\u7406\u8f6f\u4ef6\u662f\u65b0\u5174\u6280\u672f\uff0c\u6a21\u578b\u5728\u6700\u8fd1\u51e0\u4e2a\u6708\u624d\u53d8\u5f97\u8db3\u591f\u597d\u6765\u9a71\u52a8\u5b83\u4eec\u3002\u4e3a\u4e86\u5145\u5206\u5229\u7528\u8fd9\u79cd\u6280\u672f\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u7f16\u7801\u4ee3\u7406\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5728\u540c\u4e00\u4ee3\u7801\u5e93\u4e2d\u8fd0\u884c\u591a\u4e2a\u7f16\u7801\u4ee3\u7406\uff0c\u6709\u65f6\u4f7f\u7528\u591a\u4e2a\u68c0\u51fa\u6216git\u5de5\u4f5c\u6811\uff0c\u5b9e\u73b0\u5e76\u884c\u4efb\u52a1\u5904\u7406\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u589e\u52a0\u53ef\u4ee5\u5e76\u884c\u6267\u884c\u7684\u4efb\u52a1\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "conclusion": "\u5e76\u884c\u7f16\u7801\u4ee3\u7406\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u4f5c\u6a21\u5f0f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2510.6568a463", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrwebdev/1/01000199cdcff38a-cb6ce768-6648-4683-8afd-10a5e2be0b68-000000/SRCncSMsku4hBp-JvYdXkRh_qjPuAd7lGG4z9DA2F0A=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrwebdev/1/01000199cdcff38a-cb6ce768-6648-4683-8afd-10a5e2be0b68-000000/SRCncSMsku4hBp-JvYdXkRh_qjPuAd7lGG4z9DA2F0A=426", "authors": ["TLDR Newsletter"], "title": "Embracing the parallel coding agent lifestyle", "comment": "Source: TLDR Newsletter, Date: 2025-10-10, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F5%2Fparallel-coding-agents%2F%3Futm_source=tldrwebdev/1/01000199cdcff38a-cb6ce768-6648-4683-8afd-10a5e2be0b68-000000/SRCncSMsku4hBp-JvYdXkRh_qjPuAd7lGG4z9DA2F0A=426", "summary": "Embracing the parallel coding agent lifestyle (7 minute read) Using multiple coding agents in parallel can improve productivity. This approach is great for research tasks like proof-of-concepts and understanding existing code, as well as small, low-stakes maintenance tasks like fixing warnings. Carefully specified tasks result in easier code review and less buggy code overall.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528\u591a\u4e2a\u5e76\u884c\u7f16\u7801\u4ee3\u7406\u53ef\u4ee5\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7814\u7a76\u4efb\u52a1\u548c\u5c0f\u578b\u7ef4\u62a4\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u5e76\u884c\u7f16\u7801\u4ee3\u7406\u65b9\u6cd5\u5982\u4f55\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7814\u7a76\u539f\u578b\u9a8c\u8bc1\u548c\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u3002", "method": "\u91c7\u7528\u591a\u4e2a\u7f16\u7801\u4ee3\u7406\u5e76\u884c\u5de5\u4f5c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6307\u5b9a\u7684\u4efb\u52a1\u6765\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5e76\u884c\u7f16\u7801\u4ee3\u7406\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4ea7\u751f\u66f4\u6613\u4e8e\u4ee3\u7801\u5ba1\u67e5\u548c\u7f3a\u9677\u66f4\u5c11\u7684\u4ee3\u7801\u3002", "conclusion": "\u5e76\u884c\u7f16\u7801\u4ee3\u7406\u65b9\u6cd5\u662f\u4e00\u79cd\u6709\u6548\u7684\u751f\u4ea7\u529b\u63d0\u5347\u7b56\u7565\uff0c\u7279\u522b\u9002\u5408\u7814\u7a76\u4efb\u52a1\u548c\u4f4e\u98ce\u9669\u7ef4\u62a4\u5de5\u4f5c\u3002", "topic": "code agent"}}
{"id": "tldr.2510.cd4de766", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.devcycle.com%2Fwe-rebuilt-our-onboarding-around-mcp-the-result-3x-sdk-installs%2F%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/1thOBrYUqCruqMAJRlohNJEHrvq4CFquta04tfb2i08=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.devcycle.com%2Fwe-rebuilt-our-onboarding-around-mcp-the-result-3x-sdk-installs%2F%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/1thOBrYUqCruqMAJRlohNJEHrvq4CFquta04tfb2i08=426", "authors": ["TLDR Newsletter"], "title": "We Rebuilt Our Onboarding Around MCP", "comment": "Source: TLDR Newsletter, Date: 2025-10-10, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.devcycle.com%2Fwe-rebuilt-our-onboarding-around-mcp-the-result-3x-sdk-installs%2F%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/1thOBrYUqCruqMAJRlohNJEHrvq4CFquta04tfb2i08=426", "summary": "We Rebuilt Our Onboarding Around MCP (5 minute read) Most developer onboarding flows lose users before they ever see real value. DevCycle found the problem wasn't motivation, it was location. Developers don't want to leave their editors to try a demo app. So the team rebuilt onboarding around the Model-Context-Protocol (MCP), letting AI coding assistants like Cursor or VS Code install and configure the SDK directly in-editor. Two prompts now replace an entire tutorial, tripling install rates ...", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u5c06\u5f00\u53d1\u8005\u5f15\u5bfc\u6d41\u7a0b\u96c6\u6210\u5230\u7f16\u8f91\u5668\u4e2d\uff0c\u4f7f\u7528MCP\u534f\u8bae\u8ba9AI\u7f16\u7801\u52a9\u624b\u76f4\u63a5\u5728\u7f16\u8f91\u5668\u4e2d\u5b89\u88c5\u548c\u914d\u7f6eSDK\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u88c5\u7387\u3002", "motivation": "\u4f20\u7edf\u5f00\u53d1\u8005\u5f15\u5bfc\u6d41\u7a0b\u5728\u7528\u6237\u770b\u5230\u771f\u6b63\u4ef7\u503c\u4e4b\u524d\u5c31\u6d41\u5931\u4e86\u7528\u6237\uff0c\u95ee\u9898\u4e0d\u5728\u4e8e\u52a8\u673a\u800c\u5728\u4e8e\u4f4d\u7f6e\uff0c\u5f00\u53d1\u8005\u4e0d\u613f\u610f\u79bb\u5f00\u7f16\u8f91\u5668\u53bb\u5c1d\u8bd5\u6f14\u793a\u5e94\u7528\u3002", "method": "\u56f4\u7ed5\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u91cd\u5efa\u5f15\u5bfc\u6d41\u7a0b\uff0c\u8ba9AI\u7f16\u7801\u52a9\u624b\u5982Cursor\u6216VS Code\u76f4\u63a5\u5728\u7f16\u8f91\u5668\u4e2d\u5b89\u88c5\u548c\u914d\u7f6eSDK\u3002", "result": "\u4e24\u4e2a\u63d0\u793a\u73b0\u5728\u53d6\u4ee3\u4e86\u6574\u4e2a\u6559\u7a0b\uff0c\u5b89\u88c5\u7387\u63d0\u9ad8\u4e86\u4e09\u500d\u3002", "conclusion": "\u5c06\u5f15\u5bfc\u6d41\u7a0b\u96c6\u6210\u5230\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u4e2d\uff0c\u76f4\u63a5\u5728\u7f16\u8f91\u5668\u4e2d\u5b8c\u6210\u914d\u7f6e\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u5f00\u53d1\u8005\u4f53\u9a8c\u548c\u63d0\u9ad8\u91c7\u7528\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2510.83df74df", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpower.substack.com%2Fp%2Ffrom-agent-hype-to-agent-fatigue%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/JXgB-v87tOcdydkL_yQxhsuaPmM98FH9aT93naWwVUk=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpower.substack.com%2Fp%2Ffrom-agent-hype-to-agent-fatigue%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/JXgB-v87tOcdydkL_yQxhsuaPmM98FH9aT93naWwVUk=426", "authors": ["TLDR Newsletter"], "title": "From Agent Hype to Agent Fatigue", "comment": "Source: TLDR Newsletter, Date: 2025-10-10, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fproductpower.substack.com%2Fp%2Ffrom-agent-hype-to-agent-fatigue%3Futm_source=tldrfounders/1/01000199ce0583b7-8b085ca4-312e-4ac7-9d93-518f95603d65-000000/JXgB-v87tOcdydkL_yQxhsuaPmM98FH9aT93naWwVUk=426", "summary": "From Agent Hype to Agent Fatigue (4 minute read) Everyone said 2025 would be the year of AI agents, and they were right, just not in the way they expected. Companies raced to rebrand every workflow as \u201cagentic,\u201d but the reality is showing cracks: 40% of projects are predicted to fail by 2027, and employees are burning out under the pressure to use tools they barely understand.", "source": "tldr", "AI": {"tldr": "\u4eceAI\u4ee3\u7406\u70ed\u6f6e\u5230\u4ee3\u7406\u75b2\u52b3\uff1a2025\u5e74AI\u4ee3\u7406\u9879\u76ee\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c40%\u7684\u9879\u76ee\u9884\u8ba1\u57282027\u5e74\u524d\u5931\u8d25\uff0c\u5458\u5de5\u5728\u4f7f\u7528\u4e0d\u719f\u6089\u7684\u5de5\u5177\u65f6\u51fa\u73b0\u5026\u6020", "motivation": "\u5206\u6790AI\u4ee3\u7406\u9886\u57df\u7684\u73b0\u72b6\uff0c\u63ed\u793a\u4ece\u8fc7\u5ea6\u7092\u4f5c\u5230\u73b0\u5b9e\u6311\u6218\u7684\u8f6c\u53d8\uff0c\u5173\u6ce8\u9879\u76ee\u5931\u8d25\u7387\u548c\u5458\u5de5\u5026\u6020\u95ee\u9898", "method": "\u57fa\u4e8e\u884c\u4e1a\u8d8b\u52bf\u5206\u6790\u548c\u9884\u6d4b\u6570\u636e\uff0c\u8bc4\u4f30AI\u4ee3\u7406\u9879\u76ee\u7684\u5b9e\u9645\u5b9e\u65bd\u60c5\u51b5", "result": "\u53d1\u73b040%\u7684AI\u4ee3\u7406\u9879\u76ee\u9884\u8ba1\u57282027\u5e74\u524d\u5931\u8d25\uff0c\u5458\u5de5\u5728\u4f7f\u7528\u4e0d\u719f\u6089\u7684AI\u5de5\u5177\u65f6\u51fa\u73b0\u4e25\u91cd\u5026\u6020", "conclusion": "AI\u4ee3\u7406\u9886\u57df\u6b63\u4ece\u8fc7\u5ea6\u7092\u4f5c\u8f6c\u5411\u73b0\u5b9e\u6311\u6218\uff0c\u9700\u8981\u66f4\u8c28\u614e\u7684\u5b9e\u65bd\u7b56\u7565\u548c\u66f4\u597d\u7684\u5458\u5de5\u57f9\u8bad", "topic": "agent analysis"}}
