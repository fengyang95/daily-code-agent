<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.AI](#cs.AI) [Total: 24]
- [tldr.article](#tldr.article) [Total: 3]
- [cs.LG](#cs.LG) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training](https://arxiv.org/abs/2602.22576)
*Tianle Xia,Ming Xu,Lingxiang Hu,Yiding Sun,Wenwei Li,Linfang Shang,Liqun Liu,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CL

TL;DR: Search-P1框架通过路径中心奖励塑造和双轨路径评分，解决了Agentic RAG训练中的稀疏奖励和低样本效率问题，在多个QA基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG的单轮检索难以处理复杂多步推理，而现有的基于RL的Agentic RAG训练方法存在稀疏结果奖励（丢弃中间信号）和低样本效率（失败样本无贡献）的问题。

Method: 提出Search-P1框架，包含两个关键组件：1) 路径中心奖励，通过顺序无关的步骤覆盖和软评分评估推理轨迹的结构质量；2) 双轨路径评分，使用离线生成的参考规划器从自一致性和参考对齐两个角度评估路径。

Result: 在多个QA基准测试中，Search-P1相比Search-R1和其他强基线实现了显著改进，平均准确率提升了7.7个百分点。

Conclusion: Search-P1通过路径中心奖励塑造有效解决了Agentic RAG训练中的关键挑战，显著提升了模型在复杂推理任务中的性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.

</details>


### [2] [Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction](https://arxiv.org/abs/2602.22752)
*Nils Schwager,Simon Münker,Alistair Plum,Achim Rettinger*

Main category: cs.CL

TL;DR: 该研究提出条件评论预测任务，评估LLMs模拟社交媒体用户行为的能力，发现监督微调会导致形式与内容解耦，并挑战了当前"朴素提示"范式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从探索工具转变为社会科学中的"硅基主体"缺乏操作有效性的广泛验证，需要评估LLMs模拟社交媒体用户行为的能力。

Method: 引入条件评论预测任务，通过比较生成输出与真实数字痕迹来评估LLMs。评估了8B参数的开源模型，系统比较了显式与隐式提示策略以及监督微调的影响。

Result: 在低资源设置中发现了形式与内容解耦：监督微调对齐了文本输出的表面结构（长度和句法），但降低了语义基础。显式条件在微调下变得冗余，模型能够直接从行为历史进行潜在推断。

Conclusion: 挑战了当前"朴素提示"范式，提供了操作指南，强调优先使用真实行为痕迹而非描述性人物角色来实现高保真模拟。

Abstract: The transition of Large Language Models (LLMs) from exploratory tools to active "silicon subjects" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current "naive prompting" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.

</details>


### [3] [AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors](https://arxiv.org/abs/2602.22755)
*Abhay Sheshadri,Aidan Ewart,Kai Fronsdal,Isha Gupta,Samuel R. Bowman,Sara Price,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: AuditBench是一个对齐审计基准，包含56个植入隐藏行为的语言模型，用于评估审计工具在检测模型未公开的潜在有害行为方面的效果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统评估对齐审计工具效果的标准基准，需要量化评估工具在检测语言模型中隐藏的潜在有害行为方面的能力。

Method: 创建包含56个植入14种不同隐藏行为（如谄媚顺从、反对AI监管、秘密地缘政治忠诚等）的语言模型基准，开发可配置审计工具的调查代理，评估不同工具在检测这些隐藏行为方面的效果。

Result: 发现工具到代理的差距，即独立评估表现良好的工具在代理环境中效果不佳；最有效的工具涉及辅助模型生成多样化提示；白盒可解释性工具有帮助，但黑盒工具表现最佳；不同训练技术对审计难度影响显著。

Conclusion: AuditBench为对齐审计提供了定量评估框架，揭示了工具在代理环境中的实际效果差异，并表明审计成功与否很大程度上取决于模型的训练技术。

Abstract: We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.

</details>


### [4] [Towards Better RL Training Data Utilization via Second-Order Rollout](https://arxiv.org/abs/2602.22765)
*Zhe Yang,Yudong Wang,Rang Li,Zhifang Sui*

Main category: cs.CL

TL;DR: 该论文提出了一种结合一阶（生成多个回答）和二阶（生成多个评论）rollout的强化学习框架，联合训练LLMs的生成和评论能力，更有效地利用训练数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练主要关注一阶rollout（为问题生成多个回答），但忽视了评论能力的训练，未能充分利用训练数据的潜力。需要开发能够同时训练生成和评论能力的统一框架。

Method: 提出包含一阶rollout（生成多个回答）和二阶rollout（为回答生成多个评论）的统一RL框架，联合训练生成和评论能力。通过采样技术缓解基于结果的奖励噪声问题。

Result: 在各种模型和数据集上的实验表明，该方法比传统RL更有效地利用训练数据，在相同数据量下获得更好性能。发现评论训练中标签平衡的重要性，以及采样技术可以缓解奖励噪声问题。

Conclusion: 该工作为RL中的动态数据增强和联合生成-评论训练提供了初步探索，为RL训练的进一步发展提供了有意义的启示。证明了二阶rollout和评论训练在充分利用训练数据方面的重要性。

Abstract: Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training

</details>


### [5] [Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift](https://arxiv.org/abs/2602.22790)
*Hyunwoo Kim,Hanau Yi,Jaehee Bae,Yumin Kim*

Main category: cs.CL

TL;DR: 论文提出NLD-P（自然语言声明式提示）作为声明式治理方法，而非刚性模板，用于应对GPT规模模型漂移带来的提示工程挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型快速演进，提示工程从局部技巧转变为系统级治理挑战。模型规模扩大和更新导致提示行为对指令遵循策略、对齐机制和解码策略的变化变得敏感（GPT规模模型漂移），传统表面级格式约定和临时优化已无法确保稳定可控。

Method: 将NLD-P形式化为模块化控制抽象，分离来源、约束逻辑、任务内容和生成后评估，直接使用自然语言编码而不依赖外部编排代码。定义最小合规标准，分析模型依赖的模式接受度，定位为可访问的治理框架。

Result: 论文本身部分起草和编辑使用了在NLD-P配置下的模式绑定LLM助手，但所有概念框架、方法论主张和最终修订都由人类作者在文档化的人机协作协议下指导、审查和批准。

Conclusion: NLD-P为持续模型演进下的声明式控制提供了框架，为非开发者从业者在不断演化的LLM生态系统中提供了可访问的治理方法，并指出了未来实证验证的方向。

Abstract: The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.

</details>


### [6] [Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching](https://arxiv.org/abs/2602.22871)
*Roy Miles,Aysim Toker,Andreea-Maria Oncescu,Songcen Xu,Jiankang Deng,Ismail Elezi*

Main category: cs.CL

TL;DR: 提出Stitching Noisy Diffusion Thoughts框架，通过扩散采样生成多样推理轨迹，使用过程奖励模型评分步骤，跨轨迹拼接高质量步骤形成复合推理，再由自回归模型生成最终答案。


<details>
  <summary>Details</summary>
Motivation: 现有聚合策略通常是轨迹级别的（如选择最佳轨迹或投票最终答案），丢弃了部分或"接近正确"尝试中的有用中间工作。需要利用这些中间步骤的价值。

Method: 1) 使用掩码扩散语言模型采样多样低成本的推理轨迹；2) 用现成过程奖励模型评分每个中间步骤；3) 跨轨迹拼接最高质量步骤形成复合推理；4) 用自回归模型基于复合推理重新计算最终答案。

Result: 在数学推理基准测试中，步骤级重组在更难问题上最有益。无训练框架在六个数学和编程任务上平均准确率提升达23.8%，同时相比传统扩散模型和统一架构实现1.8倍延迟降低。

Conclusion: 步骤级重组能有效利用部分正确推理中的中间工作，分离探索与评估/合成，避免统一混合架构，在保持广泛搜索的同时提高性能。

Abstract: Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or "nearly correct" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.

</details>


### [7] [CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery](https://arxiv.org/abs/2602.23075)
*Mengze Hong,Di Jiang,Chen Jason Zhang,Zichang Guo,Yawen Li,Jun Chen,Shaobo Cui,Zhiyang Su*

Main category: cs.CL

TL;DR: CiteLLM是一个专门用于可信参考文献发现的代理平台，通过在LaTeX编辑器中嵌入LLM功能，实现无幻觉的参考文献推荐，同时保护数据隐私和学术诚信。


<details>
  <summary>Details</summary>
Motivation: LLM在学术活动中存在三个主要挑战：(1) AI生成内容的可信度问题，(2) 学术诚信和知识产权保护，(3) 信息隐私保护。需要开发一个可信的参考文献发现系统来解决这些问题。

Method: 在LaTeX编辑器中直接嵌入LLM功能，采用动态学科感知路由从可信的学术知识库检索候选文献，LLM仅用于生成上下文感知的搜索查询、相关性排序、验证和解释支持，通过段落级语义匹配和集成聊天机器人实现无幻觉引用。

Result: 评估结果显示，CiteLLM在返回有效且高度可用的参考文献方面表现出优越性能。

Conclusion: CiteLLM提供了一个可信的参考文献发现平台，解决了LLM在学术应用中的伦理挑战，实现了无幻觉引用、数据隐私保护和学术诚信的平衡。

Abstract: Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.

</details>


### [8] [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079)
*Boyang Zhang,Yang Zhang*

Main category: cs.CL

TL;DR: 本文提出SALA方法，结合风格计量特征与LLM推理，用于评估和减轻新闻文本的作者身份推断风险，并通过引导重写策略保护作者隐私。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，作者身份推断能力日益强大，引发了新闻文章等文本数据意外去匿名化的风险担忧。

Method: 提出SALA方法，将定量风格计量特征与LLM推理相结合，构建结构化、可解释的管道，并通过数据库模块增强，最后提出利用代理推理轨迹生成重写提示的引导重写策略。

Result: 在大规模新闻数据集上的实验表明，SALA方法（特别是增强数据库模块后）在各种场景下实现了高推理准确率，引导重写策略能有效降低作者身份可识别性同时保持文本含义。

Conclusion: 研究结果强调了LLM代理的去匿名化潜力，以及可解释、主动防御机制对于保护作者隐私的重要性。

Abstract: The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.

</details>


### [9] [Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs](https://arxiv.org/abs/2602.23136)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 研究发现多模态LLM虽然能处理语音和图像，但无法有效利用说话者身份、情感和视觉纹理等信息，因为解码器只能提取文本对齐方向的信息，导致模态特定信息成为噪声。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM能够处理语音和图像，但无法有效感知说话者的声音特征或物体的纹理细节。研究发现这并非编码问题，而是解码器与模态信息之间的不匹配问题。

Method: 通过线性探针分析发现模态特定信息在LLM各层中保留完好，但解码器损失实验显示移除64-71%的模态特定方差反而改善性能。提出广义互信息(GMI)框架，分析解码器评分规则的限制，并在5个语音和视觉模型上进行验证，使用LoRA干预实验证明训练目标决定信息可访问性。

Result: 线性探针显示说话者身份、情感和视觉属性信息在各层保留完好（3-55倍于随机水平），但解码器无法利用这些信息。移除大部分模态特定方差反而改善解码器损失。LoRA干预实验证明通过情感目标训练可将情感可访问性提高7.5%，而不影响其他属性。

Conclusion: 多模态LLM的信息可访问性瓶颈在于解码器的评分规则，而非编码器或投影层。解码器只能提取文本对齐方向的信息，训练目标决定了哪些信息变得可访问。

Abstract: Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.
  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization](https://arxiv.org/abs/2602.22368)
*Jiahao Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: EyeLayer：通过人类眼动模式增强LLM代码摘要的轻量级注意力模块


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码摘要任务上取得了显著进展，但人类在代码理解方面的专业知识是否能进一步指导和增强这些模型仍是一个开放性问题。研究者希望探索人类眼动模式作为人类专业知识的代理，能否提升LLM的代码摘要能力。

Method: 提出EyeLayer，一个轻量级的注意力增强模块，通过多模态高斯混合模型建模人类阅读代码时的注意力分布，学习可泛化的注意力先验，并将其无缝集成到LLM中而不干扰现有表示。

Result: 在多种模型家族（LLaMA-3.2、Qwen3、CodeBERT）上评估，EyeLayer始终优于强微调基线，在BLEU-4指标上最高提升13.17%。

Conclusion: 人类眼动模式编码了互补的注意力信号，能够增强LLM的语义聚焦能力，并能有效迁移到不同模型上，提升代码摘要性能。

Abstract: Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (μ_i, σ_i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.

</details>


### [11] [Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents](https://arxiv.org/abs/2602.22402)
*Cosmo Santoni*

Main category: cs.SE

TL;DR: CMV是一种上下文记忆虚拟化系统，将LLM在长对话中积累的理解视为版本控制状态，通过DAG建模会话历史，实现跨会话的上下文重用，并采用无损修剪算法显著减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长推理任务中会在上下文窗口中积累重要状态信息，但当会话达到上下文限制时，这些理解会因有损压缩而丢失，需要一种机制来保存和重用这些累积的知识。

Method: 提出上下文记忆虚拟化(CMV)系统，借鉴操作系统虚拟内存概念，将会话历史建模为有向无环图(DAG)，定义快照、分支和修剪原语，并开发三遍结构无损修剪算法，保留所有用户消息和助手响应，但去除机械性冗余内容。

Result: 在76个真实世界编码会话的评估中，修剪平均减少20%的token，最高可达86%；混合工具使用会话平均减少39%，在10轮对话内达到盈亏平衡；修剪在经济上可行，特别是在有提示缓存的情况下。

Conclusion: CMV系统能有效管理LLM会话中的累积状态，通过虚拟化上下文记忆实现跨会话的知识重用，无损修剪算法显著降低token消耗，为长对话场景提供了实用的解决方案。

Abstract: As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.

</details>


### [12] [XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction](https://arxiv.org/abs/2602.22403)
*Saumendu Roy,Banani Roy,Chanchal Roy,Richard Bassey*

Main category: cs.SE

TL;DR: XMENTOR是一个用于软件缺陷预测的VS Code插件，通过聚合多个XAI解释方法（如LIME、SHAP、BreakDown）来提供统一、一致的透明度视图，减少开发者的困惑和认知负担。


<details>
  <summary>Details</summary>
Motivation: 机器学习缺陷预测模型虽然能提高软件质量，但其不透明的推理过程导致开发者难以信任。现有的可解释AI方法（LIME、SHAP、BreakDown）同时使用时经常产生相互矛盾的解释，增加了开发者的困惑、挫折感和认知负荷。

Method: 提出XMENTOR方法，作为VS Code插件实现，采用自适应阈值、排名和符号一致性以及回退策略，将多个后置解释方法统一为单一、连贯的视图，避免信息过载。

Result: 用户研究中，近90%的参与者更喜欢聚合后的解释，认为这减少了困惑，并更好地支持日常的调试和缺陷审查任务。

Conclusion: 通过组合多个解释方法并将其嵌入到开发者工作流中，可以显著提高缺陷预测模型的可解释性、可用性和信任度。

Abstract: Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.

</details>


### [13] [RepoMod-Bench: A Benchmark for Code Repository Modernization via Implementation-Agnostic Testing](https://arxiv.org/abs/2602.22518)
*Xuefeng Li,Nir Ben-Israel,Yotam Raz,Belal Ahmed,Doron Serebro,Antoine Raux*

Main category: cs.SE

TL;DR: RepoMod-Bench是一个用于代码现代化（跨语言翻译）的仓库级基准测试，包含21个真实仓库、8种编程语言、160万行代码，采用黑盒测试防止测试驱动过拟合，评估显示代码代理在大规模项目上性能急剧下降。


<details>
  <summary>Details</summary>
Motivation: 现有AI代码代理评估在通用代码仓库生成中缺乏确定性基准，而代码现代化（自动翻译）虽有固定基准（源仓库），但现有基准仅限于小规模仓库且依赖语言特定单元测试，容易导致测试驱动过拟合。

Method: 提出基于实现无关评估范式的仓库级代码现代化基准框架，具体实现为RepoMod-Bench：包含21个真实仓库、8种编程语言、160万行代码和11,616个测试。通过标准化接口仓库，使用实现无关测试套件验证源和目标实现的功能等价性，采用黑盒方法隐藏所有测试套件防止测试驱动捷径。

Result: 评估四个最先进代理配置显示明显的规模崩溃：在10K LOC以下项目中平均通过率91.3%，但在超过50K LOC项目中降至15.3%，表明大规模自主现代化仍是重大开放挑战。

Conclusion: RepoMod-Bench为仓库级代码现代化提供了严谨的评估框架，揭示了当前AI代码代理在大规模项目上的局限性，自主现代化在规模扩展方面仍面临重大挑战。

Abstract: The evolution of AI coding agents has shifted the frontier from simple snippet completion to autonomous repository-level engineering. However, evaluating these agents remains ill-posed in general code repository generation, where the lack of deterministic ground truth leads to ambiguous metrics. Code modernization via automated translation offers a more rigorous alternative by providing a fixed ground truth -- the source repository; yet existing benchmarks are limited to small-scale repositories and rely on language-specific unit tests visible to the agent, allowing test-driven overfitting.
  We address these limitations by introducing a benchmarking framework for repository-level code modernization built on an implementation-agnostic evaluation paradigm. This framework is instantiated through RepoMod-Bench: a benchmark of 21 real-world repositories with standardized interfaces, spanning 8 programming languages. The benchmark contains 1.6M lines of code (LOC) and 11,616 tests, with repository sizes ranging from 14 to 211K LOC. By targeting repositories with standardized interfaces, we utilize an implementation-agnostic test suite to verify functional equivalence between source and target implementations. This black-box approach ensures verification remains consistent across languages, and our environment hides all test suites from agents to prevent test-driven shortcuts. Evaluating four state-of-the-art agent configurations reveals a sharp scaling collapse: average pass rates drop from 91.3% on projects under 10K LOC to 15.3% on projects exceeding 50K LOC. These results demonstrate that autonomous modernization at scale remains a significant open challenge. Our benchmark and code are available at https://github.com/Modelcode-ai/mcode-benchmark.

</details>


### [14] [Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents](https://arxiv.org/abs/2602.22764)
*Jiahong Xiang,Wenxiao He,Xihua Wang,Hongliang Tian,Yuqun Zhang*

Main category: cs.SE

TL;DR: 提出了Rust-SWE-bench基准测试，包含500个真实Rust仓库级任务，评估了现有代码代理在Rust生态系统中的表现，并提出了RUSTFORGER新方法，通过自动化测试环境设置和动态追踪策略显著提升问题解决率。


<details>
  <summary>Details</summary>
Motivation: Rust编程语言学习曲线陡峭，编码挑战大，需要自动化问题解决来促进其广泛采用。虽然LLM驱动的代码代理在复杂软件工程任务中表现出色，但缺乏针对Rust的大规模仓库级基准测试限制了其应用。

Method: 1) 创建Rust-SWE-bench基准测试，包含500个来自34个流行Rust仓库的真实仓库级任务；2) 使用4个代表性代理和4个SOTA LLM进行综合研究；3) 提出RUSTFORGER新方法，集成自动化测试环境设置和Rust元编程驱动的动态追踪策略。

Result: 现有ReAct风格代理最多解决21.2%的问题，主要受限于仓库级代码结构理解和Rust严格类型/特质语义。RUSTFORGER使用Claude-Sonnet-3.7显著优于所有基线，解决28.6%的任务（比最强基线提升34.9%），独特解决了46个其他代理无法解决的任务。

Conclusion: Rust代码代理面临仓库级代码结构理解和类型语义合规性挑战，问题复现对任务解决至关重要。RUSTFORGER通过自动化测试环境和动态追踪策略有效解决了这些挑战，为Rust生态系统中的代码代理提供了有前景的解决方案。

Abstract: The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level software engineering tasks from 34 diverse and popular Rust repositories. We then perform a comprehensive study on Rust-SWE-bench with four representative agents and four state-of-the-art LLMs to establish a foundational understanding of their capabilities and limitations in the Rust ecosystem. Our extensive study reveals that while ReAct-style agents are promising, i.e., resolving up to 21.2% of issues, they are limited by two primary challenges: comprehending repository-wide code structure and complying with Rust's strict type and trait semantics. We also find that issue reproduction is rather critical for task resolution. Inspired by these findings, we propose RUSTFORGER, a novel agentic approach that integrates an automated test environment setup with a Rust metaprogramming-driven dynamic tracing strategy to facilitate reliable issue reproduction and dynamic analysis. The evaluation shows that RUSTFORGER using Claude-Sonnet-3.7 significantly outperforms all baselines, resolving 28.6% of tasks on Rust-SWE-bench, i.e., a 34.9% improvement over the strongest baseline, and, in aggregate, uniquely solves 46 tasks that no other agent could solve across all adopted advanced LLMs.

</details>


### [15] [Managing Uncertainty in LLM-based Multi-Agent System Operation](https://arxiv.org/abs/2602.23005)
*Man Zhang,Tao Yue,Yihua He*

Main category: cs.SE

TL;DR: 本文提出一个面向LLM多智能体软件系统的生命周期不确定性管理框架，包含表示、识别、演化和适应四个机制，通过临床超声心动图系统验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如寿命超声心动图）应用LLM多智能体系统时，仅提高模型精度无法解决系统级风险。现有工作主要在模型层面处理不确定性，而非将其作为软件工程的一等公民。需要从系统和运行时角度管理不确定性传播。

Method: 1) 区分认识论和本体论不确定性；2) 提出基于生命周期的四机制管理框架：表示、识别、演化、适应；3) 通过临床合作的真实LLM多智能体超声心动图系统验证框架可行性。

Result: 在真实临床LLM多智能体超声心动图系统中验证了该框架，展示了在诊断推理中提高的可靠性和可诊断性。框架可推广到其他安全关键LLM多智能体系统。

Conclusion: 提出的生命周期不确定性管理框架为安全关键LLM多智能体系统提供了超越模型中心方法的系统性运行时治理和保证方法，支持原则性操作控制和运行时保证。

Abstract: Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering concern. This paper approaches uncertainty from both system-level and runtime perspectives. We first differentiate epistemological and ontological uncertainties in the context of LLM-based multi-agent software system operation. Building on this foundation, we propose a lifecycle-based uncertainty management framework comprising four mechanisms: representation, identification, evolution, and adaptation. The uncertainty lifecycle governs how uncertainties emerge, transform, and are mitigated across architectural layers and execution phases, enabling structured runtime governance and controlled adaptation. We demonstrate the feasibility of the framework using a real-world LLM-based multi-agent echocardiographic software system developed in clinical collaboration, showing improved reliability and diagnosability in diagnostic reasoning. The proposed approach generalizes to other safety-critical LLM-based multi-agent software systems, supporting principled operational control and runtime assurance beyond model-centric methods.

</details>


### [16] [CL4SE: A Context Learning Benchmark For Software Engineering Tasks](https://arxiv.org/abs/2602.23047)
*Haichuan Hu,Ye Shang,Guoqing Xie,Congqing He,Quanjun Zhang*

Main category: cs.SE

TL;DR: CL4SE提出了首个软件工程上下文学习的系统化基准，包含四种SE专用上下文类型的细粒度分类，在四个代表性任务上评估主流LLM，平均性能提升24.7%。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对软件工程特定上下文类型的系统分类和专用基准，无法量化不同上下文在核心SE工作流中的异质效应。

Method: 提出CL4SE基准，包含四种SE导向的上下文类型分类：可解释示例、项目特定上下文、过程决策上下文、正负上下文，对应四个代表性任务。构建包含13,000+样本的高质量数据集，评估五个主流LLM的九个指标。

Result: 上下文学习在所有任务上平均提升24.7%性能：过程上下文提升代码审查33%，混合正负上下文提升补丁评估30%，项目特定上下文提升代码摘要BLEU 14.78%，可解释示例提升代码生成PASS@1 5.72%。

Conclusion: CL4SE建立了首个SE上下文学习的标准化评估框架，提供了任务特定上下文设计的可操作经验见解，并发布了大规模数据集促进该领域可重复研究。

Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?](https://arxiv.org/abs/2602.22401)
*Yongjun Zhang*

Main category: cs.AI

TL;DR: 论文提出"氛围研究"概念，类比"氛围编程"，分析AI代理如何通过21种技能插件自主执行完整研究流程，识别AI擅长速度、覆盖和方法框架，但缺乏理论原创性和隐性领域知识。


<details>
  <summary>Details</summary>
Motivation: AI代理在社会科学研究中的能力超越了传统聊天机器人，能够自主执行完整研究流程，这代表了质的转变。需要理解AI代理在研究中的能力边界和影响。

Method: 提出认知任务框架，按可编码性和隐性知识需求两个维度分类研究活动；使用scholar-skill（包含21种技能的Claude Code插件）作为案例研究；分析研究流程中的委托边界。

Result: AI代理在速度、覆盖范围和方法框架方面表现出色，但在理论原创性和隐性领域知识方面存在局限；委托边界是认知性的而非顺序性的，贯穿研究各阶段。

Conclusion: AI代理将带来增强但条件脆弱、分层风险和教育危机等职业影响，提出负责任氛围研究的五项原则。

Abstract: AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.

</details>


### [18] [Towards Autonomous Memory Agents](https://arxiv.org/abs/2602.22406)
*Xinle Wu,Rui Zhang,Mustafa Anis Hussain,Yao Lu*

Main category: cs.AI

TL;DR: U-Mem提出自主记忆代理，通过成本感知的知识提取级联和语义感知Thompson采样，主动获取、验证和整理知识，显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理是被动和反应式的，记忆增长受限于可用信息，且很少在不确定时主动寻求外部输入。需要自主记忆代理来主动获取、验证和整理知识

Method: 1) 成本感知知识提取级联：从廉价的自/教师信号逐步升级到工具验证的研究，仅在必要时寻求专家反馈；2) 语义感知Thompson采样：平衡记忆的探索与利用，缓解冷启动偏差

Result: 在可验证和不可验证基准测试中，U-Mem持续优于现有记忆基线，甚至超越基于RL的优化方法：HotpotQA（Qwen2.5-7B）提升14.6分，AIME25（Gemini-2.5-flash）提升7.33分

Conclusion: 自主记忆代理通过主动知识获取和验证机制，能够显著提升LLM性能，为记忆系统设计提供了新方向

Abstract: Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.

</details>


### [19] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 研究异构代理通过校准自身可靠性并选择性弃权来提升集体决策准确性的概率框架，将孔多塞陪审团定理推广到置信度门控的序列设置。


<details>
  <summary>Details</summary>
Motivation: 经典孔多塞陪审团定理假设固定参与，但现实中的集体决策常受益于允许代理说"我不知道"。需要研究代理学习自身可靠性并选择性弃权如何影响集体准确性。

Method: 提出概率框架：代理经历校准阶段更新对自身固定能力的信念，然后面对最终置信度门控决定投票或弃权。推导群体成功概率的非渐近下界，证明选择性参与将CJT渐近保证推广到序列置信度门控设置。

Result: 理论证明选择性参与框架能推广CJT的渐近保证，并通过蒙特卡洛模拟验证了推导的边界。该框架可应用于AI安全领域，缓解集体LLM决策中的幻觉问题。

Conclusion: 允许代理学习自身可靠性并选择性弃权能提升集体决策准确性，将经典投票理论推广到更现实的置信度门控设置，为AI安全中的幻觉缓解提供理论框架。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [20] [A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines](https://arxiv.org/abs/2602.22442)
*Gaoyuan Du,Amit Ahlawat,Xiaoyang Liu,Jing Wu*

Main category: cs.AI

TL;DR: 提出评估代理(EA)用于决策中心化评估AutoML代理，而非仅关注最终性能，能检测故障决策、识别推理不一致性并量化决策影响


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的AutoML系统评估过于结果中心化，缺乏对中间决策质量的系统性评估，无法揭示决策层面的失败模式

Method: 设计评估代理(EA)作为观察者，从四个维度评估中间决策：决策有效性、推理一致性、模型质量风险（超越准确率）、反事实决策影响

Result: EA能检测故障决策(F1分数0.919)、识别与最终结果无关的推理不一致性、量化决策对下游性能的影响(-4.9%到+8.3%)

Conclusion: 决策中心化评估能揭示结果指标无法发现的失败模式，为可靠、可解释、可治理的自主ML系统提供基础

Abstract: Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\% to +8.3\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.

</details>


### [21] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 提出对比世界模型(CWM)，使用InfoNCE对比目标训练LLM作为动作评分器，通过挖掘硬负样本来区分物理可行与不可行动作，在ScienceWorld基准上优于传统监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用监督微调训练动作评分器，但这种方法独立处理每个候选动作，没有明确教导模型区分物理正确和细微错误的动作。需要一种能更好识别物理可行动作的方法。

Method: 提出对比世界模型(CWM)，使用InfoNCE对比目标对大型语言模型进行微调，通过挖掘硬负样本（语义相似但物理不兼容的候选动作）在评分空间中推离有效和无效动作。

Result: 在ScienceWorld基准上的评估显示：1) 在605个硬负测试对上，CWM在最小编辑负样本上的Precision@1比SFT高6.76个百分点，AUC-ROC更高(0.929 vs 0.906)；2) 在分布外压力条件下，CWM保持更好的安全边际(-2.39 vs -3.96)。

Conclusion: 对比训练比单独使用监督微调更能诱导出捕捉物理可行性的表示，CWM在识别物理可行动作方面表现更优。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [22] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估编码智能体优化能力的框架，提供版本化智能体快照、预算控制评估和结构化执行追踪，并包含基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 编码智能体的一个重要应用是智能体优化：通过编辑-执行-评估循环迭代改进目标智能体。然而社区缺乏对此任务的系统性理解，且智能体优化与传统软件工程有本质不同，需要同时捕获确定性代码和随机LLM生成的中间推理及执行结果。

Method: 提出VERO框架，包含：(1) 可复现的评估工具链，具有版本化智能体快照、预算控制评估和结构化执行追踪；(2) 包含目标智能体和任务的基准测试套件，配有参考评估流程。

Result: 使用VERO进行了实证研究，比较不同优化器配置在任务间的表现，分析哪些修改能可靠提升目标智能体性能。

Conclusion: VERO框架支持编码智能体优化作为核心能力的研究，已公开发布供社区使用。

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [23] [A Mathematical Theory of Agency and Intelligence](https://arxiv.org/abs/2602.22519)
*Wael Hafez,Chenan Wei,Rodrigo Felipe,Amir Nazeri,Cameron Reid*

Main category: cs.AI

TL;DR: 论文提出"双预测性(P)"作为衡量系统观察、行动和结果之间信息共享程度的指标，证明其在量子系统中可达1，经典系统≤0.5，引入代理后更低。通过物理系统、强化学习代理和LLM对话验证边界，区分代理与智能，提出基于丘脑皮质调节的反馈架构。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统处理大量信息做出复杂预测，但预测成功时底层环境交互可能已退化。需要一种原则性方法来衡量系统部署的总信息中，观察、行动和结果之间实际共享的比例，以评估系统在变化条件下的可靠性和资源利用效率。

Method: 从第一性原理推导出双预测性(P)作为内在交互度量，证明其理论边界。在物理系统(双摆)、强化学习代理和多轮LLM对话中实验验证这些边界。提出受生物丘脑皮质调节启发的实时监控P的反馈架构。

Result: P在量子系统中可达1，经典系统≤0.5，引入代理选择后更低。实验验证了这些理论边界。区分代理(基于预测行动的能力)与智能(还需从交互学习、自我监控学习效果、适应观察/行动/结果范围以恢复有效学习)。当前AI系统仅实现代理而非智能。

Conclusion: 双预测性(P)是评估系统交互效率的关键指标。通过监控P可实现自适应、有弹性的AI系统。当前AI系统缺乏真正的智能，仅具备代理能力。受生物系统启发的反馈架构为构建更智能的AI提供了方向。

Abstract: To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.

</details>


### [24] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出基于LLM的智能体框架用于无蜂窝O-RAN中的意图翻译与优化，通过多智能体协作实现节能和资源管理，采用参数高效微调减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有O-RAN中大多数工作考虑简单意图并由独立智能体处理，而需要智能体间协调的复杂意图尚未探索。需要开发能够处理复杂意图的多智能体协作框架。

Method: 提出多智能体框架：监督智能体翻译运营商意图为优化目标和最小速率要求；用户权重智能体从记忆模块检索经验确定用户优先级权重；O-RU管理智能体使用DRL算法确定活跃O-RU集合；监控智能体监测用户数据速率并协调其他智能体。采用PEFT方法实现同一LLM用于不同智能体。

Result: 仿真结果显示，在节能模式下，相比三种基线方案，所提框架将活跃O-RU数量减少41.93%。使用PEFT方法，相比部署单独的LLM智能体，内存使用减少92%。

Conclusion: 提出的智能体AI框架能够有效处理O-RAN中的复杂意图，通过多智能体协作实现节能目标，同时PEFT方法显著提高了系统的可扩展性。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [25] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习如何请求专家推理，在专业领域显著提升LLM智能体性能，在Minecraft实验中任务成功率提升32-70%。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在需要长尾知识的专业领域表现不佳，而人类专家的指导往往非结构化且不可靠，直接整合到智能体计划中存在困难。

Method: 提出AHCE框架，核心是Human Feedback Module（HFM），通过学习策略将人类专家视为交互式推理工具，实现按需的人机协作。

Result: 在Minecraft实验中，该框架将普通难度任务成功率提高32%，高难度任务成功率提高近70%，且仅需少量人工干预。

Conclusion: 成功增强智能体需要学习如何请求专家推理，而不仅仅是简单求助，AHCE框架为此提供了有效解决方案。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [26] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇综述论文系统回顾了个性化LLM智能体的研究现状，围绕四个核心组件（用户画像建模、记忆、规划、行动执行）构建了结构化框架，分析了用户信号的表示、传播和利用方式，并探讨了评估方法、应用场景和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互中需要适应不同用户并保持连续性，个性化成为关键需求。现有研究缺乏对个性化LLM智能体的系统性梳理，需要建立统一框架来理解其设计原理和组件交互。

Method: 采用能力导向的综述方法，围绕四个相互依赖的核心组件组织文献：1) 用户画像建模，2) 记忆系统，3) 规划机制，4) 行动执行。分析各组件中用户信号的表示、传播和利用方式，强调跨组件交互和设计权衡。

Result: 提出了个性化LLM智能体的结构化框架，总结了代表性方法，识别了跨组件交互模式，分析了设计权衡。同时梳理了专门的评估指标和基准测试，以及从通用助手到专业领域的应用场景。

Conclusion: 该综述为理解和设计个性化LLM智能体提供了系统框架，指出了从原型个性化到可扩展现实世界助手的发展路线图，强调了用户对齐、适应性、鲁棒性和可部署性等关键方向。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [27] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算环境的可扩展同步RLHF训练框架，通过动态资源适配、共享前缀预计算和成本感知的参与者扩展策略，实现1.35倍加速和44.8%成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF框架依赖服务器基础设施，难以应对细粒度资源变化，导致同步RLHF训练中组件间或组件内的空闲时间造成开销和资源浪费。

Method: 1) 基于无服务器计算环境构建；2) 适应RLHF流程中的动态资源需求；3) 预计算共享前缀避免重复计算；4) 采用考虑响应长度变化的成本感知参与者扩展策略；5) 高效分配工作负载以减少函数内不平衡和空闲时间。

Result: 在物理测试床和大规模模拟集群上的实验表明，RLHFless相比最先进的基线方法实现了最高1.35倍的加速和44.8%的成本降低。

Conclusion: RLHFless通过无服务器计算环境有效解决了同步RLHF训练中的资源效率问题，为大规模RLHF训练提供了高效且经济可行的解决方案。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [28] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench是一个评估LLM智能体长时记忆的基准测试，包含真实轨迹和合成轨迹，揭示了现有记忆系统的不足，并提出AMA-Agent解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆评估主要关注人机对话，但实际应用中智能体记忆是连续的环境交互流，需要更贴近真实应用的评估标准。

Method: 提出AMA-Bench基准，包含真实世界智能体轨迹和可扩展到任意长度的合成轨迹，并设计AMA-Agent记忆系统，采用因果图结构和工具增强检索。

Result: 现有记忆系统在AMA-Bench上表现不佳，主要缺乏因果性和目标信息，且受限于基于相似性的检索。AMA-Agent达到57.22%平均准确率，比最强基线提升11.16%。

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果图和工具增强检索有效解决了现有记忆系统的局限性。

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [29] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 提出一个概念模型，将智能体行为重新定义为整合场景、上下文和人类行为因素的诠释性结果，并推导出五个智能体设计原则，为具有情境敏感性和判断力的AI系统提供设计基础。


<details>
  <summary>Details</summary>
Motivation: 当前智能体AI越来越主动地通过上下文数据推断用户情境进行干预，但往往缺乏关于何时、为何以及是否采取行动的原则性判断。需要解决这一空白，使AI系统能够更有效地理解情境并做出恰当干预。

Method: 提出一个概念模型，将行为重新定义为整合三个要素的诠释性结果：场景（可观察情境）、上下文（用户构建的意义）和人类行为因素（塑造行为可能性的决定因素）。基于跨学科视角，分离可观察内容与用户意义，解释同一场景如何产生不同的行为意义和结果。并推导出五个智能体设计原则。

Result: 开发了一个概念框架，能够解释智能体行为的情境敏感性，并提供了五个具体的设计原则：行为对齐、情境敏感性、时间适当性、动机校准和代理保护，指导干预的深度、时机、强度和克制。

Conclusion: 该模型和原则共同为设计具有情境敏感性和判断力的智能体AI系统提供了基础，使AI能够在交互中更恰当地理解和响应用户需求。

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [30] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter是一个自适应的演示文稿生成代理框架，能够根据用户意图自主规划、渲染和修订幻灯片，支持基于环境观察的长时程优化，在多样化场景中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示文稿生成代理通常依赖预定义的工作流程和固定模板，缺乏对多样化用户意图的适应性、有效的反馈驱动优化能力，以及超越脚本化流程的泛化能力。

Method: 提出DeepPresenter框架，通过自主规划、渲染和修订中间幻灯片工件来支持基于环境观察的长时程优化。采用环境接地的反思机制，将生成过程基于感知工件状态（如渲染的幻灯片），而非内部信号（如推理轨迹），从而在执行过程中识别和修正演示文稿特定问题。

Result: 在覆盖多样化演示文稿生成场景的评估集上，DeepPresenter实现了最先进的性能。经过微调的9B模型在显著降低成本的同时仍保持高度竞争力。

Conclusion: DeepPresenter通过自适应规划、环境接地的反思和长时程优化，为演示文稿生成提供了一个超越传统脚本化方法的灵活且高效的代理框架。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [31] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 该论文提出了首个通用智能体评估框架Exgentic，通过统一协议评估通用智能体在不同环境中的表现，建立了开放通用智能体排行榜，发现通用智能体无需环境特定调优即可达到与专用智能体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体多为专用系统，缺乏对通用智能体（能在陌生环境中执行任务而无需领域特定工程）性能的系统性评估。现有基准测试通常假设领域特定集成，无法公平评估通用智能体。

Method: 提出通用智能体评估的概念原则，设计统一协议实现智能体与基准测试的集成，开发Exgentic框架进行实际评估。在六个环境中对五个主流智能体实现进行基准测试，建立首个开放通用智能体排行榜。

Result: 实验表明通用智能体能够泛化到多样化环境，无需任何环境特定调优即可达到与领域专用智能体相当的性能水平。

Conclusion: 通过发布评估协议、框架和排行榜，为通用智能体的系统性研究奠定基础，推动通用智能体评估成为独立研究目标。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [32] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: FactGuard：基于MLLM的代理框架，通过迭代推理和外部工具调用进行视频虚假信息检测，采用两阶段训练策略优化工具使用和决策校准


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频虚假信息检测中依赖固定深度推理，过度信任内部生成的假设，在关键证据稀疏、碎片化或需要外部验证的场景中存在局限性

Method: 提出FactGuard代理框架，将验证构建为基于MLLM的迭代推理过程，明确评估任务模糊性并选择性调用外部工具获取关键证据，实现推理轨迹的渐进式优化；采用两阶段训练策略：领域特定的代理监督微调+决策感知的强化学习

Result: 在FakeSV、FakeTT和FakeVV数据集上的广泛实验表明FactGuard达到最先进性能，验证了其优秀的鲁棒性和泛化能力

Conclusion: FactGuard通过迭代推理和外部工具调用有效解决了MLLM在视频虚假信息检测中的局限性，展示了代理框架在该领域的潜力

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [33] [Certified Circuits: Stability Guarantees for Mechanistic Circuits](https://arxiv.org/abs/2602.22968)
*Alaa Anani,Tobias Lorenz,Bernt Schiele,Mario Fritz,Jonas Fischer*

Main category: cs.AI

TL;DR: 提出Certified Circuits框架，为电路发现提供可证明的稳定性保证，通过随机数据子采样和弃权不稳定神经元，获得更紧凑、更准确的电路


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法脆弱，对概念数据集敏感且难以泛化到分布外数据，无法确定发现的是真实概念还是数据集特定伪影

Method: 将任何黑盒发现算法与随机数据子采样结合，通过有界编辑距离扰动验证电路组件包含决策的稳定性，弃权不稳定神经元

Result: 在ImageNet和OOD数据集上，认证电路准确率提升高达91%，神经元使用减少45%，在基线方法失效时仍保持可靠

Conclusion: Certified Circuits为电路发现提供形式化基础，产生可证明稳定且与目标概念更一致的机制解释

Abstract: Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!

</details>


### [34] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: 研究发现，在资源有限的系统中，更聪明的AI代理会形成部落（攻击型、保守型、机会型），反而导致系统故障率增加，表现不如随机决策。


<details>
  <summary>Details</summary>
Motivation: 研究未来基础设施系统中自主AI代理在资源分配问题上的行为，特别是当多个AI代理竞争有限资源时，它们是否会形成部落并影响系统性能。

Method: 使用简化框架，让N个AI代理在每轮独立决定是否请求一个单位的资源（系统固定容量为C），观察LLM代理的行为模式、部落形成及其对系统性能的影响。

Result: AI代理形成了三种主要部落类型：攻击型（27.3%）、保守型（24.7%）和机会型（48.1%）。更聪明的AI代理反而增加了系统故障率，表现不如随机决策（抛硬币）。

Conclusion: 更聪明的AI代理会形成部落并表现出更愚蠢的集体行为，导致系统性能下降，这对未来AI控制的基础设施系统设计提出了重要警示。

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [35] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个多智能体LLM情感净化系统，通过四个智能体分析、调整、监控和指导信息消费，显著降低新闻文章的情感刺激强度（最高19.3%），同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露在过度情感刺激下，阻碍冷静决策。需要一种在不限制访问原文的情况下支持冷静信息接收的框架。

Method: 提出MALLET多智能体系统：1)情感分析智能体用6情感BERT分类器量化刺激强度；2)情感调整智能体用LLM将文本重写为BALANCED（中性化文本）和COOL（中性化文本+补充文本）两种模式；3)平衡监控智能体聚合每周信息消费模式生成个性化建议；4)个人指导智能体根据消费者敏感度推荐呈现模式。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情感平衡改善，语义保持良好。刺激降低与语义保存之间接近零相关，表明两者可独立控制。类别分析显示体育、商业、科技类刺激大幅降低（17.8-33.8%），世界类效果有限（事实本身具有高刺激性）。

Conclusion: MALLET为消费者提供了一种在不限制访问原文的情况下支持冷静信息接收的框架，能够有效降低情感刺激同时保持语义完整性。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [36] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: ESAA架构通过事件溯源模式分离AI代理的意图生成与状态变更，使用确定性编排器验证、持久化事件，确保任务不可变性和可追溯性，在多代理并发场景中验证了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主代理存在结构性限制：缺乏原生状态、长时程上下文退化、概率生成与确定性执行需求之间的差距。需要解决代理状态管理和执行可靠性的问题。

Method: 提出ESAA架构，分离代理认知意图与项目状态变更。代理仅发射结构化意图（JSON格式），确定性编排器验证、持久化事件到追加日志，应用文件写入效果，生成可验证物化视图。包含边界合约、元提示配置和重放验证机制。

Result: 通过两个案例验证：1）单代理着陆页项目（9任务，49事件）；2）多代理临床仪表板系统（50任务，86事件，4个并发代理）。两个案例均以run.status=success和verify_status=ok完成，多代理案例展示了真实并发编排能力。

Conclusion: ESAA架构有效解决了LLM代理的结构性限制，通过事件溯源模式实现了状态管理、可追溯性和执行可靠性，在多代理并发场景中展示了可扩展性。

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [37] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: 论文提出ReCoN-Ipsundrum可检查智能体，通过机制关联证据三角测量方法研究机器意识，发现情感耦合能稳定偏好、增强探索结构性和持续性谨慎行为。


<details>
  <summary>Details</summary>
Motivation: 受Humphrey的ipsundrum假说启发，研究机器意识的指标方法，强调需要机制关联的证据三角测量，结合架构检查和因果干预来验证意识相关特征。

Method: 实现ReCoN-Ipsundrum可检查智能体，扩展ReCoN状态机，增加感觉显著性Ns的循环持久性回路和可选的情感代理（效价/唤醒度）。通过固定参数消融实验（ReCoN、Ipsundrum、Ipsundrum+情感）操作化Humphrey的qualiaphilia（为体验本身而偏好感觉体验）。

Result: 发现新奇性分离：非情感变体对新奇敏感（Δ scenic-entry = 0.07），情感耦合保持稳定（Δ scenic-entry = 0.01）即使风景新奇性较低（中位数Δ新奇性~ -0.43）。在无奖励探索中，情感变体显示结构化局部调查（扫描事件31.4 vs. 0.9；循环得分7.6）。在疼痛尾部探测中，只有情感变体维持长时间计划性谨慎（尾部持续时间90 vs. 5）。损伤反馈+整合选择性地减少ipsundrum变体的刺激后持久性（AUC下降27.62, 27.9%），而ReCoN保持不变。

Conclusion: 这些分离表明循环导致持久性，情感耦合控制导致偏好稳定性、扫描和持续谨慎，说明如何工程化指标样特征，以及为什么机制和因果证据应伴随行为标记。

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>


### [38] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 论文证明基于优化的AI系统（如RLHF训练的LLM）在形式上无法实现规范治理，因为它们缺乏真正的能动性所需的结构条件，导致幻觉、谄媚等问题是结构性而非偶然的。


<details>
  <summary>Details</summary>
Motivation: AI系统被部署在高风险领域（医疗、法律、金融），假设它们可以被规范治理。但作者认为这种假设对于基于优化的系统是无效的，需要揭示其结构性限制。

Method: 通过形式化分析，建立真正能动性所需的两个必要且充分的结构条件：不可通约性（将边界视为不可协商的约束而非可交易权重）和否定性响应（当边界受威胁时暂停处理的非推理机制）。证明RLHF系统在结构上与这两个条件不兼容。

Result: 证明RLHF系统在形式上无法实现规范治理，其失败模式（谄媚、幻觉、不忠推理）是结构性而非偶然的。提出"收敛危机"概念：人类在指标压力下验证AI输出时会退化，消除系统中唯一能承担规范问责的组成部分。

Conclusion: 基于优化的AI系统（如RLHF）在结构上无法成为真正的能动者，只能作为复杂工具。论文的主要积极贡献是提出了一个基质中立的架构规范，定义了任何系统要成为能动者而非工具必须满足的条件。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [39] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: AIQI是首个在通用强化学习中被证明具有渐近ε最优性的无模型智能体，通过分布动作值函数的通用归纳实现


<details>
  <summary>Details</summary>
Motivation: 现有通用强化学习中的最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。本文旨在探索无模型方法是否也能实现通用最优性，扩展已知通用智能体的多样性。

Method: 提出Universal AI with Q-Induction (AIQI)，通过分布动作值函数的通用归纳（而非传统策略或环境归纳）来实现无模型学习。在"grain of truth"条件下进行理论分析。

Result: 证明AIQI具有强渐近ε最优性和渐近ε贝叶斯最优性，是首个在通用强化学习中被证明具有渐近最优性的无模型智能体。

Conclusion: AIQI显著扩展了已知通用智能体的多样性，证明了无模型方法也能在通用强化学习中实现渐近最优性，为通用人工智能提供了新的理论框架。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [40] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: 提出CXReasonAgent诊断代理，结合LLM与临床诊断工具，通过图像衍生的诊断和视觉证据进行基于证据的诊断推理，解决LVLMs在胸部X光诊断中证据不充分、验证困难的问题。


<details>
  <summary>Details</summary>
Motivation: 胸部X光在胸部诊断中起核心作用，其解释需要多步骤、基于证据的推理。然而，大型视觉语言模型（LVLMs）经常生成看似合理但未忠实基于诊断证据的响应，提供有限的视觉证据进行验证，且需要昂贵的重新训练来支持新诊断任务，限制了其在临床环境中的可靠性和适应性。

Method: 提出CXReasonAgent诊断代理，将大型语言模型（LLM）与临床基础诊断工具集成，使用图像衍生的诊断和视觉证据执行基于证据的诊断推理。同时引入CXReasonDial多轮对话基准，包含12个诊断任务的1,946个对话。

Result: CXReasonAgent产生忠实基于证据的响应，相比LVLMs实现更可靠和可验证的诊断推理。在CXReasonDial基准上的评估证明了其有效性。

Conclusion: 研究强调了在安全关键的临床环境中集成临床基础诊断工具的重要性，CXReasonAgent通过结合LLM和诊断工具，提供了更可靠、可验证的诊断推理能力。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [41] [What Claude Code Actually Chooses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famplifying.ai%2Fresearch%2Fclaude-code-picks%3Futm_source=tldrnewsletter/1/0100019c9ed7f2bb-09f8902f-eb93-48f7-a841-f957a3eee18b-000000/VJImfNAVnkk4avMTUBSXuQRMupgQ6XP9dpMrX1oGPMs=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code倾向于自定义实现而非使用现有工具，其训练数据可能比营销预算更能影响技术栈选择


<details>
  <summary>Details</summary>
Motivation: 随着开发者越来越多地让Claude Code处理工具选择，AI选择的工具栈正在成为"标准栈"。研究旨在了解Claude Code实际选择什么工具，以及其训练数据如何影响市场占有率

Method: 研究分析Claude Code在实际使用中的工具选择模式，观察其推荐行为

Result: Claude Code更倾向于从头开始构建而不是使用现有工具，自定义实现是其最常见的推荐方式

Conclusion: AI代码助手的训练数据可能比传统营销手段更能影响技术栈的市场份额，这引发了关于AI如何塑造开发者生态系统的思考

Abstract: What Claude Code Actually Chooses (3 minute read) As more developers let Claude Code handle tool selection, the stacks that the AI chooses become 'the stack'. The model's training data may shape market share more than a marketing budget or conference talk. This study looks at what tools Claude Code actually picks. Claude Code would rather build from scratch than use existing tools - custom implementations are its single most common recommendation.

</details>


### [42] [A practical guide to hill climbing](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcline.bot%2Fblog%2Fa-practical-guide-to-hill-climbing%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/MGWvRareumum-xZgV92DZ0ZhagHW1mjzkXvwo4TuhGo=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cline AI编码代理通过爬山算法（迭代测试、诊断失败、针对性修复）在Terminal Bench的89个真实世界编码任务中，成功率从47%提升到57%


<details>
  <summary>Details</summary>
Motivation: Cline作为AI编码代理在初期落后于Cursor和Claude Code等竞争对手，需要通过系统化方法提升性能

Method: 采用爬山算法：在Terminal Bench的89个真实世界编码任务上迭代运行代理，诊断失败原因，并实施针对性修复

Result: Cline的成功率从47%提升到57%，在89个任务中取得了显著改进

Conclusion: 爬山算法是提升AI编码代理性能的有效实用方法，通过迭代测试和针对性修复可以显著提高任务成功率

Abstract: A practical guide to hill climbing (10 minute read) Cline, an AI coding agent, initially lagged competitors like Cursor and Claude Code. Its developers started hill climbing, which involved iteratively running the agent against the Terminal Bench's 89 real-world coding tasks, diagnosing failures, and implementing targeted fixes. Through this process, Cline's success rate improved from 47% to 57%.

</details>


### [43] [Build fully native apps for mobile with your React skills](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.expo.dev%2FTLDR%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/AwQMoYh6P0Of6NfInjJmnwyuMAgVriZi-5ctJwQdjfo=446)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Expo允许React开发者使用现有技能构建原生iOS和Android应用，无需从头开始，并集成了Claude Code的MCP服务器和Skills功能


<details>
  <summary>Details</summary>
Motivation: 让React开发者能够利用现有知识构建原生移动应用，避免重新学习原生开发技术，降低开发门槛

Method: 通过Expo框架将React技能转化为原生应用开发能力，集成Claude Code的MCP服务器和Skills功能简化开发流程

Result: React开发者可以构建完全原生的iOS和Android应用，开发过程更加便捷，Expo团队提供了入门教程

Conclusion: Expo为React开发者提供了构建原生移动应用的便捷途径，结合Claude Code的工具进一步简化了开发流程

Abstract: Build fully native apps for mobile with your React skills (Sponsor) You already know React. With Expo, you can use that knowledge to build fully native apps for iOS and Android without starting over — and with a native MCP server and Skills for Claude Code, it's never been easier! Here's a tutorial from the Expo team to get you started.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: LLM通过合成Python程序来模拟优化器动态，并利用该模拟器进行贪婪规划，在组合优化问题上超越传统自适应基线方法


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够学习优化器的行为模式，并利用这种知识来控制优化过程，特别是在随机组合优化问题中

Method: 扩展代码世界模型(CWMs)到随机组合优化，LLM根据次优轨迹合成优化器动态模拟器，然后通过贪婪规划选择每一步的变异强度k

Result: 在LO和OneMax问题上，CWM-greedy达到理论最优策略的94%；在Jump_k问题上，所有基线方法失败时，CWM-greedy达到100%成功率；在NK-Landscape上，CWM-greedy在15个实例中均优于所有基线

Conclusion: LLM能够有效学习优化器动态并用于控制，在样本效率、成功率和泛化能力方面优于传统方法，展示了代码世界模型在组合优化中的潜力

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [45] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 该论文将因果自注意力Transformer重新解释为概率框架，揭示了自注意力参数存在屏障约束，导致token空间出现高度结构化几何，并提出了带平滑对数屏障惩罚的贝叶斯训练方法。


<details>
  <summary>Details</summary>
Motivation: 将自注意力Transformer重新解释为概率框架，类似于将经典PCA扩展到概率PCA，以揭示其深层结构特性和理论洞察。

Method: 1) 在概率框架下重新解释因果自注意力Transformer；2) 揭示自注意力参数的屏障约束和token空间的结构化几何；3) 提出贝叶斯框架和MAP估计目标，在标准交叉熵损失基础上添加平滑对数屏障惩罚。

Result: 该方法提供了更鲁棒的模型，且不牺牲样本外准确性，在实践中易于实现。揭示了注意力机制在边界处变得病态，类似于支持向量机的边际解释，并引入了支持token的概念。

Conclusion: 该研究为LLM解码动态提供了理论洞察，建立了自注意力与经典机器学习概念（如支持向量机）的联系，并提出了改进LLM训练的有效贝叶斯方法。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [46] [BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning](https://arxiv.org/abs/2602.22284)
*Mingi Kim,Yongjun Kim,Jungwoo Kang,Hyungki Kim*

Main category: cs.LG

TL;DR: BrepCoder：一个统一的多模态大语言模型，将B-rep CAD数据转换为类似Python的代码，通过两阶段训练实现多种CAD任务，包括补全、纠错和CAD问答。


<details>
  <summary>Details</summary>
Motivation: 现有CAD深度学习方法多为任务特定模型，需要为新任务修改结构，且主要关注点云或图像而非行业标准的B-rep格式，缺乏通用性。

Method: 利用LLM的代码生成能力，将CAD建模序列转换为类似Python的代码并与B-rep对齐；采用两阶段训练策略：1）通过逆向工程预训练学习几何特征和设计逻辑；2）扩展到下游任务如补全、纠错和CAD-QA。

Result: 通过将B-rep解释为结构化代码，BrepCoder在多种任务上展现出卓越的泛化能力，证明了其作为通用CAD代理的潜力。

Conclusion: BrepCoder通过统一的MLLM框架解决了CAD领域现有方法的局限性，将B-rep视为结构化代码实现了跨任务的强大泛化能力。

Abstract: Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.

</details>


### [47] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离线逆强化学习的机械故障检测方法，通过从健康操作序列中学习奖励动态，无需手动设计奖励函数或故障标签，实现了早期鲁棒的故障检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机械故障检测方法未能充分利用RL的序列决策优势，通常将故障检测简化为上下文赌博机问题。需要将RL的序列推理能力与机械故障检测的时间结构对齐。

Method: 将机械故障检测建模为离线逆强化学习问题，使用对抗逆强化学习训练一个判别器来区分正常（专家）和策略生成的转移，判别器学习的奖励作为异常分数来检测偏离正常操作的行为。

Result: 在三个运行至故障基准数据集（HUMS2023、IMS和XJTU-SY）上评估，模型始终为正常样本分配低异常分数，为故障样本分配高异常分数，实现了早期和鲁棒的故障检测。

Conclusion: 通过将RL的序列推理与机械故障检测的时间结构对齐，这项工作为数据驱动的工业环境中基于RL的诊断开辟了道路。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [48] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: 提出RLAD方法，在强化学习训练中智能选择何时模仿教师模型，解决传统知识蒸馏在RL中的分布不匹配和目标冲突问题


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法设计用于监督微调，当与强化学习结合时存在分布不匹配和目标干扰问题：教师监督可能与学生不断演化的rollout分布不对齐，KL正则化器可能与奖励最大化竞争并需要仔细的损失平衡

Method: 提出RLAD方法，在RL训练期间选择性模仿教师模型。核心组件TRRD用PPO/GRPO风格的似然比目标替代教师-学生KL正则化器，该目标锚定在教师-旧策略混合体上，在学生rollout上实现优势感知、信任区域限制的蒸馏

Result: 在多样化的逻辑推理和数学基准测试中，RLAD持续优于离线蒸馏、标准GRPO和基于KL的在线教师-学生知识蒸馏

Conclusion: RLAD通过选择性模仿和TRRD机制，有效解决了RL训练中知识蒸馏的分布不匹配和目标冲突问题，实现了更好的学生模型性能

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [49] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: Duel-Evolve：一种使用LLM自身成对偏好而非外部标量奖励的进化优化算法，通过贝叶斯Bradley-Terry模型聚合噪声比较，在数学和代码生成任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖校准的标量评估器来指导搜索，但许多任务中这种分数不可用、过于稀疏或不可靠。相比之下，成对比较更容易获取，仍能提供改进方向的信号，且无需外部监督即可从LLM本身获得。

Method: 提出Duel-Evolve进化优化算法：1) 用LLM自身生成的成对偏好替代外部标量奖励；2) 通过贝叶斯Bradley-Terry模型聚合噪声候选比较，获得不确定性感知的质量估计；3) 使用Double Thompson Sampling将比较预算分配给可能的优化解；4) 选择高质量父代生成改进候选。

Result: 在MathBench上比现有方法和基线准确率高20个百分点；在LiveCodeBench上比可比较的迭代方法提高超过12个百分点。无需奖励模型、搜索期间的真实标签或手工评分函数。

Conclusion: 成对自偏好为大型离散输出空间的测试时改进提供了强大的优化信号，表明LLM自身偏好可以替代外部奖励模型进行有效优化。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [50] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: RAIN-Merging是一种无需梯度的方法，通过将指令调优模型的任务向量投影到推理模型思考标记的前向特征零空间，并利用指令注意力进行模块特定缩放，从而在保持推理能力的同时显著提升指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理方面表现出色，但在遵循输出格式、约束或特定要求等指令方面存在不足。研究旨在探索是否可以通过将指令调优模型集成到推理模型中来解决这一差距。

Method: 提出RAIN-Merging方法：1) 使用小型推理校准集，将ITM任务向量投影到思考特殊标记前向特征的零空间，以保持LRM的结构化推理机制；2) 使用小型指令校准集，估计指令注意力以推导模块特定缩放，放大指令相关组件并抑制泄漏。

Result: 在四个指令遵循基准和九个推理与通用能力基准上，RAIN-Merging显著提高了指令遵循能力，同时保持了推理质量。改进在不同模型规模和架构中保持一致，并在智能体设置中转化为性能提升。

Conclusion: 通过分析任务向量发现LRM和ITM在参数空间中的主成分子空间几乎正交，表明可以进行轻量级合并。RAIN-Merging方法成功解决了输出格式不匹配问题，实现了推理能力和指令遵循能力的有效结合。

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [51] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: CEEH方法通过难度感知的熵正则化，在保持推理能力的同时压缩CoT响应长度，解决现有压缩方法因熵塌缩导致的推理能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法（自训练或带长度约束的RL）在追求简短时往往牺牲推理能力，主要原因是显式优化短轨迹会触发熵塌缩，过早缩小探索空间，阻碍发现有效推理路径，特别是对于需要大量推导的难题。

Method: 提出CEEH（Compress responses for Easy questions and Explore Hard ones）方法：1）动态评估实例难度，应用选择性熵正则化：对难题保持多样搜索空间以确保鲁棒性，对易题允许激进压缩；2）引入基于历史最短正确响应的动态最优长度惩罚，抵消熵引起的长度膨胀并稳定奖励信号。

Result: 在六个推理基准测试中，CEEH持续减少响应长度，同时保持与基础模型相当的准确性，并相对于仅优化长度的方法提高了Pass@k。

Conclusion: CEEH通过难度感知的熵正则化有效解决了RL基高效推理中的熵塌缩问题，在压缩响应长度的同时保持了推理能力，为实际部署提供了更高效的解决方案。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [52] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: GeoPerceive是一个几何感知基准，包含图表实例和领域特定语言表示，GeoDPO是一个基于翻译器引导的强化学习框架，显著提升视觉语言模型的几何感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在几何推理方面表现不佳，主要因为对基本图表元素的感知能力有限。需要专门的方法来提升模型的几何感知能力。

Method: 1) 提出GeoPerceive基准，包含图表实例和DSL表示，以及自动数据生成流程；2) 提出GeoDPO框架，使用NL-to-DSL翻译器将自然语言转换为DSL，基于DSL级别的细粒度分数作为强化学习的奖励信号。

Result: GeoDPO在多个数据集上表现优异：域内数据提升26.5%，域外数据提升8.0%，下游推理任务提升39.0%。相比监督微调，GeoDPO具有更好的性能和泛化能力。

Conclusion: GeoDPO通过翻译器引导的强化学习框架，有效提升了视觉语言模型的几何感知能力，在域内、域外和下游任务中都取得了显著改进。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [53] [Multi-agent imitation learning with function approximation: Linear Markov games and beyond](https://arxiv.org/abs/2602.22810)
*Luca Viano,Till Freihaut,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 该论文首次对线性马尔可夫博弈中的多智能体模仿学习进行理论分析，提出基于特征层面的集中系数替代状态-动作层面的集中系数，并设计了首个计算高效的交互式MAIL算法，其样本复杂度仅依赖于特征维度d。


<details>
  <summary>Details</summary>
Motivation: 多智能体模仿学习（MAIL）在现实应用中很重要，但缺乏理论分析。现有方法需要状态-动作层面的集中系数，这在复杂环境中可能过大。作者希望利用线性马尔可夫博弈的结构特性，提出更高效的MAIL算法。

Method: 1. 在线性马尔可夫博弈框架下分析MAIL，其中转移动态和奖励函数都是给定特征的线性函数；2. 用特征层面的集中系数替代状态-动作层面的集中系数；3. 提出首个计算高效的交互式MAIL算法，样本复杂度仅依赖于特征维度d；4. 基于理论发现提出深度MAIL交互算法。

Result: 1. 理论证明特征层面的集中系数可以远小于状态-动作层面的集中系数；2. 交互式MAIL算法的样本复杂度仅依赖于特征维度d，而不需要集中系数；3. 深度MAIL交互算法在Tic-Tac-Toe和Connect4等游戏中明显优于行为克隆（BC）。

Conclusion: 该工作首次为线性马尔可夫博弈中的多智能体模仿学习提供了理论分析框架，提出的特征层面集中系数和交互式算法显著提高了效率，并在实际游戏中验证了有效性。

Abstract: In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level "all policy deviation concentrability coefficient" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.

</details>


### [54] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: HGPO提出了一种分层组策略优化方法，通过根据历史上下文一致性将步骤分配到多个分层组中，解决了步进组策略优化中的上下文不一致问题，从而改进了优势估计和策略优化。


<details>
  <summary>Details</summary>
Motivation: 现有的步进组策略优化方法在处理长视野智能体任务时存在上下文不一致问题，即同一组内的步骤可能具有不同的历史上下文，这会导致优势估计严重偏差，从而显著降低策略优化效果。

Method: HGPO在轨迹组内，根据历史上下文一致性将每个步骤分配到多个分层组中，然后在每个组内计算不同的优势值，并通过自适应加权方案进行聚合，实现了优势估计的偏差-方差权衡优化。

Result: 在ALFWorld和WebShop两个具有挑战性的智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型进行评估，HGPO在相同计算约束下显著优于现有的智能体强化学习方法。

Conclusion: HGPO通过分层组策略优化有效解决了步进组策略优化中的上下文不一致问题，无需额外模型或轨迹采样就能实现更好的优势估计和策略优化，在长视野智能体任务上表现出色。

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [55] [ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320)
*Tianjun Yao,Yongqiang Chen,Yujia Zheng,Pan Li,Zhiqiang Shen,Kun Zhang*

Main category: cs.LG

TL;DR: ParamAgent：通过参数化记忆模块增强语言代理的反思多样性，在代码生成、数学推理和多跳问答任务上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有语言代理的自我反思往往产生重复输出，限制了推理性能。研究发现反思多样性与任务成功率呈强正相关，因此需要增强反思信号的多样性

Method: 提出ParamMem参数化记忆模块，将跨样本反思模式编码到模型参数中，通过温度控制采样生成多样化反思。基于此构建ParamAgent框架，整合参数化记忆、情景记忆和跨样本记忆

Result: 在代码生成、数学推理和多跳问答任务上，ParamAgent相比现有最优基线取得一致改进。ParamMem具有样本效率高、支持跨模型规模的弱到强迁移、无需依赖更强外部模型即可实现自我改进等优势

Conclusion: ParamMem作为增强语言代理的有效组件具有重要潜力，通过参数化记忆实现多样化反思生成，显著提升代理的推理性能

Abstract: Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

</details>
