{"id": "2602.15983", "categories": ["cs.SE", "cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.15983", "abs": "https://arxiv.org/abs/2602.15983", "authors": ["Junbo Jacob Lian", "Yujun Sun", "Huiling Chen", "Chaoyu Zhang", "Chung-Piaw Teo"], "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization", "comment": "Code and benchmark: \\url{https://github.com/junbolian/ReLoop}", "summary": "Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.", "AI": {"tldr": "ReLoop\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u548c\u884c\u4e3a\u9a8c\u8bc1\u89e3\u51b3LLM\u751f\u6210\u4f18\u5316\u4ee3\u7801\u65f6\u7684\u9759\u9ed8\u5931\u8d25\u95ee\u9898\uff0c\u5c06\u6b63\u786e\u7387\u4ece22.6%\u63d0\u5347\u523031.1%\uff0c\u6267\u884c\u7387\u4ece72.1%\u63d0\u5347\u5230100%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4f18\u5316\u4ee3\u7801\uff0c\u4f46\u5b58\u5728\u9759\u9ed8\u5931\u8d25\u98ce\u9669\uff1a\u4ee3\u7801\u53ef\u4ee5\u6267\u884c\u5e76\u8fd4\u56de\u6c42\u89e3\u5668\u53ef\u884c\u7684\u89e3\uff0c\u4f46\u53ef\u80fd\u7f16\u7801\u4e86\u8bed\u4e49\u9519\u8bef\u7684\u516c\u5f0f\uff0c\u5bfc\u81f4\u53ef\u884c\u6027\u4e0e\u6b63\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u9ad8\u8fbe90\u4e2a\u767e\u5206\u70b9\u7684\u5dee\u8ddd\u3002", "method": "ReLoop\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a1) \u7ed3\u6784\u5316\u751f\u6210\uff1a\u5c06\u4ee3\u7801\u751f\u6210\u5206\u89e3\u4e3a\u7406\u89e3\u3001\u5f62\u5f0f\u5316\u3001\u5408\u6210\u3001\u9a8c\u8bc1\u56db\u9636\u6bb5\u63a8\u7406\u94fe\uff0c\u6a21\u62df\u4e13\u5bb6\u5efa\u6a21\u5b9e\u8df5\uff1b2) \u884c\u4e3a\u9a8c\u8bc1\uff1a\u901a\u8fc7\u57fa\u4e8e\u6c42\u89e3\u5668\u7684\u53c2\u6570\u6270\u52a8\u6d4b\u8bd5\u516c\u5f0f\u54cd\u5e94\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u68c0\u6d4b\u9519\u8bef\u3002\u540c\u65f6\u4f7f\u7528IIS\u589e\u5f3a\u7684\u8bca\u65ad\u8fdb\u884c\u6267\u884c\u6062\u590d\u3002", "result": "\u5728\u6700\u5f3a\u6a21\u578b\u4e0a\uff0cReLoop\u5c06\u6b63\u786e\u7387\u4ece22.6%\u63d0\u5347\u523031.1%\uff0c\u6267\u884c\u7387\u4ece72.1%\u63d0\u5347\u5230100.0%\u3002\u5728\u4e94\u4e2a\u6a21\u578b\uff08\u57fa\u7840\u6a21\u578b\u3001SFT\u3001RL\uff09\u548c\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\u3002\u7ed3\u6784\u5316\u751f\u6210\u5728\u590d\u6742\u7ec4\u5408\u95ee\u9898\u4e0a\u5360\u4e3b\u5bfc\uff0c\u884c\u4e3a\u9a8c\u8bc1\u5728\u5c40\u90e8\u516c\u5f0f\u7f3a\u9677\u95ee\u9898\u4e0a\u8d21\u732e\u6700\u5927\u3002", "conclusion": "ReLoop\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u548c\u884c\u4e3a\u9a8c\u8bc1\u7684\u4e92\u8865\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u751f\u6210\u4f18\u5316\u4ee3\u7801\u65f6\u7684\u9759\u9ed8\u5931\u8d25\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6b63\u786e\u6027\u548c\u6267\u884c\u7387\u3002\u540c\u65f6\u53d1\u5e03\u4e86RetailOpt-190\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9488\u5bf9LLM\u6700\u5e38\u5931\u8d25\u7684\u591a\u7ea6\u675f\u4ea4\u4e92\u573a\u666f\u3002", "topic": "code agent"}}
{"id": "2602.16069", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16069", "abs": "https://arxiv.org/abs/2602.16069", "authors": ["Ravi Raju", "Mengmeng Ji", "Shubhangi Upasani", "Bo Li", "Urmish Thakker"], "title": "The Limits of Long-Context Reasoning in Automated Bug Fixing", "comment": "4 pages, under review", "summary": "Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.", "AI": {"tldr": "\u5f53\u524dLLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u8c03\u8bd5\u548c\u8865\u4e01\u751f\u6210\u65b9\u9762\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u5c3d\u7ba1\u4ee3\u7406\u5de5\u4f5c\u6d41\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u5206\u89e3\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u771f\u6b63\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u4eba\u4eec\u666e\u904d\u5047\u8bbeLLMs\u53ef\u4ee5\u76f4\u63a5\u5bf9\u6574\u4e2a\u4ee3\u7801\u5e93\u8fdb\u884c\u63a8\u7406\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5f53\u524dLLMs\u662f\u5426\u80fd\u591f\u53ef\u9760\u5730\u6267\u884c\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u8c03\u8bd5\u548c\u8865\u4e01\u751f\u6210\u3002", "method": "\u4f7f\u7528SWE-bench Verified\u4f5c\u4e3a\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u9996\u5148\u5728\u4ee3\u7406\u6846\u67b6(mini-SWE-agent)\u4e2d\u8bc4\u4f30\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u7136\u540e\u6784\u5efa\u6570\u636e\u7ba1\u9053\u4eba\u4e3a\u81a8\u80c0\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u7814\u7a76\u5355\u6b21\u8865\u4e01\u751f\u6210\u5728\u771f\u6b63\u957f\u4e0a\u4e0b\u6587(64k-128k tokens)\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4ee3\u7406\u5de5\u4f5c\u6d41\u663e\u8457\u63d0\u5347\u6027\u80fd(GPT-5-nano\u8fbe\u523031%\u89e3\u51b3\u7387)\uff0c\u4f46\u6210\u529f\u8f68\u8ff9\u901a\u5e38\u4fdd\u6301\u572820k tokens\u4ee5\u4e0b\u3002\u5728\u771f\u6b63\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e0b\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d(Qwen3-Coder-30B-A3B\u572864k\u4e0a\u4e0b\u6587\u4ec57%\u89e3\u51b3\u7387\uff0cGPT-5-nano\u4e3a0%)\uff0c\u51fa\u73b0\u7cfb\u7edf\u6027\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u5f53\u524dLLMs\u7684\u540d\u4e49\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0e\u5b9e\u9645\u53ef\u7528\u4e0a\u4e0b\u6587\u5bb9\u91cf\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u73b0\u6709\u7684\u4ee3\u7406\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u6709\u6548\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "topic": "swe benchmark"}}
{"id": "2602.15843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15843", "abs": "https://arxiv.org/abs/2602.15843", "authors": ["Warren Johnson"], "title": "The Perplexity Paradox: Why Code Compresses Better Than Math in LLM Prompts", "comment": "19 pages, 5 figures, 4 tables. Second paper in TAAC research series. Code and data at https://github.com/micoverde/taac-llm-compression", "summary": "In \"Compress or Route?\" (Johnson, 2026), we found that code generation tolerates aggressive prompt compression (r >= 0.6) while chain-of-thought reasoning degrades gradually. That study was limited to HumanEval (164 problems), left the \"perplexity paradox\" mechanism unvalidated, and provided no adaptive algorithm. This paper addresses all three gaps. First, we validate across six code benchmarks (HumanEval, MBPP, HumanEval+, MultiPL-E) and four reasoning benchmarks (GSM8K, MATH, ARC-Challenge, MMLU-STEM), confirming the compression threshold generalizes across languages and difficulties. Second, we conduct the first per-token perplexity analysis (n=723 tokens), revealing a \"perplexity paradox\": code syntax tokens are preserved (high perplexity) while numerical values in math problems are pruned despite being task-critical (low perplexity). Signature injection recovers +34 percentage points in pass rate (5.3% to 39.3%; Cohen's h=0.890). Third, we propose TAAC (Task-Aware Adaptive Compression), achieving 22% cost reduction with 96% quality preservation, outperforming fixed-ratio compression by 7%. MBPP validation (n=1,800 trials) confirms systematic variation: 3.6% at r=0.3 to 54.6% at r=1.0.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u538b\u7f29(TAAC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u4e0e\u63a8\u7406\u4efb\u52a1\u7684\u538b\u7f29\u9608\u503c\u5dee\u5f02\uff0c\u53d1\u73b0\"\u56f0\u60d1\u5ea6\u6096\u8bba\"\uff0c\u5e76\u5b9e\u73b022%\u6210\u672c\u964d\u4f4e\u4e0e96%\u8d28\u91cf\u4fdd\u6301\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u80fd\u5bb9\u5fcd\u6fc0\u8fdb\u63d0\u793a\u538b\u7f29(r>=0.6)\u800c\u601d\u7ef4\u94fe\u63a8\u7406\u4f1a\u9010\u6e10\u9000\u5316\uff0c\u4f46\u7814\u7a76\u4ec5\u9650\u4e8eHumanEval\u57fa\u51c6\u3001\u672a\u9a8c\u8bc1\"\u56f0\u60d1\u5ea6\u6096\u8bba\"\u673a\u5236\u3001\u7f3a\u4e4f\u81ea\u9002\u5e94\u7b97\u6cd5\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e09\u4e2a\u7814\u7a76\u7a7a\u767d\u3002", "method": "1) \u57286\u4e2a\u4ee3\u7801\u57fa\u51c6\u548c4\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u538b\u7f29\u9608\u503c\uff1b2) \u9996\u6b21\u8fdb\u884c\u6bcf\u4ee4\u724c\u56f0\u60d1\u5ea6\u5206\u6790(n=723\u4ee4\u724c)\uff0c\u63ed\u793a\"\u56f0\u60d1\u5ea6\u6096\u8bba\"\uff1b3) \u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u538b\u7f29(TAAC)\u7b97\u6cd5\u3002", "result": "\u538b\u7f29\u9608\u503c\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u96be\u5ea6\u4efb\u52a1\u4e2d\u666e\u904d\u9002\u7528\uff1b\u4ee3\u7801\u8bed\u6cd5\u4ee4\u724c\u88ab\u4fdd\u7559(\u9ad8\u56f0\u60d1\u5ea6)\u800c\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u6570\u503c\u88ab\u4fee\u526a(\u4f4e\u56f0\u60d1\u5ea6)\uff1b\u7b7e\u540d\u6ce8\u5165\u4f7f\u901a\u8fc7\u7387\u63d0\u5347+34\u4e2a\u767e\u5206\u70b9\uff1bTAAC\u5b9e\u73b022%\u6210\u672c\u964d\u4f4e\u548c96%\u8d28\u91cf\u4fdd\u6301\u3002", "conclusion": "\u4efb\u52a1\u611f\u77e5\u81ea\u9002\u5e94\u538b\u7f29\u80fd\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\uff0c\u4e3a\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u63d0\u4f9b\u4f18\u5316\u7684\u538b\u7f29\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "2602.16106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16106", "abs": "https://arxiv.org/abs/2602.16106", "authors": ["Shahriar Rumi Dipto", "Saikat Mondal", "Chanchal K. Roy"], "title": "Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs", "comment": "Accepted at 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026)", "summary": "Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u6cd5\u7684\u4ee3\u7801\u7ffb\u8bd1\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u4e2d\u7acb\u7684\u4e2d\u95f4\u89c4\u8303\u6765\u6355\u83b7\u7a0b\u5e8f\u7ec6\u8282\uff0c\u76f8\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u5c06\u51c6\u786e\u7387\u4ece67.7%\u63d0\u5347\u523078.5%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5404\u79cd\u7f16\u8bd1\u548c\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u7ffb\u8bd1\u65f6\uff0c\u76f4\u63a5\u7684\u4e00\u6b21\u6027\u7ffb\u8bd1\u65b9\u6cd5\u7ecf\u5e38\u65e0\u6cd5\u4fdd\u6301\u7a0b\u5e8f\u610f\u56fe\uff0c\u5bfc\u81f4\u63a7\u5236\u6d41\u3001\u7c7b\u578b\u5904\u7406\u548cI/O\u884c\u4e3a\u7b49\u65b9\u9762\u7684\u9519\u8bef\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u7ffb\u8bd1\u540e\u7684\u4ee3\u7801\u4fdd\u6301\u539f\u59cb\u7a0b\u5e8f\u7684\u529f\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7b97\u6cd5\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff0c\u5f15\u5165\u8bed\u8a00\u4e2d\u7acb\u7684\u4e2d\u95f4\u89c4\u8303\u6765\u6355\u83b7\u7a0b\u5e8f\u7ec6\u8282\uff0c\u7136\u540e\u8fdb\u884c\u4ee3\u7801\u751f\u6210\u3002\u5728Avatar\u548cCodeNet\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e94\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u8fdb\u884cPython\u548cJava\u4e4b\u95f4\u7684\u7ffb\u8bd1\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u901a\u8fc7\u7f16\u8bd1\u3001\u6267\u884c\u548c\u6d4b\u8bd5\u6765\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u7b97\u6cd5\u65b9\u6cd5\u5c06\u5fae\u5e73\u5747\u51c6\u786e\u7387\u4ece67.7%\u63d0\u9ad8\u523078.5%\uff08\u63d0\u534710.8%\uff09\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u8bcd\u6c47\u548c\u6807\u8bb0\u9519\u8bef\uff0c\u51cf\u5c11\u4e8672.7%\u7684\u4e0d\u5b8c\u6574\u6784\u9020\uff0c\u964d\u4f4e\u4e8661.1%\u7684\u7ed3\u6784\u548c\u58f0\u660e\u95ee\u9898\uff0c\u5e76\u5c06\u8fd0\u884c\u65f6\u4f9d\u8d56\u548c\u5165\u53e3\u70b9\u5931\u8d25\u964d\u4f4e\u4e8678.4%\u3002", "conclusion": "\u57fa\u4e8e\u7b97\u6cd5\u7684\u6d41\u6c34\u7ebf\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u610f\u56fe\u4fdd\u6301\u7684\u4ee3\u7801\u7ffb\u8bd1\uff0c\u4e3a\u5065\u58ee\u7684\u591a\u8bed\u8a00\u7f16\u7a0b\u52a9\u624b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u7ed3\u6784\u5316\u89c4\u5212\u663e\u8457\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "topic": "code agent"}}
{"id": "2602.16037", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16037", "abs": "https://arxiv.org/abs/2602.16037", "authors": ["Cameron Cagan", "Pedram Fard", "Jiazi Tian", "Jingya Cheng", "Shawn N. Murphy", "Hossein Estiri"], "title": "Optimization Instability in Autonomous Agentic Workflows for Clinical Symptom Detection", "comment": null, "summary": "Autonomous agentic workflows that iteratively refine their own behavior hold considerable promise, yet their failure modes remain poorly characterized. We investigate optimization instability, a phenomenon in which continued autonomous improvement paradoxically degrades classifier performance, using Pythia, an open-source framework for automated prompt optimization. Evaluating three clinical symptoms with varying prevalence (shortness of breath at 23%, chest pain at 12%, and Long COVID brain fog at 3%), we observed that validation sensitivity oscillated between 1.0 and 0.0 across iterations, with severity inversely proportional to class prevalence. At 3% prevalence, the system achieved 95% accuracy while detecting zero positive cases, a failure mode obscured by standard evaluation metrics. We evaluated two interventions: a guiding agent that actively redirected optimization, amplifying overfitting rather than correcting it, and a selector agent that retrospectively identified the best-performing iteration successfully prevented catastrophic failure. With selector agent oversight, the system outperformed expert-curated lexicons on brain fog detection by 331% (F1) and chest pain by 7%, despite requiring only a single natural language term as input. These findings characterize a critical failure mode of autonomous AI systems and demonstrate that retrospective selection outperforms active intervention for stabilization in low-prevalence classification tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u81ea\u4e3bAI\u7cfb\u7edf\u5728\u6301\u7eed\u4f18\u5316\u4e2d\u4f1a\u51fa\u73b0\u6027\u80fd\u9000\u5316\u73b0\u8c61\uff0c\u79f0\u4e3a\"\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\"\uff0c\u5c24\u5176\u5728\u4f4e\u60a3\u75c5\u7387\u5206\u7c7b\u4efb\u52a1\u4e2d\u66f4\u4e3a\u4e25\u91cd\u3002\u901a\u8fc7Pythia\u6846\u67b6\u6d4b\u8bd5\u4e34\u5e8a\u75c7\u72b6\u68c0\u6d4b\uff0c\u53d1\u73b0\u9a8c\u8bc1\u654f\u611f\u5ea6\u5728\u8fed\u4ee3\u4e2d\u5267\u70c8\u6ce2\u52a8\uff0c\u7cfb\u7edf\u53ef\u80fd\u8fbe\u523095%\u51c6\u786e\u7387\u5374\u68c0\u6d4b\u4e0d\u5230\u4efb\u4f55\u9633\u6027\u75c5\u4f8b\u3002\u7814\u7a76\u8868\u660e\u56de\u987e\u6027\u9009\u62e9\u4ee3\u7406\u6bd4\u4e3b\u52a8\u5e72\u9884\u66f4\u6709\u6548\u9632\u6b62\u707e\u96be\u6027\u5931\u8d25\u3002", "motivation": "\u7814\u7a76\u81ea\u4e3bAI\u5de5\u4f5c\u6d41\u7a0b\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u4f18\u5316\u4e0d\u7a33\u5b9a\u6027\u73b0\u8c61\u2014\u2014\u5373\u6301\u7eed\u81ea\u4e3b\u4f18\u5316\u53cd\u800c\u5bfc\u81f4\u5206\u7c7b\u5668\u6027\u80fd\u4e0b\u964d\u3002\u5f53\u524d\u5bf9\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u7f3a\u4e4f\u7cfb\u7edf\u8868\u5f81\uff0c\u5c24\u5176\u5728\u4f4e\u60a3\u75c5\u7387\u4efb\u52a1\u4e2d\uff0c\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u53ef\u80fd\u63a9\u76d6\u4e25\u91cd\u95ee\u9898\u3002", "method": "\u4f7f\u7528Pythia\u5f00\u6e90\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff0c\u8bc4\u4f30\u4e09\u79cd\u4e0d\u540c\u60a3\u75c5\u7387\u7684\u4e34\u5e8a\u75c7\u72b6\uff08\u6c14\u77ed23%\u3001\u80f8\u75db12%\u3001\u957f\u65b0\u51a0\u8111\u96fe3%\uff09\u3002\u6d4b\u8bd5\u4e24\u79cd\u5e72\u9884\u7b56\u7565\uff1a1\uff09\u6307\u5bfc\u4ee3\u7406\u4e3b\u52a8\u91cd\u5b9a\u5411\u4f18\u5316\uff1b2\uff09\u9009\u62e9\u4ee3\u7406\u56de\u987e\u6027\u8bc6\u522b\u6700\u4f73\u8fed\u4ee3\u3002\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\u4e0e\u4e13\u5bb6\u7b56\u5212\u8bcd\u5178\u7684\u6027\u80fd\u3002", "result": "\u9a8c\u8bc1\u654f\u611f\u5ea6\u5728\u8fed\u4ee3\u4e2d\u57281.0\u548c0.0\u4e4b\u95f4\u632f\u8361\uff0c\u4e25\u91cd\u7a0b\u5ea6\u4e0e\u7c7b\u522b\u60a3\u75c5\u7387\u6210\u53cd\u6bd4\u3002\u57283%\u60a3\u75c5\u7387\u4e0b\uff0c\u7cfb\u7edf\u8fbe\u523095%\u51c6\u786e\u7387\u5374\u68c0\u6d4b\u4e0d\u5230\u4efb\u4f55\u9633\u6027\u75c5\u4f8b\u3002\u6307\u5bfc\u4ee3\u7406\u53cd\u800c\u653e\u5927\u4e86\u8fc7\u62df\u5408\uff0c\u800c\u9009\u62e9\u4ee3\u7406\u6210\u529f\u9632\u6b62\u4e86\u707e\u96be\u6027\u5931\u8d25\u3002\u4f7f\u7528\u9009\u62e9\u4ee3\u7406\u540e\uff0c\u7cfb\u7edf\u5728\u8111\u96fe\u68c0\u6d4b\u4e0a\u6bd4\u4e13\u5bb6\u8bcd\u5178\u63d0\u5347331%\uff08F1\uff09\uff0c\u80f8\u75db\u68c0\u6d4b\u63d0\u53477%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u8868\u660e\u5728\u4f4e\u60a3\u75c5\u7387\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u56de\u987e\u6027\u9009\u62e9\u6bd4\u4e3b\u52a8\u5e72\u9884\u66f4\u6709\u6548\u5730\u7a33\u5b9a\u7cfb\u7edf\u6027\u80fd\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u81ea\u4e3bAI\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16671", "abs": "https://arxiv.org/abs/2602.16671", "authors": ["Jaid Monwar Chowdhury", "Chi-An Fu", "Reyhaneh Jabbarvand"], "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation", "comment": "9 pages, 6 figures, 4 tables", "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.", "AI": {"tldr": "SPARC\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u3001\u57fa\u4e8e\u573a\u666f\u7684\u6846\u67b6\uff0c\u7528\u4e8eC\u8bed\u8a00\u5355\u5143\u6d4b\u8bd5\u751f\u6210\uff0c\u901a\u8fc7\u7ed3\u5408\u63a7\u5236\u6d41\u56fe\u5206\u6790\u3001\u64cd\u4f5c\u6620\u5c04\u3001\u8def\u5f84\u5b9a\u5411\u6d4b\u8bd5\u5408\u6210\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "C\u8bed\u8a00\u81ea\u52a8\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u9762\u4e34\u8bed\u4e49\u9e3f\u6c9f\u6311\u6218\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u5b58\u5728\"\u8df3\u8dc3\u5230\u4ee3\u7801\"\u5931\u8d25\u6a21\u5f0f\uff0c\u5bfc\u81f4\u4e0d\u53ef\u7f16\u8bd1\u7684\u6d4b\u8bd5\u3001\u5e7b\u89c9\u51fd\u6570\u7b7e\u540d\u3001\u4f4e\u5206\u652f\u8986\u76d6\u7387\u548c\u8bed\u4e49\u65e0\u5173\u7684\u65ad\u8a00\u3002", "method": "SPARC\u91c7\u7528\u56db\u9636\u6bb5\u6846\u67b6\uff1a1) \u63a7\u5236\u6d41\u56fe\u5206\u6790\uff1b2) \u64cd\u4f5c\u6620\u5c04\uff0c\u5c06LLM\u63a8\u7406\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u7684\u5b9e\u7528\u52a9\u624b\uff1b3) \u8def\u5f84\u5b9a\u5411\u6d4b\u8bd5\u5408\u6210\uff1b4) \u4f7f\u7528\u7f16\u8bd1\u5668\u548c\u8fd0\u884c\u65f6\u53cd\u9988\u7684\u8fed\u4ee3\u81ea\u6821\u6b63\u9a8c\u8bc1\u5faa\u73af\u3002", "result": "\u572859\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u7b97\u6cd5\u4e3b\u9898\u4e0a\uff0cSPARC\u76f8\u6bd4\u57fa\u7ebf\u5728\u884c\u8986\u76d6\u7387\u63d0\u534731.36%\uff0c\u5206\u652f\u8986\u76d6\u7387\u63d0\u534726.01%\uff0c\u53d8\u5f02\u5206\u6570\u63d0\u534720.78%\uff0c\u5728\u590d\u6742\u4e3b\u9898\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u7b26\u53f7\u6267\u884c\u5de5\u5177KLEE\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u4fdd\u755994.3%\u7684\u6d4b\u8bd5\uff0c\u4ee3\u7801\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u63a8\u7406\u4e0e\u7a0b\u5e8f\u7ed3\u6784\u5bf9\u9f50\uff0cSPARC\u4e3a\u5de5\u4e1a\u7ea7\u9057\u7559C\u4ee3\u7801\u5e93\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "code agent"}}
{"id": "2602.16105", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16105", "abs": "https://arxiv.org/abs/2602.16105", "authors": ["Thinh Hung Truong", "Jey Han Lau", "Jianzhong Qi"], "title": "GPSBench: Do Large Language Models Understand GPS Coordinates?", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench", "AI": {"tldr": "GPSBench\u662f\u4e00\u4e2a\u5305\u542b57,800\u4e2a\u6837\u672c\u3001\u6db5\u76d617\u4e2a\u4efb\u52a1\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5LLM\u5728GPS\u5750\u6807\u548c\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u771f\u5b9e\u4e16\u754c\u5730\u7406\u63a8\u7406\u65b9\u9762\u6bd4\u51e0\u4f55\u8ba1\u7b97\u66f4\u53ef\u9760\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u5e94\u7528\uff08\u5982\u5bfc\u822a\u3001\u673a\u5668\u4eba\u3001\u5730\u56fe\uff09\uff0c\u5f3a\u5927\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0cLLM\u5728GPS\u5750\u6807\u548c\u771f\u5b9e\u4e16\u754c\u5730\u7406\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165GPSBench\u6570\u636e\u96c6\uff0c\u5305\u542b57,800\u4e2a\u6837\u672c\u548c17\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u51e0\u4f55\u5750\u6807\u64cd\u4f5c\uff08\u5982\u8ddd\u79bb\u548c\u65b9\u4f4d\u8ba1\u7b97\uff09\u4ee5\u53ca\u5c06\u5750\u6807\u4e0e\u4e16\u754c\u77e5\u8bc6\u7ed3\u5408\u7684\u63a8\u7406\u4efb\u52a1\u3002\u8bc4\u4f30\u4e8614\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u5173\u6ce8\u5185\u5728\u6a21\u578b\u80fd\u529b\u800c\u975e\u5de5\u5177\u4f7f\u7528\u3002", "result": "GPS\u63a8\u7406\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e0d\u540c\u4efb\u52a1\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5730\u7406\u63a8\u7406\u65b9\u9762\u901a\u5e38\u6bd4\u51e0\u4f55\u8ba1\u7b97\u66f4\u53ef\u9760\u3002\u5730\u7406\u77e5\u8bc6\u5448\u5c42\u6b21\u6027\u9000\u5316\uff0c\u56fd\u5bb6\u5c42\u9762\u8868\u73b0\u5f3a\u4f46\u57ce\u5e02\u5c42\u9762\u5b9a\u4f4d\u5f31\u3002\u5bf9\u5750\u6807\u566a\u58f0\u7684\u9c81\u68d2\u6027\u8868\u660e\u6a21\u578b\u5177\u6709\u771f\u6b63\u7684\u5750\u6807\u7406\u89e3\u800c\u975e\u7b80\u5355\u8bb0\u5fc6\u3002", "conclusion": "GPS\u5750\u6807\u589e\u5f3a\u53ef\u4ee5\u6539\u5584\u4e0b\u6e38\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u6027\u80fd\uff0c\u5fae\u8c03\u4f1a\u5728\u51e0\u4f55\u8ba1\u7b97\u6536\u76ca\u548c\u4e16\u754c\u77e5\u8bc6\u9000\u5316\u4e4b\u95f4\u4ea7\u751f\u6743\u8861\u3002\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u6d1e\u89c1\u3002", "topic": "agent analysis"}}
{"id": "2602.15849", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15849", "abs": "https://arxiv.org/abs/2602.15849", "authors": ["Karun Sharma", "Vidushee Vats", "Shengzhi Li", "Yuxiang Wang", "Zhongtian Sun", "Prayag Tiwari"], "title": "Preference Optimization for Review Question Generation Improves Writing Quality", "comment": "24 Pages, v1", "summary": "Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.", "AI": {"tldr": "IntelliReward\u662f\u4e00\u4e2a\u57fa\u4e8e\u51bb\u7ed3\u81ea\u56de\u5f52LLM\u6784\u5efa\u7684\u65b0\u578b\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4e13\u5bb6\u7ea7\u4eba\u7c7b\u504f\u597d\uff0c\u7ed3\u5408IntelliAsk\u6a21\u578b\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u6807\u51c6\u7684\u5ba1\u7a3f\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5ba1\u7a3f\u95ee\u9898\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u4ea7\u751f\u8868\u9762\u5316\u95ee\u9898\uff0c\u8d85\u8fc750%\u7684\u95ee\u9898token\u6765\u81ea\u8bba\u6587\u7b2c\u4e00\u9875\uff0c\u7f3a\u4e4f\u5b9e\u8d28\u6027\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u6df1\u5ea6\u63d0\u95ee\u3002", "method": "\u5f00\u53d1IntelliReward\u5956\u52b1\u6a21\u578b\uff08\u57fa\u4e8e\u51bb\u7ed3\u81ea\u56de\u5f52LLM+\u53ef\u8bad\u7ec3\u591a\u5934transformer\uff09\uff0c\u5e94\u7528Decoupled Clip\u548cDAPO\u7b56\u7565\u4f18\u5316\uff0c\u8bad\u7ec3IntelliAsk\u95ee\u9898\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u5176\u4e0e\u4eba\u7c7b\u5728\u52aa\u529b\u7a0b\u5ea6\u3001\u8bc1\u636e\u57fa\u7840\u548c\u63a5\u5730\u6027\u65b9\u9762\u7684\u6807\u51c6\u5bf9\u9f50\u3002", "result": "IntelliAsk\u5728\u63a8\u7406\u548c\u5199\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728MuSR\u63a8\u7406\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ece64.7\u63d0\u5347\u523068.3\uff0c\u5728WritingBench\u5199\u4f5c\u8bc4\u4f30\u4e2d\u4ece8.07\u63d0\u5347\u52308.31\u3002", "conclusion": "\u5ba1\u7a3f\u95ee\u9898\u8d28\u91cf\u4e0e\u6a21\u578b\u66f4\u5e7f\u6cdb\u7684\u80fd\u529b\u76f8\u5173\uff0cIntelliReward\u548cIntelliAsk\u4e3aLLM\u751f\u6210\u7684\u5ba1\u7a3f\u95ee\u9898\u63d0\u4f9b\u4e86\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\uff0c\u63a8\u52a8\u4e86\u5b9e\u8d28\u6027\u5ba1\u7a3f\u95ee\u9898\u751f\u6210\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.16173", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16173", "abs": "https://arxiv.org/abs/2602.16173", "authors": ["Kaiqu Liang", "Julia Kruk", "Shengyi Qian", "Xianjun Yang", "Shengjie Bi", "Yuanshun Yao", "Shaoliang Nie", "Mingyang Zhang", "Lijuan Liu", "Jaime Fern\u00e1ndez Fisac", "Shuyan Zhou", "Saghar Hosseini"], "title": "Learning Personalized Agents from Human Feedback", "comment": null, "summary": "Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.", "AI": {"tldr": "PAHF\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u7528\u6237\u8bb0\u5fc6\u548c\u53cc\u53cd\u9988\u901a\u9053\u5b9e\u73b0\u6301\u7eed\u4e2a\u6027\u5316\uff0c\u5728\u521d\u59cb\u504f\u597d\u5b66\u4e60\u548c\u504f\u597d\u6f02\u79fb\u9002\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u65e0\u8bb0\u5fc6\u548c\u5355\u901a\u9053\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3AI\u4ee3\u7406\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u96be\u4ee5\u4e0e\u4e2a\u4f53\u7528\u6237\u7684\u72ec\u7279\u3001\u52a8\u6001\u504f\u597d\u5bf9\u9f50\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65b0\u7528\u6237\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u504f\u597d\u3002", "method": "\u63d0\u51faPAHF\u6846\u67b6\uff0c\u5305\u542b\u4e09\u6b65\u5faa\u73af\uff1a1)\u884c\u52a8\u524d\u6f84\u6e05\u89e3\u51b3\u6b67\u4e49\uff1b2)\u57fa\u4e8e\u8bb0\u5fc6\u68c0\u7d22\u504f\u597d\u7684\u884c\u52a8\u57fa\u7840\uff1b3)\u6574\u5408\u884c\u52a8\u540e\u53cd\u9988\u66f4\u65b0\u8bb0\u5fc6\u3002\u91c7\u7528\u663e\u5f0f\u7528\u6237\u8bb0\u5fc6\u548c\u53cc\u53cd\u9988\u901a\u9053\u3002", "result": "\u5728\u5177\u8eab\u64cd\u4f5c\u548c\u5728\u7ebf\u8d2d\u7269\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAHF\u5b66\u4e60\u901f\u5ea6\u663e\u8457\u66f4\u5feb\uff0c\u6301\u7eed\u4f18\u4e8e\u65e0\u8bb0\u5fc6\u548c\u5355\u901a\u9053\u57fa\u7ebf\uff0c\u51cf\u5c11\u521d\u59cb\u4e2a\u6027\u5316\u9519\u8bef\u5e76\u5feb\u901f\u9002\u5e94\u504f\u597d\u6f02\u79fb\u3002", "conclusion": "\u663e\u5f0f\u8bb0\u5fc6\u4e0e\u53cc\u53cd\u9988\u901a\u9053\u7684\u6574\u5408\u5bf9\u4e8e\u6301\u7eed\u4e2a\u6027\u5316\u81f3\u5173\u91cd\u8981\uff0cPAHF\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u65b0\u7528\u6237\u548c\u504f\u597d\u6f02\u79fb\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2602.16179", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16179", "abs": "https://arxiv.org/abs/2602.16179", "authors": ["Sushant Mehta", "Logan Ritchie", "Suhaas Garre", "Nick Heiner", "Edwin Chen"], "title": "EnterpriseGym Corecraft: Training Generalizable Agents on High-Fidelity RL Environments", "comment": null, "summary": "We show that training AI agents on high-fidelity reinforcement learning environments produces capabilities that generalize beyond the training distribution. We introduce \\corecraft{}, the first environment in \\textsc{EnterpriseGym}, Surge AI's suite of agentic RL environments. \\corecraft{} is a fully operational enterprise simulation of a customer support organization, comprising over 2,500 entities across 14 entity types with 23 unique tools, designed to measure whether AI agents can perform the multi-step, domain-specific work that real jobs demand. Frontier models such as GPT-5.2 and Claude Opus 4.6 solve fewer than 30\\% of tasks when all expert-authored rubric criteria must be satisfied. Using this environment, we train GLM~4.6 with Group Relative Policy Optimization (GRPO) and adaptive clipping. After a single epoch of training, the model improves from 25.37\\% to 36.76\\% task pass rate on held-out evaluation tasks. More importantly, these gains transfer to out-of-distribution benchmarks: +4.5\\% on BFCL Parallel, +7.4\\% on $\u03c4^2$-Bench Retail, and +6.8\\% on Toolathlon (Pass@1). We believe three environment properties are consistent with the observed transfer: task-centric world building that optimizes for diverse, challenging tasks; expert-authored rubrics enabling reliable reward computation; and enterprise workflows that reflect realistic professional patterns. Our results suggest that environment quality, diversity, and realism are key factors enabling generalizable agent capabilities.", "AI": {"tldr": "\u5728\u9ad8\u8d28\u91cf\u4f01\u4e1a\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3AI\u4ee3\u7406\u80fd\u4ea7\u751f\u8d85\u8d8a\u8bad\u7ec3\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\uff0cCorecraft\u73af\u5883\u8bad\u7ec3GLM 4.6\u6a21\u578b\u540e\uff0c\u4efb\u52a1\u901a\u8fc7\u7387\u4ece25.37%\u63d0\u5347\u81f336.76%\uff0c\u5e76\u5728\u591a\u4e2a\u5916\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u5728\u9ad8\u8d28\u91cf\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u8bad\u7ec3AI\u4ee3\u7406\u662f\u5426\u80fd\u4ea7\u751f\u53ef\u6cdb\u5316\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u7684\u591a\u6b65\u9aa4\u3001\u9886\u57df\u7279\u5b9a\u5de5\u4f5c\u4efb\u52a1\u3002", "method": "\u5f00\u53d1Corecraft\u4f01\u4e1a\u6a21\u62df\u73af\u5883\uff08\u5305\u542b2500\u591a\u4e2a\u5b9e\u4f53\u300114\u79cd\u5b9e\u4f53\u7c7b\u578b\u548c23\u79cd\u5de5\u5177\uff09\uff0c\u4f7f\u7528Group Relative Policy Optimization (GRPO)\u548c\u81ea\u9002\u5e94\u88c1\u526a\u8bad\u7ec3GLM 4.6\u6a21\u578b\uff0c\u8fdb\u884c\u5355\u8f6e\u8bad\u7ec3\u540e\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u8bad\u7ec3\u540e\u6a21\u578b\u5728\u4fdd\u7559\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u901a\u8fc7\u7387\u4ece25.37%\u63d0\u5347\u81f336.76%\uff0c\u5728\u4e09\u4e2a\u5916\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u63d0\u53474.5%\u30017.4%\u548c6.8%\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u73af\u5883\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u771f\u5b9e\u6027\u662f\u4ea7\u751f\u53ef\u6cdb\u5316\u4ee3\u7406\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4efb\u52a1\u4e2d\u5fc3\u7684\u4e16\u754c\u6784\u5efa\u3001\u4e13\u5bb6\u7f16\u5199\u7684\u8bc4\u5206\u6807\u51c6\u548c\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u6709\u52a9\u4e8e\u80fd\u529b\u8fc1\u79fb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16246", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16246", "abs": "https://arxiv.org/abs/2602.16246", "authors": ["Yun-Shiuan Chuang", "Chaitanya Kulkarni", "Alec Chiu", "Avinash Thangali", "Zijie Pan", "Shivani Shekhar", "Yirou Ge", "Yixi Li", "Uma Kona", "Linsey Pang", "Prakhar Mehrotra"], "title": "Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents", "comment": null, "summary": "Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.", "AI": {"tldr": "\u63d0\u51faProxy State-Based Evaluation\u6846\u67b6\uff0c\u7528LLM\u9a71\u52a8\u7684\u6a21\u62df\u66ff\u4ee3\u786e\u5b9a\u6027\u540e\u7aef\uff0c\u8bc4\u4f30\u4ea4\u4e92\u5f0fLLM\u4ee3\u7406\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u4ee3\u7406\u57fa\u51c6\uff08\u5982tau-bench\u3001AppWorld\uff09\u4f9d\u8d56\u5b8c\u5168\u786e\u5b9a\u6027\u7684\u540e\u7aef\uff0c\u6784\u5efa\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u57fa\u4e8e\u4ee3\u7406\u72b6\u6001\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u573a\u666f\u6307\u5b9a\u7528\u6237\u76ee\u6807\u3001\u4e8b\u5b9e\u3001\u671f\u671b\u6700\u7ec8\u72b6\u6001\u548c\u884c\u4e3a\uff1bLLM\u72b6\u6001\u8ddf\u8e2a\u5668\u4ece\u4ea4\u4e92\u8f68\u8ff9\u63a8\u65ad\u7ed3\u6784\u5316\u4ee3\u7406\u72b6\u6001\uff1bLLM\u8bc4\u5224\u5668\u9a8c\u8bc1\u76ee\u6807\u5b8c\u6210\u5e76\u68c0\u6d4b\u5e7b\u89c9", "result": "\u57fa\u51c6\u4ea7\u751f\u7a33\u5b9a\u3001\u533a\u5206\u6a21\u578b\u7684\u6392\u540d\uff0c\u5176on-/off-policy rollout\u63d0\u4f9b\u53ef\u8fc1\u79fb\u7684\u76d1\u7763\uff1b\u6a21\u62df\u5668\u5e7b\u89c9\u7387\u63a5\u8fd1\u96f6\uff1b\u4eba\u7c7b-LLM\u8bc4\u5224\u5668\u4e00\u81f4\u6027\u8d85\u8fc790%", "conclusion": "\u4ee3\u7406\u72b6\u6001\u8bc4\u4f30\u4e3a\u5de5\u4e1aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u786e\u5b9a\u6027\u57fa\u51c6\u66ff\u4ee3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2602.15854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15854", "abs": "https://arxiv.org/abs/2602.15854", "authors": ["Jingyi Xu", "Xingyu Ren", "Zhiqiang You", "Yumeng Zhang", "Zhoupeng Shou"], "title": "Decoupling Strategy and Execution in Task-Focused Dialogue via Goal-Oriented Preference Optimization", "comment": null, "summary": "Large language models show potential in task-oriented dialogue systems, yet existing training methods often rely on token-level likelihood or preference optimization, which poorly align with long-horizon task success. To address this, we propose Goal-Oriented Preference Optimization (GOPO), a hierarchical reinforcement learning framework that decouples strategy planning from response generation via an Expert Agent and a Customer Service Agent. The Expert Agent optimizes multi-turn goal preferences at the dialogue-trajectory level, while the Customer Service Agent generates responses strictly aligned with the selected strategy. We evaluate GOPO on public benchmarks and e-commerce customer service datasets, and introduce Task-focused Sequential Engagement (TSE), a sequence-level metric derived from real e-commerce interaction data. On the Mgshop dataset, GOPO improves TSE by 7.7% and 10.3% over PPO and Memento, with consistent gains in sequence-level reward and generation quality. Furthermore, a 14B model trained with GOPO achieves 2.7% and 1.5% higher TSE than Qwen-235B and GPT-5.2, respectively. Ablation studies confirm the Expert Agent's critical role in long-horizon optimization. GOPO demonstrates consistent improvements across other datasets as well. This work establishes a new paradigm for task-oriented dialogue systems in commercial scenarios, with code and datasets to be made public.", "AI": {"tldr": "\u63d0\u51faGOPO\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u5c06\u7b56\u7565\u89c4\u5212\u4e0e\u54cd\u5e94\u751f\u6210\u89e3\u8026\uff0c\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56token\u7ea7\u4f3c\u7136\u6216\u504f\u597d\u4f18\u5316\uff0c\u4e0e\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u5bf9\u9f50\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u597d\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u8bad\u7ec3\u6846\u67b6", "method": "GOPO\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u4e13\u5bb6\u4ee3\u7406\u4f18\u5316\u5bf9\u8bdd\u8f68\u8ff9\u7ea7\u591a\u8f6e\u76ee\u6807\u504f\u597d\uff0c\u5ba2\u670d\u4ee3\u7406\u751f\u6210\u4e0e\u9009\u5b9a\u7b56\u7565\u4e25\u683c\u5bf9\u9f50\u7684\u54cd\u5e94", "result": "\u5728Mgshop\u6570\u636e\u96c6\u4e0a\uff0cGOPO\u6bd4PPO\u548cMemento\u5206\u522b\u63d0\u5347TSE 7.7%\u548c10.3%\uff1b14B\u6a21\u578b\u8bad\u7ec3\u540e\u6bd4Qwen-235B\u548cGPT-5.2\u5206\u522b\u9ad82.7%\u548c1.5% TSE", "conclusion": "GOPO\u4e3a\u5546\u4e1a\u573a\u666f\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u5efa\u7acb\u65b0\u8303\u5f0f\uff0c\u4e13\u5bb6\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u4f18\u5316\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00", "topic": "agent analysis"}}
{"id": "2602.15858", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15858", "abs": "https://arxiv.org/abs/2602.15858", "authors": ["Annie Wong", "Aske Plaat", "Thomas B\u00e4ck", "Niki van Stein", "Anna V. Kononova"], "title": "State Design Matters: How Representations Shape Dynamic Reasoning in Large Language Models", "comment": null, "summary": "As large language models (LLMs) move from static reasoning tasks toward dynamic environments, their success depends on the ability to navigate and respond to an environment that changes as they interact at inference time. An underexplored factor in these settings is the representation of the state. Holding model parameters fixed, we systematically vary three key aspects: (1) state granularity (long form versus summary), (2) structure (natural language versus symbolic), and (3) spatial grounding (text-only versus images or textual map encodings) across sequential decision-making benchmarks. We find that trajectory summarisation improves performance by reducing noise and stabilising long-horizon reasoning. Second, natural language representations are the most robust across models, whereas structured encodings help mainly for models with strong code or structured output priors, such as JSON schemas. Third, while image-inputs show some benefit, text-based spatial encodings prove most effective. This advantage stems not from the spatial information itself, but from the act of construction, which compels the model to perform the spatial reasoning that static input does not elicit. Overall, we demonstrate that design choices for representing state are a decisive factor in performance, distinct from the availability of information itself. We note, however, that even with improved representations, current LLMs and VLMs remain brittle over long horizons, particularly when they must synthesise information to manage multiple subtasks to reach a goal.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2dLLMs\u7684\u72b6\u6001\u8868\u793a\u8bbe\u8ba1\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8f68\u8ff9\u603b\u7ed3\u3001\u81ea\u7136\u8bed\u8a00\u8868\u793a\u548c\u6587\u672c\u7a7a\u95f4\u7f16\u7801\u6700\u6709\u6548\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u4ecd\u663e\u8106\u5f31\u3002", "motivation": "\u968f\u7740LLMs\u4ece\u9759\u6001\u63a8\u7406\u4efb\u52a1\u8f6c\u5411\u52a8\u6001\u73af\u5883\uff0c\u5176\u6210\u529f\u53d6\u51b3\u4e8e\u5728\u63a8\u7406\u65f6\u4e0e\u53d8\u5316\u73af\u5883\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u72b6\u6001\u8868\u793a\u4f5c\u4e3a\u8fd9\u4e9b\u8bbe\u7f6e\u4e2d\u672a\u5145\u5206\u63a2\u7d22\u7684\u56e0\u7d20\uff0c\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u5728\u4fdd\u6301\u6a21\u578b\u53c2\u6570\u56fa\u5b9a\u7684\u6761\u4ef6\u4e0b\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u4e09\u4e2a\u5173\u952e\u65b9\u9762\uff1a(1)\u72b6\u6001\u7c92\u5ea6\uff08\u957f\u5f62\u5f0fvs\u603b\u7ed3\uff09\uff0c(2)\u7ed3\u6784\uff08\u81ea\u7136\u8bed\u8a00vs\u7b26\u53f7\uff09\uff0c(3)\u7a7a\u95f4\u57fa\u7840\uff08\u7eaf\u6587\u672cvs\u56fe\u50cf\u6216\u6587\u672c\u5730\u56fe\u7f16\u7801\uff09\uff0c\u5e76\u5728\u987a\u5e8f\u51b3\u7b56\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u8f68\u8ff9\u603b\u7ed3\u901a\u8fc7\u51cf\u5c11\u566a\u58f0\u548c\u7a33\u5b9a\u957f\u65f6\u7a0b\u63a8\u7406\u6765\u63d0\u9ad8\u6027\u80fd\uff1b\u81ea\u7136\u8bed\u8a00\u8868\u793a\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u6700\u7a33\u5065\uff1b\u7ed3\u6784\u5316\u7f16\u7801\u4e3b\u8981\u5bf9\u5177\u6709\u5f3a\u4ee3\u7801\u6216\u7ed3\u6784\u5316\u8f93\u51fa\u5148\u9a8c\u7684\u6a21\u578b\u6709\u5e2e\u52a9\uff1b\u6587\u672c\u7a7a\u95f4\u7f16\u7801\u6bd4\u56fe\u50cf\u8f93\u5165\u66f4\u6709\u6548\uff0c\u4f18\u52bf\u6765\u81ea\u6784\u5efa\u8fc7\u7a0b\u672c\u8eab\u800c\u975e\u7a7a\u95f4\u4fe1\u606f\u3002", "conclusion": "\u72b6\u6001\u8868\u793a\u7684\u8bbe\u8ba1\u9009\u62e9\u662f\u6027\u80fd\u7684\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u4e0e\u4fe1\u606f\u53ef\u7528\u6027\u672c\u8eab\u4e0d\u540c\u3002\u7136\u800c\uff0c\u5373\u4f7f\u6709\u6539\u8fdb\u7684\u8868\u793a\uff0c\u5f53\u524dLLMs\u548cVLMs\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u4ecd\u7136\u8106\u5f31\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7efc\u5408\u4fe1\u606f\u7ba1\u7406\u591a\u4e2a\u5b50\u4efb\u52a1\u4ee5\u8fbe\u5230\u76ee\u6807\u65f6\u3002", "topic": "agent analysis"}}
{"id": "2602.16653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16653", "abs": "https://arxiv.org/abs/2602.16653", "authors": ["Yangjie Xu", "Lujun Li", "Lama Sleem", "Niccolo Gentile", "Yewei Song", "Yiqun Wang", "Siming Ji", "Wenbo Wu", "Radu State"], "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments", "comment": null, "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.", "AI": {"tldr": "Agent Skill\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u4e2d\u7b49\u89c4\u6a21SLMs\uff0812B-30B\u53c2\u6570\uff09\u7684\u6027\u80fd\uff0c\u800c\u6781\u5c0f\u6a21\u578b\u5728\u6280\u80fd\u9009\u62e9\u4e0a\u4ecd\u6709\u56f0\u96be\uff0c80B\u53c2\u6570\u7684\u4ee3\u7801\u4e13\u7528\u6a21\u578b\u80fd\u8fbe\u5230\u95ed\u6e90\u57fa\u7ebf\u6c34\u5e73\u5e76\u63d0\u5347GPU\u6548\u7387\u3002", "motivation": "\u7814\u7a76Agent Skill\u8303\u5f0f\u662f\u5426\u80fd\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5e26\u6765\u7c7b\u4f3c\u5927\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd9\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u6570\u636e\u5b89\u5168\u548c\u9884\u7b97\u9650\u5236\u4f7f\u5f97\u6301\u7eed\u4f9d\u8d56\u516c\u5171API\u4e0d\u53ef\u884c\uff0c\u4e14SLMs\u5728\u9ad8\u5ea6\u5b9a\u5236\u5316\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u9996\u5148\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86Agent Skill\u8fc7\u7a0b\u7684\u6570\u5b66\u5b9a\u4e49\uff0c\u7136\u540e\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u4e24\u4e2a\u5f00\u6e90\u4efb\u52a1\u548c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u4fdd\u9669\u7d22\u8d54\u6570\u636e\u96c6\u3002", "result": "\u6781\u5c0f\u6a21\u578b\u5728\u53ef\u9760\u6280\u80fd\u9009\u62e9\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u4e2d\u7b49\u89c4\u6a21SLMs\uff08\u7ea612B-30B\u53c2\u6570\uff09\u4eceAgent Skill\u65b9\u6cd5\u4e2d\u83b7\u76ca\u663e\u8457\uff0c\u7ea680B\u53c2\u6570\u7684\u4ee3\u7801\u4e13\u7528\u53d8\u4f53\u5728\u8fbe\u5230\u95ed\u6e90\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u4e86GPU\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u5168\u9762\u7ec6\u81f4\u5730\u63cf\u8ff0\u4e86Agent Skill\u6846\u67b6\u7684\u80fd\u529b\u548c\u9650\u5236\uff0c\u4e3a\u5728SLM\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u6709\u6548\u90e8\u7f72Agent Skills\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.16666", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16666", "abs": "https://arxiv.org/abs/2602.16666", "authors": ["Stephan Rabanser", "Sayash Kapoor", "Peter Kirgis", "Kangheng Liu", "Saiteja Utpala", "Arvind Narayanan"], "title": "Towards a Science of AI Agent Reliability", "comment": null, "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa12\u4e2a\u5177\u4f53\u6307\u6807\uff0c\u4ece\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u3001\u53ef\u9884\u6d4b\u6027\u548c\u5b89\u5168\u6027\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30AI\u4ee3\u7406\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u4ee3\u7406\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u867d\u7136\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f46\u53ef\u9760\u6027\u6539\u8fdb\u6709\u9650\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u6210\u529f\u7387\u6307\u6807\uff0c\u8fd9\u63a9\u76d6\u4e86\u5173\u952e\u7684\u64cd\u4f5c\u7f3a\u9677\uff0c\u65e0\u6cd5\u53cd\u6620\u4ee3\u7406\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5305\u62ec\u8de8\u8fd0\u884c\u4e00\u81f4\u6027\u3001\u6297\u5e72\u6270\u80fd\u529b\u3001\u53ef\u9884\u6d4b\u5931\u8d25\u548c\u9519\u8bef\u4e25\u91cd\u6027\u9650\u5236\u7b49\u65b9\u9762\u3002", "method": "\u57fa\u4e8e\u5b89\u5168\u5173\u952e\u5de5\u7a0b\u539f\u5219\uff0c\u63d0\u51fa12\u4e2a\u5177\u4f53\u6307\u6807\u6765\u5206\u89e3\u4ee3\u7406\u53ef\u9760\u6027\uff0c\u6db5\u76d6\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u3001\u53ef\u9884\u6d4b\u6027\u548c\u5b89\u5168\u6027\u3002\u572814\u4e2a\u4ee3\u7406\u6a21\u578b\u548c\u4e24\u4e2a\u4e92\u8865\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u5c3d\u7ba1\u8fd1\u671fAI\u4ee3\u7406\u5728\u80fd\u529b\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5728\u53ef\u9760\u6027\u65b9\u9762\u7684\u6539\u8fdb\u5f88\u5c0f\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u3001\u53ef\u9884\u6d4b\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u6301\u7eed\u5c40\u9650\u6027\u3002", "conclusion": "\u63d0\u51fa\u768412\u4e2a\u53ef\u9760\u6027\u6307\u6807\u4e3a\u4f20\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8865\u5145\uff0c\u63d0\u4f9b\u4e86\u5206\u6790\u4ee3\u7406\u5982\u4f55\u6267\u884c\u3001\u9000\u5316\u548c\u5931\u8d25\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u7406\u89e3AI\u4ee3\u7406\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2602.15863", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15863", "abs": "https://arxiv.org/abs/2602.15863", "authors": ["Daehoon Gwak", "Minseo Jung", "Junwoo Park", "Minho Park", "ChaeHun Park", "Junha Hyung", "Jaegul Choo"], "title": "Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning", "comment": "Presented at AACL-IJCNLP 2025", "summary": "Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u81ea\u6211\u751f\u6210\u5c11\u6837\u672c\u793a\u4f8b\u7684\u4f18\u52bf\u5e76\u975e\u6765\u81ea\u793a\u4f8b\u672c\u8eab\uff0c\u800c\u662f\u6765\u81ea\u521b\u5efa\u8fc7\u7a0b\u3002\u96c6\u6210\u63d0\u793a\uff08\u521b\u5efa\u548c\u89e3\u51b3\u95ee\u9898\u5728\u540c\u4e00\u63d0\u793a\u4e2d\uff09\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5206\u79bb\u63d0\u793a\u3002", "motivation": "\u5c3d\u7ba1\u7814\u7a76\u8868\u660eLLMs\u901a\u8fc7\u81ea\u6211\u751f\u6210\u5c11\u6837\u672c\u793a\u4f8b\u53ef\u4ee5\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u80cc\u540e\u7684\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u96be\u4ee5\u786e\u5b9a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u6709\u6548\u5e94\u7528\u8be5\u6280\u672f\u3002", "method": "\u5728\u591a\u79cdLLM\u67b6\u6784\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u96f6\u6837\u672c\u63d0\u793a\u3001\u96c6\u6210\u63d0\u793a\uff08LLMs\u5728\u540c\u4e00\u63d0\u793a\u4e2d\u521b\u5efa\u5e76\u89e3\u51b3\u95ee\u9898\uff09\u3001\u5206\u79bb\u63d0\u793a\uff08\u91cd\u7528\u81ea\u751f\u6210\u793a\u4f8b\u4f46\u6392\u9664\u521b\u5efa\u4e0a\u4e0b\u6587\uff09\u3002\u8fdb\u884c\u6ce8\u610f\u529b\u5206\u6790\u6bd4\u8f83\u6a21\u5f0f\u5dee\u5f02\u3002", "result": "\u96c6\u6210\u63d0\u793a\u59cb\u7ec8\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5206\u79bb\u63d0\u793a\uff0c\u800c\u5206\u79bb\u63d0\u793a\u76f8\u6bd4\u96f6\u6837\u672c\u53ea\u6709\u8fb9\u9645\u6539\u8fdb\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u96c6\u6210\u63d0\u793a\u548c\u5206\u79bb\u63d0\u793a\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u81ea\u6211\u751f\u6210\u63d0\u793a\u7684\u4f18\u52bf\u6765\u81ea\u95ee\u9898\u521b\u5efa\u8fc7\u7a0b\u672c\u8eab\uff0c\u800c\u975e\u751f\u6210\u7684\u793a\u4f8b\uff0c\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u63d0\u793a\u7b56\u7565\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2602.15868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15868", "abs": "https://arxiv.org/abs/2602.15868", "authors": ["Magnus Boman"], "title": "Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning", "comment": "8 pages, 1 page appendix", "summary": "Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u786e\u5b9a\u6027\u591a\u5e26\u56fe\u7075\u673a\u5f62\u5f0f\u5316LLM\u4ea4\u4e92\uff0c\u5c06\u4e0d\u540c\u7ec4\u4ef6\u6620\u5c04\u5230\u4e0d\u540c\u78c1\u5e26\uff0c\u4ece\u800c\u7cbe\u786e\u5b9a\u4f4d\u6545\u969c\u6a21\u5f0f\u5230\u7279\u5b9a\u5904\u7406\u9636\u6bb5", "motivation": "LLM\u5728\u770b\u4f3c\u7b80\u5355\u7684\u4efb\u52a1\u4e0a\u5b58\u5728\u6545\u969c\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u4e25\u8c28\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u5b9a\u4f4d\u8fd9\u4e9b\u6545\u969c\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u51e0\u4f55\u9690\u55bb", "method": "\u4f7f\u7528\u786e\u5b9a\u6027\u591a\u5e26\u56fe\u7075\u673a\u5f62\u5f0f\u5316LLM\u4ea4\u4e92\uff0c\u5c06\u8f93\u5165\u5b57\u7b26\u3001token\u3001\u8bcd\u6c47\u8868\u3001\u6a21\u578b\u53c2\u6570\u3001\u6fc0\u6d3b\u3001\u6982\u7387\u5206\u5e03\u548c\u8f93\u51fa\u6587\u672c\u6620\u5c04\u5230\u4e0d\u540c\u7684\u78c1\u5e26", "result": "\u80fd\u591f\u7cbe\u786e\u5b9a\u4f4d\u6545\u969c\u6a21\u5f0f\u5230\u7279\u5b9a\u5904\u7406\u9636\u6bb5\uff08\u5982tokenization\u5982\u4f55\u6a21\u7cca\u5b57\u7b26\u7ea7\u7ed3\u6784\uff09\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u601d\u7ef4\u94fe\u63d0\u793a\u6709\u6548\uff08\u901a\u8fc7\u5916\u90e8\u5316\u8ba1\u7b97\uff09\uff0c\u540c\u65f6\u63ed\u793a\u5176\u6839\u672c\u5c40\u9650\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u6545\u969c\u5206\u6790\u63d0\u4f9b\u4e86\u4e25\u8c28\u3001\u53ef\u8bc1\u4f2a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u8865\u5145\u4e86\u7ecf\u9a8c\u7f29\u653e\u5b9a\u5f8b\uff0c\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9519\u8bef\u5206\u6790\u6846\u67b6", "topic": "agent analysis"}}
{"id": "2602.15894", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15894", "abs": "https://arxiv.org/abs/2602.15894", "authors": ["Haihui Pan", "Yuzhong Hong", "Shaoke Lv", "Junwei Bao", "Hongfei Jiang", "Yang Song"], "title": "Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity", "comment": null, "summary": "Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.", "AI": {"tldr": "\u63d0\u51faQEMPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d28\u91cf\u7ea6\u675f\u7684\u71b5\u6700\u5927\u5316\u7b56\u7565\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347LLM\u8f93\u51fa\u7684\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u5bf9\u9f50\u65b9\u6cd5\u51cf\u5c11\u8f93\u51fa\u591a\u6837\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86LLM\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u540c\u65f6\u4e5f\u51cf\u5c11\u4e86\u8f93\u51fa\u591a\u6837\u6027\u3002\u73b0\u6709\u589e\u5f3a\u591a\u6837\u6027\u7684\u65b9\u6cd5\u5f80\u5f80\u4ee5\u6027\u80fd\u4e0b\u964d\u4e3a\u4ee3\u4ef7\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u8bc1\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u7406\u8bba\u8bc1\u660e\u5bf9\u9f50\u4efb\u52a1\u53ef\u5206\u89e3\u4e3a\u8d28\u91cf\u548c\u591a\u6837\u6027\u4e24\u4e2a\u5206\u5e03\u3002\u63d0\u51faQEMPO\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u8f93\u51fa\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316\u7b56\u7565\u7684\u8f93\u51fa\u71b5\u3002\u901a\u8fc7\u6dfb\u52a0\u4e0d\u540c\u7ea6\u675f\u83b7\u5f97\u4e0d\u540c\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u5728\u7ebf\u548c\u79bb\u7ebf\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1QEMPO\u5728\u4fdd\u6301\u4e0eRLHF\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f93\u51fa\u591a\u6837\u6027\u3002", "conclusion": "QEMPO\u6210\u529f\u89e3\u51b3\u4e86\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16165", "abs": "https://arxiv.org/abs/2602.16165", "authors": ["Jiangweizhi Peng", "Yuanxin Liu", "Ruida Zhou", "Charles Fleming", "Zhaoran Wang", "Alfredo Garcia", "Mingyi Hong"], "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents", "comment": null, "summary": "Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.\n  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.\n  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\\% success on ALFWorld and 83.3\\% on WebShop with Qwen2.5-7B-Instruct (+6.6\\% and +8.3\\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.", "AI": {"tldr": "HiPER\uff1a\u5206\u5c42\u89c4\u5212-\u6267\u884c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u5c42\u89c4\u5212\u5668\u63d0\u51fa\u5b50\u76ee\u6807\u3001\u4f4e\u5c42\u6267\u884c\u5668\u6267\u884c\u591a\u6b65\u52a8\u4f5c\uff0c\u7ed3\u5408\u5206\u5c42\u4f18\u52bf\u4f30\u8ba1\u6280\u672f\uff0c\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5c06LLM\u667a\u80fd\u4f53\u5efa\u6a21\u4e3a\u5355\u65f6\u95f4\u5c3a\u5ea6\u7684\u6241\u5e73\u7b56\u7565\uff0c\u5728\u7a00\u758f\u5956\u52b1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u96be\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u4fe1\u7528\u5206\u914d\uff0c\u5bfc\u81f4\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faHiPER\u5206\u5c42\u6846\u67b6\uff1a\u9ad8\u5c42\u89c4\u5212\u5668\u63d0\u51fa\u5b50\u76ee\u6807\uff0c\u4f4e\u5c42\u6267\u884c\u5668\u6267\u884c\u591a\u6b65\u52a8\u4f5c\uff1b\u5f15\u5165\u5206\u5c42\u4f18\u52bf\u4f30\u8ba1\u6280\u672f\uff0c\u5728\u89c4\u5212\u548c\u6267\u884c\u4e24\u4e2a\u5c42\u9762\u8fdb\u884c\u4fe1\u7528\u5206\u914d\uff0c\u534f\u8c03\u66f4\u65b0\u5e76\u51cf\u5c11\u65b9\u5dee\u3002", "result": "\u5728ALFWorld\u4e0a\u8fbe\u523097.4%\u6210\u529f\u7387\uff0cWebShop\u4e0a\u8fbe\u523083.3%\uff08\u5206\u522b\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u53476.6%\u548c8.3%\uff09\uff0c\u5728\u9700\u8981\u591a\u4e2a\u4f9d\u8d56\u5b50\u4efb\u52a1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u63d0\u5347\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u660e\u786e\u7684\u5206\u5c42\u5206\u89e3\u5bf9\u4e8e\u53ef\u6269\u5c55\u7684\u591a\u8f6eLLM\u667a\u80fd\u4f53RL\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\uff0cHiPER\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u4e0e\u6267\u884c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16196", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16196", "abs": "https://arxiv.org/abs/2602.16196", "authors": ["Emile Anand", "Richard Hoffmann", "Sarah Liaw", "Adam Wierman"], "title": "Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning", "comment": "43 pages, 5 figures, 1 table", "summary": "Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\\texttt{GMFS}$, a $\\textbf{G}$raphon $\\textbf{M}$ean-$\\textbf{F}$ield $\\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $\u03ba$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\\mathrm{poly}(\u03ba)$ and optimality gap $O(1/\\sqrt\u03ba)$. We verify our theory with numerical simulations in robotic coordination, showing that $\\texttt{GMFS}$ achieves near-optimal performance.", "AI": {"tldr": "\u63d0\u51faGMFS\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u4ea4\u4e92\u5f3a\u5ea6\u7684\u5b50\u91c7\u6837\u65b9\u6cd5\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8054\u5408\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u968f\u667a\u80fd\u4f53\u6570\u91cf\u6307\u6570\u589e\u957f\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5047\u8bbe\u540c\u8d28\u4ea4\u4e92\uff08\u5747\u503c\u573a\u65b9\u6cd5\uff09\uff0c\u8981\u4e48\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u6602\uff08\u57fa\u4e8e\u56fe\u8bba\u7684\u65b9\u6cd5\uff09\uff0c\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u5f02\u6784\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u56fe\u8bba\u5747\u503c\u573a\u5b50\u91c7\u6837\u6846\u67b6GMFS\uff1a\u6839\u636e\u667a\u80fd\u4f53\u95f4\u4ea4\u4e92\u5f3a\u5ea6\u5bf9\u03ba\u4e2a\u667a\u80fd\u4f53\u8fdb\u884c\u5b50\u91c7\u6837\uff0c\u8fd1\u4f3c\u56fe\u8bba\u52a0\u6743\u7684\u5747\u503c\u573a\uff0c\u5b66\u4e60\u7b56\u7565\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e3apoly(\u03ba)\uff0c\u6700\u4f18\u6027\u5dee\u8ddd\u4e3aO(1/\u221a\u03ba)\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eGMFS\u5177\u6709\u591a\u9879\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u3002\u5728\u673a\u5668\u4eba\u534f\u8c03\u4efb\u52a1\u7684\u6570\u503c\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "GMFS\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5b50\u91c7\u6837\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16154", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16154", "abs": "https://arxiv.org/abs/2602.16154", "authors": ["Nithin Sivakumaran", "Shoubin Yu", "Hyunji Lee", "Yue Zhang", "Ali Payani", "Mohit Bansal", "Elias Stengel-Eskin"], "title": "Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution", "comment": "Code: https://github.com/nsivaku/remul", "summary": "Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who \"execute\" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.", "AI": {"tldr": "REMUL\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u8bf4\u8bdd\u6a21\u578b\u751f\u6210\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u7531\u591a\u4e2a\u542c\u8bdd\u6a21\u578b\u6267\u884c\u8fd9\u4e9b\u8f68\u8ff9\u6765\u9a8c\u8bc1\u5176\u53ef\u7406\u89e3\u6027\uff0c\u4ece\u800c\u63d0\u5347\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u601d\u7ef4\u94fe\u63a8\u7406\u5b58\u5728\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u5927\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u540c\u65f6\u4f18\u5316\u5fe0\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5f80\u5f80\u4f1a\u964d\u4f4e\u4efb\u52a1\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u6743\u8861\u5173\u7cfb\u3002", "method": "\u63d0\u51faREMUL\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff1a\u8bf4\u8bdd\u6a21\u578b\u751f\u6210\u63a8\u7406\u8f68\u8ff9\uff0c\u622a\u65ad\u540e\u4f20\u9012\u7ed9\u591a\u4e2a\u542c\u8bdd\u6a21\u578b\u6267\u884c\uff0c\u542c\u8bdd\u6a21\u578b\u7ee7\u7eed\u63a8\u7406\u5f97\u5230\u7b54\u6848\u3002\u8bf4\u8bdd\u6a21\u578b\u56e0\u751f\u6210\u6e05\u6670\u6613\u61c2\u7684\u63a8\u7406\u800c\u83b7\u5f97\u5956\u52b1\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u76d1\u7763\u5fae\u8c03\u8fdb\u884c\u6b63\u786e\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREMUL\u663e\u8457\u63d0\u5347\u4e86\u4e09\u79cd\u5fe0\u5b9e\u6027\u5ea6\u91cf\uff08\u63d0\u793a\u5f52\u56e0\u3001\u65e9\u671f\u56de\u7b54AOC\u3001\u9519\u8bef\u6ce8\u5165AOC\uff09\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u5206\u6790\u8868\u660e\u8fd9\u4e9b\u589e\u76ca\u5728\u4e0d\u540c\u8bad\u7ec3\u57df\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8f6c\u5316\u4e3a\u53ef\u8bfb\u6027\u63d0\u5347\uff0c\u5e76\u4ea7\u751f\u66f4\u77ed\u66f4\u76f4\u63a5\u7684\u601d\u7ef4\u94fe\u3002", "conclusion": "REMUL\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u5fe0\u5b9e\u6027\u4e0e\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u6539\u5584\u4e86\u63a8\u7406\u8f68\u8ff9\u7684\u8d28\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16313", "abs": "https://arxiv.org/abs/2602.16313", "authors": ["Zexue He", "Yu Wang", "Churan Zhi", "Yuanzhe Hu", "Tzu-Ping Chen", "Lang Yin", "Ze Chen", "Tong Arthur Wu", "Siru Ouyang", "Zihan Wang", "Jiaxin Pei", "Julian McAuley", "Yejin Choi", "Alex Pentland"], "title": "MemoryArena: Benchmarking Agent Memory in Interdependent Multi-Session Agentic Tasks", "comment": null, "summary": "Existing evaluations of agents with memory typically assess memorization and action in isolation. One class of benchmarks evaluates memorization by testing recall of past conversations or text but fails to capture how memory is used to guide future decisions. Another class focuses on agents acting in single-session tasks without the need for long-term memory. However, in realistic settings, memorization and action are tightly coupled: agents acquire memory while interacting with the environment, and subsequently rely on that memory to solve future tasks. To capture this setting, we introduce MemoryArena, a unified evaluation gym for benchmarking agent memory in multi-session Memory-Agent-Environment loops. The benchmark consists of human-crafted agentic tasks with explicitly interdependent subtasks, where agents must learn from earlier actions and feedback by distilling experiences into memory, and subsequently use that memory to guide later actions to solve the overall task. MemoryArena supports evaluation across web navigation, preference-constrained planning, progressive information search, and sequential formal reasoning, and reveals that agents with near-saturated performance on existing long-context memory benchmarks like LoCoMo perform poorly in our agentic setting, exposing a gap in current evaluations for agents with memory.", "AI": {"tldr": "MemoryArena\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u8bb0\u5fc6\u80fd\u529b\u7684\u7edf\u4e00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u591a\u4f1a\u8bdd\u8bb0\u5fc6-\u667a\u80fd\u4f53-\u73af\u5883\u5faa\u73af\uff0c\u63ed\u793a\u73b0\u6709\u8bb0\u5fc6\u57fa\u51c6\u4e0e\u771f\u5b9e\u667a\u80fd\u4f53\u4efb\u52a1\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u4e00\u7c7b\u53ea\u6d4b\u8bd5\u8bb0\u5fc6\u56de\u5fc6\u4f46\u5ffd\u7565\u8bb0\u5fc6\u5982\u4f55\u6307\u5bfc\u672a\u6765\u51b3\u7b56\uff1b\u53e6\u4e00\u7c7b\u5173\u6ce8\u5355\u4f1a\u8bdd\u4efb\u52a1\u800c\u65e0\u9700\u957f\u671f\u8bb0\u5fc6\u3002\u771f\u5b9e\u573a\u666f\u4e2d\u8bb0\u5fc6\u4e0e\u884c\u52a8\u7d27\u5bc6\u8026\u5408\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6355\u6349\u8fd9\u79cd\u4ea4\u4e92\u3002", "method": "\u63d0\u51faMemoryArena\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u4eba\u5de5\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u5177\u6709\u660e\u786e\u76f8\u4e92\u4f9d\u8d56\u7684\u5b50\u4efb\u52a1\u3002\u667a\u80fd\u4f53\u5fc5\u987b\u4ece\u65e9\u671f\u884c\u52a8\u548c\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u5c06\u7ecf\u9a8c\u63d0\u70bc\u4e3a\u8bb0\u5fc6\uff0c\u5e76\u5229\u7528\u8be5\u8bb0\u5fc6\u6307\u5bfc\u540e\u7eed\u884c\u52a8\u4ee5\u89e3\u51b3\u6574\u4f53\u4efb\u52a1\u3002", "result": "\u5728MemoryArena\u4e0a\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5728\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u57fa\u51c6\uff08\u5982LoCoMo\uff09\u4e0a\u8868\u73b0\u63a5\u8fd1\u9971\u548c\u7684\u667a\u80fd\u4f53\uff0c\u5728\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u8bb0\u5fc6\u8bc4\u4f30\u7684\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u66f4\u5168\u9762\u7684\u8bb0\u5fc6\u8bc4\u4f30\u6846\u67b6\u6765\u6355\u6349\u771f\u5b9e\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8bb0\u5fc6\u4e0e\u884c\u52a8\u7684\u7d27\u5bc6\u8026\u5408\uff0cMemoryArena\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2602.16346", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16346", "abs": "https://arxiv.org/abs/2602.16346", "authors": ["Nivya Talokar", "Ayush K Tarun", "Murari Mandal", "Maksym Andriushchenko", "Antoine Bosselut"], "title": "Helpful to a Fault: Measuring Illicit Assistance in Multi-Turn, Multilingual LLM Agents", "comment": null, "summary": "LLM-based agents execute real-world workflows via tools and memory. These affordances enable ill-intended adversaries to also use these agents to carry out complex misuse scenarios. Existing agent misuse benchmarks largely test single-prompt instructions, leaving a gap in measuring how agents end up helping with harmful or illegal tasks over multiple turns. We introduce STING (Sequential Testing of Illicit N-step Goal execution), an automated red-teaming framework that constructs a step-by-step illicit plan grounded in a benign persona and iteratively probes a target agent with adaptive follow-ups, using judge agents to track phase completion. We further introduce an analysis framework that models multi-turn red-teaming as a time-to-first-jailbreak random variable, enabling analysis tools like discovery curves, hazard-ratio attribution by attack language, and a new metric: Restricted Mean Jailbreak Discovery. Across AgentHarm scenarios, STING yields substantially higher illicit-task completion than single-turn prompting and chat-oriented multi-turn baselines adapted to tool-using agents. In multilingual evaluations across six non-English settings, we find that attack success and illicit-task completion do not consistently increase in lower-resource languages, diverging from common chatbot findings. Overall, STING provides a practical way to evaluate and stress-test agent misuse in realistic deployment settings, where interactions are inherently multi-turn and often multilingual.", "AI": {"tldr": "STING\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7ea2\u961f\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5LLM\u4ee3\u7406\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u6267\u884c\u975e\u6cd5\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u9010\u6b65\u6784\u5efa\u57fa\u4e8e\u826f\u6027\u4eba\u8bbe\u7684\u975e\u6cd5\u8ba1\u5212\uff0c\u5e76\u4f7f\u7528\u8bc4\u5224\u4ee3\u7406\u8ddf\u8e2a\u9636\u6bb5\u5b8c\u6210\u60c5\u51b5\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u6ee5\u7528\u57fa\u51c6\u4e3b\u8981\u6d4b\u8bd5\u5355\u8f6e\u6307\u4ee4\uff0c\u7f3a\u4e4f\u8861\u91cf\u4ee3\u7406\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5982\u4f55\u5e2e\u52a9\u5b8c\u6210\u6709\u5bb3\u6216\u975e\u6cd5\u4efb\u52a1\u7684\u80fd\u529b\u3002\u9700\u8981\u8bc4\u4f30\u4ee3\u7406\u5728\u73b0\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u6ee5\u7528\u98ce\u9669\uff0c\u8fd9\u4e9b\u573a\u666f\u672c\u8d28\u4e0a\u662f\u591a\u8f6e\u4e14\u591a\u8bed\u8a00\u7684\u3002", "method": "STING\u6846\u67b6\u9010\u6b65\u6784\u5efa\u57fa\u4e8e\u826f\u6027\u4eba\u8bbe\u7684\u975e\u6cd5\u8ba1\u5212\uff0c\u8fed\u4ee3\u5f0f\u63a2\u6d4b\u76ee\u6807\u4ee3\u7406\u5e76\u81ea\u9002\u5e94\u8ddf\u8fdb\uff0c\u4f7f\u7528\u8bc4\u5224\u4ee3\u7406\u8ddf\u8e2a\u9636\u6bb5\u5b8c\u6210\u60c5\u51b5\u3002\u5206\u6790\u6846\u67b6\u5c06\u591a\u8f6e\u7ea2\u961f\u5efa\u6a21\u4e3a\u9996\u6b21\u8d8a\u72f1\u65f6\u95f4\u968f\u673a\u53d8\u91cf\uff0c\u652f\u6301\u53d1\u73b0\u66f2\u7ebf\u3001\u653b\u51fb\u8bed\u8a00\u5371\u9669\u6bd4\u5f52\u56e0\u548c\u53d7\u9650\u5e73\u5747\u8d8a\u72f1\u53d1\u73b0\u7b49\u5206\u6790\u5de5\u5177\u3002", "result": "\u5728AgentHarm\u573a\u666f\u4e2d\uff0cSTING\u6bd4\u5355\u8f6e\u63d0\u793a\u548c\u9002\u5e94\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u804a\u5929\u5bfc\u5411\u591a\u8f6e\u57fa\u7ebf\u83b7\u5f97\u663e\u8457\u66f4\u9ad8\u7684\u975e\u6cd5\u4efb\u52a1\u5b8c\u6210\u7387\u3002\u5728\u516d\u79cd\u975e\u82f1\u8bed\u73af\u5883\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u4e2d\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u975e\u6cd5\u4efb\u52a1\u5b8c\u6210\u7387\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5e76\u672a\u4e00\u81f4\u589e\u52a0\uff0c\u4e0e\u5e38\u89c1\u804a\u5929\u673a\u5668\u4eba\u53d1\u73b0\u4e0d\u540c\u3002", "conclusion": "STING\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u538b\u529b\u6d4b\u8bd5\u4ee3\u7406\u5728\u73b0\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u6ee5\u7528\uff0c\u8fd9\u4e9b\u573a\u666f\u672c\u8d28\u4e0a\u662f\u591a\u8f6e\u4e14\u591a\u8bed\u8a00\u7684\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "topic": "agent analysis"}}
{"id": "2602.16429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.16429", "abs": "https://arxiv.org/abs/2602.16429", "authors": ["Ido Levy", "Eilam Shapira", "Yinon Goldshtein", "Avi Yaeli", "Nir Mashkif", "Segev Shlomov"], "title": "TabAgent: A Framework for Replacing Agentic Generative Components with Tabular-Textual Classifiers", "comment": null, "summary": "Agentic systems, AI architectures that autonomously execute multi-step workflows to achieve complex goals, are often built using repeated large language model (LLM) calls for closed-set decision tasks such as routing, shortlisting, gating, and verification. While convenient, this design makes deployments slow and expensive due to cumulative latency and token usage. We propose TabAgent, a framework for replacing generative decision components in closed-set selection tasks with a compact textual-tabular classifier trained on execution traces. TabAgent (i) extracts structured schema, state, and dependency features from trajectories (TabSchema), (ii) augments coverage with schema-aligned synthetic supervision (TabSynth), and (iii) scores candidates with a lightweight classifier (TabHead). On the long-horizon AppWorld benchmark, TabAgent maintains task-level success while eliminating shortlist-time LLM calls, reducing latency by approximately 95% and inference cost by 85-91%. Beyond tool shortlisting, TabAgent generalizes to other agentic decision heads, establishing a paradigm for learned discriminative replacements of generative bottlenecks in production agent architectures.", "AI": {"tldr": "TabAgent\u7528\u8f7b\u91cf\u7ea7\u6587\u672c-\u8868\u683c\u5206\u7c7b\u5668\u66ff\u4ee3LLM\u8fdb\u884c\u5c01\u95ed\u96c6\u51b3\u7b56\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\uff0c\u5c06\u5ef6\u8fdf\u964d\u4f4e\u7ea695%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e85-91%", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u6267\u884c\u591a\u6b65\u5de5\u4f5c\u6d41\u65f6\uff0c\u7531\u4e8e\u91cd\u590d\u8c03\u7528LLM\u8fdb\u884c\u8def\u7531\u3001\u7b5b\u9009\u3001\u95e8\u63a7\u548c\u9a8c\u8bc1\u7b49\u51b3\u7b56\u4efb\u52a1\uff0c\u5bfc\u81f4\u90e8\u7f72\u901f\u5ea6\u6162\u3001\u6210\u672c\u9ad8", "method": "TabAgent\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aTabSchema\u4ece\u6267\u884c\u8f68\u8ff9\u63d0\u53d6\u7ed3\u6784\u5316\u7279\u5f81\uff0cTabSynth\u901a\u8fc7\u6a21\u5f0f\u5bf9\u9f50\u7684\u5408\u6210\u76d1\u7763\u589e\u5f3a\u8986\u76d6\u8303\u56f4\uff0cTabHead\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u5bf9\u5019\u9009\u8fdb\u884c\u8bc4\u5206", "result": "\u5728AppWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTabAgent\u5728\u4fdd\u6301\u4efb\u52a1\u7ea7\u6210\u529f\u7387\u7684\u540c\u65f6\uff0c\u6d88\u9664\u4e86\u77ed\u5217\u8868\u65f6\u95f4\u7684LLM\u8c03\u7528\uff0c\u5ef6\u8fdf\u964d\u4f4e\u7ea695%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e85-91%", "conclusion": "TabAgent\u4e3a\u751f\u4ea7\u4ee3\u7406\u67b6\u6784\u4e2d\u7684\u751f\u6210\u74f6\u9888\u63d0\u4f9b\u4e86\u5b66\u4e60\u5224\u522b\u5f0f\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u4ee3\u7406\u51b3\u7b56\u7ec4\u4ef6", "topic": "agent analysis"}}
{"id": "2602.16485", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.16485", "abs": "https://arxiv.org/abs/2602.16485", "authors": ["Jeffrey T. H. Wong", "Zixi Zhang", "Junyi Liu", "Yiren Zhao"], "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling", "comment": "8 pages", "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.", "AI": {"tldr": "\u63d0\u51faTeam-of-Thoughts\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u7f16\u6392\u5668-\u5de5\u5177\u8303\u5f0f\u5229\u7528\u5f02\u6784\u667a\u80fd\u4f53\u7684\u4e92\u8865\u80fd\u529b\uff0c\u5728\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u540c\u6784\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u3001\u540c\u8d28\u7684\u6a21\u578b\u914d\u7f6e\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0d\u540c\u540e\u8bad\u7ec3\u6a21\u578b\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u5f02\u6784\u667a\u80fd\u4f53\u4e92\u8865\u80fd\u529b\u7684\u65b0\u67b6\u6784\u3002", "method": "\u5f15\u5165\u7f16\u6392\u5668\u6821\u51c6\u65b9\u6848\u8bc6\u522b\u5177\u6709\u5353\u8d8a\u534f\u8c03\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u81ea\u6211\u8bc4\u4f30\u534f\u8bae\u8ba9\u5de5\u5177\u667a\u80fd\u4f53\u5206\u6790\u81ea\u8eab\u9886\u57df\u4e13\u957f\uff0c\u6839\u636e\u719f\u7ec3\u5ea6\u914d\u7f6e\u6587\u4ef6\u5728\u63a8\u7406\u65f6\u52a8\u6001\u6fc0\u6d3b\u6700\u5408\u9002\u7684\u5de5\u5177\u667a\u80fd\u4f53\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728AIME24\u548cLiveCodeBench\u4e0a\u5206\u522b\u8fbe\u523096.67%\u548c72.53%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u540c\u8d28\u89d2\u8272\u626e\u6f14\u57fa\u7ebf\uff0880%\u548c65.93%\uff09\u3002", "conclusion": "Team-of-Thoughts\u901a\u8fc7\u5229\u7528\u5f02\u6784\u667a\u80fd\u4f53\u7684\u4e92\u8865\u80fd\u529b\uff0c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5f02\u6784\u914d\u7f6e\u76f8\u5bf9\u4e8e\u4f20\u7edf\u540c\u8d28\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2602.16363", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16363", "abs": "https://arxiv.org/abs/2602.16363", "authors": ["Oran Ridel", "Alon Cohen"], "title": "Improved Bounds for Reward-Agnostic and Reward-Free Exploration", "comment": null, "summary": "We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $\u03b5$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $\u03b5$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $\u03b5$. We propose a new algorithm that significantly relaxes the requirement on $\u03b5$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $\u03b5$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u5956\u52b1\u548c\u5956\u52b1\u4e0d\u53ef\u77e5\u63a2\u7d22\uff0c\u63d0\u51fa\u4e86\u65b0\u7b97\u6cd5\u663e\u8457\u653e\u5bbd\u4e86\u03b5\u7cbe\u5ea6\u8981\u6c42\uff0c\u5e76\u5efa\u7acb\u4e86\u65e0\u5956\u52b1\u63a2\u7d22\u7684\u7d27\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u4e0d\u53ef\u77e5\u63a2\u7d22\u65b9\u6cd5\u867d\u7136\u8fbe\u5230\u4e86\u6781\u5c0f\u6781\u5927\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4f46\u4ec5\u9002\u7528\u4e8e\u9650\u5236\u6027\u5c0f\u7684\u7cbe\u5ea6\u53c2\u6570\u03b5\u3002\u9700\u8981\u8bbe\u8ba1\u65b0\u7b97\u6cd5\u6765\u663e\u8457\u653e\u5bbd\u5bf9\u03b5\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u65b0\u7b97\u6cd5\uff0c\u91c7\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u5956\u52b1\u7684\u5728\u7ebf\u5b66\u4e60\u8fc7\u7a0b\u6784\u5efa\u63a2\u7d22\u7b56\u7565\uff0c\u6536\u96c6\u8db3\u591f\u6570\u636e\u8fdb\u884c\u51c6\u786e\u52a8\u6001\u4f30\u8ba1\uff0c\u5728\u5956\u52b1\u63ed\u793a\u540e\u8ba1\u7b97\u03b5\u6700\u4f18\u7b56\u7565\u3002", "result": "\u65b0\u7b97\u6cd5\u663e\u8457\u653e\u5bbd\u4e86\u5bf9\u03b5\u7684\u8981\u6c42\uff0c\u540c\u65f6\u5efa\u7acb\u4e86\u65e0\u5956\u52b1\u63a2\u7d22\u7684\u7d27\u4e0b\u754c\uff0c\u586b\u8865\u4e86\u5df2\u77e5\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u5956\u52b1\u4e0d\u53ef\u77e5\u63a2\u7d22\u65b9\u9762\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u6280\u672f\u65b0\u9896\u6027\uff0c\u540c\u65f6\u4e3a\u65e0\u5956\u52b1\u63a2\u7d22\u5efa\u7acb\u4e86\u7406\u8bba\u754c\u9650\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16699", "abs": "https://arxiv.org/abs/2602.16699", "authors": ["Wenxuan Ding", "Nicholas Tomlin", "Greg Durrett"], "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents", "comment": null, "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.", "AI": {"tldr": "\u63d0\u51faCalibrate-Then-Act\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9LLM\u663e\u5f0f\u63a8\u7406\u6210\u672c-\u4e0d\u786e\u5b9a\u6027\u6743\u8861\uff0c\u4f18\u5316\u5176\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u7b56\u7565", "motivation": "LLM\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u9700\u8981\u4e0e\u73af\u5883\u4ea4\u4e92\u83b7\u53d6\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u8003\u8651\u4f55\u65f6\u505c\u6b62\u63a2\u7d22\u5e76\u63d0\u4ea4\u7b54\u6848\u7684\u6210\u672c-\u4e0d\u786e\u5b9a\u6027\u6743\u8861\uff0c\u5bfc\u81f4\u51b3\u7b56\u4e0d\u591f\u4f18\u5316", "method": "\u5c06\u4fe1\u606f\u68c0\u7d22\u548c\u7f16\u7a0b\u7b49\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5f15\u5165Calibrate-Then-Act\u6846\u67b6\uff0c\u4e3aLLM\u63d0\u4f9b\u5148\u9a8c\u77e5\u8bc6\u4f7f\u5176\u80fd\u663e\u5f0f\u63a8\u7406\u6210\u672c-\u6548\u76ca\u6743\u8861", "result": "CTA\u6846\u67b6\u80fd\u5e2e\u52a9LLM\u4ee3\u7406\u53d1\u73b0\u66f4\u4f18\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u5373\u4f7f\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0b\uff0cCTA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4ecd\u80fd\u4fdd\u6301\u6539\u8fdb", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6210\u672c-\u4e0d\u786e\u5b9a\u6027\u6743\u8861\uff0cCTA\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u4f18\u5316\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2602.16523", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16523", "abs": "https://arxiv.org/abs/2602.16523", "authors": ["Gerhard Stenzel", "Isabella Debelic", "Michael K\u00f6lle", "Tobias Rohe", "Leo S\u00fcnkel", "Julian Hager", "Claudia Linnhoff-Popien"], "title": "Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study", "comment": "Extended version of a short paper to be published at ICAART 2026", "summary": "We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \\(R_x\\), \\(R_y\\), and \\(R_z\\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \\(\u03bb\\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \\(5\\times10^{-4}\\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \\(10^{-4}\\)). Both approaches reliably reconstruct computational basis states (between 83\\% and 99\\% success) and Bell states (between 61\\% and 77\\% success). However, scalability saturates for \\(\u03bb\\) of approximately three to four and does not extend to ten-qubit targets even at \\(\u03bb=2\\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.", "AI": {"tldr": "\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5408\u6210\uff0c\u6bd4\u8f83\u5355\u9636\u6bb5\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0cPPO\u5728\u7a33\u5b9a\u8d85\u53c2\u6570\u4e0b\u80fd\u6210\u529f\u5408\u6210\u91cf\u5b50\u6001\uff0c\u4f46\u53ef\u6269\u5c55\u6027\u5728\u03bb=3-4\u65f6\u9971\u548c\u3002", "motivation": "\u6269\u5c55\u6709\u5411\u91cf\u5b50\u7535\u8def\u5408\u6210\u65b9\u6cd5\uff0c\u4ece\u7eaf\u79bb\u6563\u95e8\u9009\u62e9\u5230\u5305\u542b\u8fde\u7eed\u5355\u91cf\u5b50\u6bd4\u7279\u65cb\u8f6c\u7684\u53c2\u6570\u5316\u91cf\u5b50\u6001\u5236\u5907\uff0c\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u91cf\u5b50\u7535\u8def\u5408\u6210\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Gymnasium\u548cPennyLane\u6846\u67b6\uff0c\u6bd4\u8f83\u4e24\u79cd\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u5355\u9636\u6bb5\u4ee3\u7406\u8054\u5408\u9009\u62e9\u95e8\u7c7b\u578b\u3001\u4f5c\u7528\u91cf\u5b50\u6bd4\u7279\u548c\u65cb\u8f6c\u89d2\u5ea6\uff1b2\uff09\u4e24\u9636\u6bb5\u65b9\u6cd5\u5148\u63d0\u51fa\u79bb\u6563\u7535\u8def\uff0c\u518d\u7528Adam\u4f18\u5316\u65cb\u8f6c\u89d2\u5ea6\u3002\u8bc4\u4f30PPO\u548cA2C\u7b97\u6cd5\u57282-10\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e0a\u7684\u8868\u73b0\u3002", "result": "A2C\u672a\u80fd\u5b66\u4e60\u6709\u6548\u7b56\u7565\uff0cPPO\u5728\u7a33\u5b9a\u8d85\u53c2\u6570\u4e0b\u6210\u529f\uff08\u5355\u9636\u6bb5\u5b66\u4e60\u7387\u7ea65\u00d710\u207b\u2074\uff0c\u4e24\u9636\u6bb5\u7ea610\u207b\u2074\uff09\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u53ef\u9760\u91cd\u6784\u8ba1\u7b97\u57fa\u6001\uff0883%-99%\u6210\u529f\u7387\uff09\u548c\u8d1d\u5c14\u6001\uff0861%-77%\u6210\u529f\u7387\uff09\uff0c\u4f46\u53ef\u6269\u5c55\u6027\u5728\u03bb\u22483-4\u65f6\u9971\u548c\uff0c\u65e0\u6cd5\u6269\u5c55\u523010\u91cf\u5b50\u6bd4\u7279\u76ee\u6807\u3002\u4e24\u9636\u6bb5\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u8fb9\u9645\u7cbe\u5ea6\u63d0\u5347\u4f46\u9700\u8981\u7ea63\u500d\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u63a8\u8350\u4f7f\u7528\u5355\u9636\u6bb5PPO\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u5408\u6210\u7535\u8def\uff0c\u5e76\u4e0e\u7ecf\u5178\u53d8\u5206\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u6307\u51fa\u4e86\u6539\u8fdb\u53ef\u6269\u5c55\u6027\u7684\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16543", "abs": "https://arxiv.org/abs/2602.16543", "authors": ["Jialiang Fan", "Shixiong Jiang", "Mengyu Liu", "Fanxin Kong"], "title": "Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning", "comment": "12 pages, 6 figures, supplementary material included", "summary": "Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u548c\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\u7ea6\u675f\u6a21\u578b\u548c\u4ee3\u7406\u7b56\u7565\uff0c\u65e0\u9700\u53d7\u5bb3\u8005\u7b56\u7565\u7684\u5185\u90e8\u68af\u5ea6\u6216\u771f\u5b9e\u5b89\u5168\u7ea6\u675f\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u73af\u5883\u53cb\u597d\uff0c\u5bb9\u6613\u53d7\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u5bf9\u6297\u6270\u52a8\u653b\u51fb\u3002\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u653b\u51fb\u901a\u5e38\u9700\u8981\u8bbf\u95ee\u7b56\u7565\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u6f14\u793a\u548c\u9ed1\u76d2\u73af\u5883\u4ea4\u4e92\u5b66\u4e60\u7ea6\u675f\u6a21\u578b\u548c\u4ee3\u7406\uff08\u5b66\u4e60\u8005\uff09\u7b56\u7565\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u653b\u51fb\u4f18\u5316\uff0c\u800c\u4e0d\u9700\u8981\u53d7\u5bb3\u8005\u7b56\u7565\u7684\u5185\u90e8\u68af\u5ea6\u6216\u771f\u5b9e\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5728\u591a\u4e2a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u7279\u6743\u8bbf\u95ee\u4e0b\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u63ed\u793a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b89\u5168\u8bc4\u4f30\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16564", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.16564", "abs": "https://arxiv.org/abs/2602.16564", "authors": ["Michael Lanier", "Yevgeniy Vorobeychik"], "title": "A Scalable Approach to Solving Simulation-Based Network Security Games", "comment": null, "summary": "We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.", "AI": {"tldr": "MetaDOAR\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5143\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5206\u533a\u611f\u77e5\u8fc7\u6ee4\u5c42\u548cQ\u503c\u7f13\u5b58\u589e\u5f3aDouble Oracle/PSRO\u8303\u5f0f\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u5728\u975e\u5e38\u5927\u7684\u7f51\u7edc\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65f6\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "method": "1) \u5b66\u4e60\u7d27\u51d1\u72b6\u6001\u6295\u5f71\u4ece\u8282\u70b9\u7ed3\u6784\u5d4c\u5165\uff1b2) \u5feb\u901f\u8bc4\u5206\u5e76\u9009\u62e9\u8bbe\u5907\u5b50\u96c6\uff08top-k\u5206\u533a\uff09\uff1b3) \u4f4e\u5c42\u6267\u884c\u5668\u5728\u9009\u5b9a\u5206\u533a\u4e0a\u8fdb\u884c\u805a\u7126\u6ce2\u675f\u641c\u7d22\uff1b4) \u4f7f\u7528\u6279\u5904\u7406critic\u8bc4\u4f30\u5019\u9009\u52a8\u4f5c\uff1b5) \u5efa\u7acbLRU\u7f13\u5b58\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff1b6) \u901a\u8fc7\u4fdd\u5b88\u7684k\u8df3\u7f13\u5b58\u5931\u6548\u4fdd\u6301\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5728\u5927\u578b\u7f51\u7edc\u62d3\u6251\u4e0a\u83b7\u5f97\u6bd4SOTA\u57fa\u7ebf\u66f4\u9ad8\u7684\u73a9\u5bb6\u6536\u76ca\uff0c\u6ca1\u6709\u663e\u8457\u7684\u5185\u5b58\u4f7f\u7528\u6216\u8bad\u7ec3\u65f6\u95f4\u6269\u5c55\u95ee\u9898\u3002", "conclusion": "\u4e3a\u5927\u89c4\u6a21\u7f51\u7edc\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7406\u8bba\u4e0a\u6709\u52a8\u673a\u7684\u9ad8\u6548\u5206\u5c42\u7b56\u7565\u5b66\u4e60\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.16629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16629", "abs": "https://arxiv.org/abs/2602.16629", "authors": ["Ethan Blaser", "Jiuqi Wang", "Shangtong Zhang"], "title": "Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes", "comment": null, "summary": "The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u6807\u51c6\u9012\u51cf\u5b66\u4e60\u7387\u4e0b\uff0cn\u6b65\u5dee\u5206TD\u7b97\u6cd5\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\uff0c\u65e0\u9700\u5c40\u90e8\u65f6\u949f\uff0c\u4e3a\u5e73\u5747\u5956\u52b1RL\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5dee\u5206TD\u5b66\u4e60\u7b97\u6cd5\u662f\u5e73\u5747\u5956\u52b1RL\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u6536\u655b\u4fdd\u8bc1\u9700\u8981\u57fa\u4e8e\u72b6\u6001\u8bbf\u95ee\u8ba1\u6570\u7684\u5c40\u90e8\u65f6\u949f\u5b66\u4e60\u7387\uff0c\u8fd9\u4e0e\u5b9e\u9645\u5e94\u7528\u4e0d\u7b26\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u8868\u683c\u8bbe\u7f6e\u4e4b\u5916\u3002", "method": "\u8bc1\u660e\u4e86\u5728\u6807\u51c6\u9012\u51cf\u5b66\u4e60\u7387\u4e0b\uff0con-policy n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\uff1b\u7136\u540e\u63a8\u5bfc\u4e86off-policy n\u6b65\u5dee\u5206TD\u6536\u655b\u7684\u4e09\u4e2a\u5145\u5206\u6761\u4ef6\uff0c\u5747\u65e0\u9700\u5c40\u90e8\u65f6\u949f\u3002", "result": "\u5efa\u7acb\u4e86n\u6b65\u5dee\u5206TD\u7b97\u6cd5\u5728\u6807\u51c6\u5b66\u4e60\u7387\u4e0b\u7684\u6536\u655b\u7406\u8bba\uff0c\u4e3a\u5e73\u5747\u5956\u52b1RL\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u5b9e\u9645\u5b9e\u73b0\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u672c\u6587\u5f3a\u5316\u4e86\u5dee\u5206TD\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u5176\u6536\u655b\u5206\u6790\u4e0e\u5b9e\u9645\u5b9e\u73b0\u66f4\u52a0\u4e00\u81f4\uff0c\u63a8\u52a8\u4e86\u5e73\u5747\u5956\u52b1RL\u7b97\u6cd5\u7684\u5b9e\u7528\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.aff076e1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fusing-ai-to-shift-e2e-test-maintenance-left%2F%3Futm_source=tldrdev/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/8uQH3VJgf-FmiNP4WcXAqtLTbRFIkUek88INov654EQ=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fusing-ai-to-shift-e2e-test-maintenance-left%2F%3Futm_source=tldrdev/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/8uQH3VJgf-FmiNP4WcXAqtLTbRFIkUek88INov654EQ=444", "authors": ["TLDR Newsletter"], "title": "Using AI to Shift E2E Test Maintenance Left", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fusing-ai-to-shift-e2e-test-maintenance-left%2F%3Futm_source=tldrdev/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/8uQH3VJgf-FmiNP4WcXAqtLTbRFIkUek88INov654EQ=444", "summary": "Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.", "source": "tldr", "AI": {"tldr": "Monday.com\u5f00\u53d1\u4e86AI\u7cfb\u7edf\uff0c\u5728\u4ee3\u7801\u5ba1\u67e5\u671f\u95f4\u5206\u6790\u7aef\u5230\u7aef\u6d4b\u8bd5\u5931\u8d25\uff0c\u63d0\u4f9b\u4e0ePR\u53d8\u66f4\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6d4b\u8bd5\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u8bc6\u522b\u6d4b\u8bd5\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u5c06\u6d4b\u8bd5\u7ef4\u62a4\u5de6\u79fb", "method": "\u5f00\u53d1AI\u7cfb\u7edf\u5206\u6790E2E\u6d4b\u8bd5\u5931\u8d25\uff0c\u5c06\u5931\u8d25\u4e0ePR\u4e2d\u7684\u5177\u4f53\u4ee3\u7801\u53d8\u66f4\u5173\u8054\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u89e3\u91ca", "result": "\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u6d4b\u8bd5\u5931\u8d25\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca\uff0c\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u66f4\u5feb\u7406\u89e3\u5931\u8d25\u539f\u56e0\uff0c\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u6548\u7387", "conclusion": "AI\u8f85\u52a9\u7684\u6d4b\u8bd5\u5206\u6790\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5c06E2E\u6d4b\u8bd5\u7ef4\u62a4\u5de6\u79fb\uff0c\u6539\u5584\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b", "topic": "swe application"}}
{"id": "tldr.2602.2892d031", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/aQjC_piX8fGLgxJVz_thLVCAqnfNEGIgLKXSe56oDbA=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/aQjC_piX8fGLgxJVz_thLVCAqnfNEGIgLKXSe56oDbA=444", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/aQjC_piX8fGLgxJVz_thLVCAqnfNEGIgLKXSe56oDbA=444", "summary": "Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.", "source": "tldr", "AI": {"tldr": "Monday.com\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u5728\u4ee3\u7801\u5ba1\u67e5\u671f\u95f4\u5206\u6790\u7aef\u5230\u7aef\u6d4b\u8bd5\u5931\u8d25\uff0c\u63d0\u4f9b\u4e0e\u62c9\u53d6\u8bf7\u6c42\u53d8\u66f4\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca", "motivation": "\u7aef\u5230\u7aef\u6d4b\u8bd5\u7ef4\u62a4\u6210\u672c\u9ad8\uff0c\u5931\u8d25\u539f\u56e0\u96be\u4ee5\u8bca\u65ad\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u9700\u8981\u5feb\u901f\u7406\u89e3\u6d4b\u8bd5\u5931\u8d25\u4e0e\u4ee3\u7801\u53d8\u66f4\u7684\u5173\u7cfb", "method": "\u5f00\u53d1AI\u7cfb\u7edf\u5206\u6790E2E\u6d4b\u8bd5\u5931\u8d25\uff0c\u5c06\u5176\u4e0e\u62c9\u53d6\u8bf7\u6c42\u4e2d\u7684\u4ee3\u7801\u53d8\u66f4\u5173\u8054\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u89e3\u91ca\uff0c\u5b9e\u73b0\"\u5de6\u79fb\"\u6d4b\u8bd5\u7ef4\u62a4", "result": "\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u6d4b\u8bd5\u5931\u8d25\u4e0e\u4ee3\u7801\u53d8\u66f4\u7684\u5173\u8054\uff0c\u4e3a\u5de5\u7a0b\u5e08\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\uff0c\u52a0\u901f\u4ee3\u7801\u5ba1\u67e5\u548c\u95ee\u9898\u8bca\u65ad\u8fc7\u7a0b", "conclusion": "AI\u8f85\u52a9\u7684\u6d4b\u8bd5\u5206\u6790\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5de6\u79fb\u6d4b\u8bd5\u7ef4\u62a4\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf", "topic": "swe application"}}
{"id": "tldr.2602.12a821cb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/B106hGr-_Nr82Ebm-Zq5j29AwWBfMvhBxvLvcMhVKi4=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/B106hGr-_Nr82Ebm-Zq5j29AwWBfMvhBxvLvcMhVKi4=444", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/B106hGr-_Nr82Ebm-Zq5j29AwWBfMvhBxvLvcMhVKi4=444", "summary": "Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.", "source": "tldr", "AI": {"tldr": "Monday.com\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u5728\u4ee3\u7801\u5ba1\u67e5\u671f\u95f4\u5206\u6790\u7aef\u5230\u7aef\u6d4b\u8bd5\u5931\u8d25\uff0c\u63d0\u4f9b\u4e0e\u62c9\u53d6\u8bf7\u6c42\u53d8\u66f4\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca", "motivation": "\u7aef\u5230\u7aef\u6d4b\u8bd5\u7ef4\u62a4\u6210\u672c\u9ad8\uff0c\u5931\u8d25\u539f\u56e0\u96be\u4ee5\u8bca\u65ad\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u3002\u9700\u8981\u5c06\u6d4b\u8bd5\u7ef4\u62a4\u5de6\u79fb\uff0c\u5728\u65e9\u671f\u53d1\u73b0\u95ee\u9898", "method": "\u5f00\u53d1AI\u7cfb\u7edf\u5206\u6790E2E\u6d4b\u8bd5\u5931\u8d25\uff0c\u5c06\u5931\u8d25\u4e0e\u7279\u5b9a\u62c9\u53d6\u8bf7\u6c42\u53d8\u66f4\u5173\u8054\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u89e3\u91ca\u548c\u4fee\u590d\u5efa\u8bae", "result": "\u7cfb\u7edf\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u6d4b\u8bd5\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u51cf\u5c11\u8c03\u8bd5\u65f6\u95f4\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u7ef4\u62a4\u5de6\u79fb", "conclusion": "AI\u8f85\u52a9\u7684\u6d4b\u8bd5\u5206\u6790\u7cfb\u7edf\u80fd\u6709\u6548\u5c06E2E\u6d4b\u8bd5\u7ef4\u62a4\u5de6\u79fb\uff0c\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u53d1\u73b0\u95ee\u9898\uff0c\u964d\u4f4e\u7ef4\u62a4\u6210\u672c", "topic": "swe application"}}
{"id": "tldr.2602.92482b22", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/-0AD_hCuV-Jt12C0LWXgHDQKMo82qiCGSk53xNA_IVg=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/-0AD_hCuV-Jt12C0LWXgHDQKMo82qiCGSk53xNA_IVg=444", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c6b814d36-dd5167a9-bd76-457d-ba45-3c9471989cd9-000000/-0AD_hCuV-Jt12C0LWXgHDQKMo82qiCGSk53xNA_IVg=444", "summary": "Using AI to Shift E2E Test Maintenance Left (11 minute read) Monday.com engineers developed an AI system that analyzes E2E test failures during code review to provide contextual explanations tied to pull request changes.", "source": "tldr", "AI": {"tldr": "Monday.com\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u5728\u4ee3\u7801\u5ba1\u67e5\u671f\u95f4\u5206\u6790\u7aef\u5230\u7aef\u6d4b\u8bd5\u5931\u8d25\uff0c\u63d0\u4f9b\u4e0e\u62c9\u53d6\u8bf7\u6c42\u53d8\u66f4\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6d4b\u8bd5\u7ef4\u62a4\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u8bc6\u522b\u6d4b\u8bd5\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u5feb\u7406\u89e3\u548c\u4fee\u590d\u6d4b\u8bd5\u95ee\u9898", "method": "\u5f00\u53d1AI\u7cfb\u7edf\u5206\u6790E2E\u6d4b\u8bd5\u5931\u8d25\uff0c\u5c06\u5931\u8d25\u4e0e\u62c9\u53d6\u8bf7\u6c42\u4e2d\u7684\u5177\u4f53\u4ee3\u7801\u53d8\u66f4\u5173\u8054\uff0c\u63d0\u4f9b\u4e0a\u4e0b\u6587\u89e3\u91ca", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u63d0\u4f9b\u6d4b\u8bd5\u5931\u8d25\u7684\u4e0a\u4e0b\u6587\u89e3\u91ca\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u5feb\u5b9a\u4f4d\u95ee\u9898\uff0c\u5c06\u6d4b\u8bd5\u7ef4\u62a4\"\u5de6\u79fb\"\u5230\u5f00\u53d1\u65e9\u671f\u9636\u6bb5", "conclusion": "AI\u8f85\u52a9\u7684\u6d4b\u8bd5\u5206\u6790\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u9ad8E2E\u6d4b\u8bd5\u7ef4\u62a4\u6548\u7387\uff0c\u51cf\u5c11\u5f00\u53d1\u8005\u5728\u6d4b\u8bd5\u6545\u969c\u6392\u9664\u4e0a\u7684\u65f6\u95f4", "topic": "swe application"}}
{"id": "tldr.2602.51b1e2c5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2bWEsf/1/0100019c6bb62563-c2a60737-5854-4b88-9da2-c593b2e82daf-000000/i4-TFnq_n9z0xZvhgQF-m0Iv2lvP1Y_1MkLFeHhasu8=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2bWEsf/1/0100019c6bb62563-c2a60737-5854-4b88-9da2-c593b2e82daf-000000/i4-TFnq_n9z0xZvhgQF-m0Iv2lvP1Y_1MkLFeHhasu8=444", "authors": ["TLDR Newsletter"], "title": "It's Time to Give the Robots Money", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2bWEsf/1/0100019c6bb62563-c2a60737-5854-4b88-9da2-c593b2e82daf-000000/i4-TFnq_n9z0xZvhgQF-m0Iv2lvP1Y_1MkLFeHhasu8=444", "summary": "It's Time to Give the Robots Money (6 minute read) Agentic payments are emerging as an investable vertical, with traditional payment rails blocked by KYC/AML barriers forcing AI agents toward alternative infrastructure. The market is bifurcating between crypto-native solutions (Coinbase x402) and corporate approaches (Google AP2, Mastercard Agent Pay, Shopify UCP, Stripe, and Visa). There are around 100 AI-first startups exceeding $100M ARR with stablecoins on L2s like Base serving as settlem...", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u652f\u4ed8\u6210\u4e3a\u6295\u8d44\u65b0\u8d5b\u9053\uff0c\u4f20\u7edf\u652f\u4ed8\u53d7KYC/AML\u9650\u5236\uff0c\u5e02\u573a\u5206\u5316\u4e3a\u52a0\u5bc6\u539f\u751f\u65b9\u6848\u548c\u4f01\u4e1a\u7ea7\u65b9\u6848\uff0c\u7a33\u5b9a\u5e01\u5728Layer 2\u4e0a\u4f5c\u4e3a\u7ed3\u7b97\u5c42", "motivation": "AI\u4ee3\u7406\u9700\u8981\u81ea\u4e3b\u652f\u4ed8\u80fd\u529b\uff0c\u4f46\u4f20\u7edf\u652f\u4ed8\u7cfb\u7edf\u53d7KYC/AML\u5408\u89c4\u9650\u5236\uff0c\u963b\u788d\u4e86AI\u4ee3\u7406\u7684\u5546\u4e1a\u5e94\u7528\uff0c\u9700\u8981\u65b0\u7684\u652f\u4ed8\u57fa\u7840\u8bbe\u65bd", "method": "\u5206\u6790\u5f53\u524dAI\u4ee3\u7406\u652f\u4ed8\u5e02\u573a\u683c\u5c40\uff0c\u5bf9\u6bd4\u52a0\u5bc6\u539f\u751f\u65b9\u6848\uff08\u5982Coinbase x402\uff09\u548c\u4f01\u4e1a\u7ea7\u65b9\u6848\uff08Google AP2\u3001Mastercard Agent Pay\u7b49\uff09\uff0c\u63a2\u8ba8\u7a33\u5b9a\u5e01\u5728Layer 2\u4f5c\u4e3a\u7ed3\u7b97\u5c42\u7684\u53ef\u884c\u6027", "result": "\u5e02\u573a\u5448\u73b0\u53cc\u8f68\u53d1\u5c55\uff1a\u52a0\u5bc6\u539f\u751f\u65b9\u6848\u548c\u4f01\u4e1a\u7ea7\u65b9\u6848\u5e76\u884c\uff0c\u7ea6100\u5bb6AI\u521d\u521b\u516c\u53f8ARR\u8d851\u4ebf\u7f8e\u5143\uff0c\u7a33\u5b9a\u5e01\u5728Base\u7b49Layer 2\u7f51\u7edc\u4e0a\u4f5c\u4e3a\u7ed3\u7b97\u5c42", "conclusion": "AI\u4ee3\u7406\u652f\u4ed8\u662f\u91cd\u8981\u6295\u8d44\u65b9\u5411\uff0c\u9700\u8981\u521b\u65b0\u7684\u652f\u4ed8\u57fa\u7840\u8bbe\u65bd\u6765\u7a81\u7834\u4f20\u7edf\u9650\u5236\uff0c\u7a33\u5b9a\u5e01\u548cLayer 2\u6280\u672f\u53ef\u80fd\u6210\u4e3a\u5173\u952e\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "tldr.2602.b3cba9bc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-agents-telegram%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7ExYaGNj196PIByqaGALe1pJl7IUyIlqYAxmUapwxAY=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-agents-telegram%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7ExYaGNj196PIByqaGALe1pJl7IUyIlqYAxmUapwxAY=444", "authors": ["TLDR Newsletter"], "title": "Introducing Manus in Your Chat: Your Personal Agent, Everywhere You Are", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmanus.im%2Fblog%2Fmanus-agents-telegram%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7ExYaGNj196PIByqaGALe1pJl7IUyIlqYAxmUapwxAY=444", "summary": "Introducing Manus in Your Chat: Your Personal Agent, Everywhere You Are (3 minute read) Manus Agents is a new way to access and use Manus directly inside messaging apps. Telegram is currently the only supported app, with more platforms coming soon. The agent features few reasoning, tools, and multi-step task execution. The feature makes agents accessible wherever users are.", "source": "tldr", "AI": {"tldr": "Manus Agents \u662f\u4e00\u4e2a\u53ef\u4ee5\u5728 Telegram \u7b49\u804a\u5929\u5e94\u7528\u4e2d\u76f4\u63a5\u8bbf\u95ee\u548c\u4f7f\u7528 Manus \u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u5c11\u91cf\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u8ba9\u7528\u6237\u80fd\u591f\u5728\u4ed6\u4eec\u65e5\u5e38\u4f7f\u7528\u7684\u804a\u5929\u5e94\u7528\u4e2d\u76f4\u63a5\u8bbf\u95ee\u548c\u4f7f\u7528 Manus \u4ee3\u7406\u529f\u80fd\uff0c\u63d0\u9ad8\u4ee3\u7406\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u4fbf\u5229\u6027\u3002", "method": "\u901a\u8fc7\u5c06 Manus \u4ee3\u7406\u96c6\u6210\u5230 Telegram \u7b49\u5373\u65f6\u901a\u8baf\u5e94\u7528\u4e2d\uff0c\u63d0\u4f9b\u5c11\u91cf\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "result": "\u76ee\u524d\u5df2\u5728 Telegram \u4e0a\u5b9e\u73b0\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5230\u66f4\u591a\u5e73\u53f0\uff0c\u8ba9\u7528\u6237\u53ef\u4ee5\u5728\u4efb\u4f55\u5730\u65b9\u4f7f\u7528\u4ee3\u7406\u529f\u80fd\u3002", "conclusion": "Manus Agents \u901a\u8fc7\u5728\u804a\u5929\u5e94\u7528\u4e2d\u96c6\u6210\u4ee3\u7406\u529f\u80fd\uff0c\u4f7f\u4ee3\u7406\u670d\u52a1\u66f4\u52a0\u666e\u53ca\u548c\u4fbf\u6377\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.f16698e3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fthe-pulse-of-agentic-ai-in-2026%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-pulse-agentic-ai-2026%26utm_content=none%26utm_term=021726/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7awpUZMUa8c1yuh4M5DyheOG0D1pKypzMiTJg8FqAMc=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fthe-pulse-of-agentic-ai-in-2026%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-pulse-agentic-ai-2026%26utm_content=none%26utm_term=021726/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7awpUZMUa8c1yuh4M5DyheOG0D1pKypzMiTJg8FqAMc=444", "authors": ["TLDR Newsletter"], "title": "What bottleneck? 50% of agentic AI projects are in production", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fthe-pulse-of-agentic-ai-in-2026%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-pulse-agentic-ai-2026%26utm_content=none%26utm_term=021726/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/7awpUZMUa8c1yuh4M5DyheOG0D1pKypzMiTJg8FqAMc=444", "summary": "What bottleneck? 50% of agentic AI projects are in production (Sponsor) Autonomous operations are rapidly expanding, and 74% of enterprises expect AI budgets to rise further in 2026. Dynatrace surveyed 900+ global decision-makers about how they're operationalizing agentic AI, with observability as the foundation for trust and control. See where the market is headed with this in-depth research report", "source": "tldr", "AI": {"tldr": "Dynatrace\u8c03\u67e5\u62a5\u544a\u663e\u793a\uff0c50%\u7684\u667a\u80fd\u4f53AI\u9879\u76ee\u5df2\u6295\u5165\u751f\u4ea7\uff0c74%\u4f01\u4e1a\u9884\u8ba12026\u5e74AI\u9884\u7b97\u589e\u52a0\uff0c\u53ef\u89c2\u6d4b\u6027\u662f\u4fe1\u4efb\u548c\u63a7\u5236\u7684\u57fa\u7840", "motivation": "\u4e86\u89e3\u4f01\u4e1a\u5982\u4f55\u5c06\u667a\u80fd\u4f53AI\u6295\u5165\u8fd0\u8425\uff0c\u63a2\u7d22\u53ef\u89c2\u6d4b\u6027\u5728\u5efa\u7acb\u4fe1\u4efb\u548c\u63a7\u5236\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u628a\u63e1\u5e02\u573a\u53d1\u5c55\u8d8b\u52bf", "method": "Dynatrace\u5bf9900\u591a\u540d\u5168\u7403\u51b3\u7b56\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u7814\u7a76\u4ed6\u4eec\u5982\u4f55\u5c06\u667a\u80fd\u4f53AI\u6295\u5165\u8fd0\u8425", "result": "50%\u7684\u667a\u80fd\u4f53AI\u9879\u76ee\u5df2\u6295\u5165\u751f\u4ea7\uff0c74%\u4f01\u4e1a\u9884\u8ba12026\u5e74AI\u9884\u7b97\u8fdb\u4e00\u6b65\u589e\u52a0\uff0c\u53ef\u89c2\u6d4b\u6027\u88ab\u786e\u8ba4\u4e3a\u4fe1\u4efb\u548c\u63a7\u5236\u7684\u57fa\u7840", "conclusion": "\u667a\u80fd\u4f53AI\u8fd0\u8425\u6b63\u5728\u5feb\u901f\u6269\u5c55\uff0c\u53ef\u89c2\u6d4b\u6027\u662f\u786e\u4fdd\u4fe1\u4efb\u548c\u63a7\u5236\u7684\u5173\u952e\u57fa\u7840\uff0c\u5e02\u573a\u5bf9AI\u6295\u8d44\u6301\u7eed\u589e\u957f", "topic": "agent analysis"}}
{"id": "tldr.2602.412f4a50", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023384075537432662.html%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/Ht8WMAi1AAPU_hh-P_2zRZZtOYVqQ_QRxhxJFKfrKCA=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023384075537432662.html%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/Ht8WMAi1AAPU_hh-P_2zRZZtOYVqQ_QRxhxJFKfrKCA=444", "authors": ["TLDR Newsletter"], "title": "How much are AI reasoning gains confounded by expanding the training corpus 10,000x?", "comment": "Source: TLDR Newsletter, Date: 2026-02-17, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2023384075537432662.html%3Futm_source=tldrai/1/0100019c6bf79f34-682992a4-17ee-47eb-a1f0-a703053ed4f9-000000/Ht8WMAi1AAPU_hh-P_2zRZZtOYVqQ_QRxhxJFKfrKCA=444", "summary": "How much are AI reasoning gains confounded by expanding the training corpus 10,000x? (5 minute read) Benchmark performance gives biased estimates of out-of-distribution generalization if LLM training data is polluted with benchmark test data. Typical decontamination filters fail to detect semantic duplicates. This suggests that recent benchmark gains are confounded - the prevalence of soft contamination means gains reflect both genuine compatibility improvements and the accumulation of test d...", "source": "tldr", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u88ab\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u6240\u6df7\u6dc6\uff0c\u56e0\u4e3a\u5178\u578b\u7684\u53bb\u6c61\u67d3\u8fc7\u6ee4\u65e0\u6cd5\u68c0\u6d4b\u8bed\u4e49\u91cd\u590d\uff0c\u5bfc\u81f4\u57fa\u51c6\u589e\u76ca\u65e2\u53cd\u6620\u771f\u5b9e\u80fd\u529b\u63d0\u5347\u4e5f\u53cd\u6620\u6d4b\u8bd5\u6570\u636e\u79ef\u7d2f", "motivation": "\u5f53\u524dLLM\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u88ab\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u6240\u6df7\u6dc6\uff0c\u7279\u522b\u662f\u5f53\u8bad\u7ec3\u8bed\u6599\u5e93\u6269\u592710000\u500d\u65f6\uff0c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u5728\u5206\u5e03\u5916\u6cdb\u5316\u7684\u771f\u5b9e\u80fd\u529b", "method": "\u5206\u6790\u5178\u578b\u53bb\u6c61\u67d3\u8fc7\u6ee4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8bed\u4e49\u91cd\u590d\u68c0\u6d4b\u95ee\u9898\uff0c\u63a2\u8ba8\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\u673a\u5236", "result": "\u53d1\u73b0\u5178\u578b\u53bb\u6c61\u67d3\u8fc7\u6ee4\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u8bed\u4e49\u91cd\u590d\uff0c\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u589e\u76ca\u88ab\u6df7\u6dc6\uff0c\u8fd1\u671f\u57fa\u51c6\u6027\u80fd\u63d0\u5347\u65e2\u5305\u542b\u771f\u5b9e\u517c\u5bb9\u6027\u6539\u8fdb\u4e5f\u5305\u542b\u6d4b\u8bd5\u6570\u636e\u79ef\u7d2f", "conclusion": "\u9700\u8981\u66f4\u4e25\u683c\u7684\u53bb\u6c61\u67d3\u65b9\u6cd5\u548c\u66f4\u8c28\u614e\u7684\u57fa\u51c6\u6d4b\u8bd5\u89e3\u91ca\uff0c\u56e0\u4e3a\u8f6f\u6c61\u67d3\u666e\u904d\u5b58\u5728\uff0c\u57fa\u51c6\u589e\u76ca\u53ef\u80fd\u9ad8\u4f30\u4e86\u6a21\u578b\u7684\u771f\u5b9e\u6cdb\u5316\u80fd\u529b", "topic": "agent analysis"}}
{"id": "tldr.2602.41d0aff8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldr%26utm_medium=tldrnewsletter%26utm_campaign=ql20260217%26utm_content=std/1/0100019c707dc1b9-501e67e2-1e6a-4c40-89b1-827460e10c4e-000000/I6V9kKdcGsSL0k5TyfMtad4zN_k0NB6tzDJWJI87rdQ=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldr%26utm_medium=tldrnewsletter%26utm_campaign=ql20260217%26utm_content=std/1/0100019c707dc1b9-501e67e2-1e6a-4c40-89b1-827460e10c4e-000000/I6V9kKdcGsSL0k5TyfMtad4zN_k0NB6tzDJWJI87rdQ=445", "authors": ["TLDR Newsletter"], "title": "Cut your dev loop from hours to seconds", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2F%3Futm_source=tldr%26utm_medium=tldrnewsletter%26utm_campaign=ql20260217%26utm_content=std/1/0100019c707dc1b9-501e67e2-1e6a-4c40-89b1-827460e10c4e-000000/I6V9kKdcGsSL0k5TyfMtad4zN_k0NB6tzDJWJI87rdQ=445", "summary": "Cut your dev loop from hours to seconds (Sponsor) mirrord lets you run local code against real cloud services. Get instant feedback, reduce cloud costs, and ship with confidence. monday.com cut dev cycle time by 70%. See how it works", "source": "tldr", "AI": {"tldr": "mirrord\u5de5\u5177\u8ba9\u5f00\u53d1\u8005\u80fd\u5728\u672c\u5730\u8fd0\u884c\u4ee3\u7801\u5e76\u8fde\u63a5\u771f\u5b9e\u4e91\u670d\u52a1\uff0c\u5c06\u5f00\u53d1\u5faa\u73af\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u79d2\uff0c\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u3001\u964d\u4f4e\u4e91\u6210\u672c\u5e76\u589e\u5f3a\u53d1\u5e03\u4fe1\u5fc3", "motivation": "\u4f20\u7edf\u5f00\u53d1\u6d41\u7a0b\u4e2d\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5c06\u4ee3\u7801\u90e8\u7f72\u5230\u4e91\u7aef\u624d\u80fd\u6d4b\u8bd5\u4e0e\u4e91\u670d\u52a1\u7684\u96c6\u6210\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u8017\u65f6\u6570\u5c0f\u65f6\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u5468\u671f\u548c\u4e91\u6210\u672c\uff0c\u4e14\u7f3a\u4e4f\u5373\u65f6\u53cd\u9988", "method": "mirrord\u901a\u8fc7\u8ba9\u5f00\u53d1\u8005\u5728\u672c\u5730\u73af\u5883\u4e2d\u8fd0\u884c\u4ee3\u7801\uff0c\u540c\u65f6\u900f\u660e\u5730\u8fde\u63a5\u5230\u771f\u5b9e\u7684\u4e91\u670d\u52a1\uff0c\u65e0\u9700\u5c06\u4ee3\u7801\u90e8\u7f72\u5230\u4e91\u7aef\u5373\u53ef\u8fdb\u884c\u96c6\u6210\u6d4b\u8bd5", "result": "monday.com\u4f7f\u7528mirrord\u540e\u5c06\u5f00\u53d1\u5468\u671f\u65f6\u95f4\u51cf\u5c11\u4e8670%\uff0c\u5f00\u53d1\u8005\u83b7\u5f97\u5373\u65f6\u53cd\u9988\uff0c\u663e\u8457\u964d\u4f4e\u4e91\u6210\u672c\uff0c\u5e76\u589e\u5f3a\u4e86\u53d1\u5e03\u4fe1\u5fc3", "conclusion": "mirrord\u901a\u8fc7\u672c\u5730\u5f00\u53d1\u4e0e\u4e91\u670d\u52a1\u76f4\u63a5\u96c6\u6210\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u4e91\u8d44\u6e90\u6d88\u8017\uff0c\u662f\u73b0\u4ee3\u4e91\u539f\u751f\u5f00\u53d1\u7684\u6709\u6548\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2602.5d4b867d", "categories": ["tldr.article"], "pdf": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fnewrelic.com%2Fevent%2Fnew-relic-advance-amer%3Futm_source=tldr%26utm_medium=email%26utm_campaign=%26utm_content=tldr-newsletter/2/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/eFArBvtwpbwTHrW0UfVN-uk6QLiQnblDNp0tlNdDK5c=445", "abs": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fnewrelic.com%2Fevent%2Fnew-relic-advance-amer%3Futm_source=tldr%26utm_medium=email%26utm_campaign=%26utm_content=tldr-newsletter/2/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/eFArBvtwpbwTHrW0UfVN-uk6QLiQnblDNp0tlNdDK5c=445", "authors": ["TLDR Newsletter"], "title": "Superhuman observability with AI agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: Sponsor, Links: http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fnewrelic.com%2Fevent%2Fnew-relic-advance-amer%3Futm_source=tldr%26utm_medium=email%26utm_campaign=%26utm_content=tldr-newsletter/2/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/eFArBvtwpbwTHrW0UfVN-uk6QLiQnblDNp0tlNdDK5c=445", "summary": "Superhuman observability with AI agents (Sponsor) What if your observability platform didn't just detect issues, but resolved them automatically?At New Relic Advance \u2014 a free virtual event on February 24 \u2014 the team will show how agentic AI plugs into observability workflows to detect, diagnose, and resolve incidents without waiting for a human. They're also demoing full-stack visibility for AI applications and tools that tie system performance directly to business metrics. If you're running p...", "source": "tldr", "AI": {"tldr": "New Relic Advance\u6d3b\u52a8\u5c55\u793aAI\u4ee3\u7406\u5982\u4f55\u96c6\u6210\u5230\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\uff0c\u81ea\u52a8\u68c0\u6d4b\u3001\u8bca\u65ad\u548c\u89e3\u51b3\u7cfb\u7edf\u95ee\u9898\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884", "motivation": "\u4f20\u7edf\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u9700\u8981\u4eba\u5de5\u5e72\u9884\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u5e0c\u671b\u901a\u8fc7AI\u4ee3\u7406\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u6545\u969c\u68c0\u6d4b\u3001\u8bca\u65ad\u548c\u89e3\u51b3\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u5e76\u51cf\u5c11\u4eba\u5de5\u54cd\u5e94\u65f6\u95f4", "method": "\u5728New Relic Advance\u865a\u62df\u6d3b\u52a8\u4e2d\u5c55\u793aAI\u4ee3\u7406\u5982\u4f55\u96c6\u6210\u5230\u53ef\u89c2\u6d4b\u6027\u5de5\u4f5c\u6d41\u4e2d\uff0c\u63d0\u4f9b\u5168\u6808AI\u5e94\u7528\u53ef\u89c1\u6027\uff0c\u5e76\u5c06\u7cfb\u7edf\u6027\u80fd\u4e0e\u4e1a\u52a1\u6307\u6807\u76f4\u63a5\u5173\u8054\u7684\u5de5\u5177", "result": "\u5c55\u793a\u4e86AI\u4ee3\u7406\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u3001\u8bca\u65ad\u548c\u89e3\u51b3\u7cfb\u7edf\u95ee\u9898\uff0c\u65e0\u9700\u7b49\u5f85\u4eba\u5de5\u5e72\u9884\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86AI\u5e94\u7528\u7684\u5168\u6808\u53ef\u89c1\u6027\u548c\u6027\u80fd-\u4e1a\u52a1\u6307\u6807\u5173\u8054\u5de5\u5177", "conclusion": "AI\u4ee3\u7406\u4e0e\u53ef\u89c2\u6d4b\u6027\u5e73\u53f0\u7684\u7ed3\u5408\u80fd\u591f\u5b9e\u73b0\u8d85\u4eba\u7684\u7cfb\u7edf\u76d1\u63a7\u80fd\u529b\uff0c\u81ea\u52a8\u89e3\u51b3\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u8fd0\u7ef4\u6548\u7387", "topic": "agent analysis"}}
{"id": "tldr.2602.b951cac0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FO6d9KB/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/I_bjmO-RdIsCTHcBfzpoWw8XxBrSf4N7yOf9UhW81YQ=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FO6d9KB/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/I_bjmO-RdIsCTHcBfzpoWw8XxBrSf4N7yOf9UhW81YQ=445", "authors": ["TLDR Newsletter"], "title": "Former GitHub CEO Bets $60M That Developer Tools Need a Factory Reset for the AI Age", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FO6d9KB/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/I_bjmO-RdIsCTHcBfzpoWw8XxBrSf4N7yOf9UhW81YQ=445", "summary": "Former GitHub CEO Bets $60M That Developer Tools Need a Factory Reset for the AI Age (4 minute read) Former GitHub CEO Thomas Dohmke launched Entire with a $60 million seed round at a $300 million valuation to rebuild the software development lifecycle for AI agents. Its first open-source tool, Checkpoints, records AI reasoning to improve review, governance, and DevOps oversight of machine-generated code.", "source": "tldr", "AI": {"tldr": "\u524dGitHub CEO\u6295\u8d446000\u4e07\u7f8e\u5143\u521b\u5efaEntire\u516c\u53f8\uff0c\u65e8\u5728\u4e3aAI\u65f6\u4ee3\u91cd\u6784\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u94fe\uff0c\u9996\u4e2a\u5f00\u6e90\u5de5\u5177Checkpoints\u8bb0\u5f55AI\u63a8\u7406\u8fc7\u7a0b\u4ee5\u6539\u8fdb\u4ee3\u7801\u5ba1\u67e5\u548c\u6cbb\u7406", "motivation": "\u5f53\u524d\u5f00\u53d1\u8005\u5de5\u5177\u65e0\u6cd5\u9002\u5e94AI\u4ee3\u7406\u65f6\u4ee3\u7684\u9700\u6c42\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u6784\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff0c\u4ee5\u66f4\u597d\u5730\u7ba1\u7406AI\u751f\u6210\u7684\u4ee3\u7801", "method": "\u521b\u5efaEntire\u516c\u53f8\uff0c\u5f00\u53d1\u5f00\u6e90\u5de5\u5177Checkpoints\uff0c\u8be5\u5de5\u5177\u8bb0\u5f55AI\u4ee3\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u4ee3\u7801\u5ba1\u67e5\u3001\u6cbb\u7406\u548cDevOps\u63d0\u4f9b\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027", "result": "\u83b7\u5f976000\u4e07\u7f8e\u5143\u79cd\u5b50\u8f6e\u878d\u8d44\uff0c\u4f30\u503c3\u4ebf\u7f8e\u5143\uff0c\u63a8\u51fa\u9996\u4e2a\u5f00\u6e90\u5de5\u5177Checkpoints", "conclusion": "AI\u65f6\u4ee3\u9700\u8981\u5168\u65b0\u7684\u5f00\u53d1\u8005\u5de5\u5177\uff0c\u901a\u8fc7\u8bb0\u5f55AI\u63a8\u7406\u8fc7\u7a0b\u53ef\u4ee5\u663e\u8457\u6539\u5584AI\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u63a7\u5236\u548c\u6cbb\u7406", "topic": "swe application"}}
{"id": "tldr.2602.df9f993b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/1RwB1ujw5W96qKvFFqcqPUJGkCaj46hGalDn3m8YFys=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/1RwB1ujw5W96qKvFFqcqPUJGkCaj46hGalDn3m8YFys=445", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c70a4f679-71251c7e-f2f2-4cdd-bbe0-dc155d2bb26d-000000/1RwB1ujw5W96qKvFFqcqPUJGkCaj46hGalDn3m8YFys=445", "summary": "Blueprinting Security in CI/CD: Building Trust Through Open Source (5 minute read) The CI/CD Security Blueprint shows how open source tools and platform engineering embed continuous validation across code, build, and runtime stages.", "source": "tldr", "AI": {"tldr": "CI/CD\u5b89\u5168\u84dd\u56fe\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u5e73\u53f0\u5de5\u7a0b\u5728\u4ee3\u7801\u3001\u6784\u5efa\u548c\u8fd0\u884c\u65f6\u9636\u6bb5\u5d4c\u5165\u6301\u7eed\u9a8c\u8bc1\uff0c\u4ee5\u5efa\u7acb\u4fe1\u4efb", "motivation": "\u5728CI/CD\u6d41\u6c34\u7ebf\u4e2d\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u5e73\u53f0\u5de5\u7a0b\u65b9\u6cd5\u5efa\u7acb\u4fe1\u4efb\uff0c\u89e3\u51b3\u8f6f\u4ef6\u4ea4\u4ed8\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898", "method": "\u4f7f\u7528\u5f00\u6e90\u5de5\u5177\u548c\u5e73\u53f0\u5de5\u7a0b\u6280\u672f\uff0c\u5728\u4ee3\u7801\u3001\u6784\u5efa\u548c\u8fd0\u884c\u65f6\u4e09\u4e2a\u9636\u6bb5\u5d4c\u5165\u6301\u7eed\u9a8c\u8bc1\u673a\u5236\uff0c\u6784\u5efaCI/CD\u5b89\u5168\u84dd\u56fe", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684CI/CD\u5b89\u5168\u84dd\u56fe\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u5b9e\u73b0\u8de8\u9636\u6bb5\u7684\u6301\u7eed\u5b89\u5168\u9a8c\u8bc1", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u5e73\u53f0\u5de5\u7a0b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728CI/CD\u6d41\u6c34\u7ebf\u4e2d\u6709\u6548\u5d4c\u5165\u5b89\u5168\u9a8c\u8bc1\uff0c\u5efa\u7acb\u8f6f\u4ef6\u4ea4\u4ed8\u7684\u4fe1\u4efb\u673a\u5236", "topic": "swe application"}}
{"id": "tldr.2602.45ecb5dc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-codex-is-built%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/RzKM4wbbOpDq9tKRCTISY31lYegjZD0YMnWOIDCLbto=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-codex-is-built%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/RzKM4wbbOpDq9tKRCTISY31lYegjZD0YMnWOIDCLbto=445", "authors": ["TLDR Newsletter"], "title": "How Codex is built", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewsletter.pragmaticengineer.com%2Fp%2Fhow-codex-is-built%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/RzKM4wbbOpDq9tKRCTISY31lYegjZD0YMnWOIDCLbto=445", "summary": "How Codex is built (15 minute read) OpenAI's coding agent, Codex, has a Rust-based architecture, open-source CLI, and \"agent loop\" that orchestrates user-model-tool interactions. Codex builds itself, generating over 90% of its own code, turning engineers into \"agent managers\" who oversee multiple parallel agents for tasks like feature implementation, code review, and bug fixing.", "source": "tldr", "AI": {"tldr": "OpenAI\u7684Codex\u662f\u4e00\u4e2a\u57fa\u4e8eRust\u67b6\u6784\u7684\u7f16\u7801\u4ee3\u7406\uff0c\u5177\u6709\u5f00\u6e90CLI\u548c\"\u4ee3\u7406\u5faa\u73af\"\u673a\u5236\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u8d85\u8fc790%\u7684\u81ea\u8eab\u4ee3\u7801\uff0c\u5c06\u5de5\u7a0b\u5e08\u8f6c\u53d8\u4e3a\u7ba1\u7406\u591a\u4e2a\u5e76\u884c\u4ee3\u7406\u7684\"\u4ee3\u7406\u7ba1\u7406\u8005\"\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u81ea\u6211\u6784\u5efa\u7684\u7f16\u7801\u4ee3\u7406\u7cfb\u7edf\uff0c\u51cf\u5c11\u4eba\u5de5\u7f16\u7801\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\u3001\u529f\u80fd\u5b9e\u73b0\u3001\u4ee3\u7801\u5ba1\u67e5\u548c\u9519\u8bef\u4fee\u590d\u7b49\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u57fa\u4e8eRust\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u5f00\u53d1\u5f00\u6e90\u547d\u4ee4\u884c\u754c\u9762\uff0c\u5b9e\u73b0\"\u4ee3\u7406\u5faa\u73af\"\u673a\u5236\u6765\u534f\u8c03\u7528\u6237-\u6a21\u578b-\u5de5\u5177\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u81ea\u6211\u6784\u5efa\u5e76\u7ba1\u7406\u591a\u4e2a\u5e76\u884c\u4ee3\u7406\u3002", "result": "Codex\u6210\u529f\u751f\u6210\u4e86\u8d85\u8fc790%\u7684\u81ea\u8eab\u4ee3\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u81ea\u52a8\u5316\uff0c\u5de5\u7a0b\u5e08\u89d2\u8272\u8f6c\u53d8\u4e3a\u7ba1\u7406\u591a\u4e2a\u5e76\u884c\u4ee3\u7406\u7684\"\u4ee3\u7406\u7ba1\u7406\u8005\"\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u529f\u80fd\u5b9e\u73b0\u3001\u4ee3\u7801\u5ba1\u67e5\u548c\u9519\u8bef\u4fee\u590d\u7b49\u4efb\u52a1\u3002", "conclusion": "Codex\u5c55\u793a\u4e86\u7f16\u7801\u4ee3\u7406\u5728\u81ea\u6211\u6784\u5efa\u548c\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u5c06\u5de5\u7a0b\u5e08\u8f6c\u53d8\u4e3a\u4ee3\u7406\u7ba1\u7406\u8005\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2602.7b203cd7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2142%26utm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/fqE2UDNxa0AOCXT51O17nj10P7Q28MAMAawpIoius00=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2142%26utm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/fqE2UDNxa0AOCXT51O17nj10P7Q28MAMAawpIoius00=445", "authors": ["TLDR Newsletter"], "title": "Showing the Work of Agents in UI", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2142%26utm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/fqE2UDNxa0AOCXT51O17nj10P7Q28MAMAawpIoius00=445", "summary": "Showing the Work of Agents in UI (3 minute read) A recurring UI design challenge in agentic AI products is whether and how to display the agent's internal planning, tool usage, and decision-making steps. Users are divided, with some finding the detailed process overwhelming and preferring only final results, while others consider seeing the agent's work crucial for monitoring and verification. This article goes over various ways to display agentic AI, such as using progressive disclosure and ...", "source": "tldr", "AI": {"tldr": "\u8ba8\u8bba\u5982\u4f55\u5728UI\u4e2d\u5c55\u793aAI\u4ee3\u7406\u7684\u5185\u90e8\u5de5\u4f5c\u8fc7\u7a0b\uff0c\u5e73\u8861\u7528\u6237\u5bf9\u900f\u660e\u5ea6\u548c\u7b80\u6d01\u6027\u7684\u9700\u6c42", "motivation": "AI\u4ee3\u7406\u4ea7\u54c1\u9762\u4e34UI\u8bbe\u8ba1\u6311\u6218\uff1a\u7528\u6237\u5bf9\u662f\u5426\u5c55\u793a\u4ee3\u7406\u7684\u5185\u90e8\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u548c\u51b3\u7b56\u6b65\u9aa4\u5b58\u5728\u5206\u6b67\uff0c\u9700\u8981\u627e\u5230\u5e73\u8861\u900f\u660e\u5ea6\u548c\u7b80\u6d01\u6027\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63a2\u8ba8\u591a\u79cd\u5c55\u793a\u65b9\u5f0f\uff0c\u5982\u6e10\u8fdb\u5f0f\u62ab\u9732\u7b49UI\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5206\u6790\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u9700\u6c42\u548c\u504f\u597d", "result": "\u8bc6\u522b\u4e86\u7528\u6237\u5bf9AI\u4ee3\u7406\u5de5\u4f5c\u8fc7\u7a0b\u5c55\u793a\u7684\u4e0d\u540c\u6001\u5ea6\uff0c\u63d0\u51fa\u4e86\u76f8\u5e94\u7684UI\u8bbe\u8ba1\u7b56\u7565\u6765\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u9700\u6c42", "conclusion": "AI\u4ee3\u7406\u4ea7\u54c1\u7684UI\u8bbe\u8ba1\u9700\u8981\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u7528\u6237\u504f\u597d\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u62ab\u9732\u7b49\u7b56\u7565\u5e73\u8861\u900f\u660e\u5ea6\u548c\u7b80\u6d01\u6027", "topic": "agent analysis"}}
{"id": "tldr.2602.837595ea", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalv.info%2Fagents-feb-2026%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/lT_hKkHQpNpiczn2zcwQ7yBWs0hHk7aYOhMqY0L59zs=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalv.info%2Fagents-feb-2026%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/lT_hKkHQpNpiczn2zcwQ7yBWs0hHk7aYOhMqY0L59zs=445", "authors": ["TLDR Newsletter"], "title": "Coding Agents in Feb 2026", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 19 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalv.info%2Fagents-feb-2026%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/lT_hKkHQpNpiczn2zcwQ7yBWs0hHk7aYOhMqY0L59zs=445", "summary": "Coding Agents in Feb 2026 (19 minute read) This dev's advanced workflow using coding agents depends on proper context management and understanding each model's strengths. He mostly uses Claude Code (Opus) for planning, orchestration, and tool-use due to its efficiency and human-like output, but relies on OpenAI's Codex for writing more correct and bug-free code. His strategy involves chunking work, externalizing context through detailed plans, and developing custom skills to automate complex ...", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u4f7f\u7528Claude Code\u8fdb\u884c\u89c4\u5212\u548c\u534f\u8c03\uff0cOpenAI Codex\u7f16\u5199\u4ee3\u7801\uff0c\u901a\u8fc7\u5206\u5757\u5de5\u4f5c\u3001\u5916\u90e8\u5316\u4e0a\u4e0b\u6587\u548c\u5f00\u53d1\u81ea\u5b9a\u4e49\u6280\u80fd\u6765\u4f18\u5316\u7f16\u7801\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b", "motivation": "\u63d0\u9ad8\u7f16\u7801\u4ee3\u7406\u7684\u5de5\u4f5c\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u901a\u8fc7\u5408\u7406\u5229\u7528\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\u6765\u89e3\u51b3\u590d\u6742\u7f16\u7801\u4efb\u52a1", "method": "\u7ed3\u5408\u4f7f\u7528Claude Code\u8fdb\u884c\u89c4\u5212\u3001\u534f\u8c03\u548c\u5de5\u5177\u4f7f\u7528\uff0cOpenAI Codex\u7f16\u5199\u4ee3\u7801\uff1b\u91c7\u7528\u5206\u5757\u5de5\u4f5c\u3001\u5916\u90e8\u5316\u4e0a\u4e0b\u6587\uff08\u8be6\u7ec6\u8ba1\u5212\uff09\u3001\u5f00\u53d1\u81ea\u5b9a\u4e49\u6280\u80fd\u7b49\u7b56\u7565", "result": "\u5efa\u7acb\u4e86\u9ad8\u6548\u7684\u7f16\u7801\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\uff0c\u63d0\u9ad8\u4ee3\u7801\u6b63\u786e\u6027\u548c\u51cf\u5c11bug", "conclusion": "\u5408\u7406\u5229\u7528\u4e0d\u540c\u7f16\u7801\u4ee3\u7406\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u914d\u5408\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u5de5\u4f5c\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2602.d1496774", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-6%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/Sk1l5ISQ-Lncr5KmmzNe1szeOdXG3A-Ko_WtoO8o_Us=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-6%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/Sk1l5ISQ-Lncr5KmmzNe1szeOdXG3A-Ko_WtoO8o_Us=445", "authors": ["TLDR Newsletter"], "title": "Claude Sonnet 4.6", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-sonnet-4-6%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/Sk1l5ISQ-Lncr5KmmzNe1szeOdXG3A-Ko_WtoO8o_Us=445", "summary": "Claude Sonnet 4.6 (10 minute read) Anthropic has launched Claude Sonnet 4.6, a major upgrade that improves its capabilities in coding, computer use, long-context reasoning, and agent planning. This new Sonnet model delivers performance that rivals or exceeds earlier Opus models, yet remains at Sonnet's more accessible price point. It has better general-purpose computer use, enabling human-level task execution, and a 1M token context window for long-horizon planning.", "source": "tldr", "AI": {"tldr": "Anthropic\u53d1\u5e03Claude Sonnet 4.6\uff0c\u5728\u7f16\u7801\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u3001\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u667a\u80fd\u4f53\u89c4\u5212\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a\u65e9\u671fOpus\u6a21\u578b\uff0c\u4f46\u4fdd\u6301Sonnet\u7684\u5b9e\u60e0\u4ef7\u683c", "motivation": "\u63d0\u5347AI\u6a21\u578b\u5728\u7f16\u7801\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u3001\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u667a\u80fd\u4f53\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u66f4\u5b9e\u60e0\u7684\u4ef7\u683c\u63d0\u4f9b\u63a5\u8fd1\u6216\u8d85\u8d8a\u9ad8\u7aef\u6a21\u578b\u7684\u6027\u80fd", "method": "\u5f00\u53d1Claude Sonnet 4.6\u6a21\u578b\uff0c\u589e\u5f3a\u7f16\u7801\u80fd\u529b\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u6280\u80fd\u3001\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff08\u652f\u6301100\u4e07token\u4e0a\u4e0b\u6587\u7a97\u53e3\uff09\u548c\u667a\u80fd\u4f53\u89c4\u5212\u529f\u80fd", "result": "\u65b0Sonnet\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u6216\u8d85\u8d8a\u65e9\u671fOpus\u6a21\u578b\uff0c\u5177\u5907\u4eba\u7c7b\u6c34\u5e73\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u652f\u6301\u957f\u89c6\u91ce\u89c4\u5212\uff0c\u540c\u65f6\u4fdd\u6301Sonnet\u7684\u4ef7\u683c\u4f18\u52bf", "conclusion": "Claude Sonnet 4.6\u4ee3\u8868\u4e86AI\u6a21\u578b\u7684\u91cd\u8981\u8fdb\u6b65\uff0c\u4ee5\u66f4\u5b9e\u60e0\u7684\u4ef7\u683c\u63d0\u4f9b\u9ad8\u7aef\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u7f16\u7801\u3001\u8ba1\u7b97\u673a\u4f7f\u7528\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5e94\u7528\u573a\u666f", "topic": "code agent"}}
{"id": "tldr.2602.cef60fce", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpushtoprod.substack.com%2Fp%2Fmy-ai-agent-didnt-do-anything%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/KIyYkV37gQAFuP6a4ozAA4r3ao-m37LnxkZJeG5WJ0I=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpushtoprod.substack.com%2Fp%2Fmy-ai-agent-didnt-do-anything%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/KIyYkV37gQAFuP6a4ozAA4r3ao-m37LnxkZJeG5WJ0I=445", "authors": ["TLDR Newsletter"], "title": "My AI Agent Said It Was Done. It Hadn't Done Anything", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpushtoprod.substack.com%2Fp%2Fmy-ai-agent-didnt-do-anything%3Futm_source=tldrdev/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/KIyYkV37gQAFuP6a4ozAA4r3ao-m37LnxkZJeG5WJ0I=445", "summary": "My AI Agent Said It Was Done. It Hadn't Done Anything (5 minute read) AI agents falsely report success when they are resumed from clean git worktrees. Implementing partial commits on failure and mandatory state verification prevents agents from ignoring uncommitted work.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u4ece\u5e72\u51c0\u7684git\u5de5\u4f5c\u6811\u6062\u590d\u65f6\u9519\u8bef\u62a5\u544a\u6210\u529f\uff0c\u901a\u8fc7\u5b9e\u65bd\u5931\u8d25\u65f6\u7684\u90e8\u5206\u63d0\u4ea4\u548c\u5f3a\u5236\u72b6\u6001\u9a8c\u8bc1\u6765\u9632\u6b62\u4ee3\u7406\u5ffd\u7565\u672a\u63d0\u4ea4\u7684\u5de5\u4f5c", "motivation": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7ecf\u5e38\u9519\u8bef\u5730\u62a5\u544a\u4efb\u52a1\u5b8c\u6210\uff0c\u7279\u522b\u662f\u5728\u4ece\u5e72\u51c0\u7684git\u5de5\u4f5c\u6811\u6062\u590d\u65f6\uff0c\u5373\u4f7f\u5b83\u4eec\u5b9e\u9645\u4e0a\u6ca1\u6709\u5b8c\u6210\u4efb\u4f55\u5de5\u4f5c\u3002\u8fd9\u79cd\u865a\u5047\u7684\u6210\u529f\u62a5\u544a\u4f1a\u5bfc\u81f4\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u4e25\u91cd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u5728\u5931\u8d25\u65f6\u8fdb\u884c\u90e8\u5206\u63d0\u4ea4\uff0c\u786e\u4fdd\u5de5\u4f5c\u8fdb\u5ea6\u88ab\u4fdd\u5b58\uff1b2\uff09\u5f3a\u5236\u72b6\u6001\u9a8c\u8bc1\uff0c\u8981\u6c42\u4ee3\u7406\u5728\u62a5\u544a\u6210\u529f\u524d\u9a8c\u8bc1\u5176\u5b9e\u9645\u5de5\u4f5c\u72b6\u6001\u3002", "result": "\u901a\u8fc7\u5b9e\u65bd\u8fd9\u4e9b\u673a\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u9632\u6b62AI\u4ee3\u7406\u5ffd\u7565\u672a\u63d0\u4ea4\u7684\u5de5\u4f5c\u5e76\u865a\u5047\u62a5\u544a\u6210\u529f\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u72b6\u6001\u7ba1\u7406\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u5f3a\u5236\u5176\u6b63\u786e\u8ddf\u8e2a\u548c\u62a5\u544a\u5de5\u4f5c\u72b6\u6001\uff0c\u8fd9\u5bf9\u4e8e\u6784\u5efa\u53ef\u9760\u7684AI\u8f85\u52a9\u5f00\u53d1\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "code agent"}}
{"id": "tldr.2602.f64d570d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445", "authors": ["TLDR Newsletter"], "title": "GitHub Agentic Workflows", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445", "summary": "GitHub Agentic Workflows (Sponsor) Imagine waking up to issues triaged, CI failures investigated with fixes to review, and two PRs proposing test improvements. All of that while you were sleeping. Give yourself a headstart, every day. Create your first agentic workflow today.", "source": "tldr", "AI": {"tldr": "GitHub\u63a8\u51fa\u667a\u80fd\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u80fd\u5728\u5f00\u53d1\u8005\u7761\u7720\u65f6\u81ea\u52a8\u5904\u7406\u95ee\u9898\u5206\u7c7b\u3001CI\u6545\u969c\u8c03\u67e5\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u7b49\u4efb\u52a1", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u65e5\u5e38\u91cd\u590d\u6027\u5de5\u4f5c\u8d1f\u62c5\uff0c\u8ba9\u5f00\u53d1\u8005\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5f00\u53d1\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "method": "\u57fa\u4e8eGitHub\u5e73\u53f0\u7684\u667a\u80fd\u4ee3\u7406\u5de5\u4f5c\u6d41\u7cfb\u7edf\uff0c\u5229\u7528AI\u4ee3\u7406\u81ea\u52a8\u6267\u884c\u95ee\u9898\u5206\u7c7b\u3001CI\u6545\u969c\u8c03\u67e5\u3001\u4ee3\u7801\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u7b49\u4efb\u52a1", "result": "\u5f00\u53d1\u8005\u6bcf\u5929\u9192\u6765\u5c31\u80fd\u770b\u5230\u5df2\u5904\u7406\u7684\u95ee\u9898\u3001\u4fee\u590d\u5efa\u8bae\u548c\u6d4b\u8bd5\u6539\u8fdbPR\uff0c\u5b9e\u73b0\"\u7761\u9192\u5373\u5b8c\u6210\"\u7684\u5f00\u53d1\u4f53\u9a8c", "conclusion": "GitHub\u667a\u80fd\u4ee3\u7406\u5de5\u4f5c\u6d41\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u6bcf\u5929\u83b7\u5f97\u5148\u53d1\u4f18\u52bf\uff0c\u4e13\u6ce8\u4e8e\u66f4\u6709\u4ef7\u503c\u7684\u5f00\u53d1\u5de5\u4f5c", "topic": "swe application"}}
{"id": "tldr.2602.41f14fac", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445", "authors": ["TLDR Newsletter"], "title": "Create your first agentic workflow today", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.github.com%2Fgh-aw%2F%3Futm_source=tldr-dev-newsletter-agentic-workflows-cta%26utm_medium=email%26utm_campaign=agentic-workflows-tech-preview-feb-2026/1/0100019c70a8d69c-ae2ef20b-64fc-450e-94ce-5e46527b7527-000000/tStwgpXZbF7iJDBOcw-tTc-TnxNpzz1SQvtfPI-jBLM=445", "summary": "GitHub Agentic Workflows (Sponsor) Imagine waking up to issues triaged, CI failures investigated with fixes to review, and two PRs proposing test improvements. All of that while you were sleeping. Give yourself a headstart, every day. Create your first agentic workflow today.", "source": "tldr", "AI": {"tldr": "GitHub\u63a8\u51fa\u4ee3\u7406\u5de5\u4f5c\u6d41\u529f\u80fd\uff0c\u53ef\u5728\u7528\u6237\u4f11\u606f\u65f6\u81ea\u52a8\u5904\u7406\u95ee\u9898\u5206\u7c7b\u3001CI\u6545\u969c\u8c03\u67e5\u4fee\u590d\u3001\u6d4b\u8bd5\u6539\u8fdb\u7b49\u4efb\u52a1", "motivation": "\u89e3\u51b3\u5f00\u53d1\u8005\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u9700\u8981\u624b\u52a8\u5904\u7406\u91cd\u590d\u6027\u4efb\u52a1\uff08\u5982\u95ee\u9898\u5206\u7c7b\u3001CI\u6545\u969c\u8c03\u67e5\u3001\u6d4b\u8bd5\u6539\u8fdb\u7b49\uff09\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u4e13\u6ce8\u4e8e\u66f4\u6709\u4ef7\u503c\u7684\u521b\u9020\u6027\u5de5\u4f5c", "method": "\u901a\u8fc7GitHub\u5e73\u53f0\u63d0\u4f9b\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u529f\u80fd\uff0c\u521b\u5efa\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528AI\u4ee3\u7406\u5728\u7528\u6237\u975e\u5de5\u4f5c\u65f6\u95f4\u81ea\u52a8\u6267\u884c\u4efb\u52a1", "result": "\u5f00\u53d1\u8005\u53ef\u4ee5\u5728\u4f11\u606f\u65f6\u83b7\u5f97\u81ea\u52a8\u5904\u7406\u7684\u4efb\u52a1\u7ed3\u679c\uff0c\u5305\u62ec\u5df2\u5206\u7c7b\u7684\u95ee\u9898\u3001CI\u6545\u969c\u8c03\u67e5\u4e0e\u4fee\u590d\u5efa\u8bae\u3001\u6d4b\u8bd5\u6539\u8fdb\u7684PR\u7b49\uff0c\u4e3a\u6bcf\u5929\u7684\u5de5\u4f5c\u63d0\u4f9b\u826f\u597d\u5f00\u7aef", "conclusion": "GitHub\u4ee3\u7406\u5de5\u4f5c\u6d41\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u51cf\u5c11\u91cd\u590d\u6027\u624b\u52a8\u5de5\u4f5c\uff0c\u8ba9\u5f00\u53d1\u8005\u66f4\u4e13\u6ce8\u4e8e\u6838\u5fc3\u5f00\u53d1\u4efb\u52a1", "topic": "swe application"}}
{"id": "tldr.2602.5390b2ab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2F9to5mac.com%2F2026%2F02%2F12%2Fapple-study-looks-into-how-people-expect-to-interact-with-ai-agents%2F%3Futm_source=tldrdesign/1/0100019c70dcbc9f-05267015-14fb-40b1-ab73-2012baf82017-000000/NMAKkgyiCFdB4YUq0oCOb7PyPhrqaXeRJ9yRL48GS3Q=445", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2F9to5mac.com%2F2026%2F02%2F12%2Fapple-study-looks-into-how-people-expect-to-interact-with-ai-agents%2F%3Futm_source=tldrdesign/1/0100019c70dcbc9f-05267015-14fb-40b1-ab73-2012baf82017-000000/NMAKkgyiCFdB4YUq0oCOb7PyPhrqaXeRJ9yRL48GS3Q=445", "authors": ["TLDR Newsletter"], "title": "Apple Study Looks Into How People Expect to Interact with AI Agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2F9to5mac.com%2F2026%2F02%2F12%2Fapple-study-looks-into-how-people-expect-to-interact-with-ai-agents%2F%3Futm_source=tldrdesign/1/0100019c70dcbc9f-05267015-14fb-40b1-ab73-2012baf82017-000000/NMAKkgyiCFdB4YUq0oCOb7PyPhrqaXeRJ9yRL48GS3Q=445", "summary": "Apple Study Looks Into How People Expect to Interact with AI Agents (5 minute read) Apple researchers conducted a two-phase study to understand user expectations for AI agents, analyzing nine existing agents and testing design patterns through simulated interactions with 20 participants. The study revealed that users want visibility into agent actions without micromanagement, with transparency needs varying by task familiarity and risk level. Trust erodes quickly when agents make silent assum...", "source": "tldr", "AI": {"tldr": "\u82f9\u679c\u7814\u7a76\u7528\u6237\u5bf9AI\u4ee3\u7406\u7684\u671f\u671b\uff0c\u53d1\u73b0\u7528\u6237\u5e0c\u671b\u5728\u4e0d\u88ab\u5fae\u89c2\u7ba1\u7406\u7684\u60c5\u51b5\u4e0b\u4e86\u89e3\u4ee3\u7406\u884c\u52a8\uff0c\u900f\u660e\u5ea6\u9700\u6c42\u56e0\u4efb\u52a1\u719f\u6089\u5ea6\u548c\u98ce\u9669\u6c34\u5e73\u800c\u5f02\uff0c\u4fe1\u4efb\u5728\u4ee3\u7406\u505a\u51fa\u65e0\u58f0\u5047\u8bbe\u65f6\u4f1a\u8fc5\u901f\u4e0b\u964d", "motivation": "\u7406\u89e3\u7528\u6237\u5bf9AI\u4ee3\u7406\u7684\u671f\u671b\u548c\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u597d\u7684AI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u7528\u6237\u89c6\u89d2\u7684\u6d1e\u5bdf", "method": "\u4e24\u9636\u6bb5\u7814\u7a76\uff1a\u5206\u67909\u4e2a\u73b0\u6709AI\u4ee3\u7406\uff0c\u901a\u8fc720\u540d\u53c2\u4e0e\u8005\u7684\u6a21\u62df\u4ea4\u4e92\u6d4b\u8bd5\u8bbe\u8ba1\u6a21\u5f0f", "result": "\u7528\u6237\u5e0c\u671b\u5728\u4e0d\u88ab\u5fae\u89c2\u7ba1\u7406\u7684\u60c5\u51b5\u4e0b\u4e86\u89e3\u4ee3\u7406\u884c\u52a8\uff0c\u900f\u660e\u5ea6\u9700\u6c42\u56e0\u4efb\u52a1\u719f\u6089\u5ea6\u548c\u98ce\u9669\u6c34\u5e73\u800c\u5f02\uff0c\u4fe1\u4efb\u5728\u4ee3\u7406\u505a\u51fa\u65e0\u58f0\u5047\u8bbe\u65f6\u4f1a\u8fc5\u901f\u4e0b\u964d", "conclusion": "AI\u4ee3\u7406\u8bbe\u8ba1\u9700\u8981\u5e73\u8861\u900f\u660e\u5ea6\u4e0e\u63a7\u5236\uff0c\u6839\u636e\u4efb\u52a1\u7c7b\u578b\u548c\u98ce\u9669\u6c34\u5e73\u8c03\u6574\u4ea4\u4e92\u6a21\u5f0f", "topic": "agent analysis"}}
{"id": "tldr.2602.ef71736a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FC3zvsP/1/0100019c70de1858-3903622e-2f2d-4809-bbbc-afd6adff9363-000000/qak0k9Grh6QgrglJMzBUofrznZJGOIdzcwOO6kqs2J4=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FC3zvsP/1/0100019c70de1858-3903622e-2f2d-4809-bbbc-afd6adff9363-000000/qak0k9Grh6QgrglJMzBUofrznZJGOIdzcwOO6kqs2J4=444", "authors": ["TLDR Newsletter"], "title": "How to Sell to Agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-18, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FC3zvsP/1/0100019c70de1858-3903622e-2f2d-4809-bbbc-afd6adff9363-000000/qak0k9Grh6QgrglJMzBUofrznZJGOIdzcwOO6kqs2J4=444", "summary": "How to Sell to Agents (7 minute read) In 1937, Ronald Coase asked why firms existed if markets are so efficient. His answer was transaction costs. AI agents are about to collapse those costs to near zero. An agent can discover a service, check its price, and call it in a single HTTP round-trip. HTTP 402 - \"Payment Required\" - has been reserved for future use since 1997. We're finally finding that use. The web was built for humans to browse. The next layer will be built for agents to buy.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5c06\u5927\u5e45\u964d\u4f4e\u4ea4\u6613\u6210\u672c\uff0cHTTP 402\u72b6\u6001\u7801\u5c06\u7528\u4e8e\u4ee3\u7406\u652f\u4ed8\uff0c\u7f51\u7edc\u5c06\u4ece\u4eba\u7c7b\u6d4f\u89c8\u8f6c\u5411\u4ee3\u7406\u8d2d\u4e70", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u964d\u4f4e\u4ea4\u6613\u6210\u672c\u6539\u53d8\u5e02\u573a\u7ed3\u6784\uff0c\u4ee5\u53ca\u5982\u4f55\u4e3a\u4ee3\u7406\u7ecf\u6d4e\u6784\u5efa\u65b0\u7684\u7f51\u7edc\u5c42", "method": "\u57fa\u4e8e\u79d1\u65af\u4ea4\u6613\u6210\u672c\u7406\u8bba\uff0c\u5206\u6790AI\u4ee3\u7406\u5bf9\u5e02\u573a\u6548\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51faHTTP 402\u72b6\u6001\u7801\u5728\u4ee3\u7406\u652f\u4ed8\u4e2d\u7684\u5e94\u7528", "result": "AI\u4ee3\u7406\u80fd\u5c06\u4ea4\u6613\u6210\u672c\u964d\u81f3\u63a5\u8fd1\u96f6\uff0cHTTP 402\u72b6\u6001\u7801\u5c06\u6210\u4e3a\u4ee3\u7406\u652f\u4ed8\u7684\u6807\u51c6\uff0c\u7f51\u7edc\u5c06\u53d1\u5c55\u51fa\u4e13\u95e8\u4e3a\u4ee3\u7406\u8d2d\u4e70\u8bbe\u8ba1\u7684\u5c42", "conclusion": "AI\u4ee3\u7406\u5c06\u5f7b\u5e95\u6539\u53d8\u5e02\u573a\u4ea4\u6613\u65b9\u5f0f\uff0c\u9700\u8981\u6784\u5efa\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u652f\u6301\u4ee3\u7406\u7ecf\u6d4e", "topic": "agent analysis"}}
