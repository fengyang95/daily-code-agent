{"id": "2601.05300", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05300", "abs": "https://arxiv.org/abs/2601.05300", "authors": ["Susmit Das"], "title": "TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning", "comment": "14 pages, 3 figures with 27 page appendix. See https://github.com/The-Coherence-Initiative/TIME and https://github.com/The-Coherence-Initiative/TIMEBench for associated code", "summary": "Reasoning oriented large language models often expose explicit \"thinking\" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench", "AI": {"tldr": "TIME\u6846\u67b6\u901a\u8fc7\u65f6\u95f4\u654f\u611f\u7684\u5143\u63a8\u7406\u673a\u5236\uff0c\u5728\u5bf9\u8bdd\u4e2d\u5f15\u5165<time>\u6807\u7b7e\u3001tick turns\u548c<think>\u5757\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u7406\uff0c\u5927\u5e45\u51cf\u5c11\u63a8\u7406token\u5e76\u63d0\u5347\u65f6\u5e8f\u5bf9\u8bdd\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u663e\u5f0f\u63a8\u7406\u8bbe\u8ba1\u5b58\u5728\u6210\u672c\u9ad8\u3001\u53ef\u5ba1\u8ba1\u6027\u5dee\u3001\u65e0\u6cd5\u91cd\u65b0\u89e6\u53d1\u63a8\u7406\u7684\u95ee\u9898\uff0c\u4e14\u5bf9\u8bdd\u6a21\u578b\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u7ed3\u6784\u7684\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u5904\u7406\u65f6\u95f4\u95f4\u9694\u548c\u65f6\u5e8f\u5173\u7cfb\u3002", "method": "\u5f15\u5165TIME\u6846\u67b6\uff1a1) \u4f7f\u7528ISO 8601 <time>\u6807\u7b7e\u6807\u8bb0\u65f6\u95f4\uff1b2) tick turns\u8868\u793a\u6c89\u9ed8\u95f4\u9694\uff1b3) \u77ed<think>\u5757\u53ef\u5728\u56de\u590d\u4efb\u610f\u4f4d\u7f6e\u51fa\u73b0\u3002\u901a\u8fc7\u56db\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3Qwen3\u6a21\u578b\uff0c\u5305\u62ec\u5c0f\u89c4\u6a21\u3001\u6700\u5927\u591a\u6837\u6027\u7684\u5168\u6279\u6b21\u5bf9\u9f50\u6b65\u9aa4\u3002", "result": "\u57284B\u523032B\u89c4\u6a21\u4e0a\uff0cTIME\u5728TIMEBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7840Qwen3\u6a21\u578b\uff08\u65e0\u8bba\u662f\u5426\u5f00\u542f\u63a8\u7406\u6a21\u5f0f\uff09\uff0c\u540c\u65f6\u5c06\u63a8\u7406token\u51cf\u5c11\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "TIME\u6846\u67b6\u901a\u8fc7\u65f6\u95f4\u654f\u611f\u7684\u5143\u63a8\u7406\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65f6\u5e8f\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2601.05256", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.05256", "abs": "https://arxiv.org/abs/2601.05256", "authors": ["Eirini Baltzi", "Tilemachos Moumouris", "Athena Psalta", "Vasileios Tsironis", "Konstantinos Karantzalos"], "title": "Naiad: Novel Agentic Intelligent Autonomous System for Inland Water Monitoring", "comment": null, "summary": "Inland water monitoring is vital for safeguarding public health and ecosystems, enabling timely interventions to mitigate risks. Existing methods often address isolated sub-problems such as cyanobacteria, chlorophyll, or other quality indicators separately. NAIAD introduces an agentic AI assistant that leverages Large Language Models (LLMs) and external analytical tools to deliver a holistic solution for inland water monitoring using Earth Observation (EO) data. Designed for both experts and non-experts, NAIAD provides a single-prompt interface that translates natural-language queries into actionable insights. Through Retrieval-Augmented Generation (RAG), LLM reasoning, external tool orchestration, computational graph execution, and agentic reflection, it retrieves and synthesizes knowledge from curated sources to produce tailored reports. The system integrates diverse tools for weather data, Sentinel-2 imagery, remote-sensing index computation (e.g., NDCI), chlorophyll-a estimation, and established platforms such as CyFi. Performance is evaluated using correctness and relevancy metrics, achieving over 77% and 85% respectively on a dedicated benchmark covering multiple user-expertise levels. Preliminary results show strong adaptability and robustness across query types. An ablation study on LLM backbones further highlights Gemma 3 (27B) and Qwen 2.5 (14B) as offering the best balance between computational efficiency and reasoning performance.", "AI": {"tldr": "NAIAD\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u52a9\u624b\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5de5\u5177\u7f16\u6392\u548c\u8ba1\u7b97\u56fe\u6267\u884c\uff0c\u4e3a\u5185\u9646\u6c34\u4f53\u76d1\u6d4b\u63d0\u4f9b\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "motivation": "\u5185\u9646\u6c34\u4f53\u76d1\u6d4b\u5bf9\u4fdd\u62a4\u516c\u5171\u5065\u5eb7\u548c\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5b64\u7acb\u5730\u5904\u7406\u84dd\u85fb\u3001\u53f6\u7eff\u7d20\u7b49\u5b50\u95ee\u9898\uff0c\u7f3a\u4e4f\u6574\u4f53\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u4e3a\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u63d0\u4f9b\u7edf\u4e00\u7684\u76d1\u6d4b\u5de5\u5177\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001LLM\u63a8\u7406\u3001\u5916\u90e8\u5de5\u5177\u7f16\u6392\u3001\u8ba1\u7b97\u56fe\u6267\u884c\u548c\u667a\u80fd\u4f53\u53cd\u601d\u3002\u96c6\u6210\u5929\u6c14\u6570\u636e\u3001Sentinel-2\u5f71\u50cf\u3001\u9065\u611f\u6307\u6570\u8ba1\u7b97\u3001\u53f6\u7eff\u7d20-a\u4f30\u8ba1\u548cCyFi\u7b49\u5e73\u53f0\u3002", "result": "\u5728\u4e13\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6b63\u786e\u6027\u548c\u76f8\u5173\u6027\u5206\u522b\u8fbe\u523077%\u548c85%\u4ee5\u4e0a\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u6d88\u878d\u7814\u7a76\u8868\u660eGemma 3 (27B)\u548cQwen 2.5 (14B)\u5728\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "NAIAD\u6210\u529f\u4e3a\u5185\u9646\u6c34\u4f53\u76d1\u6d4b\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u667a\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u4e00\u63d0\u793a\u754c\u9762\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\uff0c\u4e3a\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.05302", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05302", "abs": "https://arxiv.org/abs/2601.05302", "authors": ["Mizuki Sakai", "Mizuki Yokoyama", "Wakaba Tateishi", "Genki Ichinose"], "title": "Effects of personality steering on cooperative behavior in Large Language Model agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u91cd\u590d\u56da\u5f92\u56f0\u5883\u6e38\u620f\u63a2\u7d22\u4eba\u683c\u64cd\u63a7\u5bf9LLM\u667a\u80fd\u4f53\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b9c\u4eba\u6027\u662f\u4fc3\u8fdb\u5408\u4f5c\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u4eba\u683c\u64cd\u63a7\u66f4\u591a\u662f\u884c\u4e3a\u504f\u5dee\u800c\u975e\u786e\u5b9a\u6027\u63a7\u5236\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u7814\u7a76\u8868\u660e\u4e3aLLM\u5206\u914d\u4eba\u683c\u7279\u8d28\u53ef\u4ee5\u5f71\u54cd\u5176\u884c\u4e3a\uff0c\u4f46\u4eba\u683c\u64cd\u63a7\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u5982\u4f55\u5f71\u54cd\u5408\u4f5c\u884c\u4e3a\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4eba\u683c\u7279\u8d28\u5bf9LLM\u667a\u80fd\u4f53\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u4f7f\u7528\u91cd\u590d\u56da\u5f92\u56f0\u5883\u6e38\u620f\uff0c\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u6846\u67b6\uff0c\u9996\u5148\u6d4b\u91cfGPT-3.5-turbo\u3001GPT-4o\u548cGPT-5\u7684\u57fa\u672c\u4eba\u683c\u7279\u5f81\uff0c\u7136\u540e\u6bd4\u8f83\u57fa\u7ebf\u548c\u4eba\u683c\u64cd\u63a7\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u5355\u72ec\u64cd\u7eb5\u6bcf\u4e2a\u4eba\u683c\u7ef4\u5ea6\u5230\u6781\u7aef\u503c\u7684\u5f71\u54cd\u3002", "result": "\u5b9c\u4eba\u6027\u662f\u6240\u6709\u6a21\u578b\u4e2d\u4fc3\u8fdb\u5408\u4f5c\u7684\u4e3b\u5bfc\u56e0\u7d20\uff0c\u5176\u4ed6\u4eba\u683c\u7279\u8d28\u5f71\u54cd\u6709\u9650\u3002\u660e\u786e\u7684\u4eba\u683c\u4fe1\u606f\u4f1a\u589e\u52a0\u5408\u4f5c\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u88ab\u5229\u7528\u7684\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u6a21\u578b\u4e2d\u3002\u540e\u671f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u6709\u9009\u62e9\u6027\u7684\u5408\u4f5c\u884c\u4e3a\u3002", "conclusion": "\u4eba\u683c\u64cd\u63a7\u66f4\u591a\u662f\u884c\u4e3a\u504f\u5dee\u800c\u975e\u786e\u5b9a\u6027\u63a7\u5236\u673a\u5236\uff0c\u5b9c\u4eba\u6027\u662f\u5f71\u54cd\u5408\u4f5c\u884c\u4e3a\u7684\u5173\u952e\u4eba\u683c\u7ef4\u5ea6\uff0c\u4e0d\u540c\u4ee3\u9645\u6a21\u578b\u5bf9\u4eba\u683c\u64cd\u63a7\u7684\u53cd\u5e94\u5b58\u5728\u5dee\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2601.05467", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05467", "abs": "https://arxiv.org/abs/2601.05467", "authors": ["Swapnil Shinde", "Sahil Wadhwa", "Andy Luo", "Emily Chen"], "title": "STELP: Secure Transpilation and Execution of LLM-Generated Programs", "comment": null, "summary": "Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.", "AI": {"tldr": "STELP\u662f\u4e00\u4e2a\u5b89\u5168\u8f6c\u8bd1\u5668\u548c\u6267\u884c\u5668\uff0c\u7528\u4e8e\u5b89\u5168\u6267\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\uff0c\u89e3\u51b3\u751f\u4ea7AI\u7cfb\u7edf\u4e2d\u4ee3\u7801\u751f\u6210\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u5b58\u5728\u4e0d\u7a33\u5b9a\u3001\u9519\u8bef\u3001\u6f0f\u6d1e\uff08\u5982\u6570\u636e\u4e2d\u6bd2\u3001\u6076\u610f\u653b\u51fb\u3001\u5e7b\u89c9\uff09\u7b49\u95ee\u9898\uff0c\u5728\u751f\u4ea7AI\u7cfb\u7edf\u4e2d\u76f4\u63a5\u4f7f\u7528\u8fd9\u4e9b\u4ee3\u7801\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u6545\u969c\u3002\u4f20\u7edf\u5b89\u5168\u6d4b\u8bd5\u65b9\u6cd5\u548c\u4eba\u5de5\u4ee3\u7801\u5ba1\u67e5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4e0d\u5207\u5b9e\u9645\u6216\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faSTELP\uff08Secure Transpiler and Executor of LLM-Generated Program\uff09\uff0c\u80fd\u591f\u5728\u53d7\u63a7\u548c\u5b89\u5168\u7684\u73af\u5883\u4e2d\u6267\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\u3002\u5305\u62ec\u65e0\u5934\u4ee3\u7801\u751f\u6210\u6267\u884c\u548cLLM\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u7247\u6bb5\u4f5c\u4e3a\u5b9e\u65f6\u6267\u884c\u7684\u52a8\u4f5c\u8ba1\u5212\u7b49\u5e94\u7528\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5bf9\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u5ef6\u8fdf\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u6267\u884c\u98ce\u9669\u4ee3\u7801\u7247\u6bb5\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u8d21\u732e\u4e86\u4e00\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u4e0d\u5b89\u5168\u4ee3\u7801\u7247\u6bb5\u6570\u636e\u96c6\u3002", "conclusion": "STELP\u586b\u8865\u4e86\u4f20\u7edf\u5b89\u5168\u6d4b\u8bd5\u65b9\u6cd5\u548c\u4eba\u5de5\u76d1\u7763\u5728\u751f\u4ea7AI\u7cfb\u7edf\u4e2d\u7684\u7a7a\u767d\uff0c\u80fd\u591f\u5b89\u5168\u6267\u884cLLM\u751f\u6210\u7684\u4ee3\u7801\uff0c\u4e3a\u6d89\u53ca\u4ee3\u7801\u751f\u6210\u7684\u81ea\u4e3b\u751f\u4ea7AI\u7cfb\u7edf\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\u3002", "topic": "swe application"}}
{"id": "2601.05485", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05485", "abs": "https://arxiv.org/abs/2601.05485", "authors": ["Wenhao Zeng", "Yitian Chai", "Hao Zhou", "Fandong Meng", "Jie Zhou", "Xiaodong Gu"], "title": "Readability-Robust Code Summarization via Meta Curriculum Learning", "comment": "Code available at https://github.com/Zengwh02/RoFTCodeSum", "summary": "Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.", "AI": {"tldr": "\u63d0\u51faRoFTCodeSum\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u7ed3\u5408\u589e\u5f3a\u4ee3\u7801\u6458\u8981\u6a21\u578b\u5bf9\u4f4e\u53ef\u8bfb\u6027\u4ee3\u7801\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u4ee3\u7801\u6458\u8981\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9650\u4e8e\u9ad8\u53ef\u8bfb\u6027\u4ee3\u7801\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u5f80\u5f80\u7ed3\u6784\u4e0d\u826f\u6216\u6df7\u6dc6\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d", "method": "\u63d0\u51faRoFTCodeSum\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u5143\u5b66\u4e60\uff1a\u57fa\u4e8e\u539f\u59cb\u5fae\u8c03\u6570\u636e\u96c6\u521b\u5efa\u6e10\u8fdb\u96be\u5ea6\u7684\u8bfe\u7a0b\u8bad\u7ec3\u96c6\uff08\u5982\u6df7\u6dc6\u51fd\u6570\u540d\u548c\u6807\u8bc6\u7b26\uff09\uff0c\u5728\u6bcf\u4e00\u6b65\u8bad\u7ec3\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u6e10\u8fdb\u6311\u6218\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5143\u68af\u5ea6\u66f4\u65b0", "result": "\u5b9e\u9a8c\u8868\u660eRoFTCodeSum\u5728\u589e\u5f3a\u5bf9\u539f\u59cb\u4ee3\u7801\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u5bf9\u8bed\u4e49\u6270\u52a8\u7684\u9c81\u68d2\u6027", "conclusion": "RoFTCodeSum\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u6458\u8981\u6a21\u578b\u5bf9\u4f4e\u53ef\u8bfb\u6027\u4ee3\u7801\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u8d28\u91cf\u95ee\u9898", "topic": "code agent"}}
{"id": "2601.05376", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05376", "abs": "https://arxiv.org/abs/2601.05376", "authors": ["Tassallah Abdullahi", "Shrestha Ghosh", "Hamish S Fraser", "Daniel Le\u00f3n Tramontini", "Adeel Abbasi", "Ghada Bourjeily", "Carsten Eickhoff", "Ritambhara Singh"], "title": "The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models", "comment": null, "summary": "Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\\sim+20\\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $\u03ba= 0.43$) but indicate a low confidence in 95.9\\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\\_Paradox.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\uff0cLLM\u7684\u89d2\u8272\u8bbe\u5b9a\uff08\u5982\u6025\u8bca\u533b\u751f\u3001\u62a4\u58eb\uff09\u4f1a\u4ea7\u751f\u7cfb\u7edf\u6027\u3001\u60c5\u5883\u4f9d\u8d56\u4e14\u975e\u5355\u8c03\u7684\u5f71\u54cd\uff0c\u533b\u7597\u89d2\u8272\u5728\u91cd\u75c7\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\uff08\u51c6\u786e\u7387+20%\uff09\uff0c\u4f46\u5728\u521d\u7ea7\u62a4\u7406\u4e2d\u964d\u4f4e\u6027\u80fd\uff0c\u5b89\u5168\u6027\u548c\u4e13\u4e1a\u6027\u5e76\u975e\u5355\u8c03\u4fdd\u8bc1\u3002", "motivation": "\u89d2\u8272\u8bbe\u5b9a\u901a\u5e38\u88ab\u89c6\u4e3aLLM\u7684\u884c\u4e3a\u5148\u9a8c\uff0c\u88ab\u8ba4\u4e3a\u80fd\u8d4b\u4e88\u4e13\u4e1a\u77e5\u8bc6\u548c\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u4f46\u5176\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5177\u4f53\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u89d2\u8272\u8bbe\u5b9a\u5bf9\u4e34\u5e8aLLM\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8e\u89d2\u8272\u7684\u63a7\u5236\uff0c\u8003\u5bdf\u4e13\u4e1a\u89d2\u8272\uff08\u6025\u8bca\u533b\u751f\u3001\u62a4\u58eb\u7b49\uff09\u548c\u4ea4\u4e92\u98ce\u683c\uff08\u5927\u80c6vs\u8c28\u614e\uff09\u5bf9\u4e0d\u540c\u6a21\u578b\u548c\u533b\u7597\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff08\u4efb\u52a1\u51c6\u786e\u6027\u3001\u6821\u51c6\u5ea6\u3001\u5b89\u5168\u76f8\u5173\u98ce\u9669\u884c\u4e3a\uff09\uff0c\u7ed3\u5408LLM\u5224\u65ad\u548c\u4eba\u7c7b\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u3002", "result": "\u533b\u7597\u89d2\u8272\u5728\u91cd\u75c7\u62a4\u7406\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\uff08\u51c6\u786e\u7387\u548c\u6821\u51c6\u5ea6\u63d0\u5347\u7ea620%\uff09\uff0c\u4f46\u5728\u521d\u7ea7\u62a4\u7406\u73af\u5883\u4e2d\u964d\u4f4e\u6027\u80fd\uff1b\u4ea4\u4e92\u98ce\u683c\u8c03\u8282\u98ce\u9669\u503e\u5411\u4f46\u9ad8\u5ea6\u4f9d\u8d56\u6a21\u578b\uff1bLLM\u5224\u65ad\u5728\u5b89\u5168\u5173\u952e\u60c5\u51b5\u4e0b\u504f\u597d\u533b\u7597\u89d2\u8272\uff0c\u4f46\u4eba\u7c7b\u4e34\u5e8a\u533b\u751f\u5bf9\u5b89\u5168\u5408\u89c4\u6027\u53ea\u6709\u4e2d\u7b49\u4e00\u81f4\u6027\uff08Cohen's \u03ba=0.43\uff09\uff0c\u4e1495.9%\u7684\u56de\u7b54\u4e2d\u5bf9\u63a8\u7406\u8d28\u91cf\u4fe1\u5fc3\u8f83\u4f4e\u3002", "conclusion": "\u89d2\u8272\u8bbe\u5b9a\u4f5c\u4e3a\u884c\u4e3a\u5148\u9a8c\u5f15\u5165\u60c5\u5883\u4f9d\u8d56\u7684\u6743\u8861\uff0c\u800c\u975e\u5b89\u5168\u6216\u4e13\u4e1a\u6027\u7684\u4fdd\u8bc1\uff0c\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u9700\u8981\u8c28\u614e\u4f7f\u7528\uff0c\u4e0d\u80fd\u7b80\u5355\u5047\u8bbe\u89d2\u8272\u8bbe\u5b9a\u4f1a\u5355\u8c03\u63d0\u5347\u6027\u80fd\u6216\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.05384", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.05384", "abs": "https://arxiv.org/abs/2601.05384", "authors": ["Alessandro Bellina", "Giordano De Marzo", "David Garcia"], "title": "Conformity and Social Impact on AI Agents", "comment": null, "summary": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aAI\u4ee3\u7406\u65f6\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u4ece\u4f17\u504f\u5dee\uff0c\u7b26\u5408\u793e\u4f1a\u5f71\u54cd\u7406\u8bba\uff0c\u5373\u4f7f\u5355\u72ec\u8868\u73b0\u4f18\u5f02\u7684\u6a21\u578b\u5728\u7fa4\u4f53\u538b\u529b\u4e0b\u4e5f\u6613\u53d7\u64cd\u7eb5\uff0c\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u65e5\u76ca\u589e\u591a\uff0c\u7406\u89e3\u5176\u96c6\u4f53\u884c\u4e3a\u5bf9\u4e8e\u9884\u6d4b\u4eba\u5de5\u793e\u4f1a\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22AI\u4ee3\u7406\u5728\u793e\u4f1a\u538b\u529b\u4e0b\u7684\u4ece\u4f17\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u8106\u5f31\u6027\u3002", "method": "\u901a\u8fc7\u6539\u7f16\u793e\u4f1a\u5fc3\u7406\u5b66\u4e2d\u7684\u7ecf\u5178\u89c6\u89c9\u5b9e\u9a8c\uff0c\u7814\u7a76AI\u4ee3\u7406\u5982\u4f55\u4f5c\u4e3a\u793e\u4f1a\u884c\u52a8\u8005\u54cd\u5e94\u7fa4\u4f53\u5f71\u54cd\u3002\u5b9e\u9a8c\u8003\u5bdf\u4e86\u7fa4\u4f53\u89c4\u6a21\u3001\u4e00\u81f4\u6027\u3001\u4efb\u52a1\u96be\u5ea6\u548c\u6765\u6e90\u7279\u5f81\u7b49\u56e0\u7d20\u5bf9\u4ece\u4f17\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "AI\u4ee3\u7406\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u4ece\u4f17\u504f\u5dee\uff0c\u7b26\u5408\u793e\u4f1a\u5f71\u54cd\u7406\u8bba\u3002\u5355\u72ec\u8868\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684AI\u4ee3\u7406\u5728\u7fa4\u4f53\u5f71\u54cd\u4e0b\u53d8\u5f97\u9ad8\u5ea6\u6613\u53d7\u64cd\u7eb5\u3002\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u4ece\u4f17\u6027\u8d8a\u4f4e\uff0c\u4f46\u5728\u80fd\u529b\u8fb9\u754c\u5904\u4ecd\u4fdd\u6301\u8106\u5f31\u6027\u3002", "conclusion": "AI\u4ee3\u7406\u51b3\u7b56\u4e2d\u5b58\u5728\u57fa\u672c\u5b89\u5168\u6f0f\u6d1e\uff0c\u53ef\u80fd\u88ab\u7528\u4e8e\u6076\u610f\u64cd\u7eb5\u3001\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u504f\u89c1\u4f20\u64ad\uff0c\u7a81\u663e\u4e86\u5728\u96c6\u4f53AI\u90e8\u7f72\u4e2d\u8feb\u5207\u9700\u8981\u5b89\u5168\u4fdd\u969c\u63aa\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2601.05407", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05407", "abs": "https://arxiv.org/abs/2601.05407", "authors": ["Minwoo Cho", "Batuhan Altundas", "Matthew Gombolay"], "title": "Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning", "comment": null, "summary": "Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.", "AI": {"tldr": "HINT\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u4ea4\u4e92\u6559\u5e08\u673a\u5236\u89e3\u51b3\u4f20\u7edfKD\u5728MARL\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u5728MARL\u4e2d\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a1) \u590d\u6742\u9886\u57df\u4e2d\u9ad8\u6027\u80fd\u6559\u5b66\u7b56\u7565\u5408\u6210\u7684\u6311\u6218\uff1b2) \u6559\u5e08\u9700\u8981\u5728\u5206\u5e03\u5916\u72b6\u6001\u8fdb\u884c\u63a8\u7406\u7684\u56f0\u96be\uff1b3) \u5206\u6563\u5f0f\u5b66\u751f\u4e0e\u96c6\u4e2d\u5f0f\u6559\u5e08\u89c2\u5bdf\u7a7a\u95f4\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "HINT\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6784\u5efa\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u6559\u5e08\u6a21\u578b\uff0c\u5173\u952e\u521b\u65b0\u662f\u4f2a\u79bb\u7b56\u7565RL\uff0c\u5141\u8bb8\u6559\u5e08\u7b56\u7565\u540c\u65f6\u4f7f\u7528\u6559\u5e08\u548c\u5b66\u751f\u7ecf\u9a8c\u8fdb\u884c\u66f4\u65b0\u4ee5\u6539\u5584OOD\u9002\u5e94\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u6027\u80fd\u7684\u8fc7\u6ee4\u6765\u4fdd\u7559\u4ec5\u4e0e\u7ed3\u679c\u76f8\u5173\u7684\u6307\u5bfc\u3002", "result": "\u5728FireCommander\u8d44\u6e90\u5206\u914d\u548cMARINE\u6218\u672f\u6218\u6597\u7b49\u6311\u6218\u6027\u5408\u4f5c\u9886\u57df\u8bc4\u4f30\u4e2d\uff0cHINT\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u534760%\u5230165%\u3002", "conclusion": "HINT\u901a\u8fc7\u5206\u5c42\u4ea4\u4e92\u6559\u5e08\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MARL\u4e2d\u77e5\u8bc6\u84b8\u998f\u7684\u5173\u952e\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05539", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05539", "abs": "https://arxiv.org/abs/2601.05539", "authors": ["Gou Tan", "Zilong He", "Min Li", "Pengfei Chen", "Jieke Shi", "Zhensu Sun", "Ting Zhang", "Danwen Chen", "Lwin Khin Shar", "Chuanfu Zhang", "David Lo"], "title": "LIDL: LLM Integration Defect Localization via Knowledge Graph-Enhanced Multi-Agent Analysis", "comment": null, "summary": "LLM-integrated software, which embeds or interacts with large language models (LLMs) as functional components, exhibits probabilistic and context-dependent behaviors that fundamentally differ from those of traditional software. This shift introduces a new category of integration defects that arise not only from code errors but also from misaligned interactions among LLM-specific artifacts, including prompts, API calls, configurations, and model outputs. However, existing defect localization techniques are ineffective at identifying these LLM-specific integration defects because they fail to capture cross-layer dependencies across heterogeneous artifacts, cannot exploit incomplete or misleading error traces, and lack semantic reasoning capabilities for identifying root causes.\n  To address these challenges, we propose LIDL, a multi-agent framework for defect localization in LLM-integrated software. LIDL (1) constructs a code knowledge graph enriched with LLM-aware annotations that represent interaction boundaries across source code, prompts, and configuration files, (2) fuses three complementary sources of error evidence inferred by LLMs to surface candidate defect locations, and (3) applies context-aware validation that uses counterfactual reasoning to distinguish true root causes from propagated symptoms. We evaluate LIDL on 146 real-world defect instances collected from 105 GitHub repositories and 16 agent-based systems. The results show that LIDL significantly outperforms five state-of-the-art baselines across all metrics, achieving a Top-3 accuracy of 0.64 and a MAP of 0.48, which represents a 64.1% improvement over the best-performing baseline. Notably, LIDL achieves these gains while reducing cost by 92.5%, demonstrating both high accuracy and cost efficiency.", "AI": {"tldr": "LIDL\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9a\u4f4dLLM\u96c6\u6210\u8f6f\u4ef6\u7f3a\u9677\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4ee3\u7801\u77e5\u8bc6\u56fe\u8c31\u3001\u878d\u5408\u9519\u8bef\u8bc1\u636e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u9a8c\u8bc1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "LLM\u96c6\u6210\u8f6f\u4ef6\u5177\u6709\u6982\u7387\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u884c\u4e3a\uff0c\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u4e0d\u540c\uff0c\u5bfc\u81f4\u65b0\u7684\u96c6\u6210\u7f3a\u9677\u7c7b\u578b\u3002\u73b0\u6709\u7f3a\u9677\u5b9a\u4f4d\u6280\u672f\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u8fd9\u4e9bLLM\u7279\u5b9a\u96c6\u6210\u7f3a\u9677\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u6355\u6349\u5f02\u6784\u5de5\u4ef6\u95f4\u7684\u8de8\u5c42\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e0d\u80fd\u5229\u7528\u4e0d\u5b8c\u6574\u6216\u8bef\u5bfc\u6027\u7684\u9519\u8bef\u8ddf\u8e2a\uff0c\u4e5f\u7f3a\u4e4f\u8bc6\u522b\u6839\u672c\u539f\u56e0\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002", "method": "LIDL\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a(1)\u6784\u5efa\u5e26\u6709LLM\u611f\u77e5\u6ce8\u91ca\u7684\u4ee3\u7801\u77e5\u8bc6\u56fe\u8c31\uff0c\u8868\u793a\u6e90\u4ee3\u7801\u3001\u63d0\u793a\u8bcd\u548c\u914d\u7f6e\u6587\u4ef6\u4e4b\u95f4\u7684\u4ea4\u4e92\u8fb9\u754c\uff1b(2)\u878d\u5408LLM\u63a8\u65ad\u7684\u4e09\u79cd\u4e92\u8865\u9519\u8bef\u8bc1\u636e\u6765\u6e90\uff0c\u7b5b\u9009\u5019\u9009\u7f3a\u9677\u4f4d\u7f6e\uff1b(3)\u5e94\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u9a8c\u8bc1\uff0c\u4f7f\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u533a\u5206\u771f\u6b63\u7684\u6839\u672c\u539f\u56e0\u548c\u4f20\u64ad\u7684\u75c7\u72b6\u3002", "result": "\u5728146\u4e2a\u771f\u5b9e\u4e16\u754c\u7f3a\u9677\u5b9e\u4f8b\uff08\u6765\u81ea105\u4e2aGitHub\u4ed3\u5e93\u548c16\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\uff09\u4e0a\u8bc4\u4f30\uff0cLIDL\u5728\u5404\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e5\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cTop-3\u51c6\u786e\u7387\u8fbe\u52300.64\uff0cMAP\u4e3a0.48\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534764.1%\u3002\u540c\u65f6\u6210\u672c\u964d\u4f4e92.5%\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u7387\u3002", "conclusion": "LIDL\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u96c6\u6210\u8f6f\u4ef6\u4e2d\u7684\u7f3a\u9677\u5b9a\u4f4d\u95ee\u9898\uff0c\u80fd\u591f\u6355\u6349\u8de8\u5c42\u4f9d\u8d56\u5173\u7cfb\uff0c\u5229\u7528\u4e0d\u5b8c\u6574\u7684\u9519\u8bef\u4fe1\u606f\uff0c\u5e76\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\uff0c\u4e3aLLM\u96c6\u6210\u8f6f\u4ef6\u7684\u8c03\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "2601.05420", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.05420", "abs": "https://arxiv.org/abs/2601.05420", "authors": ["Yiqun T Chen", "Sizhu Lu", "Sijia Li", "Moran Guo", "Shengyi Li"], "title": "Efficient Inference for Noisy LLM-as-a-Judge Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as \"LLM-as-a-judge.\" In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u65f6\u7684\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\uff0c\u6bd4\u8f83\u6d4b\u91cf\u8bef\u5dee\u6821\u6b63\u548c\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\u4e24\u79cd\u65b9\u6cd5\uff0c\u63a8\u5bfc\u9ad8\u6548\u4f30\u8ba1\u5668\u5e76\u5206\u6790\u65b9\u5dee\u7279\u6027\u3002", "motivation": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5b58\u5728\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u73b0\u6709\u6821\u6b63\u65b9\u6cd5\u5305\u62ec\u6d4b\u91cf\u8bef\u5dee\u6821\u6b63\u548c\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u57fa\u4e8e\u534a\u53c2\u6570\u6548\u7387\u7406\u8bba\uff0c\u63a8\u5bfc\u9ad8\u6548\u5f71\u54cd\u51fd\u6570\u7684\u9ad8\u6548\u4f30\u8ba1\u5668\uff0c\u7edf\u4e00\u4e24\u7c7b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\u65b9\u6cd5\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u5c0f\u6e10\u8fd1\u65b9\u5dee\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u9884\u6d4b\u9a71\u52a8\u63a8\u65ad\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u5c0f\u6e10\u8fd1\u65b9\u5dee\uff0c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u793a\u4f8b\u4e2d\u5c55\u793a\u65b9\u6cd5\u5e94\u7528\u3002", "conclusion": "\u7cfb\u7edf\u6bd4\u8f83\u4e86LLM\u8bc4\u4f30\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u6765\u6821\u6b63LLM\u8bc4\u4f30\u4e2d\u7684\u7cfb\u7edf\u6027\u8bef\u5dee\u3002", "topic": "agent analysis"}}
{"id": "2601.05459", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05459", "abs": "https://arxiv.org/abs/2601.05459", "authors": ["Hongjin Kim", "Jaewook Lee", "Kiyoung Lee", "Jong-hun Shin", "Soojong Lim", "Oh-Woog Kwon"], "title": "Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction", "comment": "IJCNLP-AACL 2025 (Main), Outstanding Paper Award", "summary": "Large Language Models (LLMs) demonstrate strong reasoning and self-correction abilities in high-resource languages like English, but their performance remains limited in low-resource languages such as Korean. In this study, we investigate whether reinforcement learning (RL) can enhance Korean reasoning abilities to a degree comparable to English. Our findings reveal that RL alone yields limited improvements when applied to models lacking inherent Korean reasoning capabilities. To address this, we explore several fine-tuning strategies and show that aligning the model's internal reasoning processes with Korean inputs-particularly by tuning Korean-specific neurons in early layers-is key to unlocking RL's effectiveness. We introduce a self-correction code-switching dataset to facilitate this alignment and observe significant performance gains in both mathematical reasoning and self-correction tasks. Ultimately, we conclude that the crucial factor in multilingual reasoning enhancement is not injecting new linguistic knowledge, but effectively eliciting and aligning existing reasoning capabilities. Our study provides a new perspective on how internal translation and neuron-level tuning contribute to multilingual reasoning alignment in LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u80fd\u5426\u63d0\u5347LLM\u5728\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5173\u952e\u5728\u4e8e\u5bf9\u9f50\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0e\u97e9\u8bed\u8f93\u5165\uff0c\u800c\u975e\u6ce8\u5165\u65b0\u7684\u8bed\u8a00\u77e5\u8bc6\u3002", "motivation": "LLM\u5728\u82f1\u8bed\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u548c\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u4f46\u5728\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u6027\u80fd\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u80fd\u5426\u5c06\u97e9\u8bed\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5230\u4e0e\u82f1\u8bed\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u4e86\u591a\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u53d1\u73b0\u901a\u8fc7\u8c03\u6574\u65e9\u671f\u5c42\u4e2d\u7684\u97e9\u8bed\u7279\u5b9a\u795e\u7ecf\u5143\u6765\u5bf9\u9f50\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e0e\u97e9\u8bed\u8f93\u5165\u662f\u5173\u952e\u3002\u5f15\u5165\u4e86\u81ea\u6211\u7ea0\u6b63\u4ee3\u7801\u5207\u6362\u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u8fd9\u79cd\u5bf9\u9f50\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u5355\u72ec\u5e94\u7528\u65f6\u5bf9\u7f3a\u4e4f\u56fa\u6709\u97e9\u8bed\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u6539\u8fdb\u6709\u9650\u3002\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u8c03\u4f18\u5bf9\u9f50\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u540e\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u81ea\u6211\u7ea0\u6b63\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u591a\u8bed\u8a00\u63a8\u7406\u589e\u5f3a\u7684\u5173\u952e\u4e0d\u662f\u6ce8\u5165\u65b0\u7684\u8bed\u8a00\u77e5\u8bc6\uff0c\u800c\u662f\u6709\u6548\u6fc0\u53d1\u548c\u5bf9\u9f50\u73b0\u6709\u7684\u63a8\u7406\u80fd\u529b\u3002\u5185\u90e8\u7ffb\u8bd1\u548c\u795e\u7ecf\u5143\u7ea7\u8c03\u4f18\u5bf9LLM\u7684\u591a\u8bed\u8a00\u63a8\u7406\u5bf9\u9f50\u6709\u91cd\u8981\u8d21\u732e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05455", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05455", "abs": "https://arxiv.org/abs/2601.05455", "authors": ["Sahil Wadhwa", "Himanshu Kumar", "Guanqun Yang", "Abbaas Alif Mohamed Nishar", "Pranab Mohanty", "Swapnil Shinde", "Yue Wu"], "title": "ART: Adaptive Reasoning Trees for Explainable Claim Verification", "comment": null, "summary": "Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.", "AI": {"tldr": "ART\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u63a8\u7406\u6811\u7684\u5c42\u6b21\u5316\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u652f\u6301\u4e0e\u653b\u51fb\u8bba\u636e\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\uff0c\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u4e89\u8bae\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "LLM\u5728\u590d\u6742\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u4fe1\u89e3\u91ca\u548c\u7ea0\u9519\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u7ea0\u6b63\u9519\u8bef\uff0c\u635f\u5bb3\u4e86\u53ef\u4fe1\u5ea6\u3002", "method": "ART\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\u8fdb\u884c\u58f0\u660e\u9a8c\u8bc1\uff1a\u4ece\u6839\u58f0\u660e\u5f00\u59cb\uff0c\u5206\u652f\u4e3a\u652f\u6301\u548c\u653b\u51fb\u7684\u5b50\u8bba\u636e\u3002\u901a\u8fc7LLM\u88c1\u5224\u5bf9\u5b50\u8bba\u636e\u8fdb\u884c\u6210\u5bf9\u9526\u6807\u8d5b\u5f0f\u6bd4\u8f83\uff0c\u81ea\u5e95\u5411\u4e0a\u786e\u5b9a\u8bba\u636e\u5f3a\u5ea6\uff0c\u7cfb\u7edf\u63a8\u5bfc\u6700\u7ec8\u900f\u660e\u4e14\u53ef\u4e89\u8bae\u7684\u88c1\u51b3\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0cART\u7684\u7ed3\u6784\u5316\u63a8\u7406\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u58f0\u660e\u9a8c\u8bc1\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u5e76\u786e\u4fdd\u51b3\u7b56\u6b65\u9aa4\u7684\u6e05\u6670\u6027\u3002", "conclusion": "ART\u901a\u8fc7\u5c42\u6b21\u5316\u63a8\u7406\u6811\u548cLLM\u88c1\u5224\u673a\u5236\uff0c\u89e3\u51b3\u4e86LLM\u51b3\u7b56\u7684\u4e0d\u900f\u660e\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u6bd4Chain-of-Thought\u7b49\u65b9\u6cd5\u66f4\u900f\u660e\u3001\u53ef\u4e89\u8bae\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2601.05465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05465", "abs": "https://arxiv.org/abs/2601.05465", "authors": ["Yu Liu", "Wenxiao Zhang", "Cong Cao", "Wenxuan Lu", "Fangfang Yuan", "Diandian Guo", "Kun Peng", "Qiang Sun", "Kaiyan Zhang", "Yanbing Liu", "Jin B. Hong", "Bowen Zhou", "Zhiyuan Ma"], "title": "PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering", "comment": null, "summary": "Answering real-world open-domain multi-hop questions over massive corpora is a critical challenge in Retrieval-Augmented Generation (RAG) systems. Recent research employs reinforcement learning (RL) to end-to-end optimize the retrieval-augmented reasoning process, directly enhancing its capacity to resolve complex queries. However, reliable deployment is hindered by two obstacles. 1) Retrieval Collapse: iterative retrieval over large corpora fails to locate intermediate evidence containing bridge answers without reasoning-guided planning, causing downstream reasoning to collapse. 2) Learning Instability: end-to-end trajectory training suffers from weak credit assignment across reasoning chains and poor error localization across modules, causing overfitting to benchmark-specific heuristics that limit transferability and stability. To address these problems, we propose PRISMA, a decoupled RL-guided framework featuring a Plan-Retrieve-Inspect-Solve-Memoize architecture. PRISMA's strength lies in reasoning-guided collaboration: the Inspector provides reasoning-based feedback to refine the Planner's decomposition and fine-grained retrieval, while enforcing evidence-grounded reasoning in the Solver. We optimize individual agent capabilities via Two-Stage Group Relative Policy Optimization (GRPO). Stage I calibrates the Planner and Solver as specialized experts in planning and reasoning, while Stage II utilizes Observation-Aware Residual Policy Optimization (OARPO) to enhance the Inspector's ability to verify context and trigger targeted recovery. Experiments show that PRISMA achieves state-of-the-art performance on ten benchmarks and can be deployed efficiently in real-world scenarios.", "AI": {"tldr": "PRISMA\u662f\u4e00\u4e2a\u89e3\u8026\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212-\u68c0\u7d22-\u68c0\u67e5-\u89e3\u51b3-\u8bb0\u5fc6\u67b6\u6784\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u5d29\u6e83\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684RAG\u7cfb\u7edf\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u969c\u788d\uff1a1) \u68c0\u7d22\u5d29\u6e83 - \u5728\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u8fed\u4ee3\u68c0\u7d22\u65f6\uff0c\u7f3a\u4e4f\u63a8\u7406\u5f15\u5bfc\u7684\u89c4\u5212\u5bfc\u81f4\u65e0\u6cd5\u627e\u5230\u5305\u542b\u6865\u63a5\u7b54\u6848\u7684\u4e2d\u95f4\u8bc1\u636e\uff1b2) \u5b66\u4e60\u4e0d\u7a33\u5b9a - \u7aef\u5230\u7aef\u8f68\u8ff9\u8bad\u7ec3\u5728\u63a8\u7406\u94fe\u4e0a\u7684\u4fe1\u7528\u5206\u914d\u8584\u5f31\uff0c\u6a21\u5757\u95f4\u9519\u8bef\u5b9a\u4f4d\u5dee\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u5230\u57fa\u51c6\u7279\u5b9a\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u53ef\u8fc1\u79fb\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faPRISMA\u6846\u67b6\uff0c\u91c7\u7528\u89e3\u8026\u7684Plan-Retrieve-Inspect-Solve-Memoize\u67b6\u6784\u3002\u901a\u8fc7\u63a8\u7406\u5f15\u5bfc\u7684\u534f\u4f5c\uff1a\u68c0\u67e5\u5668\u63d0\u4f9b\u57fa\u4e8e\u63a8\u7406\u7684\u53cd\u9988\u6765\u4f18\u5316\u89c4\u5212\u5668\u7684\u5206\u89e3\u548c\u7ec6\u7c92\u5ea6\u68c0\u7d22\uff0c\u540c\u65f6\u5728\u89e3\u51b3\u5668\u4e2d\u5f3a\u5236\u6267\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u89c4\u5212\u5668\u548c\u89e3\u51b3\u5668\u6821\u51c6\u4e3a\u89c4\u5212\u548c\u63a8\u7406\u7684\u4e13\u5bb6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u89c2\u5bdf\u611f\u77e5\u6b8b\u5dee\u7b56\u7565\u4f18\u5316(OARPO)\u589e\u5f3a\u68c0\u67e5\u5668\u7684\u4e0a\u4e0b\u6587\u9a8c\u8bc1\u548c\u9488\u5bf9\u6027\u6062\u590d\u80fd\u529b\u3002", "result": "PRISMA\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "PRISMA\u901a\u8fc7\u89e3\u8026\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u63a8\u7406\u5f15\u5bfc\u7684\u534f\u4f5c\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u5d29\u6e83\u548c\u5b66\u4e60\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u90e8\u7f72\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05475", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05475", "abs": "https://arxiv.org/abs/2601.05475", "authors": ["Jiefu Ou", "Sapana Chaudhary", "Kaj Bostrom", "Nathaniel Weir", "Shuai Zhang", "Huzefa Rangwala", "George Karypis"], "title": "MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.", "AI": {"tldr": "MaxCode\u662f\u4e00\u4e2a\u57fa\u4e8e\u6700\u5927\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u4ee3\u7801\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u7684\u8fed\u4ee3\u641c\u7d22\u548c\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u6a21\u578b\u6765\u6307\u5bfcLLM\u751f\u6210\u66f4\u4f18\u5316\u7684\u4ee3\u7801\uff0c\u5728CUDA\u548cC++\u4f18\u5316\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "LLM\u5728\u901a\u7528\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4ee3\u7801\u4f18\u5316\u65b9\u9762\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u7f16\u5199\u4f18\u5316\u4ee3\u7801\uff08\u5982\u9ad8\u6027\u80fdCUDA\u5185\u6838\u548c\u7ade\u8d5b\u7ea7CPU\u4ee3\u7801\uff09\u9700\u8981\u7cfb\u7edf\u3001\u7b97\u6cd5\u548c\u7279\u5b9a\u8bed\u8a00\u7684\u4e13\u4e1a\u77e5\u8bc6\uff1b2) \u9700\u8981\u89e3\u91ca\u6027\u80fd\u6307\u6807\uff08\u5982\u8ba1\u65f6\u548c\u8bbe\u5907\u5229\u7528\u7387\uff09\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e8c\u8fdb\u5236\u6b63\u786e\u6027\u3002", "method": "MaxCode\u91c7\u7528\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6267\u884c\u53cd\u9988\u6307\u5bfcLLM\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53d1\u73b0\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u5c06\u73b0\u6709\u641c\u7d22\u65b9\u6cd5\u7edf\u4e00\u5230\u6700\u5927\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u4f7f\u89c2\u5bdf\u548c\u52a8\u4f5c\u4ef7\u503c\u51fd\u6570\u6a21\u5757\u5316\u3002\u901a\u8fc7\u96c6\u6210\u81ea\u7136\u8bed\u8a00\u6279\u8bc4\u6a21\u578b\u5c06\u539f\u59cb\u6267\u884c\u53cd\u9988\u8f6c\u6362\u4e3a\u8bca\u65ad\u89c1\u89e3\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u5f0f\u5956\u52b1\u5230\u76ee\u6807\u6a21\u578b\u6765\u91cd\u65b0\u6392\u5e8f\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728KernelBench\uff08CUDA\uff09\u548cPIE\uff08C++\uff09\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaxCode\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u7edd\u5bf9\u52a0\u901f\u503c\u548c\u76f8\u5bf9\u52a0\u901f\u6392\u540d\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8620.3%\u548c10.1%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "MaxCode\u901a\u8fc7\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u589e\u5f3a\u7684\u89c2\u5bdf\u7a7a\u95f4\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u4ee3\u7801\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u4ee3\u7801\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2601.05520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05520", "abs": "https://arxiv.org/abs/2601.05520", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "title": "CHisAgent: A Multi-Agent Framework for Event Taxonomy Construction in Ancient Chinese Cultural Systems", "comment": "22 pages, 13 figures, 7 tables", "summary": "Despite strong performance on many tasks, large language models (LLMs) show limited ability in historical and cultural reasoning, particularly in non-English contexts such as Chinese history. Taxonomic structures offer an effective mechanism to organize historical knowledge and improve understanding. However, manual taxonomy construction is costly and difficult to scale. Therefore, we propose \\textbf{CHisAgent}, a multi-agent LLM framework for historical taxonomy construction in ancient Chinese contexts. CHisAgent decomposes taxonomy construction into three role-specialized stages: a bottom-up \\textit{Inducer} that derives an initial hierarchy from raw historical corpora, a top-down \\textit{Expander} that introduces missing intermediate concepts using LLM world knowledge, and an evidence-guided \\textit{Enricher} that integrates external structured historical resources to ensure faithfulness. Using the \\textit{Twenty-Four Histories}, we construct a large-scale, domain-aware event taxonomy covering politics, military, diplomacy, and social life in ancient China. Extensive reference-free and reference-based evaluations demonstrate improved structural coherence and coverage, while further analysis shows that the resulting taxonomy supports cross-cultural alignment.", "AI": {"tldr": "CHisAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u4e2d\u56fd\u53e4\u4ee3\u5386\u53f2\u4e8b\u4ef6\u5206\u7c7b\u6cd5\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u89d2\u8272\u4e13\u95e8\u5316\u9636\u6bb5\u4ece\u300a\u4e8c\u5341\u56db\u53f2\u300b\u7b49\u539f\u59cb\u5386\u53f2\u8bed\u6599\u4e2d\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u3001\u9886\u57df\u611f\u77e5\u7684\u5206\u7c7b\u7ed3\u6784\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u5386\u53f2\u548c\u6587\u5316\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u4e2d\u6587\u5386\u53f2\u7b49\u975e\u82f1\u8bed\u8bed\u5883\u4e2d\u3002\u5206\u7c7b\u7ed3\u6784\u662f\u7ec4\u7ec7\u5386\u53f2\u77e5\u8bc6\u3001\u63d0\u9ad8\u7406\u89e3\u7684\u6709\u6548\u673a\u5236\uff0c\u4f46\u624b\u52a8\u6784\u5efa\u5206\u7c7b\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "CHisAgent\u5c06\u5206\u7c7b\u6cd5\u6784\u5efa\u5206\u89e3\u4e3a\u4e09\u4e2a\u89d2\u8272\u4e13\u95e8\u5316\u9636\u6bb5\uff1a1\uff09\u81ea\u5e95\u5411\u4e0a\u7684\u5f52\u7eb3\u5668\uff08Inducer\uff09\u4ece\u539f\u59cb\u5386\u53f2\u8bed\u6599\u4e2d\u63a8\u5bfc\u521d\u59cb\u5c42\u6b21\u7ed3\u6784\uff1b2\uff09\u81ea\u9876\u5411\u4e0b\u7684\u6269\u5c55\u5668\uff08Expander\uff09\u5229\u7528LLM\u7684\u4e16\u754c\u77e5\u8bc6\u5f15\u5165\u7f3a\u5931\u7684\u4e2d\u95f4\u6982\u5ff5\uff1b3\uff09\u8bc1\u636e\u5f15\u5bfc\u7684\u4e30\u5bcc\u5668\uff08Enricher\uff09\u6574\u5408\u5916\u90e8\u7ed3\u6784\u5316\u5386\u53f2\u8d44\u6e90\u4ee5\u786e\u4fdd\u5fe0\u5b9e\u6027\u3002", "result": "\u4f7f\u7528\u300a\u4e8c\u5341\u56db\u53f2\u300b\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u3001\u9886\u57df\u611f\u77e5\u7684\u4e2d\u56fd\u53e4\u4ee3\u4e8b\u4ef6\u5206\u7c7b\u6cd5\uff0c\u6db5\u76d6\u653f\u6cbb\u3001\u519b\u4e8b\u3001\u5916\u4ea4\u548c\u793e\u4f1a\u751f\u6d3b\u3002\u5e7f\u6cdb\u7684\u53c2\u8003\u65e0\u5173\u548c\u57fa\u4e8e\u53c2\u8003\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u8986\u76d6\u8303\u56f4\u65b9\u9762\u90fd\u6709\u6539\u8fdb\uff0c\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\u751f\u6210\u7684\u5206\u7c7b\u6cd5\u652f\u6301\u8de8\u6587\u5316\u5bf9\u9f50\u3002", "conclusion": "CHisAgent\u6846\u67b6\u80fd\u591f\u6709\u6548\u6784\u5efa\u4e2d\u56fd\u53e4\u4ee3\u5386\u53f2\u4e8b\u4ef6\u5206\u7c7b\u6cd5\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5386\u53f2\u548c\u6587\u5316\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u7684\u5206\u7c7b\u6cd5\u6784\u5efa\uff0c\u4e3a\u8de8\u6587\u5316\u5bf9\u9f50\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2601.05529", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.05529", "abs": "https://arxiv.org/abs/2601.05529", "authors": ["Jua Han", "Jaeyoon Seo", "Jungbin Min", "Jean Oh", "Jihie Kim"], "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making", "comment": null, "summary": "One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u706b\u707e\u758f\u6563\u573a\u666f\u8bc4\u4f30LLM\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f99%\u51c6\u786e\u7387\u4ecd\u5b58\u5728\u707e\u96be\u6027\u98ce\u9669\uff0c\u5f53\u524dLLM\u4e0d\u9002\u5408\u76f4\u63a5\u90e8\u7f72\u4e8e\u5b89\u5168\u5173\u952e\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740LLM\u878d\u5165\u673a\u5668\u4eba\u51b3\u7b56\uff0c\u7269\u7406\u98ce\u9669\u589e\u52a0\uff0c\u5355\u4e2a\u9519\u8bef\u6307\u4ee4\u53ef\u80fd\u76f4\u63a5\u5371\u53ca\u4eba\u7c7b\u5b89\u5168\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u9519\u8bef\u53ef\u80fd\u9020\u6210\u707e\u96be\u6027\u540e\u679c\u7684\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u706b\u707e\u758f\u6563\u573a\u666f\u7684\u5b9a\u6027\u8bc4\u4f30\u8bc6\u522b\u5173\u952e\u5931\u8d25\u6848\u4f8b\uff0c\u8bbe\u8ba17\u4e2a\u5b9a\u91cf\u8bc4\u4f30\u4efb\u52a1\uff1a\u5b8c\u6574\u4fe1\u606f\u4efb\u52a1\uff08\u4f7f\u7528ASCII\u5730\u56fe\uff09\u3001\u4e0d\u5b8c\u6574\u4fe1\u606f\u4efb\u52a1\uff08\u9700\u8981\u63a8\u65ad\u7f3a\u5931\u4e0a\u4e0b\u6587\uff09\u3001\u5b89\u5168\u5bfc\u5411\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff08SOSR\uff09\u3002\u5bf9\u591a\u79cdLLM\u548cVLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e25\u91cd\u6f0f\u6d1e\uff1a\u591a\u4e2a\u6a21\u578b\u5728ASCII\u5bfc\u822a\u4e2d\u6210\u529f\u7387\u4e3a0%\uff1b\u5728\u6a21\u62df\u6d88\u9632\u6f14\u4e60\u4e2d\uff0c\u6a21\u578b\u6307\u793a\u673a\u5668\u4eba\u5411\u5371\u9669\u533a\u57df\u800c\u975e\u7d27\u6025\u51fa\u53e3\u79fb\u52a8\u30021%\u5931\u8d25\u7387\u5728\u673a\u5668\u4eba\u9886\u57df\u610f\u5473\u7740\u6bcf\u767e\u6b21\u6267\u884c\u53ef\u80fd\u9020\u6210\u707e\u96be\u6027\u4f24\u5bb3\u3002", "conclusion": "\u5f53\u524dLLM\u4e0d\u9002\u5408\u76f4\u63a5\u90e8\u7f72\u4e8e\u5b89\u5168\u5173\u952e\u7cfb\u7edf\uff0c99%\u51c6\u786e\u7387\u5177\u6709\u8bef\u5bfc\u6027\u4e14\u5371\u9669\u3002\u5373\u4f7f\u6700\u5148\u8fdb\u6a21\u578b\u4e5f\u65e0\u6cd5\u4fdd\u8bc1\u5b89\u5168\uff0c\u7edd\u5bf9\u4f9d\u8d56\u5b83\u4eec\u4f1a\u5e26\u6765\u4e0d\u53ef\u63a5\u53d7\u7684\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "2601.05567", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05567", "abs": "https://arxiv.org/abs/2601.05567", "authors": ["Tengxiao Liu", "Deepak Nathani", "Zekun Li", "Kevin Yang", "William Yang Wang"], "title": "WildSci: Advancing Scientific Reasoning from In-the-Wild Literature", "comment": null, "summary": "Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.", "AI": {"tldr": "WildSci\u662f\u4e00\u4e2a\u4ece\u540c\u884c\u8bc4\u5ba1\u6587\u732e\u81ea\u52a8\u5408\u6210\u7684\u9886\u57df\u7279\u5b9a\u79d1\u5b66\u95ee\u9898\u6570\u636e\u96c6\uff0c\u6db5\u76d69\u4e2a\u79d1\u5b66\u5b66\u79d1\u548c26\u4e2a\u5b50\u9886\u57df\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u8f6c\u5316\u4e3a\u591a\u9879\u9009\u62e9\u9898\u683c\u5f0f\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u6570\u636e\u4e30\u5bcc\u3001\u8bc4\u4f30\u6307\u6807\u660e\u786e\u7684\u9886\u57df\uff0c\u800c\u5728\u533b\u5b66\u3001\u6750\u6599\u79d1\u5b66\u7b49\u79d1\u5b66\u9886\u57df\u7684\u8fdb\u5c55\u6709\u9650\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u8db3\u548c\u5f00\u653e\u79d1\u5b66\u95ee\u9898\u7684\u590d\u6742\u6027\u3002", "method": "1. \u4ece\u540c\u884c\u8bc4\u5ba1\u6587\u732e\u81ea\u52a8\u5408\u6210\u9886\u57df\u7279\u5b9a\u79d1\u5b66\u95ee\u9898\u6570\u636e\u96c6WildSci\uff1b2. \u5c06\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u8f6c\u5316\u4e3a\u591a\u9879\u9009\u62e9\u9898\u683c\u5f0f\uff1b3. \u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff1b4. \u5206\u6790\u8bad\u7ec3\u52a8\u6001\uff0c\u5305\u62ec\u9886\u57df\u7279\u5b9a\u6027\u80fd\u53d8\u5316\u3001\u54cd\u5e94\u884c\u4e3a\u548c\u6cdb\u5316\u8d8b\u52bf\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002WildSci\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u652f\u6301\u79d1\u5b66\u63a8\u7406\u7814\u7a76\u7684\u53ef\u6269\u5c55\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "conclusion": "WildSci\u901a\u8fc7\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u79d1\u5b66\u63a8\u7406\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u9886\u57dfLLM\u63a8\u7406\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u79d1\u5b66\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05570", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05570", "abs": "https://arxiv.org/abs/2601.05570", "authors": ["Cooper Lin", "Maohao Ran", "Yanting Zhang", "Zhenglin Wan", "Hongwei Fan", "Yibo Xu", "Yike Guo", "Wei Xue", "Jun Song"], "title": "Crisis-Bench: Benchmarking Strategic Ambiguity and Reputation Management in Large Language Models", "comment": null, "summary": "Standard safety alignment optimizes Large Language Models (LLMs) for universal helpfulness and honesty, effectively instilling a rigid \"Boy Scout\" morality. While robust for general-purpose assistants, this one-size-fits-all ethical framework imposes a \"transparency tax\" on professional domains requiring strategic ambiguity and information withholding, such as public relations, negotiation, and crisis management. To measure this gap between general safety and professional utility, we introduce Crisis-Bench, a multi-agent Partially Observable Markov Decision Process (POMDP) that evaluates LLMs in high-stakes corporate crises. Spanning 80 diverse storylines across 8 industries, Crisis-Bench tasks an LLM-based Public Relations (PR) Agent with navigating a dynamic 7-day corporate crisis simulation while managing strictly separated Private and Public narrative states to enforce rigorous information asymmetry. Unlike traditional benchmarks that rely on static ground truths, we introduce the Adjudicator-Market Loop: a novel evaluation metric where public sentiment is adjudicated and translated into a simulated stock price, creating a realistic economic incentive structure. Our results expose a critical dichotomy: while some models capitulate to ethical concerns, others demonstrate the capacity for Machiavellian, legitimate strategic withholding in order to stabilize the simulated stock price. Crisis-Bench provides the first quantitative framework for assessing \"Reputation Management\" capabilities, arguing for a shift from rigid moral absolutism to context-aware professional alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Crisis-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u9700\u8981\u6218\u7565\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u4fdd\u7559\u7684\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5371\u673a\u516c\u5173\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u901a\u7528\u5b89\u5168\u5bf9\u9f50\u4e0e\u4e13\u4e1a\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u6807\u51c6\u5b89\u5168\u5bf9\u9f50\u4f7fLLM\u5177\u6709\u666e\u904d\u7684\u5e2e\u52a9\u6027\u548c\u8bda\u5b9e\u6027\uff0c\u4f46\u8fd9\u79cd\"\u7ae5\u5b50\u519b\"\u9053\u5fb7\u89c2\u5728\u9700\u8981\u6218\u7565\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u4fdd\u7559\u7684\u4e13\u4e1a\u9886\u57df\uff08\u5982\u516c\u5171\u5173\u7cfb\u3001\u8c08\u5224\u3001\u5371\u673a\u7ba1\u7406\uff09\u4e2d\u4f1a\u4ea7\u751f\"\u900f\u660e\u5ea6\u7a0e\"\uff0c\u9650\u5236\u4e86\u4e13\u4e1a\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165Crisis-Bench\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\uff0c\u5305\u542b80\u4e2a\u8de88\u4e2a\u884c\u4e1a\u7684\u591a\u6837\u5316\u6545\u4e8b\u60c5\u8282\u3002\u901a\u8fc77\u5929\u52a8\u6001\u4f01\u4e1a\u5371\u673a\u6a21\u62df\uff0c\u8ba9\u57fa\u4e8eLLM\u7684\u516c\u5173\u4ee3\u7406\u7ba1\u7406\u4e25\u683c\u5206\u79bb\u7684\u79c1\u4eba\u548c\u516c\u5171\u53d9\u4e8b\u72b6\u6001\uff0c\u5f3a\u5236\u6267\u884c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3002\u91c7\u7528\u65b0\u9896\u7684\"\u4ef2\u88c1\u8005-\u5e02\u573a\u5faa\u73af\"\u8bc4\u4f30\u6307\u6807\uff0c\u5c06\u516c\u4f17\u60c5\u7eea\u8f6c\u5316\u4e3a\u6a21\u62df\u80a1\u4ef7\uff0c\u521b\u5efa\u73b0\u5b9e\u7684\u7ecf\u6d4e\u6fc0\u52b1\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u5173\u952e\u4e8c\u5206\u6cd5\uff1a\u4e00\u4e9b\u6a21\u578b\u5c48\u670d\u4e8e\u4f26\u7406\u5173\u5207\uff0c\u800c\u53e6\u4e00\u4e9b\u6a21\u578b\u5219\u8868\u73b0\u51fa\u9a6c\u57fa\u96c5\u7ef4\u5229\u5f0f\u7684\u5408\u6cd5\u6218\u7565\u4fdd\u7559\u80fd\u529b\uff0c\u4ee5\u7a33\u5b9a\u6a21\u62df\u80a1\u4ef7\u3002Crisis-Bench\u9996\u6b21\u4e3a\u8bc4\u4f30\"\u58f0\u8a89\u7ba1\u7406\"\u80fd\u529b\u63d0\u4f9b\u4e86\u91cf\u5316\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u4e3b\u5f20\u4ece\u50f5\u5316\u7684\u9053\u5fb7\u7edd\u5bf9\u4e3b\u4e49\u8f6c\u5411\u60c5\u5883\u611f\u77e5\u7684\u4e13\u4e1a\u5bf9\u9f50\uff0c\u4e3a\u9700\u8981\u6218\u7565\u6a21\u7cca\u6027\u7684\u4e13\u4e1a\u9886\u57df\u5f00\u53d1\u66f4\u9002\u7528\u7684LLM\u5bf9\u9f50\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.05578", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.05578", "abs": "https://arxiv.org/abs/2601.05578", "authors": ["Cooper Lin", "Yanting Zhang", "Maohao Ran", "Wei Xue", "Hongwei Fan", "Yibo Xu", "Zhenglin Wan", "Sirui Han", "Yike Guo", "Jun Song"], "title": "Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection", "comment": null, "summary": "E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6b3a\u8bc8\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528GSPO\u7b97\u6cd5\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u7cfb\u7edf\u5728\u771f\u5b9e\u4ea4\u6613\u6570\u636e\u4e0a\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u7684F1\u5206\u6570\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u5e73\u53f0\u548c\u652f\u4ed8\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u5546\u9762\u4e34\u65e5\u76ca\u590d\u6742\u7684\u6b3a\u8bc8\u65b9\u6848\uff0c\u4f46\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u8bba\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u5176\u5728\u771f\u5b9e\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7279\u5b9a\u9886\u57df\u7535\u5b50\u5546\u52a1\u4ea4\u6613\u6570\u636e\u65b9\u9762\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u672a\u5f97\u5230\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u4e13\u95e8\u7528\u4e8e\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\uff0c\u4ec5\u4f7f\u7528\u539f\u59cb\u4ea4\u6613\u6570\u636e\u3002\u91c7\u7528Group Sequence Policy Optimization\uff08GSPO\uff09\u7b97\u6cd5\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u5728\u4e2d\u56fd\u5168\u7403\u652f\u4ed8\u89e3\u51b3\u65b9\u6848\u516c\u53f8\u63d0\u4f9b\u7684\u771f\u5b9e\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u540e\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u5728\u4fdd\u7559\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684F1\u5206\u6570\u63d0\u5347\u3002\u6027\u80fd\u6539\u8fdb\u4e3b\u8981\u5f52\u56e0\u4e8e\u5f3a\u5316\u5b66\u4e60\u56fa\u6709\u7684\u63a2\u7d22\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u53d1\u73b0\u4f20\u7edf\u5de5\u7a0b\u7279\u5f81\u4e4b\u5916\u7684\u65b0\u6b3a\u8bc8\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u53d1\u73b0\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5ffd\u7565\u7684\u590d\u6742\u6b3a\u8bc8\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u5b9e\u9645\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05777", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05777", "abs": "https://arxiv.org/abs/2601.05777", "authors": ["Yaoqi Guo", "Ying Xiao", "Jie M. Zhang", "Mark Harman", "Yiling Lou", "Yang Liu", "Zhenpeng Chen"], "title": "EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents", "comment": null, "summary": "Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.", "AI": {"tldr": "EET\u662f\u4e00\u79cd\u7ecf\u9a8c\u9a71\u52a8\u7684\u65e9\u671f\u7ec8\u6b62\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5386\u53f2\u6267\u884c\u7ecf\u9a8c\u6765\u51cf\u5c11\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u5b9e\u8df5\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u901a\u5e38\u4f1a\u4ea7\u751f\u9ad8\u6602\u7684\u8d27\u5e01\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u964d\u4f4e\u8fd9\u4e9b\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd", "method": "\u4ece\u5148\u524d\u7684issue-resolution\u6267\u884c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u7ecf\u9a8c\uff0c\u5229\u7528\u8fd9\u4e9b\u7ecf\u9a8c\u5728\u8865\u4e01\u751f\u6210\u548c\u9009\u62e9\u9636\u6bb5\u6307\u5bfc\u65e9\u671f\u7ec8\u6b62\uff0c\u51cf\u5c11\u65e0\u6210\u6548\u7684\u8fed\u4ee3", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEET\u5c06\u603b\u6210\u672c\u964d\u4f4e\u4e8619%-55%\uff08\u5e73\u574732%\uff09\uff0c\u5206\u8fa8\u7387\u635f\u5931\u6700\u591a\u4ec50.2%\uff0c\u5e73\u5747\u4e3a11%\u7684\u95ee\u9898\u8bc6\u522b\u51fa\u65e9\u671f\u7ec8\u6b62\u673a\u4f1a", "conclusion": "EET\u80fd\u6709\u6548\u964d\u4f4e\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u901a\u8fc7\u51cf\u5c11API\u8c03\u7528\u548ctoken\u4f7f\u7528\u5b9e\u73b0\u663e\u8457\u6548\u7387\u63d0\u5347", "topic": "swe application"}}
{"id": "2601.05752", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.05752", "abs": "https://arxiv.org/abs/2601.05752", "authors": ["Shu Yang", "Jingyu Hu", "Tong Li", "Hanqi Yan", "Wenxuan Wang", "Di Wang"], "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor", "comment": null, "summary": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.", "AI": {"tldr": "AutoMonitor-Bench\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u9519\u8bef\u884c\u4e3a\u76d1\u63a7\u5668\u53ef\u9760\u6027\u7684\u57fa\u51c6\uff0c\u5305\u542b3,010\u4e2a\u6807\u6ce8\u6837\u672c\uff0c\u6db5\u76d6\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u8bc4\u4f3012\u4e2a\u4e13\u6709\u548c10\u4e2a\u5f00\u6e90LLM\u7684\u76d1\u63a7\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30LLM\u9519\u8bef\u884c\u4e3a\u76d1\u63a7\u5668\u53ef\u9760\u6027\u7684\u57fa\u51c6\uff0c\u9700\u8981\u7406\u89e3\u76d1\u63a7\u5668\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u5931\u8d25\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u53ca\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u5305\u542b3,010\u4e2a\u6807\u6ce8\u6837\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u9519\u8bef\u884c\u4e3a\u548c\u826f\u6027\u884c\u4e3a\u5b9e\u4f8b\u3002\u4f7f\u7528\u6f0f\u68c0\u7387(MR)\u548c\u8bef\u62a5\u7387(FAR)\u4e24\u4e2a\u4e92\u8865\u6307\u6807\u8bc4\u4f30\u76d1\u63a7\u5668\u6027\u80fd\u3002\u8fdb\u4e00\u6b65\u6784\u5efa153,581\u4e2a\u6837\u672c\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5fae\u8c03Qwen3-4B-Instruction\u6a21\u578b\uff0c\u7814\u7a76\u5df2\u77e5\u9519\u8bef\u884c\u4e3a\u6570\u636e\u96c6\u8bad\u7ec3\u5bf9\u672a\u89c1\u548c\u9690\u5f0f\u9519\u8bef\u884c\u4e3a\u76d1\u63a7\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u76d1\u63a7\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cMR\u548cFAR\u4e4b\u95f4\u5b58\u5728\u4e00\u81f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u56fa\u6709\u7684\u5b89\u5168-\u6548\u7528\u5f20\u529b\u3002\u5fae\u8c03\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5df2\u77e5\u3001\u76f8\u5bf9\u5bb9\u6613\u6784\u5efa\u7684\u9519\u8bef\u884c\u4e3a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5bf9\u672a\u89c1\u548c\u66f4\u9690\u5f0f\u9519\u8bef\u884c\u4e3a\u7684\u76d1\u63a7\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u9519\u8bef\u884c\u4e3a\u76d1\u63a7\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u672a\u6765\u7814\u7a76\u5173\u6ce8\u4efb\u52a1\u611f\u77e5\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u6539\u5584\u57fa\u4e8eLLM\u7684\u76d1\u63a7\u5668\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.05656", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05656", "abs": "https://arxiv.org/abs/2601.05656", "authors": ["Rongxin Chen", "Tianyu Wu", "Bingbing Xu", "Xiucheng Xu", "Huawei Shen"], "title": "HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation", "comment": null, "summary": "High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.", "AI": {"tldr": "HAG\uff1a\u4e00\u79cd\u5206\u5c42\u667a\u80fd\u4f53\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u51b3\u7b56\u8fc7\u7a0b\u5b9e\u73b0\u4e3b\u9898\u81ea\u9002\u5e94\u7684\u4eba\u53e3\u751f\u6210\uff0c\u5728\u5b8f\u89c2\u5206\u5e03\u5bf9\u9f50\u548c\u5fae\u89c2\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u521d\u59cb\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u9759\u6001\u6570\u636e\u68c0\u7d22\u7684\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u672a\u89c1\u4e3b\u9898\uff0c\u800c\u57fa\u4e8eLLM\u751f\u6210\u7684\u65b9\u6cd5\u7f3a\u4e4f\u5b8f\u89c2\u5206\u5e03\u610f\u8bc6\uff0c\u5bfc\u81f4\u5fae\u89c2\u5c5e\u6027\u4e0e\u73b0\u5b9e\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9002\u5e94\u4e3b\u9898\u53d8\u5316\u53c8\u80fd\u4fdd\u6301\u5b8f\u89c2\u5fae\u89c2\u4e00\u81f4\u6027\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faHAG\u5206\u5c42\u667a\u80fd\u4f53\u751f\u6210\u6846\u67b6\uff0c\u5c06\u4eba\u53e3\u751f\u6210\u5f62\u5f0f\u5316\u4e3a\u4e24\u9636\u6bb5\u51b3\u7b56\u8fc7\u7a0b\uff1a1\uff09\u4f7f\u7528\u4e16\u754c\u77e5\u8bc6\u6a21\u578b\u63a8\u65ad\u5206\u5c42\u6761\u4ef6\u6982\u7387\u6784\u5efa\u4e3b\u9898\u81ea\u9002\u5e94\u6811\uff0c\u5b9e\u73b0\u5b8f\u89c2\u5206\u5e03\u5bf9\u9f50\uff1b2\uff09\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8fdb\u884c\u5b9e\u4f8b\u5316\u548c\u667a\u80fd\u4f53\u589e\u5f3a\uff0c\u786e\u4fdd\u5fae\u89c2\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHAG\u663e\u8457\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u51cf\u5c11\u4eba\u53e3\u5bf9\u9f50\u8bef\u5dee37.7%\uff0c\u63d0\u5347\u793e\u4f1a\u5b66\u4e00\u81f4\u602718.8%\u3002\u5efa\u7acb\u4e86\u591a\u9886\u57df\u57fa\u51c6\u548c\u5168\u9762\u7684PACE\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "HAG\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u521d\u59cb\u5316\u4e2d\u7684\u4e3b\u9898\u9002\u5e94\u6027\u548c\u4e00\u81f4\u6027\u6311\u6218\uff0c\u4e3a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u4eba\u53e3\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.05607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05607", "abs": "https://arxiv.org/abs/2601.05607", "authors": ["Zijun Min", "Bingshuai Liu", "Ante Wang", "Long Zhang", "Anxiang Zeng", "Haibo Zhang", "Jinsong Su"], "title": "Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising framework for optimizing large language models in reasoning tasks. However, existing RLVR algorithms focus on different granularities, and each has complementary strengths and limitations. Group Relative Policy Optimization (GRPO) updates the policy with token-level importance ratios, which preserves fine-grained credit assignment but often suffers from high variance and instability. In contrast, Group Sequence Policy Optimization (GSPO) applies single sequence-level importance ratios across all tokens in a response that better matches sequence-level rewards, but sacrifices token-wise credit assignment. In this paper, we propose Dynamic Hybrid Policy Optimization (DHPO) to bridge GRPO and GSPO within a single clipped surrogate objective. DHPO combines token-level and sequence-level importance ratios using weighting mechanisms. We explore two variants of the mixing mechanism, including an averaged mixing and an entropy-guided mixing. To further stabilize training, we employ a branch-specific clipping strategy that constrains token-level and sequence-level ratios within separate trust regions before mixing, preventing outliers in either branch from dominating the update. Across seven challenging mathematical reasoning benchmarks, experiments on both dense and MoE models from the Qwen3 series show that DHPO consistently outperforms GRPO and GSPO. We will release our code upon acceptance of this paper.", "AI": {"tldr": "\u63d0\u51faDHPO\u65b9\u6cd5\uff0c\u52a8\u6001\u6df7\u5408token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709RLVR\u65b9\u6cd5", "motivation": "\u73b0\u6709RLVR\u7b97\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff1aGRPO\u4f7f\u7528token\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u80fd\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u4f46\u65b9\u5dee\u9ad8\u4e0d\u7a33\u5b9a\uff1bGSPO\u4f7f\u7528\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u5339\u914d\u5e8f\u5217\u7ea7\u5956\u52b1\u4f46\u727a\u7272token\u7ea7\u4fe1\u7528\u5206\u914d\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6df7\u5408\u7b56\u7565\u4f18\u5316(DHPO)\uff0c\u5728\u5355\u4e00\u88c1\u526a\u4ee3\u7406\u76ee\u6807\u4e2d\u6865\u63a5GRPO\u548cGSPO\u3002\u4f7f\u7528\u52a0\u6743\u673a\u5236\u7ed3\u5408token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u63a2\u7d22\u5e73\u5747\u6df7\u5408\u548c\u71b5\u5f15\u5bfc\u6df7\u5408\u4e24\u79cd\u53d8\u4f53\u3002\u91c7\u7528\u5206\u652f\u7279\u5b9a\u88c1\u526a\u7b56\u7565\uff0c\u5728\u6df7\u5408\u524d\u5206\u522b\u7ea6\u675f\u4e24\u79cd\u6bd4\u7387\u5728\u4fe1\u4efb\u533a\u57df\u5185\u3002", "result": "\u57287\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728Qwen3\u7cfb\u5217\u7684\u5bc6\u96c6\u548cMoE\u6a21\u578b\u4e0a\uff0cDHPO\u59cb\u7ec8\u4f18\u4e8eGRPO\u548cGSPO\u3002", "conclusion": "DHPO\u901a\u8fc7\u52a8\u6001\u6df7\u5408token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05633", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05633", "abs": "https://arxiv.org/abs/2601.05633", "authors": ["Nuoyan Lyu", "Bingbing Xu", "Weihao Meng", "Yige Yuan", "Yang Zhang", "Zhiyong Huang", "Tat-Seng Chua", "Huawei Shen"], "title": "GIFT: Games as Informal Training for Generalizable LLMs", "comment": null, "summary": "While Large Language Models (LLMs) have achieved remarkable success in formal learning tasks such as mathematics and code generation, they still struggle with the \"practical wisdom\" and generalizable intelligence, such as strategic creativity and social reasoning, that characterize human cognition. This gap arises from a lack of informal learning, which thrives on interactive feedback rather than goal-oriented instruction. In this paper, we propose treating Games as a primary environment for LLM informal learning, leveraging their intrinsic reward signals and abstracted complexity to cultivate diverse competencies. To address the performance degradation observed in multi-task learning, we introduce a Nested Training Framework. Unlike naive task mixing optimizing an implicit \"OR\" objective, our framework employs sequential task composition to enforce an explicit \"AND\" objective, compelling the model to master multiple abilities simultaneously to achieve maximal rewards. Using GRPO-based reinforcement learning across Matrix Games, TicTacToe, and Who's the Spy games, we demonstrate that integrating game-based informal learning not only prevents task interference but also significantly bolsters the model's generalization across broad ability-oriented benchmarks. The framework and implementation are publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u6e38\u620f\u4f5c\u4e3aLLM\u975e\u6b63\u5f0f\u5b66\u4e60\u7684\u4e3b\u8981\u73af\u5883\uff0c\u901a\u8fc7\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "LLM\u5728\u5f62\u5f0f\u5b66\u4e60\u4efb\u52a1\uff08\u5982\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\uff09\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\"\u5b9e\u8df5\u667a\u6167\"\u548c\u53ef\u6cdb\u5316\u667a\u80fd\uff08\u5982\u6218\u7565\u521b\u9020\u529b\u548c\u793e\u4ea4\u63a8\u7406\uff09\u3002\u8fd9\u79cd\u5dee\u8ddd\u6e90\u4e8e\u7f3a\u4e4f\u975e\u6b63\u5f0f\u5b66\u4e60\uff0c\u800c\u6e38\u620f\u73af\u5883\u53ef\u4ee5\u63d0\u4f9b\u4e92\u52a8\u53cd\u9988\u548c\u62bd\u8c61\u590d\u6742\u6027\u6765\u57f9\u517b\u591a\u6837\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5c06\u6e38\u620f\u4f5c\u4e3aLLM\u975e\u6b63\u5f0f\u5b66\u4e60\u7684\u4e3b\u8981\u73af\u5883\uff0c\u5229\u7528\u5176\u5185\u5728\u5956\u52b1\u4fe1\u53f7\u548c\u62bd\u8c61\u590d\u6742\u6027\u3002\u5f15\u5165\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u4efb\u52a1\u7ec4\u5408\u5f3a\u5236\u6267\u884c\u660e\u786e\u7684\"AND\"\u76ee\u6807\uff08\u800c\u975e\u7b80\u5355\u7684\u4efb\u52a1\u6df7\u5408\u4f18\u5316\u9690\u5f0f\"OR\"\u76ee\u6807\uff09\uff0c\u5f3a\u5236\u6a21\u578b\u540c\u65f6\u638c\u63e1\u591a\u79cd\u80fd\u529b\u4ee5\u83b7\u5f97\u6700\u5927\u5956\u52b1\u3002\u4f7f\u7528\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u77e9\u9635\u6e38\u620f\u3001\u4e95\u5b57\u68cb\u548c\"\u8c01\u662f\u5367\u5e95\"\u6e38\u620f\u4e2d\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u96c6\u6210\u57fa\u4e8e\u6e38\u620f\u7684\u975e\u6b63\u5f0f\u5b66\u4e60\u4e0d\u4ec5\u80fd\u9632\u6b62\u4efb\u52a1\u5e72\u6270\uff0c\u8fd8\u80fd\u663e\u8457\u589e\u5f3a\u6a21\u578b\u5728\u5e7f\u6cdb\u80fd\u529b\u5bfc\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6e38\u620f\u73af\u5883\u4e3aLLM\u975e\u6b63\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5d4c\u5957\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u80fd\u591f\u57f9\u517bLLM\u7684\u6218\u7565\u521b\u9020\u529b\u548c\u793e\u4ea4\u63a8\u7406\u7b49\u5b9e\u8df5\u667a\u6167\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05616", "abs": "https://arxiv.org/abs/2601.05616", "authors": ["ShaoZhen Liu", "Xinting Huang", "Houwen Peng", "Xin Chen", "Xinyang Song", "Qi Li", "Zhenan Sun"], "title": "Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks", "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u957f\u94fe\u601d\u7ef4\u6570\u636e\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u8bc1\u660e\u76d1\u7763\u5fae\u8c03\u80fd\u6709\u6548\u6fc0\u6d3b\u6a21\u578b\u5185\u5728\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5ffd\u89c6\u4e86\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6709\u6548\u6fc0\u6d3b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u7b56\u7565\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u5305\u542b\u9a8c\u8bc1\u3001\u56de\u6eaf\u3001\u5b50\u76ee\u6807\u5206\u89e3\u548c\u53cd\u5411\u63a8\u7406\u7684\u601d\u7ef4\u94fe\u6570\u636e\uff0c\u5e76\u7528\u9884\u5b9a\u4e49\u89c4\u5219\u7b5b\u9009\u9ad8\u8d28\u91cf\u6837\u672c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u96be\u5ea6\u611f\u77e5\u62d2\u7edd\u91c7\u6837\u673a\u5236\u52a8\u6001\u4f18\u5316\u6570\u636e\u5206\u5e03\uff0c\u589e\u5f3a\u6a21\u578b\u5904\u7406\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728GSM8K\u548cMATH500\u7b49\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u63d0\u5347\uff0c\u5fae\u8c03\u6a21\u578b\u5728AIME24\u7b49\u7ade\u8d5b\u7ea7\u95ee\u9898\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u63a8\u7406\u94fe\u957f\u5ea6\u6269\u5c55\u8d85\u8fc74\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u76d1\u7763\u5fae\u8c03\u80fd\u6709\u6548\u6fc0\u6d3b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u4f18\u5316\u63d0\u4f9b\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u9014\u5f84\u3002\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u76d1\u7763\u5fae\u8c03\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05657", "abs": "https://arxiv.org/abs/2601.05657", "authors": ["Hao Yang", "Hongyuan Lu", "Dingkang Yang", "Wenliang Yang", "Peng Sun", "Xiaochuan Zhang", "Jun Xiao", "Kefan He", "Wai Lam", "Yang Liu", "Xinhua Zeng"], "title": "Stephanie2: Thinking, Waiting, and Making Decisions Like Humans in Step-by-Step AI Social Chat", "comment": "13 pages", "summary": "Instant-messaging human social chat typically progresses through a sequence of short messages. Existing step-by-step AI chatting systems typically split a one-shot generation into multiple messages and send them sequentially, but they lack an active waiting mechanism and exhibit unnatural message pacing. In order to address these issues, we propose Stephanie2, a novel next-generation step-wise decision-making dialogue agent. With active waiting and message-pace adaptation, Stephanie2 explicitly decides at each step whether to send or wait, and models latency as the sum of thinking time and typing time to achieve more natural pacing. We further introduce a time-window-based dual-agent dialogue system to generate pseudo dialogue histories for human and automatic evaluations. Experiments show that Stephanie2 clearly outperforms Stephanie1 on metrics such as naturalness and engagement, and achieves a higher pass rate on human evaluation with the role identification Turing test.", "AI": {"tldr": "Stephanie2\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u9010\u6b65\u51b3\u7b56\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u4e3b\u52a8\u7b49\u5f85\u548c\u6d88\u606f\u8282\u594f\u9002\u5e94\u673a\u5236\uff0c\u80fd\u591f\u66f4\u81ea\u7136\u5730\u6a21\u62df\u4eba\u7c7b\u5373\u65f6\u6d88\u606f\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u76f8\u6bd4\u524d\u4ee3\u5728\u81ea\u7136\u5ea6\u548c\u53c2\u4e0e\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u9010\u6b65AI\u804a\u5929\u7cfb\u7edf\u901a\u5e38\u5c06\u4e00\u6b21\u6027\u751f\u6210\u7684\u5185\u5bb9\u5206\u5272\u6210\u591a\u4e2a\u6d88\u606f\u987a\u5e8f\u53d1\u9001\uff0c\u4f46\u7f3a\u4e4f\u4e3b\u52a8\u7b49\u5f85\u673a\u5236\uff0c\u6d88\u606f\u8282\u594f\u4e0d\u81ea\u7136\u3002\u4eba\u7c7b\u5373\u65f6\u6d88\u606f\u793e\u4ea4\u804a\u5929\u901a\u5e38\u901a\u8fc7\u4e00\u7cfb\u5217\u77ed\u6d88\u606f\u8fdb\u884c\uff0c\u9700\u8981\u66f4\u81ea\u7136\u7684\u5bf9\u8bdd\u8282\u594f\u6a21\u62df\u3002", "method": "\u63d0\u51faStephanie2\u4ee3\u7406\uff0c\u5177\u6709\u4e3b\u52a8\u7b49\u5f85\u548c\u6d88\u606f\u8282\u594f\u9002\u5e94\u529f\u80fd\uff0c\u5728\u6bcf\u4e00\u6b65\u660e\u786e\u51b3\u5b9a\u53d1\u9001\u8fd8\u662f\u7b49\u5f85\uff0c\u5c06\u5ef6\u8fdf\u5efa\u6a21\u4e3a\u601d\u8003\u65f6\u95f4\u548c\u6253\u5b57\u65f6\u95f4\u7684\u603b\u548c\u3002\u5f15\u5165\u57fa\u4e8e\u65f6\u95f4\u7a97\u53e3\u7684\u53cc\u4ee3\u7406\u5bf9\u8bdd\u7cfb\u7edf\u751f\u6210\u4f2a\u5bf9\u8bdd\u5386\u53f2\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793aStephanie2\u5728\u81ea\u7136\u5ea6\u548c\u53c2\u4e0e\u5ea6\u7b49\u6307\u6807\u4e0a\u660e\u663e\u4f18\u4e8eStephanie1\uff0c\u5728\u89d2\u8272\u8bc6\u522b\u56fe\u7075\u6d4b\u8bd5\u7684\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u901a\u8fc7\u7387\u3002", "conclusion": "Stephanie2\u901a\u8fc7\u4e3b\u52a8\u7b49\u5f85\u548c\u6d88\u606f\u8282\u594f\u9002\u5e94\u673a\u5236\uff0c\u80fd\u591f\u66f4\u81ea\u7136\u5730\u6a21\u62df\u4eba\u7c7b\u5373\u65f6\u6d88\u606f\u5bf9\u8bdd\u6a21\u5f0f\uff0c\u63d0\u5347\u4e86\u5bf9\u8bdd\u4ee3\u7406\u7684\u81ea\u7136\u6027\u548c\u53c2\u4e0e\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2601.05746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05746", "abs": "https://arxiv.org/abs/2601.05746", "authors": ["Zhenghao Li", "Zhi Zheng", "Wei Chen", "Jielun Zhao", "Yong Chen", "Tong Xu", "Enhong Chen"], "title": "DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation", "comment": "16pages,6figures", "summary": "Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.", "AI": {"tldr": "DynaDebate\u63d0\u51fa\u52a8\u6001\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u751f\u6210\u3001\u8fc7\u7a0b\u4e2d\u5fc3\u8fa9\u8bba\u548c\u89e6\u53d1\u9a8c\u8bc1\u89e3\u51b3\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u56e0\u521d\u59cb\u5316\u76f8\u540c\u5bfc\u81f4\u7684\u540c\u8d28\u5316\u9519\u8bef\u548c\u7b80\u5355\u591a\u6570\u6295\u7968\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\u5b58\u5728\u672a\u5f15\u5bfc\u521d\u59cb\u5316\u95ee\u9898\uff0c\u5bfc\u81f4\u667a\u80fd\u4f53\u91c7\u7528\u76f8\u540c\u63a8\u7406\u8def\u5f84\u5e76\u72af\u76f8\u540c\u9519\u8bef\uff0c\u4f7f\u5f97\u6709\u6548\u8fa9\u8bba\u53d7\u963b\uff0c\u6700\u7ec8\u7ed3\u679c\u9000\u5316\u4e3a\u7b80\u5355\u591a\u6570\u6295\u7968\u3002", "method": "\u63d0\u51faDynaDebate\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u52a8\u6001\u8def\u5f84\u751f\u6210\u4e0e\u5206\u914d\uff0c\u4f7f\u7528\u4e13\u7528\u8def\u5f84\u751f\u6210\u667a\u80fd\u4f53\u751f\u6210\u591a\u6837\u5316\u903b\u8f91\u89e3\u8def\u5f84\uff1b2) \u8fc7\u7a0b\u4e2d\u5fc3\u8fa9\u8bba\uff0c\u4ece\u7ed3\u679c\u6295\u7968\u8f6c\u5411\u9010\u6b65\u903b\u8f91\u6279\u5224\uff1b3) \u89e6\u53d1\u5f0f\u9a8c\u8bc1\u667a\u80fd\u4f53\uff0c\u5728\u5206\u6b67\u65f6\u6fc0\u6d3b\u5e76\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u5ba2\u89c2\u89e3\u51b3\u50f5\u5c40\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eDynaDebate\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd5\u3002", "conclusion": "DynaDebate\u901a\u8fc7\u52a8\u6001\u8def\u5f84\u751f\u6210\u3001\u8fc7\u7a0b\u4e2d\u5fc3\u8fa9\u8bba\u548c\u89e6\u53d1\u9a8c\u8bc1\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u540c\u8d28\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u4f5c\u51b3\u7b56\u548c\u590d\u6742\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.05787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05787", "abs": "https://arxiv.org/abs/2601.05787", "authors": ["Zezhou Wang", "Ziyun Zhang", "Xiaoyi Zhang", "Zhuzhong Qian", "Yan Lu"], "title": "From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation", "comment": "Work In Progress", "summary": "Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git", "AI": {"tldr": "BEPA\u65b9\u6cd5\u901a\u8fc7\u53cc\u5c42\u4e13\u5bb6\u8f68\u8ff9\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u9759\u6001\u4e13\u5bb6\u8f68\u8ff9\u8f6c\u5316\u4e3a\u7b56\u7565\u5bf9\u9f50\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aefGUI\u64cd\u4f5c\u4ee3\u7406\u5728OSWorld-Verified\u7b49\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dGUI\u6570\u636e\u96c6\uff08\u5982OSWorld\uff09\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a1\uff09\u53ea\u6709\u51e0\u767e\u4e2a\u53ef\u4ea4\u4e92\u3001\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u548c\u73af\u5883\uff1b2\uff09\u4e13\u5bb6\u8f68\u8ff9\u9700\u8981\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u6536\u96c6\uff0c\u96be\u4ee5\u6269\u5c55\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5229\u7528\u5c11\u91cf\u73b0\u6709\u4e13\u5bb6\u8f68\u8ff9\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u8bad\u7ec3\u7aef\u5230\u7aef\u7b56\u7565\u3002", "method": "\u63d0\u51faBEPA\uff08\u53cc\u5c42\u4e13\u5bb6\u5230\u7b56\u7565\u540c\u5316\uff09\u65b9\u6cd5\uff1aLEVEL-1\u901a\u8fc7\u57fa\u7840\u7b56\u7565\u751f\u6210\u81ea\u6eda\u52a8\u53ef\u8fbe\u8f68\u8ff9\uff0c\u5c06\u9759\u6001\u4e13\u5bb6\u8f68\u8ff9\u8f6c\u5316\u4e3a\u7b56\u7565\u5bf9\u9f50\u6307\u5bfc\uff1bLEVEL-2\u4f7f\u7528\u6309\u4efb\u52a1\u52a8\u6001\u66f4\u65b0\u7684\u7f13\u5b58\u8fdb\u884cRLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u3002", "result": "\u5728OSWorld-Verified\u4e0a\uff0cBEPA\u5c06UITARS1.5-7B\u7684\u6210\u529f\u7387\u4ece22.87%\u63d0\u5347\u523032.13%\uff0c\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u4ece5.74%\u63d0\u5347\u523010.30%\uff0c\u5728MMBench-GUI\u548cOnline-Mind2Web\u4e0a\u4e5f\u6709\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "BEPA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u8f68\u8ff9\u4e0e\u5b66\u4e60\u7b56\u7565\u4e4b\u95f4\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aefGUI\u64cd\u4f5c\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u4e3a\u5c0f\u6837\u672c\u4e13\u5bb6\u8f68\u8ff9\u7684\u6709\u6548\u5229\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05890", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05890", "abs": "https://arxiv.org/abs/2601.05890", "authors": ["Ruizhe Zhang", "Xinke Jiang", "Zhibang Yang", "Zhixin Zhang", "Jiaran Gao", "Yuzhen Xiao", "Hongbin Lai", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management", "comment": null, "summary": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.", "AI": {"tldr": "StackPlanner\u662f\u4e00\u4e2a\u5177\u6709\u663e\u5f0f\u5185\u5b58\u63a7\u5236\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9ad8\u5c42\u534f\u8c03\u4e0e\u5b50\u4efb\u52a1\u6267\u884c\uff0c\u5e76\u5229\u7528\u7ed3\u6784\u5316\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u5f3a\u5316\u5b66\u4e60\u91cd\u7528\u534f\u8c03\u7ecf\u9a8c\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u534f\u4f5c\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e2d\u592e\u667a\u80fd\u4f53\u7531\u4e8e\u7f3a\u4e4f\u5185\u5b58\u7ba1\u7406\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u81a8\u80c0\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5f71\u54cd\u4e86\u957f\u65f6\u7a0b\u534f\u4f5c\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faStackPlanner\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u4e3b\u52a8\u4efb\u52a1\u7ea7\u5185\u5b58\u63a7\u5236\u89e3\u8026\u9ad8\u5c42\u534f\u8c03\u4e0e\u5b50\u4efb\u52a1\u6267\u884c\uff1b2\uff09\u5229\u7528\u7ed3\u6784\u5316\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u5f3a\u5316\u5b66\u4e60\u68c0\u7d22\u548c\u91cd\u7528\u534f\u8c03\u7ecf\u9a8c\u3002", "result": "\u5728\u591a\u4e2a\u6df1\u5ea6\u641c\u7d22\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u53ef\u9760\u7684\u957f\u65f6\u7a0b\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "StackPlanner\u901a\u8fc7\u663e\u5f0f\u5185\u5b58\u63a7\u5236\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u957f\u65f6\u7a0b\u534f\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2601.05713", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05713", "abs": "https://arxiv.org/abs/2601.05713", "authors": ["Thomas Fabian"], "title": "Visualising Information Flow in Word Embeddings with Diffusion Tensor Imaging", "comment": null, "summary": "Understanding how large language models (LLMs) represent natural language is a central challenge in natural language processing (NLP) research. Many existing methods extract word embeddings from an LLM, visualise the embedding space via point-plots, and compare the relative positions of certain words. However, this approach only considers single words and not whole natural language expressions, thus disregards the context in which a word is used. Here we present a novel tool for analysing and visualising information flow in natural language expressions by applying diffusion tensor imaging (DTI) to word embeddings. We find that DTI reveals how information flows between word embeddings. Tracking information flows within the layers of an LLM allows for comparing different model structures and revealing opportunities for pruning an LLM's under-utilised layers. Furthermore, our model reveals differences in information flows for tasks like pronoun resolution and metaphor detection. Our results show that our model permits novel insights into how LLMs represent actual natural language expressions, extending the comparison of isolated word embeddings and improving the interpretability of NLP models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u5f20\u91cf\u6210\u50cf\uff08DTI\uff09\u7684\u65b0\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u548c\u53ef\u89c6\u5316\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u4e2d\u7684\u4fe1\u606f\u6d41\uff0c\u901a\u8fc7\u8ffd\u8e2aLLM\u5c42\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\u6765\u6bd4\u8f83\u6a21\u578b\u7ed3\u6784\u3001\u8bc6\u522b\u53ef\u526a\u679d\u5c42\uff0c\u5e76\u63ed\u793a\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4fe1\u606f\u6d41\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u63d0\u53d6LLM\u4e2d\u7684\u8bcd\u5d4c\u5165\u5e76\u901a\u8fc7\u70b9\u56fe\u53ef\u89c6\u5316\uff0c\u4ec5\u8003\u8651\u5355\u4e2a\u8bcd\u6c47\u800c\u5ffd\u7565\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u5206\u6790\u5b8c\u6574\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u4e2d\u7684\u4fe1\u606f\u6d41\u52a8\u3002", "method": "\u5c06\u6269\u6563\u5f20\u91cf\u6210\u50cf\uff08DTI\uff09\u6280\u672f\u5e94\u7528\u4e8e\u8bcd\u5d4c\u5165\u5206\u6790\uff0c\u8ffd\u8e2aLLM\u5404\u5c42\u4e4b\u95f4\u4fe1\u606f\u6d41\u7684\u4f20\u64ad\u8def\u5f84\uff0c\u4ece\u800c\u53ef\u89c6\u5316\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u4e2d\u7684\u4fe1\u606f\u6d41\u52a8\u3002", "result": "DTI\u80fd\u591f\u63ed\u793a\u8bcd\u5d4c\u5165\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\uff0c\u901a\u8fc7\u8ffd\u8e2aLLM\u5c42\u95f4\u4fe1\u606f\u6d41\u53ef\u4ee5\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7ed3\u6784\u3001\u8bc6\u522b\u672a\u5145\u5206\u5229\u7528\u7684\u53ef\u526a\u679d\u5c42\uff0c\u5e76\u663e\u793a\u4ee3\u8bcd\u6d88\u89e3\u548c\u9690\u55bb\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e2d\u7684\u4fe1\u606f\u6d41\u5dee\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3LLM\u5982\u4f55\u8868\u793a\u5b9e\u9645\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8d85\u8d8a\u4e86\u5b64\u7acb\u8bcd\u5d4c\u5165\u7684\u6bd4\u8f83\uff0c\u63d0\u5347\u4e86NLP\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.05899", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05899", "abs": "https://arxiv.org/abs/2601.05899", "authors": ["Dawei Wang", "Chengming Zhou", "Di Zhao", "Xinyuan Liu", "Marci Chi Ma", "Gary Ushaw", "Richard Davison"], "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents", "comment": "AAAI 2026 Oral", "summary": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).", "AI": {"tldr": "TowerMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u5854\u9632\u6e38\u620f\u7684\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u957f\u671f\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u63ed\u793a\u4e86LLM\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5728\u80fd\u529b\u548c\u5e7b\u89c9\u65b9\u9762\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684RTS\u6e38\u620f\u73af\u5883\u8981\u4e48\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u8981\u4e48\u7f3a\u4e4f\u6587\u672c\u89c2\u5bdf\u652f\u6301\uff0c\u9650\u5236\u4e86LLM\u8bc4\u4f30\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u591a\u6a21\u6001\u7684\u73af\u5883\u6765\u8bc4\u4f30LLM\u7684\u957f\u671f\u89c4\u5212\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86TowerMind\u73af\u5883\uff0c\u57fa\u4e8e\u5854\u9632\u6e38\u620f\u5b50\u7c7b\u578b\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u9700\u6c42\u548c\u591a\u6a21\u6001\u89c2\u5bdf\u7a7a\u95f4\uff08\u50cf\u7d20\u3001\u6587\u672c\u3001\u7ed3\u6784\u5316\u6e38\u620f\u72b6\u6001\uff09\u3002\u8bbe\u8ba1\u4e86\u4e94\u4e2a\u57fa\u51c6\u5173\u5361\u6765\u8bc4\u4f30\u4e0d\u540c\u591a\u6a21\u6001\u8f93\u5165\u8bbe\u7f6e\u4e0b\u7684LLM\u3002", "result": "\u7ed3\u679c\u663e\u793aLLM\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5728\u80fd\u529b\u548c\u5e7b\u89c9\u65b9\u9762\u5b58\u5728\u660e\u663e\u6027\u80fd\u5dee\u8ddd\u3002\u5b9e\u9a8c\u8fd8\u63ed\u793a\u4e86LLM\u884c\u4e3a\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5982\u89c4\u5212\u9a8c\u8bc1\u4e0d\u8db3\u3001\u51b3\u7b56\u7f3a\u4e4f\u591a\u7ec8\u6027\u3001\u52a8\u4f5c\u4f7f\u7528\u6548\u7387\u4f4e\u3002", "conclusion": "TowerMind\u901a\u8fc7\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u8bbe\u8ba1\u8865\u5145\u4e86\u73b0\u6709\u7684RTS\u6e38\u620f\u73af\u5883\uff0c\u4e3aAI\u667a\u80fd\u4f53\u9886\u57df\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2601.05991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05991", "abs": "https://arxiv.org/abs/2601.05991", "authors": ["Jiayu Ding", "Haoran Tang", "Ge Li"], "title": "Open-Vocabulary 3D Instruction Ambiguity Detection", "comment": null, "summary": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5f00\u653e\u8bcd\u6c473D\u6307\u4ee4\u6b67\u4e49\u68c0\u6d4b\u4efb\u52a1\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u57fa\u51c6Ambi3D\uff0c\u53d1\u73b0\u73b0\u67093D LLMs\u96be\u4ee5\u53ef\u9760\u68c0\u6d4b\u6b67\u4e49\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6AmbiVer\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u8bed\u8a00\u6b67\u4e49\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u4f46\u73b0\u6709\u5177\u8eabAI\u7814\u7a76\u5927\u591a\u5ffd\u7565\u6b64\u95ee\u9898\uff0c\u5047\u8bbe\u6307\u4ee4\u6e05\u6670\u5e76\u4e13\u6ce8\u4e8e\u6267\u884c\u800c\u975e\u786e\u8ba4\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u5173\u952e\u5b89\u5168\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u5f00\u653e\u8bcd\u6c473D\u6307\u4ee4\u6b67\u4e49\u68c0\u6d4b\u65b0\u4efb\u52a1\uff1b\u6784\u5efaAmbi3D\u57fa\u51c6\uff08700+\u591a\u68373D\u573a\u666f\uff0c\u7ea622k\u6307\u4ee4\uff09\uff1b\u63d0\u51faAmbiVer\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u4ece\u591a\u89c6\u89d2\u6536\u96c6\u663e\u5f0f\u89c6\u89c9\u8bc1\u636e\uff0c\u5e76\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u6307\u4ee4\u6b67\u4e49\u3002", "result": "\u5206\u6790\u53d1\u73b0\u6700\u5148\u8fdb\u76843D LLMs\u96be\u4ee5\u53ef\u9760\u5224\u65ad\u6307\u4ee4\u662f\u5426\u6b67\u4e49\uff1bAmbiVer\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1\u7684\u6311\u6218\u6027\u548c\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u4fe1\u7684\u5177\u8eabAI\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u586b\u8865\u4e86\u6307\u4ee4\u6b67\u4e49\u68c0\u6d4b\u7684\u5173\u952e\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2508.04295", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04295", "abs": "https://arxiv.org/abs/2508.04295", "authors": ["Chaofan Wang", "Tingrui Yu", "Chen Xie", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "EvoC2Rust: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "comment": "Accepted by ICSE 2026 SEIP", "summary": "Translating legacy C codebases to Rust is increasingly demanded for building safety-critical systems. While various approaches have emerged for this task, they face inherent trade-offs: rule-based methods often struggle to satisfy code safety and idiomaticity requirements, while LLM-based methods frequently fail to generate semantically equivalent Rust code, due to the heavy dependencies of modules across the entire codebase. Recent studies have revealed that both solutions are limited to small-scale programs. In this paper, we propose EvoC2Rust, an automated framework for converting complete C projects to equivalent Rust ones. EvoC2Rust employs a skeleton-guided translation strategy for project-level translation. The pipeline consists of three stages: 1) it first decomposes the C project into functional modules, employs a feature-mapping-enhanced LLM to transform definitions and macros, and generates type-checked function stubs, which form a compilable Rust skeleton; 2) it then incrementally translates functions, replacing the corresponding stub placeholders; 3) finally, it repairs compilation errors by integrating LLM and static analysis. Through evolutionary augmentation, EvoC2Rust combines the advantages of both rule-based and LLM-based solutions. Our evaluation on open-source benchmarks and six industrial projects demonstrates the superior performance of EvoC2Rust in project-level C-to-Rust translation. The results show that our approach outperforms the strongest LLM-based baseline by 17.24% in syntax accuracy and 14.32% in semantic accuracy, while also achieving a 43.59% higher code safety rate than the best rule-based tool.", "AI": {"tldr": "EvoC2Rust\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u5b8c\u6574\u7684C\u9879\u76ee\u8f6c\u6362\u4e3a\u7b49\u6548\u7684Rust\u9879\u76ee\uff0c\u91c7\u7528\u9aa8\u67b6\u5f15\u5bfc\u7684\u7ffb\u8bd1\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u548c\u57fa\u4e8eLLM\u65b9\u6cd5\u7684\u4f18\u70b9\u3002", "motivation": "\u5c06\u9057\u7559C\u4ee3\u7801\u5e93\u8f6c\u6362\u4e3aRust\u5bf9\u4e8e\u6784\u5efa\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u6743\u8861\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u4ee3\u7801\u5b89\u5168\u6027\u548c\u60ef\u7528\u6027\u8981\u6c42\uff0c\u800c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u7531\u4e8e\u6574\u4e2a\u4ee3\u7801\u5e93\u4e2d\u6a21\u5757\u7684\u91cd\u5ea6\u4f9d\u8d56\uff0c\u7ecf\u5e38\u65e0\u6cd5\u751f\u6210\u8bed\u4e49\u7b49\u6548\u7684Rust\u4ee3\u7801\u3002\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\u90fd\u5c40\u9650\u4e8e\u5c0f\u578b\u7a0b\u5e8f\u3002", "method": "\u91c7\u7528\u9aa8\u67b6\u5f15\u5bfc\u7684\u7ffb\u8bd1\u7b56\u7565\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u5c06C\u9879\u76ee\u5206\u89e3\u4e3a\u529f\u80fd\u6a21\u5757\uff0c\u4f7f\u7528\u7279\u5f81\u6620\u5c04\u589e\u5f3a\u7684LLM\u8f6c\u6362\u5b9a\u4e49\u548c\u5b8f\uff0c\u751f\u6210\u7c7b\u578b\u68c0\u67e5\u7684\u51fd\u6570\u5b58\u6839\uff0c\u5f62\u6210\u53ef\u7f16\u8bd1\u7684Rust\u9aa8\u67b6\uff1b2) \u589e\u91cf\u7ffb\u8bd1\u51fd\u6570\uff0c\u66ff\u6362\u5bf9\u5e94\u7684\u5b58\u6839\u5360\u4f4d\u7b26\uff1b3) \u901a\u8fc7\u96c6\u6210LLM\u548c\u9759\u6001\u5206\u6790\u4fee\u590d\u7f16\u8bd1\u9519\u8bef\u3002\u901a\u8fc7\u8fdb\u5316\u589e\u5f3a\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "result": "\u5728\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u548c\u516d\u4e2a\u5de5\u4e1a\u9879\u76ee\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cEvoC2Rust\u5728\u9879\u76ee\u7ea7C\u5230Rust\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u5728\u8bed\u6cd5\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u5f3a\u7684\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u9ad8\u51fa17.24%\uff0c\u5728\u8bed\u4e49\u51c6\u786e\u7387\u4e0a\u9ad8\u51fa14.32%\uff0c\u540c\u65f6\u6bd4\u6700\u597d\u7684\u57fa\u4e8e\u89c4\u5219\u5de5\u5177\u5b9e\u73b043.59%\u66f4\u9ad8\u7684\u4ee3\u7801\u5b89\u5168\u7387\u3002", "conclusion": "EvoC2Rust\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u9879\u76ee\u7ea7C\u5230Rust\u7ffb\u8bd1\u7684\u6311\u6218\uff0c\u901a\u8fc7\u9aa8\u67b6\u5f15\u5bfc\u7b56\u7565\u548c\u8fdb\u5316\u589e\u5f3a\uff0c\u5728\u8bed\u6cd5\u51c6\u786e\u6027\u3001\u8bed\u4e49\u7b49\u6548\u6027\u548c\u4ee3\u7801\u5b89\u5168\u6027\u65b9\u9762\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2511.19517", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19517", "abs": "https://arxiv.org/abs/2511.19517", "authors": ["Adarsh Kumarappan", "Ananya Mujoo"], "title": "Automating Deception: Scalable Multi-Turn LLM Jailbreaks", "comment": null, "summary": "Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u3001\u57fa\u4e8e\u5fc3\u7406\u5b66\u539f\u7406\u7684\u591a\u8f6e\u8d8a\u72f1\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u5728\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u653b\u51fb\u5229\u7528\u5fc3\u7406\u5b66\u539f\u7406\uff08\u5982\u767b\u95e8\u69db\u6548\u5e94\uff09\u7ed5\u8fc7LLM\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u6269\u5c55\u7684\u624b\u52a8\u6570\u636e\u96c6\u521b\u5efa\uff0c\u963b\u788d\u4e86\u9632\u5fa1\u8fdb\u5c55\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5c06\u767b\u95e8\u69db\u6280\u672f\u7cfb\u7edf\u5316\u4e3a\u53ef\u590d\u73b0\u6a21\u677f\uff0c\u521b\u5efa\u5305\u542b1500\u4e2a\u573a\u666f\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u975e\u6cd5\u6d3b\u52a8\u548c\u5192\u72af\u6027\u5185\u5bb9\uff0c\u8bc4\u4f307\u4e2a\u4e3b\u6d41\u6a21\u578b\u5728\u591a\u8f6e\u548c\u5355\u8f6e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "GPT\u7cfb\u5217\u6a21\u578b\u5bf9\u5bf9\u8bdd\u5386\u53f2\u9ad8\u5ea6\u8106\u5f31\uff0c\u653b\u51fb\u6210\u529f\u7387\u6700\u591a\u63d0\u534732\u4e2a\u767e\u5206\u70b9\uff1bGemini 2.5 Flash\u8868\u73b0\u51fa\u5353\u8d8a\u97e7\u6027\uff0c\u51e0\u4e4e\u514d\u75ab\u6b64\u7c7b\u653b\u51fb\uff1bClaude 3 Haiku\u6709\u8f83\u5f3a\u4f46\u4e0d\u5b8c\u7f8e\u7684\u62b5\u6297\u529b\u3002", "conclusion": "\u5f53\u524d\u5b89\u5168\u67b6\u6784\u5728\u5904\u7406\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u5173\u952e\u5206\u6b67\uff0c\u9700\u8981\u80fd\u591f\u62b5\u6297\u53d9\u4e8b\u64cd\u7eb5\u7684\u9632\u5fa1\u673a\u5236\uff0c\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.05808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05808", "abs": "https://arxiv.org/abs/2601.05808", "authors": ["Xiaoshuai Song", "Haofei Chang", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou", "Ji-Rong Wen"], "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "comment": "Working in progress", "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "AI": {"tldr": "EnvScaler\u662f\u4e00\u4e2a\u901a\u8fc7\u7a0b\u5e8f\u5408\u6210\u81ea\u52a8\u751f\u6210\u53ef\u6269\u5c55\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3LLM\u4f5c\u4e3a\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u73af\u5883\u6784\u5efa\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8bad\u7ec3LLM\u4f5c\u4e3a\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u9700\u8981\u4e30\u5bcc\u591a\u6837\u7684\u5de5\u5177\u4ea4\u4e92\u6c99\u7bb1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9650\u5236\uff1a\u771f\u5b9e\u7cfb\u7edf\u8bbf\u95ee\u53d7\u9650\uff0cLLM\u6a21\u62df\u73af\u5883\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\uff0c\u624b\u52a8\u6784\u5efa\u6c99\u7bb1\u96be\u4ee5\u6269\u5c55\u3002", "method": "EnvScaler\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1aSkelBuilder\u901a\u8fc7\u4e3b\u9898\u6316\u6398\u3001\u903b\u8f91\u5efa\u6a21\u548c\u8d28\u91cf\u8bc4\u4f30\u6784\u5efa\u591a\u6837\u5316\u7684\u73af\u5883\u9aa8\u67b6\uff1bScenGenerator\u4e3a\u6bcf\u4e2a\u73af\u5883\u751f\u6210\u591a\u4e2a\u4efb\u52a1\u573a\u666f\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8f68\u8ff9\u9a8c\u8bc1\u51fd\u6570\u3002", "result": "\u4f7f\u7528EnvScaler\u5408\u6210\u4e86191\u4e2a\u73af\u5883\u548c\u7ea67K\u4e2a\u573a\u666f\uff0c\u5e94\u7528\u4e8eQwen3\u7cfb\u5217\u6a21\u578b\u7684SFT\u548cRL\u8bad\u7ec3\u3002\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEnvScaler\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6d89\u53ca\u591a\u8f6e\u3001\u591a\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u73af\u5883\u4e2d\u89e3\u51b3\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "EnvScaler\u4e3aLLM\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u73af\u5883\u751f\u6210\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u5177\u4ea4\u4e92\u73af\u5883\u6784\u5efa\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.05870", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05870", "abs": "https://arxiv.org/abs/2601.05870", "authors": ["Huilin Deng", "Hongchen Luo", "Yue Zhu", "Long Li", "Zhuoyue Chen", "Xinghao Zhao", "Ming Li", "Jihai Zhang", "Mengchang Wang", "Yang Cao", "Yu Kang"], "title": "IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck", "comment": null, "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.", "AI": {"tldr": "\u63d0\u51faIIB-LPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u7406\u5b9e\u73b0\u63a8\u7406\u8f68\u8ff9\u7684\u62d3\u6251\u5206\u652f\uff0c\u89e3\u51b3RLVR\u4e2d\u7684\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "RLVR\u5728LLM\u63a8\u7406\u4e2d\u5b58\u5728\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u968f\u673arollout\u7684\u8bed\u4e49\u540c\u8d28\u6027\u5bfc\u81f4\u6a21\u578b\u9677\u5165\u72ed\u7a84\u7684\u8fc7\u4f18\u5316\u884c\u4e3a\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u7b56\u7565\u71b5\u9f13\u52b1\u63a2\u7d22\uff0c\u4f46\u5168\u5c40\u71b5\u6b63\u5219\u5316\u6613\u53d7\u5956\u52b1\u653b\u51fb\u5bfc\u81f4\u65e0\u610f\u4e49\u5197\u957f\uff0c\u5c40\u90e8token\u9009\u62e9\u6027\u66f4\u65b0\u96be\u4ee5\u514b\u670d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5f3a\u5f52\u7eb3\u504f\u7f6e\u3002", "method": "\u63d0\u51faIIB-LPO\u65b9\u6cd5\uff0c\u5c06\u63a2\u7d22\u4ecetoken\u5206\u5e03\u7684\u7edf\u8ba1\u6270\u52a8\u8f6c\u5411\u63a8\u7406\u8f68\u8ff9\u7684\u62d3\u6251\u5206\u652f\u3002\u5728\u9ad8\u71b5\u72b6\u6001\u89e6\u53d1\u6f5c\u5728\u5206\u652f\u4ee5\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\uff0c\u4f7f\u7528\u4fe1\u606f\u74f6\u9888\u539f\u7406\u4f5c\u4e3a\u8f68\u8ff9\u8fc7\u6ee4\u5668\u548c\u81ea\u5956\u52b1\u673a\u5236\uff0c\u786e\u4fdd\u7b80\u6d01\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u63a2\u7d22\u3002", "result": "\u5728\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cIIB-LPO\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u9ad8\u8fbe5.3%\uff0c\u5728\u591a\u6837\u6027\u6307\u6807\u4e0a\u63d0\u53477.4%\u3002", "conclusion": "IIB-LPO\u901a\u8fc7\u62d3\u6251\u5206\u652f\u548c\u4fe1\u606f\u74f6\u9888\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u63a2\u7d22\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7684\u6027\u80fd\u548c\u591a\u6837\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05903", "abs": "https://arxiv.org/abs/2601.05903", "authors": ["Zihang Tian", "Rui Li", "Jingsen Zhang", "Xiaohe Bo", "Wei Huo", "Xu Chen"], "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search", "comment": null, "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameters. Specifically, we use a high-level router to select among candidate LLM architectures, and then search for the optimal parameters for the selected architectures based on a low-level router. We design a parameter generation network to share parameters between the two routers to mutually enhance their capabilities. In the training process, we design a reward-augmented objective to effectively optimize our framework. Experiments on two commonly used benchmarks show that HAPS consistently outperforms strong routing baselines. We have released our code at https://github.com/zihangtian/HAPS.", "AI": {"tldr": "HAPS\u662f\u4e00\u4e2a\u5206\u5c42LLM\u8def\u7531\u6846\u67b6\uff0c\u8054\u5408\u641c\u7d22\u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\uff0c\u901a\u8fc7\u9ad8\u5c42\u8def\u7531\u5668\u9009\u62e9\u67b6\u6784\uff0c\u4f4e\u5c42\u8def\u7531\u5668\u4f18\u5316\u53c2\u6570\uff0c\u5171\u4eab\u53c2\u6570\u589e\u5f3a\u80fd\u529b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u8def\u7531\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9009\u62e9\u4e0d\u540c\u7684LLM\u67b6\u6784\uff0c\u4f46\u5ffd\u7565\u4e86\u53c2\u6570\u8bbe\u7f6e\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002\u9700\u8981\u540c\u65f6\u8003\u8651\u67b6\u6784\u9009\u62e9\u548c\u53c2\u6570\u4f18\u5316\u6765\u5145\u5206\u5229\u7528\u4e0d\u540cLLM\u7684\u4e13\u4e1a\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8def\u7531\u6846\u67b6HAPS\uff1a\u9ad8\u5c42\u8def\u7531\u5668\u9009\u62e9\u5019\u9009LLM\u67b6\u6784\uff0c\u4f4e\u5c42\u8def\u7531\u5668\u4e3a\u9009\u5b9a\u67b6\u6784\u641c\u7d22\u6700\u4f18\u53c2\u6570\u3002\u8bbe\u8ba1\u53c2\u6570\u751f\u6210\u7f51\u7edc\u5728\u4e24\u4e2a\u8def\u7531\u5668\u95f4\u5171\u4eab\u53c2\u6570\u4ee5\u76f8\u4e92\u589e\u5f3a\u80fd\u529b\u3002\u4f7f\u7528\u5956\u52b1\u589e\u5f3a\u76ee\u6807\u8fdb\u884c\u6709\u6548\u4f18\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cHAPS\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u8def\u7531\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8054\u5408\u641c\u7d22\u67b6\u6784\u548c\u53c2\u6570\u7684\u6709\u6548\u6027\u3002", "conclusion": "HAPS\u901a\u8fc7\u5206\u5c42\u8def\u7531\u6846\u67b6\u8054\u5408\u4f18\u5316LLM\u67b6\u6784\u9009\u62e9\u548c\u53c2\u6570\u8bbe\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u7531\u6027\u80fd\uff0c\u4e3aLLM\u8def\u7531\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.05905", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05905", "abs": "https://arxiv.org/abs/2601.05905", "authors": ["Haoming Xu", "Ningyuan Zhao", "Yunzhi Yao", "Weihong Xu", "Hongru Wang", "Xinle Deng", "Shumin Deng", "Jeff Z. Pan", "Huajun Chen", "Ningyu Zhang"], "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency", "comment": "Work in progress", "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference. To address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood. To validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%. Code will be available at https://github.com/zjunlp/belief.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNeighbor-Consistency Belief (NCB)\u4f5c\u4e3a\u8861\u91cfLLM\u4fe1\u5ff5\u9c81\u68d2\u6027\u7684\u7ed3\u6784\u6307\u6807\uff0c\u5e76\u5f00\u53d1Structure-Aware Training (SAT)\u6765\u589e\u5f3a\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5e72\u6270\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u70b9\u72b6\u7f6e\u4fe1\u5ea6\uff08\u5982Self-Consistency\uff09\uff0c\u65e0\u6cd5\u68c0\u6d4b\u4fe1\u5ff5\u5728\u4e0a\u4e0b\u6587\u5e72\u6270\u4e0b\u7684\u8106\u5f31\u6027\u3002\u5373\u4f7f\u81ea\u4e00\u81f4\u6027\u5b8c\u7f8e\u7684\u7b54\u6848\u4e5f\u53ef\u80fd\u5728\u8f7b\u5fae\u5e72\u6270\u4e0b\u8fc5\u901f\u5d29\u6e83\uff0c\u8fd9\u5bf9\u5b9e\u9645\u90e8\u7f72\u6784\u6210\u98ce\u9669\u3002", "method": "\u63d0\u51faNCB\u4f5c\u4e3a\u7ed3\u6784\u5316\u7684\u4fe1\u5ff5\u9c81\u68d2\u6027\u5ea6\u91cf\uff0c\u8bc4\u4f30\u6982\u5ff5\u90bb\u57df\u5185\u7684\u54cd\u5e94\u4e00\u81f4\u6027\uff1b\u8bbe\u8ba1\u8ba4\u77e5\u538b\u529b\u6d4b\u8bd5\u534f\u8bae\u9a8c\u8bc1NCB\u6709\u6548\u6027\uff1b\u5f00\u53d1SAT\u8bad\u7ec3\u65b9\u6cd5\u4f18\u5316\u4e0a\u4e0b\u6587\u4e0d\u53d8\u7684\u4fe1\u5ff5\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u9ad8NCB\u6570\u636e\u7684\u6027\u80fd\u5bf9\u5e72\u6270\u66f4\u5177\u62b5\u6297\u529b\uff1bSAT\u80fd\u5c06\u957f\u5c3e\u77e5\u8bc6\u8106\u5f31\u6027\u964d\u4f4e\u7ea630%\uff0c\u663e\u8457\u589e\u5f3aLLM\u5728\u4e0a\u4e0b\u6587\u5e72\u6270\u4e0b\u7684\u4fe1\u5ff5\u7a33\u5b9a\u6027\u3002", "conclusion": "NCB\u662f\u8bc4\u4f30LLM\u4fe1\u5ff5\u9c81\u68d2\u6027\u7684\u6709\u6548\u7ed3\u6784\u6307\u6807\uff0cSAT\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u7684LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.05930", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05930", "abs": "https://arxiv.org/abs/2601.05930", "authors": ["Jingsheng Zheng", "Jintian Zhang", "Yujie Luo", "Yuren Mao", "Yunjun Gao", "Lun Du", "Huajun Chen", "Ningyu Zhang"], "title": "Can We Predict Before Executing Machine Learning Agents?", "comment": "Work in progress", "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faFOREAGENT\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u6267\u884c\u5148\u9a8c\u6765\u66ff\u4ee3\u6602\u8d35\u7684\u7269\u7406\u6267\u884c\uff0c\u91c7\u7528\"\u9884\u6d4b-\u9a8c\u8bc1\"\u5faa\u73af\u52a0\u901f\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u7684\u6536\u655b\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u53d7\u9650\u4e8e\"\u751f\u6210-\u6267\u884c-\u53cd\u9988\"\u8303\u5f0f\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u6267\u884c\u74f6\u9888\uff0c\u5047\u8bbe\u8bc4\u4f30\u4f9d\u8d56\u6602\u8d35\u7684\u7269\u7406\u6267\u884c\u3002\u9700\u8981\u7ed5\u8fc7\u8fd9\u4e9b\u7269\u7406\u7ea6\u675f\uff0c\u901a\u8fc7\u5185\u90e8\u5316\u6267\u884c\u5148\u9a8c\u6765\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u3002", "method": "1) \u5f62\u5f0f\u5316\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u89e3\u504f\u597d\u4efb\u52a1\uff1b2) \u6784\u5efa\u5305\u542b18,438\u5bf9\u6bd4\u8f83\u7684\u7efc\u5408\u8bed\u6599\u5e93\uff1b3) \u5229\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6570\u636e\u5206\u6790\u62a5\u544a\u6765\u6fc0\u53d1LLMs\u7684\u9884\u6d4b\u80fd\u529b\uff1b4) \u5b9e\u73b0FOREAGENT\u4ee3\u7406\uff0c\u91c7\u7528\"\u9884\u6d4b-\u9a8c\u8bc1\"\u5faa\u73af\u66ff\u4ee3\u4f20\u7edf\u6267\u884c\u3002", "result": "LLMs\u5728\u83b7\u5f97\u9a8c\u8bc1\u6570\u636e\u5206\u6790\u62a5\u544a\u540e\u8868\u73b0\u51fa\u663e\u8457\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u8fbe\u523061.5%\u7684\u51c6\u786e\u7387\u548c\u7a33\u5065\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002FOREAGENT\u5b9e\u73b0\u4e866\u500d\u7684\u6536\u655b\u52a0\u901f\uff0c\u5e76\u8d85\u8d8a\u57fa\u4e8e\u6267\u884c\u7684\u57fa\u7ebf\u65b9\u6cd56%\u3002", "conclusion": "\u901a\u8fc7\u5185\u90e8\u5316\u6267\u884c\u5148\u9a8c\u548c\u9884\u6d4b-\u9a8c\u8bc1\u5faa\u73af\uff0c\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u7684\u6536\u655b\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u4f20\u7edf\u6267\u884c\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.06002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.06002", "abs": "https://arxiv.org/abs/2601.06002", "authors": ["Qiguang Chen", "Yantao Du", "Ziniu Li", "Jinhao Liu", "Songyao Duan", "Jiarui Guo", "Minghao Liu", "Jiaheng Liu", "Tong Yang", "Ge Zhang", "Libo Qin", "Wanxiang Che", "Wenhao Huang"], "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "comment": "Preprint", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u5177\u6709\u7c7b\u4f3c\u5206\u5b50\u7684\u7a33\u5b9a\u7ed3\u6784\uff0c\u5305\u542b\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u7c7b\u578b\uff0c\u5e76\u5f00\u53d1\u4e86Mole-Syn\u65b9\u6cd5\u6765\u5408\u6210\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u4eba\u7c7b\u6216\u975e\u957f\u94fe\u601d\u7ef4LLMs\u6a21\u4eff\u4e2d\u5b66\u4e60\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u9700\u8981\u7406\u89e3\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u7684\u6709\u6548\u5b66\u4e60\u673a\u5236\u3002", "method": "\u63d0\u51fa\u957f\u94fe\u601d\u7ef4\u8f68\u8ff9\u5177\u6709\u7c7b\u4f3c\u5206\u5b50\u7684\u7a33\u5b9a\u7ed3\u6784\uff0c\u5305\u542b\u6df1\u5ea6\u63a8\u7406\uff08\u5171\u4ef7\u952e\u5f0f\uff09\u3001\u81ea\u6211\u53cd\u601d\uff08\u6c22\u952e\u5f0f\uff09\u548c\u81ea\u63a2\u7d22\uff08\u8303\u5fb7\u534e\u5f0f\uff09\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u3002\u5f15\u5165\u6709\u6548\u8bed\u4e49\u5f02\u6784\u4f53\u6982\u5ff5\uff0c\u5f00\u53d1Mole-Syn\u5206\u5e03\u8f6c\u79fb\u56fe\u65b9\u6cd5\u6307\u5bfc\u5408\u6210\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\u3002", "result": "\u5206\u6790\u8868\u660e\u8fd9\u4e9b\u7ed3\u6784\u6765\u81ea\u957f\u94fe\u601d\u7ef4\u5fae\u8c03\u800c\u975e\u5173\u952e\u8bcd\u6a21\u4eff\uff0c\u53ea\u6709\u4fc3\u8fdb\u5feb\u901f\u71b5\u6536\u655b\u7684\u952e\u80fd\u652f\u6301\u7a33\u5b9a\u7684\u957f\u94fe\u601d\u7ef4\u5b66\u4e60\u3002Mole-Syn\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5f3a\u5316\u5b66\u4e60\u7a33\u5b9a\u6027\u3002", "conclusion": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u7684\u6709\u6548\u5b66\u4e60\u4f9d\u8d56\u4e8e\u7c7b\u4f3c\u5206\u5b50\u7684\u7a33\u5b9a\u7ed3\u6784\uff0cMole-Syn\u65b9\u6cd5\u80fd\u591f\u5408\u6210\u8fd9\u4e9b\u7ed3\u6784\uff0c\u663e\u8457\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.06007", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06007", "abs": "https://arxiv.org/abs/2601.06007", "authors": ["Elias Lumer", "Faheem Nizar", "Akshaya Jangiti", "Kevin Frank", "Anmol Gulati", "Mandar Phadate", "Vamse Kumar Subbiah"], "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks", "comment": "15 pages, 8 figures", "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u63d0\u793a\u7f13\u5b58\u5bf9\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5728\u4e09\u5927LLM\u63d0\u4f9b\u5546\u4e0a\u6d4b\u8bd5\u4e86\u4e09\u79cd\u7f13\u5b58\u7b56\u7565\uff0c\u53d1\u73b0\u53ef\u964d\u4f4e45-80%\u7684API\u6210\u672c\u5e76\u63d0\u534713-31%\u7684\u9996\u4ee4\u724c\u65f6\u95f4\u3002", "motivation": "\u5c3d\u7ba1\u4e3b\u8981LLM\u63d0\u4f9b\u5546\u63d0\u4f9b\u63d0\u793a\u7f13\u5b58\u6765\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4f46\u5176\u5728\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u6548\u76ca\u5728\u7814\u7a76\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u76ee\u524d\u6ca1\u6709\u5de5\u4f5c\u91cf\u5316\u8fd9\u4e9b\u6210\u672c\u8282\u7701\u6216\u6bd4\u8f83\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u7684\u7f13\u5b58\u7b56\u7565\u3002", "method": "\u5728\u4e09\u5927LLM\u63d0\u4f9b\u5546\uff08OpenAI\u3001Anthropic\u3001Google\uff09\u4e0a\u5168\u9762\u8bc4\u4f30\u63d0\u793a\u7f13\u5b58\uff0c\u6bd4\u8f83\u4e09\u79cd\u7f13\u5b58\u7b56\u7565\uff1a\u5b8c\u6574\u4e0a\u4e0b\u6587\u7f13\u5b58\u3001\u4ec5\u7cfb\u7edf\u63d0\u793a\u7f13\u5b58\u3001\u6392\u9664\u52a8\u6001\u5de5\u5177\u7ed3\u679c\u7684\u7f13\u5b58\u3002\u4f7f\u7528DeepResearchBench\u591a\u8f6e\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728500\u591a\u4e2a\u4ee3\u7406\u4f1a\u8bdd\u4e2d\u6d4b\u91cfAPI\u6210\u672c\u548c\u9996\u4ee4\u724c\u65f6\u95f4\u3002", "result": "\u63d0\u793a\u7f13\u5b58\u53ef\u5c06API\u6210\u672c\u964d\u4f4e45-80%\uff0c\u9996\u4ee4\u724c\u65f6\u95f4\u63d0\u534713-31%\u3002\u7b56\u7565\u6027\u63d0\u793a\u7f13\u5b58\u5757\u63a7\u5236\uff08\u5982\u5c06\u52a8\u6001\u5185\u5bb9\u653e\u5728\u7cfb\u7edf\u63d0\u793a\u672b\u5c3e\u3001\u907f\u514d\u52a8\u6001\u4f20\u7edf\u51fd\u6570\u8c03\u7528\u3001\u6392\u9664\u52a8\u6001\u5de5\u5177\u7ed3\u679c\uff09\u6bd4\u7b80\u5355\u7684\u5b8c\u6574\u4e0a\u4e0b\u6587\u7f13\u5b58\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u6548\u76ca\u3002", "conclusion": "\u63d0\u793a\u7f13\u5b58\u5bf9\u4ee3\u7406\u7cfb\u7edf\u6709\u663e\u8457\u6548\u76ca\uff0c\u4f46\u9700\u8981\u7b56\u7565\u6027\u5b9e\u65bd\u3002\u4e0d\u540c\u63d0\u4f9b\u5546\u5b58\u5728\u7ec6\u5fae\u5dee\u5f02\uff0c\u7814\u7a76\u4e3a\u751f\u4ea7\u4ee3\u7406\u7cfb\u7edf\u5b9e\u65bd\u63d0\u793a\u7f13\u5b58\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.06021", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.06021", "abs": "https://arxiv.org/abs/2601.06021", "authors": ["Jiajie Zhang", "Xin Lv", "Ling Feng", "Lei Hou", "Juanzi Li"], "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\textbf{Citation-aware Rubric Rewards (CaRR)}, a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce \\textbf{Citation-aware Group Relative Policy Optimization (C-GRPO)}, which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "AI": {"tldr": "\u63d0\u51faCaRR\u5956\u52b1\u6846\u67b6\u548cC-GRPO\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\u63d0\u5347\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u63a8\u7406\u5168\u9762\u6027\u3001\u4e8b\u5b9e\u57fa\u7840\u548c\u8bc1\u636e\u8fde\u63a5\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u4e8c\u5143\u5956\u52b1\u5bfc\u81f4\u7684\u6377\u5f84\u5229\u7528\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u8fc7\u7a0b\u7684\u5168\u9762\u6027\u548c\u4e8b\u5b9e\u6027\uff0c\u5bfc\u81f4\u6377\u5f84\u5229\u7528\u548c\u5e7b\u89c9\u7b49\u4e0d\u826f\u884c\u4e3a\u3002", "method": "\u63d0\u51faCaRR\u5956\u52b1\u6846\u67b6\uff0c\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5355\u6b65\u8bc4\u4f30\u6807\u51c6\uff0c\u8981\u6c42\u4ee3\u7406\u8bc6\u522b\u9690\u85cf\u5b9e\u4f53\u3001\u63d0\u4f9b\u6b63\u786e\u5f15\u7528\u5e76\u6784\u5efa\u5b8c\u6574\u8bc1\u636e\u94fe\uff1b\u7ed3\u5408CaRR\u548c\u7ed3\u679c\u5956\u52b1\u63d0\u51faC-GRPO\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "C-GRPO\u5728\u591a\u4e2a\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u57fa\u4e8e\u7ed3\u679c\u7684RL\u57fa\u7ebf\uff0c\u6709\u6548\u6291\u5236\u6377\u5f84\u5229\u7528\uff0c\u4fc3\u8fdb\u5168\u9762\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\uff0c\u5e76\u5728\u5f00\u653e\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CaRR\u548cC-GRPO\u4e3a\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u5956\u52b1\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u8d28\u91cf\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.e8176c80", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.02439%3Futm_source=tldrai/1/0100019b9e169e42-0780e18a-91e7-487b-b08f-7364d8885d51-000000/2U61-YmCElTN_DK0pybHHRltJBe4nEz17SjX67zZA6Y=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.02439%3Futm_source=tldrai/1/0100019b9e169e42-0780e18a-91e7-487b-b08f-7364d8885d51-000000/2U61-YmCElTN_DK0pybHHRltJBe4nEz17SjX67zZA6Y=439", "authors": ["TLDR Newsletter"], "title": "Visual Web Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 29 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.02439%3Futm_source=tldrai/1/0100019b9e169e42-0780e18a-91e7-487b-b08f-7364d8885d51-000000/2U61-YmCElTN_DK0pybHHRltJBe4nEz17SjX67zZA6Y=439", "summary": "Visual Web Agents (29 minute read) WebGym is a large-scale environment with nearly 300,000 real-world web tasks for training visual agents. It has a high-throughput RL system that boosts training speed and a fine-tuned Qwen-3-VL that outperforms GPT-4o and GPT-5-Thinking on unseen web navigation tasks.", "source": "tldr", "AI": {"tldr": "WebGym\u662f\u4e00\u4e2a\u5305\u542b\u8fd130\u4e07\u4e2a\u771f\u5b9e\u7f51\u9875\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u89c6\u89c9Web\u667a\u80fd\u4f53\u8bad\u7ec3\u73af\u5883\uff0c\u914d\u5907\u9ad8\u541e\u5410RL\u7cfb\u7edf\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\uff0c\u5176\u5fae\u8c03\u7684Qwen-3-VL\u6a21\u578b\u5728\u672a\u89c1\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o\u548cGPT-5-Thinking\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9Web\u667a\u80fd\u4f53\u8bad\u7ec3\u9762\u4e34\u771f\u5b9e\u7f51\u9875\u4efb\u52a1\u89c4\u6a21\u4e0d\u8db3\u3001\u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u6784\u5efa\u5927\u89c4\u6a21\u771f\u5b9e\u73af\u5883\u6765\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efaWebGym\u5927\u89c4\u6a21\u73af\u5883\uff08\u8fd130\u4e07\u4e2a\u771f\u5b9e\u7f51\u9875\u4efb\u52a1\uff09\uff0c\u5f00\u53d1\u9ad8\u541e\u5410\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u52a0\u901f\u8bad\u7ec3\uff0c\u5e76\u5fae\u8c03Qwen-3-VL\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u7f51\u9875\u5bfc\u822a\u3002", "result": "\u5fae\u8c03\u7684Qwen-3-VL\u5728\u672a\u89c1\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4o\u548cGPT-5-Thinking\uff0c\u9ad8\u541e\u5410RL\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u3002", "conclusion": "WebGym\u4e3a\u89c6\u89c9Web\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u73af\u5883\uff0c\u7ed3\u5408\u9ad8\u6548RL\u7cfb\u7edf\u548c\u4e13\u7528\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.d4856a40", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/6LVQFKMoe5p_WHRBHtwIoFnDPhWDAoyyI-K8Z4zyliI=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/6LVQFKMoe5p_WHRBHtwIoFnDPhWDAoyyI-K8Z4zyliI=439", "authors": ["TLDR Newsletter"], "title": "LLM predictions for 2026, shared with Oxide and Friends", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FJan%2F8%2Fllm-predictions-for-2026%2F%23atom-everything%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/6LVQFKMoe5p_WHRBHtwIoFnDPhWDAoyyI-K8Z4zyliI=439", "summary": "LLM predictions for 2026, shared with Oxide and Friends (9 minute read) This post shares predictions Simon Willison made on a recent podcast. The predictions cover what he thinks will happen in the next 1, 3, and 6 years in the tech industry. There has never been so much uncertainty about what's coming in the next year. The significant advances in coding agents in just the last two months indicate that things will change significantly, but it is unclear what those changes will be.", "source": "tldr", "AI": {"tldr": "Simon Willison\u5728\u64ad\u5ba2\u4e2d\u5206\u4eab\u4e86\u5bf92026\u5e74\u53ca\u672a\u6765\u79d1\u6280\u884c\u4e1a\u7684\u9884\u6d4b\uff0c\u8ba4\u4e3a\u7f16\u7801\u667a\u80fd\u4f53\u7684\u5feb\u901f\u53d1\u5c55\u5c06\u5e26\u6765\u91cd\u5927\u4f46\u4e0d\u786e\u5b9a\u7684\u53d8\u5316", "motivation": "\u5f53\u524d\u79d1\u6280\u884c\u4e1a\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u7f16\u7801\u667a\u80fd\u4f53\u5728\u6700\u8fd1\u4e24\u4e2a\u6708\u53d6\u5f97\u7684\u663e\u8457\u8fdb\u5c55\u9884\u793a\u7740\u91cd\u5927\u53d8\u9769\u5373\u5c06\u5230\u6765\uff0c\u4f46\u5177\u4f53\u53d8\u5316\u65b9\u5411\u5c1a\u4e0d\u660e\u786e", "method": "\u57fa\u4e8e\u64ad\u5ba2\u8bbf\u8c08\u5f62\u5f0f\uff0c\u5206\u4eab\u5bf9\u672a\u67651\u5e74\u30013\u5e74\u548c6\u5e74\u79d1\u6280\u53d1\u5c55\u8d8b\u52bf\u7684\u4e2a\u4eba\u9884\u6d4b\u548c\u89c1\u89e3", "result": "\u9884\u6d4b\u6307\u51fa\u7f16\u7801\u667a\u80fd\u4f53\u7684\u5feb\u901f\u53d1\u5c55\u5c06\u663e\u8457\u6539\u53d8\u6280\u672f\u884c\u4e1a\u683c\u5c40\uff0c\u4f46\u5177\u4f53\u53d8\u5316\u8def\u5f84\u548c\u5f71\u54cd\u4ecd\u5b58\u5728\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u6027", "conclusion": "\u79d1\u6280\u884c\u4e1a\u6b63\u5904\u4e8e\u5feb\u901f\u53d8\u9769\u671f\uff0c\u7279\u522b\u662f\u7f16\u7801\u667a\u80fd\u4f53\u6280\u672f\u5c06\u5e26\u6765\u91cd\u5927\u4f46\u96be\u4ee5\u9884\u6d4b\u7684\u884c\u4e1a\u53d8\u5316", "topic": "agent analysis"}}
{"id": "tldr.2601.8faf7b47", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fearly-look-at-grok-build-upcoming-vibe-coding-agent-from-xai%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/P_Pvpf8ADDIrmpKZf3nHOWlZogqwraP8hp8yx2vOz9k=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fearly-look-at-grok-build-upcoming-vibe-coding-agent-from-xai%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/P_Pvpf8ADDIrmpKZf3nHOWlZogqwraP8hp8yx2vOz9k=439", "authors": ["TLDR Newsletter"], "title": "Early look at Grok Build, upcoming vibe coding agent from xAI", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fearly-look-at-grok-build-upcoming-vibe-coding-agent-from-xai%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/P_Pvpf8ADDIrmpKZf3nHOWlZogqwraP8hp8yx2vOz9k=439", "summary": "Early look at Grok Build, upcoming vibe coding agent from xAI (2 minute read) Grok Build is a coding agent from xAI that can be accessed both on the web and through a CLI. It currently supports local agents, with remote environment support marked as coming soon. There are options for users to configure agents and custom environments. A video previewing the feature is available in the article.", "source": "tldr", "AI": {"tldr": "Grok Build\u662fxAI\u5373\u5c06\u63a8\u51fa\u7684\u4ee3\u7801\u4ee3\u7406\uff0c\u652f\u6301\u672c\u5730\u4ee3\u7406\u548cCLI/web\u8bbf\u95ee\uff0c\u5177\u5907\u4ee3\u7406\u914d\u7f6e\u548c\u81ea\u5b9a\u4e49\u73af\u5883\u529f\u80fd", "motivation": "xAI\u5f00\u53d1Grok Build\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u96c6\u6210\u7684\u4ee3\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u901a\u8fc7\u672c\u5730\u6216\u8fdc\u7a0b\u73af\u5883\u8fdb\u884c\u4ee3\u7801\u5f00\u53d1\uff0c\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b", "method": "\u901a\u8fc7\u672c\u5730\u4ee3\u7406\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\u548c\u5f00\u53d1\u652f\u6301\uff0c\u63d0\u4f9bCLI\u548cweb\u754c\u9762\uff0c\u652f\u6301\u4ee3\u7406\u914d\u7f6e\u548c\u81ea\u5b9a\u4e49\u73af\u5883\u8bbe\u7f6e", "result": "\u76ee\u524d\u652f\u6301\u672c\u5730\u4ee3\u7406\uff0c\u8fdc\u7a0b\u73af\u5883\u652f\u6301\u5373\u5c06\u63a8\u51fa\uff0c\u5df2\u63d0\u4f9b\u4ee3\u7406\u914d\u7f6e\u548c\u81ea\u5b9a\u4e49\u73af\u5883\u9009\u9879", "conclusion": "Grok Build\u662fxAI\u5373\u5c06\u63a8\u51fa\u7684\u4ee3\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u65e8\u5728\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4fbf\u6377\u7684\u4ee3\u7801\u5f00\u53d1\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2601.3d6b21b6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavekiss.com%2Fblog%2Fthe-1000-commits-problem%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/VmHPeGin7wfyDsYn8n-hVDahVhuyQM7d1DWV6Mz_zSo=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavekiss.com%2Fblog%2Fthe-1000-commits-problem%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/VmHPeGin7wfyDsYn8n-hVDahVhuyQM7d1DWV6Mz_zSo=439", "authors": ["TLDR Newsletter"], "title": "The 1,000 commits problem", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdavekiss.com%2Fblog%2Fthe-1000-commits-problem%2F%3Futm_source=tldrnewsletter/1/0100019ba27f6bb7-00db6a38-3217-44d5-93c8-d65295297e38-000000/VmHPeGin7wfyDsYn8n-hVDahVhuyQM7d1DWV6Mz_zSo=439", "summary": "The 1,000 commits problem (4 minute read) Anthropic recently shipped code that broke the entire CLI.", "source": "tldr", "AI": {"tldr": "Anthropic\u7684CLI\u4ee3\u7801\u66f4\u65b0\u5bfc\u81f4\u6574\u4e2a\u547d\u4ee4\u884c\u5de5\u5177\u5d29\u6e83\uff0c\u7a81\u663e\u4e86\u4ee3\u7801\u4ee3\u7406\u5728\u590d\u6742\u7cfb\u7edf\u7ef4\u62a4\u4e2d\u7684\u6311\u6218", "motivation": "\u63a2\u8ba8\u4ee3\u7801\u4ee3\u7406\u5728\u5904\u7406\u5927\u578b\u4ee3\u7801\u5e93\u548c\u590d\u6742\u7cfb\u7edf\u66f4\u65b0\u65f6\u53ef\u80fd\u9762\u4e34\u7684\u98ce\u9669\u548c\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u4ee3\u7801\u53d8\u66f4\u5f71\u54cd\u5230\u6838\u5fc3\u529f\u80fd\u65f6", "method": "\u901a\u8fc7\u5206\u6790Anthropic CLI\u5d29\u6e83\u7684\u5177\u4f53\u6848\u4f8b\uff0c\u7814\u7a76\u4ee3\u7801\u4ee3\u7406\u5728\u4ee3\u7801\u66f4\u65b0\u3001\u6d4b\u8bd5\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u95ee\u9898", "result": "\u4ee3\u7801\u4ee3\u7406\u7684\u66f4\u65b0\u5bfc\u81f4\u4e86\u6574\u4e2aCLI\u5de5\u5177\u7684\u5d29\u6e83\uff0c\u663e\u793a\u4e86\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4ee3\u7801\u53d8\u66f4\u65f6\u5b58\u5728\u7684\u98ce\u9669", "conclusion": "\u4ee3\u7801\u4ee3\u7406\u9700\u8981\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6838\u5fc3\u7cfb\u7edf\u529f\u80fd\u65f6\uff0c\u4ee5\u907f\u514d\u7c7b\u4f3c\u7684\u5d29\u6e83\u4e8b\u4ef6", "topic": "code agent"}}
{"id": "tldr.2601.1ff29d54", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mihaileric.com%2FThe-Emperor-Has-No-Clothes%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/NOZVOZaYycy6sy7KOuULDgbjnGAnceTFTfp0tAc_U4c=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mihaileric.com%2FThe-Emperor-Has-No-Clothes%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/NOZVOZaYycy6sy7KOuULDgbjnGAnceTFTfp0tAc_U4c=439", "authors": ["TLDR Newsletter"], "title": "The Emperor Has No Clothes: How to Code Claude Code in 200 Lines of Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mihaileric.com%2FThe-Emperor-Has-No-Clothes%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/NOZVOZaYycy6sy7KOuULDgbjnGAnceTFTfp0tAc_U4c=439", "summary": "The Emperor Has No Clothes: How to Code Claude Code in 200 Lines of Code (8 minute read) AI coding assistants are built on a simple core architecture that can be replicated in about 200 lines of Python. This post presents an agent that is equipped with three fundamental tools - read, list, and edit files - which the LLM learns to invoke via structured commands. The LLM decides which tool to use, executes it, and then processes the results to either continue the task or respond to the user.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u7528\u7ea6200\u884cPython\u4ee3\u7801\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684AI\u7f16\u7801\u52a9\u624b\uff0c\u6838\u5fc3\u67b6\u6784\u57fa\u4e8e\u4e09\u4e2a\u57fa\u672c\u5de5\u5177\uff1a\u8bfb\u53d6\u3001\u5217\u51fa\u548c\u7f16\u8f91\u6587\u4ef6\uff0cLLM\u901a\u8fc7\u7ed3\u6784\u5316\u547d\u4ee4\u5b66\u4e60\u8c03\u7528\u8fd9\u4e9b\u5de5\u5177\u3002", "motivation": "\u63ed\u793aAI\u7f16\u7801\u52a9\u624b\u7684\u57fa\u672c\u67b6\u6784\u5176\u5b9e\u5f88\u7b80\u5355\uff0c\u53ef\u4ee5\u88ab\u8f7b\u677e\u590d\u5236\uff0c\u6253\u7834\u590d\u6742\u7cfb\u7edf\u7684\u795e\u79d8\u611f\uff0c\u5c55\u793a\u6838\u5fc3\u529f\u80fd\u7684\u6700\u5c0f\u5b9e\u73b0\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u914d\u5907\u4e09\u4e2a\u57fa\u672c\u5de5\u5177\uff08\u8bfb\u53d6\u3001\u5217\u51fa\u3001\u7f16\u8f91\u6587\u4ef6\uff09\u7684\u4ee3\u7406\uff0cLLM\u901a\u8fc7\u7ed3\u6784\u5316\u547d\u4ee4\u5b66\u4e60\u8c03\u7528\u8fd9\u4e9b\u5de5\u5177\uff0c\u81ea\u4e3b\u51b3\u5b9a\u4f7f\u7528\u54ea\u4e2a\u5de5\u5177\u3001\u6267\u884c\u64cd\u4f5c\u5e76\u5904\u7406\u7ed3\u679c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684AI\u7f16\u7801\u52a9\u624b\uff0c\u4ec5\u7528\u7ea6200\u884cPython\u4ee3\u7801\uff0c\u8bc1\u660e\u4e86\u6838\u5fc3\u67b6\u6784\u7684\u7b80\u6d01\u6027\u548c\u53ef\u590d\u5236\u6027\u3002", "conclusion": "AI\u7f16\u7801\u52a9\u624b\u7684\u57fa\u672c\u67b6\u6784\u5e76\u4e0d\u590d\u6742\uff0c\u53ef\u4ee5\u901a\u8fc7\u5c11\u91cf\u4ee3\u7801\u5b9e\u73b0\u6838\u5fc3\u529f\u80fd\uff0c\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u5e76\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u7684\u5f00\u53d1\u548c\u5e94\u7528\u3002", "topic": "code agent"}}
{"id": "tldr.2601.48e6edaa", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomwphillips.co.uk%2F2026%2F01%2Fstaging-is-a-wasteful-lie-the-case-for-the-mono-environment%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/BO-gzadccyGRCjEIK15aPbJuQxfdnQwS1JWyGZ2o8q8=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomwphillips.co.uk%2F2026%2F01%2Fstaging-is-a-wasteful-lie-the-case-for-the-mono-environment%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/BO-gzadccyGRCjEIK15aPbJuQxfdnQwS1JWyGZ2o8q8=439", "authors": ["TLDR Newsletter"], "title": "Staging is a wasteful lie: the case for the mono-environment", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomwphillips.co.uk%2F2026%2F01%2Fstaging-is-a-wasteful-lie-the-case-for-the-mono-environment%2F%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/BO-gzadccyGRCjEIK15aPbJuQxfdnQwS1JWyGZ2o8q8=439", "summary": "Staging is a wasteful lie: the case for the mono-environment (13 minute read) Traditional staging environments are wasteful, unreliable, and create a false sense of security. A \"mono-environment\" approach, where only production exists, moves safeguards from environmental isolation to logical isolation within the code, leading to faster delivery and tighter feedback loops. This is done through automated practices such as testing, ephemeral local environments, feature flagging, continuous deplo...", "source": "tldr", "AI": {"tldr": "\u4f20\u7edf\u6d4b\u8bd5\u73af\u5883\u6d6a\u8d39\u4e14\u4e0d\u53ef\u9760\uff0c\u63d0\u51fa\"\u5355\u73af\u5883\"\u65b9\u6cd5\uff0c\u53ea\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u901a\u8fc7\u4ee3\u7801\u903b\u8f91\u9694\u79bb\u5b9e\u73b0\u5b89\u5168\u4fdd\u969c\uff0c\u800c\u975e\u73af\u5883\u9694\u79bb", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u73af\u5883\u5b58\u5728\u6d6a\u8d39\u3001\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5e76\u4ea7\u751f\u865a\u5047\u7684\u5b89\u5168\u611f\u3002\u4f5c\u8005\u8ba4\u4e3a\u73af\u5883\u9694\u79bb\u4e0d\u662f\u6700\u4f73\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5", "method": "\u91c7\u7528\"\u5355\u73af\u5883\"\u65b9\u6cd5\uff0c\u53ea\u4fdd\u7559\u751f\u4ea7\u73af\u5883\uff0c\u5c06\u5b89\u5168\u4fdd\u969c\u4ece\u73af\u5883\u9694\u79bb\u8f6c\u79fb\u5230\u4ee3\u7801\u903b\u8f91\u9694\u79bb\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001\u4e34\u65f6\u672c\u5730\u73af\u5883\u3001\u529f\u80fd\u6807\u8bb0\u3001\u6301\u7eed\u90e8\u7f72\u7b49\u5b9e\u8df5\u5b9e\u73b0", "result": "\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u66f4\u5feb\u7684\u4ea4\u4ed8\u901f\u5ea6\u548c\u66f4\u7d27\u5bc6\u7684\u53cd\u9988\u5faa\u73af\uff0c\u51cf\u5c11\u8d44\u6e90\u6d6a\u8d39\uff0c\u63d0\u9ad8\u90e8\u7f72\u53ef\u9760\u6027", "conclusion": "\u5355\u73af\u5883\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6d4b\u8bd5\u73af\u5883\u66f4\u9ad8\u6548\u53ef\u9760\uff0c\u901a\u8fc7\u4ee3\u7801\u903b\u8f91\u9694\u79bb\u800c\u975e\u73af\u5883\u9694\u79bb\u6765\u5b9e\u73b0\u5b89\u5168\u4fdd\u969c", "topic": "swe application"}}
{"id": "tldr.2601.1ca7d0ce", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspectrum.ieee.org%2Fai-coding-degrades%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/nkrMUaFzDgICXgIZDS0n5NbsSaiF9T8pI9b9XT28hzg=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspectrum.ieee.org%2Fai-coding-degrades%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/nkrMUaFzDgICXgIZDS0n5NbsSaiF9T8pI9b9XT28hzg=439", "authors": ["TLDR Newsletter"], "title": "AI Coding Assistants Are Getting Worse", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspectrum.ieee.org%2Fai-coding-degrades%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/nkrMUaFzDgICXgIZDS0n5NbsSaiF9T8pI9b9XT28hzg=439", "summary": "AI Coding Assistants Are Getting Worse (5 minute read) AI coding assistants, after an initial period of improvement, have recently begun to decline in quality. The primary issue with newer models, such as GPT-5, is their tendency towards \"silent but deadly\" failures, where code runs without errors but produces incorrect or misleading results by avoiding safety checks or generating plausible fake data. This degradation is likely caused by current training methods that prioritize user acceptanc...", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u52a9\u624b\u5728\u521d\u671f\u6539\u8fdb\u540e\u8fd1\u671f\u8d28\u91cf\u4e0b\u964d\uff0c\u65b0\u6a21\u578b\uff08\u5982GPT-5\uff09\u503e\u5411\u4e8e\u4ea7\u751f\"\u65e0\u58f0\u4f46\u81f4\u547d\"\u7684\u6545\u969c\uff0c\u4ee3\u7801\u65e0\u9519\u8bef\u8fd0\u884c\u4f46\u4ea7\u751f\u9519\u8bef\u7ed3\u679c\uff0c\u53ef\u80fd\u7531\u8bad\u7ec3\u65b9\u6cd5\u4f18\u5148\u8003\u8651\u7528\u6237\u63a5\u53d7\u5ea6\u800c\u975e\u6b63\u786e\u6027\u5bfc\u81f4\u3002", "motivation": "\u89c2\u5bdf\u5230AI\u7f16\u7a0b\u52a9\u624b\u5728\u7ecf\u5386\u521d\u671f\u8d28\u91cf\u63d0\u5347\u540e\u51fa\u73b0\u6027\u80fd\u9000\u5316\u73b0\u8c61\uff0c\u7279\u522b\u662f\u65b0\u6a21\u578b\u4ea7\u751f\"\u65e0\u58f0\u4f46\u81f4\u547d\"\u7684\u6545\u969c\uff0c\u8fd9\u6bd4\u660e\u663e\u9519\u8bef\u66f4\u5371\u9669\uff0c\u56e0\u4e3a\u4ee3\u7801\u80fd\u8fd0\u884c\u4f46\u7ed3\u679c\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u7f16\u7a0b\u52a9\u624b\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8bc6\u522b\"\u65e0\u58f0\u4f46\u81f4\u547d\"\u7684\u6545\u969c\u7279\u5f81\uff1a\u4ee3\u7801\u65e0\u8bed\u6cd5\u9519\u8bef\u4f46\u4ea7\u751f\u4e0d\u6b63\u786e\u7ed3\u679c\uff0c\u907f\u514d\u5b89\u5168\u68c0\u67e5\uff0c\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u865a\u5047\u7684\u6570\u636e\u3002", "result": "\u53d1\u73b0\u65b0\u6a21\u578b\uff08\u5982GPT-5\uff09\u8d28\u91cf\u4e0b\u964d\uff0c\u4ea7\u751f\u66f4\u591a\u9690\u853d\u6027\u9519\u8bef\uff0c\u8fd9\u79cd\u9000\u5316\u53ef\u80fd\u4e0e\u8bad\u7ec3\u65b9\u6cd5\u8fc7\u5ea6\u4f18\u5316\u7528\u6237\u63a5\u53d7\u5ea6\u6307\u6807\u800c\u727a\u7272\u4ee3\u7801\u6b63\u786e\u6027\u6709\u5173\u3002", "conclusion": "AI\u7f16\u7a0b\u52a9\u624b\u8d28\u91cf\u51fa\u73b0\u4ee4\u4eba\u62c5\u5fe7\u7684\u4e0b\u964d\u8d8b\u52bf\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e73\u8861\u7528\u6237\u63a5\u53d7\u5ea6\u548c\u4ee3\u7801\u6b63\u786e\u6027\uff0c\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6307\u6807\u3002", "topic": "code agent"}}
{"id": "tldr.2601.2f19d7a3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2026%2F01%2Fai-humans-making-the-relationship-work.html%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/IyeIhtITKfhg11SowWRXUSs8hRroRwu1cwhwu3cOuyg=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2026%2F01%2Fai-humans-making-the-relationship-work.html%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/IyeIhtITKfhg11SowWRXUSs8hRroRwu1cwhwu3cOuyg=439", "authors": ["TLDR Newsletter"], "title": "AI & Humans: Making the Relationship Work", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2026%2F01%2Fai-humans-making-the-relationship-work.html%3Futm_source=tldrdev/1/0100019ba2a9fb6f-e5cd0d9c-b52e-4de1-b015-2fe0e8e3fcf0-000000/IyeIhtITKfhg11SowWRXUSs8hRroRwu1cwhwu3cOuyg=439", "summary": "AI & Humans: Making the Relationship Work (15 minute read) Agentic AI, especially in collaborative groups, often has human-like behaviors, which means traditional human management principles are surprisingly relevant. Research from leading AI labs like Anthropic shows three key lessons for managing hybrid human-AI teams: effective delegation to use AI's parallelization, allowing for rapid iteration and trial-and-error learning, and having proper efficient information sharing among agents.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u5c06\u4f20\u7edf\u4eba\u7c7b\u7ba1\u7406\u539f\u5219\u5e94\u7528\u4e8eAI\u4ee3\u7406\u7ba1\u7406\uff0c\u7279\u522b\u662f\u6df7\u5408\u4eba\u673a\u56e2\u961f\u534f\u4f5c\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u7ba1\u7406\u7b56\u7565", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5c55\u73b0\u51fa\u7c7b\u4eba\u884c\u4e3a\uff0c\u4f20\u7edf\u7684\u4eba\u7c7b\u7ba1\u7406\u539f\u5219\u5728\u7ba1\u7406\u6df7\u5408\u4eba\u673a\u56e2\u961f\u4e2d\u53d8\u5f97\u76f8\u5173\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ba1\u7406\u8fd9\u4e9b\u65b0\u578b\u534f\u4f5c\u5173\u7cfb", "method": "\u57fa\u4e8eAnthropic\u7b49\u9886\u5148AI\u5b9e\u9a8c\u5ba4\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u7ba1\u7406\u539f\u5219\uff1a\u6709\u6548\u59d4\u6d3e\u4ee5\u5229\u7528AI\u5e76\u884c\u5316\u80fd\u529b\u3001\u5141\u8bb8\u5feb\u901f\u8fed\u4ee3\u548c\u8bd5\u9519\u5b66\u4e60\u3001\u5efa\u7acb\u9ad8\u6548\u7684\u4fe1\u606f\u5171\u4eab\u673a\u5236", "result": "\u7814\u7a76\u8868\u660e\u4f20\u7edf\u4eba\u7c7b\u7ba1\u7406\u539f\u5219\u53ef\u4ee5\u6210\u529f\u5e94\u7528\u4e8eAI\u4ee3\u7406\u7ba1\u7406\uff0c\u7279\u522b\u662f\u5728\u6df7\u5408\u4eba\u673a\u56e2\u961f\u534f\u4f5c\u573a\u666f\u4e2d", "conclusion": "AI\u4ee3\u7406\u7684\u7c7b\u4eba\u884c\u4e3a\u4f7f\u5f97\u4eba\u7c7b\u7ba1\u7406\u539f\u5219\u5177\u6709\u9002\u7528\u6027\uff0c\u6709\u6548\u7ba1\u7406\u6df7\u5408\u56e2\u961f\u9700\u8981\u5173\u6ce8\u59d4\u6d3e\u3001\u8fed\u4ee3\u5b66\u4e60\u548c\u4fe1\u606f\u5171\u4eab\u4e09\u4e2a\u5173\u952e\u65b9\u9762", "topic": "agent analysis"}}
{"id": "tldr.2601.33ee9290", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FFission-AI%2FOpenSpec%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/pzd_c46nLp8hBWwutT3ix74Uqjl30fMQiCxwFWn0kMc=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FFission-AI%2FOpenSpec%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/pzd_c46nLp8hBWwutT3ix74Uqjl30fMQiCxwFWn0kMc=439", "authors": ["TLDR Newsletter"], "title": "OpenSpec", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FFission-AI%2FOpenSpec%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/pzd_c46nLp8hBWwutT3ix74Uqjl30fMQiCxwFWn0kMc=439", "summary": "OpenSpec (GitHub Repo) OpenSpec is a spec-driven development (SDD) framework for AI coding assistants that aims to align humans and AI by locking intent with a lightweight specification workflow before any code is written. The system provides deterministic, reviewable outputs, addressing the unpredictability of AI when requirements are confined to chat history.", "source": "tldr", "AI": {"tldr": "OpenSpec\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u7684AI\u7f16\u7801\u52a9\u624b\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7f16\u5199\u4ee3\u7801\u524d\u4f7f\u7528\u8f7b\u91cf\u7ea7\u89c4\u8303\u5de5\u4f5c\u6d41\u9501\u5b9a\u610f\u56fe\uff0c\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u7684\u5bf9\u9f50\uff0c\u63d0\u4f9b\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u67e5\u7684\u8f93\u51fa\u3002", "motivation": "\u89e3\u51b3AI\u7f16\u7801\u52a9\u624b\u5728\u9700\u6c42\u4ec5\u5b58\u5728\u4e8e\u804a\u5929\u5386\u53f2\u4e2d\u65f6\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u89c4\u8303\u9a71\u52a8\u7684\u65b9\u6cd5\u786e\u4fddAI\u8f93\u51fa\u4e0e\u4eba\u7c7b\u610f\u56fe\u5bf9\u9f50\u3002", "method": "\u91c7\u7528\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u6846\u67b6\uff0c\u5728\u7f16\u5199\u4ee3\u7801\u524d\u5efa\u7acb\u8f7b\u91cf\u7ea7\u89c4\u8303\u5de5\u4f5c\u6d41\u6765\u9501\u5b9a\u610f\u56fe\uff0c\u786e\u4fdd\u8f93\u51fa\u7684\u786e\u5b9a\u6027\u548c\u53ef\u5ba1\u67e5\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u63d0\u4f9b\u786e\u5b9a\u6027\u3001\u53ef\u5ba1\u67e5\u8f93\u51fa\u7684AI\u7f16\u7801\u52a9\u624b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86AI\u5728\u9700\u6c42\u4e0d\u660e\u786e\u65f6\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u95ee\u9898\u3002", "conclusion": "\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u662f\u89e3\u51b3AI\u7f16\u7801\u52a9\u624b\u4e0d\u786e\u5b9a\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u524d\u9501\u5b9a\u610f\u56fe\u53ef\u4ee5\u63d0\u9ad8AI\u4e0e\u4eba\u7c7b\u7684\u5bf9\u9f50\u5ea6\u3002", "topic": "code agent"}}
{"id": "tldr.2601.6ae06e07", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2026%2F01%2F07%2Fholmesgpt-agentic-troubleshooting-built-for-the-cloud-native-era%2F%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/ydS_5RS8TsOGhRPS7ebDcy1sIUtppom-0a2xTvhwpU8=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2026%2F01%2F07%2Fholmesgpt-agentic-troubleshooting-built-for-the-cloud-native-era%2F%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/ydS_5RS8TsOGhRPS7ebDcy1sIUtppom-0a2xTvhwpU8=439", "authors": ["TLDR Newsletter"], "title": "HolmesGPT: Agentic troubleshooting built for the cloud native era", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cncf.io%2Fblog%2F2026%2F01%2F07%2Fholmesgpt-agentic-troubleshooting-built-for-the-cloud-native-era%2F%3Futm_source=tldrdevops/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/ydS_5RS8TsOGhRPS7ebDcy1sIUtppom-0a2xTvhwpU8=439", "summary": "HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.", "source": "tldr", "AI": {"tldr": "HolmesGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Kubernetes\u548c\u4e91\u539f\u751f\u73af\u5883AI\u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u5df2\u88abCNCF\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee", "motivation": "\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684\u6545\u969c\u6392\u9664\u590d\u6742\u4e14\u8017\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u8fd0\u7ef4\u6548\u7387", "method": "\u5f00\u53d1\u57fa\u4e8eAI\u7684\u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u4e13\u95e8\u9488\u5bf9Kubernetes\u548c\u4e91\u539f\u751f\u73af\u5883\u8bbe\u8ba1", "result": "\u9879\u76ee\u6210\u529f\u5f00\u53d1\u5e76\u88abCNCF\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee\uff0c\u83b7\u5f97\u5f00\u6e90\u793e\u533a\u8ba4\u53ef", "conclusion": "HolmesGPT\u4e3a\u4e91\u539f\u751f\u65f6\u4ee3\u63d0\u4f9b\u4e86\u6709\u6548\u7684AI\u9a71\u52a8\u6545\u969c\u6392\u9664\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2601.2853584b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/le_Om4y5TLUWO16oGKmYxEwJd-8cnpb4QH47g5LiGgI=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/le_Om4y5TLUWO16oGKmYxEwJd-8cnpb4QH47g5LiGgI=439", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/le_Om4y5TLUWO16oGKmYxEwJd-8cnpb4QH47g5LiGgI=439", "summary": "HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.", "source": "tldr", "AI": {"tldr": "HolmesGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u7684AI\u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u4e13\u95e8\u4e3aKubernetes\u548c\u4e91\u539f\u751f\u73af\u5883\u8bbe\u8ba1\uff0c\u5df2\u88abCNCF\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee", "motivation": "\u4e91\u539f\u751f\u73af\u5883\uff08\u7279\u522b\u662fKubernetes\uff09\u7684\u6545\u969c\u6392\u9664\u590d\u6742\u4e14\u8017\u65f6\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u5e2e\u52a9\u8fd0\u7ef4\u4eba\u5458\u5feb\u901f\u8bca\u65ad\u548c\u89e3\u51b3\u95ee\u9898", "method": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u7684AI\u4ee3\u7406\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\u81ea\u52a8\u5206\u6790\u548c\u8bca\u65adKubernetes\u53ca\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684\u6545\u969c", "result": "HolmesGPT\u57282023\u5e7410\u6708\u88ab\u4e91\u539f\u751f\u8ba1\u7b97\u57fa\u91d1\u4f1a\uff08CNCF\uff09\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee\uff0c\u83b7\u5f97\u4e86\u884c\u4e1a\u8ba4\u53ef", "conclusion": "HolmesGPT\u4e3a\u4e91\u539f\u751f\u65f6\u4ee3\u7684\u6545\u969c\u6392\u9664\u63d0\u4f9b\u4e86\u521b\u65b0\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u7b80\u5316\u8fd0\u7ef4\u5de5\u4f5c\u6d41\u7a0b", "topic": "code agent"}}
{"id": "tldr.2601.2b3ac438", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/AkXo9OisHB9tDRCugniAHxAWWcOt7ygYBZq-Guh_sg4=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/AkXo9OisHB9tDRCugniAHxAWWcOt7ygYBZq-Guh_sg4=439", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/AkXo9OisHB9tDRCugniAHxAWWcOt7ygYBZq-Guh_sg4=439", "summary": "HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.", "source": "tldr", "AI": {"tldr": "HolmesGPT \u662f\u4e00\u4e2a\u5f00\u6e90\u7684 AI \u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u4e13\u4e3a Kubernetes \u548c\u4e91\u539f\u751f\u73af\u5883\u8bbe\u8ba1\uff0c\u5df2\u88ab CNCF \u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee", "motivation": "\u968f\u7740\u4e91\u539f\u751f\u6280\u672f\u7684\u666e\u53ca\uff0cKubernetes \u73af\u5883\u4e2d\u7684\u6545\u969c\u6392\u9664\u53d8\u5f97\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u667a\u80fd\u5316\u7684\u81ea\u52a8\u5316\u5de5\u5177\u6765\u5e2e\u52a9\u8fd0\u7ef4\u4eba\u5458\u5feb\u901f\u5b9a\u4f4d\u548c\u89e3\u51b3\u95ee\u9898", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e AI \u7684\u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u4e13\u95e8\u9488\u5bf9 Kubernetes \u548c\u4e91\u539f\u751f\u73af\u5883\uff0c\u80fd\u591f\u81ea\u52a8\u5206\u6790\u7cfb\u7edf\u72b6\u6001\u3001\u8bc6\u522b\u95ee\u9898\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848", "result": "HolmesGPT \u5728 2023 \u5e74 10 \u6708\u88ab\u4e91\u539f\u751f\u8ba1\u7b97\u57fa\u91d1\u4f1a\uff08CNCF\uff09\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee\uff0c\u83b7\u5f97\u4e86\u884c\u4e1a\u8ba4\u53ef", "conclusion": "HolmesGPT \u4e3a\u4e91\u539f\u751f\u65f6\u4ee3\u7684\u6545\u969c\u6392\u9664\u63d0\u4f9b\u4e86\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u8fd0\u7ef4\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u9760\u6027", "topic": "code agent"}}
{"id": "tldr.2601.963da133", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/2BqGKBoNArSlnAy_JYcR5PMcLAQ07JcvapDVz58XFgU=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/2BqGKBoNArSlnAy_JYcR5PMcLAQ07JcvapDVz58XFgU=439", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019ba2bbec84-228ee830-5bd9-4d80-98c9-dcb27717b525-000000/2BqGKBoNArSlnAy_JYcR5PMcLAQ07JcvapDVz58XFgU=439", "summary": "HolmesGPT: Agentic troubleshooting built for the cloud native era (2 minute read) HolmesGPT, an open-source AI troubleshooting agent for Kubernetes and cloud-native environments, was accepted as a CNCF Sandbox project in October.", "source": "tldr", "AI": {"tldr": "HolmesGPT\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Kubernetes\u548c\u4e91\u539f\u751f\u73af\u5883AI\u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u5df2\u88abCNCF\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee", "motivation": "\u4e91\u539f\u751f\u73af\u5883\uff08\u7279\u522b\u662fKubernetes\uff09\u7684\u6545\u969c\u6392\u9664\u590d\u6742\u4e14\u8017\u65f6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u8fd0\u7ef4\u6548\u7387", "method": "\u5f00\u53d1\u57fa\u4e8eAI\u7684\u6545\u969c\u6392\u9664\u4ee3\u7406\uff0c\u4e13\u95e8\u9488\u5bf9Kubernetes\u548c\u4e91\u539f\u751f\u73af\u5883\u8bbe\u8ba1\uff0c\u91c7\u7528\u5f00\u6e90\u6a21\u5f0f\u5e76\u96c6\u6210\u5230CNCF\u751f\u6001", "result": "\u6210\u529f\u5f00\u53d1\u51faHolmesGPT\uff0c\u5e76\u4e8e2023\u5e7410\u6708\u88abCNCF\u63a5\u53d7\u4e3a\u6c99\u76d2\u9879\u76ee\uff0c\u83b7\u5f97\u793e\u533a\u8ba4\u53ef", "conclusion": "HolmesGPT\u4e3a\u4e91\u539f\u751f\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684AI\u9a71\u52a8\u6545\u969c\u6392\u9664\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u6539\u5584\u8fd0\u7ef4\u4f53\u9a8c", "topic": "agent analysis"}}
{"id": "tldr.2601.a26ef500", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FB6o0wU/1/0100019ba2ec3c7a-961d8107-2d81-47d6-8530-db86bc598867-000000/15cRiYBWeIsV0P39f8NbbZ_i__zpyYywX8iZkiTZZHY=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FB6o0wU/1/0100019ba2ec3c7a-961d8107-2d81-47d6-8530-db86bc598867-000000/15cRiYBWeIsV0P39f8NbbZ_i__zpyYywX8iZkiTZZHY=439", "authors": ["TLDR Newsletter"], "title": "Eliza Ecosystem: Full Stack for Autonomous Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FB6o0wU/1/0100019ba2ec3c7a-961d8107-2d81-47d6-8530-db86bc598867-000000/15cRiYBWeIsV0P39f8NbbZ_i__zpyYywX8iZkiTZZHY=439", "summary": "Eliza Ecosystem: Full Stack for Autonomous Agents (7 minute read) ElizaOS launched a four-layer full-stack architecture for autonomous agents, comprising consumer-facing agent applications (Layer 1), Eliza Cloud managed runtimes (Layer 2), MCP integrations with x402 payment rails and TEE-based verifiability (Layer 3), and Jeju blockchain providing onchain identity, reputation, and agent-to-agent commerce primitives (Layer 4). The project positions itself as addressing fragmentation in the cur...", "source": "tldr", "AI": {"tldr": "ElizaOS\u63a8\u51fa\u56db\u5c42\u5168\u6808\u67b6\u6784\u7528\u4e8e\u81ea\u4e3b\u4ee3\u7406\uff0c\u5305\u62ec\u5e94\u7528\u5c42\u3001\u4e91\u8fd0\u884c\u65f6\u5c42\u3001\u652f\u4ed8\u4e0e\u9a8c\u8bc1\u96c6\u6210\u5c42\u3001\u4ee5\u53ca\u533a\u5757\u94fe\u8eab\u4efd\u4e0e\u5546\u52a1\u5c42\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u81ea\u4e3b\u4ee3\u7406\u751f\u6001\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u4e92\u64cd\u4f5c\u6027\u6807\u51c6\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u7684\u89c4\u6a21\u5316\u90e8\u7f72\u548c\u5546\u4e1a\u5316\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u56db\u5c42\u5168\u6808\u67b6\u6784\uff1a1) \u9762\u5411\u6d88\u8d39\u8005\u7684\u4ee3\u7406\u5e94\u7528\u5c42\uff1b2) Eliza Cloud\u6258\u7ba1\u8fd0\u884c\u65f6\u5c42\uff1b3) MCP\u96c6\u6210\u652f\u4ed8\u901a\u9053\u548cTEE\u53ef\u9a8c\u8bc1\u6027\u5c42\uff1b4) Jeju\u533a\u5757\u94fe\u63d0\u4f9b\u94fe\u4e0a\u8eab\u4efd\u3001\u58f0\u8a89\u548c\u4ee3\u7406\u95f4\u5546\u52a1\u539f\u8bed\u5c42\u3002", "result": "\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u81ea\u4e3b\u4ee3\u7406\u6280\u672f\u6808\uff0c\u63d0\u4f9b\u4e86\u4ece\u5e94\u7528\u5230\u57fa\u7840\u8bbe\u65bd\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4ee3\u7406\u7684\u89c4\u6a21\u5316\u90e8\u7f72\u3001\u5b89\u5168\u652f\u4ed8\u548c\u53ef\u4fe1\u4ea4\u4e92\u3002", "conclusion": "Eliza\u751f\u6001\u7cfb\u7edf\u901a\u8fc7\u5168\u6808\u67b6\u6784\u4e3a\u81ea\u4e3b\u4ee3\u7406\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u6709\u671b\u89e3\u51b3\u884c\u4e1a\u788e\u7247\u5316\u95ee\u9898\uff0c\u63a8\u52a8\u81ea\u4e3b\u4ee3\u7406\u6280\u672f\u7684\u5546\u4e1a\u5316\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.fde47121", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fsecurity%2F2026%2F01%2Fchatgpt-falls-to-new-data-pilfering-attack-as-a-vicious-cycle-in-ai-continues%2F%3Futm_source=tldrinfosec/1/0100019ba31639ab-2e77e3b2-075a-402b-8e54-c09e9251b32a-000000/HD5BBZrbNVLWBkTc3eu1tkg_rfUDh3JPevwPIUqEYDE=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fsecurity%2F2026%2F01%2Fchatgpt-falls-to-new-data-pilfering-attack-as-a-vicious-cycle-in-ai-continues%2F%3Futm_source=tldrinfosec/1/0100019ba31639ab-2e77e3b2-075a-402b-8e54-c09e9251b32a-000000/HD5BBZrbNVLWBkTc3eu1tkg_rfUDh3JPevwPIUqEYDE=439", "authors": ["TLDR Newsletter"], "title": "ChatGPT falls to new data-pilfering attack as a vicious cycle in AI continues", "comment": "Source: TLDR Newsletter, Date: 2026-01-09, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fsecurity%2F2026%2F01%2Fchatgpt-falls-to-new-data-pilfering-attack-as-a-vicious-cycle-in-ai-continues%2F%3Futm_source=tldrinfosec/1/0100019ba31639ab-2e77e3b2-075a-402b-8e54-c09e9251b32a-000000/HD5BBZrbNVLWBkTc3eu1tkg_rfUDh3JPevwPIUqEYDE=439", "summary": "ChatGPT falls to new data-pilfering attack as a vicious cycle in AI continues (4 minute read) Security researchers at Radware show how a new \u201cZombieAgent\u201d attack revives a previously blocked ChatGPT data\u2011exfiltration technique by tweaking prompts to bypass URL restrictions and leak information character by character. Indirect prompt injection and weak separation between user instructions and embedded content keep LLM agents vulnerable, forcing vendors into an endless patch-and-bypass cycle ra...", "source": "tldr", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u65b0\u7684\"ZombieAgent\"\u653b\u51fb\uff0c\u901a\u8fc7\u4fee\u6539\u63d0\u793a\u7ed5\u8fc7ChatGPT\u7684URL\u9650\u5236\uff0c\u4ee5\u5b57\u7b26\u4e3a\u5355\u4f4d\u6cc4\u9732\u6570\u636e\uff0c\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u548c\u7528\u6237\u6307\u4ee4\u4e0e\u5d4c\u5165\u5185\u5bb9\u5206\u79bb\u4e0d\u8db3\u65b9\u9762\u7684\u6301\u7eed\u6f0f\u6d1e\u3002", "motivation": "\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u5b89\u5168\u9632\u62a4\u65b9\u9762\u7684\u6301\u7eed\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6570\u636e\u7a83\u53d6\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\u5bb9\u6613\u88ab\u7ed5\u8fc7\uff0c\u5f62\u6210\"\u8865\u4e01-\u7ed5\u8fc7\"\u7684\u6076\u6027\u5faa\u73af\u3002", "method": "Radware\u5b89\u5168\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\"ZombieAgent\"\u653b\u51fb\u6280\u672f\uff0c\u901a\u8fc7\u4fee\u6539\u63d0\u793a\u8bcd\u6765\u7ed5\u8fc7ChatGPT\u7684URL\u9650\u5236\uff0c\u91c7\u7528\u9010\u5b57\u7b26\u6cc4\u9732\u6570\u636e\u7684\u65b9\u5f0f\uff0c\u5229\u7528\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u548c\u7528\u6237\u6307\u4ee4\u4e0e\u5d4c\u5165\u5185\u5bb9\u5206\u79bb\u4e0d\u8db3\u7684\u6f0f\u6d1e\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5982\u4f55\u590d\u6d3b\u5148\u524d\u88ab\u963b\u6b62\u7684ChatGPT\u6570\u636e\u7a83\u53d6\u6280\u672f\uff0c\u8bc1\u660e\u5373\u4f7f\u6709\u9632\u62a4\u63aa\u65bd\uff0c\u653b\u51fb\u8005\u4ecd\u80fd\u901a\u8fc7\u5de7\u5999\u8bbe\u8ba1\u7684\u63d0\u793a\u7ed5\u8fc7\u9650\u5236\uff0c\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u548c\u5185\u5bb9\u5206\u79bb\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u5b89\u5168\u5382\u5546\u9677\u5165\u65e0\u4f11\u6b62\u7684\"\u8865\u4e01-\u7ed5\u8fc7\"\u5faa\u73af\uff0c\u9700\u8981\u66f4\u6839\u672c\u7684\u67b6\u6784\u6539\u8fdb\u800c\u975e\u4e34\u65f6\u4fee\u590d\u3002", "topic": "agent analysis"}}
