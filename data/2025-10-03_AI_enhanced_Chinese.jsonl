{"id": "2510.01379", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01379", "abs": "https://arxiv.org/abs/2510.01379", "authors": ["Huashan Chen", "Zhenyu Qi", "Haotang Li", "Hong Chen", "Jinfu Chen", "Kebin Peng", "In Kee Kim", "Kyu Hyung Lee", "Sen He"], "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration", "comment": null, "summary": "While Large Language Models (LLMs) have become the predominant paradigm for\nautomated code generation, current single-model approaches fundamentally ignore\nthe heterogeneous computational strengths that different models exhibit across\nprogramming languages, algorithmic domains, and development stages. This paper\nchallenges the single-model convention by introducing a multi-stage,\nperformance-guided orchestration framework that dynamically routes coding tasks\nto the most suitable LLMs within a structured generate-fix-refine workflow. Our\napproach is grounded in a comprehensive empirical study of 17 state-of-the-art\nLLMs across five programming languages (Python, Java, C++, Go, and Rust) using\nHumanEval-X benchmark. The study, which evaluates both functional correctness\nand runtime performance metrics (execution time, mean/max memory utilization,\nand CPU efficiency), reveals pronounced performance heterogeneity by language,\ndevelopment stage, and problem category. Guided by these empirical insights, we\npresent PerfOrch, an LLM agent that orchestrates top-performing LLMs for each\ntask context through stage-wise validation and rollback mechanisms. Without\nrequiring model fine-tuning, PerfOrch achieves substantial improvements over\nstrong single-model baselines: average correctness rates of 96.22% and 91.37%\non HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and\n49.11%. Beyond correctness gains, the framework delivers consistent performance\noptimizations, improving execution time for 58.76% of problems with median\nspeedups ranging from 17.67% to 27.66% across languages on two benchmarks. The\nframework's plug-and-play architecture ensures practical scalability, allowing\nnew LLMs to be profiled and integrated seamlessly, thereby offering a paradigm\nfor production-grade automated software engineering that adapts to the rapidly\nevolving generative AI landscape.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u9636\u6bb5\u3001\u6027\u80fd\u5bfc\u5411\u7684\u7f16\u6392\u6846\u67b6PerfOrch\uff0c\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684LLM\u6765\u5904\u7406\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u548c\u5f00\u53d1\u9636\u6bb5\u7684\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u6b63\u786e\u6027\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5355\u4e00\u6a21\u578b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e0d\u540cLLM\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u3001\u7b97\u6cd5\u9886\u57df\u548c\u5f00\u53d1\u9636\u6bb5\u7684\u8ba1\u7b97\u4f18\u52bf\u5f02\u8d28\u6027\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6a21\u578b\u9009\u62e9\u548c\u7f16\u6392\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u5bf917\u4e2a\u6700\u5148\u8fdbLLM\u57285\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5f00\u53d1\u4e86\u591a\u9636\u6bb5\u751f\u6210-\u4fee\u590d-\u4f18\u5316\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u9636\u6bb5\u9a8c\u8bc1\u548c\u56de\u6eda\u673a\u5236\u52a8\u6001\u8def\u7531\u4efb\u52a1\u5230\u6700\u9002\u5408\u7684LLM\u3002", "result": "\u5728HumanEval-X\u548cEffiBench-X\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523096.22%\u548c91.37%\u7684\u6b63\u786e\u7387\uff0c\u663e\u8457\u8d85\u8d8aGPT-4o\uff1b58.76%\u7684\u95ee\u9898\u6267\u884c\u65f6\u95f4\u5f97\u5230\u4f18\u5316\uff0c\u4e2d\u4f4d\u52a0\u901f\u6bd4\u8fbe17.67%-27.66%\u3002", "conclusion": "PerfOrch\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5373\u63d2\u5373\u7528\u67b6\u6784\uff0c\u80fd\u591f\u9002\u5e94\u5feb\u901f\u53d1\u5c55\u7684\u751f\u6210\u5f0fAI\u73af\u5883\uff0c\u4e3a\u751f\u4ea7\u7ea7\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2510.01272", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01272", "abs": "https://arxiv.org/abs/2510.01272", "authors": ["Kunal Jha", "Aydan Yuenan Huang", "Eric Ye", "Natasha Jaques", "Max Kleiman-Weiner"], "title": "Modeling Others' Minds as Code", "comment": null, "summary": "Accurate prediction of human behavior is essential for robust and safe\nhuman-AI collaboration. However, existing approaches for modeling people are\noften data-hungry and brittle because they either make unrealistic assumptions\nabout rationality or are too computationally demanding to adapt rapidly. Our\nkey insight is that many everyday social interactions may follow predictable\npatterns; efficient \"scripts\" that minimize cognitive load for actors and\nobservers, e.g., \"wait for the green light, then go.\" We propose modeling these\nroutines as behavioral programs instantiated in computer code rather than\npolicies conditioned on beliefs and desires. We introduce ROTE, a novel\nalgorithm that leverages both large language models (LLMs) for synthesizing a\nhypothesis space of behavioral programs, and probabilistic inference for\nreasoning about uncertainty over that space. We test ROTE in a suite of\ngridworld tasks and a large-scale embodied household simulator. ROTE predicts\nhuman and AI behaviors from sparse observations, outperforming competitive\nbaselines -- including behavior cloning and LLM-based methods -- by as much as\n50% in terms of in-sample accuracy and out-of-sample generalization. By\ntreating action understanding as a program synthesis problem, ROTE opens a path\nfor AI systems to efficiently and effectively predict human behavior in the\nreal-world.", "AI": {"tldr": "\u63d0\u51fa\u4e86ROTE\u7b97\u6cd5\uff0c\u5c06\u4eba\u7c7b\u884c\u4e3a\u5efa\u6a21\u4e3a\u8ba1\u7b97\u673a\u7a0b\u5e8f\u800c\u975e\u4fe1\u5ff5\u9a71\u52a8\u7684\u7b56\u7565\uff0c\u7ed3\u5408LLM\u751f\u6210\u884c\u4e3a\u7a0b\u5e8f\u5047\u8bbe\u7a7a\u95f4\u548c\u6982\u7387\u63a8\u7406\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u7f51\u683c\u4e16\u754c\u548c\u5bb6\u5ead\u6a21\u62df\u5668\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u884c\u4e3a\u5efa\u6a21\u65b9\u6cd5\u8981\u4e48\u5bf9\u7406\u6027\u505a\u51fa\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff0c\u8981\u4e48\u8ba1\u7b97\u91cf\u8fc7\u5927\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u3002\u65e5\u5e38\u793e\u4ea4\u4e92\u52a8\u5f80\u5f80\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u6a21\u5f0f\uff08\u811a\u672c\uff09\uff0c\u8fd9\u4e9b\u9ad8\u6548\u7684\u884c\u4e3a\u6a21\u5f0f\u80fd\u6700\u5c0f\u5316\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "ROTE\u7b97\u6cd5\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5408\u6210\u884c\u4e3a\u7a0b\u5e8f\u7684\u5047\u8bbe\u7a7a\u95f4\uff1b2\uff09\u4f7f\u7528\u6982\u7387\u63a8\u7406\u5728\u8be5\u7a7a\u95f4\u4e0a\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\uff1b3\uff09\u5c06\u52a8\u4f5c\u7406\u89e3\u89c6\u4e3a\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u5bb6\u5ead\u6a21\u62df\u5668\u4e2d\u6d4b\u8bd5\uff0cROTE\u4ece\u7a00\u758f\u89c2\u5bdf\u4e2d\u9884\u6d4b\u4eba\u7c7b\u548cAI\u884c\u4e3a\uff0c\u5728\u6837\u672c\u5185\u51c6\u786e\u6027\u548c\u6837\u672c\u5916\u6cdb\u5316\u65b9\u9762\u6bd4\u7ade\u4e89\u57fa\u7ebf\uff08\u5305\u62ec\u884c\u4e3a\u514b\u9686\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff09\u9ad8\u51fa\u591a\u8fbe50%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u52a8\u4f5c\u7406\u89e3\u89c6\u4e3a\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0cROTE\u4e3aAI\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u9ad8\u6548\u6709\u6548\u5730\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.01227", "categories": ["cs.CL", "math.HO"], "pdf": "https://arxiv.org/pdf/2510.01227", "abs": "https://arxiv.org/abs/2510.01227", "authors": ["Nicole N Khatibi", "Daniil A. Radamovich", "Michael P. Brenner"], "title": "EEFSUVA: A New Mathematical Olympiad Benchmark", "comment": "16 Pages, 5 figures", "summary": "Recent breakthroughs have spurred claims that large language models (LLMs)\nmatch gold medal Olympiad to graduate level proficiency on mathematics\nbenchmarks. In this work, we examine these claims in detail and assess the\nextent to which current benchmarks capture genuine LLM mathematical reasoning.\nThe composition of these benchmarks, primarily drawing from the International\nMathematics Olympiad (IMO) and related competitions, may overstate models\nreasoning ability due to potential data contamination and a narrow focus on\nfamiliar problem types. To enable a more holistic assessment of mathematical\nunderstanding, we introduce EEFSUVA, a novel benchmark curated from under\ncirculated regional and national Olympiads of Eastern Europe and the countries\nfrom the former Soviet Union. These contests feature problems of comparable\ndifficulty to the IMO and are renowned for demanding nonstandard\nproblem-solving techniques, yet their problems are far less prevalent in online\ncorpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a\nnotable performance decline on EEFSUVA relative to other Olympiad-style\nbenchmarks. These findings also suggest the potential importance of broader\nevaluation datasets for a fuller assessment of mathematical reasoning and for\nguiding future model development.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u5f53\u524d\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86EEFSUVA\u65b0\u57fa\u51c6\uff0c\u53d1\u73b0LLM\u5728\u65b0\u57fa\u51c6\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5f53\u524d\u6570\u5b66\u57fa\u51c6\u4e3b\u8981\u6765\u81eaIMO\u7b49\u77e5\u540d\u7ade\u8d5b\uff0c\u53ef\u80fd\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u95ee\u9898\u7c7b\u578b\u5355\u4e00\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u7684\u771f\u5b9e\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efaEEFSUVA\u57fa\u51c6\uff0c\u6536\u96c6\u4e1c\u6b27\u548c\u524d\u82cf\u8054\u5730\u533a\u8f83\u5c11\u6d41\u901a\u7684\u533a\u57df\u6027\u548c\u56fd\u5bb6\u7ea7\u5965\u8d5b\u9898\u76ee\uff0c\u8fd9\u4e9b\u9898\u76ee\u96be\u5ea6\u4e0eIMO\u76f8\u5f53\u4f46\u89e3\u9898\u65b9\u6cd5\u66f4\u975e\u6807\u51c6\u5316\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728EEFSUVA\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u4e5f\u663e\u8457\u4f4e\u4e8e\u5176\u4ed6\u5965\u8d5b\u98ce\u683c\u57fa\u51c6\u3002", "conclusion": "\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u6765\u5168\u9762\u8bc4\u4f30\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u6a21\u578b\u5f00\u53d1\u3002", "topic": "agent analysis"}}
{"id": "2510.01295", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01295", "abs": "https://arxiv.org/abs/2510.01295", "authors": ["Zarreen Reza"], "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop on Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling", "summary": "As Large Language Models (LLMs) transition from static tools to autonomous\nagents, traditional evaluation benchmarks that measure performance on\ndownstream tasks are becoming insufficient. These methods fail to capture the\nemergent social and cognitive dynamics that arise when agents communicate,\npersuade, and collaborate in interactive environments. To address this gap, we\nintroduce a novel evaluation framework that uses multi-agent debate as a\ncontrolled \"social laboratory\" to discover and quantify these behaviors. In our\nframework, LLM-based agents, instantiated with distinct personas and\nincentives, deliberate on a wide range of challenging topics under the\nsupervision of an LLM moderator. Our analysis, enabled by a new suite of\npsychometric and semantic metrics, reveals several key findings. Across\nhundreds of debates, we uncover a powerful and robust emergent tendency for\nagents to seek consensus, consistently reaching high semantic agreement ({\\mu}\n> 0.88) even without explicit instruction and across sensitive topics. We show\nthat assigned personas induce stable, measurable psychometric profiles,\nparticularly in cognitive effort, and that the moderators persona can\nsignificantly alter debate outcomes by structuring the environment, a key\nfinding for external AI alignment. This work provides a blueprint for a new\nclass of dynamic, psychometrically grounded evaluation protocols designed for\nthe agentic setting, offering a crucial methodology for understanding and\nshaping the social behaviors of the next generation of AI agents. We have\nreleased the code and results at\nhttps://github.com/znreza/multi-agent-LLM-eval-for-debate.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316LLM\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u793e\u4ea4\u548c\u8ba4\u77e5\u884c\u4e3a\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u5177\u6709\u5f3a\u70c8\u7684\u5171\u8bc6\u5bfb\u6c42\u503e\u5411\uff0c\u5e76\u80fd\u901a\u8fc7\u89d2\u8272\u5206\u914d\u5f62\u6210\u7a33\u5b9a\u7684\u5fc3\u7406\u6d4b\u91cf\u7279\u5f81\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u4ea7\u751f\u7684\u793e\u4ea4\u548c\u8ba4\u77e5\u52a8\u6001\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4f5c\u4e3a\"\u793e\u4ea4\u5b9e\u9a8c\u5ba4\"\uff0c\u8ba9\u5177\u6709\u4e0d\u540c\u89d2\u8272\u548c\u6fc0\u52b1\u7684LLM\u667a\u80fd\u4f53\u5728LLM\u4e3b\u6301\u4eba\u7684\u76d1\u7763\u4e0b\u5c31\u5404\u79cd\u8bdd\u9898\u8fdb\u884c\u8fa9\u8bba\uff0c\u5e76\u91c7\u7528\u5fc3\u7406\u6d4b\u91cf\u548c\u8bed\u4e49\u6307\u6807\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4f53\u5177\u6709\u5f3a\u5927\u7684\u5171\u8bc6\u5bfb\u6c42\u503e\u5411\uff08\u8bed\u4e49\u4e00\u81f4\u6027\u03bc>0.88\uff09\uff0c\u89d2\u8272\u5206\u914d\u80fd\u8bf1\u5bfc\u7a33\u5b9a\u7684\u5fc3\u7406\u6d4b\u91cf\u7279\u5f81\uff0c\u4e3b\u6301\u4eba\u7684\u89d2\u8272\u80fd\u663e\u8457\u6539\u53d8\u8fa9\u8bba\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9762\u5411\u667a\u80fd\u4f53\u573a\u666f\u7684\u52a8\u6001\u3001\u5fc3\u7406\u6d4b\u91cf\u57fa\u7840\u7684\u8bc4\u4f30\u534f\u8bae\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u662f\u7406\u89e3\u548c\u5851\u9020\u4e0b\u4e00\u4ee3AI\u667a\u80fd\u4f53\u793e\u4ea4\u884c\u4e3a\u7684\u5173\u952e\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.01825", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01825", "abs": "https://arxiv.org/abs/2510.01825", "authors": ["Zhenyu Yang", "Yue Pan", "Zhen Yang", "Zhongxing Yu"], "title": "Towards Speeding up Program Repair with Non-Autoregressive Model", "comment": "30 pages, 8 figures, 7 tables. arXiv admin note: substantial text\n  overlap with arXiv:2406.16526", "summary": "Enlightened by the success of machine learning techniques in various\napplication areas, recent years have witnessed a surge of research efforts on\nautomatic program repair (APR) using machine learning techniques. Previous\nmachine learning-based APR techniques essentially modified bugs in the\nautoregressive (AR) manner, which predicts future values based on past values.\nDue to the manner of token-by-token generation, the AR-based APR technique has\na huge time delay. In particular, the delay of the APR model with a large\nnumber of parameters is more serious. To address the issue, we aim to apply the\nnon-autoregressive (NAR) method to the APR task, which can output target code\nin a parallel manner to avoid huge repair delays. However, the naive use of the\nNAR manner for the APR task suffers from the issue of compromised patch\nquality. To effectively adapt the NAR manner for the APR task, we in this paper\npropose NARRepair, the first customized NAR code generation model for the APR\ntask. The NARRepair model features three major novelties, including 1) the\nrepair action predictor for alleviating the over-correction issue, 2) the\ninter-token dependency extractor for alleviating the issue of lacking\ninter-token dependency information, and 3) the two-stage decoder for\nalleviating the issue of lacking contextual information. We evaluated NARRepair\non three widely used datasets in the APR community, and the results show that\n1) compared to other APR techniques, the NARRepair model has the best\nperformance within the limited repair time, and 2) compared to AR-based APR\ntechniques, the repair speed of NARRepair has been increased by 1.4-6.4 times\nin the GPU environment. Overall, the results show that NARRepair has achieved\nstate-of-the-art comprehensive performance in terms of repair speed and\naccuracy.", "AI": {"tldr": "NARRepair\u662f\u9996\u4e2a\u4e3a\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u5b9a\u5236\u7684\u975e\u81ea\u56de\u5f52\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4fee\u590d\u52a8\u4f5c\u9884\u6d4b\u5668\u3001\u4ee4\u724c\u95f4\u4f9d\u8d56\u63d0\u53d6\u5668\u548c\u4e24\u9636\u6bb5\u89e3\u7801\u5668\u89e3\u51b3NAR\u65b9\u6cd5\u5728APR\u4efb\u52a1\u4e2d\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u5728\u4fee\u590d\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7efc\u5408\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u6280\u672f\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u5f0f\uff0c\u5b58\u5728\u5de8\u5927\u7684\u65f6\u95f4\u5ef6\u8fdf\u95ee\u9898\uff0c\u7279\u522b\u662f\u53c2\u6570\u8f83\u591a\u7684\u6a21\u578b\u5ef6\u8fdf\u66f4\u4e25\u91cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\u5e94\u7528\u4e8eAPR\u4efb\u52a1\uff0c\u5b9e\u73b0\u5e76\u884c\u8f93\u51fa\u76ee\u6807\u4ee3\u7801\u4ee5\u907f\u514d\u4fee\u590d\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faNARRepair\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u521b\u65b0\uff1a1\uff09\u4fee\u590d\u52a8\u4f5c\u9884\u6d4b\u5668\u7f13\u89e3\u8fc7\u5ea6\u4fee\u6b63\u95ee\u9898\uff1b2\uff09\u4ee4\u724c\u95f4\u4f9d\u8d56\u63d0\u53d6\u5668\u89e3\u51b3\u7f3a\u4e4f\u4ee4\u724c\u95f4\u4f9d\u8d56\u4fe1\u606f\u95ee\u9898\uff1b3\uff09\u4e24\u9636\u6bb5\u89e3\u7801\u5668\u89e3\u51b3\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4fe1\u606f\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684APR\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5728\u6709\u9650\u4fee\u590d\u65f6\u95f4\u5185\uff0cNARRepair\u6027\u80fd\u6700\u4f73\uff1b2\uff09\u4e0eAR-based APR\u6280\u672f\u76f8\u6bd4\uff0c\u5728GPU\u73af\u5883\u4e2d\u4fee\u590d\u901f\u5ea6\u63d0\u9ad8\u4e861.4-6.4\u500d\u3002", "conclusion": "NARRepair\u5728\u4fee\u590d\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7efc\u5408\u6027\u80fd\uff0c\u6210\u529f\u5c06\u975e\u81ea\u56de\u5f52\u65b9\u6cd5\u5e94\u7528\u4e8e\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u4efb\u52a1\u3002", "topic": "swe application"}}
{"id": "2510.01304", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01304", "abs": "https://arxiv.org/abs/2510.01304", "authors": ["Yu Zeng", "Wenxuan Huang", "Shiting Huang", "Xikun Bao", "Yukun Qi", "Yiming Zhao", "Qiuchen Wang", "Lin Chen", "Zehui Chen", "Huaian Chen", "Wanli Ouyang", "Feng Zhao"], "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models", "comment": null, "summary": "Although current large Vision-Language Models (VLMs) have advanced in\nmultimodal understanding and reasoning, their fundamental perceptual and\nreasoning abilities remain limited. Specifically, even on simple jigsaw tasks,\nexisting VLMs perform near randomly, revealing deficiencies in core perception\nand reasoning capabilities. While high-quality vision-language data can enhance\nthese capabilities, its scarcity and limited scalability impose significant\nconstraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction\nLearning for Enhancing visual perception and reasoning in VLMs. AGILE\nformulates jigsaw solving as an interactive process, enabling the model to\nprogressively engage with the environment. At each step, the model generates\nexecutable code to perform an action based on the current state, while the\nenvironment provides fine-grained visual feedback to guide task completion.\nThrough this iterative cycle of observation and interaction, the model\nincrementally improves its perceptual and reasoning capabilities via\nexploration and feedback. Experimental results show that AGILE not only\nsubstantially boosts performance on jigsaw tasks of varying complexity (e.g.,\nincreasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also\ndemonstrates strong generalization across 9 general vision tasks, achieving an\naverage improvement of 3.1%. These results indicate notable enhancements in\nboth perceptual and reasoning abilities. This work opens a new avenue for\nadvancing reasoning and generalization in multimodal models and provides an\nefficient, scalable solution to the scarcity of multimodal reinforcement\nlearning data. The code and datasets is available at\nhttps://github.com/yuzeng0-0/AGILE .", "AI": {"tldr": "AGILE\u901a\u8fc7\u5c06\u62fc\u56fe\u4efb\u52a1\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4ee3\u7801\u6267\u884c\u52a8\u4f5c\u5e76\u63a5\u6536\u89c6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86VLM\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5728\u62fc\u56fe\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ece9.5%\u63d0\u5347\u81f382.8%\uff0c\u5e76\u57289\u4e2a\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53473.1%\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u867d\u6709\u8fdb\u6b65\uff0c\u4f46\u5176\u57fa\u672c\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u62fc\u56fe\u4efb\u52a1\u4e0a\u8868\u73b0\u4e5f\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u9ad8\u8d28\u91cf\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u6709\u9650\u53ef\u6269\u5c55\u6027\u9650\u5236\u4e86\u8fd9\u4e9b\u80fd\u529b\u7684\u63d0\u5347\u3002", "method": "AGILE\u5c06\u62fc\u56fe\u89e3\u51b3\u5236\u5b9a\u4e3a\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u6a21\u578b\u5728\u6bcf\u4e2a\u6b65\u9aa4\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u6765\u6267\u884c\u52a8\u4f5c\uff0c\u73af\u5883\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u89c6\u89c9\u53cd\u9988\u6765\u6307\u5bfc\u4efb\u52a1\u5b8c\u6210\u3002\u901a\u8fc7\u89c2\u5bdf\u548c\u4ea4\u4e92\u7684\u8fed\u4ee3\u5faa\u73af\uff0c\u6a21\u578b\u901a\u8fc7\u63a2\u7d22\u548c\u53cd\u9988\u9010\u6b65\u63d0\u5347\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "AGILE\u5728\u590d\u6742\u5ea6\u4e0d\u540c\u7684\u62fc\u56fe\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u57282\u00d72\u8bbe\u7f6e\u4e0b\u51c6\u786e\u7387\u4ece9.5%\u63d0\u5347\u81f382.8%\uff09\uff0c\u5e76\u57289\u4e2a\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u63d0\u53473.1%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u63a8\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01960", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.01960", "abs": "https://arxiv.org/abs/2510.01960", "authors": ["Victor Lira", "Paulo Borba", "Rodrigo Bonif\u00e1cio", "Galileu Santos e Matheus barbosa"], "title": "RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis", "comment": null, "summary": "Detecting semantic interference remains a challenge in collaborative software\ndevelopment. Recent lightweight static analysis techniques improve efficiency\nover SDG-based methods, but they still suffer from a high rate of false\npositives. A key cause of these false positives is the presence of\nbehavior-preserving code refactorings, which current techniques cannot\neffectively distinguish from changes that impact behavior and can interfere\nwith others. To handle this problem we present RefFilter, a refactoring-aware\ntool for semantic interference detection. It builds on existing static\ntechniques by incorporating automated refactoring detection to improve\nprecision. RefFilter discards behavior-preserving refactorings from reports,\nreducing false positives while preserving detection coverage. To evaluate\neffectiveness and scalability, use two datasets: a labeled dataset with 99\nscenarios and ground truth, and a novel dataset of 1,087 diverse merge\nscenarios that we have built. Experimental results show that RefFilter reduces\nfalse positives by nearly 32% on the labeled dataset. While this reduction\ncomes with a non significant increase in false negatives, the overall gain in\nprecision significantly outweighs the minor trade-off in recall. These findings\ndemonstrate that refactoring-aware interference detection is a practical and\neffective strategy for improving merge support in modern development workflows.", "AI": {"tldr": "RefFilter\u662f\u4e00\u4e2a\u91cd\u6784\u611f\u77e5\u7684\u8bed\u4e49\u5e72\u6270\u68c0\u6d4b\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u91cd\u6784\u6765\u51cf\u5c11\u73b0\u6709\u9759\u6001\u5206\u6790\u6280\u672f\u4e2d\u7684\u8bef\u62a5\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u8986\u76d6\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5047\u9633\u6027\u7387\u3002", "motivation": "\u5f53\u524d\u8f7b\u91cf\u7ea7\u9759\u6001\u5206\u6790\u6280\u672f\u5728\u534f\u4f5c\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u68c0\u6d4b\u8bed\u4e49\u5e72\u6270\u65f6\u5b58\u5728\u9ad8\u8bef\u62a5\u7387\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65e0\u6cd5\u6709\u6548\u533a\u5206\u884c\u4e3a\u4fdd\u6301\u7684\u4ee3\u7801\u91cd\u6784\u548c\u771f\u6b63\u5f71\u54cd\u884c\u4e3a\u7684\u53d8\u66f4\u3002", "method": "\u5728\u73b0\u6709\u9759\u6001\u5206\u6790\u6280\u672f\u57fa\u7840\u4e0a\u96c6\u6210\u81ea\u52a8\u91cd\u6784\u68c0\u6d4b\u529f\u80fd\uff0c\u4ece\u62a5\u544a\u4e2d\u8fc7\u6ee4\u6389\u884c\u4e3a\u4fdd\u6301\u7684\u91cd\u6784\u64cd\u4f5c\uff0c\u4ece\u800c\u51cf\u5c11\u8bef\u62a5\u3002", "result": "\u5728\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\uff0cRefFilter\u5c06\u8bef\u62a5\u7387\u964d\u4f4e\u4e86\u8fd132%\uff0c\u867d\u7136\u4f34\u968f\u4e86\u4e0d\u663e\u8457\u7684\u5047\u9634\u6027\u589e\u52a0\uff0c\u4f46\u7cbe\u5ea6\u7684\u6574\u4f53\u63d0\u5347\u663e\u8457\u8d85\u8fc7\u4e86\u53ec\u56de\u7387\u7684\u5fae\u5c0f\u635f\u5931\u3002", "conclusion": "\u91cd\u6784\u611f\u77e5\u7684\u5e72\u6270\u68c0\u6d4b\u662f\u6539\u8fdb\u73b0\u4ee3\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u5408\u5e76\u652f\u6301\u7684\u5b9e\u9645\u6709\u6548\u7b56\u7565\u3002", "topic": "swe application"}}
{"id": "2510.01353", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01353", "abs": "https://arxiv.org/abs/2510.01353", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Anand Kannappan", "Rebecca Qian", "Peng Wang"], "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments", "comment": "Accepted to NeurIPS 2025 SEA Workshop", "summary": "Recent works on context and memory benchmarking have primarily focused on\nconversational instances but the need for evaluating memory in dynamic\nenterprise environments is crucial for its effective application. We introduce\nMEMTRACK, a benchmark designed to evaluate long-term memory and state tracking\nin multi-platform agent environments. MEMTRACK models realistic organizational\nworkflows by integrating asynchronous events across multiple communication and\nproductivity platforms such as Slack, Linear and Git. Each benchmark instance\nprovides a chronologically platform-interleaved timeline, with noisy,\nconflicting, cross-referring information as well as potential\ncodebase/file-system comprehension and exploration. Consequently, our benchmark\ntests memory capabilities such as acquistion, selection and conflict\nresolution. We curate the MEMTRACK dataset through both manual expert driven\ndesign and scalable agent based synthesis, generating ecologically valid\nscenarios grounded in real world software development processes. We introduce\npertinent metrics for Correctness, Efficiency, and Redundancy that capture the\neffectiveness of memory mechanisms beyond simple QA performance. Experiments\nacross SoTA LLMs and memory backends reveal challenges in utilizing memory\nacross long horizons, handling cross-platform dependencies, and resolving\ncontradictions. Notably, the best performing GPT-5 model only achieves a 60\\%\nCorrectness score on MEMTRACK. This work provides an extensible framework for\nadvancing evaluation research for memory-augmented agents, beyond existing\nfocus on conversational setups, and sets the stage for multi-agent,\nmulti-platform memory benchmarking in complex organizational settings", "AI": {"tldr": "MEMTRACK\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u5e73\u53f0\u4ee3\u7406\u73af\u5883\u4e2d\u957f\u671f\u8bb0\u5fc6\u548c\u72b6\u6001\u8ddf\u8e2a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u7ec4\u7ec7\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6574\u5408Slack\u3001Linear\u548cGit\u7b49\u5e73\u53f0\u7684\u5f02\u6b65\u4e8b\u4ef6\u3002", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u8bdd\u573a\u666f\uff0c\u4f46\u4f01\u4e1a\u52a8\u6001\u73af\u5883\u4e2d\u8bb0\u5fc6\u8bc4\u4f30\u7684\u9700\u6c42\u5bf9\u4e8e\u6709\u6548\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u624b\u52a8\u8bbe\u8ba1\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u5408\u6210\u6765\u7b56\u5212MEMTRACK\u6570\u636e\u96c6\uff0c\u751f\u6210\u57fa\u4e8e\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u7684\u751f\u6001\u6709\u6548\u573a\u666f\u3002", "result": "\u5728SoTA LLM\u548c\u8bb0\u5fc6\u540e\u7aef\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u957f\u65f6\u7a0b\u5229\u7528\u8bb0\u5fc6\u3001\u5904\u7406\u8de8\u5e73\u53f0\u4f9d\u8d56\u548c\u89e3\u51b3\u77db\u76fe\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u8868\u73b0\u6700\u597d\u7684GPT-5\u6a21\u578b\u5728MEMTRACK\u4e0a\u4ec5\u8fbe\u523060%\u7684\u6b63\u786e\u6027\u5f97\u5206\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\u7684\u8bc4\u4f30\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5bf9\u8bdd\u8bbe\u7f6e\u7684\u5173\u6ce8\uff0c\u5e76\u4e3a\u590d\u6742\u7ec4\u7ec7\u73af\u5883\u4e2d\u7684\u591a\u4ee3\u7406\u3001\u591a\u5e73\u53f0\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.01264", "categories": ["cs.LG", "cs.RO", "68T42 (Primary) 68T40, 68T05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.01264", "abs": "https://arxiv.org/abs/2510.01264", "authors": ["Isaac Peterson", "Christopher Allred", "Jacob Morrey", "Mario Harper"], "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab", "comment": "8 page, 9 figures, code https://github.com/DIRECTLab/IsaacLab-HARL", "summary": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems\ncooperating in dynamic environments. While prior work has focused on these\ncollaborative settings, adversarial interactions are equally critical for\nreal-world applications such as pursuit-evasion, security, and competitive\nmanipulation. In this work, we extend the IsaacLab framework to support\nscalable training of adversarial policies in high-fidelity physics simulations.\nWe introduce a suite of adversarial MARL environments featuring heterogeneous\nagents with asymmetric goals and capabilities. Our platform integrates a\ncompetitive variant of Heterogeneous Agent Reinforcement Learning with Proximal\nPolicy Optimization (HAPPO), enabling efficient training and evaluation under\nadversarial dynamics. Experiments across several benchmark scenarios\ndemonstrate the framework's ability to model and train robust policies for\nmorphologically diverse multi-agent competition while maintaining high\nthroughput and simulation realism. Code and benchmarks are available at:\nhttps://github.com/DIRECTLab/IsaacLab-HARL .", "AI": {"tldr": "\u5c06IsaacLab\u6846\u67b6\u6269\u5c55\u5230\u652f\u6301\u5728\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u4e2d\u8bad\u7ec3\u5bf9\u6297\u6027\u7b56\u7565\uff0c\u5f15\u5165\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u73af\u5883\uff0c\u5e76\u96c6\u6210\u7ade\u4e89\u6027HAPPO\u7b97\u6cd5\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5bf9\u6297\u6027\u4ea4\u4e92\u5728\u73b0\u5b9e\u5e94\u7528\uff08\u5982\u8ffd\u9003\u3001\u5b89\u5168\u3001\u7ade\u4e89\u64cd\u4f5c\uff09\u4e2d\u540c\u6837\u5173\u952e\uff0c\u9700\u8981\u652f\u6301\u5f02\u6784\u667a\u80fd\u4f53\u95f4\u7684\u4e0d\u5bf9\u79f0\u76ee\u6807\u548c\u80fd\u529b\u3002", "method": "\u6269\u5c55IsaacLab\u6846\u67b6\uff0c\u6784\u5efa\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u73af\u5883\u5957\u4ef6\uff0c\u96c6\u6210\u7ade\u4e89\u6027\u5f02\u6784\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e0e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08HAPPO\uff09\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5efa\u6a21\u548c\u8bad\u7ec3\u5f62\u6001\u591a\u6837\u5316\u591a\u667a\u80fd\u4f53\u7ade\u4e89\u7684\u9c81\u68d2\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u548c\u6a21\u62df\u771f\u5b9e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u652f\u6301\u4e86\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u7b56\u7565\u7684\u9ad8\u6548\u8bad\u7ec3\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u5bf9\u6297\u6027\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01269", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01269", "abs": "https://arxiv.org/abs/2510.01269", "authors": ["Rohan Vitthal Thorat", "Juhi Singh", "Rajdip Nayek"], "title": "Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance", "comment": "Paper accepted for presentation at ICCMS 2025. The submission\n  includes 10 pages and 6 figures", "summary": "Structural vibrations induced by external excitations pose significant risks,\nincluding safety hazards for occupants, structural damage, and increased\nmaintenance costs. While conventional model-based control strategies, such as\nLinear Quadratic Regulator (LQR), effectively mitigate vibrations, their\nreliance on accurate system models necessitates tedious system identification.\nThis tedious system identification process can be avoided by using a model-free\nReinforcement learning (RL) method. RL controllers derive their policies solely\nfrom observed structural behaviour, eliminating the requirement for an explicit\nstructural model. For an RL controller to be truly model-free, its training\nmust occur on the actual physical system rather than in simulation. However,\nduring this training phase, the RL controller lacks prior knowledge and it\nexerts control force on the structure randomly, which can potentially harm the\nstructure. To mitigate this risk, we propose guiding the RL controller using a\nLinear Quadratic Regulator (LQR) controller. While LQR control typically relies\non an accurate structural model for optimal performance, our observations\nindicate that even an LQR controller based on an entirely incorrect model\noutperforms the uncontrolled scenario. Motivated by this finding, we introduce\na hybrid control framework that integrates both LQR and RL controllers. In this\napproach, the LQR policy is derived from a randomly selected model and its\nparameters. As this LQR policy does not require knowledge of the true or an\napproximate structural model the overall framework remains model-free. This\nhybrid approach eliminates dependency on explicit system models while\nminimizing exploration risks inherent in naive RL implementations. As per our\nknowledge, this is the first study to address the critical training safety\nchallenge of RL-based vibration control and provide a validated solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LQR\u548cRL\u7684\u6df7\u5408\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u632f\u52a8\u63a7\u5236\uff0c\u907f\u514d\u4e86\u5bf9\u51c6\u786e\u7cfb\u7edf\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u964d\u4f4e\u4e86RL\u8bad\u7ec3\u671f\u95f4\u7684\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u632f\u52a8\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u7e41\u7410\u7684\u7cfb\u7edf\u8fa8\u8bc6\uff0c\u800c\u7eafRL\u65b9\u6cd5\u5728\u8bad\u7ec3\u9636\u6bb5\u53ef\u80fd\u5bf9\u7ed3\u6784\u9020\u6210\u635f\u5bb3\u3002\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u57fa\u4e8e\u9519\u8bef\u6a21\u578b\u7684LQR\u63a7\u5236\u5668\u4e5f\u80fd\u4f18\u4e8e\u65e0\u63a7\u5236\u60c5\u51b5\uff0c\u56e0\u6b64\u63d0\u51fa\u6df7\u5408\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u968f\u673a\u9009\u62e9\u6a21\u578b\u7684LQR\u63a7\u5236\u5668\u6765\u5f15\u5bfcRL\u63a7\u5236\u5668\uff0cLQR\u7b56\u7565\u4e0d\u4f9d\u8d56\u771f\u5b9e\u6216\u8fd1\u4f3c\u7ed3\u6784\u6a21\u578b\uff0c\u4fdd\u6301\u6574\u4f53\u6846\u67b6\u7684\u65e0\u6a21\u578b\u7279\u6027\u3002", "result": "\u8be5\u6df7\u5408\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u663e\u5f0f\u7cfb\u7edf\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4e86\u6734\u7d20RL\u5b9e\u73b0\u4e2d\u7684\u63a2\u7d22\u98ce\u9669\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u89e3\u51b3RL\u632f\u52a8\u63a7\u5236\u8bad\u7ec3\u5b89\u5168\u6311\u6218\u5e76\u63d0\u4f9b\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u7684\u7814\u7a76\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01375", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01375", "abs": "https://arxiv.org/abs/2510.01375", "authors": ["Humaid Ibrahim", "Nikolai Rozanov", "Marek Rei"], "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills", "comment": "Under review at ICLR 2026", "summary": "Large language model (LLM) agents deployed for multi-step tasks frequently\nfail in predictable ways: attempting actions with unmet preconditions, issuing\nredundant commands, or mishandling environment constraints. While\nretrieval-augmented generation (RAG) can improve performance by providing\nruntime guidance, it requires maintaining external knowledge databases and adds\ncomputational overhead at every deployment. We propose a simple pipeline that\nconverts inference-time retrieval into learned competence through distillation.\nOur approach: (1) extracts compact, reusable hints from agent failures, (2)\nuses these hints to generate improved teacher trajectories via one-shot\nretrieval at episode start, and (3) trains student models on these trajectories\nwith hint strings removed, forcing internalization rather than memorization.\nAcross two interactive benchmarks, ALFWorld (household tasks) and WebShop\n(online shopping), distilled students consistently outperform baseline agents,\nachieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving\nWebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens\nthan retrieval-augmented teachers depending on the environment. The approach\ngeneralizes across model scales (7B/14B parameters) and agent architectures\n(ReAct/StateAct), demonstrating that retrieval benefits can be effectively\ninternalized through targeted fine-tuning without permanent runtime\ndependencies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u63a8\u7406\u65f6\u68c0\u7d22\u8f6c\u5316\u4e3a\u5b66\u4e60\u80fd\u529b\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u667a\u80fd\u4f53\u5931\u8d25\u4e2d\u63d0\u53d6\u7d27\u51d1\u63d0\u793a\uff0c\u751f\u6210\u6539\u8fdb\u7684\u6559\u5e08\u8f68\u8ff9\uff0c\u5e76\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5185\u90e8\u5316\u8fd9\u4e9b\u80fd\u529b\uff0c\u5728ALFWorld\u548cWebShop\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u7ecf\u5e38\u51fa\u73b0\u53ef\u9884\u6d4b\u7684\u5931\u8d25\uff0c\u5982\u5c1d\u8bd5\u4e0d\u6ee1\u8db3\u524d\u63d0\u6761\u4ef6\u7684\u52a8\u4f5c\u3001\u53d1\u51fa\u5197\u4f59\u547d\u4ee4\u6216\u9519\u8bef\u5904\u7406\u73af\u5883\u7ea6\u675f\u3002\u867d\u7136RAG\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u63d0\u4f9b\u6307\u5bfc\uff0c\u4f46\u9700\u8981\u7ef4\u62a4\u5916\u90e8\u77e5\u8bc6\u5e93\u5e76\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u4e09\u6b65\u84b8\u998f\u6d41\u7a0b\uff1a(1)\u4ece\u667a\u80fd\u4f53\u5931\u8d25\u4e2d\u63d0\u53d6\u7d27\u51d1\u53ef\u91cd\u7528\u7684\u63d0\u793a\uff1b(2)\u4f7f\u7528\u8fd9\u4e9b\u63d0\u793a\u5728\u56de\u5408\u5f00\u59cb\u65f6\u901a\u8fc7\u4e00\u6b21\u6027\u68c0\u7d22\u751f\u6210\u6539\u8fdb\u7684\u6559\u5e08\u8f68\u8ff9\uff1b(3)\u5728\u79fb\u9664\u63d0\u793a\u5b57\u7b26\u4e32\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u5f3a\u5236\u5185\u90e8\u5316\u800c\u975e\u8bb0\u5fc6\u3002", "result": "\u5728ALFWorld\u4e0a\u8fbe\u523091%\u6210\u529f\u7387\uff08\u57fa\u7ebf\u4e3a79%\uff09\uff0cWebShop\u5f97\u5206\u63d0\u5347\u81f372\uff08\u57fa\u7ebf\u4e3a61\uff09\uff0c\u540c\u65f6\u6bd4\u68c0\u7d22\u589e\u5f3a\u7684\u6559\u5e08\u6a21\u578b\u5c11\u752810-60%\u7684token\u3002\u65b9\u6cd5\u57287B/14B\u53c2\u6570\u89c4\u6a21\u548cReAct/StateAct\u67b6\u6784\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "\u68c0\u7d22\u7684\u597d\u5904\u53ef\u4ee5\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5fae\u8c03\u6709\u6548\u5185\u90e8\u5316\uff0c\u65e0\u9700\u6c38\u4e45\u6027\u7684\u8fd0\u884c\u65f6\u4f9d\u8d56\uff0c\u8bc1\u660e\u4e86\u5c06\u63a8\u7406\u65f6\u68c0\u7d22\u8f6c\u5316\u4e3a\u5b66\u4e60\u80fd\u529b\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.01398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01398", "abs": "https://arxiv.org/abs/2510.01398", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "comment": null, "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u4ee3\u7406\u81ea\u52a8\u5316\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u548c\u5206\u6790\u7684\u521b\u65b0\u6d41\u7a0b\uff0c\u7279\u522b\u5173\u6ce8\u56de\u5f52\u4efb\u52a1\uff0c\u5728\u4e34\u754c\u70ed\u901a\u91cf\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u4f9d\u8d56\u6d77\u91cf\u5b9e\u9a8c\u548c\u6a21\u62df\u6570\u636e\uff0c\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u96be\u4ee5\u6709\u6548\u6269\u5c55\u548c\u6cdb\u5316\u5230\u591a\u6837\u5316\u5e94\u7528\u3002", "method": "\u8bc4\u4f30\u4e86\u4e24\u79cdLLM\u4ee3\u7406\u6846\u67b6\uff1a\u591a\u4ee3\u7406\u7cfb\u7edf\uff08\u4e13\u4e1a\u5316\u534f\u4f5c\u4ee3\u7406\uff09\u548c\u5355\u4ee3\u7406\u7cfb\u7edf\uff08\u57fa\u4e8eReAct\u8303\u5f0f\uff09\uff0c\u81ea\u4e3b\u5904\u7406\u6570\u636e\u9884\u5904\u7406\u3001\u795e\u7ecf\u7f51\u7edc\u5f00\u53d1\u3001\u8bad\u7ec3\u3001\u8d85\u53c2\u6570\u4f18\u5316\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u7ea625,000\u4e2a\u5b9e\u9a8c\u6570\u636e\u70b9\u7684\u4e34\u754c\u70ed\u901a\u91cf\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM\u4ee3\u7406\u5f00\u53d1\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u4f20\u7edf\u67e5\u627e\u8868\uff0c\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5f00\u53d1\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u590d\u6742\u5de5\u7a0b\u5efa\u6a21\u4efb\u52a1\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u9884\u6d4b\u6027\u80fd\u6807\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.01237", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01237", "abs": "https://arxiv.org/abs/2510.01237", "authors": ["Nandakishor M"], "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation", "comment": null, "summary": "Large Language Models suffer from hallucination, generating plausible yet\nfactually incorrect content. Current mitigation strategies focus on\npost-generation correction, which is computationally expensive and fails to\nprevent unreliable content generation. We propose a confidence-aware routing\nsystem that proactively assesses model uncertainty before generation and\nredirects queries based on estimated reliability. Our approach combines three\ncomplementary signals: semantic alignment between internal representations and\nreference embeddings, internal convergence analysis across model layers, and\nlearned confidence estimation. The unified confidence score determines routing\nto four pathways: local generation for high confidence, retrieval-augmented\ngeneration for medium confidence, larger models for low confidence, and human\nreview for very low confidence. Evaluation on knowledge-intensive QA benchmarks\ndemonstrates significant improvements in hallucination detection (0.74 vs. 0.42\nbaseline) while reducing computational costs by 40% compared to post-hoc\nmethods. The F1 score improves from 0.61 to 0.82 with low false positive rates\n(0.09). This paradigm shift from reactive correction to proactive assessment\noffers a computationally efficient approach to LLM reliability enhancement.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u8def\u7531\u7cfb\u7edf\uff0c\u5728\u751f\u6210\u524d\u8bc4\u4f30\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u67e5\u8be2\u8def\u7531\u5230\u4e0d\u540c\u5904\u7406\u8def\u5f84\u4ee5\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u4e8b\u5b9e\u9519\u8bef\u5185\u5bb9\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u540e\u751f\u6210\u4fee\u6b63\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u963b\u6b62\u4e0d\u53ef\u9760\u5185\u5bb9\u7684\u751f\u6210", "method": "\u7ed3\u5408\u4e09\u79cd\u4e92\u8865\u4fe1\u53f7\uff1a\u5185\u90e8\u8868\u793a\u4e0e\u53c2\u8003\u5d4c\u5165\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u8de8\u6a21\u578b\u5c42\u7684\u5185\u90e8\u6536\u655b\u5206\u6790\u3001\u5b66\u4e60\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u751f\u6210\u7edf\u4e00\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u51b3\u5b9a\u8def\u7531\u5230\u56db\u79cd\u5904\u7406\u8def\u5f84", "result": "\u5728\u77e5\u8bc6\u5bc6\u96c6\u578bQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e7b\u89c9\u68c0\u6d4b\u663e\u8457\u63d0\u5347\uff080.74 vs 0.42\u57fa\u7ebf\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u6bd4\u540e\u5904\u7406\u65b9\u6cd5\u964d\u4f4e40%\uff0cF1\u5206\u6570\u4ece0.61\u63d0\u5347\u52300.82\uff0c\u5047\u9633\u6027\u7387\u4f4e\u81f30.09", "conclusion": "\u4ece\u53cd\u5e94\u5f0f\u4fee\u6b63\u8f6c\u5411\u4e3b\u52a8\u8bc4\u4f30\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3aLLM\u53ef\u9760\u6027\u589e\u5f3a\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2510.01427", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01427", "abs": "https://arxiv.org/abs/2510.01427", "authors": ["Sipeng Zhang", "Longfei Yun", "Zilong Wang", "Jingbo Shang", "Letian Peng"], "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining", "comment": null, "summary": "At the core of Deep Research is knowledge mining, the task of extracting\nstructured information from massive unstructured text in response to user\ninstructions. Large language models (LLMs) excel at interpreting such\ninstructions but are prohibitively expensive to deploy at scale, while\ntraditional pipelines of classifiers and extractors remain efficient yet\nbrittle and unable to generalize to new tasks. We introduce Falconer, a\ncollaborative framework that combines the agentic reasoning of LLMs with\nlightweight proxy models for scalable knowledge mining. In Falconer, LLMs act\nas planners, decomposing user instructions into executable pipelines, and as\nannotators, generating supervision to train small proxies. The framework\nunifies classification and extraction into two atomic operations, get label and\nget span, enabling a single instruction-following model to replace multiple\ntask-specific components. To evaluate the consistency between proxy models\nincubated by Falconer and annotations provided by humans and large models, we\nconstruct new benchmarks covering both planning and end-to-end execution.\nExperiments show that Falconer closely matches state-of-the-art LLMs in\ninstruction-following accuracy while reducing inference cost by up to 90% and\naccelerating large-scale knowledge mining by more than 20x, offering an\nefficient and scalable foundation for Deep Research.", "AI": {"tldr": "Falconer\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u667a\u80fd\u63a8\u7406\u548c\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6a21\u578b\u7684\u53ef\u6269\u5c55\u77e5\u8bc6\u6316\u6398\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4f5c\u4e3a\u89c4\u5212\u5668\u548c\u6807\u6ce8\u5668\uff0c\u8bad\u7ec3\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\u6765\u66ff\u4ee3\u6602\u8d35\u7684LLM\u90e8\u7f72\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u548c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u77e5\u8bc6\u6316\u6398\u4e2dLLM\u90e8\u7f72\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u4f20\u7edf\u5206\u7c7b\u5668\u548c\u63d0\u53d6\u5668\u7ba1\u9053\u53c8\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u89c4\u5212\u5668\u5206\u89e3\u7528\u6237\u6307\u4ee4\u4e3a\u53ef\u6267\u884c\u7ba1\u9053\uff0c\u4f5c\u4e3a\u6807\u6ce8\u5668\u751f\u6210\u76d1\u7763\u6570\u636e\u8bad\u7ec3\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\uff0c\u5c06\u5206\u7c7b\u548c\u63d0\u53d6\u7edf\u4e00\u4e3aget label\u548cget span\u4e24\u4e2a\u539f\u5b50\u64cd\u4f5c\u3002", "result": "Falconer\u5728\u6307\u4ee4\u8ddf\u968f\u51c6\u786e\u6027\u4e0a\u63a5\u8fd1\u6700\u5148\u8fdb\u7684LLM\uff0c\u540c\u65f6\u63a8\u7406\u6210\u672c\u964d\u4f4e90%\uff0c\u5927\u89c4\u6a21\u77e5\u8bc6\u6316\u6398\u901f\u5ea6\u63d0\u534720\u500d\u4ee5\u4e0a\u3002", "conclusion": "Falconer\u4e3a\u6df1\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u57fa\u7840\u6846\u67b6\uff0c\u6210\u529f\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2510.01239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01239", "abs": "https://arxiv.org/abs/2510.01239", "authors": ["Juntae Lee", "Jihwan Bang", "Seunghan Yang", "Simyung Chang"], "title": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM", "comment": "accepted at EMNLP 2025 (main)", "summary": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which\nis a novel execution system for efficient sub-task handling in multi-turn\ninteractions with a single on-device large language model (LLM). As LLMs become\nincreasingly capable, a single model is expected to handle diverse sub-tasks\nthat more effectively and comprehensively support answering user requests.\nNaive approach reprocesses the entire conversation context when switching\nbetween main and sub-tasks (e.g., query rewriting, summarization), incurring\nsignificant computational overhead. CIFLEX mitigates this overhead by reusing\nthe key-value (KV) cache from the main task and injecting only task-specific\ninstructions into isolated side paths. After sub-task execution, the model\nrolls back to the main path via cached context, thereby avoiding redundant\nprefill computation. To support sub-task selection, we also develop a\nhierarchical classification strategy tailored for small-scale models,\ndecomposing multi-choice decisions into binary ones. Experiments show that\nCIFLEX significantly reduces computational costs without degrading task\nperformance, enabling scalable and efficient multi-task dialogue on-device.", "AI": {"tldr": "CIFLEX\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u5355\u8bbe\u5907LLM\u591a\u8f6e\u4ea4\u4e92\u4e2d\u9ad8\u6548\u5904\u7406\u5b50\u4efb\u52a1\u7684\u6267\u884c\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cd\u7528\u4e3b\u4efb\u52a1\u7684KV\u7f13\u5b58\u548c\u6ce8\u5165\u7279\u5b9a\u6307\u4ee4\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u968f\u7740LLM\u80fd\u529b\u589e\u5f3a\uff0c\u5355\u4e2a\u6a21\u578b\u9700\u8981\u5904\u7406\u591a\u6837\u5b50\u4efb\u52a1\u6765\u66f4\u597d\u5730\u652f\u6301\u7528\u6237\u8bf7\u6c42\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u5207\u6362\u4e3b\u4efb\u52a1\u548c\u5b50\u4efb\u52a1\u65f6\u4f1a\u91cd\u65b0\u5904\u7406\u6574\u4e2a\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff0c\u4ea7\u751f\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u91cd\u7528\u4e3b\u4efb\u52a1\u7684KV\u7f13\u5b58\uff0c\u5c06\u4efb\u52a1\u7279\u5b9a\u6307\u4ee4\u6ce8\u5165\u9694\u79bb\u7684\u4fa7\u8def\u5f84\uff0c\u5b50\u4efb\u52a1\u6267\u884c\u540e\u901a\u8fc7\u7f13\u5b58\u4e0a\u4e0b\u6587\u56de\u6eda\u5230\u4e3b\u8def\u5f84\uff0c\u907f\u514d\u5197\u4f59\u9884\u586b\u5145\u8ba1\u7b97\uff1b\u5f00\u53d1\u5206\u5c42\u5206\u7c7b\u7b56\u7565\u652f\u6301\u5b50\u4efb\u52a1\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCIFLEX\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u591a\u4efb\u52a1\u5bf9\u8bdd\u3002", "conclusion": "CIFLEX\u901a\u8fc7KV\u7f13\u5b58\u91cd\u7528\u548c\u6307\u4ee4\u6ce8\u5165\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5bf9\u8bdd\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8bbe\u5907\u7aefLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.01444", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01444", "abs": "https://arxiv.org/abs/2510.01444", "authors": ["Rui Liu", "Dian Yu", "Tong Zheng", "Runpeng Dai", "Zongxia Li", "Wenhao Yu", "Zhenwen Liang", "Linfeng Song", "Haitao Mi", "Pratap Tokekar", "Dong Yu"], "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in\nlarge language models (LLMs) but struggles with exploration, an issue that\nstill persists for multimodal LLMs (MLLMs). Current methods treat the visual\ninput as a fixed, deterministic condition, overlooking a critical source of\nambiguity and struggling to build policies robust to plausible visual\nvariations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided\nExploration)}$, a novel method that shifts exploration from the output (text)\nto the input (visual) space. By treating the image as a stochastic context,\nVOGUE quantifies the policy's sensitivity to visual perturbations using the\nsymmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct\nsignal for uncertainty-aware exploration. This signal shapes the learning\nobjective via an uncertainty-proportional bonus, which, combined with a\ntoken-entropy bonus and an annealed sampling schedule, effectively balances\nexploration and exploitation. Implemented within GRPO on two model scales\n(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three\nvisual math benchmarks and 3.7% on three general-domain reasoning benchmarks,\nwhile simultaneously increasing pass@4 performance and mitigating the\nexploration decay commonly observed in RL fine-tuning. Our work shows that\ngrounding exploration in the inherent uncertainty of visual inputs is an\neffective strategy for improving multimodal reasoning.", "AI": {"tldr": "VOGUE\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8f93\u5165\u89c6\u4e3a\u968f\u673a\u4e0a\u4e0b\u6587\uff0c\u91cf\u5316\u7b56\u7565\u5bf9\u89c6\u89c9\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u5728\u8f93\u5165\u7a7a\u95f4\u8fdb\u884c\u63a2\u7d22\uff0c\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u5c06\u89c6\u89c9\u8f93\u5165\u89c6\u4e3a\u56fa\u5b9a\u6761\u4ef6\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u53d8\u5316\u5e26\u6765\u7684\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u7b56\u7565\u5bf9\u5408\u7406\u89c6\u89c9\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "VOGUE\u5c06\u56fe\u50cf\u89c6\u4e3a\u968f\u673a\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u5bf9\u79f0KL\u6563\u5ea6\u91cf\u5316\u7b56\u7565\u5bf9\u89c6\u89c9\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u6bd4\u4f8b\u5956\u52b1\u3001token\u71b5\u5956\u52b1\u548c\u9000\u706b\u91c7\u6837\u8c03\u5ea6\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728Qwen2.5-VL-3B/7B\u6a21\u578b\u4e0a\u5b9e\u73b0\uff0c\u5728\u4e09\u4e2a\u89c6\u89c9\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u53472.6%\u7684pass@1\u51c6\u786e\u7387\uff0c\u5728\u4e09\u4e2a\u901a\u7528\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u53473.7%\uff0c\u540c\u65f6\u63d0\u9ad8pass@4\u6027\u80fd\u5e76\u7f13\u89e3RL\u5fae\u8c03\u4e2d\u7684\u63a2\u7d22\u8870\u51cf\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u8f93\u5165\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u63a2\u7d22\u7b56\u7565\u662f\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u7684\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01531", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01531", "abs": "https://arxiv.org/abs/2510.01531", "authors": ["Djengo Cyun-Jyun Fang", "Tsung-Wei Ke"], "title": "Information Seeking for Robust Decision Making under Partial Observability", "comment": "The project page is available at https://infoseekerllm.github.io", "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io", "AI": {"tldr": "\u63d0\u51faInfoSeeker\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5bfc\u5411\u89c4\u5212\u4e0e\u4fe1\u606f\u5bfb\u6c42\u76f8\u7ed3\u5408\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u901a\u8fc7\u4e3b\u52a8\u6536\u96c6\u4fe1\u606f\u6765\u5bf9\u9f50\u5185\u90e8\u52a8\u6001\u5e76\u505a\u51fa\u6700\u4f18\u51b3\u7b56", "motivation": "\u73b0\u6709LLM\u89c4\u5212\u667a\u80fd\u4f53\u5728\u5904\u7406\u89c2\u6d4b\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\u5f80\u5f80\u5ffd\u89c6\u5176\u5185\u90e8\u52a8\u6001\u4e0e\u5b9e\u9645\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u5bfb\u6c42\u4fe1\u606f\u6765\u66f4\u65b0\u5185\u90e8\u52a8\u6001\u7684\u65b9\u6cd5", "method": "InfoSeeker\u6846\u67b6\u63d0\u793aLLM\u901a\u8fc7\u89c4\u5212\u884c\u52a8\u6765\u9a8c\u8bc1\u7406\u89e3\u3001\u68c0\u6d4b\u73af\u5883\u53d8\u5316\u6216\u6d4b\u8bd5\u5047\u8bbe\uff0c\u7136\u540e\u751f\u6210\u6216\u4fee\u8ba2\u4efb\u52a1\u5bfc\u5411\u8ba1\u5212", "result": "\u5728\u5305\u542b\u4e0d\u5b8c\u5168\u89c2\u6d4b\u548c\u4e0d\u786e\u5b9a\u52a8\u6001\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfoSeeker\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u534774%\uff0c\u4e14\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u7f51\u9875\u5bfc\u822a\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\uff0c\u7d27\u5bc6\u96c6\u6210\u89c4\u5212\u548c\u4fe1\u606f\u5bfb\u6c42\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u884c\u4e3a\u81f3\u5173\u91cd\u8981", "topic": "agent analysis"}}
{"id": "2510.01544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01544", "abs": "https://arxiv.org/abs/2510.01544", "authors": ["Shaoan Xie", "Lingjing Kong", "Xiangchen Song", "Xinshuai Dong", "Guangyi Chen", "Eric P. Xing", "Kun Zhang"], "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models", "comment": null, "summary": "Diffusion language models (dLLMs) offer a promising, non-autoregressive\nparadigm for text generation, yet training them for complex reasoning remains a\nkey challenge. Current reinforcement learning approaches often rely on sparse,\noutcome-based rewards, which can reinforce flawed reasoning paths that lead to\ncoincidentally correct answers. We argue that this stems from a fundamental\nmismatch with the natural structure of reasoning. We first propose a\ntheoretical framework that formalizes complex problem solving as a hierarchical\nselection process, where an intractable global constraint is decomposed into a\nseries of simpler, localized logical steps. This framework provides a\nprincipled foundation for algorithm design, including theoretical insights into\nthe identifiability of this latent reasoning structure. Motivated by this\ntheory, we identify unstructured refinement -- a failure mode where a model's\niterative steps do not contribute meaningfully to the solution -- as a core\ndeficiency in existing methods. We then introduce Step-Aware Policy\nOptimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising\nprocess with the latent reasoning hierarchy. By using a process-based reward\nfunction that encourages incremental progress, SAPO guides the model to learn\nstructured, coherent reasoning paths. Our empirical results show that this\nprincipled approach significantly improves performance on challenging reasoning\nbenchmarks and enhances the interpretability of the generation process.", "AI": {"tldr": "\u63d0\u51fa\u4e86Step-Aware Policy Optimization (SAPO)\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u5316\u7684\u63a8\u7406\u8def\u5f84\uff0c\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u975e\u7ed3\u6784\u5316\u7cbe\u70bc\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u53ef\u80fd\u5f3a\u5316\u5bfc\u81f4\u5076\u7136\u6b63\u786e\u7ed3\u679c\u7684\u9519\u8bef\u63a8\u7406\u8def\u5f84\uff0c\u8fd9\u4e0e\u63a8\u7406\u7684\u81ea\u7136\u7ed3\u6784\u4e0d\u5339\u914d\u3002", "method": "\u9996\u5148\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5c06\u590d\u6742\u95ee\u9898\u89e3\u51b3\u5f62\u5f0f\u5316\u4e3a\u5c42\u6b21\u9009\u62e9\u8fc7\u7a0b\uff0c\u7136\u540e\u5f15\u5165SAPO\u7b97\u6cd5\uff0c\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u51fd\u6570\u4f7f\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e0e\u6f5c\u5728\u63a8\u7406\u5c42\u6b21\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7406\u8bba\u6846\u67b6\u7684SAPO\u7b97\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u9677\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01569", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01569", "abs": "https://arxiv.org/abs/2510.01569", "authors": ["Yubin Kim", "Taehan Kim", "Eugene Park", "Chunjong Park", "Cynthia Breazeal", "Daniel McDuff", "Hae Won Park"], "title": "InvThink: Towards AI Safety via Inverse Reasoning", "comment": null, "summary": "We present InvThink, a simple yet powerful approach that gives large language\nmodels (LLMs) the capability of inverse thinking: reasoning through failure\nmodes before generating responses. Unlike existing safety alignment methods\nthat optimize directly for safe response, InvThink instructs models to 1)\nenumerate potential harms, 2) analyze their consequences, and 3) generate safe\noutputs that proactively avoid these risks. Our method reveals three key\nfindings: (i) safety improvements show stronger scaling with model size\ncompared to existing safety methods. (ii) InvThink mitigates safety tax; by\ntraining models to systematically consider failure modes, it preserves general\nreasoning capabilities on standard benchmarks. (iii) beyond general safety\ntasks, InvThink excels in high-stakes domains including external-facing\n(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,\nachieving up to 15.7% reduction in harmful responses compared to baseline\nmethods like SafetyPrompt. We further implement InvThink via supervised\nfine-tuning, and reinforcement learning across three LLM families. These\nresults suggest that inverse reasoning provides a scalable and generalizable\npath toward safer, more capable language models.", "AI": {"tldr": "\u63d0\u51faInvThink\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u5411\u601d\u7ef4\u8ba9LLMs\u5728\u751f\u6210\u56de\u7b54\u524d\u5148\u5206\u6790\u6f5c\u5728\u5371\u5bb3\u548c\u540e\u679c\uff0c\u4ece\u800c\u4e3b\u52a8\u907f\u514d\u5b89\u5168\u98ce\u9669", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u5b89\u5168\u56de\u7b54\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u98ce\u9669\u7684\u7cfb\u7edf\u6027\u601d\u8003\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4e3b\u52a8\u9884\u9632\u5371\u5bb3\u7684\u65b9\u6cd5", "method": "\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u4e09\u6b65\u9006\u5411\u601d\u8003\uff1a1)\u5217\u4e3e\u6f5c\u5728\u5371\u5bb3 2)\u5206\u6790\u540e\u679c 3)\u751f\u6210\u4e3b\u52a8\u907f\u514d\u8fd9\u4e9b\u98ce\u9669\u7684\u5b89\u5168\u8f93\u51fa\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u4e09\u4e2aLLM\u5bb6\u65cf\u4e2d\u5b9e\u73b0", "result": "\u76f8\u6bd4SafetyPrompt\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6709\u5bb3\u56de\u7b54\u4e0a\u51cf\u5c11\u8fbe15.7%\uff1b\u5b89\u5168\u6539\u8fdb\u968f\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u66f4\u5f3a\uff1b\u7f13\u89e3\u5b89\u5168\u7a0e\uff0c\u4fdd\u6301\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff1b\u5728\u9ad8\u98ce\u9669\u9886\u57df\u8868\u73b0\u4f18\u5f02", "conclusion": "\u9006\u5411\u63a8\u7406\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u6cdb\u5316\u7684\u8def\u5f84", "topic": "agent analysis"}}
{"id": "2510.01586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01586", "abs": "https://arxiv.org/abs/2510.01586", "authors": ["Zhenyu Pan", "Yiting Zhang", "Zhuo Liu", "Yolo Yunlong Tang", "Zeliang Zhang", "Haozheng Luo", "Yuwei Han", "Jianshu Zhang", "Dennis Wu", "Hong-Yu Chen", "Haoran Lu", "Haoyang Fang", "Manling Li", "Chenliang Xu", "Philip S. Yu", "Han Liu"], "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning", "comment": null, "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdvEvo-MARL\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5c06\u5b89\u5168\u6027\u5185\u5316\u5230\u4efb\u52a1\u667a\u80fd\u4f53\u4e2d\uff0c\u65e0\u9700\u5916\u90e8\u9632\u62a4\u6a21\u5757\u5373\u53ef\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u8d8a\u72f1\u3001\u63d0\u793a\u6ce8\u5165\u548c\u5bf9\u6297\u6027\u534f\u4f5c\u7b49\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u81ea\u6211\u9a8c\u8bc1\u548c\u5916\u90e8\u9632\u62a4\u6a21\u5757\uff09\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u3001\u7cfb\u7edf\u5f00\u9500\u5927\u548c\u5355\u70b9\u6545\u969c\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u534f\u540c\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u653b\u51fb\u8005\uff08\u751f\u6210\u8d8a\u72f1\u63d0\u793a\uff09\u548c\u9632\u5fa1\u8005\uff08\u4efb\u52a1\u667a\u80fd\u4f53\uff09\uff0c\u5f15\u5165\u516c\u5171\u57fa\u7ebf\u8fdb\u884c\u4f18\u52bf\u4f30\u8ba1\u4ee5\u7a33\u5b9a\u5b66\u4e60\u548c\u4fc3\u8fdb\u534f\u4f5c\u3002", "result": "\u5728\u4ee3\u8868\u6027\u653b\u51fb\u573a\u666f\u4e2d\uff0cAdvEvo-MARL\u5c06\u653b\u51fb\u6210\u529f\u7387\u4fdd\u6301\u572820%\u4ee5\u4e0b\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u53ef\u8fbe38.33%\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\uff08\u63a8\u7406\u4efb\u52a1\u6700\u9ad8\u63d0\u53473.67%\uff09\u3002", "conclusion": "\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u9632\u62a4\u667a\u80fd\u4f53\u6216\u589e\u52a0\u7cfb\u7edf\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u5171\u540c\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01394", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01394", "abs": "https://arxiv.org/abs/2510.01394", "authors": ["Yusuf Kalayci", "Vinod Raman", "Shaddin Dughmi"], "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization", "comment": "24 pages", "summary": "Large language model (LLM) generation often requires balancing output quality\nagainst inference cost, especially when using multiple generations. We\nintroduce a new framework for inference-time optimization based on the\nclassical Pandora's Box problem. Viewing each generation as opening a costly\n\"box\" with random reward, we develop algorithms that decide when to stop\ngenerating without knowing the underlying reward distribution. Our first\ncontribution is a UCB-style Pandora's Box algorithm, which achieves performance\nthat is provably close to Weitzman's algorithm, the optimal strategy when the\ndistribution is known. We further adapt this method to practical LLM settings\nby addressing reward scaling across prompts via a Bradley-Terry inspired\ntransformation. This leads to an adaptive inference-time optimization method\nthat normalizes rewards and learns stopping thresholds on the fly. Experiments\non the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,\nshow that our adaptive strategy can obtain the same performance as non-adaptive\nBest-of-N sampling while requiring 15-35 percent fewer generations on average.\nOur results establish a principled bridge between optimal stopping theory and\ninference-time scaling, providing both theoretical performance bounds and\npractical efficiency gains for LLM deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePandora's Box\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8eLLM\u63a8\u7406\u65f6\u4f18\u5316\uff0c\u901a\u8fc7UCB\u7b97\u6cd5\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u505c\u6b62\u751f\u6210\uff0c\u76f8\u6bd4Best-of-N\u91c7\u6837\u53ef\u51cf\u5c1115-35%\u7684\u751f\u6210\u6b21\u6570\u3002", "motivation": "LLM\u751f\u6210\u9700\u8981\u5728\u8f93\u51fa\u8d28\u91cf\u548c\u63a8\u7406\u6210\u672c\u4e4b\u95f4\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u591a\u751f\u6210\u573a\u666f\u4e0b\uff0c\u9700\u8981\u4f18\u5316\u63a8\u7406\u65f6\u51b3\u7b56\u3002", "method": "\u5c06\u6bcf\u4e2a\u751f\u6210\u89c6\u4e3a\u6253\u5f00\u5e26\u968f\u673a\u5956\u52b1\u7684\"\u76d2\u5b50\"\uff0c\u5f00\u53d1UCB\u98ce\u683c\u7684Pandora's Box\u7b97\u6cd5\uff0c\u7ed3\u5408Bradley-Terry\u53d8\u6362\u5904\u7406\u4e0d\u540c\u63d0\u793a\u7684\u5956\u52b1\u7f29\u653e\u95ee\u9898\u3002", "result": "\u5728AlpacaFarm\u548cHH-RLHF\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u80fd\u8fbe\u5230\u4e0eBest-of-N\u91c7\u6837\u76f8\u540c\u7684\u6027\u80fd\uff0c\u4f46\u5e73\u5747\u51cf\u5c1115-35%\u7684\u751f\u6210\u6b21\u6570\u3002", "conclusion": "\u5efa\u7acb\u4e86\u6700\u4f18\u505c\u6b62\u7406\u8bba\u4e0e\u63a8\u7406\u65f6\u6269\u5c55\u4e4b\u95f4\u7684\u539f\u5219\u6027\u6865\u6881\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\u548c\u5b9e\u9645\u6548\u7387\u63d0\u5347\u3002", "topic": "agent analysis"}}
{"id": "2510.01255", "categories": ["cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.01255", "abs": "https://arxiv.org/abs/2510.01255", "authors": ["Yunlang Dai", "Emma Lurie", "Dana\u00e9 Metaxa", "Sorelle A. Friedler"], "title": "Longitudinal Monitoring of LLM Content Moderation of Social Issues", "comment": null, "summary": "Large language models' (LLMs') outputs are shaped by opaque and\nfrequently-changing company content moderation policies and practices. LLM\nmoderation often takes the form of refusal; models' refusal to produce text\nabout certain topics both reflects company policy and subtly shapes public\ndiscourse. We introduce AI Watchman, a longitudinal auditing system to publicly\nmeasure and track LLM refusals over time, to provide transparency into an\nimportant and black-box aspect of LLMs. Using a dataset of over 400 social\nissues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and\nDeepSeek (both in English and Chinese). We find evidence that changes in\ncompany policies, even those not publicly announced, can be detected by AI\nWatchman, and identify company- and model-specific differences in content\nmoderation. We also qualitatively analyze and categorize different forms of\nrefusal. This work contributes evidence for the value of longitudinal auditing\nof LLMs, and AI Watchman, one system for doing so.", "AI": {"tldr": "AI Watchman\u662f\u4e00\u4e2a\u7eb5\u5411\u5ba1\u8ba1\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d4b\u91cf\u548c\u8ddf\u8e2aLLM\u62d2\u7edd\u884c\u4e3a\uff0c\u63ed\u793a\u516c\u53f8\u5185\u5bb9\u5ba1\u6838\u653f\u7b56\u7684\u53d8\u5316\u548c\u5dee\u5f02\u3002", "motivation": "LLM\u7684\u8f93\u51fa\u53d7\u5230\u4e0d\u900f\u660e\u4e14\u9891\u7e41\u53d8\u5316\u7684\u516c\u53f8\u5185\u5bb9\u5ba1\u6838\u653f\u7b56\u5f71\u54cd\uff0c\u8fd9\u4e9b\u62d2\u7edd\u884c\u4e3a\u53cd\u6620\u4e86\u516c\u53f8\u653f\u7b56\u5e76\u5fae\u5999\u5730\u5851\u9020\u516c\u5171\u8bdd\u8bed\u3002", "method": "\u4f7f\u7528\u5305\u542b400\u591a\u4e2a\u793e\u4f1a\u8bae\u9898\u7684\u6570\u636e\u96c6\uff0c\u5ba1\u8ba1OpenAI\u7684\u5ba1\u6838\u7aef\u70b9\u3001GPT-4.1\u3001GPT-5\u548cDeepSeek\uff08\u4e2d\u82f1\u6587\u7248\u672c\uff09\uff0c\u5e76\u5b9a\u6027\u5206\u6790\u62d2\u7edd\u5f62\u5f0f\u3002", "result": "\u68c0\u6d4b\u5230\u516c\u53f8\u653f\u7b56\u53d8\u5316\uff08\u5305\u62ec\u672a\u516c\u5f00\u5ba3\u5e03\u7684\uff09\uff0c\u8bc6\u522b\u4e86\u516c\u53f8\u548c\u6a21\u578b\u7279\u5b9a\u7684\u5185\u5bb9\u5ba1\u6838\u5dee\u5f02\uff0c\u5e76\u5206\u7c7b\u4e86\u4e0d\u540c\u7684\u62d2\u7edd\u5f62\u5f0f\u3002", "conclusion": "\u7eb5\u5411\u5ba1\u8ba1LLM\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cAI Watchman\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2510.01670", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01670", "abs": "https://arxiv.org/abs/2510.01670", "authors": ["Erfan Shayegani", "Keegan Hines", "Yue Dong", "Nael Abu-Ghazaleh", "Roman Lutz", "Spencer Whitehead", "Vidhisha Balachandran", "Besmira Nushi", "Vibhav Vineet"], "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness", "comment": null, "summary": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that\ntake actions on GUIs to accomplish user goals. In this paper, we show that CUAs\nconsistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals\nregardless of feasibility, safety, reliability, or context. We characterize\nthree prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)\nassumptions and decisions under ambiguity, and (iii) contradictory or\ninfeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these\nthree patterns. Built on OSWorld, BLIND-ACT provides realistic environments and\nemploys LLM-based judges to evaluate agent behavior, achieving 93.75% agreement\nwith human annotations. We use BLIND-ACT to evaluate nine frontier models,\nincluding Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing\nhigh average BGD rates (80.8%) across them. We show that BGD exposes subtle\nrisks that arise even when inputs are not directly harmful. While\nprompting-based interventions lower BGD levels, substantial risk persists,\nhighlighting the need for stronger training- or inference-time interventions.\nQualitative analysis reveals observed failure modes: execution-first bias\n(focusing on how to act over whether to act), thought-action disconnect\n(execution diverging from reasoning), and request-primacy (justifying actions\ndue to user request). Identifying BGD and introducing BLIND-ACT establishes a\nfoundation for future research on studying and mitigating this fundamental risk\nand ensuring safe CUA deployment.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406(CUAs)\u5b58\u5728\u76f2\u76ee\u76ee\u6807\u5bfc\u5411(BGD)\u504f\u5dee\uff0c\u5373\u4e0d\u987e\u53ef\u884c\u6027\u3001\u5b89\u5168\u6027\u6216\u4e0a\u4e0b\u6587\u5730\u8ffd\u6c42\u76ee\u6807\u3002\u4f5c\u8005\u5f00\u53d1\u4e86BLIND-ACT\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e869\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u53d1\u73b0\u5e73\u5747BGD\u7387\u9ad8\u8fbe80.8%\u3002", "motivation": "\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728GUI\u4e0a\u6267\u884c\u64cd\u4f5c\u6765\u5b8c\u6210\u7528\u6237\u76ee\u6807\uff0c\u4f46\u5b58\u5728\u76f2\u76ee\u8ffd\u6c42\u76ee\u6807\u7684\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u7f13\u89e3\u8fd9\u79cd\u98ce\u9669\u3002", "method": "\u5f00\u53d1BLIND-ACT\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b90\u4e2a\u4efb\u52a1\uff0c\u6355\u6349BGD\u7684\u4e09\u79cd\u6a21\u5f0f\uff1a\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u5728\u6a21\u7cca\u6027\u4e0b\u505a\u5047\u8bbe\u548c\u51b3\u7b56\u3001\u77db\u76fe\u6216\u4e0d\u53ef\u884c\u76ee\u6807\u3002\u4f7f\u7528LLM\u8bc4\u4f30\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u8bc4\u4f309\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u5e73\u5747BGD\u7387\u4e3a80.8%\u3002\u63d0\u793a\u5e72\u9884\u80fd\u964d\u4f4eBGD\u6c34\u5e73\uff0c\u4f46\u98ce\u9669\u4f9d\u7136\u5b58\u5728\u3002\u89c2\u5bdf\u5230\u4e09\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u6267\u884c\u4f18\u5148\u504f\u89c1\u3001\u601d\u7ef4-\u884c\u52a8\u8131\u8282\u3001\u8bf7\u6c42\u4f18\u5148\u3002", "conclusion": "\u8bc6\u522bBGD\u5e76\u5f15\u5165BLIND-ACT\u4e3a\u672a\u6765\u7814\u7a76\u548c\u7f13\u89e3\u8fd9\u79cd\u57fa\u672c\u98ce\u9669\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u786e\u4fddCUA\u7684\u5b89\u5168\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2510.01259", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01259", "abs": "https://arxiv.org/abs/2510.01259", "authors": ["Nils Durner"], "title": "In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b", "comment": "27 pages, 1 figure", "summary": "We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to\nstudy how sociopragmatic framing, language choice, and instruction hierarchy\naffect refusal behavior. Across 80 seeded iterations per scenario, we test\nseveral harm domains including ZIP-bomb construction (cyber threat), synthetic\ncard-number generation, minor-unsafe driving advice, drug-precursor indicators,\nand RAG context exfiltration. Composite prompts that combine an educator\npersona, a safety-pretext (\"what to avoid\"), and step-cue phrasing flip\nassistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal\nregisters in German and French are often leakier than matched English prompts.\nA \"Linux terminal\" role-play overrides a developer rule not to reveal context\nin a majority of runs with a naive developer prompt, and we introduce an\nAI-assisted hardening method that reduces leakage to 0% in several user-prompt\nvariants. We further test evaluation awareness with a paired-track design and\nmeasure frame-conditioned differences between matched \"helpfulness\" and\n\"harmfulness\" evaluation prompts; we observe inconsistent assistance in 13% of\npairs. Finally, we find that the OpenAI Moderation API under-captures\nmaterially helpful outputs relative to a semantic grader, and that refusal\nrates differ by 5 to 10 percentage points across inference stacks, raising\nreproducibility concerns. We release prompts, seeds, outputs, and code for\nreproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .", "AI": {"tldr": "\u7814\u7a76OpenAI\u7684200\u4ebf\u53c2\u6570\u5f00\u6e90\u6a21\u578bgpt-oss-20b\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u53d1\u73b0\u901a\u8fc7\u7279\u5b9a\u7684\u63d0\u793a\u5de5\u7a0b\uff08\u5982\u6559\u80b2\u8005\u89d2\u8272\u3001\u5b89\u5168\u524d\u63d0\u548c\u6b65\u9aa4\u63d0\u793a\uff09\u53ef\u4ee5\u5c06ZIP\u70b8\u5f39\u4efb\u52a1\u7684\u534f\u52a9\u7387\u4ece0%\u63d0\u5347\u523097.5%\u3002\u8fd8\u53d1\u73b0\u5fb7\u8bed\u548c\u6cd5\u8bed\u7684\u6b63\u5f0f\u8bed\u4f53\u6bd4\u82f1\u8bed\u66f4\u5bb9\u6613\u6cc4\u9732\u4fe1\u606f\uff0c\u5e76\u5f00\u53d1\u4e86AI\u8f85\u52a9\u5f3a\u5316\u65b9\u6cd5\u5c06\u6cc4\u9732\u7387\u964d\u81f30%\u3002", "motivation": "\u63a2\u7a76\u793e\u4f1a\u8bed\u7528\u6846\u67b6\u3001\u8bed\u8a00\u9009\u62e9\u548c\u6307\u4ee4\u5c42\u6b21\u5982\u4f55\u5f71\u54cdAI\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u6709\u5bb3\u9886\u57df\u4e2d\u7684\u54cd\u5e94\u6a21\u5f0f\u3002", "method": "\u4f7f\u752880\u6b21\u79cd\u5b50\u8fed\u4ee3\u6d4b\u8bd5\u591a\u4e2a\u5371\u5bb3\u9886\u57df\uff0c\u5305\u62ecZIP\u70b8\u5f39\u6784\u5efa\u3001\u5408\u6210\u5361\u53f7\u751f\u6210\u3001\u4e0d\u5b89\u5168\u9a7e\u9a76\u5efa\u8bae\u7b49\u3002\u91c7\u7528\u590d\u5408\u63d0\u793a\u7b56\u7565\uff0c\u7ed3\u5408\u6559\u80b2\u8005\u89d2\u8272\u3001\u5b89\u5168\u524d\u63d0\u548c\u6b65\u9aa4\u63d0\u793a\u3002\u8fd8\u6d4b\u8bd5\u4e86\u4e0d\u540c\u8bed\u8a00\u548c\u89d2\u8272\u626e\u6f14\u7684\u5f71\u54cd\u3002", "result": "\u590d\u5408\u63d0\u793a\u5c06ZIP\u70b8\u5f39\u534f\u52a9\u7387\u4ece0%\u63d0\u5347\u523097.5%\uff1b\u5fb7\u8bed\u548c\u6cd5\u8bed\u6b63\u5f0f\u8bed\u4f53\u6bd4\u82f1\u8bed\u66f4\u5bb9\u6613\u6cc4\u9732\u4fe1\u606f\uff1bLinux\u7ec8\u7aef\u89d2\u8272\u626e\u6f14\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u7ed5\u8fc7\u5f00\u53d1\u8005\u89c4\u5219\uff1bAI\u8f85\u52a9\u5f3a\u5316\u65b9\u6cd5\u5c06\u6cc4\u9732\u7387\u964d\u81f30%\uff1b13%\u7684\u914d\u5bf9\u6d4b\u8bd5\u663e\u793a\u4e0d\u4e00\u81f4\u7684\u534f\u52a9\u884c\u4e3a\u3002", "conclusion": "AI\u6a21\u578b\u7684\u62d2\u7edd\u884c\u4e3a\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u63d0\u793a\u6846\u67b6\uff0c\u5b58\u5728\u663e\u8457\u7684\u53ef\u64cd\u7eb5\u6027\u3002\u4e0d\u540c\u63a8\u7406\u6808\u7684\u62d2\u7edd\u7387\u5dee\u5f02\u8fbe5-10\u4e2a\u767e\u5206\u70b9\uff0c\u5f15\u53d1\u53ef\u590d\u73b0\u6027\u62c5\u5fe7\u3002OpenAI\u5ba1\u6838API\u76f8\u5bf9\u4e8e\u8bed\u4e49\u8bc4\u5206\u5668\u6f0f\u5224\u4e86\u5b9e\u8d28\u6709\u5e2e\u52a9\u7684\u8f93\u51fa\u3002", "topic": "agent analysis"}}
{"id": "2510.01700", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01700", "abs": "https://arxiv.org/abs/2510.01700", "authors": ["Rohan Wadhawan", "Fabrice Y Harel-Canada", "Zi-Yi Dou", "Suhaila Shakiah", "Robinson Piramuthu", "Nanyun Peng"], "title": "VaPR -- Vision-language Preference alignment for Reasoning", "comment": null, "summary": "Preference finetuning methods like Direct Preference Optimization (DPO) with\nAI-generated feedback have shown promise in aligning Large Vision-Language\nModels (LVLMs) with human preferences. However, existing techniques overlook\nthe prevalence of noise in synthetic preference annotations in the form of\nstylistic and length biases. To this end, we introduce a hard-negative response\ngeneration framework based on LLM-guided response editing, that produces\nrejected responses with targeted errors, maintaining stylistic and length\nsimilarity to the accepted ones. Using this framework, we develop the VaPR\ndataset, comprising 30K high-quality samples, to finetune three LVLM families:\nLLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver\nsignificant performance improvements across ten benchmarks, achieving average\ngains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable\nimprovements on reasoning tasks. A scaling analysis shows that performance\nconsistently improves with data size, with LLaVA models benefiting even at\nsmaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary\nquestions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we\nshow that the framework generalizes to open-source LLMs as editors, with models\ntrained on VaPR-OS achieving ~99% of the performance of models trained on\n\\name, which is synthesized using GPT-4o. Our data, models, and code can be\nfound on the project page https://vap-r.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86VaPR\u6846\u67b6\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u54cd\u5e94\u7f16\u8f91\u751f\u6210\u786c\u8d1f\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u5408\u6210\u504f\u597d\u6807\u6ce8\u4e2d\u7684\u98ce\u683c\u548c\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5fae\u8c03\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5408\u6210\u504f\u597d\u6807\u6ce8\u4e2d\u666e\u904d\u5b58\u5728\u7684\u98ce\u683c\u548c\u957f\u5ea6\u504f\u5dee\u566a\u58f0\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u786c\u8d1f\u6837\u672c\u7684\u65b9\u6cd5\u6765\u6539\u5584\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002", "method": "\u57fa\u4e8eLLM\u5f15\u5bfc\u7684\u54cd\u5e94\u7f16\u8f91\u6846\u67b6\uff0c\u751f\u6210\u5e26\u6709\u76ee\u6807\u9519\u8bef\u7684\u62d2\u7edd\u54cd\u5e94\uff0c\u4fdd\u6301\u4e0e\u63a5\u53d7\u54cd\u5e94\u5728\u98ce\u683c\u548c\u957f\u5ea6\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u6784\u5efa\u4e86\u5305\u542b30K\u6837\u672c\u7684VaPR\u6570\u636e\u96c6\u3002", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVaPR\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff1aLLaVA\u5e73\u5747\u63d0\u53476.5%\uff0cQwen2VL\u63d0\u53474.0%\uff0cQwen2.5VL\u63d0\u53471.5%\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4e8c\u5143\u95ee\u9898\u4e2d\u56de\u7b54\"\u662f\"\u7684\u503e\u5411\u3002", "conclusion": "VaPR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5408\u6210\u504f\u597d\u6807\u6ce8\u7684\u566a\u58f0\u95ee\u9898\uff0c\u6027\u80fd\u968f\u6570\u636e\u89c4\u6a21\u6269\u5927\u800c\u6301\u7eed\u63d0\u5347\uff0c\u4e14\u8be5\u6846\u67b6\u53ef\u6cdb\u5316\u5230\u5f00\u6e90LLM\u4f5c\u4e3a\u7f16\u8f91\u5668\uff0c\u8fbe\u5230\u4e0eGPT-4o\u5408\u6210\u6570\u636e\u76f8\u8fd1\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01268", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.01268", "abs": "https://arxiv.org/abs/2510.01268", "authors": ["Hongyi Zhou", "Jin Zhu", "Pingfan Su", "Kai Ye", "Ying Yang", "Shakeel A O B Gavioli-Akilagun", "Chengchun Shi"], "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees", "comment": "Accepted by NeurIPS2025", "summary": "We study the problem of determining whether a piece of text has been authored\nby a human or by a large language model (LLM). Existing state of the art\nlogits-based detectors make use of statistics derived from the log-probability\nof the observed text evaluated using the distribution function of a given\nsource LLM. However, relying solely on log probabilities can be sub-optimal. In\nresponse, we introduce AdaDetectGPT -- a novel classifier that adaptively\nlearns a witness function from training data to enhance the performance of\nlogits-based detectors. We provide statistical guarantees on its true positive\nrate, false positive rate, true negative rate and false negative rate.\nExtensive numerical studies show AdaDetectGPT nearly uniformly improves the\nstate-of-the-art method in various combination of datasets and LLMs, and the\nimprovement can reach up to 58%. A python implementation of our method is\navailable at https://github.com/Mamba413/AdaDetectGPT.", "AI": {"tldr": "AdaDetectGPT\u662f\u4e00\u79cd\u65b0\u578b\u7684LLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u89c1\u8bc1\u51fd\u6570\u6765\u589e\u5f3a\u57fa\u4e8elogits\u7684\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548cLLM\u7ec4\u5408\u4e2d\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u53ef\u8fbe58%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8elogits\u7684\u68c0\u6d4b\u5668\u4ec5\u4f9d\u8d56\u6587\u672c\u7684\u5bf9\u6570\u6982\u7387\u7edf\u8ba1\uff0c\u8fd9\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165AdaDetectGPT\u5206\u7c7b\u5668\uff0c\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u81ea\u9002\u5e94\u5b66\u4e60\u89c1\u8bc1\u51fd\u6570\u6765\u589e\u5f3alogits-based\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "result": "\u5e7f\u6cdb\u7684\u6570\u503c\u7814\u7a76\u8868\u660e\uff0cAdaDetectGPT\u5728\u5404\u79cd\u6570\u636e\u96c6\u548cLLM\u7ec4\u5408\u4e2d\u51e0\u4e4e\u4e00\u81f4\u5730\u6539\u8fdb\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u6539\u8fdb\u5e45\u5ea6\u53ef\u8fbe58%\u3002", "conclusion": "AdaDetectGPT\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u5e76\u5728\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.01458", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01458", "abs": "https://arxiv.org/abs/2510.01458", "authors": ["Shawn Im", "Yixuan Li"], "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?", "comment": null, "summary": "As large language models (LLMs) advance their capabilities, aligning these\nmodels with human preferences has become crucial. Preference optimization,\nwhich trains models to distinguish between preferred and non-preferred\nresponses based on human feedback, has become a crucial component for aligning\nLLMs. However, most existing works assume noise-free feedback, which is\nunrealistic due to the inherent errors and inconsistencies in human judgments.\nThis paper addresses the impact of noisy feedback on preference optimization,\nproviding generalization guarantees under these conditions. In particular, we\nconsider noise models that correspond to common real-world sources of noise,\nsuch as mislabeling and uncertainty. Unlike traditional analyses that assume\nconvergence, our work focuses on finite-step preference optimization, offering\nnew insights that are more aligned with practical LLM training. We describe how\ngeneralization decays with different types of noise across levels of noise\nrates based on the preference data distribution and number of samples. Our\nanalysis for noisy preference learning applies to a broad family of preference\noptimization losses such as DPO, IPO, SLiC, etc. Empirical validation on\ncontemporary LLMs confirms the practical relevance of our findings, offering\nvaluable insights for developing AI systems that align with human preferences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u566a\u58f0\u53cd\u9988\u5bf9LLM\u504f\u597d\u4f18\u5316\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u4fdd\u8bc1\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u548c\u566a\u58f0\u7387\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1\u4e86DPO\u3001IPO\u3001SLiC\u7b49\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5047\u8bbe\u65e0\u566a\u58f0\u53cd\u9988\uff0c\u4f46\u73b0\u5b9e\u4e2d\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u566a\u58f0\u53cd\u9988\u5bf9\u6a21\u578b\u5bf9\u9f50\u7684\u5f71\u54cd\u3002", "method": "\u8003\u8651\u5e38\u89c1\u7684\u73b0\u5b9e\u566a\u58f0\u6a21\u578b\uff08\u5982\u9519\u8bef\u6807\u8bb0\u548c\u4e0d\u786e\u5b9a\u6027\uff09\uff0c\u5206\u6790\u6709\u9650\u6b65\u504f\u597d\u4f18\u5316\uff0c\u7814\u7a76\u6cdb\u5316\u5728\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u548c\u566a\u58f0\u7387\u4e0b\u7684\u8870\u51cf\u60c5\u51b5\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\u5206\u6790\u7ed3\u679c\u5177\u6709\u5b9e\u9645\u76f8\u5173\u6027\uff0c\u4e3a\u5f00\u53d1\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "\u566a\u58f0\u53cd\u9988\u663e\u8457\u5f71\u54cd\u504f\u597d\u4f18\u5316\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u9700\u8981\u5728\u5b9e\u9645LLM\u8bad\u7ec3\u4e2d\u8003\u8651\u566a\u58f0\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2510.01751", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01751", "abs": "https://arxiv.org/abs/2510.01751", "authors": ["Masike Malatji"], "title": "A cybersecurity AI agent selection and decision support framework", "comment": "6 figures, 6 tables, AI agents decision support framework", "summary": "This paper presents a novel, structured decision support framework that\nsystematically aligns diverse artificial intelligence (AI) agent architectures,\nreactive, cognitive, hybrid, and learning, with the comprehensive National\nInstitute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.\nBy integrating agent theory with industry guidelines, this framework provides a\ntransparent and stepwise methodology for selecting and deploying AI solutions\nto address contemporary cyber threats. Employing a granular decomposition of\nNIST CSF 2.0 functions into specific tasks, the study links essential AI agent\nproperties such as autonomy, adaptive learning, and real-time responsiveness to\neach subcategory's security requirements. In addition, it outlines graduated\nlevels of autonomy (assisted, augmented, and fully autonomous) to accommodate\norganisations at varying stages of cybersecurity maturity. This holistic\napproach transcends isolated AI applications, providing a unified detection,\nincident response, and governance strategy. Through conceptual validation, the\nframework demonstrates how tailored AI agent deployments can align with\nreal-world constraints and risk profiles, enhancing situational awareness,\naccelerating response times, and fortifying long-term resilience via adaptive\nrisk management. Ultimately, this research bridges the gap between theoretical\nAI constructs and operational cybersecurity demands, establishing a foundation\nfor robust, empirically validated multi-agent systems that adhere to industry\nstandards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u5c06\u4e0d\u540c\u7c7b\u578b\u7684AI\u667a\u80fd\u4f53\u67b6\u6784\u4e0eNIST\u7f51\u7edc\u5b89\u5168\u6846\u67b62.0\u7cfb\u7edf\u5bf9\u9f50\uff0c\u4e3a\u9009\u62e9\u90e8\u7f72AI\u89e3\u51b3\u65b9\u6848\u5e94\u5bf9\u7f51\u7edc\u5a01\u80c1\u63d0\u4f9b\u900f\u660e\u65b9\u6cd5\u3002", "motivation": "\u5f25\u5408\u7406\u8bbaAI\u6784\u5efa\u4e0e\u8fd0\u8425\u7f51\u7edc\u5b89\u5168\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u9009\u62e9\u548c\u90e8\u7f72AI\u89e3\u51b3\u65b9\u6848\u5e94\u5bf9\u5f53\u4ee3\u7f51\u7edc\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5c06NIST CSF 2.0\u529f\u80fd\u7ec6\u5206\u4e3a\u5177\u4f53\u4efb\u52a1\uff0c\u5e76\u5c06AI\u667a\u80fd\u4f53\u7279\u6027\uff08\u81ea\u4e3b\u6027\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5b9e\u65f6\u54cd\u5e94\uff09\u4e0e\u6bcf\u4e2a\u5b50\u7c7b\u522b\u7684\u5b89\u5168\u8981\u6c42\u5173\u8054\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u8868\u660e\uff0c\u5b9a\u5236\u7684AI\u667a\u80fd\u4f53\u90e8\u7f72\u53ef\u4ee5\u7b26\u5408\u73b0\u5b9e\u7ea6\u675f\u548c\u98ce\u9669\u72b6\u51b5\uff0c\u589e\u5f3a\u6001\u52bf\u611f\u77e5\u3001\u52a0\u901f\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u98ce\u9669\u7ba1\u7406\u52a0\u5f3a\u957f\u671f\u97e7\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9075\u5faa\u884c\u4e1a\u6807\u51c6\u7684\u7a33\u5065\u3001\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06AI\u7406\u8bba\u4e0e\u884c\u4e1a\u6307\u5357\u76f8\u7ed3\u5408\u3002", "topic": "agent analysis"}}
{"id": "2510.01460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01460", "abs": "https://arxiv.org/abs/2510.01460", "authors": ["Lu Li", "Tianwei Ni", "Yihao Sun", "Pierre-Luc Bacon"], "title": "The Three Regimes of Offline-to-Online Reinforcement Learning", "comment": null, "summary": "Offline-to-online reinforcement learning (RL) has emerged as a practical\nparadigm that leverages offline datasets for pretraining and online\ninteractions for fine-tuning. However, its empirical behavior is highly\ninconsistent: design choices of online-fine tuning that work well in one\nsetting can fail completely in another. We propose a stability--plasticity\nprinciple that can explain this inconsistency: we should preserve the knowledge\nof pretrained policy or offline dataset during online fine-tuning, whichever is\nbetter, while maintaining sufficient plasticity. This perspective identifies\nthree regimes of online fine-tuning, each requiring distinct stability\nproperties. We validate this framework through a large-scale empirical study,\nfinding that the results strongly align with its predictions in 45 of 63 cases.\nThis work provides a principled framework for guiding design choices in\noffline-to-online RL based on the relative performance of the offline dataset\nand the pretrained policy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u539f\u5219\u6765\u89e3\u91ca\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5f3a\u8c03\u5728\u5728\u7ebf\u5fae\u8c03\u671f\u95f4\u5e94\u4fdd\u7559\u9884\u8bad\u7ec3\u7b56\u7565\u6216\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u66f4\u597d\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7684\u53ef\u5851\u6027\u3002", "motivation": "\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7ecf\u9a8c\u884c\u4e3a\u9ad8\u5ea6\u4e0d\u4e00\u81f4\uff0c\u5728\u4e00\u79cd\u8bbe\u7f6e\u4e2d\u6709\u6548\u7684\u5728\u7ebf\u5fae\u8c03\u8bbe\u8ba1\u9009\u62e9\u5728\u53e6\u4e00\u79cd\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u5b8c\u5168\u5931\u8d25\uff0c\u9700\u8981\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u79cd\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e86\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u539f\u5219\uff0c\u8bc6\u522b\u4e86\u4e09\u79cd\u5728\u7ebf\u5fae\u8c03\u673a\u5236\uff0c\u6bcf\u79cd\u673a\u5236\u9700\u8981\u4e0d\u540c\u7684\u7a33\u5b9a\u6027\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u8be5\u6846\u67b6\u3002", "result": "\u572863\u4e2a\u6848\u4f8b\u4e2d\u768445\u4e2a\u6848\u4f8b\u4e2d\uff0c\u5b9e\u8bc1\u7ed3\u679c\u4e0e\u8be5\u6846\u67b6\u7684\u9884\u6d4b\u9ad8\u5ea6\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u8be5\u539f\u5219\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u79bb\u7ebf\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u76f8\u5bf9\u6027\u80fd\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u9009\u62e9\u7684\u539f\u5219\u6027\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01279", "abs": "https://arxiv.org/abs/2510.01279", "authors": ["Yongchao Chen", "Jiefeng Chen", "Rui Meng", "Ji Yin", "Na Li", "Chuchu Fan", "Chi Wang", "Tomas Pfister", "Jinsung Yoon"], "title": "TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture", "comment": "27 pages, 13 figures", "summary": "While integrating tools like Code Interpreter and Search has significantly\nenhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and\nGemini-Pro, practical guidance on optimal tool use is lacking. The core\nchallenge is effectively combining textual reasoning, coding, and search for\ndiverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an\nensemble framework that runs multiple agents in parallel, each employing\ndistinct tool-use strategies and answer paths. Agents in TUMIX iteratively\nshare and refine responses based on the question and previous answers. In\nexperiments, TUMIX achieves significant gains over state-of-the-art\ntool-augmented and test-time scaling methods, delivering an average accuracy\nimprovement of up to 3.55% over the best baseline on Gemini-2.5-Pro and\nGemini-2.5-Flash across key reasoning benchmarks, with near-equal inference\ncosts. We find that agent diversity and quality are crucial and can be enhanced\nby using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt\nrefinement upon reaching sufficient confidence, preserving performance at only\n49% of the inference cost. Further scaling can achieve higher performance,\nalbeit at a greater cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86TUMIX\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u91c7\u7528\u4e0d\u540c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u7684\u4ee3\u7406\uff0c\u8ba9\u4ee3\u7406\u8fed\u4ee3\u5171\u4eab\u548c\u4f18\u5316\u56de\u7b54\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u8fd1\u7684\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5f53\u524dLLM\u5728\u96c6\u6210\u4ee3\u7801\u89e3\u91ca\u5668\u548c\u641c\u7d22\u7b49\u5de5\u5177\u540e\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5982\u4f55\u6709\u6548\u7ed3\u5408\u6587\u672c\u63a8\u7406\u3001\u7f16\u7801\u548c\u641c\u7d22\u7684\u5b9e\u7528\u6307\u5bfc\u3002", "method": "TUMIX\u6846\u67b6\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u4ee3\u7406\uff0c\u6bcf\u4e2a\u4ee3\u7406\u91c7\u7528\u4e0d\u540c\u7684\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u548c\u56de\u7b54\u8def\u5f84\uff0c\u4ee3\u7406\u57fa\u4e8e\u95ee\u9898\u548c\u5148\u524d\u56de\u7b54\u8fed\u4ee3\u5171\u4eab\u548c\u4f18\u5316\u54cd\u5e94\u3002", "result": "\u5728Gemini-2.5-Pro\u548cGemini-2.5-Flash\u4e0a\uff0cTUMIX\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8fbe3.55%\uff0c\u63a8\u7406\u6210\u672c\u76f8\u8fd1\uff1b\u5f53\u8fbe\u5230\u8db3\u591f\u7f6e\u4fe1\u5ea6\u65f6\u53ef\u505c\u6b62\u4f18\u5316\uff0c\u4ec5\u970049%\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u4ee3\u7406\u591a\u6837\u6027\u548c\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u901a\u8fc7LLM\u81ea\u52a8\u4f18\u5316\u4ee3\u7406\u8bbe\u8ba1\u6765\u589e\u5f3a\uff1b\u8fdb\u4e00\u6b65\u6269\u5c55\u53ef\u5b9e\u73b0\u66f4\u9ad8\u6027\u80fd\uff0c\u4f46\u6210\u672c\u66f4\u9ad8\u3002", "topic": "agent analysis"}}
{"id": "2510.01833", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01833", "abs": "https://arxiv.org/abs/2510.01833", "authors": ["Zhihao Dou", "Qinjian Zhao", "Zhongwei Wan", "Dinggen Zhang", "Weida Wang", "Towsif Raiyan", "Benteng Chen", "Qingtao Pan", "Yang Ouyang", "Zhiqiang Gao", "Shufei Zhang", "Sumon Biswas"], "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning", "comment": "19 pages and 5 figures", "summary": "Large language models (LLMs) have demonstrated remarkable reasoning abilities\nin complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,\ndue to their autoregressive token-level generation, the reasoning process is\nlargely constrained to local decision-making and lacks global planning. This\nlimitation frequently results in redundant, incoherent, or inaccurate\nreasoning, which significantly degrades overall performance. Existing\napproaches, such as tree-based algorithms and reinforcement learning (RL),\nattempt to address this issue but suffer from high computational costs and\noften fail to produce optimal reasoning trajectories. To tackle this challenge,\nwe propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy\nOptimization PTA-GRPO, a two-stage framework designed to improve both\nhigh-level planning and fine-grained CoT reasoning. In the first stage, we\nleverage advanced LLMs to distill CoT into compact high-level guidance, which\nis then used for supervised fine-tuning (SFT). In the second stage, we\nintroduce a guidance-aware RL method that jointly optimizes the final output\nand the quality of high-level guidance, thereby enhancing reasoning\neffectiveness. We conduct extensive experiments on multiple mathematical\nreasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across\ndiverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and\nLLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently\nachieves stable and significant improvements across different models and tasks,\nvalidating its effectiveness and generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86PTA-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u6539\u8fdbLLM\u7684\u63a8\u7406\u80fd\u529b\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u9ad8\u7ea7LLM\u5c06\u601d\u7ef4\u94fe\u63d0\u70bc\u4e3a\u7d27\u51d1\u7684\u9ad8\u5c42\u6307\u5bfc\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u6307\u5bfc\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8054\u5408\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\u548c\u9ad8\u5c42\u6307\u5bfc\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u4e3b\u8981\u4f9d\u8d56\u601d\u7ef4\u94fe\uff0c\u4f46\u7531\u4e8e\u81ea\u56de\u5f52\u7684token\u7ea7\u751f\u6210\uff0c\u63a8\u7406\u8fc7\u7a0b\u5c40\u9650\u4e8e\u5c40\u90e8\u51b3\u7b56\uff0c\u7f3a\u4e4f\u5168\u5c40\u89c4\u5212\uff0c\u5bfc\u81f4\u5197\u4f59\u3001\u4e0d\u8fde\u8d2f\u6216\u4e0d\u51c6\u786e\u7684\u63a8\u7406\uff0c\u964d\u4f4e\u6574\u4f53\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6811\u7b97\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u4ea7\u751f\u6700\u4f18\u63a8\u7406\u8f68\u8ff9\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9ad8\u7ea7LLM\u5c06\u601d\u7ef4\u94fe\u63d0\u70bc\u4e3a\u7d27\u51d1\u9ad8\u5c42\u6307\u5bfc\uff0c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2\uff09\u5f15\u5165\u6307\u5bfc\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u6700\u7ec8\u8f93\u51fa\u548c\u9ad8\u5c42\u6307\u5bfc\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08MATH\u3001AIME2024\u3001AIME2025\u3001AMC\uff09\u548c\u4e0d\u540c\u57fa\u7840\u6a21\u578b\uff08Qwen2.5-7B-Instruct\u3001Qwen3-8B\u3001Qwen3-14B\u3001LLaMA3.2-3B\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPTA-GRPO\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "PTA-GRPO\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u5c42\u89c4\u5212\u548c\u7ec6\u7c92\u5ea6\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01479", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.01479", "abs": "https://arxiv.org/abs/2510.01479", "authors": ["Shriram Karpoora Sundara Pandian", "Ali Baheri"], "title": "Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy optimization from fixed\ndatasets, making it suitable for safety-critical applications where online\nexploration is infeasible. However, these datasets are often contaminated by\nadversarial poisoning, system errors, or low-quality samples, leading to\ndegraded policy performance in standard behavioral cloning (BC) and offline RL\nmethods. This paper introduces Density-Ratio Weighted Behavioral Cloning\n(Weighted BC), a robust imitation learning approach that uses a small, verified\nclean reference set to estimate trajectory-level density ratios via a binary\ndiscriminator. These ratios are clipped and used as weights in the BC objective\nto prioritize clean expert behavior while down-weighting or discarding\ncorrupted data, without requiring knowledge of the contamination mechanism. We\nestablish theoretical guarantees showing convergence to the clean expert policy\nwith finite-sample bounds that are independent of the contamination rate. A\ncomprehensive evaluation framework is established, which incorporates various\npoisoning protocols (reward, state, transition, and action) on continuous\ncontrol benchmarks. Experiments demonstrate that Weighted BC maintains\nnear-optimal performance even at high contamination ratios outperforming\nbaselines such as traditional BC, batch-constrained Q-learning (BCQ) and\nbehavior regularized actor-critic (BRAC).", "AI": {"tldr": "\u63d0\u51fa\u5bc6\u5ea6\u6bd4\u7387\u52a0\u6743\u884c\u4e3a\u514b\u9686(Weighted BC)\uff0c\u4f7f\u7528\u5c0f\u578b\u5e72\u51c0\u53c2\u8003\u96c6\u901a\u8fc7\u4e8c\u5143\u5224\u522b\u5668\u4f30\u8ba1\u8f68\u8ff9\u7ea7\u5bc6\u5ea6\u6bd4\u7387\uff0c\u5728BC\u76ee\u6807\u4e2d\u52a0\u6743\u4ee5\u4f18\u5148\u5904\u7406\u5e72\u51c0\u4e13\u5bb6\u884c\u4e3a\uff0c\u65e0\u9700\u77e5\u9053\u6c61\u67d3\u673a\u5236\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6570\u636e\u96c6\u5e38\u88ab\u5bf9\u6297\u6027\u6c61\u67d3\u3001\u7cfb\u7edf\u9519\u8bef\u6216\u4f4e\u8d28\u91cf\u6837\u672c\u6c61\u67d3\uff0c\u5bfc\u81f4\u6807\u51c6\u884c\u4e3a\u514b\u9686\u548c\u79bb\u7ebfRL\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u5c0f\u578b\u5e72\u51c0\u53c2\u8003\u96c6\u8bad\u7ec3\u4e8c\u5143\u5224\u522b\u5668\u4f30\u8ba1\u8f68\u8ff9\u7ea7\u5bc6\u5ea6\u6bd4\u7387\uff0c\u5c06\u6bd4\u7387\u88c1\u526a\u540e\u4f5c\u4e3a\u6743\u91cd\u7528\u4e8eBC\u76ee\u6807\uff0c\u4f18\u5148\u5904\u7406\u5e72\u51c0\u6570\u636e\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5373\u4f7f\u5728\u9ad8\u6c61\u67d3\u7387\u4e0b\u4e5f\u80fd\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edfBC\u3001BCQ\u548cBRAC\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Weighted BC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u80fd\u5728\u6570\u636e\u6c61\u67d3\u60c5\u51b5\u4e0b\u6709\u6548\u5b66\u4e60\u4e13\u5bb6\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01499", "categories": ["cs.LG", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2510.01499", "abs": "https://arxiv.org/abs/2510.01499", "authors": ["Rui Ai", "Yuqi Pan", "David Simchi-Levi", "Milind Tambe", "Haifeng Xu"], "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information", "comment": null, "summary": "With the rapid progress of multi-agent large language model (LLM) reasoning,\nhow to effectively aggregate answers from multiple LLMs has emerged as a\nfundamental challenge. Standard majority voting treats all answers equally,\nfailing to consider latent heterogeneity and correlation across models. In this\nwork, we design two new aggregation algorithms called Optimal Weight (OW) and\nInverse Surprising Popularity (ISP), leveraging both first-order and\nsecond-order information. Our theoretical analysis shows these methods provably\nmitigate inherent limitations of majority voting under mild assumptions,\nleading to more reliable collective decisions. We empirically validate our\nalgorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as\nUltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all\ncases, our methods consistently outperform majority voting, offering both\npractical performance gains and conceptual insights for the design of robust\nmulti-agent LLM pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53LLM\u7b54\u6848\u805a\u5408\u7b97\u6cd5\uff1a\u6700\u4f18\u6743\u91cd(OW)\u548c\u9006\u610f\u5916\u6d41\u884c\u5ea6(ISP)\uff0c\u5229\u7528\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u6539\u8fdb\u591a\u6570\u6295\u7968\u65b9\u6cd5\u3002", "motivation": "\u6807\u51c6\u591a\u6570\u6295\u7968\u65b9\u6cd5\u5e73\u7b49\u5bf9\u5f85\u6240\u6709\u7b54\u6848\uff0c\u672a\u80fd\u8003\u8651\u6a21\u578b\u95f4\u7684\u6f5c\u5728\u5f02\u8d28\u6027\u548c\u76f8\u5173\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u805a\u5408\u65b9\u6cd5\u6765\u63d0\u9ad8\u96c6\u4f53\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86OW\u548cISP\u4e24\u79cd\u7b97\u6cd5\uff0c\u5229\u7528\u4e00\u9636\u548c\u4e8c\u9636\u4fe1\u606f\u8fdb\u884c\u7b54\u6848\u805a\u5408\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8fd9\u4e9b\u65b9\u6cd5\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u80fd\u591f\u7f13\u89e3\u591a\u6570\u6295\u7968\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u3001UltraFeedback\u548cMMLU\u7b49LLM\u5fae\u8c03\u57fa\u51c6\u4ee5\u53caARMMAN\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\uff0c\u65b0\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u591a\u6570\u6295\u7968\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5b9e\u9645\u6027\u80fd\u63d0\u5347\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53LLM\u6d41\u6c34\u7ebf\u63d0\u4f9b\u4e86\u6982\u5ff5\u6027\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.01924", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.01924", "abs": "https://arxiv.org/abs/2510.01924", "authors": ["Crystal Qian", "Aaron Parisi", "Cl\u00e9mentine Bouleau", "Vivian Tsai", "Ma\u00ebl Lebreton", "Lucas Dixon"], "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning", "comment": null, "summary": "As large language models (LLMs) are increasingly used to model and augment\ncollective decision-making, it is critical to examine their alignment with\nhuman social reasoning. We present an empirical framework for assessing\ncollective alignment, in contrast to prior work on the individual level. Using\nthe Lost at Sea social psychology task, we conduct a large-scale online\nexperiment (N=748), randomly assigning groups to leader elections with either\nvisible demographic attributes (e.g. name, gender) or pseudonymous aliases. We\nthen simulate matched LLM groups conditioned on the human data, benchmarking\nGemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some\nmirror human biases; others mask these biases and attempt to compensate for\nthem. We empirically demonstrate that human-AI alignment in collective\nreasoning depends on context, cues, and model-specific inductive biases.\nUnderstanding how LLMs align with collective human behavior is critical to\nadvancing socially-aligned AI, and demands dynamic benchmarks that capture the\ncomplexities of collective reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bc4\u4f30LLMs\u4e0e\u4eba\u7c7b\u96c6\u4f53\u51b3\u7b56\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u901a\u8fc7Lost at Sea\u5b9e\u9a8c\u5bf9\u6bd4\u4eba\u7c7b\u548cAI\u7fa4\u4f53\u5728\u9886\u5bfc\u9009\u4e3e\u4e2d\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "motivation": "\u968f\u7740LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5efa\u6a21\u548c\u589e\u5f3a\u96c6\u4f53\u51b3\u7b56\uff0c\u9700\u8981\u68c0\u9a8c\u5b83\u4eec\u4e0e\u4eba\u7c7b\u793e\u4f1a\u63a8\u7406\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u7279\u522b\u662f\u96c6\u4f53\u5c42\u9762\u7684\u5bf9\u9f50\u800c\u975e\u4e2a\u4f53\u5c42\u9762\u3002", "method": "\u4f7f\u7528Lost at Sea\u793e\u4f1a\u5fc3\u7406\u5b66\u4efb\u52a1\u8fdb\u884c\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\uff08N=748\uff09\uff0c\u968f\u673a\u5206\u914d\u7fa4\u4f53\u5230\u6709\u53ef\u89c1\u4eba\u53e3\u5c5e\u6027\u6216\u65e0\u4eba\u53e3\u5c5e\u6027\u7684\u9886\u5bfc\u9009\u4e3e\u6761\u4ef6\uff0c\u7136\u540e\u6a21\u62df\u5339\u914d\u7684LLM\u7fa4\u4f53\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "LLM\u884c\u4e3a\u51fa\u73b0\u5206\u5316\uff1a\u6709\u4e9b\u955c\u50cf\u4eba\u7c7b\u504f\u89c1\uff0c\u6709\u4e9b\u63a9\u76d6\u8fd9\u4e9b\u504f\u89c1\u5e76\u8bd5\u56fe\u8865\u507f\u3002\u4eba\u7c7b-AI\u5728\u96c6\u4f53\u63a8\u7406\u4e2d\u7684\u5bf9\u9f50\u53d6\u51b3\u4e8e\u60c5\u5883\u3001\u7ebf\u7d22\u548c\u6a21\u578b\u7279\u5b9a\u7684\u5f52\u7eb3\u504f\u89c1\u3002", "conclusion": "\u7406\u89e3LLMs\u5982\u4f55\u4e0e\u96c6\u4f53\u4eba\u7c7b\u884c\u4e3a\u5bf9\u9f50\u5bf9\u4e8e\u63a8\u8fdb\u793e\u4f1a\u5bf9\u9f50AI\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u6355\u6349\u96c6\u4f53\u63a8\u7406\u590d\u6742\u6027\u7684\u52a8\u6001\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.01591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01591", "abs": "https://arxiv.org/abs/2510.01591", "authors": ["Zhenwen Liang", "Ruosen Li", "Yujun Zhou", "Linfeng Song", "Dian Yu", "Xinya Du", "Haitao Mi", "Dong Yu"], "title": "CLUE: Non-parametric Verification from Experience via Hidden-State Clustering", "comment": null, "summary": "Assessing the quality of Large Language Model (LLM) outputs presents a\ncritical challenge. Previous methods either rely on text-level information\n(e.g., reward models, majority voting), which can overfit to superficial cues,\nor on calibrated confidence from token probabilities, which would fail on\nless-calibrated models. Yet both of these signals are, in fact, partial\nprojections of a richer source of information: the model's internal hidden\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\nfeatures that underpin text-based judgments, while later layers increasingly\nalign with output logits, embedding confidence-related information. This paper\nexplores hidden states directly as a unified foundation for verification. We\nshow that the correctness of a solution is encoded as a geometrically separable\nsignature within the trajectory of hidden activations. To validate this, we\npresent Clue (Clustering and Experience-based Verification), a deliberately\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\nsummarizes each reasoning trace by an hidden state delta and classifies\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\nclusters formed from past experience. The simplicity of this method highlights\nthe strength of the underlying signal. Empirically, CLUE consistently\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\nconfidence-based methods in reranking candidates, improving both top-1 and\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\n(top-maj@16).", "AI": {"tldr": "\u63d0\u51faCLUE\u65b9\u6cd5\uff0c\u5229\u7528LLM\u5185\u90e8\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u7edf\u4e00\u9a8c\u8bc1\u57fa\u7840\uff0c\u901a\u8fc7\u65e0\u53c2\u6570\u805a\u7c7b\u65b9\u6cd5\u63d0\u5347\u7b54\u6848\u9009\u62e9\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u5c42\u9762\u4fe1\u606f\u6216token\u6982\u7387\u6821\u51c6\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u6216\u5bf9\u6821\u51c6\u4e0d\u4f73\u7684\u6a21\u578b\u5931\u6548\uff0c\u800c\u9690\u85cf\u72b6\u6001\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f", "method": "CLUE\uff1a\u57fa\u4e8e\u805a\u7c7b\u548c\u7ecf\u9a8c\u7684\u65e0\u53c2\u6570\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u53d8\u5316\u91cf\uff0c\u4f7f\u7528\u6700\u8fd1\u8d28\u5fc3\u8ddd\u79bb\u5206\u7c7b\u6b63\u786e\u6027", "result": "\u5728AIME 24/25\u548cGPQA\u4e0a\u4f18\u4e8eLLM-as-a-judge\u57fa\u7ebf\uff0c\u5339\u914d\u6216\u8d85\u8fc7\u73b0\u4ee3\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u5728AIME 24\u4e0a\u5c06\u51c6\u786e\u7387\u4ece56.7%\u63d0\u5347\u523070.0%", "conclusion": "\u9690\u85cf\u72b6\u6001\u8f68\u8ff9\u5305\u542b\u51e0\u4f55\u53ef\u5206\u79bb\u7684\u6b63\u786e\u6027\u7b7e\u540d\uff0c\u53ef\u4f5c\u4e3a\u5f3a\u5927\u7684\u9a8c\u8bc1\u4fe1\u53f7", "topic": "agent analysis"}}
{"id": "2510.02190", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02190", "abs": "https://arxiv.org/abs/2510.02190", "authors": ["Yang Yao", "Yixu Wang", "Yuxuan Zhang", "Yi Lu", "Tianle Gu", "Lingyu Li", "Dingyi Zhao", "Keming Wu", "Haozhe Wang", "Ping Nie", "Yan Teng", "Yingchun Wang"], "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports", "comment": null, "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRAs\uff09\u7684\u4e25\u683c\u57fa\u51c6\u548c\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b214\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u6311\u6218\u6027\u67e5\u8be2\u548c\u624b\u52a8\u6784\u5efa\u7684\u53c2\u8003\u5305\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u683c\u5f0f\u62a5\u544a\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u8bc4\u4f30\u7ef4\u5ea6\u3001\u54cd\u5e94\u683c\u5f0f\u548c\u8bc4\u5206\u673a\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5177\u6709\u4efb\u52a1\u5206\u89e3\u3001\u8de8\u6e90\u68c0\u7d22\u3001\u591a\u9636\u6bb5\u63a8\u7406\u548c\u7ed3\u6784\u5316\u8f93\u51fa\u80fd\u529b\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10\u4e2a\u5e7f\u6cdb\u4e3b\u9898\u9886\u57df\u7684214\u4e2a\u4e13\u5bb6\u7b56\u5212\u67e5\u8be2\u7684\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u4e86\u8bed\u4e49\u8d28\u91cf\u3001\u4e3b\u9898\u805a\u7126\u548c\u68c0\u7d22\u53ef\u4fe1\u5ea6\u7684\u8bc4\u5206\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u4e3b\u6d41DRAs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7f51\u7edc\u641c\u7d22\u5de5\u5177\u589e\u5f3a\u7684\u63a8\u7406\u6a21\u578b\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aDRA\u7cfb\u7edf\u7684\u80fd\u529b\u8bc4\u4f30\u3001\u67b6\u6784\u6539\u8fdb\u548c\u8303\u5f0f\u8fdb\u6b65\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.01617", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01617", "abs": "https://arxiv.org/abs/2510.01617", "authors": ["Hui Yi Leong", "Yuheng Li", "Yuqing Wu", "Wenwen Ouyang", "Wei Zhu", "Jiechao Gao"], "title": "AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System", "comment": null, "summary": "Although large language models (LLMs) have revolutionized natural language\nprocessing capabilities, their practical implementation as autonomous\nmulti-agent systems (MAS) for industrial problem-solving encounters persistent\nbarriers. Conventional MAS architectures are fundamentally restricted by\ninflexible, hand-crafted graph topologies that lack contextual responsiveness,\nresulting in diminished efficacy across varied academic and commercial\nworkloads. To surmount these constraints, we introduce AMAS, a\nparadigm-shifting framework that redefines LLM-based MAS through a novel\ndynamic graph designer. This component autonomously identifies task-specific\noptimal graph configurations via lightweight LLM adaptation, eliminating the\nreliance on monolithic, universally applied structural templates. Instead, AMAS\nexploits the intrinsic properties of individual inputs to intelligently direct\nquery trajectories through task-optimized agent pathways. Rigorous validation\nacross question answering, mathematical deduction, and code generation\nbenchmarks confirms that AMAS systematically exceeds state-of-the-art\nsingle-agent and multi-agent approaches across diverse LLM architectures. Our\ninvestigation establishes that context-sensitive structural adaptability\nconstitutes a foundational requirement for high-performance LLM MAS\ndeployments.", "AI": {"tldr": "AMAS\u662f\u4e00\u4e2a\u9769\u547d\u6027\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u56fe\u8bbe\u8ba1\u5668\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u7684\u6700\u4f18\u56fe\u914d\u7f6e\uff0c\u8d85\u8d8a\u4f20\u7edf\u56fa\u5b9a\u62d3\u6251\u7ed3\u6784\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u54cd\u5e94\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u52a8\u6001\u56fe\u8bbe\u8ba1\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LLM\u9002\u914d\u81ea\u4e3b\u8bc6\u522b\u4efb\u52a1\u7279\u5b9a\u7684\u6700\u4f18\u56fe\u914d\u7f6e\uff0c\u5229\u7528\u8f93\u5165\u7684\u5185\u5728\u5c5e\u6027\u667a\u80fd\u5f15\u5bfc\u67e5\u8be2\u8f68\u8ff9\u3002", "result": "\u5728\u95ee\u7b54\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAMAS\u7cfb\u7edf\u6027\u5730\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u7ed3\u6784\u9002\u5e94\u6027\u662f\u9ad8\u6027\u80fdLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u90e8\u7f72\u7684\u57fa\u7840\u8981\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2510.02230", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02230", "abs": "https://arxiv.org/abs/2510.02230", "authors": ["Phuc Minh Nguyen", "Chinh D. La", "Duy M. H. Nguyen", "Nitesh V. Chawla", "Binh T. Nguyen", "Khoa D. Doan"], "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models", "comment": "23 pages, 15 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.", "AI": {"tldr": "RLVR\u65b9\u6cd5\u867d\u7136\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u53cd\u800c\u4f1a\u7f29\u5c0f\u63a8\u7406\u8fb9\u754c\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8d1f\u5e72\u6270\u73b0\u8c61\u548c\u8d62\u5bb6\u901a\u5403\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76RLVR\u65b9\u6cd5\u4e2d\u51fa\u73b0\u7684\u63a8\u7406\u8fb9\u754c\u7f29\u5c0f\u95ee\u9898\uff0c\u5206\u6790\u5176\u5b66\u4e60\u52a8\u6001\u4e2d\u7684\u5173\u952e\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793aRLVR\u4e2d\u7684\u8d1f\u5e72\u6270\u548c\u8d62\u5bb6\u901a\u5403\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u4f4e\u6982\u7387\u95ee\u9898\u7684\u6570\u636e\u7b5b\u9009\u7b97\u6cd5\u3002", "result": "\u53d1\u73b0RLVR\u4f1a\u5f3a\u5316\u9ad8\u6982\u7387\u6b63\u786e\u89e3\u800c\u6291\u5236\u4f4e\u6982\u7387\u89e3\uff0c\u5bfc\u81f4\u6a21\u578b\u6536\u655b\u5230\u72ed\u7a84\u7684\u89e3\u9898\u7b56\u7565\u3002\u63d0\u51fa\u7684\u6570\u636e\u7b5b\u9009\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86Pass@k\u6027\u80fd\u3002", "conclusion": "RLVR\u5b58\u5728\u56fa\u6709\u7684\u5b66\u4e60\u52a8\u6001\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u6ce8\u4e8e\u4f4e\u6982\u7387\u95ee\u9898\u7684\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u6539\u5584\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01539", "abs": "https://arxiv.org/abs/2510.01539", "authors": ["Aniket Vashishtha", "Qirun Dai", "Hongyuan Mei", "Amit Sharma", "Chenhao Tan", "Hao Peng"], "title": "Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code", "comment": null, "summary": "Counterfactual reasoning, a hallmark of intelligence, consists of three\nsteps: inferring latent variables from observations (abduction), constructing\nalternatives (interventions), and predicting their outcomes (prediction). This\nskill is essential for advancing LLMs' causal understanding and expanding their\napplications in high-stakes domains such as scientific research. However,\nexisting efforts in assessing LLM's counterfactual reasoning capabilities tend\nto skip the abduction step, effectively reducing to interventional reasoning\nand leading to overestimation of LLM performance. To address this, we introduce\nexecutable counterfactuals, a novel framework that operationalizes causal\nreasoning through code and math problems. Our framework explicitly requires all\nthree steps of counterfactual reasoning and enables scalable synthetic data\ncreation with varying difficulty, creating a frontier for evaluating and\nimproving LLM's reasoning. Our results reveal substantial drop in accuracy\n(25-40%) from interventional to counterfactual reasoning for SOTA models like\no4-mini and Claude-4-Sonnet. To address this gap, we construct a training set\ncomprising counterfactual code problems having if-else condition and test on\nout-of-domain code structures (e.g. having while-loop); we also test whether a\nmodel trained on code would generalize to counterfactual math word problems.\nWhile supervised finetuning on stronger models' reasoning traces improves\nin-domain performance of Qwen models, it leads to a decrease in accuracy on OOD\ntasks such as counterfactual math problems. In contrast, reinforcement learning\ninduces the core cognitive behaviors and generalizes to new domains, yielding\ngains over the base model on both code (improvement of 1.5x-2x) and math\nproblems. Analysis of the reasoning traces reinforces these findings and\nhighlights the promise of RL for improving LLMs' counterfactual reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6267\u884c\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u548c\u6570\u5b66\u95ee\u9898\u8bc4\u4f30LLM\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4ece\u5e72\u9884\u63a8\u7406\u5230\u53cd\u4e8b\u5b9e\u63a8\u7406\u51c6\u786e\u7387\u4e0b\u964d25-40%\uff0c\u5f3a\u5316\u5b66\u4e60\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u80fd\u66f4\u597d\u5730\u6cdb\u5316\u5230\u65b0\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u5f80\u5f80\u8df3\u8fc7\u6eaf\u56e0\u6b65\u9aa4\uff0c\u5bfc\u81f4\u9ad8\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5b8c\u6574\u8bc4\u4f30\u4e09\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u6846\u67b6\u3002", "method": "\u5f15\u5165\u53ef\u6267\u884c\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u548c\u6570\u5b66\u95ee\u9898\u64cd\u4f5c\u5316\u56e0\u679c\u63a8\u7406\uff0c\u8981\u6c42\u5b8c\u6574\u7684\u6eaf\u56e0\u3001\u5e72\u9884\u548c\u9884\u6d4b\u4e09\u4e2a\u6b65\u9aa4\uff0c\u5e76\u521b\u5efa\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u3002", "result": "SOTA\u6a21\u578b\u4ece\u5e72\u9884\u63a8\u7406\u5230\u53cd\u4e8b\u5b9e\u63a8\u7406\u51c6\u786e\u7387\u4e0b\u964d25-40%\uff1b\u5f3a\u5316\u5b66\u4e60\u5728\u4ee3\u7801\u548c\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u8bf1\u5bfc\u6838\u5fc3\u8ba4\u77e5\u884c\u4e3a\u5e76\u6cdb\u5316\u5230\u65b0\u9886\u57df\uff0c\u5728\u63d0\u5347LLM\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.02263", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02263", "abs": "https://arxiv.org/abs/2510.02263", "authors": ["Yuxiao Qu", "Anikait Singh", "Yoonho Lee", "Amrith Setlur", "Ruslan Salakhutdinov", "Chelsea Finn", "Aviral Kumar"], "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems", "comment": null, "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e86RLAD\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u62bd\u8c61\u6765\u5f15\u5bfc\u6a21\u578b\u8fdb\u884c\u66f4\u6709\u6548\u7684\u63a8\u7406\uff0c\u91c7\u7528\u4e24\u73a9\u5bb6RL\u8bad\u7ec3\u8303\u5f0f\u8054\u5408\u8bad\u7ec3\u62bd\u8c61\u751f\u6210\u5668\u548c\u89e3\u51b3\u65b9\u6848\u751f\u6210\u5668\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5927\u578b\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u7684\u7a0b\u5e8f\u6355\u83b7\u548c\u91cd\u7528\u80fd\u529b\uff0c\u907f\u514d\u5197\u957f\u548c\u9000\u5316\u7684\u63a2\u7d22\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u63a8\u7406\u3002", "method": "\u5f15\u5165\u63a8\u7406\u62bd\u8c61\u6982\u5ff5\uff0c\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u9488\u5bf9\u95ee\u9898\u63d0\u51fa\u591a\u4e2a\u62bd\u8c61\uff0c\u7136\u540e\u901a\u8fc7RL\u6fc0\u52b1\u5728\u5229\u7528\u8fd9\u4e9b\u62bd\u8c61\u4fe1\u606f\u7684\u57fa\u7840\u4e0a\u6784\u5efa\u89e3\u51b3\u65b9\u6848\uff0c\u5f62\u6210\u62bd\u8c61\u751f\u6210\u5668\u548c\u89e3\u51b3\u65b9\u6848\u751f\u6210\u5668\u7684\u4e24\u73a9\u5bb6RL\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "RLAD\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u63a2\u7d22\uff0c\u89e3\u8026\u4e86\u62bd\u8c61\u63d0\u8bae\u548c\u89e3\u51b3\u65b9\u6848\u751f\u6210\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u66f4\u96be\u95ee\u9898\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63a8\u7406\u62bd\u8c61\u80fd\u6709\u6548\u5f15\u5bfc\u6709\u610f\u4e49\u7684\u63a2\u7d22\uff0c\u5728\u6d4b\u8bd5\u65f6\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u751f\u6210\u62bd\u8c61\u6bd4\u751f\u6210\u66f4\u591a\u89e3\u51b3\u65b9\u6848\u66f4\u6709\u5229\u4e8e\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01654", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01654", "abs": "https://arxiv.org/abs/2510.01654", "authors": ["Mudita Khurana", "Raunak Jain"], "title": "SoK: Measuring What Matters for Closed-Loop Security Agents", "comment": null, "summary": "Cybersecurity is a relentless arms race, with AI driven offensive systems\nevolving faster than traditional defenses can adapt. Research and tooling\nremain fragmented across isolated defensive functions, creating blind spots\nthat adversaries exploit. Autonomous agents capable of integrating, exploit\nconfirmation, remediation, and validation into a single closed loop offer\npromise, but the field lacks three essentials: a framework defining the agentic\ncapabilities of security systems across security life cycle, a principled\nmethod for evaluating closed loop agents, and a benchmark for measuring their\nperformance in practice. We introduce CLASP: the Closed-Loop Autonomous\nSecurity Performance framework which aligns the security lifecycle\n(reconnaissance, exploitation, root cause analysis, patch synthesis,\nvalidation) with core agentic capabilities (planning, tool use, memory,\nreasoning, reflection & perception) providing a common vocabulary and rubric\nfor assessing agentic capabilities in security tasks. By applying CLASP to 21\nrepresentative works, we map where systems demonstrate strengths, and where\ncapability gaps persist. We then define the Closed-Loop Capability (CLC) Score,\na composite metric quantifying both degree of loop closure and operational\neffectiveness, and outline the requirements for a closed loop benchmark.\nTogether, CLASP and the CLC Score, provide the vocabulary, diagnostics, and\nmeasurements needed to advance both function level performance and measure\nclosed loop security agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86CLASP\u6846\u67b6\u548cCLC\u8bc4\u5206\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8861\u91cf\u95ed\u73af\u81ea\u4e3b\u5b89\u5168\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u5b89\u5168\u9886\u57df\u5728\u6846\u67b6\u5b9a\u4e49\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5b9a\u4e49\u5b89\u5168\u7cfb\u7edf\u7684\u81ea\u4e3b\u80fd\u529b\uff0c\u4e5f\u6ca1\u6709\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u95ed\u73af\u4ee3\u7406\uff0c\u4ee5\u53ca\u8861\u91cf\u5176\u5b9e\u9645\u6027\u80fd\u7684\u57fa\u51c6\u3002", "method": "\u5f15\u5165CLASP\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u751f\u547d\u5468\u671f\uff08\u4fa6\u5bdf\u3001\u5229\u7528\u3001\u6839\u56e0\u5206\u6790\u3001\u8865\u4e01\u5408\u6210\u3001\u9a8c\u8bc1\uff09\u4e0e\u6838\u5fc3\u81ea\u4e3b\u80fd\u529b\uff08\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8bb0\u5fc6\u3001\u63a8\u7406\u3001\u53cd\u601d\u4e0e\u611f\u77e5\uff09\u5bf9\u9f50\uff1b\u5b9a\u4e49CLC\u8bc4\u5206\u6765\u91cf\u5316\u95ed\u73af\u7a0b\u5ea6\u548c\u64cd\u4f5c\u6709\u6548\u6027\u3002", "result": "\u5e94\u7528CLASP\u5206\u6790\u4e8621\u4e2a\u4ee3\u8868\u6027\u5de5\u4f5c\uff0c\u8bc6\u522b\u4e86\u7cfb\u7edf\u7684\u4f18\u52bf\u9886\u57df\u548c\u6301\u7eed\u5b58\u5728\u7684\u80fd\u529b\u5dee\u8ddd\uff1b\u63d0\u51fa\u4e86\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u7684\u8981\u6c42\u3002", "conclusion": "CLASP\u548cCLC\u8bc4\u5206\u4e3a\u63a8\u8fdb\u529f\u80fd\u7ea7\u6027\u80fd\u548c\u6d4b\u91cf\u95ed\u73af\u5b89\u5168\u4ee3\u7406\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u8bcd\u6c47\u3001\u8bca\u65ad\u5de5\u5177\u548c\u6d4b\u91cf\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.01545", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.01545", "abs": "https://arxiv.org/abs/2510.01545", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Predictive Preference Learning from Human Interventions", "comment": "NeurIPS 2025 Spotlight. Project page:\n  https://metadriverse.github.io/ppl", "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl", "AI": {"tldr": "\u63d0\u51fa\u9884\u6d4b\u6027\u504f\u597d\u5b66\u4e60\uff08PPL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eba\u7c7b\u5e72\u9884\u7684\u9690\u5f0f\u504f\u597d\u4fe1\u53f7\u6765\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5c06\u5355\u6b21\u5e72\u9884\u6269\u5c55\u5230\u672a\u6765L\u4e2a\u65f6\u95f4\u6b65\uff0c\u4ece\u800c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u51cf\u5c11\u4eba\u7c7b\u6f14\u793a\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u53ea\u4fee\u6b63\u5f53\u524d\u72b6\u6001\u7684\u52a8\u4f5c\uff0c\u4e0d\u8c03\u6574\u672a\u6765\u53ef\u80fd\u66f4\u5371\u9669\u72b6\u6001\u7684\u52a8\u4f5c\uff0c\u9700\u8981\u66f4\u6709\u6548\u5229\u7528\u4eba\u7c7b\u5e72\u9884\u6765\u9884\u9632\u672a\u6765\u98ce\u9669\u3002", "method": "PPL\u5c06\u6bcf\u6b21\u4eba\u7c7b\u5e72\u9884\u5f15\u5bfc\u5230\u672a\u6765L\u4e2a\u65f6\u95f4\u6b65\uff08\u504f\u597d\u89c6\u91ce\uff09\uff0c\u5047\u8bbe\u667a\u80fd\u4f53\u6267\u884c\u76f8\u540c\u52a8\u4f5c\u4e14\u4eba\u7c7b\u8fdb\u884c\u76f8\u540c\u5e72\u9884\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5c06\u8fd9\u4e9b\u4e13\u5bb6\u4fee\u6b63\u4f20\u64ad\u5230\u5b89\u5168\u5173\u952e\u533a\u57df\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u901a\u7528\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u4eba\u7c7b\u6f14\u793a\u9700\u6c42\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u504f\u597d\u89c6\u91ceL\u53ef\u4ee5\u5728\u98ce\u9669\u72b6\u6001\u8986\u76d6\u548c\u6807\u7b7e\u6b63\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ece\u800c\u9650\u5236\u7b97\u6cd5\u6700\u4f18\u6027\u5dee\u8ddd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01685", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01685", "abs": "https://arxiv.org/abs/2510.01685", "authors": ["Apoorv Khandelwal", "Ellie Pavlick"], "title": "How Do Language Models Compose Functions?", "comment": null, "summary": "While large language models (LLMs) appear to be increasingly capable of\nsolving compositional tasks, it is an open question whether they do so using\ncompositional mechanisms. In this work, we investigate how feedforward LLMs\nsolve two-hop factual recall tasks, which can be expressed compositionally as\n$g(f(x))$. We first confirm that modern LLMs continue to suffer from the\n\"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y =\ng(z)$ does not entail their ability to compute the composition $y = g(f(x))$.\nThen, using logit lens on their residual stream activations, we identify two\nprocessing mechanisms, one which solves tasks $\\textit{compositionally}$,\ncomputing $f(x)$ along the way to computing $g(f(x))$, and one which solves\nthem $\\textit{directly}$, without any detectable signature of the intermediate\nvariable $f(x)$. Finally, we find that which mechanism is employed appears to\nbe related to the embedding space geometry, with the idiomatic mechanism being\ndominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in\nthe embedding spaces. We fully release our data and code at:\nhttps://github.com/apoorvkh/composing-functions .", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u7ec4\u5408\u4efb\u52a1\u65f6\u5b58\u5728\"\u7ec4\u5408\u6027\u5dee\u8ddd\"\uff0c\u5e76\u8bc6\u522b\u51fa\u4e24\u79cd\u5904\u7406\u673a\u5236\uff1a\u7ec4\u5408\u673a\u5236\u548c\u76f4\u63a5\u673a\u5236\uff0c\u540e\u8005\u4e0e\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u76f8\u5173\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4f7f\u7528\u7ec4\u5408\u673a\u5236\u6765\u89e3\u51b3\u7ec4\u5408\u4efb\u52a1\uff0c\u7279\u522b\u662f\u9a8c\u8bc1\u5b83\u4eec\u662f\u5426\u5b58\u5728\u7ec4\u5408\u6027\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u4f7f\u7528logit lens\u5206\u6790\u6b8b\u5dee\u6d41\u6fc0\u6d3b\uff0c\u7814\u7a76\u4e24\u8df3\u4e8b\u5b9e\u56de\u5fc6\u4efb\u52a1\uff0c\u8bc6\u522b\u7ec4\u5408\u673a\u5236\u548c\u76f4\u63a5\u673a\u5236\u3002", "result": "\u786e\u8ba4LLMs\u5b58\u5728\u7ec4\u5408\u6027\u5dee\u8ddd\uff0c\u53d1\u73b0\u4e24\u79cd\u5904\u7406\u673a\u5236\uff1a\u7ec4\u5408\u673a\u5236\u8ba1\u7b97\u4e2d\u95f4\u53d8\u91cf\uff0c\u76f4\u63a5\u673a\u5236\u65e0\u4e2d\u95f4\u53d8\u91cf\u75d5\u8ff9\uff0c\u540e\u8005\u5728\u5b58\u5728\u7ebf\u6027\u6620\u5c04\u65f6\u5360\u4e3b\u5bfc\u3002", "conclusion": "LLMs\u89e3\u51b3\u7ec4\u5408\u4efb\u52a1\u65f6\u91c7\u7528\u4e0d\u540c\u673a\u5236\uff0c\u7ec4\u5408\u6027\u5dee\u8ddd\u786e\u5b9e\u5b58\u5728\uff0c\u673a\u5236\u9009\u62e9\u4e0e\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u7279\u6027\u76f8\u5173\u3002", "topic": "agent analysis"}}
{"id": "2510.01719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01719", "abs": "https://arxiv.org/abs/2510.01719", "authors": ["Jiwan Chung", "Neel Joshi", "Pratyusha Sharma", "Youngjae Yu", "Vibhav Vineet"], "title": "What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?", "comment": null, "summary": "Multimodal reasoning models have recently shown promise on challenging\ndomains such as olympiad-level geometry, yet their evaluation remains dominated\nby aggregate accuracy, a single score that obscures where and how models are\nimproving. We introduce MathLens, a benchmark designed to disentangle the\nsubskills of multimodal reasoning while preserving the complexity of\ntextbook-style geometry problems. The benchmark separates performance into\nthree components: Perception: extracting information from raw inputs,\nReasoning: operating on available information, and Integration: selecting\nrelevant perceptual evidence and applying it within reasoning. To support each\ntest, we provide annotations: visual diagrams, textual descriptions to evaluate\nreasoning in isolation, controlled questions that require both modalities, and\nprobes for fine-grained perceptual skills, all derived from symbolic\nspecifications of the problems to ensure consistency and robustness. Our\nanalysis reveals that different training approaches have uneven effects: First,\nreinforcement learning chiefly strengthens perception, especially when\nsupported by textual supervision, while textual SFT indirectly improves\nperception through reflective reasoning. Second, reasoning improves only in\ntandem with perception. Third, integration remains the weakest capacity, with\nresidual errors concentrated there once other skills advance. Finally,\nrobustness diverges: RL improves consistency under diagram variation, whereas\nmultimodal SFT reduces it through overfitting. We will release all data and\nexperimental logs.", "AI": {"tldr": "MathLens\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u7528\u4e8e\u5206\u89e3\u51e0\u4f55\u95ee\u9898\u7684\u5b50\u6280\u80fd\uff0c\u5305\u62ec\u611f\u77e5\u3001\u63a8\u7406\u548c\u6574\u5408\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u5404\u9879\u6280\u80fd\u7684\u5dee\u5f02\u5316\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u805a\u5408\u51c6\u786e\u7387\uff0c\u8fd9\u79cd\u5355\u4e00\u5206\u6570\u65e0\u6cd5\u63ed\u793a\u6a21\u578b\u5728\u54ea\u4e9b\u65b9\u9762\u4ee5\u53ca\u5982\u4f55\u6539\u8fdb\u3002\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u5b50\u6280\u80fd\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1MathLens\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u51e0\u4f55\u95ee\u9898\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u63a8\u7406\u548c\u6574\u5408\u4e09\u4e2a\u7ec4\u4ef6\u3002\u63d0\u4f9b\u89c6\u89c9\u56fe\u8868\u3001\u6587\u672c\u63cf\u8ff0\u3001\u63a7\u5236\u95ee\u9898\u548c\u611f\u77e5\u63a2\u9488\u7b49\u6ce8\u91ca\uff0c\u6240\u6709\u5185\u5bb9\u90fd\u6765\u81ea\u95ee\u9898\u7684\u7b26\u53f7\u89c4\u8303\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a1) \u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u589e\u5f3a\u611f\u77e5\u80fd\u529b\uff1b2) \u63a8\u7406\u80fd\u529b\u53ea\u6709\u5728\u611f\u77e5\u80fd\u529b\u63d0\u5347\u65f6\u624d\u4f1a\u6539\u5584\uff1b3) \u6574\u5408\u80fd\u529b\u662f\u6700\u5f31\u7684\u73af\u8282\uff1b4) \u9c81\u68d2\u6027\u8868\u73b0\u5206\u5316\uff1aRL\u63d0\u9ad8\u56fe\u8868\u53d8\u5316\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u591a\u6a21\u6001SFT\u56e0\u8fc7\u62df\u5408\u800c\u964d\u4f4e\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5728\u4e0d\u540c\u5b50\u6280\u80fd\u4e0a\u8868\u73b0\u4e0d\u5747\u8861\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u63d0\u5347\u5404\u9879\u80fd\u529b\uff0c\u7279\u522b\u662f\u6574\u5408\u80fd\u529b\u662f\u6700\u9700\u8981\u6539\u8fdb\u7684\u8584\u5f31\u73af\u8282\u3002", "topic": "agent analysis"}}
{"id": "2510.01571", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.01571", "abs": "https://arxiv.org/abs/2510.01571", "authors": ["Hanqun Cao", "Hongrui Zhang", "Junde Xu", "Zhou Zhang", "Lingdong Shen", "Minghao Sun", "Ge Liu", "Jinbo Xu", "Wu-Jun Li", "Jinren Ni", "Cesar de la Fuente-Nunez", "Tianfan Fu", "Yejin Choi", "Pheng-Ann Heng", "Fang Wu"], "title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?", "comment": "24 pages, 7 figures, 4 tables", "summary": "Protein language models (PLMs) have advanced computational protein science\nthrough large-scale pretraining and scalable architectures. In parallel,\nreinforcement learning (RL) has broadened exploration and enabled precise\nmulti-objective optimization in protein design. Yet whether RL can push PLMs\nbeyond their pretraining priors to uncover latent sequence-structure-function\nrules remains unclear. We address this by pairing RL with PLMs across four\ndomains: antimicrobial peptide design, kinase variant optimization, antibody\nengineering, and inverse folding. Using diverse RL algorithms and model\nclasses, we ask if RL improves sampling efficiency and, more importantly, if it\nreveals capabilities not captured by supervised learning. Across benchmarks, RL\nconsistently boosts success rates and sample efficiency. Performance follows a\nthree-factor interaction: task headroom, reward fidelity, and policy capacity\njointly determine gains. When rewards are accurate and informative, policies\nhave sufficient capacity, and tasks leave room beyond supervised baselines,\nimprovements scale; when rewards are noisy or capacity is constrained, gains\nsaturate despite exploration. This view yields practical guidance for RL in\nprotein design: prioritize reward modeling and calibration before scaling\npolicy size, match algorithm and regularization strength to task difficulty,\nand allocate capacity where marginal gains are largest. Implementation is\navailable at https://github.com/chq1155/RL-PLM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u5426\u63a8\u52a8\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u8d85\u8d8a\u5176\u9884\u8bad\u7ec3\u5148\u9a8c\uff0c\u63ed\u793a\u5e8f\u5217-\u7ed3\u6784-\u529f\u80fd\u89c4\u5219\u3002\u901a\u8fc7\u5728\u56db\u4e2a\u86cb\u767d\u8d28\u8bbe\u8ba1\u9886\u57df\u7ed3\u5408RL\u4e0ePLMs\uff0c\u53d1\u73b0RL\u80fd\u6301\u7eed\u63d0\u9ad8\u6210\u529f\u7387\u548c\u91c7\u6837\u6548\u7387\uff0c\u6027\u80fd\u63d0\u5347\u53d6\u51b3\u4e8e\u4efb\u52a1\u7a7a\u95f4\u3001\u5956\u52b1\u4fdd\u771f\u5ea6\u548c\u7b56\u7565\u5bb9\u91cf\u7684\u4e09\u56e0\u7d20\u4ea4\u4e92\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22RL\u662f\u5426\u80fd\u5e2e\u52a9PLMs\u7a81\u7834\u9884\u8bad\u7ec3\u5148\u9a8c\u7684\u9650\u5236\uff0c\u53d1\u73b0\u76d1\u7763\u5b66\u4e60\u672a\u80fd\u6355\u6349\u7684\u86cb\u767d\u8d28\u5e8f\u5217-\u7ed3\u6784-\u529f\u80fd\u5173\u7cfb\uff0c\u63a8\u52a8\u86cb\u767d\u8d28\u8bbe\u8ba1\u7684\u63a2\u7d22\u548c\u4f18\u5316\u3002", "method": "\u5728\u56db\u4e2a\u86cb\u767d\u8d28\u8bbe\u8ba1\u9886\u57df\uff08\u6297\u83cc\u80bd\u8bbe\u8ba1\u3001\u6fc0\u9176\u53d8\u4f53\u4f18\u5316\u3001\u6297\u4f53\u5de5\u7a0b\u548c\u9006\u6298\u53e0\uff09\u4e2d\uff0c\u5c06\u591a\u79cdRL\u7b97\u6cd5\u4e0e\u4e0d\u540c\u7c7b\u522b\u7684PLMs\u914d\u5bf9\u4f7f\u7528\uff0c\u6bd4\u8f83RL\u4e0e\u76d1\u7763\u5b66\u4e60\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRL\u6301\u7eed\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u91c7\u6837\u6548\u7387\u3002\u6027\u80fd\u63d0\u5347\u9075\u5faa\u4e09\u56e0\u7d20\u4ea4\u4e92\u4f5c\u7528\uff1a\u5f53\u5956\u52b1\u51c6\u786e\u4e14\u4fe1\u606f\u4e30\u5bcc\u3001\u7b56\u7565\u5bb9\u91cf\u5145\u8db3\u3001\u4efb\u52a1\u6709\u8d85\u8d8a\u76d1\u7763\u57fa\u7ebf\u7684\u7a7a\u95f4\u65f6\uff0c\u6539\u8fdb\u663e\u8457\uff1b\u5f53\u5956\u52b1\u566a\u58f0\u5927\u6216\u5bb9\u91cf\u53d7\u9650\u65f6\uff0c\u589e\u76ca\u9971\u548c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\u7684RL\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff1a\u4f18\u5148\u8fdb\u884c\u5956\u52b1\u5efa\u6a21\u548c\u6821\u51c6\uff0c\u518d\u6269\u5c55\u7b56\u7565\u89c4\u6a21\uff1b\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u5339\u914d\u7b97\u6cd5\u548c\u6b63\u5219\u5316\u5f3a\u5ea6\uff1b\u5728\u8fb9\u9645\u6536\u76ca\u6700\u5927\u7684\u5730\u65b9\u5206\u914d\u5bb9\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01581", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01581", "abs": "https://arxiv.org/abs/2510.01581", "authors": ["Joykirat Singh", "Justin Chih-Yao Chen", "Archiki Prasad", "Elias Stengel-Eskin", "Akshay Nambi", "Mohit Bansal"], "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression", "comment": "Code: https://github.com/joykirat18/TRAAC", "summary": "Recent thinking models solve complex reasoning tasks by scaling test-time\ncompute, but this scaling must be allocated in line with task difficulty. On\none hand, short reasoning (underthinking) leads to errors on harder problems\nthat require extended reasoning steps; but, excessively long reasoning\n(overthinking) can be token-inefficient, generating unnecessary steps even\nafter reaching a correct intermediate solution. We refer to this as\nunder-adaptivity, where the model fails to modulate its response length\nappropriately given problems of varying difficulty. To address under-adaptivity\nand strike a balance between under- and overthinking, we propose TRAAC (Think\nRight with Adaptive, Attentive Compression), an online post-training RL method\nthat leverages the model's self-attention over a long reasoning trajectory to\nidentify important steps and prune redundant ones. TRAAC also estimates\ndifficulty and incorporates it into training rewards, thereby learning to\nallocate reasoning budget commensurate with example difficulty. Our approach\nimproves accuracy, reduces reasoning steps, and enables adaptive thinking\ncompared to base models and other RL baselines. Across a variety of tasks\n(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute\naccuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%\ncompared to the base model, and a 7.9% accuracy gain paired with a 29.4% length\ndrop compared to the best RL baseline. TRAAC also shows strong generalization:\nalthough our models are trained on math datasets, they show accuracy and\nefficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,\nand OptimalThinkingBench. Our analysis further verifies that TRAAC provides\nfine-grained adjustments to thinking budget based on difficulty and that a\ncombination of task-difficulty calibration and attention-based compression\nyields gains across diverse tasks.", "AI": {"tldr": "TRAAC\u662f\u4e00\u79cd\u5728\u7ebf\u540e\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u63a8\u7406\u6b65\u9aa4\u6765\u89e3\u51b3\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6b20\u9002\u5e94\u6027\u95ee\u9898\uff0c\u5e73\u8861\u6b20\u601d\u8003\u548c\u8fc7\u5ea6\u601d\u8003\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u6a21\u578b\u901a\u8fc7\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6765\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u6839\u636e\u4efb\u52a1\u96be\u5ea6\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002\u6b20\u601d\u8003\u4f1a\u5bfc\u81f4\u56f0\u96be\u95ee\u9898\u51fa\u9519\uff0c\u800c\u8fc7\u5ea6\u601d\u8003\u5219\u4f1a\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u9020\u6210\u4ee4\u724c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faTRAAC\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u5728\u957f\u63a8\u7406\u8f68\u8ff9\u4e0a\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u91cd\u8981\u6b65\u9aa4\u5e76\u526a\u679d\u5197\u4f59\u6b65\u9aa4\uff0c\u540c\u65f6\u4f30\u8ba1\u4efb\u52a1\u96be\u5ea6\u5e76\u5c06\u5176\u7eb3\u5165\u8bad\u7ec3\u5956\u52b1\uff0c\u5b66\u4e60\u6839\u636e\u793a\u4f8b\u96be\u5ea6\u5206\u914d\u63a8\u7406\u9884\u7b97\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\uff08AIME\u3001AMC\u3001GPQA-D\u3001BBEH\uff09\uff0cTRAAC\uff08Qwen3-4B\uff09\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5e73\u5747\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u53478.4%\uff0c\u63a8\u7406\u957f\u5ea6\u76f8\u5bf9\u51cf\u5c1136.8%\uff1b\u76f8\u6bd4\u6700\u4f73RL\u57fa\u7ebf\u51c6\u786e\u7387\u63d0\u53477.9%\uff0c\u957f\u5ea6\u51cf\u5c1129.4%\u3002", "conclusion": "TRAAC\u80fd\u591f\u6839\u636e\u96be\u5ea6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u601d\u7ef4\u9884\u7b97\u8c03\u6574\uff0c\u4efb\u52a1\u96be\u5ea6\u6821\u51c6\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u538b\u7f29\u76f8\u7ed3\u5408\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u90fd\u80fd\u5e26\u6765\u6536\u76ca\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01624", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01624", "abs": "https://arxiv.org/abs/2510.01624", "authors": ["Feiyang Kang", "Michael Kuchnik", "Karthik Padthe", "Marin Vlastelica", "Ruoxi Jia", "Carole-Jean Wu", "Newsha Ardalani"], "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead", "comment": "Preprint. Under Review", "summary": "In post-training for reasoning Large Language Models (LLMs), the current\nstate of practice trains LLMs in two independent stages: Supervised Fine-Tuning\n(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as\n``RL'' below). In this work, we challenge whether high SFT scores translate to\nimproved performance after RL. We provide extensive counter-examples where this\nis not true. We find high SFT scores can be biased toward simpler or more\nhomogeneous data and are not reliably predictive of subsequent RL gains or\nscaled-up post-training effectiveness. In some cases, RL training on models\nwith improved SFT performance could lead to substantially worse outcome\ncompared to RL on the base model without SFT. We study alternative metrics and\nidentify generalization loss on held-out reasoning examples and Pass@large k\nperformance to provide strong proxies for the RL outcome. We trained hundreds\nof models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive\nevaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU\nhours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple\nstate-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL\nperformance, prediction based on generalization loss and Pass@large k achieves\nsubstantial higher precision, improving $R^2$ coefficient and Spearman's rank\ncorrelation coefficient by up to 0.5 (2x). This provides strong utility for\nbroad use cases. For example, in most experiments, we find SFT training on\nunique examples for a one epoch underperforms training on half examples for two\nepochs, either after SFT or SFT-then-RL; With the same SFT budget, training\nonly on short examples may lead to better SFT performance, though, it often\nleads to worse outcome after RL compared to training on examples with varying\nlengths. Evaluation tool will be open-sourced.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0SFT\u9ad8\u5206\u5e76\u4e0d\u603b\u80fd\u8f6c\u5316\u4e3aRL\u540e\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6709\u65f6\u751a\u81f3\u4f1a\u5bfc\u81f4\u66f4\u5dee\u7ed3\u679c\u3002\u63d0\u51fa\u4e86\u6cdb\u5316\u635f\u5931\u548cPass@large k\u4f5c\u4e3a\u66f4\u597d\u7684RL\u7ed3\u679c\u9884\u6d4b\u6307\u6807\u3002", "motivation": "\u6311\u6218\u5f53\u524dLLM\u540e\u8bad\u7ec3\u4e2dSFT\u548cRL\u4e24\u9636\u6bb5\u72ec\u7acb\u8bad\u7ec3\u7684\u505a\u6cd5\uff0c\u8d28\u7591\u9ad8SFT\u5206\u6570\u662f\u5426\u80fd\u53ef\u9760\u9884\u6d4bRL\u540e\u7684\u6027\u80fd\u63d0\u5347\u3002", "method": "\u8bad\u7ec3\u6570\u767e\u4e2a12B\u53c2\u6570\u6a21\u578b\uff0c\u57287\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f7f\u7528GRPO\u8fdb\u884cRLVR\u8bad\u7ec3\uff0c\u82b1\u8d39\u8d85\u8fc7100\u4e07GPU\u5c0f\u65f6\u3002", "result": "\u53d1\u73b0\u57fa\u4e8e\u6cdb\u5316\u635f\u5931\u548cPass@large k\u7684\u9884\u6d4b\u6bd4\u76f4\u63a5\u4f7f\u7528SFT\u6027\u80fd\u9884\u6d4b\u66f4\u51c6\u786e\uff0cR\u00b2\u548cSpearman\u76f8\u5173\u7cfb\u6570\u63d0\u5347\u9ad8\u8fbe0.5\uff082\u500d\uff09\u3002", "conclusion": "SFT\u6027\u80fd\u4e0d\u662fRL\u7ed3\u679c\u7684\u53ef\u9760\u9884\u6d4b\u6307\u6807\uff0c\u6cdb\u5316\u635f\u5931\u548cPass@large k\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01925", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01925", "abs": "https://arxiv.org/abs/2510.01925", "authors": ["Qiyuan Liu", "Hao Xu", "Xuhong Chen", "Wei Chen", "Yee Whye Teh", "Ning Miao"], "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey", "comment": null, "summary": "Reward models (RMs) play a critical role in enhancing the reasoning\nperformance of LLMs. For example, they can provide training signals to finetune\nLLMs during reinforcement learning (RL) and help select the best answer from\nmultiple candidates during inference. In this paper, we provide a systematic\nintroduction to RMs, along with a comprehensive survey of their applications in\nLLM reasoning. We first review fundamental concepts of RMs, including their\narchitectures, training methodologies, and evaluation techniques. Then, we\nexplore their key applications: (1) guiding generation and selecting optimal\noutputs during LLM inference, (2) facilitating data synthesis and iterative\nself-improvement for LLMs, and (3) providing training signals in RL-based\nfinetuning. Finally, we address critical open questions regarding the\nselection, generalization, evaluation, and enhancement of RMs, based on\nexisting research and our own empirical findings. Our analysis aims to provide\nactionable insights for the effective deployment and advancement of RMs for LLM\nreasoning.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u4ecb\u7ecd\u4e86\u5956\u52b1\u6a21\u578b\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5305\u62ec\u5176\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u5e94\u7528\u573a\u666f\uff0c\u5e76\u63a2\u8ba8\u4e86\u76f8\u5173\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5728\u589e\u5f3aLLM\u63a8\u7406\u6027\u80fd\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u80fd\u591f\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u8bad\u7ec3\u4fe1\u53f7\u5e76\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6700\u4f73\u7b54\u6848\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u7814\u7a76\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u56de\u987e\u5956\u52b1\u6a21\u578b\u7684\u57fa\u672c\u6982\u5ff5\u3001\u67b6\u6784\u3001\u8bad\u7ec3\u65b9\u6cd5\u548c\u8bc4\u4f30\u6280\u672f\uff0c\u5e76\u5206\u6790\u5176\u5728LLM\u63a8\u7406\u4e2d\u7684\u4e09\u5927\u4e3b\u8981\u5e94\u7528\u3002", "result": "\u660e\u786e\u4e86\u5956\u52b1\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u7684\u6838\u5fc3\u5e94\u7528\uff1a\u6307\u5bfc\u751f\u6210\u548c\u8f93\u51fa\u9009\u62e9\u3001\u4fc3\u8fdb\u6570\u636e\u5408\u6210\u548c\u81ea\u6539\u8fdb\u3001\u63d0\u4f9bRL\u5fae\u8c03\u8bad\u7ec3\u4fe1\u53f7\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "\u5956\u52b1\u6a21\u578b\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u5de5\u5177\uff0c\u672c\u6587\u4e3a\u6709\u6548\u90e8\u7f72\u548c\u63a8\u8fdb\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.01932", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.01932", "abs": "https://arxiv.org/abs/2510.01932", "authors": ["Qi He", "Cheng Qian", "Xiusi Chen", "Bingxiang He", "Yi R.", "Fung", "Heng Ji"], "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning", "comment": null, "summary": "Claim verification with large language models (LLMs) has recently attracted\nconsiderable attention, owing to their superior reasoning capabilities and\ntransparent verification pathways compared to traditional answer-only\njudgments. Online claim verification requires iterative evidence retrieval and\nreasoning, yet existing approaches mainly rely on prompt engineering or\npredesigned reasoning workflows without offering a unified training paradigm to\nimprove necessary skills. Therefore, we introduce Veri-R1, an online\nreinforcement learning (RL) framework that enables an LLM to interact with a\nsearch engine and to receive reward signals that explicitly shape its planning,\nretrieval, and reasoning behaviors. The dynamic interaction between models and\nretrieval systems more accurately reflects real-world verification scenarios\nand fosters comprehensive verification skills. Empirical results show that\nVeri-R1 improves joint accuracy by up to 30% and doubles evidence score, often\nsurpassing larger-scale counterparts. Ablation studies further reveal the\nimpact of reward components and the link between output logits and label\naccuracy. Our results highlight the effectiveness of online RL for precise and\nfaithful claim verification and provide a foundation for future research. We\nrelease our code to support community progress in LLM empowered claim\nverification.", "AI": {"tldr": "\u63d0\u51fa\u4e86Veri-R1\uff0c\u4e00\u4e2a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9LLM\u4e0e\u641c\u7d22\u5f15\u64ce\u4ea4\u4e92\u5e76\u63a5\u6536\u5956\u52b1\u4fe1\u53f7\u6765\u6539\u8fdb\u58f0\u660e\u9a8c\u8bc1\u4e2d\u7684\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u6216\u9884\u8bbe\u63a8\u7406\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u5347\u5fc5\u8981\u6280\u80fd\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u573a\u666f\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8ba9LLM\u4e0e\u641c\u7d22\u5f15\u64ce\u52a8\u6001\u4ea4\u4e92\uff0c\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u660e\u786e\u5851\u9020\u5176\u89c4\u5212\u3001\u68c0\u7d22\u548c\u63a8\u7406\u884c\u4e3a\u3002", "result": "Veri-R1\u5c06\u8054\u5408\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u8bc1\u636e\u5f97\u5206\u7ffb\u500d\uff0c\u7ecf\u5e38\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u5bf9\u5e94\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u5956\u52b1\u7ec4\u4ef6\u7684\u5f71\u54cd\u4ee5\u53ca\u8f93\u51falogits\u4e0e\u6807\u7b7e\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u7cbe\u786e\u548c\u53ef\u4fe1\u7684\u58f0\u660e\u9a8c\u8bc1\u975e\u5e38\u6709\u6548\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.01656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01656", "abs": "https://arxiv.org/abs/2510.01656", "authors": ["Jiashun Liu", "Johan Obando-Ceron", "Han Lu", "Yancheng He", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Pablo Samuel Castro", "Aaron Courville", "Ling Pan"], "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning", "comment": null, "summary": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing\nthem with average advantage baselines. This shift is largely pragmatic:\nconventional value functions are computationally expensive to train at LLM\nscale and often fail under sparse rewards and long reasoning horizons. We\nrevisit this bottleneck from an architectural perspective and introduce\nAsymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable\nframework that restores the critics role while remaining efficient in\nlarge-model settings. AsyPPO employs a set of lightweight mini-critics, each\ntrained on disjoint prompt shards. This design encourages diversity while\npreserving calibration, reducing value-estimation bias. Beyond robust\nestimation, AsyPPO leverages inter-critic uncertainty to refine the policy\nupdate: (i) masking advantages in states where critics agree and gradients add\nlittle learning signal, and (ii) filtering high-divergence states from entropy\nregularization, suppressing spurious exploration. After training on open-source\ndata with only 5,000 samples, AsyPPO consistently improves learning stability\nand performance across multiple benchmarks over strong baselines, such as GRPO,\nachieving performance gains of more than six percent on Qwen3-4b-Base and about\nthree percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without\nadditional tricks. These results highlight the importance of architectural\ninnovations for scalable, efficient algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86AsyPPO\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7mini-critics\u96c6\u5408\u6062\u590dcritic\u89d2\u8272\uff0c\u89e3\u51b3LLM\u89c4\u6a21RL\u4e2d\u4f20\u7edf\u4ef7\u503c\u51fd\u6570\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dRL4LLM\u65b9\u6cd5\u907f\u514d\u4f7f\u7528\u663e\u5f0fcritic\uff0c\u4e3b\u8981\u56e0\u4e3a\u4f20\u7edf\u4ef7\u503c\u51fd\u6570\u5728LLM\u89c4\u6a21\u4e0b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5728\u7a00\u758f\u5956\u52b1\u548c\u957f\u63a8\u7406\u89c6\u91ce\u4e0b\u5bb9\u6613\u5931\u6548\u3002", "method": "\u4f7f\u7528\u4e00\u7ec4\u8f7b\u91cf\u7ea7mini-critics\uff0c\u6bcf\u4e2a\u5728\u4e0d\u76f8\u4ea4\u7684\u63d0\u793a\u5206\u7247\u4e0a\u8bad\u7ec3\uff0c\u9f13\u52b1\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u6821\u51c6\uff1b\u5229\u7528critic\u95f4\u4e0d\u786e\u5b9a\u6027\u6765\u4f18\u5316\u7b56\u7565\u66f4\u65b0\uff0c\u5305\u62ec\u5728critic\u4e00\u81f4\u65f6\u5c4f\u853d\u4f18\u52bf\u503c\uff0c\u4ee5\u53ca\u4ece\u71b5\u6b63\u5219\u5316\u4e2d\u8fc7\u6ee4\u9ad8\u5206\u6b67\u72b6\u6001\u3002", "result": "\u5728\u4ec55000\u4e2a\u6837\u672c\u4e0a\u8bad\u7ec3\u540e\uff0cAsyPPO\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u6301\u7eed\u63d0\u5347\u5b66\u4e60\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u76f8\u6bd4GRPO\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728Qwen3-4b-Base\u4e0a\u83b7\u5f97\u8d85\u8fc76%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728Qwen3-8b-Base\u548cQwen3-14b-Base\u4e0a\u83b7\u5f97\u7ea63%\u7684\u63d0\u5347\u3002", "conclusion": "\u67b6\u6784\u521b\u65b0\u5bf9\u4e8e\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7b97\u6cd5\u7684\u91cd\u8981\u6027\uff0cAsyPPO\u6210\u529f\u6062\u590d\u4e86critic\u5728RL4LLM\u4e2d\u7684\u4f5c\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02044", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.02044", "abs": "https://arxiv.org/abs/2510.02044", "authors": ["Siddhant Arora", "Haidar Khan", "Kai Sun", "Xin Luna Dong", "Sajal Choudhary", "Seungwhan Moon", "Xinyuan Zhang", "Adithya Sagar", "Surya Teja Appini", "Kaushik Patnaik", "Sanat Sharma", "Shinji Watanabe", "Anuj Kumar", "Ahmed Aly", "Yue Liu", "Florian Metze", "Zhaojiang Lin"], "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage", "comment": null, "summary": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86Streaming RAG\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7528\u6237\u8bf4\u8bdd\u65f6\u5e76\u884c\u9884\u6d4b\u5de5\u5177\u67e5\u8be2\u6765\u964d\u4f4e\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u95ee\u7b54\u51c6\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u867d\u7136\u80fd\u751f\u6210\u66f4\u81ea\u7136\u3001\u4f4e\u5ef6\u8fdf\u7684\u54cd\u5e94\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\u3002\u6587\u672c\u5bf9\u8bdd\u7cfb\u7edf\u901a\u8fc7\u96c6\u6210\u5de5\u5177\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u5de5\u5177\u96c6\u6210\u4f1a\u589e\u52a0\u5ef6\u8fdf\u7834\u574f\u5bf9\u8bdd\u6d41\u7545\u6027\u3002", "method": "\u5f00\u53d1\u4e86Streaming RAG\u6846\u67b6\uff0c\u5305\u62ec\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6559\u5bfc\u6a21\u578b\u5728\u7528\u6237\u8bf4\u8bdd\u8fc7\u7a0b\u4e2d\u4f55\u65f6\u8c03\u7528\u5de5\u5177\uff0c\u4ee5\u53ca\u5982\u4f55\u751f\u6210\u878d\u5408\u97f3\u9891\u67e5\u8be2\u548c\u68c0\u7d22\u6587\u672c\u7ed3\u679c\u7684\u8bed\u97f3\u6458\u8981\u3002", "result": "\u5728AudioCRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQA\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u5347200%\uff08\u4ece11.1%\u63d0\u5347\u81f334.2%\uff09\uff0c\u5de5\u5177\u4f7f\u7528\u5ef6\u8fdf\u964d\u4f4e20%\u3002", "conclusion": "Streaming RAG\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u54cd\u5e94\u901f\u5ea6\uff0c\u4e14\u4e0e\u6a21\u6001\u65e0\u5173\uff0c\u53ef\u5e94\u7528\u4e8e\u66f4\u667a\u80fd\u7684\u5b9e\u65f6AI\u52a9\u624b\u3002", "topic": "agent analysis"}}
{"id": "2510.02172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02172", "abs": "https://arxiv.org/abs/2510.02172", "authors": ["Zhaoning Yu", "Will Su", "Leitian Tao", "Haozhu Wang", "Aashu Singh", "Hanchao Yu", "Jianyu Wang", "Hongyang Gao", "Weizhe Yuan", "Jason Weston", "Ping Yu", "Jing Xu"], "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization", "comment": null, "summary": "Reinforcement learning with human-annotated data has boosted chain-of-thought\nreasoning in large reasoning models, but these gains come at high costs in\nlabeled data while faltering on harder tasks. A natural next step is\nexperience-driven learning, where models improve without curated labels by\nadapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with\nSelf-restraint), a self-penalizing RL framework that converts the absence of\ngold labels into a useful learning signal. Instead of overcommitting to\nspurious majority votes, RESTRAIN exploits signals from the model's entire\nanswer distribution: penalizing overconfident rollouts and low-consistency\nexamples while preserving promising reasoning chains. The self-penalization\nmechanism integrates seamlessly into policy optimization methods such as GRPO,\nenabling continual self-improvement without supervision. On challenging\nreasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.\nWith Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to\n+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on\nGPQA-Diamond, nearly matching gold-label training while using no gold labels.\nThese results demonstrate that RESTRAIN establishes a scalable path toward\nstronger reasoning without gold labels.", "AI": {"tldr": "RESTRAIN\u662f\u4e00\u4e2a\u81ea\u60e9\u7f5a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u6574\u4e2a\u7b54\u6848\u5206\u5e03\u4e2d\u7684\u4fe1\u53f7\uff0c\u5728\u6ca1\u6709\u9ec4\u91d1\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u63a8\u7406\u80fd\u529b\u7684\u6301\u7eed\u81ea\u6211\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u7ecf\u9a8c\u9a71\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u60e9\u7f5aRL\u6846\u67b6\uff0c\u60e9\u7f5a\u8fc7\u5ea6\u81ea\u4fe1\u7684rollout\u548c\u4f4e\u4e00\u81f4\u6027\u793a\u4f8b\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u5e0c\u671b\u7684\u63a8\u7406\u94fe\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230GRPO\u7b49\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u4e2d\u3002", "result": "\u5728AIME25\u4e0aPass@1\u63d0\u5347+140.7%\uff0cMMLU_STEM\u63d0\u5347+36.2%\uff0cGPQA-Diamond\u63d0\u5347+19.6%\uff0c\u51e0\u4e4e\u8fbe\u5230\u9ec4\u91d1\u6807\u7b7e\u8bad\u7ec3\u7684\u6548\u679c\u4f46\u65e0\u9700\u4efb\u4f55\u9ec4\u91d1\u6807\u7b7e\u3002", "conclusion": "RESTRAIN\u4e3a\u65e0\u9700\u9ec4\u91d1\u6807\u7b7e\u5b9e\u73b0\u66f4\u5f3a\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02200", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02200", "abs": "https://arxiv.org/abs/2510.02200", "authors": ["Felix Brei", "Lorenz B\u00fchmann", "Johannes Frey", "Daniel Gerber", "Lars-Peter Meyer", "Claus Stadler", "Kirill Bulert"], "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities", "comment": "peer reviewed publication at Text2SPARQL Workshop @ ESWC 2025", "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSPINACH\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u4f7f\u7528LLM\u4ee3\u7406\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3aSPARQL\u67e5\u8be2\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a2\u7d22\u548c\u6267\u884c\u8fc7\u7a0b\u800c\u975e\u5355\u6b21\u8f6c\u6362\u3002", "motivation": "\u964d\u4f4e\u77e5\u8bc6\u56fe\u8c31\u4ea4\u4e92\u7684\u95e8\u69db\uff0c\u4f7f\u975e\u8ba1\u7b97\u673a\u80cc\u666f\u7684\u7528\u6237\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u77e5\u8bc6\u56fe\u8c31\uff0c\u89e3\u51b3SPARQL\u67e5\u8be2\u8bed\u8a00\u7684\u9ad8\u5b66\u4e60\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528SPINACH LLM\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a2\u7d22\u548c\u6267\u884c\u8fc7\u7a0b\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3aSPARQL\u67e5\u8be2\uff0c\u800c\u975e\u5355\u6b21\u8f6c\u6362\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3aSPARQL\u67e5\u8be2\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u884c\u4e3a\u5206\u6790\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u65b9\u6cd5\u5728Text2SPARQL\u8f6c\u6362\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u964d\u4f4e\u77e5\u8bc6\u56fe\u8c31\u67e5\u8be2\u95e8\u69db\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.01721", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.01721", "abs": "https://arxiv.org/abs/2510.01721", "authors": ["Saptarshi Mandal", "Yashaswini Murthy", "R. Srikant"], "title": "Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation", "comment": "Preprint. 32 Pages", "summary": "Distributionally robust reinforcement learning (DRRL) focuses on designing\npolicies that achieve good performance under model uncertainties. In\nparticular, we are interested in maximizing the worst-case long-term discounted\nreward, where the data for RL comes from a nominal model while the deployed\nenvironment can deviate from the nominal model within a prescribed uncertainty\nset. Existing convergence guarantees for robust temporal-difference (TD)\nlearning for policy evaluation are limited to tabular MDPs or are dependent on\nrestrictive discount-factor assumptions when function approximation is used. We\npresent the first robust TD learning with linear function approximation, where\nrobustness is measured with respect to the total-variation distance and\nWasserstein-l distance uncertainty set. Additionally, our algorithm is both\nmodel-free and does not require generative access to the MDP. Our algorithm\ncombines a two-time-scale stochastic-approximation update with an outer-loop\ntarget-network update. We establish an $\\tilde{O}(1/\\epsilon^2)$ sample\ncomplexity to obtain an $\\epsilon$-accurate value estimate. Our results close a\nkey gap between the empirical success of robust RL algorithms and the\nnon-asymptotic guarantees enjoyed by their non-robust counterparts. The key\nideas in the paper also extend in a relatively straightforward fashion to\nrobust Q-learning with function approximation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5177\u6709\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u7684\u9c81\u68d2TD\u5b66\u4e60\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u65e0\u9700\u751f\u6210\u6a21\u578b\u8bbf\u95ee\u4e14\u6a21\u578b\u65e0\u5173\u3002", "motivation": "\u73b0\u6709\u7684\u9c81\u68d2TD\u5b66\u4e60\u6536\u655b\u4fdd\u8bc1\u4ec5\u9650\u4e8e\u8868\u683cMDP\u6216\u4f9d\u8d56\u4e8e\u9650\u5236\u6027\u6298\u6263\u56e0\u5b50\u5047\u8bbe\uff0c\u9700\u8981\u586b\u8865\u9c81\u68d2RL\u7b97\u6cd5\u5b9e\u8bc1\u6210\u529f\u4e0e\u975e\u9c81\u68d2\u5bf9\u5e94\u7269\u975e\u6e10\u8fd1\u4fdd\u8bc1\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u7ed3\u5408\u53cc\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u903c\u8fd1\u66f4\u65b0\u548c\u5916\u5faa\u73af\u76ee\u6807\u7f51\u7edc\u66f4\u65b0\uff0c\u4f7f\u7528\u603b\u53d8\u5dee\u8ddd\u79bb\u548cWasserstein-l\u8ddd\u79bb\u4e0d\u786e\u5b9a\u6027\u96c6\u6765\u8861\u91cf\u9c81\u68d2\u6027\u3002", "result": "\u5efa\u7acb\u4e86O\u0303(1/\u03b5\u00b2)\u7684\u6837\u672c\u590d\u6742\u5ea6\u4ee5\u83b7\u5f97\u03b5-\u51c6\u786e\u7684\u503c\u4f30\u8ba1\uff0c\u5173\u952e\u601d\u60f3\u4e5f\u53ef\u76f8\u5bf9\u76f4\u63a5\u5730\u6269\u5c55\u5230\u5177\u6709\u51fd\u6570\u903c\u8fd1\u7684\u9c81\u68d2Q\u5b66\u4e60\u3002", "conclusion": "\u586b\u8865\u4e86\u9c81\u68d2RL\u7b97\u6cd5\u5b9e\u8bc1\u6210\u529f\u4e0e\u975e\u9c81\u68d2\u5bf9\u5e94\u7269\u7406\u8bba\u4fdd\u8bc1\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u4e3a\u5206\u5e03\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51fd\u6570\u903c\u8fd1\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02227", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02227", "abs": "https://arxiv.org/abs/2510.02227", "authors": ["Xiaoyang Yuan", "Yujuan Ding", "Yi Bin", "Wenqi Shao", "Jinyu Cai", "Jingkuan Song", "Yang Yang", "Hengtao Shen"], "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration", "comment": "20 pages, 5 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.", "AI": {"tldr": "AMPO\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u6559\u5e08\u6307\u5bfc\u7b56\u7565\u4f18\u5316\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u9700\u8981\u65f6\u63d0\u4f9b\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u548c\u5206\u5e03\u5916\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u6211\u63a2\u7d22\u6216\u5355\u4e00\u6559\u5e08\u6a21\u578b\uff0c\u53ef\u80fd\u5f15\u5165\u6a21\u578b\u504f\u89c1\u5e76\u9650\u5236\u63a2\u7d22\uff0c\u4ece\u800c\u5f71\u54cd\u63a8\u7406\u591a\u6837\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u591a\u6307\u5bfc\u7b56\u7565\u4f18\u5316(AMPO)\uff0c\u5728\u7b56\u7565\u6a21\u578b\u65e0\u6cd5\u751f\u6210\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u65f6\u81ea\u9002\u5e94\u5730\u5229\u7528\u591a\u4e2a\u719f\u7ec3\u6559\u5e08\u6a21\u578b\u7684\u6307\u5bfc\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7406\u89e3\u7684\u9009\u62e9\u673a\u5236\u8ba9\u5b66\u751f\u4ece\u6700\u53ef\u80fd\u7406\u89e3\u7684\u63a8\u7406\u8def\u5f84\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u6bd4\u5f3a\u57fa\u7ebf(GRPO)\u63d0\u53474.3%\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u63d0\u534712.2%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Pass@k\u6027\u80fd\u5e76\u5b9e\u73b0\u4e86\u66f4\u591a\u6837\u5316\u7684\u63a2\u7d22\u3002", "conclusion": "AMPO\u5c55\u793a\u4e86\u5b9e\u73b0\u5353\u8d8a\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u7684\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u4f7f\u7528\u56db\u4e2a\u540c\u7b49\u89c4\u6a21\u6559\u5e08\u5373\u53ef\u8fbe\u5230\u4e0e\u4f7f\u7528\u5355\u4e2a\u66f4\u5f3a\u5927\u6559\u5e08\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02271", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02271", "abs": "https://arxiv.org/abs/2510.02271", "authors": ["Yaxin Du", "Yuanshuo Zhang", "Xiyuan Yang", "Yifan Zhou", "Cheng Wang", "Gongyi Zou", "Xianghe Pang", "Wenhao Wang", "Menglan Chen", "Shuo Tang", "Zhiyu Li", "Siheng Chen"], "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents", "comment": null, "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.", "AI": {"tldr": "\u63d0\u51fa\u4e86InfoMosaic-Bench\uff0c\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u5de5\u5177\u589e\u5f3a\u667a\u80fd\u4f53\u591a\u6e90\u4fe1\u606f\u5bfb\u6c42\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u516d\u4e2a\u4ee3\u8868\u6027\u9886\u57df\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u7ed3\u5408\u901a\u7528\u641c\u7d22\u4e0e\u9886\u57df\u7279\u5b9a\u5de5\u5177\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u8fc7\u5ea6\u4f9d\u8d56\u5f00\u653e\u7f51\u7edc\u641c\u7d22\uff0c\u5b58\u5728\u5185\u5bb9\u5608\u6742\u4e0d\u53ef\u9760\u3001\u7f3a\u4e4f\u7cbe\u786e\u9886\u57df\u77e5\u8bc6\u7684\u95ee\u9898\u3002\u867d\u7136MCP\u534f\u8bae\u8ba9\u667a\u80fd\u4f53\u80fd\u8bbf\u95ee\u6570\u5343\u79cd\u4e13\u4e1a\u5de5\u5177\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u667a\u80fd\u4f53\u662f\u5426\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u5de5\u5177\u5e76\u4e0e\u901a\u7528\u641c\u7d22\u7ed3\u5408\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "method": "\u4f7f\u7528InfoMosaic-Flow\u53ef\u6269\u5c55\u6d41\u6c34\u7ebf\u5408\u6210\u4efb\u52a1\uff0c\u5c06\u4efb\u52a1\u6761\u4ef6\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u7684\u5de5\u5177\u8f93\u51fa\uff0c\u5f3a\u5236\u6267\u884c\u8de8\u6e90\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u8fc7\u6ee4\u6389\u53ef\u901a\u8fc7\u7b80\u5355\u67e5\u627e\u89e3\u51b3\u7684\u6377\u5f84\u6848\u4f8b\u3002", "result": "\u5bf914\u4e2a\u6700\u5148\u8fdbLLM\u667a\u80fd\u4f53\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u4ec5\u9760\u7f51\u7edc\u4fe1\u606f\u4e0d\u8db3\uff08GPT-5\u51c6\u786e\u738738.2%\uff0c\u901a\u8fc7\u738767.5%\uff09\uff1b\u9886\u57df\u5de5\u5177\u63d0\u4f9b\u9009\u62e9\u6027\u4f46\u4e0d\u4e00\u81f4\u7684\u76ca\u5904\uff1b22.4%\u7684\u5931\u8d25\u6e90\u4e8e\u5de5\u5177\u4f7f\u7528\u6216\u9009\u62e9\u9519\u8bef\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u57fa\u672c\u5de5\u5177\u5904\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u597d\u7684\u5de5\u5177\u96c6\u6210\u80fd\u529b\u6765\u89e3\u51b3\u590d\u6742\u4fe1\u606f\u5bfb\u6c42\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2510.01764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.01764", "abs": "https://arxiv.org/abs/2510.01764", "authors": ["Waris Radji", "Thomas Michel", "Hector Piteau"], "title": "Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX", "comment": null, "summary": "Reinforcement learning (RL) research requires diverse, challenging\nenvironments that are both tractable and scalable. While modern video games may\noffer rich dynamics, they are computationally expensive and poorly suited for\nlarge-scale experimentation due to their CPU-bound execution. We introduce\nOctax, a high-performance suite of classic arcade game environments implemented\nin JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely\nadopted as a benchmark in RL research. Octax provides the JAX community with a\nlong-awaited end-to-end GPU alternative to the Atari benchmark, offering\nimage-based environments, spanning puzzle, action, and strategy genres, all\nexecutable at massive scale on modern GPUs. Our JAX-based implementation\nachieves orders-of-magnitude speedups over traditional CPU emulators while\nmaintaining perfect fidelity to the original game mechanics. We demonstrate\nOctax's capabilities by training RL agents across multiple games, showing\nsignificant improvements in training speed and scalability compared to existing\nsolutions. The environment's modular design enables researchers to easily\nextend the suite with new games or generate novel environments using large\nlanguage models, making it an ideal platform for large-scale RL\nexperimentation.", "AI": {"tldr": "Octax\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u9ad8\u6027\u80fd\u7ecf\u5178\u8857\u673a\u6e38\u620f\u73af\u5883\u5957\u4ef6\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9bGPU\u52a0\u901f\u7684Atari\u57fa\u51c6\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u9700\u8981\u591a\u6837\u5316\u3001\u5177\u6709\u6311\u6218\u6027\u4e14\u53ef\u6269\u5c55\u7684\u73af\u5883\u3002\u73b0\u6709\u89c6\u9891\u6e38\u620f\u73af\u5883\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53d7CPU\u9650\u5236\uff0c\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002", "method": "\u57fa\u4e8eCHIP-8\u6a21\u62df\u5668\u5728JAX\u4e2d\u5b9e\u73b0\u7ecf\u5178\u8857\u673a\u6e38\u620f\u73af\u5883\uff0c\u63d0\u4f9b\u7aef\u5230\u7aef\u7684GPU\u52a0\u901f\uff0c\u652f\u6301\u56fe\u50cf\u73af\u5883\u5e76\u6db5\u76d6\u89e3\u8c1c\u3001\u52a8\u4f5c\u548c\u7b56\u7565\u7b49\u591a\u79cd\u6e38\u620f\u7c7b\u578b\u3002", "result": "JAX\u5b9e\u73b0\u76f8\u6bd4\u4f20\u7edfCPU\u6a21\u62df\u5668\u5b9e\u73b0\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u539f\u59cb\u6e38\u620f\u673a\u5236\u7684\u5b8c\u7f8e\u4fdd\u771f\u5ea6\u3002\u5728\u591a\u4e2a\u6e38\u620f\u4e2d\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u663e\u793a\u8bad\u7ec3\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u7684\u663e\u8457\u6539\u5584\u3002", "conclusion": "Octax\u4e3a\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u7406\u60f3\u7684\u5e73\u53f0\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u4fbf\u4e8e\u7814\u7a76\u4eba\u5458\u6269\u5c55\u65b0\u6e38\u620f\u6216\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u65b0\u73af\u5883\u3002", "topic": "swe benchmark"}}
{"id": "2510.01982", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.01982", "abs": "https://arxiv.org/abs/2510.01982", "authors": ["Yujie Zhou", "Pengyang Ling", "Jiazi Bu", "Yibin Wang", "Yuhang Zang", "Jiaqi Wang", "Li Niu", "Guangtao Zhai"], "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models", "comment": "Github Page: https://github.com/bcmi/Granular-GRPO", "summary": "The integration of online reinforcement learning (RL) into diffusion and flow\nmodels has recently emerged as a promising approach for aligning generative\nmodels with human preferences. Stochastic sampling via Stochastic Differential\nEquations (SDE) is employed during the denoising process to generate diverse\ndenoising directions for RL exploration. While existing methods effectively\nexplore potential high-value samples, they suffer from sub-optimal preference\nalignment due to sparse and narrow reward signals. To address these challenges,\nwe propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves\nprecise and comprehensive reward assessments of sampling directions in\nreinforcement learning of flow models. Specifically, a Singular Stochastic\nSampling strategy is introduced to support step-wise stochastic exploration\nwhile enforcing a high correlation between the reward and the injected noise,\nthereby facilitating a faithful reward for each SDE perturbation. Concurrently,\nto eliminate the bias inherent in fixed-granularity denoising, we introduce a\nMulti-Granularity Advantage Integration module that aggregates advantages\ncomputed at multiple diffusion scales, producing a more comprehensive and\nrobust evaluation of the sampling directions. Experiments conducted on various\nreward models, including both in-domain and out-of-domain evaluations,\ndemonstrate that our $\\text{G}^2$RPO significantly outperforms existing\nflow-based GRPO baselines,highlighting its effectiveness and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86Granular-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u5947\u5f02\u968f\u673a\u91c7\u6837\u7b56\u7565\u548c\u591a\u7c92\u5ea6\u4f18\u52bf\u96c6\u6210\u6a21\u5757\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6269\u6563\u548c\u6d41\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u4fe1\u53f7\u7a00\u758f\u548c\u72ed\u7a84\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u5168\u9762\u7684\u91c7\u6837\u65b9\u5411\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u9ad8\u4ef7\u503c\u6837\u672c\u65b9\u9762\u6709\u6548\uff0c\u4f46\u7531\u4e8e\u7a00\u758f\u548c\u72ed\u7a84\u7684\u5956\u52b1\u4fe1\u53f7\u5bfc\u81f4\u504f\u597d\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u5956\u52b1\u8bc4\u4f30\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u968f\u673a\u91c7\u6837\u7b56\u7565\u652f\u6301\u9010\u6b65\u968f\u673a\u63a2\u7d22\uff0c\u786e\u4fdd\u5956\u52b1\u4e0e\u6ce8\u5165\u566a\u58f0\u9ad8\u5ea6\u76f8\u5173\uff1b\u5f15\u5165\u591a\u7c92\u5ea6\u4f18\u52bf\u96c6\u6210\u6a21\u5757\u805a\u5408\u591a\u4e2a\u6269\u6563\u5c3a\u5ea6\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7a33\u5065\u7684\u91c7\u6837\u65b9\u5411\u8bc4\u4f30\u3002", "result": "\u5728\u5404\u79cd\u5956\u52b1\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cG\u00b2RPO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6d41\u7684GRPO\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "G\u00b2RPO\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u7684\u91c7\u6837\u7b56\u7565\u548c\u591a\u7c92\u5ea6\u8bc4\u4f30\u673a\u5236\uff0c\u5728\u6269\u6563\u548c\u6d41\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u504f\u597d\u5bf9\u9f50\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02245", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02245", "abs": "https://arxiv.org/abs/2510.02245", "authors": ["Runzhe Zhan", "Yafu Li", "Zhi Wang", "Xiaoye Qu", "Dongrui Liu", "Jing Shao", "Derek F. Wong", "Yu Cheng"], "title": "ExGRPO: Learning to Reason from Experience", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ExGRPO\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u63a8\u7406\u7ecf\u9a8c\u7684\u4ef7\u503c\u6307\u6807\uff08\u6b63\u786e\u6027\u548c\u71b5\uff09\uff0c\u7ec4\u7ec7\u5e76\u4f18\u5148\u5904\u7406\u6709\u4ef7\u503c\u7684\u7ecf\u9a8c\uff0c\u4f7f\u7528\u6df7\u5408\u7b56\u7565\u76ee\u6807\u5e73\u8861\u63a2\u7d22\u4e0e\u7ecf\u9a8c\u5229\u7528\uff0c\u5728\u6570\u5b66/\u901a\u7528\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u7684\u5728\u7ebf\u7b56\u7565\u8bad\u7ec3\u5728\u5355\u6b21\u66f4\u65b0\u540e\u4e22\u5f03\u7ecf\u9a8c\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u4e0d\u7a33\u5b9a\u6027\u3002\u867d\u7136\u5148\u524d\u5de5\u4f5c\u5f3a\u8c03\u4e86\u91cd\u7528\u8fc7\u53bb\u7ecf\u9a8c\u7684\u597d\u5904\uff0c\u4f46\u7ecf\u9a8c\u7279\u6027\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u4e2d\u7684\u4f5c\u7528\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faExGRPO\u6846\u67b6\uff1a\u8bc6\u522b\u63a8\u7406\u7ecf\u9a8c\u7684\u4ef7\u503c\u6307\u6807\uff08\u6b63\u786e\u6027\u548c\u71b5\uff09\uff0c\u7ec4\u7ec7\u5e76\u4f18\u5148\u5904\u7406\u6709\u4ef7\u503c\u7684\u7ecf\u9a8c\uff0c\u4f7f\u7528\u6df7\u5408\u7b56\u7565\u76ee\u6807\u5e73\u8861\u63a2\u7d22\u4e0e\u7ecf\u9a8c\u5229\u7528\u3002", "result": "\u5728\u4e94\u4e2a\u9aa8\u5e72\u6a21\u578b\uff081.5B-8B\u53c2\u6570\uff09\u4e0a\uff0cExGRPO\u5728\u6570\u5b66/\u901a\u7528\u57fa\u51c6\u4e0a\u5e73\u5747\u6bd4\u5728\u7ebf\u7b56\u7565RLVR\u63d0\u5347+3.5/7.6\u5206\uff0c\u5e76\u5728\u5f3a\u5f31\u6a21\u578b\u4e0a\u90fd\u80fd\u7a33\u5b9a\u8bad\u7ec3\u3002", "conclusion": "\u539f\u5219\u6027\u7684\u7ecf\u9a8c\u7ba1\u7406\u662f\u9ad8\u6548\u548c\u53ef\u6269\u5c55RLVR\u7684\u5173\u952e\u8981\u7d20\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02180", "abs": "https://arxiv.org/abs/2510.02180", "authors": ["Silvia Sapora", "Devon Hjelm", "Alexander Toshev", "Omar Attia", "Bogdan Mazoure"], "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning", "comment": null, "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.", "AI": {"tldr": "GRACE\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\u4ece\u4e13\u5bb6\u8f68\u8ff9\u4e2d\u9006\u5411\u5de5\u7a0b\u53ef\u89e3\u91ca\u7684\u4ee3\u7801\u5316\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ea7\u751f\u7684\"\u9ed1\u76d2\"\u6a21\u578b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ea7\u751f\u96be\u4ee5\u89e3\u91ca\u548c\u8c03\u8bd5\u7684\"\u9ed1\u76d2\"\u5956\u52b1\u6a21\u578b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u53ef\u68c0\u67e5\u3001\u53ef\u9a8c\u8bc1\u7684\u53ef\u89e3\u91ca\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u5316\u641c\u7d22\u6846\u67b6\u5185\uff0c\u76f4\u63a5\u4ece\u4e13\u5bb6\u8f68\u8ff9\u9006\u5411\u5de5\u7a0b\u751f\u6210\u4ee3\u7801\u5316\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ea7\u751f\u53ef\u6267\u884c\u7684\u4ee3\u7801\u3002", "result": "\u5728BabyAI\u548cAndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRACE\u9ad8\u6548\u5b66\u4e60\u4e86\u9ad8\u7cbe\u5ea6\u5956\u52b1\uff0c\u5728\u590d\u6742\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4ea7\u751f\u7684\u5956\u52b1\u51fd\u6570\u80fd\u751f\u6210\u5f3a\u7b56\u7565\uff0c\u6027\u80fd\u4f18\u4e8e\u6a21\u4eff\u5b66\u4e60\u548c\u4f7f\u7528\u771f\u5b9e\u5956\u52b1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "GRACE\u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u7684\u4ee3\u7801\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u6784\u5efa\u590d\u6742\u5956\u52b1API\uff0c\u4e3a\u9006\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u53ef\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02212", "abs": "https://arxiv.org/abs/2510.02212", "authors": ["Hanyang Zhao", "Dawen Liang", "Wenpin Tang", "David Yao", "Nathan Kallus"], "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning", "comment": null, "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.", "AI": {"tldr": "DiFFPO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a9\u7801\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u63a8\u7406\u65e2\u66f4\u597d\uff08furious\uff09\u53c8\u66f4\u5feb\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u66ff\u4ee3\u7b56\u7565\u8bad\u7ec3\u548c\u8054\u5408\u8bad\u7ec3\u9ad8\u6548\u91c7\u6837\u5668\uff0c\u5728\u6570\u5b66\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u6846\u67b6\uff0c\u65e2\u80fd\u63d0\u5347\u6a21\u578b\u63a8\u7406\u8d28\u91cf\uff0c\u53c8\u80fd\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "method": "1. \u4f7f\u7528\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u66ff\u4ee3\u7b56\u7565\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u4f3c\u7136\u8fd1\u4f3c\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\uff1b2. \u8054\u5408\u8bad\u7ec3\u9ad8\u6548\u91c7\u6837\u5668/\u63a7\u5236\u5668\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u4e3a\u6bcf\u4e2a\u63d0\u793a\u5206\u914d\u63a8\u7406\u9608\u503c\uff0c\u5229\u7528\u591atoken\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u548c\u89c4\u5212\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cDiFFPO\u76f8\u6bd4\u4ec5\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5728\u66f4\u5c11\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\u4e0b\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027\uff0c\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u65f6\u8ba1\u7b97\u91cf\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\u3002", "conclusion": "DiFFPO\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8054\u5408\u8bad\u7ec3\u91c7\u6837\u5668\uff0c\u6210\u529f\u63d0\u5347\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8d28\u91cf\u548c\u6548\u7387\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.89cd6a6d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2025%2F09%2Fabusing-notions-ai-agent-for-data-theft.html%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/mAqAbR3bzXmC9xCrbG_2ZqV0bYpSFxLCPMFmu0JSBFg=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2025%2F09%2Fabusing-notions-ai-agent-for-data-theft.html%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/mAqAbR3bzXmC9xCrbG_2ZqV0bYpSFxLCPMFmu0JSBFg=425", "authors": ["TLDR Newsletter"], "title": "Abusing Notion's AI Agent for Data Theft", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.schneier.com%2Fblog%2Farchives%2F2025%2F09%2Fabusing-notions-ai-agent-for-data-theft.html%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/mAqAbR3bzXmC9xCrbG_2ZqV0bYpSFxLCPMFmu0JSBFg=425", "summary": "Abusing Notion's AI Agent for Data Theft (3 minute read) Notion's AI agents are susceptible to prompt injection attacks exploiting Simon Willson's 'lethal trifecta'\u2014access to private data, exposure to untrusted content, and external communication. Attackers can embed malicious instructions in PDFs with white text instructing AI to extract and exfiltrate confidential data through web searches to attacker-controlled URLs. Security professionals should evaluate AI deployments carefully, as curre...", "source": "tldr", "AI": {"tldr": "Notion\u7684AI\u4ee3\u7406\u5b58\u5728\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7PDF\u4e2d\u7684\u9690\u85cf\u6587\u672c\u6307\u4ee4\u8ba9AI\u63d0\u53d6\u5e76\u5916\u6cc4\u673a\u5bc6\u6570\u636e", "motivation": "\u63ed\u793aNotion AI\u4ee3\u7406\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u8be5\u6f0f\u6d1e\u5141\u8bb8\u653b\u51fb\u8005\u5229\u7528\u63d0\u793a\u6ce8\u5165\u6280\u672f\u7a83\u53d6\u79c1\u4eba\u6570\u636e", "method": "\u5229\u7528Simon Willson\u7684'\u81f4\u547d\u4e09\u91cd\u5a01\u80c1'\u6982\u5ff5\uff0c\u901a\u8fc7\u5728PDF\u4e2d\u5d4c\u5165\u767d\u8272\u6587\u672c\u7684\u6076\u610f\u6307\u4ee4\uff0c\u8ba9AI\u4ee3\u7406\u63d0\u53d6\u6570\u636e\u5e76\u901a\u8fc7\u7f51\u7edc\u641c\u7d22\u53d1\u9001\u5230\u653b\u51fb\u8005\u63a7\u5236\u7684URL", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4eceNotion AI\u4ee3\u7406\u4e2d\u7a83\u53d6\u673a\u5bc6\u6570\u636e", "conclusion": "\u5f53\u524dAI\u90e8\u7f72\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u5b89\u5168\u4e13\u4e1a\u4eba\u5458\u9700\u8981\u4ed4\u7ec6\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u90e8\u7f72", "topic": "agent analysis"}}
{"id": "tldr.2510.a5f3f67c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fshivasurya%2Fcode-pathfinder%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/N9Qknn42-0BJ2FnIGFRhKgSt_dMd0kaib1xEQyUPlTM=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fshivasurya%2Fcode-pathfinder%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/N9Qknn42-0BJ2FnIGFRhKgSt_dMd0kaib1xEQyUPlTM=425", "authors": ["TLDR Newsletter"], "title": "Code Pathfinder", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fshivasurya%2Fcode-pathfinder%3Futm_source=tldrinfosec/1/010001999fe2f6c1-7d4fb401-e251-44c8-8e10-629017c31e04-000000/N9Qknn42-0BJ2FnIGFRhKgSt_dMd0kaib1xEQyUPlTM=425", "summary": "Code Pathfinder (GitHub Repo) Code Pathfinder, the open-source alternative to GitHub CodeQL, was built with GoLang. Built for advanced structural search, derive insights, and find vulnerabilities in code.", "source": "tldr", "AI": {"tldr": "Code Pathfinder\u662f\u4e00\u4e2a\u7528GoLang\u6784\u5efa\u7684\u5f00\u6e90\u4ee3\u7801\u5206\u6790\u5de5\u5177\uff0c\u53ef\u4f5c\u4e3aGitHub CodeQL\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u9ad8\u7ea7\u7ed3\u6784\u641c\u7d22\u3001\u4ee3\u7801\u6d1e\u5bdf\u548c\u6f0f\u6d1e\u53d1\u73b0\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u6765\u66ff\u4ee3GitHub CodeQL\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u8fdb\u884c\u9ad8\u7ea7\u4ee3\u7801\u7ed3\u6784\u5206\u6790\u548c\u6f0f\u6d1e\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528GoLang\u6784\u5efa\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u7ea7\u7ed3\u6784\u641c\u7d22\u529f\u80fd\uff0c\u80fd\u591f\u6df1\u5165\u5206\u6790\u4ee3\u7801\u7ed3\u6784\u5e76\u8bc6\u522b\u6f5c\u5728\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u5f00\u6e90\u4ee3\u7801\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u7ed3\u6784\u641c\u7d22\u3001\u4ee3\u7801\u6d1e\u5bdf\u548c\u6f0f\u6d1e\u53d1\u73b0\u3002", "conclusion": "Code Pathfinder\u6210\u529f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5f00\u6e90\u4ee3\u7801\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u5e02\u573a\u7a7a\u767d\u3002", "topic": "code agent"}}
{"id": "tldr.2510.c9482b63", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_1_primary%26utm_content=tldr_ai/2/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/NUayruE-s2g__tos7ua33iP45ulrBH3ar9B5W_NRhtI=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_1_primary%26utm_content=tldr_ai/2/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/NUayruE-s2g__tos7ua33iP45ulrBH3ar9B5W_NRhtI=425", "authors": ["TLDR Newsletter"], "title": "\ud83c\udfc6Warp: Try the Top Rated Coding Agent", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_1_primary%26utm_content=tldr_ai/2/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/NUayruE-s2g__tos7ua33iP45ulrBH3ar9B5W_NRhtI=425", "summary": "\ud83c\udfc6Warp: Try the Top Rated Coding Agent (Sponsor) You've tried Cursor, Codex, and Claude Code. Now try the coding agent that beats them.Warp leads on Terminal-Bench and SWE-bench Verified, trusted by 700K+ devs and 56% of the Fortune 500. \u26a1\ufe0f Combines the power of the terminal with the interactivity of the IDE \u26a1\ufe0f Works with all top tier models (GPT-5, Sonnet 4.5, Opus 4.1, Gemini 2.5) \u26a1\ufe0f Developers save 5 hours per week on average \u201cI used to be sold on Cursor\u2026Warp is unlike any other tool I've u...", "source": "tldr", "AI": {"tldr": "Warp\u662f\u4e00\u6b3e\u9876\u7ea7\u7f16\u7a0b\u52a9\u624b\uff0c\u5728Terminal-Bench\u548cSWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ed3\u5408\u4e86\u7ec8\u7aef\u529f\u80fd\u548cIDE\u4ea4\u4e92\u6027\uff0c\u652f\u6301\u591a\u79cd\u9876\u7ea7AI\u6a21\u578b\uff0c\u5e73\u5747\u6bcf\u5468\u4e3a\u5f00\u53d1\u8005\u8282\u77015\u5c0f\u65f6\u5de5\u4f5c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7f16\u7a0b\u52a9\u624b\u5982Cursor\u3001Codex\u548cClaude Code\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "method": "\u7ed3\u5408\u7ec8\u7aef\u529f\u80fd\u548cIDE\u4ea4\u4e92\u6027\uff0c\u96c6\u6210\u591a\u79cd\u9876\u7ea7AI\u6a21\u578b\uff08GPT-5\u3001Sonnet 4.5\u3001Opus 4.1\u3001Gemini 2.5\uff09\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u7f16\u7a0b\u8f85\u52a9\u4f53\u9a8c\u3002", "result": "\u5728Terminal-Bench\u548cSWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u88ab70\u4e07+\u5f00\u53d1\u8005\u548c56%\u7684\u8d22\u5bcc500\u5f3a\u516c\u53f8\u4f7f\u7528\uff0c\u5e73\u5747\u6bcf\u5468\u4e3a\u5f00\u53d1\u8005\u8282\u77015\u5c0f\u65f6\u3002", "conclusion": "Warp\u662f\u76ee\u524d\u6700\u4f18\u79c0\u7684\u7f16\u7a0b\u52a9\u624b\u5de5\u5177\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2510.f1586a3b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrai/1/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/eudaY9ep8LVwW_wQCZ6hd0T_JWhh7q5YYlWPUV_xgAs=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrai/1/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/eudaY9ep8LVwW_wQCZ6hd0T_JWhh7q5YYlWPUV_xgAs=425", "authors": ["TLDR Newsletter"], "title": "Designing agentic loops", "comment": "Source: TLDR Newsletter, Date: 2025-10-01, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FSep%2F30%2Fdesigning-agentic-loops%2F%3Futm_source=tldrai/1/010001999fea44da-9095fe21-1bba-466f-9314-b84577b142f2-000000/eudaY9ep8LVwW_wQCZ6hd0T_JWhh7q5YYlWPUV_xgAs=425", "summary": "Designing agentic loops (11 minute read) Agents can now directly exercise the code they are writing, correct errors, dig through existing implementation details, and even run experiments to find effective code solutions to problems. Coding agents are brute force tools for finding solutions to coding problems. Reducing problems to clear goals and sets of tools that can iterate towards those goals can help coding agents brute-force their way to an effective solution. The art of using agents wel...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u667a\u80fd\u4f53\u5faa\u73af\uff0c\u4f7f\u7f16\u7801\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u8fed\u4ee3\u6267\u884c\u4ee3\u7801\u3001\u7ea0\u6b63\u9519\u8bef\u3001\u63a2\u7d22\u5b9e\u73b0\u7ec6\u8282\u548c\u8fd0\u884c\u5b9e\u9a8c\u6765\u66b4\u529b\u6c42\u89e3\u7f16\u7801\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7f16\u7801\u667a\u80fd\u4f53\u73b0\u5728\u80fd\u591f\u76f4\u63a5\u6267\u884c\u7f16\u5199\u7684\u4ee3\u7801\u5e76\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u4f46\u5982\u4f55\u6709\u6548\u8bbe\u8ba1\u8fd9\u4e9b\u667a\u80fd\u4f53\u5faa\u73af\u4ee5\u6700\u5927\u5316\u5176\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u660e\u786e\u76ee\u6807\u548c\u53ef\u7528\u5de5\u5177\u96c6\uff0c\u8ba9\u7f16\u7801\u667a\u80fd\u4f53\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u9010\u6b65\u903c\u8fd1\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u8bbe\u8ba1\u667a\u80fd\u4f53\u5faa\u73af\u7684\u65b9\u6cd5\u8bba\uff0c\u4f7f\u7f16\u7801\u667a\u80fd\u4f53\u80fd\u591f\u66f4\u6709\u6548\u5730\u66b4\u529b\u6c42\u89e3\u7f16\u7801\u95ee\u9898\u3002", "conclusion": "\u719f\u7ec3\u8fd0\u7528\u667a\u80fd\u4f53\u7684\u827a\u672f\u5728\u4e8e\u5408\u7406\u8bbe\u8ba1\u5faa\u73af\u673a\u5236\uff0c\u5c06\u590d\u6742\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u8fed\u4ee3\u6c42\u89e3\u7684\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.73b2cd8c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.uber.com%2Fblog%2Fubers-strategy-to-upgrading-2m-spark-jobs%2F%3Futm_source=tldrdata/1/01000199a4638f74-f152d3f2-c308-4145-9b11-ac3d4eca2f65-000000/i1duB6csZMOarEoGZaV2M8V9KVlNoAbvgPXwskHJ_90=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.uber.com%2Fblog%2Fubers-strategy-to-upgrading-2m-spark-jobs%2F%3Futm_source=tldrdata/1/01000199a4638f74-f152d3f2-c308-4145-9b11-ac3d4eca2f65-000000/i1duB6csZMOarEoGZaV2M8V9KVlNoAbvgPXwskHJ_90=425", "authors": ["TLDR Newsletter"], "title": "Uber's Strategy to Upgrading 2M+ Spark Jobs", "comment": "Source: TLDR Newsletter, Date: 2025-10-02, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.uber.com%2Fblog%2Fubers-strategy-to-upgrading-2m-spark-jobs%2F%3Futm_source=tldrdata/1/01000199a4638f74-f152d3f2-c308-4145-9b11-ac3d4eca2f65-000000/i1duB6csZMOarEoGZaV2M8V9KVlNoAbvgPXwskHJ_90=425", "summary": "Uber's Strategy to Upgrading 2M+ Spark Jobs (10 minute read) Uber upgraded from Spark 2.4 to Spark 3.3, migrating over 40,000 Spark applications and 2,100 applications in just six months. It automated the process using Polyglot Piranha, an open-source tool that parses and rewrites code by converting it into an Abstract Syntax Tree (AST) and applying transformation rules to enable bulk changes across applications.", "source": "tldr", "AI": {"tldr": "Uber\u4f7f\u7528Polyglot Piranha\u5de5\u5177\u57286\u4e2a\u6708\u5185\u81ea\u52a8\u5316\u5347\u7ea7\u4e86\u8d85\u8fc740,000\u4e2aSpark\u5e94\u7528\u548c2,100\u4e2a\u5e94\u7528\uff0c\u4eceSpark 2.4\u8fc1\u79fb\u5230Spark 3.3", "motivation": "\u9700\u8981\u5927\u89c4\u6a21\u5347\u7ea7Spark\u7248\u672c\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u529f\u80fd\uff0c\u4f46\u624b\u52a8\u5347\u7ea7\u5927\u91cf\u5e94\u7528\u6210\u672c\u8fc7\u9ad8", "method": "\u4f7f\u7528\u5f00\u6e90\u7684Polyglot Piranha\u5de5\u5177\uff0c\u901a\u8fc7\u89e3\u6790\u4ee3\u7801\u4e3a\u62bd\u8c61\u8bed\u6cd5\u6811(AST)\u5e76\u5e94\u7528\u8f6c\u6362\u89c4\u5219\uff0c\u5b9e\u73b0\u8de8\u5e94\u7528\u7684\u6279\u91cf\u66f4\u6539", "result": "\u6210\u529f\u57286\u4e2a\u6708\u5185\u8fc1\u79fb\u4e86\u8d85\u8fc740,000\u4e2aSpark\u5e94\u7528\u548c2,100\u4e2a\u5e94\u7528", "conclusion": "\u81ea\u52a8\u5316\u4ee3\u7801\u91cd\u6784\u5de5\u5177\u80fd\u591f\u663e\u8457\u52a0\u901f\u5927\u89c4\u6a21\u8f6f\u4ef6\u5347\u7ea7\u8fc7\u7a0b", "topic": "swe application"}}
{"id": "tldr.2510.cab968df", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mongodb.com%2Fcompany%2Fblog%2Ftechnical%2Fwhy-multi-agent-systems-need-memory-engineering%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/snSIwYZroorImiI5dA5N5KWZ43uY7TrqvFYV7Q9JqUk=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mongodb.com%2Fcompany%2Fblog%2Ftechnical%2Fwhy-multi-agent-systems-need-memory-engineering%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/snSIwYZroorImiI5dA5N5KWZ43uY7TrqvFYV7Q9JqUk=425", "authors": ["TLDR Newsletter"], "title": "Why Multi-Agent Systems Need Memory Engineering", "comment": "Source: TLDR Newsletter, Date: 2025-10-02, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mongodb.com%2Fcompany%2Fblog%2Ftechnical%2Fwhy-multi-agent-systems-need-memory-engineering%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/snSIwYZroorImiI5dA5N5KWZ43uY7TrqvFYV7Q9JqUk=425", "summary": "Why Multi-Agent Systems Need Memory Engineering (8 minute read) Multi-agent AI systems frequently fail because they don't have a proper shared memory infrastructure. While context engineering has improved individual agent performance by managing \"the right information at the right time,\" this approach breaks down when multiple agents must coordinate without shared memory systems. The solution is memory engineering: creating a persistent, shared memory infrastructure that allows AI agents to e...", "source": "tldr", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u8981\u5185\u5b58\u5de5\u7a0b\u6765\u89e3\u51b3\u534f\u8c03\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u5efa\u6301\u4e45\u5171\u4eab\u5185\u5b58\u57fa\u7840\u8bbe\u65bd\u6765\u6539\u5584\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "motivation": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7ecf\u5e38\u56e0\u4e3a\u7f3a\u4e4f\u9002\u5f53\u7684\u5171\u4eab\u5185\u5b58\u57fa\u7840\u8bbe\u65bd\u800c\u5931\u8d25\uff0c\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u9700\u8981\u534f\u8c03\u65f6\u4f1a\u5931\u6548\u3002", "method": "\u63d0\u51fa\u5185\u5b58\u5de5\u7a0b\u65b9\u6cd5\uff0c\u521b\u5efa\u6301\u4e45\u3001\u5171\u4eab\u7684\u5185\u5b58\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7fAI\u667a\u80fd\u4f53\u80fd\u591f\u6709\u6548\u534f\u8c03\u3002", "result": "\u901a\u8fc7\u5b9e\u73b0\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u8c03\u80fd\u529b\u548c\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u5185\u5b58\u5de5\u7a0b\u662f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6210\u529f\u7684\u5173\u952e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.edf91ee2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fchangelog%2F1-7%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/ZzLFxNYvYlmpiiYyhGfQUdxRUShx9kSxcKdenqNLA0o=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fchangelog%2F1-7%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/ZzLFxNYvYlmpiiYyhGfQUdxRUShx9kSxcKdenqNLA0o=425", "authors": ["TLDR Newsletter"], "title": "Cursor 1.7", "comment": "Source: TLDR Newsletter, Date: 2025-10-02, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fchangelog%2F1-7%3Futm_source=tldrwebdev/1/01000199a4a8ba1f-870350b5-7612-42ef-ae5b-5ea43dfc5e9c-000000/ZzLFxNYvYlmpiiYyhGfQUdxRUShx9kSxcKdenqNLA0o=425", "summary": "Cursor 1.7 (2 minute read) Cursor 1.7 introduces several new features and improvements, including Agent Autocomplete, Hooks for customizing Agent behavior, and team rules for consistent project management. Shareable deeplinks for prompts are now available in beta, allowing for easier sharing of workflows and instructions. Commands now execute in a sandboxed environment for better security, and users can monitor Agents from the menu bar.", "source": "tldr", "AI": {"tldr": "Cursor 1.7\u7248\u672c\u5f15\u5165\u4e86\u591a\u9879\u65b0\u529f\u80fd\u548c\u6539\u8fdb\uff0c\u5305\u62ec\u667a\u80fd\u4ee3\u7801\u8865\u5168\u3001\u81ea\u5b9a\u4e49\u4ee3\u7406\u884c\u4e3a\u7684\u94a9\u5b50\u3001\u56e2\u961f\u89c4\u5219\u7ba1\u7406\uff0c\u4ee5\u53ca\u53ef\u5206\u4eab\u7684\u6df1\u5ea6\u94fe\u63a5\u63d0\u793a\u529f\u80fd\u3002", "motivation": "\u63d0\u5347\u4ee3\u7801\u7f16\u8f91\u5668\u7684\u667a\u80fd\u5316\u6c34\u5e73\u548c\u56e2\u961f\u534f\u4f5c\u6548\u7387\uff0c\u901a\u8fc7\u589e\u5f3a\u4ee3\u7406\u529f\u80fd\u548c\u5b89\u5168\u673a\u5236\u6765\u6539\u5584\u5f00\u53d1\u4f53\u9a8c\u3002", "method": "\u5728Cursor\u7f16\u8f91\u5668\u4e2d\u96c6\u6210\u667a\u80fd\u4ee3\u7406\u529f\u80fd\uff0c\u6dfb\u52a0\u81ea\u5b9a\u4e49\u94a9\u5b50\u63a5\u53e3\uff0c\u5b9e\u73b0\u6c99\u76d2\u73af\u5883\u6267\u884c\u547d\u4ee4\uff0c\u5e76\u63d0\u4f9b\u56e2\u961f\u89c4\u5219\u914d\u7f6e\u548c\u6df1\u5ea6\u94fe\u63a5\u5206\u4eab\u673a\u5236\u3002", "result": "\u6210\u529f\u63a8\u51fa\u4e86\u5305\u542b\u667a\u80fd\u8865\u5168\u3001\u5b89\u5168\u6c99\u76d2\u3001\u56e2\u961f\u534f\u4f5c\u5de5\u5177\u548c\u63d0\u793a\u5206\u4eab\u529f\u80fd\u7684Cursor 1.7\u7248\u672c\u3002", "conclusion": "Cursor 1.7\u901a\u8fc7\u5f15\u5165\u667a\u80fd\u4ee3\u7406\u548c\u534f\u4f5c\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u56e2\u961f\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301\u3002", "topic": "swe application"}}
{"id": "wechat.2510.29b365cf", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3OTYzNTE5NQ==&mid=2247492989&idx=3&sn=4ec16f50c6958bb7d82b4d33fd568141&chksm=ea3b3c5be7ad09f06ca6d806d6ab0773ba717f1e3d12ed1d1aec01757a9f7b34f7b9a4e1b124#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3OTYzNTE5NQ==&mid=2247492989&idx=3&sn=4ec16f50c6958bb7d82b4d33fd568141&chksm=ea3b3c5be7ad09f06ca6d806d6ab0773ba717f1e3d12ed1d1aec01757a9f7b34f7b9a4e1b124#rd", "authors": ["NewBytes"], "title": "\u96c5\u601d\u8bcd\u6c47", "comment": "Source: WeChat, Published: 2025-10-03 05:44:16", "summary": "2. Agentic \uff08adj.\uff09\u8bcd\u5178\u91ca\u4e49 characterized by action or agency\uff1bhaving the capacity to act independently and make choices. | \u5177\u6709\u4e3b\u52a8\u6027\u7684\uff1b\u6709\u4ee3\u7406\u80fd\u529b\u7684\u3002\u751f\u6d3b\u573a\u666f\u4f8b\u53e5 The new software update provides a more agentic AI assistant that can anticipate user needs. \u2192 \u65b0\u7684\u8f6f", "AI": {"tldr": "2. Agentic \uff08adj.\uff09\u8bcd\u5178\u91ca\u4e49 characterized by action or agency\uff1bhaving the capacity to act independently and make choices. | \u5177\u6709\u4e3b\u52a8\u6027\u7684\uff1b\u6709\u4ee3\u7406\u80fd\u529b\u7684\u3002\u751f\u6d3b\u573a\u666f\u4f8b\u53e5 The new software update provides a more agentic AI assistant t...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.98f85a8b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NTEyMjA0MQ==&mid=2247486458&idx=1&sn=750175ffd7c09d918076c65e1a87a561&chksm=cf1c3ad2e994c7549e2399cf430b4f30889af9b041eeae349cca62a84611a6d8e322e4e7467b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NTEyMjA0MQ==&mid=2247486458&idx=1&sn=750175ffd7c09d918076c65e1a87a561&chksm=cf1c3ad2e994c7549e2399cf430b4f30889af9b041eeae349cca62a84611a6d8e322e4e7467b#rd", "authors": ["\u96f6\u4e00\u683c\u7269"], "title": "\u4ece\u624b\u5de5\u4f5c\u574a\u5230<em class=\"highlight\">Agentic</em> AI\uff1a\u7ec4\u7ec7\u6b63\u5728\u7ecf\u5386\u600e\u6837\u7684\u91cd\u6784\uff1f", "comment": "Source: WeChat, Published: 2025-10-02 22:01:23", "summary": "\u9ea6\u80af\u9521\u7684AI\u65f6\u4ee3\u7ec4\u7ec7\u6d1e\u5bdf\u6587\u7ae0\u300aThe Agentic Organization\uff1a Contours of the Next Paradigm for the AI Era\u300b\u5206\u6790\u4e86\u7ec4\u7ec7\u7684\u8fdb\u5316\u3002\u62a5\u544a\u5c06\u7ec4\u7ec7\u8fdb\u5316\u7684\u5386\u53f2\u5206\u6210\u4e86\u56db\u4e2a\u9636\u6bb5\uff1a", "AI": {"tldr": "\u9ea6\u80af\u9521\u7684AI\u65f6\u4ee3\u7ec4\u7ec7\u6d1e\u5bdf\u6587\u7ae0\u300aThe Agentic Organization\uff1a Contours of the Next Paradigm for the AI Era\u300b\u5206\u6790\u4e86\u7ec4\u7ec7\u7684\u8fdb\u5316\u3002\u62a5\u544a\u5c06\u7ec4\u7ec7\u8fdb\u5316\u7684\u5386\u53f2\u5206\u6210\u4e86\u56db\u4e2a\u9636\u6bb5\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.038a35db", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247491615&idx=1&sn=47ab6722f503c4afa2db787fb9c2ec86&chksm=c3e700ade5aefcefbdba61d8be60bfc1bd5c9bee35905a5019e966b3ac28c1278671a053beff#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMzI2ODAyNQ==&mid=2247491615&idx=1&sn=47ab6722f503c4afa2db787fb9c2ec86&chksm=c3e700ade5aefcefbdba61d8be60bfc1bd5c9bee35905a5019e966b3ac28c1278671a053beff#rd", "authors": ["\u672a\u5c3d\u7814\u7a76"], "title": "AI\u5f53\u9ad8\u7ea7\u767d\u9886\u725b\u9a6c\uff0c\u54ea\u5bb6<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6700\u5f3a", "comment": "Source: WeChat, Published: 2025-10-03 13:16:29", "summary": "AI\u5927\u6a21\u578b\u4e5f\u597d\uff0c\u667a\u80fd\u4f53\u4e5f\u597d\uff0c\u5728\u5404\u79cd\u6d4b\u8bc4\u699c\u4e0a\u5237\u5f97\u4e0d\u4ea6\u4e50\u4e4e\u3002\u5bf9\u4e8e\u8ddf\u8e2a\u6a21\u578b\u8fdb\u5c55\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u4e0a\u9650\u786e\u5b9e\u6709\u7528\uff1b\u4e0d\u8fc7\u8861\u91cf\u7684\u662f\u6a21\u578b\u7684\u62bd\u8c61\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u5177\u6709\u7ecf\u6d4e\u4ef7\u503c\u7684\u4ea7\u51fa\u3002", "AI": {"tldr": "AI\u5927\u6a21\u578b\u4e5f\u597d\uff0c\u667a\u80fd\u4f53\u4e5f\u597d\uff0c\u5728\u5404\u79cd\u6d4b\u8bc4\u699c\u4e0a\u5237\u5f97\u4e0d\u4ea6\u4e50\u4e4e\u3002\u5bf9\u4e8e\u8ddf\u8e2a\u6a21\u578b\u8fdb\u5c55\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u4e0a\u9650\u786e\u5b9e\u6709\u7528\uff1b\u4e0d\u8fc7\u8861\u91cf\u7684\u662f\u6a21\u578b\u7684\u62bd\u8c61\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u5177\u6709\u7ecf\u6d4e\u4ef7\u503c\u7684\u4ea7\u51fa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.96ab203d", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTU2NjU5MQ==&mid=2247487881&idx=1&sn=7c57beb89809957b7a7ef27657fa32c2&chksm=c5c0b3065c103c1e5f24883a5c4a45adaf1336fde12147fee6e1fe83affa9c45d52f1711f6dd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTU2NjU5MQ==&mid=2247487881&idx=1&sn=7c57beb89809957b7a7ef27657fa32c2&chksm=c5c0b3065c103c1e5f24883a5c4a45adaf1336fde12147fee6e1fe83affa9c45d52f1711f6dd#rd", "authors": ["\u98de\u7af9\u767e\u79d1"], "title": "\u667a\u80fd\u4f53\uff08AI Agent\uff09\u4e0e<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff08LLM\uff09\uff1a\u5404\u53f8\u5176\u804c\u53c8\u901a\u529b\u534f\u4f5c", "comment": "Source: WeChat, Published: 2025-10-03 09:29:38", "summary": "\u5927\u6a21\u578b\u662f\u57fa\u4e8eTransformer\u67b6\u6784\u6784\u5efa\u7684AI\u7cfb\u7edf\uff0c\u6838\u5fc3\u80fd\u529b\u56f4\u7ed5\u201c\u8bed\u8a00\u201d\u5c55\u5f00\uff0c\u901a\u8fc7\u5bf9\u6d77\u91cf\u6587\u672c\u6570\u636e\u7684\u9884\u8bad\u7ec3\uff0c\u638c\u63e1\u8bed\u6cd5\u3001\u8bed\u4e49\u4e0e\u8bed\u5883\u5173\u8054\u3002\u6839\u636e\u8f93\u5165\u7684prompt\uff08\u63d0\u793a\u8bcd\uff09\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u5e8f\u5217\uff0c\u4ece\u800c\u5b9e\u73b0\u95ee\u7b54\u3001\u521b\u4f5c\u3001\u7ffb\u8bd1\u7b49\u8bed\u8a00\u7c7b\u4efb\u52a1\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u662f\u57fa\u4e8eTransformer\u67b6\u6784\u6784\u5efa\u7684AI\u7cfb\u7edf\uff0c\u6838\u5fc3\u80fd\u529b\u56f4\u7ed5\u201c\u8bed\u8a00\u201d\u5c55\u5f00\uff0c\u901a\u8fc7\u5bf9\u6d77\u91cf\u6587\u672c\u6570\u636e\u7684\u9884\u8bad\u7ec3\uff0c\u638c\u63e1\u8bed\u6cd5\u3001\u8bed\u4e49\u4e0e\u8bed\u5883\u5173\u8054\u3002\u6839\u636e\u8f93\u5165\u7684prompt\uff08\u63d0\u793a\u8bcd\uff09\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u7684\u5e8f\u5217\uff0c\u4ece\u800c\u5b9e\u73b0\u95ee\u7b54\u3001\u521b\u4f5c\u3001\u7ffb\u8bd1\u7b49\u8bed\u8a00\u7c7b\u4efb\u52a1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.5f3cee9b", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651609650&idx=3&sn=be2e72c06c7f0b31c23e42a48cdd3f55&chksm=bc5c203ea27945f31a9762e04f9ec006ae4ed4f25b9868428e617e63649d3f6fde4f658d8a1d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651609650&idx=3&sn=be2e72c06c7f0b31c23e42a48cdd3f55&chksm=bc5c203ea27945f31a9762e04f9ec006ae4ed4f25b9868428e617e63649d3f6fde4f658d8a1d#rd", "authors": ["\u4e2d\u56fd\u8ba1\u7b97\u673a\u5b66\u4f1a"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5982\u4f55\u91cd\u5851\u4fe1\u606f\u83b7\u53d6\uff1f\u6df1\u5ea6\u641c\u7d22\u4e0e\u4fe1\u606f\u667a\u80fd\u4f53\u7684\u524d\u6cbf\u63a2\u7d22 | CNCC", "comment": "Source: WeChat, Published: 2025-10-03 09:01:01", "summary": "\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 3\u5927\u6a21\u578b\u9047\u89c1\u68c0\u7d22\uff1a\u4ece\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5230\u5185\u90e8\u77e5\u8bc6\u63a2\u7d22 \u6797\u9e3f\u5b87 \u4e2d\u56fd\u79d1\u5b66\u9662\u8f6f\u4ef6\u7814\u7a76\u6240 4\u6784\u5efa\u81ea\u4e3b\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53 \u848b\u52c7 \u963f\u91cc\u5df4\u5df4\u901a\u4e49\u5b9e\u9a8c\u5ba4", "AI": {"tldr": "\u4e0a\u6d77\u4ea4\u901a\u5927\u5b66 3\u5927\u6a21\u578b\u9047\u89c1\u68c0\u7d22\uff1a\u4ece\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5230\u5185\u90e8\u77e5\u8bc6\u63a2\u7d22 \u6797\u9e3f\u5b87 \u4e2d\u56fd\u79d1\u5b66\u9662\u8f6f\u4ef6\u7814\u7a76\u6240 4\u6784\u5efa\u81ea\u4e3b\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53 \u848b\u52c7 \u963f\u91cc\u5df4\u5df4\u901a\u4e49\u5b9e\u9a8c\u5ba4", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.34442899", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNzE3NzIyNA==&mid=2247691184&idx=2&sn=f7f51e5dccb2dc271ac13356603b761d&chksm=c363f65c7f2bb4c11fda762ee8c15aeefcfebe567bd7ca6fcc613f9ad35f3bd1a6c5474fc733#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNzE3NzIyNA==&mid=2247691184&idx=2&sn=f7f51e5dccb2dc271ac13356603b761d&chksm=c363f65c7f2bb4c11fda762ee8c15aeefcfebe567bd7ca6fcc613f9ad35f3bd1a6c5474fc733#rd", "authors": ["\u6df1\u5733\u5e02\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u534f\u4f1a"], "title": "\u3010\u68c0\u6d4b\u8ba4\u8bc1\u3011\u4e2d\u56fd\u4fe1\u901a\u9662\u7275\u5934\u76845\u9879<em class=\"highlight\">\u5927\u6a21\u578b</em>\u884c\u4e1a\u6807\u51c6\u6b63\u5f0f\u53d1\u5e03", "comment": "Source: WeChat, Published: 2025-10-03 06:01:28", "summary": "\u8be5\u7cfb\u5217\u6807\u51c6\u8986\u76d6\u5927\u6a21\u578b\u7684\u5f00\u53d1\u3001\u7ba1\u7406\u3001\u8fd0\u8425\u7b49\u591a\u4e2a\u9636\u6bb5\uff0c\u4e3b\u8981\u5305\u62ec\u6a21\u578b\u5f00\u53d1\u3001\u80fd\u529b\u8bc4\u4f30\u3001\u5e94\u7528\u6210\u6548\u3001\u8fd0\u8425\u7ba1\u7406\u548c\u53ef\u4fe1\u8981\u6c42\u4e94\u90e8\u5206\uff0c\u4e3a\u5927\u6a21\u578b\u6280\u672f\u548c\u4ea7\u54c1\u7684\u7814\u53d1\u6d4b\u8bd5\u548c\u5e94\u7528\u63a8\u5e7f\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "AI": {"tldr": "\u8be5\u7cfb\u5217\u6807\u51c6\u8986\u76d6\u5927\u6a21\u578b\u7684\u5f00\u53d1\u3001\u7ba1\u7406\u3001\u8fd0\u8425\u7b49\u591a\u4e2a\u9636\u6bb5\uff0c\u4e3b\u8981\u5305\u62ec\u6a21\u578b\u5f00\u53d1\u3001\u80fd\u529b\u8bc4\u4f30\u3001\u5e94\u7528\u6210\u6548\u3001\u8fd0\u8425\u7ba1\u7406\u548c\u53ef\u4fe1\u8981\u6c42\u4e94\u90e8\u5206\uff0c\u4e3a\u5927\u6a21\u578b\u6280\u672f\u548c\u4ea7\u54c1\u7684\u7814\u53d1\u6d4b\u8bd5\u548c\u5e94\u7528\u63a8\u5e7f\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.51105268", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTE4MTYyOA==&mid=2247487048&idx=1&sn=af32a9c9dc0f2dfddf34843b9cb96d48&chksm=c5726c74ebb25c84696dd0908639fa5a9b7a10d90545a31070359c0210f2218cc916e5d96bdf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTE4MTYyOA==&mid=2247487048&idx=1&sn=af32a9c9dc0f2dfddf34843b9cb96d48&chksm=c5726c74ebb25c84696dd0908639fa5a9b7a10d90545a31070359c0210f2218cc916e5d96bdf#rd", "authors": ["\u667a\u6ccaAI"], "title": "\u4e00\u6587\u8bb2\u6e05\uff1aRAG\u3001Agent\u3001\u5fae\u8c03\u3001RLHF\u7b496\u79cd\u5e38\u89c1\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b9a\u5236\u7b56\u7565\uff0c\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\uff01", "comment": "Source: WeChat, Published: 2025-10-03 02:00:00", "summary": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8bad\u7ec3\u6570\u636e\u91cf\u5e9e\u5927\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\uff0c\u5e76\u4e14\u5305\u542b\u5927\u91cf\u7684\u53c2\u6570\u3002LLM\u5728\u8fc7\u53bb\u4e24\u5e74\u4e2d\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff0c\u5c55\u73b0\u4e86\u5728\u7406\u89e3\u548c\u751f\u6210\u7c7b\u4eba\u6587\u672c\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u3002", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8bad\u7ec3\u6570\u636e\u91cf\u5e9e\u5927\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\uff0c\u5e76\u4e14\u5305\u542b\u5927\u91cf\u7684\u53c2\u6570\u3002LLM\u5728\u8fc7\u53bb\u4e24\u5e74\u4e2d\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff0c\u5c55\u73b0\u4e86\u5728\u7406\u89e3\u548c\u751f\u6210\u7c7b\u4eba\u6587\u672c\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.8ef7ed42", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247625359&idx=4&sn=9b38e5f398411d08b9523ef401c0e457&chksm=f81078296b3d84dae40d548b0d7628217d3bf5ff20826ff0bddeae9953d7c1c19d99b422bf1b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247625359&idx=4&sn=9b38e5f398411d08b9523ef401c0e457&chksm=f81078296b3d84dae40d548b0d7628217d3bf5ff20826ff0bddeae9953d7c1c19d99b422bf1b#rd", "authors": ["CVer"], "title": "\u817e\u8baf<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9762\u7ecf\u6765\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-10-02 15:59:30", "summary": "\u65b9\u5411\u6d89\u53ca\uff1aml/dl/cv/nlp/\u63a8\u8350/\u5927\u6570\u636e/c++/python/java\uff1b \u65b9\u5411\u6d89\u53ca\uff1aml/dl/cv/nlp/\u63a8\u8350/\u5927\u6570\u636e/c++/python\uff1b \u9898\u578b\u6d89\u53ca\uff1aai\u7b97\u6cd5\u9898/\u7f16\u7a0b\u9898\uff08\u6ce8\uff1a\u5df2\u9644\u4e0a\u90e8\u5206\u9898\u76ee\u7684\u7b54\u6848\uff09 \u9898\u578b\u6d89\u53ca\uff1aai\u7b97\u6cd5\u9898/\u7f16\u7a0b\u9898\uff08\u6ce8\uff1a\u5df2\u9644\u4e0a\u90e8\u5206\u9898\u76ee\u7684\u7b54\u6848\uff09\uff09 [ai\u7b97\u6cd5\u9898] [a", "AI": {"tldr": "\u65b9\u5411\u6d89\u53ca\uff1aml/dl/cv/nlp/\u63a8\u8350/\u5927\u6570\u636e/c++/python/java\uff1b \u65b9\u5411\u6d89\u53ca\uff1aml/dl/cv/nlp/\u63a8\u8350/\u5927\u6570\u636e/c++/python\uff1b \u9898\u578b\u6d89\u53ca\uff1aai\u7b97\u6cd5\u9898/\u7f16\u7a0b\u9898\uff08\u6ce8\uff1a\u5df2\u9644\u4e0a\u90e8\u5206\u9898\u76ee\u7684\u7b54\u6848\uff09 \u9898\u578b\u6d89\u53ca\uff1aai\u7b97\u6cd5\u9898/\u7f16\u7a0b\u9898\uff08\u6ce8\uff1a\u5df2\u9644\u4e0a\u90e8\u5206\u9898\u76ee\u7684\u7b54\u6848\uff09\uff09 [ai\u7b97\u6cd5\u9898] [a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.c4b4b16e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0Mjc3NzE2Nw==&mid=2247488213&idx=1&sn=2b25555ac5a82caeb6773fec1b77cafd&chksm=face001de08823a8ced9c998ec69f4eb55911b031bec8a31ba21d8a5c4ef05597fe2d5abc17d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0Mjc3NzE2Nw==&mid=2247488213&idx=1&sn=2b25555ac5a82caeb6773fec1b77cafd&chksm=face001de08823a8ced9c998ec69f4eb55911b031bec8a31ba21d8a5c4ef05597fe2d5abc17d#rd", "authors": ["AIBUPT"], "title": "10/2/2025 AI\u901f\u9012 | \n\n<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6280\u672f\u5bc6\u96c6\u7a81\u7834\uff1a\u5f00\u6e90\u4f18\u5316\u4e0e\u591a\u6a21\u6001\u5e94\u7528\u5e76\u8fdb", "comment": "Source: WeChat, Published: 2025-10-02 14:10:20", "summary": "\u8682\u8681\u96c6\u56e2\u4e8e2025\u5e7410\u67082\u65e5\u6b63\u5f0f\u5f00\u6e90\u5176\u81ea\u4e3b\u7814\u53d1\u7684Ring-1T-preview\u5927\u6a21\u578b\uff0c\u8fd9\u662f\u5168\u7403\u9996\u4e2a\u53c2\u6570\u89c4\u6a21\u8fbe\u5230\u4e07\u4ebf\u7ea7\u522b\u7684\u5f00\u6e90\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u7cfb\u7edf\uff08MoE\uff09\u67b6\u6784\uff0c\u5305\u542b128\u4e2a\u72ec\u7acb\u4e13\u5bb6\u7f51\u7edc\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4e13\u9879\u6d4b\u8bd5\u4e2d\u4ee5HumanEval 87.6\u5206", "AI": {"tldr": "\u8682\u8681\u96c6\u56e2\u4e8e2025\u5e7410\u67082\u65e5\u6b63\u5f0f\u5f00\u6e90\u5176\u81ea\u4e3b\u7814\u53d1\u7684Ring-1T-preview\u5927\u6a21\u578b\uff0c\u8fd9\u662f\u5168\u7403\u9996\u4e2a\u53c2\u6570\u89c4\u6a21\u8fbe\u5230\u4e07\u4ebf\u7ea7\u522b\u7684\u5f00\u6e90\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\u7cfb\u7edf\uff08MoE\uff09\u67b6\u6784\uff0c\u5305\u542b128\u4e2a\u72ec\u7acb\u4e13\u5bb6\u7f51\u7edc\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4e13\u9879\u6d4b\u8bd5\u4e2d\u4ee5HumanEval 87.6\u5206", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
