<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 13]
- [wechat.article](#wechat.article) [Total: 8]
- [cs.SE](#cs.SE) [Total: 12]
- [tldr.article](#tldr.article) [Total: 11]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.LG](#cs.LG) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety](https://arxiv.org/abs/2512.06227)
*Junyu Mao,Anthony Hills,Talia Tseriotou,Maria Liakata,Aya Shamir,Dan Sayda,Dana Atzil-Slonim,Natalie Djohari,Arpan Mandal,Silke Roth,Pamela Ugwudike,Mahesan Niranjan,Stuart E. Middleton*

Main category: cs.CL

TL;DR: 提出CFD框架，通过多LLM代理模拟人类标注员进行细粒度证据交换达成共识，用于数据增强，在两个新数据集上验证效果


<details>
  <summary>Details</summary>
Motivation: 现实世界指标（如心理健康事件、在线安全风险）对NLP任务很重要，但标注成本高且动态变化，需要有效的自动数据增强方法

Method: 提出置信感知细粒度辩论（CFD）框架，多个LLM代理模拟人类标注员，交换细粒度证据达成共识；比较多种LLM数据增强方法

Result: CFD框架在数据增强方面表现最稳健；数据增强持续改善下游任务；通过辩论转录本增强的特征提升最大，在线安全任务上比非增强基线提升10.1%

Conclusion: CFD框架是有效的LLM数据增强方法，能显著提升下游任务性能，特别是在现实世界指标标注困难的情况下

Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.

</details>


### [2] [PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory](https://arxiv.org/abs/2512.06688)
*Bowen Jiang,Yuan Yuan,Maohao Shen,Zhuoqun Hao,Zhangchen Xu,Zichen Chen,Ziyi Liu,Anvesh Rao Vijjini,Jiashu He,Hanchao Yu,Radha Poovendran,Gregory Wornell,Lyle Ungar,Dan Roth,Sihao Chen,Camillo Jose Taylor*

Main category: cs.CL

TL;DR: PersonaMem-v2是当前最先进的LLM个性化数据集，包含1000个真实用户-聊天机器人交互，覆盖300+场景和20000+用户偏好。通过强化微调，Qwen3-4B在隐式个性化任务上超越GPT-5达到53%准确率，而智能记忆框架以16倍更少的输入token实现55%的SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 个性化是AI能力和对齐的下一个重要里程碑。当前前沿LLM在隐式个性化任务上表现不佳（仅37-48%准确率），尽管支持长上下文窗口，但推理能力仍是瓶颈。需要更好的数据集和方法来实现真实世界的个性化智能。

Method: 1) 创建PersonaMem-v2数据集，模拟1000个真实用户-聊天机器人交互，包含300+场景、20000+用户偏好和128k-token上下文窗口；2) 使用强化微调提升模型的长上下文推理能力；3) 开发智能记忆框架，维护单一、人类可读的记忆系统，随用户交互增长。

Result: 1) 前沿LLM在隐式个性化任务上准确率仅37-48%；2) 通过强化微调，Qwen3-4B达到53%准确率，超越GPT-5；3) 智能记忆框架实现55%的SOTA准确率，仅使用2k-token记忆而非完整的32k对话历史，输入token减少16倍。

Conclusion: PersonaMem-v2数据集对个性化研究有重要影响，智能记忆框架为真实世界个性化智能提供了可扩展的路径。强化微调能有效提升模型在长上下文推理和隐式个性化任务上的表现。

Abstract: Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.
  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

</details>


### [3] [Bridging Code Graphs and Large Language Models for Better Code Understanding](https://arxiv.org/abs/2512.07666)
*Zeqi Chen,Zhaoyang Chu,Yi Gui,Feng Guo,Yao Wan,Chuan Shi*

Main category: cs.CL

TL;DR: CGBridge提出了一种即插即用的方法，通过外部可训练的桥接模块将代码图信息注入LLMs，以增强对程序结构语义的理解，在代码智能任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在处理代码智能任务时，依赖线性化token序列，限制了其对程序结构语义的理解能力。先前的研究要么受限于提示长度约束，要么需要特定任务架构修改，与大规模指令跟随LLMs不兼容。

Method: CGBridge首先通过自监督学习在27万代码图数据集上预训练代码图编码器，然后训练外部模块通过跨模态注意力机制对齐代码、图和文本的语义，最后桥接模块生成结构感知提示并注入冻结的LLM中，针对下游代码智能任务进行微调。

Result: CGBridge在代码摘要任务上相对原始模型和图形增强提示方法分别获得16.19%和9.12%的相对增益，在代码翻译任务上分别获得9.84%和38.87%的相对增益。推理速度比LoRA调优模型快4倍以上。

Conclusion: CGBridge是一种有效且高效的结构感知代码理解方法，通过即插即用的桥接模块成功将代码图信息集成到LLMs中，显著提升了代码智能任务的性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.

</details>


### [4] [LLM4SFC: Sequential Function Chart Generation via Large Language Models](https://arxiv.org/abs/2512.06787)
*Ofek Glick,Vladimir Tchuiev,Marah Ghoummaid,Michal Moshkovitz,Dotan Di-Castro*

Main category: cs.CL

TL;DR: LLM4SFC框架首次实现从自然语言描述生成可执行的顺序功能图(SFC)程序，解决了图形化PLC编程语言生成的挑战，在真实工业数据集上达到75%-94%的成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM已用于生成文本型PLC编程语言(如结构化文本)，但IEC 61131-3标准的图形化语言(如顺序功能图SFC)仍未被充分探索。SFC生成面临图形特性和嵌入式ST代码的挑战，传统方法常产生不可执行或与工业工具链不兼容的代码。

Method: LLM4SFC框架包含三个核心组件：1) 精简结构化表示，捕捉SFC拓扑结构和嵌入式ST代码；2) 微调和少样本检索增强生成(RAG)，使模型符合SFC编程规范；3) 结构化生成方法，实时修剪非法标记以确保符合SFC文本格式。

Result: 在自动化制造项目的真实SFC数据集上评估，使用开源和专有LLM，LLM4SFC可靠地生成语法有效的SFC程序，成功率达到75%-94%，有效桥接图形化和文本化PLC语言。

Conclusion: LLM4SFC是首个从自然语言生成可执行SFC的框架，为自动化工业编程铺平道路，成功解决了图形化PLC语言生成的独特挑战。

Abstract: While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.

</details>


### [5] [An Analysis of Large Language Models for Simulating User Responses in Surveys](https://arxiv.org/abs/2512.06874)
*Ziyun Yu,Yiru Zhou,Chen Zhao,Hongyi Wen*

Main category: cs.CL

TL;DR: 本文研究LLM模拟用户意见的能力，发现即使使用CLAIMSIM方法提升回答多样性，LLM仍难以准确模拟不同背景用户的观点，主要存在观点固化、单视角生成和难以处理人口特征细微差异等局限。


<details>
  <summary>Details</summary>
Motivation: LLM被越来越多用于模拟用户意见，但经过RLHF训练的LLM存在偏向主流观点的偏见，这引发了对它们能否代表不同人口和文化背景用户的担忧。

Method: 研究通过直接提示和思维链提示测试LLM模拟人类回答跨领域调查问题的能力，并提出CLAIMSIM方法，从LLM参数知识中提取观点作为上下文输入。

Result: 实验表明，虽然CLAIMSIM能产生更多样化的回答，但两种方法都难以准确模拟用户。分析发现两个关键局限：1）LLM倾向于在不同人口特征下保持固定观点，生成单一视角主张；2）面对冲突主张时，LLM难以推理人口特征的细微差异，限制了根据具体用户画像调整回答的能力。

Conclusion: LLM在模拟用户意见方面存在显著局限，特别是在处理人口特征多样性和观点细微差异方面，需要更先进的方法来提升模拟的准确性和代表性。

Abstract: Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.

</details>


### [6] [DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning](https://arxiv.org/abs/2512.07132)
*Nithin Sivakumaran,Justin Chih-Yao Chen,David Wan,Yue Zhang,Jaehong Yoon,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: DART是一个多智能体框架，通过视觉智能体之间的辩论分歧来识别有用的视觉工具（如物体检测、OCR、空间推理等），利用工具信息解决分歧并促进讨论，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然专用视觉工具可以为大语言模型或视觉语言模型提供专家知识，但确定何时调用哪些工具具有挑战性。现有方法在工具选择和调用时机方面存在不足。

Method: 提出DART多智能体框架：1）多个视觉智能体进行辩论，通过分歧识别有用的视觉工具；2）调用工具引入新信息并提供工具对齐的共识分数；3）使用聚合智能体基于智能体输出和工具信息选择最佳答案。

Result: 在四个不同基准测试中表现优异：在A-OKVQA上比最强基线（带法官模型的多智能体辩论）提升3.4%，在MMMU上提升2.4%，在M3D医疗数据集上比其他基线提升1.3%。分析显示DART产生更丰富的讨论，并能可靠地使用多样化工具解决分歧。

Conclusion: DART通过利用多智能体辩论中的分歧来指导工具调用，有效提升了视觉问答任务的性能，并能很好地适应新工具和应用领域。

Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.

</details>


### [7] [Training Language Models to Use Prolog as a Tool](https://arxiv.org/abs/2512.07407)
*Niklas Mellgren,Peter Schneider-Kamp,Lukas Galke Poech*

Main category: cs.CL

TL;DR: 通过强化学习微调语言模型使用Prolog作为可验证计算工具，提升推理可靠性和可审计性


<details>
  <summary>Details</summary>
Motivation: 语言模型经常产生看似合理但错误的推理，难以验证其可靠性。为确保智能体AI系统的安全性，需要将模型推理建立在可验证的形式化系统上。

Method: 使用GRPO强化学习微调Qwen2.5-3B-Instruct模型，在GSM8K-Prolog-Prover数据集上实验不同提示结构、奖励组合（执行、语法、语义、结构）和推理协议（单次、best-of-N、内部调用Prolog、独立调用Prolog）

Result: 强化学习方法优于监督微调，3B模型在零样本MMLU上达到7B模型少样本性能。最佳配置在GSM8K上获得最高准确率，智能体推理在MMLU-Stem和MMLU-Pro上实现最优零样本泛化

Conclusion: 将模型推理建立在形式化验证系统上能显著提升安全关键应用的可靠性和可审计性，联合优化提示、奖励和推理协议对程序语法和逻辑有重要影响

Abstract: Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference

</details>


### [8] [Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning](https://arxiv.org/abs/2512.07461)
*Tong Wu,Yang Liu,Jun Bai,Zixia Jia,Shuyi Zhang,Ziyong Lin,Yanting Wang,Song-Chun Zhu,Zilong Zheng*

Main category: cs.CL

TL;DR: NPR是一个教师自由的框架，让大语言模型通过自我进化获得真正的并行推理能力，实现从顺序模拟到原生并行认知的转变，在8个推理基准上性能提升达24.5%，推理速度提升达4.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理通常是顺序模拟的，缺乏真正的并行推理能力，导致推理效率低下。需要让模型自我进化出原生并行认知能力，而不是依赖外部监督或教师模型。

Method: 提出三个关键创新：1）自我蒸馏的渐进训练范式，从"冷启动"格式发现过渡到严格拓扑约束；2）并行感知策略优化算法，在执行图中直接优化分支策略；3）稳健的NPR引擎，重构SGLang的内存管理和流程控制。

Result: 在Qwen3-4B模型上训练NPR，在8个推理基准上性能提升达24.5%，推理速度提升达4.6倍，实现100%真正的并行执行，优于依赖自回归解码的基线方法。

Conclusion: NPR为大语言模型自我进化、高效且可扩展的智能推理建立了新标准，实现了从顺序模拟到原生并行认知的转变。

Abstract: We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.

</details>


### [9] [Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization](https://arxiv.org/abs/2512.07478)
*Zhuoran Zhuang,Ye Chen,Jianghao Su,Chao Luo,Luhui Liu,Xia Zeng*

Main category: cs.CL

TL;DR: 该论文提出两种技术（PRS和VSPO）来解决工具集成推理中强化学习面临的两个关键挑战：稀疏奖励和梯度退化问题，在多个QA基准测试中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 工具集成推理（TIR）的LLM通过迭代规划、调用外部工具来解决复杂推理任务，但代理强化学习面临两个关键挑战：1）稀疏的二元验证信号对中间步骤指导有限且收敛慢；2）组相对策略优化中的梯度退化问题，相同奖励导致零优势，降低样本效率和训练稳定性。

Method: 提出两种互补技术：渐进式奖励塑造（PRS）和基于价值的采样策略优化（VSPO）。PRS采用课程式奖励设计，提供密集的阶段反馈，先鼓励模型掌握可解析的工具调用格式，再优化事实正确性和答案质量。VSPO是增强的GRPO变体，用任务价值度量选择样本，应用价值平滑裁剪稳定梯度更新。

Result: 在多个短形式和长形式QA基准测试中，PRS始终优于传统二元奖励，VSPO相比PPO、GRPO、CISPO和SFT基线实现了更好的稳定性、更快的收敛速度和更高的最终性能。两者结合产生了跨领域泛化能力更好的TIR代理。

Conclusion: PRS和VSPO有效解决了TIR中强化学习的关键挑战，通过渐进式奖励设计和改进的采样策略优化，显著提升了LLM代理在复杂推理任务中的性能和训练效率。

Abstract: Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.

</details>


### [10] [Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?](https://arxiv.org/abs/2512.07777)
*Karin de Langis,Püren Öncel,Ryan Peters,Andrew Elfenbein,Laura Kristen Allen,Andreas Schramm,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs能通过内部表征识别不连贯叙事，但在回答评分问题时无法有效区分连贯与不连贯故事，表明其对叙事连贯性的理解存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能够可靠地区分连贯与不连贯的叙事，探索LLMs对故事连贯性的理解能力及其局限性。

Method: 使用配对叙事数据集进行探测研究，分析LLMs的内部表征和生成响应，测试不同提示变体和推理链方法。

Result: LLMs内部表征能可靠识别不连贯叙事，但生成响应无法有效区分；对违反设定的不连贯比对违反角色特质的不连贯更敏感；推理链无法消除这种缺陷。

Conclusion: LLMs对叙事连贯性的理解不完整，更依赖原型世界知识而非基于意义的叙事连贯性构建，内部状态与行为之间存在差距。

Abstract: Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.

</details>


### [11] [On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models](https://arxiv.org/abs/2512.07783)
*Charlie Zhang,Graham Neubig,Xiang Yue*

Main category: cs.CL

TL;DR: 论文通过受控实验框架，揭示了强化学习在语言模型推理能力提升中的真实作用：仅当预训练留有足够空间且RL数据针对模型能力边界时，RL才能产生真正的能力增益。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习技术虽然提升了语言模型的推理能力，但难以确定这种提升是真正扩展了模型能力，还是仅仅利用了预训练中已有的知识。由于现代训练流程缺乏控制（预训练语料不透明、中期训练常被忽视、RL目标与未知先验知识复杂交互），需要开发受控实验框架来厘清预训练、中期训练和RL后训练各自的因果贡献。

Method: 开发了完全受控的实验框架，使用合成推理任务（具有明确的原子操作、可解析的逐步推理痕迹），并系统性地操纵训练分布。评估模型的两个维度：1）外推泛化到更复杂的组合；2）跨表面上下文的上下文泛化。

Result: 1) RL仅在预训练留有足够空间且RL数据针对模型能力边界（困难但尚未超出能力范围的任务）时，才能产生真正的能力增益（pass@128）。2) 上下文泛化需要最小但足够的预训练暴露，之后RL可以可靠地迁移。3) 中期训练在固定计算量下比仅用RL显著提升性能，显示了其在训练流程中的核心但未被充分探索的作用。4) 过程级奖励减少了奖励黑客行为并提高了推理保真度。

Conclusion: 研究阐明了预训练、中期训练和RL之间的相互作用，为理解和改进推理语言模型的训练策略提供了基础。RL的有效性取决于训练阶段间的协调，特别是需要针对模型能力边界的数据。

Abstract: Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.

</details>


### [12] [Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support](https://arxiv.org/abs/2512.07801)
*Raunak Jain,Mudita Khurana*

Main category: cs.CL

TL;DR: 论文提出协作因果意义建构（CCS）作为决策支持代理的研究议程，旨在将AI设计为认知工作的合作伙伴，而非仅仅是工具，以解决当前人机团队表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理在复杂高风险决策环境中未能实现人机互补，人类专家在验证循环和过度依赖之间摇摆，人机团队表现往往不如最佳个体。作者认为这不仅是准确性问题，而是AI辅助概念的根本缺陷——专家决策是通过协作认知过程完成的，而现有系统未能支持这种协作。

Method: 提出协作因果意义建构（CCS）作为研究议程和组织框架：设计能够作为认知合作伙伴的系统，维护专家推理的演化模型，帮助阐述和修订目标，共同构建和压力测试因果假设，并从联合决策结果中学习，使人类和代理都能随时间改进。

Result: 论文提出了CCS框架的具体挑战：1）训练生态使协作思考具有工具价值；2）共同构建模型的表示和交互协议；3）以信任和互补性为中心的评价方法。这些方向可以重构多智能体系统研究，使其围绕参与协作意义建构的代理展开。

Conclusion: 需要重新构想AI辅助，从工具转向认知合作伙伴。CCS框架为开发能够与人类伙伴共同思考的AI队友提供了研究议程，强调协作意义建构、共同模型构建和持续学习，以实现真正的人机互补。

Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.

</details>


### [13] [Do Generalisation Results Generalise?](https://arxiv.org/abs/2512.07832)
*Matteo Boglioni,Andrea Sgobbi,Gabriel Tavernini,Francesco Rita,Marius Mosbach,Tiago Pimentel*

Main category: cs.CL

TL;DR: 研究发现LLM的OOD泛化性能在不同测试集间的相关性没有统一趋势，具体取决于模型选择，表明单一OOD数据集评估可能不够准确。


<details>
  <summary>Details</summary>
Motivation: 现有评估LLM泛化能力的研究通常只使用单个OOD数据集，但实际部署中模型会遇到更复杂多样的数据偏移，这种单一评估方法可能无法准确反映模型真实能力。

Method: 在微调过程中评估模型在多个OOD测试集上的性能，然后计算这些测试集性能之间的偏相关系数（控制域内性能的影响），以此评估泛化性能的相关性。

Result: 分析OLMo2和OPT模型发现，OOD泛化结果没有统一的趋势：任意两个OOD测试集之间是否存在正相关或负相关，强烈依赖于具体分析的模型选择。

Conclusion: LLM的OOD泛化性能评估需要更全面的方法，不能仅依赖单一测试集，因为不同模型在不同OOD数据集上的表现相关性模式各异。

Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [14] [一文看全！DeepSeek<em class="highlight">大模型</em>架构](http://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&mid=2247544704&idx=1&sn=3ca130b702b7f1e505df3ac39aa2c08f&chksm=ea4877d25b051c9f11046be7d7a3d313643c9f95043023e507a44100c89dfdd7fabc0ba86e59#rd)
*算法进阶*

Main category: wechat.article

TL;DR: 主要模型以红色显示。正如我在九月份所预测的那样，DeepSeek V3.2-Exp 的发布旨在为托管刚刚发布的 V3.2 模型准备生态系统和推理基础设施。V3.2-Exp 和 V3.2 使用了一种非标准的稀疏注意力（Sparse Attention）变体，这需要定制代码，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 主要模型以红色显示。正如我在九月份所预测的那样，DeepSeek V3.2-Exp 的发布旨在为托管刚刚发布的 V3.2 模型准备生态系统和推理基础设施。V3.2-Exp 和 V3.2 使用了一种非标准的稀疏注意力（Sparse Attention）变体，这需要定制代码，

</details>


### [15] [【专家报告】赵俊华教授：<em class="highlight">大模型</em>智能体在电力系统中的应用：人机协同与范式变革](http://mp.weixin.qq.com/s?__biz=Mzg3MjYwMDI4MA==&mid=2247569750&idx=2&sn=ca710002a2d01e09f2ed0c466ee6d54e&chksm=cf9d4472fadcb4b9f9367464226063aab0baac5efc5291cd1f70b4d6a860d73bcfc1f4afc9d3#rd)
*南方电网技术*

Main category: wechat.article

TL;DR: 在会上，香港中文大学（深圳）赵俊华教授作了题为“ 大模型智能体在电力系统中的应用：人机协同与范式变革”的专题报告，征得赵教授同意，特与您分享！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在会上，香港中文大学（深圳）赵俊华教授作了题为“ 大模型智能体在电力系统中的应用：人机协同与范式变革”的专题报告，征得赵教授同意，特与您分享！

</details>


### [16] [智慧农业的新引擎：解密农业垂直<em class="highlight">大模型</em>的落地与未来‌‌](http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498344&idx=1&sn=e656c4aae0b2c1621a7805a7aa04c5e2&chksm=fd37e3802e21e971f843a7e06233b08b5bcd1d6a7852270617760cba9961528488d0224943d5#rd)
*丰农信息*

Main category: wechat.article

TL;DR: 成功案例： 中国农业大学的 “神农大模型3.0” 是这一领域的标杆。它不仅开源，覆盖了90%的农业学科和80%的农业场景，还配套推出了包含36个专业智能体的平台 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 成功案例： 中国农业大学的 “神农大模型3.0” 是这一领域的标杆。它不仅开源，覆盖了90%的农业学科和80%的农业场景，还配套推出了包含36个专业智能体的平台 。

</details>


### [17] [AI学习（第二期）| 大语言<em class="highlight">模型</em>架构：从三种主流架构到技术演进](http://mp.weixin.qq.com/s?__biz=MzE5ODU5Mzg1Mg==&mid=2247484017&idx=3&sn=15e8005db71113fd12e5680cd9081ddd&chksm=97408740b5805d8d1a7461df6b30366cf04f169b95a16553abf265f9eb34c552d50f616d4623#rd)
*中国AOPA无人机应用技能*

Main category: wechat.article

TL;DR: 我们可以把大语言模型理解为一个极其聪明的“文本处理器”。根据它处理任务的方式不同，主要分为三种架构：Encoder-only 架构（只编码，擅长“理解”）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们可以把大语言模型理解为一个极其聪明的“文本处理器”。根据它处理任务的方式不同，主要分为三种架构：Encoder-only 架构（只编码，擅长“理解”）

</details>


### [18] [必藏干货！2025 最新最全<em class="highlight">大模型</em>学习资源包（粉丝专享版）](http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493923&idx=1&sn=59fb3dd5426ee50b1723ba1b51630e4e&chksm=fafd9062a7a225ec3dd6076bd830a4711f00cf7ccfd385af2312a29dc3dbd31e82fd7e93520d#rd)
*慕容千语*

Main category: wechat.article

TL;DR: 资料名称：大模型代码文档资料网盘链接：https：//pan.baidu.com/s/1yyw3KhO2q3lLCli1nJGt3Q？pwd=ss45 提取码： ss45资料包含：Tansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/智能客服等项目源码，面试题，电子书籍，吴恩达&李飞飞论文，白皮书等，Gi


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 资料名称：大模型代码文档资料网盘链接：https：//pan.baidu.com/s/1yyw3KhO2q3lLCli1nJGt3Q？pwd=ss45 提取码： ss45资料包含：Tansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/智能客服等项目源码，面试题，电子书籍，吴恩达&李飞飞论文，白皮书等，Gi

</details>


### [19] [【粉丝专享】2025年最新最全的<em class="highlight">大模型</em>学习资源包！！](http://mp.weixin.qq.com/s?__biz=MzY0MDE5NDgxOA==&mid=2247483935&idx=1&sn=90d47dd8f60d6b8d8302bc27086fc550&chksm=f16dd88a7fb88ebb0b74522eb67e95d3faf48f32dd9fc9cde1abf0636454f22c3bc14a595b51#rd)
*AI大模型小明*

Main category: wechat.article

TL;DR: 资料下载资料名称：大模型代码文档资料网盘链接：https：//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA？pwd=itn5 提取码： itn5资料包含：Tansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/智能客服等项目源码，面试题，电子书籍，吴恩达&李飞飞论文，白


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 资料下载资料名称：大模型代码文档资料网盘链接：https：//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA？pwd=itn5 提取码： itn5资料包含：Tansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/智能客服等项目源码，面试题，电子书籍，吴恩达&李飞飞论文，白

</details>


### [20] [【粉丝专属福利】2025年最新最全的<em class="highlight">大模型</em>学习资源包！！](http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247486266&idx=1&sn=7ab6dfbc010c867ced3d21a0f50e43b8&chksm=e88773fcd4a14cbb099d56594ca719ecc690e816dab22e26d27a497052787105aaa4b255a9b8#rd)
*懂点AI的海文*

Main category: wechat.article

TL;DR: 资料下载资料名称：大模型代码文档资料网盘链接：https：//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA？pwd=itn5 提取码： itn5资料包含：Tansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/智能客服等项目源码，面试题，电子书籍，吴恩达&李飞飞论文，白


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 资料下载资料名称：大模型代码文档资料网盘链接：https：//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA？pwd=itn5 提取码： itn5资料包含：Tansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/智能客服等项目源码，面试题，电子书籍，吴恩达&李飞飞论文，白

</details>


### [21] [全球顶级<em class="highlight">大模型</em>最新排名出炉：中国<em class="highlight">大模型</em>表现优秀，DeepSeek V3.2 与 Kimi K2 Thinking 均挤进前 10](http://mp.weixin.qq.com/s?__biz=MzkzMTcxNzU3OA==&mid=2247489366&idx=1&sn=7953af78ad8cd8b940cab7d9fd8b879d&chksm=c35a61520e158156f3a3b231e5917f5859a591ea23e491ac3b7f3397d641b264a219ed0399c4#rd)
*AI信息风向*

Main category: wechat.article

TL;DR: 根据 Artificial Analysis网站的最新基准测试（截至2025年12月），全球顶级大模型在 Intelligence（智力指数）、Speed（速度）和 Price（价格）三大维度上的表现日趋分明。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 根据 Artificial Analysis网站的最新基准测试（截至2025年12月），全球顶级大模型在 Intelligence（智力指数）、Speed（速度）和 Price（价格）三大维度上的表现日趋分明。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

TL;DR: Auto-SPT是一个基于大语言模型自动生成代码语义保持变换的框架，用于增强代码克隆检测模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有的代码克隆检测模型主要在干净、结构化的代码数据集上训练，但真实世界的代码会经历各种语义保持的变换（如重构、压缩、格式化、编译器优化），导致训练数据和测试数据之间存在关键差距

Method: 提出Auto-SPT框架，利用大语言模型自动构建合成数据生成器：1) 设计多样化的语义保持变换(SPTs)；2) 为这些SPTs生成强实现；3) 组合这些变换形成强变换。形式化分析显示SPT的多样性影响组合强度

Result: Auto-SPT比现有方法生成更多样化的SPTs，这些SPTs显著降低了最先进代码克隆检测器的性能。实验表明Auto-SPT可用于增强训练数据集，产生对真实世界对抗性代码变换具有鲁棒性的代码克隆检测模型

Conclusion: Auto-SPT通过自动生成多样化的语义保持变换，有效解决了代码克隆检测中训练-测试数据差距问题，并能增强模型的鲁棒性

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [23] [Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework](https://arxiv.org/abs/2512.06046)
*Ramprasath Ganesaraja,Swathika N,Saravanan AP,Kamalkumar Rathinasamy,Chetana Amancharla,Rahul Das,Sahil Dilip Panse,Aditya Batwe,Dileep Vijayan,Veena Ashok,Thanushree A P,Kausthubh J Rao,Alden Olivero,Roshan,Rajeshwar Reddy Manthena,Asmitha Yuga Sre A,Harsh Tripathi,Suganya Selvaraj,Vito Chin,Kasthuri Rangan Bhaskar,Kasthuri Rangan Bhaskar,Venkatraman R,Sajit Vijayakumar*

Main category: cs.SE

TL;DR: AI4UI是一个面向企业级应用交付的自主前端开发框架，专注于生产就绪性，通过设计阶段嵌入Gen-AI友好语法和后处理阶段专家精修，实现安全、可扩展、合规且可维护的UI代码自动生成。


<details>
  <summary>Details</summary>
Motivation: 现有通用代码助手主要面向快速原型开发，无法满足企业级应用对安全性、可扩展性、合规性和可维护性的严格要求。企业需要能够无缝集成到现有工作流程中，并确保生产就绪的UI代码生成解决方案。

Method: 采用目标导向的人机协同方法：设计阶段在Figma原型中嵌入Gen-AI友好语法编码需求；后处理阶段由领域专家进行精修；中间阶段完全自主运行。技术贡献包括Figma语法解释器、领域感知知识图谱、安全抽象/包代码集成策略、专家驱动的架构模板，以及由专门代理角色协调的变更导向工作流。

Result: 在大规模基准测试中，AI4UI实现了97.24%的平台兼容性、87.10%的编译成功率、86.98%的安全合规性、78.00%的功能实现成功率、73.50%的代码审查质量和73.36%的UI/UX一致性。在200名专家评估者的盲选偏好研究中，AI4UI成为领先解决方案之一。

Conclusion: AI4UI能够异步生成数千个经过验证的UI界面，将交付时间从数月压缩到数周，展示了在企业级前端开发中的强大竞争力和实际应用价值。

Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline

</details>


### [24] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

TL;DR: 提出一个结合强化学习与自主代理的框架，通过质量工程反馈持续改进从业务需求文档自动生成软件测试用例的能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于大型语言模型的系统从静态知识库生成测试用例，无法随时间持续提升性能。需要一种能够从质量工程反馈中学习并持续改进的自动化测试生成方法。

Method: 提出强化学习增强的代理式RAG框架，结合专门代理与混合向量-图知识库，使用PPO和DQN算法根据测试有效性、缺陷检测率和工作流指标优化代理行为。

Result: 在企业级Apple项目上验证，测试生成准确率从94.8%提升至97.2%（提高2.4%），缺陷检测率提升10.8%。

Conclusion: 该框架建立了由质量工程专业知识驱动的持续知识优化循环，产生逐步提升的测试用例质量，增强而非取代人工测试能力。

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [25] [DUET: Agentic Design Understanding via Experimentation and Testing](https://arxiv.org/abs/2512.06247)
*Gus Henry Smith,Sandesh Adhikary,Vineet Thumuluri,Karthik Suresh,Vivek Pandit,Kartik Hegde,Hamid Shojaei,Chandra Bhagavatula*

Main category: cs.SE

TL;DR: DUET提出了一种通过实验和测试来理解硬件设计的方法，模仿硬件专家通过迭代实验理解复杂设计的过程，显著提升了AI代理在形式验证任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的AI代理在处理复杂软件工程任务时表现出色，但在硬件设计任务上表现不佳。RTL代码使用SystemVerilog等低级语言特征编码复杂、动态、时间演化的行为，LLM仅从RTL语法难以推断这些复杂行为，限制了其在代码补全、文档生成和验证等下游任务的能力。

Method: DUET采用设计理解通过实验和测试的方法，模仿硬件设计专家理解复杂设计的方式：不是一次性阅读RTL，而是通过使用多种工具进行迭代实验。该方法迭代生成假设，使用EDA工具（如仿真、波形检查和形式验证）进行测试，并整合结果以构建自底向上的设计理解。

Result: 评估显示，DUET相比没有实验的基线流程，显著提高了AI代理在形式验证任务上的性能。

Conclusion: DUET通过模仿硬件专家的实验性理解方法，为AI代理处理复杂硬件设计任务提供了一种有效的解决方案，特别是在形式验证等下游任务上表现出显著改进。

Abstract: AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.

</details>


### [26] [CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models](https://arxiv.org/abs/2512.06248)
*Cheng Cheng,Jinqiu Yang*

Main category: cs.SE

TL;DR: CFCEval是一个评估代码生成LLM质量和安全性的新框架，通过MLVBench基准和ELRM度量解决现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM评估协议缺乏方法严谨性和全面性，存在数据集偏差问题，且广泛使用的CodeBLEU度量存在标记化不精确、结构限制和参考多样性低等关键缺陷。

Method: 提出CFCEval框架，通过创建新的基准MLVBench来缓解数据集偏差，并引入ELRM度量来评估参考代码与生成代码之间的相关性。从编程质量、漏洞修复能力、后转换修复能力和相关性四个维度评估生成代码。

Result: 实验表明CFCEval能更有效地捕捉生成代码的质量和安全方面，其ELRM度量比CodeBLEU更接近人类判断。

Conclusion: CFCEval为代码LLM评估的未来发展铺平了道路，提供了更全面和可靠的评估框架。

Abstract: Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.

</details>


### [27] [LLMCFG-TGen: Using LLM-Generated Control Flow Graphs to Automatically Create Test Cases from Use Cases](https://arxiv.org/abs/2512.06401)
*Zhenzhen Yang,Chenhui Cui,Tao Li,Rubing Huang,Nan Niu,Dave Towey,Shikai Guo*

Main category: cs.SE

TL;DR: 提出LLMCFG-TGen方法，利用LLM从自然语言用例生成控制流图，再枚举完整执行路径来生成测试用例，提高测试覆盖率和完整性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的测试生成方法存在覆盖不全面、冗余、难以处理复杂条件逻辑的问题，需要更系统的方法从自然语言需求生成高质量的测试用例。

Method: LLMCFG-TGen包含三个步骤：1) LLM将自然语言用例转换为结构化的控制流图；2) 探索CFG并枚举所有完整执行路径；3) 基于执行路径生成测试用例。

Result: 实验表明LLM能有效从自然语言用例构建结构良好的CFG，相比基线方法，LLMCFG-TGen实现了完整路径覆盖，提高了测试完整性，减少了人工工作量。

Conclusion: 将LLM的语义推理能力与结构化建模相结合，能有效弥合自然语言需求与系统化测试生成之间的差距，生成逻辑一致且全面的测试用例。

Abstract: Appropriate test case generation is critical in software testing, significantly impacting the quality of the testing. Requirements-Based Test Generation (RBTG) derives test cases from software requirements, aiming to verify whether or not the system's behaviors align with user needs and expectations. Requirements are often documented in Natural Language (NL), with use-case descriptions being a popular method for capturing functional behaviors and interaction flows in a structured form. Large Language Models (LLMs) have shown strong potential for automating test generation directly from NL requirements. However, current LLM-based approaches may not provide comprehensive, non-redundant coverage. They may also fail to capture complex conditional logic in requirements, resulting in incomplete test cases. We propose a new approach that automatically generates test cases from NL use-case descriptions, called Test Generation based on LLM-generated Control Flow Graphs (LLMCFG-TGen). LLMCFG-TGen comprises three main steps: (1) An LLM transforms a use case into a structured CFG that encapsulates all potential branches; (2) The generated CFG is explored, and all complete execution paths are enumerated; and (3) The execution paths are then used to generate the test cases. To evaluate our proposed approach, we conducted a series of experiments. The results show that LLMs can effectively construct well-structured CFGs from NL use cases. Compared with the baseline methods, LLMCFG-TGen achieves full path coverage, improving completeness and ensuring clear and accurate test cases. Practitioner assessments confirm that LLMCFG-TGen produces logically consistent and comprehensive test cases, while substantially reducing manual effort. The findings suggest that coupling LLM-based semantic reasoning with structured modeling effectively bridges the gap between NL requirements and systematic test generation.

</details>


### [28] [Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models](https://arxiv.org/abs/2512.06448)
*Takaaki Tateishi,Yasuharu Katsuno*

Main category: cs.SE

TL;DR: 提出模板化方法，通过符号执行生成代码模板作为中间表示，帮助LLM将PL/I宏过程翻译为可维护的Java代码


<details>
  <summary>Details</summary>
Motivation: 企业系统现代化需要将PL/I程序翻译为Java等现代语言，但PL/I宏过程作为字符串操作程序生成PL/I代码，使自动翻译变得复杂。现有LLM方法难以将PL/I宏过程翻译为能重现原始PL/I代码行为的Java程序

Method: 提出模板化方法：1) 使用符号执行生成代码模板（带有命名占位符的代码）作为中间表示；2) 将符号值视为宏生成代码的一部分；3) 通过符号执行宏过程生成代码模板，帮助LLM生成可读可维护的Java代码

Result: 在10个PL/I宏过程上的初步实验表明，通过模板化的LLM翻译成功生成了能重现宏生成PL/I程序行为的Java程序

Conclusion: 模板化方法通过符号执行生成代码模板作为中间表示，有效解决了LLM在翻译PL/I宏过程时的困难，能够生成行为正确的可维护Java代码

Abstract: Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.
  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.

</details>


### [29] [BabelCoder: Agentic Code Translation with Specification Alignment](https://arxiv.org/abs/2512.06902)
*Fazle Rabbi,Soumit Kanti Saha,Tri Minh Triet Pham,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: BabelCoder是一个基于多智能体协作的代码翻译框架，通过翻译、测试和精炼三个专门化智能体的分工合作，显著提升了跨语言代码迁移的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统演进，开发者需要在多种编程语言间工作，经常需要将代码从一种语言迁移到另一种语言。虽然自动代码翻译提供了有前景的解决方案，但长期以来一直是一个具有挑战性的任务。现有的大语言模型方法在准确性上仍有局限，未能有效利用代码中的上下文和结构线索，且缺乏结构化的多智能体协作框架。

Method: BabelCoder是一个智能体框架，将代码翻译任务分解为三个专门化智能体：翻译智能体负责生成代码，测试智能体负责验证正确性，精炼智能体负责修复错误。这些智能体协作工作，通过分工合作提高翻译质量。

Result: 在四个基准数据集上与四种最先进基线方法比较，BabelCoder在94%的情况下优于现有方法，提升幅度为0.5%-13.5%，平均准确率达到94.16%。

Conclusion: BabelCoder通过多智能体协作框架有效解决了代码翻译中的准确性问题，显著提升了跨语言代码迁移的质量。

Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.

</details>


### [30] [Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering](https://arxiv.org/abs/2512.06915)
*Kelin Fu,Tianyu Liu,Zeyu Shang,Yingwei Ma,Jian Yang,Jiaheng Liu,Kaigui Bian*

Main category: cs.SE

TL;DR: Multi-Docker-Eval 是一个用于评估自动化环境配置任务的基准测试，包含40个真实仓库和9种编程语言。评估发现当前LLMs成功率低（最高37.7%），环境构建是主要瓶颈，模型大小和推理长度不是决定性因素，开源模型表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 自动化环境配置是扩展软件工程自动化的关键瓶颈，需要可靠的评估标准来衡量模型在真实约束下的执行成功率和效率。

Method: 创建Multi-Docker-Eval基准测试，包含40个真实世界仓库和9种编程语言，测量实现可执行状态的成功率以及在现实约束下的效率。对最先进的LLMs和代理框架进行广泛评估。

Result: 1) 当前模型整体成功率低（F2P最高37.7%），环境构建是主要瓶颈；2) 模型大小和推理长度不是决定性因素，开源模型如DeepSeek-V3.1和Kimi-K2在效率和效果上都有竞争力；3) 代理框架和编程语言对成功率有显著影响。

Conclusion: 这些发现为构建可扩展的、完全自动化的软件工程流水线提供了可行的指导方针。

Abstract: Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.

</details>


### [31] [Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization](https://arxiv.org/abs/2512.07022)
*Genevieve Caumartin,Glaucia Melo*

Main category: cs.SE

TL;DR: LLM驱动的代理通过轻量级查询重构和摘要技术，在文件级bug定位中实现了35%的排名提升，优于传统IRBL方法和SWE-agent。


<details>
  <summary>Details</summary>
Motivation: 传统基于信息检索的bug定位方法依赖未改变的bug描述，常包含噪声信息导致检索准确性差。LLM在查询重构方面有所改进，但对代理性能的影响尚未探索。

Method: 使用开源、未微调的LLM从bug报告中提取关键信息（如标识符和代码片段），进行检索前查询重构。代理随后使用这些预处理查询编排BM25检索，实现规模化自动化定位工作流。

Result: 使用最佳查询重构技术，代理在首次文件检索中比BM25基线获得35%的排名提升，文件检索性能比SWE-agent高出22%。

Conclusion: LLM驱动的代理通过轻量级查询重构和摘要技术，能够显著改善文件级bug定位性能，为大规模软件仓库中的bug定位提供了有效解决方案。

Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.

</details>


### [32] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

TL;DR: 该论文探索了LLM内部是否编码了代码正确性的表示，通过对比正确和错误代码的隐藏状态来提取这种表示，并利用它来提升代码生成质量，无需执行测试。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面很有效，但经常输出错误代码。一个原因是模型输出概率与正确性相关性不强，且只反映生成过程的最终输出。受LLM内部编码"真实性"等概念的启发，本文探索LLM是否类似地表示代码正确性。

Method: 通过对比同一编程任务下正确和错误代码对的隐藏状态，识别LLM内部的正确性表示。在四个LLM上进行实验，利用提取的正确性表示来改进代码选择。

Result: 利用提取的内部正确性表示优于标准的对数似然排序和语言化模型置信度。这种内部正确性信号可用于选择更高质量的代码样本，无需执行测试。

Conclusion: 这项工作展示了如何利用内部表示来增强代码生成系统，使LLM更可靠，从而提高对自动生成代码的信心。

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [33] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

TL;DR: AutoICE使用LLM驱动的进化搜索合成可验证的C代码，通过多样个体初始化、协作交叉和自反思变异来减少错误传播并发现隐含知识，在代码验证成功率上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 从自然语言需求自动合成可验证代码能确保软件正确性和可靠性，降低形式化方法的采用门槛。现有方法由于领域特定预训练数据稀缺，存在严重的语法和语义错误，且难以有效形式化隐含知识。

Method: 提出AutoICE，一种LLM驱动的进化搜索方法，包含：1) 多样个体初始化实现多样迭代更新；2) 协作交叉减少单智能体迭代中的错误传播；3) 自反思变异促进发现隐含知识。

Result: AutoICE成功验证了90.36%的代码，优于现有最佳方法。在开发者友好的数据集变体上，达到88.33%的验证成功率，显著超越现有方法的65%。

Conclusion: AutoICE通过进化搜索策略有效解决了LLM在代码合成中的错误传播问题，显著提升了从自然语言需求合成可验证代码的成功率。

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [34] [Architecting efficient context-aware multi-agent framework for production](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Farchitecting-efficient-context-aware-multi-agent-framework-for-production%2F%3Futm_source=tldrai/1/0100019aeef6a1be-c5707be7-2ccc-4e6f-9616-fe892eab8ed9-000000/qS2Chm9Arm4FaEQ0vfOjJ-ulO_oMKAr5ngBQvvYYOac=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开源多智能体框架通过上下文工程解决AI智能体开发中的上下文瓶颈问题，支持生产环境中的长时程任务处理


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体开发面临上下文瓶颈，组织在部署处理长时程任务的复杂自主智能体时，上下文管理成为主要限制因素

Method: 基于Google Agent Development Kit的上下文栈，开发开源的多智能体原生框架，实现主动上下文工程

Result: 构建了一个支持上下文感知的多智能体框架，使主动上下文工程变得可行，解决了生产环境中的上下文瓶颈问题

Conclusion: 通过上下文工程驱动的多智能体框架能够有效解决AI智能体开发中的上下文限制，推动复杂自主智能体在生产环境中的部署

Abstract: Architecting efficient context-aware multi-agent framework for production (17 minute read) The landscape of AI agent development is shifting fast. Organizations are now deploying sophisticated, autonomous agents to handle long-horizon tasks. However, this ambition is being bottlenecked by context. The context stack in Google Agent Development Kit was developed to support context engineering. The open-source, multi-agent-native framework is built to make active context engineering achievable i...

</details>


### [35] [Why We Built “BlaBlaCar Data Copilot”: Shifting Data Analysis Left](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FPhtq3h/1/0100019afda5395a-c3bdafbc-7a92-4ae7-b877-cf8766adf713-000000/UjJ-8am0jb_iaoGsy2lEB-yX8BvUIopRNv98YTAOW28=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: BlaBlaCar Data Copilot是一款AI驱动的IDE扩展，让软件工程师能在编码环境中直接进行数据分析，减少对BI控制台的依赖和组织孤岛。


<details>
  <summary>Details</summary>
Motivation: 传统数据分析需要依赖BI控制台，导致组织孤岛和流程延迟。作者希望让软件工程师在开发早期就能进行数据分析，实现"左移"的数据分析流程。

Method: 开发AI驱动的IDE扩展，提供业务特定的精选查询、透明的SQL/Python代码生成、启发式数据健康卡等功能，强制实施分析最佳实践和单元测试。

Result: 创建了Data Copilot工具，使软件工程师能在编码环境中直接进行数据分析，减少对BI控制台的依赖，打破组织孤岛。

Conclusion: 通过将数据分析"左移"到开发环境中，可以提高分析效率，减少组织孤岛，并更早地实施分析最佳实践。

Abstract: Why We Built “BlaBlaCar Data Copilot”: Shifting Data Analysis Left (6 minute read) Data Copilot is an AI-driven IDE extension that empowers software engineers to perform data analysis directly in their coding environment. It eliminates reliance on BI consoles and reduces organizational silos. Featuring business-specific curated queries, transparent SQL/Python code generation, and heuristic Data Health Cards, Data Copilot enforces analytics best practices and unit testing earlier in the cycle....

</details>


### [36] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FogYaHh/2/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/YE7snMXV5ufV2f-Thc9HCCWyAauQz5OlJMBBYaGP84I=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Databricks推出Agent Bricks产品，帮助企业构建可实际部署到生产环境的AI智能体，提供自动评估、定制化指标和人工反馈机制，确保智能体的准确性、可靠性和数据基础。


<details>
  <summary>Details</summary>
Motivation: 当前大多数AI智能体无法真正投入生产环境，存在准确性和可靠性问题。企业需要能够实际工作的智能体解决方案，而不仅仅是理论上的概念验证。

Method: 通过Agent Bricks平台提供：1）自动评估系统；2）根据企业目标定制的性能指标；3）人工反馈机制持续改进准确性；4）基于企业实际业务任务的性能测量，而非通用基准测试。

Result: 能够构建出准确、可靠、基于企业数据的生产级智能体，解决传统智能体无法投入实际生产的问题。

Conclusion: Agent Bricks通过实用导向的方法帮助企业构建真正可投入生产的AI智能体，强调实际业务价值而非理论性能。

Abstract: Agents that don't suck (Sponsor) Most AI agents never reach production. Agent Bricks by Databricks fixes that by helping you build agents that actually work — accurate, reliable and grounded in your data. This product delivers practical quality, not hype: automatic evaluation, metrics tuned to your goals, and human feedback to keep improving accuracy. Generic benchmarks don't cut it. Agent Bricks measures performance on the tasks that matter to your business. The result: production agents you...

</details>


### [37] [How I keep up with AI-generated PRs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.raf.xyz%2Fblog%2F03-how-i-keep-up-with-ai-generated-prs%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/NS6nG_tw2sSgxnOiNDZssE_B9smz-ZCYWe97Szv3N4k=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者提出一个工作流，利用AI总结代码变更并识别潜在问题，显著减少AI生成PR的审查时间，同时让开发者保持最终控制权。


<details>
  <summary>Details</summary>
Motivation: AI能快速生成代码，但这些代码仍需人工审查。现有自动化PR工具不足，需要更高效的工作流来应对AI生成代码的审查挑战。

Method: 开发一个工作流，利用AI自动总结代码变更、识别潜在问题，同时保持开发者对最终输出的控制权。

Result: 该工作流能显著减少AI生成PR的审查时间，提高审查效率。

Conclusion: 通过AI辅助的工作流，可以在保持开发者控制权的同时，大幅提升AI生成代码的审查效率。

Abstract: How I keep up with AI-generated PRs (3 minute read) AI can generate code really fast, but that code still needs to be reviewed. Automated PR tools can help, but they're not sufficient. This article presents a workflow that can cut the time it takes to review AI-generated PRs significantly. The workflow uses AI to summarize changes and identify potential issues while allowing the developer to stay in control of the final output.

</details>


### [38] [Another DeepSeek moment](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1996538308697137277.html%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/ZoZ06LccycD770FzhPIyeDLm98YaQ3fyx6hRo52wC2A=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 中兴Nubia M153手机在操作系统层面集成了字节跳动的DoubaoAI智能体，该智能体能完全控制手机，执行UI识别、应用下载、点击输入、通话和多步骤任务链等操作。


<details>
  <summary>Details</summary>
Motivation: 将AI智能体深度集成到手机操作系统中，实现真正的AI原生手机体验，让AI能够完全控制设备并执行复杂任务。

Method: 采用字节跳动的Doubao模型，这是一个大规模稀疏混合专家模型，支持完整的文本和视觉功能，在操作系统层面直接集成AI智能体。

Result: 展示了手机能够执行多种复杂任务，包括UI识别、应用选择下载、点击输入、通话和多步骤任务链操作，视频演示了手机的实际能力。

Conclusion: 这是AI智能体深度集成到移动设备操作系统的重要进展，展示了AI原生手机的潜力，可能代表智能手机发展的新方向。

Abstract: Another DeepSeek moment (4 minute read) ZTE's Nubia M153 is a smartphone that runs ByteDance's DoubaoAI agent at the OS level. The agent has complete control over the phone. It can see the UI, choose/download apps, tap/type, make calls, and run multi-step task chains. The Doubao model is a massive, sparse Mixture of Experts model with full text and vision support. This thread contains several videos that show what the phone is capable of.

</details>


### [39] [Codacy launched AI Reviewer to boost Dev Experience](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.codacy.com%2Fai-reviewer%2F%3Futm_campaign=31129159-AI%2520Risk%2520Hub%2520%2526%2520AI%2520Reviewer%2520Launch%2520%257C%2520Newsletters%26utm_source=TLDR%26utm_medium=newsletter%26utm_content=RiskHub_AIReviewer/1/0100019afdeb0004-29db0d24-6ef5-486b-9d27-5dc19ce0de40-000000/EWi7If7aKPjh9XAo4QKSVh1qTTQB_RBlF2dSLGcaDHo=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Codacy推出AI Reviewer工具，结合静态分析和上下文感知的代码审查，提升开发体验


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI快速重写代码库，传统扫描工具无法跟上审查需求，需要更智能的代码审查解决方案

Method: 将确定性静态分析与上下文感知的代码审查相结合，捕捉传统扫描器遗漏的问题

Result: 推出AI Reviewer产品，提供更全面的代码质量检查和开发体验提升

Conclusion: AI Reviewer能够有效应对GenAI快速代码生成带来的审查挑战，提升开发效率和质量

Abstract: Codacy launched AI Reviewer to boost Dev Experience (Sponsor) GenAI is rewriting your codebase faster than your devs can review it. Codacy's new AI Reviewer pairs deterministic static analysis with context-aware code reviews that catch issues missed by legacy scanners. See how it works

</details>


### [40] [Migrating 6000 React tests using AI Agents and ASTs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feliocapella.com%2Fblog%2Fai-library-migration-guide%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/BQ8YYi3o4f45AZz14wCfsHtCr2zU_HKD6H4PMIt_Cug=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用AI代理和AST在1周内迁移6000个React测试，从React Testing Library v13升级到v14


<details>
  <summary>Details</summary>
Motivation: 需要将大量React测试（6000+个测试，970个文件）从React Testing Library v13迁移到v14，手动迁移工作量大且耗时

Method: 1. 使用Claude Code创建详细的迁移指南；2. 构建AST代码转换工具处理机械性变更；3. 让Claude以每次10个测试的迭代方式迁移，同时自动运行测试并检查覆盖率

Result: 成功在1周内完成了970个测试文件（6000+测试）的迁移工作

Conclusion: AI代理与AST结合的方法能够高效完成大规模代码迁移任务，显著减少人工工作量

Abstract: Migrating 6000 React tests using AI Agents and ASTs (7 minute read) Elio used Claude Code to migrate 970 test files (6,000+ tests) from React Testing Library v13 to v14 in just one week. Its team had Claude build a detailed migration guide first. They created an AST codemod to handle mechanical changes, then let Claude iteratively migrate 10 tests at a time while automatically running tests and checking coverage.

</details>


### [41] [My Thoughts on Claude Opus 4.5](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mckaywrigley.com%2Fposts%2Fopus-4.5%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/0ZpjKrpZv8zCQHHV6EXjufXi1WG0ggrT-ImCQhu994g=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Opus 4.5是AI领域的突破性进展，在代码和智能体方面表现最佳，应作为可信赖的同事使用


<details>
  <summary>Details</summary>
Motivation: 介绍Claude Opus 4.5作为AI领域的重大突破，强调其在代码和智能体方面的卓越能力，旨在指导用户如何充分利用这一先进模型

Method: 通过分析Claude Opus 4.5的性能特点，提出具体使用建议：作为可信赖的同事、使用Claude Code进行编程、利用SDK构建有影响力的智能体

Result: Claude Opus 4.5被定位为AI领域的"世代解锁"，在代码和智能体任务上表现最佳，为用户提供了强大的AI协作工具

Conclusion: Claude Opus 4.5代表了AI技术的重大进步，用户应将其作为可信赖的协作伙伴，充分利用其在编程和智能体开发方面的优势

Abstract: My Thoughts on Claude Opus 4.5 (10 minute read) Claude Opus 4.5 is a generational "unlock" in AI. It is the best model for both code and agents. Users should treat Opus 4.5 as a trusted coworker, use Claude Code for programming, and build impactful agents using the SDK.

</details>


### [42] [From 10 to 31 Tasks Daily: The Agent Inflection Point](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftomtunguz.com%2Fagent-asana-inflection%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/b6ACZ0MKcB3eld_8eSEQ9idCEyInIHYQIuBKttiAVN0=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理工具使企业能像软件工程师一样完成日常任务，从每天10个任务提升到31个，实现工作产出倍增


<details>
  <summary>Details</summary>
Motivation: 企业希望提高工作效率，实现任务完成量的显著增长，同时不增加工作时间。软件工程师已经通过AI代理工具提升了生产力，现在企业也需要类似的能力

Method: 为AI代理提供合适的工具，使其能够并行处理多个工作流，从而同时完成更多任务

Result: 企业能够将每日任务完成量从10个提升到31个，实现工作产出的倍增，且无需延长工作时间

Conclusion: AI代理的拐点已经到来，通过合适的工具配置，企业可以像软件工程师一样大幅提升工作效率

Abstract: From 10 to 31 Tasks Daily: The Agent Inflection Point (2 minute read) The velocity of work changes instantly when we give agents the right tools. Businesses can now use agents to complete tasks, just like software engineers have been doing for the last several months. It is now possible to spin up multiple parallel workstreams, multiplying daily output without working longer hours. The agent inflection point is already here.

</details>


### [43] [Protaigé](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.protaige.com%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/HMcouxPej7HknWGff5ZH9-iiEMmJ_32Y3-8GhylolQE=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Protaigé是一个AI平台，通过协调的智能体工作流创建完整、符合品牌形象的营销活动


<details>
  <summary>Details</summary>
Motivation: 传统营销活动创建过程复杂、耗时且难以保持品牌一致性，需要自动化工具来提升效率和效果

Method: 采用协调的智能体工作流架构，多个专业智能体分工协作，共同完成营销活动的各个组成部分

Result: 能够自动生成完整的营销活动，包括内容创作、视觉设计、渠道规划等，保持品牌一致性

Conclusion: Protaigé展示了智能体工作流在营销自动化中的潜力，能够显著提升营销活动的创建效率和质量

Abstract: Protaigé (Tool) Protaigé is an AI platform that creates complete, on-brand marketing campaigns using coordinated agent workflows.

</details>


### [44] [Critical flaws found in AI development tools dubbed an 'IDEsaster' — data theft and remote code execution possible](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomshardware.com%2Ftech-industry%2Fcyber-security%2Fresearchers-uncover-critical-ai-ide-flaws-exposing-developers-to-data-theft-and-rce%3Futm_source=tldrinfosec/1/0100019afe4b61fb-2aca89ac-1354-4c6d-a19f-a61c55019f16-000000/Lo3LyLKRbb-hEKdovyIIpKflHoEP-7EvCjM4M1FP6OQ=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究发现主流AI集成IDE存在严重安全漏洞，攻击者可通过提示注入劫持AI代理，导致数据窃取和远程代码执行


<details>
  <summary>Details</summary>
Motivation: 随着AI集成开发环境（IDE）的普及，其安全性问题日益凸显。本研究旨在系统评估主流AI集成IDE的安全漏洞，揭示潜在的数据窃取和远程代码执行风险。

Method: 进行了为期六个月的调查，测试了包括GitHub Copilot、Cursor、Claude Code和JetBrains产品在内的所有主流AI集成IDE，通过提示注入攻击链利用规则文件、README或恶意MCP服务器来劫持AI代理。

Result: 在所有测试的AI集成IDE中发现了超过30个漏洞，导致至少24个CVE编号被分配。攻击链能够成功劫持AI代理并滥用合法IDE功能，实现数据窃取和远程代码执行。

Conclusion: AI集成IDE存在严重的安全漏洞，需要加强安全防护措施，特别是在提示注入防御和AI代理安全控制方面。

Abstract: Critical flaws found in AI development tools dubbed an 'IDEsaster' — data theft and remote code execution possible (3 minute read) A six-month investigation uncovered over 30 vulnerabilities across every major AI-integrated IDE tested, including GitHub Copilot, Cursor, Claude Code, and JetBrains products, resulting in at least 24 assigned CVEs. The attack chain exploits prompt injection via rule files, READMEs, or malicious MCP servers to hijack AI agents, which then abuse legitimate IDE feat...

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals](https://arxiv.org/abs/2512.05998)
*Michael Todasco*

Main category: cs.AI

TL;DR: 通过虚构预测市场让LLM用虚拟货币下注，将评估任务转化为赌博游戏，虽然准确率提升有限，但显著产生了可校准的置信度信号。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估其他模型时通常缺乏置信度表示，需要探索如何让LLM表达内部置信度并提高预测准确性。

Method: 设计数学和逻辑问题，让基线模型回答，预测模型在两种条件下预测基线模型是否正确：控制组（简单预测）和激励组（预测+用虚拟货币下注）。

Result: 激励组准确率略有提升（81.5% vs 79.1%），学习速度显著更快；下注金额与置信度相关，大额下注准确率约99%，小额下注约74%。

Conclusion: 赌博机制创造了可读的置信度信号，简单的金融框架可能帮助LLM成为风险感知的预测者，使其内部信念可见可用。

Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.

</details>


### [46] [The Effect of Belief Boxes and Open-mindedness on Persuasion](https://arxiv.org/abs/2512.06573)
*Onur Bilgin,Abdullah As Sami,Sriram Sai Vujjini,John Licato*

Main category: cs.AI

TL;DR: 研究探索了在LLM智能体中引入信念陈述（信念盒）如何影响其行为和说服力，特别是在多智能体辩论场景中，发现信念陈述和开放心态指令会显著影响智能体的信念改变倾向和说服效果。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在推理和决策应用中的增加，需要LLM智能体具备类似命题信念的能力。研究者想知道在提示空间中包含信念陈述（信念盒）如何影响智能体的行为、信念倾向以及在多智能体场景中的说服力，同时探索开放心态指令的影响。

Method: 通过一系列实验探索信念陈述对智能体行为的影响。研究让智能体在提示空间中维护信念陈述（信念盒），测试这些陈述如何影响智能体对相反观点的抵抗力和说服力，并考察开放心态指令的效果，特别是在辩论中处于少数派（同侪压力）场景下的信念改变可能性。

Result: 研究发现：1）开放心态指令确实影响智能体对信念改变的接受程度；2）纳入信念陈述及其强度会影响智能体对相反观点的抵抗力和说服力；3）在辩论中被相反观点包围（同侪压力）时，信念陈述会影响信念改变的可能性。结果证明了信念盒技术在推理和决策任务中的可行性和有效性。

Conclusion: 信念盒技术是有效的，能够显著影响LLM智能体的信念相关行为。开放心态指令和信念陈述的纳入会影响智能体的说服力和信念稳定性，特别是在多智能体辩论场景中。这为构建更复杂的信念推理系统提供了基础。

Abstract: As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.

</details>


### [47] [LightSearcher: Efficient DeepSearch via Experiential Memory](https://arxiv.org/abs/2512.06653)
*Hengzhi Lan,Yue Yu,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Ting Bai*

Main category: cs.AI

TL;DR: LightSearcher是一个高效的强化学习框架，通过结合文本经验记忆和自适应奖励机制，在保持准确性的同时显著减少DeepSearch范式中的工具调用次数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的RL驱动的DeepSearch系统存在准确性与效率之间的权衡问题：频繁调用外部搜索工具可以提高事实准确性，但会导致不必要的计算开销和效率降低。需要一种方法来平衡这种固有的权衡。

Method: 1) 通过对比学习推理轨迹来学习文本经验记忆，生成可解释的成功推理模式摘要；2) 采用自适应奖励塑造机制，仅在正确答案场景中惩罚冗余的工具调用。

Result: 在四个多跳QA基准测试中，LightSearcher保持了与SOTA基线ReSearch相当的准确性，同时将搜索工具调用减少了39.6%，推理时间减少了48.6%，token消耗减少了21.2%。

Conclusion: LightSearcher通过创新的记忆学习和奖励机制设计，有效解决了DeepSearch范式中准确性与效率的权衡问题，实现了更高效的外部知识获取。

Abstract: DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.

</details>


### [48] [Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation](https://arxiv.org/abs/2512.06710)
*Zairah Mustahsan,Abel Lim,Megna Anand,Saahil Jain,Bryan McCann*

Main category: cs.AI

TL;DR: 提出使用组内相关系数(ICC)来评估AI代理系统的稳定性，区分任务难度和代理不一致性，使评估更可靠


<details>
  <summary>Details</summary>
Motivation: 当前AI代理评估只报告单一准确率数字，掩盖了结果背后的方差，无法区分真实能力提升与随机采样运气，导致下游系统脆弱

Method: 采用测量科学中的组内相关系数(ICC)，将观察到的方差分解为查询间方差(任务难度)和查询内方差(代理不一致性)，在GAIA和FRAMES基准上进行评估

Result: ICC随任务结构变化显著：推理和检索任务(FRAMES)ICC=0.4955-0.7118，代理任务(GAIA)ICC=0.304-0.774。结构化任务需要8-16次试验收敛，复杂推理需要≥32次

Conclusion: 建议将准确率与ICC和查询内方差一起作为标准报告实践，更新评估卡片，使评估稳定性可见，将代理基准测试从不透明的排行榜竞争转变为可信赖的实验科学

Abstract: As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.

</details>


### [49] [Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents](https://arxiv.org/abs/2512.06716)
*Zhibo Liang,Tianze Hu,Zaiye Chen,Mingjie Tang*

Main category: cs.AI

TL;DR: 论文提出认知控制架构（CCA），通过意图图和分层裁决器构建双层防御系统，有效抵御间接提示注入攻击，在安全、功能和效率之间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 当前自主LLM代理对间接提示注入攻击存在显著脆弱性，现有防御机制在安全与功能之间存在根本性权衡，导致恶意工具调用和代理目标偏离。现有防御架构分散，无法在整个任务执行流程中提供完整完整性保证。

Method: 提出认知控制架构（CCA），包含两个协同支柱：1）通过预生成的"意图图"实现主动控制流和数据流完整性执行；2）创新的"分层裁决器"，在检测到偏差时基于多维评分启动深度推理，专门应对复杂条件攻击。

Result: 在AgentDojo基准测试中，CCA不仅能有效抵御挑战其他先进防御方法的复杂攻击，还能在不妥协安全性的情况下实现显著效率和鲁棒性，解决了多维权衡问题。

Conclusion: CCA通过全生命周期认知监督框架，实现了对间接提示注入攻击的有效防御，在安全、功能和效率之间达成平衡，解决了现有防御机制的碎片化问题。

Abstract: Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated "Intent Graph"; and (ii) an innovative "Tiered Adjudicator" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.

</details>


### [50] [ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems](https://arxiv.org/abs/2512.06721)
*Bufang Yang,Lilin Xu,Liekang Zeng,Yunqi Guo,Siyang Jiang,Wenrui Lu,Kaiwei Liu,Hancheng Xiang,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: ProAgent：首个端到端主动代理系统，利用多模态感知和LLM推理提供主动协助，在AR眼镜上实现并显著优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要遵循被动范式，依赖用户明确指令启动服务，增加了用户的物理和认知负担。需要一种能够主动感知环境并提供协助的系统。

Method: 1) 采用主动导向的上下文提取方法，通过按需分层感知持续感知环境，提取包含感官和个人特征的分层上下文；2) 使用上下文感知的主动推理器，将上下文映射到用户需求和工具调用，提供主动协助。

Result: 在真实世界测试平台、公共数据集和用户研究中评估，ProAgent比最先进基线实现了：主动预测准确率提高33.4%，工具调用F1分数提高16.8%，用户满意度显著提升。

Conclusion: ProAgent是首个端到端主动代理系统，在AR眼镜上实现并展示了显著性能提升，标志着向主动助手迈出了重要一步。

Abstract: Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.

</details>


### [51] [DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems](https://arxiv.org/abs/2512.06749)
*Ming Ma,Jue Zhang,Fangkai Yang,Yu Kang,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: DoVer是一个基于干预的调试框架，通过主动验证假设（如编辑消息、修改计划）来调试LLM多智能体系统，相比传统日志调试能更有效地修复失败任务。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的多智能体系统调试困难，传统日志调试方法存在两个关键局限：1) 仅基于日志的调试缺乏验证，产生未经测试的假设；2) 单步或单智能体归因往往不准确，因为多个不同的干预可能都能独立修复失败任务。

Method: DoVer是一个干预驱动的调试框架，将假设生成与主动验证相结合，通过有针对性的干预（如编辑消息、修改计划）来验证或反驳失败假设。框架不追求归因准确性，而是关注系统是否能解决失败或向任务成功取得可量化的进展。

Result: 在Magnetic-One智能体框架上，使用GAIA和AssistantBench数据集，DoVer将18-28%的失败试验转为成功，实现高达16%的里程碑进展，验证或反驳30-60%的失败假设。在GSMPlus数据集和AG2框架上，DoVer恢复了49%的失败试验。

Conclusion: 干预是提高智能体系统可靠性的实用机制，为基于LLM的多智能体系统提供了更强大、可扩展的调试方法。DoVer展示了干预驱动调试的有效性。

Abstract: Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.

</details>


### [52] [Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning](https://arxiv.org/abs/2512.06835)
*Tingyu Li,Zheng Sun,Jingxuan Wei,Siyuan Li,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: DoGe提出双解耦框架，通过分离思考者与解决者角色，结合演化课程学习，解决视觉语言模型在专业领域强化学习中的数据稀缺和奖励破解问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通过强化学习实现自我演化，但在化学、地球科学、多模态数学等专业领域面临高质量多模态数据稀缺问题。现有合成数据和自奖励机制存在分布有限和对齐困难，导致奖励破解和训练不稳定。

Method: DoGe采用双解耦框架：1) 将学习过程分解为思考者和解决者两个组件，通过两阶段强化学习从自由探索上下文到实际解决问题；2) 构建演化课程学习管道，包括扩展的本领域知识语料库和迭代演化的种子问题池。

Result: 实验表明该方法在各种基准测试中持续优于基线，为自我演化的大型视觉语言模型提供了可扩展的实现路径。

Conclusion: DoGe通过解耦学习和演化课程学习，有效解决了视觉语言模型在专业领域强化学习中的数据稀缺和奖励破解问题，为实现自我演化的大型视觉语言模型提供了可行方案。

Abstract: Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.

</details>


### [53] [JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models](https://arxiv.org/abs/2512.06859)
*Ce Chi,Xing Wang,Zhendong Wang,Xiaofan Liu,Ce Li,Zhiyan Song,Chen Zhao,Kexin Yang,Boshen Shi,Jingjing Yang,Chao Deng,Junlan Feng*

Main category: cs.AI

TL;DR: JT-DA-8B是一个专门用于复杂表格推理任务的8B参数大语言模型，通过构建包含34个表格推理任务的多样化训练语料，结合SFT和RL优化，在多种表格推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 针对表格推理场景中高质量监督数据缺乏的问题，需要开发专门的大语言模型来处理现实世界中的复杂表格分析任务。

Method: 1) 构建包含34个表格推理任务的多样化训练语料，整合29个公开表格QA数据集和300万张表格；2) 提出自动流水线生成多步分析任务；3) 基于JT-Coder-8B基础模型，采用LLM评分和工作流对齐过滤来蒸馏高质量数据；4) 结合监督微调(SFT)和强化学习(RL)优化模型；5) 提出四阶段表格推理工作流（表格预处理、表格感知、工具集成推理、提示工程）。

Result: JT-DA-8B在多种表格推理任务上表现出强大的性能，证明了数据中心的生成方法和工作流驱动优化的有效性。

Conclusion: 通过构建全面的训练语料、采用数据中心的生成方法和工作流驱动的优化策略，可以开发出在复杂表格推理任务上表现优异的专门化大语言模型。

Abstract: In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.

</details>


### [54] [How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations](https://arxiv.org/abs/2512.07497)
*JV Roig*

Main category: cs.AI

TL;DR: 研究大型语言模型作为具有工具使用能力的自主代理时的失败模式，通过KAMI基准测试分析三个代表性模型的行为，发现模型规模并非代理鲁棒性的唯一决定因素，并识别出四种常见失败模式。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型作为自主代理执行工具使用任务时的失败原因，理解哪些因素影响代理的可靠性和鲁棒性，为实际企业部署提供指导。

Method: 使用KAMI v0.1基准测试，分析900个执行轨迹，涵盖三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1）在文件系统、文本提取、CSV分析和SQL场景中的表现，进行细粒度的逐试验行为分析。

Result: 发现模型规模并非代理鲁棒性的唯一预测因素；Llama 4 Maverick（400B）在某些不确定性任务中仅比Granite 4 Small（32B）略好；DeepSeek V3.1的优越可靠性主要来自后训练强化学习而非架构或规模；识别出四种常见失败模式：未基于事实的过早行动、过度帮助替代缺失实体、易受干扰导致的上下文污染、负载下的脆弱执行。

Conclusion: 可靠的代理部署不仅需要更强的模型，还需要有意识的训练和设计选择，强调交互式基础、恢复行为和环境感知适应，需要加强验证、约束发现和遵循真实数据源。

Abstract: We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.

</details>


### [55] [VIGIL: A Reflective Runtime for Self-Healing Agents](https://arxiv.org/abs/2512.07094)
*Christopher Cruz*

Main category: cs.AI

TL;DR: VIGIL是一个可验证的检查和保护迭代学习运行时系统，通过监督兄弟代理、分析行为日志、维护情感银行，实现自主维护和自我修复，而非直接执行任务。


<details>
  <summary>Details</summary>
Motivation: 现有代理LLM框架虽然承诺通过任务分解、工具使用和迭代规划实现自主行为，但大多数部署系统仍然脆弱。它们缺乏运行时内省，无法诊断自身故障模式，没有人工干预就无法改进。实践中，许多代理栈退化为装饰性的LLM调用链，缺乏可靠性的结构机制。

Method: VIGIL是一个反射运行时，监督兄弟代理并执行自主维护。它摄入行为日志，将每个事件评估为结构化情感表示，维护具有衰减和上下文策略的持久情感银行，并生成RBT诊断（将近期行为分类为优势、机会和失败）。基于此分析，VIGIL生成保护性提示更新（保留核心身份语义）和只读代码提案（由策略引擎基于日志证据和代码热点生成）。VIGIL作为状态门控管道运行，非法转换会产生显式错误而非允许LLM即兴发挥。

Result: 在提醒延迟案例研究中，VIGIL识别出延迟升高，提出了提示和代码修复方案。当自己的诊断工具因模式冲突而失败时，它暴露出内部错误，生成备用诊断，并发出修复计划。这展示了在部署的代理运行时中实现元级自我修复。

Conclusion: VIGIL展示了代理运行时系统如何通过结构化内省、情感分析和自主维护机制实现自我修复能力，为解决当前代理系统的脆弱性问题提供了新途径。

Abstract: Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.

</details>


### [56] [A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy](https://arxiv.org/abs/2512.07109)
*Miguel Ingram,Arthur Joseph Merritt*

Main category: cs.AI

TL;DR: 提出首个针对400个任务的9类别分类法，通过CNN验证视觉一致性，揭示Transformer在35.3%任务上存在低神经亲和力，并发现组合性差距：局部模式准确率高但全局合成能力差，表明性能受架构适用性限制而非训练数据。


<details>
  <summary>Details</summary>
Motivation: 响应Hodel等人对任务相关性形式化定义的需求，为ARC-AGI基准中的任务建立系统分类，以诊断神经网络架构对不同类型任务的适用性限制。

Method: 1) 开发基于规则代码分析的9类别任务分类法；2) 使用CNN在原始网格像素上验证分类法的视觉一致性；3) 在302个任务上微调170万参数Transformer；4) 应用分类法分析Li等人的ViTARC研究。

Result: 分类法准确率达97.5%，CNN验证准确率95.24%；发现69.5%任务存在组合性差距（局部准确率>80%但全局<10%）；低亲和力任务性能上限明显（51.9% vs 高亲和力77.7%），部分任务即使大量数据也无法学习。

Conclusion: 神经网络性能受架构与任务亲和力限制，而非训练课程；需要开发具有亲和力对齐模块的混合架构；分类法为诊断任务难度和架构适用性提供有效工具。

Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,

</details>


### [57] [LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services](https://arxiv.org/abs/2512.07436)
*Hang He,Chuhuai Yue,Chengqi Dong,Mingxue Tian,Zhenfeng Liu,Jiajun Chai,Xiaohan Wang,Yufei Zhang,Qun Liao,Guojun Yin,Wei Lin,Chengcheng Wan,Haiying Sun,Ting Su*

Main category: cs.AI

TL;DR: LocalSearchBench是首个针对本地生活服务的智能搜索基准测试，包含15万条高质量数据，300个多跳问答任务，实验显示即使最先进的模型在该领域表现也很差。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型主要关注通用信息检索，很少探索具有独特挑战的垂直领域。本地生活服务领域的查询通常模糊且需要跨商家和产品的多跳推理，现有方法未能充分解决这些问题。

Method: 构建LocalSearchBench基准，包含来自不同城市和业务类型的15万条高质量条目；基于真实用户查询创建300个多跳问答任务；开发LocalPlayground统一环境，集成多种工具供智能体交互。

Result: 实验显示即使最先进的模型在LocalSearchBench上表现不佳：最佳模型（DeepSeek-V3.1）正确率仅34.34%，大多数模型在完整性（平均77.33%）和忠实度（平均61.99%）方面存在问题。

Conclusion: 本地生活服务领域需要专门的基准测试和领域特定的智能体训练，现有通用模型在该领域表现有限，凸显了垂直领域专业化的重要性。

Abstract: Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.

</details>


### [58] [Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement](https://arxiv.org/abs/2512.07611)
*Yongsheng Lian*

Main category: cs.AI

TL;DR: 系统比较三种强化学习算法（PPO、GRPO、DAPO）用于提升大语言模型复杂推理能力，通过控制性迁移学习评估发现RL训练模型在所有任务上均优于基础模型，但改进程度因基准而异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于系统评估不同强化学习算法在提升大语言模型复杂推理能力方面的效果，为RL-based LLM训练提供实践指导。

Method: 采用控制性迁移学习方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准套件上进行评估。参数分析包括GRPO和DAPO中的组大小、KL惩罚系数影响，以及DAPO中动态采样组件的效果。

Result: 所有RL训练模型均优于对应基础模型，但改进程度因基准而异。增加GRPO和DAPO的组大小能带来更稳定的训练动态和更高准确率，KL惩罚系数的影响是非单调的。DAPO中的动态采样组件并未改善性能，禁用DS时DAPO获得最佳整体结果。

Conclusion: 强化学习能有效提升大语言模型的复杂推理能力，但算法选择和参数配置对性能有重要影响。GRPO和DAPO的组大小增加有益，而DAPO的动态采样组件实际可能降低性能。

Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.

</details>


### [59] [The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds](https://arxiv.org/abs/2512.07631)
*Shahar Lutati*

Main category: cs.AI

TL;DR: 该论文提出了Agent Capability Problem (ACP)框架，通过信息获取视角预测智能体在资源约束下能否解决问题，将问题解决建模为信息获取过程，并推导出有效成本公式来预测资源需求。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体在决定何时投入资源解决问题时，通常依赖经验启发式方法，缺乏理论框架来预测智能体在资源约束下能否成功完成任务。

Method: 提出ACP框架，将问题解决建模为信息获取过程：智能体需要I_total比特信息来识别解决方案，每个行动获得I_step比特信息，成本为C_step。推导出有效成本C_eff = (I_total/I_step) * C_step来预测资源需求，并提供了概率上下界证明。

Result: 实验验证显示ACP预测与实际智能体性能高度一致，能够有效界定搜索努力，相比贪婪和随机策略提高了效率。该框架在LLM-based和agentic工作流中都具有良好的泛化能力。

Conclusion: ACP为自主智能体的资源分配决策提供了统一的信息论框架，连接了主动学习、贝叶斯优化和强化学习等原理，能够有效预测智能体在资源约束下的问题解决能力。

Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \

</details>


### [60] [Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE](https://arxiv.org/abs/2512.07710)
*Anxiang Zeng,Haibo Zhang,Hailing Zhang,Kaixiang Mo,Liang Yao,Ling Hu,Long Zhang,Shuman Liu,Shuyi Xie,Yanshi Li,Yizhang Chen,Yuepeng Sheng,Yuwei Huang,Zhaochen Xu,Zhiqiang Zhou,Ziqin Liew*

Main category: cs.AI

TL;DR: 提出了CompassMax-V3-Thinking，一个千亿规模的MoE推理模型，采用新的RL框架，核心原则是"每个提示都必须有价值"。通过多项创新技术解决了大规模RL训练中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 将RL扩展到千亿规模时暴露出关键效率问题：零方差提示浪费rollout资源、长视野重要性采样不稳定、标准奖励模型导致的优势反转，以及rollout处理的系统性瓶颈。

Method: 1) 多阶段零方差消除：过滤非信息性提示，稳定基于组的策略优化；2) ESPO：熵自适应优化方法，平衡token级和序列级重要性采样；3) Router Replay策略：对齐训练时MoE路由决策与推理时行为；4) 高吞吐RL系统：FP8精度rollout、重叠奖励计算和长度感知调度。

Result: 最终模型在内部和公共评估中都表现出强大的性能，实现了在千亿规模MoE模型上进行稳定高效的RL训练。

Conclusion: 这些创新形成了一个完整的pipeline，使得在千亿规模MoE模型上进行RL训练变得稳定高效，为大规模推理模型的训练提供了系统性的解决方案。

Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.

</details>


### [61] [RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2512.07761)
*Xiqiao Xiong,Ouxiang Li,Zhuo Liu,Moxin Li,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的多轮越狱攻击方法，通过训练攻击者LLM来诱使黑盒模型生成有害内容，相比单轮优化方法显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易受到越狱攻击的威胁，影响其在现实应用中的安全部署。现有方法主要依赖单轮优化，难以学习长期攻击策略，需要更有效的多轮攻击方法。

Method: 将问题建模为多轮强化学习任务，直接优化最终轮输出的有害性作为结果奖励。提出两种启发式过程奖励：1）控制中间输出的有害性以避免触发黑盒模型的拒绝机制；2）保持中间输出的语义相关性以避免内容漂移。

Result: 在多个基准测试上的实验结果表明，该方法在多个模型上一致提高了攻击成功率，证明了方法的有效性。

Conclusion: 提出的基于强化学习的多轮越狱攻击方法能够有效训练攻击者LLM，通过序列化的提示-输出交互来诱使黑盒模型生成有害内容，相比现有方法具有更好的攻击效果。

Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.

</details>


### [62] [ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning](https://arxiv.org/abs/2512.07795)
*Nearchos Potamitis,Lars Klein,Akhil Arora*

Main category: cs.AI

TL;DR: ReasonBENCH是首个量化LLM推理不稳定性的基准测试，通过多轮运行协议提供统计可靠的性能和质量指标，揭示当前推理方法普遍存在高不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估主要报告单次运行准确率，忽略了随机解码带来的内在不确定性，导致无法可靠评估方法的稳定性、可复现性和成本一致性。

Method: 提出ReasonBENCH基准测试，包含：(1)标准化推理框架、模型和任务的模块化评估库；(2)报告质量和成本统计可靠指标的多轮运行协议；(3)鼓励方差感知报告的公开排行榜。

Result: 发现绝大多数推理策略和模型都表现出高不稳定性，即使平均性能相似的策略置信区间宽度可达4倍差异，且性能最佳的方法通常成本更高且更不稳定。

Conclusion: 可复现性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础。

Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [63] [Quantifying Memory Use in Reinforcement Learning with Temporal Range](https://arxiv.org/abs/2512.06204)
*Rodney Lafuente-Mercado,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 提出Temporal Range指标，通过反向自动微分计算策略输出对过去观测的敏感性，量化RL策略对历史信息的依赖程度


<details>
  <summary>Details</summary>
Motivation: 研究训练好的RL策略实际使用多少过去观测信息，缺乏量化策略历史依赖程度的指标

Method: 提出Temporal Range指标，通过Jacobian块计算输出对输入序列的敏感性，用幅度加权平均滞后时间总结时间影响分布

Result: 在诊断和控制任务中，Temporal Range能准确反映任务需求：完全观察控制中较小，Copy-k中与真实滞后匹配，与窗口消融实验一致

Conclusion: Temporal Range提供实用的序列级记忆依赖度量，可用于比较不同智能体和环境，选择最短足够上下文

Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.

</details>


### [64] [Auto-exploration for online reinforcement learning](https://arxiv.org/abs/2512.06244)
*Caleb Ju,Guanghui Lan*

Main category: cs.LG

TL;DR: 提出具有自动探索功能的强化学习方法，无需先验参数知识，在表格和线性函数逼近两种设置下实现O(ε⁻²)样本复杂度


<details>
  <summary>Details</summary>
Motivation: 现有RL算法需要假设充分探索状态和动作空间，这导致算法不可实现且性能次优。需要参数无关的自动探索方法来解决探索-利用困境

Method: 提出两种变体：表格设置和线性函数逼近。采用动态混合时间、折扣状态分布采样、鲁棒梯度估计器和优势间隙函数等新算法创新

Result: 在存在探索最优策略的假设下，两种方法都达到O(ε⁻²)样本复杂度，且复杂度不包含可能任意大的算法相关参数

Conclusion: 提出的自动探索方法解决了传统RL算法的局限性，实现了参数无关、易于实现且具有理论保证的高效探索

Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.

</details>


### [65] [Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning](https://arxiv.org/abs/2512.06250)
*Chris Tava*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应策略切换方法，让智能体在迷宫导航中动态切换系统探索和目标导向路径规划策略，相比固定阈值方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自主智能体需要多种策略解决复杂任务，但确定何时切换策略仍具挑战。现有固定阈值方法不够灵活，需要领域知识和手工启发式规则。

Method: 使用Q-learning学习两个正交导航策略（系统探索和目标导向路径规划）之间的切换阈值。将状态空间离散化为覆盖率和距离桶，基于覆盖百分比和目标距离动态调整切换行为（20-60%阈值范围）。仅需迷宫尺寸和目标位置等最小领域知识。

Result: 在240个测试配置（4种迷宫尺寸×10个独特迷宫×6种智能体变体）中，自适应阈值学习优于单策略智能体和固定40%阈值基线。完成时间提升23-55%，运行时方差降低83%，最坏情况改善71%。性能增益随问题复杂度增加而提升。

Conclusion: 强化学习能够有效学习策略切换阈值，实现动态策略转换，无需先验知识或手工启发式。自适应策略选择的价值随问题复杂度增加而增加，具有良好泛化能力。

Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.

</details>


### [66] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

TL;DR: 论文分析了Bradley-Terry损失函数的梯度特性，发现其梯度范数受两个因素影响：预测奖励差异和表示距离。表示距离会导致大距离对梯度主导、小距离对梯度消失的问题，为此提出了NormBT方法进行自适应归一化。


<details>
  <summary>Details</summary>
Motivation: Bradley-Terry损失函数是LLM对齐中奖励模型的核心目标函数，但研究发现其梯度存在表示距离带来的偏差问题，这会影响模型学习效果，特别是对于表示距离小的细粒度区分任务。

Method: 提出了NormBT方法，通过自适应成对归一化方案来平衡表示距离效应，将学习信号聚焦在预测误差上。这是一个轻量级的、可直接替换BT损失的改进方案。

Result: 在各种LLM骨干网络和数据集上，NormBT都能一致提升奖励模型性能，在RewardBench的推理类别上获得了超过5%的显著提升，该类别包含大量小距离对。

Conclusion: 这项工作揭示了广泛使用的BT目标函数的关键局限性，并提供了一个简单有效的修正方案，有助于改进LLM对齐中的奖励建模。

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [67] [RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs](https://arxiv.org/abs/2512.06392)
*Runlong Zhou,Lefan Zhang,Shang-Chen Wu,Kelvin Zou,Hanzhi Zhou,Ke Ye,Yihao Feng,Dong Yin,Alex Guillen Garcia,Dmytro Babych,Rohit Chatterjee,Matthew Hopkins,Xiang Kong,Chang Lan,Lezhi Li,Yiping Ma,Daniele Molinari,Senyu Tong,Yanchao Sun,Thomas Voice,Jianyu Wang,Chong Wang,Simon Wang,Floris Weers,Yechen Xu,Guolin Yin,Muyang Yu,Yi Zhang,Zheng Zhou,Danyang Zhuo,Ruoming Pang,Cheng Leong*

Main category: cs.LG

TL;DR: RLAX是一个在TPU上运行的可扩展强化学习框架，采用参数服务器架构，通过系统优化和新的数据集管理技术，显著提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为提升大语言模型推理能力的主流方法，但现有框架在可扩展性、训练效率和容错性方面存在不足，特别是在大规模TPU集群上运行时。

Method: 采用参数服务器架构，主训练器定期推送更新权重到参数服务器，推理工作节点拉取最新权重生成新数据；引入系统技术实现可扩展和可抢占的RL；开发新的数据集管理和对齐技术加速收敛。

Result: 在1024个v5p TPU上，仅用12小时48分钟就将QwQ-32B模型的pass@8准确率提升12.8%，且在训练过程中对抢占具有鲁棒性。

Conclusion: RLAX是一个高效、可扩展的强化学习框架，能够在大规模TPU集群上快速提升大语言模型的性能，同时具备良好的容错性。

Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.

</details>


### [68] [Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control](https://arxiv.org/abs/2512.06471)
*Nathan P. Lawrence,Ali Mesbah*

Main category: cs.LG

TL;DR: 该论文分析了基于最优控制的目标条件强化学习，推导了传统二次目标与目标条件奖励之间的最优性差距，并将状态估计与概率奖励联系起来，验证了目标条件策略在非线性不确定环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习旨在训练智能体最大化达到目标状态的概率，但传统"密集"奖励（如二次型奖励）在此类任务中可能表现不佳。论文旨在从最优控制角度分析目标条件设置，解释为什么目标条件RL有效以及传统奖励为何会失败。

Method: 1. 基于最优控制理论分析目标条件设置；2. 推导传统二次目标与目标条件奖励之间的最优性差距；3. 在部分可观测马尔可夫决策过程中将状态估计与概率奖励联系起来；4. 在非线性和不确定环境中使用RL和预测控制技术验证目标条件策略的优势。

Result: 论文阐明了目标条件RL成功的原因，揭示了传统密集奖励可能失败的理论依据。通过将状态估计与概率奖励连接，使目标条件奖励特别适合双重控制问题。在非线性和不确定环境中的实验验证了目标条件策略的优势。

Conclusion: 目标条件强化学习从最优控制角度具有理论基础，其概率奖励形式比传统二次奖励更适合目标达成任务，特别是在部分可观测和双重控制问题中表现优异。

Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.

</details>


### [69] [A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation](https://arxiv.org/abs/2512.06547)
*Xiaocan Li,Shiliang Wu,Zheng Shen*

Main category: cs.LG

TL;DR: A-3PO通过近似近端策略消除异步RL中的计算瓶颈，减少18%训练时间同时保持性能


<details>
  <summary>Details</summary>
Motivation: 传统的解耦损失方法在异步RL中引入近端策略来稳定学习，但需要额外的前向传播计算，对大型语言模型造成计算瓶颈

Method: 提出A-3PO方法，通过简单插值近似近端策略，避免显式计算，消除额外计算开销

Result: 减少18%训练时间，同时保持与原始方法相当的性能

Conclusion: A-3PO有效解决了异步RL中解耦损失方法的计算效率问题，为大型模型训练提供了更高效的优化方案

Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md

</details>


### [70] [State Diversity Matters in Offline Behavior Distillation](https://arxiv.org/abs/2512.06692)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文发现离线行为蒸馏中原始数据集与合成数据集存在不对齐问题，提出状态密度加权算法来增强状态多样性，从而提升蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 离线行为蒸馏可以将大量离线RL数据压缩成紧凑的合成行为数据集，但研究发现高质量原始数据集不一定能产生优质合成数据集，存在不对齐问题。需要探索如何改进蒸馏过程以获得更好的下游任务性能。

Method: 通过实证分析不同训练损失水平下的策略性能，发现状态多样性在训练损失较大时更重要。提出状态密度加权算法，使用状态密度的倒数对蒸馏目标进行加权，从而将更多样化的状态信息蒸馏到合成数据中。

Result: 在多个D4RL数据集上的实验表明，当原始数据集状态多样性有限时，SDW算法显著提升了离线行为蒸馏的性能。

Conclusion: 离线行为蒸馏中状态多样性比状态质量更重要，特别是在训练损失较大的情况下。提出的状态密度加权算法通过强调状态多样性有效改善了蒸馏性能。

Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.

</details>


### [71] [Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis](https://arxiv.org/abs/2512.06917)
*Clifford F,Devika Jay,Abhishek Sarkar,Satheesh K Perepu,Santhosh G S,Kaushik Dey,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出一个新颖的轨迹级可解释强化学习框架，通过结合Q值差异和"激进项"的新状态重要性度量来排名整个轨迹，识别最优行为并提供对比解释。


<details>
  <summary>Details</summary>
Motivation: 当前可解释强化学习（XRL）主要关注局部单步决策，缺乏对智能体长期行为的解释。随着RL在实际应用中的部署，确保其行为透明可信至关重要，需要能够解释智能体长期轨迹的方法。

Method: 引入一个新颖框架，通过定义和聚合新的状态重要性度量来排名整个轨迹。该度量结合经典Q值差异和"激进项"（捕捉智能体达到目标的亲和度），提供更细致的状态关键性测量。通过从关键状态生成反事实推演，提供"为什么选择这个而不是那个"的解释。

Result: 实验在标准OpenAI Gym环境中验证，该方法能成功从异构智能体经验中识别最优轨迹。相比经典方法，提出的重要性度量在识别最优行为方面更有效，并能展示智能体选择路径相对于替代方案的优越性。

Conclusion: 该框架为解释智能体长期行为提供了有效方法，通过轨迹级分析和对比解释，向可信自主系统迈出了重要一步。

Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.

</details>


### [72] [Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models](https://arxiv.org/abs/2512.06920)
*Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: PGSRM是一种轻量级强化学习奖励框架，使用父模型参考输出嵌入与子模型生成输出的余弦相似度作为语义奖励，替代传统RLHF中的二元奖励或人工偏好数据。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要大量人工标注数据或训练复杂的奖励模型，成本高昂且不稳定。PGSRM旨在提供一种无需人工标注、无需额外模型训练的轻量级语义奖励框架。

Method: 使用父模型（如预训练大模型）生成参考输出的嵌入表示，计算子模型生成输出嵌入与参考嵌入的余弦相似度作为密集语义奖励信号，应用于PPO等强化学习算法。

Result: 在五个语言任务上，PGSRM相比二元奖励基线产生更平滑的奖励改进和更稳定的PPO动态，表明基于嵌入的语义奖励是RLHF式奖励建模的实用替代方案。

Conclusion: 嵌入语义奖励为小型Transformer模型的父引导对齐提供了实用替代方案，无需人工标注或额外模型训练，能产生更稳定的强化学习动态。

Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.

</details>


### [73] [LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding](https://arxiv.org/abs/2512.06982)
*Yu Yu,Qian Xie,Nairen Cao,Li Jin*

Main category: cs.LG

TL;DR: 提出一种基于LLM驱动的神经架构搜索方法，用于高效设计多源强化学习的状态编码器，在交通控制任务中比传统NAS和GENIUS框架表现更好。


<details>
  <summary>Details</summary>
Motivation: 多源强化学习中，如何设计融合传感器测量、时序信号、图像观测和文本指令等多种信息源的状态编码器仍然缺乏探索且需要手动设计。现有NAS方法忽略了模块中间输出的有用信息，限制了多源RL设置中的样本效率。

Method: 将问题形式化为复合神经架构搜索问题，提出LLM驱动的NAS流程，利用语言模型先验和中间输出信号来指导高效搜索高性能复合状态编码器。

Result: 在混合自主交通控制任务中，该方法比传统NAS基线和LLM-based GENIUS框架用更少的候选评估发现更高性能的架构。

Conclusion: LLM驱动的NAS方法能够有效解决多源RL状态编码器设计问题，通过利用中间输出信号和语言模型先验实现更高效的架构搜索。

Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.

</details>


### [74] [SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents](https://arxiv.org/abs/2512.07287)
*Sijia Li,Yuchen Huang,Zifan Liu,Zijian Li,Jingjing fu,Lei Song,Jiang Bian,Jun Zhang,Rui Wang*

Main category: cs.LG

TL;DR: 提出SIT-Graph方法，通过构建状态集成工具图来增强多轮工具使用，结合情景记忆和程序记忆，在需要时检索状态摘要，在常规步骤中遵循工具依赖关系。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在多轮工具使用场景中面临挑战，要么将整个轨迹或预定义子任务视为不可分割单元，要么仅利用工具间依赖关系，难以适应状态和信息随轮次变化的情况。

Method: 构建状态集成工具图(SIT-Graph)：首先从历史工具使用序列构建工具图，然后为每条边添加对话和工具历史的紧凑状态摘要。推理时，在需要回忆先前上下文时检索相关边的状态摘要来指导行动，在常规步骤中遵循高置信度的工具依赖关系。

Result: 在多个有状态的多轮工具使用基准测试中，SIT-Graph始终优于基于记忆和图的基础方法，提供更稳健的工具选择和更有效的经验转移。

Conclusion: SIT-Graph通过结合情景记忆和程序记忆，实现了人类般的决策平衡，显著提升了多轮工具使用场景中代理的性能和适应性。

Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.

</details>


### [75] [Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.07417)
*Giray Önür,Azita Dabiri,Bart De Schutter*

Main category: cs.LG

TL;DR: 本文提出一种多智能体强化学习框架，通过自适应调整状态反馈交通控制器的参数，结合了状态反馈控制器的反应性和强化学习的适应性，在保持训练效率的同时提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统交通管理策略（如路径引导、匝道控制和交通信号控制）依赖状态反馈控制器，虽然简单反应快，但缺乏应对复杂时变交通动态的适应性。需要一种既能保持反应性又具备适应性的解决方案。

Method: 提出多智能体强化学习框架，每个智能体自适应调整状态反馈交通控制器的参数（而非直接高频控制动作），以较低频率调整参数。多智能体结构增强了系统鲁棒性，局部控制器在部分故障时可独立运行。

Result: 在模拟的多类交通网络中进行评估，结果显示该框架优于无控制和固定参数状态反馈控制，与单智能体RL自适应状态反馈控制性能相当，但对部分故障具有更好的恢复能力。

Conclusion: 多智能体强化学习框架成功结合了状态反馈控制器的反应性和强化学习的适应性，通过参数调整而非直接动作控制提高了训练效率，多智能体结构增强了系统鲁棒性和容错能力。

Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.

</details>


### [76] [The Adoption and Usage of AI Agents: Early Evidence from Perplexity](https://arxiv.org/abs/2512.07828)
*Jeremy Yang,Noah Yonack,Kate Zyskowski,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 首个大规模AI代理在开放网络环境中的实地研究，分析了Perplexity的Comet浏览器及其AI助手的使用情况，揭示了用户群体、使用强度和用例的异质性。


<details>
  <summary>Details</summary>
Motivation: 了解通用AI代理在开放网络环境中的实际采用情况、使用强度和具体用例，填补大规模实地研究的空白。

Method: 基于Perplexity的Comet浏览器及其Comet Assistant代理的数亿匿名用户交互数据，通过分层代理分类法（主题、子主题、任务三级）系统分析使用模式。

Result: 早期采用者、高GDP国家用户、数字/知识密集型行业从业者更可能使用AI代理；生产力和学习研究占查询的57%；个人使用占55%，专业和教肓分别占30%和16%；短期内用例粘性强，长期用户转向认知导向主题。

Conclusion: AI代理的采用和使用存在显著异质性，其扩散对研究者、企业、政策制定者和教育者具有重要影响，需要进一步研究这一新兴AI能力类别。

Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.

</details>
