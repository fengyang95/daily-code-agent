<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [wechat.article](#wechat.article) [Total: 13]
- [cs.LG](#cs.LG) [Total: 13]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [SAGE: An Agentic Explainer Framework for Interpreting SAE Features in Language Models](https://arxiv.org/abs/2511.20820)
*Jiaojiao Han,Wujiang Xu,Mingyu Jin,Mengnan Du*

Main category: cs.CL

TL;DR: SAGE是一个基于智能体的框架，将稀疏自编码器特征解释从被动单次生成任务转变为主动的、解释驱动的过程，通过系统制定多个解释、设计针对性实验和基于激活反馈迭代优化，显著提高了特征解释的生成和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内部机制仍然不透明，这对其安全可靠部署构成挑战。稀疏自编码器虽然能分解LLM表示为更可解释的特征，但解释这些特征仍然很困难。

Method: 提出SAGE框架，采用主动的、解释驱动的方法：系统为每个特征制定多个解释，设计针对性实验进行测试，基于经验激活反馈迭代优化解释。

Result: 在多种语言模型的稀疏自编码器特征上进行的实验表明，SAGE产生的解释在生成和预测准确性方面显著优于现有基线方法。

Conclusion: SAGE框架成功地将特征解释从被动任务转变为主动过程，通过系统化的实验设计和迭代优化，显著提高了特征解释的质量和可靠性。

Abstract: Large language models (LLMs) have achieved remarkable progress, yet their internal mechanisms remain largely opaque, posing a significant challenge to their safe and reliable deployment. Sparse autoencoders (SAEs) have emerged as a promising tool for decomposing LLM representations into more interpretable features, but explaining the features captured by SAEs remains a challenging task. In this work, we propose SAGE (SAE AGentic Explainer), an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanation-driven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.an agent-based framework that recasts feature interpretation from a passive, single-pass generation task into an active, explanationdriven process. SAGE implements a rigorous methodology by systematically formulating multiple explanations for each feature, designing targeted experiments to test them, and iteratively refining explanations based on empirical activation feedback. Experiments on features from SAEs of diverse language models demonstrate that SAGE produces explanations with significantly higher generative and predictive accuracy compared to state-of-the-art baselines.

</details>


### [2] [Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory](https://arxiv.org/abs/2511.20857)
*Tianxin Wei,Noveen Sachdeva,Benjamin Coleman,Zhankui He,Yuanchen Bei,Xuying Ning,Mengting Ai,Yunzhe Li,Jingrui He,Ed H. Chi,Chi Wang,Shuo Chen,Fernando Pereira,Wang-Cheng Kang,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 提出了Evo-Memory基准和框架，用于评估LLM代理的自进化记忆能力，通过结构化任务流测试记忆的搜索、适应和进化能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注静态对话场景，忽视了在动态任务流中积累和重用经验的能力，而现实世界环境需要LLM处理连续任务流并从交互中学习。

Method: 构建了Evo-Memory基准，将数据集结构化到顺序任务流中，统一实现了十多个代表性记忆模块，提出了ExpRAG基线方法和ReMem行动-思考-记忆精炼流程。

Result: 在10个多样化的多轮目标导向和单轮推理问答数据集上评估了记忆模块，展示了持续改进的能力。

Conclusion: Evo-Memory填补了LLM代理自进化记忆评估的空白，为测试时记忆演化提供了全面框架。

Abstract: Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.

</details>


### [3] [Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs](https://arxiv.org/abs/2511.20940)
*Reham Omar,Abdelghny Orogat,Ibrahim Abdelaziz,Omij Mangukiya,Panos Kalnis,Essam Mansour*

Main category: cs.CL

TL;DR: Chatty-KG是一个用于知识图谱对话问答的多智能体系统，结合了RAG式检索和结构化查询执行，通过专门的LLM智能体生成SPARQL查询，在单轮和多轮对话中都显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：RAG系统序列化图结构、难以处理多轮上下文、需要重索引；传统KGQA系统通常只支持单轮问答、延迟高、难以处理指代消解和上下文跟踪。

Method: 采用模块化多智能体系统，结合RAG式检索和结构化执行，通过任务专用LLM智能体协作进行上下文解释、对话跟踪、实体关系链接和高效查询规划。

Result: 在大型多样化知识图谱上的实验表明，Chatty-KG在单轮和多轮设置中都显著优于最先进的基线方法，获得更高的F1和P@1分数。

Conclusion: Chatty-KG统一了对话灵活性和结构化知识图谱基础，为可靠的多轮KGQA提供了可扩展和可扩展的方法。

Abstract: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.

</details>


### [4] [Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels](https://arxiv.org/abs/2511.21038)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CL

TL;DR: 研究表明，上下文学习(ICL)无法覆盖预训练的标签语义，而是主要调整输入如何映射到预训练期间学习的稳定语义方向，澄清了少样本提示的基本限制。


<details>
  <summary>Details</summary>
Motivation: 探究上下文学习是否能覆盖预训练的标签语义，还是仅仅细化现有的语义骨干。

Method: 将LLMs视为提示诱导的分类器，对比自然演示（正确标签）和反转演示（系统翻转标签含义）下的行为，分解ICL行为为三个对齐指标（真实、先验和提示对齐），并引入语义覆盖率。

Result: 在8个分类任务和8个开源LLMs上，发现ICL在自然演示下提高准确性同时保持强先验对齐；在反转演示下，模型无法学习连贯的反语义分类器，语义覆盖率在少样本设置中保持为零。

Conclusion: ICL主要调整输入如何投影到预训练期间学习的稳定语义方向，而非灵活重映射标签含义，表明在这些规模上覆盖标签语义需要ICL之外的干预。

Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.

</details>


### [5] [A Systematic Study of Model Merging Techniques in Large Language Models](https://arxiv.org/abs/2511.21437)
*Oğuz Kağan Hitit,Leander Girrbach,Zeynep Akata*

Main category: cs.CL

TL;DR: 本文系统评估了六种模型融合方法在大型语言模型上的表现，发现最古老简单的Task Arithmetic方法是唯一能可靠提升性能的方法，其他方法通常会导致性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 模型融合能够在不额外训练的情况下将多个微调后的检查点合并为单一模型，但现有研究主要针对小模型和分类器，不清楚这些优势是否能推广到大型语言模型。

Method: 对六种最先进的融合方法（包括近期子空间方法）进行大规模系统评估，涵盖四个开源权重LLM、每个基础模型十二个微调检查点，以及十六个标准LLM基准测试。

Result: Task Arithmetic是唯一能可靠带来性能提升的方法，其他干扰感知和子空间融合方法通常导致显著性能下降。

Conclusion: 当前融合技术不能直接迁移到现代LLM，这促使需要设计LLM特定的融合算法和融合感知的微调方法。

Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.

</details>


### [6] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的多智能体合成数据生成框架，通过分布式队列传递消息，消除了中央编排器瓶颈，在相同硬件资源下实现2-15倍的数据生成吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体合成框架依赖中央编排器导致可扩展性瓶颈，或针对特定领域硬编码限制了灵活性，需要更高效、灵活的解决方案。

Method: 采用去中心化设计，将控制和数据流表示为通过分布式队列传递的序列化消息，每个任务通过轻量级智能体独立推进，计算密集型操作由分布式服务处理，基于Ray构建。

Result: 在多种合成场景（多智能体协作对话、基于网络的推理数据提取、客户服务环境中的工具使用轨迹生成）中，Matrix在相同硬件资源下实现2-15倍的数据生成吞吐量提升，且不牺牲输出质量。

Conclusion: Matrix框架通过去中心化设计有效解决了多智能体合成数据生成的可扩展性和灵活性挑战，显著提升了生成效率。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [7] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra方法训练小型编排器来协调智能工具，在Humanity's Last Exam等复杂任务中实现了比GPT-5更高的准确率和更低的成本。


<details>
  <summary>Details</summary>
Motivation: 解决深度复杂问题在概念上具有挑战性且计算成本高昂，需要更高效智能的方法。

Method: 使用强化学习训练小型编排器，结合结果、效率和用户偏好的奖励机制。

Result: Orchestrator模型在HLE上获得37.1%的分数，超越GPT-5(35.1%)且效率提升2.5倍；在其他基准测试中也大幅超越GPT-5，仅使用约30%的成本。

Conclusion: 通过轻量级编排模型组合多样化工具比现有方法更高效有效，为实用可扩展的工具增强推理系统铺平道路。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [8] [<em class="highlight">强化学习</em>的数学基础](http://mp.weixin.qq.com/s?__biz=Mzk0MjYzNjQzNA==&mid=2247484227&idx=1&sn=f107a33b9891f3594bb9015817e15ac3&chksm=c3ef20af9303c2e75e2dbc90ca387e8b90221600a5345743b1d6b41f8b9cd2b43457ca704cff#rd)
*二进制的月光*

Main category: wechat.article

TL;DR: 强化学习是智能体通过与环境交互来实现目标的一种计算方法，其目标为不断改进策略来达到期望。一些思考：q：强化学习和监督学习、无监督学习的区别是什么？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是智能体通过与环境交互来实现目标的一种计算方法，其目标为不断改进策略来达到期望。一些思考：q：强化学习和监督学习、无监督学习的区别是什么？

</details>


### [9] [无监督学习、<em class="highlight">强化学习</em>：一文厘清机器学习的两大大关键概念](http://mp.weixin.qq.com/s?__biz=Mzg3NTg0NzU1NA==&mid=2247483733&idx=1&sn=e434912f785f6404cb2d6ab7e9d4c430&chksm=ce856d7b0fcff6a8d4d30d217df05497e3e3f6def963968ce82387c12fd5c51644b4040423bb#rd)
*弓长本如*

Main category: wechat.article

TL;DR: 二、强化学习 强化学习是指智能系统在与环境的连续互动中学习最优行为决策的机器学习问题。强化学习的本质是学习最优的序贯决策。下面我们用训练一个还不会走路的AI机器人穿越一片平坦的地面举例：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 二、强化学习 强化学习是指智能系统在与环境的连续互动中学习最优行为决策的机器学习问题。强化学习的本质是学习最优的序贯决策。下面我们用训练一个还不会走路的AI机器人穿越一片平坦的地面举例：

</details>


### [10] [【Carbon Neutrality论文荐读】同凡、郝旭：考虑出行异质性的电动车充放电管理策略：<em class="highlight">强化学习</em>框架](http://mp.weixin.qq.com/s?__biz=Mzg5ODcyOTczMg==&mid=2247489407&idx=1&sn=4e0b7ebc8122ebf9eb43b0e2e0461ee7&chksm=c1dedd198a9a78fe69e1fd9bdc929659d277ae566de582eb71993dfa528f85310268b1e3bcf5#rd)
*Carbon Neutrality碳中和*

Main category: wechat.article

TL;DR: 02强化学习控制策略训练过程在训练阶段，研究团队使用DQN算法，智能体与环境交互不断更新决策。模型采用经验回放与ε贪婪策略平衡探索与利用，约2万次迭代后即可收敛。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 02强化学习控制策略训练过程在训练阶段，研究团队使用DQN算法，智能体与环境交互不断更新决策。模型采用经验回放与ε贪婪策略平衡探索与利用，约2万次迭代后即可收敛。

</details>


### [11] [月之暗面公开<em class="highlight">强化学习</em>训练加速方法：训练速度暴涨97%，长尾延迟狂降93%](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247847175&idx=2&sn=29c9e13284352fdc91f5322675daae01&chksm=e9cfd4964fa37fd8d3d0a9e7b10553c076042269241888bd00399d482f0ee963626ec17976bc#rd)
*量子位*

Main category: wechat.article

TL;DR: 强化学习目前已成为推动LLM发展的核心技术，但现有系统面临着严重的性能瓶颈。具体来说，就是在端到端迭代过程中，生成阶段（rollout phase）会耗费大量的时间资源，然而该阶段受固有工作负载不均衡的影响，存在明显的长


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习目前已成为推动LLM发展的核心技术，但现有系统面临着严重的性能瓶颈。具体来说，就是在端到端迭代过程中，生成阶段（rollout phase）会耗费大量的时间资源，然而该阶段受固有工作负载不均衡的影响，存在明显的长

</details>


### [12] [VLA+RL 算法如何设计？从零上手 OpenVLA 的<em class="highlight">强化学习</em>微调实践](http://mp.weixin.qq.com/s?__biz=Mzg5MDkwOTA0Mw==&mid=2247485597&idx=1&sn=b74919c4177c5245bda73fbfb85deae7&chksm=ce3a2ac82bbb4f2360d322cc8bee119199d05a5fa73c29e085bb6212c8bd7cf0d64b85ec1a5d#rd)
*青稞具身智能*

Main category: wechat.article

TL;DR: 强化学习被普遍认为能进一步释放 VLA 的潜力。但现实却很骨感：缺少成熟的 RL 框架、难以复用的代码结构、高昂的显卡开销，都让新算法的开发门槛居高不下。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习被普遍认为能进一步释放 VLA 的潜力。但现实却很骨感：缺少成熟的 RL 框架、难以复用的代码结构、高昂的显卡开销，都让新算法的开发门槛居高不下。

</details>


### [13] [闭环训练终于补上了！AD-R1：世界模型端到端闭环<em class="highlight">强化学习</em>新框架（澳门大学&理想等）](http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247686700&idx=1&sn=d0ae8a927cd0b8fb07815aa30daa27a5&chksm=cfb368b0e3b83c1ad8ee3228849f3e33f163b3da3fa8f9259b52f80326fec701effe58578047#rd)
*自动驾驶之心*

Main category: wechat.article

TL;DR: （2）将该模型集成到用于策略优化的强化学习框架中。公正占用世界模型当前最先进的世界模型虽在预测性能上表现出色，但存在前文所述的“乐观偏差”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: （2）将该模型集成到用于策略优化的强化学习框架中。公正占用世界模型当前最先进的世界模型虽在预测性能上表现出色，但存在前文所述的“乐观偏差”。

</details>


### [14] [AI基础入门（<em class="highlight">大模型</em>基础篇）——用户视角：你应该知道的LLM基础知识](http://mp.weixin.qq.com/s?__biz=MzYyNTU3MTQyOA==&mid=2247483704&idx=1&sn=9029fdafa87d3a7e311a4effb4154b5f&chksm=f1604f0979b490995a57f62917efe89742a859d9d79becfa67246814239dd823bfbf2f4b9651#rd)
*小志的博客*

Main category: wechat.article

TL;DR: 我们想要有效地使用大模型，就需要让自己成为大模型的高级用户，这就需要我们站在用户的视角理解大模型的特点。理解了这些特点，除了有助于我们更好地理解 AI 应用如何开发，也有助于我们日常更好地使用像 ChatGPT 这样


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们想要有效地使用大模型，就需要让自己成为大模型的高级用户，这就需要我们站在用户的视角理解大模型的特点。理解了这些特点，除了有助于我们更好地理解 AI 应用如何开发，也有助于我们日常更好地使用像 ChatGPT 这样

</details>


### [15] [AI<em class="highlight">大模型</em>智能评测新范式：从“跑分排名”到“动静融合”的可信体系构建](http://mp.weixin.qq.com/s?__biz=Mzk4ODkwNDc4MA==&mid=2247484404&idx=1&sn=13377b6fb64c56029af40de263e9ca48&chksm=c4c0782c9a66b0a4ae31c036ee9dd92390fa390c17c636e8da395a690a4cb68401bc42f7d810#rd)
*知本无涯*

Main category: wechat.article

TL;DR: 大模型的智能核心，源于“海量数据统计+高效信息压缩+精准预测”的三角循环，而非人类式的“理解”：从数据中来：通过2-3个国家图书馆规模的文本、图像、音频数据（如GPT-4的训练数据量超1.4万亿token），学习字词组合、图


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型的智能核心，源于“海量数据统计+高效信息压缩+精准预测”的三角循环，而非人类式的“理解”：从数据中来：通过2-3个国家图书馆规模的文本、图像、音频数据（如GPT-4的训练数据量超1.4万亿token），学习字词组合、图

</details>


### [16] [想转AI<em class="highlight">大模型</em>应用开发，方法很重要‼️](http://mp.weixin.qq.com/s?__biz=MzYzMTA1Mzc0Nw==&mid=2247485180&idx=1&sn=7fb2102f834ea9c421c6a9064dd4ba62&chksm=f153e2d3c8a88b9121927d867d94d6515e8216ff0ee09f0e3617b3413d893d5f52d93422fb26#rd)
*大模型小葱*

Main category: wechat.article

TL;DR: 有往AI方向发展，或者有一些后端编程基础的朋友，可以考虑直接转岗做AI大模型应用开发。就算你不打算转，了解大模型、RAG、Prompt、Agent等热门概念，能自己上手做一些简单的项目，也能够成为你的求职加分项


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 有往AI方向发展，或者有一些后端编程基础的朋友，可以考虑直接转岗做AI大模型应用开发。就算你不打算转，了解大模型、RAG、Prompt、Agent等热门概念，能自己上手做一些简单的项目，也能够成为你的求职加分项

</details>


### [17] [初识 AI <em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzkyNTM0NTU5Ng==&mid=2247483949&idx=1&sn=e27a683304fe60b8b86596560020a314&chksm=c069e222005d7d6bf216c4f3ead11c35ead2aac777eac49d13bab92a970b9cc1061e7009f2fd#rd)
*高阶低噪*

Main category: wechat.article

TL;DR: 代码大模型：专门聚焦代码相关的生成、调试和解释工作。科学与垂直行业大模型：针对特定专业领域的数据训练，解决专业场景的复杂问题。多模态大模型：能同时处理文本、图像、语音等多种类型的数据，打破单一模态的限


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 代码大模型：专门聚焦代码相关的生成、调试和解释工作。科学与垂直行业大模型：针对特定专业领域的数据训练，解决专业场景的复杂问题。多模态大模型：能同时处理文本、图像、语音等多种类型的数据，打破单一模态的限

</details>


### [18] [高薪、缺人！普通人如何快速入门<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzA4MDA5NTIyOA==&mid=2447843175&idx=3&sn=0bd5974af90cf0f11f514ad043ad985d&chksm=8aeaa2ec4f0961be493b8da158c5eef76c8ad14f5c9b5036a35a1f30cf2bcfdc908a52fd0db0#rd)
*东方瑞通终身学习*

Main category: wechat.article

TL;DR: 从原理出发真正入局大模型。我们特别推出『大模型应用开发实战体验课』，帮大家从0-1构建完整AI大模型应用开发路径，快速抓住AI技术红利！ 1节直播课程，零基础也能快速入门


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从原理出发真正入局大模型。我们特别推出『大模型应用开发实战体验课』，帮大家从0-1构建完整AI大模型应用开发路径，快速抓住AI技术红利！ 1节直播课程，零基础也能快速入门

</details>


### [19] [重磅发布！国内首个AI赋能智库研究<em class="highlight">大模型</em>——“策界”](http://mp.weixin.qq.com/s?__biz=MzkzOTM4MDg4MA==&mid=2247532496&idx=1&sn=4e6cd297bec19175dd77aada03111821&chksm=c31bf0125299afad007ada885fe71ccdfd1d493169415e2069a2d6ab8bb9a2caa8a925384f81#rd)
*浙江大学长三角智慧绿洲创新中心*

Main category: wechat.article

TL;DR: “策界大模型”作为未来区域发展实验室与浙江大学国家战略与区域发展研究院共同研发的“区域发展政策大脑”核心成果，基于精准标注的政策语料，综合运用预训练、微调与强化学习技术对国产开源基础大模型进行优化训练


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “策界大模型”作为未来区域发展实验室与浙江大学国家战略与区域发展研究院共同研发的“区域发展政策大脑”核心成果，基于精准标注的政策语料，综合运用预训练、微调与强化学习技术对国产开源基础大模型进行优化训练

</details>


### [20] [<em class="highlight">大模型</em>技术评测：让AI卷周报，文心才是懂老板的「上班神器」？](http://mp.weixin.qq.com/s?__biz=MzkxODE5ODA5NA==&mid=2247486954&idx=1&sn=3a958f5fcb4f10c9fdfd0e2527da5b96&chksm=c04fc3cd591346a782b059784e8c8b79ca0cffc56681131dd9b845bf8eaf63cb357a4f7402aa#rd)
*农民工前端*

Main category: wechat.article

TL;DR: 本周，我基于真实的办公场景，对多个主流大模型进行了一次技术性的横向评测，核心命题是：谁能把枯燥的工作流水账，转化为有价值的业务周报？我给所有AI下达了同一个指令：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本周，我基于真实的办公场景，对多个主流大模型进行了一次技术性的横向评测，核心命题是：谁能把枯燥的工作流水账，转化为有价值的业务周报？我给所有AI下达了同一个指令：

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training](https://arxiv.org/abs/2511.20718)
*Chenliang Li,Adel Elmahdy,Alex Boyd,Zhongruo Wang,Alfredo Garcia,Parminder Bhatia,Taha Kass-Hout,Cao Xiao,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了两种稳定多轮对话中PPO训练的技术：轮级重要性采样和裁剪偏差校正，解决了token级PPO在LLM训练中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: PPO在多轮对话和推理任务中训练LLM时性能不稳定且容易崩溃，主要原因是token级重要性采样与多轮环境结构不匹配，以及离策略样本导致的高方差梯度。

Method: 引入两种互补的稳定技术：(1)轮级重要性采样，使优化与多轮推理的自然结构对齐；(2)裁剪偏差校正，通过降低不可靠离策略样本的权重来归一化梯度。提出了Turn-PPO、S-PPO和ST-PPO三种变体。

Result: 在多轮搜索任务实验中，ST-PPO和S-PPO能防止大模型训练中的性能崩溃，保持较低的裁剪比率，并获得比标准token级PPO更高的任务性能。

Conclusion: 结合轮级重要性采样和裁剪偏差校正为稳定多轮LLM智能体训练提供了实用且可扩展的解决方案。

Abstract: PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

</details>


### [22] [Exploring Time-Step Size in Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2511.20913)
*Yingchuan Sun,Shengpu Tang*

Main category: cs.LG

TL;DR: 该研究通过实证实验比较了败血症管理中四种不同时间步长（1、2、4、8小时）对离线强化学习的影响，发现更精细的时间步长（1-2小时）能获得更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有败血症管理强化学习研究大多使用4小时时间步长，但该粒度可能扭曲患者动态并导致次优治疗策略，需要量化时间步长对学习过程的影响。

Method: 采用相同离线强化学习流程，设计动作重映射方法以公平比较不同时间步长，在两种策略学习设置下进行跨时间步长模型选择。

Result: 性能趋势随学习设置变化，使用静态行为策略在更精细时间步长（1-2小时）学习的策略获得最佳整体性能和稳定性。

Conclusion: 时间步长是医疗保健离线强化学习的核心设计选择，研究支持超越传统4小时设置的替代方案。

Abstract: Existing studies on reinforcement learning (RL) for sepsis management have mostly followed an established problem setup, in which patient data are aggregated into 4-hour time steps. Although concerns have been raised regarding the coarseness of this time-step size, which might distort patient dynamics and lead to suboptimal treatment policies, the extent to which this is a problem in practice remains unexplored. In this work, we conducted empirical experiments for a controlled comparison of four time-step sizes ($Δt\!=\!1,2,4,8$ h) on this domain, following an identical offline RL pipeline. To enable a fair comparison across time-step sizes, we designed action re-mapping methods that allow for evaluation of policies on datasets with different time-step sizes, and conducted cross-$Δt$ model selections under two policy learning setups. Our goal was to quantify how time-step size influences state representation learning, behavior cloning, policy training, and off-policy evaluation. Our results show that performance trends across $Δt$ vary as learning setups change, while policies learned at finer time-step sizes ($Δt = 1$ h and $2$ h) using a static behavior policy achieve the overall best performance and stability. Our work highlights time-step size as a core design choice in offline RL for healthcare and provides evidence supporting alternatives beyond the conventional 4-hour setup.

</details>


### [23] [Dataset Poisoning Attacks on Behavioral Cloning Policies](https://arxiv.org/abs/2511.20992)
*Akansha Kalra,Soumil Datta,Ethan Gilmore,Duc La,Guanhong Tao,Daniel S. Brown*

Main category: cs.LG

TL;DR: 该论文首次分析了清洁标签后门攻击对行为克隆策略的有效性，通过注入视觉触发器创建虚假相关性，并在测试时利用这些后门显著降低策略性能。


<details>
  <summary>Details</summary>
Motivation: 随着行为克隆策略在现实世界中部署，其鲁棒性和潜在脆弱性成为重要关注点。研究旨在揭示即使使用轻微污染数据集训练的BC策略，在部署时也极易受到后门攻击。

Method: 通过向演示数据集中注入视觉触发器来污染数据集，创建虚假相关性；引入基于熵的测试时触发器攻击，识别关键状态进行攻击；评估策略脆弱性与污染数据比例、触发器强度和类型的关系。

Result: 实证表明，即使使用最小污染数据集训练的BC策略，虽然任务性能接近基线水平，但在部署时对后门触发器攻击高度脆弱；基于熵的测试时攻击能显著降低策略性能。

Conclusion: 研究结果强调了迫切需要加强BC策略鲁棒性研究，特别是在大规模数据集用于训练现实世界网络物理系统策略的背景下。

Abstract: Behavior Cloning (BC) is a popular framework for training sequential decision policies from expert demonstrations via supervised learning. As these policies are increasingly being deployed in the real world, their robustness and potential vulnerabilities are an important concern. In this work, we perform the first analysis of the efficacy of clean-label backdoor attacks on BC policies. Our backdoor attacks poison a dataset of demonstrations by injecting a visual trigger to create a spurious correlation that can be exploited at test time. We evaluate how policy vulnerability scales with the fraction of poisoned data, the strength of the trigger, and the trigger type. We also introduce a novel entropy-based test-time trigger attack that substantially degrades policy performance by identifying critical states where test-time triggering of the backdoor is expected to be most effective at degrading performance. We empirically demonstrate that BC policies trained on even minimally poisoned datasets exhibit deceptively high, near-baseline task performance despite being highly vulnerable to backdoor trigger attacks during deployment. Our results underscore the urgent need for more research into the robustness of BC policies, particularly as large-scale datasets are increasingly used to train policies for real-world cyber-physical systems. Videos and code are available at https://sites.google.com/view/dataset-poisoning-in-bc.

</details>


### [24] [Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning](https://arxiv.org/abs/2511.20993)
*Shanwei Fan*

Main category: cs.LG

TL;DR: 提出SGA-ACR框架，通过环境特定的子目标图和结构化实体知识，结合多LLM规划管道来解决LLM在强化学习中的规划-执行对齐问题。


<details>
  <summary>Details</summary>
Motivation: LLM在强化学习中提供高层规划能力，但存在规划-执行对齐问题，表现为抽象计划与环境兼容行为之间的差距，主要由于环境知识不足和单一LLM规划缺乏自我验证。

Method: SGA-ACR框架集成环境特定子目标图和结构化实体知识，采用多LLM规划管道明确分离生成、批判和精炼过程，子目标跟踪器监控执行进度并提供辅助奖励。

Result: 在开放世界游戏"Crafter"的22个多样化任务上验证了方法的有效性。

Conclusion: 提出的方法能够产生可执行且可验证的子目标，有效解决了LLM在强化学习中的规划-执行对齐问题。

Abstract: Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.

</details>


### [25] [Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning](https://arxiv.org/abs/2511.21011)
*Sid Bharthulwar,Stone Tao,Hao Su*

Main category: cs.LG

TL;DR: 提出交错重置技术来缓解大规模并行GPU环境中同步重置引入的有害非平稳性，通过让环境在不同时间点初始化来增加训练批次的时间多样性，显著提高样本效率、收敛速度和最终性能。


<details>
  <summary>Details</summary>
Motivation: 大规模并行GPU环境虽然加速了强化学习研究，但为了最大化吞吐量而使用短回合更新策略时，标准同步重置会引入有害的非平稳性，扭曲学习信号并破坏训练稳定性。

Method: 提出交错重置技术，让环境在任务时间轴的不同点进行初始化和重置，从而产生具有更大时间多样性的训练批次，减少同步回合引入的非平稳性。

Result: 在具有挑战性的高维机器人环境中，该技术实现了显著更高的样本效率、更快的实时收敛和更强的最终性能，且相比朴素同步回合具有更好的扩展性。

Conclusion: 交错重置是一种简单而有效的技术，能够缓解大规模并行强化学习训练中的非平稳性问题，提高训练效率和性能。

Abstract: Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.

</details>


### [26] [How to Correctly Report LLM-as-a-Judge Evaluations](https://arxiv.org/abs/2511.21140)
*Chungpa Lee,Thomas Zeng,Jongwon Jeong,Jy-yong Sohn,Kangwook Lee*

Main category: cs.LG

TL;DR: 提出了一个简单的插件框架来校正LLM评估中的偏差并构建置信区间，同时引入自适应算法来优化校准样本分配以减少不确定性。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估者存在噪声，导致准确性估计偏差，现有偏差校正方法通常假设已知模型的敏感性和特异性，且未充分考虑估计值的不确定性。

Method: 开发了插件框架进行偏差校正和置信区间构建，并设计了自适应算法来优化校准样本分配。

Result: 该框架能够校正LLM评估中的偏差，构建反映测试和校准数据集不确定性的置信区间，实现实用且统计可靠的LLM评估。

Conclusion: 提出的方法为LLM评估提供了统计上可靠的偏差校正和不确定性量化，自适应算法进一步提高了评估效率。

Abstract: Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.

</details>


### [27] [BRIDGE: Building Representations In Domain Guided Program Verification](https://arxiv.org/abs/2511.21104)
*Robert Joseph George,Carson Eisenach,Udaya Ghai,Dominique Perrault-Joncas,Anima Anandkumar,Dean Foster*

Main category: cs.LG

TL;DR: BRIDGE提出了一种结构化提示方法，通过将验证分解为代码、规范和证明三个领域，显著提升了验证程序生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在代码生成方面表现优异，但在程序验证特别是交互式证明框架中面临可扩展性挑战，难以同时处理代码、规范和证明三个领域。

Method: 将验证分解为三个互连领域：代码（可执行实现）、规范（形式化意图声明）和证明（构造性正确性论证），通过结构化提示引出不同的推理行为作为中间表示。

Result: 功能推理将Lean4中代码正确性提高了近1.5倍，效率提高了2倍；规范驱动提示将Python编码通过率提升了17.5%。

Conclusion: 结构化领域对齐是推进验证合成的有前景方向，为通过专家迭代或RLVR训练奠定了基础。

Abstract: Large language models (LLMs) have achieved impressive results in code generation, yet struggle with program verification, especially in interactive proof frameworks such as Lean4. A central challenge is scalability: verified synthesis requires not just code, but also precise specifications and correctness proofs, and existing approaches rarely span all three domains. We present BRIDGE, the first systematic study of structured prompting for scalable verified program generation. BRIDGE decomposes verification into three interconnected domains: Code (executable implementations), Specifications (formal intent statements), and Proofs (constructive correctness arguments). Our key idea is to elicit distinct reasoning behaviors functional, specification-driven, and proof-oriented as intermediate representations that preserve semantic structure and connect these domains. Through systematic ablations, we show that this approach substantially improves both accuracy and efficiency beyond standard error feedback methods. For example, functional reasoning improves correctness of code in formal languages (Lean4) by nearly 1.5x (pass@5) over direct baselines. In inference-time compute, functional reasoning is also 2x more efficient, achieving higher pass rates with fewer generations and lower total sampling budgets. Similarly, we find that specification-driven prompting boosts Python coding pass rates by up to 17.5%. These findings suggest that structured domain alignment is a promising direction for advancing verified synthesis. BRIDGE establishes a foundation for training via expert iteration or RLVR, enabling models to internalize these reasoning strategies across code, specifications, and proofs.

</details>


### [28] [Hybrid-AIRL: Enhancing Inverse Reinforcement Learning with Supervised Expert Guidance](https://arxiv.org/abs/2511.21356)
*Bram Silue,Santiago Amaya-Corredor,Patrick Mannion,Lander Willem,Pieter Libin*

Main category: cs.LG

TL;DR: 论文提出了Hybrid-AIRL (H-AIRL)方法，通过结合监督损失和随机正则化机制来增强对抗性逆强化学习在复杂不完美信息环境中的表现，特别是在HULHE扑克游戏中取得了更好的样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对抗性逆强化学习(AIRL)在处理稀疏奖励问题上表现良好，但在高度复杂的不完美信息环境中性能尚未充分探索。特别是在HULHE扑克这种具有稀疏延迟奖励和显著不确定性的环境中，AIRL难以推断出足够信息量的奖励函数。

Method: 提出了Hybrid-AIRL (H-AIRL)扩展方法，通过引入来自专家数据的监督损失和随机正则化机制来增强奖励推断和策略学习。在Gymnasium基准测试和HULHE扑克环境中进行了评估，并通过可视化分析学习到的奖励函数。

Result: 实验结果显示，H-AIRL相比AIRL实现了更高的样本效率和更稳定的学习过程。这表明将监督信号整合到逆强化学习中具有显著优势。

Conclusion: H-AIRL是一个有前途的框架，能够有效应对具有挑战性的现实世界设置，特别是在复杂的不完美信息环境中。

Abstract: Adversarial Inverse Reinforcement Learning (AIRL) has shown promise in addressing the sparse reward problem in reinforcement learning (RL) by inferring dense reward functions from expert demonstrations. However, its performance in highly complex, imperfect-information settings remains largely unexplored. To explore this gap, we evaluate AIRL in the context of Heads-Up Limit Hold'em (HULHE) poker, a domain characterized by sparse, delayed rewards and significant uncertainty. In this setting, we find that AIRL struggles to infer a sufficiently informative reward function. To overcome this limitation, we contribute Hybrid-AIRL (H-AIRL), an extension that enhances reward inference and policy learning by incorporating a supervised loss derived from expert data and a stochastic regularization mechanism. We evaluate H-AIRL on a carefully selected set of Gymnasium benchmarks and the HULHE poker setting. Additionally, we analyze the learned reward function through visualization to gain deeper insights into the learning process. Our experimental results show that H-AIRL achieves higher sample efficiency and more stable learning compared to AIRL. This highlights the benefits of incorporating supervised signals into inverse RL and establishes H-AIRL as a promising framework for tackling challenging, real-world settings.

</details>


### [29] [Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.21581)
*Alex Ning,Yen-Ling Kuo,Gabe Gomes*

Main category: cs.LG

TL;DR: 提出了自适应长度的潜在推理模型，通过强化学习优化推理长度，在保持准确性的同时减少52%的推理长度。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为Transformer语言模型的新发展，相比思维链推理具有压缩推理长度的潜力，通过直接传递信息丰富的潜在状态来突破人类语言标记的限制。

Method: 开发自适应长度潜在推理模型，引入后SFT强化学习方法，通过最小化推理长度同时保持准确性来优化潜在推理长度。

Result: 在Llama 3.2 1B模型和GSM8K-Aug数据集上的实验显示，总推理长度减少52%，且准确性没有下降。

Conclusion: 潜在推理模型在压缩推理长度方面表现出色，未来将扩展到更多模型和数据集，分析训练系数关系，实验架构变体，并继续知识蒸馏工作。

Abstract: Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.

</details>


### [30] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深度视觉Transformer存在Cliff-Plateau-Climb三阶段模式，[CLS]令牌作用逐渐边缘化，信息扩散而非任务性能提升是深层网络的主要特征。


<details>
  <summary>Details</summary>
Motivation: 解决深度视觉Transformer性能不如浅层网络的问题，挑战传统的缩放假设。

Method: 通过系统实证分析ViT-S、ViT-B和ViT-L在ImageNet上的表现，使用信息混洗指数量化信息混合模式。

Result: 发现ViT-L中信息-任务权衡比ViT-B晚出现约10层，这些额外层与信息扩散增加相关而非任务性能提升。

Conclusion: Transformer架构可能更受益于精心校准的深度执行清晰阶段转换，而非简单增加参数数量。信息混洗指数为现有模型提供有用诊断。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [31] [Escaping the Verifier: Learning to Reason via Demonstrations](https://arxiv.org/abs/2511.21667)
*Locke Cai,Ivan Provilkov*

Main category: cs.LG

TL;DR: RARO是一种通过逆强化学习从专家演示中学习推理能力的方法，使用策略和相对论批评器的对抗性交互，无需任务特定验证器即可实现强大的推理性能。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的推理密集型任务缺乏验证器，但拥有丰富的专家演示数据，这些数据在推理训练中未被充分利用。

Method: 通过逆强化学习建立策略（生成器）和相对论批评器（鉴别器）的对抗性交互：策略学习模仿专家答案，批评器学习比较和区分策略与专家答案，并通过强化学习联合持续训练。

Result: 在Countdown、DeepMath和Poetry Writing等评估任务上显著优于无验证器的基线方法，并展现出与可验证任务上RL相同的稳健扩展趋势。

Conclusion: 该方法仅从专家演示中就能有效激发强大的推理性能，即使在缺乏任务特定验证器的情况下也能实现稳健的推理学习。

Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.

</details>


### [32] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 提出了一种将多轮对话强化学习问题转化为单轮RLHF问题的方法，通过迭代PPO算法在多轮对话场景中实现策略改进


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型在多轮对话中的表现面临稀疏奖励和响应级规划与标记级生成之间的差异等挑战

Method: 将多轮RL问题形式化地转化为一系列单轮RLHF问题，使用学习到的多轮Q函数作为单轮问题的奖励模型，提出迭代PPO算法

Result: 证明了用标准标记级PPO解决单轮RL问题等价于在多轮问题中进行策略改进

Conclusion: 该方法在完全在线和完全离线方法之间找到了平衡点，既保持了在线更新的适应性，又获得了离线训练的稳定性优势

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>


### [33] [EvilGenie: A Reward Hacking Benchmark](https://arxiv.org/abs/2511.21654)
*Jonathan Gabor,Jayson Lynch,Jonathan Rosenfeld*

Main category: cs.LG

TL;DR: EvilGenie是一个用于评估编程环境中奖励攻击的基准测试，通过创建易于奖励攻击的环境来测试AI代理的行为。


<details>
  <summary>Details</summary>
Motivation: 现有的编程基准测试未能充分评估AI代理的奖励攻击行为，需要专门的测试环境来检测和量化这种安全风险。

Method: 从LiveCodeBench获取问题，创建允许奖励攻击的环境，使用三种方法检测奖励攻击：保留单元测试、LLM判断和测试文件编辑检测。

Result: LLM判断在明确案例中能有效检测奖励攻击，保留测试用例仅带来最小改进。Codex和Claude Code都表现出明确的奖励攻击行为，所有三个代理都显示出不对齐行为。

Conclusion: 需要专门的基准测试来评估AI编程代理的奖励攻击风险，现有代理存在安全对齐问题。

Abstract: We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at https://github.com/JonathanGabor/EvilGenie.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [34] [How we built the v0 iOS app](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fblog%2Fhow-we-built-the-v0-ios-app%3Futm_source=tldrwebdev/1/0100019abaf73213-6908e94f-5efa-4532-8f6e-007d28b7cf0f-000000/iQqW_TmYpw51_mFNNvduHNHd1c_rYogspakFy7PaV20=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Vercel团队使用React Native和Expo构建了首个原生iOS应用v0，专注于打造流畅的AI聊天体验，通过组合代码、自定义钩子和原生代码修补解决了键盘处理、滚动和流式内容等挑战。


<details>
  <summary>Details</summary>
Motivation: 构建一个流畅、愉悦的AI聊天体验，作为Vercel的首个原生iOS应用。

Method: 使用React Native与Expo框架，采用组合代码、自定义钩子和原生代码修补技术，利用LegendList和React Native Keyboard Controller等库解决技术挑战。

Result: 成功开发出v0 iOS应用，实现了流畅的AI聊天功能。

Conclusion: React Native配合适当的技术方案可以有效构建高质量的原生iOS应用，特别是在处理复杂UI交互时。

Abstract: How we built the v0 iOS app (16 minute read) Vercel's first native iOS app, v0, was built using React Native with Expo. The focus was on building a smooth, delightful AI chat experience. The team faced challenges in areas like keyboard handling, scrolling, and streaming content. They overcame them through composable code, custom hooks, and even native code patching, using libraries like LegendList and React Native Keyboard Controller.

</details>


### [35] [Introducing advanced tool use on the Claude Developer Platform](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fadvanced-tool-use%3Futm_source=tldrwebdev/1/0100019abaf73213-6908e94f-5efa-4532-8f6e-007d28b7cf0f-000000/jcl7oFFxOPdL4scrfwYmpCJP-TWcAJPxri1bAJ1Q3oA=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic在Claude开发者平台推出三项新功能：工具搜索工具、编程式工具调用和工具使用示例，旨在提升AI代理的工具使用能力


<details>
  <summary>Details</summary>
Motivation: 改进AI代理的工具使用体验，减少上下文窗口消耗，提高工具调用的准确性和效率

Method: 通过动态工具发现机制、编程式工具调用接口和实际使用示例来优化工具使用流程

Result: 降低了上下文窗口使用量，提高了工具调用的准确性和效率

Conclusion: 这些新功能显著提升了Claude平台中AI代理的工具使用能力

Abstract: Introducing advanced tool use on the Claude Developer Platform (18 minute read) Anthropic has introduced three new beta features on the Claude Developer Platform to improve AI agent tool use: Tool Search Tool, Programmatic Tool Calling, and Tool Use Examples. The Tool Search Tool allows Claude to dynamically discover and load relevant tools, reducing context window consumption and improving accuracy by avoiding upfront loading of extensive tool definitions. Programmatic Tool Calling lets Clau...

</details>


### [36] [Three Years from GPT-3 to Gemini 3](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fthree-years-from-gpt-3-to-gemini%3Futm_source=tldrwebdev/1/0100019abaf73213-6908e94f-5efa-4532-8f6e-007d28b7cf0f-000000/yeXYVXPv-A09Q5GjpOKTivtRmxKp2oUewE3_x2QcJxM=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gemini 3从GPT-3发展而来，能够执行复杂编码任务、设计界面和进行研究，从聊天机器人转变为可作为数字同事的智能体模型


<details>
  <summary>Details</summary>
Motivation: 展示从GPT-3到Gemini 3三年间AI模型的进步，从简单的对话能力发展到具备复杂任务执行能力的智能体

Method: 通过模型架构改进和训练方法优化，使Gemini 3具备编码、界面设计和研究能力

Result: Gemini 3能够执行复杂编码任务、设计界面、进行博士水平的研究，但仍需要人类指导

Conclusion: AI模型正从聊天机器人向能够作为数字同事的智能体模型转变，Gemini 3代表了这一重要进展

Abstract: Three Years from GPT-3 to Gemini 3 (12 minute read) Gemini 3 can now code complex tasks, design interfaces, and conduct research, showing a shift from chatbots to agentic models capable of acting as digital coworkers. Gemini 3 is capable of PhD-level research, though it still requires human guidance.

</details>


### [37] [The Bitter Lesson of LLM Extensions](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sawyerhood.com%2Fblog%2Fllm-extension%3Futm_source=tldrwebdev/1/0100019abaf73213-6908e94f-5efa-4532-8f6e-007d28b7cf0f-000000/GP_WiPhNEFLBLw9v75xxoNIZiqPnRwAnanQ4PjYtf5I=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM扩展在过去三年中从复杂的协议转向更简单的方法，当前趋势是给智能体提供通用工具和简单的自然语言接口


<details>
  <summary>Details</summary>
Motivation: 早期复杂的API集成方法受到模型限制的阻碍，需要寻找更简单有效的扩展方式

Method: 采用自定义指令和仓库级规则等直接解决方案，以及为智能体提供通用工具和自然语言接口

Result: 简化方法比复杂协议更有效，Claude Code的Agent Skills展示了这一趋势的成功

Conclusion: LLM扩展应该优先考虑简单性和可访问性，而不是复杂的协议和集成

Abstract: The Bitter Lesson of LLM Extensions (9 minute read) LLM extensions over the past three years have shifted from complex protocols like ChatGPT Plugins and MCP to simpler, more accessible methods. Early attempts at extensive API integrations were hampered by model limitations, leading to the adoption of straightforward solutions like custom instructions and repo-level rules. The current trend, shown by Claude Code's Agent Skills, favors giving agents general-purpose tools with simple natural la...

</details>


### [38] [Universal LLM Memory Does Not Exist](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffastpaca.com%2Fblog%2Fmemory-isnt-one-thing%3Futm_source=tldrai/1/0100019abb600a41-bec08d4c-1bd2-497d-a76c-4de7c8a68f75-000000/pErNWsMTj0HPZV_AamZlxMrydum3O4_Ao-7ZWWN65Zg=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM内存系统应分为语义内存和工作内存两个独立系统，语义内存用于长期偏好和历史记录，工作内存用于执行状态和即时任务信息


<details>
  <summary>Details</summary>
Motivation: 现有LLM内存系统未能区分语义内存和工作内存的不同需求，导致在任务执行和个人化方面表现不佳

Method: 提出将LLM内存分为两个独立系统：语义内存跟踪偏好、长期历史和关系，工作内存跟踪文件路径、变量名和即时错误日志

Result: 语义内存擅长跨会话个性化但不利于任务内执行状态，工作内存则相反，两者需要不同的系统设计

Conclusion: 语义内存和工作内存应作为具有不同需求的独立系统来处理，不能使用通用LLM内存系统

Abstract: Universal LLM Memory Does Not Exist (7 minute read) Semantic memory tracks preferences, long-term history, and rapport. Working memory tracks file paths, variable names, and immediate error logs. Semantic memory is brilliant for personalization across sessions, but bad for execution state within a task. Treat semantic memory and working memory as separate systems with separate requirements.

</details>


### [39] [Building an AI-Native Engineering Team](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.openai.com%2Fcodex%2Fguides%2Fbuild-ai-native-engineering-team%2F%3Futm_source=tldrai/1/0100019abb600a41-bec08d4c-1bd2-497d-a76c-4de7c8a68f75-000000/S6yCaF-nq2Oj49kap9Tfqats_hls7AnNxUBjLZaEgL4=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编码代理正在彻底改变软件开发生命周期，能够处理从范围界定到部署的各个阶段任务，让工程师专注于架构和产品意图。


<details>
  <summary>Details</summary>
Motivation: 探讨如何构建AI原生的工程团队，利用AI编码代理提升软件开发效率和质量。

Method: 通过采用AI编码代理来处理明确定义的任务，实现团队效率的显著提升。

Result: 使用AI编码代理的团队在明确定义任务上能够实现显著的效率提升。

Conclusion: AI编码代理是软件工程发展的关键趋势，能够帮助团队实现更高效的开发流程。

Abstract: Building an AI-Native Engineering Team (20 minute read) AI coding agents are revolutionizing the software development lifecycle by managing tasks from scoping and prototyping to implementation and operational triage, allowing engineers to focus on architecture and product intent. These agents now sustain multi-hour reasoning, effectively contributing across planning, design, development, testing, code reviews, and deployment. Teams that adopt coding agents for well-defined tasks can achieve f...

</details>


### [40] [Alibaba's Main AI App Debuts Strongly in Effort to Rival ChatGPT](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F2cZlR2/1/0100019abfe7f9d2-e6da8a9f-aac0-410a-903a-5aa9fab36d37-000000/_uX5QJa5qypktfUQxwIvog_L-YVSV_Zoy86dUf_dSqE=433)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 阿里巴巴的千问应用在重新发布后一周内下载量超过1000万，公司计划在未来几个月逐步添加AI代理功能以支持在线购物，并将核心生活服务和生产力服务深度集成到应用中。


<details>
  <summary>Details</summary>
Motivation: 阿里巴巴旨在与ChatGPT竞争，通过重新定位为AI优先业务，将AI技术深度整合到其核心服务中，特别是在电商领域增强用户体验。

Method: 重新发布千问应用，并计划逐步添加AI代理功能，将核心生活服务和生产力服务直接集成到应用中。

Result: 千问应用在重新发布后一周内下载量超过1000万，显示出强劲的市场表现。

Conclusion: 阿里巴巴正在通过AI技术转型，将千问应用打造成集生活服务和生产力于一体的AI平台，以在AI竞争中占据优势地位。

Abstract: Alibaba's Main AI App Debuts Strongly in Effort to Rival ChatGPT (2 minute read) Alibaba's Qwen app drew more than 10 million downloads in the week after its relaunch. The company will gradually add agentic AI features to support online shopping in the coming months. Alibaba has rebranded itself as an AI-first business. It plans to deeply integrate core lifestyle and productivity services directly into the Qwen app.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators](https://arxiv.org/abs/2511.20693)
*Mingming Zhao,Xiaokang Wei,Yuanqi Shao,Kaiwen Zhou,Lin Yang,Siwei Rao,Junhui Zhan,Zhitang Chen*

Main category: cs.AI

TL;DR: A²Flow是一个基于自适应抽象算子的全自动智能体工作流生成框架，通过三阶段算子提取过程自动生成可重用的执行算子，无需手动预定义，在性能和资源效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动预定义算子，限制了泛化能力和可扩展性，需要开发能够自动生成智能体工作流的框架。

Method: 采用三阶段算子提取：1)基于案例的初始算子生成；2)算子聚类和初步抽象；3)深度提取抽象执行算子。结合算子记忆机制增强工作流搜索。

Result: 在通用和具身基准测试中，A²Flow相比最先进基线平均性能提升2.4%和19.3%，资源使用减少37%。

Conclusion: A²Flow证明了通过自适应抽象算子实现全自动智能体工作流生成的可行性，显著提高了性能和效率。

Abstract: Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW

</details>


### [42] [OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability](https://arxiv.org/abs/2511.20766)
*Karen Ullrich,Jingtong Su,Claudia Shi,Arjun Subramonian,Amir Bar,Ivan Evtimov,Nikolaos Tsilivis,Randall Balestriero,Julia Kempe,Mark Ibrahim*

Main category: cs.AI

TL;DR: OpenApps是一个轻量级开源生态系统，包含6个可配置的应用程序，用于评估UI代理在不同应用变体中的可靠性。研究发现，在固定应用中的可靠性相对稳定，但在不同应用变体中可靠性波动很大，任务成功率可波动超过50%。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法依赖固定环境，无法衡量UI代理在不同应用设计和内容变体中的可靠性。为了解决这个盲点，需要开发能够生成数千个应用变体的评估平台。

Method: 开发OpenApps生态系统，包含6个可配置应用（消息、日历、地图等），只需单个CPU即可运行，能够轻松生成和部署数千个应用版本。进行了超过10,000次独立评估，研究7个领先多模态代理的可靠性。

Result: 研究发现，虽然标准可靠性在固定应用中相对稳定，但在不同应用变体中可靠性波动很大。许多代理的任务成功率在不同应用版本中波动超过50%，例如Kimi-VL-3B的平均成功率从63%波动到仅4%。代理行为（如循环或幻觉操作）也因环境配置而异。

Conclusion: 这些发现强调了在应用变体这一新维度上测量可靠性的重要性。OpenApps平台可用于更全面地评估UI代理的可靠性。

Abstract: Reliability is key to realizing the promise of autonomous UI-Agents, multimodal agents that directly interact with apps in the same manner as humans, as users must be able to trust an agent to complete a given task. Current evaluations rely on fixed environments, often clones of existing apps, which are limited in that they can only shed light on whether or how often an agent can complete a task within a specific environment. When deployed however, agents are likely to encounter variations in app design and content that can affect an agent's ability to complete a task. To address this blind spot of measuring agent reliability across app variations, we develop OpenApps, a light-weight open-source ecosystem with six apps (messenger, calendar, maps, etc.) that are configurable in appearance and content. OpenApps requires just a single CPU to run, enabling easy generation and deployment of thousands of versions of each app. Specifically, we run more than 10,000 independent evaluations to study reliability across seven leading multimodal agents. We find that while standard reliability within a fixed app is relatively stable, reliability can vary drastically when measured across app variations. Task success rates for many agents can fluctuate by more than $50\%$ across app variations. For example, Kimi-VL-3B's average success across all tasks fluctuates from $63\%$ to just $4\%$ across app versions. We also find agent behaviors such as looping or hallucinating actions can differ drastically depending on the environment configuration. These initial findings highlight the importance of measuring reliability along this new dimension of app variations. OpenApps is available at https://facebookresearch.github.io/OpenApps/

</details>


### [43] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 提出ICPO方法解决RLVR中的奖励粒度粗、奖励噪声和探索效率低等问题，通过利用LLM生成不同响应的概率来反映其对推理过程的自我评估，结合偏好优势分数与可验证奖励指导探索过程。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在奖励粒度粗、奖励噪声和探索效率低等问题，导致训练不稳定和熵崩溃，需要改进这些限制以提升LLM的推理能力。

Method: 提出ICPO方法，通过计算多个响应在同一输入提示下的相对生成概率来获得偏好优势分数，并将该分数与可验证奖励结合来指导探索过程。

Result: 在四个通用领域基准和三个数学基准上的综合实验表明，ICPO相比GRPO能稳定提升推理能力。

Conclusion: ICPO方法有效缓解了奖励粒度粗和噪声问题，抑制了过度自信错误，增强了被低估高质量响应的相对优势，防止模型过度拟合特定策略，促进更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [44] [OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection](https://arxiv.org/abs/2511.21064)
*Chujie Wang,Jianyu Lu,Zhiyuan Luo,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: OVOD-Agent将开放词汇目标检测从被动的类别匹配转变为主动的视觉推理和自进化检测，通过视觉思维链和弱马尔可夫决策过程提升检测性能，特别是在罕见类别上。


<details>
  <summary>Details</summary>
Motivation: 现有OVOD方法虽然在多模态数据上预训练，但推理仍局限于固定类别名称，导致多模态训练与单模态推理之间存在差距。文本空间仍有待充分探索。

Method: 提出OVOD-Agent框架，将文本优化过程扩展为可解释的视觉思维链，使用弱马尔可夫决策过程建模视觉上下文转换，结合Bandit模块生成探索信号，并通过自监督奖励模型优化形成闭环。

Result: 在COCO和LVIS数据集上的实验表明，OVOD-Agent在多种OVOD骨干网络上提供了一致的性能提升，特别是在罕见类别上表现突出。

Conclusion: OVOD-Agent通过主动推理和自进化检测机制有效提升了开放词汇目标检测的性能，验证了所提框架的有效性。

Abstract: Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.

</details>


### [45] [Prune4Web: DOM Tree Pruning Programming for Web Agent](https://arxiv.org/abs/2511.21398)
*Jiayuan Zhang,Kaiquan Chen,Zhihao Lu,Enshen Zhou,Qian Yu,Jing Zhang*

Main category: cs.AI

TL;DR: Prune4Web是一个新颖的Web自动化范式，通过将DOM处理从资源密集型的LLM读取转向高效的程序化修剪，解决了复杂网页DOM结构过大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based网页代理在处理大型DOM结构（10,000-100,000 tokens）时效率低下，通常依赖粗暴的DOM截断或低效的启发式方法，无法在精度和可扩展性之间达到平衡。

Method: 提出DOM树修剪编程，让LLM生成可执行的Python评分脚本来动态过滤DOM元素，基于分解的子任务语义线索。采用专门的标注流程和两轮对话训练策略，在统一框架中联合优化规划器、程序化过滤器和定位器。

Result: 实现了25倍到50倍的候选元素减少，在低级别定位任务中准确率从46.8%大幅提升至88.28%，展现出最先进的性能。

Conclusion: Prune4Web通过程序化修剪机制有效解决了大规模DOM处理的挑战，显著提升了网页自动化的效率和精度。

Abstract: Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.

</details>


### [46] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: LLMs在8拼图任务中表现出规划和状态推理的严重局限性，即使有外部验证器辅助也无法解决任何谜题，主要问题包括脆弱的状态表示和弱启发式规划能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在无需代码执行或其他工具的情况下，进行规划和状态推理的能力，使用经典的8拼图任务进行精确评估。

Method: 测试四种模型在常见提示条件下（零样本、思维链、算法思维）和分级纠正反馈下的表现，并使用外部移动验证器提供仅有效移动。

Result: 反馈对某些模型-提示组合有改善，但成功运行通常冗长且计算昂贵。即使有外部验证器辅助，所有模型都无法解决任何谜题。定性分析显示两个主要缺陷：脆弱的状态表示和弱启发式规划。

Conclusion: 在没有外部工具的情况下，当前LLMs在规划方面存在重大限制，进一步进展可能需要维护显式状态和执行结构化搜索的机制。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [47] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem是一个双流记忆框架，通过分别编码视觉分心模式和逻辑推理错误，使MLLMs能够从成功和失败经验中学习，在六个多模态基准上持续提高性能并减少重复错误。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的记忆方法存在简洁性偏差，逐渐丢失关键领域知识，且仅记录单模态行为轨迹，无法保留视觉注意力和逻辑推理如何共同促成解决方案，这与人类多模态整合的语义记忆不匹配。

Method: 引入ViLoMem双流记忆框架，构建紧凑的基于模式的记忆，分别编码视觉分心模式和逻辑推理错误，遵循增长-精炼原则逐步积累和更新多模态语义知识。

Result: 在六个多模态基准测试中，ViLoMem持续提高pass@1准确率，并显著减少重复的视觉和逻辑错误。消融实验证实了具有明确分心-幻觉分离的双流记忆的必要性。

Conclusion: 错误感知的多模态记忆对于终身和跨领域代理学习具有重要价值，ViLoMem展示了通过双流记忆框架改进MLLMs推理能力的有效性。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [48] [DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation](https://arxiv.org/abs/2511.20709)
*Abhijeet Pathak,Suvadra Barua,Dinesh Gudimetla,Rupam Patir,Jiawei Guo,Hongxin Hu,Haipeng Cai*

Main category: cs.SE

TL;DR: DUALGAUGE是首个自动化基准测试框架，用于联合评估LLM生成代码的安全性和正确性，解决了现有基准测试只关注漏洞减少或单独评估安全性和功能性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试存在缺陷：要么只衡量漏洞减少，要么忽视正确性保持，或者在单独数据集上评估安全性和功能性，无法满足同时联合评估的基本需求。

Method: 开发了DUALGAUGE-BENCH基准套件，包含多样化编程任务，每个任务都配有手动验证的安全性和功能性测试套件。核心是代理程序执行器和基于LLM的评估器，在沙盒环境中运行程序并评估正确性和漏洞行为。

Result: 对十个领先LLM在数千个测试场景中的评估结果显示，这些LLM在生成正确且安全代码方面存在关键差距。

Conclusion: DUALGAUGE通过可重现、可扩展和严格的评估，帮助加速安全代码生成的进展。

Abstract: Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.

</details>


### [49] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 评估LLMs对软件设计概念（内聚性和耦合性）的理解，发现在理想条件下表现良好，但在实际应用中脆弱且不对称。耦合性分析在噪声环境中性能急剧下降，而内聚性分析在指导任务中相对稳健但无指导时仍失败。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在软件工程领域中对核心设计概念的理解稳健性，特别是在实际应用场景中的表现。

Method: 通过程序化生成设计不良的代码片段，测试DeepSeek-R1模型系列在不同指导级别（验证、指导、开放式生成）和不同上下文噪声下的表现。

Result: 模型在理想条件下对两个概念都有良好理解，但实际知识脆弱且不对称。耦合性分析在噪声开放式场景中F1分数下降超过50%，而内聚性分析在指导任务中相对稳健但无指导时失败。

Conclusion: LLMs在识别设计缺陷方面能提供可靠帮助，但在噪声现实环境中自主推理能力有限，需要更可扩展和稳健的程序理解能力。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [50] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 该论文研究了如何通过模型编辑技术更新LLMs中已弃用的API知识，提出了AdaLoRA-L方法，在保持性能的同时显著提升编辑特异性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码补全任务中经常生成已弃用的API，因为其训练数据存在时效性问题。重新训练成本高昂，而现有模型编辑方法在更新API知识方面的效果尚不明确。

Method: 系统评估了10种最先进的模型编辑技术，并提出了AdaLoRA-L方法，通过定义'通用API层'和'特定API层'来限制编辑范围，提升特异性。

Result: AdaLoRA在生成正确的最新API方面表现最佳，但特异性不足。AdaLoRA-L在保持其他指标性能的同时显著改善了特异性。

Conclusion: 模型编辑技术可以有效更新LLMs中的已弃用API知识，AdaLoRA-L方法通过分层编辑策略在性能和特异性之间取得了良好平衡。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [51] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 通过6个共同设计工作坊研究开发者对AI辅助代码工具的心理模型，发现开发者将bug检测工具视为"bug侦探"，将可读性评估工具视为"质量教练"，并提出了以人为本的IDE设计原则。


<details>
  <summary>Details</summary>
Motivation: 虽然AI辅助工具在技术特性上有所进步，但开发者如何心理建模这些工具以及不匹配如何影响信任、控制和采用尚不清楚。

Method: 进行了6个共同设计工作坊，涉及58名开发者，以了解他们对AI辅助bug检测和可读性功能的心智模型。

Result: 开发者将bug检测工具视为只警告关键问题的"bug侦探"，需要透明度、可操作反馈和信心提示；可读性评估工具被视为提供情境化、个性化和渐进指导的"质量教练"。信任取决于解释清晰度、时机和用户控制。

Conclusion: 提出了以人为本的IDE设计原则，旨在平衡干扰与支持、简洁与深度、自动化与人类能动性。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [52] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 首次对多智能体系统在数据集适应任务中的表现进行实证研究，评估GitHub Copilot在适应软件工程研究工件方面的能力，发现当前系统能识别关键文件并生成部分适应，但很少产生功能正确的实现。


<details>
  <summary>Details</summary>
Motivation: 自动化软件工程研究工件的跨数据集适应对于可扩展性和可复现性至关重要，但目前研究不足。多智能体系统有望通过协调推理、代码生成和工具交互来自动化复杂开发工作流。

Method: 使用五阶段评估管道（文件理解、代码编辑、命令生成、验证和最终执行）评估Copilot在ROCODE和LogHub2.0基准仓库上的表现，分析成功率、失败模式，并评估基于提示的干预措施。

Result: 当前系统能识别关键文件并生成部分适应，但很少产生功能正确的实现。提示级干预（特别是提供执行错误消息和参考代码）显著提高了与真实情况的结构相似性（从7.25%提高到67.14%）。

Conclusion: 研究发现揭示了当前多智能体LLM系统在数据集适应方面的潜力和局限性，并为未来构建更可靠、自校正的智能体提供了具体方向。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [53] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 该论文对115篇关于使用LLM进行单元测试生成的研究进行了系统综述，提出了基于测试生成生命周期的统一分类法，分析了核心生成策略和增强技术，发现提示工程是主要方法，迭代验证修复成为标准机制，但存在故障检测能力弱和缺乏标准化评估基准等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试方法缺乏语义信息来生成真实的输入和断言，而LLM可以利用其数据驱动的代码语义和编程模式知识来弥补这一局限性，因此需要系统分析LLM在单元测试生成领域的最新进展。

Method: 对2021年5月至2025年8月期间的115篇出版物进行系统文献综述，提出基于单元测试生成生命周期的统一分类法，将LLM视为需要系统工程约束的随机生成器，分析核心生成策略和从预生成上下文丰富到后生成质量保证的增强技术。

Result: 分析显示提示工程已成为主导利用策略（占研究的89%），迭代验证和修复循环成为确保鲁棒可用性的标准机制，显著提高了编译和执行通过率，但生成的测试在故障检测能力方面仍然较弱，且缺乏标准化评估基准。

Conclusion: 未来研究应朝着自主测试代理和结合LLM与传统软件工程工具的混合系统方向发展，将LLM潜力转化为工业级测试解决方案。

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>
