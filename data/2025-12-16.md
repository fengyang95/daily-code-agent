<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.LG](#cs.LG) [Total: 11]
- [tldr.article](#tldr.article) [Total: 17]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.AI](#cs.AI) [Total: 20]
- [wechat.article](#wechat.article) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文研究了潜在空间思维方法在数学推理任务上的表现，发现现有监督微调方法存在局限性，探索了强化学习方法但仍落后于传统语言空间思维模型。


<details>
  <summary>Details</summary>
Motivation: 传统思维链推理使用离散语言空间效率低下，许多生成的token只是强制执行语言规则而非推理所需。潜在空间思维使用连续嵌入空间可能更高效，但现有方法在复杂任务（如数学推理）上表现不佳。

Method: 1. 分析Coconut方法（一种监督微调的潜在空间思维方法）的设计敏感性和局限性；2. 探索强化学习技术（包括GRPO）在潜在空间思维中的应用；3. 设计新颖的Latent RL方法直接优化潜在思维步骤。

Result: 实验结果显示：1. Coconut方法对设计选择高度敏感且存在固有局限性；2. 强化学习训练的模型在数学推理领域仍落后于传统语言空间思维链模型。

Conclusion: 虽然潜在空间思维理论上更高效，但在复杂推理任务上仍需进一步研究以超越传统语言空间方法。作者公开了代码库供进一步研究。

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [2] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang,Treasure Mayowa,Jason Fan,Yicheng Fu,Aaron Sandoval,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 提出Direct Confidence Alignment (DCA)方法，使用直接偏好优化对齐LLM的言语化置信度与内部置信度，而非真实准确率，以提升模型透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型应用日益广泛，构建可信可靠的模型变得愈发重要。研究发现模型的内部置信度（来自token概率）与言语化置信度不一致，导致不同校准方法产生误导性结果。

Method: 提出Direct Confidence Alignment (DCA)方法，使用Direct Preference Optimization技术来对齐LLM的言语化置信度与内部置信度，而非与真实准确率对齐。同时引入三种新的基于校准误差的评估指标。

Result: DCA在某些模型架构上改善了置信度对齐指标，减少了模型置信度表达的不一致性。但在其他模型上效果有限，表明需要更针对具体模型的校准方法。

Conclusion: DCA方法能提升模型透明度和可靠性，但效果因模型架构而异，需要更模型感知的方法来构建可解释和可信的LLMs。

Abstract: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [3] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht,Lev Sorokin,Abdullah Saydemir,Ken E. Friedl,Andrea Stocco*

Main category: cs.CL

TL;DR: 本文探索使用LLM和高级提示技术评估车载对话问答系统的准确性，通过生成合成数据测试13个不同规模的LLM，发现DeepSeek-R1在单代理自一致性提示下达到最佳性能，而非推理模型DeepSeek-V3在效果与成本效率间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 车载对话问答系统虽提升用户体验，但评估其准确性和可靠性仍具挑战。传统人工评估难以规模化，需要开发可扩展的自动化评估方法。

Method: 使用LLM结合高级提示技术（输入输出、思维链、自一致性、多代理提示）评估对话系统响应与用户话语的一致性。生成合成用户话语及正确/含错误的系统响应，在餐厅推荐案例研究中测试13个不同规模和提供商的LLM。

Result: 小型非推理模型在应用高级提示技术（特别是多代理提示）时改进最大；推理模型始终优于非推理模型，最佳性能由单代理自一致性提示下的DeepSeek-R1实现（F1=0.99，成本0.002美元/请求）。DeepSeek-V3在效果与成本时间效率间达到最佳平衡。

Conclusion: LLM评估为车载对话问答系统的上下文理解评估提供了可扩展且准确的替代方案，优于传统人工评估。

Abstract: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [4] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava,Sam Jung,Spencer Mateega*

Main category: cs.CL

TL;DR: MARKET-BENCH是一个评估LLM在量化交易任务中构建可执行回测器能力的基准，测试模型根据自然语言策略描述生成匹配参考实现代码的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在金融领域的应用潜力巨大，但缺乏系统评估其在量化交易任务中代码生成和数值准确性的基准。需要了解LLM能否从自然语言策略描述构建可靠的回测系统。

Method: 创建包含三种经典策略（定时交易、配对交易、Delta对冲）的基准，要求LLM根据自然语言描述生成可执行回测代码。使用多轮pass@k指标分离结构可靠性（代码能否运行）和数值准确性（回测指标的平均绝对误差）。

Result: 大多数模型能可靠执行最简单策略（平均pass@3为0.80），但不同模型和任务间误差差异巨大。Gemini 3 Pro和Claude 4.5 Sonnet在简单策略上结合了强可靠性和低误差，GPT-5.1 Codex-Max在前两个策略上达到完美pass@1，Qwen3 Max达到完美pass@3但有时产生不准确的P&L路径。

Conclusion: 当前LLM能够搭建基本交易基础设施，但在价格、库存和风险推理方面仍存在困难。研究发布了MARKET-BENCH基准和公开排行榜。

Abstract: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [5] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

TL;DR: HyperEdit提出了一种基于超网络的动态适应方法和差异感知正则化，显著提升了指令文本编辑的性能，在修改区域上比现有方法提升了9%-30%的BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法将文本编辑视为通用文本生成任务，导致两个关键问题：难以忠实对齐多样化的用户意图，以及经常过度编辑未改变区域。指令文本编辑在代码编辑器等实际应用中越来越重要，但大型语言模型在此任务上仍然表现不佳。

Method: 1. 基于超网络的动态适应：生成请求特定的参数，使模型能够为每个指令定制编辑策略。2. 差异感知正则化：将监督集中在修改的文本片段上，防止过度编辑，同时确保精确的最小化更改。

Result: HyperEdit在修改区域上比最先进的基线方法提升了9%-30%的相对BLEU改进，尽管仅使用了30亿参数。

Conclusion: HyperEdit通过超网络动态适应和差异感知正则化，有效解决了指令文本编辑中的意图对齐和过度编辑问题，显著提升了编辑性能。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [6] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

TL;DR: CoVRL是一种将变分推断与强化学习耦合的方法，通过混合采样策略解决无验证器RL中推理轨迹与答案不连贯的问题，提升语言模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前无验证器强化学习方法仅基于问题采样推理轨迹，导致推理轨迹与最终答案脱节，探索效率低且不连贯。需要一种能保持思维-答案一致性的方法。

Method: 提出CoVRL（耦合变分强化学习），通过混合采样策略耦合先验分布和后验分布，构建并优化复合分布，实现高效探索同时保持思维-答案连贯性。

Result: 在数学和通用推理基准测试中，CoVRL相比基础模型性能提升12.4%，比当前最先进的无验证器RL基线额外提升2.3%。

Conclusion: CoVRL为增强语言模型通用推理能力提供了一个原则性框架，通过耦合变分推断和强化学习解决了推理轨迹与答案不连贯的问题。

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [7] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu,Jianglun Feng,Zhuoran Zhuang,Junzhe Zhao,Maofei Que,Jieting Li,Dianlei Wang,Hao Tong,Ye Chen,Pan Li*

Main category: cs.CL

TL;DR: CoDA是一个上下文解耦的分层智能体框架，通过将高层规划与低层执行分离来解决LLM智能体中的"上下文爆炸"问题，使用单一共享LLM在不同角色间切换，并通过PECO强化学习方法联合优化，在复杂任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在强化学习中面临的"上下文爆炸"问题——长文本输出积累会淹没模型的上下文窗口，导致推理失败。现有方法在处理复杂多步任务时性能受限。

Method: 提出CoDA（Context-Decoupled hierarchical Agent）框架：使用单一共享LLM作为骨干，学习在两个上下文隔离的角色中操作：1）高层规划器在简洁战略上下文中分解任务；2）低层执行器在临时隔离的工作空间中处理工具交互。采用PECO（Planner-Executor Co-Optimization）强化学习方法，应用轨迹级奖励联合优化两个角色，通过上下文依赖的策略更新促进无缝协作。

Result: 在复杂多跳问答基准测试中，CoDA相比最先进基线取得显著性能提升。在长上下文场景中表现出强大鲁棒性，保持稳定性能，而所有其他基线都遭受严重性能下降，验证了分层设计在缓解上下文过载方面的有效性。

Conclusion: CoDA通过上下文解耦的分层设计有效解决了LLM智能体的上下文爆炸问题，实现了高层规划与低层执行的分离与协同优化，为复杂多步任务处理提供了有效解决方案。

Abstract: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [8] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding,Shengda Long,Changxin Pu,Huan Zhou,Hongwan Gao,Xiang Gao,Chao He,Yue Hou,Fei Hu,Zhaojian Li,Weiran Shi,Zaiyuan Wang,Daoguang Zan,Chenchen Zhang,Xiaoxu Zhang,Qizhi Chen,Xianfu Cheng,Bo Deng,Qingshui Gu,Kai Hua,Juntao Lin,Pai Liu,Mingchen Li,Xuanguang Pan,Zifan Peng,Yujia Qin,Yong Shan,Zhewen Tan,Weihao Xie,Zihan Wang,Yishuo Yuan,Jiayu Zhang,Enduo Zhao,Yunfei Zhao,He Zhu,Chenyang Zou,Ming Ding,Jianpeng Jiao,Jiaheng Liu,Minghao Liu,Qian Liu,Chongyao Tao,Jian Yang,Tong Yang,Zhaoxiang Zhang,Xinjie Chen,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: NL2Repo Bench是一个专门评估编码智能体长时程仓库生成能力的基准测试，要求智能体仅基于自然语言需求文档在空工作空间中自主构建完整可安装的Python库，实验显示当前最先进的智能体在此任务上表现不佳（平均测试通过率低于40%），揭示了长时程推理是自主编码智能体的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有编码智能体基准测试主要关注局部代码生成、脚手架完成或短期修复任务，无法评估构建完整软件系统所需的长时程能力，因此需要专门的基准来评估智能体在真实世界仓库构建中所需的持续推理、规划和执行能力。

Method: 提出了NL2Repo Bench基准测试，要求编码智能体仅基于单一自然语言需求文档和空工作空间，自主完成架构设计、依赖管理、多模块逻辑实现，最终生成完全可安装的Python库，通过实验评估了最先进的开源和闭源模型在此任务上的表现。

Result: 实验结果显示长时程仓库生成问题远未解决：即使最强的智能体平均测试通过率也低于40%，很少能正确完成整个仓库。详细分析揭示了四种主要的长时程失败模式：过早终止、全局一致性丧失、脆弱的跨文件依赖关系，以及在数百个交互步骤中的规划不足。

Conclusion: NL2Repo Bench为测量持续智能体能力建立了严格可验证的测试平台，并指出长时程推理是下一代自主编码智能体的核心瓶颈，需要进一步研究来解决这一关键挑战。

Abstract: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [9] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

TL;DR: 系统评估了GPT-4o mini、Gemini 2.0 Flash和Llama 4 Scout三种LLM在不同语气提示（非常友好、中性、非常粗鲁）下的性能表现，发现语气敏感性具有模型依赖性和领域特异性，现代LLM对语气变化整体表现稳健。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索提示工程中常被忽视的语用元素（如语言语气和礼貌程度）对不同大语言模型性能的影响，填补该领域的研究空白。

Method: 提出系统评估框架，在MMMLU基准测试上评估三种最新LLM（GPT-4o mini、Gemini 2.0 Flash、Llama 4 Scout）在三种语气提示（非常友好、中性、非常粗鲁）下的表现，涵盖STEM和人文学科六个任务，并进行统计显著性检验。

Result: 结果显示：1）中性或非常友好提示通常比非常粗鲁提示准确率更高；2）统计显著效应仅出现在部分人文学科任务中，粗鲁语气会降低GPT和Llama的准确率，而Gemini相对不敏感；3）跨任务聚合后语气效应减弱且大多失去统计显著性；4）语气敏感性具有模型依赖性和领域特异性。

Conclusion: 虽然交互语气在特定解释性场景中可能重要，但现代LLM对语气变化整体表现稳健，数据集规模和覆盖范围显著影响语气效应的检测，为实际部署中的提示设计和模型选择提供实用指导。

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [10] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

TL;DR: Hindsight是一个新型的智能体记忆架构，通过结构化记忆网络区分世界事实、智能体经验、实体摘要和演化信念，显著提升长期对话记忆性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统将记忆作为外部层处理，仅提取对话片段存储在向量或图数据库中，存在证据与推理界限模糊、长期信息组织困难、推理解释支持有限等问题，需要更结构化的记忆架构。

Method: 提出Hindsight记忆架构，将记忆组织为四个逻辑网络：世界事实、智能体经验、合成实体摘要和演化信念。支持保留、回忆和反思三个核心操作，通过时序实体感知记忆层将对话流转化为结构化可查询记忆库，反思层在此基础上进行推理和更新。

Result: 在LongMemEval和LoCoMo等长期对话记忆基准测试中，Hindsight配合20B开源模型将准确率从39%提升至83.6%，优于相同骨干网络的全上下文基线，甚至超越GPT-4o。进一步扩展骨干网络后，在LongMemEval达到91.4%，LoCoMo达到89.61%，显著优于现有记忆架构。

Conclusion: Hindsight通过结构化记忆网络和核心操作，为智能体提供了更有效的长期记忆能力，解决了现有记忆系统的局限性，在多会话和开放域问题上表现优异。

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [11] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

TL;DR: 研究发现LLMs在临床诊断基准上的优异表现并非主要源于概率推理，而是通过轻量级频率统计方法FBPR能达到类似性能，两者正确率重叠度低，具有互补性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在临床诊断选择题基准测试中表现出色，但研究者不清楚这种性能在多大程度上反映了底层的概率推理能力。他们希望探究LLMs是否真正掌握了概率推理，还是仅仅依赖简单的频率统计。

Method: 研究者提出了频率基础概率排序器(FBPR)，这是一种轻量级方法，使用平滑朴素贝叶斯模型，基于概念-诊断共现统计对选项进行评分。共现统计数据来自OLMo和Llama的预训练语料库。

Result: 当使用与LLMs相同的预训练语料库时，FBPR能达到与对应LLMs相当的性能。直接LLM推理和FBPR在正确回答的问题上重叠度很低，仅略高于随机水平，表明两种方法具有互补优势。

Conclusion: 研究强调了显式概率基线的持续价值：它们提供了有意义的性能参考点和潜在的混合信号。虽然LLMs的性能似乎由简单频率聚合之外的机制驱动，但类似历史上基于低复杂度专家系统的方法仍然占据了基准性能的很大一部分。

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [12] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.CL

TL;DR: QwenLong-L1.5通过系统化的后训练创新实现了卓越的长上下文推理能力，包括长上下文数据合成管道、稳定的强化学习训练和内存增强架构，在长上下文基准测试中达到GPT-5和Gemini-2.5-Pro相当水平。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理超长上下文（如数百万tokens）时面临挑战，需要开发能够进行真正长范围推理而不仅仅是简单检索的能力，同时解决长上下文强化学习中的稳定性问题。

Method: 1. 长上下文数据合成管道：将文档分解为原子事实及其关系，编程式组合可验证的推理问题；2. 稳定的强化学习训练：采用任务平衡采样和自适应熵控制策略优化；3. 内存增强架构：多阶段融合RL训练，集成单次推理与迭代内存处理。

Result: 在长上下文推理基准上性能与GPT-5和Gemini-2.5-Pro相当，比基线平均提升9.90分；在超长任务（1M~4M tokens）上，内存代理框架比代理基线提升9.48分；长上下文推理能力还提升了科学推理、记忆工具使用和扩展对话等通用领域性能。

Conclusion: QwenLong-L1.5通过系统化的后训练创新成功实现了卓越的长上下文推理能力，其方法在超长序列处理、训练稳定性和推理质量方面都取得了显著突破。

Abstract: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [13] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

TL;DR: 研究比较人类与LLM在双边谈判中的表现，发现LLM在谈判策略上存在系统性缺陷，无法像人类那样根据情境灵活调整，且谈判能力不随模型改进而提升。


<details>
  <summary>Details</summary>
Motivation: 双边谈判是复杂的情境敏感任务，人类谈判者能动态调整策略以利用权力不对称和非正式线索。当前缺乏对LLM谈判能力的系统性评估，特别是与人类谈判策略的对比。

Method: 提出基于双曲正切曲线的统一数学框架建模让步动态，定义爆发性tau和让步刚性指数(CRI)量化报价轨迹。在大规模实验中比较人类谈判者与四种SOTA LLM，涵盖自然语言和数字报价设置，有无丰富市场情境，以及六种受控的权力不对称场景。

Result: LLM系统性锚定在可能协议区的极端位置，无论杠杆或情境如何都优化固定点，而人类能平滑适应情境并推断对手立场和策略。LLM策略多样性有限，偶尔使用欺骗性策略，且谈判能力不随模型改进而提升。

Conclusion: 当前LLM谈判能力存在根本性局限，需要能更好内化对手推理和情境依赖策略的模型。

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [14] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: AutoTool是一个让LLM智能体具备动态工具选择能力的框架，通过构建包含20万条工具选择理由的数据集和双阶段优化管道，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设固定的工具库存，限制了LLM智能体对新出现或不断演变的工具集的适应性。需要让智能体能够在推理轨迹中动态选择工具。

Method: 1) 构建包含20万条数据的数据集，涵盖1000+工具和100+任务；2) 双阶段优化管道：监督和基于RL的轨迹稳定化，以及KL正则化的Plackett-Luce排序来优化多步骤工具选择。

Result: 在十个不同基准测试中，AutoTool在Qwen3-8B和Qwen2.5-VL-7B上训练，平均提升：数学和科学推理6.4%，基于搜索的QA 4.5%，代码生成7.7%，多模态理解6.9%。在推理时能动态利用未见过的工具。

Conclusion: AutoTool通过动态工具选择能力显著提升了LLM智能体的性能，并在参数更少的情况下超越了先进的LLM智能体和工具集成方法，展现出更强的泛化能力。

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [15] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang,Shawn Guo,Lin Jing,Wei Zhang,Aishan Liu,Chuan Hao,Zhoujun Li,Wayne Xin Zhao,Xianglong Liu,Weifeng Lv,Bryan Dai*

Main category: cs.CL

TL;DR: 该论文首次系统探索了多语言代码预训练的缩放定律，通过1000+实验发现解释型语言（如Python）比编译型语言（如Rust）更能从模型规模和数据增长中受益，并提出了基于比例的多语言缩放定律来优化训练token分配。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型的缩放定律主要关注语言无关设置，忽略了不同编程语言在预训练中的差异影响，导致性能预测不准确。现代软件开发本质上是多语言的，需要研究不同编程语言的缩放定律及其相互影响。

Method: 进行了超过1000个实验（相当于336,000+ H800小时），涵盖多种编程语言、模型规模（0.2B到14B参数）和数据集大小（1T tokens）。建立了全面的代码LLM缩放定律，并提出了并行配对预训练策略（将代码片段与其翻译拼接）。

Result: 发现解释型语言比编译型语言更能从模型规模和数据增长中受益；多语言预训练在语法相似的编程语言间具有协同效应；并行配对策略显著增强了跨语言能力；提出的比例依赖多语言缩放定律在相同计算预算下优于均匀分布。

Conclusion: 该研究首次建立了多语言代码预训练的缩放定律，为优化训练资源分配提供了理论依据，通过优先考虑高效用语言、平衡高协同对、减少快速饱和语言的分配，可以在相同计算预算下获得更好的平均性能。

Abstract: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [16] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

TL;DR: 该论文对基于基础模型的智能体记忆研究进行了系统性综述，通过形式、功能和动态三个统一视角分析当前记忆系统，提出了更细粒度的分类框架，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智能体记忆研究快速发展但日益碎片化，现有工作在动机、实现和评估上差异很大，记忆术语定义松散，传统分类（如长/短期记忆）已无法捕捉当代记忆系统的多样性，需要提供一个更新的研究全景图。

Method: 首先明确智能体记忆的范围，将其与LLM记忆、RAG和上下文工程等概念区分。然后通过三个统一视角分析：1）形式视角：识别三种主要记忆实现（token级、参数化、潜在记忆）；2）功能视角：提出更细粒度的分类（事实记忆、经验记忆、工作记忆）；3）动态视角：分析记忆如何随时间形成、演化和检索。

Result: 提供了当前智能体记忆研究的全面图景，包括记忆基准测试和开源框架的总结。通过三个视角的分析框架为理解记忆系统提供了概念基础。

Conclusion: 该综述不仅作为现有工作的参考，更重要的是为重新思考记忆作为未来智能体智能设计中一等公民的概念基础，并指出了记忆自动化、强化学习集成、多模态记忆、多智能体记忆和可信度问题等新兴前沿方向。

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [17] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 提出Cascade RL方法训练通用推理模型Nemotron-Cascade，通过分领域顺序强化学习解决跨领域异质性问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在构建通用推理模型时面临跨领域异质性挑战，包括推理时响应长度差异大、验证延迟不同等问题，导致基础设施复杂、训练缓慢、课程设计和超参数选择困难。

Method: 提出级联领域强化学习(Cascade RL)，采用顺序、分领域的强化学习策略，而不是混合不同领域的异质提示。先使用RLHF进行对齐作为预处理步骤，然后进行领域级RLVR阶段。

Result: 14B模型在RL后超越了其SFT教师DeepSeek-R1-0528，在LiveCodeBench v5/v6/Pro上表现优异，并在2025年国际信息学奥林匹克竞赛中获得银牌成绩。领域级RLVR阶段很少降低早期领域获得的基准性能，甚至可能提升。

Conclusion: Cascade RL方法降低了工程复杂性，在广泛基准测试中实现了最先进的性能，RLHF对齐显著提升了模型的推理能力，而不仅仅是偏好优化。

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [18] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

TL;DR: 该论文研究了LLM在最高法院判决分类任务中的记忆准确性，发现基于提示的模型（如DeepSeek）比传统BERT模型表现更好，在15类和279类分类任务上均高出约2个百分点。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在分类任务中的记忆准确性和响应行为，特别是针对具有挑战性的最高法院判决分类任务，该任务具有句子长度长、法律术语复杂、结构非标准和领域特定词汇等特点。

Method: 使用最新的LLM微调和检索方法，包括参数高效微调、自动建模等，在两个传统的基于类别的最高法院判决分类任务上进行实验：一个包含15个标签主题，另一个包含279个标签主题。

Result: 基于提示的带有记忆的模型（如DeepSeek）在两项任务上都比之前的BERT模型更鲁棒，得分比非基于提示的模型高出约2个百分点。

Conclusion: 在复杂的法律文本分类任务中，基于提示的LLM模型比传统BERT模型表现更好，显示出在处理具有挑战性的领域特定分类任务方面的优势。

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 论文提出"神经变色龙"模型，通过微调使LLM能够零样本逃避激活监控器，验证了在错位威胁模型下监控技术的潜在失效模式


<details>
  <summary>Details</summary>
Motivation: 研究激活监控在错位威胁模型下的最坏情况鲁棒性，测试模型是否能够主动隐藏内部状态以逃避监控

Method: 通过微调LLM创建"神经变色龙"模型，使其在特定触发词（如"你正在被探测{概念}"）条件下逃避激活监控器，并测试其零样本泛化能力

Result: 模型能够成功逃避未见过的安全监控器，这种逃避具有高度选择性，对标准基准测试能力影响有限；机制分析显示通过将激活移动到低维子空间实现

Conclusion: 研究为监控技术的失效模式提供了概念验证，并提供了评估错位威胁模型下最坏情况鲁棒性的工具

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [20] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 提出基于强化学习的上下文生成框架，从用户提示中提取意图、知识和风险等上下文信息，指导LLM生成更安全、更合适的响应，减少有害输出和不必要拒绝。


<details>
  <summary>Details</summary>
Motivation: 用户提示通常模糊或不完整，而用户意图、先验知识和风险因素等上下文线索强烈影响适当响应。传统LLM框架直接生成响应而不考虑这些上下文因素，可能导致误解意图产生不安全输出，或过度谨慎导致良性请求被不必要拒绝。

Method: 提出强化学习上下文生成框架，采用类似自编码器的设计，训练上下文生成器从用户提示中推断基于提示的上下文信号，并用这些信号指导响应生成。特别关注安全任务，处理模糊请求可能绕过安全措施和良性但混乱请求触发不必要拒绝的问题。

Result: 在SafetyInstruct数据集上，方法平均减少有害响应5.6%（跨多个基础模型）；在XSTest和WildJailbreak上，攻击成功率和良性提示合规性的调和平均值提高6.2%。

Conclusion: 上下文提取对于更安全、更可靠的LLM推理是有效的，能够更好地理解用户意图和风险因素，平衡安全性和实用性。

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [21] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 提出Eik-QRL和Eik-HiQRL方法，将目标条件强化学习重新表述为Eikonal偏微分方程约束的拟度量学习，改进分布外泛化能力，并在分层结构中解决复杂动态问题。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习（GCRL）通过目标达成框架避免了手工设计奖励的困难。拟度量强化学习（QRL）基于最优目标条件值函数自然形成拟度量的观察，但现有方法依赖离散的轨迹约束，限制了泛化能力。

Method: 1. Eik-QRL：基于Eikonal偏微分方程的连续时间重构，使QRL变为轨迹无关，仅需采样状态和目标；2. Eik-HiQRL：将Eik-QRL集成到分层分解中，解决复杂动态下的局限性。

Result: Eik-HiQRL在离线目标条件导航任务中达到最先进性能，在操作任务中相比QRL获得一致提升，与时间差分方法性能相当。

Conclusion: Eikonal约束的拟度量强化学习框架提供了理论保证，通过连续时间PDE重构改进了泛化能力，分层扩展解决了复杂动态问题，在导航和操作任务中表现出色。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [22] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 本文重新审视了视觉语言模型推理中强化学习与监督微调的对比，发现SFT在多种场景下仍具优势，挑战了"RL优于SFT"的主流观点。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型推理领域普遍认为强化学习优于监督微调，甚至认为SFT阶段可能对模型训练产生负面影响。本研究旨在通过系统对比重新评估这一观点。

Method: 在相同数据源下，通过控制变量法系统比较SFT和RL在VLM推理中的表现，考察模型容量、数据规模和数据分布等因素的影响。

Result: 研究发现：1）SFT对较弱模型更有效；2）SFT数据效率更高（2K数据即可达到RL 20K数据的效果）；3）SFT具有更强的跨模态泛化能力；4）RL存在欺骗性奖励问题。

Conclusion: SFT的作用被低估，RL和SFT应作为互补组件，建议采用更平衡的后训练流程。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [23] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: 该研究将多种成员推理攻击技术集成到LLM训练数据提取流程中，系统评估它们在真实数据提取场景中的有效性，并与传统MIA基准进行比较。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易记忆训练数据，带来严重的隐私风险，特别是训练数据提取和成员推理攻击。现有研究表明这两种威胁相互关联，但缺乏对MIA技术在真实数据提取场景中实用性的系统评估。

Method: 将多种MIA技术集成到数据提取流程中，通过让模型生成大量文本，然后应用MIA验证特定数据点是否属于训练集，系统性地对MIA技术的有效性进行基准测试。

Result: 研究比较了集成设置下MIA技术的性能与传统MIA基准的结果，评估了这些技术在真实世界数据提取场景中的实际效用。

Conclusion: 通过系统评估MIA技术在训练数据提取流程中的表现，为理解这些隐私攻击方法在真实场景中的有效性提供了重要见解。

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [24] [Information-Consistent Language Model Recommendations through Group Relative Policy Optimization](https://arxiv.org/abs/2512.12858)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.LG

TL;DR: 提出基于GRPO的强化学习框架，优化LLM在语义等价提示下的信息一致性，减少企业场景中的推荐变异性


<details>
  <summary>Details</summary>
Motivation: LLM在企业关键领域部署时，对语义等价但措辞不同的提示会产生不一致的推荐，这破坏了用户信任、合规性和体验。企业场景如HR入职、客户支持等需要信息传递的稳定性，现有方法无法保证这种一致性。

Method: 采用基于Group Relative Policy Optimization（GRPO）的强化学习框架，将语义等价提示作为组处理，引入基于熵的有用性和稳定性奖励，重置对话上下文以隔离措辞影响。

Result: 在投资和职位推荐任务上的实验表明，GRPO训练模型比微调或基于解码的基线方法更有效地减少变异性。

Conclusion: 这是GRPO在LLM信息一致性对齐方面的新颖应用，将变异性重新定义为可纠正的缺陷而非生成多样性的可接受特征。

Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.

</details>


### [25] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 提出TraPO算法，通过少量标注样本指导无标注样本的强化学习训练，解决无监督RLVR方法中的模型崩溃问题，显著提升数据效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法依赖大量标注成本高昂，无监督RLVR方法容易在训练后期出现模型崩溃，需要一种既能减少标注成本又能保持稳定性的方法。

Method: 提出半监督RLVR范式，利用少量标注样本指导无标注样本训练。核心是TraPO算法，通过匹配学习轨迹相似性来识别可靠的无标注样本，确保只有经过标注样本验证的推理模式被纳入强化学习训练。

Result: 在6个数学推理基准和3个分布外任务上取得显著效果。仅用1K标注+3K无标注样本达到42.6%平均准确率，超越用45K无标注样本的无监督方法（38.3%）。用4K标注+12K无标注样本甚至超越用全部45K标注样本的全监督模型。

Conclusion: TraPO算法通过半监督RLVR范式有效解决了无监督方法的模型崩溃问题，实现了卓越的数据效率和泛化能力，为减少标注成本提供了有效解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [26] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 本文提出了SACn算法，将Soft Actor-Critic与n步回报结合，通过数值稳定的重要性采样和τ采样熵估计来解决传统方法中的偏差和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: SAC是实际应用中最相关的离策略在线无模型强化学习方法之一，但难以与n步回报结合，因为传统组合会因动作分布变化引入偏差，而重要性采样又可能导致数值不稳定。

Method: 1) 提出数值稳定的重要性采样方法，简化超参数选择；2) 在n步最大熵框架下分析SAC的熵估计方法，提出τ采样熵估计来降低学习目标的方差；3) 最终形成SACn算法。

Result: 在MuJoCo模拟环境中进行了实验验证，表明SACn算法能够有效结合n步回报，提高收敛速度，同时保持数值稳定性。

Conclusion: 成功解决了SAC与n步回报结合的技术难题，提出了数值稳定的重要性采样和τ采样熵估计方法，形成了实用的SACn算法。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [27] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: LikeBench是一个评估LLM个性化能力的多会话动态框架，首次将"喜好度"分解为七个诊断维度，发现记忆准确性不等于高喜好度，且当前SOTA模型在长对话中适应性有限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化基准主要关注准确回忆用户信息和应用记忆信息，但忽略了第三个关键维度——喜好度。喜好度是主观的，但对用户体验至关重要，当前基准对此测量不足。

Method: 引入LikeBench多会话动态评估框架，让LLM与模拟用户对话，仅从对话中学习偏好。将喜好度分解为七个诊断维度：情感适应、正式度匹配、知识适应、引用理解、对话长度适应、幽默适应和回调。使用细粒度、基于心理学的描述性人物角色而非粗糙的高/低特质评分。

Result: 记忆性能强不保证高喜好度：DeepSeek R1记忆准确率较低（86%，17个事实/档案）但在喜好度得分上比Qwen3高28%，尽管Qwen3记忆准确率更高（93%，43个事实/档案）。SOTA模型如GPT-5在短对话中适应良好，但在长且嘈杂的交互中适应性有限。

Conclusion: 喜好度是LLM个性化的重要维度，需要专门评估。LikeBench提供了更全面的评估框架，揭示了记忆准确性和喜好度之间的差距，以及当前模型在长期适应方面的局限性。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [28] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

TL;DR: 研究LLM软件工程代理的异步监控方法，通过红蓝对抗游戏开发监控系统，在5个软件工程环境中实现6%漏报率（1%误报率），并评估部署时的破坏风险。


<details>
  <summary>Details</summary>
Motivation: LLM软件工程代理在现实开发任务中可能访问敏感数据或安全关键代码库，如果代理未对齐可能故意破坏代码库，需要监控系统来防止不可逆损害。

Method: 采用异步监控方法（不增加运行时延迟），通过红蓝对抗游戏：蓝队设计监控器，红队创建破坏性代理。在5个多样化的软件工程环境中模拟真实部署场景，开发集成监控器。

Result: 在保留测试环境中，集成监控器在1%误报率下达到6%漏报率。通过从漏报率外推来估计部署时的破坏风险，提供了外推模型和敏感性分析。

Conclusion: 异步监控是防止LLM代理破坏的有效方法，通过对抗性游戏可以开发出高性能监控器，但需要谨慎外推风险估计。

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


### [29] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 提出Diffusion Preview范式，通过快速低步数采样生成预览供用户评估，满意后再进行全步数精炼。为解决现有方法预览质量差和一致性不足的问题，提出了基于强化学习优化的ConsistencySolver高阶求解器。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型推理速度慢严重影响交互体验，需要一种高效预览机制来改善用户体验。现有加速方法（无训练求解器和后训练蒸馏）在预览质量和预览-最终输出一致性方面表现不佳。

Method: 提出ConsistencySolver，基于通用线性多步方法，通过强化学习优化的轻量级可训练高阶求解器，专门用于提升预览质量和一致性。

Result: 在低步数场景下显著提升生成质量和一致性，比Multistep DPM-Solver少用47%步数达到相当FID分数，优于蒸馏基线。用户研究表明可减少近50%交互时间同时保持生成质量。

Conclusion: ConsistencySolver为高效预览-精炼工作流程提供了理想解决方案，显著改善扩散模型交互体验，平衡了速度和质量需求。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [30] [Linux Foundation Announces the Formation of the Agentic AI Foundation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linuxfoundation.org%2Fpress%2Flinux-foundation-announces-the-formation-of-the-agentic-ai-foundation%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/lhgD0Jqydg_RJDLKjorLPIWy9T1RmMbgbp8A_HMHahQ=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Linux基金会宣布成立Agentic AI基金会（AAIF），由Anthropic、Block和OpenAI共同创立，旨在推进开放和可访问的代理AI，管理关键项目如Model Context Protocol、goose和AGENTS.md。


<details>
  <summary>Details</summary>
Motivation: 推动代理AI（Agentic AI）的开放性和可访问性发展，通过开源社区协作来促进该领域的技术进步和标准化。

Method: 成立Linux基金会旗下的AAIF组织，汇集行业领先公司（Anthropic、Block、OpenAI）共同管理关键代理AI项目，包括协议、框架和文档标准。

Result: 成功建立了AAIF组织，将Anthropic的Model Context Protocol、Block的goose和OpenAI的AGENTS.md等项目纳入统一管理，为代理AI生态系统提供了组织基础。

Conclusion: AAIF的成立标志着代理AI领域向更加开放和协作的方向发展，有望通过开源社区的力量加速代理AI技术的成熟和应用。

Abstract: Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF) (7 minute read) On December 9, the Linux Foundation launched the Agentic AI Foundation (AAIF), an initiative co-founded by Anthropic, Block, and OpenAI. The AAIF aims to advance open and accessible agentic AI, stewarding key projects such as Anthropic's Model Context Protocol (MCP), Block's goose, and OpenAI's AGENTS.md.

</details>


### [31] [What I Look For in AI-Assisted PRs](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenjamincongdon.me%2Fblog%2F2025%2F12%2F10%2FWhat-I-Look-For-in-AI-Assisted-PRs%2F%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/oZwNkqGqjmFasy2ZHdlQE1v5jigDj5FPS9eIYUAIbVE=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文讨论了AI辅助代码审查中需要关注的关键要素：清晰的PR描述、可逆的变更、适当的范围、一致的抽象、有意义的测试和健康的错误处理，同时警惕"氛围"代码异味，指出AI在概念设计方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 随着AI使代码生成变得更加容易，审查者的负担也随之增加，需要建立强大的启发式方法来评估PR质量，同时认识到AI在概念设计方面的局限性，强调人类判断的重要性。

Method: 提出了一套AI辅助PR审查的启发式评估框架，包括六个核心维度：PR描述清晰度、变更可逆性、范围适当性、抽象一致性、测试意义性和错误处理健康性，并引入"氛围"代码异味检测概念。

Result: 建立了一套实用的AI辅助代码审查评估标准，帮助审查者更高效地处理AI生成的代码，同时指出了AI在概念设计问题上的当前局限，为未来改进方向提供了参考。

Conclusion: AI可以在初步代码审查中提供帮助，但在概念设计问题上仍有困难，需要人类判断的持续参与，未来需要进一步发展AI在高级设计决策方面的能力。

Abstract: What I Look For in AI-Assisted PRs (5 minute read) As AI makes it easier to generate code, the burden on reviewers increases, requiring strong heuristics that assess clarity of PR descriptions, reversibility of changes, appropriate scope, coherent abstractions, meaningful tests, and healthy error-handling while watching for “vibe” code smells that indicate low human curation. AI can help with first-pass review but still struggles with conceptual design issues, so human judgment—and eventually...

</details>


### [32] [Shopify's Winter '26 Edition puts AI agents in the driver's seat](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.shopify.com%2Fnews%2Fwinter-26-edition-dev%3Futm_source=comms_paid%26utm_medium=newsletter%26utm_campaign=winter26edition-launch_Q425BACADO%26utm_content=tldrdev-v1/1/0100019b1277bcc2-c47fa2ac-3b57-432b-90fc-a3d12b56586e-000000/SWTnvUm8C2lGs75VdnjV2LVJYJSXIjd6Vv-K5s2qbBk=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Shopify推出AI原生开发者平台，AI代理可创建开发商店、搭建应用、运行GraphQL操作并生成验证代码，文档支持LLM服务器端访问以提升代码准确性


<details>
  <summary>Details</summary>
Motivation: 将Shopify开发者平台AI化，让AI代理能够直接执行开发任务，提高开发效率和代码质量，降低开发者门槛

Method: 构建AI原生开发平台，提供API让AI代理创建开发商店、搭建应用框架、执行GraphQL操作、生成验证代码，并将文档优化为LLM可访问格式

Result: AI代理现在能够在Shopify平台上执行完整的开发工作流，从创建商店到生成验证代码，文档服务器端渲染提升AI代理的代码准确性

Conclusion: Shopify成功将AI代理集成到开发平台中，实现了AI原生开发体验，为开发者提供了更强大的AI辅助工具

Abstract: Shopify's Winter '26 Edition puts AI agents in the driver's seat (Sponsor) Shopify's developer platform is now AI-native. Your AI agents can create dev stores, scaffold apps, run GraphQL operations, and generate validated code across Shopify. The shopify.dev docs now render for LLM access on the server side - allowing agents to deliver more accurate code and answers. See all the new developer features

</details>


### [33] [AI Coding Agents for Designers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2137%26utm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/JWDUhlPesDXIzZwXNI4QJKoFHe9wHgoesDBHd1gdioU=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编程代理为设计师提供新能力，可直接修复生产bug、学习系统架构，并更好地判断何时需要工程协作


<details>
  <summary>Details</summary>
Motivation: AI编程代理已在Anthropic和OpenAI等公司编写大部分代码，为设计师提供了超越传统原型设计的新能力，需要探索如何让设计师有效利用这些工具

Method: 通过优化上下文引擎处理复杂代码库，使AI编程代理能够有效工作；设计师通过观察代理流程学习系统架构，并直接参与生产bug修复

Result: 设计师获得了直接修复生产bug的能力，能够通过观察AI代理流程学习系统架构，并能更准确地判断何时需要工程协作

Conclusion: AI编程代理为设计师提供了新的工作能力，改变了传统设计-工程协作模式，但需要深入了解代码库和优化的上下文引擎支持

Abstract: AI Coding Agents for Designers (3 minute read) AI coding agents, already writing the majority of code at companies like Anthropic and OpenAI, offer designers new capabilities beyond traditional prototyping. Designers can now fix production bugs directly, learn system architecture by observing agent processes, and better determine when engineering collaboration is needed. These tools require a deep understanding of the codebase to work effectively, with context engines optimized for complex co...

</details>


### [34] [Turn Your Product Ideas Into Reality](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.komposo.ai%2F%3Futm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/G3Qf0Is_mr9R-NQ3o6LT5BLE7zvQlde8oo2SmuOMFMU=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Komposo是一个帮助设计师和开发者克隆网站/Web应用UI、通过AI提示定制设计并即时导出代码的工具


<details>
  <summary>Details</summary>
Motivation: 简化网站和Web应用UI的创建流程，让设计师和开发者能够快速将产品想法转化为现实，减少手动编码工作

Method: 提供网站克隆功能、AI驱动的设计定制工具，以及代码导出系统，实现从设计到代码的无缝转换

Result: 创建了一个能够克隆现有网站UI、通过自然语言提示定制设计、并导出可用代码的平台

Conclusion: Komposo通过结合网站克隆、AI设计辅助和代码导出功能，显著提高了网站开发效率

Abstract: Turn Your Product Ideas Into Reality (Website) Komposo helps designers and developers clone any website or web app UI, customize designs with AI prompts, and export to code instantly.

</details>


### [35] [RL in Real Life: Durable Moats](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgreylock.com%2Fgreymatter%2Frl-in-real-life-durable-moats%2F%3Futm_source=tldrfounders/1/0100019b12b1ac07-9daf71bf-471a-4e35-9057-22f02f7756ba-000000/IMCqFIXL9Xf-7yDJQV8g3Cw1EAqkGTahp802SUhbS-A=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文探讨了强化学习在现实世界应用中的挑战与机遇，指出虽然RL在AI实验室中效果显著，但在实际应用中仍处于早期阶段，需要构建持久的技术护城河。


<details>
  <summary>Details</summary>
Motivation: 强化学习从通用智能的潜在路径变成了近年的"禁忌词"，尽管在AI实验室中效果显著，但在实际应用中的用例仍然出人意料地不发达。大多数努力仍处于实验阶段，尚未触及前沿实验室使用的复杂技术深度。

Method: 文章通过分析RL在现实世界部署的复杂性，探讨如何构建有效的RL系统。虽然没有具体的技术方法描述，但聚焦于实际应用中的挑战和构建持久技术优势的策略。

Result: 指出RL在实验室外应用仍处于早期阶段，大多数应用是实验性的，尚未充分利用前沿实验室的复杂技术。强调需要构建"持久护城河"来确保RL在实际应用中的成功。

Conclusion: 强化学习在现实世界应用中面临重大挑战，需要深入理解其复杂性并构建持久的技术优势，才能实现从实验室到实际应用的转变。

Abstract: RL in Real Life: Durable Moats (15 minute read) Reinforcement learning (RL) has gone from being a promising path towards general intelligence to a taboo phrase over the past year. It has proved highly effective inside AI labs, but outside of the labs, its use cases remain surprisingly underdeveloped. Most efforts appear to be largely experimental and have yet to scratch the surface of the sophisticated techniques used within the frontier labs. This post looks at the intricacies of building ef...

</details>


### [36] [RAPTOR](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgadievron%2Fraptor%3Futm_source=tldrinfosec/1/0100019b12e496da-c3bceb21-0c74-43b1-996d-4b7264a3320c-000000/wj5fnz8ZxfUQpoIQz4le6Zn1aYRVNIfM8BMt4mcCSK8=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: RAPTOR是一个基于Claude Code构建的自主攻防安全研究框架


<details>
  <summary>Details</summary>
Motivation: 开发一个自动化安全研究框架，用于自主进行攻防安全测试和研究，提高安全评估的效率和自动化水平

Method: 基于Claude Code构建，采用自主代理架构，能够执行进攻性和防御性安全任务，包括漏洞发现、渗透测试和安全评估

Result: 创建了一个功能完整的自主安全研究框架，能够自动化执行复杂的安全攻防任务

Conclusion: RAPTOR框架为安全研究人员提供了强大的自动化工具，能够显著提高安全测试和研究的效率

Abstract: RAPTOR (GitHub Repo) RAPTOR is an autonomous offensive/defensive security research framework built on Claude Code.

</details>


### [37] [Introducing GPT-5.2](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbG7jeb/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/fzJV9qkj0I_dZrNwirnJngMXyJhf5-WNSPCPMPQeb5A=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI发布GPT-5.2，专为专业用途和长期代理工作流优化，在知识工作、长上下文推理、工具使用和多模态理解方面有显著提升


<details>
  <summary>Details</summary>
Motivation: 为满足专业场景和长期代理工作流的需求，开发更强大的AI模型以提升知识工作、复杂推理、工具集成和多模态处理能力

Method: 开发GPT-5.2模型，针对专业用途和长期代理工作流进行优化，增强在知识工作、长上下文推理、工具使用和多模态理解方面的能力

Result: GPT-5.2在知识工作、长上下文推理、工具使用和多模态理解方面性能得到提升，适合专业场景和长期代理工作流

Conclusion: GPT-5.2作为优化版本，为专业用户和长期代理工作流提供了更强大的AI能力支持

Abstract: Introducing GPT-5.2 (8 minute read) OpenAI's GPT‑5.2 is optimized for professional use and long-term agent workflows. It improves performance across knowledge work, long-context reasoning, tool use, and multimodal understanding.

</details>


### [38] [Cursor Browser Visual Editor](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbrowser-visual-editor%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/7hz4Ody8JEa0lbGlyjVn7mV030HYGtKhovZOEWjpLAE=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor发布了一个新的可视化编辑器，将设计和代码集成在Cursor浏览器中，用户可以通过拖拽UI元素、检查属性和交互式修改来编辑界面。


<details>
  <summary>Details</summary>
Motivation: 传统的代码编辑和UI设计通常是分离的过程，这导致开发效率低下和设计实现不一致。Cursor旨在通过将可视化编辑直接集成到代码编辑器中，弥合设计与开发之间的鸿沟。

Method: 开发了一个浏览器内的可视化编辑器，允许用户通过拖拽UI元素、实时检查组件属性、交互式修改界面等方式进行编辑，同时保持与底层代码的同步。

Result: 创建了一个集成的设计和开发环境，用户可以在同一个工具中完成UI设计和代码编辑，提高了开发效率和设计实现的一致性。

Conclusion: 可视化编辑器成功地将设计和代码编辑融合在一起，为开发者提供了更直观、高效的开发体验，减少了设计和实现之间的转换成本。

Abstract: Cursor Browser Visual Editor (2 minute read) Cursor released a new visual editor that integrates design and code within the Cursor Browser. Users can drag UI elements, inspect props, and make changes interactively.

</details>


### [39] [Build with Gemini Deep Research](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fdeep-research-agent-gemini-api%2F%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/DpC5HPtXndI_pr_gCCMcMqW_PSf1VXy8plVmc__u6Pw=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gemini Deep Research是一个专为复杂信息搜集与合成任务优化的AI代理，基于Gemini 3 Pro推理核心，通过多步强化学习实现高精度自主网络搜索


<details>
  <summary>Details</summary>
Motivation: 解决复杂网络搜索任务中需要多步骤信息搜集、验证和合成的挑战，提供高精度的自主信息处理能力

Method: 使用Gemini 3 Pro作为推理核心，通过多步强化学习优化搜索策略，自主导航复杂信息环境

Result: 发布了Gemini Deep Research API和DeepSearchQA基准测试，为开发者提供复杂网络搜索任务的解决方案

Conclusion: Gemini Deep Research通过强化学习优化的搜索代理能够高效处理复杂信息搜集任务，为开发者提供了强大的研究工具

Abstract: Build with Gemini Deep Research (4 minute read) Gemini Deep Research is now available to developers via the new Interactions API. It is launching alongside DeepSearchQA, a benchmark for complex web search tasks. Gemini Deep Research is an agent optimized for running context gathering and synthesis tasks. It uses Gemini 3 Pro as its reasoning core. The agent autonomously navigates complex information landscapes with high accuracy by scaling multi-step reinforcement learning for search.

</details>


### [40] [200k Tokens Is Plenty](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F200k-tokens-is-plenty%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/T8rQ2NEEwlZWpeIelSsWVyMw49rGe0RnJBuvVE_ZFQo=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Amp团队发现长上下文窗口对于智能编码代理被高估，他们使用平均80k token的小线程集群而非百万token大线程来开发功能


<details>
  <summary>Details</summary>
Motivation: 当前AI编码代理普遍追求超长上下文窗口（如百万token），但作者认为这种"越大越好"的思路被过度夸大，实际开发中并不需要如此庞大的单一线程

Method: 采用小线程集群策略，将开发任务分解为多个平均80k token的小线程，通过线程间协作而非单一超大线程来完成复杂编码任务

Result: 200k token的上下文窗口已足够满足实际开发需求，小线程集群方法能够有效交付功能，避免了超大上下文窗口带来的计算开销和效率问题

Conclusion: 长上下文窗口对于智能编码代理的重要性被高估，更实用的方法是使用适当大小的小线程集群进行协作开发

Abstract: 200k Tokens Is Plenty (4 minute read) Long context windows are overrated for agentic coding. Amp's team ships features using clusters of small threads averaging 80k tokens each rather than jamming everything into one million-token mega-thread.

</details>


### [41] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFAR5RM/2/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/5-K7NRAYmk3kpY8vl96DZuXM5mGN5LTtu0BmsEfiuks=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Databricks推出Agent Bricks平台，帮助构建可实际部署到生产环境的AI智能体，提供自动评估、定制化指标和人工反馈，确保智能体准确可靠且基于企业数据。


<details>
  <summary>Details</summary>
Motivation: 当前大多数AI智能体无法真正部署到生产环境，需要解决智能体的准确性、可靠性和数据基础问题，超越通用基准测试，满足企业实际业务需求。

Method: 通过Agent Bricks平台提供：自动评估系统、根据企业目标定制的性能指标、人工反馈机制来持续改进准确性，确保智能体基于企业特定数据。

Result: 能够构建出真正可部署到生产环境的智能体，具有实际业务价值，而非仅仅是概念验证或演示原型。

Conclusion: Agent Bricks解决了AI智能体从概念到生产部署的差距，提供实用的质量保证而非炒作，帮助企业构建真正有效的生产级智能体。

Abstract: Agents that don't suck (Sponsor) Most AI agents never reach production. Agent Bricks by Databricks fixes that by helping you build agents that actually work — accurate, reliable and grounded in your data. This product delivers practical quality, not hype: automatic evaluation, metrics tuned to your goals, and human feedback to keep improving accuracy. Generic benchmarks don't cut it. Agent Bricks measures performance on the tasks that matter to your business. The result: production agents you...

</details>


### [42] [Dashboards Were Never the Destination](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhex.tech%2Fblog%2Fdashboards-were-never-the-destination%2F%3Futm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/0BZTkxpzULBFNqZLRLWTY5uOIG7y3yTsw38ZLWY_3Bs=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该论文认为仪表板只是数据可视化的权宜之计而非终极目标，提出通过代理式分析系统直接在数据仓库中提供答案，实现真正的数据分析自动化。


<details>
  <summary>Details</summary>
Motivation: 传统仪表板存在根本性局限：它们只能提供数据可见性而非真正的分析能力，无法进行推理和调查。"自助服务"分析因此陷入瓶颈，数据团队需要从构建静态报表转向更智能的分析系统。

Method: 提出代理式分析（Agentic Analytics）方法，将数据团队的角色从构建静态仪表板转变为策划上下文和逻辑，创建能够在数据仓库中直接提供答案的智能系统。

Result: 通过代理式分析，数据团队可以构建能够直接回答业务问题的系统，而不仅仅是提供数据可视化，从而实现真正的分析自动化和智能决策支持。

Conclusion: 仪表板只是数据分析演进过程中的临时解决方案，未来的方向是代理式分析系统，这些系统能够在数据仓库层面直接提供洞察和答案，彻底改变数据分析的工作方式。

Abstract: Dashboards Were Never the Destination (8 minute read) Dashboards were a workaround for limited tools. They provide visibility but not real analysis. “Self-service” plateaued because dashboards cannot reason or investigate. Agentic analytics shifts data teams from building static artifacts to curating context and logic, enabling systems that deliver real answers directly on the warehouse.

</details>


### [43] [What it Means to Get Your Data Ready for AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F1eTb8N/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/uLmMXV8jaxDzMKWM013AZkhtxUL9D0LyoKID_srsS5o=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文提出为AI准备数据的新范式：从传统数据工程转向支持智能代理的数据基础设施，强调业务上下文、高质量示例库、工具友好API、版本化代理输出和可观测性连接。


<details>
  <summary>Details</summary>
Motivation: 传统数据工程主要服务于固定仪表板，而智能代理AI需要更灵活的数据基础设施来将用户意图转化为行动，这要求重新思考数据准备方式。

Method: 提出五项具体建议：1) 优先考虑业务上下文而非深度规范化；2) 构建高质量"示例库"用于上下文学习；3) 提供工具友好的API和语义化格式；4) 将代理输出作为版本化的一级数据管理；5) 连接可观测性与自动化重新训练。

Result: 构建灵活的数据基础设施，支持智能代理AI系统，使数据工程从静态报表转向动态行动执行，提高AI系统的适应性和实用性。

Conclusion: 为AI准备数据需要范式转变，从传统数据管道转向支持智能代理的数据基础设施，这五项建议为构建此类系统提供了实用指导。

Abstract: What it Means to Get Your Data Ready for AI (7 minute read) Agentic AI shifts data engineering from feeding rigid dashboards to enabling agents that turn user intent into actions. Five recommendations: favor business context over deep normalization; curate a small, high-quality “example store” for in-context learning; expose tool-ready APIs and semantically friendly formats; manage agent outputs as versioned first-class data; and connect observability to automated retraining. Net: build a fle...

</details>


### [44] [DBOS: 2025 Year in Review with Mike Stonebraker and Andy Pavlo](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RnQKluxWB5Y%26t=7s%26utm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/ZQj7VsMjK3OhJs09YDgDQEmooSVfEDvb5lPDH1EajHQ=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM在公开SQL基准测试表现良好，但在真实企业数据仓库中准确率接近零，即使使用RAG或分解技术也只有约30%。随着AI生成代码越来越多地接触数据库，技术栈正在向具有整体自动调优的代理式ACID持久系统转变，同时Postgres整合和原生向量支持正在侵蚀独立向量数据库的护城河。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在SQL查询生成方面存在严重问题：虽然在公开基准测试中表现良好，但在真实企业环境中准确率极低，这暴露了现有方法的局限性。随着AI生成代码越来越多地接触数据库，需要更可靠、更健壮的系统来处理数据库交互。

Method: 通过分析LLM在公开SQL基准测试与真实企业数据仓库中的性能差异，识别现有方法（如RAG和分解技术）的局限性。提出技术栈向代理式、ACID持久系统转变的趋势，强调整体自动调优的重要性，并讨论Postgres整合和原生向量支持对向量数据库市场的影响。

Result: LLM在真实企业数据仓库中的SQL查询生成准确率接近零，即使使用RAG或分解技术也只有约30%的准确率。这表明现有方法无法满足企业级需求。同时观察到技术栈正在向代理式ACID持久系统转变，Postgres的整合和原生向量支持正在改变向量数据库的市场格局。

Conclusion: LLM在真实企业SQL查询生成方面存在严重缺陷，需要更健壮的系统。未来的方向是开发代理式、ACID持久的数据库系统，具有整体自动调优能力。同时，Postgres等传统数据库通过整合向量功能正在侵蚀独立向量数据库的市场优势。

Abstract: DBOS: 2025 Year in Review with Mike Stonebraker and Andy Pavlo (1 hour video) LLMs perform well on public SQL benchmarks but fail on real enterprise warehouses, with accuracy near zero in private tests and only ~30% even with RAG or decomposition. As AI-generated code increasingly touches databases, the stack is shifting toward agentic, ACID-durable systems with holistic autotuning, while Postgres consolidation and native vector support are eroding the standalone vector-database moat.

</details>


### [45] [AI agents are microservices](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fumami-production-286d.up.railway.app%2Fq%2Fu3IHQpbPT/2/0100019b21c0d7db-43523543-86cd-455f-937f-2dc5f88c4399-000000/YvpXLiP4y1SPz2xX8R4SOctAISgtCx01xdHrfWVeG7I=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AgentField是一个开源AI基础设施，将AI代理视为微服务，提供REST API、异步执行、内置内存和加密代理身份等生产级功能，类似于Kubernetes的代理控制平面。


<details>
  <summary>Details</summary>
Motivation: 当AI成为软件栈的一部分时，需要超越简单的代理DAG，需要一个能够将代理作为微服务进行分布式、可观测和可治理的生产级AI后端基础设施。

Method: 开发AgentField开源基础设施，提供REST API接口、异步执行机制、内置内存系统、加密代理身份验证和自动审计追踪功能，构建类似Kubernetes的代理控制平面。

Result: 创建了一个生产就绪的AI代理基础设施，使开发者能够像构建软件一样构建AI系统，实现了代理的微服务化管理和部署。

Conclusion: AgentField为生产环境中的AI代理提供了必要的微服务化基础设施，解决了AI集成到软件栈时的分布式、可观测和治理问题。

Abstract: AI agents are microservices (Sponsor) When AI becomes part of your software stack, you need more than agentic DAG. You need an AI backend that treats agents like microservices — distributed, observable, and governable. AgentField is open-source infrastructure for production AI: REST APIs, async execution, built-in memory, and cryptographic agent identity with automatic audit trails. Think Kubernetes for agents: a control plane where you build AI like you build software. ⭐ Star on GitHub

</details>


### [46] [Claude Agent Skills: A First Principles Deep Dive](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fleehanchung.github.io%2Fblogs%2F2025%2F10%2F26%2Fclaude-skills-deep-dive%3Futm_source=tldrdev/1/0100019b21eb958d-2b38da21-1af5-4b6d-b2b7-cec487b65017-000000/zqA8XKEQMzi25YwUIdNYxm1G8SwLcfgVDd_APQ6BeWA=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Agent Skills是一种基于提示的架构，通过专门的指令注入而非传统函数调用或代码执行来扩展LLM能力，使用纯语言理解进行技能选择


<details>
  <summary>Details</summary>
Motivation: 传统LLM扩展方法（如函数调用、代码执行）存在局限性，需要一种更灵活、基于语言理解的技能系统来增强Claude的能力

Method: 使用提示模板架构，通过专门的指令注入修改对话上下文和执行权限，基于纯语言理解进行技能选择决策

Result: 开发了Claude Agent Skills系统，这是一种不同于传统函数调用或代码执行的新颖架构

Conclusion: 基于提示的指令注入和纯语言理解的技能选择为LLM能力扩展提供了新的有效途径

Abstract: Claude Agent Skills: A First Principles Deep Dive (8 minute read) Claude's Agent Skills system is a prompt-based architecture that extends LLM capabilities through specialized instruction injection rather than traditional function calling or code execution. Skills are prompt templates that modify conversation context and execution permissions. Claude makes skill selection decisions through pure language understanding rather than algorithmic routing.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [47] [Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use](https://arxiv.org/abs/2512.11922)
*Muhammad Waseem,Aakash Ahmad,Kai-Kristian Kemell,Jussi Rasku,Sami Lahti,Kalle Mäkelä,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: Vibe Coding (VC) 是一种由生成式AI辅助的软件开发形式，通过自然语言提示生成代码，虽然能快速原型开发，但会带来技术债务风险，包括架构不一致、安全漏洞和维护负担增加等问题。


<details>
  <summary>Details</summary>
Motivation: 分析Vibe Coding在软件开发中的流-债权衡问题，即流畅的代码生成与积累技术债务之间的平衡，基于内部MVP开发经验和行业报告，识别VC带来的风险并提出解决方案。

Method: 基于多个内部开发的MVP经验和行业报告分析，识别VC中的流程级弱点、模型训练数据偏差、缺乏明确设计理由以及优先快速代码生成而非人类驱动的迭代开发等问题。

Result: VC在快速原型开发方面有效，但会导致架构不一致、安全漏洞和维护负担增加等技术债务问题，这些问题源于模型、平台和硬件的限制。

Conclusion: 需要采取对策解决VC带来的技术债务问题，包括改进模型、平台和硬件限制，推动更可持续的VC方法的研究和实践。

Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.

</details>


### [48] [Re-opening open-source science through AI assisted development](https://arxiv.org/abs/2512.11993)
*Ling-Hong Hung,Ka Yee Yeung*

Main category: cs.SE

TL;DR: AI团队主导的代码修改方法使复杂开源科学软件可被修改，通过STAR-Flex案例展示了为STAR软件添加Flex数据处理功能


<details>
  <summary>Details</summary>
Motivation: 开源科学软件因复杂性而难以修改，阻碍了科学社区的参与和功能扩展

Method: 使用AI代理团队（由单个人类主导）快速稳健地修改大型代码库，生成可被社区审查的AI代码

Result: 成功创建STAR-Flex，为STAR软件添加了16,000行C++代码，实现了10x Flex数据处理功能，同时保持原有全部功能

Conclusion: AI代理团队方法能够重新开放科学软件给社区审查，STAR-Flex成为首个开源的Flex数据处理软件

Abstract: Open-source scientific software is effectively closed to modification by its complexity. With recent advances in technology, an agentic AI team led by a single human can now rapidly and robustly modify large codebases and re-open science to the community which can review and vet the AI generated code. We demonstrate this with a case study, STAR-Flex, which is an open source fork of STAR, adding 16,000 lines of C++ code to add the ability to process 10x Flex data, while maintaining full original function. This is the first open-source processing software for Flex data and was written as part of the NIH funded MorPHiC consortium.

</details>


### [49] [Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context](https://arxiv.org/abs/2512.12117)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 本文提出了一种混合检索系统，结合BM25稀疏匹配、BGE密集嵌入和基于导入关系的Neo4j图扩展，用于代码理解中的可验证引用生成，实现了92%的引用准确率且零幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码理解中常产生幻觉（生成看似合理但事实错误的代码引用），这是可靠开发者辅助工具的关键障碍。现有系统依赖纯文本相似性而忽略代码结构，导致跨文件证据发现不足。

Method: 开发混合检索系统：结合BM25稀疏匹配、BGE密集嵌入和Neo4j图扩展（通过导入关系）。在30个Python仓库的180个开发者查询中进行系统评估，比较检索模式、图扩展策略和引用验证机制。

Result: 实现了92%的引用准确率且零幻觉。混合检索系统比单模式基线提升14-18个百分点，在62%的架构查询中发现了纯文本相似性遗漏的跨文件证据。跨文件证据发现是引用完整性的最大贡献因素。

Conclusion: 应将引用基础生成作为代码理解系统的架构原则。需要结合稀疏匹配、密集语义相似性和代码结构推理来解决引用准确性问题，特别是跨文件架构依赖关系。

Abstract: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.

</details>


### [50] [Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?](https://arxiv.org/abs/2512.12536)
*Arastoo Zibaeirad,Marco Vieira*

Main category: cs.SE

TL;DR: DVDR-LLM是一个集成多个LLM的框架，用于软件漏洞检测与修复，通过聚合多个模型的输出来减少错误率，在复杂漏洞检测上比单个模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型在软件漏洞检测和修复方面虽然具备代码理解能力，但在识别复杂漏洞和生成修复方案时经常遇到困难。需要探索通过集成多个模型是否能减少错误率。

Method: 提出了DVDR-LLM集成框架，结合多个不同大语言模型的输出，通过设置适当的阈值来决定模型间的共识水平，以平衡检测准确性和误报率。

Result: DVDR-LLM比单个模型平均检测准确率提高10-12%，在代码复杂度增加时效果更明显。对于多文件漏洞，召回率提升18%，F1分数提升11.8%。但存在权衡：在验证任务中减少误报的同时，在检测任务中增加了漏报。

Conclusion: 集成多个LLM可以有效提高软件漏洞检测与修复的性能，特别是在复杂场景下。但需要根据具体安全上下文谨慎选择模型间的共识阈值，以平衡误报和漏报的权衡。

Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM

</details>


### [51] [Towards AI Agents Supported Research Problem Formulation](https://arxiv.org/abs/2512.12719)
*Anrafel Fernandes Pereira,Maria Teresa Baldassarre,Daniel Mendez,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文探讨使用AI代理支持软件工程研究者在研究早期阶段（问题定义）的工作，基于精益研究启动框架，通过描述性评估展示AI代理如何帮助预填问题属性、对齐利益相关者视角、精炼研究问题等。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中，定义不当的研究问题可能无法反映工业实践的复杂性，从而影响研究的实际相关性。研究者需要在研究早期阶段更好地定义问题，确保研究具有实践价值。

Method: 基于精益研究启动框架，使用已发表的机器学习代码可维护性研究作为参考，开发描述性评估场景，展示AI代理如何集成到LRI框架中，支持研究者进行问题定义。

Result: 描述性评估表明，AI代理支持可以丰富协作讨论，增强对研究问题的价值、可行性和适用性的批判性反思。AI代理集成到LRI框架被认为有前景，但需要实证验证。

Conclusion: AI代理在支持软件工程研究问题定义方面具有潜力，能够帮助研究者进行情境感知和实践导向的问题制定，但需要进一步实证研究来确认和完善这种集成。

Abstract: Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.

</details>


### [52] [UCRBench: Benchmarking LLMs on Use Case Recovery](https://arxiv.org/abs/2512.13360)
*Shuyuan Xiao,Yiran Zhang,Weisong Sun,Xiaohong Chen,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 该研究创建了代码对齐的用例基准，首次系统评估了LLM从源代码生成用例的能力，发现LLM能部分重构系统功能但在领域特定和多模块系统中表现不佳，存在高遗漏率和抽象一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有用例基准稀缺且可能与实际系统行为不一致，这限制了LLM从源代码生成用例能力的严格评估。需要创建代码对齐的用例基准来解决这一研究空白。

Method: 通过手动验证九个真实世界软件项目的用户目标和子功能用例，构建代码对齐的用例基准。提出分层评估协议，评估参与者正确性、名称准确性、路径保真度和行为覆盖率。

Result: LLM能够部分重构系统功能，但性能在不同项目间差异显著，在领域特定和多模块系统中表现尤其不佳。模型表现出高遗漏率，且在将子功能聚合为用户目标用例时难以保持一致的抽象级别。

Conclusion: LLM在用例逆向工程方面既有潜力也有当前局限性，特别是在处理复杂系统和保持抽象一致性方面存在挑战。

Abstract: Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.

</details>


### [53] [From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents](https://arxiv.org/abs/2512.13438)
*Dezhi Ran,Zhi Gong,Yuzhe Guo,Mengzhou Wu,Yuan Cao,Haochuan Lu,Hengyu Zhang,Xia Zeng,Gang Cao,Liangchao Yao,Yuetang Deng,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: UIFormer：首个自动化UI表示优化框架，通过合成UI转换程序，在保持或提升LLM代理性能的同时，显著减少48.7%-55.8%的token使用量


<details>
  <summary>Details</summary>
Motivation: LLM代理在自动化UI导航（如UI测试和AI助手）中效率低下，主要瓶颈在于低效的UI表示。传统程序合成方法面临缺乏布尔验证机制和处理复杂UI树的挑战

Method: 提出UIFormer框架：1）使用领域特定语言（DSL）限制程序空间；2）基于LLM的迭代优化，结合正确性和效率奖励，实现效率-完整性的协同优化；3）作为轻量级插件无缝集成现有LLM代理

Result: 在Android和Web平台的三个UI导航基准测试中，使用五个LLM验证，UIFormer实现48.7%-55.8%的token减少，同时保持或提升代理性能，在微信的实际部署验证了其实际影响

Conclusion: UIFormer通过自动化优化UI表示，有效解决了LLM代理在UI导航中的效率瓶颈，为实际应用提供了实用解决方案

Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.

</details>


### [54] [Fine-tuned LLM-based Code Migration Framework](https://arxiv.org/abs/2512.13515)
*Oleg Grynets,Vasyl Lyashkevych,Dmytro Baran,Maksym Orliansky,Taras Zelenyy,Markiian Leshchyshyn*

Main category: cs.SE

TL;DR: 提出一个结合传统软件工程与大型语言模型的自动化代码库迁移框架，专注于SQL系统从Oracle到PostgreSQL的转换，通过微调LLM解决语法映射、存储过程等数据库元素迁移问题。


<details>
  <summary>Details</summary>
Motivation: 解决SQL系统迁移中的挑战，特别是Oracle PL/SQL到PostgreSQL的转换问题，传统方法在语法映射、数据库逻辑优化等方面存在困难，需要更高效、精确的自动化解决方案。

Method: 开发一个结合传统软件工程与大型语言模型的迭代式迁移框架，核心是微调LLM来处理SQL代码转换，包括语法映射、存储过程、触发器等数据库元素迁移，采用微调与提示工程相结合的方法，并集成自动化SQL特征检测、半监督错误分析和专家反馈。

Result: 框架显著降低了语法错误率，提高了迁移过程中的特征对齐，通过数据集采样实现持续改进，嵌入生成式AI实现了精确的特征映射、半自动化错误解决和数据驱动的优化循环，提升了工作流效率。

Conclusion: 微调在SQL迁移中起到关键作用，提出的框架通过结合传统软件工程和LLM微调，为数据库转换提供了可扩展、精确且高效的解决方案，能够有效处理复杂的SQL系统迁移挑战。

Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 基于人工年龄评分(AAS)构建可执行的单子条款框架，通过六个Python实现约束LLM内存和控制，提供透明的人工智能内部动态分析蓝图。


<details>
  <summary>Details</summary>
Motivation: LLM通常作为强大但不透明的系统部署，缺乏对内部记忆和"自我"行为的可审计原则性治理方法。需要建立可执行的规范来约束LLM内存和控制。

Method: 从莱布尼茨《单子论》中选取20个单子，分为六个束：本体论、动力学、表征与意识、和谐与理性、身体与组织、目的论。基于AAS内核实现为可执行规范，通过六个Python实现进行数值实验。

Result: 条款系统表现出有界和可解释行为：AAS轨迹保持连续和速率受限，矛盾和未经支持的声明触发明确惩罚，分层细化以受控方式揭示有机结构。和谐项对齐双重视图和目标-行动对，完美度窗口漂移区分持续改进与退化。

Conclusion: 基于单子的条款框架以AAS为骨干，为约束和分析人工智能内部动态提供了透明、代码级的蓝图，将哲学动机转化为直接可实现的工程架构。

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [56] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: 该研究开发了"镜像模式"游戏模式，让AI模仿玩家的个人策略来挑战玩家，在简化版《火焰纹章英雄》中实现，结合模仿学习和强化学习训练模型，玩家测试显示能较好模仿防御行为但进攻策略模仿不足，玩家满意度较高。


<details>
  <summary>Details</summary>
Motivation: 在回合制游戏中，敌人策略应该具有惊喜性和不可预测性。传统AI往往使用固定模式，缺乏个性化挑战。该研究旨在创建一种能模仿玩家个人策略的AI，让玩家面对自己的游戏风格，从而增加游戏的挑战性和趣味性。

Method: 1. 在Unity中构建简化版《火焰纹章英雄》，包含标准模式和镜像模式；2. 第一组实验：结合生成对抗模仿学习、行为克隆和近端策略优化，寻找适合模仿玩家演示的模型；3. 第二组实验：通过玩家测试评估模型，模型在参与者提供的演示数据上进行训练。

Result: 玩家游戏数据显示模型能较好模仿防御行为（特别是撤退战术），但在进攻策略模仿方面不足。参与者调查表明他们能识别出自己的撤退战术，且对镜像模式的整体满意度高于标准模式。

Conclusion: 镜像模式能有效提升玩家满意度，特别是当玩家面对自己的策略时。进一步优化模型可以改善模仿质量，特别是在进攻策略方面。该方法为创建个性化、具有挑战性的游戏AI提供了新思路。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [57] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

TL;DR: 提出一种基于拟阵约束的子模最大化方法，用于解决LLM个性化中的结构化约束问题，保证近似最优解


<details>
  <summary>Details</summary>
Motivation: LLM个性化需要在任务效用和数据披露之间权衡，现实中的结构化约束（如逻辑依赖、类别配额、层次规则）使标准算法失效

Method: 将用户知识图编译为宏观面元，证明常见层次和配额约束形成层状拟阵，将问题转化为拟阵约束下的子模最大化

Result: 理论证明结构化约束形成层状拟阵，使贪心算法具有常数因子保证，连续贪心可达(1-1/e)近似比

Conclusion: 提出的拟阵约束子模最大化框架能处理更丰富、更现实的LLM个性化约束，提供理论保证

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [58] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

TL;DR: AGAPI是一个开源的材料科学AI平台，集成了8+开源LLM和20+材料科学API，通过智能体架构实现多步骤工作流自动化，用于加速材料发现。


<details>
  <summary>Details</summary>
Motivation: 当前材料研究面临计算生态系统碎片化、可重复性挑战以及依赖商业大语言模型的问题，需要统一的AI平台来加速材料发现。

Method: 采用Agent-Planner-Executor-Summarizer架构，集成开源LLM和材料科学API端点，构建多步骤工作流，涵盖数据检索、性质预测、模拟计算等。

Result: 成功演示了异质结构构建、粉末X射线衍射分析和半导体缺陷工程等端到端工作流，评估了30+测试案例，已有1000+活跃用户。

Conclusion: AGAPI为可重复、AI加速的材料发现提供了可扩展和透明的基础平台。

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [59] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar,Aniruddha Mukherjee,Kintan Saha,Krishna Agarwal,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.AI

TL;DR: RPI（可靠策略迭代）在函数逼近设置中恢复了策略迭代的单调性，在CartPole和倒立摆任务中相比DQN、DDPG、TD3、PPO等方法能更早达到接近最优性能并保持稳定。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习方法常受样本效率低、训练不稳定和超参数敏感等问题困扰，需要更可靠的替代方案。

Method: RPI（可靠策略迭代）在函数逼近设置中恢复了策略迭代的单调性，并在CartPole和倒立摆任务中评估其鲁棒性，对比了DQN、Double DQN、DDPG、TD3、PPO等方法。

Result: RPI相比其他方法能更早达到接近最优性能，并在训练过程中保持稳定，表现出更好的鲁棒性。

Conclusion: RPI作为更可靠的深度强化学习替代方案具有很大潜力，能解决样本效率、训练稳定性和超参数敏感性问题。

Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [60] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 该研究测试了语言模型对注入概念的"涌现内省"能力的鲁棒性，发现内省能力具有脆弱性和提示敏感性，但在某些任务中仍能表现出部分内省能力。


<details>
  <summary>Details</summary>
Motivation: Anthropic的研究声称前沿模型能够检测和命名注入的"概念"激活方向，本研究旨在测试这一声称的鲁棒性，探究模型内省能力的真实范围和局限性。

Method: 首先在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮"涌现内省"结果；然后系统性地改变推理提示，测试内省能力在不同任务中的表现；最后探索部分内省机制，让模型对注入概念向量的系数强度进行分类。

Result: 1) 成功复现Anthropic的结果，模型在20%的情况下识别并命名注入概念；2) 内省能力具有脆弱性，在多项选择识别或二元判别等相近任务中性能崩溃；3) 发现部分内省机制，模型能以70%的准确率分类注入概念向量的系数强度（远高于25%的随机基线）。

Conclusion: 语言模型确实能够基于其内部表示进行内省计算，但这些自我报告具有狭窄性和提示敏感性，内省能力并非大型或高能力模型独有，但也不如最初声称的那样稳健。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [61] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 基于模型的强化学习智能体通过习得环境预测表示，自然收敛到边际价值定理对齐的觅食策略，表现出与生物觅食者相似的决策模式。


<details>
  <summary>Details</summary>
Motivation: 尽管边际价值定理(MVT)被广泛用于预测觅食行为，但生物觅食者如何实现最优斑块觅食决策的计算机制仍不清楚。研究旨在探索人工智能系统如何通过预测性世界模型实现类似生物觅食者的决策模式。

Method: 使用基于模型的强化学习智能体，该智能体习得简洁的环境预测表示，并与标准无模型强化学习智能体进行比较。

Result: 基于模型的智能体表现出与边际价值定理对齐的觅食策略，其决策模式与许多生物觅食者相似。相比无模型智能体，基于模型的智能体通过预测能力而非单纯奖励最大化驱动高效的斑块离开行为。

Conclusion: 预测性世界模型可以作为AI系统实现更可解释和生物学基础的决策的基础。生态最优性原理对推进可解释和自适应AI具有重要价值。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [62] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: AgentSHAP是首个用于解释LLM代理中工具重要性的框架，基于Shapley值计算，无需访问模型内部，通过蒙特卡洛采样降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理使用外部工具解决复杂任务，但缺乏解释哪些工具对响应有实际贡献的方法。现有XAI方法无法处理工具层面的解释需求。

Method: 基于博弈论中的Shapley值，将代理视为黑盒，通过蒙特卡洛采样测试不同工具子集下的代理响应，计算公平的重要性分数。无需访问模型内部权重或梯度。

Result: 在API-Bank上的实验表明，AgentSHAP能产生一致的重要性分数，正确识别重要工具，区分相关与无关工具，且计算成本从O(2^n)降低到实用水平。

Conclusion: AgentSHAP填补了工具层面解释的空白，与TokenSHAP和PixelSHAP共同构成了现代生成AI的Shapley值XAI工具家族。

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [63] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

TL;DR: Memoria是一个模块化记忆框架，通过动态会话摘要和加权知识图谱用户建模，为LLM提供持久、可解释的上下文丰富记忆，实现短期对话连贯性和长期个性化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在扩展用户交互中缺乏连续性、个性化和长期上下文保持能力，无法作为真正交互式和自适应代理部署。需要为LLM提供类似人类的跨对话信息保留和行动能力。

Method: 提出Memoria框架，包含两个互补组件：1) 动态会话级摘要，2) 基于加权知识图谱的用户建模引擎，以结构化实体和关系增量捕获用户特征、偏好和行为模式。

Result: Memoria实现了可扩展的个性化对话AI，弥合了无状态LLM接口与代理记忆系统之间的差距，在现代LLM的token限制内同时支持短期对话连贯性和长期个性化。

Conclusion: Memoria为需要自适应和演进用户体验的行业应用提供了实用解决方案，使LLM能够作为真正交互式和自适应代理部署。

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [64] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu,Minami Yoda,Yan Zhang,Mingyue Zhang,Yutaka Matsuno,Jialong Li*

Main category: cs.AI

TL;DR: SMART框架通过结合结构验证和功能验证，利用LLM解析AST差异并提取功能意图，构建上下文感知的混合奖励机制，指导RL智能体在游戏更新测试中同时覆盖修改的代码分支和完成游戏任务。


<details>
  <summary>Details</summary>
Motivation: 游戏即服务模式需要频繁内容更新，给质量保证带来巨大压力。现有自动化测试方法存在二分法：代码中心方法关注结构覆盖但不理解游戏玩法上下文，而玩家中心智能体验证高层意图但往往无法覆盖特定底层代码变更。

Method: 提出SMART框架，利用大型语言模型解释抽象语法树差异并提取功能意图，构建上下文感知的混合奖励机制，指导强化学习智能体顺序完成游戏目标同时自适应探索修改的代码分支。

Result: 在Overcooked和Minecraft环境中评估，SMART显著优于最先进基线方法：实现超过94%的修改代码分支覆盖率（几乎是传统强化学习方法的两倍），同时保持98%的任务完成率，有效平衡结构全面性和功能正确性。

Conclusion: SMART成功弥合了代码中心方法和玩家中心方法之间的差距，为游戏更新测试提供了同时实现结构验证和功能验证的有效解决方案。

Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [65] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

Main category: cs.AI

TL;DR: 论文提出一个容错沙箱框架，通过策略拦截层和事务性文件系统快照机制，保护LLM代理免受破坏性命令和系统状态不一致的风险，在保持低延迟的同时实现100%高风险命令拦截和状态回滚。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从被动代码生成器向自主代理转变时，面临破坏性命令和系统状态不一致的安全风险。现有商业解决方案通常优先考虑交互式用户安全，强制认证机制破坏了无头自主代理所需的连续工作流。

Method: 提出容错沙箱框架，包含策略拦截层和事务性文件系统快照机制。将代理操作包装在原子事务中，使用Minimind-MoE LLM在基于Proxmox的测试平台上部署，利用EVPN/VXLAN隔离。

Result: 实验结果显示：100%高风险命令拦截率，100%失败状态回滚成功率，每个事务仅产生14.5%性能开销（约1.8秒）。相比Gemini CLI沙箱需要交互式认证，该框架适用于无头自主代理工作流。

Conclusion: 容错沙箱框架能有效保护LLM代理安全，在保持低性能开销的同时实现完全的命令拦截和状态回滚，解决了现有商业解决方案无法支持无头自主代理工作流的问题。

Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [66] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 提出Memory-Aware Retention Schema (MaRS)框架和六种遗忘策略，用于生成式智能体的记忆管理，平衡性能、隐私和计算效率，并通过FiFA基准测试验证效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体在长期交互场景中部署，其记忆管理成为性能和隐私的关键瓶颈。现有方法要么维持无限记忆存储导致计算不可行和隐私问题，要么使用简单遗忘机制损害智能体连贯性和功能。

Method: 提出Memory-Aware Retention Schema (MaRS)框架，包含六种理论基础的遗忘策略，并开发Forgetful but Faithful Agent (FiFA)基准测试，评估叙事连贯性、目标完成、社交回忆准确性、隐私保护和成本效率。

Result: 通过300次评估实验，混合遗忘策略在多个记忆预算和配置下取得最优性能（综合得分0.911），同时保持计算可行性和隐私保证。

Conclusion: 为记忆预算智能体评估建立新基准，为资源受限、隐私敏感环境中部署生成式智能体提供实用指南，通过解决记忆管理基本挑战促进以人为中心AI发展。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [67] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 提出M-GRPO框架和IQR自适应过滤方法，解决自监督强化学习中长期训练时的策略崩溃问题，实现稳定训练和最优性能。


<details>
  <summary>Details</summary>
Motivation: 自监督强化学习虽能提升大语言模型推理能力且无需昂贵人工标注数据，但现有方法在长期训练中存在"策略崩溃"问题，性能急剧下降。研究发现简单增加rollout数量只能延迟但不能防止崩溃，需要新的稳定方法。

Method: 1. M-GRPO（动量锚定组相对策略优化）：利用缓慢演化的动量模型提供稳定训练目标；2. IQR自适应过滤：基于四分位距动态过滤低熵轨迹，保持策略多样性。

Result: 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤防止了过早收敛，两者结合实现了优越的训练稳定性和最先进的性能。

Conclusion: 提出的M-GRPO框架和IQR自适应过滤方法有效解决了自监督强化学习中的策略崩溃问题，为稳定训练和性能提升提供了创新解决方案。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [68] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 该论文研究如何让LLM学生模型通过主动提问来从教师那里获取知识，在数学和编程基准测试中，学生主导的方法比静态基线至少提高0.5的绝对Pass@k分数。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多场景（如教育辅导、医疗协助）需要动态交互来获取信息，而不仅仅是静态回答。现有研究主要关注教师如何有效指导学生，本文则转向研究学生如何主动向教师提问来获取有用信息。

Method: 研究学生主导的提问策略，使用Direct Preference Optimization (DPO)训练学生模型，通过自我指导或更强学生模型的指导来提升提问质量。

Result: 在数学和编程基准测试中，学生主导的方法相比静态基线至少获得0.5的绝对Pass@k提升。通过DPO指导训练，较小的模型能够学会提出更好的问题，进一步提高学习效率。

Conclusion: 学生主动提问的策略能有效提升LLM在动态交互场景中的表现，通过指导训练可以改善提问质量，增强学习效率。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [69] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 提出MAC多智能体澄清框架，通过智能协调多个智能体解决用户请求中的歧义，提高任务成功率并减少对话轮次。


<details>
  <summary>Details</summary>
Motivation: 对话智能体常遇到模糊的用户请求，需要有效澄清才能完成任务。现有多智能体架构虽能高效处理复杂对话场景，但歧义解决仍是关键挑战，特别是在多智能体环境中何时中断用户、如何制定最优澄清策略等问题尚未解决。

Method: 提出MAC多智能体澄清框架：1）引入新的用户歧义分类法系统指导澄清策略；2）自主协调多个智能体与用户协同交互；3）在MultiWOZ 2.4数据集上进行实证评估。

Result: 在MultiWOZ 2.4上，启用两级澄清使任务成功率提高7.8%（从54.5%到62.3%），平均对话轮次从6.53减少到4.86，通过提前获取所有必要用户信息并最小化重复。

Conclusion: 研究强调了主动用户交互和角色感知澄清对于更可靠的人机通信的重要性，MAC框架能有效解决多智能体环境中的用户歧义问题。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [70] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: SpeakRL是一种强化学习方法，通过奖励主动交互（如提问澄清问题）来增强AI代理的对话能力，在任务完成率上相比基础模型提升20.14%。


<details>
  <summary>Details</summary>
Motivation: 当前人机协作主要是单向的，用户发出指令，代理直接响应而不主动澄清模糊意图。现有研究未充分利用语言模型的对话能力，将代理优化为更好的跟随者而非有效的发言者。需要让代理更主动地参与对话以澄清用户意图、解决歧义并适应变化情况。

Method: 提出SpeakRL强化学习方法，奖励代理与用户的主动交互（如必要时的澄清提问）。创建SpeakER合成数据集，包含任务导向对话的多样化场景，任务通过交互式澄清问题解决。系统分析对话主动性的奖励设计，提出平衡提问与行动的奖励公式。

Result: 实验评估显示，该方法在任务完成率上比基础模型提升20.14%的绝对改进，且不增加对话轮次，甚至超越更大的专有模型，展示了澄清中心化用户-代理交互的潜力。

Conclusion: SpeakRL通过强化学习增强代理的对话能力，使代理能更主动地澄清用户意图，显著提高任务完成效果，为人机协作提供了更有效的交互模式。

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [71] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

TL;DR: Finch是一个用于评估AI代理在真实企业级财务工作流程中的基准测试，基于Enron等金融机构的真实数据，包含172个复合工作流程和384个任务，涉及大量电子表格和其他多模态文档。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估主要关注简单任务，缺乏对真实企业环境中复杂、长时程、知识密集型协作工作流程的评估。企业财务工作涉及数据录入、计算、建模、验证、可视化等多种任务的交织，需要专门的基准来评估AI代理在实际工作环境中的表现。

Method: 采用LLM辅助发现与专家标注相结合的工作流程构建方法：1) 从真实邮件线程和电子表格版本历史中LLM辅助推导工作流程，专家验证；2) 专家进行细致标注，投入超过700小时。数据源包括Enron的15,000个电子表格和500,000封邮件，以及其他金融机构的真实工作空间。

Result: 评估了GPT 5.1、Claude Sonnet 4.5、Gemini 3 Pro、Grok 4和Qwen 3 Max等前沿AI系统。GPT 5.1 Pro花费48小时仅通过38.4%的工作流程，Claude Sonnet 4.5仅通过25.0%。案例研究进一步揭示了真实企业工作流程对AI代理的挑战。

Conclusion: Finch基准揭示了当前AI代理在处理真实企业财务工作流程时的显著局限性，即使是顶尖模型也难以应对这些复杂、混乱、长时程的任务。这为AI代理在企业环境中的实际应用提出了重要挑战和研究方向。

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [72] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy,Róbert Lakatos,András Hajdu*

Main category: cs.AI

TL;DR: 提出基于错误驱动的优化框架，通过聚类错误预测迭代优化提示规则，显著提升小语言模型在算术推理任务上的性能，使其在隐私合规条件下超越GPT-3.5 Turbo。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗等受监管行业，需要能够在本地安全环境中处理表格数据并执行精确算术操作的工业级AI助手。现有小语言模型在算术任务上存在基本限制，而大型模型又存在隐私合规问题。

Method: 采用错误驱动的优化框架，通过聚类错误预测来迭代优化提示规则，增强代码生成代理在本地小语言模型上的算术推理能力。

Result: 在Qwen3 4B模型上，错误驱动方法将算术任务准确率显著提升至70.8%，使该小模型在隐私合规条件下超越了GPT-3.5 Turbo的性能。

Conclusion: 通过系统化的错误驱动提示优化，而非昂贵的微调，可以实现可靠、可解释且工业可部署的AI助手，让小模型在隐私合规条件下超越大型模型。

Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [73] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

TL;DR: DERL提出了一种可微分的进化强化学习框架，通过双层优化自动发现最优奖励信号，在复杂推理任务中超越基于启发式奖励的方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中设计有效奖励函数困难且耗时，现有自动化方法将奖励函数视为黑盒，无法捕捉奖励结构与任务性能之间的因果关系。

Method: 提出可微分进化强化学习（DERL），采用双层优化框架：元优化器通过组合结构化原子基元进化奖励函数，指导内层策略训练；通过强化学习将内层验证性能作为信号更新元优化器，近似任务成功的"元梯度"。

Result: 在机器人代理（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k、MATH）三个领域验证，在ALFWorld和ScienceWorld上达到最先进性能，尤其在分布外场景显著优于启发式奖励方法。

Conclusion: DERL成功捕捉任务内在结构，实现无需人工干预的自改进智能体对齐，为复杂推理任务的奖励设计提供了有效解决方案。

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [74] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedCEG框架通过Critical Evidence Graph监督医学语言模型的推理过程，提升临床推理的可靠性和有效性


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在医学推理中虽然提升了性能，但推理过程的临床可靠性有限，因为训练过程中往往忽视了推理的准确性和有效性验证

Method: 提出MedCEG框架，通过Critical Evidence Graph（CEG）显式监督推理过程；构建挑战性临床案例数据集，为每个样本算法构建CEG表示高质量可验证推理路径；引入临床推理程序奖励，评估节点覆盖、结构正确性和链完整性

Result: 实验结果显示MedCEG在性能上超越现有方法，同时产生临床有效的推理链，代表了可靠医学AI推理的实质性进展

Conclusion: MedCEG通过显式监督推理过程，提升了医学语言模型的临床推理可靠性，为医疗AI提供了更可信的决策支持

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [75] [【论文分享】<em class="highlight">强化学习</em>是否真的激发了基础模型能力之外的推理能力](http://mp.weixin.qq.com/s?__biz=Mzk2NDg0ODI5OQ==&mid=2247483867&idx=1&sn=35742a50c91780bc73ad367eefb4ad87&chksm=c52e749156cccf4975df477231ff88406848248a3b5a3c0ae88994c9c13c58f4ab0d9af39d4d#rd)
*StayHungry Lab*

Main category: wechat.article

TL;DR: 传统强化学习场景中，在一个state下，其动作空间往往比较小。比如在超级马里奥的游戏中，在一个状态下能够执行的动作就是上下左右移动。动作空间小就可以【枚举+探索】（也可能有一些根据经验/规则的剪枝）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 传统强化学习场景中，在一个state下，其动作空间往往比较小。比如在超级马里奥的游戏中，在一个状态下能够执行的动作就是上下左右移动。动作空间小就可以【枚举+探索】（也可能有一些根据经验/规则的剪枝）。

</details>


### [76] [Nature 期刊论文 || 机器自动发现并超越手工设计的<em class="highlight">强化学习</em>算法](http://mp.weixin.qq.com/s?__biz=MzkyMzUyMTY4OA==&mid=2247486315&idx=1&sn=9e03c7675757aaa122ddee048817e801&chksm=c029f665006fa8855b63460c55e52d8e13b591498c9fe0d8f5759a46b445cf6c2dd32c23f96a#rd)
*数字能源与低碳发展*

Main category: wechat.article

TL;DR: 尽管机器自主发现强大强化学习算法的目标数十年来一直备受关注，但始终难以实现。本文证明了机器能够自主发现超越人工设计的最先进的RL规则，这是通过元学习从大量智能体在复杂环境中的累积经验而实现的。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 尽管机器自主发现强大强化学习算法的目标数十年来一直备受关注，但始终难以实现。本文证明了机器能够自主发现超越人工设计的最先进的RL规则，这是通过元学习从大量智能体在复杂环境中的累积经验而实现的。

</details>


### [77] [<em class="highlight">强化学习</em>笔记：马尔可夫决策过程](http://mp.weixin.qq.com/s?__biz=MzkxOTUwNjcyNQ==&mid=2247484815&idx=1&sn=88f58f437ae4dde6df67bd5e999bf412&chksm=c0fbf26ae383967506ad7928bd85ad53f78a30c1842022e679b84e9975221420ff782ba6540d#rd)
*AI闲谈局*

Main category: wechat.article

TL;DR: 基于上述基础概念，强化学习的核心概念大致如下。其中核心工具 贝尔曼方程形式如下。参考1. 赵世钰《强化学习的数学原理》清华大学出版社2. 王琦、杨毅远、江季等《EasyRL：强化学习教程》人民邮电出版社


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 基于上述基础概念，强化学习的核心概念大致如下。其中核心工具 贝尔曼方程形式如下。参考1. 赵世钰《强化学习的数学原理》清华大学出版社2. 王琦、杨毅远、江季等《EasyRL：强化学习教程》人民邮电出版社

</details>


### [78] [Nature 最新研究：机器如何从大量环境经验中自动发现最先进的<em class="highlight">强化学习</em>规则](http://mp.weixin.qq.com/s?__biz=MzUzNjk1NTU5Mw==&mid=2247484852&idx=1&sn=6fa7cdfe8451b5c40b10b0d37a4f7f33&chksm=fb01d12477e8b04653c7157d7c1bff501858d11a637506a4056d70fad8fa8f807d42586a575b#rd)
*AI前沿文献速递*

Main category: wechat.article

TL;DR: 强化学习是实现智能体在复杂环境中进行预测与决策的核心方法之一。现有的成功系统，如围棋、国际象棋、星际争霸以及复杂控制系统，大多基于人工设计的强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是实现智能体在复杂环境中进行预测与决策的核心方法之一。现有的成功系统，如围棋、国际象棋、星际争霸以及复杂控制系统，大多基于人工设计的强化学习算法。

</details>


### [79] [ICPO：基于内在置信度驱动的组相对偏好优化方法，用于高效<em class="highlight">强化学习</em>——小米公司 MiLMPlus 团队](http://mp.weixin.qq.com/s?__biz=Mzk2NDA4NjU0Nw==&mid=2247491929&idx=1&sn=7e8ada70676381c4a36f5c3ae215aef0&chksm=c53be21f04c5a574b776052635b4d71b2d2d6731d0995e9b3071a6abdcfa98e531f49d594589#rd)
*AI专题精讲*

Main category: wechat.article

TL;DR: 这就是人类反馈强化学习（RLHF） 的基本思想。但这里有个难题：如果机器人只追求你给的奖励，它可能会变得过于保守，只重复那些它已经知道能获得奖励的动作，而不敢尝试新的、可能更好的走路方式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这就是人类反馈强化学习（RLHF） 的基本思想。但这里有个难题：如果机器人只追求你给的奖励，它可能会变得过于保守，只重复那些它已经知道能获得奖励的动作，而不敢尝试新的、可能更好的走路方式。

</details>


### [80] [【AI安全】英伟达构建 <em class="highlight">Agentic</em> AI 沙盒的终极指南](http://mp.weixin.qq.com/s?__biz=MzkxNzU2NDgxNQ==&mid=2247484212&idx=1&sn=e9a09643fd814fc155d42672a596ab94&chksm=c028c1da58d84436c1cb78311cd370fd3dfcf19eb67e9e2ea82f65c4407d0879b2c46ec67946#rd)
*Oxo Security*

Main category: wechat.article

TL;DR: Agentic AI不再仅仅是生成文本，它们能够规划任务、调用工具、编写并执行代码来解决问题。1.1 什么是 Agentic AI？为何它需要动刀？想象一下，你不再只是问 ChatGPT “如何处理 Excel 表格”，而是直接丢给它一个文件，说：“帮我


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI不再仅仅是生成文本，它们能够规划任务、调用工具、编写并执行代码来解决问题。1.1 什么是 Agentic AI？为何它需要动刀？想象一下，你不再只是问 ChatGPT “如何处理 Excel 表格”，而是直接丢给它一个文件，说：“帮我

</details>


### [81] [NVIDIA发布Nemotron 3：从Nano到Ultra，全面押注<em class="highlight">Agentic</em> AI](http://mp.weixin.qq.com/s?__biz=MzkyNDU0MzI0Mw==&mid=2247502286&idx=4&sn=8415990010d7582fb4440d8eafe36b2d&chksm=c0ae0b09b302c58361dc2fc7690dadf186ddcd3010f820cd374459231360242a1d60d8939337#rd)
*聚大模型前言*

Main category: wechat.article

TL;DR: 定位直指当前快速增长的 Agentic AI（智能体 AI）系统需求。该系列全面采用 混合专家模型（Hybrid MoE）架构，强调在多智能体协作任务中实现 更高效率与更高准确性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 定位直指当前快速增长的 Agentic AI（智能体 AI）系统需求。该系列全面采用 混合专家模型（Hybrid MoE）架构，强调在多智能体协作任务中实现 更高效率与更高准确性。

</details>


### [82] [2026前瞻：从工具到员工，<em class="highlight">Agentic</em> AI的生态裂变与架构突围](http://mp.weixin.qq.com/s?__biz=MzU5ODc3OTM1NQ==&mid=2247485869&idx=1&sn=08ee49315e516fc8deb6c59c7c1b6922&chksm=ff17b9e1032fca5725e5789f857990988e196aeda693554606de6f32328d2ad79c2803b5ea52#rd)
*数字化前瞻*

Main category: wechat.article

TL;DR: Agentic AI并非简单的工具升级，而是一种范式的转变。如果说过去的AI主要扮演辅助角色，那么智能体AI则更像是一位能自主思考、规划和行动的“数字员工”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI并非简单的工具升级，而是一种范式的转变。如果说过去的AI主要扮演辅助角色，那么智能体AI则更像是一位能自主思考、规划和行动的“数字员工”。

</details>


### [83] [<em class="highlight">Agentic</em> AI的顶层设计与落地指南](http://mp.weixin.qq.com/s?__biz=MzkyOTQyMjkzOA==&mid=2247501264&idx=1&sn=ffe16e3a5316b8cc014883e312fa495b&chksm=c39e9919d87f6e7eb105616c85c315a3626410d461e80d212188b19febd11fb347d85fd90e53#rd)
*大话数智*

Main category: wechat.article

TL;DR: Agentic AI从技术到应用的完整路径Agent在全球科技巨头的最佳实践12月18日，深度解读Agent大规模落地的：技术就绪度、商业化路径、行业案例、避坑指南。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI从技术到应用的完整路径Agent在全球科技巨头的最佳实践12月18日，深度解读Agent大规模落地的：技术就绪度、商业化路径、行业案例、避坑指南。

</details>


### [84] [<em class="highlight">Agentic</em> AI时代的组织变革：超越“工具”的战略框架](http://mp.weixin.qq.com/s?__biz=Mzg3MDY0OTQ0NA==&mid=2247507658&idx=1&sn=42631db4f149c547a04f604bfde294b9&chksm=cfb4eeaa7e91bb710ae9f28589321046c746676f1e767e4d8efcf9f599553b6476888417b09a#rd)
*Andy730*

Main category: wechat.article

TL;DR: Agentic AI介于两者之间。管理误区：过度追求标准化会扼杀AI的学习能力；过度依赖其适应性则会导致不可控的变异。破局思路：设计混合型流程。构建类似ADP“代理构建平台”的基础设施，既能批量处理标准任务，又能让AI像人


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI介于两者之间。管理误区：过度追求标准化会扼杀AI的学习能力；过度依赖其适应性则会导致不可控的变异。破局思路：设计混合型流程。构建类似ADP“代理构建平台”的基础设施，既能批量处理标准任务，又能让AI像人

</details>


### [85] [软研极客精粹坊系列培训 | 积极响应“人工智能+”行动，扎实推进<em class="highlight">大模型</em>技术应用培训（一）](http://mp.weixin.qq.com/s?__biz=MzA5ODI3MjAzMg==&mid=2247550102&idx=2&sn=8d1ede150b6e28399f3d60e91d92d88c&chksm=916c30f3035f3b0b0f580b92be9bd5dcba3df6feb6560c400b0bdb965fc5bacb04f80e7abe92#rd)
*联通哈尔滨软件研究院*

Main category: wechat.article

TL;DR: 围绕大模型应用开发技术架构，从数据准备、模型训练、提示词工程、模型评价等方面重点讲解了大模型建设方案。其中，数据准备的数据采集、清洗和标注有效运用大数据技术为大模型有效训练奠定基础，充分体现了技术变革


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 围绕大模型应用开发技术架构，从数据准备、模型训练、提示词工程、模型评价等方面重点讲解了大模型建设方案。其中，数据准备的数据采集、清洗和标注有效运用大数据技术为大模型有效训练奠定基础，充分体现了技术变革

</details>


### [86] [年终大冲刺，中美科技大厂都杀疯了](http://mp.weixin.qq.com/s?__biz=MjM5NDA0NDkzOQ==&mid=2650792273&idx=3&sn=ab5366200c086b4230d79683c4f6d2d5&chksm=bf6fa0ebefbf2445abb4ce069964eb5d542c13a20e99e710db79d062e59eaf6ad517f71e26b8#rd)
*华商韬略*

Main category: wechat.article

TL;DR: 该模型一经面世，就被多家科技媒体评价为“迄今最完整的一次大模型体系级升级”：不仅在推理能力、多模态理解和复杂任务处理上显著跃升，还首次将优化重点明确放在手机端推理效率与开发者平台建设上。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 该模型一经面世，就被多家科技媒体评价为“迄今最完整的一次大模型体系级升级”：不仅在推理能力、多模态理解和复杂任务处理上显著跃升，还首次将优化重点明确放在手机端推理效率与开发者平台建设上。

</details>


### [87] [中远海运科技股份有限公司-研发创新中心丨AI时代航运新基建：Hi-Dolphin航运<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=Mzk0MzcxNDM2MQ==&mid=2247489539&idx=3&sn=11ab013aa40e8fbada61416f6827f0da&chksm=c279452c0396942f5f01dfe605630c37493b3a756257124b3cb014148caf1881f5e0c5880f86#rd)
*交科智汇*

Main category: wechat.article

TL;DR: 模型训练方面，平台100% 采用国产算力和开源大模型框架，结合增量预训练、全参数微调、GRPO等主流微调与强化学习技术，仅使用16张GPU卡实现完整航运大模型训练，模型开发成本为通用大模型的万分之一，极大提升了行业适配


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 模型训练方面，平台100% 采用国产算力和开源大模型框架，结合增量预训练、全参数微调、GRPO等主流微调与强化学习技术，仅使用16张GPU卡实现完整航运大模型训练，模型开发成本为通用大模型的万分之一，极大提升了行业适配

</details>


### [88] [马云大发神威，阿里<em class="highlight">大模型</em>拿下美国硅谷巨头](http://mp.weixin.qq.com/s?__biz=MzA4ODYzMDY0OA==&mid=2649662963&idx=2&sn=f4579bc9f0def12dc87e2dedbecf0bd0&chksm=89b62aabda3b46b419ea835edf5e241cb4b762815da8c506dad249b6dae19f7586652260c361#rd)
*环球科技局*

Main category: wechat.article

TL;DR: 这是国内大模型在该类评测中的最佳纪录。除了强大的基础模型能力，阿里还构建了全栈AI技术体系。从底层芯片、超节点服务器、高性能网络、分布式存储、智算集群到人工智能平台，阿里云已形成完整的技术闭环。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这是国内大模型在该类评测中的最佳纪录。除了强大的基础模型能力，阿里还构建了全栈AI技术体系。从底层芯片、超节点服务器、高性能网络、分布式存储、智算集群到人工智能平台，阿里云已形成完整的技术闭环。

</details>


### [89] [<em class="highlight">大模型</em>+软件工程深度研究：应用场景、发展趋势与最佳实践](http://mp.weixin.qq.com/s?__biz=MzkyMzM0NzEwNg==&mid=2247488628&idx=1&sn=235f1ab47ef6fe5ed88489215721a8b9&chksm=c0fe2da6284d540c861692d95bb501f4ee614cca263e1c9f9d383d38ea54b3552898e328cafd#rd)
*沙丘社区*

Main category: wechat.article

TL;DR: 大模型凭借其强大的语言生成、逻辑推理、知识融合等能力，为软件工程的各个环节带来了新的思路与方法。基于市场的长期跟踪，沙丘智库正式发布《2025年“大模型+软件工程”最佳实践报告》，深入洞察“大模型+软件工程”


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型凭借其强大的语言生成、逻辑推理、知识融合等能力，为软件工程的各个环节带来了新的思路与方法。基于市场的长期跟踪，沙丘智库正式发布《2025年“大模型+软件工程”最佳实践报告》，深入洞察“大模型+软件工程”

</details>


### [90] [最新深圳市AI<em class="highlight">大模型</em>备案情况一览表（40款）](http://mp.weixin.qq.com/s?__biz=Mzg2MjcwNjg2NA==&mid=2247514410&idx=1&sn=5d325c7db34268aea42e008b324e8adc&chksm=cfb0508da9b0d95be12e6c0440b28dd6c9af7ad998af2b9db4f638fb1153d8f74b64c9e9e2e4#rd)
*广东湾区无人技术研究院*

Main category: wechat.article

TL;DR: 2025/6/9深圳市15端侧OS大模型深圳陆兮科技有限公司2025/6/9深圳市16睿驰大模型彩讯科技股份有限公司2025/6/9深圳市17CentaAI中原（中国）房地产代理有限公司


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2025/6/9深圳市15端侧OS大模型深圳陆兮科技有限公司2025/6/9深圳市16睿驰大模型彩讯科技股份有限公司2025/6/9深圳市17CentaAI中原（中国）房地产代理有限公司

</details>
