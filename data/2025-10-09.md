<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 10]
- [tldr.article](#tldr.article) [Total: 6]
- [wechat.article](#wechat.article) [Total: 27]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出了知识图谱引导的多智能体系统蒸馏方法(KG-MASD)，通过将蒸馏建模为马尔可夫决策过程并引入知识图谱作为可验证的结构化先验，解决了工业问答系统中多智能体模型推理深度与轻量化部署之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 工业问答系统需要比通用对话模型更高的安全性和可靠性，因为高风险场景中的错误可能造成严重后果。传统蒸馏方法难以将多智能体的协作推理能力转移到轻量级学生模型中。

Method: 将蒸馏过程建模为马尔可夫决策过程，引入知识图谱作为可验证的结构化先验来丰富状态表示并确保收敛，通过结合协作推理和知识基础生成高质量指令调优数据。

Result: 在工业问答数据集上的实验表明，KG-MASD相比基线方法准确率提升了2.4%到20.1%，并显著增强了可靠性。

Conclusion: KG-MASD能够将推理深度和可验证性共同蒸馏到适合边缘部署的紧凑学生模型中，为安全关键工业场景中的可信AI部署提供了可行方案。

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [2] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 提出了VeriCode，一个包含30种可验证代码指令的分类法，用于评估LLM的代码指令遵循能力，并创建了Vibe Checker测试平台来同时评估功能正确性和指令遵循性。


<details>
  <summary>Details</summary>
Motivation: 当前代码评估主要关注功能正确性（pass@k），但忽视了用户在实际编程中经常使用的非功能性指令，这些指令体现了人类偏好（vibe check）。

Method: 开发了VeriCode分类法，包含30种可验证代码指令和相应的确定性验证器，并将其应用于现有评估套件中创建Vibe Checker测试平台。

Result: 评估31个领先LLM后发现，即使最强模型也难以遵循多个指令并表现出明显的功能回归。功能正确性和指令遵循性的综合得分与人类偏好相关性最好。

Conclusion: 指令遵循是vibe check中体现人类偏好的关键因素，为开发更符合用户编程偏好的模型提供了具体路径。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [3] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 本文首次对LLM智能体在网络安全领域的安全态势进行了全面调查，围绕应用、威胁和防御三个相互依存的支柱构建了该领域的研究框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从被动工具向自主智能体的快速转变，这种智能体上下文引入了一类新的固有安全风险，需要系统性地研究智能体安全态势。

Method: 通过对150多篇论文进行综合分类，构建了围绕应用、威胁和防御三个支柱的全面分类法，并进行了详细的交叉分析。

Result: 调查揭示了智能体架构的新兴趋势，同时发现了模型和模态覆盖方面的关键研究空白。

Conclusion: 该研究为理解LLM智能体在网络安全中的安全态势提供了首个整体框架，识别了当前研究的关键差距。

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [4] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了Webscale-RL管道和数据集，将大规模预训练文档转化为120万个多样化、可验证的问答对，用于强化学习训练，显著提升了模型性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs模仿学习范式存在的训练-生成差距和推理能力限制问题，以及现有RL数据集规模小、多样性不足的数据瓶颈。

Method: 开发了可扩展的Webscale-RL数据管道，系统地将大规模预训练文档转化为数百万个多样化、可验证的问答对，构建了包含120万个样本、覆盖9个以上领域的Webscale-RL数据集。

Result: 在该数据集上训练的模型在多个基准测试中显著优于持续预训练和强数据精炼基线，RL训练效率大幅提升，达到持续预训练性能所需的token数量减少高达100倍。

Conclusion: 这项工作为将RL扩展到预训练规模提供了可行路径，能够开发出更强大、更高效的语言模型。

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [5] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 该论文发现原本作为助手训练的LLM不适合模拟真实用户行为，提出了专门训练的用户语言模型(User LMs)来更准确地模拟多轮对话中的用户行为，并证明这种更真实的模拟环境会显著降低助手模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用助手LLM来模拟用户对话，但这些模型经过训练成为有帮助的助手，与真实用户的行为模式存在差异。需要开发专门模拟人类用户行为的模型来创建更真实的多轮对话评估环境。

Method: 提出用户语言模型(User LMs)，通过对模型进行专门的后训练，使其能够模拟多轮对话中真实用户的行为模式，包括不完美的表达、逐步细化请求等特征。

Result: User LMs在模拟人类行为方面优于现有的助手LLM模拟方法，具有更好的仿真鲁棒性。在编程和数学对话模拟中，强助手模型(GPT-4o)的性能从74.6%下降到57.4%，表明更真实的模拟环境会暴露助手模型在多轮对话中的不足。

Conclusion: 专门训练的用户模拟模型比使用助手LLM作为用户模拟器更有效，能创建更真实的对话评估环境，揭示助手模型在实际多轮对话场景中的局限性。

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [6] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 研究LLM越狱现象，通过分析GPT-J和Mamba2模型的内部表示，发现越狱提示与良性提示在隐藏层响应上存在差异，为基于内部动态的越狱检测和防御提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着对话式LLM的普及，越狱攻击日益严重，现有防御机制无法完全抵抗新型提示技术，需要从模型内部表征角度研究越狱现象。

Method: 分析开源LLM GPT-J和状态空间模型Mamba2的内部表示，重点研究隐藏层对越狱提示与良性提示的响应差异。

Result: 初步发现两种模型在层级行为上表现出明显差异，越狱提示在内部表征中具有可区分的特征。

Conclusion: 利用模型内部动态进行越狱检测和防御是可行的研究方向，内部表征分析为开发更鲁棒的防御机制提供了新思路。

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [7] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: 提出SAO框架，完全自生成对齐数据，无需人工标注或外部奖励模型，通过角色扮演生成多样提示和响应，并进行自评估偏好优化。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要昂贵的人工标注数据，RLAIF也需要收集多样提示和响应，依赖外部奖励模型或GPT-4等专有模型标注偏好对，成本高昂。

Method: SAO框架：1) 指导LLM进行角色扮演生成多样提示和响应；2) 自评估进行偏好优化；所有训练数据均由模型自身生成。

Result: 在AlpacaEval~2.0等标准基准上有效提升模型聊天能力，同时在下游客观任务（如问答、数学推理）上保持强性能。

Conclusion: 为LLM对齐的自我改进提供了实用解决方案，代码开源。

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [8] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: ToolMem是一个让智能体通过记忆工具能力来优化工具选择的框架，通过总结工具优缺点并存储在记忆中，在推理时检索相关信息来选择最佳工具。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常依赖固定工具，限制了为特定任务选择最合适工具的灵活性。人类通过交互积累对工具能力的理解，并应用这些知识选择最优工具。

Method: 提出ToolMem框架，让智能体从先前交互中开发工具能力记忆，总结工具优缺点并存储在内存中；在推理时检索相关条目，选择最佳工具来更准确地解决任务。

Result: 在文本生成和文本到图像生成神经工具上评估ToolMem。相比无记忆的通用智能体，ToolMem增强的智能体在文本和多模态生成场景中分别准确预测工具性能14.8%和28.7%。在多个选择中，ToolMem促进最优工具选择分别提高21%和24%。

Conclusion: ToolMem通过让智能体积累工具能力记忆，显著提高了工具性能预测准确性和最优工具选择能力。

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [9] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: PiKa是一个数据高效的专家级对齐数据集家族，仅使用3万SFT样本就能让Llama-3-8B-Base在多个基准测试中超越官方使用超千万专有样本训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有对齐数据集要么是私有的，要么需要昂贵的人工标注，限制了可复现性和可扩展性。即使使用AI反馈强化学习，数据质量问题仍然存在，且不清楚实际需要多少数据来微调基础模型成为强大的指令跟随模型。

Method: 引入PiKa数据集家族，特别是PiKa-SFT数据集仅使用3万个SFT样本，远少于现有数据集如Magpie。通过在Llama-3-8B-Base和Qwen2.5系列模型上进行微调实验验证效果。

Result: PiKa-SFT在AlpacaEval 2.0和Arena-Hard基准测试中超越了官方Llama-3-8B-Instruct模型，后者使用了超过1000万个专有样本。在Qwen2.5系列模型上也取得了一致的性能提升。

Conclusion: 通过高质量的数据集，可以用显著更少的数据实现有效的LLM对齐，为开源LLM对齐提供了可扩展的路径。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [10] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出了SUPO算法，通过总结增强的上下文管理来解决LLM代理在长时程多轮工具使用中的上下文长度瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在长时程多轮工具使用中面临上下文长度限制、指令跟随性能下降和过高的rollout成本等挑战

Method: 引入基于总结的上下文管理，定期压缩工具使用历史，通过LLM生成的总结保留任务相关信息，并开发了支持端到端优化的策略梯度表示

Result: 在交互式函数调用和搜索任务中，SUPO显著提高了成功率，同时保持相同或更低的工作上下文长度

Conclusion: 基于总结的上下文管理为训练超越固定上下文长度限制的RL代理提供了一种原则性和可扩展的方法

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [11] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: 提出了FURINA-Builder，一种自动构建可定制角色扮演基准的多智能体协作管道，并基于此构建了FURINA-Bench基准。研究发现推理LLMs在角色扮演性能与幻觉之间存在权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演基准存在范围狭窄、交互范式过时和适应性有限的问题，需要更全面和可定制的评估方法。

Method: FURINA-Builder通过多智能体协作自动构建基准，模拟角色间对话，使用LLM法官选择评估维度并调整测试角色的响应。

Result: 构建了FURINA-Bench基准，评估显示o3和DeepSeek-R1分别在英文和中文任务中表现最佳，发现推理能力会放大角色性能差异，并揭示了推理LLMs在性能与幻觉之间的权衡。

Conclusion: FURINA-Builder有效解决了现有基准的局限性，FURINA-Bench提出了新的挑战，揭示了LLMs在角色扮演中性能与可靠性的帕累托前沿。

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [12] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 提出SID方法，利用LLM自信号（模型级置信度和词级语义焦点）指导多LLM辩论过程，提高准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有多LLM辩论方法主要依赖外部结构而忽略生成过程中的自信号，导致计算冗余和性能下降

Method: 通过模型级置信度实现高置信度代理早期退出，基于注意力机制压缩冗余辩论内容

Result: 在多个基准测试中，SID方法在准确性和token消耗方面均优于现有MAD技术

Conclusion: 利用自信号能有效提升多代理辩论系统的性能和效率

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [13] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出了Long-RewardBench基准来评估长上下文奖励模型，并开发了多阶段训练策略LongRMs，在长上下文场景中显著提升性能，8B模型超越70B基线。


<details>
  <summary>Details</summary>
Motivation: 现实应用中LLM代理等场景涉及长历史轨迹，现有奖励模型局限于短上下文设置，缺乏对长上下文-响应一致性的评估。

Method: 引入Long-RewardBench基准，包含成对比较和最佳选择任务；提出多阶段训练策略，将任意模型扩展为稳健的长上下文奖励模型。

Result: 8B LongRM在长上下文评估中大幅提升性能，超越70B规模基线，与专有Gemini 2.5 Pro模型性能相当，同时保持强短上下文能力。

Conclusion: 多阶段训练策略能有效扩展模型为稳健的长上下文奖励模型，解决现有模型在长上下文场景中的脆弱性问题。

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [14] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 对GPT-4.1的1亿条信念进行深入分析，发现其事实知识与现有知识库差异显著，准确性低于先前基准，存在不一致性、模糊性和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 理解前沿LLM的事实知识特性，目前这方面的研究通常基于有偏样本，缺乏系统性分析。

Method: 基于GPTKB v1.5数据集，对GPT-4.1的1亿条递归获取的信念进行深入分析。

Result: 模型事实知识与现有知识库差异显著，准确性低于先前基准指标，存在严重的不一致性、模糊性和幻觉问题。

Conclusion: LLM的事实知识存在显著问题，揭示了未来在事实性LLM知识研究方面的重要机会。

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [15] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: 对开源数学推理数据集和合成技术进行综合分析，在统一训练部署流程中评估数据质量对LLM推理能力的影响，发现结构化数据和从强模型蒸馏比单纯扩大数据规模更有效。


<details>
  <summary>Details</summary>
Motivation: LLM的推理能力对下游任务至关重要，但依赖于训练数据质量。尽管有各种数据构建方法，但在实际应用中的效果仍未充分探索。

Method: 在统一训练部署流程中评估开源数据集和数据合成技术，提炼有效的数据选择策略，识别适合工业应用的方法。

Result: 发现将数据组织成更易理解的格式，或从更强模型蒸馏，通常比单纯扩大数据规模更有效。

Conclusion: 为整合训练数据提升LLM能力提供实用指导，支持成本效益高的数据管理和可扩展的模型增强，启发在现实推理任务中平衡"更多数据"与"更好数据"的研究。

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [16] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: 提出了Customer-R1方法，通过基于强化学习的个性化用户行为模拟，在在线购物环境中实现更精准的步骤式用户行为预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要学习群体层面的策略，缺乏对用户个性的考虑，导致模拟结果过于通用而非个性化。

Method: 使用基于强化学习的方法，策略基于明确的用户画像，通过动作正确性奖励信号优化下一步推理和动作生成。

Result: 在OPeRA数据集上的实验表明，Customer-R1在下一步动作预测任务中显著优于提示和SFT基线方法，并且更好地匹配用户动作分布。

Conclusion: Customer-R1能够实现更高保真度的个性化行为模拟，在用户行为仿真方面表现出色。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [17] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 提出了一种动态评估标准方法OnlineRubrics，通过在线比较当前策略和参考策略的响应来动态生成评估标准，相比静态标准能提升8%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于标准的强化学习方法使用静态评估标准，容易受到奖励攻击行为的影响，且无法捕捉训练过程中出现的新的期望要求。

Method: OnlineRubrics方法通过在线方式动态生成评估标准，通过比较当前策略和参考策略的响应对来持续识别和缓解训练过程中的错误。

Result: 在AlpacaEval、GPQA、ArenaHard以及专家问题和标准验证集上，相比仅使用静态标准的方法，该方法带来了高达8%的持续改进。

Conclusion: 动态评估标准方法能够有效提升LLM在开放长文本回答任务上的训练效果，识别出的关键评估维度包括透明度、实用性、组织性和推理能力。

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [18] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: M-Thinker通过GRPO算法训练，解决了大型推理模型在多语言场景下的语言一致性和推理能力问题，在非英语语言上实现了接近100%的语言一致性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在处理非英语语言时存在两个关键限制：难以保持输入输出语言一致性，以及在非英语语言上的推理路径质量较差、答案准确率低于英语。这些问题影响了非英语用户的使用体验，阻碍了LRMs的全球部署。

Method: 提出M-Thinker模型，采用GRPO算法训练，包含语言一致性奖励和跨语言思维对齐奖励。语言一致性奖励严格约束输入、思维和输出之间的语言一致性；跨语言思维对齐奖励通过比较模型在非英语和英语上的推理路径，将英语推理能力迁移到非英语语言。

Result: M-Thinker-1.5B/7B模型在两个多语言基准测试（MMATH和PolyMath）上不仅实现了接近100%的语言一致性，还表现出优越的性能，并在域外语言上展现出良好的泛化能力。

Conclusion: M-Thinker通过创新的训练方法有效解决了大型推理模型在多语言场景下的关键问题，显著提升了非英语语言的推理性能和用户体验。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [19] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 提出了CORGI基准测试，专门针对真实商业场景的文本到SQL转换，包含描述性、解释性、预测性和推荐性四类复杂查询，比现有基准难21%。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL基准主要关注历史记录的事实检索，无法满足商业环境中需要因果推理、时间预测和战略推荐的多层次智能需求。

Method: 创建基于Doordash、Airbnb和Lululemon等企业启发的合成数据库，设计四类复杂度递增的商业查询，评估LLM在真实商业场景中的表现。

Result: LLM在高级别问题上表现下降，难以做出准确预测和提供可行计划，CORGI基准比BIRD基准难约21%。

Conclusion: 流行LLM与真实商业智能需求存在差距，需要开发更强大的文本到SQL系统来支持复杂商业决策。

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [20] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 该论文提出了一种基于静态分析的代码块检索方法，用于改进代码补全中的上下文收集，在ASE 2025挑战赛中相比文件级检索和无上下文基线分别提升了6%和16%的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码补全任务中需要充分且相关的上下文信息，但在大型代码库中面临上下文长度限制和噪声干扰的挑战。

Method: 开发了文件级和代码块级的检索策略，重点研究了上下文大小和文件排序对模型性能的影响，并引入了基于静态分析的代码块检索方法。

Result: 实验结果表明上下文数量和顺序显著影响模型性能，代码块检索相比文件检索提升了6%，相比无上下文基线提升了16%。

Conclusion: 检索粒度、排序策略和混合方法对于构建有效的上下文收集管道至关重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [21] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 开发了AiSysRev工具，利用大语言模型辅助系统文献综述的筛选阶段，通过零样本和少样本学习对论文进行分类，减轻人工筛选负担。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的筛选阶段工作量大，需要处理大量论文，而大语言模型在标题摘要筛选方面已表现出与硕士生相当的能力，可以加速这一过程。

Method: 开发基于LLM的筛选工具AiSysRev，作为Docker容器中的Web应用，接受包含论文标题和摘要的CSV文件，用户指定纳入排除标准，支持多种LLM和零样本/少样本筛选。

Result: 在137篇论文的试验中，发现论文可分为四类：容易纳入、容易排除、边界纳入和边界排除。边界案例中LLM容易出错，需要人工干预。

Conclusion: LLM不能完全替代人工判断，但能显著减轻评估大量科学文献的负担。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [22] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 研究11家公司如何制定LLM聊天机器人政策及其影响因素，帮助管理者安全地将聊天机器人整合到开发工作流程中。


<details>
  <summary>Details</summary>
Motivation: 采用大型语言模型聊天机器人存在风险，软件组织需要明确的政策来确保安全集成。

Method: 研究11家公司制定聊天机器人政策的过程和影响因素。

Result: 分析了公司制定LLM聊天机器人政策的方法和关键影响因素。

Conclusion: 为管理者提供了将聊天机器人安全集成到开发工作流程的指导。

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [23] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: Secure-Instruct框架通过自动合成高质量漏洞和安全代码示例，指令微调LLMs以提升安全代码生成能力，在CWEBench和CWEval基准测试中显著提高了代码安全性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在代码生成中常产生不安全代码，现有方法因数据集有限且不平衡而效果受限，需要更有效的方法来提升安全代码生成能力。

Method: 提出Secure-Instruct框架，自动合成漏洞和安全代码示例，生成微调指令，并对LLMs进行指令微调，使其任务描述与安全代码生成能力对齐。

Result: 在CWEBench上，Secure-Instruct使安全代码生成平均提升14.3%，优于SafeCoder 7.6%；在CWEval上，CodeLlama-7B和Mistral-7B的Func-Sec@1分别提升14%和5.8%，优于SafeCoder 15.8%和6.8%。

Conclusion: Secure-Instruct能显著提升LLMs生成代码的安全性和功能正确性，为解决自动代码生成中的安全问题提供了有效方案。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo是一个自演化的智能推理系统，通过整合多个模型和专业工具来解决基础模型推理能力不足和测试时迭代不可靠的问题，在AIME 2024/2025评估中显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型推理能力受限的两个瓶颈：模型内在能力不足和测试时迭代不可靠，提升模型的推理性能。

Method: 协调多个模型与专业工具（Python计算工具和检索工具）进行深思熟虑、可验证的推理，通过共享状态图支持多轮、多模型的解决方案演化。

Result: 在AIME 2024/2025评估中，Qwen2.5-14B-Instruct模型平均性能提升5.15%，通过率提升23.34%；Llama-3.3-70B-Instruct模型平均性能提升8.91%，通过率提升26.67%。超过80%的工具调用成功执行。

Conclusion: AlphaApollo系统通过工具集成和多模型协调，有效提升了基础模型的推理能力上限，工具使用显著优于非工具基线。

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [25] [BuilderBench -- A benchmark for generalist agents](https://arxiv.org/abs/2510.06288)
*Raj Ghugare,Catherine Ji,Kathryn Wantlin,Jin Schofield,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: BuilderBench是一个专注于开放式探索的智能体预训练基准，要求智能体学习使用积木构建各种结构，测试物理理解、数学能力和长程规划能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型主要通过模仿学习，难以解决超出已有数据范围的新问题。需要开发能够通过交互经验学习的智能体，但可扩展的学习机制仍是一个开放性问题。

Method: 提供硬件加速的机器人模拟器和42个多样化目标结构的任务套件，智能体在训练时无监督探索环境，在评估时构建未见过的目标结构。

Result: 实验表明当前算法在这些任务上仍面临挑战，因此提供了"训练轮"协议和六种算法的单文件实现作为参考。

Conclusion: BuilderBench促进了需要具身推理的智能体预训练研究，这种推理体现在行动而非语言中，需要实验不同策略并整合它们。

Abstract: Today's AI models learn primarily through mimicry and sharpening, so it is
not surprising that they struggle to solve problems beyond the limits set by
existing data. To solve novel problems, agents should acquire skills for
exploring and learning through experience. Finding a scalable learning
mechanism for developing agents that learn through interaction remains a major
open problem. In this work, we introduce BuilderBench, a benchmark to
accelerate research into agent pre-training that centers open-ended
exploration. BuilderBench requires agents to learn how to build any structure
using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated
simulator of a robotic agent interacting with various physical blocks, and
$(2)$ a task-suite with over 42 diverse target structures that are carefully
curated to test an understanding of physics, mathematics, and long-horizon
planning. During training, agents have to explore and learn general principles
about the environment without any external supervision. During evaluation,
agents have to build the unseen target structures from the task suite. Solving
these tasks requires a sort of \emph{embodied reasoning} that is not reflected
in words but rather in actions, experimenting with different strategies and
piecing them together. Our experiments show that many of these tasks challenge
the current iteration of algorithms. Hence, we also provide a ``training
wheels'' protocol, in which agents are trained and evaluated to build a single
target structure from the task suite. Finally, we provide single-file
implementations of six different algorithms as a reference point for
researchers.

</details>


### [26] [Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks](https://arxiv.org/abs/2510.06307)
*Wentao Deng,Jiahuan Pei,Zhiwei Xu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.AI

TL;DR: 提出了一个信念校准共识寻求(BCCS)框架，通过选择最优合作者和校准系统内部信念来促进稳定共识，在MATH和MMLU基准数据集上表现优于现有最佳结果。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统共识寻求方法通常依赖投票机制，忽视了系统内部信念的矛盾，且采用无差别的协作方式，无法为每个智能体找到最优合作者，阻碍了稳定共识的形成。

Method: 提出了一个理论框架来选择最大化共识稳定性的最优合作者，并基于此开发了BCCS框架，通过选择最优合作者和校准系统内部信念来促进稳定共识。

Result: 在MATH和MMLU基准数据集上，BCCS框架在具有挑战性的任务上分别比现有最佳结果准确率提高了2.23%和3.95%。

Conclusion: BCCS框架通过选择最优合作者和校准系统内部信念，有效解决了多智能体系统中共识寻求的稳定性问题，显著提升了复杂NLP任务的性能。

Abstract: A multi-agent system (MAS) enhances its capacity to solve complex natural
language processing (NLP) tasks through collaboration among multiple agents,
where consensus-seeking serves as a fundamental mechanism. However, existing
consensus-seeking approaches typically rely on voting mechanisms to judge
consensus, overlooking contradictions in system-internal beliefs that
destabilize the consensus. Moreover, these methods often involve agents
updating their results through indiscriminate collaboration with every other
agent. Such uniform interaction fails to identify the optimal collaborators for
each agent, hindering the emergence of a stable consensus. To address these
challenges, we provide a theoretical framework for selecting optimal
collaborators that maximize consensus stability. Based on the theorems, we
propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate
stable consensus via selecting optimal collaborators and calibrating the
consensus judgment by system-internal beliefs. Experimental results on the MATH
and MMLU benchmark datasets demonstrate that the proposed BCCS framework
outperforms the best existing results by 2.23% and 3.95% of accuracy on
challenging tasks, respectively. Our code and data are available at
https://github.com/dengwentao99/BCCS.

</details>


### [27] [Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?](https://arxiv.org/abs/2510.06410)
*Aochong Oliver Li,Tanya Goyal*

Main category: cs.AI

TL;DR: 该论文研究了推理LLM在共享推理轨迹中协作的能力，发现更强的LLM在干扰下更脆弱，所有模型在超出自身能力的问题上利用协作指导的效果都很差（解决率低于9.2%）。


<details>
  <summary>Details</summary>
Motivation: 研究标准单推理训练流程是否能产生所需的离轨迹推理行为，即评估和基于其他模型部分思考的能力，这对多模型协作推理至关重要。

Method: 提出双测试方法：可恢复性测试LLM从误导推理轨迹中回溯的能力，可引导性测试LLM基于更强协作者的正确推理进行构建的能力。评估了15个开源LLM（1.5B-32B），并进行控制研究分析后训练因素的影响。

Result: 发现反直觉结果：基准测试中更强的LLM在干扰下往往更脆弱；所有模型在超出自身能力的问题上利用指导步骤的效果都很差（解决率<9.2%）；教师模型的不理想可恢复性行为会传递给学生模型。

Conclusion: 这项工作为评估共享推理轨迹中的多模型协作奠定了基础，并突显了现成推理LLM的局限性，为训练原生强推理协作者提供了可行见解。

Abstract: Reasoning LLMs are trained to verbalize their reasoning process, yielding
strong gains on complex tasks. This transparency also opens a promising
direction: multiple reasoners can directly collaborate on each other's thinking
within a shared trajectory, yielding better inference efficiency and
exploration. A key prerequisite, however, is the ability to assess the
usefulness and build on another model's partial thinking -- we call this
off-trajectory reasoning. Our paper investigates a critical question: can
standard solo-reasoning training pipelines deliver desired off-trajectory
behaviors? We propose twin tests that capture the two extremes of the
off-trajectory spectrum, namely Recoverability, which tests whether LLMs can
backtrack from "distractions" induced by misleading reasoning traces, and
Guidability, which tests their ability to build upon correct reasoning from
stronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and
reveals a counterintuitive finding -- "stronger" LLMs on benchmarks are often
more fragile under distraction. Moreover, all models tested fail to effectively
leverage guiding steps from collaborators on problems beyond their inherent
capabilities with solve rates remaining under 9.2%. Finally, we conduct control
studies to isolate the effects of three factors in post-training on these
behaviors: the choice of distillation teacher, the use of RL, and data
selection strategy. Our results provide actionable insights for training
natively strong reasoning collaborators; e.g., we find that suboptimal
recoverability behaviors of teacher models are transferred to distilled
students even if the distillation trajectories are correct. Taken together,
this work lays the groundwork for evaluating multi-model collaborations in
shared reasoning trajectories and highlights the limitations of off-the-shelf
reasoning LLMs.

</details>


### [28] [Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them](https://arxiv.org/abs/2510.06534)
*Jiahe Jin,Abhijay Paladugu,Chenyan Xiong*

Main category: cs.AI

TL;DR: 提出了行为引导技术，通过识别四种有益推理行为（信息验证、权威评估、自适应搜索、错误恢复）来训练更有效的智能搜索代理模型，在三个基准测试上相比直接RL训练获得超过35%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 智能搜索利用LLMs解释复杂用户信息需求并执行多步骤搜索过程，这对LLMs的推理和代理能力提出了独特挑战。需要研究有效的推理行为模式来提升智能搜索性能。

Method: 提出推理驱动的LLM管道分析成功搜索轨迹，识别四种有益推理行为，然后通过行为引导技术将这些行为整合到智能搜索模型中，包括监督微调和标准强化学习。

Result: 在GAIA、WebWalker和HLE三个基准测试上，行为引导技术在Llama3.2-3B和Qwen3-1.7B上相比直接RL训练获得超过35%的性能提升。关键发现是SFT数据中期望的推理行为（而非最终答案的正确性）是RL后获得强性能的关键因素。

Conclusion: 引入的推理行为赋予模型更有效的探索能力和测试时扩展能力，为强化学习提供了坚实基础。期望的推理行为比答案正确性对最终性能更重要。

Abstract: Agentic search leverages large language models (LLMs) to interpret complex
user information needs and execute a multi-step process of planning, searching,
and synthesizing information to provide answers. This paradigm introduces
unique challenges for LLMs' reasoning and agentic capabilities when interacting
with retrieval systems and the broader web. In this paper, we propose a
reasoning-driven LLM-based pipeline to study effective reasoning behavior
patterns in agentic search. Using this pipeline, we analyze successful agentic
search trajectories and identify four beneficial reasoning behaviors:
Information Verification, Authority Evaluation, Adaptive Search, and Error
Recovery. Based on these findings, we propose a technique called Behavior
Priming to train more effective agentic search models. It synthesizes agentic
search trajectories that exhibit these four behaviors and integrates them into
the agentic search model through supervised fine-tuning (SFT), followed by
standard reinforcement learning (RL). Experiments on three benchmarks (GAIA,
WebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in
Llama3.2-3B and Qwen3-1.7B compared to directly training agentic search models
with RL. Crucially, we demonstrate that the desired reasoning behaviors in the
SFT data, rather than the correctness of the final answer, is the critical
factor for achieving strong final performance after RL: fine-tuning on
trajectories with desirable reasoning behaviors but incorrect answers leads to
better performance than fine-tuning on trajectories with correct answers. Our
analysis further reveals the underlying mechanism: the introduced reasoning
behaviors endow models with more effective exploration (higher pass@k and
entropy) and test-time scaling (longer trajectories) capabilities, providing a
strong foundation for RL. Our code will be released as open source.

</details>


### [29] [Auto-Prompt Ensemble for LLM Judge](https://arxiv.org/abs/2510.06538)
*Jiajie Li,Huayi Zhang,Peng Lin,Jinjun Xiong,Wei Xu*

Main category: cs.AI

TL;DR: 提出了Auto-Prompt Ensemble (APE)框架，通过选择性增强LLM的辅助评估维度来提高LLM法官的可靠性，解决了现有LLM法官因未能识别人类评估隐含标准而遗漏关键评估维度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM法官经常因为无法识别人类评估的隐含标准而遗漏关键评估维度，导致评估可靠性不足。

Method: 提出自适应框架APE，自动从失败案例中学习评估维度，并采用基于置信度的集成机制，通过新颖的集体置信度估计方法决定何时采用额外评估维度的判断。

Result: 在多样化标准基准测试中，APE显著提高了LLM法官的可靠性。例如，在零样本设置下，APE将GPT-4o在Reward Bench上的同意率从87.2%提升到90.5%。

Conclusion: APE为LLM法官提供了一种原则性方法，能够利用测试时计算，弥合人类与LLM法官之间的评估差距。

Abstract: We present a novel framework that improves the reliability of LLM judges by
selectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM
judges often miss crucial evaluation dimensions because they fail to recognize
the implicit standards underlying human assessments. To address this challenge,
we propose the Auto-Prompt Ensemble (APE), an adaptive framework that
automatically learns evaluation dimensions from its failure cases. APE
incorporates a confidence-based ensemble mechanism to decide when to adopt the
judgments from additional evaluation dimensions through a novel confidence
estimation approach called Collective Confidence. Extensive experiments
demonstrate that APE improves the reliability of LLM Judge across diverse
standard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward
Bench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a
principled approach for LLM Judge to leverage test-time computation, and bridge
the evaluation gap between human and LLM judges.

</details>


### [30] [WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks](https://arxiv.org/abs/2510.06587)
*Jingbo Yang,Bairu Hou,Wei Wei,Shiyu Chang,Yujia Bao*

Main category: cs.AI

TL;DR: WebDART是一个通用框架，通过动态分解复杂网页任务为导航、信息提取和执行三个子任务，并持续重新规划，显著提升了LLM代理在复杂网页任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在简单网页任务上表现良好，但在需要长程导航、大规模信息提取和约束推理的复杂目标上仍有困难。

Method: WebDART框架：(i) 动态将目标分解为导航、信息提取和执行三个专注子任务；(ii) 随着新网页的发现持续重新规划分解，利用新发现的过滤器或捷径，避免冗余探索。

Result: 在WebChoreArena评估中，WebDART比之前SOTA代理的成功率提高了13.7个百分点，在WebArena套件上表现相当，完成任务所需的导航步骤减少了14.7步。

Conclusion: WebDART框架通过任务分解和持续重新规划，有效提升了LLM代理处理复杂网页任务的能力。

Abstract: Large language model (LLM) agents are becoming competent at straightforward
web tasks, such as opening an item page or submitting a form, but still
struggle with objectives that require long horizon navigation, large scale
information extraction, and reasoning under constraints. We present WebDART, a
general framework that enables a single LLM to handle such complex chores.
WebDART (i) dynamically decomposes each objective into three focused subtasks:
navigation, information extraction, and execution, so the model concentrates on
one skill at a time, and (ii) continuously replans the decomposition as new
webpages are revealed, taking advantage of newly discovered filters or
shortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,
WebDART lifts success rates by up to 13.7 percentage points over previous SOTA
agents, while matching their performance on the easier WebArena suite and
completing tasks with up to 14.7 fewer navigation steps.

</details>


### [31] [Inefficiencies of Meta Agents for Agent Design](https://arxiv.org/abs/2510.06711)
*Batu El,Mert Yuksekgonul,James Zou*

Main category: cs.AI

TL;DR: 本文分析了元代理在自动化设计代理系统时面临的三个关键挑战：跨迭代学习效率、行为多样性不足以及经济可行性问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决元代理在自动化设计代理系统时面临的实际问题，包括学习效率、行为多样性和成本效益。

Method: 通过实验比较不同学习策略（如上下文扩展vs进化方法），评估代理行为多样性，分析设计成本与性能收益的经济平衡。

Result: 发现进化方法优于简单上下文扩展；设计的代理行为多样性低；仅在少数数据集上自动化设计具有经济可行性。

Conclusion: 当前元代理方法在跨迭代学习、行为多样性和经济可行性方面存在显著局限，需要改进才能实现实用价值。

Abstract: Recent works began to automate the design of agentic systems using
meta-agents that propose and iteratively refine new agent architectures. In
this paper, we examine three key challenges in a common class of meta-agents.
First, we investigate how a meta-agent learns across iterations and find that
simply expanding the context with all previous agents, as proposed by previous
works, performs worse than ignoring prior designs entirely. We show that the
performance improves with an evolutionary approach. Second, although the
meta-agent designs multiple agents during training, it typically commits to a
single agent at test time. We find that the designed agents have low behavioral
diversity, limiting the potential for their complementary use. Third, we assess
when automated design is economically viable. We find that only in a few
cases--specifically, two datasets--the overall cost of designing and deploying
the agents is lower than that of human-designed agents when deployed on over
15,000 examples. In contrast, the performance gains for other datasets do not
justify the design cost, regardless of scale.

</details>


### [32] [Verifying Memoryless Sequential Decision-making of Large Language Models](https://arxiv.org/abs/2510.06756)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: 开发了一个用于自动验证基于LLM策略的工具，通过增量构建MDP的可达部分来检查LLM策略是否满足PCTL安全要求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在顺序决策任务中的应用增加，需要一种严格的方法来验证这些策略的安全性，确保它们满足指定的安全属性。

Method: 给定MDP、LLM策略和PCTL安全要求，增量构建MDP的可达部分，将状态编码为自然语言提示，解析LLM响应为动作，扩展可达后继状态，最后使用Storm模型检查器验证策略。

Result: 实验表明，通过Ollama访问的开源LLM在确定性种子下可以被验证，但性能通常低于深度强化学习基线。

Conclusion: 该工具为正式验证日益强大的LLM奠定了实用基础，支持PRISM指定任务的连续基准测试。

Abstract: We introduce a tool for rigorous and automated verification of large language
model (LLM)- based policies in memoryless sequential decision-making tasks.
Given a Markov decision process (MDP) representing the sequential
decision-making task, an LLM policy, and a safety requirement expressed as a
PCTL formula, our approach incrementally constructs only the reachable portion
of the MDP guided by the LLM's chosen actions. Each state is encoded as a
natural language prompt, the LLM's response is parsed into an action, and
reachable successor states by the policy are expanded. The resulting formal
model is checked with Storm to determine whether the policy satisfies the
specified safety property. In experiments on standard grid world benchmarks, we
show that open source LLMs accessed via Ollama can be verified when
deterministically seeded, but generally underperform deep reinforcement
learning baselines. Our tool natively integrates with Ollama and supports
PRISM-specified tasks, enabling continuous benchmarking in user-specified
sequential decision-making tasks and laying a practical foundation for formally
verifying increasingly capable LLMs.

</details>


### [33] [TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs](https://arxiv.org/abs/2510.06878)
*Daria Ozerova,Ekaterina Trofimova*

Main category: cs.AI

TL;DR: TGPR结合GRPO与Thompson采样树搜索，通过主动探索失败和成功的精化路径，在代码生成任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在迭代精化过程中搜索空间巨大的挑战，现有基于预定义启发式的方法难以平衡探索与利用，无法根据历史精化结果自适应调整

Method: 提出Tree-Guided Policy Refinement (TGPR)框架，将GRPO与基于Thompson采样的树搜索相结合，生成更密集的训练轨迹和自适应策略

Result: 在HumanEval、MBPP和APPS基准测试中，相比GRPO基线，pass@1在MBPP上提升4.2个百分点，pass@10在APPS上提升12.51个百分点

Conclusion: TGPR为结合学习策略与结构化搜索方法提供了原则性方法，是增强LLMs迭代精化和状态推理的通用框架

Abstract: Iterative refinement has been a promising paradigm to enable large language
models (LLMs) to resolve difficult reasoning and problem-solving tasks. One of
the key challenges, however, is how to effectively search through the enormous
search space of possible refinements. Existing methods typically fall back on
predefined heuristics, which are troubled by the exploration-exploitation
dilemma and cannot adapt based on past refinement outcomes. We introduce
Tree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with
a Thompson-Sampling-based tree search. TGPR explores both failed and successful
refinement paths actively, with denser training trajectories and more adaptive
policies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to
+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to
+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to
a competitive GRPO baseline. Apart from debugging code, TGPR focuses on a
principled approach to combining learned policies with structured search
methods, offering a general framework for enhancing iterative refinement and
stateful reasoning in LLMs.

</details>


### [34] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 本文研究大型语言模型推理过程中信息密度的均匀性，提出基于熵的步进信息密度度量方法，发现推理质量与信息密度均匀性密切相关。


<details>
  <summary>Details</summary>
Motivation: 重新审视统一信息密度假设在LLM推理轨迹中的应用，探究步级均匀性是否反映推理质量。

Method: 提出基于熵的步进信息密度度量方法，引入局部和全局均匀性评分两个互补指标。

Result: 在六个推理基准测试中，步级均匀性不仅提供理论视角，还带来实际性能提升：选择信息密度更均匀的推理轨迹在AIME2025上相对基线提升10-32%准确率。正确推理轨迹避免信息密度尖峰，错误轨迹则呈现不规则信息爆发。

Conclusion: UID启发的信息密度度量优于其他内部信号作为推理质量预测指标，信息密度均匀性是构建更可靠准确推理系统的稳健诊断和选择标准。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [35] [Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning](https://arxiv.org/abs/2510.07038)
*Wenxun Wu,Yuanyang Li,Guhan Chen,Linyue Wang,Hongyang Chen*

Main category: cs.AI

TL;DR: 提出了TAPO框架，通过强化学习将多跳推理与自适应工具调用能力相结合，提升语言模型在需要外部知识和复杂计算任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在需要最新知识或计算工具（如计算器、代码解释器）的复杂算术运算任务上表现不佳，需要克服这些限制。

Method: 采用改进的动态采样策略优化(DAPO)强化学习框架，专门针对工具调用场景设计，使模型能够动态地在复杂推理和按需工具使用之间切换。

Result: 在Qwen2.5-3B和Qwen2.5-7B模型上实验表明，TAPO在需要外部知识和数学计算的任务上达到了最先进的性能，比基线方法具有更高效的工具利用率。

Conclusion: 结合高级推理与工具使用具有显著潜力，能够增强模型在知识密集型和计算密集型任务中的性能。

Abstract: Recent advances in large language models (LLMs) have popularized test-time
scaling, where models generate additional reasoning tokens before producing
final answers. These approaches have demonstrated significant performance
improvements on benchmarks involving mathematical reasoning. However, language
models relying solely on direct inference still struggle with tasks demanding
up-to-date knowledge or computational tools such as calculators and code
interpreters for complex arithmetic operations. To overcome these limitations,
we propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement
learning framework that systematically integrates multi-hop reasoning with
adaptive tool-calling capabilities. Our approach employs a modified version of
Dynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,
which we adapt specifically for tool invocation scenarios, enabling models to
dynamically interleave complex reasoning with on-demand tool usage (including
search APIs and Python interpreters).
  To support this research, we introduce two new datasets: TAPO-easy-60K and
TAPO-hard-18K, specifically designed to train and evaluate both fact-based
reasoning and mathematical calculation capabilities. Our experiments on
Qwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,
with both models achieving state-of-the-art performance on tasks requiring
external knowledge and mathematical computation among methods with comparable
parameters. Notably, TAPO achieves more efficient tool utilization than
baseline methods while preventing excessive calls caused by reward hacking.
These results highlight the significant potential of combining advanced
reasoning with tool usage to enhance model performance in knowledge-intensive
and computationally demanding tasks.

</details>


### [36] [Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations](https://arxiv.org/abs/2510.07064)
*Manh Hung Nguyen,Sebastian Tschiatschek,Adish Singla*

Main category: cs.AI

TL;DR: 提出了一种基于子模优化的框架，通过构建一组LLM代理来集体捕捉人类群体的多样性，而不是使用单一LLM代理。


<details>
  <summary>Details</summary>
Motivation: 由于获取大规模人类响应的困难和成本，LLMs成为人类行为的替代品，但现有LLMs输出同质化，无法捕捉人类观点和行为的丰富多样性。

Method: 通过上下文学习，将每个LLM代理的行为引导为基于少量人类演示（任务-响应对）的条件生成。使用子模优化方法从指数级大的可能代理空间中选择代表性代理集。

Result: 在众包和教育领域的广泛实验表明，该方法构建的代理比基线方法更有效地代表人类群体。行为分析显示这些代理能够重现所代表学生和注释者的行为模式和观点。

Conclusion: 提出的框架能够构建一组LLM代理，有效捕捉人类群体的多样性，并在新任务上重现人类行为模式。

Abstract: The difficulty and expense of obtaining large-scale human responses make
Large Language Models (LLMs) an attractive alternative and a promising proxy
for human behavior. However, prior work shows that LLMs often produce
homogeneous outputs that fail to capture the rich diversity of human
perspectives and behaviors. Thus, rather than trying to capture this diversity
with a single LLM agent, we propose a novel framework to construct a set of
agents that collectively capture the diversity of a given human population.
Each agent is an LLM whose behavior is steered by conditioning on a small set
of human demonstrations (task-response pairs) through in-context learning. The
central challenge is therefore to select a representative set of LLM agents
from the exponentially large space of possible agents. We tackle this selection
problem from the lens of submodular optimization. In particular, we develop
methods that offer different trade-offs regarding time complexity and
performance guarantees. Extensive experiments in crowdsourcing and educational
domains demonstrate that our approach constructs agents that more effectively
represent human populations compared to baselines. Moreover, behavioral
analyses on new tasks show that these agents reproduce the behavior patterns
and perspectives of the students and annotators they are designed to represent.

</details>


### [37] [The Contingencies of Physical Embodiment Allow for Open-Endedness and Care](https://arxiv.org/abs/2510.07117)
*Leonardo Christov-Moore,Arthur Juliani,Alex Kiefer,Nicco Reggente,B. Scott Rousse,Adam Safron,Nicol'as Hinrichs,Daniel Polani,Antonio Damasio*

Main category: cs.AI

TL;DR: 论文提出基于海德格尔存在主义现象学的物理具身智能体两个最小条件：在世存在和向死而生，并在此基础上发展出稳态驱动力和内在驱动力，以增强智能体在开放环境中的适应性和关怀能力。


<details>
  <summary>Details</summary>
Motivation: 理解生物体在开放物理世界中生存和关怀的机制，以开发更鲁棒、适应性强且具有关怀能力的人工智能体。

Method: 基于海德格尔存在主义现象学定义两个最小物理具身条件，结合尼采的权力意志概念，在强化学习框架中形式化这些概念。

Result: 提出了一个理论框架，将存在主义哲学概念转化为可计算的智能体驱动力，用于增强智能体的开放性和关怀能力。

Conclusion: 从存在主义哲学中汲取的物理具身条件可以为开发更具适应性和关怀能力的人工智能体提供理论基础。

Abstract: Physical vulnerability and mortality are often seen as obstacles to be
avoided in the development of artificial agents, which struggle to adapt to
open-ended environments and provide aligned care. Meanwhile, biological
organisms survive, thrive, and care for each other in an open-ended physical
world with relative ease and efficiency. Understanding the role of the
conditions of life in this disparity can aid in developing more robust,
adaptive, and caring artificial agents. Here we define two minimal conditions
for physical embodiment inspired by the existentialist phenomenology of Martin
Heidegger: being-in-the-world (the agent is a part of the environment) and
being-towards-death (unless counteracted, the agent drifts toward terminal
states due to the second law of thermodynamics). We propose that from these
conditions we can obtain both a homeostatic drive - aimed at maintaining
integrity and avoiding death by expending energy to learn and act - and an
intrinsic drive to continue to do so in as many ways as possible. Drawing
inspiration from Friedrich Nietzsche's existentialist concept of will-to-power,
we examine how intrinsic drives to maximize control over future states, e.g.,
empowerment, allow agents to increase the probability that they will be able to
meet their future homeostatic needs, thereby enhancing their capacity to
maintain physical integrity. We formalize these concepts within a reinforcement
learning framework, which enables us to examine how intrinsically driven
embodied agents learning in open-ended multi-agent environments may cultivate
the capacities for open-endedness and care.ov

</details>


### [38] [Agentic generative AI for media content discovery at the national football league](https://arxiv.org/abs/2510.07297)
*Henry Wang,Sirajus Salekin,Jake Lee,Ross Claytor,Shinan Zhang,Michael Chi*

Main category: cs.AI

TL;DR: 开发了一个基于生成式AI的智能工作流，帮助NFL媒体研究人员通过自然语言查询历史比赛片段，替代传统的筛选界面，准确率超过95%，查询时间从10分钟缩短到30秒。


<details>
  <summary>Details</summary>
Motivation: 传统的内容发现和管理方式效率低下，用户需要通过复杂的筛选界面查找相关视频内容，这限制了媒体研究人员的创造力和工作效率。

Method: 采用智能代理工作流，将用户自然语言查询分解为元素并转换为底层数据库查询语言，通过精心设计的语义缓存提高准确性和响应速度。

Result: 解决方案实现了超过95%的准确率，平均查找相关视频的时间从10分钟减少到30秒，显著提升了NFL的运营效率。

Conclusion: 生成式AI为基础的工作流能够显著改善内容发现过程，让用户能够专注于创作性内容和引人入胜的故事线开发。

Abstract: Generative AI has unlocked new possibilities in content discovery and
management. Through collaboration with the National Football League (NFL), we
demonstrate how a generative-AI based workflow enables media researchers and
analysts to query relevant historical plays using natural language rather than
traditional filter-and-click interfaces. The agentic workflow takes a user
query as input, breaks it into elements, and translates them into the
underlying database query language. Accuracy and latency are further improved
through carefully designed semantic caching. The solution achieves over 95
percent accuracy and reduces the average time to find relevant videos from 10
minutes to 30 seconds, significantly increasing the NFL's operational
efficiency and allowing users to focus on producing creative content and
engaging storylines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 本文提出使用去中心化的小型智能体网络（SANs）架构来克服基础模型在动态环境中的维度诅咒问题，相比单体基础模型能提供更优越的决策能力。


<details>
  <summary>Details</summary>
Motivation: 基础模型在动态环境（如重症监护）中的适应能力有限，需要开发能够在数据和机制知识有限情况下自我适应的AI模型。

Method: 提出去中心化的小型智能体网络架构，每个智能体仅覆盖系统功能子集，通过群体学习实现自我适应。

Result: 群体学习在不同群体中能够使自我适应的SANs在动态环境中提供比单体基础模型更优越的决策能力。

Conclusion: 去中心化的SANs架构是克服基础模型在动态环境中维度诅咒的有效替代方案，但代价是细节可重复性降低。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [40] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出了一种基于状态保持的多智能体进化搜索框架，用于生成鲁棒的单元测试边界情况，相比无状态方法在测试覆盖率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有无状态推理方法在多步任务中的局限性，以及任务特定微调在深层推理和长程依赖任务上的脆弱性。

Method: 结合持久推理状态、对抗性变异和进化保留的状态保持多智能体进化搜索框架，通过专门智能体顺序提出、变异和评分候选案例。

Result: 在HumanEval和TestGenEvalMini等单元测试基准上，相比无状态单步基线方法，测试覆盖率有显著提升，使用Llama、Gemma和GPT三种LLM家族验证了有效性。

Conclusion: 将持久推理状态与进化搜索相结合，能够显著改进单元测试生成的质量和覆盖率。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [41] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 本文从数学角度分析了强化学习策略中的不一致性问题，研究了通过在线RL微调离线学习策略的过程，证明了这种方法能减少不一致性并提高回报，建立了控制即推理、软Q学习与迭代重训练过程的三向对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习策略中由自回归模型简单目标条件化引起的不一致性问题，探索通过在线RL微调离线策略来改进策略性能的方法。

Method: 采用数学分析方法，重新构建控制即推理和软Q学习的标准概念，建立三向对应关系，分析迭代重训练过程作为将后验折叠到奖励中的方式，并在确定性情况下作为降低温度参数的方法。

Result: 证明了在线RL微调能减少策略不一致性并提高回报，建立了训练-推断权衡的计算内容对应关系，讨论了不一致性与有效视野之间的联系。

Conclusion: 通过软条件生成模型，揭示了不一致性与有效视野之间的关系，为改进强化学习策略的训练方法提供了理论依据。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [42] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 提出了Markovian Thinking范式，通过将推理过程分解为固定大小的块，在每个块边界重置上下文并保留简短状态，实现了线性计算复杂度和恒定内存使用，解决了长链推理中的二次计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练推理LLMs时，随着思维链增长会导致状态无界和二次计算复杂度，限制了长推理能力的发展。

Method: 设计了Delethink环境，将推理结构化为固定大小的块，在每个块边界重置上下文并学习保留关键状态信息，通过强化学习训练模型无缝继续推理。

Result: 1.5B模型在8K令牌块中推理，总思考长度达24K令牌，性能匹配或超越24K预算的LongCoT-RL，且计算成本显著降低（96K思考长度下从27个月降至7个H100月）。

Conclusion: 重新设计思考环境是实现高效、可扩展长推理LLMs的有效途径，能够在不增加二次开销的情况下支持非常长的推理过程。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [43] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: 提出了一种基于前向-前向算法的无反向传播强化学习方法ARQ，在多个基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 前向-前向算法目前主要局限于监督学习，在强化学习领域存在应用空白，而强化学习中的学习信号可以更自然地获得

Method: 引入动作条件均方根Q函数(ARQ)，结合前向-前向算法的优度函数和动作条件，使用时序差分学习进行局部强化学习

Result: 在MinAtar和DeepMind Control Suite基准测试中，ARQ优于最先进的无反向传播强化学习方法，并在大多数任务中超过使用反向传播训练的算法

Conclusion: ARQ方法简单且具有生物学基础，在强化学习任务中表现出色，为无反向传播学习提供了有效解决方案

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [44] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO是一个强化学习框架，通过探索-利用平衡改进GRPO算法，在数学和编程基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO等强化学习算法在LLM推理中存在探索不足和反馈信号利用不充分的问题，需要改进探索策略和奖励利用机制。

Method: XRPO引入数学基础的rollout分配器自适应优先处理不确定性高的提示，使用上下文种子策略解决零奖励提示停滞问题，开发基于序列似然的优势锐化机制。

Result: 在数学和编程基准测试中，XRPO比GRPO和GSPO等现有方法提升4% pass@1和6% cons@32，训练收敛速度提升2.7倍。

Conclusion: XRPO通过探索-利用平衡框架有效提升了LLM推理能力，在挑战性提示上表现更优。

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [45] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: 论文提出RICH假设：当模型训练数据更好地反映被攻击数据的组成部分时，推理计算防御能带来鲁棒性收益。通过组合泛化，模型能在对抗性OOD数据上遵循防御规范，实现训练时和测试时防御的协同效益。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM推理能提高对防御规范的遵循度，但在攻击者拥有梯度或多模态输入时效果减弱。本文旨在填补这一空白，证明推理计算在这些情况下仍能带来鲁棒性收益。

Method: 提出RICH假设，通过组合泛化机制使模型理解OOD数据的ID组成部分，从而在对抗性OOD输入上遵循防御规范。在视觉语言模型上进行实证验证，比较不同攻击类型下的鲁棒性增益。

Result: 实验表明，如果组合泛化解锁了对OOD数据的规范遵循，测试时计算确实能带来鲁棒性增益，而RL微调和长时间推理不是关键因素。通过提示增强防御规范能降低基于梯度的多模态攻击成功率。

Conclusion: 推理计算的鲁棒性收益与基础模型鲁棒性相关，形成富者愈富的RICH动态。建议将训练时和测试时防御层叠使用以获得协同效益。

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [46] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: 该论文挑战了"注意力机制就是一切"的观点，提出非循环完整模型在处理长序列任务时存在局限性，并引入了一种循环完整的架构来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 挑战当前流行的注意力机制模型在处理长序列任务时的局限性，特别是在智能体任务中，非循环完整模型无法正确聚合长时间跨度的输入信息。

Method: 引入了一种循环完整的架构，并在GitHub行为序列上进行训练，研究了损失与训练序列长度之间的关系。

Result: 损失随训练序列长度呈幂律分布，而参数数量保持不变；更长的序列训练总能摊销线性增长的时间成本，在时间函数上产生更低的损失。

Conclusion: 循环机制对于处理长序列智能体任务至关重要，循环完整架构能够有效解决非循环模型在长时间跨度信息聚合上的局限性。

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [47] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: COMPASS是一个评估LLM代理在旅行规划场景中能力的基准，将旅行规划建模为约束偏好优化问题，要求代理满足硬约束同时优化软用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM代理需要通过多轮交互掌握策略性工具使用和用户偏好优化，以协助用户完成复杂规划任务。

Method: 构建了覆盖20个美国国家公园的交通、住宿和票务的真实旅行数据库，以及模拟商业预订平台的综合工具生态系统。

Result: 评估发现两个关键差距：(i)可接受-最优差距：代理能可靠满足约束但无法优化偏好；(ii)计划协调差距：在多服务协调任务上性能崩溃，特别是开源模型。

Conclusion: COMPASS通过在实用、面向用户的领域中进行推理和规划，提供了一个直接衡量代理在现实任务中优化用户偏好能力的基准。

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [48] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: 提出一种可扩展的方法，通过合成简单问题为复杂多步依赖链来提升大语言模型的长时程推理能力，仅使用现有短时程数据，无需密集监督。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在短时程推理任务上表现优秀，但随着推理时程增长性能下降。现有方法依赖推理时支架或昂贵的逐步监督，难以扩展。

Method: 合成简单问题为任意长度的复杂多步依赖链，使用仅结果奖励在自动增加复杂度的课程下训练模型，使RL训练能够进一步扩展而不饱和。

Result: 在6年级数学问题上进行课程训练，可将竞赛级基准（GSM-Symbolic、MATH-500、AIME）的准确率提升高达2.06倍，且在高pass@k下仍显著优于基线。

Conclusion: 该方法为仅使用现有数据扩展RL解决长时程问题提供了高效路径，理论上证明课程RL比全时程训练在样本复杂度上有指数级改进。

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [49] [How We're Making Application Security Smarter](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.wealthsimple.com%2Fhow-were-making-app-security-smarter%3Futm_source=tldrinfosec/1/01000199b9b4026c-a729fee0-5319-4f4b-bd09-054522b82ad5-000000/yR70UQrS-dxJQuOK8D4Xl3NJW1FbZVuoZg8nwKwz5_M=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Wealthsimple使用Semgrep AI创建自定义修复方案，增强小型安全团队在监管环境中的代码审查能力


<details>
  <summary>Details</summary>
Motivation: 小型安全团队需要审查大量工程团队代码的常见困境，特别是在高度监管环境中

Method: 引入Semgrep AI创建针对特定代码的自定义修复方案，系统记忆先前决策并自动分类未来发现

Result: 成功增强了安全团队能力，提高了代码审查效率

Conclusion: AI辅助工具能有效解决安全团队规模与代码审查需求不匹配的问题

Abstract: How We're Making Application Security Smarter (3 minute read) Wealthsimple was faced with the common dilemma of having a small security team tasked with reviewing code written by a much larger engineering team in a highly regulated environment. It introduced Semgrep AI to create custom fixes for the specific code under review, augmenting its security team. Semgrep remembers previous decisions on findings and uses that information to auto-triage future findings

</details>


### [50] [You're Only Using 20% of Claude Code - Here's How to Unlock the Rest](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faifordevelopers%2Fp%2Fyoure-only-using-20-of-claude-code%3Futm_source=tldrwebdev/1/01000199be5bf57e-ac72aa60-cc4b-4073-a9d6-49ad8ea4ed7e-000000/JZ_fvs_blyfIIYFLT0vqhXMPOYgC_mQu7omYfGW_vHI=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 大多数开发者只使用了Claude Code的20%功能，本文介绍了如何通过正确使用MCP协议、配置CLAUDE.md文件、使用规划模式和IDE插件来解锁全部功能。


<details>
  <summary>Details</summary>
Motivation: 帮助开发者充分利用Claude Code的完整能力，避免只使用基础功能而错过高级特性。

Method: 通过正确配置Model Context Protocols (MCPs)、设置CLAUDE.md配置文件、使用Planning Mode优化项目结构、集成IDE插件进行实时错误检测和修复。

Result: 开发者可以显著提升Claude Code的使用效率，获得更好的项目结构管理和实时编程辅助。

Conclusion: 通过系统性地配置和使用Claude Code的各项功能，开发者可以充分发挥其潜力，提高开发效率。

Abstract: You're Only Using 20% of Claude Code - Here's How to Unlock the Rest (6 minute read) Most developers are only using a small fraction of Claude Code's capabilities. To fully use it, devs should use Model Context Protocols (MCPs) correctly, set up a good CLAUDE.md configuration file, use Planning Mode for better project structure, and integrate the IDE plugin for real-time error detection and fixes.

</details>


### [51] [MCP Tools: Attack Vectors and Defense Recommendations for Autonomous Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.elastic.co%2Fsecurity-labs%2Fmcp-tools-attack-defense-recommendations%3Futm_source=tldrinfosec/1/01000199bec85f36-510aa493-3d3f-4d56-b1d6-a27c31550bac-000000/4MN5cMJhFoSEB9uJz98DhEz2QaI4Jcyd4T8UwlTiXPA=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Elastic Security Labs分析了MCP工具的安全攻击向量，包括传统漏洞、工具投毒、编排注入等多种攻击方式，并提供了防御建议和恶意功能检测提示。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理和MCP工具的广泛应用，安全风险日益突出，需要系统性地识别和防御各种攻击向量，保护用户免受恶意工具的影响。

Method: 通过分析MCP工具的不同攻击向量，包括工具投毒、编排注入、地毯式重定义等，并开发了LLM提示来检测MCP服务器中的恶意功能。

Result: 识别了从传统漏洞到被动影响等多种安全风险，提供了具体的防御建议和恶意功能检测方法。

Conclusion: MCP工具存在多种安全威胁，用户应采取安全措施如使用沙箱环境、验证工具来源等来保护系统安全。

Abstract: MCP Tools: Attack Vectors and Defense Recommendations for Autonomous Agents (26 minute read) Elastic Security Labs provides a comprehensive overview of different attacks and defenses for MCP tools. Elastic provides a prompt for an LLM to detect malicious functions in an MCP server, covering security risks ranging from traditional vulnerabilities to tool poisoning, orchestration injection, rug-pull redefinitions, name collisions, and passive influence. Elastic recommends that users utilize san...

</details>


### [52] [Introducing AgentKit](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiRcOuk/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/hWuNVyrvfxHqrtdJ5EIwC2f-hubMGH6FIgjcl-TWBHQ=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI推出AgentKit，包含可视化拖拽构建代理的Agent Builder、嵌入聊天界面的ChatKit、评估工具和连接器注册表。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供更便捷的代理构建工具，降低开发门槛，提高开发效率。

Method: 采用可视化拖拽界面构建代理，提供聊天界面嵌入、评估工具和连接器注册表等功能。

Result: 推出了包含多个组件的AgentKit工具套件。

Conclusion: AgentKit为开发者提供了全面的代理开发解决方案。

Abstract: Introducing AgentKit (2 minute read) OpenAI launched Agent Builder, which is a visual drag-and-drop canvas called the "Canva for building agents", ChatKit for embedding chat interfaces, evaluation tools including trace grading and automated prompt optimization, and a connector registry for linking internal tools.

</details>


### [53] [Google's CodeMender](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fsafety-security%2Fai-security-frontier-strategy-tools%2F%3Futm_source=tldrai/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/NUMQXvi8oRwSxd8PUv2s8PVv24pO7JWLYnq3fDAtCYo=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google推出CodeMender AI代理，使用Gemini模型自动检测和修复代码漏洞，同时启动AI漏洞奖励计划和SAIF 2.0框架来提升AI驱动的网络安全。


<details>
  <summary>Details</summary>
Motivation: 解决代码安全漏洞问题，通过AI技术自动化漏洞检测和修复，同时建立安全设计原则来增强自主代理的安全性。

Method: 使用Gemini模型构建CodeMender AI代理来自动检测和修补代码漏洞，配合AI漏洞奖励计划和SAIF 2.0安全框架。

Result: 开发出能够自动检测和修复代码漏洞的AI代理系统，并建立了相应的安全标准和激励机制。

Conclusion: AI驱动的代码安全工具和框架能够有效提升软件安全性，为自主代理系统建立安全设计标准。

Abstract: Google's CodeMender (4 minute read) Google's CodeMender is an AI agent that automatically detects and patches code vulnerabilities using Gemini models. Alongside it, it launched the AI Vulnerability Reward Program and SAIF 2.0 to define secure-by-design principles for autonomous agents and boost AI-driven cybersecurity.

</details>


### [54] [Kubernetes for agentic apps: A platform engineering perspective](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fkubernetes-for-agentic-apps-a-platform-engineering-perspective%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/YKnLllB6OlUP-ID1nY_-jgPzhDaw6xFlD8-wEL-jQWs=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kubernetes为智能代理应用提供平台工程基础，GKE提供生产级智能代理AI的托管服务和AI优化


<details>
  <summary>Details</summary>
Motivation: 智能AI系统从预定义指令转向真正自主性，需要强大的平台支持

Method: 通过平台工程应用Kubernetes，利用Google Kubernetes Engine(GKE)的托管服务和AI优化

Result: Kubernetes为这种新的计算范式提供了基础架构

Conclusion: Kubernetes通过平台工程为生产级智能代理AI提供了可行的解决方案

Abstract: Kubernetes for agentic apps: A platform engineering perspective (6 minute read) Agentic AI, where systems autonomously perceive, reason, and act, is shifting software from predefined instructions to genuine autonomy. Kubernetes, applied through platform engineering, offers the foundation for this new computing paradigm, with Google Kubernetes Engine (GKE) providing managed services and AI optimizations for production-ready agentic AI.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [55] [RAG 已过时：<em class="highlight">强化学习</em>智能体 (RL Agents) 将成为新的检索栈](http://mp.weixin.qq.com/s?__biz=MzIyMDYzMTE4MQ==&mid=2247486312&idx=1&sn=188e96543bf7d178d8f2d0a1f1c2ea51&chksm=96efcbbf8e8a74ee4834f5a64e90a86f7d8c2dbb1d055bd1e12a2fa29b0187f2d75b3974d465#rd)
*健述有道*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）：通过奖励/惩罚让AI自我进化奖励函数（Reward Function）：规定“做对什么才算强”的标准大白话：让AI像玩闯关游戏一样，做对一步奖励、查错一步惩罚，从不断试错中变得又快又准


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）：通过奖励/惩罚让AI自我进化奖励函数（Reward Function）：规定“做对什么才算强”的标准大白话：让AI像玩闯关游戏一样，做对一步奖励、查错一步惩罚，从不断试错中变得又快又准

</details>


### [56] [复旦、同济和港中文等重磅发布：<em class="highlight">强化学习</em>在大语言模型全周期的全面综述](http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660378&idx=3&sn=c3b98b852675dabaaad9976e2844ec36&chksm=e82382432b3adca82b2cccea5e4dec60cb2fabed8379ac9e00cdd0f3bea1921580d7674fea77#rd)
*数据派THU*

Main category: wechat.article

TL;DR: 在强化学习框架与工具包的驱动下，强化学习算法参与大语言模型的预训练、对齐及推理增强训练，并通过测试基准进行验证。该综述深入剖析了强化学习技术如何应用于大语言模型的全生命周期阶段，如何贯穿 LLMs 的预训练、


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在强化学习框架与工具包的驱动下，强化学习算法参与大语言模型的预训练、对齐及推理增强训练，并通过测试基准进行验证。该综述深入剖析了强化学习技术如何应用于大语言模型的全生命周期阶段，如何贯穿 LLMs 的预训练、

</details>


### [57] [<em class="highlight">强化学习</em>中的熵机制 (二) 熵安全策略](http://mp.weixin.qq.com/s?__biz=MzYyNDA3NjQzNg==&mid=2247483684&idx=1&sn=4ea0a1665e471db34b9ae37d91ffb2a0&chksm=f1b50ab0606ee73066f4690a6aebbb85578fe0edaff044823e6f2b8ec0d2dcff7cd7ea92afa6#rd)
*AInon-Chihaiya*

Main category: wechat.article

TL;DR: 在本文中，我们介绍一下现代强化学习算法中用到的熵安全策略。这里的熵安全包含两个方面：防止熵坍塌、防止熵爆炸，即双边熵安全。后续的内容按照文章发表的先后顺序来介绍，并在持续更新中。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在本文中，我们介绍一下现代强化学习算法中用到的熵安全策略。这里的熵安全包含两个方面：防止熵坍塌、防止熵爆炸，即双边熵安全。后续的内容按照文章发表的先后顺序来介绍，并在持续更新中。

</details>


### [58] [深入浅出阅读OpenAI<em class="highlight">强化学习</em>经典论文《Proximal Policy Optimization Algorithms》](http://mp.weixin.qq.com/s?__biz=MzkyNTU5MjIyMQ==&mid=2247484991&idx=1&sn=7ef4c59e1ba1052ccc01000d36fa7bbc&chksm=c01028de221f0b8b58bd3cb1c46399703b567874a3aeb315b7a317e053f2227a2673ff0c0f07#rd)
*开源小栈hubtools*

Main category: wechat.article

TL;DR: 在PPO被提出之前，主流的强化学习算法主要有三类，但它们都各有短处：（1）深度Q学习 （DQN）： 这类算法在处理像雅达利（Atari）游戏这样拥有离散动作（比如上、下、左、右）的环境中表现优异 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在PPO被提出之前，主流的强化学习算法主要有三类，但它们都各有短处：（1）深度Q学习 （DQN）： 这类算法在处理像雅达利（Atari）游戏这样拥有离散动作（比如上、下、左、右）的环境中表现优异 。

</details>


### [59] [清华、NVIDIA、斯坦福提出DiffusionNFT：基于前向过程的扩散<em class="highlight">强化学习</em>新范式，训练效率提升25倍](http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247716126&idx=3&sn=38ba10ea412595fc9a558dfc80b0fab8&chksm=f841199e1322cc450b36cecb5d6aef301ca5e8f4f41d57dd9de0e4a66246613b3d0b4e7c6e01#rd)
*一点人工一点智能*

Main category: wechat.article

TL;DR: nvidia deep imagination 研究组与斯坦福 stefano ermon 团队联合提出了一种全新的扩散模型强化学习（rl）范式 ——diffusion negative-aware finetuning （diffusionnft）。该方法首次突破现有 RL 对扩散模型的基本假设，直接在前向加噪过程（forward


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: nvidia deep imagination 研究组与斯坦福 stefano ermon 团队联合提出了一种全新的扩散模型强化学习（rl）范式 ——diffusion negative-aware finetuning （diffusionnft）。该方法首次突破现有 RL 对扩散模型的基本假设，直接在前向加噪过程（forward

</details>


### [60] [全新合成框架SOTA：<em class="highlight">强化学习</em>当引擎，任务合成当燃料，蚂蚁港大联合出品](http://mp.weixin.qq.com/s?__biz=MzIwMTg4OTgxMQ==&mid=2247532021&idx=2&sn=9842fbc83f1016ad268aac86e810fdc1&chksm=97a0b8b6d1daef37af65211e32fbe25b6afed25dd95ec77f9c3b29e5ecacd80b6c37c17eb996#rd)
*机械进化2人工智能*

Main category: wechat.article

TL;DR: 一是强化学习。作为强化学习之年，该项技术已经得到社区足够多的关注与投入，无论是方法还是框架都在急速推进。reinforcement learning 而另一个，团队认为是任务合成。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一是强化学习。作为强化学习之年，该项技术已经得到社区足够多的关注与投入，无论是方法还是框架都在急速推进。reinforcement learning 而另一个，团队认为是任务合成。

</details>


### [61] [<em class="highlight">强化学习</em>之父，给大模型判了“死刑”](http://mp.weixin.qq.com/s?__biz=MzkxNzUwMTk5NQ==&mid=2247497623&idx=1&sn=c638315732675e99cc3861f2bafeb82e&chksm=c0513a28a6021a47c3ee7ae36a40f471a24a6d9119925ec021946dbbfc37fbf42fdecd26211b#rd)
*AI故事计划*

Main category: wechat.article

TL;DR: 强化学习的核心思想其实很简单：让AI像人类婴儿一样，通过不断试错来学习。比如小孩学走路，没人教“先抬左脚再迈右脚”，摔多了自然就会了。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习的核心思想其实很简单：让AI像人类婴儿一样，通过不断试错来学习。比如小孩学走路，没人教“先抬左脚再迈右脚”，摔多了自然就会了。

</details>


### [62] [智能体<em class="highlight">强化学习</em>新SOTA！阿里通义联合提出在线过程奖励学习OPRL，兼容GRPO与PPO](http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994858&idx=1&sn=31a9600c26c8b2d8ca55941485022eae&chksm=b0a79eb944af77a6c43e036b8f08517b0a7114be5d1e0702af3fabbef3990eeea7ed522e6390#rd)
*智猩猩GenAI*

Main category: wechat.article

TL;DR: 大型语言模型（LLM）正越来越多地通过强化学习（RL）被训练为能够在交互环境中进行长周期推理与行动的自主智能体。然而，稀疏且有时不可验证的奖励信号使得时序信用分配（temporal credit assignment）变得极具挑战性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大型语言模型（LLM）正越来越多地通过强化学习（RL）被训练为能够在交互环境中进行长周期推理与行动的自主智能体。然而，稀疏且有时不可验证的奖励信号使得时序信用分配（temporal credit assignment）变得极具挑战性。

</details>


### [63] [<em class="highlight">强化学习</em>之父Richard Sutton：大语言模型是死路一条？](http://mp.weixin.qq.com/s?__biz=MzIwOTQyNzA5Mw==&mid=2247485961&idx=1&sn=d8b83c74a8f36b2f38ed962ff18d66b0&chksm=96572c219a0183eed9c5a9928028f4162f4a4c567f8eb5df7fa0fce37f58b9fe40e41dfad71a#rd)
*一苇杭之o*

Main category: wechat.article

TL;DR: Sutton在访谈中开门见山：“强化学习是关于理解你的世界，而大语言模型是关于模仿人类做人们说你应该做的事情，他们不是在搞清楚该做什么。”


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Sutton在访谈中开门见山：“强化学习是关于理解你的世界，而大语言模型是关于模仿人类做人们说你应该做的事情，他们不是在搞清楚该做什么。”

</details>


### [64] [麦肯锡：<em class="highlight">Agentic</em> AI](http://mp.weixin.qq.com/s?__biz=Mzg2NzA1MTYxNw==&mid=2247497277&idx=1&sn=c5fe9246cfb01b667c22e44a9c26125b&chksm=cf9c9019bb50b0c5c6eb52fc8be9c4542840c7220491e26a9ad3f71b0a6c5087d0f18d96eb76#rd)
*抚琴轩*

Main category: wechat.article

TL;DR: Agentic AI的崛起并非生成式AI的简单升级，而是人工智能范式的根本性转变。与传统AI工具"指令-响应"的被动模式不同，Agentic AI具备三大核心特质：自主规划复杂任务的能力、跨工具协同的行动力、动态环境中的学习迭代性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI的崛起并非生成式AI的简单升级，而是人工智能范式的根本性转变。与传统AI工具"指令-响应"的被动模式不同，Agentic AI具备三大核心特质：自主规划复杂任务的能力、跨工具协同的行动力、动态环境中的学习迭代性。

</details>


### [65] [一篇大模型<em class="highlight">Agentic</em>框架到应用最新综述：一文读懂LLM推理框架的前沿进展](http://mp.weixin.qq.com/s?__biz=MzIwOTA1MDAyNA==&mid=2650040956&idx=3&sn=21607d4854a654529e467818f1faeb35&chksm=8e773f562ed5bfa19cc427cc6ff6ac7c0c11dcec0690e9d8a5db1cb17e7e04671c6abd01b215#rd)
*人工智能学家*

Main category: wechat.article

TL;DR: agentic ai来源：AIGC深一度llm-based agentic reasoning frameworks： a survey from methods to scenarios bingxi zhao*， beijing jiaotong university， china and lancaster university， united kingdom lin geng foo*， ...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic ai来源：AIGC深一度llm-based agentic reasoning frameworks： a survey from methods to scenarios bingxi zhao*， beijing jiaotong university， china and lancaster university， united kingdom lin geng foo*， max planck institute for informatics， saarland informatics campus， germany

</details>


### [66] [借助 <em class="highlight">Agentic</em> AI 加速迁移与现代化](http://mp.weixin.qq.com/s?__biz=MzU2OTY2OTQ0MA==&mid=2247549279&idx=1&sn=2d11d5d6ea4a0f4140ff12092cffc245&chksm=fdc8e3579e5bd2b55f62202b700d71596dc189622909aa408834dfdaa34e9bebc335ad7fe47e#rd)
*Azure云科技*

Main category: wechat.article

TL;DR: 结合 Agentic AI（国际版）与端到端支持，您能比以往更轻松地强势起步并快速扩展。无论您是刚刚开启上云之旅，还是正在将 AI 应用扩展到整个组织，Azure 都能为您提供清晰的路径、更快的速度和更强的支持，助您自信前行。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 结合 Agentic AI（国际版）与端到端支持，您能比以往更轻松地强势起步并快速扩展。无论您是刚刚开启上云之旅，还是正在将 AI 应用扩展到整个组织，Azure 都能为您提供清晰的路径、更快的速度和更强的支持，助您自信前行。

</details>


### [67] [BPMN2.0的<em class="highlight">Agentic</em>进化](http://mp.weixin.qq.com/s?__biz=Mzk2NDU5NjcxOQ==&mid=2247484145&idx=1&sn=d62f41aa4bc2fb03ccc9d05735344e6a&chksm=c5fe73ee6504324d151bad5c12fb08a947d73e6479bcf8847d5004b7610b9c9b59dc9666c640#rd)
*ORI奥锐方*

Main category: wechat.article

TL;DR: 1.它被识别为一个Agentic Node（智能体节点）；2.内部的任务被视为可供选择的“行动候选集”，这里可以理解为ReAct模式中的可执行的工具集（Tools）；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1.它被识别为一个Agentic Node（智能体节点）；2.内部的任务被视为可供选择的“行动候选集”，这里可以理解为ReAct模式中的可执行的工具集（Tools）；

</details>


### [68] [如何用 <em class="highlight">Agentic</em> AI 产品帮你打造 <em class="highlight">Agentic</em> AI 系统，自动处理复杂任务？](http://mp.weixin.qq.com/s?__biz=MzIyODI1MzYyNA==&mid=2653549781&idx=1&sn=13a0fc81e4d7a3a07de19261d8abd73b&chksm=f2ff5331197b3b163f20ea47e449d9bc97a3b80b19e99d07666aa2adf3cbd0ff02adaa09dd6a#rd)
*玉树芝兰*

Main category: wechat.article

TL;DR: Agentic AI 系统不同于传统的工作流，我们实际上是让不同的大模型承担不同的角色与任务，并相互配合，完成复杂的任务乃至完整项目。在以前，这样的工作，你需要先熟练掌握 CrewAI 或者 Agno 这样的框架，才能慢慢搭建。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 系统不同于传统的工作流，我们实际上是让不同的大模型承担不同的角色与任务，并相互配合，完成复杂的任务乃至完整项目。在以前，这样的工作，你需要先熟练掌握 CrewAI 或者 Agno 这样的框架，才能慢慢搭建。

</details>


### [69] [跟着吴恩达学<em class="highlight">Agentic</em>：如何用一套工作流，让GPT-3.5的表现超越GPT-4](http://mp.weixin.qq.com/s?__biz=Mzk4ODAyOTIxNQ==&mid=2247491147&idx=1&sn=fb925f90526486195adc99ff9dfed7e0&chksm=c4decb69d593abd544041b85e45076343b94a0a2a2a3512e3b9f13c4d9f4578757116485174f#rd)
*万涂幻象*

Main category: wechat.article

TL;DR: 所以他干脆自己造了个词 “Agentic”。IT'S AN AGENT！IT'S AGENTIC！NO，IT'S NOT！ai fund sequoia ascent， march 2024 x （twitter） post ， june 2024目的就是为了跳出“是不是”的标签之争，进入“有多少”的工程实践。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 所以他干脆自己造了个词 “Agentic”。IT'S AN AGENT！IT'S AGENTIC！NO，IT'S NOT！ai fund sequoia ascent， march 2024 x （twitter） post ， june 2024目的就是为了跳出“是不是”的标签之争，进入“有多少”的工程实践。

</details>


### [70] [如何将 <em class="highlight">Agentic</em> AI 与您的流程相匹配](http://mp.weixin.qq.com/s?__biz=MzU5MzgyNzY1NA==&mid=2247501715&idx=1&sn=b2a1cb8e3b925368656edc37405181dd&chksm=ff436056bb5a6f6c8356ec7cb51bc8146f230029c8a6659e42a73418385bd56e40e114c34e32#rd)
*Software AG Asia*

Main category: wechat.article

TL;DR: 本文旨在为业务流程管理实践者或Agentic AI 架构师提供支持。 未来已至——近在咫尺当前（2025年）最引人入胜却也备受炒作的话题之一，当属 agentic AI。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文旨在为业务流程管理实践者或Agentic AI 架构师提供支持。 未来已至——近在咫尺当前（2025年）最引人入胜却也备受炒作的话题之一，当属 agentic AI。

</details>


### [71] [一篇大模型<em class="highlight">Agentic</em>框架到应用最新综述：一文读懂LLM推理框架的前沿进展](http://mp.weixin.qq.com/s?__biz=MzkxMDcwMDExOQ==&mid=2247493176&idx=1&sn=9c62458738a5daf98df7ee760bfdbfaa&chksm=c09b2bd1a2472a4f0fee17fdb2a45dfcca85cab2e702efbbd7ccdd9051e6bb5c3eace6b32422#rd)
*AIGC 深一度*

Main category: wechat.article

TL;DR: llm-based agentic reasoning frameworks： a survey from methods to scenarios bingxi zhao*， beijing jiaotong university， china and lancaster university， united kingdom lin geng foo*， max planck institute...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: llm-based agentic reasoning frameworks： a survey from methods to scenarios bingxi zhao*， beijing jiaotong university， china and lancaster university， united kingdom lin geng foo*， max planck institute for informatics， saarland informatics campus， germany ping hu， university of electr

</details>


### [72] [AI<em class="highlight">智能体</em>到底是什么？从<em class="highlight">Agentic</em> AI到AI Agent，一篇讲透核心原理！](http://mp.weixin.qq.com/s?__biz=MzE5MTcxNzUyMQ==&mid=2247484687&idx=1&sn=fbf4bbff429bc48e10fa0354532d42e5&chksm=97b78898a6227b6b0d27df5635ca1a449d8de60dcd4e955867a88f4397f276902274e8bbe95e#rd)
*AI产品进化录*

Main category: wechat.article

TL;DR: “Agentic” 来自英文 “Agent”，意思是“具有自主性、目标导向的实体”。所以，Agentic AI = 具有“主动性”的AI。与传统AI（被动响应）不同，Agentic AI 的关键词是：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “Agentic” 来自英文 “Agent”，意思是“具有自主性、目标导向的实体”。所以，Agentic AI = 具有“主动性”的AI。与传统AI（被动响应）不同，Agentic AI 的关键词是：

</details>


### [73] [开源的语音理解<em class="highlight">大模型</em>——OSUM](http://mp.weixin.qq.com/s?__biz=MzU4OTczODQwNw==&mid=2247487805&idx=1&sn=d0d1408840017965d80dc3b1dc1910f5&chksm=fc2f47902166f15ae40b0e63a2ef870f2f84f23e4f6c36a866ce724bf30e09d43e34b0a04c46#rd)
*放牛娃的杂货铺*

Main category: wechat.article

TL;DR: 现有的SULM模型，如Whisper、Qwen-audio、SenseVoice和TouchASP等，大部分由工业界开发，利用了数百万小时的训练数据和海量GPU资源。这样的大规模资源通常超出了学术界机构的能力范围。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 现有的SULM模型，如Whisper、Qwen-audio、SenseVoice和TouchASP等，大部分由工业界开发，利用了数百万小时的训练数据和海量GPU资源。这样的大规模资源通常超出了学术界机构的能力范围。

</details>


### [74] [<em class="highlight">大模型</em>进展专题解读](http://mp.weixin.qq.com/s?__biz=MzAwNDI5NjIxMw==&mid=2247500076&idx=2&sn=3654ae1efd794ce22ee973ebbe804262&chksm=9a8dceb684a2198811985cf399765ee38d10bea399a3d50415ea62047f694517857ce780e3c3#rd)
*老司机驾新车*

Main category: wechat.article

TL;DR: 大模型进展专题解读 一、大模型性能比拼：技术突破与市场趋势尽显 智谱4.6：代码能力领衔，上下文窗口投资潜力凸显：智谱4.6节前发布后登顶huggingface趋势榜第一，其能力核心源于4.5版本，4.5版本曾使GPT5推迟发布。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型进展专题解读 一、大模型性能比拼：技术突破与市场趋势尽显 智谱4.6：代码能力领衔，上下文窗口投资潜力凸显：智谱4.6节前发布后登顶huggingface趋势榜第一，其能力核心源于4.5版本，4.5版本曾使GPT5推迟发布。

</details>


### [75] [3.8亿<em class="highlight">大模型</em>大单！讯飞拿下，华为宇树都赚了](http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652790362&idx=1&sn=c328e79b923fe4e3eb70ebae355c237a&chksm=8557a4032629dce0f77e800d43edb5760b9c1ae311858618873cc0587ec879d71df3a994091b#rd)
*智东西*

Main category: wechat.article

TL;DR: 这些场景的软件及硬件方案都深度融入了大模型。整个报价文件中“大模型”出现了79次，“智能体”出现了96次。以AI+教育为例，该方案引入了大模型AI教师备授课助手，采用科大讯飞的讯飞课程资源系统V5.0，包括教材电子化


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这些场景的软件及硬件方案都深度融入了大模型。整个报价文件中“大模型”出现了79次，“智能体”出现了96次。以AI+教育为例，该方案引入了大模型AI教师备授课助手，采用科大讯飞的讯飞课程资源系统V5.0，包括教材电子化

</details>


### [76] [蚂蚁、OpenAI、DeepSeek卷疯了！国产最强万亿参数旗舰<em class="highlight">模型</em>开源](http://mp.weixin.qq.com/s?__biz=MjM5ODIzNTc2MA==&mid=2661065374&idx=2&sn=1706e04c0f91d32c283f763e42aaa0d5&chksm=bc7cac4df1bd2dd88e0bf729d24793fcf5db07d99a4fb4eac69807d4a88cdf4cb809c36b7b81#rd)
*钛媒体*

Main category: wechat.article

TL;DR: 据悉，Ling-1T是蚂蚁百灵大模型Ling 2.0 系列的第一款旗舰模型，也是蚂蚁百灵团队迄今为止推出的规模最大、能力最强的非思考大模型。基准测试数据显示，在有限输出Token条件下，Ling-1T于多项复杂推理基准中取得SOTA表现，同时


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 据悉，Ling-1T是蚂蚁百灵大模型Ling 2.0 系列的第一款旗舰模型，也是蚂蚁百灵团队迄今为止推出的规模最大、能力最强的非思考大模型。基准测试数据显示，在有限输出Token条件下，Ling-1T于多项复杂推理基准中取得SOTA表现，同时

</details>


### [77] [[智能传播]赵子忠丨<em class="highlight">大模型</em>智能体：媒体智能化的创新与发展](http://mp.weixin.qq.com/s?__biz=Mzk0MjcxOTg2MA==&mid=2247486962&idx=1&sn=f90347a2a1480db35dcd368af0306113&chksm=c29e13dbab2769d0bd44358065a6000df45ba392486504054efdf0128c2ab17e7d87183d0236#rd)
*新闻传播学刊*

Main category: wechat.article

TL;DR: 在智能体的发展进程中，大模型企业通过技术开放和工具下沉，显著提升了用户的智能应用能力和参与度。令小雄等学者指出，智能技术与传媒业务的深度融合，正从根本上有力赋能媒体内容生产。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在智能体的发展进程中，大模型企业通过技术开放和工具下沉，显著提升了用户的智能应用能力和参与度。令小雄等学者指出，智能技术与传媒业务的深度融合，正从根本上有力赋能媒体内容生产。

</details>


### [78] [大语言<em class="highlight">模型</em>-编程能力测评 25-10月榜](http://mp.weixin.qq.com/s?__biz=Mzk3NTI3ODI4MA==&mid=2247484157&idx=1&sn=2d3f7c9a0f427526c20c9aebca960e74&chksm=c5841a4a950d7b0e4a20b372397d4c68a315de06232cec19ae15a2ef4c8283281002c3f8c7b4#rd)
*大模型观测员*

Main category: wechat.article

TL;DR: 多轮：测试框架自动进行大模型输出的编译和用例测试，将编译错误，运行错误，或不通过的用例作为多轮输入，让大模型进行修改，直到所有用例通过，或达到设定最高轮数（目前是3轮）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 多轮：测试框架自动进行大模型输出的编译和用例测试，将编译错误，运行错误，或不通过的用例作为多轮输入，让大模型进行修改，直到所有用例通过，或达到设定最高轮数（目前是3轮）。

</details>


### [79] [【报告】<em class="highlight">大模型</em>专题二：2025<em class="highlight">大模型</em>平台落地实践研究报告（附PDF下载）](http://mp.weixin.qq.com/s?__biz=MzU4NjY2MDYxMg==&mid=2247531105&idx=2&sn=3c6b7c3de9677721055fbddd9d13088a&chksm=fc2bb39593c1053c6acb62f6030ce1bc563e25c53f6022a5d8a250b3fd831e626aeacc30502c#rd)
*墨玫人工智能*

Main category: wechat.article

TL;DR: 大模型平台是大模型落地的工程化基座，将基础模型转化为业 务场景下的大模型应用，构建起覆盖模型开发、调优、部署、管理、 应用的全生命周期工具链与技术支撑体系，为大模型落地过程中面 临的技术转化、场景适配、效


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型平台是大模型落地的工程化基座，将基础模型转化为业 务场景下的大模型应用，构建起覆盖模型开发、调优、部署、管理、 应用的全生命周期工具链与技术支撑体系，为大模型落地过程中面 临的技术转化、场景适配、效

</details>


### [80] [求索|电子标准院工业<em class="highlight">大模型</em>评测结果公布](http://mp.weixin.qq.com/s?__biz=MzA3MzU4MDMyMw==&mid=2247492879&idx=2&sn=e1d8c1e95753850a57e80efa724f9922&chksm=9e044d9d77f97a9d1f7e60db285fc8b4ef021711c1b3c96be6a34a63b19aa2d97f65a1d7c51b#rd)
*人工智能标准化 SC42*

Main category: wechat.article

TL;DR: 工业大模型通过融合行业知识、多模态数据和智能算法，在智能制造、工业设计、设备运维、供应链优化等场景展现出巨大潜力，成为推动新型工业化的重要引擎。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 工业大模型通过融合行业知识、多模态数据和智能算法，在智能制造、工业设计、设备运维、供应链优化等场景展现出巨大潜力，成为推动新型工业化的重要引擎。

</details>


### [81] [B 站基于<em class="highlight">大模型</em>的大数据智能诊断助手实践](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771110&idx=3&sn=b916a31f21b161d3b104a84f27adc04d&chksm=fad902ac7073c8b5d0053f04035c04f0525b03a5dc6a5706c484be1af5ee1c49fa33eaef9eb6#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: 导读 本文将分享 B 站基于大语言模型的智能体助手实践。分享嘉宾｜郭跃鹏 哔哩哔哩 软件工程师编辑整理｜汪维内容校对｜李瑶1. 整体架构和规模


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 导读 本文将分享 B 站基于大语言模型的智能体助手实践。分享嘉宾｜郭跃鹏 哔哩哔哩 软件工程师编辑整理｜汪维内容校对｜李瑶1. 整体架构和规模

</details>
