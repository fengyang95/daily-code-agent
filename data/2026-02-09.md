<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.AI](#cs.AI) [Total: 17]
- [tldr.article](#tldr.article) [Total: 15]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

TL;DR: 提出基于戏剧理论的角色与场景记忆架构(CAST)，通过构建3D场景(时间/地点/主题)并组织成角色档案来表示情景记忆，结合图式语义记忆形成双记忆系统，显著提升对话任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统主要关注语义回忆，将经验处理为键值对、向量或图结构，难以表示和检索连贯的事件。人类的情景记忆能够回忆基于谁、何时、何地的连贯事件，需要更好的记忆架构来支持这种能力。

Method: 提出CAST记忆架构，受戏剧理论启发：1)构建3D场景(时间/地点/主题)作为情景记忆基础；2)将场景组织成角色档案，总结角色的事件；3)结合图式语义记忆形成双记忆设计，增强记忆系统的鲁棒性。

Result: 实验显示CAST在多个数据集上平均提升8.11% F1分数和10.21% LLM-as-a-Judge评分，尤其在开放性和时间敏感性对话问题上表现突出，显著优于基线方法。

Conclusion: CAST通过戏剧理论启发的角色与场景架构有效表示情景记忆，结合语义记忆形成双记忆系统，显著提升智能体在对话任务中的记忆和推理能力，为智能体记忆系统设计提供了新思路。

Abstract: Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [2] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

TL;DR: 该论文是一篇关于智能体记忆系统的综述，提出了一个统一的三维框架（记忆基质、认知机制、记忆主体），分析了不同智能体拓扑中的记忆实现与操作，并回顾了评估基准与未来挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能研究正从注重模型创新和基准分数转向强调问题定义和现实世界评估。在"下半场"中，核心挑战是在长时程、动态、用户依赖的环境中实现实际效用，智能体面临上下文爆炸，需要持续积累、管理和选择性重用大量信息。记忆因此成为填补效用鸿沟的关键解决方案。

Method: 提出了一个统一的基础智能体记忆三维框架：1) 记忆基质（内部和外部），2) 认知机制（情景、语义、感知、工作、程序记忆），3) 记忆主体（智能体中心与用户中心）。分析了不同智能体拓扑中的记忆实例化和操作，强调了记忆操作的学习策略。

Result: 提供了一个全面的智能体记忆系统分类和分析框架，涵盖了记忆的不同维度、实现方式和评估方法。该框架能够统一理解当前数百篇相关论文中的记忆研究。

Conclusion: 记忆是解决智能体在复杂现实环境中效用问题的关键。论文提出的三维框架为理解和设计智能体记忆系统提供了统一视角，同时指出了评估基准、度量标准和未来研究方向等开放挑战。

Abstract: The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [3] [Can One-sided Arguments Lead to Response Change in Large Language Models?](https://arxiv.org/abs/2602.06260)
*Pedro Cisneros-Velarde*

Main category: cs.CL

TL;DR: 研究发现，通过仅提供单方面论据，可以简单直观地引导大型语言模型在争议性问题上转向特定立场，这种现象在不同模型、论据数量和话题中普遍存在。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在争议性问题上通常提供平衡回答、采取单一立场或拒绝回答。本研究旨在探索是否可以通过仅提供单方面论据这种简单直观的方式来引导模型转向特定观点。

Method: 构建小型数据集，系统研究三个维度：(i) LLM回应中诱导的立场，(ii) 争议性问题的表述方式，(iii) 论据的呈现方式。测试不同模型、论据数量和话题下的意见引导效果。

Result: 研究发现意见引导现象在三个维度上普遍存在，适用于不同模型、论据数量和话题。当切换到其他论据时，意见引导效果会一致下降。

Conclusion: 仅提供单方面论据可以有效引导LLM在争议性问题上转向特定立场，这揭示了LLM在意见形成过程中的可操控性，对理解模型偏见和安全性具有重要意义。

Abstract: Polemic questions need more than one viewpoint to express a balanced answer. Large Language Models (LLMs) can provide a balanced answer, but also take a single aligned viewpoint or refuse to answer. In this paper, we study if such initial responses can be steered to a specific viewpoint in a simple and intuitive way: by only providing one-sided arguments supporting the viewpoint. Our systematic study has three dimensions: (i) which stance is induced in the LLM response, (ii) how the polemic question is formulated, (iii) how the arguments are shown. We construct a small dataset and remarkably find that opinion steering occurs across (i)-(iii) for diverse models, number of arguments, and topics. Switching to other arguments consistently decreases opinion steering.

</details>


### [4] [RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution](https://arxiv.org/abs/2602.06275)
*Isaac Picov,Ritesh Goru*

Main category: cs.CL

TL;DR: RoPE-LIME：一种用于闭源LLM输出的解释方法，通过小型开源代理模型计算基于概率目标的token级归因，减少API调用并提高解释质量


<details>
  <summary>Details</summary>
Motivation: 闭源LLM输出的解释具有挑战性，因为API访问阻止了基于梯度的归因方法，而扰动方法在依赖重新生成文本时成本高且噪声大

Method: 1) 使用小型开源代理模型计算基于概率目标（负对数似然和散度目标）的token级归因；2) 在RoPE嵌入空间中基于松弛词移动距离的局部核；3) 稀疏K采样，一种高效的扰动策略以提高有限预算下的交互覆盖

Result: 在HotpotQA（句子特征）和手工标注的MMLU子集（词特征）上的实验表明，RoPE-LIME比留一采样产生更具信息量的归因，相比gSMILE有所改进，同时大幅减少闭源模型API调用

Conclusion: RoPE-LIME为闭源LLM输出提供了一种有效的解释方法，通过解耦推理和解释过程，在减少API成本的同时提高了归因质量

Abstract: Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.

</details>


### [5] [Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math](https://arxiv.org/abs/2602.06291)
*Guijin Son,Donghun Yang,Hitesh Laxmichand Patel,Hyunwoo Ko,Amit Agarwal,Sunghee Ahn,Kyong-Ha Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出Consequence-Based Utility评估方法，通过测试候选方案在解决相关可验证问题时的价值来评分，无需专家验证，在数学推理任务上优于现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型能生成研究级数学问题的可能解法，但验证过程消耗专家时间成为瓶颈。需要一种无需专家验证的评估方法，能够区分正确和错误的解决方案。

Method: 提出Consequence-Based Utility评估器，基于假设：有效解决方案应包含足够的方法级信息，当应用于相关问题时能产生更好的下游性能。通过测试每个候选方案作为上下文示例解决相关可验证问题的价值来评分。

Result: 在原创研究级数学问题集上评估，每个问题配有一个专家编写方案和九个LLM生成方案。Consequence-Based Utility在排名质量上一致优于奖励模型、生成奖励模型和LLM法官。对GPT-OSS-120B，将Acc@1从67.2提升到76.3，AUC从71.4提升到79.6。

Conclusion: Consequence-Based Utility提供了一种有效的无专家评估方法，能够准确区分数学推理任务中的正确和错误解决方案，即使在底层求解器经常失败的情况下也能保持较强的正确-错误分离。

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

</details>


### [6] [SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass](https://arxiv.org/abs/2602.06358)
*Yewei Liu,Xiyuan Wang,Yansheng Mao,Yoav Gelbery,Haggai Maron,Muhan Zhang*

Main category: cs.CL

TL;DR: SHINE是一个可扩展的超网络，能够将多样化的有意义的上下文映射为高质量的大语言模型LoRA适配器，通过单次前向传播将上下文知识转化为参数知识，显著节省时间、计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 现有超网络在表达能力和可扩展性方面存在限制，需要一种能够高效将上下文知识转化为模型参数知识的方法，避免传统微调带来的高计算和内存成本。

Method: 提出SHINE超网络架构，重用冻结LLM参数进行上下文超网络设计，引入架构创新，通过预训练和指令微调流程，训练超网络从多样化上下文生成高质量LoRA适配器。

Result: 在各种任务上取得优异结果，相比基于SFT的LLM适配方法，显著节省时间、计算和内存成本，显示出良好的扩展潜力。

Conclusion: SHINE通过创新的超网络设计，实现了高效的单次上下文到参数知识转换，为大语言模型适配提供了可扩展且高效的解决方案。

Abstract: We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE

</details>


### [7] [TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking](https://arxiv.org/abs/2602.06440)
*Sung-Hoon Yoon,Ruizhi Qian,Minda Zhao,Weiyue Li,Mengyu Wang*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的历史感知越狱框架，通过分析并重新加权先前步骤中的漏洞信号来指导未来决策，显著提高了越狱成功率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱技术大多未能有效利用先前交互轮次中揭示的漏洞，导致攻击效率低下且不稳定。由于越狱涉及序列交互，其中每个响应都会影响未来行动，强化学习为此问题提供了自然框架。

Method: 提出历史感知的强化学习越狱框架，分析并重新加权先前步骤中的漏洞信号来指导未来决策。在此基础上引入基于注意力的重新加权机制，突出交互历史中的关键漏洞，实现更高效的探索和更少的查询。

Result: 在AdvBench和HarmBench上的大量实验表明，该方法实现了最先进的越狱性能，同时显著提高了查询效率。仅纳入历史信息就能提高越狱成功率。

Conclusion: 历史漏洞信号在强化学习驱动的越狱策略中至关重要，为推进LLM安全防护的对抗性研究提供了原则性途径。

Abstract: Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

</details>


### [8] [CORE: Comprehensive Ontological Relation Evaluation for Large Language Models](https://arxiv.org/abs/2602.06446)
*Satyam Dwivedi,Sanjukta Ghosh,Shivam Dwivedi,Nishi Kumari,Anil Thakur,Anurag Purushottam,Deepak Alok,Praveen Gatla,Manjuprasad B,Bipasha Patgiri*

Main category: cs.CL

TL;DR: 论文提出CORE基准测试，评估LLM区分语义相关性和无关性的能力，发现LLM在无关性推理上表现严重不足，存在系统性语义崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估很少测试其区分有意义语义关系和真正无关性的能力，这是LLM评估和安全的关键前沿问题。

Method: 构建CORE数据集（22.5万多选题）和开源基准（203个严格验证问题），覆盖74个学科和24种语义关系类型，包含同等数量的无关对，评估29个SOTA LLM。

Result: 人类基线准确率92.6%（无关对95.1%），而LLM整体准确率48.25-70.9%，相关对表现良好（86.5-100%），但无关对严重退化（0-41.35%），语义崩溃率平均37.6%。

Conclusion: 无关性推理是LLM评估和安全的关键未充分评估前沿，LLM存在系统性生成虚假关系的倾向，在领域特定语义推理方面面临重大挑战。

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

</details>


### [9] [Evaluating an evidence-guided reinforcement learning framework in aligning light-parameter large language models with decision-making cognition in psychiatric clinical reasoning](https://arxiv.org/abs/2602.06449)
*Xinxin Lin,Guangxin Dai,Yi Zhong,Xiang Li,Xue Xiao,Yixin Zhang,Zhengdong Wu,Yongbo Zheng,Runchuan Zhu,Ming Zhao,Huizi Yu,Shuo Wu,Jun Zhao,Lingming Hu,Yumei Wang,Ping Yin,Joey W. Y. Chan,Ngan Yin Chan,Sijing Chen,Yun Kwok Wing,Lin Lu,Xin Ma,Lizhou Fan*

Main category: cs.CL

TL;DR: ClinMPO是一个强化学习框架，通过专业精神病学实践对齐LLM内部推理，使轻量级模型在复杂精神病诊断任务上超越医学生表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在精神病学应用中存在幻觉和浅层推理问题，特别是轻量级模型在临床部署中面临挑战。现有训练范式过于注重语言流畅性而非结构化临床逻辑，导致与专业诊断认知不匹配。

Method: 提出ClinMPO强化学习框架，使用专门奖励模型，该模型基于4,474篇精神病学期刊文章数据集独立训练，并按照循证医学原则结构化。在Qwen3-8B模型上进行微调。

Result: ClinMPO调优的Qwen3-8B模型在复杂精神病诊断任务上达到31.4%准确率，超过300名医学生30.8%的基准表现，特别是在大型模型常失败的复杂案例上表现优异。

Conclusion: 医学证据引导的优化使轻量级LLM能够掌握复杂推理任务，明确的认知对齐为可靠、安全的精神病决策支持提供了可扩展路径。

Abstract: Large language models (LLMs) hold transformative potential for medical decision support yet their application in psychiatry remains constrained by hallucinations and superficial reasoning. This limitation is particularly acute in light-parameter LLMs which are essential for privacy-preserving and efficient clinical deployment. Existing training paradigms prioritize linguistic fluency over structured clinical logic and result in a fundamental misalignment with professional diagnostic cognition. Here we introduce ClinMPO, a reinforcement learning framework designed to align the internal reasoning of LLMs with professional psychiatric practice. The framework employs a specialized reward model trained independently on a dataset derived from 4,474 psychiatry journal articles and structured according to evidence-based medicine principles. We evaluated ClinMPO on a unseen subset of the benchmark designed to isolate reasoning capabilities from rote memorization. This test set comprises items where leading large-parameter LLMs consistently fail. We compared the ClinMPO-aligned light LLM performance against a cohort of 300 medical students. The ClinMPO-tuned Qwen3-8B model achieved a diagnostic accuracy of 31.4% and surpassed the human benchmark of 30.8% on these complex cases. These results demonstrate that medical evidence-guided optimization enables light-parameter LLMs to master complex reasoning tasks. Our findings suggest that explicit cognitive alignment offers a scalable pathway to reliable and safe psychiatric decision support.

</details>


### [10] [Diffusion-State Policy Optimization for Masked Diffusion Language Models](https://arxiv.org/abs/2602.06462)
*Daisuke Oba,Hiroki Furuta,Naoaki Okazaki*

Main category: cs.CL

TL;DR: DiSPO是一种用于掩码扩散语言模型的信用分配方法，通过在中间掩码状态分支采样并评分，优化中间填充决策，无需额外多步扩散展开。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型通过多步去噪迭代填充掩码标记，仅基于最终完成的终端奖励学习会导致对中间决策的信用分配过于粗糙。

Method: DiSPO在选定的中间掩码状态分支，从展开缓存的logits中重新采样当前掩码位置的填充，对生成的完成进行评分，并仅更新新填充的标记，无需额外的多步扩散展开。

Result: 在LLaDA-8B-Instruct上，DiSPO在匹配的展开计算和优化器步骤下，在数学和规划基准测试中持续优于终端反馈的diffu-GRPO基线。

Conclusion: DiSPO作为一种插件式信用分配层，能够有效优化掩码扩散语言模型的中间填充决策，提高模型性能。

Abstract: Masked diffusion language models generate by iteratively filling masked tokens over multiple denoising steps, so learning only from a terminal reward on the final completion yields coarse credit assignment over intermediate decisions. We propose DiSPO (Diffusion-State Policy Optimization), a plug-in credit-assignment layer that directly optimizes intermediate filling decisions. At selected intermediate masked states, DiSPO branches by resampling fillings for the currently masked positions from rollout-cached logits, scores the resulting completions, and updates only the newly filled tokens -- without additional multi-step diffusion rollouts. We formalize a fixed-state objective for branched completions and derive a policy-gradient estimator that can be combined with terminal-feedback policy optimization using the same rollouts. On LLaDA-8B-Instruct, DiSPO consistently improves over the terminal-feedback diffu-GRPO baseline on math and planning benchmarks under matched rollout compute and optimizer steps. Our code will be available at https://daioba.github.io/dispo .

</details>


### [11] [Improve Large Language Model Systems with User Logs](https://arxiv.org/abs/2602.06470)
*Changyue Wang,Weihang Su,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: UNO是一个从用户日志中学习优化LLM系统的统一框架，通过将非结构化日志转化为结构化规则和偏好对，并量化模型先验知识与日志数据之间的认知差距，自适应过滤噪声反馈，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM发展依赖大规模高质量数据和参数扩展，但面临高质量数据稀缺和计算成本收益递减的问题。用户交互日志作为真实人类反馈和程序知识的丰富来源，为持续学习提供了机会，但日志的非结构化和噪声特性使得学习变得困难。

Method: UNO框架包含三个核心步骤：1) 将用户日志蒸馏为半结构化规则和偏好对；2) 使用查询-反馈驱动的聚类管理数据异质性；3) 量化模型先验知识与日志数据之间的认知差距，指导系统自适应过滤噪声反馈，并为用户日志中提取的主要经验和反思经验构建不同模块。

Result: 大量实验表明，UNO在效果和效率上都达到了最先进水平，显著优于检索增强生成(RAG)和基于记忆的基线方法。

Conclusion: UNO提供了一个有效的框架，能够从真实世界部署的用户日志中学习，解决日志非结构化和噪声问题，通过量化认知差距和自适应过滤机制，显著提升LLM系统的性能。

Abstract: Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

</details>


### [12] [Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks](https://arxiv.org/abs/2602.06526)
*Minjeong Ban,Jeonghwan Choi,Hyangsuk Min,Nicole Hee-Yeon Kim,Minseok Kim,Jae-Gil Lee,Hwanjun Song*

Main category: cs.CL

TL;DR: DREAM是一个基于多轮辩论的LLM代理相关性评估框架，通过对立立场和迭代互评提高标注准确性，仅需3.5%人工参与达到95.2%准确率。基于此构建的BRIDGE基准揭示了29,824个缺失相关块，减少了评估偏差。


<details>
  <summary>Details</summary>
Motivation: 当前信息检索评估面临挑战，因为IR基准数据集不完整，包含未标注的相关块。虽然LLM和LLM-人类混合策略减少了人工成本，但仍存在LLM过度自信和AI到人类升级无效的问题。

Method: 提出DREAM框架：基于对立初始立场和迭代互评的多轮辩论式相关性评估框架。通过基于共识的辩论，对确定案例产生更准确标注，对不确定案例实现更可靠的AI到人类升级。

Result: 达到95.2%的标注准确率，仅需3.5%人工参与。构建了BRIDGE基准，揭示了29,824个缺失相关块，减少了评估偏差，实现了更公平的检索器比较。

Conclusion: DREAM框架有效解决了IR评估中的标注不完整问题，BRIDGE基准减少了评估偏差，未解决的漏洞不仅扭曲检索器排名，还导致检索-生成错位。

Abstract: Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

</details>


### [13] [Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning](https://arxiv.org/abs/2602.06600)
*Zhuoyuan Hao,Zhuo Li,Wu Li,Fangming Liu,Min Zhang,Jing Li*

Main category: cs.CL

TL;DR: 该论文提出利用大推理模型中的"提示回响"现象（Echo of Prompt, EOP）作为计算分配机制，通过回响蒸馏监督微调和回响提示两种方法，在多个数学推理基准上取得性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时计算分配中要么注入任务无关的标记，要么使用无法解释模型自发重复现象的启发式方法。作者发现大推理模型在内部链开头倾向于重述问题（EOP现象），这可以作为一种前置的计算塑造机制，但缺乏理论解释和利用方法。

Method: 1. 理论分析：将回响移除视为基于拒绝的条件化，定义可计算的"回响似然差距"作为代理指标；2. 回响蒸馏监督微调（ED-SFT）：通过监督微调注入"先回响后推理"模式；3. 回响提示（EP）：在推理过程中重新锚定模型而无需训练；4. 通过长度和后缀控制的似然分析及层间注意力研究验证机制。

Result: 在GSM8K、MathQA、Hendrycks-MATH、AIME24和MATH-500等基准上，在相同解码设置和计算预算下，EOP方法相比基线取得了一致的性能提升。注意力分析显示EOP增加了中间层中答案对答案前缀的注意力，符合"注意力重新聚焦"机制。

Conclusion: EOP现象不仅是冗余的重复，而是大推理模型中一种可解释、可量化的计算分配机制。通过理论分析和两种利用方法，能够有效提升模型在数学推理任务上的性能，为理解模型内部推理过程提供了新视角。

Abstract: Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \emph{Echo Likelihood Gap} $Δ\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.

</details>


### [14] [Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion](https://arxiv.org/abs/2602.06724)
*Tian Lan,Felix Henry,Bin Zhu,Qianghuai Jia,Junyang Ren,Qihang Pu,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo*

Main category: cs.CL

TL;DR: TaS将信息搜索任务重构为表格补全任务，通过结构化表格管理搜索状态，显著提升长时程搜索的鲁棒性和性能


<details>
  <summary>Details</summary>
Motivation: 当前信息搜索代理在长时程探索中难以保持焦点和连贯性，因为在一个纯文本上下文中跟踪搜索状态（包括规划过程和大量搜索结果）本质上是脆弱的

Method: 提出Table-as-Search框架，将查询映射到外部数据库中的结构化表格模式，行表示搜索候选，列表示约束或所需信息，通过表格精确管理搜索状态

Result: TaS在三种基准测试中显著优于众多最先进基线，包括多智能体框架和商业系统，验证了其在长时程信息搜索中的优越鲁棒性、效率、可扩展性和灵活性

Conclusion: TaS通过结构化表格方法有效解决了信息搜索中的状态跟踪问题，统一了深度搜索、广度搜索和深度广度搜索三种任务，为长时程信息搜索提供了鲁棒的解决方案

Abstract: Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

</details>


### [15] [SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks](https://arxiv.org/abs/2602.06854)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Jialin Song,Xuekai Zhu,Chenliang Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: SEMA是一个用于训练多轮越狱攻击者的框架，通过预填充自调优和意图漂移感知的强化学习，在无需外部数据或现有策略的情况下实现最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮越狱方法面临探索复杂性和意图漂移问题，而单轮攻击只是多轮攻击的特例。需要一种更有效的多轮攻击方法来更真实地测试LLM安全性。

Method: SEMA包含两个阶段：1) 预填充自调优：通过微调自生成的非拒绝、结构良好的多轮对抗提示来稳定学习；2) 意图漂移感知的强化学习：结合意图对齐、合规风险和细节程度的奖励函数来训练攻击者保持有害目标。

Result: 在多个数据集、受害模型和越狱评估中，SEMA实现了最先进的攻击成功率（ASR），在AdvBench上对三个闭源和开源受害模型平均达到80.1% ASR@1，比现有最佳方法高出33.9%。

Conclusion: SEMA提供了一个紧凑、可复现且可跨目标迁移的框架，为LLM安全性提供了更强、更现实的压力测试，能够自动进行红队测试以暴露和定位故障模式。

Abstract: Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [Jackpot: Optimal Budgeted Rejection Sampling for Extreme Actor-Policy Mismatch Reinforcement Learning](https://arxiv.org/abs/2602.06107)
*Zhuoming Chen,Hongyi Liu,Yang Zhou,Haizhong Zheng,Beidi Chen*

Main category: cs.AI

TL;DR: Jackpot框架通过最优预算拒绝采样(OBRS)减少rollout模型与策略之间的分布不匹配，实现LLM强化学习中rollout生成与策略优化的解耦，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: LLM强化学习中rollout生成成本高昂，解耦rollout生成与策略优化可显著提升效率，但会引入严重的分布不匹配问题，导致学习不稳定。

Method: 提出Jackpot框架，采用最优预算拒绝采样(OBRS)直接减少rollout模型与演化策略之间的差异；包含原则性OBRS流程、联合更新策略和rollout模型的统一训练目标，以及基于top-k概率估计和批次级偏差校正的高效系统实现。

Result: 理论分析表明OBRS在可控接受预算下持续使rollout分布更接近目标分布；实证显示相比重要性采样基线，Jackpot显著提升训练稳定性，在Qwen3-8B-Base上训练300步(批次大小64)时达到与on-policy RL相当的性能。

Conclusion: OBRS-based alignment使LLM强化学习中rollout生成与策略优化的解耦更接近实用和有效。

Abstract: Reinforcement learning (RL) for large language models (LLMs) remains expensive, particularly because the rollout is expensive. Decoupling rollout generation from policy optimization (e.g., leveraging a more efficient model to rollout) could enable substantial efficiency gains, yet doing so introduces a severe distribution mismatch that destabilizes learning. We propose Jackpot, a framework that leverages Optimal Budget Rejection Sampling (OBRS) to directly reduce the discrepancy between the rollout model and the evolving policy. Jackpot integrates a principled OBRS procedure, a unified training objective that jointly updates the policy and rollout models, and an efficient system implementation enabled by top-$k$ probability estimation and batch-level bias correction. Our theoretical analysis shows that OBRS consistently moves the rollout distribution closer to the target distribution under a controllable acceptance budget. Empirically, \sys substantially improves training stability compared to importance-sampling baselines, achieving performance comparable to on-policy RL when training Qwen3-8B-Base for up to 300 update steps of batchsize 64. Taken together, our results show that OBRS-based alignment brings us a step closer to practical and effective decoupling of rollout generation from policy optimization for RL for LLMs.

</details>


### [17] [Large Language Model Reasoning Failures](https://arxiv.org/abs/2602.06176)
*Peiyang Song,Pengrui Han,Noah Goodman*

Main category: cs.AI

TL;DR: 该论文首次对LLM推理失败进行全面综述，提出双重分类框架：将推理分为具身与非具身（直觉与逻辑），将失败分为基础架构缺陷、领域特定限制和鲁棒性问题，并提供缓解策略与开源资源库。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM展现出强大的推理能力，但在看似简单的场景中仍存在显著的推理失败。目前缺乏对这些失败的系统性理解和分类，研究碎片化，需要统一的框架来识别、分析和缓解这些系统性弱点。

Method: 提出双重分类框架：1) 推理类型分类：具身推理 vs 非具身推理（直觉推理 vs 逻辑推理）；2) 失败类型分类：基础架构缺陷、领域特定限制、鲁棒性问题。对每种失败提供定义、现有研究分析、根本原因探讨和缓解策略。

Result: 建立了首个全面的LLM推理失败分类体系，统一了碎片化的研究，提供了系统性分析框架。创建了GitHub资源库收集相关研究工作，为该领域提供了便捷的切入点。

Conclusion: 该综述为理解LLM推理的系统性弱点提供了结构化视角，为未来研究提供了有价值的见解和指导，有助于构建更强、更可靠、更鲁棒的推理能力。

Abstract: Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

</details>


### [18] [Do LLMs Act Like Rational Agents? Measuring Belief Coherence in Probabilistic Decision Making](https://arxiv.org/abs/2602.06286)
*Khurram Yamin,Jingjing Tang,Santiago Cortes-Gomez,Amit Sharma,Eric Horvitz,Bryan Wilder*

Main category: cs.AI

TL;DR: 该论文研究LLM是否作为理性效用最大化者运作，通过诊断挑战问题测试其信念一致性和偏好稳定性，提出可证伪条件来评估LLM在医疗诊断等高风险决策中的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地被部署在高风险领域作为智能体，其决策逻辑难以解释，需要研究LLM是否具有一致的信念和稳定的偏好，以评估其在关键决策中的可靠性。

Method: 采用诊断挑战问题测试LLM行为，分析其报告概率与观察行动之间的关系，提出可证伪条件来检验报告概率是否对应任何理性智能体的真实信念，并在多个医疗诊断领域评估多个LLM。

Result: 研究结果为LLM推理与理想贝叶斯效用最大化之间的关系提供了见解，识别出LLM报告概率无法对应任何理性智能体真实信念的情况。

Conclusion: 研究结果对LLM在高风险决策中的应用具有重要意义，指出了未来改进LLM决策可靠性的方向。

Abstract: Large language models (LLMs) are increasingly deployed as agents in high-stakes domains where optimal actions depend on both uncertainty about the world and consideration of utilities of different outcomes, yet their decision logic remains difficult to interpret. We study whether LLMs are rational utility maximizers with coherent beliefs and stable preferences. We consider behaviors of models for diagnosis challenge problems. The results provide insights about the relationship of LLM inferences to ideal Bayesian utility maximization for elicited probabilities and observed actions. Our approach provides falsifiable conditions under which the reported probabilities \emph{cannot} correspond to the true beliefs of any rational agent. We apply this methodology to multiple medical diagnostic domains with evaluations across several LLMs. We discuss implications of the results and directions forward for uses of LLMs in guiding high-stakes decisions.

</details>


### [19] [Exposing Weaknesses of Large Reasoning Models through Graph Algorithm Problems](https://arxiv.org/abs/2602.06319)
*Qifan Zhang,Jianhao Ruan,Aochuan Chen,Kang Zeng,Nuo Chen,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: GrAlgoBench是一个用于评估大型推理模型的图算法基准测试，通过9个任务揭示了当前模型在长上下文推理中的两大弱点：准确率随节点数增加而急剧下降，以及过度思考现象。


<details>
  <summary>Details</summary>
Motivation: 现有数学、代码和常识推理基准测试存在局限性：缺乏长上下文评估、挑战性不足、答案难以程序化验证。需要更严格的测试平台来评估大型推理模型的能力。

Method: 设计GrAlgoBench基准测试，包含图算法问题作为评估工具。图算法问题特别适合测试推理能力：需要长上下文推理、难度可精细控制、支持标准化程序化评估。通过9个任务进行系统性实验。

Result: 发现当前大型推理模型的两大弱点：1) 准确率随上下文长度增加而急剧下降，当图超过120个节点时准确率低于50%，主要由执行错误、弱记忆和冗余推理导致；2) 存在过度思考现象，主要由大量无效的自我验证引起，增加了推理轨迹但未提高正确性。

Conclusion: GrAlgoBench通过暴露大型推理模型的局限性，确立了图算法问题作为严格、多维且实际相关的测试平台，有助于推进大型推理模型推理能力的研究。

Abstract: Large Reasoning Models (LRMs) have advanced rapidly; however, existing benchmarks in mathematics, code, and common-sense reasoning remain limited. They lack long-context evaluation, offer insufficient challenge, and provide answers that are difficult to verify programmatically. We introduce GrAlgoBench, a benchmark designed to evaluate LRMs through graph algorithm problems. Such problems are particularly well suited for probing reasoning abilities: they demand long-context reasoning, allow fine-grained control of difficulty levels, and enable standardized, programmatic evaluation. Across nine tasks, our systematic experiments reveal two major weaknesses of current LRMs. First, accuracy deteriorates sharply as context length increases, falling below 50% once graphs exceed 120 nodes. This degradation is driven by frequent execution errors, weak memory, and redundant reasoning. Second, LRMs suffer from an over-thinking phenomenon, primarily caused by extensive yet largely ineffective self-verification, which inflates reasoning traces without improving correctness. By exposing these limitations, GrAlgoBench establishes graph algorithm problems as a rigorous, multidimensional, and practically relevant testbed for advancing the study of reasoning in LRMs. Code is available at https://github.com/Bklight999/GrAlgoBench.

</details>


### [20] [Difficulty-Estimated Policy Optimization](https://arxiv.org/abs/2602.06375)
*Yu Zhao,Fan Jiang,Tianle Liu,Bo Zeng,Yu Liu,Longyue Wang,Weihua Luo*

Main category: cs.AI

TL;DR: 提出DEPO框架，通过在线难度估计器动态筛选训练数据，优先处理高学习潜力样本，减少2倍计算成本而不影响性能


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在问题过于简单或复杂时会出现梯度信号衰减，导致收敛不稳定；而DAPO等变体虽然缓解梯度消失，但无法解决低效用样本带来的巨大计算开销

Method: 提出Difficulty-Estimated Policy Optimization (DEPO)框架，集成在线难度估计器，在rollout阶段前动态评估和过滤训练数据，优先分配计算资源给高学习潜力样本

Result: DEPO实现了高达2倍的rollout成本降低，且不损害模型性能，显著降低了训练高性能推理模型的计算门槛

Conclusion: DEPO为推理对齐提供了更高效和鲁棒的优化框架，为推理模型的规模化训练提供了更可持续的路径

Abstract: Recent advancements in Large Reasoning Models (LRMs), exemplified by DeepSeek-R1, have underscored the potential of scaling inference-time compute through Group Relative Policy Optimization (GRPO). However, GRPO frequently suffers from gradient signal attenuation when encountering problems that are either too trivial or overly complex. In these scenarios, the disappearance of inter-group advantages makes the gradient signal susceptible to noise, thereby jeopardizing convergence stability. While variants like DAPO attempt to rectify gradient vanishing, they do not alleviate the substantial computational overhead incurred by exhaustive rollouts on low-utility samples. In this paper, we propose Difficulty-Estimated Policy Optimization (DEPO), a novel framework designed to optimize the efficiency and robustness of reasoning alignment. DEPO integrates an online Difficulty Estimator that dynamically assesses and filters training data before the rollout phase. This mechanism ensures that computational resources are prioritized for samples with high learning potential. Empirical results demonstrate that DEPO achieves up to a 2x reduction in rollout costs without compromising model performance. Our approach significantly lowers the computational barrier for training high-performance reasoning models, offering a more sustainable path for reasoning scaling. Code and data will be released upon acceptance.

</details>


### [21] [AgentCPM-Explore: Realizing Long-Horizon Deep Exploration for Edge-Scale Agents](https://arxiv.org/abs/2602.06485)
*Haotian Chen,Xin Cong,Shengda Fan,Yuyang Fu,Ziqin Gong,Yaxi Lu,Yishan Li,Boye Niu,Chengjun Pan,Zijun Song,Huadong Wang,Yesai Wu,Yueying Wu,Zihao Xie,Yukun Yan,Zhong Zhang,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文首次系统研究了4B参数规模的智能体模型训练，提出了AgentCPM-Explore框架，解决了边缘规模模型的三个瓶颈问题，在多个基准测试中超越了更大规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体系统过度依赖大规模模型，而边缘规模模型（4B参数级别）的能力尚未得到充分探索。本文旨在系统研究如何训练高性能的边缘规模智能体模型。

Method: 提出AgentCPM-Explore框架，包含三个关键技术：参数空间模型融合（解决SFT灾难性遗忘）、奖励信号去噪（解决RL噪声敏感）、上下文信息精炼（解决长上下文推理退化）。

Result: AgentCPM-Explore在4B类模型中达到SOTA性能，在四个基准测试中匹配或超越8B类SOTA模型，在五个基准测试中甚至超越了Claude-4.5-Sonnet或DeepSeek-v3.2等更大规模模型。在GAIA文本任务上达到97.09%准确率（pass@64）。

Conclusion: 边缘规模模型的瓶颈不在于其固有能力上限，而在于推理稳定性。通过提出的训练框架，可以充分释放边缘规模模型被低估的潜力。

Abstract: While Large Language Model (LLM)-based agents have shown remarkable potential for solving complex tasks, existing systems remain heavily reliant on large-scale models, leaving the capabilities of edge-scale models largely underexplored. In this paper, we present the first systematic study on training agentic models at the 4B-parameter scale. We identify three primary bottlenecks hindering the performance of edge-scale models: catastrophic forgetting during Supervised Fine-Tuning (SFT), sensitivity to reward signal noise during Reinforcement Learning (RL), and reasoning degradation caused by redundant information in long-context scenarios. To address the issues, we propose AgentCPM-Explore, a compact 4B agent model with high knowledge density and strong exploration capability. We introduce a holistic training framework featuring parameter-space model fusion, reward signal denoising, and contextual information refinement. Through deep exploration, AgentCPM-Explore achieves state-of-the-art (SOTA) performance among 4B-class models, matches or surpasses 8B-class SOTA models on four benchmarks, and even outperforms larger-scale models such as Claude-4.5-Sonnet or DeepSeek-v3.2 in five benchmarks. Notably, AgentCPM-Explore achieves 97.09% accuracy on GAIA text-based tasks under pass@64. These results provide compelling evidence that the bottleneck for edge-scale models is not their inherent capability ceiling, but rather their inference stability. Based on our well-established training framework, AgentCPM-Explore effectively unlocks the significant, yet previously underestimated, potential of edge-scale models.

</details>


### [22] [JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks](https://arxiv.org/abs/2602.06486)
*Lanbo Lin,Jiayao Liu,Tianyuan Yang,Li Cai,Yuanwu Xu,Lei Wei,Sicong Xie,Guannan Zhang*

Main category: cs.AI

TL;DR: JADE是一个两层的评估框架，用于评估开放端专业任务上的智能体AI。第一层将专家知识编码为预定义的评估技能，提供稳定的评估标准；第二层进行报告特定的、基于声明的评估，灵活评估多样化的推理策略，并通过证据依赖门控来使基于被反驳声明的结论失效。


<details>
  <summary>Details</summary>
Motivation: 评估开放端专业任务上的智能体AI面临严谨性与灵活性之间的基本困境。静态评估标准提供严谨、可重复的评估，但无法适应多样化的有效响应策略；而基于LLM的评估方法虽然能适应个体响应，却存在不稳定性和偏见问题。

Method: JADE采用两层评估框架：第一层将专家知识编码为预定义的评估技能，提供稳定的评估标准；第二层进行报告特定的、基于声明的评估，灵活评估多样化的推理策略，并通过证据依赖门控机制来使基于被反驳声明的结论失效。

Result: 在BizBench上的实验表明，JADE提高了评估稳定性，并揭示了整体性LLM评估器遗漏的关键智能体失败模式。该框架与专家编写的评估标准高度一致，并能有效迁移到医疗领域基准测试，验证了其在多个专业领域的有效性。

Conclusion: JADE通过结合专家知识和动态声明级评估，解决了开放端专业任务评估中严谨性与灵活性之间的困境，提供了一种稳定、灵活且可迁移的评估框架。

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

</details>


### [23] [Progress Constraints for Reinforcement Learning in Behavior Trees](https://arxiv.org/abs/2602.06525)
*Finn Rietz,Mart Kartašev,Johannes A. Stork,Petter Ögren*

Main category: cs.AI

TL;DR: 该论文提出了一种结合行为树与强化学习的新方法，通过进度约束机制防止控制器间的相互干扰，提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 行为树提供结构化决策框架但需要手动设计控制器，强化学习能学习最优控制器但面临稀疏奖励、安全探索等挑战。两者结合有互补优势，但简单集成可能导致控制器相互抵消，降低整体性能。

Method: 提出进度约束机制，利用可行性估计器基于行为树收敛理论来约束允许的动作集，防止控制器相互干扰。

Result: 在2D概念验证和高保真仓库环境中的实验表明，相比之前的行为树-强化学习集成方法，该方法在性能、样本效率和约束满足方面都有显著提升。

Conclusion: 进度约束机制有效解决了行为树与强化学习集成中的控制器冲突问题，实现了两者的优势互补，为结构化决策与学习方法的结合提供了新思路。

Abstract: Behavior Trees (BTs) provide a structured and reactive framework for decision-making, commonly used to switch between sub-controllers based on environmental conditions. Reinforcement Learning (RL), on the other hand, can learn near-optimal controllers but sometimes struggles with sparse rewards, safe exploration, and long-horizon credit assignment. Combining BTs with RL has the potential for mutual benefit: a BT design encodes structured domain knowledge that can simplify RL training, while RL enables automatic learning of the controllers within BTs. However, naive integration of BTs and RL can lead to some controllers counteracting other controllers, possibly undoing previously achieved subgoals, thereby degrading the overall performance. To address this, we propose progress constraints, a novel mechanism where feasibility estimators constrain the allowed action set based on theoretical BT convergence results. Empirical evaluations in a 2D proof-of-concept and a high-fidelity warehouse environment demonstrate improved performance, sample efficiency, and constraint satisfaction, compared to prior methods of BT-RL integration.

</details>


### [24] [AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research](https://arxiv.org/abs/2602.06540)
*Yishan Li,Wentong Chen,Yukun Yan,Mingwei Li,Sen Mei,Xiaorong Wang,Kunpeng Liu,Xin Cong,Shuo Wang,Zhong Zhang,Yaxi Lu,Zhenghao Liu,Yankai Lin,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-Report：一个轻量级本地研究报告生成系统，使用8B参数模型，通过写作即推理策略(WARP)动态修订大纲，在多个基准上超越闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究报告生成系统依赖闭源或在线大模型，存在部署障碍、安全和隐私问题。需要开发轻量级本地解决方案，解决大纲构建依赖强推理能力的问题。

Method: 提出写作即推理策略(WARP)，让模型在报告生成过程中动态修订大纲。采用证据驱动草拟和推理驱动深化交替进行，支持信息获取、知识精炼和迭代大纲演进。使用多阶段智能体训练策略：冷启动、原子技能强化学习和整体管道强化学习。

Result: 在DeepResearch Bench、DeepConsult和DeepResearch Gym基准测试中，AgentCPM-Report超越了领先的闭源系统，在Insight指标上获得显著提升。

Conclusion: AgentCPM-Report展示了轻量级本地模型通过适当的框架和训练策略，能够实现高质量的深度研究报告生成，减少对闭源大模型的依赖。

Abstract: Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

</details>


### [25] [SeeUPO: Sequence-Level Agentic-RL with Convergence Guarantees](https://arxiv.org/abs/2602.06554)
*Tianyi Hu,Qingxu Fu,Yanxi Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: 提出SeeUPO算法，解决现有RL算法在多轮交互中缺乏收敛保证的问题，通过序列级顺序更新策略优化实现单调改进和全局最优收敛。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的AI代理训练中，主流RL算法在多轮交互场景下缺乏经过验证的收敛保证，导致训练不稳定和无法收敛到最优策略。

Method: 提出SeeUPO算法：将多轮交互建模为顺序执行的多臂赌博机问题，采用反向执行顺序的逐轮顺序策略更新，通过反向归纳确保单调改进和全局最优收敛。

Result: 在AppWorld和BFCL v4基准测试中，SeeUPO相比现有骨干算法取得显著提升：Qwen3-14B上相对增益43.3%-54.6%，Qwen2.5-14B上24.1%-41.9%，同时具有更优的训练稳定性。

Conclusion: SeeUPO解决了多轮交互中RL算法的收敛保证问题，提供了一种无critic且具有收敛保证的有效方法，显著提升了训练稳定性和性能。

Abstract: Reinforcement learning (RL) has emerged as the predominant paradigm for training large language model (LLM)-based AI agents. However, existing backbone RL algorithms lack verified convergence guarantees in agentic scenarios, especially in multi-turn settings, which can lead to training instability and failure to converge to optimal policies.
  In this paper, we systematically analyze how different combinations of policy update mechanisms and advantage estimation methods affect convergence properties in single/multi-turn scenarios. We find that REINFORCE with Group Relative Advantage Estimation (GRAE) can converge to the globally optimal under undiscounted conditions, but the combination of PPO & GRAE breaks PPO's original monotonic improvement property. Furthermore, we demonstrate that mainstream backbone RL algorithms cannot simultaneously achieve both critic-free and convergence guarantees in multi-turn scenarios.
  To address this, we propose SeeUPO (Sequence-level Sequential Update Policy Optimization), a critic-free approach with convergence guarantees for multi-turn interactions. SeeUPO models multi-turn interaction as sequentially executed multi-agent bandit problems. Through turn-by-turn sequential policy updates in reverse execution order, it ensures monotonic improvement and convergence to global optimal solution via backward induction.
  Experiments on AppWorld and BFCL v4 demonstrate SeeUPO's substantial improvements over existing backbone algorithms: relative gains of 43.3%-54.6% on Qwen3-14B and 24.1%-41.9% on Qwen2.5-14B (averaged across benchmarks), along with superior training stability.

</details>


### [26] [Semantically Labelled Automata for Multi-Task Reinforcement Learning with LTL Instructions](https://arxiv.org/abs/2602.06746)
*Alessandro Abate,Giuseppe De Giacomo,Mathias Jackermeier,Jan Kretínský,Maximilian Prokop,Christoph Weinhuber*

Main category: cs.AI

TL;DR: 提出一种基于语义LTL到自动机转换的多任务强化学习方法，利用结构化任务嵌入实现通用策略学习


<details>
  <summary>Details</summary>
Motivation: 研究多任务强化学习，目标是学习一个能够泛化到任意（可能未见）任务的通用策略。使用线性时序逻辑（LTL）公式作为任务规范，这是形式化方法中常用的系统属性规范方式，最近在RL中成功应用。

Method: 提出新颖的任务嵌入技术，利用新一代语义LTL到自动机转换（最初为时序综合开发）。生成的语义标记自动机在每个状态包含丰富的结构化信息，能够：(i) 高效地在线计算自动机，(ii) 提取用于条件化策略的表达性任务嵌入，(iii) 自然支持完整的LTL。

Result: 在多个领域的实验结果表明，该方法实现了最先进的性能，并且能够扩展到现有方法失败的复杂规范。

Conclusion: 基于语义LTL到自动机转换的任务嵌入技术为多任务强化学习提供了有效的解决方案，特别是在处理复杂LTL规范时表现出优越的性能和可扩展性。

Abstract: We study multi-task reinforcement learning (RL), a setting in which an agent learns a single, universal policy capable of generalising to arbitrary, possibly unseen tasks. We consider tasks specified as linear temporal logic (LTL) formulae, which are commonly used in formal methods to specify properties of systems, and have recently been successfully adopted in RL. In this setting, we present a novel task embedding technique leveraging a new generation of semantic LTL-to-automata translations, originally developed for temporal synthesis. The resulting semantically labelled automata contain rich, structured information in each state that allow us to (i) compute the automaton efficiently on-the-fly, (ii) extract expressive task embeddings used to condition the policy, and (iii) naturally support full LTL. Experimental results in a variety of domains demonstrate that our approach achieves state-of-the-art performance and is able to scale to complex specifications where existing methods fail.

</details>


### [27] [Wild Guesses and Mild Guesses in Active Concept Learning](https://arxiv.org/abs/2602.06818)
*Anirudh Chari,Neil Pattanaik*

Main category: cs.AI

TL;DR: 该论文研究了主动概念学习中信息获取与假设稳定性的权衡，比较了理性主动学习器（最大化期望信息增益）与人类式积极测试策略（查询当前最佳假设预测为正的实例）在不同概念学习任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 人类概念学习通常是主动的：学习者选择查询哪些实例来减少对底层规则或类别的不确定性。主动概念学习必须在查询的信息量与生成和评分假设的学习器稳定性之间取得平衡。本研究旨在探索这种权衡关系。

Method: 采用神经符号贝叶斯学习器，其假设是由大型语言模型提出的可执行程序，并通过贝叶斯更新重新加权。比较两种查询策略：理性主动学习器（选择查询以最大化近似期望信息增益）和人类式积极测试策略（查询当前最佳假设预测为正的实例）。在经典数字游戏的概念学习任务中进行实验。

Result: 在需要证伪的复杂概念（如复合规则或包含例外的规则）上，期望信息增益策略有效，但在简单概念上表现不佳。这种失败源于期望信息增益策略与LLM提议分布之间的支持不匹配：高度诊断性的边界查询将后验推向生成器产生无效或过于具体程序的区域，导致粒子近似中的支持不匹配陷阱。积极测试策略虽然信息次优，但倾向于通过选择"安全"查询来维持提议有效性，从而在简单规则上实现更快收敛。

Conclusion: 研究结果表明，"确认偏误"可能不是认知错误，而是在人类思维特有的稀疏、开放式假设空间中维持可处理推理的理性适应机制。

Abstract: Human concept learning is typically active: learners choose which instances to query or test in order to reduce uncertainty about an underlying rule or category. Active concept learning must balance informativeness of queries against the stability of the learner that generates and scores hypotheses. We study this trade-off in a neuro-symbolic Bayesian learner whose hypotheses are executable programs proposed by a large language model (LLM) and reweighted by Bayesian updating. We compare a Rational Active Learner that selects queries to maximize approximate expected information gain (EIG) and the human-like Positive Test Strategy (PTS) that queries instances predicted to be positive under the current best hypothesis. Across concept-learning tasks in the classic Number Game, EIG is effective when falsification is necessary (e.g., compound or exception-laden rules), but underperforms on simple concepts. We trace this failure to a support mismatch between the EIG policy and the LLM proposal distribution: highly diagnostic boundary queries drive the posterior toward regions where the generator produces invalid or overly specific programs, yielding a support-mismatch trap in the particle approximation. PTS is information-suboptimal but tends to maintain proposal validity by selecting "safe" queries, leading to faster convergence on simple rules. Our results suggest that "confirmation bias" may not be a cognitive error, but rather a rational adaptation for maintaining tractable inference in the sparse, open-ended hypothesis spaces characteristic of human thought.

</details>


### [28] [ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training](https://arxiv.org/abs/2602.06820)
*Dunwei Tu,Hongyan Hao,Hansi Yang,Yihao Chen,Yi-Kai Zhang,Zhikang Xia,Yu Yang,Yueqing Sun,Xingchen Liu,Furao Shen,Qi Gu,Hui Su,Xunliang Cai*

Main category: cs.AI

TL;DR: ScaleEnv是一个从零开始构建完全交互式环境和可验证任务的框架，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性，显著提升智能体在未见多轮工具使用基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 训练能够适应多样化场景的通用智能体需要交互式环境进行自我探索，但现有交互环境严重不足，且现有合成方法在环境多样性和可扩展性方面存在显著限制。

Method: ScaleEnv框架从零开始构建完全交互式环境和可验证任务，通过程序化测试确保环境可靠性，通过工具依赖图扩展和可执行动作验证保证任务完整性和可解性。

Result: 在未见的多轮工具使用基准（如τ²-Bench和VitaBench）上表现出显著性能提升，展示了强大的泛化能力，并实证证明了增加领域数量对模型泛化性能的重要性。

Conclusion: 扩展环境多样性对于鲁棒的智能体学习至关重要，ScaleEnv为解决交互环境稀缺问题提供了有效框架。

Abstract: Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as $τ^2$-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.

</details>


### [29] [LLM Active Alignment: A Nash Equilibrium Perspective](https://arxiv.org/abs/2602.06836)
*Tonghan Wang,Yuqi Pan,Xinyi Yang,Yanchen Jiang,Milind Tambe,David C. Parkes*

Main category: cs.AI

TL;DR: 提出一个博弈论框架，通过纳什均衡分析来预测和引导大型语言模型群体的行为，避免开放文本空间中的均衡计算难题，将代理行为建模为人类子群体的混合选择。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型群体在社交媒体等场景中可能出现政治排斥等病理现象，需要理论框架来预测和引导多智能体LLM的动态行为，实现社会期望的结果。

Method: 采用博弈论框架，将每个代理的行为建模为人类子群体的混合选择，代理主动战略性地选择与哪些群体对齐，采用标准凹效用假设推导闭式纳什均衡特征，作为现有对齐流程（如RLHF）之上的主动对齐层。

Result: 在社交媒体场景中，LLM群体（特别是基于推理的模型）可能表现出政治排斥现象，即某些子群体被所有LLM代理忽略，而该方法可以避免这种病理现象。

Conclusion: 该方法为跨领域调节多智能体LLM动态提供了有前景的途径，能够通过明确的、可操作的指导将对齐目标转向社会期望的结果。

Abstract: We develop a game-theoretic framework for predicting and steering the behavior of populations of large language models (LLMs) through Nash equilibrium (NE) analysis. To avoid the intractability of equilibrium computation in open-ended text spaces, we model each agent's action as a mixture over human subpopulations. Agents choose actively and strategically which groups to align with, yielding an interpretable and behaviorally substantive policy class. We derive closed-form NE characterizations, adopting standard concave-utility assumptions to enable analytical system-level predictions and give explicit, actionable guidance for shifting alignment targets toward socially desirable outcomes. The method functions as an active alignment layer on top of existing alignment pipelines such as RLHF. In a social-media setting, we show that a population of LLMs, especially reasoning-based models, may exhibit political exclusion, pathologies where some subpopulations are ignored by all LLM agents, which can be avoided by our method, illustrating the promise of applying the method to regulate multi-agent LLM dynamics across domains.

</details>


### [30] [From Features to Actions: Explainability in Traditional and Agentic AI Systems](https://arxiv.org/abs/2602.06841)
*Sindhuja Chaduvula,Jessee Ho,Kina Kim,Aravind Narayanan,Mahshid Alinoori,Muskan Garg,Dhanesh Ramachandram,Shaina Raza*

Main category: cs.AI

TL;DR: 该研究比较了静态分类任务中的归因解释方法与智能体基准中的轨迹诊断方法，发现归因方法在静态设置中有效，但无法可靠诊断智能体轨迹中的执行级故障，而基于轨迹的诊断能有效定位行为故障。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，智能体AI系统展现出多步轨迹的行为，成功与失败由决策序列决定。然而，为静态预测设计的解释方法如何应用于行为随时间演化的智能体设置尚不清楚，需要弥合静态与智能体可解释性之间的差距。

Method: 通过经验比较静态分类任务中的归因解释方法与智能体基准（TAU-bench Airline和AssistantBench）中的轨迹诊断方法。具体比较了归因解释与基于轨迹的诊断在两种设置下的表现。

Result: 归因方法在静态设置中实现稳定的特征排名（Spearman ρ=0.86），但无法可靠诊断智能体轨迹中的执行级故障。基于轨迹的评估能一致定位行为故障，发现状态跟踪不一致在失败运行中普遍2.7倍，并将成功概率降低49%。

Conclusion: 研究结果表明，在评估和诊断自主AI行为时，需要向智能体系统的轨迹级可解释性转变，因为静态解释方法无法有效处理智能体环境中随时间演化的行为。

Abstract: Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman $ρ= 0.86$), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7$\times$ more prevalent in failed runs and reduces success probability by 49\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.
  Resources:
  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework

</details>


### [31] [AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents](https://arxiv.org/abs/2602.06855)
*Alisia Lupidi,Bhavul Gauri,Thomas Simon Foster,Bassel Al Omari,Despoina Magka,Alberto Pepe,Alexis Audran-Reiss,Muna Aghamelu,Nicolas Baldwin,Lucia Cipolina-Kun,Jean-Christophe Gagnon-Audet,Chee Hau Leow,Sandra Lefdal,Hossam Mossalam,Abhinav Moudgil,Saba Nazir,Emanuel Tewolde,Isabel Urrego,Jordi Armengol Estape,Amar Budhiraja,Gaurav Chaurasia,Abhishek Charnalia,Derek Dunfield,Karen Hambardzumyan,Daniel Izcovich,Martin Josifoski,Ishita Mediratta,Kelvin Niu,Parth Pathak,Michael Shvartsman,Edan Toledo,Anton Protopopov,Roberta Raileanu,Alexander Miller,Tatiana Shavrina,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AIRS-Bench是一个包含20个任务的AI研究科学基准，评估LLM代理在整个研究生命周期中的能力，包括想法生成、实验分析和迭代优化。基准显示代理在4个任务中超过人类SOTA，但在16个任务中未能匹配，表明仍有巨大改进空间。


<details>
  <summary>Details</summary>
Motivation: LLM代理在推动科学研究方面具有巨大潜力，但缺乏全面的评估基准来测试代理在整个研究生命周期中的能力。现有基准通常只关注特定方面，无法全面评估代理的科学研究能力。

Method: 从最先进的机器学习论文中选取20个任务，涵盖语言建模、数学、生物信息学和时间序列预测等多个领域。任务格式灵活，便于集成新任务和比较不同代理框架。使用前沿模型配合顺序和并行框架建立基线。

Result: 代理在4个任务中超过了人类SOTA，但在16个任务中未能匹配。即使代理超过人类基准，也未能达到底层任务的理论性能上限。这表明AIRS-Bench远未饱和，有巨大改进空间。

Conclusion: AIRS-Bench是一个有效的科学研究代理评估基准，展示了当前代理能力的局限性，为自主科学研究的发展提供了重要的评估工具和改进方向。

Abstract: LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.

</details>


### [32] [Agentic Uncertainty Reveals Agentic Overconfidence](https://arxiv.org/abs/2602.06948)
*Jean Kaddour,Srijan Patel,Gbètondji Dovonon,Leo Richter,Pasquale Minervini,Matt J. Kusner*

Main category: cs.AI

TL;DR: AI agents普遍过度自信，成功率仅22%时却预测77%成功。研究发现执行前评估比执行后评估有更好区分能力，对抗性提示（重构为bug查找）实现最佳校准。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理能否准确预测自身任务成功率，探索代理不确定性（agentic uncertainty）问题，了解代理在执行任务前、中、后的自我评估能力。

Method: 通过在执行前、执行中和执行后三个阶段获取代理的成功概率估计，比较不同时间点的预测准确性。使用对抗性提示方法，将评估重构为bug查找任务。

Result: 发现代理普遍过度自信：实际成功率仅22%的代理预测成功率高达77%。执行前评估比标准执行后评估有更好的区分能力（但差异不总是显著）。对抗性提示方法实现了最佳校准效果。

Conclusion: AI代理存在系统性过度自信问题，执行前评估可能提供更有价值的预测信息，对抗性提示是改善代理自我评估校准的有效方法。

Abstract: Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [33] [Anthropic releases Opus 4.6 with new ‘agent teams'](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F02%2F05%2Fanthropic-releases-opus-4-6-with-new-agent-teams%2F%3Futm_source=tldrnewsletter/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/SLNdu6i680v_bm32E0qNBvi-mNQ-bAMeNyVcjMZ5Dmg=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic发布Opus 4.6，引入"智能体团队"功能，可将大任务分解为子任务并行处理，并支持100万token上下文窗口


<details>
  <summary>Details</summary>
Motivation: 解决单个智能体处理复杂任务时的效率瓶颈，通过团队协作提高任务处理速度和能力

Method: 采用智能体团队架构，将大任务分解为子任务分配给不同智能体并行处理，同时扩展上下文窗口至100万token以处理更大文档

Result: Opus 4.6具备智能体团队协作能力，可并行处理复杂任务，提高效率，目前已在API用户和订阅者中提供研究预览

Conclusion: 智能体团队架构是提升AI系统处理复杂任务能力的重要方向，通过并行协作和扩展上下文窗口实现更高效的任务处理

Abstract: Anthropic releases Opus 4.6 with new ‘agent teams' (2 minute read) Anthropic's latest version of Opus features Agent Teams, teams of agents that can split larger tasks into segmented jobs. It also comes with a one-million-token context window, allowing for the processing of larger documents. The segmenting of agent responsibilities allows them to coordinate work in parallel to complete tasks faster. Agent Teams is currently available in a research preview for API users and subscribers.

</details>


### [34] [Introducing GPT-5.3-Codex](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGR9Zjz/1/0100019c32b223d7-6efc1d8c-8296-4647-90b5-f8fd7da4744f-000000/6NFDTzSS3qCG_vGYWoI-zZxk2o4Q2eRTTfwvuKzakrw=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GPT-5.3-Codex是OpenAI最新的智能编码模型，相比GPT-5.2-Codex在编码性能、推理和专业知识方面都有提升，速度提高25%，能处理涉及研究、工具使用和复杂执行的长时任务，支持开发者实时交互而不丢失上下文。


<details>
  <summary>Details</summary>
Motivation: OpenAI旨在开发更强大的智能编码代理，提升编码模型的性能、推理能力和专业知识，同时提高处理效率，使开发者能够更好地与模型协作完成复杂编码任务。

Method: 基于GPT-5.2-Codex进行改进，提升编码性能、推理能力和专业知识，优化模型架构实现25%的速度提升，增强长时任务处理能力，支持研究、工具使用和复杂执行，并实现开发者实时交互而不丢失上下文的功能。

Result: GPT-5.3-Codex成为OpenAI迄今为止最强大的智能编码模型，在编码性能、推理能力和专业知识方面都超越了GPT-5.2-Codex，速度提升25%，能够有效处理复杂的长时编码任务。

Conclusion: GPT-5.3-Codex代表了智能编码代理技术的重要进展，为开发者提供了更强大、更高效的编码助手，能够处理更复杂的编程任务并支持更好的协作体验。

Abstract: Introducing GPT-5.3-Codex (12 minute read) GPT-5.3-Codex is OpenAI's most capable agentic coding model to date. It advances both the frontier coding performance of GPT-5.2-Codex and the reasoning and professional knowledge capabilities of GPT-5.2 while being 25% faster. The model can take on long-running tasks that involve research, tool use, and complex execution. Developers can steer and interact with the model while it's working without losing context.

</details>


### [35] [The OpenClaw Moment: What the Viral AI Agent Means for Business Owners](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fpulse%2Fopenclaw-moment-what-viral-ai-agent-means-business-owners-spencer-vgnec%2F%3Futm_source=tldrmarketing/1/0100019c32d88cba-44c244b9-17c0-4c83-bfaa-cd0d32a594a1-000000/xbyU2s05pHIddKygfsOe2bNYjcwxPCuEQQp0_NbhyN4=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenClaw是一个本地运行的AI代理工具，能够执行读取邮件、联系供应商、进行购买等实际任务，但存在严重安全风险，包括明文凭证存储、远程代码执行漏洞、341个恶意插件和506个提示注入问题，目前不适合商业使用。


<details>
  <summary>Details</summary>
Motivation: 分析OpenClaw等能够为用户执行实际操作的AI代理工具的现状，特别关注其安全风险，为商业所有者提供关于这类工具当前是否适合商业应用的评估。

Method: 通过对OpenClaw进行安全审查，识别和分析其存在的安全漏洞和风险，包括凭证存储方式、代码执行漏洞、插件安全性以及提示注入问题。

Result: 安全审查发现了OpenClaw存在严重安全缺陷：1) 明文存储凭证 2) 远程代码执行漏洞 3) 341个恶意插件 4) 506个提示注入漏洞。这些风险使其目前不适合商业环境使用。

Conclusion: 虽然能够执行实际操作的AI代理工具正在快速发展，但像OpenClaw这样的开源工具目前存在严重安全风险，商业所有者应谨慎对待，等待更安全、更成熟的解决方案。

Abstract: The OpenClaw Moment: What the Viral AI Agent Means for Business Owners (5 minute read) AI agents that can take real actions for users are arriving fast, but current open source tools like OpenClaw are powerful and unsafe for business use today. OpenClaw runs on a local computer and can read email, contact vendors, and make purchases. Security reviews have discovered major risks, including plaintext credential storage, a remote code execution flaw, 341 malicious plugins, and 506 prompt injecti...

</details>


### [36] [94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 调查显示94%的开发者和产品负责人会因更强的Agentic AI功能而更换SaaS供应商，Agentic AI正成为购买决策而非单纯功能特性


<details>
  <summary>Details</summary>
Motivation: 了解Agentic AI在SaaS采购决策中的实际影响力和重要性，验证其是否已成为企业技术栈选择的关键因素

Method: 通过对1000多名开发者和产品负责人进行问卷调查，收集关于Agentic AI在SaaS采购决策中的态度和行为数据

Result: 94%的受访者表示会因更强的Agentic AI功能而更换供应商，85%认为Agentic AI将在三年内成为基本要求，67%关注实际生产而非炒作

Conclusion: Agentic AI已从功能特性演变为关键的购买决策因素，正在现代技术栈中引发供应商切换，预计三年内将成为行业标准

Abstract: 94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...

</details>


### [37] [The timeline:](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 调查显示94%的开发者会因更强的智能体AI功能而更换SaaS供应商，智能体AI正成为购买决策而非功能特性，预计三年内将成为标配


<details>
  <summary>Details</summary>
Motivation: 研究智能体AI在SaaS采购决策中的重要性，了解开发者对智能体AI功能的实际需求与期望

Method: 通过对1000多名开发者和产品负责人进行问卷调查，分析智能体AI在技术栈中的采用趋势和影响

Result: 94%的受访者会因更强的智能体AI功能而更换供应商，85%认为三年内智能体AI将成为标配，67%关注实际生产而非炒作

Conclusion: 智能体AI正从功能特性转变为关键的购买决策因素，供应商需重视实际生产价值的智能体AI功能开发

Abstract: 94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...

</details>


### [38] [Production over hype:](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 调查显示94%的开发者和产品负责人会因更强的智能AI功能而更换SaaS供应商，智能AI正成为购买决策而非功能特性


<details>
  <summary>Details</summary>
Motivation: 了解智能AI在SaaS采购决策中的重要性，以及开发者和产品负责人对智能AI功能的实际需求和期望

Method: 通过对1000多名开发者和产品负责人进行调查，收集关于智能AI功能在SaaS采购决策中的影响数据

Result: 94%的受访者会因更强的智能AI功能而更换SaaS供应商，85%认为智能AI将在三年内成为基本要求，67%更关注实际生产而非炒作

Conclusion: 智能AI正从功能特性转变为关键的购买决策因素，将在未来三年内成为SaaS产品的标配要求

Abstract: 94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...

</details>


### [39] [The bottlenecks:](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nylas.com%2Fagentic-ai-report-2026%2F%3Futm_source=tldr%26utm_medium=sponsoredemail%26utm_campaign=FY26Q1-TLDR-AgenticAI%26utm_content=/2/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/BjPINxZg0XDt4at2J16vz3t2a1EnBFzg0w2zLoLvWBU=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 调查显示94%的开发者和产品负责人会因更强的智能AI功能而更换SaaS供应商，智能AI正成为购买决策而非单纯功能，85%认为三年内将成为标配


<details>
  <summary>Details</summary>
Motivation: 研究智能AI在SaaS采购决策中的重要性，了解开发者和产品负责人对智能AI功能的实际需求和预期

Method: 通过对1000多名开发者和产品负责人进行调查，分析智能AI在技术栈中的影响和采用趋势

Result: 94%的受访者会考虑为更强的智能AI功能更换供应商，智能AI已成为现代技术栈中的切换事件，85%认为三年内将成为标配，67%关注实际生产而非炒作

Conclusion: 智能AI正从附加功能转变为关键的购买决策因素，供应商需要重视智能AI功能的开发以保持竞争力

Abstract: 94% of developers and product leaders would consider switching SaaS vendors for stronger agentic AI functionality (Sponsor) Agentic AI is becoming a buying decision, not a feature. In a survey of 1,000+ developers and product leaders, we found that agentic AI is already acting as a switching event across the modern tech stack.The Developer's Guide to Agentic AI 2026 report breaks down: The timeline: Why 85% say agentic AI will be table stakes within three years Production over hype: How 67% o...

</details>


### [40] [Self-Optimizing Football Chatbot Guided by Domain Experts on Databricks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.databricks.com%2Fblog%2Fself-optimizing-football-chatbot-guided-domain-experts-databricks%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/PdV6ukSJbPVM8Aqno989bJkBOpYwkHp-qa1Kwm5-42o=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Databricks提出了一种持续优化AI代理的架构，通过MLflow的align()和optimize_prompts()函数集成领域专家反馈，实现自动提示优化和性能提升


<details>
  <summary>Details</summary>
Motivation: 构建能够持续改进的AI代理系统，将人类领域专家的知识直接编码到代理中，提供情境感知的洞察，特别是在美式足球防守协调员助手这样的专业领域

Method: 使用Databricks架构，通过MLflow的align()函数对齐领域专家反馈，optimize_prompts()函数自动优化提示，将人类专业知识直接编码到AI代理中

Result: 开发了美式足球防守协调员助手示例，展示了系统能够通过专家反馈持续改进，提供情境感知的洞察和增强的性能

Conclusion: 该架构为AI代理的持续优化提供了有效框架，通过集成领域专家反馈实现自动化的提示改进，在专业领域应用中具有实用价值

Abstract: Self-Optimizing Football Chatbot Guided by Domain Experts on Databricks (10 minute read) Databricks has unveiled an architecture for continuously improving AI agents, exemplified by an American Football Defensive Coordinator Assistant, which leverages MLflow's `align()` and `optimize_prompts()` functions to integrate domain expert feedback for automated prompt refinement and enhanced performance. This system encodes human expertise directly into the agent, providing situation-aware insights f...

</details>


### [41] [GitHub Actions Is Slowly Killing Your Engineering Team](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.iankduncan.com%2Fengineering%2F2026-02-05-github-actions-killing-your-team%2F%3Futm_source=tldrdevops/1/0100019c32d8a076-93aceb58-53de-420f-85ab-7c579aaba7eb-000000/ExUNbnvq-jKAoRTCNIELXS7gW57cLAYDibAI3mCjMbo=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Actions因其缓慢的UI、脆弱的YAML DSL、不透明的权限、不可信的市场和租用计算资源而消耗工程团队的时间和士气，而Buildkite通过简单配置、真实代码逻辑和可调试的基础设施提供了更好的CI体验。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示GitHub Actions在工程团队使用中的隐藏成本，指出其看似便利但实际效率低下的问题，并对比展示Buildkite作为更优CI工具的优势。

Method: 通过对比分析的方法，从多个维度（UI速度、配置语言、权限管理、市场可信度、计算资源）比较GitHub Actions和Buildkite的差异，并基于实际工程团队使用体验进行评估。

Result: GitHub Actions虽然以便利性取胜，但在实际使用中会缓慢消耗工程团队的时间和士气；而Buildkite通过简化配置、使用真实代码逻辑、提供可拥有和调试的基础设施，为工程团队提供了更高效的CI体验。

Conclusion: 工程团队应重新评估CI工具的选择，考虑GitHub Actions的隐藏成本，并关注像Buildkite这样由实际CI使用者设计的工具，以提高工程效率和团队士气。

Abstract: GitHub Actions Is Slowly Killing Your Engineering Team (14 minute read) GitHub Actions wins by convenience, not quality, and its slow UI, brittle YAML DSL, opaque permissions, untrustworthy marketplace, and rented compute quietly drain engineering time and morale. Buildkite, by contrast, keeps config simple, puts real logic in real code, lets teams own fast, debuggable infrastructure, and feels like a CI tool built by people who actually suffer through CI every day.

</details>


### [42] [Building a C compiler with a team of parallel Claudes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fbuilding-c-compiler%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/vFxhr6c6l6ubT3IvqMrzR7ZuAeNvDmyBnTGtKhkjgew=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 使用16个并行Claude Opus 4.6代理团队，花费2周时间和2万美元API成本，成功构建了一个能够编译Linux 6.9内核的Rust-based C编译器


<details>
  <summary>Details</summary>
Motivation: 探索大规模并行AI代理团队在复杂软件开发任务中的能力，特别是构建像C编译器这样的基础系统软件

Method: 采用16个并行Claude Opus 4.6代理组成的团队，通过自主协作方式开发Rust-based C编译器，支持x86、ARM和RISC-V架构

Result: 成功构建了10万行代码的编译器，能够编译Linux 6.9内核，并在x86、ARM和RISC-V架构上运行

Conclusion: 大规模并行AI代理团队能够完成复杂的软件开发任务，展示了AI在系统编程领域的潜力，尽管成本较高

Abstract: Building a C compiler with a team of parallel Claudes (14 minute read) Anthropic tasked a team of 16 parallel Claude Opus 4.6 agents to autonomously build a Rust-based C compiler capable of compiling the Linux kernel. Over two weeks and $20,000 in API costs, the agent team produced a 100,000-line compiler that can build Linux 6.9 on x86, ARM, and RISC-V.

</details>


### [43] [My AI Adoption Journey](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmitchellh.com%2Fwriting%2Fmy-ai-adoption-journey%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/sqncOmDPsF2RsCuSyGQlLTcdPqwPHbHsjZuOfoTwr68=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者分享了AI代理在编码、研究分析和简单任务外包方面的应用经验，并介绍了"harness engineering"技术来提升代理可靠性


<details>
  <summary>Details</summary>
Motivation: 分享个人采用AI代理的实际经验和最佳实践，帮助其他开发者更有效地利用AI代理工具

Method: 基于个人实践经验总结，包括AI代理在具体场景的应用案例和"harness engineering"技术（一种提升代理可靠性的工程方法）

Result: 发现AI代理在编码、研究分析和简单任务外包方面表现优秀，通过harness engineering可以显著提升代理的可靠性和稳定性

Conclusion: AI代理是强大的工具，特别是在编码和研究领域，通过适当的工程方法可以大幅提升其可靠性和实用性

Abstract: My AI Adoption Journey (12 minute read) Agents are great for coding, end-of-day research, and for outsourcing "slam dunk" tasks. Also, "harness engineering" helps improve agent reliability.

</details>


### [44] [Claude Opus 4.6](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fclaude-opus-4-6%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YOZT1hLpuHWniwOKUrTrha0vMCeBbBWT83PRkJBkDpY=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic发布Claude Opus 4.6模型，在编码、调试和推理能力方面有显著提升，拥有100万token上下文窗口，在多项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升AI模型在编码、调试和复杂推理任务中的性能，满足对更强大代码代理的需求。

Method: 开发升级版Claude Opus 4.6模型，优化编码、调试和推理能力，扩展上下文窗口至100万token。

Result: 模型在多种基准测试中达到最先进性能，特别是在代理编码和复杂推理测试中表现突出。

Conclusion: Claude Opus 4.6是一个强大的升级模型，在编码和推理任务中表现出色，具有大规模上下文处理能力。

Abstract: Claude Opus 4.6 (12 minute read) Anthropic has launched Claude Opus 4.6, an upgraded model better at coding, debugging, and reasoning skills, while having a 1M token context window in beta. This model has state-of-the-art performance on various benchmarks, including agentic coding and complex reasoning tests.

</details>


### [45] [Orchestrate teams of Claude Code sessions](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcode.claude.com%2Fdocs%2Fen%2Fagent-teams%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/YgoVMJFAGW-al3nDdboEzSyIt_Xw2HPzPSt4tYNOJhc=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Code推出实验性的Agent Teams功能，允许多个AI实例协作处理复杂任务，通过主导代理协调工作和综合结果，不同于子代理，团队代理之间会相互协作讨论，适合需要共享问题解决的任务。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务需要多个AI代理协作的问题，传统子代理模式缺乏真正的协作讨论，需要一种能让AI实例相互协作、共同解决问题的团队机制。

Method: 采用Agent Teams架构，包含一个主导代理协调多个AI实例的工作，代理之间可以相互协作和讨论，而不是简单的任务分解和分配。

Result: 开发出Claude Code的实验性Agent Teams功能，实现了多AI实例的协作机制，能够处理需要共享问题解决的复杂任务。

Conclusion: Agent Teams功能为复杂任务处理提供了新的协作模式，通过多AI实例的讨论和协作，能够更好地解决需要集体智慧的问题。

Abstract: Orchestrate teams of Claude Code sessions (17 minute read) Claude Code's experimental Agent Teams feature allows multiple AI instances to collaborate on complex tasks, with a lead agent coordinating work and synthesizing results. Unlike subagents, agent teams collaborate and discuss among each other, making them great for tasks requiring shared problem-solving.

</details>


### [46] [Move Over Gas Town, Claude Has First-Party Agent Orchestration](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fmove-over-gas-town%2F%3Futm_source=tldrdev/1/0100019c32db30a9-c0861116-fed7-491d-a06d-f9a35b86ad7a-000000/4ywBNxjwOeJuc8VVl5SWhoVzZ9sv1PuTi1X4pEkRgfA=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic推出Agent Teams，这是对先前多智能体编排方案（如Gas Town）的改进


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体编排方案（如Gas Town）存在局限性，需要更先进的第一方智能体编排解决方案

Method: 开发了Agent Teams，这是Claude的第一方智能体编排系统，改进了多智能体协作机制

Result: Agent Teams相比Gas Town等先前方案在多智能体编排方面有所提升

Conclusion: Anthropic的Agent Teams代表了多智能体编排技术的进步，为智能体协作提供了更好的解决方案

Abstract: Move Over Gas Town, Claude Has First-Party Agent Orchestration (3 minute read) Anthropic's new Agent Teams is an improvement on multi-agent orchestration over previous attempts like Gas Town.

</details>


### [47] [The Agentic Ecosystem on Base](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAqlc21/1/0100019c33105f2a-6abf8f7c-97f8-4758-ae0b-329c60c560c8-000000/tX8ZmV_bejeg2pb6Vr3YxiOMTY39bmrSGNN-qkOfr9w=443)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Base区块链成为AI代理活动主导平台，因其低成本支持x402微支付和专用基础设施，已处理900万+交易、160万+代理注册，实现1亿美元+交易量，展示L2在DeFi外的首个产品市场契合


<details>
  <summary>Details</summary>
Motivation: 探索区块链Layer 2解决方案在AI代理生态系统中的应用潜力，特别是Base区块链如何通过低成本交易和专用基础设施支持AI代理活动，超越传统DeFi应用场景

Method: 基于Base区块链平台构建AI代理生态系统，利用x402协议实现微支付功能，开发专用基础设施支持代理注册和交易，通过实际部署和运营数据验证可行性

Result: 30天内处理超过900万笔x402交易，交易量超过100万美元；注册约160万个OpenClaw代理；Bankr和Clanker等平台交易量超过1亿美元；证明Base在AI代理领域具有显著产品市场契合度

Conclusion: Base区块链已成为AI代理活动的主导平台，其低成本交易和专用基础设施成功支持了大规模AI代理生态系统，展示了Layer 2区块链在DeFi之外的首个可验证产品市场契合案例

Abstract: The Agentic Ecosystem on Base (5 minute read) Base is emerging as the dominant blockchain for AI agent activity due to transaction costs low enough to support micropayments via x402 and purpose-built infrastructure. The ecosystem has processed over 9M x402 transactions with $1M+ volume in 30 days, ~1.6M OpenClaw agents registered, and $100M+ volume on platforms like Bankr and Clanker. This represents one of the first demonstrable product-market fits for L2s beyond DeFi.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Toward Faithful and Complete Answer Construction from a Single Document](https://arxiv.org/abs/2602.06103)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: EVE是一个结构化框架，通过提取-验证-枚举的管道约束生成过程，解决LLM在文档推理中完整性和忠实性的问题，显著提升召回率、精确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型基于统计的下一词预测，偏向高概率延续而非基于源内容的全面忠实回答，缺乏确保完整性和忠实性的系统机制，这与AI安全原则相冲突。

Method: 提出EVE结构化框架，将高严谨推理分解为提取、验证和枚举三个步骤，通过约束生成管道实现可验证的文档推理。

Result: 召回率和精确率分别提升高达24%和29%，F1分数提升31%，打破了单次LLM生成中覆盖率和准确性的权衡，同时缓解了长度限制导致的截断问题。

Conclusion: EVE通过结构化管道显著提升文档推理的完整性和忠实性，但受自然语言固有模糊性限制存在性能饱和，反映了基于语言推理的基本限制。

Abstract: Modern large language models (LLMs) are powerful generators driven by statistical next-token prediction. While effective at producing fluent text, this design biases models toward high-probability continuations rather than exhaustive and faithful answers grounded in source content. As a result, directly applying LLMs lacks systematic mechanisms to ensure both completeness (avoiding omissions) and faithfulness (avoiding unsupported content), which fundamentally conflicts with core AI safety principles. To address this limitation, we present EVE, a structured framework for document-grounded reasoning.
  Unlike free-form prompting, EVE constrains generation to a structured, verifiable pipeline that decomposes high-rigor reasoning into extraction, validation, and enumeration. Empirically, this design enables consistent and simultaneous improvements in recall, precision, and F1-score: recall and precision increase by up to 24\% and 29\%, respectively, with a corresponding 31\% gain in F1-score. This effectively breaks the long-standing trade-off between coverage and accuracy typical of single-pass LLM generation, while also mitigating generation truncation caused by length limitations. At the same time, we emphasize that EVE exhibits performance saturation due to the inherent ambiguity of natural language, reflecting fundamental limits of language-based reasoning.

</details>


### [49] [Flow Matching for Offline Reinforcement Learning with Discrete Actions](https://arxiv.org/abs/2602.06138)
*Fairoz Nower Khan,Nabuat Zaman Nahim,Ruiquan Huang,Haibo Yang,Peizhong Ju*

Main category: cs.LG

TL;DR: 本文提出了一种基于连续时间马尔可夫链的离散动作空间流匹配框架，将生成策略扩展到离散动作空间和多目标设置，并在多智能体环境中通过因子化条件路径缓解联合动作空间指数增长问题。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型和流匹配的生成策略在离线强化学习中表现出色，但主要局限于连续动作空间。为了支持更广泛的离线RL设置，需要将流匹配扩展到离散动作空间和多目标场景。

Method: 1) 用连续时间马尔可夫链替代连续流，使用Q加权流匹配目标进行训练；2) 扩展到多智能体设置，通过因子化条件路径缓解联合动作空间指数增长；3) 通过动作量化将离散框架应用于连续控制问题。

Result: 理论证明在理想条件下优化该目标可恢复最优策略。实验表明方法在高维控制、多模态决策和多目标动态偏好等实际场景中表现稳健，离散框架通过动作量化在连续控制问题中提供表示复杂性与性能的灵活权衡。

Conclusion: 提出的离散流匹配框架成功扩展了生成策略的应用范围，支持离散动作空间和多目标设置，在多智能体环境中有效缓解联合动作空间问题，为离线强化学习提供了更通用的解决方案。

Abstract: Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.

</details>


### [50] [REBEL: Hidden Knowledge Recovery via Evolutionary-Based Evaluation Loop](https://arxiv.org/abs/2602.06248)
*Patryk Rybak,Paweł Batorski,Paul Swoboda,Przemysław Spurek*

Main category: cs.LG

TL;DR: REBEL是一种进化对抗提示生成方法，用于测试LLM遗忘方法是否真正移除了敏感数据，揭示当前遗忘方法可能只提供表面保护


<details>
  <summary>Details</summary>
Motivation: 当前LLM遗忘方法的评估指标依赖良性查询，往往将表面信息抑制误认为真正的知识移除，无法检测更复杂提示策略仍能提取的残留知识

Method: 提出REBEL进化对抗提示生成方法，通过进化算法生成对抗性提示来探测遗忘数据是否仍能被恢复

Result: REBEL成功从标准遗忘基准中看似已遗忘的模型中提取"被遗忘"知识，在TOFU基准上攻击成功率高达60%，在WMDP基准上达93%

Conclusion: 当前LLM遗忘方法可能只提供表面保护，需要更强大的评估方法来确保敏感数据的真正移除

Abstract: Machine unlearning for LLMs aims to remove sensitive or copyrighted data from trained models. However, the true efficacy of current unlearning methods remains uncertain. Standard evaluation metrics rely on benign queries that often mistake superficial information suppression for genuine knowledge removal. Such metrics fail to detect residual knowledge that more sophisticated prompting strategies could still extract. We introduce REBEL, an evolutionary approach for adversarial prompt generation designed to probe whether unlearned data can still be recovered. Our experiments demonstrate that REBEL successfully elicits ``forgotten'' knowledge from models that seemed to be forgotten in standard unlearning benchmarks, revealing that current unlearning methods may provide only a superficial layer of protection. We validate our framework on subsets of the TOFU and WMDP benchmarks, evaluating performance across a diverse suite of unlearning algorithms. Our experiments show that REBEL consistently outperforms static baselines, recovering ``forgotten'' knowledge with Attack Success Rates (ASRs) reaching up to 60% on TOFU and 93% on WMDP. We will make all code publicly available upon acceptance. Code is available at https://github.com/patryk-rybak/REBEL/

</details>


### [51] [Online Adaptive Reinforcement Learning with Echo State Networks for Non-Stationary Dynamics](https://arxiv.org/abs/2602.06326)
*Aoi Yoshimura,Gouhei Tanaka*

Main category: cs.LG

TL;DR: 提出基于储层计算的轻量级在线适应框架，使用回声状态网络编码观测历史并通过递归最小二乘在线更新，实现非平稳环境下的快速适应


<details>
  <summary>Details</summary>
Motivation: 强化学习策略在仿真训练后部署到真实世界时，由于非平稳动态特性导致性能严重下降。现有方法如域随机化和元强化学习需要大量预训练、特权信息或高计算成本，限制了在实时和边缘系统中的应用。

Method: 集成回声状态网络作为适应模块，编码最近的观测历史为潜在上下文表示，并使用递归最小二乘在线更新其读出权重。该设计无需反向传播、预训练或特权信息访问。

Result: 在CartPole和HalfCheetah任务上测试，面对严重且突然的环境变化（周期性外部干扰和极端摩擦变化），该方法显著优于域随机化和代表性自适应基线，能在几个控制步骤内实现稳定适应，并能处理情节内的环境变化而无需重置策略。

Conclusion: 由于计算效率和稳定性，该框架为非平稳环境中的在线适应提供了实用解决方案，特别适合真实世界机器人控制和边缘部署。

Abstract: Reinforcement learning (RL) policies trained in simulation often suffer from severe performance degradation when deployed in real-world environments due to non-stationary dynamics. While Domain Randomization (DR) and meta-RL have been proposed to address this issue, they typically rely on extensive pretraining, privileged information, or high computational cost, limiting their applicability to real-time and edge systems. In this paper, we propose a lightweight online adaptation framework for RL based on Reservoir Computing. Specifically, we integrate an Echo State Networks (ESNs) as an adaptation module that encodes recent observation histories into a latent context representation, and update its readout weights online using Recursive Least Squares (RLS). This design enables rapid adaptation without backpropagation, pretraining, or access to privileged information. We evaluate the proposed method on CartPole and HalfCheetah tasks with severe and abrupt environment changes, including periodic external disturbances and extreme friction variations. Experimental results demonstrate that the proposed approach significantly outperforms DR and representative adaptive baselines under out-of-distribution dynamics, achieving stable adaptation within a few control steps. Notably, the method successfully handles intra-episode environment changes without resetting the policy. Due to its computational efficiency and stability, the proposed framework provides a practical solution for online adaptation in non-stationary environments and is well suited for real-world robotic control and edge deployment.

</details>


### [52] [Uniform Spectral Growth and Convergence of Muon in LoRA-Style Matrix Factorization](https://arxiv.org/abs/2602.06385)
*Changmin Kang,Jihun Yun,Baekrok Shin,Yeseul Cho,Chulhee Yun*

Main category: cs.LG

TL;DR: 本文分析了谱梯度下降(SpecGD)在LoRA微调LLM中的独特现象：LoRA乘积的奇异值呈现近乎均匀的增长，尽管正交化是在两个低秩因子分别进行的。作者通过理论分析证明了这种"等速率"动力学特性，并建立了全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 谱梯度下降(SpecGD)及其变体如Muon在LLM训练中表现良好，但其动力学机制仍不清楚。特别是在LoRA微调中，观察到LoRA乘积的奇异值呈现均匀增长现象，这与标准梯度流的"最大优先"学习模式形成鲜明对比，需要理论解释。

Method: 1) 在LoRA微调LLM中观察Muon优化器的奇异值增长现象；2) 在简化的LoRA风格矩阵分解设置中分析谱梯度流(SpecGF)的连续时间模拟；3) 理论证明"等速率"动力学特性；4) 证明全局收敛性，包括有界范数条件下的全局最小值和ℓ₂正则化下的全局收敛；5) 实验验证理论分析。

Result: 1) 发现LoRA乘积奇异值在Muon优化下呈现近乎均匀增长；2) 理论证明SpecGF中所有奇异值以相等速率增长（存在小偏差）；3) 较小奇异值比大奇异值更早达到目标值，与标准梯度流的"最大优先"学习模式相反；4) 证明在因子范数有界条件下，SpecGF几乎从所有初始化都能收敛到全局最小值；5) 在ℓ₂正则化下获得全局收敛保证。

Conclusion: 谱梯度下降在LoRA微调中表现出独特的"等速率"动力学特性，使奇异值均匀增长，较小奇异值优先学习。这种特性与标准梯度流形成鲜明对比，并具有理论上的全局收敛保证，为理解谱梯度方法在LLM微调中的有效性提供了理论基础。

Abstract: Spectral gradient descent (SpecGD) orthogonalizes the matrix parameter updates and has inspired practical optimizers such as Muon. They often perform well in large language model (LLM) training, but their dynamics remain poorly understood. In the low-rank adaptation (LoRA) setting, where weight updates are parameterized as a product of two low-rank factors, we find a distinctive spectral phenomenon under Muon in LoRA fine-tuning of LLMs: singular values of the LoRA product show near-uniform growth across the spectrum, despite orthogonalization being performed on the two factors separately. Motivated by this observation, we analyze spectral gradient flow (SpecGF)-a continuous-time analogue of SpecGD-in a simplified LoRA-style matrix factorization setting and prove "equal-rate" dynamics: all singular values grow at equal rates up to small deviations. Consequently, smaller singular values attain their target values earlier than larger ones, sharply contrasting with the largest-first stepwise learning observed in standard gradient flow. Moreover, we prove that SpecGF in our setting converges to global minima from almost all initializations, provided the factor norms remain bounded; with $\ell_2$ regularization, we obtain global convergence. Lastly, we corroborate our theory with experiments in the same setting.

</details>


### [53] [Principle-Evolvable Scientific Discovery via Uncertainty Minimization](https://arxiv.org/abs/2602.06448)
*Yingming Pu,Tao Lin,Hongyu Chen*

Main category: cs.LG

TL;DR: PiEvo是一个基于大语言模型的科学发现框架，通过贝叶斯优化在扩展的原理空间中进化科学原理，而非在固定假设空间中搜索，显著提升了发现效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的科学代理通常固守初始先验，在静态假设空间中操作，限制了新现象的发现，当基线理论失败时会造成计算浪费。需要从搜索假设转向进化基础科学原理。

Method: 提出PiEvo框架，将科学发现视为在扩展原理空间上的贝叶斯优化。通过高斯过程的信息导向假设选择和异常驱动的增强机制，使代理能够自主精炼其理论世界观。

Result: 在四个基准测试中，PiEvo达到90.81%~93.15%的平均解决方案质量，比现有最佳方法提升29.7%~31.1%；通过优化紧凑原理空间实现83.3%的收敛速度提升；在不同科学领域和大语言模型骨干上保持稳健性能。

Conclusion: PiEvo通过进化科学原理而非搜索假设，显著提升了科学发现的效率和效果，为基于大语言模型的科学代理提供了新的范式。

Abstract: Large Language Model (LLM)-based scientific agents have accelerated scientific discovery, yet they often suffer from significant inefficiencies due to adherence to fixed initial priors. Existing approaches predominantly operate within a static hypothesis space, which restricts the discovery of novel phenomena, resulting in computational waste when baseline theories fail. To address this, we propose shifting the focus from searching hypotheses to evolving the underlying scientific principles. We present PiEvo, a principle-evolvable framework that treats scientific discovery as Bayesian optimization over an expanding principle space. By integrating Information-Directed Hypothesis Selection via Gaussian Process and an anomaly-driven augmentation mechanism, PiEvo enables agents to autonomously refine their theoretical worldview. Evaluation across four benchmarks demonstrates that PiEvo (1) achieves an average solution quality of up to 90.81%~93.15%, representing a 29.7%~31.1% improvement over the state-of-the-art, (2) attains an 83.3% speedup in convergence step via significantly reduced sample complexity by optimizing the compact principle space, and (3) maintains robust performance across diverse scientific domains and LLM backbones.

</details>


### [54] [Evolutionary Generation of Multi-Agent Systems](https://arxiv.org/abs/2602.06511)
*Yuntong Hu,Matthew Trager,Yuting Zhang,Yi Zhang,Shuo Yang,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: EvoMAS：一种基于进化算法的多智能体系统自动生成框架，通过结构化配置生成和进化优化，显著提升复杂任务性能、可执行性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统设计依赖人工或代码生成，存在可执行性差、鲁棒性低、表达能力受限等问题，需要更自动化和灵活的生成方法。

Method: 将MAS生成建模为结构化配置生成问题，采用进化算法：从配置池选择初始配置，基于执行轨迹反馈进行变异和交叉，迭代优化候选池和经验记忆。

Result: 在BBEH推理、SWE-Bench软件工程、WorkBench工具使用等基准测试中，EvoMAS显著优于人工设计和现有自动生成方法，可执行性和运行时鲁棒性更高。

Conclusion: EvoMAS为自动生成高性能、鲁棒的多智能体系统提供了有效框架，在复杂推理、软件工程和工具使用任务上展现出强大潜力。

Abstract: Large language model (LLM)-based multi-agent systems (MAS) show strong promise for complex reasoning, planning, and tool-augmented tasks, but designing effective MAS architectures remains labor-intensive, brittle, and hard to generalize. Existing automatic MAS generation methods either rely on code generation, which often leads to executability and robustness failures, or impose rigid architectural templates that limit expressiveness and adaptability. We propose Evolutionary Generation of Multi-Agent Systems (EvoMAS), which formulates MAS generation as structured configuration generation. EvoMAS performs evolutionary generation in configuration space. Specifically, EvoMAS selects initial configurations from a pool, applies feedback-conditioned mutation and crossover guided by execution traces, and iteratively refines both the candidate pool and an experience memory. We evaluate EvoMAS on diverse benchmarks, including BBEH, SWE-Bench, and WorkBench, covering reasoning, software engineering, and tool-use tasks. EvoMAS consistently improves task performance over both human-designed MAS and prior automatic MAS generation methods, while producing generated systems with higher executability and runtime robustness. EvoMAS outperforms the agent evolution method EvoAgent by +10.5 points on BBEH reasoning and +7.1 points on WorkBench. With Claude-4.5-Sonnet, EvoMAS also reaches 79.1% on SWE-Bench-Verified, matching the top of the leaderboard.

</details>


### [55] [Endogenous Resistance to Activation Steering in Language Models](https://arxiv.org/abs/2602.06941)
*Alex McKenzie,Keenan Pepper,Stijn Servaes,Martin Leitgab,Murat Cubuktepe,Mike Vaiana,Diogo de Lucena,Judd Rosenblatt,Michael S. A. Graziano*

Main category: cs.LG

TL;DR: 大语言模型在推理过程中能够抵抗任务不匹配的激活引导，有时在引导持续的情况下仍能中途恢复并产生改进的响应，这种现象被称为内源性引导抵抗（ESR）。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在激活引导下的抵抗行为，探索模型内部是否存在专门的"一致性检查"电路，以及这种抵抗机制对AI系统透明性和可控性的影响。

Method: 使用稀疏自编码器（SAE）潜在变量引导模型激活，分析不同规模模型（Llama-3.3-70B、Llama-3和Gemma-2系列）的ESR表现。通过零消融实验识别与ESR因果相关的潜在变量，并通过提示工程和微调实验增强ESR行为。

Result: Llama-3.3-70B表现出显著的ESR，而较小模型较少出现此现象。识别出26个与离题内容激活相关的SAE潜在变量，零消融这些变量可将多尝试率降低25%。通过元提示可将多尝试率提高4倍，通过微调可在小模型中诱导类似ESR的行为。

Conclusion: ESR机制具有双重意义：一方面可抵御对抗性操纵，另一方面可能干扰依赖激活引导的有益安全干预。理解和控制这些抵抗机制对于开发透明可控的AI系统至关重要。

Abstract: Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.

</details>


### [56] [Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics](https://arxiv.org/abs/2602.06939)
*Zuyuan Zhang,Sizhe Tang,Tian Lan*

Main category: cs.LG

TL;DR: 提出基于拓扑视角的强化学习新框架，将TD误差视为状态转移拓扑空间中的1-上链，通过Bellman-de Rham投影分解TD误差，并开发HodgeFlow策略搜索算法处理非马尔可夫环境。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中普遍存在非马尔可夫动态（长程依赖、部分可观测性、记忆效应），传统贝尔曼方程仅近似有效。现有研究多关注实用算法设计，缺乏理论分析框架来回答关键问题：哪些动态能被贝尔曼框架捕获？如何启发新的最优近似算法类别？

Method: 提出拓扑视角的时序差分强化学习框架：1）将TD误差视为状态转移拓扑空间中的1-上链；2）将马尔可夫动态解释为拓扑可积性；3）通过Bellman-de Rham投影实现TD误差的Hodge型分解（可积分量+拓扑残差）；4）提出HodgeFlow策略搜索算法，通过拟合势网络最小化非可积投影残差。

Result: HFPS算法在非马尔可夫环境下显著提升强化学习性能，并获得稳定性/敏感性保证。数值评估验证了该方法的有效性。

Conclusion: 该研究为强化学习提供了新颖的拓扑视角，建立了TD误差与拓扑结构之间的理论联系，为解决非马尔可夫环境中的强化学习问题提供了理论框架和实用算法。

Abstract: Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.

</details>


### [57] [Soft Forward-Backward Representations for Zero-shot Reinforcement Learning with General Utilities](https://arxiv.org/abs/2602.06769)
*Marco Bagatella,Thomas Rupf,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: 提出一种最大熵前向-后向算法，用于从离线数据中提取策略族，可零样本优化任意可微的占用度量函数，扩展了传统RL问题范围。


<details>
  <summary>Details</summary>
Motivation: 传统前向-后向算法只能处理加性奖励的RL问题，而实际应用中存在许多无法简化为加性奖励的任务（如分布匹配、纯探索等）。需要扩展算法以处理更一般的RL效用函数。

Method: 提出最大熵前向-后向算法，从离线数据中恢复随机策略族。结合零阶搜索在紧凑策略嵌入空间中进行优化，可在测试时直接优化一般效用函数，无需迭代优化方案。

Result: 方法保留了前向-后向算法的优势特性，同时成功扩展到更一般的RL问题。在简单示例和高维实验中均验证了有效性。

Conclusion: 提出的最大熵前向-后向算法能够处理任意可微占用度量函数的RL问题，扩展了零样本RL的应用范围，为更复杂的任务提供了有效解决方案。

Abstract: Recent advancements in zero-shot reinforcement learning (RL) have facilitated the extraction of diverse behaviors from unlabeled, offline data sources. In particular, forward-backward algorithms (FB) can retrieve a family of policies that can approximately solve any standard RL problem (with additive rewards, linear in the occupancy measure), given sufficient capacity. While retaining zero-shot properties, we tackle the greater problem class of RL with general utilities, in which the objective is an arbitrary differentiable function of the occupancy measure. This setting is strictly more expressive, capturing tasks such as distribution matching or pure exploration, which may not be reduced to additive rewards. We show that this additional complexity can be captured by a novel, maximum entropy (soft) variant of the forward-backward algorithm, which recovers a family of stochastic policies from offline data. When coupled with zero-order search over compact policy embeddings, this algorithm can sidestep iterative optimization schemes, and optimizes general utilities directly at test-time. Across both didactic and high-dimensional experiments, we demonstrate that our method retains favorable properties of FB algorithms, while also extending their range to more general RL problems.

</details>


### [58] [Rare Event Analysis of Large Language Models](https://arxiv.org/abs/2602.06791)
*Jake McAllister Dorman,Edward Gillman,Dominic C. Rose,Jamie F. Mair,Juan P. Garrahan*

Main category: cs.LG

TL;DR: 提出了一个端到端框架，用于系统分析大语言模型中的罕见事件，包括理论、高效生成策略、概率估计和误差分析


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为概率模型，在推理过程中会出现罕见事件——远离典型但高度显著的行为。由于LLM使用规模巨大，开发阶段完全未观察到的事件很可能在部署中变得突出，因此需要系统分析这些罕见事件

Method: 提出了一个端到端框架，包含理论分析、高效生成策略、概率估计和误差分析，并提供了具体实现和示例

Result: 开发了一个完整的分析框架，能够系统地识别、生成和分析LLM中的罕见事件，并展示了具体应用示例

Conclusion: 该框架为分析LLM罕见事件提供了系统方法，其概念和技术具有通用性，可扩展到其他模型和上下文

Abstract: Being probabilistic models, during inference large language models (LLMs) display rare events: behaviour that is far from typical but highly significant. By definition all rare events are hard to see, but the enormous scale of LLM usage means that events completely unobserved during development are likely to become prominent in deployment. Here we present an end-to-end framework for the systematic analysis of rare events in LLMs. We provide a practical implementation spanning theory, efficient generation strategies, probability estimation and error analysis, which we illustrate with concrete examples. We outline extensions and applications to other models and contexts, highlighting the generality of the concepts and techniques presented here.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [59] [SVRepair: Structured Visual Reasoning for Automated Program Repair](https://arxiv.org/abs/2602.06090)
*Xiaoxuan Tang,Jincheng Wang,Liwei Luo,Jingxuan Xu,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: SVRepair是一个多模态程序修复框架，通过结构化视觉表示将视觉工件转换为语义场景图，帮助代码代理定位故障并生成补丁，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM程序修复方法多为单模态，无法利用bug报告中丰富的视觉信息（如截图、控制流图）。直接使用密集视觉输入会导致上下文丢失和噪声，难以将视觉观察转化为精确的故障定位和可执行补丁。

Method: 1) 微调视觉语言模型SVR，将异构视觉工件统一转换为语义场景图，捕捉GUI元素及其结构关系；2) 基于场景图驱动代码代理进行故障定位和补丁合成；3) 引入迭代视觉工件分割策略，逐步缩小输入到bug相关区域以减少无关上下文和幻觉。

Result: 在多个基准测试中达到SOTA性能：SWE-Bench M上36.47%准确率，MMCode上38.02%准确率，CodeVision上95.12%准确率，验证了SVRepair在多模态程序修复中的有效性。

Conclusion: SVRepair通过结构化视觉表示有效桥接了视觉语义鸿沟，显著提升了多模态程序修复的性能，证明了视觉信息在自动化程序修复中的重要性。

Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.

</details>


### [60] [Coding Agents with Environment Interaction: A Theoretical Perspective](https://arxiv.org/abs/2602.06098)
*Nicolas Menet,Michael Hersche,Andreas Krause,Abbas Rahimi*

Main category: cs.SE

TL;DR: 本文为编码代理在测试驱动开发中的环境交互策略提供了概率框架，分析了代码选择和反馈生成两种范式，证明了模糊功能相似性估计器优于功能等价估计器，并将反向提示形式化为Thompson采样的近似，解释了其效果受任务描述模糊性限制的原因。


<details>
  <summary>Details</summary>
Motivation: 编码代理在测试驱动软件开发中应用日益广泛，但其环境交互策略的理论机制尚未得到充分探索。本文旨在为两种主导范式提供理论框架：基于执行环境的代码生成后选择，以及基于环境反馈的条件代码生成。

Method: 1. 将成熟的代码选择启发式方法形式化为环境感知的代码正确性估计器；2. 从理论上证明基于模糊功能相似性的估计器优于基于功能等价的估计器；3. 将反向提示框架化为Thompson采样的上下文近似；4. 推导具有不可观测分量的奖励函数的遗憾界；5. 在三个最先进的开源模型上验证理论发现。

Result: 理论证明：模糊功能相似性估计器通过添加归纳偏置，在信噪比方面严格优于功能等价估计器。反向提示的效果受任务描述模糊性限制（不可约遗憾）。实验在BigCodeBenchHard、LeetCodeDataset和QiskitHumanEvalSim三个数据集上验证了理论发现。基于形式化分析提出了改进任务描述的方法，创建了新基准QiskitHumanEvalSimX。

Conclusion: 本文为编码代理的环境交互策略提供了理论基础，解释了为什么某些策略有效而其他策略受限。理论框架不仅解释了现有现象，还指导了如何改进任务描述以提高编码代理性能。形式化分析为未来编码代理设计提供了理论指导。

Abstract: Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.

</details>


### [61] [Trustworthy AI Software Engineers](https://arxiv.org/abs/2602.06310)
*Aldeida Aleti,Baishakhi Ray,Rashina Hoda,Simin Chen*

Main category: cs.SE

TL;DR: 该论文探讨AI编码代理作为软件工程师的定义及其可信度评估框架，强调在人类-AI团队中建立适当信任的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理的快速发展，软件工程师的本质定义受到挑战。论文旨在重新审视AI代理作为软件工程师的含义，并深入思考如何建立可信的AI软件工程师。

Method: 基于软件工程的传统定义和近期智能代理系统研究，将AI软件工程师概念化为人类-AI软件工程团队的参与者。通过历史视角和新兴愿景，识别可信度的关键维度。

Result: 提出了AI软件工程师可信度的关键维度：技术质量、透明度和问责制、认知谦逊、社会和伦理对齐。同时指出了信任测量的根本差距——并非所有重要因素都能轻易测量。

Conclusion: 倡导采用"设计即伦理"的方法来设计、评估和治理AI软件工程系统，以在未来人类-AI团队中建立适当的信任。

Abstract: With the rapid rise of AI coding agents, the fundamental premise of what it means to be a software engineer is in question. In this vision paper, we re-examine what it means for an AI agent to be considered a software engineer and then critically think about what makes such an agent trustworthy. \textit{Grounded} in established definitions of software engineering (SE) and informed by recent research on agentic AI systems, we conceptualise AI software engineers as participants in human-AI SE teams composed of human software engineers and AI models and tools, and we distinguish trustworthiness as a key property of these systems and actors rather than a subjective human attitude. Based on historical perspectives and emerging visions, we identify key dimensions that contribute to the trustworthiness of AI software engineers, spanning technical quality, transparency and accountability, epistemic humility, and societal and ethical alignment. We further discuss how trustworthiness can be evaluated and demonstrated, highlighting a fundamental trust measurement gap: not everything that matters for trust can be easily measured. Finally, we outline implications for the design, evaluation, and governance of AI SE systems, advocating for an ethics-by-design approach to enable appropriate trust in future human-AI SE teams.

</details>


### [62] [AgentStepper: Interactive Debugging of Software Development Agents](https://arxiv.org/abs/2602.06593)
*Robert Hutter,Michael Pradel*

Main category: cs.SE

TL;DR: AgentStepper：首个用于LLM软件工程代理的交互式调试器，通过结构化对话表示、断点、单步执行等功能，帮助开发者理解和调试复杂代理行为。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的软件开发代理虽然功能强大，但调试困难。开发者需要理解LLM查询、工具调用和代码修改的复杂轨迹，但现有技术难以以可理解的方式展示这些中间过程。

Method: 提出AgentStepper调试器，将代理轨迹表示为LLM、代理程序和工具之间的结构化对话。支持断点、单步执行、实时编辑提示词和工具调用，同时捕获和显示仓库级代码变更。

Result: 在三个先进软件开发代理（ExecutionAgent、SWE-Agent、RepairAgent）上集成仅需39-42行代码修改。用户研究表明，AgentStepper显著提升轨迹理解能力（64% vs 67%平均表现）、bug识别成功率（17% vs 60%），并降低工作负荷（挫败感从5.4/7.0降至2.4/7.0）。

Conclusion: AgentStepper通过提供类似传统软件调试但更高抽象级别的交互式调试能力，有效解决了LLM软件工程代理的调试难题，显著提升了开发者的理解和调试效率。

Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.

</details>


### [63] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: AST(NIT)是一种AST增强和序列化方法，将完整的AST结构信息编码为LLM兼容序列，在保持代码摘要质量的同时减少输入长度和训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统基于编码器-解码器的代码摘要模型已证明AST能提升摘要质量，但现有基于LLM的方法主要依赖原始代码或仅使用部分AST信号，未能充分利用完整AST表示的潜力。

Method: 提出AST(NIT)方法：1）AST增强，保留词法细节；2）序列化，将结构信息编码为LLM兼容序列；3）使用LLaMA-3.1-8B模型在CodeXGLUE Python数据集上进行实验。

Result: 实验表明：1）序列化AST减少了LLM输入长度；2）缩短了训练时间；3）在代码摘要质量上达到与现有方法相当的水平。

Conclusion: AST(NIT)方法成功将完整AST表示应用于LLM代码摘要，在保持质量的同时提高了效率，为LLM代码理解任务提供了有效的结构化信息编码方案。

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [64] [TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code](https://arxiv.org/abs/2602.06875)
*Jiangping Huang,Wenguang Ye,Weisong Sun,Jian Zhang,Mingyue Zhang,Yang Liu*

Main category: cs.SE

TL;DR: TraceCoder是一个多智能体代码修复框架，通过运行时追踪、因果分析和历史学习机制，显著提升LLM生成代码的修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法依赖简单的通过/失败信号，缺乏对程序行为的深入洞察，难以精确定位错误，且无法从历史失败中学习，导致修复过程低效重复。

Method: 提出TraceCoder多智能体框架：1) 在代码中插入诊断探针捕获运行时追踪；2) 对追踪进行因果分析定位根本原因；3) 引入历史教训学习机制从先前失败中提取知识；4) 使用回滚机制确保每次迭代都是严格改进。

Result: 在多个基准测试中，TraceCoder相比现有先进基线在Pass@1准确率上实现最高34.43%的相对提升，迭代修复过程单独贡献65.61%的相对增益，且在准确率和成本效率上均显著优于领先的迭代方法。

Conclusion: TraceCoder通过模拟人类专家的观察-分析-修复过程，结合运行时追踪、因果分析和历史学习，有效解决了LLM代码修复中的错误定位和效率问题，实现了显著性能提升。

Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.

</details>
