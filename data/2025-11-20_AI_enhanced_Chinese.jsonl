{"id": "2511.14798", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.14798", "abs": "https://arxiv.org/abs/2511.14798", "authors": ["Ahmad Memon", "Abdallah Mohamed"], "title": "Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods", "comment": "10 pages, 5 figures. This version corresponds to the paper accepted for presentation at CASCON 2025", "summary": "Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.\n  This paper compares two AI-based grading techniques: \\textit{Direct}, where the AI model applies a rubric directly to student code, and \\textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.\n  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cdAI\u8bc4\u5206\u65b9\u6cd5\uff1a\u76f4\u63a5\u8bc4\u5206\u6cd5\uff08AI\u76f4\u63a5\u5e94\u7528\u8bc4\u5206\u6807\u51c6\uff09\u548c\u53cd\u5411\u8bc4\u5206\u6cd5\uff08AI\u5148\u4fee\u590d\u9519\u8bef\u518d\u63a8\u65ad\u5206\u6570\uff09\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u7f16\u7a0b\u4f5c\u4e1a\u81ea\u52a8\u8bc4\u5206\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u7f16\u7a0b\u4f5c\u4e1a\u4eba\u5de5\u8bc4\u5206\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8bc4\u5206\u65b9\u6cd5\uff0c\u63d0\u9ad8\u8bc4\u5206\u7684\u5ba2\u89c2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e24\u79cdAI\u8bc4\u5206\u65b9\u6cd5\uff1a\u76f4\u63a5\u8bc4\u5206\u6cd5\u548c\u53cd\u5411\u8bc4\u5206\u6cd5\u3002\u4f7f\u7528\u539f\u59cb\u8bc4\u5206\u6807\u51c6\u548c10\u500d\u6269\u5c55\u8bc4\u5206\u6807\u51c6\u8bc4\u4f30\u51c6\u786e\u6027\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8fdb\u884c\u5bf9\u6bd4\u3002\u8fd8\u4f7f\u7528\u5408\u6210\u5b66\u751f\u4ee3\u7801\u6d4b\u8bd5\u4e00\u81f4\u6027\u3002", "result": "\u76f4\u63a5\u65b9\u6cd5\u66f4\u5feb\u66f4\u76f4\u63a5\uff0c\u4f46\u53cd\u5411\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u8bcd\uff0c\u7279\u522b\u662f\u5728\u5206\u914d\u90e8\u5206\u5206\u6570\u548c\u5904\u7406\u903b\u8f91\u9519\u8bef\u65f6\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u52bf\uff0c\u9700\u8981\u7ed3\u5408\u4f7f\u7528\u3002\u672a\u6765\u5e94\u5f00\u53d1\u6df7\u5408\u4eba\u673a\u8bc4\u5206\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u8bc4\u5206\u7684\u6548\u7387\u3001\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u3002", "topic": "swe application"}}
{"id": "2511.14780", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14780", "abs": "https://arxiv.org/abs/2511.14780", "authors": ["Keith Moore", "Jun W. Kim", "David Lyu", "Jeffrey Heo", "Ehsan Adeli"], "title": "Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents", "comment": "Preprint. Accepted for publication at AIAS 2025", "summary": "We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors (\"act like a neurologist\", \"act like an infectious disease specialist\"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.", "AI": {"tldr": "Ask WhAI\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u67e5\u548c\u6270\u52a8\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u4fe1\u5ff5\u72b6\u6001\u7684\u7cfb\u7edf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u8bb0\u5f55\u56de\u653e\u4ea4\u4e92\u3001\u67e5\u8be2\u667a\u80fd\u4f53\u4fe1\u5ff5\u548c\u63a8\u7406\u8fc7\u7a0b\u3001\u6ce8\u5165\u53cd\u4e8b\u5b9e\u8bc1\u636e\u6765\u6d4b\u8bd5\u4fe1\u5ff5\u7ed3\u6784\u5bf9\u65b0\u4fe1\u606f\u7684\u54cd\u5e94\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u5f62\u6210\u548c\u8ba4\u77e5\u5b64\u5c9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u53ef\u91cd\u73b0\u7684\u65b9\u6cd5\u6765\u5206\u6790\u667a\u80fd\u4f53\u4fe1\u5ff5\u52a8\u6001\uff0c\u8fd9\u5728\u4eba\u7c7b\u4e13\u5bb6\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bb0\u5f55\u56de\u653e\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u652f\u6301\u5e26\u5916\u67e5\u8be2\u667a\u80fd\u4f53\u4fe1\u5ff5\u548c\u63a8\u7406\uff0c\u5e76\u80fd\u591f\u6ce8\u5165\u53cd\u4e8b\u5b9e\u8bc1\u636e\u3002\u5e94\u7528\u4e8e\u533b\u7597\u6848\u4f8b\u6a21\u62df\u5668\uff0c\u5176\u4e2d\u5305\u542b\u591a\u667a\u80fd\u4f53\u5171\u4eab\u8bb0\u5fc6\uff08\u65f6\u95f4\u6233\u7535\u5b50\u75c5\u5386\uff09\u548c\u6301\u6709\u771f\u5b9e\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u7684\u795e\u8c15\u667a\u80fd\u4f53\u3002", "result": "\u6a21\u62df\u663e\u793a\u667a\u80fd\u4f53\u4fe1\u5ff5\u901a\u5e38\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u5b66\u79d1\u7acb\u573a\uff0c\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u7ecf\u5178\u7814\u7a76\u548c\u62b5\u6297\u53cd\u8bc1\u636e\uff0c\u8fd9\u4e9b\u4fe1\u5ff5\u53ef\u4ee5\u4ee5\u4eba\u7c7b\u4e13\u5bb6\u65e0\u6cd5\u5b9e\u73b0\u7684\u65b9\u5f0f\u8fdb\u884c\u8ffd\u8e2a\u548c\u8d28\u8be2\u3002", "conclusion": "Ask WhAI\u901a\u8fc7\u4f7f\u8fd9\u4e9b\u52a8\u6001\u53ef\u89c1\u548c\u53ef\u6d4b\u8bd5\uff0c\u4e3a\u7814\u7a76\u591a\u667a\u80fd\u4f53\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u5f62\u6210\u548c\u8ba4\u77e5\u5b64\u5c9b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u73b0\u7684\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2511.14773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14773", "abs": "https://arxiv.org/abs/2511.14773", "authors": ["Joey David"], "title": "Temporal Predictors of Outcome in Reasoning Language Models", "comment": "4 pages, 4 figures", "summary": "The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.", "AI": {"tldr": "\u7814\u7a76\u8868\u660eLLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f88\u65e9\u5c31\u5bf9\u6700\u7ec8\u7ed3\u679c\u505a\u51fa\u5185\u90e8\u627f\u8bfa\uff0c\u5373\u4f7f\u9700\u8981\u66f4\u957f\u7684\u8f93\u51fa\u624d\u80fd\u5f97\u5230\u660e\u786e\u7b54\u6848\uff0c\u6b63\u786e\u6027\u5728\u524d\u51e0\u4e2a\u63a8\u7406token\u540e\u5c31\u80fd\u9ad8\u5ea6\u9884\u6d4b\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f55\u65f6\u5bf9\u6700\u7ec8\u7ed3\u679c\u505a\u51fa\u5185\u90e8\u627f\u8bfa\uff0c\u4ee5\u53ca\u8fd9\u79cd\u65e9\u671f\u627f\u8bfa\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u65f6\u63a7\u5236\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u7684\u524dt\u4e2atoken\u540e\u7684\u9690\u85cf\u72b6\u6001\u4e0a\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\uff0c\u5206\u6790\u6a21\u578b\u5bf9\u6700\u7ec8\u6b63\u786e\u6027\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u4ec5\u51e0\u4e2a\u63a8\u7406token\u540e\u5c31\u80fd\u9ad8\u5ea6\u9884\u6d4b\u6700\u7ec8\u6b63\u786e\u6027\uff1b\u5bf9\u4e8e\u96be\u9898\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u4e0b\u964d\u63ed\u793a\u4e86\u9009\u62e9\u504f\u5dee\uff1a\u96be\u9898\u5728\u957fCoT\u4e2d\u5360\u6bd4\u8fc7\u9ad8\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u7684\u5185\u90e8\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u5728\u4ec5\u51e0\u4e2atoken\u540e\u5c31\u51fa\u73b0\uff0c\u8fd9\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u65f6\u63a7\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2511.14776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14776", "abs": "https://arxiv.org/abs/2511.14776", "authors": ["Snigdha Pandya", "Rohan Nagale", "Kenji Sahay", "Anna Lin", "Shikhar Shiromani", "Kevin Zhu", "Dev Sunishchal"], "title": "COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation", "comment": "9 pages, 6 figures including algorithmns, 2 tables", "summary": "Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.", "AI": {"tldr": "COMPASS\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5185\u53cd\u9988\u5faa\u73af\u52a8\u6001\u8c03\u8282\u6ce8\u610f\u529b\u5934\uff0c\u51cf\u5c11LLM\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\uff0c\u540c\u65f6\u63d0\u4f9b\u5bf9\u6a21\u578b\u8bc1\u636e\u5bf9\u9f50\u673a\u5236\u7684\u79d1\u5b66\u7406\u89e3\u3002", "motivation": "LLM\u7ecf\u5e38\u751f\u6210\u6d41\u7545\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u9648\u8ff0\uff0c\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\u6e90\u4e8e\u5b83\u4eec\u5728\u4e0a\u4e0b\u6587\u77e5\u8bc6\u548c\u53c2\u6570\u77e5\u8bc6\u4e4b\u95f4\u5206\u914d\u6ce8\u610f\u529b\u7684\u65b9\u5f0f\u3002\u7406\u89e3\u548c\u5f15\u5bfc\u8fd9\u79cd\u5185\u90e8\u884c\u4e3a\u5bf9\u4e8e\u53ef\u4fe1\u90e8\u7f72\u548c\u6a21\u578b\u673a\u5236\u7684\u79d1\u5b66\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165COMPASS\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4f9d\u8d56\u8bc4\u5206(CRS)\u91cf\u5316\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5ea6\uff0c\u4f5c\u4e3a\u6ce8\u610f\u529b\u5934\u5982\u4f55\u5c06\u751f\u6210\u57fa\u4e8e\u8bc1\u636e\u7684\u5728\u7ebf\u63a2\u9488\u3002\u4f7f\u7528\u8fd9\u4e2a\u53ef\u89e3\u91ca\u4fe1\u53f7\uff0cPID\u63a7\u5236\u5668\u52a8\u6001\u8c03\u8282\u6ce8\u610f\u529b\u5934\u4ee5\u4fdd\u6301\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u591a\u8f6e\u89e3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5(HotpotQA\u3001XSum\u3001HaluEval\u3001RAGTruth)\u4e0a\uff0cCOMPASS\u6301\u7eed\u51cf\u5c11\u4e0a\u4e0b\u6587\u5e7b\u89c9\u7387(\u7edd\u5bf9\u964d\u4f4e2.8%\u52305.8%)\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5bf9\u8bc1\u636e\u5bf9\u9f50\u7684\u8d21\u732e\u3002", "conclusion": "\u53cd\u9988\u9a71\u52a8\u7684\u53ef\u89e3\u91ca\u6027\u4e3aLLM\u884c\u4e3a\u7684\u79d1\u5b66\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\uff0cCOMPASS\u6846\u67b6\u5728\u51cf\u5c11\u5e7b\u89c9\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u5bf9\u6a21\u578b\u673a\u5236\u7684\u7406\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2511.14846", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.14846", "abs": "https://arxiv.org/abs/2511.14846", "authors": ["Yifeng Ding", "Hung Le", "Songyang Han", "Kangrui Ruan", "Zhenghui Jin", "Varun Kumar", "Zijian Wang", "Anoop Deoras"], "title": "Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization", "comment": null, "summary": "Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.", "AI": {"tldr": "\u63d0\u51fa\u4e86GTPO\u7b97\u6cd5\uff0c\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u8bad\u7ec3LLM\u5728\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u56de\u5408\u7ea7\u5956\u52b1\u5206\u914d\u3001\u57fa\u4e8e\u56de\u62a5\u7684\u4f18\u52bf\u4f30\u8ba1\u548c\u81ea\u76d1\u7763\u5956\u52b1\u5851\u9020\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u505c\u6ede\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u8bad\u7ec3\u505c\u6ede\u95ee\u9898\uff0c\u56e0\u4e3a\u8f68\u8ff9\u7ea7\u5956\u52b1\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u65e0\u6cd5\u4e3a\u590d\u6742\u7684\u591a\u8f6e\u4ea4\u4e92\u63d0\u4f9b\u8db3\u591f\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "GTPO\u7b97\u6cd5\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u56de\u5408\u7ea7\u5956\u52b1\u5206\u914d\u3001\u57fa\u4e8e\u56de\u62a5\u7684\u4f18\u52bf\u4f30\u8ba1\u3001\u4ee5\u53ca\u5229\u7528\u751f\u6210\u4ee3\u7801\u81ea\u76d1\u7763\u4fe1\u53f7\u6765\u4e30\u5bcc\u7a00\u758f\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\u7684\u81ea\u76d1\u7763\u5956\u52b1\u5851\u9020\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGTPO\u5e73\u5747\u6bd4GRPO\u65b9\u6cd5\u8868\u73b0\u63d0\u53473.0%\u3002", "conclusion": "GTPO\u7b97\u6cd5\u6709\u6548\u63a8\u8fdb\u4e86\u590d\u6742\u6570\u5b66\u63a8\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14967", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14967", "abs": "https://arxiv.org/abs/2511.14967", "authors": ["Basel Shbita", "Farhan Ahmed", "Chad DeLuca"], "title": "MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86MermaidSeqBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210Mermaid\u5e8f\u5217\u56fe\u7684\u80fd\u529b\uff0c\u5305\u542b132\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u6837\u672c\uff0c\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u6269\u5c55\uff0c\u5e76\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u5728\u751f\u6210\u7ed3\u6784\u5316\u56fe\u8868\uff08\u7279\u522b\u662fMermaid\u5e8f\u5217\u56fe\uff09\u65b9\u9762\u7684\u6b63\u786e\u6027\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u4eba\u5de5\u6807\u6ce8\u3001\u4e0a\u4e0b\u6587LLM\u63d0\u793a\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u53d8\u4f53\u751f\u6210\uff0c\u6784\u5efa\u5305\u542b132\u4e2a\u6837\u672c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u6a21\u578b\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u548c\u8bc4\u4f30\u6a21\u5f0f\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u7ed3\u6784\u5316\u56fe\u8868\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u4e25\u683c\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "swe benchmark"}}
{"id": "2511.15002", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15002", "abs": "https://arxiv.org/abs/2511.15002", "authors": ["Fatemeh Lotfi", "Hossein Rajoli", "Fatemeh Afghah"], "title": "Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning", "comment": "Accepted to be published in IEEE Transaction on Machine Learning in Communication and Networking (TMLCN)", "summary": "Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $\u03c1$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316(SAM)\u548c\u8f6f\u6f14\u5458\u8bc4\u8bba\u5bb6(SAC)\u7b97\u6cd5\u7684\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8eO-RAN\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u901a\u8fc7\u57fa\u4e8eTD\u8bef\u5dee\u65b9\u5dee\u7684\u9002\u5e94\u6027SAM\u673a\u5236\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u7f51\u7edc\u91c7\u7528O-RAN\u67b6\u6784\u5b9e\u73b0\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\uff0c\u4f46\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5f80\u5f80\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u5728\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5c06SAC\u7b97\u6cd5\u4e0eSAM\u7ed3\u5408\uff0c\u5f15\u5165\u57fa\u4e8eTD\u8bef\u5dee\u65b9\u5dee\u7684\u9002\u5e94\u6027\u9009\u62e9\u6027SAM\u673a\u5236\uff0c\u4ec5\u5bf9\u73af\u5883\u590d\u6742\u5ea6\u9ad8\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u540c\u65f6\u91c7\u7528\u52a8\u6001\u03c1\u8c03\u5ea6\u65b9\u6848\u4f18\u5316\u63a2\u7d22-\u5229\u7528\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edfDRL\u65b9\u6cd5\uff0c\u8d44\u6e90\u5206\u914d\u6548\u7387\u63d0\u5347\u9ad8\u8fbe22%\uff0c\u5e76\u5728\u591a\u6837\u5316O-RAN\u5207\u7247\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684QoS\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u9488\u5bf9\u6027\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5b66\u4e60\u6548\u7387\uff0c\u4e3aO-RAN\u7f51\u7edc\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.14887", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14887", "abs": "https://arxiv.org/abs/2511.14887", "authors": ["Nathan M. Roberts", "Xiaosong Du"], "title": "Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone", "comment": "Conference version with 12 pages and 2 figures", "summary": "The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\\times10^6$ time steps, representing 25% of the $19.79\\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u5f15\u5bfc\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316eVTOL\u65e0\u4eba\u673a\u8d77\u98de\u8f68\u8ff9\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\uff0c\u76f8\u6bd4\u4f20\u7edfDRL\u8bad\u7ec3\u6548\u7387\u63d0\u534775%\uff0c\u80fd\u8017\u4f18\u5316\u51c6\u786e\u7387\u8fbe\u523097.2%\u3002", "motivation": "eVTOL\u98de\u673a\u6709\u671b\u7f13\u89e3\u57ce\u5e02\u4ea4\u901a\u62e5\u5835\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u6700\u4f18\u8d77\u98de\u8f68\u8ff9\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\u3002\u4f20\u7edf\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u53d7\u9650\u4e8e\u95ee\u9898\u7ef4\u5ea6\u548c\u590d\u6742\u6027\uff0c\u800c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTransformer\u5f15\u5bfc\u7684DRL\u65b9\u6cd5\uff0c\u4f7f\u7528Transformer\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u63a2\u7d22\u771f\u5b9e\u72b6\u6001\u7a7a\u95f4\uff0c\u5e94\u7528\u4e8eeVTOL\u65e0\u4eba\u673a\u6700\u4f18\u8d77\u98de\u8f68\u8ff9\u8bbe\u8ba1\uff0c\u63a7\u5236\u53d8\u91cf\u4e3a\u529f\u7387\u548c\u673a\u7ffc\u89d2\u5ea6\u3002", "result": "Transformer\u5f15\u5bfc\u7684DRL\u4ec5\u97004.57\u00d710^6\u65f6\u95f4\u6b65\u5b8c\u6210\u8bad\u7ec3\uff0c\u662f\u4f20\u7edfDRL\u6240\u9700\u65f6\u95f4\u6b65\u768425%\uff0c\u80fd\u8017\u4f18\u5316\u51c6\u786e\u7387\u8fbe\u523097.2%\uff08\u4f20\u7edfDRL\u4e3a96.3%\uff09\u3002", "conclusion": "Transformer\u5f15\u5bfc\u7684DRL\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6700\u4f18\u8bbe\u8ba1\u9a8c\u8bc1\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edfDRL\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15107", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15107", "abs": "https://arxiv.org/abs/2511.15107", "authors": ["Yuan Jiang", "Zehao Li", "Shan Huang", "Christoph Treude", "Xiaohong Su", "Tiantian Wang"], "title": "Effective Code Membership Inference for Code Completion Models via Adversarial Prompts", "comment": null, "summary": "Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.", "AI": {"tldr": "AdvPrompt-MIA\u662f\u4e00\u79cd\u9488\u5bf9\u4ee3\u7801\u8865\u5168\u6a21\u578b\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4ee3\u7801\u7279\u5b9a\u7684\u5bf9\u6297\u6027\u63d0\u793a\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u81ea\u52a8\u533a\u5206\u8bad\u7ec3\u96c6\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u548c\u7070\u76d2\u6210\u5458\u63a8\u7406\u653b\u51fb\u4f9d\u8d56\u6602\u8d35\u7684\u4ee3\u7406\u6a21\u578b\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u96be\u4ee5\u6355\u6349\u8fc7\u53c2\u6570\u5316\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u9690\u79c1\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u4e00\u7cfb\u5217\u5bf9\u6297\u6027\u63d0\u793a\u6765\u8bf1\u5bfc\u53d7\u5bb3\u8005\u4ee3\u7801\u6a21\u578b\u8f93\u51fa\u53d8\u5316\uff0c\u901a\u8fc7\u6bd4\u8f83\u8fd9\u4e9b\u8f93\u51fa\u4e0e\u771f\u5b9e\u8865\u5168\u7ed3\u679c\u6784\u5efa\u7279\u5f81\u5411\u91cf\uff0c\u8bad\u7ec3\u5206\u7c7b\u5668\u81ea\u52a8\u533a\u5206\u6210\u5458\u548c\u975e\u6210\u5458\u6837\u672c\u3002", "result": "\u5728Code Llama 7B\u7b49\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728APPS\u548cHumanEval\u57fa\u51c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff0cAUC\u63d0\u5347\u9ad8\u8fbe102%\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6a21\u578b\u548c\u6570\u636e\u96c6\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "AdvPrompt-MIA\u80fd\u591f\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u8bb0\u5fc6\u6a21\u5f0f\uff0c\u51c6\u786e\u63a8\u65ad\u8bad\u7ec3\u96c6\u6210\u5458\u5173\u7cfb\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.15055", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15055", "abs": "https://arxiv.org/abs/2511.15055", "authors": ["Jian-Ting Guo", "Yu-Cheng Chen", "Ping-Chun Hsieh", "Kuo-Hao Ho", "Po-Wei Huang", "Ti-Rong Wu", "I-Chen Wu"], "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization", "comment": "Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Macro Action Quantization (MAQ)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u6f14\u793a\u63d0\u70bc\u4e3a\u5b8f\u52a8\u4f5c\u6765\u8bad\u7ec3\u7c7b\u4eba\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5728D4RL Adroit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u867d\u7136\u5728\u8bb8\u591a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f80\u5f80\u4ea7\u751f\u4e0e\u4eba\u7c7b\u884c\u4e3a\u76f8\u6bd4\u4e0d\u81ea\u7136\u7684\u884c\u4e3a\uff0c\u8fd9\u5f15\u53d1\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u7684\u62c5\u5fe7\u3002\u76ee\u6807\u662f\u8bbe\u8ba1\u80fd\u591f\u4ea7\u751f\u7c7b\u4eba\u884c\u4e3a\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u3002", "method": "\u5c06\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u5efa\u6a21\u4e3a\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u5f15\u5165MAQ\u6846\u67b6\uff0c\u901a\u8fc7Vector-Quantized VAE\u5c06\u4eba\u7c7b\u6f14\u793a\u63d0\u70bc\u4e3a\u5b8f\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u540e\u9000\u65f6\u57df\u63a7\u5236\u4f5c\u4e3a\u53ef\u5904\u7406\u4e14\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002", "result": "\u5728D4RL Adroit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMAQ\u663e\u8457\u63d0\u9ad8\u4e86\u4eba\u7c7b\u76f8\u4f3c\u5ea6\uff0c\u589e\u52a0\u4e86\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u5e76\u5728\u4eba\u7c7b\u8bc4\u4f30\u7814\u7a76\u4e2d\u83b7\u5f97\u4e86\u6240\u6709RL\u667a\u80fd\u4f53\u4e2d\u6700\u9ad8\u7684\u7c7b\u4eba\u6392\u540d\u3002", "conclusion": "MAQ\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u5404\u79cd\u73b0\u6210\u7684RL\u7b97\u6cd5\u4e2d\uff0c\u4e3a\u5b66\u4e60\u7c7b\u4ebaRL\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15061", "categories": ["cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15061", "abs": "https://arxiv.org/abs/2511.15061", "authors": ["Haodong Chen", "Guido Zuccon", "Teerapong Leelanupab"], "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering", "comment": "This paper has been accepted to SIGIR-AP 2025", "summary": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.\n  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.\n  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.", "AI": {"tldr": "OpenBioLLM\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u56e0\u7ec4\u95ee\u7b54\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u89e3\u51b3GeneGPT\u4f9d\u8d56\u4e13\u6709\u6a21\u578b\u5e26\u6765\u7684\u53ef\u6269\u5c55\u6027\u3001\u8fd0\u8425\u6210\u672c\u3001\u6570\u636e\u9690\u79c1\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\uff0c\u63a2\u7d22\u5f00\u6e90\u6a21\u578b\u5728\u57fa\u56e0\u7ec4\u95ee\u7b54\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5f15\u5165\u5de5\u5177\u8def\u7531\u3001\u67e5\u8be2\u751f\u6210\u548c\u54cd\u5e94\u9a8c\u8bc1\u7684\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\uff0c\u5b9e\u73b0\u534f\u8c03\u63a8\u7406\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u4efb\u52a1\u6267\u884c\u3002", "result": "\u572890%\u4ee5\u4e0a\u7684\u57fa\u51c6\u4efb\u52a1\u4e2d\u5339\u914d\u6216\u8d85\u8d8aGeneGPT\uff0c\u5728Gene-Turing\u548cGeneHop\u4e0a\u5206\u522b\u83b7\u5f970.849\u548c0.830\u7684\u5e73\u5747\u5206\uff0c\u5ef6\u8fdf\u964d\u4f4e40-50%\u3002", "conclusion": "\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u57fa\u56e0\u7ec4\u95ee\u7b54\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.15074", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15074", "abs": "https://arxiv.org/abs/2511.15074", "authors": ["Henrik Bradland", "Morten Goodwin", "Vladimir I. Zadorozhny", "Per-Arne Andersen"], "title": "Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents", "comment": "19 pages, 4 figures, in review", "summary": "The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a \"flooding-pruning\" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.", "AI": {"tldr": "Rogue One\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u8fed\u4ee3\u6765\u53d1\u73b0\u3001\u751f\u6210\u548c\u9a8c\u8bc1\u9884\u6d4b\u6027\u7279\u5f81\uff0c\u5728\u8868\u683c\u6570\u636e\u7279\u5f81\u5de5\u7a0b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5b58\u5728\u5355\u4e00\u4f53\u67b6\u6784\u3001\u7b80\u5355\u91cf\u5316\u53cd\u9988\u548c\u7f3a\u4e4f\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u6574\u5408\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u6563\u5f0f\u4e09\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u79d1\u5b66\u5bb6\u3001\u63d0\u53d6\u5668\u3001\u6d4b\u8bd5\u5668\uff09\uff0c\u7ed3\u5408\u4e30\u5bcc\u7684\u5b9a\u6027\u53cd\u9988\u673a\u5236\u548c\"\u6cdb\u6ee5-\u4fee\u526a\"\u7b56\u7565\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u3002", "result": "\u572819\u4e2a\u5206\u7c7b\u548c9\u4e2a\u56de\u5f52\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u80fd\u591f\u53d1\u73b0\u65b0\u9896\u53ef\u6d4b\u8bd5\u7684\u5047\u8bbe\uff08\u5982\u5fc3\u808c\u6570\u636e\u96c6\u4e2d\u7684\u65b0\u751f\u7269\u6807\u5fd7\u7269\uff09\u3002", "conclusion": "Rogue One\u4e0d\u4ec5\u63d0\u4f9b\u7edf\u8ba1\u4e0a\u5f3a\u5927\u7684\u7279\u5f81\uff0c\u8fd8\u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u4e14\u53ef\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u53ef\u4f5c\u4e3a\u79d1\u5b66\u53d1\u73b0\u7684\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2511.15293", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15293", "abs": "https://arxiv.org/abs/2511.15293", "authors": ["Jia Li", "Zhi Jin", "Kechi Zhang", "Huangzhao Zhang", "Jiaru Qian", "Tiankuo Zhao"], "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development", "comment": null, "summary": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.", "AI": {"tldr": "\u63d0\u51faAutoSW\u613f\u666f\u2014\u2014\u4e00\u79cd\u8fed\u4ee3\u5f0f\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u6790-\u89c4\u5212-\u5b9e\u73b0-\u4ea4\u4ed8\u5faa\u73af\uff0c\u8ba9AI\u7cfb\u7edf\u4f5c\u4e3a\u4eba\u7c7b\u4f19\u4f34\u5c06\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u8f6f\u4ef6\u3002", "motivation": "\u5f53\u524dAI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4e3b\u8981\u4f5c\u4e3a\u5de5\u5177\u6216\u52a9\u624b\uff0c\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u53c2\u4e0e\u3002\u4f5c\u8005\u9884\u89c1AI\u5c06\u53c2\u4e0e\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff0c\u6269\u5c55\u5168\u6808\u5f00\u53d1\u8fb9\u754c\u3002", "method": "\u8bbe\u8ba1\u5206\u6790-\u89c4\u5212-\u5b9e\u73b0-\u4ea4\u4ed8\u5faa\u73af\u7684AutoSW\u8303\u5f0f\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u539f\u578b\u5e76\u6267\u884c\u4ee3\u8868\u6027\u6848\u4f8b\u3002", "result": "AutoSW\u80fd\u591f\u6210\u529f\u4ea4\u4ed8\u53ef\u6267\u884c\u8f6f\u4ef6\uff0c\u4e3a\u771f\u6b63\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002", "conclusion": "AutoSW\u5c55\u793a\u4e86AI\u7cfb\u7edf\u4f5c\u4e3a\u4eba\u7c7b\u4f19\u4f34\u53c2\u4e0e\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u53ef\u884c\u6027\uff0c\u662f\u5b9e\u73b0\u8f6f\u4ef6\u81ea\u52a8\u5316\u5f00\u53d1\u7684\u91cd\u8981\u8fdb\u5c55\u3002", "topic": "swe application"}}
{"id": "2511.15210", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15210", "abs": "https://arxiv.org/abs/2511.15210", "authors": ["Vladislav Pedashenko", "Laida Kushnareva", "Yana Khassan Nibal", "Eduard Tulchinskii", "Kristian Kuznetsov", "Vladislav Zharchinskii", "Yury Maximov", "Irina Piontkovskaya"], "title": "Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story", "comment": null, "summary": "Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text \"representationally simple\" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively \"easy\", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error", "topics": "Error"}}
{"id": "2511.15665", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15665", "abs": "https://arxiv.org/abs/2511.15665", "authors": ["Huixiang Zhang", "Mahzabeen Emu"], "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation", "comment": "This is a preprint version, full paper has been accepted in IEEE CASCON 2025 and will appear on lEEE Xplore", "summary": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u7684\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u89c4\u8303\u8f6c\u5316\u4e3a\u7ec4\u5408\u4f18\u5316\u4efb\u52a1\uff0c\u4f7f\u7528\u91cf\u5b50\u9000\u706b\u5668\u89e3\u51b3\u6d4b\u8bd5\u7528\u4f8b\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u7cbe\u786e\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u6548\u7b80\u6d01\u4ee3\u7801\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u4ee3\u7801\u89c4\u8303\u5230\u4f18\u5316\u4efb\u52a1\u7684\u8f6c\u6362\u95ee\u9898\u3002", "method": "\u6846\u67b6\u9996\u5148\u63d0\u793aLLM\u751f\u6210\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7136\u540e\u5c06\u6d4b\u8bd5\u7528\u4f8b\u6700\u5c0f\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u8fdb\u5236\u4f18\u5316\u6a21\u578b\uff0c\u517c\u5bb9\u7ecf\u5178\u6c42\u89e3\u5668\u548c\u91cf\u5b50\u9000\u706b\u5668\u7b49\u65b0\u5174\u786c\u4ef6\u3002", "result": "\u91cf\u5b50\u9000\u706b\u89e3\u51b3\u6838\u5fc3TCM\u4efb\u52a1\u6bd4\u6a21\u62df\u9000\u706b\u5feb16\u500d\uff0c\u7aef\u5230\u7aef\u6846\u67b6\u51cf\u5c11\u603btoken\u6d88\u801736.5%\uff0c\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u4e0e\u7ec4\u5408\u4f18\u5316\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5f3a\u5927\u534f\u540c\u4f5c\u7528\uff0c\u7a81\u663e\u4e86\u7cbe\u786e\u6a21\u578b\u5236\u5b9a\u7684\u91cd\u8981\u6027\u3002", "topic": "code agent"}}
{"id": "2511.15351", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15351", "abs": "https://arxiv.org/abs/2511.15351", "authors": ["Yifu Guo", "Zishan Xu", "Zhiyuan Yao", "Yuquan Lu", "Jiaye Lin", "Sen Hu", "Zhenheng Tang", "Yingchao Li", "Huacan Wang", "Ronghao Chen"], "title": "Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration", "comment": null, "summary": "Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Octopus\uff1a\u4e00\u79cd\u5177\u6709\u516d\u79cd\u80fd\u529b\u534f\u8c03\u7684\u591a\u6a21\u6001\u4ee3\u7406\u63a8\u7406\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u81ea\u4e3b\u63a2\u7d22\u63a8\u7406\u8def\u5f84\u5e76\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u5b58\u5728\u67b6\u6784\u9650\u5236\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u63a2\u7d22\u591a\u6837\u5316\u63a8\u7406\u8def\u5f84\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u4efb\u52a1\u9700\u6c42\uff0c\u800c\u4eba\u7c7b\u5177\u6709\u4e92\u8865\u7684\u601d\u7ef4\u80fd\u529b\u3002", "method": "\u5b9a\u4e49\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u516d\u4e2a\u6838\u5fc3\u80fd\u529b\uff0c\u6784\u5efa\u4e86Octopus-Bench\u8bc4\u4f30\u57fa\u51c6\uff0cOctopus\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u63a2\u7d22\u5e76\u6839\u636e\u5f53\u524d\u72b6\u6001\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aOctopus\u5728Octopus-Bench\u7684\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u80fd\u529b\u534f\u8c03\u5728\u4ee3\u7406\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2511.15378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15378", "abs": "https://arxiv.org/abs/2511.15378", "authors": ["Trevor McInroe"], "title": "Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents", "comment": null, "summary": "We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.", "AI": {"tldr": "Terra Nova\u662f\u4e00\u4e2a\u53d7\u300a\u6587\u660eV\u300b\u542f\u53d1\u7684\u7efc\u5408\u6311\u6218\u73af\u5883\uff0c\u65e8\u5728\u540c\u65f6\u6d4b\u8bd5\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u4fe1\u7528\u5206\u914d\u3001\u8868\u793a\u5b66\u4e60\u3001\u5de8\u5927\u52a8\u4f5c\u7a7a\u95f4\u7b49\u591a\u4e2a\u7ecf\u5178\u6311\u6218\u4e2d\u7684\u7efc\u5408\u80fd\u529b\u3002", "motivation": "\u5f53\u524dRL\u7814\u7a76\u7f3a\u4e4f\u80fd\u591f\u540c\u65f6\u6d4b\u8bd5\u591a\u4e2a\u4ea4\u4e92\u6311\u6218\u7684\u73af\u5883\uff0c\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u53ea\u662f\u7b80\u5355\u805a\u5408\u4e0d\u76f8\u5173\u7684\u4efb\u52a1\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u300a\u6587\u660eV\u300b\u6e38\u620f\u6784\u5efa\u7efc\u5408\u6311\u6218\u73af\u5883\uff0c\u8981\u6c42\u4ee3\u7406\u5728\u5355\u4e00\u73af\u5883\u4e2d\u5904\u7406\u591a\u4e2a\u76f8\u4e92\u5173\u8054\u7684RL\u6311\u6218\uff0c\u800c\u975e\u7b80\u5355\u7684\u591a\u4efb\u52a1\u805a\u5408\u3002", "result": "\u63d0\u51fa\u4e86Terra Nova\u73af\u5883\u6846\u67b6\uff0c\u5f3a\u8c03\u5bf9\u96c6\u6210\u3001\u957f\u89c6\u91ce\u7406\u89e3\u80fd\u529b\u7684\u6d4b\u8bd5\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u3002", "conclusion": "Terra Nova\u4e3aRL\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u7efc\u5408\u6311\u6218\u6d4b\u8bd5\u5e73\u53f0\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u4ee3\u7406\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u7efc\u5408\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.15407", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.15407", "abs": "https://arxiv.org/abs/2511.15407", "authors": ["Mingyu Zhang", "Lifeng Zhuo", "Tianxi Tan", "Guocan Xie", "Xian Nie", "Yan Li", "Renjie Zhao", "Zizhu He", "Ziyu Wang", "Jiting Cai", "Yong-Lu Li"], "title": "IPR-1: Interactive Physical Reasoner", "comment": "11 pages, 5 figures", "summary": "Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIPR\uff08\u4ea4\u4e92\u5f0f\u7269\u7406\u63a8\u7406\u5668\uff09\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u63a8\u6f14\u6765\u8bc4\u4f30\u548c\u5f3a\u5316VLM\u7b56\u7565\uff0c\u5e76\u5f15\u5165PhysCode\u7269\u7406\u4e2d\u5fc3\u52a8\u4f5c\u7f16\u7801\uff0c\u57281000+\u6e38\u620f\u4e2d\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u4e09\u4e2a\u63a8\u7406\u7ea7\u522b\u4e0a\u8868\u73b0\u7a33\u5065\uff0c\u6574\u4f53\u5339\u914dGPT-5\u5e76\u5728\u597d\u5947\u5fc3\u7ea7\u522b\u8d85\u8d8a\u5b83\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u4f53\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u4ea4\u4e92\u5b66\u4e60\u83b7\u5f97\u7c7b\u4eba\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u66f4\u591a\u7ecf\u9a8c\u4e2d\u6301\u7eed\u6539\u8fdb\u3002", "method": "\u63d0\u51faIPR\u6846\u67b6\uff0c\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u63a8\u6f14\u6765\u8bc4\u5206\u548c\u5f3a\u5316VLM\u7b56\u7565\uff0c\u5f15\u5165PhysCode\u7269\u7406\u4e2d\u5fc3\u52a8\u4f5c\u7f16\u7801\uff0c\u57281000+\u5f02\u8d28\u6e38\u620f\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "IPR\u5728\u4e09\u4e2a\u63a8\u7406\u7ea7\u522b\uff08\u751f\u5b58\u3001\u597d\u5947\u5fc3\u3001\u5b9e\u7528\u6027\uff09\u8868\u73b0\u7a33\u5065\uff0c\u6574\u4f53\u6027\u80fd\u5339\u914dGPT-5\uff0c\u5728\u597d\u5947\u5fc3\u7ea7\u522b\u8d85\u8d8aGPT-5\uff0c\u6027\u80fd\u968f\u8bad\u7ec3\u6e38\u620f\u548c\u4ea4\u4e92\u6b65\u9aa4\u589e\u52a0\u800c\u63d0\u5347\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u672a\u89c1\u6e38\u620f\u3002", "conclusion": "\u7269\u7406\u4e2d\u5fc3\u7684\u4ea4\u4e92\u662f\u6301\u7eed\u6539\u8fdb\u7269\u7406\u63a8\u7406\u7684\u6709\u6548\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15370", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15370", "abs": "https://arxiv.org/abs/2511.15370", "authors": ["Guoqiang Liang", "Jingqian Gong", "Mengxuan Li", "Gege Lin", "Shuo Zhang"], "title": "The Empowerment of Science of Science by Large Language Models: New Tools and Methods", "comment": "The manuscript is currently ongoing the underreview process of the journal of information science", "summary": "Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.", "AI": {"tldr": "\u672c\u6587\u4ece\u7528\u6237\u89d2\u5ea6\u5168\u9762\u56de\u987e\u4e86\u652f\u6491\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u6280\u672f\uff0c\u5305\u62ec\u63d0\u793a\u5de5\u7a0b\u3001\u77e5\u8bc6\u589e\u5f3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5fae\u8c03\u3001\u9884\u8bad\u7ec3\u548c\u5de5\u5177\u5b66\u4e60\uff0c\u5e76\u63a2\u8ba8\u4e86LLMs\u5728\u79d1\u5b66\u8ba1\u91cf\u5b66\u9886\u57df\u7684\u6f5c\u5728\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u3001\u56fe\u50cf\u8bc6\u522b\u53ca\u591a\u6a21\u6001\u4efb\u52a1\u65b9\u9762\u5c55\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\uff0c\u6b63\u671d\u7740AGI\u65b9\u5411\u53d1\u5c55\uff0c\u6210\u4e3a\u5168\u7403\u6280\u672f\u7ade\u4e89\u7684\u6838\u5fc3\u8bae\u9898\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406LLMs\u7684\u6838\u5fc3\u6280\u672f\u53d1\u5c55\u5386\u7a0b\uff0c\u5e76\u7ed3\u5408\u79d1\u5b66\u8ba1\u91cf\u5b66\u53d1\u5c55\u5386\u53f2\uff0c\u63d0\u51fa\u524d\u77bb\u6027\u5e94\u7528\u89c6\u89d2\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8eAI\u4ee3\u7406\u7684\u79d1\u5b66\u8bc4\u4f30\u6a21\u578b\uff0c\u4ee5\u53ca\u5229\u7528LLMs\u8fdb\u884c\u65b0\u7814\u7a76\u524d\u6cbf\u68c0\u6d4b\u548c\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u65b0\u65b9\u6cd5\u3002", "conclusion": "LLMs\u5728\u79d1\u5b66\u8ba1\u91cf\u5b66\u9886\u57df\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\uff0c\u80fd\u591f\u63a8\u52a8\u79d1\u5b66\u8bc4\u4f30\u548c\u7814\u7a76\u524d\u6cbf\u53d1\u73b0\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2511.15456", "categories": ["cs.AI", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2511.15456", "abs": "https://arxiv.org/abs/2511.15456", "authors": ["Qian'ang Mao", "Yuxuan Zhang", "Jiaman Chen", "Wenjun Zhou", "Jiaqi Yan"], "title": "Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining", "comment": "Written in 2025 Q1", "summary": "As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.", "AI": {"tldr": "\u63d0\u51fa\u4e86TIM\u6846\u67b6\uff0c\u901a\u8fc7DeFi\u610f\u56fe\u5206\u7c7b\u6cd5\u548c\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u6765\u63a8\u65ad\u7528\u6237\u4ea4\u6613\u610f\u56fe\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "DeFi\u4ea4\u6613\u4e2d\u7528\u6237\u610f\u56fe\u7406\u89e3\u81f3\u5173\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5ea6\u8bed\u4e49\u6d1e\u5bdf\uff0c\u9700\u8981\u89e3\u51b3\u590d\u6742\u667a\u80fd\u5408\u7ea6\u4ea4\u4e92\u3001\u591a\u56e0\u7d20\u5f71\u54cd\u548c\u4e0d\u900f\u660e\u65e5\u5fd7\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u624e\u6839\u7406\u8bba\u7684DeFi\u610f\u56fe\u5206\u7c7b\u6cd5\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\uff0c\u5305\u62ec\u5143\u7ea7\u89c4\u5212\u5668\u52a8\u6001\u534f\u8c03\u9886\u57df\u4e13\u5bb6\u5206\u89e3\u4efb\u52a1\uff0c\u95ee\u9898\u6c42\u89e3\u5668\u5904\u7406\u591a\u6a21\u6001\u94fe\u4e0a/\u94fe\u4e0b\u6570\u636e\uff0c\u8ba4\u77e5\u8bc4\u4f30\u5668\u51cf\u8f7bLLM\u5e7b\u89c9\u5e76\u786e\u4fdd\u53ef\u9a8c\u8bc1\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTIM\u663e\u8457\u4f18\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3001\u5355\u4e00LLM\u548c\u5355\u4e00\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u80fd\u591f\u5206\u6790\u610f\u56fe\u63a8\u7406\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u66f4\u53ef\u9760\u5730\u7406\u89e3DeFi\u4e2d\u7528\u6237\u52a8\u673a\uff0c\u4e3a\u590d\u6742\u533a\u5757\u94fe\u6d3b\u52a8\u63d0\u4f9b\u60c5\u5883\u611f\u77e5\u89e3\u91ca\u3002", "topic": "agent analysis"}}
{"id": "2511.15392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15392", "abs": "https://arxiv.org/abs/2511.15392", "authors": ["Sirui Chen", "Mengshi Zhao", "Lei Xu", "Yuying Zhao", "Beier Zhu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "title": "DEPO: Dual-Efficiency Preference Optimization for LLM Agents", "comment": "Accepted to AAAI 2026", "summary": "Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.", "AI": {"tldr": "DEPO\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6548\u7387\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5956\u52b1\u7b80\u6d01\u54cd\u5e94\u548c\u66f4\u5c11\u884c\u52a8\u6b65\u9aa4\uff0c\u663e\u8457\u51cf\u5c11LLM\u4ee3\u7406\u7684token\u4f7f\u7528\u548c\u6b65\u9aa4\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u5f80\u5f80\u5bfc\u81f4\u601d\u7ef4\u94fe\u8fc7\u957f\uff0c\u5f71\u54cd\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u6548\u7387\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9LLM\u4ee3\u7406\u6548\u7387\u7684\u7cfb\u7edf\u5b9a\u4e49\u3002", "method": "\u63d0\u51fa\u53cc\u6548\u7387\u5b9a\u4e49\uff08\u6b65\u9aa4\u7ea7\u6548\u7387\u548c\u8f68\u8ff9\u7ea7\u6548\u7387\uff09\uff0c\u5e76\u5f00\u53d1DEPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u4f18\u5316\u8054\u5408\u5956\u52b1\u7b80\u6d01\u54cd\u5e94\u548c\u66f4\u5c11\u884c\u52a8\u6b65\u9aa4\u3002", "result": "\u5728WebShop\u548cBabyAI\u4e0a\uff0cDEPO\u51cf\u5c11token\u4f7f\u7528\u8fbe60.9%\uff0c\u6b65\u9aa4\u51cf\u5c11\u8fbe26.9%\uff0c\u6027\u80fd\u63d0\u5347\u8fbe29.3%\u3002\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u80fd\u6cdb\u5316\uff0c\u4ec5\u752825%\u6570\u636e\u8bad\u7ec3\u4ecd\u4fdd\u6301\u6548\u7387\u589e\u76ca\u3002", "conclusion": "DEPO\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.15593", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15593", "abs": "https://arxiv.org/abs/2511.15593", "authors": ["Alexis Audran-Reiss", "Jordi Armengol Estap\u00e9", "Karen Hambardzumyan", "Amar Budhiraja", "Martin Josifoski", "Edan Toledo", "Rishi Hazra", "Despoina Magka", "Michael Shvartsman", "Parth Pathak", "Justine T Kao", "Lucia Cipolina-Kun", "Bhavul Gauri", "Jean-Christophe Gagnon-Audet", "Emanuel Tewolde", "Jenny Zhang", "Taco Cohen", "Yossi Adi", "Tatiana Shavrina", "Yoram Bachrach"], "title": "What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity", "comment": null, "summary": "AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AI\u7814\u7a76\u4ee3\u7406\u4e2d\u6784\u601d\u591a\u6837\u6027\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728MLE-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6784\u601d\u591a\u6837\u6027\u66f4\u9ad8\u7684\u4ee3\u7406\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "motivation": "AI\u7814\u7a76\u4ee3\u7406\u6709\u671b\u52a0\u901f\u79d1\u5b66\u8fdb\u6b65\uff0c\u4f46\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u6784\u601d\u591a\u6837\u6027\u5728\u4ee3\u7406\u6027\u80fd\u4e2d\u7684\u4f5c\u7528\u3002", "method": "1) \u5206\u6790MLE-bench\u4e0a\u4e0d\u540c\u6a21\u578b\u548c\u4ee3\u7406\u6846\u67b6\u7684\u8f68\u8ff9\uff1b2) \u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\u8c03\u6574\u6784\u601d\u591a\u6837\u6027\u7a0b\u5ea6\uff1b3) \u4f7f\u7528\u9664\u6807\u51c6\u5956\u724c\u8bc4\u5206\u5916\u7684\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u548c\u4ee3\u7406\u6846\u67b6\u4ea7\u751f\u4e0d\u540c\u7a0b\u5ea6\u7684\u6784\u601d\u591a\u6837\u6027\uff0c\u6027\u80fd\u66f4\u9ad8\u7684\u4ee3\u7406\u5f80\u5f80\u5177\u6709\u66f4\u9ad8\u7684\u6784\u601d\u591a\u6837\u6027\u3002\u63a7\u5236\u5b9e\u9a8c\u8bc1\u5b9e\u66f4\u9ad8\u7684\u6784\u601d\u591a\u6837\u6027\u5e26\u6765\u66f4\u5f3a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6784\u601d\u591a\u6837\u6027\u662f\u5f71\u54cdAI\u7814\u7a76\u4ee3\u7406\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u9ad8\u6784\u601d\u591a\u6837\u6027\u53ef\u4ee5\u663e\u8457\u6539\u5584\u4ee3\u7406\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2511.15408", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2511.15408", "abs": "https://arxiv.org/abs/2511.15408", "authors": ["Shanlin Zhou", "Xinpeng Wang", "Jianxun Lian", "Zhenghao Liu", "Laks V. S. Lakshmanan", "Xiaoyuan Yi", "Yongtao Hao"], "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework", "comment": "13 pages,9 figures. This work has been submitted to the IEEE for possible publication", "summary": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.", "AI": {"tldr": "\u63d0\u51fa\u4e86NAMEGEn\u591a\u667a\u80fd\u4f53\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4e2d\u6587\u8d77\u540d\u8fd9\u4e00\u77ed\u6587\u672c\u521b\u610f\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u76ee\u6807\u63d0\u53d6\u3001\u540d\u79f0\u751f\u6210\u548c\u8bc4\u4f30\u8fc7\u7a0b\u6765\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u5e76\u751f\u6210\u51c6\u786e\u89e3\u91ca\u3002", "motivation": "\u521b\u610f\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9762\u4e34\u591a\u76ee\u6807\u7075\u6d3b\u6027\u548c\u89e3\u91ca\u590d\u6742\u6027\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u77ed\u6587\u672c\u751f\u6210\u4e2d\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u4e2a\u6027\u5316\u3001\u7ec6\u7c92\u5ea6\u548c\u591a\u5143\u5316\u7684\u7528\u6237\u9700\u6c42\uff0c\u5e76\u7406\u89e3\u9690\u542b\u610f\u4e49\u4ee5\u589e\u5f3a\u7528\u6237\u611f\u77e5\u3002", "method": "NAMEGEn\u591a\u667a\u80fd\u4f53\u4f18\u5316\u6846\u67b6\uff0c\u5305\u542b\u76ee\u6807\u63d0\u53d6\u3001\u540d\u79f0\u751f\u6210\u548c\u8bc4\u4f30\u4e09\u4e2a\u8fed\u4ee3\u6b65\u9aa4\uff1b\u6784\u5efa\u4e8617k+\u53e4\u5178\u8bd7\u6b4c\u8bed\u6599\u5e93\u589e\u5f3a\u7f8e\u5b66\u6027\uff0c\u5e76\u5f15\u5165CBNames\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNAMEGEn\u80fd\u6709\u6548\u751f\u6210\u6ee1\u8db3\u591a\u6837\u5316\u4e2a\u6027\u5316\u9700\u6c42\u7684\u521b\u610f\u540d\u79f0\u5e76\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u89e3\u91ca\uff0c\u5728\u516d\u4e2a\u4e0d\u540cLLM\u57fa\u7ebf\u7684\u57fa\u51c6\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u3002", "conclusion": "NAMEGEn\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u521b\u610f\u77ed\u6587\u672c\u751f\u6210\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u548c\u89e3\u91ca\u590d\u6742\u6027\u6311\u6218\uff0c\u4e3a\u4e2a\u6027\u5316\u521b\u610f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.15137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15137", "abs": "https://arxiv.org/abs/2511.15137", "authors": ["Xiaoxuan Wang", "Bo Liu", "Song Jiang", "Jingzhou Liu", "Jingyuan Qi", "Xia Chen", "Baosheng He"], "title": "From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs", "comment": null, "summary": "The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86GRPO-Verif\u7b97\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u635f\u5931\u51fd\u6570\u8054\u5408\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u751f\u6210\u548c\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\uff0c\u53ef\u8c03\u8282\u9a8c\u8bc1\u4fe1\u53f7\u6743\u91cd", "motivation": "\u5c3d\u7ba1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4f46LLMs\u4ecd\u96be\u4ee5\u6301\u7eed\u9a8c\u8bc1\u81ea\u8eab\u63a8\u7406\u8f68\u8ff9\uff0c\u9700\u8981\u589e\u5f3a\u5176\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u5e76\u63a2\u7d22\u8fd9\u79cd\u80fd\u529b\u662f\u5426\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u63a8\u7406\u6027\u80fd", "method": "GRPO-Verif\u7b97\u6cd5\uff0c\u5728\u7edf\u4e00\u635f\u5931\u51fd\u6570\u4e2d\u8054\u5408\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u751f\u6210\u548c\u81ea\u6211\u9a8c\u8bc1\uff0c\u5305\u542b\u53ef\u8c03\u8282\u9a8c\u8bc1\u4fe1\u53f7\u6743\u91cd\u7684\u8d85\u53c2\u6570", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b", "conclusion": "GRPO-Verif\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u63a8\u7406\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2511.15190", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15190", "abs": "https://arxiv.org/abs/2511.15190", "authors": ["Yuxuan Gu", "Weimin Bai", "Yifei Wang", "Weijian Luo", "He Sun"], "title": "Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning", "comment": null, "summary": "Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.", "AI": {"tldr": "MARVAL\u901a\u8fc7\u53d8\u5206\u84b8\u998f\u5c06\u63a9\u7801\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u7684\u6269\u6563\u94fe\u538b\u7f29\u4e3a\u5355\u6b65\u751f\u6210\uff0c\u5b9e\u73b030\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6837\u672c\u8d28\u91cf\uff0c\u5e76\u4f7f\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u53d8\u5f97\u5b9e\u7528\u3002", "motivation": "\u89e3\u51b3\u63a9\u7801\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5176\u5206\u5c42\u63a8\u7406\u673a\u5236\uff08\u5916\u5faa\u73af\u89e3\u63a9\u7801\u548c\u5185\u5faa\u73af\u6269\u6563\u53bb\u566a\uff09\u4e0d\u4ec5\u5f71\u54cd\u751f\u6210\u6548\u7387\uff0c\u8fd8\u963b\u788d\u4e86\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faMARVAL\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u53d8\u5206\u76ee\u6807\u5c06\u63a9\u7801\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5355\u6b65\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u7075\u6d3b\u7684\u81ea\u56de\u5f52\u89e3\u63a9\u7801\u987a\u5e8f\uff1b\u5e76\u5f00\u53d1MARVAL-RL\u8fdb\u884c\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u3002", "result": "\u5728ImageNet 256*256\u4e0a\uff0cMARVAL-Huge\u8fbe\u5230FID 2.00\uff0c\u76f8\u6bd4MAR-diffusion\u52a0\u901f30\u500d\u4ee5\u4e0a\uff1bMARVAL-RL\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u63d0\u5347CLIP\u548c\u56fe\u50cf\u5956\u52b1\u5206\u6570\u3002", "conclusion": "MARVAL\u4e3a\u63a9\u7801\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u7684\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b9e\u7528\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u91c7\u6837\u548c\u66f4\u597d\u7684\u504f\u597d\u5bf9\u9f50\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15208", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15208", "abs": "https://arxiv.org/abs/2511.15208", "authors": ["Ranfei Chen", "Ming Chen", "Kaifei Wang"], "title": "Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured \"zones of confusion\": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u8f68\u8ff9\u7b56\u7565\u4f18\u5316\uff08ATPO\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u548c\u91cd\u5206\u914d\u68af\u5ea6\u66f4\u65b0\u5230\u9ad8\u5f71\u54cd\u529b\u7684\u53bb\u566a\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8f68\u8ff9\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5747\u5300\u5206\u914d\u7b56\u7565\u68af\u5ea6\u5230\u6240\u6709\u53bb\u566a\u6b65\u9aa4\uff0c\u9690\u542b\u5047\u8bbe\u6240\u6709\u6b65\u9aa4\u540c\u7b49\u91cd\u8981\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u53d1\u73b0\u8f68\u8ff9\u4e2d\u5b58\u5728\u7ed3\u6784\u5316\u7684\"\u6df7\u6dc6\u533a\"\u2014\u2014\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u7a33\u5b9a\u7684\u77ac\u65f6\u5cf0\u503c\uff0c\u8fd9\u4e9b\u533a\u57df\u5f3a\u70c8\u9884\u6d4b\u6700\u7ec8\u6210\u529f\u6216\u5931\u8d25\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u7f6e\u4fe1\u5ea6-\u8fb9\u7f18\u4e0d\u786e\u5b9a\u6027\u548c\u71b5\u53d8\u5316\u7387\u7b49\u6b65\u9aa4\u7ea7\u6307\u6807\u5206\u6790\u8f68\u8ff9\uff0c\u8bc6\u522b\u9ad8\u5f71\u54cd\u529b\u6b65\u9aa4\u3002\u63d0\u51faATPO\u65b9\u6cd5\uff0c\u91c7\u7528\u6df7\u5408RoEC+CM\u89c4\u5219\u52a8\u6001\u91cd\u5206\u914d\u68af\u5ea6\u66f4\u65b0\u5230\u8fd9\u4e9b\u5173\u952e\u6b65\u9aa4\uff0c\u800c\u4e0d\u6539\u53d8RL\u76ee\u6807\u3001\u5956\u52b1\u6216\u8ba1\u7b97\u9884\u7b97\u3002", "result": "ATPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u8bc1\u5b9e\u5229\u7528\u8f68\u8ff9\u52a8\u6001\u662f\u63a8\u8fdbdLLM RL\u7684\u5173\u952e\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u548c\u91cd\u5206\u914d\u68af\u5ea6\u66f4\u65b0\u5230\u9ad8\u5f71\u54cd\u529b\u6b65\u9aa4\uff0cATPO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8f68\u8ff9\u52a8\u6001\u5206\u6790\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15248", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15248", "abs": "https://arxiv.org/abs/2511.15248", "authors": ["Kai Yang", "Xin Xu", "Yangkun Chen", "Weijie Liu", "Jiafei Lyu", "Zichuan Lin", "Deheng Ye", "Saiyong Yang"], "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control", "comment": null, "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.", "AI": {"tldr": "\u63d0\u51faEntroPIC\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u4f8b-\u79ef\u5206\u63a7\u5236\u52a8\u6001\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u7684\u635f\u5931\u7cfb\u6570\uff0c\u7a33\u5b9a\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u71b5\u503c\uff0c\u9632\u6b62\u6a21\u578b\u9677\u5165\u6b21\u4f18\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ef4\u6301\u9002\u5f53\u7684\u71b5\u6c34\u5e73\uff0c\u56e0\u4e3a\u6b63\u8d1f\u6837\u672c\u4ee5\u4e0d\u540c\u65b9\u5f0f\u5f71\u54cd\u71b5\u503c\u53d8\u5316\uff0c\u5bfc\u81f4\u63a2\u7d22\u4e0d\u7a33\u5b9a\u548c\u8fc7\u65e9\u6536\u655b\u5230\u6b21\u4f18\u89e3\u3002", "method": "\u57fa\u4e8e\u6bd4\u4f8b-\u79ef\u5206\u63a7\u5236\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u7684\u635f\u5931\u7cfb\u6570\uff0c\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u71b5\u503c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u7ef4\u6301\u671f\u671b\u7684\u71b5\u6c34\u5e73\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7a33\u5b9a\u4e14\u6700\u4f18\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "conclusion": "EntroPIC\u65b9\u6cd5\u80fd\u6709\u6548\u63a7\u5236\u71b5\u503c\uff0c\u786e\u4fdd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u4fdd\u6301\u7a33\u5b9a\u63a2\u7d22\u548c\u6301\u7eed\u8fdb\u6b65\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15652", "abs": "https://arxiv.org/abs/2511.15652", "authors": ["Kim N. Nolle", "Ivana Dusparic", "Rhodri Cusack", "Vinny Cahill"], "title": "Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges", "comment": "5 pages, 5 figures, Accepted to RLDM 2025", "summary": "Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.\n  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.\n  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u73af\u5883\u62bd\u8c61\u3001\u8d85\u53c2\u6570\u654f\u611f\u6027\u3001\u707e\u96be\u6027\u9057\u5fd8\u548c\u795e\u7ecf\u7f51\u7edc\u5bb9\u91cf\u5229\u7528\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u5728\u975e\u5e73\u7a33\u73af\u5883\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u76ee\u524d\u5c06\u6301\u7eed\u5b66\u4e60\u6210\u529f\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22CRL\u7684\u5b9e\u9645\u6311\u6218\u3002", "method": "\u5728\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8ba9\u667a\u80fd\u4f53\u4f9d\u6b21\u5b66\u4e60\u56db\u79cd\u4e0d\u540c\u89d2\u5ea6\u7684\u505c\u8f66\u573a\u666f\uff0c\u6784\u5efa\u6301\u7eed\u5b66\u4e60\u73af\u5883\u8fdb\u884c\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86CRL\u7684\u56db\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u73af\u5883\u62bd\u8c61\u8868\u793a\u56f0\u96be\u3001\u8d85\u53c2\u6570\u8fc7\u5ea6\u654f\u611f\u3001\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u4ee5\u53ca\u795e\u7ecf\u7f51\u7edc\u5bb9\u91cf\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "CRL\u9762\u4e34\u4e25\u5cfb\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\uff0c\u5e76\u5f00\u5c55\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u795e\u7ecf\u79d1\u5b66\u7b49\u8de8\u5b66\u79d1\u7814\u7a76\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15669", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.15669", "abs": "https://arxiv.org/abs/2511.15669", "authors": ["Cheng Yin", "Yankai Lin", "Wang Xu", "Sikyuen Tam", "Xiangrui Zeng", "Zhiyuan Liu", "Zhouping Yin"], "title": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models", "comment": "16 pages, 6 figures, conference", "summary": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.", "AI": {"tldr": "DeepThinkVLA\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u89e3\u7801\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3VLA\u6a21\u578b\u4e2d\u601d\u7ef4\u4e0e\u52a8\u4f5c\u7684\u51b2\u7a81\uff0c\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523097.0%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4f7f\u7528\u5355\u4e00\u81ea\u56de\u5f52\u89e3\u7801\u5668\u540c\u65f6\u5904\u7406\u987a\u5e8f\u63a8\u7406\u548c\u9ad8\u7ef4\u5e76\u884c\u52a8\u4f5c\uff0c\u5bfc\u81f4\u8fd0\u52a8\u63a7\u5236\u6027\u80fd\u4e0b\u964d\u548c\u601d\u7ef4-\u52a8\u4f5c\u56e0\u679c\u8054\u7cfb\u8584\u5f31\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6ce8\u610f\u529b\u89e3\u7801\u5668\uff1a\u7528\u56e0\u679c\u6ce8\u610f\u529b\u751f\u6210\u987a\u5e8fCoT\u63a8\u7406\uff0c\u7136\u540e\u5207\u6362\u5230\u53cc\u5411\u6ce8\u610f\u529b\u5e76\u884c\u89e3\u7801\u52a8\u4f5c\u5411\u91cf\uff1b\u914d\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u7528SFT\u6559\u6388\u57fa\u7840\u63a8\u7406\uff0c\u518d\u7528RL\u901a\u8fc7\u4efb\u52a1\u6210\u529f\u5956\u52b1\u5bf9\u9f50\u63a8\u7406-\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523097.0%\u7684\u6210\u529f\u7387\uff0c\u6df7\u5408\u67b6\u6784\u6bd4\u6807\u51c6\u89e3\u7801\u5668\u63d0\u534715.5%\uff0cRL\u9636\u6bb5\u989d\u5916\u63d0\u4f9b2%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DeepThinkVLA\u901a\u8fc7\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u4e2d\u601d\u7ef4\u4e0e\u52a8\u4f5c\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15694", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15694", "abs": "https://arxiv.org/abs/2511.15694", "authors": ["Medha Kumar", "Zifei Xu", "Xin Wang", "Tristan Webb"], "title": "The Impact of Quantization on Large Reasoning Model Reinforcement Learning", "comment": "Accepted to the NeurIPS 2025 Efficient Reasoning Workshop", "summary": "Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.", "AI": {"tldr": "\u91cf\u5316\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5bf9\u5927\u63a8\u7406\u6a21\u578b\u6709\u5bb3\uff0c\u800c\u8bad\u7ec3\u540e\u91cf\u5316\u548cQLoRA\u65b9\u6cd5\u8868\u73b0\u66f4\u597d", "motivation": "\u7814\u7a76\u91cf\u5316\u5982\u4f55\u5f71\u54cd\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u56e0\u4e3a\u867d\u7136\u91cf\u5316\u5728\u5fae\u8c03\u80cc\u666f\u4e0b\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e", "method": "\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u8bad\u7ec3\u540e\u91cf\u5316\u4e0e\u91cf\u5316\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u63a8\u7406\u6027\u80fd", "result": "\u53d1\u73b0\u8bad\u7ec3\u540e\u91cf\u5316\u6a21\u578b\u4e0e\u91cf\u5316\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u63a8\u7406\u6027\u80fd\u5dee\u8ddd\uff0c\u91cf\u5316\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd", "conclusion": "\u91cf\u5316\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5bf9\u5927\u63a8\u7406\u6a21\u578b\u6709\u5bb3\uff0c\u800c\u8bad\u7ec3\u540e\u91cf\u5316\u548cQLoRA\u65b9\u6cd5\u80fd\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.da3f171e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fproductivity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/btl4YJp2vnCBgQ27vAD_Kg_QgehD-hPOmmooLVvkVPA=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fproductivity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/btl4YJp2vnCBgQ27vAD_Kg_QgehD-hPOmmooLVvkVPA=430", "authors": ["TLDR Newsletter"], "title": "The productivity impact of coding agents", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fproductivity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/btl4YJp2vnCBgQ27vAD_Kg_QgehD-hPOmmooLVvkVPA=430", "summary": "The productivity impact of coding agents (3 minute read) A large study of tens of thousands of Cursor users found that after agents became the default, organizations merged 39% more PRs with no increase in revert or bug-fix rates. Senior developers accepted agent-written code more often, planned more before coding, and generally used agents more effectively, while most user requests (61%) were for code implementation", "source": "tldr", "AI": {"tldr": "\u5bf9Cursor\u7528\u6237\u7684\u5e7f\u6cdb\u7814\u7a76\u53d1\u73b0\uff0cAI\u7f16\u7801\u52a9\u624b\u6210\u4e3a\u9ed8\u8ba4\u8bbe\u7f6e\u540e\uff0c\u7ec4\u7ec7\u5408\u5e76\u7684PR\u6570\u91cf\u589e\u52a0\u4e8639%\uff0c\u4e14\u6ca1\u6709\u589e\u52a0\u4ee3\u7801\u56de\u6eda\u6216bug\u4fee\u590d\u7387\u3002\u9ad8\u7ea7\u5f00\u53d1\u8005\u66f4\u9891\u7e41\u63a5\u53d7AI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u5728\u7f16\u7801\u524d\u8fdb\u884c\u66f4\u591a\u89c4\u5212\uff0c\u5e76\u80fd\u66f4\u6709\u6548\u5730\u4f7f\u7528AI\u52a9\u624b\u3002", "motivation": "\u7814\u7a76AI\u7f16\u7801\u52a9\u624b\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u6210\u4e3a\u9ed8\u8ba4\u5de5\u5177\u540e\u7684\u4f7f\u7528\u6548\u679c\u548c\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u5f00\u53d1\u8005\u7684\u4f7f\u7528\u5dee\u5f02\u3002", "method": "\u5bf9\u6570\u4e07\u540dCursor\u7528\u6237\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u5206\u6790AI\u52a9\u624b\u6210\u4e3a\u9ed8\u8ba4\u8bbe\u7f6e\u524d\u540e\u7684PR\u5408\u5e76\u6570\u91cf\u3001\u4ee3\u7801\u56de\u6eda\u7387\u3001bug\u4fee\u590d\u7387\u7b49\u6307\u6807\uff0c\u5e76\u8c03\u67e5\u4e0d\u540c\u7ea7\u522b\u5f00\u53d1\u8005\u7684\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u7ec4\u7ec7PR\u5408\u5e76\u6570\u91cf\u589e\u52a039%\uff0c\u6ca1\u6709\u89c2\u5bdf\u5230\u4ee3\u7801\u56de\u6eda\u6216bug\u4fee\u590d\u7387\u4e0a\u5347\u3002\u9ad8\u7ea7\u5f00\u53d1\u8005\u66f4\u613f\u610f\u63a5\u53d7AI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u5728\u7f16\u7801\u524d\u8fdb\u884c\u66f4\u591a\u89c4\u5212\uff0c61%\u7684\u7528\u6237\u8bf7\u6c42\u662f\u9488\u5bf9\u4ee3\u7801\u5b9e\u73b0\u3002", "conclusion": "AI\u7f16\u7801\u52a9\u624b\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u7279\u522b\u662f\u5bf9\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\u6548\u679c\u66f4\u660e\u663e\uff0c\u4e14\u4e0d\u4f1a\u964d\u4f4e\u4ee3\u7801\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "tldr.2511.84ac53d1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2Fblog%2Fintroducing-google-antigravity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/z4xOVIYNTf_SUcGgzLTrNKf7Q8NIDdH-MNvw_gx5AtE=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2Fblog%2Fintroducing-google-antigravity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/z4xOVIYNTf_SUcGgzLTrNKf7Q8NIDdH-MNvw_gx5AtE=430", "authors": ["TLDR Newsletter"], "title": "Google Antigravity", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2Fblog%2Fintroducing-google-antigravity%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/z4xOVIYNTf_SUcGgzLTrNKf7Q8NIDdH-MNvw_gx5AtE=430", "summary": "Google Antigravity (Resource) Google Antigravity is an agent-first development platform that pairs an AI-powered IDE with autonomous agents capable of planning and executing complex software tasks across multiple surfaces. It adds task-level transparency, async agent management, easy feedback, and built-in learning to support higher-autonomy coding with models like Gemini 3.", "source": "tldr", "AI": {"tldr": "Google Antigravity\u662f\u4e00\u4e2a\u9762\u5411\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u5e73\u53f0\uff0c\u96c6\u6210\u4e86AI\u9a71\u52a8\u7684IDE\u548c\u80fd\u591f\u8de8\u591a\u4e2a\u754c\u9762\u89c4\u5212\u6267\u884c\u590d\u6742\u8f6f\u4ef6\u4efb\u52a1\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u4efb\u52a1\u7ba1\u7406\u7684\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u66f4\u9ad8\u81ea\u4e3b\u6027\u7684\u7f16\u7801\u652f\u6301\u3002", "method": "\u91c7\u7528AI\u9a71\u52a8\u7684IDE\u4e0e\u81ea\u4e3b\u667a\u80fd\u4f53\u76f8\u7ed3\u5408\uff0c\u652f\u6301\u4efb\u52a1\u7ea7\u900f\u660e\u5ea6\u3001\u5f02\u6b65\u667a\u80fd\u4f53\u7ba1\u7406\u3001\u7b80\u6613\u53cd\u9988\u548c\u5185\u7f6e\u5b66\u4e60\u673a\u5236\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u652f\u6301Gemini 3\u7b49\u6a21\u578b\u7684\u667a\u80fd\u4f53\u4f18\u5148\u5f00\u53d1\u5e73\u53f0\u3002", "conclusion": "\u8be5\u5e73\u53f0\u901a\u8fc7\u96c6\u6210AI IDE\u548c\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u548c\u66f4\u9ad8\u7a0b\u5ea6\u7684\u81ea\u52a8\u5316\u3002", "topic": "swe application"}}
{"id": "tldr.2511.06778a66", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/CqTqgksp6mPGgf-OlqZOQD_u1cpTMSxy-TvUO5Zwlqk=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/CqTqgksp6mPGgf-OlqZOQD_u1cpTMSxy-TvUO5Zwlqk=430", "authors": ["TLDR Newsletter"], "title": "How Dash uses context engineering for smarter AI", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdevops/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/CqTqgksp6mPGgf-OlqZOQD_u1cpTMSxy-TvUO5Zwlqk=430", "summary": "How Dash uses context engineering for smarter AI (7 minute read) Dropbox Dash has evolved from a search system into an agentic AI to better interpret, summarize, and act on information, requiring a shift towards context engineering. Dash curates context through retrieval consolidation, relevant context filtering, and specialized task agents to improve the AI's decision-making, addressing issues like \"analysis paralysis\" and \"context rot\" that arise with too many tools and excessive data.", "source": "tldr", "AI": {"tldr": "Dropbox Dash\u4ece\u641c\u7d22\u7cfb\u7edf\u6f14\u53d8\u4e3a\u667a\u80fdAI\u4ee3\u7406\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6765\u66f4\u597d\u5730\u89e3\u91ca\u3001\u603b\u7ed3\u548c\u64cd\u4f5c\u4fe1\u606f\uff0c\u89e3\u51b3\u5de5\u5177\u8fc7\u591a\u548c\u6570\u636e\u8fc7\u8f7d\u5bfc\u81f4\u7684'\u5206\u6790\u762b\u75ea'\u548c'\u4e0a\u4e0b\u6587\u8150\u5316'\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5de5\u5177\u548c\u6570\u636e\u7684\u6fc0\u589e\uff0cAI\u7cfb\u7edf\u9762\u4e34'\u5206\u6790\u762b\u75ea'\uff08\u4fe1\u606f\u8fc7\u591a\u96be\u4ee5\u51b3\u7b56\uff09\u548c'\u4e0a\u4e0b\u6587\u8150\u5316'\uff08\u4fe1\u606f\u8fc7\u65f6\u6216\u4e0d\u76f8\u5173\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5305\u62ec\u68c0\u7d22\u6574\u5408\u3001\u76f8\u5173\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u548c\u4e13\u7528\u4efb\u52a1\u4ee3\u7406\uff0c\u6765\u4f18\u5316AI\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "Dash\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6210\u529f\u63d0\u5347\u4e86AI\u5bf9\u4fe1\u606f\u7684\u89e3\u91ca\u3001\u603b\u7ed3\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u6539\u5584\u4e86\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u89e3\u51b3AI\u7cfb\u7edf\u4e2d\u4fe1\u606f\u8fc7\u8f7d\u548c\u51b3\u7b56\u56f0\u96be\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u53ef\u4ee5\u663e\u8457\u63d0\u5347AI\u4ee3\u7406\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.283eed5f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/LGVU2aOSDxKzzsz2GM1lMpF-AcZTL86kpSouHzFemNg=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/LGVU2aOSDxKzzsz2GM1lMpF-AcZTL86kpSouHzFemNg=430", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a9c026fa0-5c1cd56d-afab-40ce-ae87-cc4118084616-000000/LGVU2aOSDxKzzsz2GM1lMpF-AcZTL86kpSouHzFemNg=430", "summary": "GKE: From containers to agents, the unified platform for every modern workload (5 minute read) Google Kubernetes Engine marked its 10th anniversary with major innovations to support AI and agentic workloads.", "source": "tldr", "AI": {"tldr": "Google Kubernetes Engine (GKE) \u572810\u5468\u5e74\u4e4b\u9645\u63a8\u51fa\u91cd\u5927\u521b\u65b0\uff0c\u652f\u6301AI\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4ece\u5bb9\u5668\u6269\u5c55\u5230\u667a\u80fd\u4f53\u5e73\u53f0\u3002", "motivation": "\u968f\u7740AI\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u7684\u666e\u53ca\uff0c\u9700\u8981\u7edf\u4e00\u7684\u5e73\u53f0\u6765\u652f\u6301\u8fd9\u4e9b\u73b0\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\uff0cGKE\u65e8\u5728\u4ece\u5bb9\u5668\u5e73\u53f0\u6f14\u8fdb\u4e3a\u652f\u6301AI\u548c\u667a\u80fd\u4f53\u7684\u7edf\u4e00\u5e73\u53f0\u3002", "method": "\u901a\u8fc7GKE\u5e73\u53f0\u7684\u521b\u65b0\u5347\u7ea7\uff0c\u6269\u5c55\u5176\u80fd\u529b\u4ee5\u652f\u6301AI\u6a21\u578b\u90e8\u7f72\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u5e73\u53f0\u89e3\u51b3\u65b9\u6848\u3002", "result": "GKE\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5bb9\u5668\u5e73\u53f0\u5411\u652f\u6301AI\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7edf\u4e00\u5e73\u53f0\u7684\u8f6c\u578b\uff0c\u4e3a\u73b0\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u5168\u9762\u652f\u6301\u3002", "conclusion": "GKE\u7684\u6f14\u8fdb\u5c55\u793a\u4e86\u5bb9\u5668\u5e73\u53f0\u5411AI\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u652f\u6301\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u4e3a\u73b0\u4ee3\u5e94\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "swe application"}}
{"id": "tldr.2511.1849a203", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2F%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/aRfKW5_33RVTctfYvipZXtST9gaDvjmcyhZR9pETlE8=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2F%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/aRfKW5_33RVTctfYvipZXtST9gaDvjmcyhZR9pETlE8=432", "authors": ["TLDR Newsletter"], "title": "Google Antigravity", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fantigravity.google%2F%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/aRfKW5_33RVTctfYvipZXtST9gaDvjmcyhZR9pETlE8=432", "summary": "Google Antigravity (Website) Google's Antigravity is a new AI-powered agentic development platform built on Gemini 3. Antigravity has both an AI-powered IDE experience (\"Editor view\") and an agent-first interface for orchestrating multiple agents (\"Manager surface\").", "source": "tldr", "AI": {"tldr": "Google Antigravity\u662f\u4e00\u4e2a\u57fa\u4e8eGemini 3\u6784\u5efa\u7684AI\u9a71\u52a8\u7684\u4ee3\u7406\u5f00\u53d1\u5e73\u53f0\uff0c\u5305\u542bAI\u589e\u5f3a\u7684IDE\u4f53\u9a8c\u548c\u9762\u5411\u591a\u4ee3\u7406\u7f16\u6392\u7684\u4ee3\u7406\u4f18\u5148\u754c\u9762\u3002", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684AI\u9a71\u52a8\u5f00\u53d1\u5de5\u5177\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u667a\u80fd\u4ee3\u7406\u6765\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u57fa\u4e8eGemini 3\u7684AI\u5e73\u53f0\uff0c\u63d0\u4f9b\u4e24\u79cd\u4e3b\u8981\u754c\u9762\uff1aEditor view\uff08AI\u589e\u5f3a\u7684IDE\u4f53\u9a8c\uff09\u548cManager surface\uff08\u591a\u4ee3\u7406\u7f16\u6392\u7684\u4ee3\u7406\u4f18\u5148\u754c\u9762\uff09\u3002", "result": "\u5f00\u53d1\u4e86Antigravity\u5e73\u53f0\uff0c\u6574\u5408\u4e86AI\u9a71\u52a8\u7684\u4ee3\u7801\u7f16\u8f91\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u529f\u80fd\u3002", "conclusion": "Antigravity\u4ee3\u8868\u4e86AI\u9a71\u52a8\u8f6f\u4ef6\u5f00\u53d1\u7684\u65b0\u65b9\u5411\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u548cAI\u589e\u5f3a\u5de5\u5177\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2511.edd732b0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbdtechtalks.com%2F2025%2F11%2F17%2Fai-is-writing-your-code-but-whos-reviewing-it%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/yiL992ubdwpMQleLDg0W3c8GYTTRnBsHmWk-dqrF7z8=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbdtechtalks.com%2F2025%2F11%2F17%2Fai-is-writing-your-code-but-whos-reviewing-it%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/yiL992ubdwpMQleLDg0W3c8GYTTRnBsHmWk-dqrF7z8=432", "authors": ["TLDR Newsletter"], "title": "AI is writing your code, but who's reviewing it?", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbdtechtalks.com%2F2025%2F11%2F17%2Fai-is-writing-your-code-but-whos-reviewing-it%3Futm_source=tldrwebdev/1/0100019a9c1b9508-6fafb6a9-4866-48c1-9c25-83c80a0cd52c-000000/yiL992ubdwpMQleLDg0W3c8GYTTRnBsHmWk-dqrF7z8=432", "summary": "AI is writing your code, but who's reviewing it? (8 minute read) AI codegen tools are creating a new form of technical debt (\u201cAI slop\u201d): bloated, buggy code that doesn't have proper architecture and includes redundant functions. To address this problem, AI-powered code review tools act as automated senior engineers, analyzing code for security vulnerabilities, performance issues, and architectural problems. This leads to an agentic software development lifecycle where specialized AI agents ha...", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u4ea7\u751f\u4e86\u6280\u672f\u503a\u52a1\u95ee\u9898\uff0c\u9700\u8981AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u6765\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u3001\u6027\u80fd\u95ee\u9898\u548c\u67b6\u6784\u95ee\u9898\uff0c\u5b9e\u73b0\u667a\u80fd\u5316\u7684\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u4ea7\u751f\u7684\u6280\u672f\u503a\u52a1\u95ee\u9898\uff0c\u5982\u4ee3\u7801\u81c3\u80bf\u3001bug\u591a\u3001\u67b6\u6784\u4e0d\u5f53\u548c\u529f\u80fd\u5197\u4f59\u7b49\u3002", "method": "\u4f7f\u7528AI\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u50cf\u81ea\u52a8\u5316\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u4e00\u6837\u5206\u6790\u4ee3\u7801\uff0c\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u3001\u6027\u80fd\u95ee\u9898\u548c\u67b6\u6784\u95ee\u9898\u3002", "result": "\u5b9e\u73b0\u4e86\u667a\u80fd\u5316\u7684\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\uff0c\u5176\u4e2d\u4e13\u95e8\u7684AI\u4ee3\u7406\u8d1f\u8d23\u4ee3\u7801\u5ba1\u67e5\u3002", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u662f\u89e3\u51b3AI\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2511.dbd60d85", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strongdm.com%2Fblog%2Fpolicy-enforcement-for-agentic-ai-with-leash%3Futm_source=tldrinfosec/1/0100019a9c711ace-b8efe0e9-e262-4896-b282-396a1bfed22d-000000/oWZp5GJknd6vp4h4hpVFehX2gkPJ-6zjXLia1e1MTt8=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strongdm.com%2Fblog%2Fpolicy-enforcement-for-agentic-ai-with-leash%3Futm_source=tldrinfosec/1/0100019a9c711ace-b8efe0e9-e262-4896-b282-396a1bfed22d-000000/oWZp5GJknd6vp4h4hpVFehX2gkPJ-6zjXLia1e1MTt8=432", "authors": ["TLDR Newsletter"], "title": "\ud83c\udd95 StrongDM launches Leash: open-source policy enforcement for AI agents", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strongdm.com%2Fblog%2Fpolicy-enforcement-for-agentic-ai-with-leash%3Futm_source=tldrinfosec/1/0100019a9c711ace-b8efe0e9-e262-4896-b282-396a1bfed22d-000000/oWZp5GJknd6vp4h4hpVFehX2gkPJ-6zjXLia1e1MTt8=432", "summary": "\ud83c\udd95 StrongDM launches Leash: open-source policy enforcement for AI agents (Sponsor) AI agents are the fastest-growing class of identity. They're connecting to tools and systems 24/7 - requiring a new approach to access control. Leash is StrongDM's new open-source project that extends runtime control to agentic systems. Leash works at kernel level (<1ms latency overhead) to evaluate agent activity against human-readable guardrails. Read the blog", "source": "tldr", "AI": {"tldr": "StrongDM\u63a8\u51faLeash\uff1a\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u5f00\u6e90\u7b56\u7565\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u6838\u7ea7\u63a7\u5236\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u8fd0\u884c\u65f6\u8bbf\u95ee\u63a7\u5236", "motivation": "AI\u4ee3\u7406\u6210\u4e3a\u589e\u957f\u6700\u5feb\u7684\u8eab\u4efd\u7c7b\u522b\uff0c\u9700\u89817x24\u5c0f\u65f6\u8fde\u63a5\u5230\u5de5\u5177\u548c\u7cfb\u7edf\uff0c\u8fd9\u8981\u6c42\u65b0\u7684\u8bbf\u95ee\u63a7\u5236\u65b9\u6cd5", "method": "Leash\u5728\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u5c42\u9762\u5de5\u4f5c\uff08<1ms\u5ef6\u8fdf\u5f00\u9500\uff09\uff0c\u6839\u636e\u4eba\u7c7b\u53ef\u8bfb\u7684\u9632\u62a4\u680f\u8bc4\u4f30\u4ee3\u7406\u6d3b\u52a8", "result": "Leash\u5c06\u8fd0\u884c\u65f6\u63a7\u5236\u6269\u5c55\u5230\u4ee3\u7406\u7cfb\u7edf\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u5b89\u5168\u7b56\u7565\u6267\u884c", "conclusion": "Leash\u4e3aAI\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u8fd0\u884c\u65f6\u8bbf\u95ee\u63a7\u5236\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "wechat.2511.50b4825b", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2ODgzMTM5NA==&mid=2247503881&idx=1&sn=17f1d05f1d78464ce56193ca4d8a7fe7&chksm=fd9d18459110a7f09a9ddafb6847100f21de0992d3b242c1fe214301f3652752c4629750b884#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2ODgzMTM5NA==&mid=2247503881&idx=1&sn=17f1d05f1d78464ce56193ca4d8a7fe7&chksm=fd9d18459110a7f09a9ddafb6847100f21de0992d3b242c1fe214301f3652752c4629750b884#rd", "authors": ["CAAI\u8ba4\u77e5\u7cfb\u7edf\u4e0e\u4fe1\u606f\u5904\u7406\u4e13\u59d4\u4f1a"], "title": "\u6e05\u534e\u4e0e\u5357\u6d0b\u7406\u5de5\u63d0\u51faVLA-RL\uff1a\u7528<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u589e\u5f3a\u673a\u5668\u4eba\u5927\u6a21\u578b", "comment": "Source: WeChat, Published: 2025-11-20 13:06:51", "summary": "\u800c\u8fd9\u6b63\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u62ff\u624b\u597d\u620f\u3002\u901a\u8fc7\u5728\u7ebf\u6536\u96c6\u6570\u636e\u5e76\u6839\u636e\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\uff0cRL\u80fd\u8ba9\u667a\u80fd\u4f53\u53d1\u73b0\u6bd4\u793a\u6559\u6570\u636e\u66f4\u4f18\u7684\u7b56\u7565\u3002\u4e0a\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86VLA-RL\u4e0e\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u533a\u522b\uff0c\u4ee5\u53ca\u5b83\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e26\u6765\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "AI": {"tldr": "\u800c\u8fd9\u6b63\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u62ff\u624b\u597d\u620f\u3002\u901a\u8fc7\u5728\u7ebf\u6536\u96c6\u6570\u636e\u5e76\u6839\u636e\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\uff0cRL\u80fd\u8ba9\u667a\u80fd\u4f53\u53d1\u73b0\u6bd4\u793a\u6559\u6570\u636e\u66f4\u4f18\u7684\u7b56\u7565\u3002\u4e0a\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86VLA-RL\u4e0e\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u533a\u522b\uff0c\u4ee5\u53ca\u5b83\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e26\u6765\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.89257fa7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651615664&idx=1&sn=c78426fb67016e1a6fbff6c8cde15849&chksm=bc49b50ebfe2d2bee413ca6574a7d76c5784fdb2842967e39cf15d5f01f7ae71f1f3194367d1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651615664&idx=1&sn=c78426fb67016e1a6fbff6c8cde15849&chksm=bc49b50ebfe2d2bee413ca6574a7d76c5784fdb2842967e39cf15d5f01f7ae71f1f3194367d1#rd", "authors": ["\u4e2d\u56fd\u8ba1\u7b97\u673a\u5b66\u4f1a"], "title": "\u6df1\u601d\u8003\u65f6\u4ee3\u2014\u2014\u6df1\u601d\u8003\u6a21\u578b\u4e0e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em> | CCCF\u7cbe\u9009", "comment": "Source: WeChat, Published: 2025-11-20 09:15:42", "summary": "\u5728\u672c\u671f\u201c\u6df1\u601d\u8003\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u201d\u4e13\u9898\u4e2d\uff0c\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4e00\u7ebf\u4e13\u5bb6\u5b66\u8005\u4ece\u57fa\u672c\u6982\u5ff5\u3001\u65b9\u6cd5\u8fed\u4ee3\u3001\u8f6f\u786c\u534f\u540c\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5c55\u5f00\u8bba\u8ff0\uff0c\u4e89\u53d6\u4e3a\u8bfb\u8005\u5448\u73b0\u4e00\u5e45\u5173\u4e8e\u6df1\u601d\u8003\u65f6\u4ee3\u7684\u7acb\u4f53\u5168\u666f\u56fe\u3002", "AI": {"tldr": "\u5728\u672c\u671f\u201c\u6df1\u601d\u8003\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u201d\u4e13\u9898\u4e2d\uff0c\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4e00\u7ebf\u4e13\u5bb6\u5b66\u8005\u4ece\u57fa\u672c\u6982\u5ff5\u3001\u65b9\u6cd5\u8fed\u4ee3\u3001\u8f6f\u786c\u534f\u540c\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5c55\u5f00\u8bba\u8ff0\uff0c\u4e89\u53d6\u4e3a\u8bfb\u8005\u5448\u73b0\u4e00\u5e45\u5173\u4e8e\u6df1\u601d\u8003\u65f6\u4ee3\u7684\u7acb\u4f53\u5168\u666f\u56fe\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.b7f53f3b", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651739423&idx=1&sn=3f10f98a3df5e8ba3aaad52f0f40d8b7&chksm=bc467c2cc72f3dee8292377ba1d9feb49799faac39bacac725c8a8e20513051867c59d0194a2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&mid=2651739423&idx=1&sn=3f10f98a3df5e8ba3aaad52f0f40d8b7&chksm=bc467c2cc72f3dee8292377ba1d9feb49799faac39bacac725c8a8e20513051867c59d0194a2#rd", "authors": ["\u5927\u6570\u636e\u6587\u6458"], "title": "\u4e0a\u4ea4\u535a\u58eb\u6700\u65b0\u601d\u8003\uff1a\u4ec5\u7528\u4e24\u4e2a\u95ee\u9898\u8bb2\u6e05<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-11-20 04:04:27", "summary": "\u5f3a\u5316\u5b66\u4e60\u7684\u8fc7\u7a0b\uff0c\u672c\u8d28\u4e0a\u662f\u667a\u80fd\u4f53\u4e0d\u65ad\u6536\u96c6\u7ecf\u9a8c\u3001\u5e76\u7528\u8fd9\u4e9b\u7ecf\u9a8c\u6539\u8fdb\u7b56\u7565\u7684\u5faa\u73af\u3002\u4e0d\u540c\u7b97\u6cd5\u7684\u5dee\u5f02\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5b83\u4eec\u4f9d\u8d56\u4ec0\u4e48\u6837\u7684\u6570\u636e\u3002\u6700\u76f4\u63a5\u7684\u65b9\u5f0f\u662f\u201c\u5728\u7b56\u7565\u5b66\u4e60\u201d\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7684\u8fc7\u7a0b\uff0c\u672c\u8d28\u4e0a\u662f\u667a\u80fd\u4f53\u4e0d\u65ad\u6536\u96c6\u7ecf\u9a8c\u3001\u5e76\u7528\u8fd9\u4e9b\u7ecf\u9a8c\u6539\u8fdb\u7b56\u7565\u7684\u5faa\u73af\u3002\u4e0d\u540c\u7b97\u6cd5\u7684\u5dee\u5f02\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5b83\u4eec\u4f9d\u8d56\u4ec0\u4e48\u6837\u7684\u6570\u636e\u3002\u6700\u76f4\u63a5\u7684\u65b9\u5f0f\u662f\u201c\u5728\u7b56\u7565\u5b66\u4e60\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.a6ffc015", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247492289&idx=1&sn=e1e3ef17fd24b3343af1c354b607fbb2&chksm=cf6e9c27dd73cefa36c89ad111af8cb118e65b9dd2de435916e0f6a178be6dd68b52913e20c2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MTg4ODc4Mg==&mid=2247492289&idx=1&sn=e1e3ef17fd24b3343af1c354b607fbb2&chksm=cf6e9c27dd73cefa36c89ad111af8cb118e65b9dd2de435916e0f6a178be6dd68b52913e20c2#rd", "authors": ["\u8682\u8681\u5f00\u6e90"], "title": "\u8682\u8681\u5f00\u6e90\u4e07\u4ebf\u53c2\u6570<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u9ad8\u6027\u80fd\u6743\u91cd\u4ea4\u6362\u6846\u67b6 Awex", "comment": "Source: WeChat, Published: 2025-11-20 01:03:51", "summary": "\u8fc7\u53bb\u51e0\u5e74\uff0c\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c\u7b80\u79f0 RL\uff09\u5df2\u7ecf\u6210\u4e3a\u5927\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6838\u5fc3\u6280\u672f\u3002\u4ece ChatGPT \u7684 RLHF\uff0c\u5230 DeepSeek/Claude/Llama \u7684\u540e\u8bad\u7ec3\u4f53\u7cfb\uff0c\u90fd\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u8ba9\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3001\u5177\u5907\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u6269\u5927\u6a21\u578b\u667a\u80fd\u8fb9", "AI": {"tldr": "\u8fc7\u53bb\u51e0\u5e74\uff0c\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c\u7b80\u79f0 RL\uff09\u5df2\u7ecf\u6210\u4e3a\u5927\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u6838\u5fc3\u6280\u672f\u3002\u4ece ChatGPT \u7684 RLHF\uff0c\u5230 DeepSeek/Claude/Llama \u7684\u540e\u8bad\u7ec3\u4f53\u7cfb\uff0c\u90fd\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u8ba9\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3001\u5177\u5907\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fdb\u4e00\u6b65\u6269\u5927\u6a21\u578b\u667a\u80fd\u8fb9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.cde4b9ce", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzODkxNzQzMw==&mid=2247495855&idx=1&sn=796609bf779bda6c85fd04bb856cfd19&chksm=fba0e2f6ae31341731de2967231af84805d2db4ac455482186a6cb0b8e57fab968fb150c0817#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzODkxNzQzMw==&mid=2247495855&idx=1&sn=796609bf779bda6c85fd04bb856cfd19&chksm=fba0e2f6ae31341731de2967231af84805d2db4ac455482186a6cb0b8e57fab968fb150c0817#rd", "authors": ["VLer"], "title": "\u6e05\u534e\u4e0e\u5357\u6d0b\u7406\u5de5\u63d0\u51faVLA-RL\uff1a\u7528<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u589e\u5f3a\u673a\u5668\u4eba\u5927\u6a21\u578b\uff0c\u544a\u522b\u6a21\u4eff\u5b66\u4e60\u74f6\u9888", "comment": "Source: WeChat, Published: 2025-11-19 23:02:47", "summary": "\u800c\u8fd9\u6b63\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u62ff\u624b\u597d\u620f\u3002\u901a\u8fc7\u5728\u7ebf\u6536\u96c6\u6570\u636e\u5e76\u6839\u636e\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\uff0cRL\u80fd\u8ba9\u667a\u80fd\u4f53\u53d1\u73b0\u6bd4\u793a\u6559\u6570\u636e\u66f4\u4f18\u7684\u7b56\u7565\u3002\u4e0a\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86VLA-RL\u4e0e\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u533a\u522b\uff0c\u4ee5\u53ca\u5b83\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e26\u6765\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "AI": {"tldr": "\u800c\u8fd9\u6b63\u662f\u5f3a\u5316\u5b66\u4e60\u7684\u62ff\u624b\u597d\u620f\u3002\u901a\u8fc7\u5728\u7ebf\u6536\u96c6\u6570\u636e\u5e76\u6839\u636e\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\uff0cRL\u80fd\u8ba9\u667a\u80fd\u4f53\u53d1\u73b0\u6bd4\u793a\u6559\u6570\u636e\u66f4\u4f18\u7684\u7b56\u7565\u3002\u4e0a\u56fe\u6e05\u6670\u5730\u5c55\u793a\u4e86VLA-RL\u4e0e\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u533a\u522b\uff0c\u4ee5\u53ca\u5b83\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e26\u6765\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.7f2405d8", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247513505&idx=1&sn=27cf583b1b9fdac5569055dc354b2e19&chksm=ea5f84117d0b44c8fbb99336ef7887d8a4a8d35f7fd31a941e1af33b7ded1bd8ccfe5a8a3d39#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247513505&idx=1&sn=27cf583b1b9fdac5569055dc354b2e19&chksm=ea5f84117d0b44c8fbb99336ef7887d8a4a8d35f7fd31a941e1af33b7ded1bd8ccfe5a8a3d39#rd", "authors": ["\u4ea7\u4e1a\u667a\u80fd\u5b98"], "title": "\u3010<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u5f00\u53d1\u3011\u7cfb\u7edf\u89e3\u6784AI Agent\u4e0e<em class=\"highlight\">Agentic</em> AI\u7684\u6838\u5fc3\u5dee\u5f02", "comment": "Source: WeChat, Published: 2025-11-20 04:22:19", "summary": "Agentic AI\uff08\u4ee3\u7406\u5f0fAI\uff09\uff1a\u7531\u591a\u4e2a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u534f\u540c\u7cfb\u7edf\uff0c\u6838\u5fc3\u80fd\u529b\u662f\u76ee\u6807\u62c6\u89e3-\u4efb\u52a1\u5206\u914d-\u7ed3\u679c\u6574\u5408\u3002\u7cfb\u7edf\u5305\u542b\u4e2d\u592e\u534f\u8c03\u8005\uff08\u5982\u5143\u667a\u80fd\u4f53\uff09\uff0c\u80fd\u52a8\u6001\u5206\u914d\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u5171\u4eab\u8bb0\u5fc6\u5b9e\u73b0\u590d\u6742\u5de5\u4f5c\u6d41\uff08\u5982\u81ea\u52a8\u5316\u7814\u7a76\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\uff09", "AI": {"tldr": "Agentic AI\uff08\u4ee3\u7406\u5f0fAI\uff09\uff1a\u7531\u591a\u4e2a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u534f\u540c\u7cfb\u7edf\uff0c\u6838\u5fc3\u80fd\u529b\u662f\u76ee\u6807\u62c6\u89e3-\u4efb\u52a1\u5206\u914d-\u7ed3\u679c\u6574\u5408\u3002\u7cfb\u7edf\u5305\u542b\u4e2d\u592e\u534f\u8c03\u8005\uff08\u5982\u5143\u667a\u80fd\u4f53\uff09\uff0c\u80fd\u52a8\u6001\u5206\u914d\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u5171\u4eab\u8bb0\u5fc6\u5b9e\u73b0\u590d\u6742\u5de5\u4f5c\u6d41\uff08\u5982\u81ea\u52a8\u5316\u7814\u7a76\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.a2f74392", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0NjMyMjA2NQ==&mid=2247499477&idx=1&sn=611099b295b40bb96d7a7e34de5912bb&chksm=c20e7f0f2bcc53b156a659a49e306407395ce85375e59ecbf94e542b3faf1b2119c14e6c9476#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0NjMyMjA2NQ==&mid=2247499477&idx=1&sn=611099b295b40bb96d7a7e34de5912bb&chksm=c20e7f0f2bcc53b156a659a49e306407395ce85375e59ecbf94e542b3faf1b2119c14e6c9476#rd", "authors": ["Omdia"], "title": "Omdia\uff1a<em class=\"highlight\">Agentic</em> AI\u5c06\u9a71\u52a8\u7535\u4fe1\u7f51\u7edc\u81ea\u4e3b\u5316\uff0c41%\u8fd0\u8425\u5546\u805a\u7126\u7f51\u7edc\u7ba1\u7406", "comment": "Source: WeChat, Published: 2025-11-20 04:03:18", "summary": "Omdia\u6700\u65b0\u7814\u7a76\u663e\u793a\uff0cAgentic AI\uff08\u81ea\u4e3b\u667a\u80fd\u4f53AI\uff09\u6b63\u6210\u4e3a\u63a8\u52a8\u7535\u4fe1\u8fd0\u8425\u5546\uff08CSP\uff09\u7f51\u7edc\u81ea\u4e3b\u5316\u7684\u5173\u952e\u529b\u91cf\u3002\u6839\u636eOmdia\u53d1\u5e03\u7684\u300aAgentic AI\uff1a An Evolution with Transformative Potential for Telecom Operations\u300b\u62a5\u544a\uff0c41%\u7684CSP\u8ba4\u4e3a\u7f51\u7edc\u7ba1\u7406\u5c06\u662fAgentic AI\u5f71\u54cd\u6700\u5927", "AI": {"tldr": "Omdia\u6700\u65b0\u7814\u7a76\u663e\u793a\uff0cAgentic AI\uff08\u81ea\u4e3b\u667a\u80fd\u4f53AI\uff09\u6b63\u6210\u4e3a\u63a8\u52a8\u7535\u4fe1\u8fd0\u8425\u5546\uff08CSP\uff09\u7f51\u7edc\u81ea\u4e3b\u5316\u7684\u5173\u952e\u529b\u91cf\u3002\u6839\u636eOmdia\u53d1\u5e03\u7684\u300aAgentic AI\uff1a An Evolution with Transformative Potential for Telecom Operations\u300b\u62a5\u544a\uff0c41%\u7684CSP\u8ba4\u4e3a\u7f51\u7edc\u7ba1\u7406\u5c06\u662fAgentic AI\u5f71\u54cd\u6700\u5927", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.d6d489fc", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzgyMDgyNg==&mid=2247484273&idx=1&sn=ffffc36e845a52244f8dd1b9b74125a5&chksm=fec3a0a970a6725f1ee19327979ccb748ed945f7905cf94832b8e38fb1dabf6dfddcd7eb493e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzgyMDgyNg==&mid=2247484273&idx=1&sn=ffffc36e845a52244f8dd1b9b74125a5&chksm=fec3a0a970a6725f1ee19327979ccb748ed945f7905cf94832b8e38fb1dabf6dfddcd7eb493e#rd", "authors": ["\u667a\u9020\u7f16\u7a0b"], "title": "Gemini 3 Pro \u6df1\u5ea6\u6280\u672f\u5256\u6790\uff1a\u9762\u5411\u672a\u6765\u7684 <em class=\"highlight\">Agentic</em> \u9769\u547d", "comment": "Source: WeChat, Published: 2025-11-20 00:45:48", "summary": "\u672c\u5468 Google DeepMind \u53d1\u5e03\u7684 Gemini 3 Pro \u6a21\u578b\uff0c\u901a\u8fc7\u9769\u65b0\u6027\u7684 Agentic\uff08\u81ea\u4e3b\u667a\u80fd\u4f53\uff09\u80fd\u529b\u548c\u8d85\u6df1\u5ea6\u63a8\u7406\uff0c\u91cd\u65b0\u786e\u7acb\u4e86 SOTA \u6a21\u578b\u7684\u6807\u51c6\u3002\u672c\u6587\u5c06\u4ece\u67b6\u6784\u3001\u6027\u80fd\u3001\u884c\u4e1a\u7ade\u4e89\u548c\u5de5\u7a0b\u5b9e\u8df5\u56db\u4e2a\u7ef4\u5ea6\uff0c\u4e3a\u6280\u672f\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u6df1\u5165\u7684\u5206\u6790\u3002", "AI": {"tldr": "\u672c\u5468 Google DeepMind \u53d1\u5e03\u7684 Gemini 3 Pro \u6a21\u578b\uff0c\u901a\u8fc7\u9769\u65b0\u6027\u7684 Agentic\uff08\u81ea\u4e3b\u667a\u80fd\u4f53\uff09\u80fd\u529b\u548c\u8d85\u6df1\u5ea6\u63a8\u7406\uff0c\u91cd\u65b0\u786e\u7acb\u4e86 SOTA \u6a21\u578b\u7684\u6807\u51c6\u3002\u672c\u6587\u5c06\u4ece\u67b6\u6784\u3001\u6027\u80fd\u3001\u884c\u4e1a\u7ade\u4e89\u548c\u5de5\u7a0b\u5b9e\u8df5\u56db\u4e2a\u7ef4\u5ea6\uff0c\u4e3a\u6280\u672f\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u3001\u6df1\u5165\u7684\u5206\u6790\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.3443894b", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzOTIzNDIxNg==&mid=2247483669&idx=1&sn=ce1cd75f72802fb63956c35ef81a62a4&chksm=f1f1e09fce1128fe56a61fb292a8e196ac84c3e9944c3fc1332d5869edd6c98b882660738074#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzOTIzNDIxNg==&mid=2247483669&idx=1&sn=ce1cd75f72802fb63956c35ef81a62a4&chksm=f1f1e09fce1128fe56a61fb292a8e196ac84c3e9944c3fc1332d5869edd6c98b882660738074#rd", "authors": ["\u4e00\u6761\u89c1\u95fb"], "title": "AI\u6d4f\u89c8\u5668\u5f7b\u5e95\u6539\u53d8\u4e86\u6211\u4eec\u4f7f\u7528\u4e92\u8054\u7f51\u7684\u65b9\u5f0f", "comment": "Source: WeChat, Published: 2025-11-20 00:30:35", "summary": "\u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u6765\u770b\u770bAgentic\u6d4f\u89c8\u53ef\u80fd\u4f1a\u7ed9\u4e92\u8054\u7f51\u7684\u672a\u6765\u3001\u4fe1\u606f\u4ee5\u53ca\u4eba\u7c7b\u4e0e\u673a\u5668\u7684\u4ea4\u4e92\u65b9\u5f0f\u5e26\u6765\u600e\u6837\u7684\u5f71\u54cd\u3002Agentic \u6d4f\u89c8\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f\u8fd1\u51e0\u4e2a\u6708\u6765\uff0c\u5df2\u6709\u591a\u6b3e\u652f\u6301Agent\u7684\u7f51\u7edc\u6d4f\u89c8\u5668\u5411\u516c\u4f17\u53d1\u5e03\uff1a\u5176\u4e2d\u5305\u62ec\u76ee\u524d\u4ec5\u9002\u7528\u4e8e Mac \u7cfb\u7edf\u7684 Chat", "AI": {"tldr": "\u90a3\u4e48\uff0c\u8ba9\u6211\u4eec\u6765\u770b\u770bAgentic\u6d4f\u89c8\u53ef\u80fd\u4f1a\u7ed9\u4e92\u8054\u7f51\u7684\u672a\u6765\u3001\u4fe1\u606f\u4ee5\u53ca\u4eba\u7c7b\u4e0e\u673a\u5668\u7684\u4ea4\u4e92\u65b9\u5f0f\u5e26\u6765\u600e\u6837\u7684\u5f71\u54cd\u3002Agentic \u6d4f\u89c8\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f\u8fd1\u51e0\u4e2a\u6708\u6765\uff0c\u5df2\u6709\u591a\u6b3e\u652f\u6301Agent\u7684\u7f51\u7edc\u6d4f\u89c8\u5668\u5411\u516c\u4f17\u53d1\u5e03\uff1a\u5176\u4e2d\u5305\u62ec\u76ee\u524d\u4ec5\u9002\u7528\u4e8e Mac \u7cfb\u7edf\u7684 Chat", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.d5c5fa25", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484332&idx=1&sn=9fe5dfa3bbbc82ba9d1394e73575315a&chksm=c03788e2cf94e1cf6dc497faf6289afc8e16f43022c0a1762d50cc97749f732aa55f32ccfc1f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484332&idx=1&sn=9fe5dfa3bbbc82ba9d1394e73575315a&chksm=c03788e2cf94e1cf6dc497faf6289afc8e16f43022c0a1762d50cc97749f732aa55f32ccfc1f#rd", "authors": ["AI Lab Dev"], "title": "<em class=\"highlight\">Agentic</em>21\u79cd\u8bbe\u8ba1\u6a21\u5f0f12-Exception Handling andRecovery", "comment": "Source: WeChat, Published: 2025-11-20 00:19:16", "summary": "\u6570\u636e\u5904\u7406\u4ee3\u7406\uff1a\u8d1f\u8d23\u5904\u7406\u4e00\u6279\u6587\u4ef6\u7684agent\u5728\u5904\u7406\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u9047\u5230\u635f\u574f\u7684\u6587\u4ef6\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5e94\u8be5\u8df3\u8fc7\u8fd9\u4e9b\u635f\u574f\u7684\u6587\u4ef6\uff0c\u8bb0\u5f55\u9519\u8bef\u4fe1\u606f\uff0c\u7ee7\u7eed\u5904\u7406\u5176\u4ed6\u6587\u4ef6\uff0c\u5e76\u5728\u5904\u7406\u5b8c\u6210\u540e\u62a5\u544a\u88ab\u8df3\u8fc7\u7684\u6587\u4ef6\uff0c\u800c\u4e0d\u4f1a\u56e0\u6b64\u505c\u6b62\u6574\u4e2a\u5904\u7406\u6d41\u7a0b", "AI": {"tldr": "\u6570\u636e\u5904\u7406\u4ee3\u7406\uff1a\u8d1f\u8d23\u5904\u7406\u4e00\u6279\u6587\u4ef6\u7684agent\u5728\u5904\u7406\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u9047\u5230\u635f\u574f\u7684\u6587\u4ef6\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u5e94\u8be5\u8df3\u8fc7\u8fd9\u4e9b\u635f\u574f\u7684\u6587\u4ef6\uff0c\u8bb0\u5f55\u9519\u8bef\u4fe1\u606f\uff0c\u7ee7\u7eed\u5904\u7406\u5176\u4ed6\u6587\u4ef6\uff0c\u5e76\u5728\u5904\u7406\u5b8c\u6210\u540e\u62a5\u544a\u88ab\u8df3\u8fc7\u7684\u6587\u4ef6\uff0c\u800c\u4e0d\u4f1a\u56e0\u6b64\u505c\u6b62\u6574\u4e2a\u5904\u7406\u6d41\u7a0b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.847fb21b", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNDA0MDk0OA==&mid=2650380584&idx=1&sn=5d57e1ba5de6455c1ed9b557afea74ad&chksm=f12b5b825be5ca851b6e7799b8dece60bd8e2aa1ef408722e875459e88aa92c9c5a5aaaee8e4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNDA0MDk0OA==&mid=2650380584&idx=1&sn=5d57e1ba5de6455c1ed9b557afea74ad&chksm=f12b5b825be5ca851b6e7799b8dece60bd8e2aa1ef408722e875459e88aa92c9c5a5aaaee8e4#rd", "authors": ["SAPHANA"], "title": "\u91cd\u65b0\u5b9a\u4e49\u4eba\u5de5\u667a\u80fd\uff1a<em class=\"highlight\">Agentic</em> AI \u7684\u5d1b\u8d77\u4e0e\u672a\u6765", "comment": "Source: WeChat, Published: 2025-11-19 23:00:38", "summary": "Agentic AI \u80fd\uff1a\u81ea\u4e3b\u7406\u89e3\u76ee\u6807\uff1b\u5236\u5b9a\u8ba1\u5212\uff1b\u8c03\u7528\u5de5\u5177\u6216API\uff1b\u4e0e\u4eba\u7c7b\u6216\u5176\u4ed6AI\u534f\u4f5c\uff1b\u4e0d\u65ad\u53cd\u601d\u5e76\u4f18\u5316\u7b56\u7565\u3002\u201c\u4ece\u8bed\u8a00\u6a21\u578b\u5230\u667a\u80fd\u4f53\uff1aAI\u7684\u80fd\u529b\u8dc3\u8fc1\u201d\u4e8c\u3001Agentic AI \u7684\u6838\u5fc3\u7ed3\u6784\uff1a\u80fd\u601d\u8003\u3001\u80fd\u884c\u52a8\u3001\u80fd\u5b66\u4e60", "AI": {"tldr": "Agentic AI \u80fd\uff1a\u81ea\u4e3b\u7406\u89e3\u76ee\u6807\uff1b\u5236\u5b9a\u8ba1\u5212\uff1b\u8c03\u7528\u5de5\u5177\u6216API\uff1b\u4e0e\u4eba\u7c7b\u6216\u5176\u4ed6AI\u534f\u4f5c\uff1b\u4e0d\u65ad\u53cd\u601d\u5e76\u4f18\u5316\u7b56\u7565\u3002\u201c\u4ece\u8bed\u8a00\u6a21\u578b\u5230\u667a\u80fd\u4f53\uff1aAI\u7684\u80fd\u529b\u8dc3\u8fc1\u201d\u4e8c\u3001Agentic AI \u7684\u6838\u5fc3\u7ed3\u6784\uff1a\u80fd\u601d\u8003\u3001\u80fd\u884c\u52a8\u3001\u80fd\u5b66\u4e60", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.d0f49283", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNDkwMzUzMA==&mid=2247502828&idx=1&sn=37509860f6288af76986e789511c9c86&chksm=fbd29225806f1544895d99ce00ad7d3250e7a59da6a0bf9ed4eff35485731759ba44ad3c1b92#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNDkwMzUzMA==&mid=2247502828&idx=1&sn=37509860f6288af76986e789511c9c86&chksm=fbd29225806f1544895d99ce00ad7d3250e7a59da6a0bf9ed4eff35485731759ba44ad3c1b92#rd", "authors": ["\u7528\u6570\u8bf4"], "title": "\u4f01\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\u4e2d\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0e\u667a\u80fd\u4f53\uff1a\u7814\u7a76\u6846\u67b6\u3001\u7406\u8bba\u8fdb\u5c55\u4e0e\u672a\u6765\u65b9\u5411", "comment": "Source: WeChat, Published: 2025-11-20 13:27:55", "summary": "\u5927\u8bed\u8a00\u6a21\u578b \uff08llm\uff09 \u2014\u2014 \u4f01\u4e1a\u7684\u201c\u8ba4\u77e5\u5f15\u64ce\u201d\uff1allm\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u901a\u8fc7\u6d77\u91cf\u6587\u672c\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u6982\u7387\u6a21\u578b\uff0c\u5b83\u5177\u5907\u4e86\u901a\u7528\u7684\u8bed\u8a00\u7406\u89e3\u3001\u903b\u8f91\u63a8\u7406\u548c\u5185\u5bb9\u751f\u6210\u80fd\u529b\u3002", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b \uff08llm\uff09 \u2014\u2014 \u4f01\u4e1a\u7684\u201c\u8ba4\u77e5\u5f15\u64ce\u201d\uff1allm\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u901a\u8fc7\u6d77\u91cf\u6587\u672c\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u6982\u7387\u6a21\u578b\uff0c\u5b83\u5177\u5907\u4e86\u901a\u7528\u7684\u8bed\u8a00\u7406\u89e3\u3001\u903b\u8f91\u63a8\u7406\u548c\u5185\u5bb9\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.0573fc9b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUwOTM2OTc4MA==&mid=2247487150&idx=1&sn=8b27b33c4ef6f5498b8caa5ee97f1adf&chksm=f8fc6df16754e16c12fc44b9967044bbbe434033a82cef2af03fb8ec614da7796999e2bff3e3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUwOTM2OTc4MA==&mid=2247487150&idx=1&sn=8b27b33c4ef6f5498b8caa5ee97f1adf&chksm=f8fc6df16754e16c12fc44b9967044bbbe434033a82cef2af03fb8ec614da7796999e2bff3e3#rd", "authors": ["\u77e5\u679c\u65e5\u8bb0"], "title": "\u5168\u7403\u9876\u5c16<em class=\"highlight\">\u5927\u6a21\u578b</em>Top 10\u603b\u7ed3", "comment": "Source: WeChat, Published: 2025-11-20 12:57:50", "summary": "\u5168\u7403\u9876\u5c16\u5927\u6a21\u578btop 10\u603b\u7ed3 \u5927\u81f4\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2a\u68af\u961f\uff1a\u201c\u9886\u5934\u7f8a\u201d\u3001\u201c\u9876\u7ea7\u7ade\u4e89\u8005\u201d \u548c \u201c\u7279\u8272\u9c9c\u660e\u7684\u5f3a\u8005\u201d\u3002\u7b2c\u4e00\u68af\u961f\uff1a\u516c\u8ba4\u7684\u201c\u9886\u5934\u7f8a\u201d \u8fd9\u4e9b\u6a21\u578b\u5728\u7edd\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u4f4d\u5217\u524d\u4e09\uff0c\u4ee3\u8868\u4e86\u5f53\u524d\u5927\u6a21\u578b\u7684\u6700\u9ad8\u6c34\u51c6\u3002", "AI": {"tldr": "\u5168\u7403\u9876\u5c16\u5927\u6a21\u578btop 10\u603b\u7ed3 \u5927\u81f4\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2a\u68af\u961f\uff1a\u201c\u9886\u5934\u7f8a\u201d\u3001\u201c\u9876\u7ea7\u7ade\u4e89\u8005\u201d \u548c \u201c\u7279\u8272\u9c9c\u660e\u7684\u5f3a\u8005\u201d\u3002\u7b2c\u4e00\u68af\u961f\uff1a\u516c\u8ba4\u7684\u201c\u9886\u5934\u7f8a\u201d \u8fd9\u4e9b\u6a21\u578b\u5728\u7edd\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u4f4d\u5217\u524d\u4e09\uff0c\u4ee3\u8868\u4e86\u5f53\u524d\u5927\u6a21\u578b\u7684\u6700\u9ad8\u6c34\u51c6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.cb69ab09", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDE4NDAwOA==&mid=2247487553&idx=1&sn=0324e731ba97d03656c40a1342721f5a&chksm=c5b3a22d59a3924e7eba0ccd6e24d7d9d9296b9a98bf9860c87e9116951a7a2330d9a3c9dafd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDE4NDAwOA==&mid=2247487553&idx=1&sn=0324e731ba97d03656c40a1342721f5a&chksm=c5b3a22d59a3924e7eba0ccd6e24d7d9d9296b9a98bf9860c87e9116951a7a2330d9a3c9dafd#rd", "authors": ["\u5927\u6a21\u578b\u4e4b\u7a97"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5f00\u53d1\u8def\u7ebf\u5b8c\u6574\u7248\uff01", "comment": "Source: WeChat, Published: 2025-11-20 12:32:34", "summary": "#AI\u5de5\u4f5c\u539f\u7406 #\u5927\u6a21\u578b #RAG #ai #\u4eba\u5de5\u667a\u80fd #\u5927\u8bed\u8a00\u6a21\u578b #\u667a\u80fd\u4f53 #\u5fae\u8c03 #transformer #\u7f16\u7a0b #LLM", "AI": {"tldr": "#AI\u5de5\u4f5c\u539f\u7406 #\u5927\u6a21\u578b #RAG #ai #\u4eba\u5de5\u667a\u80fd #\u5927\u8bed\u8a00\u6a21\u578b #\u667a\u80fd\u4f53 #\u5fae\u8c03 #transformer #\u7f16\u7a0b #LLM", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.68ee082f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493392&idx=1&sn=65fbae1935394c7e54f035aef3c068fa&chksm=fa8b42df73858830545ff4710e845bac4132551ed244e62d0bfa8d730091cbe1129554bff3c1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493392&idx=1&sn=65fbae1935394c7e54f035aef3c068fa&chksm=fa8b42df73858830545ff4710e845bac4132551ed244e62d0bfa8d730091cbe1129554bff3c1#rd", "authors": ["\u6155\u5bb9\u5343\u8bed"], "title": "\u3010\u6536\u85cf\u7ea7\u3011AI Agent \u4ece\u5165\u95e8\u5230\u7cbe\u901a\uff1a\u5c0f\u767d & \u7a0b\u5e8f\u5458\u5fc5\u5b66\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u6307\u5357", "comment": "Source: WeChat, Published: 2025-11-20 11:00:19", "summary": "\u56e0\u6b64\u5728\u6211\u7684\u5b9a\u4e49\u4e2d\u76f4\u63a5\u7684\u5927\u6a21\u578b\u5bf9\u8bdd\u6bd4\u5982 deepseek\uff0cchatgpt\uff0c\u8fd9\u4e9b\u5e76\u4e0d\u7b97\u662f AI Agent\u3002\u57fa\u4e8e\u4f20\u7edf\u610f\u56fe\u8bc6\u522b\u6a21\u578b\u7684 AI \u7cfb\u7edf\uff1a\u5728\u5927\u6a21\u578b\u4e4b\u524d\u5c31\u6709\u4e0d\u5c11\u7684\u6280\u672f\u5c1d\u8bd5\u901a\u8fc7\u7b80\u5355\u6216\u590d\u6742\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u6765\u8def\u7531\u7528\u6237\u7684\u8f93\u5165\uff0c\u5e76\u5bfc\u51fa\u5230\u4e0d\u540c\u7684\u6267\u884c\u5668", "AI": {"tldr": "\u56e0\u6b64\u5728\u6211\u7684\u5b9a\u4e49\u4e2d\u76f4\u63a5\u7684\u5927\u6a21\u578b\u5bf9\u8bdd\u6bd4\u5982 deepseek\uff0cchatgpt\uff0c\u8fd9\u4e9b\u5e76\u4e0d\u7b97\u662f AI Agent\u3002\u57fa\u4e8e\u4f20\u7edf\u610f\u56fe\u8bc6\u522b\u6a21\u578b\u7684 AI \u7cfb\u7edf\uff1a\u5728\u5927\u6a21\u578b\u4e4b\u524d\u5c31\u6709\u4e0d\u5c11\u7684\u6280\u672f\u5c1d\u8bd5\u901a\u8fc7\u7b80\u5355\u6216\u590d\u6742\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u6765\u8def\u7531\u7528\u6237\u7684\u8f93\u5165\uff0c\u5e76\u5bfc\u51fa\u5230\u4e0d\u540c\u7684\u6267\u884c\u5668", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.000aab2e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MzcxNDM2MQ==&mid=2247489185&idx=1&sn=81a774698cdb7bd009369c2213bb73d6&chksm=c282042066a150d36981d633640d8c844939454596a994684f134ca65fb44d12d1e7d547c576#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MzcxNDM2MQ==&mid=2247489185&idx=1&sn=81a774698cdb7bd009369c2213bb73d6&chksm=c282042066a150d36981d633640d8c844939454596a994684f134ca65fb44d12d1e7d547c576#rd", "authors": ["\u4ea4\u79d1\u667a\u6c47"], "title": "\u884c\u4e1a\u6210\u679c | \u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7cfb\u5217\u56fd\u5bb6\u6807\u51c6\u89e3\u8bfb\uff08\u4e09\uff09\u2014\u2014\u300a\u4eba\u5de5\u667a\u80fd <em class=\"highlight\">\u5927\u6a21\u578b</em> \u7b2c3\u90e8\u5206\uff1a\u670d\u52a1\u80fd\u529b\u6210\u719f\u5ea6\u8bc4\u4f30\u300b", "comment": "Source: WeChat, Published: 2025-11-20 10:50:55", "summary": "\u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b\u786e\u7acb\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u63cf\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002\u8be5\u6807\u51c6\u4e3a\u7b2c3\u90e8\u5206\uff0c\u7ed9\u51fa\u4e86\u5927\u6a21\u578b\u670d\u52a1\u80fd\u529b\u6846\u67b6\u548c\u6210\u719f\u5ea6\u7b49\u7ea7\uff0c\u63cf\u8ff0\u4e86\u5927\u6a21\u578b\u670d\u52a1\u80fd\u529b\u8bc4\u4f30\u6307\u6807\u548c\u8bc4\u4f30\u65b9\u6cd5", "AI": {"tldr": "\u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b\u786e\u7acb\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u63cf\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002\u8be5\u6807\u51c6\u4e3a\u7b2c3\u90e8\u5206\uff0c\u7ed9\u51fa\u4e86\u5927\u6a21\u578b\u670d\u52a1\u80fd\u529b\u6846\u67b6\u548c\u6210\u719f\u5ea6\u7b49\u7ea7\uff0c\u63cf\u8ff0\u4e86\u5927\u6a21\u578b\u670d\u52a1\u80fd\u529b\u8bc4\u4f30\u6307\u6807\u548c\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.22aee090", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498310&idx=1&sn=4c3fb3e9b0b333fe78af49c831379ac4&chksm=fd6ae44c22593b5cd1b4750feb509f0ea8eeeb1e5f4cbd7de0704413944954e4e57c12e42b26#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498310&idx=1&sn=4c3fb3e9b0b333fe78af49c831379ac4&chksm=fd6ae44c22593b5cd1b4750feb509f0ea8eeeb1e5f4cbd7de0704413944954e4e57c12e42b26#rd", "authors": ["\u4e30\u519c\u4fe1\u606f"], "title": "\u5404\u79cd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u67b6\u6784\u6bd4\u8f83", "comment": "Source: WeChat, Published: 2025-11-20 10:40:00", "summary": "\u4f20\u7edf\u7684NLP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5982MMLU\uff08\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\uff09\uff0c\u6b63\u8fc5\u901f\u8d8b\u4e8e\u9971\u548c\uff0c\u5bf9\u4e8e\u533a\u5206\u524d\u6cbf\u6a21\u578b\u7684\u80fd\u529b\u6108\u53d1\u6709\u9650\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u4e00\u7c7b\u4e13\u6ce8\u4e8e\u590d\u6742\u63a8\u7406\uff08\u5982GPQA\uff0c AIME\uff09\u548c\u667a\u80fd\u4f53\u6267\u884c\uff08\u5982SWE - bench\uff0c Terminal - bench\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u5df2\u6210\u4e3a\u8861\u91cfSOTA", "AI": {"tldr": "\u4f20\u7edf\u7684NLP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5982MMLU\uff08\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\uff09\uff0c\u6b63\u8fc5\u901f\u8d8b\u4e8e\u9971\u548c\uff0c\u5bf9\u4e8e\u533a\u5206\u524d\u6cbf\u6a21\u578b\u7684\u80fd\u529b\u6108\u53d1\u6709\u9650\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u4e00\u7c7b\u4e13\u6ce8\u4e8e\u590d\u6742\u63a8\u7406\uff08\u5982GPQA\uff0c AIME\uff09\u548c\u667a\u80fd\u4f53\u6267\u884c\uff08\u5982SWE - bench\uff0c Terminal - bench\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u5df2\u6210\u4e3a\u8861\u91cfSOTA", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.cf460169", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772271&idx=1&sn=14ce17cb492fd8a91d29b699d40478a7&chksm=fa70e09e981e7c12aaa3b51aa06f384f7917d813026489a63844580cd7cf4bbca9ababc5894c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247772271&idx=1&sn=14ce17cb492fd8a91d29b699d40478a7&chksm=fa70e09e981e7c12aaa3b51aa06f384f7917d813026489a63844580cd7cf4bbca9ababc5894c#rd", "authors": ["DataFunTalk"], "title": "\u4ece\u80fd\u529b\u5230\u6548\u7387\uff0c\u591a\u7ba1\u9f50\u4e0b\u63d0\u5347<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u667a\u80fd\u201c\u5bc6\u5ea6\u201d", "comment": "Source: WeChat, Published: 2025-11-20 05:00:00", "summary": "\u5927\u6a21\u578b\u7684\u8fd9\u4e9b\u7279\u70b9\u6765\u81ea\u4e8e\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u4e00\u662f\u201c\u63a8\u7406\u4e0e\u89c4\u5212\u80fd\u529b\u201d\uff0c\u8ba9\u5927\u6a21\u578b\u80fd\u591f\u5bf9\u590d\u6742\u4efb\u52a1\u8fdb\u884c\u63a8\u7406\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u7684\u6b65\u9aa4\uff0c\u5e76\u89c4\u5212\u884c\u52a8\u987a\u5e8f\uff0c\u8fdb\u4e00\u6b65\u4f53\u73b0\u5927\u6a21\u578b\u5728\u8c03\u7528\u5de5\u5177\u548c\u4e0e\u73af\u5883\u4e92\u52a8\u65f6\u7684\u80fd\u529b\uff1b", "AI": {"tldr": "\u5927\u6a21\u578b\u7684\u8fd9\u4e9b\u7279\u70b9\u6765\u81ea\u4e8e\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u4e00\u662f\u201c\u63a8\u7406\u4e0e\u89c4\u5212\u80fd\u529b\u201d\uff0c\u8ba9\u5927\u6a21\u578b\u80fd\u591f\u5bf9\u590d\u6742\u4efb\u52a1\u8fdb\u884c\u63a8\u7406\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u7684\u6b65\u9aa4\uff0c\u5e76\u89c4\u5212\u884c\u52a8\u987a\u5e8f\uff0c\u8fdb\u4e00\u6b65\u4f53\u73b0\u5927\u6a21\u578b\u5728\u8c03\u7528\u5de5\u5177\u548c\u4e0e\u73af\u5883\u4e92\u52a8\u65f6\u7684\u80fd\u529b\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.171614bc", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDEwMzMyMA==&mid=2247483650&idx=3&sn=fbe88a2a37124280f439469e3e56e863&chksm=a72fc51f45b66eccb0fa0b0335e9cc70cd1af92af146bf49ad1a29e3fca2a23714ca6a938e17#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDEwMzMyMA==&mid=2247483650&idx=3&sn=fbe88a2a37124280f439469e3e56e863&chksm=a72fc51f45b66eccb0fa0b0335e9cc70cd1af92af146bf49ad1a29e3fca2a23714ca6a938e17#rd", "authors": ["\u5317\u65b9\u5927\u6a21\u578b"], "title": "\u56fd\u5916\u4e3b\u6d41\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bc4\u6d4b\u6c47\u603b", "comment": "Source: WeChat, Published: 2025-11-20 03:22:59", "summary": "\u6839\u636e2025\u5e74\u65af\u5766\u798fHAI\u300aAI\u6307\u6570\u62a5\u544a\u300b\u53ca\u884c\u4e1a\u5b9e\u6d4b\u6570\u636e\uff0c\u56fd\u5916\u7efc\u5408\u80fd\u529b\u9886\u5148\u7684\u5927\u6a21\u578b\u96c6\u4e2d\u5728\u901a\u7528\u667a\u80fd\u3001\u957f\u6587\u672c\u5904\u7406\u3001\u591a\u6a21\u6001\u878d\u5408\u7b49\u9886\u57df\u3002\u524d\u56db\u7532\u5206\u522b\u4e3aGPT-4 Turbo\u3001Claude 3 Opus\u3001Gemini 1.5 Pro\u3001LLaMA 3\uff0c\u5404\u6a21\u578b\u6838\u5fc3\u5dee\u5f02\u5982\u4e0b\uff1a", "AI": {"tldr": "\u6839\u636e2025\u5e74\u65af\u5766\u798fHAI\u300aAI\u6307\u6570\u62a5\u544a\u300b\u53ca\u884c\u4e1a\u5b9e\u6d4b\u6570\u636e\uff0c\u56fd\u5916\u7efc\u5408\u80fd\u529b\u9886\u5148\u7684\u5927\u6a21\u578b\u96c6\u4e2d\u5728\u901a\u7528\u667a\u80fd\u3001\u957f\u6587\u672c\u5904\u7406\u3001\u591a\u6a21\u6001\u878d\u5408\u7b49\u9886\u57df\u3002\u524d\u56db\u7532\u5206\u522b\u4e3aGPT-4 Turbo\u3001Claude 3 Opus\u3001Gemini 1.5 Pro\u3001LLaMA 3\uff0c\u5404\u6a21\u578b\u6838\u5fc3\u5dee\u5f02\u5982\u4e0b\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
