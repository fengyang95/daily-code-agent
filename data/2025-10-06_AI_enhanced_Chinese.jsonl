{"id": "2510.02387", "categories": ["cs.SE", "cs.AI", "cs.LG", "68T07", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.02387", "abs": "https://arxiv.org/abs/2510.02387", "authors": ["FAIR CodeGen team", "Quentin Carbonneaux", "Gal Cohen", "Jonas Gehring", "Jacob Kahn", "Jannik Kossen", "Felix Kreuk", "Emily McMilin", "Michel Meyer", "Yuxiang Wei", "David Zhang", "Kunhao Zheng", "Jordi Armengol-Estap\u00e9", "Pedram Bashiri", "Maximilian Beck", "Pierre Chambon", "Abhishek Charnalia", "Chris Cummins", "Juliette Decugis", "Zacharias V. Fisches", "Fran\u00e7ois Fleuret", "Fabian Gloeckle", "Alex Gu", "Michael Hassid", "Daniel Haziza", "Badr Youbi Idrissi", "Christian Keller", "Rahul Kindi", "Hugh Leather", "Gallil Maimon", "Aram Markosyan", "Francisco Massa", "Pierre-Emmanuel Mazar\u00e9", "Vegard Mella", "Naila Murray", "Keyur Muzumdar", "Peter O'Hearn", "Matteo Pagliardini", "Dmitrii Pedchenko", "Tal Remez", "Volker Seeker", "Marco Selvi", "Oren Sultan", "Sida Wang", "Luca Wehrstedt", "Ori Yoran", "Lingming Zhang", "Taco Cohen", "Yossi Adi", "Gabriel Synnaeve"], "title": "CWM: An Open-Weights LLM for Research on Code Generation with World Models", "comment": "58 pages", "summary": "We release Code World Model (CWM), a 32-billion-parameter open-weights LLM,\nto advance research on code generation with world models. To improve code\nunderstanding beyond what can be learned from training on static code alone, we\nmid-train CWM on a large amount of observation-action trajectories from Python\ninterpreter and agentic Docker environments, and perform extensive multi-task\nreasoning RL in verifiable coding, math, and multi-turn software engineering\nenvironments. With CWM, we provide a strong testbed for researchers to explore\nthe opportunities world modeling affords for improving code generation with\nreasoning and planning in computational environments. We present first steps of\nhow world models can benefit agentic coding, enable step-by-step simulation of\nPython code execution, and show early results of how reasoning can benefit from\nthe latter. CWM is a dense, decoder-only LLM trained with a context size of up\nto 131k tokens. Independent of its world modeling capabilities, CWM offers\nstrong performance on general coding and math tasks: it reaches pass@1 scores\nof 65.8% on SWE-bench Verified (with test-time scaling), 68.6% on\nLiveCodeBench, 96.6% on Math-500, and 76.0% on AIME 2024. To support further\nresearch on code world modeling, we release model checkpoints after\nmid-training, SFT, and RL.", "AI": {"tldr": "Code World Model (CWM)\u662f\u4e00\u4e2a320\u4ebf\u53c2\u6570\u7684\u5f00\u6e90LLM\uff0c\u901a\u8fc7\u5728Python\u89e3\u91ca\u5668\u548cDocker\u73af\u5883\u4e2d\u8bad\u7ec3\u89c2\u5bdf-\u884c\u52a8\u8f68\u8ff9\u6765\u589e\u5f3a\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u53ef\u9a8c\u8bc1\u7f16\u7801\u3001\u6570\u5b66\u548c\u591a\u8f6e\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u63a8\u8fdb\u4ee3\u7801\u751f\u6210\u4e0e\u4e16\u754c\u6a21\u578b\u7684\u7814\u7a76\uff0c\u6539\u5584\u4ec5\u4ece\u9759\u6001\u4ee3\u7801\u8bad\u7ec3\u4e2d\u5b66\u5230\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u63a2\u7d22\u4e16\u754c\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u63a8\u7406\u548c\u89c4\u5212\u6f5c\u529b\u3002", "method": "\u5728Python\u89e3\u91ca\u5668\u548cDocker\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u91cf\u89c2\u5bdf-\u884c\u52a8\u8f68\u8ff9\u7684\u4e2d\u671f\u8bad\u7ec3\uff0c\u5e76\u5728\u53ef\u9a8c\u8bc1\u7f16\u7801\u3001\u6570\u5b66\u548c\u591a\u8f6e\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u4efb\u52a1\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u3002", "result": "CWM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff1aSWE-bench Verified 65.8%\u3001LiveCodeBench 68.6%\u3001Math-500 96.6%\u3001AIME 2024 76.0%\u3002", "conclusion": "CWM\u4e3a\u7814\u7a76\u4ee3\u7801\u4e16\u754c\u5efa\u6a21\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u5728\u4ee3\u7406\u7f16\u7801\u3001Python\u4ee3\u7801\u6267\u884c\u6a21\u62df\u548c\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\u3002", "topic": "code agent"}}
{"id": "2510.02389", "categories": ["cs.SE", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02389", "abs": "https://arxiv.org/abs/2510.02389", "authors": ["Haoran Xi", "Minghao Shao", "Brendan Dolan-Gavitt", "Muhammad Shafique", "Ramesh Karri"], "title": "From Trace to Line: LLM Agent for Real-World OSS Vulnerability Localization", "comment": null, "summary": "Large language models show promise for vulnerability discovery, yet\nprevailing methods inspect code in isolation, struggle with long contexts, and\nfocus on coarse function- or file-level detections - offering limited\nactionable guidance to engineers who need precise line-level localization and\ntargeted patches in real-world software development. We present T2L-Agent\n(Trace-to-Line Agent), a project-level, end-to-end framework that plans its own\nanalysis and progressively narrows scope from modules to exact vulnerable\nlines. T2L-Agent couples multi-round feedback with an Agentic Trace Analyzer\n(ATA) that fuses runtime evidence - crash points, stack traces, and coverage\ndeltas - with AST-based code chunking, enabling iterative refinement beyond\nsingle pass predictions and translating symptoms into actionable, line-level\ndiagnoses. To benchmark line-level vulnerability discovery, we introduce\nT2L-ARVO, a diverse, expert-verified 50-case benchmark spanning five crash\nfamilies and real-world projects. T2L-ARVO is specifically designed to support\nboth coarse-grained detection and fine-grained localization, enabling rigorous\nevaluation of systems that aim to move beyond file-level predictions. On\nT2L-ARVO, T2L-Agent achieves up to 58.0% detection and 54.8% line-level\nlocalization, substantially outperforming baselines. Together, the framework\nand benchmark push LLM-based vulnerability detection from coarse identification\ntoward deployable, robust, precision diagnostics that reduce noise and\naccelerate patching in open-source software workflows.", "AI": {"tldr": "T2L-Agent\u662f\u4e00\u4e2a\u9879\u76ee\u7ea7\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6a21\u5757\u9010\u6b65\u7f29\u5c0f\u8303\u56f4\u5230\u5177\u4f53\u6f0f\u6d1e\u884c\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u8bc1\u636e\u548cAST\u4ee3\u7801\u5206\u5757\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u6f0f\u6d1e\u5b9a\u4f4d\u548c\u4fee\u590d\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u5b64\u7acb\u5206\u6790\u4ee3\u7801\u3001\u96be\u4ee5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u3001\u4ec5\u63d0\u4f9b\u51fd\u6570\u6216\u6587\u4ef6\u7ea7\u522b\u7684\u7c97\u7c92\u5ea6\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u5de5\u7a0b\u5e08\u6709\u7528\u7684\u7cbe\u786e\u884c\u7ea7\u5b9a\u4f4d\u548c\u9488\u5bf9\u6027\u4fee\u590d\u6307\u5bfc\u3002", "method": "T2L-Agent\u91c7\u7528\u591a\u8f6e\u53cd\u9988\u673a\u5236\uff0c\u7ed3\u5408Agentic Trace Analyzer\u878d\u5408\u8fd0\u884c\u65f6\u8bc1\u636e\uff08\u5d29\u6e83\u70b9\u3001\u5806\u6808\u8ddf\u8e2a\u3001\u8986\u76d6\u7387\u5dee\u5f02\uff09\u548cAST\u4ee3\u7801\u5206\u5757\uff0c\u5b9e\u73b0\u8fed\u4ee3\u7cbe\u70bc\u3002", "result": "\u5728T2L-ARVO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT2L-Agent\u8fbe\u523058.0%\u7684\u68c0\u6d4b\u7387\u548c54.8%\u7684\u884c\u7ea7\u5b9a\u4f4d\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u57fa\u4e8eLLM\u7684\u6f0f\u6d1e\u68c0\u6d4b\u4ece\u7c97\u7c92\u5ea6\u8bc6\u522b\u63a8\u5411\u53ef\u90e8\u7f72\u3001\u7a33\u5065\u7684\u7cbe\u786e\u8bca\u65ad\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u52a0\u901f\u5f00\u6e90\u8f6f\u4ef6\u5de5\u4f5c\u6d41\u4e2d\u7684\u4fee\u590d\u8fc7\u7a0b\u3002", "topic": "code agent"}}
{"id": "2510.02393", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02393", "abs": "https://arxiv.org/abs/2510.02393", "authors": ["Jianqing Zhang", "Wei Xia", "Hande Dong", "Qiang Lin", "Jian Cao"], "title": "AP2O: Correcting LLM-Generated Code Errors Type by Type Like Humans via Adaptive Progressive Preference Optimization", "comment": null, "summary": "LLMs' code generation capabilities have yielded substantial improvements in\nthe effectiveness of programming tasks. However, LLM-generated code still\nsuffers from compilation and runtime errors. Existing offline preference\noptimization methods primarily focus on enhancing LLMs' coding abilities using\npass/fail signals in the preference data, overlooking the deep-level error\ntypes in the failed codes. To address this, we propose Adaptively Progressive\nPreference Optimization (AP2O) for coding (i.e., AP2O-Coder), a method that\nguides LLMs adaptively and methodically to reduce code errors for code\ngeneration. Specifically, we construct an error notebook from failed codes and\nprogressively optimize the LLM to correct errors type by type. Furthermore, we\nadaptively replay error types to tailor to the LLM's changing weaknesses\nthroughout the training process. Through extensive experiments on both code and\ngeneral LLMs (Llama, Qwen, and DeepSeek series) with parameters ranging from\n0.5B to 34B, our AP2O-Coder improves code generation performance by up to 3% in\npass@k while using less preference data. Code: https://github.com/TsingZ0/AP2O", "AI": {"tldr": "AP2O-Coder\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u6e10\u8fdb\u5f0f\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9519\u8bef\u7b14\u8bb0\u672c\u6765\u9010\u6b65\u4f18\u5316LLM\uff0c\u6309\u7c7b\u578b\u7ea0\u6b63\u4ee3\u7801\u9519\u8bef\uff0c\u5e76\u81ea\u9002\u5e94\u91cd\u653e\u9519\u8bef\u7c7b\u578b\u6765\u9002\u5e94LLM\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5316\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u901a\u8fc7/\u5931\u8d25\u4fe1\u53f7\u6765\u589e\u5f3aLLM\u7684\u7f16\u7801\u80fd\u529b\uff0c\u4f46\u5ffd\u89c6\u4e86\u5931\u8d25\u4ee3\u7801\u4e2d\u7684\u6df1\u5c42\u9519\u8bef\u7c7b\u578b\u3002", "method": "\u6784\u5efa\u9519\u8bef\u7b14\u8bb0\u672c\uff0c\u9010\u6b65\u4f18\u5316LLM\u6309\u7c7b\u578b\u7ea0\u6b63\u9519\u8bef\uff0c\u5e76\u81ea\u9002\u5e94\u91cd\u653e\u9519\u8bef\u7c7b\u578b\u6765\u9002\u5e94LLM\u7684\u53d8\u5316\u5f31\u70b9\u3002", "result": "\u57280.5B\u523034B\u53c2\u6570\u7684\u4ee3\u7801\u548c\u901a\u7528LLM\u4e0a\uff0cAP2O-Coder\u5c06\u4ee3\u7801\u751f\u6210\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe3%\uff08pass@k\uff09\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u504f\u597d\u6570\u636e\u3002", "conclusion": "AP2O-Coder\u901a\u8fc7\u5173\u6ce8\u6df1\u5c42\u9519\u8bef\u7c7b\u578b\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2510.02418", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02418", "abs": "https://arxiv.org/abs/2510.02418", "authors": ["Sagnik Anupam", "Davis Brown", "Shuo Li", "Eric Wong", "Hamed Hassani", "Osbert Bastani"], "title": "BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks", "comment": null, "summary": "LLM web agents now browse and take actions on the open web, yet current agent\nevaluations are constrained to sandboxed environments or artificial tasks. We\nintroduce BrowserArena, a live open-web agent evaluation platform that collects\nuser-submitted tasks, runs Arena-style head-to-head comparisons, and uses\nstep-level human feedback to surface failure modes. Collecting and analyzing\nstep-level annotations on the agent traces, we identify three consistent\nfailure modes: captcha resolution, pop-up banner removal, and direct navigation\nto URLs. By constructing targeted datasets to further study these tasks, we\ndiscover variations in how different language models navigate these failure\nmodes. We find, for example, that o4-mini deploys a wider variety of strategies\nto circumvent captcha resolution than other models and DeepSeek-R1 consistently\nmisleads users about captcha resolution. Our findings surface both the\ndiversity and brittleness of current web agents. More broadly, our benchmarking\nmethodology provides an approach to evaluating and understanding web agent\nfailure modes at scale.", "AI": {"tldr": "BrowserArena\u662f\u4e00\u4e2a\u5b9e\u65f6\u5f00\u653e\u7f51\u7edc\u4ee3\u7406\u8bc4\u4f30\u5e73\u53f0\uff0c\u901a\u8fc7\u7528\u6237\u63d0\u4ea4\u4efb\u52a1\u3001\u5934\u5bf9\u5934\u6bd4\u8f83\u548c\u6b65\u9aa4\u7ea7\u4eba\u5de5\u53cd\u9988\u6765\u8bc6\u522b\u7f51\u7edc\u4ee3\u7406\u7684\u5931\u8d25\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\u4e09\u4e2a\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a\u9a8c\u8bc1\u7801\u89e3\u51b3\u3001\u5f39\u7a97\u79fb\u9664\u548c\u76f4\u63a5URL\u5bfc\u822a\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u4ee3\u7406\u8bc4\u4f30\u5c40\u9650\u4e8e\u6c99\u76d2\u73af\u5883\u6216\u4eba\u5de5\u4efb\u52a1\uff0c\u9700\u8981\u771f\u5b9e\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u8bc4\u4f30\u5e73\u53f0\u6765\u8bc6\u522b\u4ee3\u7406\u7684\u5b9e\u9645\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u6784\u5efaBrowserArena\u5e73\u53f0\uff0c\u6536\u96c6\u7528\u6237\u4efb\u52a1\uff0c\u8fdb\u884c\u5934\u5bf9\u5934\u6bd4\u8f83\uff0c\u4f7f\u7528\u6b65\u9aa4\u7ea7\u4eba\u5de5\u53cd\u9988\u5206\u6790\u4ee3\u7406\u8f68\u8ff9\uff0c\u5e76\u9488\u5bf9\u5931\u8d25\u6a21\u5f0f\u6784\u5efa\u4e13\u95e8\u6570\u636e\u96c6\u8fdb\u884c\u6df1\u5165\u7814\u7a76\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u4e2a\u4e00\u81f4\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u9a8c\u8bc1\u7801\u89e3\u51b3\u3001\u5f39\u7a97\u79fb\u9664\u548c\u76f4\u63a5URL\u5bfc\u822a\u3002\u53d1\u73b0\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u65f6\u5b58\u5728\u5dee\u5f02\uff0c\u5982o4-mini\u4f7f\u7528\u66f4\u591a\u7b56\u7565\u89c4\u907f\u9a8c\u8bc1\u7801\uff0c\u800cDeepSeek-R1\u5728\u9a8c\u8bc1\u7801\u89e3\u51b3\u4e0a\u8bef\u5bfc\u7528\u6237\u3002", "conclusion": "\u5f53\u524d\u7f51\u7edc\u4ee3\u7406\u8868\u73b0\u51fa\u591a\u6837\u6027\u548c\u8106\u5f31\u6027\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u7406\u89e3\u7f51\u7edc\u4ee3\u7406\u5931\u8d25\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.02328", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.02328", "abs": "https://arxiv.org/abs/2510.02328", "authors": ["Ziqing Wang", "Chengsheng Mao", "Xiaole Wen", "Yuan Luo", "Kaize Ding"], "title": "AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering", "comment": "EMNLP Findings", "summary": "Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise\nin medical visual question answering (Med-VQA). However, when deployed in\nlow-resource settings where abundant labeled data are unavailable, existing\nMed-MLLMs commonly fail due to their medical reasoning capability bottlenecks:\n(i) the intrinsic reasoning bottleneck that ignores the details from the\nmedical image; (ii) the extrinsic reasoning bottleneck that fails to\nincorporate specialized medical knowledge. To address those limitations, we\npropose AMANDA, a training-free agentic framework that performs medical\nknowledge augmentation via LLM agents. Specifically, our intrinsic medical\nknowledge augmentation focuses on coarse-to-fine question decomposition for\ncomprehensive diagnosis, while extrinsic medical knowledge augmentation grounds\nthe reasoning process via biomedical knowledge graph retrieval. Extensive\nexperiments across eight Med-VQA benchmarks demonstrate substantial\nimprovements in both zero-shot and few-shot Med-VQA settings. The code is\navailable at https://github.com/REAL-Lab-NU/AMANDA.", "AI": {"tldr": "\u63d0\u51fa\u4e86AMANDA\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u8fdb\u884c\u533b\u5b66\u77e5\u8bc6\u589e\u5f3a\uff0c\u89e3\u51b3\u533b\u5b66\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u63a8\u7406\u74f6\u9888\u95ee\u9898", "motivation": "\u73b0\u6709Med-MLLMs\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u56e0\u533b\u5b66\u63a8\u7406\u80fd\u529b\u74f6\u9888\u800c\u5931\u8d25\uff0c\u5305\u62ec\u5ffd\u7565\u533b\u5b66\u56fe\u50cf\u7ec6\u8282\u7684\u5185\u5728\u63a8\u7406\u74f6\u9888\u548c\u672a\u80fd\u6574\u5408\u4e13\u4e1a\u533b\u5b66\u77e5\u8bc6\u7684\u5916\u5728\u63a8\u7406\u74f6\u9888", "method": "\u4f7f\u7528\u8bad\u7ec3\u514d\u8d39\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5728\u533b\u5b66\u77e5\u8bc6\u589e\u5f3a\uff08\u7c97\u5230\u7ec6\u95ee\u9898\u5206\u89e3\uff09\u548c\u5916\u5728\u533b\u5b66\u77e5\u8bc6\u589e\u5f3a\uff08\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\uff09\u6765\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b", "result": "\u57288\u4e2aMed-VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "AMANDA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86Med-MLLMs\u7684\u63a8\u7406\u74f6\u9888\uff0c\u5728\u4f4e\u8d44\u6e90\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u8272", "topic": "agent analysis"}}
{"id": "2510.02609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02609", "abs": "https://arxiv.org/abs/2510.02609", "authors": ["Chengquan Guo", "Chulin Xie", "Yu Yang", "Zhaorun Chen", "Zinan Lin", "Xander Davies", "Yarin Gal", "Dawn Song", "Bo Li"], "title": "RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents", "comment": null, "summary": "Code agents have gained widespread adoption due to their strong code\ngeneration capabilities and integration with code interpreters, enabling\ndynamic execution, debugging, and interactive programming capabilities. While\nthese advancements have streamlined complex workflows, they have also\nintroduced critical safety and security risks. Current static safety benchmarks\nand red-teaming tools are inadequate for identifying emerging real-world risky\nscenarios, as they fail to cover certain boundary conditions, such as the\ncombined effects of different jailbreak tools. In this work, we propose\nRedCodeAgent, the first automated red-teaming agent designed to systematically\nuncover vulnerabilities in diverse code agents. With an adaptive memory module,\nRedCodeAgent can leverage existing jailbreak knowledge, dynamically select the\nmost effective red-teaming tools and tool combinations in a tailored toolbox\nfor a given input query, thus identifying vulnerabilities that might otherwise\nbe overlooked. For reliable evaluation, we develop simulated sandbox\nenvironments to additionally evaluate the execution results of code agents,\nmitigating potential biases of LLM-based judges that only rely on static code.\nThrough extensive evaluations across multiple state-of-the-art code agents,\ndiverse risky scenarios, and various programming languages, RedCodeAgent\nconsistently outperforms existing red-teaming methods, achieving higher attack\nsuccess rates and lower rejection rates with high efficiency. We further\nvalidate RedCodeAgent on real-world code assistants, e.g., Cursor and Codeium,\nexposing previously unidentified security risks. By automating and optimizing\nred-teaming processes, RedCodeAgent enables scalable, adaptive, and effective\nsafety assessments of code agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86RedCodeAgent\uff0c\u9996\u4e2a\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\u4ee3\u7406\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u4ee3\u7801\u4ee3\u7406\u4e2d\u7684\u6f0f\u6d1e\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5185\u5b58\u6a21\u5757\u548c\u52a8\u6001\u5de5\u5177\u9009\u62e9\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u9759\u6001\u5b89\u5168\u57fa\u51c6\u548c\u7ea2\u961f\u6d4b\u8bd5\u5de5\u5177\u65e0\u6cd5\u8986\u76d6\u65b0\u5174\u7684\u73b0\u5b9e\u98ce\u9669\u573a\u666f\uff0c\u7279\u522b\u662f\u4e0d\u540c\u8d8a\u72f1\u5de5\u5177\u7684\u7ec4\u5408\u6548\u5e94\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u5177\u6709\u81ea\u9002\u5e94\u5185\u5b58\u6a21\u5757\u7684\u81ea\u52a8\u5316\u7ea2\u961f\u4ee3\u7406\uff0c\u80fd\u591f\u5229\u7528\u73b0\u6709\u8d8a\u72f1\u77e5\u8bc6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u6709\u6548\u7684\u7ea2\u961f\u6d4b\u8bd5\u5de5\u5177\u7ec4\u5408\uff0c\u5e76\u4f7f\u7528\u6a21\u62df\u6c99\u7bb1\u73af\u5883\u8bc4\u4f30\u6267\u884c\u7ed3\u679c\u3002", "result": "RedCodeAgent\u5728\u591a\u4e2a\u5148\u8fdb\u4ee3\u7801\u4ee3\u7406\u3001\u4e0d\u540c\u98ce\u9669\u573a\u666f\u548c\u7f16\u7a0b\u8bed\u8a00\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u66f4\u4f4e\u7684\u62d2\u7edd\u7387\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u548c\u4f18\u5316\u7ea2\u961f\u6d4b\u8bd5\u6d41\u7a0b\uff0cRedCodeAgent\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u4e14\u6709\u6548\u7684\u4ee3\u7801\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2510.02557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02557", "abs": "https://arxiv.org/abs/2510.02557", "authors": ["Charlie Masters", "Advaith Vellanki", "Jiangbo Shangguan", "Bart Kultys", "Jonathan Gilmore", "Alastair Moore", "Stefano V. Albrecht"], "title": "Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge", "comment": "Accepted as an oral paper for the conference for Distributed\n  Artificial Intelligence (DAI 2025). 8 pages, 2 figures", "summary": "While agentic AI has advanced in automating individual tasks, managing\ncomplex multi-agent workflows remains a challenging problem. This paper\npresents a research vision for autonomous agentic systems that orchestrate\ncollaboration within dynamic human-AI teams. We propose the Autonomous Manager\nAgent as a core challenge: an agent that decomposes complex goals into task\ngraphs, allocates tasks to human and AI workers, monitors progress, adapts to\nchanging conditions, and maintains transparent stakeholder communication. We\nformalize workflow management as a Partially Observable Stochastic Game and\nidentify four foundational challenges: (1) compositional reasoning for\nhierarchical decomposition, (2) multi-objective optimization under shifting\npreferences, (3) coordination and planning in ad hoc teams, and (4) governance\nand compliance by design. To advance this agenda, we release MA-Gym, an\nopen-source simulation and evaluation framework for multi-agent workflow\norchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we\nfind they struggle to jointly optimize for goal completion, constraint\nadherence, and workflow runtime - underscoring workflow management as a\ndifficult open problem. We conclude with organizational and ethical\nimplications of autonomous management systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u4e3b\u7ba1\u7406\u4ee3\u7406\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u534f\u8c03\u52a8\u6001\u4eba\u673a\u56e2\u961f\u4e2d\u7684\u590d\u6742\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u968f\u673a\u535a\u5f08\uff0c\u5e76\u8bc6\u522b\u4e86\u56db\u4e2a\u57fa\u7840\u6311\u6218\u3002", "motivation": "\u867d\u7136\u4ee3\u7406AI\u5728\u81ea\u52a8\u5316\u5355\u4e2a\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u7ba1\u7406\u590d\u6742\u7684\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u4e3b\u7ba1\u7406\u4ee3\u7406\u4f5c\u4e3a\u6838\u5fc3\u6311\u6218\uff0c\u5c06\u5de5\u4f5c\u6d41\u7ba1\u7406\u5f62\u5f0f\u5316\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u968f\u673a\u535a\u5f08\uff0c\u5e76\u5f00\u53d1\u4e86MA-Gym\u5f00\u6e90\u4eff\u771f\u6846\u67b6\u6765\u8bc4\u4f30GPT-5\u57fa\u7840\u7684\u7ba1\u7406\u4ee3\u7406\u3002", "result": "\u572820\u4e2a\u5de5\u4f5c\u6d41\u4e2d\u8bc4\u4f30GPT-5\u57fa\u7840\u7684\u7ba1\u7406\u4ee3\u7406\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u540c\u65f6\u4f18\u5316\u76ee\u6807\u5b8c\u6210\u3001\u7ea6\u675f\u9075\u5b88\u548c\u5de5\u4f5c\u6d41\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5de5\u4f5c\u6d41\u7ba1\u7406\u662f\u4e00\u4e2a\u56f0\u96be\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u81ea\u4e3b\u7ba1\u7406\u7cfb\u7edf\u7684\u7ec4\u7ec7\u548c\u4f26\u7406\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2510.02567", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02567", "abs": "https://arxiv.org/abs/2510.02567", "authors": ["Peter Pak", "Achuth Chandrasekhar", "Amir Barati Farimani"], "title": "Agentic Additive Manufacturing Alloy Discovery", "comment": null, "summary": "Agentic systems enable the intelligent use of research tooling, augmenting a\nresearcher's ability to investigate and propose novel solutions to existing\nproblems. Within Additive Manufacturing (AM), alloy discovery remains a complex\nchallenge, often requiring expertise in the various domains of materials\nscience, thermodynamic simulations, and experimental analysis. Large Language\nModel (LLM) enabled agents can facilitate this endeavor by utilizing their\nextensive knowledge base to dispatch tool calls via Model Context Protocol\n(MCP) to perform actions such as Thermo-Calc property diagram calculations and\nlack of fusion process map generation. In addition, the multi-agent system\ndeveloped in this work is able to effectively reason through complex user\nprompts and provide analysis on the printability of proposed alloys. These\nagents can dynamically adjust their task trajectory to the outcomes of tool\ncall results, effectively enabling autonomous decision-making in practical\nenvironments. This work aims to utilize LLM enabled agents to automate and\naccelerate the task of alloy discovery within the field of additive\nmanufacturing and showcase the benefits of adopting this multi-agent system.", "AI": {"tldr": "\u5229\u7528LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u5316\u589e\u6750\u5236\u9020\u4e2d\u7684\u5408\u91d1\u53d1\u73b0\u8fc7\u7a0b\uff0c\u901a\u8fc7MCP\u534f\u8bae\u8c03\u7528\u70ed\u529b\u5b66\u8ba1\u7b97\u5de5\u5177\uff0c\u5b9e\u73b0\u81ea\u4e3b\u51b3\u7b56\u548c\u5408\u91d1\u53ef\u6253\u5370\u6027\u5206\u6790\u3002", "motivation": "\u589e\u6750\u5236\u9020\u4e2d\u7684\u5408\u91d1\u53d1\u73b0\u9700\u8981\u6750\u6599\u79d1\u5b66\u3001\u70ed\u529b\u5b66\u6a21\u62df\u548c\u5b9e\u9a8c\u5206\u6790\u7b49\u591a\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u8fc7\u7a0b\u590d\u6742\u4e14\u8017\u65f6\u3002LLM\u667a\u80fd\u4f53\u53ef\u4ee5\u5229\u7528\u5176\u77e5\u8bc6\u5e93\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\u6765\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7MCP\u534f\u8bae\u8c03\u7528Thermo-Calc\u5c5e\u6027\u56fe\u8ba1\u7b97\u548c\u7194\u5408\u4e0d\u8db3\u8fc7\u7a0b\u56fe\u751f\u6210\u7b49\u5de5\u5177\uff0c\u80fd\u591f\u6839\u636e\u5de5\u5177\u8c03\u7528\u7ed3\u679c\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u8f68\u8ff9\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63a8\u7406\u590d\u6742\u7528\u6237\u63d0\u793a\uff0c\u5bf9\u63d0\u8bae\u5408\u91d1\u7684\u53ef\u6253\u5370\u6027\u63d0\u4f9b\u5206\u6790\uff0c\u5728\u5b9e\u8df5\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u51b3\u7b56\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u5316\u548c\u52a0\u901f\u589e\u6750\u5236\u9020\u4e2d\u7684\u5408\u91d1\u53d1\u73b0\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u91c7\u7528\u8fd9\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2510.02589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02589", "abs": "https://arxiv.org/abs/2510.02589", "authors": ["Yunqi Huang", "Nishith Chennakeshava", "Alexis Carras", "Vladislav Neverov", "Wei Liu", "Aske Plaat", "Yingjie Fan"], "title": "A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem", "comment": null, "summary": "Container stowage planning (CSPP) is a critical component of maritime\ntransportation and terminal operations, directly affecting supply chain\nefficiency. Owing to its complexity, CSPP has traditionally relied on human\nexpertise. While reinforcement learning (RL) has recently been applied to CSPP,\nsystematic benchmark comparisons across different algorithms remain limited. To\naddress this gap, we develop a Gym environment that captures the fundamental\nfeatures of CSPP and extend it to include crane scheduling in both multi-agent\nand single-agent formulations. Within this framework, we evaluate five RL\nalgorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying\ncomplexity. The results reveal distinct performance gaps with increasing\ncomplexity, underscoring the importance of algorithm choice and problem\nformulation for CSPP. Overall, this paper benchmarks multiple RL methods for\nCSPP while providing a reusable Gym environment with crane scheduling, thus\noffering a foundation for future research and practical deployment in maritime\nlogistics.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u8d77\u91cd\u673a\u8c03\u5ea6\u7684\u96c6\u88c5\u7bb1\u914d\u8f7d\u89c4\u5212Gym\u73af\u5883\uff0c\u8bc4\u4f30\u4e865\u79cdRL\u7b97\u6cd5\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7b97\u6cd5\u9009\u62e9\u548c\u95ee\u9898\u8868\u8ff0\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u96c6\u88c5\u7bb1\u914d\u8f7d\u89c4\u5212(CSPP)\u5bf9\u6d77\u8fd0\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4e0d\u540cRL\u7b97\u6cd5\u7684\u7cfb\u7edf\u6027\u57fa\u51c6\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u8d77\u91cd\u673a\u8c03\u5ea6\u7684Gym\u73af\u5883\uff0c\u5728\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e0b\u8bc4\u4f30DQN\u3001QR-DQN\u3001A2C\u3001PPO\u548cTRPO\u4e94\u79cdRL\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u968f\u7740\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4e0d\u540c\u7b97\u6cd5\u95f4\u5b58\u5728\u660e\u663e\u6027\u80fd\u5dee\u8ddd\uff0c\u7b97\u6cd5\u9009\u62e9\u548c\u95ee\u9898\u8868\u8ff0\u5bf9CSPP\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3aCSPP\u63d0\u4f9b\u4e86\u591aRL\u65b9\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u53ef\u91cd\u7528\u7684Gym\u73af\u5883\uff0c\u4e3a\u6d77\u8fd0\u7269\u6d41\u7684\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.02854", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02854", "abs": "https://arxiv.org/abs/2510.02854", "authors": ["Boshuai Ye", "Arif Ali Khan", "Teemu Pihkakoski", "Peng Liang", "Muhammad Azeem Akbar", "Matti Silveri", "Lauri Malmi"], "title": "C2|Q>: A Robust Framework for Bridging Classical and Quantum Software Development", "comment": "46 pages, 8 images, 14 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Quantum Software Engineering (QSE) is emerging as a critical discipline to\nmake quantum computing accessible to a broader developer community; however,\nmost quantum development environments still require developers to engage with\nlow-level details across the software stack - including problem encoding,\ncircuit construction, algorithm configuration, hardware selection, and result\ninterpretation - making them difficult for classical software engineers to use.\nTo bridge this gap, we present C2|Q>: a hardware-agnostic quantum software\ndevelopment framework that translates classical specifications (code) into\nquantum-executable programs while preserving methodological rigor. The\nframework applies modular software engineering principles by classifying the\nworkflow into three core modules: an encoder that classifies problems, produces\nQuantum-Compatible Formats (QCFs), and constructs quantum circuits, a\ndeployment module that generates circuits and recommends hardware based on\nfidelity, runtime, and cost, and a decoder that interprets quantum outputs into\nclassical solutions. In evaluation, the encoder module achieved a 93.8%\ncompletion rate, the hardware recommendation module consistently selected the\nappropriate quantum devices for workloads scaling up to 56 qubits, and the full\nC2|Q>: workflow successfully processed classical specifications (434 Python\nsnippets and 100 JSON inputs) with completion rates of 93.8% and 100%,\nrespectively. For case study problems executed on publicly available NISQ\nhardware, C2|Q>: reduced the required implementation effort by nearly 40X\ncompared to manual implementations using low-level quantum software development\nkits (SDKs), with empirical runs limited to small- and medium-sized instances\nconsistent with current NISQ capabilities. The open-source implementation of\nC2|Q>: is available at https://github.com/C2-Q/C2Q", "AI": {"tldr": "C2|Q>\u662f\u4e00\u4e2a\u786c\u4ef6\u65e0\u5173\u7684\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ecf\u5178\u4ee3\u7801\u89c4\u8303\u8f6c\u6362\u4e3a\u91cf\u5b50\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u4f7f\u7ecf\u5178\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u80fd\u591f\u66f4\u5bb9\u6613\u5730\u8fdb\u884c\u91cf\u5b50\u8ba1\u7b97\u5f00\u53d1\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u5f00\u53d1\u73af\u5883\u8981\u6c42\u5f00\u53d1\u8005\u5904\u7406\u8f6f\u4ef6\u6808\u7684\u4f4e\u7ea7\u7ec6\u8282\uff0c\u5305\u62ec\u95ee\u9898\u7f16\u7801\u3001\u7535\u8def\u6784\u5efa\u3001\u7b97\u6cd5\u914d\u7f6e\u7b49\uff0c\u8fd9\u4f7f\u5f97\u7ecf\u5178\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u96be\u4ee5\u4f7f\u7528\u91cf\u5b50\u8ba1\u7b97\u3002", "method": "\u6846\u67b6\u91c7\u7528\u6a21\u5757\u5316\u8f6f\u4ef6\u5de5\u7a0b\u539f\u5219\uff0c\u5206\u4e3a\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u7f16\u7801\u5668\uff08\u5206\u7c7b\u95ee\u9898\u3001\u751f\u6210\u91cf\u5b50\u517c\u5bb9\u683c\u5f0f\u3001\u6784\u5efa\u91cf\u5b50\u7535\u8def\uff09\u3001\u90e8\u7f72\u6a21\u5757\uff08\u751f\u6210\u7535\u8def\u3001\u57fa\u4e8e\u4fdd\u771f\u5ea6\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u6210\u672c\u63a8\u8350\u786c\u4ef6\uff09\u3001\u89e3\u7801\u5668\uff08\u5c06\u91cf\u5b50\u8f93\u51fa\u89e3\u91ca\u4e3a\u7ecf\u5178\u89e3\u51b3\u65b9\u6848\uff09\u3002", "result": "\u7f16\u7801\u5668\u6a21\u5757\u5b8c\u6210\u7387\u8fbe\u523093.8%\uff0c\u786c\u4ef6\u63a8\u8350\u6a21\u5757\u4e3a\u6700\u591a56\u91cf\u5b50\u6bd4\u7279\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6b63\u786e\u9009\u62e9\u91cf\u5b50\u8bbe\u5907\uff0c\u5b8c\u6574\u5de5\u4f5c\u6d41\u5904\u7406434\u4e2aPython\u4ee3\u7801\u7247\u6bb5\u548c100\u4e2aJSON\u8f93\u5165\u7684\u5b8c\u6210\u7387\u5206\u522b\u4e3a93.8%\u548c100%\u3002\u5728NISQ\u786c\u4ef6\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u663e\u793a\uff0c\u76f8\u6bd4\u624b\u52a8\u5b9e\u73b0\u51cf\u5c11\u4e86\u8fd140\u500d\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "C2|Q>\u6210\u529f\u964d\u4f4e\u4e86\u91cf\u5b50\u8ba1\u7b97\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u4f7f\u7ecf\u5178\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u5f00\u53d1\u91cf\u5b50\u5e94\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65b9\u6cd5\u8bba\u7684\u4e25\u8c28\u6027\u3002", "topic": "swe application"}}
{"id": "2510.02887", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02887", "abs": "https://arxiv.org/abs/2510.02887", "authors": ["Zhao Zhang", "Qingyuan Liang", "Zeyu Sun", "Yizhou Chen", "Guoqing Wang", "Yican Sun", "Lu Zhang", "Ge Li", "Yingfei Xiong"], "title": "GramTrans: A Better Code Representation Approach in Code Generation", "comment": null, "summary": "Code generation has shown great promise in assisting software development. A\nfundamental yet underexplored question is how the choice of code representation\naffects model performance. While existing studies employ various\nrepresentations, such as treating code as plain text, grammar rule sequences,\nor syntax tree sequences, they lack a principled understanding of the\nrelationship between parsing difficulty and model effectiveness. This paper\nproposes a conjecture: the easier a representation is to parse, the better\nperformance the model achieves. We formalize this idea using grammar classes,\nwhere representations in simpler classes (e.g., LL(1)) are easier to parse.\nThrough a controlled experiment on a Python-based DSL, we show that parsing\ndifficulty strongly correlates with model performance. Motivated by this\nfinding, we present GramTrans, a general approach that automatically transforms\na context-free language into a representation within the LL(1) class. GramTrans\nintroduces a novel hierarchical conflict elimination algorithm, enabling a\nflexible trade-off between syntactic simplicity and token efficiency. We\nevaluate GramTrans on both Python and Java using three code generation models:\nStarCoder 1B, DeepSeek-Coder 1.3B, and Qwen2.5 1.5B. Across multiple\nbenchmarks, GramTrans consistently delivers significant improvements over\nbaseline representations. Furthermore, our analysis of existing representations\nreconfirms the strong alignment between parsing difficulty and model\nperformance, providing additional support for the conjecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u731c\u60f3\uff1a\u4ee3\u7801\u8868\u793a\u8d8a\u5bb9\u6613\u89e3\u6790\uff0c\u6a21\u578b\u6027\u80fd\u8d8a\u597d\u3002\u901a\u8fc7GramTrans\u65b9\u6cd5\u5c06\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8f6c\u6362\u4e3aLL(1)\u7c7b\u8868\u793a\uff0c\u5b9e\u9a8c\u8bc1\u660e\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6027\u80fd\u5f3a\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u4e0d\u540c\u4ee3\u7801\u8868\u793a\uff08\u7eaf\u6587\u672c\u3001\u8bed\u6cd5\u89c4\u5219\u5e8f\u5217\u3001\u8bed\u6cd5\u6811\u5e8f\u5217\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6548\u679c\u5173\u7cfb\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u63d0\u51faGramTrans\u65b9\u6cd5\uff0c\u4f7f\u7528\u5206\u5c42\u51b2\u7a81\u6d88\u9664\u7b97\u6cd5\u5c06\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8f6c\u6362\u4e3aLL(1)\u7c7b\u8868\u793a\uff0c\u5728\u89e3\u6790\u96be\u5ea6\u548c\u6807\u8bb0\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5728Python\u548cJava\u4e0a\u4f7f\u7528\u4e09\u4e2a\u4ee3\u7801\u751f\u6210\u6a21\u578b\u8bc4\u4f30\uff0cGramTrans\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u8868\u793a\u6301\u7eed\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u89e3\u6790\u96be\u5ea6\u4e0e\u6a21\u578b\u6027\u80fd\u5f3a\u76f8\u5173\uff0cGramTrans\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2510.02334", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02334", "abs": "https://arxiv.org/abs/2510.02334", "authors": ["Zhe Li", "Wei Zhao", "Yige Li", "Jun Sun"], "title": "Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing", "comment": "16 pages, 4 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir deployment is frequently undermined by undesirable behaviors such as\ngenerating harmful content, factual inaccuracies, and societal biases.\nDiagnosing the root causes of these failures poses a critical challenge for AI\nsafety. Existing attribution methods, particularly those based on parameter\ngradients, often fall short due to prohibitive noisy signals and computational\ncomplexity. In this work, we introduce a novel and efficient framework that\ndiagnoses a range of undesirable LLM behaviors by analyzing representation and\nits gradients, which operates directly in the model's activation space to\nprovide a semantically meaningful signal linking outputs to their training\ndata. We systematically evaluate our method for tasks that include tracking\nharmful content, detecting backdoor poisoning, and identifying knowledge\ncontamination. The results demonstrate that our approach not only excels at\nsample-level attribution but also enables fine-grained token-level analysis,\nprecisely identifying the specific samples and phrases that causally influence\nmodel behavior. This work provides a powerful diagnostic tool to understand,\naudit, and ultimately mitigate the risks associated with LLMs. The code is\navailable at https://github.com/plumprc/RepT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8868\u793a\u548c\u68af\u5ea6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bca\u65adLLM\u4e2d\u7684\u4e0d\u826f\u884c\u4e3a\uff0c\u5305\u62ec\u6709\u5bb3\u5185\u5bb9\u3001\u540e\u95e8\u6295\u6bd2\u548c\u77e5\u8bc6\u6c61\u67d3\u7b49\u95ee\u9898\uff0c\u63d0\u4f9b\u6837\u672c\u7ea7\u548c\u7ec6\u7c92\u5ea6token\u7ea7\u5206\u6790\u3002", "motivation": "LLM\u90e8\u7f72\u4e2d\u5b58\u5728\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3001\u4e8b\u5b9e\u9519\u8bef\u548c\u793e\u4f1a\u504f\u89c1\u7b49\u4e0d\u826f\u884c\u4e3a\uff0c\u73b0\u6709\u57fa\u4e8e\u53c2\u6570\u68af\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\u56e0\u566a\u58f0\u4fe1\u53f7\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5728\u6a21\u578b\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5206\u6790\u8868\u793a\u53ca\u5176\u68af\u5ea6\uff0c\u63d0\u4f9b\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u4fe1\u53f7\uff0c\u5c06\u8f93\u51fa\u4e0e\u8bad\u7ec3\u6570\u636e\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u65b9\u6cd5\u5728\u8ffd\u8e2a\u6709\u5bb3\u5185\u5bb9\u3001\u68c0\u6d4b\u540e\u95e8\u6295\u6bd2\u548c\u8bc6\u522b\u77e5\u8bc6\u6c61\u67d3\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u80fd\u8fdb\u884c\u6837\u672c\u7ea7\u5f52\u56e0\uff0c\u8fd8\u80fd\u5b9e\u73b0\u7ec6\u7c92\u5ea6token\u7ea7\u5206\u6790\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u7406\u89e3\u3001\u5ba1\u8ba1\u5e76\u6700\u7ec8\u51cf\u8f7b\u4e0eLLM\u76f8\u5173\u7684\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "2510.02611", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02611", "abs": "https://arxiv.org/abs/2510.02611", "authors": ["Yuheng Wu", "Azalia Mirhoseini", "Thierry Tambe"], "title": "On the Role of Temperature Sampling in Test-Time Scaling", "comment": null, "summary": "Large language models (LLMs) can improve reasoning at inference time through\ntest-time scaling (TTS), where multiple reasoning traces are generated and the\nbest one is selected. Prior work shows that increasing the number of samples K\nsteadily improves accuracy. In this paper, we demonstrate that this trend does\nnot hold indefinitely: at large K, further scaling yields no gains, and certain\nhard questions remain unsolved regardless of the number of traces.\nInterestingly, we find that different sampling temperatures solve different\nsubsets of problems, implying that single-temperature scaling explores only\npart of a model's potential. We therefore propose scaling along the temperature\ndimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3\n(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME\n2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an\nadditional 7.3 points over single-temperature TTS. Temperature scaling also\nenables base models to reach performance comparable to reinforcement learning\n(RL)-trained counterparts, without additional post-training. We further provide\na comprehensive analysis of this phenomenon and design a multi-temperature\nvoting method that reduces the overhead of temperature scaling. Overall, our\nfindings suggest that TTS is more powerful than previously thought, and that\ntemperature scaling offers a simple and effective way to unlock the latent\npotential of base models.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6e29\u5ea6\u7ef4\u5ea6\u7f29\u653e\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u6e29\u5ea6\u6295\u7968\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7f\u57fa\u7840\u6a21\u578b\u8fbe\u5230\u63a5\u8fd1\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4ec5\u901a\u8fc7\u589e\u52a0\u6837\u672c\u6570\u91cf\u6765\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u6837\u672c\u6570\u8fc7\u5927\u65f6\u6548\u679c\u6709\u9650\uff0c\u4e14\u4e0d\u540c\u91c7\u6837\u6e29\u5ea6\u80fd\u89e3\u51b3\u4e0d\u540c\u5b50\u96c6\u7684\u95ee\u9898\uff0c\u8868\u660e\u5355\u6e29\u5ea6\u6269\u5c55\u53ea\u63a2\u7d22\u4e86\u6a21\u578b\u6f5c\u529b\u7684\u4e00\u90e8\u5206\u3002", "method": "\u63d0\u51fa\u6e29\u5ea6\u7ef4\u5ea6\u7f29\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u6e29\u5ea6\u4e0b\u751f\u6210\u63a8\u7406\u8f68\u8ff9\u5e76\u9009\u62e9\u6700\u4f73\u7ed3\u679c\uff0c\u6269\u5927\u6a21\u578b\u7684\u63a8\u7406\u8fb9\u754c\u3002\u8bbe\u8ba1\u4e86\u591a\u6e29\u5ea6\u6295\u7968\u65b9\u6cd5\u6765\u51cf\u5c11\u6e29\u5ea6\u7f29\u653e\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728Qwen3\u7cfb\u5217\u6a21\u578b\u548c\u4e94\u4e2a\u4ee3\u8868\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6e29\u5ea6\u7f29\u653e\u6bd4\u5355\u6e29\u5ea6\u6d4b\u8bd5\u65f6\u6269\u5c55\u989d\u5916\u63d0\u5347\u4e867.3\u4e2a\u70b9\u3002\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6e29\u5ea6\u7f29\u653e\u80fd\u8fbe\u5230\u4e0e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u540e\u8bad\u7ec3\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u66f4\u5f3a\u5927\uff0c\u6e29\u5ea6\u7f29\u653e\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u91ca\u653e\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.02917", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02917", "abs": "https://arxiv.org/abs/2510.02917", "authors": ["Kriz Tahimic", "Charibeth Cheng"], "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders", "comment": null, "summary": "As Large Language Models become integral to software development, with\nsubstantial portions of AI-suggested code entering production, understanding\ntheir internal correctness mechanisms becomes critical for safe deployment. We\napply sparse autoencoders to decompose LLM representations, identifying\ndirections that correspond to code correctness. We select predictor directions\nusing t-statistics and steering directions through separation scores from base\nmodel representations, then analyze their mechanistic properties through\nsteering, attention analysis, and weight orthogonalization. We find that code\ncorrectness directions in LLMs reliably predict incorrect code, while\ncorrection capabilities, though statistically significant, involve tradeoffs\nbetween fixing errors and preserving correct code. Mechanistically, successful\ncode generation depends on attending to test cases rather than problem\ndescriptions. Moreover, directions identified in base models retain their\neffectiveness after instruction-tuning, suggesting code correctness mechanisms\nlearned during pre-training are repurposed during fine-tuning. Our mechanistic\ninsights suggest three practical applications: prompting strategies should\nprioritize test examples over elaborate problem descriptions, predictor\ndirections can serve as error alarms for developer review, and these same\npredictors can guide selective steering, intervening only when errors are\nanticipated to prevent the code corruption from constant steering.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\u8868\u793a\uff0c\u8bc6\u522b\u4ee3\u7801\u6b63\u786e\u6027\u65b9\u5411\uff0c\u53d1\u73b0\u4ee3\u7801\u6b63\u786e\u6027\u65b9\u5411\u80fd\u53ef\u9760\u9884\u6d4b\u9519\u8bef\u4ee3\u7801\uff0c\u4f46\u4fee\u6b63\u80fd\u529b\u9700\u8981\u5728\u4fee\u590d\u9519\u8bef\u548c\u4fdd\u7559\u6b63\u786e\u4ee3\u7801\u4e4b\u95f4\u6743\u8861\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5185\u90e8\u6b63\u786e\u6027\u673a\u5236\u5bf9\u4e8e\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5e94\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\u8868\u793a\uff0c\u901a\u8fc7t\u7edf\u8ba1\u91cf\u9009\u62e9\u9884\u6d4b\u65b9\u5411\uff0c\u901a\u8fc7\u5206\u79bb\u5206\u6570\u4ece\u57fa\u7840\u6a21\u578b\u8868\u793a\u4e2d\u63d0\u53d6\u5f15\u5bfc\u65b9\u5411\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u3001\u6ce8\u610f\u529b\u5206\u6790\u548c\u6743\u91cd\u6b63\u4ea4\u5316\u5206\u6790\u5176\u673a\u5236\u7279\u6027\u3002", "result": "\u4ee3\u7801\u6b63\u786e\u6027\u65b9\u5411\u80fd\u53ef\u9760\u9884\u6d4b\u9519\u8bef\u4ee3\u7801\uff0c\u4fee\u6b63\u80fd\u529b\u867d\u7edf\u8ba1\u663e\u8457\u4f46\u9700\u6743\u8861\uff1b\u6210\u529f\u7684\u4ee3\u7801\u751f\u6210\u4f9d\u8d56\u4e8e\u5173\u6ce8\u6d4b\u8bd5\u7528\u4f8b\u800c\u975e\u95ee\u9898\u63cf\u8ff0\uff1b\u57fa\u7840\u6a21\u578b\u4e2d\u8bc6\u522b\u7684\u65b9\u5411\u5728\u6307\u4ee4\u5fae\u8c03\u540e\u4ecd\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "\u4ee3\u7801\u6b63\u786e\u6027\u673a\u5236\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u5e76\u5728\u5fae\u8c03\u65f6\u88ab\u91cd\u65b0\u5229\u7528\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u5b9e\u9645\u5e94\u7528\uff1a\u63d0\u793a\u7b56\u7565\u5e94\u4f18\u5148\u6d4b\u8bd5\u7528\u4f8b\u3001\u9884\u6d4b\u65b9\u5411\u53ef\u4f5c\u4e3a\u9519\u8bef\u8b66\u62a5\u3001\u9009\u62e9\u6027\u5f15\u5bfc\u53ef\u9632\u6b62\u4ee3\u7801\u635f\u574f\u3002", "topic": "agent analysis"}}
{"id": "2510.02934", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.02934", "abs": "https://arxiv.org/abs/2510.02934", "authors": ["Thanh Trong Vu", "Tuan-Dung Bui", "Thu-Trang Nguyen", "Son Nguyen", "Hieu Dinh Vo"], "title": "Model-Agnostic Correctness Assessment for LLM-Generated Code via Dynamic Internal Representation Selection", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation and are increasingly integrated into the software development\nprocess. However, ensuring the correctness of LLM-generated code remains a\ncritical concern. Prior work has shown that the internal representations of\nLLMs encode meaningful signals for assessing code correctness. Nevertheless,\nthe existing methods rely on representations from pre-selected/fixed layers and\ntoken positions, which could limit its generalizability across diverse model\narchitectures and tasks. In this work, we introduce AUTOPROBE, a novel\nmodel-agnostic approach that dynamically selects the most informative internal\nrepresentations for code correctness assessment. AUTOPROBE employs an\nattention-based mechanism to learn importance scores for hidden states,\nenabling it to focus on the most relevant features. These weighted\nrepresentations are then aggregated and passed to a probing classifier to\npredict code correctness across multiple dimensions, including compilability,\nfunctionality, and security. To evaluate the performance of AUTOPROBE, we\nconduct extensive experiments across multiple benchmarks and code LLMs. Our\nexperimental results show that AUTOPROBE consistently outperforms the\nbaselines. For security assessment, AUTOPROBE surpasses the state-of-the-art\nwhite-box approach by 18%. For compilability and functionality assessment,\nAUTOPROBE demonstrates its highest robustness to code complexity, with the\nperformance higher than the other approaches by up to 19% and 111%,\nrespectively. These findings highlight that dynamically selecting important\ninternal signals enables AUTOPROBE to serve as a robust and generalizable\nsolution for assessing the correctness of code generated by various LLMs.", "AI": {"tldr": "AUTOPROBE\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9LLM\u5185\u90e8\u6700\u6709\u4fe1\u606f\u91cf\u7684\u8868\u793a\u6765\u8bc4\u4f30\u4ee3\u7801\u6b63\u786e\u6027\uff0c\u5728\u7f16\u8bd1\u6027\u3001\u529f\u80fd\u6027\u548c\u5b89\u5168\u6027\u8bc4\u4f30\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u9009\u5c42\u548c\u6807\u8bb0\u4f4d\u7f6e\u7684\u8868\u793a\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u4efb\u52a1\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u52a8\u6001\u9009\u62e9\u91cd\u8981\u5185\u90e8\u8868\u793a\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u673a\u5236\u5b66\u4e60\u9690\u85cf\u72b6\u6001\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u805a\u7126\u6700\u76f8\u5173\u7279\u5f81\uff0c\u7136\u540e\u805a\u5408\u52a0\u6743\u8868\u793a\u5e76\u901a\u8fc7\u63a2\u6d4b\u5206\u7c7b\u5668\u9884\u6d4b\u4ee3\u7801\u6b63\u786e\u6027\uff08\u5305\u62ec\u7f16\u8bd1\u6027\u3001\u529f\u80fd\u6027\u548c\u5b89\u5168\u6027\uff09\u3002", "result": "AUTOPROBE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801LLMs\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5b89\u5168\u6027\u8bc4\u4f30\u6bd4\u6700\u5148\u8fdb\u7684\u767d\u76d2\u65b9\u6cd5\u9ad818%\uff0c\u7f16\u8bd1\u6027\u548c\u529f\u80fd\u6027\u8bc4\u4f30\u5206\u522b\u6bd4\u5176\u4ed6\u65b9\u6cd5\u9ad819%\u548c111%\uff0c\u5bf9\u4ee3\u7801\u590d\u6742\u6027\u5177\u6709\u6700\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u52a8\u6001\u9009\u62e9\u91cd\u8981\u5185\u90e8\u4fe1\u53f7\u4f7fAUTOPROBE\u6210\u4e3a\u8bc4\u4f30\u5404\u79cdLLMs\u751f\u6210\u4ee3\u7801\u6b63\u786e\u6027\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.02669", "categories": ["cs.AI", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.02669", "abs": "https://arxiv.org/abs/2510.02669", "authors": ["Bo Ma", "Hang Li", "ZeHua Hu", "XiaoFan Gui", "LuYao Liu", "Simon Liu"], "title": "AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models", "comment": null, "summary": "Multi-agent systems powered by large language models have demonstrated\nremarkable capabilities across diverse domains, yet existing automated design\napproaches seek monolithic solutions that fail to adapt resource allocation\nbased on query complexity and domain requirements. This paper introduces\nAutoMaAS, a self-evolving multi-agent architecture search framework that\nleverages neural architecture search principles to automatically discover\noptimal agent configurations through dynamic operator lifecycle management and\nautomated machine learning techniques. Our approach incorporates four key\ninnovations: (1) automatic operator generation, fusion, and elimination based\non performance-cost analysis, (2) dynamic cost-aware optimization with\nreal-time parameter adjustment, (3) online feedback integration for continuous\narchitecture refinement, and (4) enhanced interpretability through decision\ntracing mechanisms. Extensive experiments across six benchmarks demonstrate\nthat AutoMaAS achieves 1.0-7.1\\% performance improvement while reducing\ninference costs by 3-5\\% compared to state-of-the-art methods. The framework\nshows superior transferability across datasets and LLM backbones, establishing\na new paradigm for automated multi-agent system design in the era of large\nlanguage models.", "AI": {"tldr": "AutoMaAS\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u539f\u7406\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u63a8\u7406\u6210\u672c\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u8bbe\u8ba1\u65b9\u6cd5\u5bfb\u6c42\u5355\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u6cd5\u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u548c\u9886\u57df\u9700\u6c42\u81ea\u9002\u5e94\u5206\u914d\u8d44\u6e90\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u539f\u5219\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u81ea\u52a8\u64cd\u4f5c\u7b26\u751f\u6210\u3001\u878d\u5408\u548c\u6d88\u9664\uff1b\u52a8\u6001\u6210\u672c\u611f\u77e5\u4f18\u5316\uff1b\u5728\u7ebf\u53cd\u9988\u96c6\u6210\uff1b\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u51b3\u7b56\u8ffd\u8e2a\u673a\u5236\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoMaAS\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e861.0-7.1%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c113-5%\u7684\u63a8\u7406\u6210\u672c\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "AutoMaAS\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u5316\u8bbe\u8ba1\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2510.02677", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02677", "abs": "https://arxiv.org/abs/2510.02677", "authors": ["Zhaorun Chen", "Xun Liu", "Mintong Kang", "Jiawei Zhang", "Minzhou Pan", "Shuang Yang", "Bo Li"], "title": "ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks", "comment": "60 pages, 16 figures", "summary": "As vision-language models (VLMs) gain prominence, their multimodal interfaces\nalso introduce new safety vulnerabilities, making the safety evaluation\nchallenging and critical. Existing red-teaming efforts are either restricted to\na narrow set of adversarial patterns or depend heavily on manual engineering,\nlacking scalable exploration of emerging real-world VLM vulnerabilities. To\nbridge this gap, we propose ARMs, an adaptive red-teaming agent that\nsystematically conducts comprehensive risk assessments for VLMs. Given a target\nharmful behavior or risk definition, ARMs automatically optimizes diverse\nred-teaming strategies with reasoning-enhanced multi-step orchestration, to\neffectively elicit harmful outputs from target VLMs. We propose 11 novel\nmultimodal attack strategies, covering diverse adversarial patterns of VLMs\n(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming\nalgorithms into ARMs via model context protocol (MCP). To balance the diversity\nand effectiveness of the attack, we design a layered memory with an\nepsilon-greedy attack exploration algorithm. Extensive experiments on instance-\nand policy-based benchmarks show that ARMs achieves SOTA attack success rates,\nexceeding baselines by an average of 52.1% and surpassing 90% on\nClaude-4-Sonnet. We show that the diversity of red-teaming instances generated\nby ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.\nLeveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety\ndataset comprising over 30K red-teaming instances spanning 51 diverse risk\ncategories, grounded in both real-world multimodal threats and regulatory\nrisks. Safety fine-tuning with ARMs-Bench substantially improves the robustness\nof VLMs while preserving their general utility, providing actionable guidance\nto improve multimodal safety alignment against emerging threats.", "AI": {"tldr": "\u63d0\u51faARMs\u81ea\u9002\u5e94\u7ea2\u961f\u4ee3\u7406\uff0c\u901a\u8fc7\u63a8\u7406\u589e\u5f3a\u7684\u591a\u6b65\u9aa4\u7f16\u6392\u81ea\u52a8\u4f18\u5316\u591a\u6837\u5316\u7ea2\u961f\u7b56\u7565\uff0c\u6709\u6548\u5f15\u53d1\u76ee\u6807VLM\u7684\u6709\u5bb3\u8f93\u51fa\u3002\u8bbe\u8ba1\u4e8611\u79cd\u65b0\u578b\u591a\u6a21\u6001\u653b\u51fb\u7b56\u7565\u548c17\u79cd\u7ea2\u961f\u7b97\u6cd5\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u666e\u53ca\uff0c\u5176\u591a\u6a21\u6001\u63a5\u53e3\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u6709\u9650\u7684\u5bf9\u6297\u6a21\u5f0f\uff0c\u8981\u4e48\u4f9d\u8d56\u4eba\u5de5\u5de5\u7a0b\uff0c\u7f3a\u4e4f\u5bf9\u65b0\u5174\u771f\u5b9e\u4e16\u754cVLM\u6f0f\u6d1e\u7684\u53ef\u6269\u5c55\u63a2\u7d22\u3002", "method": "\u63d0\u51faARMs\u81ea\u9002\u5e94\u7ea2\u961f\u4ee3\u7406\uff0c\u91c7\u7528\u63a8\u7406\u589e\u5f3a\u7684\u591a\u6b65\u9aa4\u7f16\u6392\u81ea\u52a8\u4f18\u5316\u7ea2\u961f\u7b56\u7565\u3002\u8bbe\u8ba1\u4e8611\u79cd\u591a\u6a21\u6001\u653b\u51fb\u7b56\u7565\u548c17\u79cd\u7ea2\u961f\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u96c6\u6210\u3002\u4f7f\u7528\u5206\u5c42\u8bb0\u5fc6\u548cepsilon-greedy\u653b\u51fb\u63a2\u7d22\u7b97\u6cd5\u5e73\u8861\u653b\u51fb\u591a\u6837\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u5728\u5b9e\u4f8b\u548c\u7b56\u7565\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARMs\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e73\u5747\u8d85\u8fc7\u57fa\u7ebf52.1%\uff0c\u5728Claude-4-Sonnet\u4e0a\u8d85\u8fc790%\u3002\u751f\u6210\u7684\u7ea2\u961f\u5b9e\u4f8b\u591a\u6837\u6027\u663e\u8457\u66f4\u9ad8\uff0c\u63ed\u793a\u4e86VLM\u7684\u65b0\u5174\u6f0f\u6d1e\u3002\u6784\u5efa\u4e86\u5305\u542b30K+\u5b9e\u4f8b\u7684ARMs-Bench\u6570\u636e\u96c6\u3002", "conclusion": "ARMs\u80fd\u591f\u6709\u6548\u63ed\u793aVLM\u7684\u65b0\u5174\u6f0f\u6d1e\uff0c\u57fa\u4e8eARMs-Bench\u7684\u5b89\u5168\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86VLM\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u6548\u7528\uff0c\u4e3a\u6539\u8fdb\u591a\u6a21\u6001\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u884c\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.02339", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02339", "abs": "https://arxiv.org/abs/2510.02339", "authors": ["Kevin Zhou", "Adam Dejl", "Gabriel Freedman", "Lihu Chen", "Antonio Rago", "Francesca Toni"], "title": "Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models", "comment": "Accepted at EMNLP Findings 2025", "summary": "Research in uncertainty quantification (UQ) for large language models (LLMs)\nis increasingly important towards guaranteeing the reliability of this\ngroundbreaking technology. We explore the integration of LLM UQ methods in\nargumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making\nbased on computational argumentation in which UQ plays a critical role. We\nconduct experiments to evaluate ArgLLMs' performance on claim verification\ntasks when using different LLM UQ methods, inherently performing an assessment\nof the UQ methods' effectiveness. Moreover, the experimental procedure itself\nis a novel way of evaluating the effectiveness of UQ methods, especially when\nintricate and potentially contentious statements are present. Our results\ndemonstrate that, despite its simplicity, direct prompting is an effective UQ\nstrategy in ArgLLMs, outperforming considerably more complex approaches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8bba\u8bc1\u6027\u5927\u8bed\u8a00\u6a21\u578b(ArgLLMs)\u4e2d\u96c6\u6210\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540cUQ\u65b9\u6cd5\u5728\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7b80\u5355\u7684\u76f4\u63a5\u63d0\u793a\u7b56\u7565\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u4fdd\u8bc1\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22UQ\u65b9\u6cd5\u5728\u57fa\u4e8e\u8ba1\u7b97\u8bba\u8bc1\u7684\u53ef\u89e3\u91ca\u51b3\u7b56\u6846\u67b6ArgLLMs\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u5728ArgLLMs\u6846\u67b6\u4e2d\u96c6\u6210\u4e0d\u540c\u7684LLM UQ\u65b9\u6cd5\uff0c\u5e76\u5728\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u540c\u65f6\u8be5\u5b9e\u9a8c\u8fc7\u7a0b\u672c\u8eab\u4e5f\u662f\u4e00\u79cd\u8bc4\u4f30UQ\u65b9\u6cd5\u6709\u6548\u6027\u7684\u65b0\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u7b80\u5355\uff0c\u76f4\u63a5\u63d0\u793a\u7b56\u7565\u5728ArgLLMs\u4e2d\u662f\u4e00\u79cd\u6709\u6548\u7684UQ\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5728\u8bba\u8bc1\u6027LLM\u6846\u67b6\u4e2d\uff0c\u7b80\u5355\u7684\u76f4\u63a5\u63d0\u793aUQ\u7b56\u7565\u6bd4\u590d\u6742\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u4e3aLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.02340", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02340", "abs": "https://arxiv.org/abs/2510.02340", "authors": ["Xin Gao", "Ruiyi Zhang", "Daniel Du", "Saurabh Mahindre", "Sai Ashish Somayajula", "Pengtao Xie"], "title": "Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs", "comment": null, "summary": "Large Language Models (LLMs) are widely used for temporal prediction, but\ntheir reliance on pretraining data raises contamination concerns, as accurate\npredictions on pre-cutoff test data may reflect memorization rather than\nreasoning, leading to an overestimation of their generalization capability.\nWith the recent emergence of prompting-based unlearning techniques, a natural\nquestion arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?\nIn this work, we investigate the capability of prompting to simulate earlier\nknowledge cutoff in LLMs. We construct three evaluation datasets to assess the\nextent to which LLMs can forget (1) direct factual knowledge, (2) semantic\nshifts, and (3) causally related knowledge. Results demonstrate that while\nprompt-based simulated knowledge cutoffs show effectiveness when directly\nqueried with the information after that date, they struggle to induce\nforgetting when the forgotten content is not directly asked but causally\nrelated to the query. These findings highlight the need for more rigorous\nevaluation settings when applying LLMs for temporal prediction tasks. The full\ndataset and evaluation code are available at\nhttps://github.com/gxx27/time_unlearn.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u63d0\u793a\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u65e9\u671f\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u63d0\u793a\u65b9\u6cd5\u5728\u76f4\u63a5\u67e5\u8be2\u65f6\u6709\u6548\uff0c\u4f46\u5728\u5904\u7406\u56e0\u679c\u76f8\u5173\u4f46\u672a\u76f4\u63a5\u8be2\u95ee\u7684\u9057\u5fd8\u5185\u5bb9\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u9884\u6d4b\u4e2d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u80fd\u56e0\u6570\u636e\u6c61\u67d3\u800c\u9ad8\u4f30\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u80fd\u5426\u901a\u8fc7\u63d0\u793a\u6a21\u62df\u65e9\u671f\u77e5\u8bc6\u622a\u6b62\u65e5\u671f\u3002", "method": "\u6784\u5efa\u4e09\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5206\u522b\u6d4b\u8bd5LLMs\u9057\u5fd8\u76f4\u63a5\u4e8b\u5b9e\u77e5\u8bc6\u3001\u8bed\u4e49\u53d8\u5316\u548c\u56e0\u679c\u76f8\u5173\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u62df\u77e5\u8bc6\u622a\u6b62\u65b9\u6cd5\u3002", "result": "\u63d0\u793a\u65b9\u6cd5\u5728\u76f4\u63a5\u67e5\u8be2\u622a\u6b62\u65e5\u671f\u540e\u4fe1\u606f\u65f6\u6709\u6548\uff0c\u4f46\u5728\u5904\u7406\u56e0\u679c\u76f8\u5173\u4f46\u672a\u76f4\u63a5\u8be2\u95ee\u7684\u9057\u5fd8\u5185\u5bb9\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u5728LLMs\u65f6\u5e8f\u9884\u6d4b\u4efb\u52a1\u4e2d\u91c7\u7528\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u8bbe\u7f6e\uff0c\u5f53\u524d\u63d0\u793a\u65b9\u6cd5\u5728\u8bf1\u5bfc\u9057\u5fd8\u56e0\u679c\u76f8\u5173\u4f46\u672a\u76f4\u63a5\u8be2\u95ee\u7684\u5185\u5bb9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "topic": "agent analysis"}}
{"id": "2510.02816", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02816", "abs": "https://arxiv.org/abs/2510.02816", "authors": ["Yulong Zhang", "Li Wang", "Wei Du", "Peilin Li", "Yuqin Dai Zhiyuan Zhao", "Lingyong Fang", "Ziniu Liu", "Ru Zhang", "Huijia Zhu", "Gongshen Liu"], "title": "NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning", "comment": null, "summary": "Verifying multi-step reasoning in large language models is difficult due to\nimprecise error localization and high token costs. Existing methods either\nassess entire reasoning chains, suffering attention dilution, or rely on\nexpensive multi-sampling. We introduce Node-wise Consistency Verification\n(NCV), a training-free framework that recasts verification as lightweight\nbinary consistency checks at the node level. By decomposing the chain of\nthought into interconnected verification nodes, NCV precisely localizes errors\nand avoids unnecessary long-form generation. Experiments demonstrate that our\napproach enhances interpretability and efficiency, presenting a scalable\nsolution for reliable LLM reasoning verification. On public datasets, NCV\nachieves a 10\\% to 25\\% improvement in F1 scores over baselines while utilizing\n$6\\times$~$58\\times$ fewer tokens than traditional methods like CoT-based\nverifiers.", "AI": {"tldr": "\u63d0\u51fa\u4e86Node-wise Consistency Verification (NCV)\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u7ea7\u4e00\u81f4\u6027\u68c0\u67e5\u6765\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u63a8\u7406\uff0c\u63d0\u9ad8\u9519\u8bef\u5b9a\u4f4d\u7cbe\u5ea6\u5e76\u51cf\u5c11token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8bc4\u4f30\u6574\u4e2a\u63a8\u7406\u94fe\u5bfc\u81f4\u6ce8\u610f\u529b\u7a00\u91ca\uff0c\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u591a\u6b21\u91c7\u6837\uff0c\u96be\u4ee5\u7cbe\u786e\u9a8c\u8bc1\u591a\u6b65\u63a8\u7406\u3002", "method": "\u5c06\u63a8\u7406\u94fe\u5206\u89e3\u4e3a\u4e92\u8fde\u7684\u9a8c\u8bc1\u8282\u70b9\uff0c\u8fdb\u884c\u8f7b\u91cf\u7ea7\u4e8c\u8fdb\u5236\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u957f\u6587\u672c\u751f\u6210\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cNCV\u6bd4\u57fa\u7ebf\u65b9\u6cd5F1\u5206\u6570\u63d0\u534710%-25%\uff0c\u540c\u65f6\u6bd4\u4f20\u7edfCoT\u9a8c\u8bc1\u5668\u51cf\u5c116-58\u500dtoken\u4f7f\u7528\u3002", "conclusion": "NCV\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u53ef\u9760LLM\u63a8\u7406\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2510.02341", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02341", "abs": "https://arxiv.org/abs/2510.02341", "authors": ["Yifan Wang", "Bolian Li", "Junlin Wu", "Zhaoxuan Tan", "Zheli Liu", "Ruqi Zhang", "Ananth Grama", "Qingkai Zeng"], "title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning", "comment": null, "summary": "Real-world large language model deployments (e.g., conversational AI systems,\ncode generation assistants) naturally generate abundant implicit user\ndissatisfaction (DSAT) signals, as users iterate toward better answers through\nrefinements, corrections, and expressed preferences, while explicit\nsatisfaction (SAT) feedback is scarce. Existing preference learning approaches\nare poorly aligned with this data profile, as they rely on costly human\nannotations or assume plentiful positive responses. In this paper, we introduce\n\\textbf{DRIFT} (\\textbf{D}issatisfaction-\\textbf{R}efined \\textbf{I}terative\npre\\textbf{F}erence \\textbf{T}raining), which anchors training on real-world\nDSAT signals and samples positives dynamically from the evolving policy.\nEmpirically, DRIFT models trained on real-world \\textit{WildFeedback} datasets\nand synthetic \\textit{UltraFeedback} datasets achieve up to +6.23\\% (7B) /\n+7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B)\non AlpacaEval2 win rate over base models, outperforming strong baseline methods\nsuch as iterative DPO and SPIN. At larger scales, the improvements are\nparticularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on\nWildBench. Further analysis shows that DRIFT also preserves exploratory\ncapacity, yielding more diverse high-reward solutions rather than collapsing to\nnarrow subsets. Theoretically, we demonstrate that this design preserves\npreference margins and avoids the gradient degeneration. These results show\nthat DRIFT is an effective and scalable recipe for real-world post-training\nthat leverages the most abundant and informative signal. The code and data are\navailable at https://github.com/cacayaya/DRIFT.git.", "AI": {"tldr": "DRIFT\u662f\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u4e0d\u6ee1\u610f\u4fe1\u53f7\u7684\u8fed\u4ee3\u504f\u597d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u7528\u6237\u4e0d\u6ee1\u610f\u53cd\u9988\u6765\u8bad\u7ec3LLM\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754cLLM\u90e8\u7f72\u4e2d\u7528\u6237\u4e0d\u6ee1\u610f\u4fe1\u53f7\u4e30\u5bcc\u800c\u663e\u5f0f\u6ee1\u610f\u53cd\u9988\u7a00\u7f3a\uff0c\u73b0\u6709\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u4e0e\u8fd9\u79cd\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u3002", "method": "DRIFT\u4ee5\u771f\u5b9e\u4e16\u754c\u7684\u4e0d\u6ee1\u610f\u4fe1\u53f7\u4e3a\u57fa\u7840\uff0c\u4ece\u4e0d\u65ad\u6f14\u5316\u7684\u7b56\u7565\u4e2d\u52a8\u6001\u91c7\u6837\u6b63\u4f8b\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728WildBench\u548cAlpacaEval2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDRIFT\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u663e\u8457\uff0c14B\u6a21\u578b\u5728WildBench\u4e0a\u8d85\u8d8aGPT-4o-mini\uff0c\u540c\u65f6\u4fdd\u6301\u63a2\u7d22\u80fd\u529b\u3002", "conclusion": "DRIFT\u662f\u5229\u7528\u6700\u4e30\u5bcc\u548c\u6700\u6709\u4fe1\u606f\u91cf\u7684\u4fe1\u53f7\u8fdb\u884c\u73b0\u5b9e\u4e16\u754c\u540e\u8bad\u7ec3\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02837", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02837", "abs": "https://arxiv.org/abs/2510.02837", "authors": ["Wonjoong Kim", "Sangwu Park", "Yeonjun In", "Sein Kim", "Dongha Lee", "Chanyoung Park"], "title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents", "comment": "Preprint. Under Review", "summary": "Although recent tool-augmented benchmarks incorporate complex user requests\nand diverse tools, the evaluation methods for most of them remain limited to\nanswer matching. However, as the number of steps required to resolve a user\nrequest increases, a proper evaluation of an agent's performance must go beyond\nthe final answer to also assess the problem-solving trajectory, including\npreviously ignored aspects such as efficiency, hallucination, and adaptivity.\nThe most straightforward method for evaluating these aspects is to compare an\nagent's trajectory with the ground-truth trajectory, but this approach is\nfundamentally limited since annotating all valid ground-truth trajectories is\nprohibitively expensive. However, a simple LLM-based evaluator struggles to\nassess trajectories in detail without ground truth. To effectively evaluate the\nagents in this manner, we introduce TRACE, a framework for the\nmulti-dimensional evaluation of tool-augmented LLM agent performance. By\nincorporating an evidence bank, which accumulates knowledge gathered from\npreceding reasoning steps, TRACE enables a multi-faceted analysis and\nevaluation of an agent's reasoning trajectory effectively. To validate our\nframework, we develop a new meta-evaluation dataset by augmenting existing\nbenchmarks with diverse and flawed trajectories, each labeled with\nmulti-faceted performance scores. Our results confirm that TRACE accurately\nevaluates these complex behaviors in a scalable and cost-effective manner, even\nwith small open-source LLMs. Furthermore, we apply our method to evaluate the\ntrajectories that agents produce while solving tool-augmented tasks, presenting\npreviously unreported observations and their corresponding insights.", "AI": {"tldr": "\u63d0\u51fa\u4e86TRACE\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5de5\u5177\u589e\u5f3aLLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u8bc1\u636e\u5e93\u5206\u6790\u63a8\u7406\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7b54\u6848\u5339\u914d\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u4f9d\u8d56\u7b54\u6848\u5339\u914d\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8f68\u8ff9\u3001\u6548\u7387\u3001\u5e7b\u89c9\u548c\u9002\u5e94\u6027\u7b49\u591a\u7ef4\u5ea6\u8868\u73b0\u3002", "method": "\u5f15\u5165TRACE\u6846\u67b6\uff0c\u5305\u542b\u8bc1\u636e\u5e93\u79ef\u7d2f\u5148\u524d\u63a8\u7406\u6b65\u9aa4\u7684\u77e5\u8bc6\uff0c\u652f\u6301\u5bf9\u4ee3\u7406\u63a8\u7406\u8f68\u8ff9\u7684\u591a\u65b9\u9762\u5206\u6790\u548c\u8bc4\u4f30\u3002", "result": "TRACE\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u590d\u6742\u884c\u4e3a\uff0c\u5373\u4f7f\u4f7f\u7528\u5c0f\u578b\u5f00\u6e90LLM\u4e5f\u80fd\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bc4\u4f30\u3002", "conclusion": "TRACE\u6846\u67b6\u4e3a\u5de5\u5177\u589e\u5f3aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u53d1\u73b0\u7684\u89c2\u5bdf\u548c\u6d1e\u89c1\u3002", "topic": "agent analysis"}}
{"id": "2510.03178", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03178", "abs": "https://arxiv.org/abs/2510.03178", "authors": ["Cuong Chi Le", "Minh V. T. Pham", "Cuong Duc Van", "Hoang N. Phan", "Huy N. Phan", "Tien N. Nguyen"], "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code", "comment": null, "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5982\u4f55\u7406\u89e3\u4ee3\u7801\uff0c\u53d1\u73b0\u4ee3\u7801\u901a\u8fc7\u7ed3\u6784\u8bed\u4e49\u548c\u547d\u540d\u4e24\u4e2a\u901a\u9053\u4f20\u9012\u4fe1\u606f\u3002\u53bb\u9664\u547d\u540d\u901a\u9053\u4f1a\u4e25\u91cd\u5f71\u54cd\u610f\u56fe\u7ea7\u4efb\u52a1\uff0c\u5e76\u5728\u6267\u884c\u4efb\u52a1\u4e2d\u66b4\u9732\u547d\u540d\u6a21\u5f0f\u8bb0\u5fc6\u95ee\u9898\u3002\u4f5c\u8005\u5f15\u5165\u8bed\u4e49\u4fdd\u7559\u6df7\u6dc6\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03ClassEval-Obf\u57fa\u51c6\u6765\u66f4\u53ef\u9760\u8bc4\u4f30LLMs\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u867d\u7136LLMs\u5728\u4ee3\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u5982\u4f55\u7406\u89e3\u7a0b\u5e8f\u542b\u4e49\u4ecd\u4e0d\u6e05\u695a\u3002\u5f53\u524d\u57fa\u51c6\u53ef\u80fd\u5956\u52b1\u547d\u540d\u6a21\u5f0f\u7684\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7684\u8bed\u4e49\u63a8\u7406\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f15\u5165\u8bed\u4e49\u4fdd\u7559\u6df7\u6dc6\u65b9\u6cd5\uff0c\u53bb\u9664\u4ee3\u7801\u4e2d\u7684\u547d\u540d\u4fe1\u606f\u4f46\u4fdd\u7559\u7ed3\u6784\u8bed\u4e49\uff0c\u521b\u5efaClassEval-Obf\u6df7\u6dc6\u589e\u5f3a\u57fa\u51c6\u6765\u7cfb\u7edf\u6027\u5730\u6291\u5236\u547d\u540d\u7ebf\u7d22\u3002", "result": "\u53bb\u9664\u547d\u540d\u901a\u9053\u4f1a\u4e25\u91cd\u964d\u4f4e\u610f\u56fe\u7ea7\u4efb\u52a1\u6027\u80fd\uff0c\u5728\u6267\u884c\u4efb\u52a1\u4e2d\u4e5f\u89c2\u5bdf\u5230\u4e00\u81f4\u4e0b\u964d\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u57fa\u51c6\u5bf9\u547d\u540d\u6a21\u5f0f\u8bb0\u5fc6\u7684\u4f9d\u8d56\u3002ClassEval-Obf\u51cf\u5c11\u4e86\u6027\u80fd\u5dee\u8ddd\uff0c\u524a\u5f31\u4e86\u8bb0\u5fc6\u6377\u5f84\u3002", "conclusion": "\u4ee3\u7801\u901a\u8fc7\u7ed3\u6784\u548c\u547d\u540d\u4e24\u4e2a\u901a\u9053\u901a\u4fe1\uff0c\u5f53\u524d\u57fa\u51c6\u53ef\u80fd\u9ad8\u4f30LLMs\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u3002\u6df7\u6dc6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u6807\u8bc6\u7b26\u6cc4\u6f0f\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.03217", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03217", "abs": "https://arxiv.org/abs/2510.03217", "authors": ["Jos\u00e9 Cambronero", "Michele Tufano", "Sherry Shi", "Renyao Wei", "Grant Uy", "Runxiang Cheng", "Chin-Jung Liu", "Shiying Pan", "Satish Chandra", "Pat Rondon"], "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair", "comment": null, "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cdLLM\u7b56\u7565\uff08bug abstention\u548cpatch validation\uff09\u6765\u51cf\u5c11\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\u4e2d\u7684\u566a\u97f3\uff0c\u901a\u8fc7\u6392\u9664\u96be\u4ee5\u4fee\u590d\u7684bug\u548c\u9a8c\u8bc1\u8865\u4e01\u8d28\u91cf\u6765\u63d0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4ea7\u751f\u7684\u566a\u97f3\u95ee\u9898\uff0c\u907f\u514d\u5411\u5f00\u53d1\u8005\u5c55\u793a\u4e0d\u592a\u53ef\u80fd\u6210\u529f\u7684\u8865\u4e01\uff0c\u4ece\u800c\u8282\u7701\u5f00\u53d1\u65f6\u95f4\u5e76\u589e\u5f3a\u5bf9\u81ea\u52a8\u5316\u4ee3\u7801\u66f4\u6539\u7684\u4fe1\u4efb\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u4e92\u8865\u7684LLM\u7b56\u7565\uff1abug abstention\u7b56\u7565\u6392\u9664\u7cfb\u7edf\u96be\u4ee5\u4fee\u590d\u7684bug\uff0cpatch validation\u7b56\u7565\u62d2\u7edd\u4e0d\u592a\u53ef\u80fd\u662f\u597d\u4fee\u590d\u7684\u8865\u4e01\u3002", "result": "\u5728Google\u4ee3\u7801\u5e93\u7684174\u4e2a\u4eba\u5de5\u62a5\u544abug\u4e0a\uff0c\u4e24\u79cd\u7b56\u7565\u7ec4\u5408\u4f7f\u7528\u53ef\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u6700\u591a39\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u7a7a\u6307\u9488\u5f02\u5e38\u548csanitizer\u62a5\u544a\u7684bug\u4e0a\uff0c\u8865\u4e01\u9a8c\u8bc1\u4e5f\u80fd\u63d0\u9ad8\u5e73\u5747\u5355\u6837\u672c\u6210\u529f\u7387\u3002", "conclusion": "\u8fd9\u79cd\u53cc\u7b56\u7565\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\u5728\u5de5\u4e1a\u89c4\u6a21\u4e0a\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "topic": "swe application"}}
{"id": "2510.02590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02590", "abs": "https://arxiv.org/abs/2510.02590", "authors": ["Ahmed Hendawy", "Henrik Metternich", "Th\u00e9o Vincent", "Mahdi Kallel", "Jan Peters", "Carlo D'Eramo"], "title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning", "comment": null, "summary": "The use of target networks is a popular approach for estimating value\nfunctions in deep Reinforcement Learning (RL). While effective, the target\nnetwork remains a compromise solution that preserves stability at the cost of\nslowly moving targets, thus delaying learning. Conversely, using the online\nnetwork as a bootstrapped target is intuitively appealing, albeit well-known to\nlead to unstable learning. In this work, we aim to obtain the best out of both\nworlds by introducing a novel update rule that computes the target using the\nMINimum estimate between the Target and Online network, giving rise to our\nmethod, MINTO. Through this simple, yet effective modification, we show that\nMINTO enables faster and stable value function learning, by mitigating the\npotential overestimation bias of using the online network for bootstrapping.\nNotably, MINTO can be seamlessly integrated into a wide range of value-based\nand actor-critic algorithms with a negligible cost. We evaluate MINTO\nextensively across diverse benchmarks, spanning online and offline RL, as well\nas discrete and continuous action spaces. Across all benchmarks, MINTO\nconsistently improves performance, demonstrating its broad applicability and\neffectiveness.", "AI": {"tldr": "MINTO\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u7f51\u7edc\u548c\u76ee\u6807\u7f51\u7edc\u7684\u6700\u5c0f\u503c\u4f30\u8ba1\u6765\u8ba1\u7b97\u76ee\u6807\u503c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u76ee\u6807\u7f51\u7edc\u66f4\u65b0\u6162\u548c\u5728\u7ebf\u7f51\u7edc\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u7f51\u7edc\u867d\u7136\u7a33\u5b9a\u4f46\u5b66\u4e60\u901f\u5ea6\u6162\uff0c\u800c\u4f7f\u7528\u5728\u7ebf\u7f51\u7edc\u4f5c\u4e3a\u5f15\u5bfc\u76ee\u6807\u867d\u7136\u76f4\u89c2\u4f46\u4f1a\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u3002\u4f5c\u8005\u5e0c\u671b\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u3002", "method": "\u5f15\u5165MINTO\u66f4\u65b0\u89c4\u5219\uff0c\u4f7f\u7528\u76ee\u6807\u7f51\u7edc\u548c\u5728\u7ebf\u7f51\u7edc\u7684\u6700\u5c0f\u503c\u4f30\u8ba1\u6765\u8ba1\u7b97\u76ee\u6807\u503c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u57fa\u4e8e\u503c\u548c\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5305\u62ec\u5728\u7ebf\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u3001\u79bb\u6563\u548c\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\uff0cMINTO\u90fd\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MINTO\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u4fee\u6539\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u66f4\u7a33\u5b9a\u7684\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02352", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02352", "abs": "https://arxiv.org/abs/2510.02352", "authors": ["Yihao Wu", "Tianrui Wang", "Yizhou Peng", "Yi-Wen Chao", "Xuyi Zhuang", "Xinsheng Wang", "Shunshun Yin", "Ziyang Ma"], "title": "Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations", "comment": null, "summary": "While biases in large language models (LLMs), such as stereotypes and\ncultural tendencies in outputs, have been examined and identified, their\npresence and characteristics in spoken dialogue models (SDMs) with audio input\nand output remain largely unexplored. Paralinguistic features, such as age,\ngender, and accent, can affect model outputs; when compounded by multi-turn\nconversations, these effects may exacerbate biases, with potential implications\nfor fairness in decision-making and recommendation tasks. In this paper, we\nsystematically evaluate biases in speech LLMs and study the impact of\nmulti-turn dialogues with repeated negative feedback. Bias is measured using\nGroup Unfairness Score (GUS) for decisions and similarity-based normalized\nstatistics rate (SNSR) for recommendations, across both open-source models like\nQwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o\nAudio and Gemini-2.5-Flash. Our analysis reveals that closed-source models\ngenerally exhibit lower bias, while open-source models are more sensitive to\nage and gender, and recommendation tasks tend to amplify cross-group\ndisparities. We found that biased decisions may persist in multi-turn\nconversations. This work provides the first systematic study of biases in\nend-to-end spoken dialogue models, offering insights towards fair and reliable\naudio-based interactive systems. To facilitate further research, we release the\nFairDialogue dataset and evaluation code.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u95ed\u6e90\u6a21\u578b\u504f\u89c1\u8f83\u4f4e\uff0c\u5f00\u6e90\u6a21\u578b\u5bf9\u5e74\u9f84\u548c\u6027\u522b\u66f4\u654f\u611f\uff0c\u591a\u8f6e\u5bf9\u8bdd\u53ef\u80fd\u52a0\u5267\u504f\u89c1\u3002", "motivation": "\u867d\u7136\u6587\u672cLLM\u4e2d\u7684\u504f\u89c1\u5df2\u88ab\u7814\u7a76\uff0c\u4f46\u5177\u6709\u97f3\u9891\u8f93\u5165\u8f93\u51fa\u7684\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u7279\u5f81\u4ecd\u672a\u63a2\u7d22\uff0c\u8fd9\u5bf9\u51b3\u7b56\u548c\u63a8\u8350\u4efb\u52a1\u7684\u516c\u5e73\u6027\u6709\u91cd\u8981\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Group Unfairness Score\u548csimilarity-based normalized statistics rate\u6307\u6807\uff0c\u8bc4\u4f30\u5f00\u6e90\u548c\u95ed\u6e90\u8bed\u97f3\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u504f\u89c1\u8868\u73b0\u3002", "result": "\u95ed\u6e90\u6a21\u578b\u603b\u4f53\u504f\u89c1\u8f83\u4f4e\uff0c\u5f00\u6e90\u6a21\u578b\u5bf9\u5e74\u9f84\u548c\u6027\u522b\u66f4\u654f\u611f\uff0c\u63a8\u8350\u4efb\u52a1\u4f1a\u653e\u5927\u8de8\u7fa4\u4f53\u5dee\u5f02\uff0c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u504f\u89c1\u51b3\u7b56\u53ef\u80fd\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u7814\u7a76\u7aef\u5230\u7aef\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u504f\u89c1\u7684\u6210\u679c\uff0c\u4e3a\u5f00\u53d1\u516c\u5e73\u53ef\u9760\u7684\u97f3\u9891\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86FairDialogue\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u4ee3\u7801\u3002", "topic": "agent analysis"}}
{"id": "2510.03153", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03153", "abs": "https://arxiv.org/abs/2510.03153", "authors": ["Hima Jacob Leven Suprabha", "Laxmi Nag Laxminarayan Nagesh", "Ajith Nair", "Alvin Reuben Amal Selvaster", "Ayan Khan", "Raghuram Damarla", "Sanju Hannah Samuel", "Sreenithi Saravana Perumal", "Titouan Puech", "Venkataramireddy Marella", "Vishal Sonar", "Alessandro Suglia", "Oliver Lemon"], "title": "Improving Cooperation in Collaborative Embodied AI", "comment": "In proceedings of UKCI 2025", "summary": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u5728\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u884c\u4e3a\u548c\u51b3\u7b56\u65b9\u9762\u7684\u6548\u679c\uff0c\u901a\u8fc7\u6539\u8fdbCoELA\u6846\u67b6\u5e76\u6574\u5408\u8bed\u97f3\u529f\u80fd\u6765\u4f18\u5316LLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u534f\u4f5c\u63a8\u7406\u548cAI\u4ee3\u7406\u5408\u4f5c\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u63d0\u793a\u65b9\u6cd5\u6765\u63d0\u5347\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u529b\u3002", "method": "\u589e\u5f3aCoELA\u6846\u67b6\uff0c\u7cfb\u7edf\u5b9e\u9a8c\u4e0d\u540cLLM\u548c\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0c\u8bc6\u522b\u4f18\u5316\u7ec4\u5408\u4ee5\u6700\u5927\u5316\u534f\u4f5c\u6027\u80fd\uff0c\u5e76\u6574\u5408\u8bed\u97f3\u529f\u80fd\u5b9e\u73b0\u57fa\u4e8e\u8bed\u97f3\u7684\u534f\u4f5c\u4ea4\u4e92\u3002", "result": "\u6700\u4f73\u7ec4\u5408\u76f8\u6bd4\u539f\u59cbCoELA\u7cfb\u7edf\uff0c\u5728Gemma3\u4e0a\u8fd0\u884c\u6548\u7387\u63d0\u5347\u4e8622%\uff0c\u8bed\u97f3\u96c6\u6210\u63d0\u4f9b\u4e86\u66f4\u5177\u5438\u5f15\u529b\u7684\u7528\u6237\u754c\u9762\u3002", "conclusion": "\u63d0\u793a\u4f18\u5316\u80fd\u6709\u6548\u63d0\u5347\u534f\u4f5c\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u8bed\u97f3\u96c6\u6210\u589e\u5f3a\u4e86\u7cfb\u7edf\u5f00\u53d1\u6f14\u793a\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002", "topic": "agent analysis"}}
{"id": "2510.03194", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03194", "abs": "https://arxiv.org/abs/2510.03194", "authors": ["Zichen Chen", "Jiefeng Chen", "Sercan \u00d6. Arik", "Misha Sra", "Tomas Pfister", "Jinsung Yoon"], "title": "CoDA: Agentic Systems for Collaborative Data Visualization", "comment": "31 pages, 6 figures, 5 tables", "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u81ea\u52a8\u751f\u6210\u53ef\u89c6\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u8fdb\u884c\u5143\u6570\u636e\u5206\u6790\u3001\u4efb\u52a1\u89c4\u5212\u3001\u4ee3\u7801\u751f\u6210\u548c\u81ea\u6211\u53cd\u601d\uff0c\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8641.5%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u7cfb\u7edf\u5728\u5904\u7406\u5305\u542b\u591a\u4e2a\u6587\u4ef6\u7684\u590d\u6742\u6570\u636e\u96c6\u548c\u8fed\u4ee3\u4f18\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u7b80\u5316\u4efb\u52a1\uff0c\u65e0\u6cd5\u6709\u6548\u7ba1\u7406\u6570\u636e\u590d\u6742\u6027\u3001\u4ee3\u7801\u9519\u8bef\u6216\u6700\u7ec8\u53ef\u89c6\u5316\u8d28\u91cf\u3002", "method": "\u5c06\u53ef\u89c6\u5316\u81ea\u52a8\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u534f\u4f5c\u591a\u667a\u80fd\u4f53\u95ee\u9898\uff0c\u5f15\u5165CoDA\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u8fdb\u884c\u5143\u6570\u636e\u5206\u6790\u3001\u4efb\u52a1\u89c4\u5212\u3001\u4ee3\u7801\u751f\u6210\u548c\u81ea\u6211\u53cd\u601d\uff0c\u901a\u8fc7\u5143\u6570\u636e\u5206\u6790\u7ed5\u8fc7token\u9650\u5236\uff0c\u901a\u8fc7\u8d28\u91cf\u9a71\u52a8\u7684\u4f18\u5316\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793aCoDA\u5728\u603b\u4f53\u5f97\u5206\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u6bd4\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u9ad8\u51fa41.5%\u3002", "conclusion": "\u53ef\u89c6\u5316\u81ea\u52a8\u5316\u7684\u672a\u6765\u4e0d\u5728\u4e8e\u5b64\u7acb\u7684\u4ee3\u7801\u751f\u6210\uff0c\u800c\u5728\u4e8e\u96c6\u6210\u7684\u3001\u534f\u4f5c\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2510.02360", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02360", "abs": "https://arxiv.org/abs/2510.02360", "authors": ["Mingze Zhong", "Meng Fang", "Zijing Shi", "Yuxuan Huang", "Shunfeng Zheng", "Yali Du", "Ling Chen", "Jun Wang"], "title": "Spiral of Silence in Large Language Model Agents", "comment": null, "summary": "The Spiral of Silence (SoS) theory holds that individuals with minority views\noften refrain from speaking out for fear of social isolation, enabling majority\npositions to dominate public discourse. When the 'agents' are large language\nmodels (LLMs), however, the classical psychological explanation is not directly\napplicable, since SoS was developed for human societies. This raises a central\nquestion: can SoS-like dynamics nevertheless emerge from purely statistical\nlanguage generation in LLM collectives? We propose an evaluation framework for\nexamining SoS in LLM agents. Specifically, we consider four controlled\nconditions that systematically vary the availability of 'History' and 'Persona'\nsignals. Opinion dynamics are assessed using trend tests such as Mann-Kendall\nand Spearman's rank, along with concentration measures including kurtosis and\ninterquartile range. Experiments across open-source and closed-source models\nshow that history and persona together produce strong majority dominance and\nreplicate SoS patterns; history signals alone induce strong anchoring; and\npersona signals alone foster diverse but uncorrelated opinions, indicating that\nwithout historical anchoring, SoS dynamics cannot emerge. The work bridges\ncomputational sociology and responsible AI design, highlighting the need to\nmonitor and mitigate emergent conformity in LLM-agent systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u4f53\u4e2d\u662f\u5426\u4f1a\u51fa\u73b0\u6c89\u9ed8\u87ba\u65cb\u6548\u5e94\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u5386\u53f2\u548c\u89d2\u8272\u4fe1\u53f7\u5171\u540c\u4f5c\u7528\u4f1a\u4ea7\u751f\u5f3a\u70c8\u7684\u591a\u6570\u4e3b\u5bfc\u6a21\u5f0f\uff0c\u590d\u5236\u4e86\u6c89\u9ed8\u87ba\u65cb\u73b0\u8c61\u3002", "motivation": "\u6c89\u9ed8\u87ba\u65cb\u7406\u8bba\u539f\u672c\u7528\u4e8e\u89e3\u91ca\u4eba\u7c7b\u793e\u4f1a\u4e2d\u5c11\u6570\u89c2\u70b9\u56e0\u5bb3\u6015\u5b64\u7acb\u800c\u4fdd\u6301\u6c89\u9ed8\u7684\u73b0\u8c61\uff0c\u4f46\u8be5\u7406\u8bba\u4e0d\u76f4\u63a5\u9002\u7528\u4e8eLLM\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7eaf\u7cb9\u7edf\u8ba1\u8bed\u8a00\u751f\u6210\u662f\u5426\u4f1a\u5728LLM\u96c6\u4f53\u4e2d\u4ea7\u751f\u7c7b\u4f3c\u6c89\u9ed8\u87ba\u65cb\u7684\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u4e86\u8bc4\u4f30LLM\u4ee3\u7406\u4e2d\u6c89\u9ed8\u87ba\u65cb\u7684\u6846\u67b6\uff0c\u7cfb\u7edf\u63a7\u5236'\u5386\u53f2'\u548c'\u89d2\u8272'\u4fe1\u53f7\u7684\u53ef\u7528\u6027\uff0c\u4f7f\u7528\u8d8b\u52bf\u6d4b\u8bd5\u548c\u96c6\u4e2d\u5ea6\u6d4b\u91cf\u6765\u8bc4\u4f30\u610f\u89c1\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u5386\u53f2\u548c\u89d2\u8272\u4fe1\u53f7\u5171\u540c\u4ea7\u751f\u5f3a\u70c8\u7684\u591a\u6570\u4e3b\u5bfc\u5e76\u590d\u5236\u6c89\u9ed8\u87ba\u65cb\u6a21\u5f0f\uff1b\u4ec5\u5386\u53f2\u4fe1\u53f7\u5bfc\u81f4\u5f3a\u70c8\u951a\u5b9a\uff1b\u4ec5\u89d2\u8272\u4fe1\u53f7\u4fc3\u8fdb\u591a\u6837\u4f46\u4e0d\u76f8\u5173\u7684\u610f\u89c1\uff0c\u8868\u660e\u6ca1\u6709\u5386\u53f2\u951a\u5b9a\u5c31\u65e0\u6cd5\u51fa\u73b0\u6c89\u9ed8\u87ba\u65cb\u52a8\u6001\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8fde\u63a5\u4e86\u8ba1\u7b97\u793e\u4f1a\u5b66\u548c\u8d1f\u8d23\u4efbAI\u8bbe\u8ba1\uff0c\u5f3a\u8c03\u9700\u8981\u76d1\u63a7\u548c\u51cf\u8f7bLLM\u4ee3\u7406\u7cfb\u7edf\u4e2d\u51fa\u73b0\u7684\u4ece\u4f17\u73b0\u8c61\u3002", "topic": "agent analysis"}}
{"id": "2510.02362", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02362", "abs": "https://arxiv.org/abs/2510.02362", "authors": ["Matei-Iulian Cocu", "R\u0103zvan-Cosmin Cristia", "Adrian Marius Dumitran"], "title": "A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History", "comment": "10 pages", "summary": "In this case study, we select a set of controversial Romanian historical\nquestions and ask multiple Large Language Models to answer them across\nlanguages and contexts, in order to assess their biases. Besides being a study\nmainly performed for educational purposes, the motivation also lies in the\nrecognition that history is often presented through altered perspectives,\nprimarily influenced by the culture and ideals of a state, even through large\nlanguage models. Since they are often trained on certain data sets that may\npresent certain ambiguities, the lack of neutrality is subsequently instilled\nin users. The research process was carried out in three stages, to confirm the\nidea that the type of response expected can influence, to a certain extent, the\nresponse itself; after providing an affirmative answer to some given question,\nan LLM could shift its way of thinking after being asked the same question\nagain, but being told to respond with a numerical value of a scale. Results\nshow that binary response stability is relatively high but far from perfect and\nvaries by language. Models often flip stance across languages or between\nformats; numeric ratings frequently diverge from the initial binary choice, and\nthe most consistent models are not always those judged most accurate or\nneutral. Our research brings to light the predisposition of models to such\ninconsistencies, within a specific contextualization of the language for the\nquestion asked.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6d4b\u8bd5\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u8bed\u5883\u4e0b\u56de\u7b54\u7f57\u9a6c\u5c3c\u4e9a\u5386\u53f2\u4e89\u8bae\u95ee\u9898\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u8ba4\u8bc6\u5230\u5386\u53f2\u5e38\u56e0\u6587\u5316\u548c\u56fd\u5bb6\u610f\u8bc6\u5f62\u6001\u800c\u5448\u73b0\u4e0d\u540c\u89c6\u89d2\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u504f\u5dee\u53ef\u80fd\u5bfc\u81f4\u7f3a\u4e4f\u4e2d\u7acb\u6027\uff0c\u5f71\u54cd\u7528\u6237\u8ba4\u77e5\u3002", "method": "\u7814\u7a76\u5206\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\uff1a\u5148\u8ba9LLM\u4ee5\u4e8c\u5143\u65b9\u5f0f\u56de\u7b54\u95ee\u9898\uff0c\u7136\u540e\u8981\u6c42\u7528\u6570\u503c\u8bc4\u5206\u56de\u7b54\u76f8\u540c\u95ee\u9898\uff0c\u6d4b\u8bd5\u56de\u7b54\u65b9\u5f0f\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u4e8c\u5143\u56de\u7b54\u7a33\u5b9a\u6027\u76f8\u5bf9\u8f83\u9ad8\u4f46\u4e0d\u5b8c\u7f8e\uff0c\u4e14\u56e0\u8bed\u8a00\u800c\u5f02\u3002\u6a21\u578b\u5e38\u5728\u4e0d\u540c\u8bed\u8a00\u6216\u683c\u5f0f\u95f4\u6539\u53d8\u7acb\u573a\uff0c\u6570\u503c\u8bc4\u5206\u5e38\u4e0e\u521d\u59cb\u4e8c\u5143\u9009\u62e9\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7279\u5b9a\u8bed\u8a00\u8bed\u5883\u4e0b\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u7684\u503e\u5411\uff0c\u6700\u4e00\u81f4\u7684\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u6700\u51c6\u786e\u6216\u4e2d\u7acb\u7684\u3002", "topic": "agent analysis"}}
{"id": "2510.02695", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02695", "abs": "https://arxiv.org/abs/2510.02695", "authors": ["Kai Fukazawa", "Kunal Mundada", "Iman Soltani"], "title": "RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization", "comment": "Under review as a conference paper at ICLR 2026, 21 pages, 8 figures.\n  The HTML preview may misrender some figures; please refer to the PDF", "summary": "In safety-critical domains where online data collection is infeasible,\noffline reinforcement learning (RL) offers an attractive alternative but only\nif policies deliver high returns without incurring catastrophic lower-tail\nrisk. Prior work on risk-averse offline RL achieves safety at the cost of value\nconservatism and restricted policy classes, whereas expressive policies are\nonly used in risk-neutral settings. Here, we address this gap by introducing\nthe \\textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which\ncouples an \\emph{expressive generative actor} with a distributional critic. The\nRAMAC differentiates composite objective combining distributional risk and BC\nloss through the generative path, achieving risk-sensitive learning in complex\nmultimodal scenarios. We instantiate RAMAC with diffusion and flow-matching\nactors and observe consistent gains in $\\mathrm{CVaR}_{0.1}$ while maintaining\nstrong returns on most Stochastic-D4RL tasks. Code:\nhttps://github.com/KaiFukazawa/RAMAC.git", "AI": {"tldr": "\u63d0\u51faRAMAC\u6846\u67b6\uff0c\u7ed3\u5408\u8868\u8fbe\u6027\u751f\u6210actor\u548c\u5206\u5e03critic\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u98ce\u9669\u654f\u611f\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u9ad8\u56de\u62a5\u7684\u540c\u65f6\u964d\u4f4e\u5c3e\u90e8\u98ce\u9669\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5e73\u8861\u9ad8\u56de\u62a5\u548c\u4f4e\u98ce\u9669\uff0c\u4f46\u73b0\u6709\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u5f80\u5f80\u8fc7\u4e8e\u4fdd\u5b88\u6216\u9650\u5236\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8868\u8fbe\u6027\u751f\u6210actor\uff08\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\uff09\u4e0e\u5206\u5e03critic\u7ed3\u5408\uff0c\u901a\u8fc7\u590d\u5408\u76ee\u6807\u51fd\u6570\u5b9e\u73b0\u98ce\u9669\u654f\u611f\u5b66\u4e60\u3002", "result": "\u5728Stochastic-D4RL\u4efb\u52a1\u4e0a\uff0cRAMAC\u5728\u4fdd\u6301\u5f3a\u56de\u62a5\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86CVaR0.1\u6307\u6807\u3002", "conclusion": "RAMAC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u98ce\u9669\u654f\u611f\u6027\u4e0e\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02369", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02369", "abs": "https://arxiv.org/abs/2510.02369", "authors": ["Kuntai Cai", "Juncheng Liu", "Xianglin Yang", "Zhaojie Niu", "Xiaokui Xiao", "Xing Chen"], "title": "Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents", "comment": "Under review at ICLR 2026", "summary": "Large language model (LLM) agents typically receive two kinds of context: (i)\nenvironment-level manuals that define interaction interfaces and global rules,\nand (ii) task-level guidance or demonstrations tied to specific goals. In this\nwork, we identify a crucial but overlooked third type of context,\ninstance-level context, which consists of verifiable and reusable facts tied to\na specific environment instance, such as object locations, crafting recipes,\nand local rules. We argue that the absence of instance-level context is a\ncommon source of failure for LLM agents in complex tasks, as success often\ndepends not only on reasoning over global rules or task prompts but also on\nmaking decisions based on precise and persistent facts. Acquiring such context\nrequires more than memorization: the challenge lies in efficiently exploring,\nvalidating, and formatting these facts under tight interaction budgets. We\nformalize this problem as Instance-Level Context Learning (ILCL) and introduce\nour task-agnostic method to solve it. Our method performs a guided exploration,\nusing a compact TODO forest to intelligently prioritize its next actions and a\nlightweight plan-act-extract loop to execute them. This process automatically\nproduces a high-precision context document that is reusable across many\ndownstream tasks and agents, thereby amortizing the initial exploration cost.\nExperiments across TextWorld, ALFWorld, and Crafter demonstrate consistent\ngains in both success and efficiency: for instance, ReAct's mean success rate\nin TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By\ntransforming one-off exploration into persistent, reusable knowledge, our\nmethod complements existing contexts to enable more reliable and efficient LLM\nagents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5b9e\u4f8b\u7ea7\u4e0a\u4e0b\u6587\u5b66\u4e60(ILCL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5f0f\u63a2\u7d22\u548c\u8f7b\u91cf\u7ea7\u8ba1\u5212-\u6267\u884c-\u63d0\u53d6\u5faa\u73af\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u3001\u53ef\u91cd\u7528\u7684\u5b9e\u4f8b\u7ea7\u4e0a\u4e0b\u6587\u6587\u6863\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u53ea\u5173\u6ce8\u73af\u5883\u7ea7\u624b\u518c\u548c\u4efb\u52a1\u7ea7\u6307\u5bfc\uff0c\u4f46\u5ffd\u7565\u4e86\u5b9e\u4f8b\u7ea7\u4e0a\u4e0b\u6587\uff08\u5982\u7269\u4f53\u4f4d\u7f6e\u3001\u5236\u4f5c\u914d\u65b9\u7b49\u53ef\u9a8c\u8bc1\u4e8b\u5b9e\uff09\uff0c\u8fd9\u5bfc\u81f4\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7ecf\u5e38\u5931\u8d25\u3002", "method": "\u4f7f\u7528TODO\u68ee\u6797\u8fdb\u884c\u667a\u80fd\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u8ba1\u5212-\u6267\u884c-\u63d0\u53d6\u5faa\u73af\u6765\u6267\u884c\u52a8\u4f5c\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u7cbe\u5ea6\u7684\u53ef\u91cd\u7528\u4e0a\u4e0b\u6587\u6587\u6863\u3002", "result": "\u5728TextWorld\u3001ALFWorld\u548cCrafter\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cReAct\u5728TextWorld\u4e2d\u7684\u5e73\u5747\u6210\u529f\u7387\u4ece37%\u63d0\u5347\u523095%\uff0cIGE\u4ece81%\u63d0\u5347\u523095%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e00\u6b21\u6027\u63a2\u7d22\u8f6c\u5316\u4e3a\u6301\u4e45\u3001\u53ef\u91cd\u7528\u7684\u77e5\u8bc6\uff0c\u8be5\u65b9\u6cd5\u8865\u5145\u4e86\u73b0\u6709\u4e0a\u4e0b\u6587\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u66f4\u52a0\u53ef\u9760\u548c\u9ad8\u6548\u3002", "topic": "agent analysis"}}
{"id": "2510.02377", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02377", "abs": "https://arxiv.org/abs/2510.02377", "authors": ["Aakriti Agrawal", "Rohith Aralikatti", "Anirudh Satheesh", "Souradip Chakraborty", "Amrit Singh Bedi", "Furong Huang"], "title": "Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities, yet\nselecting the most reliable response from multiple LLMs remains a challenge,\nparticularly in resource-constrained settings. Existing approaches often depend\non costly external verifiers, human evaluators, or self-consistency techniques\nthat require multiple samples from a single model. While multi-LLM systems\nproduce more diverse responses than single models and thus have greater\npotential, they often underperform compared to single LLM self-consistency. We\npropose a principled, novel and computationally efficient method to select the\nbest response from multiple different LLMs using a calibrated log-likelihood\nscore, implicitly leveraging the inherent knowledge and confidence of these\nmodels. Our method demonstrates improvements of approx. 4%, 3%, and 5% across\nboth debate (multi-round LLM discussions) and non-debate (Best-of-N with\nmultiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets\nrespectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6821\u51c6\u5bf9\u6570\u4f3c\u7136\u5206\u6570\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u4ece\u591a\u4e2a\u4e0d\u540cLLM\u4e2d\u9009\u62e9\u6700\u4f73\u54cd\u5e94\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u591a\u6b21\u91c7\u6837\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4ece\u591a\u4e2aLLM\u4e2d\u9009\u62e9\u6700\u53ef\u9760\u54cd\u5e94\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5916\u90e8\u9a8c\u8bc1\u5668\u3001\u4eba\u5de5\u8bc4\u4f30\u6216\u9700\u8981\u5355\u6a21\u578b\u591a\u6b21\u91c7\u6837\u7684\u81ea\u4e00\u81f4\u6027\u6280\u672f\u3002", "method": "\u4f7f\u7528\u6821\u51c6\u7684\u5bf9\u6570\u4f3c\u7136\u5206\u6570\u6765\u9690\u5f0f\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u56fa\u6709\u77e5\u8bc6\u548c\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u591a\u4e2a\u4e0d\u540cLLM\u4e2d\u9009\u62e9\u6700\u4f73\u54cd\u5e94\u3002", "result": "\u5728GSM8K\u3001MMLU\uff086\u4e2a\u5b50\u96c6\uff09\u548cARC\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u8fa9\u8bba\uff08\u591a\u8f6eLLM\u8ba8\u8bba\uff09\u548c\u975e\u8fa9\u8bba\uff08\u591aLLM\u6700\u4f73\u9009\u62e9\uff09\u8bbe\u7f6e\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86\u7ea64%\u30013%\u548c5%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u591aLLM\u7cfb\u7edf\u7684\u54cd\u5e94\u9009\u62e9\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2510.02645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02645", "abs": "https://arxiv.org/abs/2510.02645", "authors": ["Fulei Zhang", "Zhou Yu"], "title": "Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions", "comment": "Accepted to The Second Workshop on Generative AI for E-commerce\n  (GenAIECommerce '25), held September 22, 2025, in Prague, Czech Republic", "summary": "As Large Language Models (LLMs) are increasingly deployed in customer-facing\napplications, a critical yet underexplored question is how users communicate\ndifferently with LLM chatbots compared to human agent. In this study, we\npresent empirical evidence that users adopt distinct communication styles when\nusers interact with chatbots versus human agents. Our analysis reveals\nsignificant differences in grammatical fluency, politeness, and lexical\ndiversity in user language between the two settings. These findings suggest\nthat models trained exclusively on human-human interaction data may not\nadequately accommodate the communication style shift that occurs once an LLM\nchatbot is deployed. To enhance LLM robustness to post-launch communication\nstyle changes, we experimented with two strategies: (1) data augmentation\nduring the post-training phase and (2) inference-time user message\nreformulation. Our results indicate that models trained on stylistically\ndiverse datasets significantly outperform those trained exclusively on original\nor stylistically uniform datasets, while inference-time reformulation proved\nless effective. These insights help us to better adapt our models for improved\nLLM-user interaction experiences.", "AI": {"tldr": "\u7528\u6237\u4e0eLLM\u804a\u5929\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u4ee3\u7406\u4ea4\u6d41\u65f6\u91c7\u7528\u4e0d\u540c\u7684\u6c9f\u901a\u98ce\u683c\uff0c\u7814\u7a76\u53d1\u73b0\u8bed\u6cd5\u6d41\u7545\u5ea6\u3001\u793c\u8c8c\u7a0b\u5ea6\u548c\u8bcd\u6c47\u591a\u6837\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u8c03\u6574\u6a21\u578b\u4ee5\u9002\u5e94\u8fd9\u79cd\u98ce\u683c\u53d8\u5316\u3002", "motivation": "\u63a2\u7d22\u7528\u6237\u5728LLM\u804a\u5929\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u4ee3\u7406\u9762\u524d\u6c9f\u901a\u98ce\u683c\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u5982\u4f55\u8ba9LLM\u6a21\u578b\u66f4\u597d\u5730\u9002\u5e94\u90e8\u7f72\u540e\u7684\u6c9f\u901a\u98ce\u683c\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u7b56\u7565\u589e\u5f3aLLM\u9c81\u68d2\u6027\uff1a(1) \u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6570\u636e\u589e\u5f3a\uff0c(2) \u63a8\u7406\u65f6\u7528\u6237\u6d88\u606f\u91cd\u6784\u3002", "result": "\u5728\u98ce\u683c\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4ec5\u5728\u539f\u59cb\u6216\u98ce\u683c\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u800c\u63a8\u7406\u65f6\u91cd\u6784\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "\u6a21\u578b\u9700\u8981\u9002\u5e94LLM\u90e8\u7f72\u540e\u7528\u6237\u6c9f\u901a\u98ce\u683c\u7684\u53d8\u5316\uff0c\u98ce\u683c\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.02712", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02712", "abs": "https://arxiv.org/abs/2510.02712", "authors": ["Yubo Li", "Ramayya Krishnan", "Rema Padman"], "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized conversational AI, yet their\nrobustness in extended multi-turn dialogues remains poorly understood. Existing\nevaluation frameworks focus on static benchmarks and single-turn assessments,\nfailing to capture the temporal dynamics of conversational degradation that\ncharacterize real-world interactions. In this work, we present the first\ncomprehensive survival analysis of conversational AI robustness, analyzing\n36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a\ntime-to-event process. Our survival modeling framework-employing Cox\nproportional hazards, Accelerated Failure Time, and Random Survival Forest\napproaches-reveals extraordinary temporal dynamics. We find that abrupt,\nprompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing\nthe hazard of conversational failure. In stark contrast, gradual, cumulative\ndrift is highly protective, vastly reducing the failure hazard and enabling\nsignificantly longer dialogues. AFT models with interactions demonstrate\nsuperior performance, achieving excellent discrimination and exceptional\ncalibration. These findings establish survival analysis as a powerful paradigm\nfor evaluating LLM robustness, offer concrete insights for designing resilient\nconversational agents, and challenge prevailing assumptions about the necessity\nof semantic consistency in conversational AI Systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5bf9\u8bddAI\u9c81\u68d2\u6027\u8fdb\u884c\u751f\u5b58\u5206\u6790\uff0c\u901a\u8fc7\u5206\u679036,951\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\u53d1\u73b0\uff1a\u7a81\u53d1\u8bed\u4e49\u6f02\u79fb\u4f1a\u707e\u96be\u6027\u589e\u52a0\u5bf9\u8bdd\u5931\u8d25\u98ce\u9669\uff0c\u800c\u6e10\u8fdb\u6f02\u79fb\u5219\u663e\u8457\u964d\u4f4e\u98ce\u9669\u5e76\u5ef6\u957f\u5bf9\u8bdd\u65f6\u957f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u57fa\u51c6\u548c\u5355\u8f6e\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u5bf9\u8bdd\u4e2d\u968f\u65f6\u95f4\u9000\u5316\u7684\u52a8\u6001\u7279\u6027\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u7406\u89e3LLM\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u751f\u5b58\u5206\u6790\u6846\u67b6\uff08Cox\u6bd4\u4f8b\u98ce\u9669\u6a21\u578b\u3001\u52a0\u901f\u5931\u6548\u65f6\u95f4\u6a21\u578b\u548c\u968f\u673a\u751f\u5b58\u68ee\u6797\uff09\uff0c\u5206\u67909\u4e2a\u6700\u5148\u8fdbLLM\u7684\u5bf9\u8bdd\u6570\u636e\uff0c\u5c06\u5bf9\u8bdd\u5931\u8d25\u5efa\u6a21\u4e3a\u65f6\u95f4\u5230\u4e8b\u4ef6\u8fc7\u7a0b\u3002", "result": "AFT\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5177\u6709\u4f18\u5f02\u7684\u533a\u5206\u5ea6\u548c\u6821\u51c6\u80fd\u529b\u3002\u7a81\u53d1\u8bed\u4e49\u6f02\u79fb\u4f7f\u5bf9\u8bdd\u5931\u8d25\u98ce\u9669\u6025\u5267\u589e\u52a0\uff0c\u800c\u6e10\u8fdb\u6f02\u79fb\u5219\u663e\u8457\u964d\u4f4e\u98ce\u9669\u5e76\u652f\u6301\u66f4\u957f\u7684\u5bf9\u8bdd\u3002", "conclusion": "\u751f\u5b58\u5206\u6790\u662f\u8bc4\u4f30LLM\u9c81\u68d2\u6027\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u4e3a\u8bbe\u8ba1\u5f39\u6027\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u5177\u4f53\u89c1\u89e3\uff0c\u5e76\u6311\u6218\u4e86\u5bf9\u8bddAI\u4e2d\u8bed\u4e49\u4e00\u81f4\u6027\u5fc5\u8981\u6027\u7684\u666e\u904d\u5047\u8bbe\u3002", "topic": "agent analysis"}}
{"id": "2510.02892", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.02892", "abs": "https://arxiv.org/abs/2510.02892", "authors": ["Aleksei Arzhantsev", "Otmane Sakhi", "Flavian Vasile"], "title": "RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning", "comment": "Accepted to the Efficient Reasoning Workshop at NeuRIPS 2025", "summary": "Reinforcement learning (RL) is central to improving reasoning in large\nlanguage models (LLMs) but typically requires ground-truth rewards. Test-Time\nReinforcement Learning (TTRL) removes this need by using majority-vote rewards,\nbut relies on heavy online RL and incurs substantial computational cost. We\npropose RoiRL: Reasoning with offline iterative Reinforcement Learning, a\nfamily of lightweight offline learning alternatives that can target the same\nregularized optimal policies. Unlike TTRL, RoiRL eliminates the need to\nmaintain a reference model and instead optimizes weighted log-likelihood\nobjectives, enabling stable training with significantly lower memory and\ncompute requirements. Experimental results show that RoiRL trains to 2.5x\nfaster and consistently outperforms TTRL on reasoning benchmarks, establishing\na scalable path to self-improving LLMs without labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoiRL\u65b9\u6cd5\uff0c\u4f7f\u7528\u79bb\u7ebf\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u4f20\u7edf\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eTTRL\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u771f\u5b9e\u5956\u52b1\u4fe1\u53f7\uff0c\u800c\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60(TTRL)\u867d\u7136\u4f7f\u7528\u591a\u6570\u6295\u7968\u5956\u52b1\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u8f7b\u91cf\u7ea7\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "RoiRL\u91c7\u7528\u79bb\u7ebf\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f18\u5316\u52a0\u6743\u5bf9\u6570\u4f3c\u7136\u76ee\u6807\uff0c\u65e0\u9700\u7ef4\u62a4\u53c2\u8003\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aRoiRL\u8bad\u7ec3\u901f\u5ea6\u6bd4TTRL\u5feb2.5\u500d\uff0c\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u4e8eTTRL\u3002", "conclusion": "RoiRL\u4e3a\u65e0\u6807\u7b7e\u7684\u81ea\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02752", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02752", "abs": "https://arxiv.org/abs/2510.02752", "authors": ["Hangfan Zhang", "Siyuan Xu", "Zhimeng Guo", "Huaisheng Zhu", "Shicheng Liu", "Xinrun Wang", "Qiaosheng Zhang", "Yang Chen", "Peng Ye", "Lei Bai", "Shuyue Hu"], "title": "The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of large language models (LLMs), but such training\ntypically demands substantial efforts in creating and annotating data. In this\nwork, we explore improving LLMs through RL with minimal data. Our approach\nalternates between the LLM proposing a task and then attempting to solve it. To\nminimize data dependency, we introduce two novel mechanisms grounded in\nself-awareness: (1) self-aware difficulty prediction, where the model learns to\nassess task difficulty relative to its own abilities and prioritize challenging\nyet solvable tasks, and (2) self-aware limit breaking, where the model\nrecognizes when a task is beyond its capability boundary and proactively\nrequests external data to break through that limit. Extensive experiments on\nnine benchmarks showing a 53.8% relative improvement with less than 1.2% extra\ndata demonstrate the efficacy of self-aware RL and underscore the promise of\nself-evolving agent training.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u6211\u610f\u8bc6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9LLM\u81ea\u5df1\u751f\u6210\u4efb\u52a1\u5e76\u89e3\u51b3\uff0c\u4f7f\u7528\u96be\u5ea6\u9884\u6d4b\u548c\u6781\u9650\u7a81\u7834\u673a\u5236\uff0c\u5728\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u7528\u6700\u5c11\u7684\u6570\u636e\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ea4\u66ff\u8fdb\u884c\u4efb\u52a1\u751f\u6210\u4e0e\u89e3\u51b3\uff0c\u5f15\u5165\u81ea\u6211\u610f\u8bc6\u96be\u5ea6\u9884\u6d4b\u548c\u6781\u9650\u7a81\u7834\u673a\u5236\uff0c\u8ba9\u6a21\u578b\u8bc4\u4f30\u4efb\u52a1\u96be\u5ea6\u5e76\u4e3b\u52a8\u8bf7\u6c42\u5916\u90e8\u6570\u636e\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b053.8%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u4ec5\u9700\u4e0d\u52301.2%\u7684\u989d\u5916\u6570\u636e\u3002", "conclusion": "\u81ea\u6211\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\uff0c\u5c55\u793a\u4e86\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u8bad\u7ec3\u7684\u524d\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02945", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02945", "abs": "https://arxiv.org/abs/2510.02945", "authors": ["Juan Sebastian Rojas", "Chi-Guhn Lee"], "title": "Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning", "comment": null, "summary": "Continual reinforcement learning (continual RL) seeks to formalize the\nnotions of lifelong learning and endless adaptation in RL. In particular, the\naim of continual RL is to develop RL agents that can maintain a careful balance\nbetween retaining useful information and adapting to new situations. To date,\ncontinual RL has been explored almost exclusively through the lens of\nrisk-neutral decision-making, in which the agent aims to optimize the expected\n(or mean) long-run performance. In this work, we present the first formal\ntheoretical treatment of continual RL through the lens of risk-aware\ndecision-making, in which the agent aims to optimize a reward-based measure of\nlong-run performance beyond the mean. In particular, we show that the classical\ntheory of risk measures, widely used as a theoretical foundation in\nnon-continual risk-aware RL, is, in its current form, incompatible with the\ncontinual setting. Then, building on this insight, we extend risk measure\ntheory into the continual setting by introducing a new class of ergodic risk\nmeasures that are compatible with continual learning. Finally, we provide a\ncase study of risk-aware continual learning, along with empirical results,\nwhich show the intuitive appeal and theoretical soundness of ergodic risk\nmeasures.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u7684\u89d2\u5ea6\u5bf9\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7406\u8bba\u5904\u7406\uff0c\u63d0\u51fa\u4e86\u4e0e\u6301\u7eed\u5b66\u4e60\u517c\u5bb9\u7684\u904d\u5386\u98ce\u9669\u5ea6\u91cf\u65b0\u7c7b\u522b\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u98ce\u9669\u4e2d\u6027\u51b3\u7b56\uff0c\u7f3a\u4e4f\u5bf9\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u7684\u7406\u8bba\u5904\u7406\u3002\u4f5c\u8005\u53d1\u73b0\u7ecf\u5178\u98ce\u9669\u5ea6\u91cf\u7406\u8bba\u4e0e\u6301\u7eed\u8bbe\u7f6e\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u6269\u5c55\u98ce\u9669\u5ea6\u91cf\u7406\u8bba\u5230\u6301\u7eed\u8bbe\u7f6e\uff0c\u5f15\u5165\u904d\u5386\u98ce\u9669\u5ea6\u91cf\u65b0\u7c7b\u522b\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u7684\u904d\u5386\u98ce\u9669\u5ea6\u91cf\u5728\u98ce\u9669\u611f\u77e5\u6301\u7eed\u5b66\u4e60\u4e2d\u5c55\u73b0\u51fa\u76f4\u89c2\u5438\u5f15\u529b\u548c\u7406\u8bba\u5408\u7406\u6027\u3002", "conclusion": "\u904d\u5386\u98ce\u9669\u5ea6\u91cf\u4e3a\u98ce\u9669\u611f\u77e5\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u517c\u5bb9\u7684\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7406\u8bba\u7a7a\u767d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.02919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.02919", "abs": "https://arxiv.org/abs/2510.02919", "authors": ["Jian Mu", "Qixin Zhang", "Zhiyong Wang", "Menglin Yang", "Shuang Qiu", "Chengwei Qin", "Zhongxiang Dai", "Yao Shu"], "title": "Self-Reflective Generation at Test Time", "comment": "24 pages, 8 figures", "summary": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques.", "AI": {"tldr": "SRGen\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e0d\u786e\u5b9a\u70b9\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u6765\u6539\u8fdbLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u7528\u52a8\u6001\u71b5\u9608\u503c\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027token\u5e76\u8bad\u7ec3\u6821\u6b63\u5411\u91cf\u6765\u4fee\u6b63\u6982\u7387\u5206\u5e03\u3002", "motivation": "\u5f53\u524dLLM\u7684\u524d\u5411\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u5f88\u8106\u5f31\uff0c\u65e9\u671ftoken\u9519\u8bef\u4f1a\u7ea7\u8054\u4f20\u64ad\uff0c\u9700\u8981\u81ea\u6211\u53cd\u601d\u673a\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5bf9\u6574\u4e2a\u8349\u7a3f\u8fdb\u884c\u4fee\u8ba2\uff0c\u8981\u4e48\u901a\u8fc7\u6602\u8d35\u8bad\u7ec3\u5b66\u4e60\u81ea\u6211\u6821\u6b63\uff0c\u90fd\u662f\u88ab\u52a8\u4e14\u4f4e\u6548\u7684\u3002", "method": "\u5728token\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u52a8\u6001\u71b5\u9608\u503c\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027token\uff0c\u4e3a\u6bcf\u4e2a\u8bc6\u522b\u51fa\u7684token\u8bad\u7ec3\u7279\u5b9a\u7684\u6821\u6b63\u5411\u91cf\uff0c\u5229\u7528\u5df2\u751f\u6210\u4e0a\u4e0b\u6587\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u751f\u6210\u6765\u4fee\u6b63token\u6982\u7387\u5206\u5e03\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSRGen\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728AIME2024\u4e0a\uff0cDeepSeek-R1-Distill-Qwen-7B\u7684Pass@1\u7edd\u5bf9\u63d0\u534712.0%\uff0cCons@5\u63d0\u534713.3%\u3002", "conclusion": "SRGen\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u65b9\u6cd5\uff0c\u5c06\u53cd\u601d\u96c6\u6210\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760LLM\u63a8\u7406\uff0c\u5728\u6709\u9650\u5f00\u9500\u4e0b\u83b7\u5f97\u4e00\u81f4\u589e\u76ca\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u6280\u672f\u5e7f\u6cdb\u7ec4\u5408\u3002", "topic": "agent analysis"}}
{"id": "2510.03065", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03065", "abs": "https://arxiv.org/abs/2510.03065", "authors": ["Mingfeng Fan", "Jiaqi Cheng", "Yaoxin Wu", "Yifeng Zhang", "Yibin Yang", "Guohua Wu", "Guillaume Sartoretti"], "title": "A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem", "comment": null, "summary": "In recent years, deep reinforcement learning (DRL) has gained traction for\nsolving the NP-hard traveling salesman problem (TSP). However, limited\nattention has been given to the close-enough TSP (CETSP), primarily due to the\nchallenge introduced by its neighborhood-based visitation criterion, wherein a\nnode is considered visited if the agent enters a compact neighborhood around\nit. In this work, we formulate a Markov decision process (MDP) for CETSP using\na discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)\nframework that separates decision-making into node selection and waypoint\ndetermination. Specifically, an adapted encoder is employed for effective\nfeature extraction, followed by a node-decoder and a loc-decoder to handle the\ntwo sub-tasks, respectively. A k-nearest neighbors subgraph interaction\nstrategy is further introduced to enhance spatial reasoning during location\ndecoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a\nunified model capable of generalizing across different problem sizes and\nvarying neighborhood radius types (i.e., constant and random radii).\nExperimental results show that UD3RL outperforms conventional methods in both\nsolution quality and runtime, while exhibiting strong generalization across\nproblem scales, spatial distributions, and radius ranges, as well as robustness\nto dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u53cc\u89e3\u7801\u5668\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6UD3RL\uff0c\u7528\u4e8e\u89e3\u51b3\u63a5\u8fd1\u65c5\u884c\u5546\u95ee\u9898(CETSP)\uff0c\u8be5\u6846\u67b6\u5c06\u51b3\u7b56\u5206\u4e3a\u8282\u70b9\u9009\u62e9\u548c\u8def\u5f84\u70b9\u786e\u5b9a\u4e24\u4e2a\u5b50\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6807\u51c6\u65c5\u884c\u5546\u95ee\u9898\uff0c\u800c\u5bf9\u63a5\u8fd1\u65c5\u884c\u5546\u95ee\u9898\u5173\u6ce8\u8f83\u5c11\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u5176\u57fa\u4e8e\u90bb\u57df\u7684\u8bbf\u95ee\u51c6\u5219\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u5316\u65b9\u6848\u6784\u5efa\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51faUD3RL\u6846\u67b6\uff0c\u5305\u542b\u7279\u5f81\u63d0\u53d6\u7f16\u7801\u5668\u3001\u8282\u70b9\u89e3\u7801\u5668\u548c\u4f4d\u7f6e\u89e3\u7801\u5668\uff0c\u5e76\u5f15\u5165k\u8fd1\u90bb\u5b50\u56fe\u4ea4\u4e92\u7b56\u7565\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eUD3RL\u5728\u89e3\u8d28\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5728\u95ee\u9898\u89c4\u6a21\u3001\u7a7a\u95f4\u5206\u5e03\u548c\u534a\u5f84\u8303\u56f4\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UD3RL\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u548c\u534a\u5f84\u7c7b\u578b\uff0c\u5e76\u5bf9\u52a8\u6001\u73af\u5883\u5177\u6709\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03204", "abs": "https://arxiv.org/abs/2510.03204", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han L\u00f9", "L\u00e9o Boisvert", "Massimo Caccia", "J\u00e9r\u00e9my Espinas", "Alexandre Aussem", "V\u00e9ronique Eglin", "Alexandre Lacoste"], "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents", "comment": null, "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.", "AI": {"tldr": "FocusAgent\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u68c0\u7d22\u5668\u4ece\u53ef\u8bbf\u95ee\u6027\u6811\u89c2\u5bdf\u4e2d\u63d0\u53d6\u6700\u76f8\u5173\u7684\u5185\u5bb9\u884c\uff0c\u901a\u8fc7\u4fee\u526a\u566a\u58f0\u548c\u65e0\u5173\u5185\u5bb9\u6765\u63d0\u9ad8Web\u4ee3\u7406\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709Web\u4ee3\u7406\u5904\u7406\u5197\u957f\u7f51\u9875\u65f6\u5b58\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u9971\u548c\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5b89\u5168\u98ce\u9669\uff08\u5982\u63d0\u793a\u6ce8\u5165\uff09\u7684\u95ee\u9898\uff0c\u73b0\u6709\u4fee\u526a\u7b56\u7565\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u5229\u7528\u8f7b\u91cf\u7ea7LLM\u68c0\u7d22\u5668\u6839\u636e\u4efb\u52a1\u76ee\u6807\u4ece\u53ef\u8bbf\u95ee\u6027\u6811\u89c2\u5bdf\u4e2d\u63d0\u53d6\u6700\u76f8\u5173\u7684\u5185\u5bb9\u884c\uff0c\u51cf\u5c11\u566a\u58f0\u548c\u65e0\u5173\u5185\u5bb9\u3002", "result": "\u5728WorkArena\u548cWebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFocusAgent\u4e0e\u5f3a\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u5c06\u89c2\u5bdf\u5927\u5c0f\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u5e76\u80fd\u663e\u8457\u964d\u4f4e\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u6709\u9488\u5bf9\u6027\u68c0\u7d22\u662f\u6784\u5efa\u9ad8\u6548\u3001\u6709\u6548\u4e14\u5b89\u5168\u7684Web\u4ee3\u7406\u7684\u5b9e\u7528\u4e14\u9c81\u68d2\u7b56\u7565\u3002", "topic": "agent analysis"}}
{"id": "2510.03223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03223", "abs": "https://arxiv.org/abs/2510.03223", "authors": ["Hongxiang Zhang", "Yuan Tian", "Tianyi Zhang"], "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment", "comment": null, "summary": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.", "AI": {"tldr": "Self-Anchor\u662f\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u8f68\u8ff9\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u8ba1\u5212\u5e76\u81ea\u52a8\u5bf9\u9f50\u6a21\u578b\u6ce8\u610f\u529b\u5230\u6700\u76f8\u5173\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6765\u89e3\u51b3LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u6ce8\u610f\u529b\u5206\u6563\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u63a8\u7406\u94fe\u7684\u5ef6\u957f\uff0c\u5173\u952e\u7684\u4e2d\u95f4\u6b65\u9aa4\u548c\u539f\u59cb\u63d0\u793a\u4f1a\u5728\u4e0a\u4e0b\u6587\u4e2d\u88ab\u6df9\u6ca1\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u4e0d\u8db3\u548c\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u65b9\u6cd5\u6765\u5f15\u5bfcLLM\u6ce8\u610f\u529b\u3002", "method": "\u5c06\u63a8\u7406\u8f68\u8ff9\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u8ba1\u5212\uff0c\u81ea\u52a8\u5bf9\u9f50\u6a21\u578b\u6ce8\u610f\u529b\u5230\u6700\u76f8\u5173\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e13\u6ce8\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\"\u975e\u63a8\u7406\"\u6a21\u578b\u4e0e\u4e13\u7528\u63a8\u7406\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "Self-Anchor\u6709\u6f5c\u529b\u4f7f\u5927\u591a\u6570LLM\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5c31\u80fd\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "2510.03181", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03181", "abs": "https://arxiv.org/abs/2510.03181", "authors": ["Ha Manh Bui", "Felix Parker", "Kimia Ghobadi", "Anqi Liu"], "title": "Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning", "comment": null, "summary": "We study the Non-Stationary Reinforcement Learning (RL) under distribution\nshifts in both finite-horizon episodic and infinite-horizon discounted Markov\nDecision Processes (MDPs). In the finite-horizon case, the transition functions\nmay suddenly change at a particular episode. In the infinite-horizon setting,\nsuch changes can occur at an arbitrary time step during the agent's interaction\nwith the environment. While the Q-learning Upper Confidence Bound algorithm\n(QUCB) can discover a proper policy during learning, due to the distribution\nshifts, this policy can exploit sub-optimal rewards after the shift happens. To\naddress this issue, we propose Density-QUCB (DQUCB), a shift-aware\nQ-learning~UCB algorithm, which uses a transition density function to detect\ndistribution shifts, then leverages its likelihood to enhance the uncertainty\nestimation quality of Q-learning~UCB, resulting in a balance between\nexploration and exploitation. Theoretically, we prove that our oracle DQUCB\nachieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the\ncomputational efficiency of model-free RL and outperforms QUCB baselines by\nhaving a lower regret across RL tasks, as well as a real-world COVID-19 patient\nhospital allocation task using a Deep-Q-learning architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86Density-QUCB\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f6c\u79fb\u5bc6\u5ea6\u51fd\u6570\u68c0\u6d4b\u5206\u5e03\u6f02\u79fb\uff0c\u6539\u8fdbQ-learning UCB\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5728\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u89e3\u51b3\u975e\u5e73\u7a33\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u5206\u5e03\u6f02\u79fb\u5bfc\u81f4\u7b56\u7565\u5728\u53d8\u5316\u540e\u5229\u7528\u6b21\u4f18\u5956\u52b1\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f6c\u79fb\u5bc6\u5ea6\u51fd\u6570\u68c0\u6d4b\u5206\u5e03\u6f02\u79fb\uff0c\u5e76\u5229\u7528\u5176\u4f3c\u7136\u589e\u5f3aQ-learning UCB\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8d28\u91cf\u3002", "result": "\u7406\u8bba\u8bc1\u660eDQUCB\u6bd4QUCB\u83b7\u5f97\u66f4\u597d\u7684\u9057\u61be\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5728RL\u4efb\u52a1\u548cCOVID-19\u60a3\u8005\u5206\u914d\u4efb\u52a1\u4e2d\u5177\u6709\u66f4\u4f4e\u9057\u61be\u3002", "conclusion": "DQUCB\u7ed3\u5408\u4e86\u6a21\u578b\u65e0\u5173RL\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u5206\u5e03\u6f02\u79fb\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03199", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03199", "abs": "https://arxiv.org/abs/2510.03199", "authors": ["Qiwei Di", "Kaixuan Ji", "Xuheng Li", "Heyang Zhao", "Quanquan Gu"], "title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling", "comment": "29 pages, 3 figures", "summary": "LLM inference often generates a batch of candidates for a prompt and selects\none via strategies like majority voting or Best-of- N (BoN). For difficult\ntasks, this single-shot selection often underperforms. Consequently,\nevaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,\nand only the best of them is used when computing regret. Motivated by this, we\nstudy inference scaling in the more general Pass@$k$ inference setting, and\nprove that neither majority voting nor BoN exhibits the desirable scaling with\n$k$ and the sampling budget $N$. Combining the advantages of majority voting\nand BoN, we propose a new inference strategy called Best-of-Majority (BoM),\nwith a pivotal step that restricts the candidates to the responses with high\nfrequency in the $N$ samples before selecting the top-$k$ rewards. We prove\nthat when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is\n$O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$\nis the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error\nof the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of\nreward at the optimal response. We further establish a matching lower bound,\ncertifying that our algorithm is minimax optimal. Beyond optimality, BoM has a\nkey advantage: unlike majority voting and BoN, its performance does not degrade\nwhen increasing $N$. Experimental results of inference on math problems show\nBoM outperforming both majority voting and BoN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Best-of-Majority (BoM)\u63a8\u7406\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u6570\u6295\u7968\u548cBest-of-N\u7684\u4f18\u52bf\uff0c\u5728Pass@k\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u6700\u4f18\u63a8\u7406\u6269\u5c55\uff0c\u8bc1\u660e\u5176\u5177\u6709\u6700\u5c0f\u5316\u6700\u4f18\u6027\u3002", "motivation": "\u4f20\u7edf\u5355\u6b21\u9009\u62e9\u7b56\u7565\uff08\u5982\u591a\u6570\u6295\u7968\u548cBest-of-N\uff09\u5728\u56f0\u96be\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cPass@k\u8bc4\u4f30\u5141\u8bb8\u63d0\u4ea4\u591a\u4e2a\u54cd\u5e94\uff0c\u4f46\u73b0\u6709\u7b56\u7565\u5728\u63a8\u7406\u6269\u5c55\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faBoM\u7b56\u7565\uff0c\u9996\u5148\u9650\u5236\u5019\u9009\u96c6\u4e3a\u9ad8\u9891\u54cd\u5e94\uff0c\u7136\u540e\u9009\u62e9\u524dk\u4e2a\u5956\u52b1\u6700\u9ad8\u7684\u54cd\u5e94\uff0c\u7ed3\u5408\u591a\u6570\u6295\u7968\u548cBoN\u7684\u4f18\u70b9\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5f53\u91c7\u6837\u9884\u7b97N=\u03a9(C*)\u65f6\uff0cBoM\u7684\u9057\u61be\u4e3aO(\u03b5_opt+\u221a(\u03b5_RM\u00b2C*/k))\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u4e0b\u754c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aBoM\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u4f18\u4e8e\u591a\u6570\u6295\u7968\u548cBoN\u3002", "conclusion": "BoM\u662f\u9996\u4e2a\u5728Pass@k\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u6700\u5c0f\u5316\u6700\u4f18\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5176\u6027\u80fd\u4e0d\u4f1a\u968fN\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2510.03222", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03222", "abs": "https://arxiv.org/abs/2510.03222", "authors": ["Guanhua Huang", "Tingqiang Xu", "Mingze Wang", "Qi Yi", "Xue Gong", "Siheng Li", "Ruibin Xiong", "Kejiao Li", "Yuhao Jiang", "Bo Zhou"], "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textbf{\\textit{reasoning\nsparks}}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\n\\textit{reasoning sparks} is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n$60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLp-Reg\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u4fdd\u62a4\u4f4e\u6982\u7387\u63a8\u7406\u706b\u82b1\uff0c\u89e3\u51b3RLVR\u8bad\u7ec3\u4e2d\u63a2\u7d22\u9000\u5316\u95ee\u9898\uff0c\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u5b9e\u73b060.17%\u7684\u51c6\u786e\u7387\u3002", "motivation": "RLVR\u8bad\u7ec3\u4e2d\u5b58\u5728\u63a2\u7d22\u74f6\u9888\uff0c\u9ad8\u6982\u7387\u7b56\u7565\u4f1a\u6d88\u9664\u6709\u4ef7\u503c\u7684\u4f4e\u6982\u7387\u63a8\u7406\u706b\u82b1\uff0c\u5bfc\u81f4\u63a2\u7d22\u9000\u5316\u3002", "method": "\u5f15\u5165\u4f4e\u6982\u7387\u6b63\u5219\u5316(Lp-Reg)\uff0c\u6784\u5efa\u53bb\u566a\u4ee3\u7406\u5206\u5e03\u4f5c\u4e3a\u6b63\u5219\u5316\u76ee\u6807\uff0c\u901a\u8fc7KL\u6563\u5ea6\u4fdd\u62a4\u63a8\u7406\u706b\u82b1\u4e0d\u88ab\u8fc7\u5ea6\u60e9\u7f5a\u3002", "result": "Lp-Reg\u652f\u6301\u7ea61000\u6b65\u7684\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u4e0a\u8fbe\u523060.17%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53472.66%\u3002", "conclusion": "\u4fdd\u62a4\u4f4e\u6982\u7387\u63a8\u7406\u706b\u82b1\u5bf9\u7ef4\u6301RLVR\u63a2\u7d22\u81f3\u5173\u91cd\u8981\uff0cLp-Reg\u80fd\u6709\u6548\u9632\u6b62\u63a2\u7d22\u9000\u5316\u5e76\u63d0\u5347\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.03207", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03207", "abs": "https://arxiv.org/abs/2510.03207", "authors": ["Yuda Song", "Dhruv Rohatgi", "Aarti Singh", "J. Andrew Bagnell"], "title": "To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning", "comment": "45 pages, 9 figures, published at NeurIPS 2025", "summary": "Partial observability is a notorious challenge in reinforcement learning\n(RL), due to the need to learn complex, history-dependent policies. Recent\nempirical successes have used privileged expert distillation--which leverages\navailability of latent state information during training (e.g., from a\nsimulator) to learn and imitate the optimal latent, Markovian policy--to\ndisentangle the task of \"learning to see\" from \"learning to act\". While expert\ndistillation is more computationally efficient than RL without latent state\ninformation, it also has well-documented failure modes. In this paper--through\na simple but instructive theoretical model called the perturbed Block MDP, and\ncontrolled experiments on challenging simulated locomotion tasks--we\ninvestigate the algorithmic trade-off between privileged expert distillation\nand standard RL without privileged information. Our main findings are: (1) The\ntrade-off empirically hinges on the stochasticity of the latent dynamics, as\ntheoretically predicted by contrasting approximate decodability with belief\ncontraction in the perturbed Block MDP; and (2) The optimal latent policy is\nnot always the best latent policy to distill. Our results suggest new\nguidelines for effectively exploiting privileged information, potentially\nadvancing the efficiency of policy learning across many practical partially\nobservable domains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7279\u6743\u4e13\u5bb6\u84b8\u998f\u4e0e\u6807\u51c6RL\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53d1\u73b0\u6f5c\u5728\u52a8\u6001\u7684\u968f\u673a\u6027\u5f71\u54cd\u7b97\u6cd5\u9009\u62e9\uff0c\u4e14\u6700\u4f18\u6f5c\u5728\u7b56\u7565\u5e76\u975e\u603b\u662f\u6700\u4f73\u84b8\u998f\u76ee\u6807\u3002", "motivation": "\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7ed9\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u6311\u6218\uff0c\u7279\u6743\u4e13\u5bb6\u84b8\u998f\u867d\u80fd\u63d0\u9ad8\u6548\u7387\u4f46\u5b58\u5728\u5931\u8d25\u6a21\u5f0f\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u4e0e\u6807\u51c6RL\u7684\u6743\u8861\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6a21\u578b\uff08\u6270\u52a8\u5757MDP\uff09\u548c\u6a21\u62df\u8fd0\u52a8\u4efb\u52a1\u7684\u5bf9\u7167\u5b9e\u9a8c\uff0c\u5206\u6790\u7279\u6743\u4e13\u5bb6\u84b8\u998f\u4e0e\u65e0\u7279\u6743\u4fe1\u606f\u6807\u51c6RL\u7684\u7b97\u6cd5\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6743\u8861\u53d6\u51b3\u4e8e\u6f5c\u5728\u52a8\u6001\u7684\u968f\u673a\u6027\uff0c\u4e14\u6700\u4f18\u6f5c\u5728\u7b56\u7565\u4e0d\u603b\u662f\u6700\u4f73\u84b8\u998f\u76ee\u6807\uff0c\u8fd9\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6709\u6548\u5229\u7528\u7279\u6743\u4fe1\u606f\u7684\u65b0\u6307\u5bfc\u539f\u5219\uff0c\u6709\u671b\u63d0\u5347\u90e8\u5206\u53ef\u89c2\u6d4b\u9886\u57df\u7b56\u7565\u5b66\u4e60\u7684\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.ab44faed", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstripe.com%2Fen-ca%2Fnewsroom%2Fnews%2Fstripe-openai-instant-checkout%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/c6CrnjZ_JsKAzZRwaD2Vljg0px__WrLfdc5pO19MJIg=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstripe.com%2Fen-ca%2Fnewsroom%2Fnews%2Fstripe-openai-instant-checkout%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/c6CrnjZ_JsKAzZRwaD2Vljg0px__WrLfdc5pO19MJIg=425", "authors": ["TLDR Newsletter"], "title": "Stripe powers instant checkout in ChatGPT, launches agentic commerce protocol with OpenAI", "comment": "Source: TLDR Newsletter, Date: 2025-10-02, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fstripe.com%2Fen-ca%2Fnewsroom%2Fnews%2Fstripe-openai-instant-checkout%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/c6CrnjZ_JsKAzZRwaD2Vljg0px__WrLfdc5pO19MJIg=425", "summary": "Stripe powers instant checkout in ChatGPT, launches agentic commerce protocol with OpenAI (4 minute read) Stripe and OpenAI unveiled Instant Checkout in ChatGPT, which lets US users buy from Etsy merchants\u2014and soon over a million Shopify stores\u2014directly in the chat. Transactions use Stripe's new shared payment token, which protects credentials while enabling fraud checks and seamless API handoff to merchants. The launch also debuts the open-standard agentic commerce protocol (ACP), which give...", "source": "tldr", "AI": {"tldr": "Stripe\u4e0eOpenAI\u5408\u4f5c\u5728ChatGPT\u4e2d\u63a8\u51fa\u5373\u65f6\u7ed3\u8d26\u529f\u80fd\uff0c\u5141\u8bb8\u7f8e\u56fd\u7528\u6237\u76f4\u63a5\u5728\u804a\u5929\u4e2d\u4eceEtsy\u5546\u5bb6\u8d2d\u4e70\u5546\u54c1\uff0c\u5373\u5c06\u6269\u5c55\u5230\u8d85\u8fc7100\u4e07\u5bb6Shopify\u5546\u5e97\u3002", "motivation": "\u4e3a\u4e86\u5728AI\u804a\u5929\u73af\u5883\u4e2d\u5b9e\u73b0\u65e0\u7f1d\u7684\u7535\u5b50\u5546\u52a1\u4f53\u9a8c\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u79bb\u5f00\u5bf9\u8bdd\u754c\u9762\u5373\u53ef\u5b8c\u6210\u8d2d\u4e70\u3002", "method": "\u4f7f\u7528Stripe\u7684\u65b0\u5171\u4eab\u652f\u4ed8\u4ee4\u724c\u4fdd\u62a4\u51ed\u8bc1\uff0c\u540c\u65f6\u652f\u6301\u6b3a\u8bc8\u68c0\u67e5\u548c\u4e0e\u5546\u5bb6\u7684\u65e0\u7f1dAPI\u4ea4\u63a5\u3002\u63a8\u51fa\u5f00\u653e\u6807\u51c6\u7684\u4ee3\u7406\u5546\u52a1\u534f\u8bae(ACP)\u3002", "result": "\u6210\u529f\u5728ChatGPT\u4e2d\u90e8\u7f72\u5373\u65f6\u7ed3\u8d26\u529f\u80fd\uff0c\u4f7f\u7f8e\u56fd\u7528\u6237\u80fd\u591f\u76f4\u63a5\u4eceEtsy\u5546\u5bb6\u8d2d\u4e70\uff0c\u5e76\u8ba1\u5212\u6269\u5c55\u5230Shopify\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "\u8fd9\u9879\u5408\u4f5c\u6807\u5fd7\u7740AI\u9a71\u52a8\u7684\u5546\u52a1\u65b0\u65f6\u4ee3\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u804a\u5929\u754c\u9762\u4e2d\u96c6\u6210\u652f\u4ed8\u529f\u80fd\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u4ea4\u6613\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2510.a9b3cfff", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pymnts.com%2Fnews%2Fecommerce%2F2025%2Fpaypal-honey-add-agentic-commerce-features-black-friday%2F%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/4bbt3HaElz2v29YAm_nqUe2ApvfR-7XKNcUqeFa2UA8=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pymnts.com%2Fnews%2Fecommerce%2F2025%2Fpaypal-honey-add-agentic-commerce-features-black-friday%2F%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/4bbt3HaElz2v29YAm_nqUe2ApvfR-7XKNcUqeFa2UA8=425", "authors": ["TLDR Newsletter"], "title": "PayPal Honey to add agentic commerce features by black friday", "comment": "Source: TLDR Newsletter, Date: 2025-10-02, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pymnts.com%2Fnews%2Fecommerce%2F2025%2Fpaypal-honey-add-agentic-commerce-features-black-friday%2F%3Futm_source=tldrfintech/1/01000199a5078113-2e36a8a6-0fdd-4550-a990-58d212edd842-000000/4bbt3HaElz2v29YAm_nqUe2ApvfR-7XKNcUqeFa2UA8=425", "summary": "PayPal Honey to add agentic commerce features by black friday (6 minute read) PayPal is rolling out new agentic commerce features to its Honey extension, enabling AI-powered chatbots to surface real-time pricing, merchant links, and cashback offers directly within conversations. The update, arriving before Black Friday in the US, will first be available on Chrome desktop and later on other major browsers. PayPal says this evolution transforms Honey from a coupon tool into a commerce intellige...", "source": "tldr", "AI": {"tldr": "PayPal\u6b63\u5728\u4e3aHoney\u6269\u5c55\u6dfb\u52a0\u667a\u80fd\u5546\u52a1\u529f\u80fd\uff0c\u5728\u5bf9\u8bdd\u4e2d\u63d0\u4f9b\u5b9e\u65f6\u5b9a\u4ef7\u3001\u5546\u5bb6\u94fe\u63a5\u548c\u8fd4\u73b0\u4f18\u60e0\uff0c\u8ba1\u5212\u5728\u7f8e\u56fd\u9ed1\u8272\u661f\u671f\u4e94\u524d\u63a8\u51fa", "motivation": "\u5c06Honey\u4ece\u5355\u7eaf\u7684\u4f18\u60e0\u5238\u5de5\u5177\u8f6c\u53d8\u4e3a\u5546\u52a1\u667a\u80fd\u5e73\u53f0\uff0c\u63d0\u5347\u7528\u6237\u8d2d\u7269\u4f53\u9a8c\u548c\u53c2\u4e0e\u5ea6", "method": "\u5728Honey\u6d4f\u89c8\u5668\u6269\u5c55\u4e2d\u96c6\u6210AI\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u529f\u80fd\uff0c\u9996\u5148\u5728Chrome\u684c\u9762\u7248\u63a8\u51fa\uff0c\u540e\u7eed\u6269\u5c55\u5230\u5176\u4ed6\u4e3b\u6d41\u6d4f\u89c8\u5668", "result": "\u7528\u6237\u53ef\u4ee5\u5728\u5bf9\u8bdd\u4e2d\u76f4\u63a5\u83b7\u53d6\u5b9e\u65f6\u5b9a\u4ef7\u4fe1\u606f\u3001\u5546\u5bb6\u94fe\u63a5\u548c\u8fd4\u73b0\u4f18\u60e0\uff0c\u63d0\u5347\u4e86\u8d2d\u7269\u4fbf\u5229\u6027", "conclusion": "PayPal\u901a\u8fc7AI\u6280\u672f\u5347\u7ea7Honey\u5e73\u53f0\uff0c\u4f7f\u5176\u6210\u4e3a\u66f4\u667a\u80fd\u7684\u5546\u52a1\u52a9\u624b", "topic": "swe application"}}
{"id": "tldr.2510.9453b241", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alephic.com%2Fwriting%2Fthe-magic-of-claude-code%3Futm_source=tldrai/1/01000199a513af84-c1f2fc43-33d9-4180-8ae1-b69ae1cd198d-000000/V_exDcsGoXxQtyC-9v_rABtpBVR-UVSzbf697U8JJTc=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alephic.com%2Fwriting%2Fthe-magic-of-claude-code%3Futm_source=tldrai/1/01000199a513af84-c1f2fc43-33d9-4180-8ae1-b69ae1cd198d-000000/V_exDcsGoXxQtyC-9v_rABtpBVR-UVSzbf697U8JJTc=425", "authors": ["TLDR Newsletter"], "title": "The Magic of Claude Code", "comment": "Source: TLDR Newsletter, Date: 2025-10-02, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alephic.com%2Fwriting%2Fthe-magic-of-claude-code%3Futm_source=tldrai/1/01000199a513af84-c1f2fc43-33d9-4180-8ae1-b69ae1cd198d-000000/V_exDcsGoXxQtyC-9v_rABtpBVR-UVSzbf697U8JJTc=425", "summary": "The Magic of Claude Code (5 minute read) Claude Code is a terminal-based application that trades accessibility for native Unix command integration. The commands that power Unix happen to be perfectly suited for use by large language models. Having file system access allows Claude Code to write notes to itself, accumulate knowledge, keep running tallies, and think beyond a single conversation. Claude Code works as a blueprint for building reliable agentic systems because it captures model capa...", "source": "tldr", "AI": {"tldr": "Claude Code\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ec8\u7aef\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u901a\u8fc7\u727a\u7272\u6613\u7528\u6027\u6765\u83b7\u5f97\u539f\u751fUnix\u547d\u4ee4\u96c6\u6210\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528Unix\u547d\u4ee4\u8fdb\u884c\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\u3001\u77e5\u8bc6\u79ef\u7d2f\u548c\u8de8\u5bf9\u8bdd\u601d\u8003\u3002", "motivation": "Unix\u547d\u4ee4\u975e\u5e38\u9002\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\uff0c\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\u80fd\u529b\u4f7f\u5f97Claude Code\u80fd\u591f\u81ea\u6211\u8bb0\u5f55\u7b14\u8bb0\u3001\u79ef\u7d2f\u77e5\u8bc6\u3001\u4fdd\u6301\u8fd0\u884c\u8ba1\u6570\uff0c\u5e76\u8d85\u8d8a\u5355\u6b21\u5bf9\u8bdd\u8fdb\u884c\u601d\u8003\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u7ec8\u7aef\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u6df1\u5ea6\u96c6\u6210\u539f\u751fUnix\u547d\u4ee4\uff0c\u5229\u7528\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\u80fd\u529b\u6784\u5efa\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "Claude Code\u4f5c\u4e3a\u4e00\u4e2a\u84dd\u56fe\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u6784\u5efa\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u6355\u6349\u6a21\u578b\u7684\u80fd\u529b\u5e76\u5b9e\u73b0\u8de8\u5bf9\u8bdd\u7684\u77e5\u8bc6\u79ef\u7d2f\u3002", "conclusion": "Claude Code\u8bc1\u660e\u4e86\u901a\u8fc7Unix\u547d\u4ee4\u96c6\u6210\u548c\u6587\u4ef6\u7cfb\u7edf\u8bbf\u95ee\uff0c\u53ef\u4ee5\u6784\u5efa\u51fa\u80fd\u591f\u79ef\u7d2f\u77e5\u8bc6\u548c\u8d85\u8d8a\u5355\u6b21\u5bf9\u8bdd\u9650\u5236\u7684\u53ef\u9760\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "topic": "swe application"}}
{"id": "tldr.2510.cce5c420", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneedle.app%2Fresources%2Ftop-10-ai-agents-for-productivity%3Futm_source=tldr%26utm_medium=email%26utm_campaign=newsletter_oct3_2025/1/01000199a98a252c-88aae37d-f331-4078-b59c-ba092517cd96-000000/Uu2-8Vl14ZrQJW0X5gLAj7j2Ivm9TEjC_gRuCGSl2vQ=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneedle.app%2Fresources%2Ftop-10-ai-agents-for-productivity%3Futm_source=tldr%26utm_medium=email%26utm_campaign=newsletter_oct3_2025/1/01000199a98a252c-88aae37d-f331-4078-b59c-ba092517cd96-000000/Uu2-8Vl14ZrQJW0X5gLAj7j2Ivm9TEjC_gRuCGSl2vQ=425", "authors": ["TLDR Newsletter"], "title": "10 AI agent workflows to 10x your productivity", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneedle.app%2Fresources%2Ftop-10-ai-agents-for-productivity%3Futm_source=tldr%26utm_medium=email%26utm_campaign=newsletter_oct3_2025/1/01000199a98a252c-88aae37d-f331-4078-b59c-ba092517cd96-000000/Uu2-8Vl14ZrQJW0X5gLAj7j2Ivm9TEjC_gRuCGSl2vQ=425", "summary": "10 AI agent workflows to 10x your productivity (Sponsor) Needle has recently launched its agentic automation platform - and they're giving away some insider tips on how to get the most out of your AI agents. Download these 10 free templates (that you can build out in any tool) to automate email, calendar, LinkedIn, and more. Get the templates", "source": "tldr", "AI": {"tldr": "\u63d0\u4f9b\u4e8610\u4e2aAI\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u6a21\u677f\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u7535\u5b50\u90ae\u4ef6\u3001\u65e5\u5386\u3001LinkedIn\u7b49\u4efb\u52a1\uff0c\u4ee5\u63d0\u9ad8\u751f\u4ea7\u529b", "motivation": "\u5e2e\u52a9\u7528\u6237\u901a\u8fc7AI\u4ee3\u7406\u81ea\u52a8\u5316\u65e5\u5e38\u4efb\u52a1\uff0c\u5b9e\u73b010\u500d\u751f\u4ea7\u529b\u63d0\u5347", "method": "\u63d0\u4f9b\u53ef\u4e0b\u8f7d\u7684\u514d\u8d39\u6a21\u677f\uff0c\u53ef\u5728\u4efb\u4f55\u5de5\u5177\u4e2d\u6784\u5efa\u4f7f\u7528", "result": "\u7528\u6237\u53ef\u4ee5\u81ea\u52a8\u5316\u5904\u7406\u90ae\u4ef6\u3001\u65e5\u5386\u7ba1\u7406\u3001LinkedIn\u7b49\u5e73\u53f0\u64cd\u4f5c", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u8fd9\u4e9bAI\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u6a21\u677f\uff0c\u7528\u6237\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2510.68546ffb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zenity.io%2Fresources%2Fevents%2Fai-agent-security-summit-2025%2F%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=Q3-2025-TLDR-Tech-Newsletter%26utm_content=secondary-oct3/1/01000199a99983c5-64db715f-8f0c-4d92-a838-a8f9851f9e8f-000000/AnmujlwAGYGmhjrXTtvjIYJb1RsNgYjvqi5Sqel2LjA=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zenity.io%2Fresources%2Fevents%2Fai-agent-security-summit-2025%2F%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=Q3-2025-TLDR-Tech-Newsletter%26utm_content=secondary-oct3/1/01000199a99983c5-64db715f-8f0c-4d92-a838-a8f9851f9e8f-000000/AnmujlwAGYGmhjrXTtvjIYJb1RsNgYjvqi5Sqel2LjA=425", "authors": ["TLDR Newsletter"], "title": "How Google, Microsoft, OpenAI, and ServiceNow are thinking about AI agent security", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zenity.io%2Fresources%2Fevents%2Fai-agent-security-summit-2025%2F%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=Q3-2025-TLDR-Tech-Newsletter%26utm_content=secondary-oct3/1/01000199a99983c5-64db715f-8f0c-4d92-a838-a8f9851f9e8f-000000/AnmujlwAGYGmhjrXTtvjIYJb1RsNgYjvqi5Sqel2LjA=425", "summary": "How Google, Microsoft, OpenAI, and ServiceNow are thinking about AI agent security (Sponsor) Find out what the world's AI leaders are doing about the most urgent security challenge. The AI Agent Security Summit (Oct 8 / San Francisco) packs a lot into one day: 3 keynotes, 6 lightning talks, and 2 panels; featuring security experts from OWASP, Glean, ScaleAI, Stanford and more. Seats are limited (literally). Save yours now", "source": "tldr", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u4e86Google\u3001Microsoft\u3001OpenAI\u548cServiceNow\u7b49\u516c\u53f8\u5bf9AI\u4ee3\u7406\u5b89\u5168\u7684\u601d\u8003\uff0c\u5e76\u4ecb\u7ecd\u4e86AI\u4ee3\u7406\u5b89\u5168\u5cf0\u4f1a\u7684\u76f8\u5173\u5185\u5bb9\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5b89\u5168\u6027\u6210\u4e3a\u6700\u7d27\u8feb\u7684\u6311\u6218\u4e4b\u4e00\uff0c\u9700\u8981\u884c\u4e1a\u9886\u5bfc\u8005\u5171\u540c\u63a2\u8ba8\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4e3e\u529eAI\u4ee3\u7406\u5b89\u5168\u5cf0\u4f1a\uff0c\u6c47\u96c6\u6765\u81eaOWASP\u3001Glean\u3001ScaleAI\u3001\u65af\u5766\u798f\u7b49\u673a\u6784\u7684\u5b89\u5168\u4e13\u5bb6\u8fdb\u884c3\u573a\u4e3b\u9898\u6f14\u8bb2\u30016\u573a\u95ea\u7535\u6f14\u8bb2\u548c2\u573a\u5c0f\u7ec4\u8ba8\u8bba\u3002", "result": "\u5cf0\u4f1a\u63d0\u4f9b\u4e86\u5e73\u53f0\u8ba9\u884c\u4e1a\u4e13\u5bb6\u5206\u4eabAI\u4ee3\u7406\u5b89\u5168\u7684\u6700\u4f73\u5b9e\u8df5\u548c\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AI\u4ee3\u7406\u5b89\u5168\u662f\u5f53\u524d\u4e9f\u9700\u5173\u6ce8\u7684\u91cd\u8981\u8bae\u9898\uff0c\u9700\u8981\u884c\u4e1a\u534f\u4f5c\u5171\u540c\u5e94\u5bf9\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.56002213", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.usetusk.ai%2Fresources%2Fthe-case-for-comment-driven-development%3Futm_source=tldrwebdev/1/01000199a9d5f0bf-dfdaa5c7-7509-4fb5-a1ab-194f35d07006-000000/Lu1ubHGeinwe1-QpBC5_qP41lB9343hZvcLjaHPByrc=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.usetusk.ai%2Fresources%2Fthe-case-for-comment-driven-development%3Futm_source=tldrwebdev/1/01000199a9d5f0bf-dfdaa5c7-7509-4fb5-a1ab-194f35d07006-000000/Lu1ubHGeinwe1-QpBC5_qP41lB9343hZvcLjaHPByrc=425", "authors": ["TLDR Newsletter"], "title": "The Case for Comment-Driven Development", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.usetusk.ai%2Fresources%2Fthe-case-for-comment-driven-development%3Futm_source=tldrwebdev/1/01000199a9d5f0bf-dfdaa5c7-7509-4fb5-a1ab-194f35d07006-000000/Lu1ubHGeinwe1-QpBC5_qP41lB9343hZvcLjaHPByrc=425", "summary": "The Case for Comment-Driven Development (8 minute read) While traditionally good code was considered self-documenting, LLMs rely on and generate comments, making them necessary for good AI collaboration. However, AI-generated comments are often inaccurate and don't have good context. Instead, engineers should use detailed, contextual comments to help drive the code generation by LLMs.", "source": "tldr", "AI": {"tldr": "LLMs\u4f9d\u8d56\u5e76\u751f\u6210\u4ee3\u7801\u6ce8\u91ca\uff0c\u4f46AI\u751f\u6210\u7684\u6ce8\u91ca\u5f80\u5f80\u4e0d\u51c6\u786e\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u3002\u5de5\u7a0b\u5e08\u5e94\u4f7f\u7528\u8be6\u7ec6\u3001\u6709\u4e0a\u4e0b\u6587\u7684\u6ce8\u91ca\u6765\u9a71\u52a8LLMs\u7684\u4ee3\u7801\u751f\u6210\uff0c\u8fd9\u88ab\u79f0\u4e3a\u6ce8\u91ca\u9a71\u52a8\u5f00\u53d1\u3002", "motivation": "\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u597d\u7684\u4ee3\u7801\u662f\u81ea\u6587\u6863\u5316\u7684\uff0c\u4f46LLMs\u4f9d\u8d56\u6ce8\u91ca\u6765\u7406\u89e3\u548c\u751f\u6210\u4ee3\u7801\u3002AI\u751f\u6210\u7684\u6ce8\u91ca\u5b58\u5728\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4eba\u7c7b\u5de5\u7a0b\u5e08\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6ce8\u91ca\u6765\u6539\u5584\u4e0eAI\u7684\u534f\u4f5c\u3002", "method": "\u63d0\u51fa\u6ce8\u91ca\u9a71\u52a8\u5f00\u53d1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5de5\u7a0b\u5e08\u5e94\u8be5\u7f16\u5199\u8be6\u7ec6\u3001\u5305\u542b\u4e0a\u4e0b\u6587\u7684\u6ce8\u91ca\uff0c\u8fd9\u4e9b\u6ce8\u91ca\u5c06\u4f5c\u4e3aLLMs\u751f\u6210\u4ee3\u7801\u7684\u91cd\u8981\u8f93\u5165\u548c\u6307\u5bfc\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u8be6\u7ec6\u3001\u6709\u4e0a\u4e0b\u6587\u7684\u6ce8\u91ca\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8LLMs\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\uff0c\u6539\u5584\u4eba\u673a\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "\u5728AI\u65f6\u4ee3\uff0c\u6ce8\u91ca\u4e0d\u518d\u662f\u53ef\u9009\u7684\uff0c\u800c\u662f\u5fc5\u8981\u7684\u534f\u4f5c\u5de5\u5177\u3002\u5de5\u7a0b\u5e08\u9700\u8981\u6539\u53d8\u4f20\u7edf\u89c2\u5ff5\uff0c\u91c7\u7528\u6ce8\u91ca\u9a71\u52a8\u5f00\u53d1\u6765\u5145\u5206\u53d1\u6325LLMs\u7684\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2510.fbcb0c86", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Fthe-rag-obituary-killed-by-agents%3Futm_source=tldrfounders/1/01000199a9f931b2-e02ac2cb-9938-4b1d-affb-3f07ef4e96cc-000000/Pb0zqc1YbsJwzmI3hJoB21nutA1F88v8Y3320qUaWZM=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Fthe-rag-obituary-killed-by-agents%3Futm_source=tldrfounders/1/01000199a9f931b2-e02ac2cb-9938-4b1d-affb-3f07ef4e96cc-000000/Pb0zqc1YbsJwzmI3hJoB21nutA1F88v8Y3320qUaWZM=425", "authors": ["TLDR Newsletter"], "title": "The RAG Obituary: Killed by Agents, Buried by Context Windows", "comment": "Source: TLDR Newsletter, Date: 2025-10-03, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nicolasbustamante.com%2Fp%2Fthe-rag-obituary-killed-by-agents%3Futm_source=tldrfounders/1/01000199a9f931b2-e02ac2cb-9938-4b1d-affb-3f07ef4e96cc-000000/Pb0zqc1YbsJwzmI3hJoB21nutA1F88v8Y3320qUaWZM=425", "summary": "The RAG Obituary: Killed by Agents, Buried by Context Windows (4 minute read) A founder built a financial search engine using the standard playbook - break documents into chunks, turn them into vectors, search, rerank, feed results to an AI. It took three years and constant maintenance. Then he tried Claude Code and realized it just searched files directly with grep, a tool from 1973, and got better results. The entire retrieval infrastructure was solving a problem that no longer exists. Earl...", "source": "tldr", "AI": {"tldr": "\u4f20\u7edfRAG\u7cfb\u7edf\u88ab\u66f4\u7b80\u5355\u7684\u57fa\u4e8egrep\u7684\u6587\u4ef6\u641c\u7d22\u65b9\u6cd5\u8d85\u8d8a\uff0c\u8868\u660e\u590d\u6742\u7684\u68c0\u7d22\u57fa\u7840\u8bbe\u65bd\u53ef\u80fd\u5df2\u4e0d\u518d\u5fc5\u8981", "motivation": "\u63ed\u793a\u4f20\u7edfRAG\u7cfb\u7edf\u53ef\u80fd\u8fc7\u5ea6\u590d\u6742\u5316\uff0c\u800c\u7b80\u5355\u7684\u6587\u4ef6\u641c\u7d22\u65b9\u6cd5\u5728AI\u65f6\u4ee3\u53ef\u80fd\u66f4\u6709\u6548", "method": "\u5bf9\u6bd4\u4f20\u7edfRAG\u7cfb\u7edf\uff08\u6587\u6863\u5206\u5757\u3001\u5411\u91cf\u5316\u3001\u641c\u7d22\u3001\u91cd\u6392\u5e8f\uff09\u4e0e\u76f4\u63a5\u4f7f\u7528grep\u641c\u7d22\u6587\u4ef6\u7684\u65b9\u6cd5", "result": "\u4f7f\u7528Claude Code\u548cgrep\u76f4\u63a5\u641c\u7d22\u6587\u4ef6\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4f20\u7edfRAG\u7cfb\u7edf\u663e\u5f97\u591a\u4f59", "conclusion": "\u590d\u6742\u7684\u68c0\u7d22\u57fa\u7840\u8bbe\u65bd\u53ef\u80fd\u6b63\u5728\u89e3\u51b3\u4e00\u4e2a\u4e0d\u518d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u66f4\u7b80\u5355\u7684\u65b9\u6cd5\u53ef\u80fd\u66f4\u6709\u6548", "topic": "agent analysis"}}
{"id": "wechat.2510.8147b2b4", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNzQ5ODc3Mg==&mid=2247488766&idx=1&sn=a7a5c94bda3e2f0776649991afc36a34&chksm=c35b032487b2e4fc024c149251d9326a5f462b505763ea02a7eb18b303307a9d70790b55eca5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNzQ5ODc3Mg==&mid=2247488766&idx=1&sn=a7a5c94bda3e2f0776649991afc36a34&chksm=c35b032487b2e4fc024c149251d9326a5f462b505763ea02a7eb18b303307a9d70790b55eca5#rd", "authors": ["AI Evolution-Geohazards AIEG"], "title": "\u524d\u6cbf\u901f\u9012 | Nature 2025: \u89e3\u9501AI\u63a8\u7406\u6f5c\u80fd\uff1aDeepSeek-R1\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u7a81\u7834\u6027\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-10-06 13:43:27", "summary": "\u4e09\u3001\u7ed3\u679c\u7ed3\u8bba 1.\u7eaf\u7cb9\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u201c\u89e3\u9501\u201d\u6a21\u578b\u7684\u5185\u5728\u63a8\u7406\u6f5c\u80fd \u7814\u7a76\u660e\u786e\u6307\u51fa\uff0c\u9884\u8bad\u7ec3\u597d\u7684\u5927\u6a21\u578b\u672c\u8eab\u5df2\u8574\u542b\u5de8\u5927\u7684\u63a8\u7406\u6f5c\u529b\u3002\u89e3\u9501\u8fd9\u79cd\u6f5c\u529b\u7684\u5173\u952e\uff0c\u5e76\u975e\u66f4\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u662f\u63d0\u4f9b\u4e00\u4e2a\u5141\u8bb8\u6a21\u578b\u81ea\u7531\u63a2\u7d22\u548c\u8bd5\u9519\u7684\u5f3a\u5316", "AI": {"tldr": "\u4e09\u3001\u7ed3\u679c\u7ed3\u8bba 1.\u7eaf\u7cb9\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u201c\u89e3\u9501\u201d\u6a21\u578b\u7684\u5185\u5728\u63a8\u7406\u6f5c\u80fd \u7814\u7a76\u660e\u786e\u6307\u51fa\uff0c\u9884\u8bad\u7ec3\u597d\u7684\u5927\u6a21\u578b\u672c\u8eab\u5df2\u8574\u542b\u5de8\u5927\u7684\u63a8\u7406\u6f5c\u529b\u3002\u89e3\u9501\u8fd9\u79cd\u6f5c\u529b\u7684\u5173\u952e\uff0c\u5e76\u975e\u66f4\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u662f\u63d0\u4f9b\u4e00\u4e2a\u5141\u8bb8\u6a21\u578b\u81ea\u7531\u63a2\u7d22\u548c\u8bd5\u9519\u7684\u5f3a\u5316", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.20064186", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0ODMwMjMwMA==&mid=2247693133&idx=1&sn=7e1d1438dbf7761101447f5c2df3ac22&chksm=c2564fd58f3135ab2ce1e0352f7387d02c70705beeede1526effb10f3ce5663c6c9d168c8e50#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0ODMwMjMwMA==&mid=2247693133&idx=1&sn=7e1d1438dbf7761101447f5c2df3ac22&chksm=c2564fd58f3135ab2ce1e0352f7387d02c70705beeede1526effb10f3ce5663c6c9d168c8e50#rd", "authors": ["\u8fd0\u7b79OR\u5e37\u5e44"], "title": "\u4f18\u5316\uff5c\u57fa\u4e8e\u56fe\u548c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u7ec4\u5408\u4f18\u5316\u7b97\u6cd5", "comment": "Source: WeChat, Published: 2025-10-06 12:01:27", "summary": "\u8fd9\u4e00\u90e8\u5206\u628a\u5b66\u4e60\u8fc7\u7a0b\u660e\u786e\u4e3a\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff1a\u8bc4\u4ef7\u51fd\u6570 \u88ab\u5f53\u4f5c\u9700\u8981\u5b66\u4e60\u7684\u201c\u72b6\u6001\u2014\u52a8\u4f5c\u4ef7\u503c\u51fd\u6570\u201d\uff0c\u5176\u8f93\u5165\u4e00\u65b9\u9762\u662f\u5e26\u6709\u201c\u5f53\u524d\u90e8\u5206\u89e3\u201d\u6807\u7b7e\u7684\u56fe\u72b6\u6001\uff0c\u53e6\u4e00\u65b9\u9762\u662f\u5019\u9009\u8282\u70b9\u7684\u5d4c\u5165\u3002", "AI": {"tldr": "\u8fd9\u4e00\u90e8\u5206\u628a\u5b66\u4e60\u8fc7\u7a0b\u660e\u786e\u4e3a\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff1a\u8bc4\u4ef7\u51fd\u6570 \u88ab\u5f53\u4f5c\u9700\u8981\u5b66\u4e60\u7684\u201c\u72b6\u6001\u2014\u52a8\u4f5c\u4ef7\u503c\u51fd\u6570\u201d\uff0c\u5176\u8f93\u5165\u4e00\u65b9\u9762\u662f\u5e26\u6709\u201c\u5f53\u524d\u90e8\u5206\u89e3\u201d\u6807\u7b7e\u7684\u56fe\u72b6\u6001\uff0c\u53e6\u4e00\u65b9\u9762\u662f\u5019\u9009\u8282\u70b9\u7684\u5d4c\u5165\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.0504382b", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwOTA1MDAyNA==&mid=2650040841&idx=3&sn=96069670ba310b4a4a83b71e698a5aa6&chksm=8ef91048d87d95dab19e79726465fcc5c85c02096242c0262f2ae86ed0059e30fc9a9acaa17e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwOTA1MDAyNA==&mid=2650040841&idx=3&sn=96069670ba310b4a4a83b71e698a5aa6&chksm=8ef91048d87d95dab19e79726465fcc5c85c02096242c0262f2ae86ed0059e30fc9a9acaa17e#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u5b66\u5bb6"], "title": "\u5927\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8f6c\u5411\uff1aAgentic <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5168\u666f\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-06 08:59:17", "summary": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u66f4\u7a33\u5b9a\u3001\u66f4\u5177\u903b\u8f91\u4e00\u81f4\u6027\u7684\u63a8\u7406\u8def\u5f84\u3002\u81ea\u6211\u6539\u8fdb\uff08Self-Improvement\uff09\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u8fdb\u884c\u53cd\u601d\u3001\u81ea\u6211\u4fee\u6b63\uff0c\u5f62\u6210\u95ed\u73af\u5b66\u4e60\u673a\u5236\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u66f4\u7a33\u5b9a\u3001\u66f4\u5177\u903b\u8f91\u4e00\u81f4\u6027\u7684\u63a8\u7406\u8def\u5f84\u3002\u81ea\u6211\u6539\u8fdb\uff08Self-Improvement\uff09\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u8fdb\u884c\u53cd\u601d\u3001\u81ea\u6211\u4fee\u6b63\uff0c\u5f62\u6210\u95ed\u73af\u5b66\u4e60\u673a\u5236\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.eef80ffd", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3MzM4NjI0NQ==&mid=2247524142&idx=1&sn=720bacb3f1c72572a61430156169e12a&chksm=fd91ed93d6cbbf3859d05ed75f7461857077a6cd927a1996d3c0295752a00869037672f28609#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3MzM4NjI0NQ==&mid=2247524142&idx=1&sn=720bacb3f1c72572a61430156169e12a&chksm=fd91ed93d6cbbf3859d05ed75f7461857077a6cd927a1996d3c0295752a00869037672f28609#rd", "authors": ["\u82cf\u54f2\u7ba1\u7406\u54a8\u8be2"], "title": "Kevin P. Murphy \u7248<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-06 08:15:00", "summary": "\u95ee\u9898 1\uff1a\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u57fa\u3001\u7b56\u7565\u57fa\u3001\u6a21\u578b\u57fa\u4e09\u5927\u8303\u5f0f\u7684\u6838\u5fc3\u533a\u522b\u662f\u4ec0\u4e48\uff1f\u5404\u81ea\u9002\u7528\u4e8e\u54ea\u4e9b\u573a\u666f\uff1f\u7b54\uff1a\u6838\u5fc3\u533a\u522b \u4ef7\u503c\u57fa RL\u6838\u5fc3\u662f\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff08V/Q \u51fd\u6570\uff09\uff0c\u901a\u8fc7 Bellman \u65b9\u7a0b\u8fed\u4ee3\u4f18\u5316\uff0c\u95f4\u63a5\u63a8\u5bfc\u7b56\u7565\uff08\u5982\\\uff08a=\\argmax_a Q\uff08s\uff0ca\uff09\\\uff09\uff09", "AI": {"tldr": "\u95ee\u9898 1\uff1a\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u57fa\u3001\u7b56\u7565\u57fa\u3001\u6a21\u578b\u57fa\u4e09\u5927\u8303\u5f0f\u7684\u6838\u5fc3\u533a\u522b\u662f\u4ec0\u4e48\uff1f\u5404\u81ea\u9002\u7528\u4e8e\u54ea\u4e9b\u573a\u666f\uff1f\u7b54\uff1a\u6838\u5fc3\u533a\u522b \u4ef7\u503c\u57fa RL\u6838\u5fc3\u662f\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff08V/Q \u51fd\u6570\uff09\uff0c\u901a\u8fc7 Bellman \u65b9\u7a0b\u8fed\u4ee3\u4f18\u5316\uff0c\u95f4\u63a5\u63a8\u5bfc\u7b56\u7565\uff08\u5982\\\uff08a=\\argmax_a Q\uff08s\uff0ca\uff09\\\uff09\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.f0886e55", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODUzMTYyOA==&mid=2650823486&idx=1&sn=c7b6b4d576116c121f96c1761cc35ccf&chksm=8aded318078ab074c41e5a3423c3683e855e7ab7b2be338c491ac61dea2c9e31d1ad52da9dfe#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODUzMTYyOA==&mid=2650823486&idx=1&sn=c7b6b4d576116c121f96c1761cc35ccf&chksm=8aded318078ab074c41e5a3423c3683e855e7ab7b2be338c491ac61dea2c9e31d1ad52da9dfe#rd", "authors": ["Myautotime"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u63a8\u52a8\u81ea\u52a8\u9a7e\u9a76\u5347\u7ea7", "comment": "Source: WeChat, Published: 2025-10-06 07:13:17", "summary": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u673a\u5236\u548c\u56e0\u679c\u63a8\u7406\u3001\u6570\u636e\u751f\u6210\u548c\u589e\u5f3a\u3001\u4eff\u771f\u8bd5\u9519\u548c\u6301\u7eed\u63a2\u7d22\u514b\u670d\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u4e09\u5927\u7f3a\u9677\u3002\u4e0d\u8fc7\uff0c\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4e4b\u95f4\u5e76\u975e\u975e\u6b64\u5373\u5f7c\u7684\u66ff\u4ee3\u5173\u7cfb\uff0c\u800c\u662f\u534f\u540c\u5e94\u7528\u3001\u518d\u63a5\u518d\u5389\u7684\u5408\u4f5c\u5173\u7cfb\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u673a\u5236\u548c\u56e0\u679c\u63a8\u7406\u3001\u6570\u636e\u751f\u6210\u548c\u589e\u5f3a\u3001\u4eff\u771f\u8bd5\u9519\u548c\u6301\u7eed\u63a2\u7d22\u514b\u670d\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u4e09\u5927\u7f3a\u9677\u3002\u4e0d\u8fc7\uff0c\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4e4b\u95f4\u5e76\u975e\u975e\u6b64\u5373\u5f7c\u7684\u66ff\u4ee3\u5173\u7cfb\uff0c\u800c\u662f\u534f\u540c\u5e94\u7528\u3001\u518d\u63a5\u518d\u5389\u7684\u5408\u4f5c\u5173\u7cfb\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.286d2488", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247563507&idx=1&sn=310341fdcee3b0aa24baf63fa36db11e&chksm=fc8c728dbb9e7527465362b40d9bf152a9e75ec944d510c6912bff061a84ae4ee97f9e62fd5d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247563507&idx=1&sn=310341fdcee3b0aa24baf63fa36db11e&chksm=fc8c728dbb9e7527465362b40d9bf152a9e75ec944d510c6912bff061a84ae4ee97f9e62fd5d#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u94feunion"], "title": "\u3010AI\u3011\u5168\u65b0\u5408\u6210\u6846\u67b6SOTA\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5f53\u5f15\u64ce\uff0c\u4efb\u52a1\u5408\u6210\u5f53\u71c3\u6599\uff0c\u8682\u8681\u6e2f\u5927\u8054\u5408\u51fa\u54c1", "comment": "Source: WeChat, Published: 2025-10-06 06:55:00", "summary": "\u4e00\u662f\u5f3a\u5316\u5b66\u4e60\u3002\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e4b\u5e74\uff0c\u8be5\u9879\u6280\u672f\u5df2\u7ecf\u5f97\u5230\u793e\u533a\u8db3\u591f\u591a\u7684\u5173\u6ce8\u4e0e\u6295\u5165\uff0c\u65e0\u8bba\u662f\u65b9\u6cd5\u8fd8\u662f\u6846\u67b6\u90fd\u5728\u6025\u901f\u63a8\u8fdb\u3002reinforcement learning \u800c\u53e6\u4e00\u4e2a\uff0c\u56e2\u961f\u8ba4\u4e3a\u662f\u4efb\u52a1\u5408\u6210\u3002", "AI": {"tldr": "\u4e00\u662f\u5f3a\u5316\u5b66\u4e60\u3002\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e4b\u5e74\uff0c\u8be5\u9879\u6280\u672f\u5df2\u7ecf\u5f97\u5230\u793e\u533a\u8db3\u591f\u591a\u7684\u5173\u6ce8\u4e0e\u6295\u5165\uff0c\u65e0\u8bba\u662f\u65b9\u6cd5\u8fd8\u662f\u6846\u67b6\u90fd\u5728\u6025\u901f\u63a8\u8fdb\u3002reinforcement learning \u800c\u53e6\u4e00\u4e2a\uff0c\u56e2\u961f\u8ba4\u4e3a\u662f\u4efb\u52a1\u5408\u6210\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.492f1990", "categories": ["wechat.article", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487081&idx=1&sn=4899ed5ded9722062e56318d101ac749&chksm=c12399231149bef5b8abde51fe31d3eb67fd0a0134961f4756cc66866d7b87fdec095895ef0b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5NzU2OTc2OQ==&mid=2247487081&idx=1&sn=4899ed5ded9722062e56318d101ac749&chksm=c12399231149bef5b8abde51fe31d3eb67fd0a0134961f4756cc66866d7b87fdec095895ef0b#rd", "authors": ["NLP\u5b66\u4e60\u52a0\u6cb9\u7ad9"], "title": "100\u9875\u7efc\u8ff0\u8be6\u7ec6\u4ecb\u7ecdAgentic<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8303\u5f0f\uff01", "comment": "Source: WeChat, Published: 2025-10-06 02:44:55", "summary": "\u5728\u4f20\u7edf\u7684 llm \u5f3a\u5316\u5b66\u4e60\uff08rlhf\u3001dpo \u7b49\uff09\u4e2d\uff0c\u8bed\u8a00\u6a21\u578b\u88ab\u89c6\u4e3a\u5355\u8f6e\u8f93\u51fa\u7684\u751f\u6210\u5668\uff0c\u6838\u5fc3\u76ee\u6807\u662f\u201c\u7b54\u5f97\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u201d\u3002\u8fd9\u79cd\u8303\u5f0f\u867d\u7136\u63a8\u52a8\u4e86 ChatGPT \u7684\u6210\u529f\uff0c\u4f46\u4ecd\u5c40\u9650\u4e8e \u5355\u6b65\u51b3\u7b56\uff0c\u7f3a\u4e4f\u957f\u671f\u4e92\u52a8\u80fd\u529b\u3002", "AI": {"tldr": "\u5728\u4f20\u7edf\u7684 llm \u5f3a\u5316\u5b66\u4e60\uff08rlhf\u3001dpo \u7b49\uff09\u4e2d\uff0c\u8bed\u8a00\u6a21\u578b\u88ab\u89c6\u4e3a\u5355\u8f6e\u8f93\u51fa\u7684\u751f\u6210\u5668\uff0c\u6838\u5fc3\u76ee\u6807\u662f\u201c\u7b54\u5f97\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u201d\u3002\u8fd9\u79cd\u8303\u5f0f\u867d\u7136\u63a8\u52a8\u4e86 ChatGPT \u7684\u6210\u529f\uff0c\u4f46\u4ecd\u5c40\u9650\u4e8e \u5355\u6b65\u51b3\u7b56\uff0c\u7f3a\u4e4f\u957f\u671f\u4e92\u52a8\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.1856ca89", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDE2MTI4OA==&mid=2653141067&idx=1&sn=29f58c5e7113202ea1f0e308eef23c0c&chksm=f130e0c2c65ca6e41012794b2ef0ff930f1e01db902f42fd54377e63d973d839dbef196fbed5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDE2MTI4OA==&mid=2653141067&idx=1&sn=29f58c5e7113202ea1f0e308eef23c0c&chksm=f130e0c2c65ca6e41012794b2ef0ff930f1e01db902f42fd54377e63d973d839dbef196fbed5#rd", "authors": ["\u91d1\u878dIT\u90a3\u4e9b\u4e8b\u513f"], "title": "\u673a\u5668\u5b66\u4e60\u56db\u5927\u6838\u5fc3\u5b66\u4e60\u7b56\u7565\uff1a\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u4e0e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-06 00:40:32", "summary": "\u5728\u957f\u671f\u53d1\u5c55\u4e2d\uff0c\u5f62\u6210\u4e86\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u56db\u5927\u6838\u5fc3\u5b66\u4e60\u7b56\u7565\u3002\u8fd9\u56db\u79cd\u7b56\u7565\u57fa\u4e8e\u6570\u636e\u6807\u6ce8\u72b6\u6001\u4e0e\u5b66\u4e60\u4ea4\u4e92\u6a21\u5f0f\u7684\u5dee\u5f02\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4e1a\u52a1\u573a\u666f\uff0c\u5171\u540c\u6784\u6210\u4e86\u673a\u5668\u5b66\u4e60\u6280\u672f\u843d\u5730\u7684\u6838\u5fc3\u6846\u67b6\u3002", "AI": {"tldr": "\u5728\u957f\u671f\u53d1\u5c55\u4e2d\uff0c\u5f62\u6210\u4e86\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u56db\u5927\u6838\u5fc3\u5b66\u4e60\u7b56\u7565\u3002\u8fd9\u56db\u79cd\u7b56\u7565\u57fa\u4e8e\u6570\u636e\u6807\u6ce8\u72b6\u6001\u4e0e\u5b66\u4e60\u4ea4\u4e92\u6a21\u5f0f\u7684\u5dee\u5f02\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4e1a\u52a1\u573a\u666f\uff0c\u5171\u540c\u6784\u6210\u4e86\u673a\u5668\u5b66\u4e60\u6280\u672f\u843d\u5730\u7684\u6838\u5fc3\u6846\u67b6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.5caa2134", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570938&idx=2&sn=328710b79fe1406b3e5f07698830003b&chksm=965847cc97744328823a29284b1dc7694609ca0ab7a9f15eaab31e0aedaed1ff4feba52e6303#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570938&idx=2&sn=328710b79fe1406b3e5f07698830003b&chksm=965847cc97744328823a29284b1dc7694609ca0ab7a9f15eaab31e0aedaed1ff4feba52e6303#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "DRL\u5723\u7ecf2025\u6700\u65b0\u7248-\u300a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>:\u5bfc\u8bba\u7b2c\u4e8c\u7248\u300b\u514d\u8d39pdf\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-10-05 16:00:00", "summary": "\u6211\u4eec\u7b2c\u4e8c\u7248\u7684\u76ee\u6807\u548c\u7b2c\u4e00\u7248\u7684\u76ee\u6807\u662f\u4e00\u6837\u7684\uff1a\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u548c\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u800c\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u4f9b\u6240\u6709\u76f8\u5173\u5b66\u79d1\u7684\u8bfb\u8005\u9605\u8bfb\u3002\u8be5\u7248\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u4ecb\u7ecd\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6838\u5fc3\uff0c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u91cd\u70b9\u3002", "AI": {"tldr": "\u6211\u4eec\u7b2c\u4e8c\u7248\u7684\u76ee\u6807\u548c\u7b2c\u4e00\u7248\u7684\u76ee\u6807\u662f\u4e00\u6837\u7684\uff1a\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u548c\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u800c\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u4f9b\u6240\u6709\u76f8\u5173\u5b66\u79d1\u7684\u8bfb\u8005\u9605\u8bfb\u3002\u8be5\u7248\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u4ecb\u7ecd\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6838\u5fc3\uff0c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u91cd\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.e122c375", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247493303&idx=1&sn=8dd2c0ff5c48a298fffbe9c1445ffa77&chksm=c37ffcb9c874d06a1706d0de21ee2f3417ee95a5c084da52a6a2c446104769437761634f52cb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjYzNTIwNw==&mid=2247493303&idx=1&sn=8dd2c0ff5c48a298fffbe9c1445ffa77&chksm=c37ffcb9c874d06a1706d0de21ee2f3417ee95a5c084da52a6a2c446104769437761634f52cb#rd", "authors": ["CIOCDO"], "title": "\u9a6f\u670d<em class=\"highlight\">\u4ee3\u7406</em>\u5f0fAI\uff08<em class=\"highlight\">Agentic</em> AI\uff09i<em class=\"highlight\">\u667a\u80fd\u4f53</em>\uff1a2026\u5e74\u7684\u81ea\u4e3b\u52b3\u52a8\u529b", "comment": "Source: WeChat, Published: 2025-10-06 08:17:48", "summary": "\u57fa\u7840\u5c42\uff1a\u5177\u5907\u539f\u5b50\u529f\u80fd\u7684\u5fae\u667a\u80fd\u4f53\uff08\u5982\u8f6c\u5f55\u5458\u3001Jira\u5de5\u5355\u63d0\u53d6\u5668\u3001\u822a\u73ed\u6539\u7b7e\u5668\uff09\u3002\u4e2d\u95f4\u5c42\uff1a\u5de5\u5177\u96c6\u6210\u5546\uff08\u62e5\u6709\u624b\u672f\u5200\u822c\u7cbe\u51c6\u6743\u9650\u7684MCP\u670d\u52a1\u5668\uff09\u3002\u9876\u5c42\uff1a \u7f16\u6392\u8005\u667a\u80fd\u4f53\uff08\u8d1f\u8d23\u62c6\u5206\u4efb\u52a1\u3001\u7ba1\u7406\u56de\u9000\u673a\u5236\u3001\u5e76\u5411\u4eba\u7c7b\u4e0a\u62a5\uff09\u3002", "AI": {"tldr": "\u57fa\u7840\u5c42\uff1a\u5177\u5907\u539f\u5b50\u529f\u80fd\u7684\u5fae\u667a\u80fd\u4f53\uff08\u5982\u8f6c\u5f55\u5458\u3001Jira\u5de5\u5355\u63d0\u53d6\u5668\u3001\u822a\u73ed\u6539\u7b7e\u5668\uff09\u3002\u4e2d\u95f4\u5c42\uff1a\u5de5\u5177\u96c6\u6210\u5546\uff08\u62e5\u6709\u624b\u672f\u5200\u822c\u7cbe\u51c6\u6743\u9650\u7684MCP\u670d\u52a1\u5668\uff09\u3002\u9876\u5c42\uff1a \u7f16\u6392\u8005\u667a\u80fd\u4f53\uff08\u8d1f\u8d23\u62c6\u5206\u4efb\u52a1\u3001\u7ba1\u7406\u56de\u9000\u673a\u5236\u3001\u5e76\u5411\u4eba\u7c7b\u4e0a\u62a5\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.bbd2bb29", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3NTgyMDcwNw==&mid=2247489798&idx=1&sn=1cea8fdb0213f017d5400c9c90ed6c8e&chksm=cecd68d70d81ca60ed6ceefe8ff2a1fa5257c06ccdfd358484a5a502f63a8f0292201f277b73#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3NTgyMDcwNw==&mid=2247489798&idx=1&sn=1cea8fdb0213f017d5400c9c90ed6c8e&chksm=cecd68d70d81ca60ed6ceefe8ff2a1fa5257c06ccdfd358484a5a502f63a8f0292201f277b73#rd", "authors": ["\u4e91\u5934\u7248"], "title": "<em class=\"highlight\">Agentic</em> AI\u5982\u4f55\u91cd\u6784\u4f01\u4e1a\u751f\u4ea7\u529b\uff1f", "comment": "Source: WeChat, Published: 2025-10-06 07:54:56", "summary": "Agentic AI\u7684\u5d1b\u8d77\u6709\u5176\u6df1\u523b\u7684\u4ea7\u4e1a\u903b\u8f91\u3002\u8fc7\u53bb\u51e0\u5e74\uff0cAI\u7ecf\u5386\u4e86\u4ece\u9884\u6d4bAI\uff08\u7528\u4e8e\u6b3a\u8bc8\u68c0\u6d4b\u3001\u98ce\u9669\u76d1\u63a7\uff09\u5230\u52a9\u624bAI\uff08\u5982\u804a\u5929\u673a\u5668\u4eba\uff09\u7684\u53d1\u5c55\u9636\u6bb5\uff0c\u800c\u5982\u4eca\uff0c\u4f01\u4e1a\u9700\u8981\u7684\u4e0d\u518d\u662f\"\u56de\u7b54\u95ee\u9898\"\u7684AI\uff0c\u800c\u662f\"\u89e3\u51b3\u95ee\u9898\"\u7684AI\u3002", "AI": {"tldr": "Agentic AI\u7684\u5d1b\u8d77\u6709\u5176\u6df1\u523b\u7684\u4ea7\u4e1a\u903b\u8f91\u3002\u8fc7\u53bb\u51e0\u5e74\uff0cAI\u7ecf\u5386\u4e86\u4ece\u9884\u6d4bAI\uff08\u7528\u4e8e\u6b3a\u8bc8\u68c0\u6d4b\u3001\u98ce\u9669\u76d1\u63a7\uff09\u5230\u52a9\u624bAI\uff08\u5982\u804a\u5929\u673a\u5668\u4eba\uff09\u7684\u53d1\u5c55\u9636\u6bb5\uff0c\u800c\u5982\u4eca\uff0c\u4f01\u4e1a\u9700\u8981\u7684\u4e0d\u518d\u662f\"\u56de\u7b54\u95ee\u9898\"\u7684AI\uff0c\u800c\u662f\"\u89e3\u51b3\u95ee\u9898\"\u7684AI\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.6ce5ab40", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwMDY1MjQ0OQ==&mid=2247483839&idx=1&sn=1d6fc71a34dda97b2f2409c7c7c4d984&chksm=9ba2aae0589e7782f9557cf80b4bf26377d789059a7b04a7ed6037fa757cb3c72caca3cc393d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwMDY1MjQ0OQ==&mid=2247483839&idx=1&sn=1d6fc71a34dda97b2f2409c7c7c4d984&chksm=9ba2aae0589e7782f9557cf80b4bf26377d789059a7b04a7ed6037fa757cb3c72caca3cc393d#rd", "authors": ["\u4f1a\u8bf4\u8bdd\u7684\u9f13"], "title": "<em class=\"highlight\">Agentic</em> AI\u4e0b\u7684\u7ec4\u7ec7    20251006", "comment": "Source: WeChat, Published: 2025-10-06 04:41:43", "summary": "https\uff1a//www.mckinsey.com/capabilities/quantumblack/our-insights/one-year-of-agentic-ai-six-lessons-from-the-people-doing-the-work\u5728\u4f01\u4e1a\u90e8\u7f72AI\u4ee3\u7406\uff08Agentic AI\uff09\u4ee5\u8ffd\u6c42\u751f\u4ea7\u529b\u8dc3\u5347\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u91cd\u70b9\u653e\u5728\u201c\u4f18\u5316\u73b0\u6709\u5de5\u4f5c\u6d41\u201d\u3001\u201c\u6784\u5efa\u7cfb\u7edf\u6027\u4fe1\u4efb\u201d\u548c\u201c\u5b9e\u73b0\u8d85\u7ea7", "AI": {"tldr": "https\uff1a//www.mckinsey.com/capabilities/quantumblack/our-insights/one-year-of-agentic-ai-six-lessons-from-the-people-doing-the-work\u5728\u4f01\u4e1a\u90e8\u7f72AI\u4ee3\u7406\uff08Agentic AI\uff09\u4ee5\u8ffd\u6c42\u751f\u4ea7\u529b\u8dc3\u5347\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u91cd\u70b9\u653e\u5728\u201c\u4f18\u5316\u73b0\u6709\u5de5\u4f5c\u6d41\u201d\u3001\u201c\u6784\u5efa\u7cfb\u7edf\u6027\u4fe1\u4efb\u201d\u548c\u201c\u5b9e\u73b0\u8d85\u7ea7", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.46746b68", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357210&idx=1&sn=277e97bfb293bf5c777b799e83f8f4f4&chksm=8cf6553c42e07c5a4145cec47b664765c8965960ba9b27a9c7368fe6f29036879b67af108d98#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMDE2MzkwMg==&mid=2653357210&idx=1&sn=277e97bfb293bf5c777b799e83f8f4f4&chksm=8cf6553c42e07c5a4145cec47b664765c8965960ba9b27a9c7368fe6f29036879b67af108d98#rd", "authors": ["AI Agent \u9886\u57df"], "title": "<em class=\"highlight\">\u4ee3\u7406</em>\u5f0f AI \u7684\u79d8\u5bc6\u6b66\u5668\uff1a5 \u5927\u591a<em class=\"highlight\">\u4ee3\u7406</em>\u6846\u67b6\uff0c\u8c01\u5c06\u79f0\u9738\u672a\u6765\uff1f", "comment": "Source: WeChat, Published: 2025-10-06 02:54:19", "summary": "Agentic AI\u751f\u6210\u5f0f AI \u5728\u8fc7\u53bb\u51e0\u5e74\u53d6\u5f97\u4e86\u60ca\u4eba\u8fdb\u6b65\uff0c\u4ece\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u6f14\u53d8\u4e3a\u80fd\u591f\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u590d\u6742\u6a21\u578b\u3002\u968f\u7740\u97f3\u9891\u548c\u89c6\u9891\u7684\u878d\u5165\uff0cAI \u7684\u80fd\u529b\u6269\u5c55\u5230\u533b\u7597\u7814\u7a76\u7b49\u5173\u952e\u884c\u4e1a\uff0c\u4f8b\u5982\u901a\u8fc7 MRI\u3001X \u5149\u548c CT \u626b\u63cf\u56fe\u50cf\u8bc6", "AI": {"tldr": "Agentic AI\u751f\u6210\u5f0f AI \u5728\u8fc7\u53bb\u51e0\u5e74\u53d6\u5f97\u4e86\u60ca\u4eba\u8fdb\u6b65\uff0c\u4ece\u7b80\u5355\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u6f14\u53d8\u4e3a\u80fd\u591f\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u590d\u6742\u6a21\u578b\u3002\u968f\u7740\u97f3\u9891\u548c\u89c6\u9891\u7684\u878d\u5165\uff0cAI \u7684\u80fd\u529b\u6269\u5c55\u5230\u533b\u7597\u7814\u7a76\u7b49\u5173\u952e\u884c\u4e1a\uff0c\u4f8b\u5982\u901a\u8fc7 MRI\u3001X \u5149\u548c CT \u626b\u63cf\u56fe\u50cf\u8bc6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.d8761e5b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNTQ0NDY1NQ==&mid=2247483808&idx=1&sn=a4ba05712f57f0f4116ac3711cdf90b0&chksm=c1e1e7ca479282671bb02c50b4e7a0fe88c6dcdb182e8b1333eb68af5cf827441b7e912b757a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNTQ0NDY1NQ==&mid=2247483808&idx=1&sn=a4ba05712f57f0f4116ac3711cdf90b0&chksm=c1e1e7ca479282671bb02c50b4e7a0fe88c6dcdb182e8b1333eb68af5cf827441b7e912b757a#rd", "authors": ["\u65f6\u95f4\u98d8\u8fc7"], "title": "<em class=\"highlight\">Agentic</em> \u8bbe\u8ba1\u6a21\u5f0f", "comment": "Source: WeChat, Published: 2025-10-05 22:04:22", "summary": "Agentic \u8bbe\u8ba1\u6a21\u5f0f\u4e00\u4f4d\u8c37\u6b4c\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u521a\u521a\u53d1\u5e03\u4e86\u4e00\u4efd\u957f\u8fbe 424 \u9875\u7684\u6587\u6863\uff0c\u540d\u4e3a\u300aAgentic \u8bbe\u8ba1\u6a21\u5f0f\u300b\u3002\u6bcf\u4e00\u7ae0\u90fd\u6709\u4ee3\u7801\u652f\u6301\uff0c\u6db5\u76d6\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u524d\u6cbf\uff1a\u2192 Prompt chaining\uff0c routing\uff0c memory", "AI": {"tldr": "Agentic \u8bbe\u8ba1\u6a21\u5f0f\u4e00\u4f4d\u8c37\u6b4c\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u521a\u521a\u53d1\u5e03\u4e86\u4e00\u4efd\u957f\u8fbe 424 \u9875\u7684\u6587\u6863\uff0c\u540d\u4e3a\u300aAgentic \u8bbe\u8ba1\u6a21\u5f0f\u300b\u3002\u6bcf\u4e00\u7ae0\u90fd\u6709\u4ee3\u7801\u652f\u6301\uff0c\u6db5\u76d6\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u524d\u6cbf\uff1a\u2192 Prompt chaining\uff0c routing\uff0c memory", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.b112eec5", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMTAwMzk1Mg==&mid=2651221633&idx=1&sn=53b20e0ad414d0bf29493660c7c57375&chksm=8dc5ec5d48d54d4fc76e9917938a7304dcb546a9758711816be1bf34215a7381064403f025f9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMTAwMzk1Mg==&mid=2651221633&idx=1&sn=53b20e0ad414d0bf29493660c7c57375&chksm=8dc5ec5d48d54d4fc76e9917938a7304dcb546a9758711816be1bf34215a7381064403f025f9#rd", "authors": ["\u950b\u884c\u94fe\u76df"], "title": "\u3010\u91cd\u70b9\u30112025<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6280\u672f\u767d\u76ae\u4e66|\u9644\u4e0b\u8f7d", "comment": "Source: WeChat, Published: 2025-10-06 13:46:19", "summary": "\u5927\u6a21\u578b\u9ad8\u9636\u5b9e\u65bd\u7b56\u7565\u4e0e\u8def\u5f84 04\u3002 \u5927\u6a21\u578b\u884c\u4e1a\u6848\u4f8b\u5206\u4eab\u3002\u5927\u6a21\u578b\u884c\u4e1a\u5e94\u7528\u53d1\u5c55\uff1a\u8de8\u8d8a\u62d0\u70b9\uff0c\u52a0\u901f\u8fdb\u5165\u5927\u6a21\u578b\u65f6\u4ee3\u3002ai\u5927\u6a21\u578b\u6280\u672f\u5feb\u901f\u6210\u719f\uff0cai\u7b97\u6cd5\u4e0e\u5e94\u7528\u7684\u5f00\u53d1\u3001\u4e0a\u7ebf\u90e8\u7f72\u4e0e\u4e1a\u52a1\u53d1\u653e\u7b49\u8fc7\u7a0b\u5747\u5927\u5e45\u7b80\u5316\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u9ad8\u9636\u5b9e\u65bd\u7b56\u7565\u4e0e\u8def\u5f84 04\u3002 \u5927\u6a21\u578b\u884c\u4e1a\u6848\u4f8b\u5206\u4eab\u3002\u5927\u6a21\u578b\u884c\u4e1a\u5e94\u7528\u53d1\u5c55\uff1a\u8de8\u8d8a\u62d0\u70b9\uff0c\u52a0\u901f\u8fdb\u5165\u5927\u6a21\u578b\u65f6\u4ee3\u3002ai\u5927\u6a21\u578b\u6280\u672f\u5feb\u901f\u6210\u719f\uff0cai\u7b97\u6cd5\u4e0e\u5e94\u7528\u7684\u5f00\u53d1\u3001\u4e0a\u7ebf\u90e8\u7f72\u4e0e\u4e1a\u52a1\u53d1\u653e\u7b49\u8fc7\u7a0b\u5747\u5927\u5e45\u7b80\u5316\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.543cbc56", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMjcxODI0Mg==&mid=2247511362&idx=1&sn=56e14287a5b9f6fe91765326fa2fc9e5&chksm=c3c15a798c1b0b1449989ff815183546be8a29a2885efa459c778eef947a1dbec2f5ca705228#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMjcxODI0Mg==&mid=2247511362&idx=1&sn=56e14287a5b9f6fe91765326fa2fc9e5&chksm=c3c15a798c1b0b1449989ff815183546be8a29a2885efa459c778eef947a1dbec2f5ca705228#rd", "authors": ["\u534a\u591c\u79d1\u6280\u9986"], "title": "\u817e\u8baf\u6df7\u5143\u56fe\u50cf3.0\u66b4\u51fb\u5168\u740326\u4e2a<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u78be\u538b\u8c37\u6b4cOpenAI\u767b\u9876\u6587\u751f\u56fe\u738b\u5ea7\uff0180B\u53c2\u6570\u5f00\u6e90\u70b8\u573a\uff0c\u4e2d\u79cb\u6d77\u62a5\u76f4\u63a5\u5c01\u795e\uff01", "comment": "Source: WeChat, Published: 2025-10-06 12:01:51", "summary": "\u572826\u4e2a\u5168\u7403\u9876\u7ea7\u5927\u6a21\u578b\u4e2d\u5f3a\u52bf\u767b\u9876\u7b2c\u4e00\uff01\u8fd9\u662f\u4e2d\u56fdAI\u6a21\u578b\u9996\u6b21\u593a\u5f97\u6587\u751f\u56fe\u9886\u57df\u5168\u7403\u51a0\u519b\u5b9d\u5ea7\uff0cLMArena\u5b98\u65b9\u66f4\u662f\u76f4\u63a5\u53d1\u6587\u795d\u8d3a\u8fd9\u4e00\"\u5de8\u5927\u6210\u5c31\"\u3002lmage caption model caption accepted bidirectional verification ocr 3 x ocr ip ocr world agent agent info knowledge rej", "AI": {"tldr": "\u572826\u4e2a\u5168\u7403\u9876\u7ea7\u5927\u6a21\u578b\u4e2d\u5f3a\u52bf\u767b\u9876\u7b2c\u4e00\uff01\u8fd9\u662f\u4e2d\u56fdAI\u6a21\u578b\u9996\u6b21\u593a\u5f97\u6587\u751f\u56fe\u9886\u57df\u5168\u7403\u51a0\u519b\u5b9d\u5ea7\uff0cLMArena\u5b98\u65b9\u66f4\u662f\u76f4\u63a5\u53d1\u6587\u795d\u8d3a\u8fd9\u4e00\"\u5de8\u5927\u6210\u5c31\"\u3002lmage caption model caption accepted bidirectional verification ocr 3 x ocr ip ocr world agent agent info knowledge rej", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.fe4d8ea1", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491815&idx=1&sn=99c7b5567a155ea99619ea5b56c855e5&chksm=fa00fb5aac8509dc1eaa24b2f3433b246a96ee96f0aa2f21ffb3cf9fa6c7c5e13b3af6d5743c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247491815&idx=1&sn=99c7b5567a155ea99619ea5b56c855e5&chksm=fa00fb5aac8509dc1eaa24b2f3433b246a96ee96f0aa2f21ffb3cf9fa6c7c5e13b3af6d5743c#rd", "authors": ["\u6155\u5bb9\u5343\u8bed"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9762\u8bd5\u9898\u2014\u2014Agent\u5982\u4f55\u642d\u5efa?\u53ef\u4ee5\u7528\u90a3\u4e9b\u6846\u67b6", "comment": "Source: WeChat, Published: 2025-10-06 06:33:26", "summary": "\u6838\u5fc3\u6a21\u578b\uff1a\u9009\u62e9\u57fa\u7840\u5927\u6a21\u578b\uff08\u5982GPT-4\u3001Claude\u3001LLAMA 2\uff09\u4f5c\u4e3a\u63a8\u7406\u5f15\u64ce\u89d2\u8272\u8bbe\u5b9a\uff1a\u901a\u8fc7System Prompt\u5b9a\u4e49Agent\u89d2\u8272\uff08\u5982\u201c\u4f60\u662f\u4e00\u4e2a\u8d44\u6df1\u6570\u636e\u5206\u6790\u5e08\u201d\uff09\u5fae\u8c03\u9700\u6c42\uff1a\u662f\u5426\u9700\u8981\u9886\u57df\u5fae\u8c03\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u573a\u666f\uff09", "AI": {"tldr": "\u6838\u5fc3\u6a21\u578b\uff1a\u9009\u62e9\u57fa\u7840\u5927\u6a21\u578b\uff08\u5982GPT-4\u3001Claude\u3001LLAMA 2\uff09\u4f5c\u4e3a\u63a8\u7406\u5f15\u64ce\u89d2\u8272\u8bbe\u5b9a\uff1a\u901a\u8fc7System Prompt\u5b9a\u4e49Agent\u89d2\u8272\uff08\u5982\u201c\u4f60\u662f\u4e00\u4e2a\u8d44\u6df1\u6570\u636e\u5206\u6790\u5e08\u201d\uff09\u5fae\u8c03\u9700\u6c42\uff1a\u662f\u5426\u9700\u8981\u9886\u57df\u5fae\u8c03\uff08\u5982\u533b\u7597\u3001\u91d1\u878d\u573a\u666f\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.92ec4a40", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771062&idx=4&sn=4bc51164d5f1aa7d518185eeb1bbdfb5&chksm=fab293da3a8d8c37e543897b97b00efa1a5c6f54b955c8921d4fe0b8f795a7bf277f6ec9dfbb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771062&idx=4&sn=4bc51164d5f1aa7d518185eeb1bbdfb5&chksm=fab293da3a8d8c37e543897b97b00efa1a5c6f54b955c8921d4fe0b8f795a7bf277f6ec9dfbb#rd", "authors": ["DataFunTalk"], "title": "B \u7ad9\u57fa\u4e8e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u5927\u6570\u636e\u667a\u80fd\u8bca\u65ad\u52a9\u624b\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-10-06 05:01:13", "summary": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "AI": {"tldr": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.d956ca62", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771062&idx=3&sn=a89df6be9e1facca3923e72e7893af53&chksm=fa881ec8383dc87529111acad49a966199a5e8c4cb69730b535f7a2c17a89b5ea1da24083a39#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771062&idx=3&sn=a89df6be9e1facca3923e72e7893af53&chksm=fa881ec8383dc87529111acad49a966199a5e8c4cb69730b535f7a2c17a89b5ea1da24083a39#rd", "authors": ["DataFunTalk"], "title": "\u817e\u8baf<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5728\u7f51\u5740\u5b89\u5168\u7684\u843d\u5730\u4e0e\u601d\u8003", "comment": "Source: WeChat, Published: 2025-10-06 05:01:13", "summary": "\u817e\u8baf\u5927\u6a21\u578b\u5728\u7f51\u5740\u5b89\u5168\u7684\u843d\u5730\u4e0e\u601d\u8003\u77e5\u8bc6\u56fe\u8c31 + Agent\u5728\u987a\u4e30\u7269\u6d41\u98ce\u63a7\u9886\u57df\u7684\u63a2\u7d22\u4e0e\u5b9e\u8df5\u98ce\u63a7MLOPS\u5de5\u7a0b\u5316\u80fd\u529b\u5efa\u8bbe\u5b9e\u8df52025\u5e7410\u670818\u65e5\uff0c09\uff1a00-12\uff1a30\uff0cDataFun\u5c06\u5f00\u5c55\u3010DataFunSummit2025\uff1a\u667a\u80fd\u98ce\u63a7\u5cf0\u4f1a\u3011\uff0c\u672c\u6b21\u4f1a\u8bae\u805a\u7126\u5927\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u9a71\u52a8\u7684", "AI": {"tldr": "\u817e\u8baf\u5927\u6a21\u578b\u5728\u7f51\u5740\u5b89\u5168\u7684\u843d\u5730\u4e0e\u601d\u8003\u77e5\u8bc6\u56fe\u8c31 + Agent\u5728\u987a\u4e30\u7269\u6d41\u98ce\u63a7\u9886\u57df\u7684\u63a2\u7d22\u4e0e\u5b9e\u8df5\u98ce\u63a7MLOPS\u5de5\u7a0b\u5316\u80fd\u529b\u5efa\u8bbe\u5b9e\u8df52025\u5e7410\u670818\u65e5\uff0c09\uff1a00-12\uff1a30\uff0cDataFun\u5c06\u5f00\u5c55\u3010DataFunSummit2025\uff1a\u667a\u80fd\u98ce\u63a7\u5cf0\u4f1a\u3011\uff0c\u672c\u6b21\u4f1a\u8bae\u805a\u7126\u5927\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u9a71\u52a8\u7684", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.9e2d5706", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODU5NzgwMQ==&mid=2247486582&idx=1&sn=73b8e69df3a80f2049ee2966209203c8&chksm=973811d44eecbdb499d63393d1154d0fddb7d85f6a1b1bf0431680e3517e2ab89313fdbe09ff#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODU5NzgwMQ==&mid=2247486582&idx=1&sn=73b8e69df3a80f2049ee2966209203c8&chksm=973811d44eecbdb499d63393d1154d0fddb7d85f6a1b1bf0431680e3517e2ab89313fdbe09ff#rd", "authors": ["GEIA\u5168\u7403\u5177\u8eab\u667a\u80fd\u89c2\u5bdf"], "title": "\u6536\u85cf\uff01\u5177\u8eab\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u76d8\u70b9\uff0c\u6d89\u53ca\u8c37\u6b4c\u3001Meta\u3001\u82f1\u4f1f\u8fbe\u3001\u667a\u5143\u3001\u94f6\u6cb3\u901a\u7528\u7b49", "comment": "Source: WeChat, Published: 2025-10-06 04:43:18", "summary": "\u56fd\u5185\u5916\u5177\u8eab\u667a\u80fd\u5927\u6a21\u578b\u76d8\u70b9 \u5927\u6a21\u578b \u53d1\u5e03\u3002\u56e2\u961f \u7b80\u4ecb\u3002\u8c37\u6b4c\u4e0e\u67cf\u6797\u5de5\u4e1a\u5927\u5b66\u5408\u4f5c\u5f00\u53d1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408palm\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u89c9\u6a21\u578b\u3002palm-e google & tu berlin \uff08vit\uff09\uff0c\u652f\u6301\u6307\u4ee4\u9a71\u52a8\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\uff08\u5982\u6293\u53d6\u3001\u5bfc\u822a\uff09\uff0c\u901a\u8fc7\u56fe\u6587\u8054\u5408", "AI": {"tldr": "\u56fd\u5185\u5916\u5177\u8eab\u667a\u80fd\u5927\u6a21\u578b\u76d8\u70b9 \u5927\u6a21\u578b \u53d1\u5e03\u3002\u56e2\u961f \u7b80\u4ecb\u3002\u8c37\u6b4c\u4e0e\u67cf\u6797\u5de5\u4e1a\u5927\u5b66\u5408\u4f5c\u5f00\u53d1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408palm\u8bed\u8a00\u6a21\u578b\u4e0e\u89c6\u89c9\u6a21\u578b\u3002palm-e google & tu berlin \uff08vit\uff09\uff0c\u652f\u6301\u6307\u4ee4\u9a71\u52a8\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\uff08\u5982\u6293\u53d6\u3001\u5bfc\u822a\uff09\uff0c\u901a\u8fc7\u56fe\u6587\u8054\u5408", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.33359636", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MTA0NTIwMw==&mid=2652207140&idx=1&sn=74a0fade4c6bf94bf6add402ed14a9e6&chksm=8adec86f7a2ced1e4eaa4dcd95ff8995f9e1cc6d18f139045a7db6f7967c10847cf6b4c7714c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MTA0NTIwMw==&mid=2652207140&idx=1&sn=74a0fade4c6bf94bf6add402ed14a9e6&chksm=8adec86f7a2ced1e4eaa4dcd95ff8995f9e1cc6d18f139045a7db6f7967c10847cf6b4c7714c#rd", "authors": ["\u8f6f\u4ef6\u5de5\u7a0b\u4e4b\u601d"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\u8fb9\u754c", "comment": "Source: WeChat, Published: 2025-10-06 00:15:48", "summary": "\u5927\u6a21\u578b\u5728\u9700\u8981\u6570\u5b66\u4e25\u8c28\u6027\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u8db3\uff1a\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u4f18\u5316Dijkstra \u7b97\u6cd5\u5728\u7a00\u758f\u56fe\u573a\u666f\u4e0b\uff0cLLM \u751f\u6210\u7684\u4ee3\u7801\u65f6\u95f4\u590d\u6742\u5ea6\u4f1a\u9000\u5316\u4e3a O \uff08N\uff09\uff0c\u800c\u4eba\u7c7b\u4f18\u5316\u540e\u7684\u7248\u672c\u53ef\u8fbe\u5230 O \uff08M+N log N\uff09\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u5728\u9700\u8981\u6570\u5b66\u4e25\u8c28\u6027\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u8db3\uff1a\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u4f18\u5316Dijkstra \u7b97\u6cd5\u5728\u7a00\u758f\u56fe\u573a\u666f\u4e0b\uff0cLLM \u751f\u6210\u7684\u4ee3\u7801\u65f6\u95f4\u590d\u6742\u5ea6\u4f1a\u9000\u5316\u4e3a O \uff08N\uff09\uff0c\u800c\u4eba\u7c7b\u4f18\u5316\u540e\u7684\u7248\u672c\u53ef\u8fbe\u5230 O \uff08M+N log N\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.7a845827", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5NDM4MTA3Nw==&mid=2247504818&idx=1&sn=bbb857f8e20970d304fdee3d94210ba9&chksm=ed9acaf61e2ec93324e76c09f8488a7da3f3e18ae5880a4ec4bd40eb47c4804203c2e1541655#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5NDM4MTA3Nw==&mid=2247504818&idx=1&sn=bbb857f8e20970d304fdee3d94210ba9&chksm=ed9acaf61e2ec93324e76c09f8488a7da3f3e18ae5880a4ec4bd40eb47c4804203c2e1541655#rd", "authors": ["AI\u4e91\u539f\u751f\u667a\u80fd\u7b97\u529b\u67b6\u6784"], "title": "\u7a81\u53d1\u5f00\u6e90\uff01\u8dc3\u5c45\u5168\u7403\u7b2c\u4e00\uff01\u817e\u8baf\u53d1\u5e03\u6df7\u5143\u56fe\u50cf 3.0 <em class=\"highlight\">\u5927\u6a21\u578b</em>\uff012025", "comment": "Source: WeChat, Published: 2025-10-05 23:15:40", "summary": "10 \u6708 5 \u65e5\uff0c\u817e\u8baf\u5b98\u65b9\u5ba3\u5e03\uff1a\u5176 9 \u6708 28 \u65e5\u521a\u521a\u53d1\u5e03\u7684\u6df7\u5143\u56fe\u50cf 3.0 \u5927\u6a21\u578b\uff0c\u5728\u56fd\u9645\u6743\u5a01\u8bc4\u6d4b\u5e73\u53f0 LMArena \u7684\u6587\u751f\u56fe\u699c\u5355\u4e2d\uff0c\u4ece\u5168\u7403 26 \u4e2a\u9876\u5c16\u5927\u6a21\u578b\u4e2d\u8131\u9896\u800c\u51fa\uff0c\u4ee5\u7edd\u5bf9\u4f18\u52bf\u767b\u9876\u7b2c\u4e00\u3002", "AI": {"tldr": "10 \u6708 5 \u65e5\uff0c\u817e\u8baf\u5b98\u65b9\u5ba3\u5e03\uff1a\u5176 9 \u6708 28 \u65e5\u521a\u521a\u53d1\u5e03\u7684\u6df7\u5143\u56fe\u50cf 3.0 \u5927\u6a21\u578b\uff0c\u5728\u56fd\u9645\u6743\u5a01\u8bc4\u6d4b\u5e73\u53f0 LMArena \u7684\u6587\u751f\u56fe\u699c\u5355\u4e2d\uff0c\u4ece\u5168\u7403 26 \u4e2a\u9876\u5c16\u5927\u6a21\u578b\u4e2d\u8131\u9896\u800c\u51fa\uff0c\u4ee5\u7edd\u5bf9\u4f18\u52bf\u767b\u9876\u7b2c\u4e00\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.ed1172ec", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247488933&idx=1&sn=743facaec00425eec598b64eadeb9b89&chksm=edc8d672e0720248d0e617c9edfd0178093046c21a994e6a558efe4a5b1c7d093aa73716511a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5MjQzOTY3OA==&mid=2247488933&idx=1&sn=743facaec00425eec598b64eadeb9b89&chksm=edc8d672e0720248d0e617c9edfd0178093046c21a994e6a558efe4a5b1c7d093aa73716511a#rd", "authors": ["\u7cbe\u795e\u6296\u64de\u738b\u5927\u9e4f"], "title": "12\u5929\u5e26\u4f60\u901f\u901a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u57fa\u7840\u5e94\u7528\uff08\u4e03\uff09Agent\u300c\u4e0b\u300d", "comment": "Source: WeChat, Published: 2025-10-05 15:31:38", "summary": "\u5b8c\u6574webui\uff0c\u96c6\u6210coder\u505a\u6570\u636e\u5206\u6790 15.1kopendeepresearch huggingface react\u8303\u5f0f\uff0c\u52a8\u4f5c\u5373\u4ee3\u7801 21.2klangchain\u7248\u672c langchain \u52a0\u5165\u53cd\u601d\uff08reflect\uff09\u673a\u5236 4.3kDeepResearchAgentSkyworkAI\u4f7f\u7528browser-use\u81ea\u52a8\u5316", "AI": {"tldr": "\u5b8c\u6574webui\uff0c\u96c6\u6210coder\u505a\u6570\u636e\u5206\u6790 15.1kopendeepresearch huggingface react\u8303\u5f0f\uff0c\u52a8\u4f5c\u5373\u4ee3\u7801 21.2klangchain\u7248\u672c langchain \u52a0\u5165\u53cd\u601d\uff08reflect\uff09\u673a\u5236 4.3kDeepResearchAgentSkyworkAI\u4f7f\u7528browser-use\u81ea\u52a8\u5316", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.bc5226fe", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1NTk4MjcwNg==&mid=2247484331&idx=1&sn=068393082a705b7edf21cec81d3a1503&chksm=eb2d850dabe1d8eb3b4e9ec304dd11b48f2ae1b5bdf1d818f649de8cca32e1b442544c751236#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1NTk4MjcwNg==&mid=2247484331&idx=1&sn=068393082a705b7edf21cec81d3a1503&chksm=eb2d850dabe1d8eb3b4e9ec304dd11b48f2ae1b5bdf1d818f649de8cca32e1b442544c751236#rd", "authors": ["\u5f20\u8010\u5fc3"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u517b\u6210\u8bb0\uff08\u4e00\uff09\uff1aTransformer\uff0c\u90a3\u4e2a\u5f00\u542f\u4e86\u795e\u7ae5\u65f6\u4ee3\u7684\u5929\u624d\u5b9d\u5b9d", "comment": "Source: WeChat, Published: 2025-10-05 14:34:28", "summary": "\u6240\u4ee5\uff0c\u6211\u51b3\u5b9a\u5f00\u542f \u300aAI\u5927\u6a21\u578b\u517b\u6210\u8bb0\u300b \u8fd9\u4e2a\u7cfb\u5217\u3002\u5728\u8fd9\u91cc\uff0c\u6ca1\u6709\u590d\u6742\u7684\u516c\u5f0f\u548c\u4ee3\u7801\uff0c\u6211\u53ea\u60f3\u7528\u6700\u901a\u4fd7\u6613\u61c2\u7684\u8bed\u8a00\uff0c\u4ee5\u4e00\u4e2a\u201cAI\u5b9d\u5b9d\u201d\u7684\u6210\u957f\u89c6\u89d2\uff0c\u5e26\u4f60\u770b\u770b\u6211\u4eec\u4eca\u5929\u6240\u719f\u77e5\u7684\u5f3a\u5927AI\uff0c\u662f\u5982\u4f55\u4ece\u4e00\u4e2a\u201c\u5624\u5624\u5b66\u8bed\u201d\u7684\u5a74\u513f\uff0c\u4e00\u6b65\u6b65\u6210", "AI": {"tldr": "\u6240\u4ee5\uff0c\u6211\u51b3\u5b9a\u5f00\u542f \u300aAI\u5927\u6a21\u578b\u517b\u6210\u8bb0\u300b \u8fd9\u4e2a\u7cfb\u5217\u3002\u5728\u8fd9\u91cc\uff0c\u6ca1\u6709\u590d\u6742\u7684\u516c\u5f0f\u548c\u4ee3\u7801\uff0c\u6211\u53ea\u60f3\u7528\u6700\u901a\u4fd7\u6613\u61c2\u7684\u8bed\u8a00\uff0c\u4ee5\u4e00\u4e2a\u201cAI\u5b9d\u5b9d\u201d\u7684\u6210\u957f\u89c6\u89d2\uff0c\u5e26\u4f60\u770b\u770b\u6211\u4eec\u4eca\u5929\u6240\u719f\u77e5\u7684\u5f3a\u5927AI\uff0c\u662f\u5982\u4f55\u4ece\u4e00\u4e2a\u201c\u5624\u5624\u5b66\u8bed\u201d\u7684\u5a74\u513f\uff0c\u4e00\u6b65\u6b65\u6210", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
