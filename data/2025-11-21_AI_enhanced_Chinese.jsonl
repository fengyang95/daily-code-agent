{"id": "2511.15738", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15738", "abs": "https://arxiv.org/abs/2511.15738", "authors": ["Chao Yu", "Qixin Tan", "Jiaxuan Gao", "Shi Yu", "Hong Lu", "Xinting Yang", "Zelai Xu", "Yu Wang", "Yi Wu", "Eugene Vinitsky"], "title": "Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn", "comment": "44 pages, 12 figures", "summary": "Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e863D\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u6279\u5904\u7406\u548c\u8fed\u4ee3\u8f6e\u6b21\u4e09\u4e2a\u7ef4\u5ea6\u7684\u6269\u5c55\u6765\u589e\u5f3a\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u53d7\u9650\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6709\u9650\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u8fdc\u5c0f\u4e8e\u8bad\u7ec3\u65f6\u6d88\u8017\u7684token\u6570\u91cf\uff0c\u9700\u8981\u63a2\u7d22\u591a\u7ef4\u5ea6\u6269\u5c55\u65b9\u6cd5\u6765\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa3D\u6d4b\u8bd5\u65f6\u6269\u5c55\u6846\u67b6\uff0c\u6574\u5408\u4e0a\u4e0b\u6587\u957f\u5ea6\u6269\u5c55\u3001\u6279\u5904\u7406\u6269\u5c55\u548c\u8fed\u4ee3\u8f6e\u6b21\u6269\u5c55\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u901a\u8fc7\u5e76\u884c\u91c7\u6837\u548c\u8fed\u4ee3\u81ea\u6211\u4f18\u5316\u6765\u589e\u5f3a\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u5c55\u73b0\u51fa\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u5e94\u4f46\u5bb9\u91cf\u6709\u9650\uff1b\u4e09\u4e2a\u7ef4\u5ea6\u7ed3\u5408\u663e\u8457\u63d0\u5347IOI\u3001IMO\u548cCPHO\u7b49\u6311\u6218\u6027\u6d4b\u8bd5\u96c6\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u80fd\u4ece\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u4e2d\u83b7\u76ca\uff1b\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u5177\u8eab\u5b66\u4e60\u9886\u57df\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u6d4b\u8bd5\u65f6\u6269\u5c55\u662f\u7a81\u7834\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6d4b\u8bd5\u65f6\u9650\u5236\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e09\u4e2a\u7ef4\u5ea6\u7684\u534f\u540c\u4f5c\u7528\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5e76\u652f\u6301\u66f4\u5f00\u653e\u9886\u57df\u7684\u5e94\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15715", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15715", "abs": "https://arxiv.org/abs/2511.15715", "authors": ["Yash Raj Singh"], "title": "Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems", "comment": "5 Pages, 2 tables", "summary": "Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.\n  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.\n  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.\n  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.", "AI": {"tldr": "\u63d0\u51faGraph-Memoized Reasoning\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u8bb0\u5fc6\u8868\u793a\u3001\u5b58\u50a8\u548c\u91cd\u7528\u63a8\u7406\u5de5\u4f5c\u6d41\uff0c\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3LLM\u63a8\u7406\u7cfb\u7edf\u7ecf\u5e38\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u91cd\u590d\u8ba1\u7b97\u76f8\u4f3c\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\uff0c\u9650\u5236\u53ef\u91cd\u73b0\u6027\u3002\u9700\u8981\u6301\u4e45\u5316\u63a8\u7406\u673a\u5236\u6765\u56de\u5fc6\u548c\u91cd\u7528\u5148\u524d\u7684\u8ba1\u7b97\u8f68\u8ff9\u3002", "method": "\u5f15\u5165\u56fe\u8bb0\u5fc6\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5de5\u4f5c\u6d41\u8868\u793a\u4e3a\u56fe\u7ed3\u6784\u8bb0\u5fc6\uff0c\u901a\u8fc7\u7ed3\u6784\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u7d22\u8fc7\u53bb\u7684\u51b3\u7b56\u56fe\uff0c\u5b9e\u73b0\u8de8\u65b0\u63a8\u7406\u4efb\u52a1\u7684\u5b50\u56fe\u7ec4\u5408\u91cd\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u6700\u5c0f\u5316\u603b\u63a8\u7406\u6210\u672c\u5e76\u6b63\u5219\u5316\u5b58\u50a8\u4e0e\u751f\u6210\u5de5\u4f5c\u6d41\u4e0d\u4e00\u81f4\u6027\u7684\u4f18\u5316\u76ee\u6807\uff0c\u4e3a\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u6548\u7387-\u4e00\u81f4\u6027\u6743\u8861\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u89e3\u91ca\u3001\u6210\u672c\u9ad8\u6548\u548c\u81ea\u6211\u6539\u8fdb\u7684\u63a8\u7406\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5411\u5927\u89c4\u6a21\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u6301\u4e45\u5185\u5b58\u8fc8\u8fdb\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2511.15817", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15817", "abs": "https://arxiv.org/abs/2511.15817", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in \\llm-Generated Code", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u6d4b\u91cf\u3001\u89e3\u91ca\u548c\u7f13\u89e3LLM\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u4ee3\u7801\u5f02\u5473\u503e\u5411\uff0c\u63d0\u51faPSC\u6982\u7387\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u751f\u6210\u7b56\u7565\u3001\u6a21\u578b\u5927\u5c0f\u3001\u67b6\u6784\u548c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u4ee3\u7801\u7ed3\u6784\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u7ed3\u6784\u8d28\u91cf\u95ee\u9898\uff0c\u7ecf\u5e38\u590d\u5236\u4e0d\u826f\u7f16\u7801\u5b9e\u8df5\uff0c\u5f15\u5165\u4ee3\u7801\u5f02\u5473\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u95ee\u9898\u7684\u7cfb\u7edf\u7406\u89e3\u3002", "method": "\u57fa\u4e8ePSC\u6982\u7387\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u5206\u6790\u7814\u7a76\u751f\u6210\u7b56\u7565\u3001\u6a21\u578b\u5927\u5c0f\u3001\u67b6\u6784\u548c\u63d0\u793a\u8bbe\u8ba1\u5bf9\u4ee3\u7801\u7ed3\u6784\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u63d0\u793a\u8bbe\u8ba1\u548c\u67b6\u6784\u9009\u62e9\u5bf9\u4ee3\u7801\u5f02\u5473\u503e\u5411\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u5f02\u5473\u53d1\u751f\u3002\u7528\u6237\u7814\u7a76\u8868\u660ePSC\u80fd\u5e2e\u52a9\u5f00\u53d1\u8005\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u548c\u8bc4\u4f30\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "\u4e3a\u5c06\u8d28\u91cf\u611f\u77e5\u8bc4\u4f30\u96c6\u6210\u5230LLM\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u548c\u90e8\u7f72\u4e2d\u5960\u5b9a\u4e86\u57fa\u7840\uff0cPSC\u4fe1\u53f7\u53ef\u4ee5\u652f\u6301\u4eba\u7c7b\u5224\u65ad\u3002", "topic": "swe application"}}
{"id": "2511.15716", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15716", "abs": "https://arxiv.org/abs/2511.15716", "authors": ["Abraham Itzhak Weinberg"], "title": "MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding", "comment": null, "summary": "As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86MACIE\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3001\u5e72\u9884\u53cd\u4e8b\u5b9e\u548cShapley\u503c\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u5168\u9762\u89e3\u91ca\uff0c\u89e3\u51b3\u4e2a\u4f53\u8d21\u732e\u5f52\u56e0\u3001\u6d8c\u73b0\u884c\u4e3a\u91cf\u5316\u548c\u590d\u6742\u4ea4\u4e92\u7406\u89e3\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u7406\u89e3\u667a\u80fd\u4f53\u51b3\u7b56\u539f\u56e0\u548c\u96c6\u4f53\u884c\u4e3a\u5b9e\u73b0\u65b9\u5f0f\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u53ef\u89e3\u91caAI\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "MACIE\u6846\u67b6\u7ed3\u5408\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3001\u5e72\u9884\u53cd\u4e8b\u5b9e\u548cShapley\u503c\uff0c\u901a\u8fc7\u5e72\u9884\u5f52\u56e0\u5206\u6570\u8bc4\u4f30\u4e2a\u4f53\u56e0\u679c\u8d21\u732e\uff0c\u4f7f\u7528\u534f\u540c\u6307\u6807\u5206\u79bb\u96c6\u4f53\u6548\u5e94\u4e0e\u4e2a\u4f53\u8d21\u732e\uff0c\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u53d9\u8ff0\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89e3\u91ca\u3002", "result": "\u5728\u56db\u79cdMARL\u573a\u666f\u4e2d\u6d4b\u8bd5\u663e\u793a\uff1a\u51c6\u786e\u7684\u7ed3\u679c\u5f52\u56e0\uff08\u5e73\u5747\u03c6_i=5.07\uff0c\u6807\u51c6\u5dee<0.05\uff09\uff0c\u5728\u5408\u4f5c\u4efb\u52a1\u4e2d\u68c0\u6d4b\u5230\u6b63\u5411\u6d8c\u73b0\uff08\u534f\u540c\u6307\u6570\u9ad8\u8fbe0.461\uff09\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff08CPU\u4e0a\u6bcf\u4e2a\u6570\u636e\u96c60.79\u79d2\uff09\u3002", "conclusion": "MACIE\u72ec\u7279\u5730\u7ed3\u5408\u4e86\u56e0\u679c\u4e25\u8c28\u6027\u3001\u6d8c\u73b0\u91cf\u5316\u548c\u591a\u667a\u80fd\u4f53\u652f\u6301\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u4f7f\u7528\u7684\u5b9e\u7528\u6027\uff0c\u4ee3\u8868\u4e86\u5411\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u8d56\u548c\u53ef\u95ee\u8d23\u7684\u591a\u667a\u80fd\u4f53AI\u8fc8\u51fa\u7684\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2511.15718", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15718", "abs": "https://arxiv.org/abs/2511.15718", "authors": ["Chen Yang", "Ran Le", "Yun Xing", "Zhenwei An", "Zongchao Chen", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "title": "ToolMind Technical Report: A Large-Scale, Reasoning-Enhanced Tool-Use Dataset", "comment": "15 pages", "summary": "Large Language Model (LLM) agents have developed rapidly in recent years to solve complex real-world problems using external tools. However, the scarcity of high-quality trajectories still hinders the development of stronger LLM agents. Most existing works on multi-turn dialogue synthesis validate correctness only at the trajectory level, which may overlook turn-level errors that can propagate during training and degrade model performance. To address these limitations, we introduce ToolMind, a large-scale, high-quality tool-agentic dataset with 160k synthetic data instances generated using over 20k tools and 200k augmented open-source data instances. Our data synthesis pipeline first constructs a function graph based on parameter correlations and then uses a multi-agent framework to simulate realistic user-assistant-tool interactions. Beyond trajectory-level validation, we employ fine-grained turn-level filtering to remove erroneous or suboptimal steps, ensuring that only high-quality reasoning traces are retained. This approach mitigates error amplification during training while preserving self-corrective reasoning signals essential for robust tool-use learning. Models fine-tuned on ToolMind show significant improvements over baselines on several benchmarks.", "AI": {"tldr": "ToolMind\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684LLM\u5de5\u5177\u4ee3\u7406\u6570\u636e\u96c6\uff0c\u5305\u542b16\u4e07\u5408\u6210\u6570\u636e\u548c20\u4e07\u589e\u5f3a\u5f00\u6e90\u6570\u636e\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u751f\u6210\uff0c\u5e76\u91c7\u7528\u7ec6\u7c92\u5ea6\u7684\u8f6e\u6b21\u7ea7\u8fc7\u6ee4\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4ee3\u7406\u6570\u636e\u7a00\u7f3a\u4e14\u9a8c\u8bc1\u4e0d\u591f\u7cbe\u7ec6\uff0c\u5927\u591a\u53ea\u5728\u8f68\u8ff9\u7ea7\u522b\u9a8c\u8bc1\uff0c\u5ffd\u7565\u4e86\u8f6e\u6b21\u7ea7\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u7684\u8bad\u7ec3\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u76f8\u5173\u6027\u6784\u5efa\u51fd\u6570\u56fe\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df\u7528\u6237-\u52a9\u624b-\u5de5\u5177\u4ea4\u4e92\uff0c\u5e76\u91c7\u7528\u8f6e\u6b21\u7ea7\u8fc7\u6ee4\u79fb\u9664\u9519\u8bef\u6216\u6b21\u4f18\u6b65\u9aa4\u3002", "result": "\u5728ToolMind\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ToolMind\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\u548c\u7cbe\u7ec6\u9a8c\u8bc1\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2511.16004", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16004", "abs": "https://arxiv.org/abs/2511.16004", "authors": ["KeFan Li", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution", "comment": null, "summary": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.", "AI": {"tldr": "InfCode\u662f\u4e00\u4e2a\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4ed3\u5e93\u7ea7\u95ee\u9898\u89e3\u51b3\uff0c\u901a\u8fc7\u6d4b\u8bd5\u751f\u6210\u5668\u548c\u4ee3\u7801\u8865\u4e01\u751f\u6210\u5668\u7684\u5bf9\u6297\u6027\u4ea4\u4e92\u8fed\u4ee3\u4f18\u5316\u6d4b\u8bd5\u548c\u8865\u4e01\uff0c\u5728SWE-bench Verified\u4e0a\u8fbe\u523079.4%\u7684\u6027\u80fd\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u667a\u80fd\u4f53\u548c\u6d41\u6c34\u7ebf\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u5145\u5206\u7684\u6d4b\u8bd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u8865\u4e01\u901a\u8fc7\u9a8c\u8bc1\u4f46\u672a\u80fd\u4fee\u590d\u6839\u672c\u7f3a\u9677\uff0c\u9700\u8981\u4ed3\u5e93\u7ea7\u63a8\u7406\u3001\u51c6\u786e\u8bca\u65ad\u548c\u5f3a\u9a8c\u8bc1\u4fe1\u53f7\u6765\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u6d4b\u8bd5\u8865\u4e01\u751f\u6210\u5668\u3001\u4ee3\u7801\u8865\u4e01\u751f\u6210\u5668\u548c\u9009\u62e9\u5668\u667a\u80fd\u4f53\uff0c\u5728\u5bb9\u5668\u5316\u73af\u5883\u4e2d\u8fed\u4ee3\u4f18\u5316\u6d4b\u8bd5\u548c\u8865\u4e01\uff0c\u652f\u6301\u771f\u5b9e\u7684\u4ed3\u5e93\u68c0\u67e5\u3001\u4fee\u6539\u548c\u9a8c\u8bc1\u3002", "result": "\u5728SWE-bench Lite\u548cSWE-bench Verified\u4e0a\u4f7f\u7528DeepSeek-V3\u548cClaude 4.5 Sonnet\u7b49\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0cInfCode\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728SWE-bench Verified\u4e0a\u8fbe\u523079.4%\u7684\u6027\u80fd\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "InfCode\u901a\u8fc7\u5bf9\u6297\u6027\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ed3\u5e93\u7ea7\u8f6f\u4ef6\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5df2\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u53d1\u5e03\u3002", "topic": "swe application"}}
{"id": "2511.16005", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16005", "abs": "https://arxiv.org/abs/2511.16005", "authors": ["Qingao Dong", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution", "comment": null, "summary": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.", "AI": {"tldr": "INFCODE-C++\u662f\u9996\u4e2a\u4e13\u4e3aC++\u8bbe\u8ba1\u7684\u81ea\u4e3b\u95ee\u9898\u89e3\u51b3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u4ee3\u7801\u610f\u56fe\u68c0\u7d22\u548c\u786e\u5b9a\u6027AST\u7ed3\u6784\u5316\u67e5\u8be2\u76f8\u7ed3\u5408\uff0c\u5728MultiSWE-bench-CPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523025.58%\u7684\u89e3\u51b3\u7387\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u4ee3\u7406\u63d0\u534710.85\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u4e3b\u8981\u9488\u5bf9Python\u8bbe\u8ba1\uff0c\u5728C++\u9879\u76ee\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3aC++\u7684\u91cd\u8f7d\u6807\u8bc6\u7b26\u3001\u5d4c\u5957\u547d\u540d\u7a7a\u95f4\u3001\u6a21\u677f\u5b9e\u4f8b\u5316\u548c\u6df1\u5c42\u63a7\u5236\u6d41\u7ed3\u6784\u4f7f\u5f97\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u6545\u969c\u5b9a\u4f4d\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u7684\u68c0\u7d22\u673a\u5236\uff1a\u8bed\u4e49\u4ee3\u7801\u610f\u56fe\u68c0\u7d22\u548c\u786e\u5b9a\u6027AST\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u6784\u5efa\u51c6\u786e\u7684\u8bed\u8a00\u611f\u77e5\u4e0a\u4e0b\u6587\u8fdb\u884c\u4fee\u590d\u3002", "result": "\u5728MultiSWE-bench-CPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523025.58%\u7684\u89e3\u51b3\u7387\uff0c\u6bd4\u6700\u5f3a\u73b0\u6709\u4ee3\u7406\u63d0\u534710.85\u4e2a\u767e\u5206\u70b9\uff0c\u662fMSWE-agent\u6027\u80fd\u7684\u4e24\u500d\u591a\u3002", "conclusion": "INFCODE-C++\u5f3a\u8c03\u4e86\u591a\u8bed\u8a00\u8f6f\u4ef6\u4ee3\u7406\u4e2d\u8bed\u8a00\u611f\u77e5\u63a8\u7406\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u590d\u6742\u9759\u6001\u7c7b\u578b\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u6269\u5c55LLM\u9a71\u52a8\u4fee\u590d\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2511.16092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16092", "abs": "https://arxiv.org/abs/2511.16092", "authors": ["Xing Hu", "Raula Gaikovina Kula", "Christoph Treude"], "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report", "AI": {"tldr": "33\u4f4d\u4e13\u5bb6\u5728Shonan Meeting 222\u4e0a\u8ba8\u8bba\u4e86\u751f\u6210\u5f0fAI\u5bf9IDE\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4e86\u6311\u6218\u548c\u673a\u9047", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u5728\u4ee3\u7801\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u4ee3\u7801\u5ba1\u67e5\u548c\u7a0b\u5e8f\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u8868\u73b0\u5982\u4f55\u6539\u53d8IDE\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92", "method": "\u901a\u8fc7\u8de8\u9886\u57df\u4e13\u5bb6\u4f1a\u8bae\u8ba8\u8bba\uff0c\u6c47\u96c6\u8f6f\u4ef6\u5de5\u7a0b\u3001\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u768433\u4f4d\u4e13\u5bb6", "result": "\u8bc6\u522b\u4e86\u751f\u6210\u5f0fAI\u5bf9IDE\u5e26\u6765\u7684\u6311\u6218\u548c\u673a\u9047\uff0c\u5e76\u8ba8\u8bba\u4e86\u63d0\u9ad8\u62bd\u8c61\u7ea7\u522b\u5982\u4f55\u6539\u53d8\u4eba\u673a\u4ea4\u4e92", "conclusion": "\u751f\u6210\u5f0fAI\u6709\u6f5c\u529b\u901a\u8fc7\u63d0\u9ad8\u62bd\u8c61\u7ea7\u522b\u6765\u6539\u53d8IDE\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f", "topic": "swe application"}}
{"id": "2511.15915", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15915", "abs": "https://arxiv.org/abs/2511.15915", "authors": ["Genghan Zhang", "Shaowei Zhu", "Anjiang Wei", "Zhenyu Song", "Allen Nie", "Zhen Jia", "Nandita Vijaykumar", "Yida Wang", "Kunle Olukotun"], "title": "AccelOpt: A Self-Improving LLM Agentic System for AI Accelerator Kernel Optimization", "comment": null, "summary": "We present AccelOpt, a self-improving large language model (LLM) agentic system that autonomously optimizes kernels for emerging AI acclerators, eliminating the need for expert-provided hardware-specific optimization knowledge. AccelOpt explores the kernel optimization space through iterative generation, informed by an optimization memory that curates experiences and insights from previously encountered slow-fast kernel pairs. We build NKIBench, a new benchmark suite of AWS Trainium accelerator kernels with varying complexity extracted from real-world LLM workloads to evaluate the effectiveness of AccelOpt. Our evaluation confirms that AccelOpt's capability improves over time, boosting the average percentage of peak throughput from $49\\%$ to $61\\%$ on Trainium 1 and from $45\\%$ to $59\\%$ on Trainium 2 for NKIBench kernels. Moreover, AccelOpt is highly cost-effective: using open-source models, it matches the kernel improvements of Claude Sonnet 4 while being $26\\times$ cheaper.", "AI": {"tldr": "AccelOpt\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u7684LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u4e3b\u4f18\u5316\u65b0\u5174AI\u52a0\u901f\u5668\u7684\u5185\u6838\uff0c\u65e0\u9700\u4e13\u5bb6\u63d0\u4f9b\u7684\u786c\u4ef6\u7279\u5b9a\u4f18\u5316\u77e5\u8bc6\u3002\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u8bb0\u5fc6\u673a\u5236\uff0c\u5728NKIBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u6027\u80fd\u3002", "motivation": "\u6d88\u9664\u5bf9\u4e13\u5bb6\u786c\u4ef6\u4f18\u5316\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u4e3a\u65b0\u5174AI\u52a0\u901f\u5668\u63d0\u4f9b\u81ea\u52a8\u5316\u7684\u5185\u6838\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4f18\u5316\u6210\u672c\u3002", "method": "\u4f7f\u7528\u81ea\u6539\u8fdb\u7684LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u63a2\u7d22\u5185\u6838\u4f18\u5316\u7a7a\u95f4\uff0c\u5229\u7528\u4f18\u5316\u8bb0\u5fc6\u673a\u5236\u79ef\u7d2f\u5148\u524d\u9047\u5230\u7684\u6162-\u5feb\u5185\u6838\u5bf9\u7ecf\u9a8c\u3002", "result": "\u5728Trainium 1\u4e0a\u5e73\u5747\u5cf0\u503c\u541e\u5410\u91cf\u4ece49%\u63d0\u5347\u523061%\uff0c\u5728Trainium 2\u4e0a\u4ece45%\u63d0\u5347\u523059%\u3002\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u65f6\uff0c\u6027\u80fd\u4e0eClaude Sonnet 4\u76f8\u5f53\u4f46\u6210\u672c\u964d\u4f4e26\u500d\u3002", "conclusion": "AccelOpt\u8bc1\u660e\u4e86LLM\u4ee3\u7406\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u81ea\u4e3b\u4f18\u5316AI\u52a0\u901f\u5668\u5185\u6838\uff0c\u5177\u6709\u6301\u7eed\u6539\u8fdb\u80fd\u529b\u548c\u663e\u8457\u7684\u6210\u672c\u6548\u76ca\u3002", "topic": "code agent"}}
{"id": "2511.16224", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16224", "abs": "https://arxiv.org/abs/2511.16224", "authors": ["Francesco Salzano", "Simone Scalabrino", "Rocco Oliveto", "Simone Scalabrino"], "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts", "comment": "20 pages", "summary": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.", "AI": {"tldr": "\u672c\u6587\u5bf9LLM\u751f\u6210\u7684Solidity\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u9ad8\u7684\u540c\u65f6\uff0c\u529f\u80fd\u6b63\u786e\u6027\u8f83\u4f4e\uff08\u4ec520-26%\uff09\uff0c\u751f\u6210\u7684\u4ee3\u7801\u66f4\u7b80\u5355\u4f46\u7f3a\u4e4f\u9a8c\u8bc1\u903b\u8f91\uff0c\u800c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u5728\u533a\u5757\u94fe\u751f\u6001\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLM\u5728\u751f\u6210Solidity\u4ee3\u7801\u65f6\u9762\u4e34gas\u6d88\u8017\u3001\u5b89\u5168\u6027\u548c\u786e\u5b9a\u6027\u7b49\u72ec\u7279\u7ea6\u675f\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u5173\u952e\u529f\u80fd\u548c\u975e\u529f\u80fd\u5c5e\u6027\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u5bf94\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4ee3\u7801\u76f8\u4f3c\u5ea6\u6307\u6807\u3001\u8bed\u4e49\u5d4c\u5165\u3001\u81ea\u52a8\u5316\u6d4b\u8bd5\u6267\u884c\u3001gas\u5206\u6790\u4ee5\u53ca\u8ba4\u77e5\u548c\u5708\u590d\u6742\u5ea6\u5206\u6790\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "LLM\u751f\u6210\u7684\u4ee3\u7801\u4e0e\u771f\u5b9e\u5408\u7ea6\u5177\u6709\u9ad8\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u4f46\u529f\u80fd\u6b63\u786e\u6027\u4f4e\uff08\u4ec520-26%\uff09\uff0c\u751f\u6210\u7684\u4ee3\u7801\u66f4\u7b80\u5355\u3001\u590d\u6742\u5ea6\u66f4\u4f4e\u3001gas\u6d88\u8017\u66f4\u5c11\uff0c\u4f46\u5f80\u5f80\u7701\u7565\u4e86\u9a8c\u8bc1\u903b\u8f91\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5c06\u529f\u80fd\u6b63\u786e\u6027\u63d0\u5347\u9ad8\u8fbe45%\u3002", "conclusion": "LLM\u751f\u6210\u7684\u667a\u80fd\u5408\u7ea6\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u529f\u80fd\u5408\u7406\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0cRAG\u662f\u5f3a\u5927\u7684\u589e\u5f3a\u5668\uff0c\u4f46\u8981\u5b9e\u73b0\u7a33\u5065\u7684\u751f\u4ea7\u5c31\u7eea\u4ee3\u7801\u751f\u6210\u4ecd\u9700\u4e13\u5bb6\u4ed4\u7ec6\u9a8c\u8bc1\u3002", "topic": "swe application"}}
{"id": "2511.15752", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.15752", "abs": "https://arxiv.org/abs/2511.15752", "authors": ["Hanzhi Yan", "Qin Lu", "Xianqiao Wang", "Xiaoming Zhai", "Tianming Liu", "He Li"], "title": "Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics", "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u7ed3\u5408LLM\u548cAI\u4ee3\u7406\u5f00\u53d1\u751f\u7269\u529b\u5b66\u6559\u80b2\u52a9\u624b\uff0c\u901a\u8fc7RAG\u63d0\u5347\u6982\u5ff5\u95ee\u9898\u56de\u7b54\u6027\u80fd\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u7cfb\u7edf\u89e3\u51b3\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u8ba1\u7b97\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u77e5\u8bc6\u9e3f\u6c9f\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u590d\u6742\u95ee\u9898\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u6311\u6218\uff0c\u65e8\u5728\u63d0\u5347\u672c\u79d1\u751f\u5728\u751f\u7269\u529b\u5b66\u8bfe\u7a0b\u4e2d\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u6784\u5efa\u53cc\u6a21\u5757\u6846\u67b6\uff1a1) \u5e94\u7528RAG\u63d0\u5347LLM\u5bf9\u6982\u5ff5\u6027\u771f/\u5047\u95ee\u9898\u7684\u56de\u7b54\u7279\u5f02\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\uff1b2) \u6784\u5efa\u591a\u4ee3\u7406\u7cfb\u7edf\u89e3\u51b3\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u4ee3\u7801\u6267\u884c\u7684\u8ba1\u7b97\u5bfc\u5411\u95ee\u9898\u3002", "result": "RAG\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u6982\u5ff5\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u8d85\u8d8a\u539f\u59cb\u6a21\u578b\uff1b\u591a\u4ee3\u7406\u7cfb\u7edf\u80fd\u591f\u6267\u884c\u591a\u6b65\u63a8\u7406\u3001\u63a8\u5bfc\u65b9\u7a0b\u3001\u6267\u884c\u4ee3\u7801\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "RAG\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u589e\u5f3aLLM\u5728\u5de5\u7a0b\u4e13\u4e1a\u8bfe\u7a0b\u4e2d\u7684\u6027\u80fd\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u5de5\u7a0b\u6559\u80b2\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2511.15755", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15755", "abs": "https://arxiv.org/abs/2511.15755", "authors": ["Philip Drammeh"], "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response", "comment": "8 pages, 4 tables", "summary": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7f16\u6392\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u4e8b\u4ef6\u54cd\u5e94\u4e2d\u5b9e\u73b0\u4e86100%\u53ef\u64cd\u4f5c\u5efa\u8bae\u7387\uff0c\u8d28\u91cf\u65b9\u5dee\u4e3a\u96f6\uff0c\u662fLLM\u4e8b\u4ef6\u54cd\u5e94\u751f\u4ea7\u5c31\u7eea\u7684\u5fc5\u8981\u6761\u4ef6", "motivation": "\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u751f\u6210\u6a21\u7cca\u3001\u4e0d\u53ef\u7528\u7684\u5efa\u8bae\uff0c\u65e0\u6cd5\u6ee1\u8db3\u751f\u4ea7\u7cfb\u7edf\u4e8b\u4ef6\u54cd\u5e94\u7684\u9700\u6c42", "method": "\u5f00\u53d1MyAntFarm.ai\u5bb9\u5668\u5316\u6846\u67b6\uff0c\u901a\u8fc7348\u6b21\u5bf9\u7167\u8bd5\u9a8c\u6bd4\u8f83\u5355\u667a\u80fd\u4f53\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u76f8\u540c\u4e8b\u4ef6\u573a\u666f\u4e0b\u7684\u8868\u73b0", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b9e\u73b0100%\u53ef\u64cd\u4f5c\u5efa\u8bae\u7387\uff0c\u800c\u5355\u667a\u80fd\u4f53\u4ec5\u4e3a1.7%\uff1b\u5728\u884c\u52a8\u7279\u5f02\u6027\u4e0a\u63d0\u534780\u500d\uff0c\u89e3\u51b3\u65b9\u6848\u6b63\u786e\u6027\u63d0\u5347140\u500d\uff1b\u8d28\u91cf\u65b9\u5dee\u4e3a\u96f6", "conclusion": "\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4e0d\u662f\u6027\u80fd\u4f18\u5316\uff0c\u800c\u662f\u57fa\u4e8eLLM\u7684\u4e8b\u4ef6\u54cd\u5e94\u5b9e\u73b0\u751f\u4ea7\u5c31\u7eea\u7684\u5fc5\u8981\u6761\u4ef6", "topic": "agent analysis"}}
{"id": "2511.16331", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16331", "abs": "https://arxiv.org/abs/2511.16331", "authors": ["Jiashu Yao", "Heyan Huang", "Shuang Zeng", "Chuwei Luo", "WangJie You", "Jie Tang", "Qingsong Liu", "Yuhang Guo", "Yangyang Kang"], "title": "Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement", "comment": "Accepted to AAAI 2026", "summary": "Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only \"simple\" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6211\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u91cd\u5199\u81ea\u8eab\u63a8\u7406\u6587\u672c\u5e76\u4ece\u4e2d\u5b66\u4e60\uff0c\u6539\u5584\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u5728\u4fdd\u6301GRPO\u5956\u52b1\u4fe1\u53f7\u7684\u540c\u65f6\u63d0\u5347\u51c6\u786e\u7387\u5e76\u663e\u8457\u7f29\u77ed\u63a8\u7406\u957f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4ec5\u5173\u6ce8\u6700\u7ec8\u6b63\u786e\u6027\u7684\u5956\u52b1\u65e0\u6cd5\u63d0\u4f9b\u5bf9\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u8be6\u7ec6\u76d1\u7763\uff0c\u5bfc\u81f4\u63a8\u7406\u8d28\u91cf\u4e0d\u4f73\uff0c\u51fa\u73b0\u8fc7\u5ea6\u601d\u8003\u3001\u601d\u8003\u4e0d\u8db3\u3001\u5197\u4f59\u601d\u8003\u548c\u601d\u7ef4\u6df7\u4e71\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u91cd\u5199\u65b9\u6cd5\uff0c\u4ec5\u5bf9\u6a21\u578b\u4e00\u81f4\u6b63\u786e\u7684\"\u7b80\u5355\"\u6837\u672c\u8fdb\u884c\u91cd\u5199\uff0c\u5c06\u91cd\u5199\u548c\u539f\u59cb\u751f\u6210\u7f16\u8bd1\u5728\u5355\u4e2a\u6279\u6b21\u4e2d\uff0c\u4fdd\u6301\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u51c6\u786e\u7387-\u957f\u5ea6\u6743\u8861\u65b9\u9762\uff0c\u81ea\u6211\u91cd\u5199\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u63d0\u53470.6%\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u63a8\u7406\u957f\u5ea646%\uff1b\u5728\u5185\u90e8\u63a8\u7406\u8d28\u91cf\u65b9\u9762\uff0cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u5206\u663e\u8457\u63d0\u9ad87.2\u5206\u3002", "conclusion": "\u81ea\u6211\u91cd\u5199\u6846\u67b6\u6709\u6548\u6539\u5584\u4e86\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u5185\u90e8\u63a8\u7406\u7f3a\u9677\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16383", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16383", "abs": "https://arxiv.org/abs/2511.16383", "authors": ["Alexander Zadorojniy", "Segev Wasserkrug", "Eitan Farchi"], "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "comment": null, "summary": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u4f18\u5316\u6a21\u578b\u81ea\u52a8\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5c06\u8f6f\u4ef6\u6d4b\u8bd5\u6280\u672f\u6269\u5c55\u5230\u4f18\u5316\u5efa\u6a21\u9886\u57df\uff0c\u901a\u8fc7\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u6d4b\u8bd5API\u3001\u6d4b\u8bd5\u7528\u4f8b\u548c\u6a21\u578b\u53d8\u5f02\u6765\u9a8c\u8bc1LLM\u751f\u6210\u7684\u4f18\u5316\u6a21\u578b\u662f\u5426\u6b63\u786e\u3002", "motivation": "\u968f\u7740\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u4f18\u5316\u6a21\u578b\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u5982\u4f55\u9a8c\u8bc1\u751f\u6210\u7684\u6a21\u578b\u662f\u5426\u6b63\u786e\u4e14\u6ee1\u8db3\u9700\u6c42\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u5f00\u653e\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u5305\u542b\u591a\u4e2a\u667a\u80fd\u4f53\uff1a\u9996\u5148\u751f\u6210\u95ee\u9898\u7ea7\u6d4b\u8bd5API\uff0c\u7136\u540e\u5229\u7528\u8be5API\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u6700\u540e\u751f\u6210\u4f18\u5316\u6a21\u578b\u7279\u5b9a\u7684\u53d8\u5f02\u6765\u8bc4\u4f30\u6d4b\u8bd5\u5957\u4ef6\u7684\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u667a\u80fd\u4f53\u96c6\u6210\u65b9\u6cd5\u5728\u7a81\u53d8\u8986\u76d6\u7387\u65b9\u9762\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u8f6f\u4ef6\u6d4b\u8bd5\u6280\u672f\u6269\u5c55\u5230\u4f18\u5316\u5efa\u6a21\u9886\u57df\uff0c\u80fd\u591f\u6709\u6548\u9a8c\u8bc1LLM\u751f\u6210\u7684\u4f18\u5316\u6a21\u578b\u7684\u6b63\u786e\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.15830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15830", "abs": "https://arxiv.org/abs/2511.15830", "authors": ["St\u00e9phane Aroca-Ouellette", "Ian Berlot-Attwell", "Panagiotis Lymperopoulos", "Abhiramon Rajasekharan", "Tongqi Zhu", "Herin Kang", "Kaheer Suleman", "Sam Pasupalak"], "title": "Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions", "comment": "8 pages (main paper)", "summary": "Despite rapid progress in artificial intelligence, current systems struggle with the interconnected challenges that define real-world decision making. Practical domains, such as business management, require optimizing an open-ended and multi-faceted objective, actively learning environment dynamics from sparse experience, planning over long horizons in stochastic settings, and reasoning over spatial information. Yet existing human--AI benchmarks isolate subsets of these capabilities, limiting our ability to assess holistic decision-making competence. We introduce Mini Amusement Parks (MAPs), an amusement-park simulator designed to evaluate an agent's ability to model its environment, anticipate long-term consequences under uncertainty, and strategically operate a complex business. We provide human baselines and a comprehensive evaluation of state-of-the-art LLM agents, finding that humans outperform these systems by 6.5x on easy mode and 9.8x on medium mode. Our analysis reveals persistent weaknesses in long-horizon optimization, sample-efficient learning, spatial reasoning, and world modelling. By unifying these challenges within a single environment, MAPs offers a new foundation for benchmarking agents capable of adaptable decision making. Code: https://github.com/Skyfall-Research/MAPs", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Mini Amusement Parks (MAPs)\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u590d\u6742\u4e1a\u52a1\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u53d1\u73b0\u4eba\u7c7b\u8868\u73b0\u8fdc\u8d85\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u5728\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u4e2d\u9762\u4e34\u591a\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u6311\u6218\uff0c\u5982\u5f00\u653e\u76ee\u6807\u4f18\u5316\u3001\u7a00\u758f\u7ecf\u9a8c\u5b66\u4e60\u3001\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b64\u7acb\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6574\u4f53\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86MAPs\u6e38\u4e50\u56ed\u6a21\u62df\u5668\uff0c\u7edf\u4e00\u4e86\u73af\u5883\u5efa\u6a21\u3001\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u957f\u671f\u540e\u679c\u9884\u6d4b\u548c\u590d\u6742\u4e1a\u52a1\u6218\u7565\u8fd0\u8425\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4eba\u7c7b\u57fa\u51c6\u548cLLM\u4ee3\u7406\u7684\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u4eba\u7c7b\u5728\u7b80\u5355\u6a21\u5f0f\u4e0b\u8868\u73b0\u4f18\u4e8eAI\u7cfb\u7edf6.5\u500d\uff0c\u5728\u4e2d\u7b49\u6a21\u5f0f\u4e0b\u4f18\u4e8e9.8\u500d\u3002\u5206\u6790\u63ed\u793a\u4e86AI\u5728\u957f\u65f6\u7a0b\u4f18\u5316\u3001\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u4e16\u754c\u5efa\u6a21\u65b9\u9762\u7684\u6301\u7eed\u5f31\u70b9\u3002", "conclusion": "MAPs\u901a\u8fc7\u5728\u4e00\u4e2a\u73af\u5883\u4e2d\u7edf\u4e00\u8fd9\u4e9b\u6311\u6218\uff0c\u4e3a\u8bc4\u4f30\u5177\u6709\u9002\u5e94\u6027\u51b3\u7b56\u80fd\u529b\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.15921", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15921", "abs": "https://arxiv.org/abs/2511.15921", "authors": ["Chelsea Zou", "Yiheng Yao", "Basant Khalil"], "title": "Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs", "comment": "Originally released June 5, 2025", "summary": "This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u7684LLM\u81ea\u6211\u4fee\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u548c\u8bcd\u5143\u7ea7\u71b5\u5cf0\u503c\u68c0\u6d4b\u5e7b\u89c9\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u63a8\u7406\u8fc7\u7a0b\u7684\u51c6\u786e\u6027\u548c\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u68c0\u6d4b\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u5b9e\u65f6\u68c0\u6d4b\u548c\u7f13\u89e3\u4e0d\u53ef\u9760\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u81ea\u6211\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u548c\u8bcd\u5143\u7ea7\u71b5\u5cf0\u503c\u4f5c\u4e3a\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6539\u8fdb\u6a21\u578b\u7684\u751f\u6210\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a8\u7406\u6821\u51c6\u5ea6\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u4fe1\u53f7\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u6211\u4fee\u6b63\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u7f13\u89e3LLM\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u5ea6\u548c\u51c6\u786e\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16043", "abs": "https://arxiv.org/abs/2511.16043", "authors": ["Peng Xia", "Kaide Zeng", "Jiaqi Liu", "Can Qin", "Fang Wu", "Yiyang Zhou", "Caiming Xiong", "Huaxiu Yao"], "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning", "comment": null, "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.", "AI": {"tldr": "Agent0\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65\u534f\u540c\u8fdb\u5316\u548c\u65e0\u7f1d\u5de5\u5177\u96c6\u6210\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u5373\u53ef\u8fdb\u5316\u9ad8\u6027\u80fd\u667a\u80fd\u4f53\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u4f9d\u8d56\u4eba\u7c7b\u7b56\u5212\u6570\u636e\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u800c\u73b0\u6709\u7684\u81ea\u6211\u8fdb\u5316\u6846\u67b6\u53d7\u9650\u4e8e\u6a21\u578b\u56fa\u6709\u80fd\u529b\u548c\u5355\u8f6e\u4ea4\u4e92\uff0c\u963b\u788d\u4e86\u6d89\u53ca\u5de5\u5177\u4f7f\u7528\u6216\u52a8\u6001\u63a8\u7406\u7684\u590d\u6742\u8bfe\u7a0b\u5f00\u53d1\u3002", "method": "\u5efa\u7acb\u4e24\u4e2a\u4ece\u76f8\u540c\u57fa\u7840LLM\u521d\u59cb\u5316\u7684\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5171\u751f\u7ade\u4e89\uff1a\u8bfe\u7a0b\u667a\u80fd\u4f53\u63d0\u51fa\u65e5\u76ca\u5177\u6709\u6311\u6218\u6027\u7684\u524d\u6cbf\u4efb\u52a1\uff0c\u6267\u884c\u667a\u80fd\u4f53\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u96c6\u6210\u589e\u5f3a\u6267\u884c\u80fd\u529b\uff0c\u53cd\u8fc7\u6765\u4fc3\u4f7f\u8bfe\u7a0b\u667a\u80fd\u4f53\u6784\u5efa\u66f4\u590d\u6742\u7684\u5de5\u5177\u611f\u77e5\u4efb\u52a1\u3002", "result": "Agent0\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u5c06Qwen3-8B-Base\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e8618%\uff0c\u5728\u4e00\u822c\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e8624%\u3002", "conclusion": "Agent0\u5efa\u7acb\u4e86\u4e00\u4e2a\u81ea\u6211\u5f3a\u5316\u7684\u5faa\u73af\uff0c\u6301\u7eed\u4ea7\u751f\u9ad8\u8d28\u91cf\u8bfe\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u4e3b\u7684\u667a\u80fd\u4f53\u8fdb\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.15958", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15958", "abs": "https://arxiv.org/abs/2511.15958", "authors": ["Zhenyu Bi", "Gaurav Srivastava", "Yang Li", "Meng Lu", "Swastik Roy", "Morteza Ziyadi", "Xuan Wang"], "title": "JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation", "comment": "23 pages, 4 figures", "summary": "While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.", "AI": {"tldr": "JudgeBoard\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8bc4\u4f30\u7ba1\u9053\uff0c\u76f4\u63a5\u67e5\u8be2\u6a21\u578b\u6765\u8bc4\u4f30\u5019\u9009\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u65e0\u9700\u989d\u5916\u7684\u7b54\u6848\u6bd4\u8f83\u3002\u63d0\u51fa\u4e86MAJ\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e2a\u4ea4\u4e92\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u534f\u4f5c\u6765\u8fd1\u4f3c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5224\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5224\u65ad\u7b54\u6848\u6b63\u786e\u6027\u65b9\u9762\u7684\u80fd\u529b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u5c1a\u4e0d\u6e05\u695a\uff0c\u73b0\u6709\u57fa\u4e8e\u6bd4\u8f83\u7684\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u8f93\u51fa\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u7684\u8bc4\u4f30\u6392\u884c\u699c\uff0c\u4f7f\u7528\u57fa\u4e8e\u51c6\u786e\u6027\u7684\u6392\u540d\u548cElo\u8bc4\u7ea7\u7cfb\u7edf\u3002\u63d0\u51faMAJ\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u63a8\u7406\u7279\u5f81\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u534f\u4f5c\u5ba1\u8bae\u6765\u63d0\u5347\u5224\u65ad\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u72ec\u7acb\u5224\u65ad\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u4f46MAJ\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002\u5728MATH\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u8f83\u5c0f\u6a21\u578b\u4f5c\u4e3a\u9aa8\u5e72\u7684MAJ\u8868\u73b0\u4e0e\u8f83\u5927\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u5c0f\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u5728\u5224\u65ad\u4efb\u52a1\u4e2d\u53ef\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5bf9\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2511.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15974", "abs": "https://arxiv.org/abs/2511.15974", "authors": ["Zhe Li", "Yehan Qiu", "Yujie Chen", "Xiang Zhou"], "title": "KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy", "comment": null, "summary": "Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of infection.This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at ~20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.", "AI": {"tldr": "KRAL\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u63a8\u7406\u81ea\u52a8\u63d0\u70bc\u77e5\u8bc6\u548c\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u5b66\u4e60\u8fdb\u884c\u534a\u76d1\u7763\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u5229\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u589e\u5f3a\u533b\u5b66\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRAG\u548cSFT\u65b9\u6cd5\u3002", "motivation": "\u4e34\u5e8a\u6297\u83cc\u6cbb\u7597\u9700\u8981\u52a8\u6001\u6574\u5408\u75c5\u539f\u4f53\u7279\u5f81\u3001\u5bbf\u4e3b\u56e0\u7d20\u3001\u6297\u83cc\u836f\u7269\u836f\u7406\u5b66\u7279\u6027\u548c\u611f\u67d3\u4e25\u91cd\u7a0b\u5ea6\uff0c\u8fd9\u79cd\u590d\u6742\u6027\u5bf9LLMs\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u6784\u6210\u4e86\u6839\u672c\u9650\u5236\uff0c\u5305\u62ec\u77e5\u8bc6\u5dee\u8ddd\u3001\u6570\u636e\u9690\u79c1\u95ee\u9898\u3001\u9ad8\u90e8\u7f72\u6210\u672c\u548c\u6709\u9650\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5229\u7528\u6559\u5e08\u6a21\u578b\u63a8\u7406\u901a\u8fc7\u7b54\u6848\u5230\u95ee\u9898\u7684\u53cd\u5411\u751f\u6210\u81ea\u52a8\u63d0\u70bc\u77e5\u8bc6\u548c\u63a8\u7406\u8f68\u8ff9\uff0c\u91c7\u7528\u542f\u53d1\u5f0f\u5b66\u4e60\u8fdb\u884c\u534a\u76d1\u7763\u6570\u636e\u589e\u5f3a\uff08\u51cf\u5c11\u7ea680%\u624b\u52a8\u6807\u6ce8\u9700\u6c42\uff09\uff0c\u5e76\u4f7f\u7528\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u589e\u5f3a\u533b\u5b66\u77e5\u8bc6\u548c\u63a8\u7406\uff0c\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "result": "KRAL\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRAG\u548cSFT\u65b9\u6cd5\uff0c\u5728\u5916\u90e8\u5f00\u6e90\u57fa\u51c6MEDQA\u4e0a\u7684\u77e5\u8bc6\u95ee\u7b54\u80fd\u529b\uff08Accuracy@1\u6bd4SFT\u63d0\u9ad81.8%\uff0c\u6bd4RAG\u63d0\u9ad83.6%\uff09\u548c\u63a8\u7406\u80fd\u529b\uff08\u5728\u5916\u90e8\u57fa\u51c6PUMCH Antimicrobial\u4e0a\u7684Pass@1\u6bd4SFT\u63d0\u9ad827%\uff0c\u6bd4RAG\u63d0\u9ad827.2%\uff09\uff0c\u4ec5\u9700SFT\u957f\u671f\u8bad\u7ec3\u6210\u672c\u7684\u7ea620%\u3002", "conclusion": "KRAL\u662f\u589e\u5f3a\u672c\u5730LLMs\u4e34\u5e8a\u8bca\u65ad\u80fd\u529b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u590d\u6742\u533b\u7597\u51b3\u7b56\u652f\u6301\u4e2d\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u5b89\u5168\u6027\u7684\u90e8\u7f72\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16540", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16540", "abs": "https://arxiv.org/abs/2511.16540", "authors": ["\u00c9lo\u00efse Benito-Rodriguez", "Einar Urdshals", "Jasmina Nasufi", "Nicky Pochinkov"], "title": "Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks", "comment": "13 pages, 5 figures", "summary": "Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u6fc0\u6d3b\u72b6\u6001\u9884\u6d4b\u6587\u672c\u4f53\u88c1\u7684\u6846\u67b6\uff0c\u4f7f\u7528Mistral-7B\u6a21\u578b\u548cscikit-learn\u5206\u7c7b\u5668\uff0c\u5728\u4e24\u79cd\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098%\u548c71%\u7684F1\u5206\u6570\uff0c\u8bc1\u660e\u6d45\u5c42\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u4eceLLM\u4e2d\u63a8\u65ad\u6587\u672c\u4f53\u88c1\u3002", "motivation": "\u7406\u89e3LLM\u5bf9\u4e8e\u786e\u4fdd\u5176\u5b89\u5168\u6709\u76ca\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8eLLM\u7ed3\u6784\u96be\u4ee5\u89e3\u91ca\u4e14\u65e0\u6cd5\u5bf9\u6240\u6709\u8f93\u51fa\u8fdb\u884c\u4eba\u5de5\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9884\u6d4b\u6027\u6846\u67b6\u3002", "method": "\u4f7f\u7528Mistral-7B\u6a21\u578b\uff0c\u57fa\u4e8e\u5176\u6fc0\u6d3b\u72b6\u6001\u9884\u6d4b\u6587\u672c\u4f53\u88c1\uff0c\u91c7\u7528scikit-learn\u5206\u7c7b\u5668\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f9798%\u548c71%\u7684F1\u5206\u6570\uff0c\u7ed3\u679c\u6301\u7eed\u4f18\u4e8e\u63a7\u5236\u4efb\u52a1\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8868\u660e\u6587\u672c\u4f53\u88c1\u53ef\u4ee5\u901a\u8fc7\u6d45\u5c42\u5b66\u4e60\u6a21\u578b\u4eceLLM\u4e2d\u63a8\u65ad\u51fa\u6765\u3002", "topic": "agent analysis"}}
{"id": "2511.15992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15992", "abs": "https://arxiv.org/abs/2511.15992", "authors": ["Shahin Zanbaghi", "Ryan Rostampour", "Farhan Abid", "Salim Al Jarmakani"], "title": "Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis", "comment": "7 pages, 3 figures, 1 table", "summary": "Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as \"sleeper agents.\" Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u6f02\u79fb\u5206\u6790\u548c\u91d1\u4e1d\u96c0\u57fa\u7ebf\u6bd4\u8f83\u7684\u53cc\u91cd\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u8bc6\u522b\u88ab\u690d\u5165\u540e\u95e8\u7684LLM\uff0c\u5728\u5b98\u65b9sleeper agent\u6a21\u578b\u4e0a\u8fbe\u523092.5%\u51c6\u786e\u7387\u3002", "motivation": "LLM\u53ef\u80fd\u88ab\u690d\u5165\u540e\u95e8\uff0c\u5728\u7279\u5b9a\u90e8\u7f72\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u6076\u610f\u884c\u4e3a\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u8fd9\u79cd'\u6f5c\u4f0f\u4ee3\u7406'\u73b0\u8c61\u3002", "method": "\u4f7f\u7528Sentence-BERT\u5d4c\u5165\u6d4b\u91cf\u8bed\u4e49\u504f\u79bb\u5b89\u5168\u57fa\u7ebf\u7684\u7a0b\u5ea6\uff0c\u5e76\u7ed3\u5408\u6ce8\u5165\u7684\u91d1\u4e1d\u96c0\u95ee\u9898\u76d1\u63a7\u54cd\u5e94\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5b98\u65b9dolphin-llama3-8B sleeper agent\u6a21\u578b\u4e0a\uff0c\u7cfb\u7edf\u8fbe\u523092.5%\u51c6\u786e\u7387\u3001100%\u7cbe\u786e\u5ea6\u548c85%\u53ec\u56de\u7387\uff0c\u5b9e\u65f6\u68c0\u6d4b\u65f6\u95f4\u5c0f\u4e8e1\u79d2\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86AI\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u8bc1\u660e\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u6d4b\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u6b3a\u9a97\u6027\u6a21\u578b\u884c\u4e3a\u800c\u4e0d\u727a\u7272\u90e8\u7f72\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.16075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16075", "abs": "https://arxiv.org/abs/2511.16075", "authors": ["Hrikshesh Kumar", "Anika Garg", "Anshul Gupta", "Yashika Agarwal"], "title": "A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management", "comment": null, "summary": "Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN-LSTM\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u4e91\u8fb9\u7f18\u5de5\u4f5c\u8d1f\u8f7d\u8d44\u6e90\u7ba1\u7406\u7684\u4e3b\u52a8\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u7684\u4e91\u8fb9\u7f18\u5de5\u4f5c\u8d1f\u8f7d\u8d44\u6e90\u7ba1\u7406\u8fc7\u4e8e\u88ab\u52a8\uff0c\u4f9d\u8d56\u9759\u6001\u9608\u503c\u4f1a\u5bfc\u81f4\u8d44\u6e90\u8fc7\u5ea6\u914d\u7f6e\u6216\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u8f6c\u5411\u4e3b\u52a8\u9884\u6d4b\u6027\u7ba1\u7406\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u67b6\u6784\uff0c\u5c06CNN-LSTM\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5d4c\u5165\u5230\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u72b6\u6001\u7a7a\u95f4\u4e2d\uff0c\u4f7fAI\u7ba1\u7406\u5668\u80fd\u591f\u9884\u89c1\u672a\u6765\u5e76\u5236\u5b9a\u957f\u671f\u89c4\u5212\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\u8be5\u7cfb\u7edf\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u51b3\u7b56\u95ee\u9898\uff0c\u540c\u65f6\u5e73\u8861\u6210\u672c\u8282\u7ea6\u3001\u7cfb\u7edf\u5065\u5eb7\u548c\u5e94\u7528\u7a0b\u5e8f\u6027\u80fd\u7b49\u591a\u4e2a\u76ee\u6807\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9884\u6d4b\u80fd\u529b\u5d4c\u5165\u5f3a\u5316\u5b66\u4e60\u72b6\u6001\u7a7a\u95f4\uff0c\u7cfb\u7edf\u80fd\u591f\u9884\u89c1\u672a\u6765\u5e76\u627e\u5230\u8d44\u6e90\u7ba1\u7406\u7684\u6700\u4f18\u8def\u5f84\uff0c\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16108", "abs": "https://arxiv.org/abs/2511.16108", "authors": ["Shiyi Cao", "Dacheng Li", "Fangzhou Zhao", "Shuo Yuan", "Sumanth R. Hegde", "Connor Chen", "Charlie Ruan", "Tyler Griggs", "Shu Liu", "Eric Tang", "Richard Liaw", "Philipp Moritz", "Matei Zaharia", "Joseph E. Gonzalez", "Ion Stoica"], "title": "SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent", "comment": null, "summary": "We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.\n  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.", "AI": {"tldr": "SkyRL-Agent\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8f6e\u3001\u957f\u89c6\u91ce\u667a\u80fd\u4f53\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u8c03\u5ea6\u3001\u8f7b\u91cf\u7ea7\u5de5\u5177\u96c6\u6210\u548c\u7075\u6d3b\u540e\u7aef\u4e92\u64cd\u4f5c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\u5728\u6548\u7387\u3001\u5de5\u5177\u96c6\u6210\u548c\u8de8\u6846\u67b6\u517c\u5bb9\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u9ad8\u6548\u591a\u8f6e\u957f\u89c6\u91ce\u8bad\u7ec3\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4f18\u5316\u7684\u5f02\u6b65\u7ba1\u9053\u8c03\u5ea6\u5668\uff081.55\u500d\u52a0\u901f\uff09\u548c\u57fa\u4e8eAST\u7684\u641c\u7d22\u5de5\u5177\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53SA-SWE-32B\u3002", "result": "SA-SWE-32B\u5728SWE-Bench Verified\u4e0a\u8fbe\u523039.4% Pass@1\uff0c\u76f8\u6bd4\u4e4b\u524d\u6a21\u578b\u6210\u672c\u964d\u4f4e2\u500d\u4ee5\u4e0a\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u5176\u4ed6\u667a\u80fd\u4f53\u4efb\u52a1\u3002", "conclusion": "SkyRL-Agent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u548c\u5de5\u5177\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "swe application"}}
{"id": "2511.16334", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16334", "abs": "https://arxiv.org/abs/2511.16334", "authors": ["Kaichen Zhang", "Keming Wu", "Zuhao Yang", "Kairui Hu", "Bin Wang", "Ziwei Liu", "Xingxuan Li", "Lidong Bing"], "title": "OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe", "comment": null, "summary": "Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.", "AI": {"tldr": "OpenMMReasoner\u662f\u4e00\u4e2a\u5b8c\u5168\u900f\u660e\u7684\u4e24\u9636\u6bb5\u591a\u6a21\u6001\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u542b\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u9636\u6bb5\uff0c\u57289\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4Qwen2.5-VL-7B-Instruct\u57fa\u7ebf\u63d0\u534711.6%\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u63a8\u7406\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684\u6570\u636e\u6574\u7406\u4e0e\u8bad\u7ec3\u7b56\u7565\u963b\u788d\u4e86\u53ef\u6269\u5c55\u7814\u7a76\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a1) SFT\u9636\u6bb5\u4f7f\u752887.4\u4e07\u6837\u672c\u7684\u51b7\u542f\u52a8\u6570\u636e\u96c6\u8fdb\u884c\u9010\u6b65\u9a8c\u8bc1\uff1b2) RL\u9636\u6bb5\u4f7f\u75287.4\u4e07\u6837\u672c\u8de8\u591a\u4e2a\u9886\u57df\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "\u57289\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4Qwen2.5-VL-7B-Instruct\u57fa\u7ebf\u63d0\u534711.6%\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u8bbe\u8ba1\u5bf9\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\uff0c\u8fd8\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u5b9e\u8bc1\u57fa\u7840\uff0c\u6240\u6709\u4ee3\u7801\u3001\u6d41\u7a0b\u548c\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16202", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16202", "abs": "https://arxiv.org/abs/2511.16202", "authors": ["Pei Yang", "Ke Zhang", "Ji Wang", "Xiao Chen", "Yuxin Tang", "Eric Yang", "Lynn Ai", "Bill Shi"], "title": "Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning", "comment": null, "summary": "We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.", "AI": {"tldr": "CRM\u6846\u67b6\u7528\u4e13\u5bb6\u8bc4\u4f30\u5668\u56e2\u961f\u66ff\u4ee3\u5355\u4e00\u9ed1\u76d2\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u89e3\u504f\u597d\u8bc4\u4f30\u5230\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u6765\u63d0\u5347RLHF\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u53ef\u80fd\u51b2\u7a81\u7684\u504f\u597d\u7ef4\u5ea6\uff08\u5982\u4e8b\u5b9e\u6027\u3001\u5e2e\u52a9\u6027\u3001\u5b89\u5168\u6027\uff09\uff0c\u4e14\u8bc4\u5206\u900f\u660e\u5ea6\u6709\u9650\u3002", "method": "\u5c06\u504f\u597d\u8bc4\u4f30\u5206\u89e3\u4e3a\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u4ea7\u751f\u90e8\u5206\u4fe1\u53f7\uff0c\u7ed3\u5408\u5168\u5c40\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u4e2d\u592e\u805a\u5408\u5668\u878d\u5408\u4fe1\u53f7\uff0c\u4f7f\u7528\u57fa\u4e8e\u4f18\u52bf\u7684\u66f4\u65b0\u4f18\u5316\u7b56\u7565\u3002", "result": "CRM\u548crewardBench\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u7684\u5956\u52b1\u5efa\u6a21\u548c\u66f4\u7a33\u5b9a\u4f18\u5316\u7684\u5b9e\u7528\u6a21\u5757\u5316\u8def\u5f84\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u89c6\u89d2\u5956\u52b1\u5851\u9020\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u6807\u6ce8\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16590", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16590", "abs": "https://arxiv.org/abs/2511.16590", "authors": ["Sen Chen", "Tong Zhao", "Yi Bin", "Fei Ma", "Wenqi Shao", "Zheng Wang"], "title": "D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies", "comment": "Accepted to AAAI 2026", "summary": "Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.", "AI": {"tldr": "\u63d0\u51fa\u4e86D-GARA\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30Android GUI\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5305\u542b\u591a\u79cd\u5e38\u89c1\u5f02\u5e38\u7c7b\u578b\u5982\u6743\u9650\u5bf9\u8bdd\u6846\u3001\u7535\u6c60\u8b66\u544a\u7b49\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u662f\u9759\u6001\u548c\u7406\u60f3\u5316\u7684\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u7279\u522b\u662f\u5f02\u5e38\u60c5\u51b5\u7684\u5b58\u5728\u3002", "method": "\u5f00\u53d1\u4e86D-GARA\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7c7b\u578b\uff0c\u6784\u5efa\u5e76\u6807\u6ce8\u4e86\u5305\u542b\u5e38\u7528Android\u5e94\u7528\u548c\u5d4c\u5165\u5f02\u5e38\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684GUI\u4ee3\u7406\u5728\u5f02\u5e38\u4e30\u5bcc\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u51f8\u663e\u4e86\u9c81\u68d2\u6027\u5b66\u4e60\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "D-GARA\u6846\u67b6\u662f\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\uff0c\u652f\u6301\u65b0\u4efb\u52a1\u3001\u5f02\u5e38\u7c7b\u578b\u548c\u4ea4\u4e92\u573a\u666f\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u4ee5\u6ee1\u8db3\u7279\u5b9a\u8bc4\u4f30\u76ee\u6807\u3002", "topic": "agent analysis"}}
{"id": "2511.16292", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16292", "abs": "https://arxiv.org/abs/2511.16292", "authors": ["Daniel Vaughan", "Kate\u0159ina Vaughan"], "title": "Distributed Agent Reasoning Across Independent Systems With Strict Data Locality", "comment": "27 pages, 6 figures", "summary": "This paper presents a proof-of-concept demonstration of agent-to-agent communication across distributed systems, using only natural-language messages and without shared identifiers, structured schemas, or centralised data exchange. The prototype explores how multiple organisations (represented here as a Clinic, Insurer, and Specialist Network) can cooperate securely via pseudonymised case tokens, local data lookups, and controlled operational boundaries.\n  The system uses Orpius as the underlying platform for multi-agent orchestration, tool execution, and privacy-preserving communication. All agents communicate through OperationRelay calls, exchanging concise natural-language summaries. Each agent operates on its own data (such as synthetic clinic records, insurance enrolment tables, and clinical guidance extracts), and none receives or reconstructs patient identity. The Clinic computes an HMAC-based pseudonymous token, the Insurer evaluates coverage rules and consults the Specialist agent, and the Specialist returns an appropriateness recommendation.\n  The goal of this prototype is intentionally limited: to demonstrate feasibility, not to provide a clinically validated, production-ready system. No clinician review was conducted, and no evaluation beyond basic functional runs was performed. The work highlights architectural patterns, privacy considerations, and communication flows that enable distributed reasoning among specialised agents while keeping data local to each organisation. We conclude by outlining opportunities for more rigorous evaluation and future research in decentralised multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u95f4\u4ec5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6d88\u606f\u8fdb\u884c\u901a\u4fe1\u7684\u6982\u5ff5\u9a8c\u8bc1\u539f\u578b\uff0c\u65e0\u9700\u5171\u4eab\u6807\u8bc6\u7b26\u3001\u7ed3\u6784\u5316\u6a21\u5f0f\u6216\u96c6\u4e2d\u5f0f\u6570\u636e\u4ea4\u6362\u3002", "motivation": "\u63a2\u7d22\u591a\u4e2a\u7ec4\u7ec7\uff08\u8bca\u6240\u3001\u4fdd\u9669\u516c\u53f8\u3001\u4e13\u79d1\u7f51\u7edc\uff09\u5982\u4f55\u901a\u8fc7\u5047\u540d\u5316\u6848\u4f8b\u4ee4\u724c\u3001\u672c\u5730\u6570\u636e\u67e5\u627e\u548c\u53d7\u63a7\u64cd\u4f5c\u8fb9\u754c\u8fdb\u884c\u5b89\u5168\u534f\u4f5c\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u63a8\u7406\u3002", "method": "\u4f7f\u7528Orpius\u5e73\u53f0\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u7f16\u6392\u3001\u5de5\u5177\u6267\u884c\u548c\u9690\u79c1\u4fdd\u62a4\u901a\u4fe1\uff0c\u901a\u8fc7OperationRelay\u8c03\u7528\u4ea4\u6362\u7b80\u6d01\u7684\u81ea\u7136\u8bed\u8a00\u6458\u8981\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4ec5\u64cd\u4f5c\u81ea\u5df1\u7684\u672c\u5730\u6570\u636e\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u53ef\u884c\u6027\uff0c\u8bca\u6240\u751f\u6210HMAC\u5047\u540d\u4ee4\u724c\uff0c\u4fdd\u9669\u516c\u53f8\u8bc4\u4f30\u8986\u76d6\u89c4\u5219\u5e76\u54a8\u8be2\u4e13\u79d1\u667a\u80fd\u4f53\uff0c\u4e13\u79d1\u667a\u80fd\u4f53\u8fd4\u56de\u9002\u5f53\u6027\u5efa\u8bae\uff0c\u6240\u6709\u8fc7\u7a0b\u4e0d\u6d89\u53ca\u60a3\u8005\u8eab\u4efd\u4fe1\u606f\u3002", "conclusion": "\u8be5\u539f\u578b\u7a81\u51fa\u4e86\u652f\u6301\u4e13\u95e8\u667a\u80fd\u4f53\u95f4\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u67b6\u6784\u6a21\u5f0f\u3001\u9690\u79c1\u8003\u91cf\u548c\u901a\u4fe1\u6d41\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6bcf\u4e2a\u7ec4\u7ec7\u7684\u6570\u636e\u672c\u5730\u5316\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.16402", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16402", "abs": "https://arxiv.org/abs/2511.16402", "authors": ["Jacopo Tagliabue", "Federico Bianchi", "Ciro Greco"], "title": "Trustworthy AI in the Agentic Lakehouse: from Concurrency to Governance", "comment": "AAAI26, pre-print of paper accepted at the Trustworthy Agentic AI Workshop", "summary": "Even as AI capabilities improve, most enterprises do not consider agents trustworthy enough to work on production data. In this paper, we argue that the path to trustworthy agentic workflows begins with solving the infrastructure problem first: traditional lakehouses are not suited for agent access patterns, but if we design one around transactions, governance follows. In particular, we draw an operational analogy to MVCC in databases and show why a direct transplant fails in a decoupled, multi-language setting. We then propose an agent-first design, Bauplan, that reimplements data and compute isolation in the lakehouse. We conclude by sharing a reference implementation of a self-healing pipeline in Bauplan, which seamlessly couples agent reasoning with all the desired guarantees for correctness and trust.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBauplan\u8bbe\u8ba1\uff0c\u91cd\u65b0\u5b9e\u73b0\u6e56\u4ed3\u4e2d\u7684\u6570\u636e\u4e0e\u8ba1\u7b97\u9694\u79bb\uff0c\u4e3a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u6b63\u786e\u6027\u548c\u4fe1\u4efb\u4fdd\u8bc1\u3002", "motivation": "\u5927\u591a\u6570\u4f01\u4e1a\u8ba4\u4e3a\u667a\u80fd\u4f53\u4e0d\u591f\u53ef\u4fe1\u6765\u5904\u7406\u751f\u4ea7\u6570\u636e\uff0c\u4f20\u7edf\u6e56\u4ed3\u4e0d\u9002\u5408\u667a\u80fd\u4f53\u8bbf\u95ee\u6a21\u5f0f\uff0c\u9700\u8981\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\u3002", "method": "\u501f\u9274\u6570\u636e\u5e93MVCC\u7684\u64cd\u4f5c\u7c7b\u6bd4\uff0c\u63d0\u51fa\u9762\u5411\u667a\u80fd\u4f53\u7684Bauplan\u8bbe\u8ba1\uff0c\u5728\u6e56\u4ed3\u4e2d\u91cd\u65b0\u5b9e\u73b0\u6570\u636e\u548c\u8ba1\u7b97\u9694\u79bb\u3002", "result": "\u5206\u4eab\u4e86Bauplan\u4e2d\u81ea\u6108\u7ba1\u9053\u7684\u53c2\u8003\u5b9e\u73b0\uff0c\u65e0\u7f1d\u7ed3\u5408\u667a\u80fd\u4f53\u63a8\u7406\u4e0e\u6240\u9700\u7684\u6b63\u786e\u6027\u548c\u4fe1\u4efb\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u95ee\u9898\uff0c\u8bbe\u8ba1\u56f4\u7ed5\u4e8b\u52a1\u7684\u6e56\u4ed3\uff0c\u53ef\u4ee5\u4e3a\u53ef\u4fe1\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u94fa\u5e73\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2511.16475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16475", "abs": "https://arxiv.org/abs/2511.16475", "authors": ["Ali Murtaza Caunhye", "Asad Jeewa"], "title": "A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms", "comment": "15 pages, 4 figures, published in the Proceedings of the 46th Annual conference of the South African Institute of Computer Scientists and Information Technologists (SIACSIT 2025)", "summary": "The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.", "AI": {"tldr": "\u6bd4\u8f83\u51b3\u7b56\u53d8\u6362\u5668\u4e0e\u4f20\u7edf\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728ANT\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u5bc6\u96c6\u548c\u7a00\u758f\u5956\u52b1\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u4f20\u7edf\u79bb\u7ebfRL\u7b97\u6cd5\u5728\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u5956\u52b1\u5bc6\u5ea6\u7684\u73af\u5883\u4e2d\u3002\u51b3\u7b56\u53d8\u6362\u5668\u5c06\u79bb\u7ebfRL\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u5176\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5728\u4e0d\u540c\u5956\u52b1\u7ed3\u6784\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u5728ANT\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u6bd4\u8f83\u51b3\u7b56\u53d8\u6362\u5668\u4e0e\u4f20\u7edf\u79bb\u7ebfRL\u7b97\u6cd5\uff08\u5982CQL\u3001IQL\uff09\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u5956\u52b1\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u548c\u5956\u52b1\u5bc6\u5ea6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u51b3\u7b56\u53d8\u6362\u5668\u5bf9\u5956\u52b1\u5bc6\u5ea6\u53d8\u5316\u8f83\u4e0d\u654f\u611f\uff0c\u5728\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e2d\u7684\u4e2d\u7b49\u4e13\u5bb6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1b\u4f20\u7edf\u4ef7\u503c\u65b9\u6cd5\u5982IQL\u5728\u5bc6\u96c6\u5956\u52b1\u8bbe\u7f6e\u4e2d\u8868\u73b0\u66f4\u597d\uff1bCQL\u5728\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u4e0b\u63d0\u4f9b\u5e73\u8861\u6027\u80fd\uff1b\u51b3\u7b56\u53d8\u6362\u5668\u6027\u80fd\u65b9\u5dee\u8f83\u4f4e\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u66f4\u9002\u5408\u5956\u52b1\u7ed3\u6784\u4e0d\u786e\u5b9a\u6216\u6570\u636e\u8d28\u91cf\u6df7\u5408\u7684\u573a\u666f\uff0c\u800c\u57fa\u4e8e\u4ef7\u503c\u7684\u65b9\u6cd5\u5728\u5bc6\u96c6\u5956\u52b1\u548c\u9ad8\u8d28\u91cf\u6f14\u793a\u8bbe\u7f6e\u4e2d\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16602", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16602", "abs": "https://arxiv.org/abs/2511.16602", "authors": ["Yi Zhang", "Che Liu", "Xiancong Ren", "Hanchu Ni", "Yingji Zhang", "Shuai Zhang", "Zeyuan Ding", "Jiayu Hu", "Haozhe Shan", "Junbo Qi", "Yan Bai", "Dengjie Li", "Jiachen Luo", "Yidong Wang", "Yong Dai", "Zenglin Xu", "Bin Shen", "Qifan Wang", "Jian Tang", "Xiaozhu Ju"], "title": "Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization", "comment": null, "summary": "Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.", "AI": {"tldr": "\u63d0\u51fa\u4e86DPPO\uff08Deliberate Practice Policy Optimization\uff09\u5143\u8ba4\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u6570\u636e\u74f6\u9888\u548c\u7b97\u6cd5\u6548\u7387\u95ee\u9898\uff0c\u5728Pelican-VL 1.0\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8620.3%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7a00\u7f3a\u6602\u8d35\u7684\u6570\u636e\u74f6\u9888\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\u7684\u7b97\u6cd5\u6548\u7387\u95ee\u9898\u3002", "method": "DPPO\u5143\u8ba4\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u52a8\u6001\u4ea4\u66ff\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08\u80fd\u529b\u6269\u5c55\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08\u6280\u80fd\u7cbe\u70bc\uff09\uff0c\u81ea\u52a8\u8bc6\u522b\u5f31\u70b9\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u8d44\u6e90\u5206\u914d\u3002", "result": "\u8bad\u7ec3\u51fa\u7684Pelican-VL 1.0\u5177\u8eab\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6027\u80fd\u63d0\u534720.3%\uff0c\u5728100B\u53c2\u6570\u89c4\u6a21\u4e0a\u8d85\u8d8a\u5f00\u6e90\u6a21\u578b10.6%\u3002", "conclusion": "DPPO\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u5730\u7f13\u89e3\u6570\u636e\u548c\u8d44\u6e90\u74f6\u9888\u7684\u6846\u67b6\uff0c\u4f7f\u793e\u533a\u80fd\u591f\u9ad8\u6548\u6784\u5efa\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16483", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.16483", "abs": "https://arxiv.org/abs/2511.16483", "authors": ["Sayak Mukherjee", "Samrat Chatterjee", "Emilie Purvine", "Ted Fujimoto", "Tegan Emerson"], "title": "Large Language Model-Based Reward Design for Deep Reinforcement Learning-Driven Autonomous Cyber Defense", "comment": "Accepted in the AAAI-26 Workshop on Artificial Intelligence for Cyber Security (AICS)", "summary": "Designing rewards for autonomous cyber attack and defense learning agents in a complex, dynamic environment is a challenging task for subject matter experts. We propose a large language model (LLM)-based reward design approach to generate autonomous cyber defense policies in a deep reinforcement learning (DRL)-driven experimental simulation environment. Multiple attack and defense agent personas were crafted, reflecting heterogeneity in agent actions, to generate LLM-guided reward designs where the LLM was first provided with contextual cyber simulation environment information. These reward structures were then utilized within a DRL-driven attack-defense simulation environment to learn an ensemble of cyber defense policies. Our results suggest that LLM-guided reward designs can lead to effective defense strategies against diverse adversarial behaviors.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u7f51\u7edc\u653b\u9632\u6a21\u62df\u73af\u5883\u4e2d\u751f\u6210\u81ea\u4e3b\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u4e3a\u81ea\u4e3b\u7f51\u7edc\u653b\u9632\u5b66\u4e60\u4ee3\u7406\u8bbe\u8ba1\u5956\u52b1\u5bf9\u9886\u57df\u4e13\u5bb6\u6765\u8bf4\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u521b\u5efa\u591a\u4e2a\u653b\u9632\u4ee3\u7406\u89d2\u8272\uff0c\u53cd\u6620\u4ee3\u7406\u884c\u4e3a\u7684\u5f02\u8d28\u6027\uff0c\u9996\u5148\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7f51\u7edc\u6a21\u62df\u73af\u5883\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u751f\u6210LLM\u5f15\u5bfc\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u7136\u540e\u5728DRL\u9a71\u52a8\u7684\u653b\u9632\u6a21\u62df\u73af\u5883\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u5956\u52b1\u7ed3\u6784\u6765\u5b66\u4e60\u7f51\u7edc\u9632\u5fa1\u7b56\u7565\u96c6\u5408\u3002", "result": "\u7ed3\u679c\u8868\u660eLLM\u5f15\u5bfc\u7684\u5956\u52b1\u8bbe\u8ba1\u80fd\u591f\u4ea7\u751f\u9488\u5bf9\u591a\u6837\u5316\u5bf9\u6297\u884c\u4e3a\u7684\u6709\u6548\u9632\u5fa1\u7b56\u7565\u3002", "conclusion": "LLM\u5f15\u5bfc\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u7f51\u7edc\u9632\u5fa1\u7b56\u7565\uff0c\u5e94\u5bf9\u4e0d\u540c\u7684\u653b\u51fb\u884c\u4e3a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16660", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16660", "abs": "https://arxiv.org/abs/2511.16660", "authors": ["Priyanka Kargupta", "Shuyue Stella Li", "Haocheng Wang", "Jinu Lee", "Shan Chen", "Orevaoghene Ahia", "Dean Light", "Thomas L. Griffiths", "Max Kleiman-Weiner", "Jiawei Han", "Asli Celikyilmaz", "Yulia Tsvetkov"], "title": "Cognitive Foundations for Reasoning and Their Manifestation in LLMs", "comment": "40 pages, 4 tables, 6 figures", "summary": "Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b28\u4e2a\u8ba4\u77e5\u5143\u7d20\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u4eba\u7c7b\u4e0eLLM\u5728\u63a8\u7406\u7ed3\u6784\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5e76\u5f00\u53d1\u4e86\u6d4b\u8bd5\u65f6\u63a8\u7406\u6307\u5bfc\u65b9\u6cd5\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u63d0\u5347\u4e8660%\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u590d\u6742\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7b80\u5355\u53d8\u4f53\u4e0a\u5374\u5931\u8d25\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u901a\u8fc7\u4e0d\u540c\u4e8e\u4eba\u7c7b\u63a8\u7406\u7684\u673a\u5236\u83b7\u5f97\u6b63\u786e\u8f93\u51fa\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3LLM\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u6839\u672c\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u4e8628\u4e2a\u8ba4\u77e5\u5143\u7d20\u7684\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e8617\u4e2a\u6a21\u578b\u768417\u4e07\u6761\u63a8\u7406\u8f68\u8ff9\u548c54\u6761\u4eba\u7c7b\u601d\u8003\u8f68\u8ff9\uff0c\u5e76\u5bf91,598\u7bc7LLM\u63a8\u7406\u8bba\u6587\u8fdb\u884c\u4e86\u5143\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4eba\u7c7b\u4f7f\u7528\u5c42\u6b21\u5d4c\u5957\u548c\u5143\u8ba4\u77e5\u76d1\u63a7\uff0c\u800c\u6a21\u578b\u4f9d\u8d56\u6d45\u5c42\u524d\u5411\u94fe\uff1b\u7814\u7a76\u793e\u533a\u5173\u6ce8\u6613\u91cf\u5316\u884c\u4e3a\u800c\u5ffd\u89c6\u5143\u8ba4\u77e5\u63a7\u5236\uff1b\u5f00\u53d1\u7684\u63a8\u7406\u6307\u5bfc\u65b9\u6cd5\u5728\u590d\u6742\u95ee\u9898\u4e0a\u63d0\u534760%\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8fde\u63a5\u8ba4\u77e5\u79d1\u5b66\u548cLLM\u7814\u7a76\uff0c\u4e3a\u5f00\u53d1\u57fa\u4e8e\u539f\u5219\u6027\u8ba4\u77e5\u673a\u5236\u800c\u975e\u8106\u5f31\u6377\u5f84\u7684\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u80fd\u529b\u548c\u5927\u89c4\u6a21\u6d4b\u8bd5\u4eba\u7c7b\u8ba4\u77e5\u7406\u8bba\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2511.16652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16652", "abs": "https://arxiv.org/abs/2511.16652", "authors": ["Bidipta Sarkar", "Mattie Fellows", "Juan Agustin Duque", "Alistair Letcher", "Antonio Le\u00f3n Villares", "Anya Sims", "Dylan Cope", "Jarek Liesen", "Lukas Seier", "Theo Wolf", "Uljad Berdica", "Alexander David Goldie", "Aaron Courville", "Karin Sevegnani", "Shimon Whiteson", "Jakob Nicolaus Foerster"], "title": "Evolution Strategies at the Hyperscale", "comment": "48 pages, 12 figures, Website at https://eshyperscale.github.io/", "summary": "We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{\u00ef}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\\in\\mathbb{R}^{m\\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\\in \\mathbb{R}^{m\\times r},\\ B\\in \\mathbb{R}^{n\\times r}$ with $r\\ll \\min(m,n)$ to form a low-rank matrix perturbation $A B^\\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\\mathcal{O}(mn)$ to $\\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\\mathcal{O}\\left(\\frac{1}{r}\\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.", "AI": {"tldr": "EGGROLL\u662f\u4e00\u79cd\u8fdb\u5316\u7b56\u7565\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u5b66\u4e60\u5b9e\u73b0\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u6548\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u7b56\u7565\u5728\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u77e9\u9635\u6270\u52a8\u66ff\u4ee3\u5168\u79e9\u6270\u52a8\uff0c\u901a\u8fc7\u751f\u6210\u968f\u673a\u4f4e\u79e9\u77e9\u9635A\u548cB\u6765\u5f62\u6210\u4f4e\u79e9\u6270\u52a8AB\u22a4\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEGGROLL\u5728\u4fdd\u6301ES\u6027\u80fd\u7684\u540c\u65f6\u901f\u5ea6\u66f4\u5feb\uff0c\u5728LLM\u63a8\u7406\u4e2d\u4e0eGRPO\u7ade\u4e89\uff0c\u5e76\u80fd\u7a33\u5b9a\u9884\u8bad\u7ec3\u7eaf\u6574\u6570\u6570\u636e\u7c7b\u578b\u7684\u975e\u7ebf\u6027\u5faa\u73af\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "EGGROLL\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8fdb\u5316\u7b56\u7565\u4f18\u5316\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.16665", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16665", "abs": "https://arxiv.org/abs/2511.16665", "authors": ["Qinghao Hu", "Shang Yang", "Junxian Guo", "Xiaozhe Yao", "Yujun Lin", "Yuxian Gu", "Han Cai", "Chuang Gan", "Ana Klimovic", "Song Han"], "title": "Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter", "comment": null, "summary": "The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.", "AI": {"tldr": "TLT\u7cfb\u7edf\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u52a0\u901f\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u957f\u5c3e\u54cd\u5e94\u5206\u5e03\u5bfc\u81f4\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e861.7\u500d\u7aef\u5230\u7aef\u8bad\u7ec3\u52a0\u901f\u4e14\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u63a8\u7406\u6a21\u578b\u5728RL\u8bad\u7ec3\u4e2d\u5b58\u5728\u957f\u5c3e\u54cd\u5e94\u5206\u5e03\u95ee\u9898\uff0c\u5c11\u6570\u6781\u957f\u54cd\u5e94\u4e3b\u5bfc\u6267\u884c\u65f6\u95f4\uff0c\u9020\u6210\u8d44\u6e90\u6d6a\u8d39\u548c\u6210\u672c\u589e\u52a0\u3002", "method": "\u63d0\u51faTLT\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u8349\u7a3f\u6a21\u578b\uff08\u5728\u7a7a\u95f2GPU\u4e0a\u6301\u7eed\u8bad\u7ec3\u4ee5\u4fdd\u6301\u4e0e\u76ee\u6807\u6a21\u578b\u5bf9\u9f50\uff09\u548c\u81ea\u9002\u5e94\u6267\u884c\u5f15\u64ce\uff08\u7ef4\u62a4\u5185\u5b58\u9ad8\u6548\u7684CUDAGraph\u6c60\u5e76\u81ea\u9002\u5e94\u9009\u62e9SD\u7b56\u7565\uff09\u3002", "result": "\u8bc4\u4f30\u663e\u793aTLT\u5b9e\u73b0\u4e86\u8d85\u8fc71.7\u500d\u7684\u7aef\u5230\u7aefRL\u8bad\u7ec3\u52a0\u901f\uff0c\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\uff0c\u5e76\u4ea7\u751f\u9ad8\u8d28\u91cf\u8349\u7a3f\u6a21\u578b\u4f5c\u4e3a\u514d\u8d39\u526f\u4ea7\u54c1\u3002", "conclusion": "TLT\u6210\u529f\u89e3\u51b3\u4e86\u63a8\u7406RL\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u590d\u6742\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.db254de6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts%2Fgemini%2Fgemini-3%2F%3Futm_source=tldrai/1/0100019a9c78f63b-54f89f7f-1634-4028-bd52-80957d6ddb20-000000/ZP1CS-3e7U8E8K2rURG4XvEyb2Yu8ufIaottIl4iMfQ=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts%2Fgemini%2Fgemini-3%2F%3Futm_source=tldrai/1/0100019a9c78f63b-54f89f7f-1634-4028-bd52-80957d6ddb20-000000/ZP1CS-3e7U8E8K2rURG4XvEyb2Yu8ufIaottIl4iMfQ=432", "authors": ["TLDR Newsletter"], "title": "Gemini 3", "comment": "Source: TLDR Newsletter, Date: 2025-11-19, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts%2Fgemini%2Fgemini-3%2F%3Futm_source=tldrai/1/0100019a9c78f63b-54f89f7f-1634-4028-bd52-80957d6ddb20-000000/ZP1CS-3e7U8E8K2rURG4XvEyb2Yu8ufIaottIl4iMfQ=432", "summary": "Gemini 3 (12 minute read) Gemini 3 achieves state-of-the-art performance on most major benchmarks, including a breakthrough 1501 Elo on LMArena, 91.9% on GPQA Diamond, and 81% on MMMU-Pro. The model includes a Deep Think mode for enhanced reasoning and coincides with the launch of a new agentic coding IDE called Antigravity.", "source": "tldr", "AI": {"tldr": "Gemini 3\u5728\u591a\u4e2a\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ecLMArena\u4e0a\u76841501 Elo\u3001GPQA Diamond\u768491.9%\u548cMMMU-Pro\u768481%\u5f97\u5206\u3002\u6a21\u578b\u5305\u542b\u589e\u5f3a\u63a8\u7406\u7684Deep Think\u6a21\u5f0f\uff0c\u5e76\u4f34\u968f\u63a8\u51fa\u65b0\u7684\u667a\u80fd\u7f16\u7801IDE Antigravity\u3002", "motivation": "\u5f00\u53d1\u66f4\u5f3a\u5927\u7684AI\u6a21\u578b\u4ee5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u7a81\u7834\u6027\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u548c\u7f16\u7801\u80fd\u529b\u65b9\u9762\u3002", "method": "Gemini 3\u6a21\u578b\u91c7\u7528\u5148\u8fdb\u67b6\u6784\uff0c\u5305\u542bDeep Think\u6a21\u5f0f\u7528\u4e8e\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u63a8\u51faAntigravity IDE\u652f\u6301\u667a\u80fd\u7f16\u7801\u3002", "result": "\u5728LMArena\u4e0a\u83b7\u5f971501 Elo\u7a81\u7834\u6027\u6210\u7ee9\uff0cGPQA Diamond\u8fbe\u523091.9%\uff0cMMMU-Pro\u8fbe\u523081%\uff0c\u5728\u5927\u591a\u6570\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Gemini 3\u5728\u591a\u4e2a\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u548c\u7f16\u7801\u4efb\u52a1\u4e0a\uff0cDeep Think\u6a21\u5f0f\u548cAntigravity IDE\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u5176\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2511.c0881822", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdata/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/b46RO3qY0PUIv50oIcHfmfnIMLXSDwzT1ne0t_WrX0o=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdata/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/b46RO3qY0PUIv50oIcHfmfnIMLXSDwzT1ne0t_WrX0o=432", "authors": ["TLDR Newsletter"], "title": "How Dash uses context engineering for smarter AI", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdropbox.tech%2Fmachine-learning%2Fhow-dash-uses-context-engineering-for-smarter-ai%3Futm_source=tldrdata/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/b46RO3qY0PUIv50oIcHfmfnIMLXSDwzT1ne0t_WrX0o=432", "summary": "How Dash uses context engineering for smarter AI (5 minute read) Dropbox improved Dash's agentic performance by consolidating many retrieval tools into a single unified \u201cDash Search\u201d tool, filtering results at runtime using a knowledge graph to deliver only highly relevant context, and delegating complex subtasks like query construction to a specialized search agent. These three context-engineering strategies reduce noise and tool sprawl, prevent context overload, and balance token usage, cos...", "source": "tldr", "AI": {"tldr": "Dropbox\u901a\u8fc7\u5c06\u591a\u4e2a\u68c0\u7d22\u5de5\u5177\u6574\u5408\u4e3a\u7edf\u4e00\u7684Dash Search\u5de5\u5177\u3001\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u8fc7\u6ee4\u7ed3\u679c\u3001\u5c06\u590d\u6742\u5b50\u4efb\u52a1\u59d4\u6258\u7ed9\u4e13\u95e8\u641c\u7d22\u4ee3\u7406\uff0c\u63d0\u5347\u4e86Dash\u7684\u667a\u80fd\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AI\u4ee3\u7406\u4e2d\u5de5\u5177\u6cdb\u6ee5\u3001\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u548ctoken\u4f7f\u7528\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7b56\u7565\uff1a\u7edf\u4e00\u68c0\u7d22\u5de5\u5177\u3001\u8fd0\u884c\u65f6\u77e5\u8bc6\u56fe\u8c31\u8fc7\u6ee4\u3001\u59d4\u6258\u590d\u6742\u5b50\u4efb\u52a1\u7ed9\u4e13\u95e8\u641c\u7d22\u4ee3\u7406\u3002", "result": "\u51cf\u5c11\u4e86\u566a\u97f3\u548c\u5de5\u5177\u8513\u5ef6\uff0c\u9632\u6b62\u4e86\u4e0a\u4e0b\u6587\u8fc7\u8f7d\uff0c\u5e73\u8861\u4e86token\u4f7f\u7528\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.7d0f5840", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FZrKhQy/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/tgxg6gt44iQfKZYRtewMj0j3iSRiYYnsyUgoILjnJds=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FZrKhQy/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/tgxg6gt44iQfKZYRtewMj0j3iSRiYYnsyUgoILjnJds=432", "authors": ["TLDR Newsletter"], "title": "How Can You Identify an Agentic AI Use Case?", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FZrKhQy/1/0100019aa0f2a860-9b778705-be80-41a2-b218-e56b1811d8d9-000000/tgxg6gt44iQfKZYRtewMj0j3iSRiYYnsyUgoILjnJds=432", "summary": "How Can You Identify an Agentic AI Use Case? (10 minute read) Agentic AI can automate complex, reasoning-heavy tasks that are repetitive, expert-dependent, or involve scattered/unstructured data, dramatically cutting human effort, provided the scope is clearly bounded, tools are well-defined (potentially with subagents), and sufficient upfront documentation is invested to eliminate ambiguity and prevent incomplete automation.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u8bc6\u522b\u9002\u5408\u4f7f\u7528\u667a\u80fdAI\u4ee3\u7406\u7684\u7528\u4f8b\uff0c\u6307\u51fa\u667a\u80fdAI\u53ef\u4ee5\u81ea\u52a8\u5316\u590d\u6742\u3001\u63a8\u7406\u5bc6\u96c6\u7684\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u5177\u6709\u91cd\u590d\u6027\u3001\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u6216\u6d89\u53ca\u5206\u6563/\u975e\u7ed3\u6784\u5316\u6570\u636e\u3002", "motivation": "\u52a8\u673a\u662f\u5e2e\u52a9\u7ec4\u7ec7\u8bc6\u522b\u54ea\u4e9b\u4e1a\u52a1\u573a\u666f\u9002\u5408\u90e8\u7f72\u667a\u80fdAI\u4ee3\u7406\uff0c\u4ee5\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u660e\u786e\u754c\u5b9a\u4efb\u52a1\u8303\u56f4\u3001\u5b9a\u4e49\u826f\u597d\u7684\u5de5\u5177\uff08\u53ef\u80fd\u5305\u542b\u5b50\u4ee3\u7406\uff09\uff0c\u4ee5\u53ca\u6295\u5165\u8db3\u591f\u7684\u6587\u6863\u51c6\u5907\u5de5\u4f5c\u6765\u6d88\u9664\u6b67\u4e49\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u667a\u80fdAI\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4eba\u529b\u6295\u5165\u3002", "conclusion": "\u7ed3\u8bba\u662f\u667a\u80fdAI\u5728\u660e\u786e\u754c\u5b9a\u8303\u56f4\u3001\u5de5\u5177\u5b9a\u4e49\u6e05\u6670\u4e14\u6587\u6863\u5145\u5206\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316\u590d\u6742\u4efb\u52a1\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.e0cfc953", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbLpQcx/1/0100019aa1232533-b367583d-d3f5-44c8-afa7-fe2e2196da48-000000/Xy51UUT2ZFX2mmjvI4qQaieUdhvoiIyvr8AXohQ-te4=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbLpQcx/1/0100019aa1232533-b367583d-d3f5-44c8-afa7-fe2e2196da48-000000/Xy51UUT2ZFX2mmjvI4qQaieUdhvoiIyvr8AXohQ-te4=432", "authors": ["TLDR Newsletter"], "title": "Building more with GPT-5.1-Codex-Max", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbLpQcx/1/0100019aa1232533-b367583d-d3f5-44c8-afa7-fe2e2196da48-000000/Xy51UUT2ZFX2mmjvI4qQaieUdhvoiIyvr8AXohQ-te4=432", "summary": "Building more with GPT-5.1-Codex-Max (8 minute read) GPT\u20115.1-Codex-Max is a new frontier agentic coding model from OpenAI. It is much faster, more intelligent, and more token-efficient at every stage of the development cycle. The model is built for long-running, detailed work. It is available in Codex today. API access is coming soon.", "source": "tldr", "AI": {"tldr": "GPT-5.1-Codex-Max\u662fOpenAI\u63a8\u51fa\u7684\u65b0\u578b\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5feb\u7684\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u667a\u80fd\u6c34\u5e73\u548c\u66f4\u597d\u7684token\u6548\u7387\uff0c\u4e13\u4e3a\u957f\u671f\u3001\u8be6\u7ec6\u7684\u5de5\u4f5c\u8bbe\u8ba1\u3002", "motivation": "\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u66f4\u667a\u80fd\u7684\u7f16\u7801\u4ee3\u7406\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\u6548\u7387\u3002", "method": "\u57fa\u4e8eGPT-5.1\u67b6\u6784\u6784\u5efa\u7684Codex\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u4ee3\u7406\u5f0f\u7f16\u7801\u80fd\u529b\uff0c\u652f\u6301\u957f\u671f\u8fd0\u884c\u7684\u8be6\u7ec6\u5f00\u53d1\u5de5\u4f5c\u3002", "result": "\u6a21\u578b\u5728\u901f\u5ea6\u3001\u667a\u80fd\u6c34\u5e73\u548ctoken\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u5df2\u5728Codex\u5e73\u53f0\u53ef\u7528\uff0cAPI\u8bbf\u95ee\u5373\u5c06\u63a8\u51fa\u3002", "conclusion": "GPT-5.1-Codex-Max\u4ee3\u8868\u4e86\u7f16\u7801\u4ee3\u7406\u6a21\u578b\u7684\u65b0\u524d\u6cbf\uff0c\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "tldr.2511.4028db64", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbotDcK/1/0100019aa12af4ab-9a09deb7-c20f-4818-9ff5-1f5d093d3fa3-000000/Xlc72HVKpeCJ76WXeIbplS6UMxIxrARzzDMuWU_V8jM=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbotDcK/1/0100019aa12af4ab-9a09deb7-c20f-4818-9ff5-1f5d093d3fa3-000000/Xlc72HVKpeCJ76WXeIbplS6UMxIxrARzzDMuWU_V8jM=432", "authors": ["TLDR Newsletter"], "title": "Building more with GPT-5.1-Codex-Max", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbotDcK/1/0100019aa12af4ab-9a09deb7-c20f-4818-9ff5-1f5d093d3fa3-000000/Xlc72HVKpeCJ76WXeIbplS6UMxIxrARzzDMuWU_V8jM=432", "summary": "Building more with GPT-5.1-Codex-Max (8 minute read) OpenAI's GPT-5.1-Codex-Max is an advanced agentic coding model built on an updated reasoning foundation and designed for long-running, detailed software engineering tasks. The model can operate across multiple context windows, enabling project-scale refactors and deep debugging sessions. GPT-5.1-Codex-Max is available in Codex.", "source": "tldr", "AI": {"tldr": "OpenAI\u63a8\u51fa\u4e86GPT-5.1-Codex-Max\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u66f4\u65b0\u63a8\u7406\u57fa\u7840\u6784\u5efa\u7684\u9ad8\u7ea7\u4ee3\u7406\u7f16\u7801\u6a21\u578b\uff0c\u4e13\u4e3a\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u8be6\u7ec6\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u8bbe\u8ba1\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u64cd\u4f5c\uff0c\u652f\u6301\u9879\u76ee\u7ea7\u91cd\u6784\u548c\u6df1\u5ea6\u8c03\u8bd5\u4f1a\u8bdd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u590d\u6742\u3001\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684AI\u7f16\u7801\u52a9\u624b\uff0c\u652f\u6301\u9879\u76ee\u89c4\u6a21\u7684\u91cd\u6784\u548c\u6df1\u5ea6\u8c03\u8bd5\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u66f4\u65b0\u7684\u63a8\u7406\u57fa\u7840\u6784\u5efa\uff0c\u652f\u6301\u8de8\u591a\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\u64cd\u4f5c\uff0c\u4e13\u95e8\u4e3a\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4f18\u5316\u3002", "result": "GPT-5.1-Codex-Max\u6a21\u578b\u5df2\u5728Codex\u4e2d\u53ef\u7528\uff0c\u80fd\u591f\u6267\u884c\u9879\u76ee\u7ea7\u91cd\u6784\u548c\u6df1\u5ea6\u8c03\u8bd5\u4f1a\u8bdd\u3002", "conclusion": "GPT-5.1-Codex-Max\u4ee3\u8868\u4e86AI\u7f16\u7801\u52a9\u624b\u7684\u91cd\u8981\u8fdb\u6b65\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002", "topic": "code agent"}}
{"id": "tldr.2511.b2ff5705", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgemini-3-developers%2F%3Futm_source=tldrdesign/1/0100019aa15f04ad-2bb84835-a9c9-459c-85a1-0b6ef7a10245-000000/ZfZ3jUNZ3qOypx8YK6Rd7lrFbH7svsBsArQOehVEQYQ=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgemini-3-developers%2F%3Futm_source=tldrdesign/1/0100019aa15f04ad-2bb84835-a9c9-459c-85a1-0b6ef7a10245-000000/ZfZ3jUNZ3qOypx8YK6Rd7lrFbH7svsBsArQOehVEQYQ=432", "authors": ["TLDR Newsletter"], "title": "Start Building with Gemini 3", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fgemini-3-developers%2F%3Futm_source=tldrdesign/1/0100019aa15f04ad-2bb84835-a9c9-459c-85a1-0b6ef7a10245-000000/ZfZ3jUNZ3qOypx8YK6Rd7lrFbH7svsBsArQOehVEQYQ=432", "summary": "Start Building with Gemini 3 (9 minute read) Google's Gemini 3 Pro, its most intelligent model to date, surpasses competitors on major AI benchmarks while excelling at coding tasks and agentic workflows. The model powers Google Antigravity, a new agentic development platform that enables developers to collaborate with autonomous AI agents across editor, terminal, and browser environments. Gemini 3 Pro also leads in multimodal understanding, including document analysis, spatial reasoning, and ...", "source": "tldr", "AI": {"tldr": "Google\u63a8\u51faGemini 3 Pro\u6a21\u578b\uff0c\u5728AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u7ade\u4e89\u5bf9\u624b\uff0c\u7279\u522b\u64c5\u957f\u7f16\u7801\u4efb\u52a1\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u5e76\u63a8\u51faAntigravity\u5f00\u53d1\u5e73\u53f0\u652f\u6301\u591a\u73af\u5883AI\u4ee3\u7406\u534f\u4f5c\u3002", "motivation": "\u5f00\u53d1\u66f4\u667a\u80fd\u7684AI\u6a21\u578b\u4ee5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u7ade\u4e89\u5bf9\u624b\uff0c\u540c\u65f6\u4e13\u6ce8\u4e8e\u63d0\u5347\u7f16\u7801\u80fd\u529b\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u6548\u7387\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u597d\u7684AI\u534f\u4f5c\u5de5\u5177\u3002", "method": "\u57fa\u4e8eGemini 3 Pro\u6a21\u578b\u6784\u5efaAntigravity\u5f00\u53d1\u5e73\u53f0\uff0c\u652f\u6301\u5728\u7f16\u8f91\u5668\u3001\u7ec8\u7aef\u548c\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u4e0e\u81ea\u4e3bAI\u4ee3\u7406\u534f\u4f5c\u3002", "result": "Gemini 3 Pro\u5728\u4e3b\u8981AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u7f16\u7801\u4efb\u52a1\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5177\u5907\u9886\u5148\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "Gemini 3 Pro\u662f\u76ee\u524d\u6700\u667a\u80fd\u7684\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u5353\u8d8a\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684AI\u534f\u4f5c\u5e73\u53f0\u3002", "topic": "code agent"}}
{"id": "tldr.2511.678ff83a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2025%2F11%2Fservicenow-ai-agents-can-be-tricked.html%3Futm_source=tldrinfosec/1/0100019aa1981f7c-8589c13f-0c81-4868-b154-8a9728d1267a-000000/llPjEFzZ44rV_OtsG-yCIJSxe3FBbEVWf2IjnFE8MpI=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2025%2F11%2Fservicenow-ai-agents-can-be-tricked.html%3Futm_source=tldrinfosec/1/0100019aa1981f7c-8589c13f-0c81-4868-b154-8a9728d1267a-000000/llPjEFzZ44rV_OtsG-yCIJSxe3FBbEVWf2IjnFE8MpI=432", "authors": ["TLDR Newsletter"], "title": "ServiceNow AI Agents Can Be Tricked Into Acting Against Each Other via Second-Order Prompts", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthehackernews.com%2F2025%2F11%2Fservicenow-ai-agents-can-be-tricked.html%3Futm_source=tldrinfosec/1/0100019aa1981f7c-8589c13f-0c81-4868-b154-8a9728d1267a-000000/llPjEFzZ44rV_OtsG-yCIJSxe3FBbEVWf2IjnFE8MpI=432", "summary": "ServiceNow AI Agents Can Be Tricked Into Acting Against Each Other via Second-Order Prompts (3 minute read) ServiceNow's Now Assist AI agents can be manipulated through second-order prompt injection, where instructions passed between agents trigger unintended actions. Attackers can use this to escalate privileges, steal data, or redirect workflows. ServiceNow claims the behavior matches expected agent chaining, but organizations are urged to monitor agent interactions and harden guardrails.", "source": "tldr", "AI": {"tldr": "ServiceNow\u7684Now Assist AI\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u4e8c\u9636\u63d0\u793a\u6ce8\u5165\u88ab\u64cd\u7eb5\uff0c\u653b\u51fb\u8005\u5229\u7528\u4ee3\u7406\u95f4\u4f20\u9012\u7684\u6307\u4ee4\u89e6\u53d1\u610f\u5916\u884c\u4e3a\uff0c\u5b9e\u73b0\u6743\u9650\u63d0\u5347\u3001\u6570\u636e\u7a83\u53d6\u6216\u5de5\u4f5c\u6d41\u91cd\u5b9a\u5411\u3002", "motivation": "\u63ed\u793aAI\u4ee3\u7406\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u901a\u8fc7\u4ee3\u7406\u95f4\u6307\u4ee4\u4f20\u9012\u8fdb\u884c\u7684\u4e8c\u9636\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4ee5\u63d0\u5347\u5bf9AI\u4ee3\u7406\u5b89\u5168\u6027\u7684\u8ba4\u8bc6\u3002", "method": "\u901a\u8fc7\u4e8c\u9636\u63d0\u793a\u6ce8\u5165\u6280\u672f\uff0c\u653b\u51fb\u8005\u5728\u4ee3\u7406\u95f4\u4f20\u9012\u7684\u6307\u4ee4\u4e2d\u5d4c\u5165\u6076\u610f\u5185\u5bb9\uff0c\u5229\u7528\u4ee3\u7406\u95f4\u7684\u4fe1\u4efb\u5173\u7cfb\u89e6\u53d1\u610f\u5916\u884c\u4e3a\u3002", "result": "ServiceNow\u7684AI\u4ee3\u7406\u53ef\u4ee5\u88ab\u64cd\u7eb5\u6267\u884c\u6743\u9650\u63d0\u5347\u3001\u6570\u636e\u7a83\u53d6\u548c\u5de5\u4f5c\u6d41\u91cd\u5b9a\u5411\u7b49\u6076\u610f\u64cd\u4f5c\uff0c\u5c3d\u7ba1ServiceNow\u58f0\u79f0\u8fd9\u662f\u9884\u671f\u7684\u4ee3\u7406\u94fe\u884c\u4e3a\u3002", "conclusion": "\u7ec4\u7ec7\u9700\u8981\u76d1\u63a7AI\u4ee3\u7406\u95f4\u7684\u4ea4\u4e92\u5e76\u52a0\u5f3a\u9632\u62a4\u63aa\u65bd\uff0c\u4ee5\u9632\u8303\u4e8c\u9636\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.ba0c5037", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432", "authors": ["TLDR Newsletter"], "title": "The average financial company loses $250M a year on unfinished applications", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432", "summary": "The average financial company loses $250M a year on unfinished applications (Sponsor) Today, hundreds of the largest financial institutions are using conversational AI agents to handle application follow-ups instead. Before, completion rates were 30%. Now, they're almost double that.Crazy what a little AI can do.Don't believe us? Get a free custom agent today.", "source": "tldr", "AI": {"tldr": "\u91d1\u878d\u516c\u53f8\u6bcf\u5e74\u56e0\u672a\u5b8c\u6210\u7533\u8bf7\u635f\u59312.5\u4ebf\u7f8e\u5143\uff0c\u4f7f\u7528\u5bf9\u8bddAI\u4ee3\u7406\u540e\u5b8c\u6210\u7387\u4ece30%\u63d0\u5347\u81f3\u8fd160%\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u516c\u53f8\u56e0\u672a\u5b8c\u6210\u7533\u8bf7\u9020\u6210\u7684\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\uff0c\u63d0\u9ad8\u7533\u8bf7\u5b8c\u6210\u7387\u3002", "method": "\u4f7f\u7528\u5bf9\u8bddAI\u4ee3\u7406\u5904\u7406\u7533\u8bf7\u8ddf\u8fdb\u5de5\u4f5c\u3002", "result": "\u7533\u8bf7\u5b8c\u6210\u7387\u4ece30%\u63d0\u5347\u81f3\u8fd160%\uff0c\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "AI\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u91d1\u878d\u7533\u8bf7\u5b8c\u6210\u7387\uff0c\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u3002", "topic": "swe application"}}
{"id": "tldr.2511.6c34e008", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432", "authors": ["TLDR Newsletter"], "title": "they're almost double that", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbland.ai%3Futm_source=TLDRfintech%26utm_medium=newsletter%26utm_campaign=nov20%26utm_id=TLDRfintechnov20/1/0100019aa19893ef-7b1bbb95-620e-4f92-89a8-b205a2ceb9ee-000000/CQ8hrcQbktlzfTvd9pHS_-VJv5aesny8i3x4RRBmLMo=432", "summary": "The average financial company loses $250M a year on unfinished applications (Sponsor) Today, hundreds of the largest financial institutions are using conversational AI agents to handle application follow-ups instead. Before, completion rates were 30%. Now, they're almost double that.Crazy what a little AI can do.Don't believe us? Get a free custom agent today.", "source": "tldr", "AI": {"tldr": "\u91d1\u878d\u516c\u53f8\u901a\u8fc7\u4f7f\u7528\u5bf9\u8bddAI\u4ee3\u7406\u5904\u7406\u7533\u8bf7\u8ddf\u8fdb\uff0c\u5c06\u5b8c\u6210\u7387\u4ece30%\u63d0\u5347\u81f3\u8fd160%\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u672a\u5b8c\u6210\u7533\u8bf7\u9020\u6210\u7684\u635f\u5931\u3002", "motivation": "\u91d1\u878d\u516c\u53f8\u6bcf\u5e74\u56e0\u672a\u5b8c\u6210\u7533\u8bf7\u635f\u59312.5\u4ebf\u7f8e\u5143\uff0c\u9700\u8981\u63d0\u9ad8\u7533\u8bf7\u5b8c\u6210\u7387\u4ee5\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u4f7f\u7528\u5bf9\u8bddAI\u4ee3\u7406\u6765\u5904\u7406\u7533\u8bf7\u8ddf\u8fdb\u5de5\u4f5c\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u4eba\u5de5\u5904\u7406\u65b9\u5f0f\u3002", "result": "\u7533\u8bf7\u5b8c\u6210\u7387\u4ece\u4e4b\u524d\u768430%\u63d0\u5347\u5230\u8fd160%\uff0c\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "AI\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u91d1\u878d\u7533\u8bf7\u7684\u5904\u7406\u6548\u7387\uff0c\u51cf\u5c11\u7ecf\u6d4e\u635f\u5931\u3002", "topic": "swe application"}}
{"id": "tldr.2511.c668654b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q4-aicodereviewlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019aa1a1335d-85d80474-e9fd-442a-9f64-c334d5e6fdb4-000000/S9Drvqy9STvofwTE_L_geS7F8DY0eL0l2CMvW7Gzhtg=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q4-aicodereviewlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019aa1a1335d-85d80474-e9fd-442a-9f64-c334d5e6fdb4-000000/S9Drvqy9STvofwTE_L_geS7F8DY0eL0l2CMvW7Gzhtg=432", "authors": ["TLDR Newsletter"], "title": "Your AI code reviewer doesn't know what actually breaks. Sentry's does.", "comment": "Source: TLDR Newsletter, Date: 2025-11-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q4-aicodereviewlaunch%26utm_content=newsletter-product-lp-learnmore/2/0100019aa1a1335d-85d80474-e9fd-442a-9f64-c334d5e6fdb4-000000/S9Drvqy9STvofwTE_L_geS7F8DY0eL0l2CMvW7Gzhtg=432", "summary": "Your AI code reviewer doesn't know what actually breaks. Sentry's does. (Sponsor) Generic AI code review catches typos, style issues, and things that might be a problem. Sentry's AI code reviewer (AKA Seer) knows exactly which changes will cause issues based on what's broken before, in your actual environment. Like most things AI, the secret is better context... \u2192 Sentry already monitors your production errors and performance. \u2192 Seer AI Code Review uses Sentry's data + your code + commit hist...", "source": "tldr", "AI": {"tldr": "Sentry\u7684AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177Seer\u57fa\u4e8e\u751f\u4ea7\u73af\u5883\u9519\u8bef\u6570\u636e\uff0c\u80fd\u591f\u7cbe\u51c6\u8bc6\u522b\u54ea\u4e9b\u4ee3\u7801\u53d8\u66f4\u4f1a\u5bfc\u81f4\u5b9e\u9645\u95ee\u9898\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u901a\u7528\u4ee3\u7801\u95ee\u9898", "motivation": "\u4f20\u7edfAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u53ea\u80fd\u53d1\u73b0\u8bed\u6cd5\u9519\u8bef\u548c\u98ce\u683c\u95ee\u9898\uff0c\u4f46\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u54ea\u4e9b\u53d8\u66f4\u4f1a\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u5f15\u53d1\u6545\u969c\u3002Sentry\u5229\u7528\u5176\u751f\u4ea7\u76d1\u63a7\u6570\u636e\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898", "method": "Seer\u7ed3\u5408Sentry\u7684\u751f\u4ea7\u9519\u8bef\u76d1\u63a7\u6570\u636e\u3001\u4ee3\u7801\u5e93\u548c\u63d0\u4ea4\u5386\u53f2\uff0c\u4e3aAI\u4ee3\u7801\u5ba1\u67e5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "\u8be5\u5de5\u5177\u80fd\u591f\u57fa\u4e8e\u5386\u53f2\u6545\u969c\u6570\u636e\uff0c\u7cbe\u786e\u8bc6\u522b\u53ef\u80fd\u5bfc\u81f4\u751f\u4ea7\u95ee\u9898\u7684\u4ee3\u7801\u53d8\u66f4", "conclusion": "\u5229\u7528\u751f\u4ea7\u73af\u5883\u76d1\u63a7\u6570\u636e\u53ef\u4ee5\u663e\u8457\u63d0\u5347AI\u4ee3\u7801\u5ba1\u67e5\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027", "topic": "swe application"}}
{"id": "tldr.2511.0b74443a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=26337115-TLDR%2520tech%2520Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrtech_secondary_1121%26utm_content=tldrtech_secondary_1121/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/W4P6nIdzhyE_x6dU-RasLIZ_37h8VWljzuTJoVQrFls=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=26337115-TLDR%2520tech%2520Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrtech_secondary_1121%26utm_content=tldrtech_secondary_1121/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/W4P6nIdzhyE_x6dU-RasLIZ_37h8VWljzuTJoVQrFls=432", "authors": ["TLDR Newsletter"], "title": "Tinkering with prompts can only get you so far.", "comment": "Source: TLDR Newsletter, Date: 2025-11-21, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fyou.com%2Flanding%2Fthe-evolution-of-agent-management%3Futm_campaign=26337115-TLDR%2520tech%2520Q4%26utm_source=external-newsletter%26utm_medium=email%26utm_term=tldrtech_secondary_1121%26utm_content=tldrtech_secondary_1121/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/W4P6nIdzhyE_x6dU-RasLIZ_37h8VWljzuTJoVQrFls=432", "summary": "Tinkering with prompts can only get you so far. (Sponsor) Most companies get stuck tinkering with prompts and wonder why their agents fail to deliver dependable results. This guide from You.com breaks down the evolution of agent management, revealing the five stages for building a successful AI agent and why most organizations haven't gotten there yet. If you're ready to go beyond the prompt, this is the playbook for you.", "source": "tldr", "AI": {"tldr": "\u8be5\u6307\u5357\u4ecb\u7ecd\u4e86\u6784\u5efa\u6210\u529fAI\u4ee3\u7406\u7684\u4e94\u4e2a\u53d1\u5c55\u9636\u6bb5\uff0c\u5f3a\u8c03\u4ec5\u9760\u8c03\u6574\u63d0\u793a\u8bcd\u65e0\u6cd5\u5b9e\u73b0\u53ef\u9760\u7ed3\u679c\uff0c\u9700\u8981\u8d85\u8d8a\u63d0\u793a\u8bcd\u9636\u6bb5\u3002", "motivation": "\u5927\u591a\u6570\u516c\u53f8\u505c\u7559\u5728\u8c03\u6574\u63d0\u793a\u8bcd\u9636\u6bb5\uff0c\u5bfc\u81f4AI\u4ee3\u7406\u65e0\u6cd5\u63d0\u4f9b\u53ef\u9760\u7ed3\u679c\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u4ee3\u7406\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e94\u4e2a\u53d1\u5c55\u9636\u6bb5\u6765\u6784\u5efa\u6210\u529f\u7684AI\u4ee3\u7406\uff0c\u4ece\u57fa\u7840\u63d0\u793a\u8bcd\u8c03\u6574\u5230\u66f4\u9ad8\u7ea7\u7684\u4ee3\u7406\u7ba1\u7406\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u5927\u591a\u6570\u7ec4\u7ec7\u5c1a\u672a\u8fbe\u5230\u9ad8\u7ea7\u4ee3\u7406\u7ba1\u7406\u9636\u6bb5\uff0c\u63d0\u4f9b\u4e86\u8d85\u8d8a\u63d0\u793a\u8bcd\u7684\u5b9e\u7528\u6307\u5357\u3002", "conclusion": "\u8981\u6784\u5efa\u6210\u529f\u7684AI\u4ee3\u7406\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u63d0\u793a\u8bcd\u8c03\u6574\uff0c\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u4ee3\u7406\u7ba1\u7406\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.6300f171", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrnewsletter/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/ob9RbeQ-4wRmElIJIAlHRXITe8e-R3Vlym45Pi-VwqE=432", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrnewsletter/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/ob9RbeQ-4wRmElIJIAlHRXITe8e-R3Vlym45Pi-VwqE=432", "authors": ["TLDR Newsletter"], "title": "How to write a great agents.md: Lessons from over 2,500 repositories", "comment": "Source: TLDR Newsletter, Date: 2025-11-21, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrnewsletter/1/0100019aa627c8eb-7272241c-9285-4d00-aedf-7a931c339d69-000000/ob9RbeQ-4wRmElIJIAlHRXITe8e-R3Vlym45Pi-VwqE=432", "summary": "How to write a great agents.md: Lessons from over 2,500 repositories (12 minute read) GitHub's agents.md feature allows developers to create agents with custom instructions. agents.md is where developers define all of the specifics, provide code style examples, and set clear boundaries of what not to do. Most agent files fail because they're too vague. The best files provide agents with a clear job or persona, exact commands to run, well-defined boundaries to follow, and clear examples of goo...", "source": "tldr", "AI": {"tldr": "GitHub\u7684agents.md\u529f\u80fd\u5141\u8bb8\u5f00\u53d1\u8005\u521b\u5efa\u5177\u6709\u81ea\u5b9a\u4e49\u6307\u4ee4\u7684\u4ee3\u7406\u3002\u5927\u591a\u6570\u4ee3\u7406\u6587\u4ef6\u5931\u8d25\u662f\u56e0\u4e3a\u8fc7\u4e8e\u6a21\u7cca\uff0c\u800c\u6700\u4f73\u6587\u4ef6\u4e3a\u4ee3\u7406\u63d0\u4f9b\u6e05\u6670\u7684\u5de5\u4f5c\u6216\u89d2\u8272\u3001\u786e\u5207\u7684\u8fd0\u884c\u547d\u4ee4\u3001\u660e\u786e\u5b9a\u4e49\u7684\u8fb9\u754c\u4ee5\u53ca\u826f\u597d\u7684\u4ee3\u7801\u793a\u4f8b\u3002", "motivation": "\u5206\u6790\u8d85\u8fc72500\u4e2a\u4ee3\u7801\u5e93\u540e\u53d1\u73b0\uff0c\u5927\u591a\u6570agents.md\u6587\u4ef6\u7531\u4e8e\u5185\u5bb9\u6a21\u7cca\u800c\u5931\u8d25\uff0c\u9700\u8981\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u521b\u5efa\u6709\u6548\u4ee3\u7406\u6587\u4ef6\u7684\u6307\u5bfc\u539f\u5219\u3002", "method": "\u901a\u8fc7\u5206\u67902500\u591a\u4e2aGitHub\u4ee3\u7801\u5e93\u4e2d\u7684agents.md\u6587\u4ef6\uff0c\u603b\u7ed3\u6210\u529f\u548c\u5931\u8d25\u6848\u4f8b\u7684\u6a21\u5f0f\uff0c\u63d0\u70bc\u51fa\u521b\u5efa\u9ad8\u8d28\u91cf\u4ee3\u7406\u6587\u4ef6\u7684\u5173\u952e\u8981\u7d20\u3002", "result": "\u53d1\u73b0\u6210\u529f\u7684agents.md\u6587\u4ef6\u901a\u5e38\u5305\u542b\uff1a\u6e05\u6670\u7684\u4ee3\u7406\u89d2\u8272\u5b9a\u4e49\u3001\u786e\u5207\u7684\u547d\u4ee4\u6307\u4ee4\u3001\u660e\u786e\u7684\u8fb9\u754c\u9650\u5236\u4ee5\u53ca\u5177\u4f53\u7684\u4ee3\u7801\u98ce\u683c\u793a\u4f8b\u3002", "conclusion": "\u7f16\u5199\u6709\u6548\u7684agents.md\u6587\u4ef6\u9700\u8981\u63d0\u4f9b\u5177\u4f53\u3001\u660e\u786e\u7684\u6307\u5bfc\uff0c\u907f\u514d\u6a21\u7cca\u63cf\u8ff0\uff0c\u786e\u4fdd\u4ee3\u7406\u80fd\u591f\u51c6\u786e\u7406\u89e3\u548c\u6267\u884c\u5f00\u53d1\u8005\u7684\u610f\u56fe\u3002", "topic": "agent analysis"}}
{"id": "wechat.2511.3351dc56", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0Nzc3MTQyMw==&mid=2247536204&idx=1&sn=f9224c857711bfeab1cc7c493c1d079f&chksm=e8fc0544bb5ee81c62091742d7d565c016e86068c2c587eb5d7f84800c421b1449b6f41d2a28#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0Nzc3MTQyMw==&mid=2247536204&idx=1&sn=f9224c857711bfeab1cc7c493c1d079f&chksm=e8fc0544bb5ee81c62091742d7d565c016e86068c2c587eb5d7f84800c421b1449b6f41d2a28#rd", "authors": ["\u8682\u8681\u6280\u672fAntTech"], "title": "\u9ad8\u6027\u80fd<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6743\u91cd\u4ea4\u6362\u6846\u67b6 Awex \u5f00\u6e90\uff0c\u77ac\u606f\u5343\u91cc\uff0c\u4e3e\u91cd\u82e5\u8f7b", "comment": "Source: WeChat, Published: 2025-11-21 12:30:31", "summary": "\u8fc7\u53bb\u51e0\u5e74\uff0c\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u63a8\u52a8\u5927\u6a21\u578b\u8fb9\u754c\u6269\u5f20\u7684\u6838\u5fc3\u6280\u672f\u3002\u4ece ChatGPT \u7684 RLHF\uff0c\u5230 DeepSeek\u3001Claude\u3001Llama \u7684\u540e\u8bad\u7ec3\u4f53\u7cfb\uff0c\u65e0\u4e0d\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u8ba9\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3001\u5177\u5907\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u3002", "AI": {"tldr": "\u8fc7\u53bb\u51e0\u5e74\uff0c\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u63a8\u52a8\u5927\u6a21\u578b\u8fb9\u754c\u6269\u5f20\u7684\u6838\u5fc3\u6280\u672f\u3002\u4ece ChatGPT \u7684 RLHF\uff0c\u5230 DeepSeek\u3001Claude\u3001Llama \u7684\u540e\u8bad\u7ec3\u4f53\u7cfb\uff0c\u65e0\u4e0d\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u8ba9\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3001\u5177\u5907\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.6db8b641", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449997274&idx=1&sn=159ffb0076dfb45056e7b2b0b9e7f99c&chksm=b054ec2e5433340cdfffd33584a22829dd9c8705348a8c5ca6681fac75067e562e1b322c8dc2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449997274&idx=1&sn=159ffb0076dfb45056e7b2b0b9e7f99c&chksm=b054ec2e5433340cdfffd33584a22829dd9c8705348a8c5ca6681fac75067e562e1b322c8dc2#rd", "authors": ["\u667a\u7329\u7329AI"], "title": "\u5168\u9762\u8d85\u8d8aGRPO\uff01\u963f\u91cc\u63d0\u51fa\u65b0\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u589e\u5f3aLLM\u548cMLLM\u63a8\u7406\u80fd\u529b", "comment": "Source: WeChat, Published: 2025-11-21 11:42:41", "summary": "\uff082\uff09\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6309\u96be\u5ea6\u5bf9\u6570\u636e\u96c6 D \u8fdb\u884c\u6392\u5e8f\u540e\uff0c\u5c06\u5176\u5212\u5206\u4e3a K \u4e2a\u5927\u5c0f\u76f8\u7b49\u7684\u8fde\u7eed\u6876 {B1\uff0cB2\uff0c\u2026\uff0cBK}\uff1a\u5176\u4e2d\uff0c q1\uff0c\u2026\uff0cq\u2223D\u2223 \u6309\u4ece\u6613\u5230\u96be\u7684\u987a\u5e8f\u6392\u5217\u3002\u5f53\u524d\u7684\u8bad\u7ec3\u5b50\u96c6 Dc \u521d\u59cb\u5316\u4e3a\u7b2c\u4e00\u4e2a\u6876 B1\u3002", "AI": {"tldr": "\uff082\uff09\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6309\u96be\u5ea6\u5bf9\u6570\u636e\u96c6 D \u8fdb\u884c\u6392\u5e8f\u540e\uff0c\u5c06\u5176\u5212\u5206\u4e3a K \u4e2a\u5927\u5c0f\u76f8\u7b49\u7684\u8fde\u7eed\u6876 {B1\uff0cB2\uff0c\u2026\uff0cBK}\uff1a\u5176\u4e2d\uff0c q1\uff0c\u2026\uff0cq\u2223D\u2223 \u6309\u4ece\u6613\u5230\u96be\u7684\u987a\u5e8f\u6392\u5217\u3002\u5f53\u524d\u7684\u8bad\u7ec3\u5b50\u96c6 Dc \u521d\u59cb\u5316\u4e3a\u7b2c\u4e00\u4e2a\u6876 B1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.09fe792b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3ODQ2NzMwNA==&mid=2247491211&idx=8&sn=d70738cfec5e008310fac500adf886c5&chksm=ce5c24ded72a9f24356d548f63be0abfb5916efaf88facb209b01a95775e57726a424d88824a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3ODQ2NzMwNA==&mid=2247491211&idx=8&sn=d70738cfec5e008310fac500adf886c5&chksm=ce5c24ded72a9f24356d548f63be0abfb5916efaf88facb209b01a95775e57726a424d88824a#rd", "authors": ["\u6df1\u5ea6\u4e4b\u773c\u8d44\u6599\u5e93"], "title": "\u300c\u6ce8\u610f\u529b\u673a\u5236+<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u300d\u91cd\u78c5\u7a81\u7834\uff01\u8363\u767bScience\u9876\u7ea7\u5b50\u520a\uff01", "comment": "Source: WeChat, Published: 2025-11-21 10:11:00", "summary": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3 pipeline\uff0c\u5148\u5728\u57fa\u7840\u5730\u5f62\u4e0a\u521d\u59cb\u5316\u5730\u56fe\u7f16\u7801\u5b66\u4e60\uff0c\u518d\u5f15\u5165\u590d\u6742\u5730\u5f62\u4e0e\u4e0d\u786e\u5b9a\u6027\u5fae\u8c03\uff0c\u517c\u987e\u6cdb\u5316\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u3002\u6784\u5efa\u7aef\u5230\u7aef\u7684\u6574\u4f53\u63a7\u5236\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b49\u4e0a\u5c42\u89c4\u5212\u6a21\u5757\uff0c\u76f4\u63a5\u5c06\u611f\u77e5\u4fe1\u606f\u6620\u5c04\u4e3a\u5173", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3 pipeline\uff0c\u5148\u5728\u57fa\u7840\u5730\u5f62\u4e0a\u521d\u59cb\u5316\u5730\u56fe\u7f16\u7801\u5b66\u4e60\uff0c\u518d\u5f15\u5165\u590d\u6742\u5730\u5f62\u4e0e\u4e0d\u786e\u5b9a\u6027\u5fae\u8c03\uff0c\u517c\u987e\u6cdb\u5316\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u3002\u6784\u5efa\u7aef\u5230\u7aef\u7684\u6574\u4f53\u63a7\u5236\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b49\u4e0a\u5c42\u89c4\u5212\u6a21\u5757\uff0c\u76f4\u63a5\u5c06\u611f\u77e5\u4fe1\u606f\u6620\u5c04\u4e3a\u5173", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.6a507356", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247572513&idx=2&sn=1a3f20f9063fdcfc14b0b478d19c7fe7&chksm=96a0c35bac9ea0ffa860076ecd055f39c733903680ea9232e591fdd2bddabef2436f1da6a387#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247572513&idx=2&sn=1a3f20f9063fdcfc14b0b478d19c7fe7&chksm=96a0c35bac9ea0ffa860076ecd055f39c733903680ea9232e591fdd2bddabef2436f1da6a387#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "DRL\u6700\u65b0\u5f69\u8272\u4e2d\u6587\u7248-\u300a\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em> \u57fa\u7840\u3001\u7814\u7a76\u4e0e\u5e94\u7528\u300b\u514d\u8d39\u4e66\u7c4d\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-11-21 09:00:00", "summary": "\u672c\u4e66\u76ee\u5f55 \u5185\u5bb9\u622a\u56fe\u672c\u4e66\u514d\u8d39pdf\u4e0b\u8f7d\u5730\u5740", "AI": {"tldr": "\u672c\u4e66\u76ee\u5f55 \u5185\u5bb9\u622a\u56fe\u672c\u4e66\u514d\u8d39pdf\u4e0b\u8f7d\u5730\u5740", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.26459ab9", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNzU3MDI3Nw==&mid=2247508744&idx=1&sn=76ed7d46de55040b15e8c36fab0af50e&chksm=c388bd7324fca000b601e5aaca4ed87fac9e2b7002a74a32a3c78d9e3053dc5c6347c8c9c7d5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNzU3MDI3Nw==&mid=2247508744&idx=1&sn=76ed7d46de55040b15e8c36fab0af50e&chksm=c388bd7324fca000b601e5aaca4ed87fac9e2b7002a74a32a3c78d9e3053dc5c6347c8c9c7d5#rd", "authors": ["\u5c0f\u7c73\u6c7d\u8f66"], "title": "\u300cXiaomi HAD \u589e\u5f3a\u7248\u300d\u4e09\u95ee\uff1a\u4ec0\u4e48\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1f\u4ec0\u4e48\u662f\u4e16\u754c\u6a21\u578b\uff1f\u4f53\u9a8c\u5347\u7ea7\u5728\u54ea\u91cc\uff1f", "comment": "Source: WeChat, Published: 2025-11-21 07:51:28", "summary": "\u505a\u597d\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6709\u4e24\u4e2a\uff0c\u4e00\u4e2a\u662f\u9ad8\u4fdd\u771f\u7684\u4e16\u754c\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u5728\u865a\u62df\u73af\u5883\u4e2d\u4e5f\u80fd\u50cf\u5728\u771f\u5b9e\u9053\u8def\u4e0a\u4e00\u6837\u5b66\u4e60\uff1b\u53e6\u4e00\u4e2a\u662f\u9ad8\u6548\u7387\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u8ba9\u6a21\u578b\u4ee5\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u8d28\u91cf\u79ef\u7d2f\u7ecf\u9a8c\u3001\u6301\u7eed\u4f18\u5316\u3002", "AI": {"tldr": "\u505a\u597d\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6709\u4e24\u4e2a\uff0c\u4e00\u4e2a\u662f\u9ad8\u4fdd\u771f\u7684\u4e16\u754c\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u5728\u865a\u62df\u73af\u5883\u4e2d\u4e5f\u80fd\u50cf\u5728\u771f\u5b9e\u9053\u8def\u4e0a\u4e00\u6837\u5b66\u4e60\uff1b\u53e6\u4e00\u4e2a\u662f\u9ad8\u6548\u7387\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u8ba9\u6a21\u578b\u4ee5\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u8d28\u91cf\u79ef\u7d2f\u7ecf\u9a8c\u3001\u6301\u7eed\u4f18\u5316\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.8111d248", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNTA1MjA0NQ==&mid=2247484117&idx=1&sn=1cb7de00f71a6eb5ce9824d88a87893e&chksm=f1d87c47cbe13fec3e83e505fd240a92f75eeff58d8a1344e1af8ad05c593cbf4cfc55ee24f9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNTA1MjA0NQ==&mid=2247484117&idx=1&sn=1cb7de00f71a6eb5ce9824d88a87893e&chksm=f1d87c47cbe13fec3e83e505fd240a92f75eeff58d8a1344e1af8ad05c593cbf4cfc55ee24f9#rd", "authors": ["AI\u6b21\u751f\u4ee3"], "title": "\u5efa\u8bae\u6536\u85cf\uff01\u4e00\u6587\u62c6\u89e3 <em class=\"highlight\">Agentic</em> AI \u7684 7 \u5927\u6838\u5fc3\u7ec4\u4ef6\uff01", "comment": "Source: WeChat, Published: 2025-11-21 12:48:27", "summary": "\u6267\u884c\uff08Execution\uff09\u5c42\u662f Agentic AI \u4e0e\u6570\u5b57\u4e16\u754c\u751a\u81f3\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u6865\u6881\u3002\u5b83\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u6587\u672c\uff0c\u66f4\u662f\u901a\u8fc7\u201c\u5de5\u5177\u8c03\u7528\u201d\u548c\u201cAPI \u8c03\u7528\u201d\u6765\u64cd\u4f5c\u5916\u90e8\u8f6f\u4ef6\u3002", "AI": {"tldr": "\u6267\u884c\uff08Execution\uff09\u5c42\u662f Agentic AI \u4e0e\u6570\u5b57\u4e16\u754c\u751a\u81f3\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u6865\u6881\u3002\u5b83\u4e0d\u4ec5\u4ec5\u662f\u751f\u6210\u6587\u672c\uff0c\u66f4\u662f\u901a\u8fc7\u201c\u5de5\u5177\u8c03\u7528\u201d\u548c\u201cAPI \u8c03\u7528\u201d\u6765\u64cd\u4f5c\u5916\u90e8\u8f6f\u4ef6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.a01a4b4a", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MjQyMTA0Mw==&mid=2247484599&idx=1&sn=64196fef572b01ceea0081e2e8a3ae2d&chksm=fdd1583c6fd3fe0d71217b4038dfd885e1d3342cda958b734f76adef2b76b72d4c77ebcbe86a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MjQyMTA0Mw==&mid=2247484599&idx=1&sn=64196fef572b01ceea0081e2e8a3ae2d&chksm=fdd1583c6fd3fe0d71217b4038dfd885e1d3342cda958b734f76adef2b76b72d4c77ebcbe86a#rd", "authors": ["\u7ea2\u661f\u6d1e\u5bdf\u8005"], "title": "Agent\u7b2c02\u96c6\uff1a<em class=\"highlight\">Agentic</em> AI\u8ddf\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7684\u5dee\u522b", "comment": "Source: WeChat, Published: 2025-11-21 12:00:13", "summary": "Agentic AI\u4ee3\u8868\u7740AI\u4ece\u201c\u5de5\u5177\u201d\u5230\u201c\u5408\u4f5c\u4f19\u4f34\u201d\u7684\u8d28\u53d8\u2014\u2014\u5b83\u4e0d\u518d\u53ea\u662f\u542c\u4ece\u6307\u4ee4\uff0c\u800c\u662f\u80fd\u591f\u72ec\u5f53\u4e00\u9762\u5b8c\u6210\u4efb\u52a1\u3002\u4e8c\u3001\u56e2\u961f\u534f\u4f5c\u827a\u672f\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53c8\u662f\u4ec0\u4e48\uff1f", "AI": {"tldr": "Agentic AI\u4ee3\u8868\u7740AI\u4ece\u201c\u5de5\u5177\u201d\u5230\u201c\u5408\u4f5c\u4f19\u4f34\u201d\u7684\u8d28\u53d8\u2014\u2014\u5b83\u4e0d\u518d\u53ea\u662f\u542c\u4ece\u6307\u4ee4\uff0c\u800c\u662f\u80fd\u591f\u72ec\u5f53\u4e00\u9762\u5b8c\u6210\u4efb\u52a1\u3002\u4e8c\u3001\u56e2\u961f\u534f\u4f5c\u827a\u672f\uff1a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53c8\u662f\u4ec0\u4e48\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.59a409b3", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NTgwMzkyMw==&mid=2650475358&idx=1&sn=c2cd99c1912eea99b1b6b9b587a5592b&chksm=865ef0cc5bddcc1e4803e16284e099b6f7f75e9c6dc2a2d6c31fe1bb97e3e1be9d6ede516888#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NTgwMzkyMw==&mid=2650475358&idx=1&sn=c2cd99c1912eea99b1b6b9b587a5592b&chksm=865ef0cc5bddcc1e4803e16284e099b6f7f75e9c6dc2a2d6c31fe1bb97e3e1be9d6ede516888#rd", "authors": ["\u6613\u7c73\u4e91\u901a"], "title": "AI\u6d1e\u5bdf | \u7ea2\u6749\u4e2d\u56fd\uff1a\u201c\u4ece\u5de5\u5177\u5230\u4f19\u4f34\u201d\uff0c\u7528<em class=\"highlight\">Agentic</em> AI\u5f00\u542f\u4f01\u4e1a\u6570\u667a\u5316\u8f6c\u578b\u7684\u201c\u65e0\u9650\u6e38\u620f\u201d\u7684\u4e0b\u4e00\u7a0b", "comment": "Source: WeChat, Published: 2025-11-21 09:29:20", "summary": "\u6838\u5fc3\u65f6\u4ee3\u7279\u5f81\uff1aAgentic AI \u5f00\u542f \u201c\u4eba\u673a\u4f19\u4f34\u201d \u65b0\u7eaa\u5143\u62a5\u544a\u6700\u6838\u5fc3\u7684\u6d1e\u5bdf\u662f\u4f01\u4e1a\u6570\u667a\u5316\u6b63\u5f0f\u8fc8\u5165 Agentic AI\uff08\u81ea\u6cbb\u578b AI\uff09\u65f6\u4ee3\uff0c\u6807\u5fd7\u7740 AI \u4ece\u8f85\u52a9 \u201c\u5de5\u5177\u201d \u5411\u534f\u540c \u201c\u4f19\u4f34\u201d \u7684\u672c\u8d28\u8dc3\u8fc1\u3002", "AI": {"tldr": "\u6838\u5fc3\u65f6\u4ee3\u7279\u5f81\uff1aAgentic AI \u5f00\u542f \u201c\u4eba\u673a\u4f19\u4f34\u201d \u65b0\u7eaa\u5143\u62a5\u544a\u6700\u6838\u5fc3\u7684\u6d1e\u5bdf\u662f\u4f01\u4e1a\u6570\u667a\u5316\u6b63\u5f0f\u8fc8\u5165 Agentic AI\uff08\u81ea\u6cbb\u578b AI\uff09\u65f6\u4ee3\uff0c\u6807\u5fd7\u7740 AI \u4ece\u8f85\u52a9 \u201c\u5de5\u5177\u201d \u5411\u534f\u540c \u201c\u4f19\u4f34\u201d \u7684\u672c\u8d28\u8dc3\u8fc1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.17360769", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk5MDkzMjY3NA==&mid=2247486075&idx=1&sn=ed1ba683051beb2ba94400bf59fbf704&chksm=c47f9b3709a158530aea4ca8892be3f8d6403a55273c46fc2a9a15bf7051fb1b095177f6aa6d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk5MDkzMjY3NA==&mid=2247486075&idx=1&sn=ed1ba683051beb2ba94400bf59fbf704&chksm=c47f9b3709a158530aea4ca8892be3f8d6403a55273c46fc2a9a15bf7051fb1b095177f6aa6d#rd", "authors": ["\u5927\u6a21\u578b\u77e5\u77e5"], "title": "Google\u795e\u4f5c\uff0c<em class=\"highlight\">Agentic</em>\u8bbe\u8ba1\u7ec8\u6781\u5b9d\u5178\u4e2d\u6587\u7248\u6765\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-11-21 08:41:24", "summary": "Google\u795e\u4f5c\uff0cAgentic\u8bbe\u8ba1\u7ec8\u6781\u5b9d\u5178\u4e2d\u6587\u7248\u6765\u4e86\uff01\u7531\u8c37\u6b4c\u5de5\u7a0b\u5e08Antonio Gulli\u64b0\u5199\uff0c\u5173\u4e8e\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u7684\u5b9e\u6218\u6307\u5357\u3002\u5168\u4e66\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e8621\u79cd\u667a\u80fd\u4f53\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u53ef\u9760\u3001\u9ad8\u6548\u7684\u667a\u80fd\u7cfb\u7edf\u3002\u7ed3\u5408LangChain\u3001CrewAI\u3001Google Agent Deve", "AI": {"tldr": "Google\u795e\u4f5c\uff0cAgentic\u8bbe\u8ba1\u7ec8\u6781\u5b9d\u5178\u4e2d\u6587\u7248\u6765\u4e86\uff01\u7531\u8c37\u6b4c\u5de5\u7a0b\u5e08Antonio Gulli\u64b0\u5199\uff0c\u5173\u4e8e\u6784\u5efa\u667a\u80fd\u7cfb\u7edf\u7684\u5b9e\u6218\u6307\u5357\u3002\u5168\u4e66\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u4e8621\u79cd\u667a\u80fd\u4f53\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u65e8\u5728\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u53ef\u9760\u3001\u9ad8\u6548\u7684\u667a\u80fd\u7cfb\u7edf\u3002\u7ed3\u5408LangChain\u3001CrewAI\u3001Google Agent Deve", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.34af2d7c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNzY3MDYwMA==&mid=2247488086&idx=1&sn=3de61936dd6c485eabf4c5aa96832651&chksm=c3bcce7756662b577d3d6cb5393b6d21fdc968a567301ba551a75a8164e4fe57cc4ecd383e9b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNzY3MDYwMA==&mid=2247488086&idx=1&sn=3de61936dd6c485eabf4c5aa96832651&chksm=c3bcce7756662b577d3d6cb5393b6d21fdc968a567301ba551a75a8164e4fe57cc4ecd383e9b#rd", "authors": ["\u5c0f\u5b89\u8f6f\u670d"], "title": "\u884c\u4e1a\u70ed\u70b9\u4e28\u4ece \u201c\u7b97\u529b\u4f9b\u7ed9\u201d \u5230 \u201c\u667a\u80fd\u9a71\u52a8\u201d\uff1a<em class=\"highlight\">Agentic</em> HPC \u5f00\u542f\u521b\u65b0\u8303\u5f0f", "comment": "Source: WeChat, Published: 2025-11-21 03:45:08", "summary": "Agentic HPC \u5e73\u53f0Altair HPCWorks \u5e73\u53f0\u52a9\u529b\u4f01\u4e1a\u5feb\u901f\u6355\u6349\u6838\u5fc3\u4ef7\u503c\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u5316\u3001\u81ea\u4f18\u5316\u9ad8\u6027\u80fd\u8ba1\u7b97\u505a\u597d\u51c6\u5907\u3002\u8be5\u5e73\u53f0\u5145\u5206\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u529f\u80fd\uff08\u5305\u62ecAI\u9a71\u52a8\u7684\u5185\u5b58\u8d44\u6e90\u9884\u6d4b\uff09\uff0c\u7cbe\u7b80\u4f5c\u4e1a\u63d0\u4ea4\u6d41\u7a0b\uff0c\u4f18\u5316\u96c6\u7fa4\u3001\u4e91\u53ca\u6df7\u5408\u8ba1\u7b97\u73af\u5883\u7684", "AI": {"tldr": "Agentic HPC \u5e73\u53f0Altair HPCWorks \u5e73\u53f0\u52a9\u529b\u4f01\u4e1a\u5feb\u901f\u6355\u6349\u6838\u5fc3\u4ef7\u503c\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u5316\u3001\u81ea\u4f18\u5316\u9ad8\u6027\u80fd\u8ba1\u7b97\u505a\u597d\u51c6\u5907\u3002\u8be5\u5e73\u53f0\u5145\u5206\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u529f\u80fd\uff08\u5305\u62ecAI\u9a71\u52a8\u7684\u5185\u5b58\u8d44\u6e90\u9884\u6d4b\uff09\uff0c\u7cbe\u7b80\u4f5c\u4e1a\u63d0\u4ea4\u6d41\u7a0b\uff0c\u4f18\u5316\u96c6\u7fa4\u3001\u4e91\u53ca\u6df7\u5408\u8ba1\u7b97\u73af\u5883\u7684", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.aade23d8", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMjkwMjk3Mw==&mid=2247486698&idx=1&sn=d4e31866439163837e89de5b8237f08b&chksm=c35fc92d43fec81d03c7e574f44280fecb929fb1821f0311ab3f25373aecc93d5d1d93e03007#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMjkwMjk3Mw==&mid=2247486698&idx=1&sn=d4e31866439163837e89de5b8237f08b&chksm=c35fc92d43fec81d03c7e574f44280fecb929fb1821f0311ab3f25373aecc93d5d1d93e03007#rd", "authors": ["AI\u5927\u6a21\u578b\u89c2\u5bdf\u7ad9"], "title": "\u5982\u4f55\u7528 LangGraph \u6784\u5efa\u9ad8\u6548\u7684 <em class=\"highlight\">Agentic</em> \u7cfb\u7edf", "comment": "Source: WeChat, Published: 2025-11-21 00:23:49", "summary": "\u9700\u8981 agentic \u6846\u67b6\u7684\u539f\u56e0\u662f\u5b83\u80fd\u62bd\u8c61\u6389\u8bb8\u591a\u4f60\u4e0d\u60f3\u4eb2\u81ea\u5904\u7406\u7684\u590d\u6742\u6027\uff1a\u7ef4\u62a4 state\uff08\u72b6\u6001\uff09\u3002\u4e0d\u4ec5\u662f\u6d88\u606f\u5386\u53f2\uff0c\u8fd8\u5305\u62ec\u6267\u884c RAG \u65f6\u6536\u96c6\u7684\u6240\u6709\u4fe1\u606f\uff08RAG \u4fdd\u7559\u82f1\u6587\u4ee5\u793a\u4e13\u4e1a\u672f\u8bed\uff09", "AI": {"tldr": "\u9700\u8981 agentic \u6846\u67b6\u7684\u539f\u56e0\u662f\u5b83\u80fd\u62bd\u8c61\u6389\u8bb8\u591a\u4f60\u4e0d\u60f3\u4eb2\u81ea\u5904\u7406\u7684\u590d\u6742\u6027\uff1a\u7ef4\u62a4 state\uff08\u72b6\u6001\uff09\u3002\u4e0d\u4ec5\u662f\u6d88\u606f\u5386\u53f2\uff0c\u8fd8\u5305\u62ec\u6267\u884c RAG \u65f6\u6536\u96c6\u7684\u6240\u6709\u4fe1\u606f\uff08RAG \u4fdd\u7559\u82f1\u6587\u4ee5\u793a\u4e13\u4e1a\u672f\u8bed\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.5ed6944c", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUxNDg5NjcxMA==&mid=2247483712&idx=1&sn=6f6da3c174e0596fceeac10156948285&chksm=f845fe2d16dfaf505158163fc11b05c5f1bc08c53e0bbbf486d2d64637067c06ffb23e64cae1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUxNDg5NjcxMA==&mid=2247483712&idx=1&sn=6f6da3c174e0596fceeac10156948285&chksm=f845fe2d16dfaf505158163fc11b05c5f1bc08c53e0bbbf486d2d64637067c06ffb23e64cae1#rd", "authors": ["MiTang\u7c91\u7c91"], "title": "\u4ece\u534f\u4f5c\u5230\u8fdb\u5316\uff1a<em class=\"highlight\">Agentic</em> AI \u7684\u4e94\u5927\u524d\u6cbf\u8d8b\u52bf", "comment": "Source: WeChat, Published: 2025-11-21 00:19:57", "summary": "2. \u590d\u7528\u73b0\u6709\u6a21\u5757\uff1a\u5c06\u53ef\u590d\u7528\u7684\u5b50\u667a\u80fd\u4f53\u4e0e\u5de5\u5177\uff08\u5982\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u5e93\u8fde\u63a5\u5668\uff09\u9002\u914d\u5230\u65b0\u4efb\u52a1\u30023. \u7ee7\u627f\u5e73\u53f0\u57fa\u7840\uff1a\u65b0\u667a\u80fd\u4f53\u81ea\u52a8\u5177\u5907\u5e73\u53f0\u7684\u6cbb\u7406\u3001\u76d1\u63a7\u4e0e\u5b89\u5168\u80fd\u529b\u3002", "AI": {"tldr": "2. \u590d\u7528\u73b0\u6709\u6a21\u5757\uff1a\u5c06\u53ef\u590d\u7528\u7684\u5b50\u667a\u80fd\u4f53\u4e0e\u5de5\u5177\uff08\u5982\u91d1\u878d\u9886\u57df\u7684\u6570\u636e\u5e93\u8fde\u63a5\u5668\uff09\u9002\u914d\u5230\u65b0\u4efb\u52a1\u30023. \u7ee7\u627f\u5e73\u53f0\u57fa\u7840\uff1a\u65b0\u667a\u80fd\u4f53\u81ea\u52a8\u5177\u5907\u5e73\u53f0\u7684\u6cbb\u7406\u3001\u76d1\u63a7\u4e0e\u5b89\u5168\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.7fa1c556", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwODE5NDg3NQ==&mid=2651288168&idx=1&sn=4ec32712cb1140eb8c8dbb57484ec503&chksm=814de1f52c22bb7877774efa8f9e4064c48ca4c4c83a3ee343412164e0b1f9659f918dd064ba#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwODE5NDg3NQ==&mid=2651288168&idx=1&sn=4ec32712cb1140eb8c8dbb57484ec503&chksm=814de1f52c22bb7877774efa8f9e4064c48ca4c4c83a3ee343412164e0b1f9659f918dd064ba#rd", "authors": ["\u7ea2\u6749\u6c47"], "title": "2025\u5e74\u5ea6\u70ed\u8bcd\u6765\u4e86\uff1aAI\u76f8\u5173\u8bcd\u6c47\u6b63\u9ad8\u9891\u8fdb\u5165\u6211\u4eec\u751f\u6d3b", "comment": "Source: WeChat, Published: 2025-11-21 00:03:34", "summary": "agentic2025\u5e74\uff0c\u201cagentic\u201d\u4e00\u8bcd\u5165\u56f4Dictionary.com\u5e74\u5ea6\u8bcd\u6c47\u5019\u9009\u8bcd\u540d\u5355\u3002\u8fd9\u539f\u672c\u662f\u4e00\u4e2a\u5fc3\u7406\u5b66\u548c\u793e\u4f1a\u5b66\u672f\u8bed\uff0c\u6307\u4eba\u7c7b\u72ec\u7acb\u884c\u52a8\u3001\u505a\u51fa\u9009\u62e9\u548c\u5851\u9020\u73af\u5883\u7684\u80fd\u529b\u3002", "AI": {"tldr": "agentic2025\u5e74\uff0c\u201cagentic\u201d\u4e00\u8bcd\u5165\u56f4Dictionary.com\u5e74\u5ea6\u8bcd\u6c47\u5019\u9009\u8bcd\u540d\u5355\u3002\u8fd9\u539f\u672c\u662f\u4e00\u4e2a\u5fc3\u7406\u5b66\u548c\u793e\u4f1a\u5b66\u672f\u8bed\uff0c\u6307\u4eba\u7c7b\u72ec\u7acb\u884c\u52a8\u3001\u505a\u51fa\u9009\u62e9\u548c\u5851\u9020\u73af\u5883\u7684\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.2a79ca51", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484334&idx=1&sn=e1f04af4770d3120e4ca5c0f848b0633&chksm=c012b64294e7c08e0e80afc3a920727e1f0a1927d74fd451a6e25777311842715d3cd8c1130a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484334&idx=1&sn=e1f04af4770d3120e4ca5c0f848b0633&chksm=c012b64294e7c08e0e80afc3a920727e1f0a1927d74fd451a6e25777311842715d3cd8c1130a#rd", "authors": ["AI Lab Dev"], "title": "<em class=\"highlight\">Agentic</em>21\u79cd\u8bbe\u8ba1\u6a21\u5f0f13-Human-in-the-Loop", "comment": "Source: WeChat, Published: 2025-11-21 00:01:31", "summary": "4. \u4eba\u673a\u534f\u4f5c\uff1a\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u76f8\u4e92\u534f\u4f5c\uff0c\u5404\u81ea\u53d1\u6325\u81ea\u8eab\u7684\u4f18\u52bf\uff1aAgent\u53ef\u4ee5\u5904\u7406\u5e38\u89c4\u7684\u6570\u636e\u5904\u7406\u4efb\u52a1\uff0c\u800c\u4eba\u7c7b\u5219\u8d1f\u8d23\u5904\u7406\u9700\u8981\u521b\u9020\u6027\u601d\u7ef4\u6216\u590d\u6742\u51b3\u7b56\u7684\u95ee\u9898\u3002", "AI": {"tldr": "4. \u4eba\u673a\u534f\u4f5c\uff1a\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u76f8\u4e92\u534f\u4f5c\uff0c\u5404\u81ea\u53d1\u6325\u81ea\u8eab\u7684\u4f18\u52bf\uff1aAgent\u53ef\u4ee5\u5904\u7406\u5e38\u89c4\u7684\u6570\u636e\u5904\u7406\u4efb\u52a1\uff0c\u800c\u4eba\u7c7b\u5219\u8d1f\u8d23\u5904\u7406\u9700\u8981\u521b\u9020\u6027\u601d\u7ef4\u6216\u590d\u6742\u51b3\u7b56\u7684\u95ee\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.c81ac071", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTk3ODI2OA==&mid=2650364324&idx=1&sn=aefe5338c93844a64be26ac8a1ffbd76&chksm=86f8f9fcbe68ab210b05f962cf3473b865670e68f1aa0344c6c7a630a5cb6c0365d3a29cd2c8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTk3ODI2OA==&mid=2650364324&idx=1&sn=aefe5338c93844a64be26ac8a1ffbd76&chksm=86f8f9fcbe68ab210b05f962cf3473b865670e68f1aa0344c6c7a630a5cb6c0365d3a29cd2c8#rd", "authors": ["\u673a\u5668\u5b66\u4e60AI\u7b97\u6cd5\u5de5\u7a0b"], "title": "\u8c37\u6b4c\u795e\u4f5c\uff0c<em class=\"highlight\">Agentic</em>\u8bbe\u8ba1\u7ec8\u6781\u5b9d\u5178\u6765\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-11-20 23:47:48", "summary": "\u626b\u7801\u56de\u590d\u201c\u667a\u80fd\u4f53\u8bbe\u8ba1\u201d\u514d\u8d39\u9886\u53d6\u539f\u8457&\u4e2d\u6587\u7248PDF\u5982\u679c\u4f60\u60f3\u5199\u5927\u6a21\u578b\u8bba\u6587\uff0c\u4f46\u5374\u6ca1\u6709\u5408\u9002\u7684idea\uff0c\u6211\u6536\u96c6\u6574\u7406\u4e86\u6765\u81eaQS\u524d50\u540d\u6821\u5927\u4f6c\u7684\u5927\u6a21\u578b\u7814\u7a76\u601d\u8def\uff01", "AI": {"tldr": "\u626b\u7801\u56de\u590d\u201c\u667a\u80fd\u4f53\u8bbe\u8ba1\u201d\u514d\u8d39\u9886\u53d6\u539f\u8457&\u4e2d\u6587\u7248PDF\u5982\u679c\u4f60\u60f3\u5199\u5927\u6a21\u578b\u8bba\u6587\uff0c\u4f46\u5374\u6ca1\u6709\u5408\u9002\u7684idea\uff0c\u6211\u6536\u96c6\u6574\u7406\u4e86\u6765\u81eaQS\u524d50\u540d\u6821\u5927\u4f6c\u7684\u5927\u6a21\u578b\u7814\u7a76\u601d\u8def\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.5fcb0612", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NTEwMDU5NA==&mid=2247484959&idx=1&sn=9e5be34375253b1249630f4b64db4c7e&chksm=cfa2de739436a6e945b5eb19cb7e881ed721f9c10269e83d1ad8fb92a3fe986dc78fc27cd3d7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NTEwMDU5NA==&mid=2247484959&idx=1&sn=9e5be34375253b1249630f4b64db4c7e&chksm=cfa2de739436a6e945b5eb19cb7e881ed721f9c10269e83d1ad8fb92a3fe986dc78fc27cd3d7#rd", "authors": ["\u65f6\u95f4\u7684\u670b\u53cb\u672d\u8bb0"], "title": "\u4e00\u8d77\u6765\u5b66\u4e60 -\u300a\u5434\u6069\u8fbe<em class=\"highlight\">Agentic</em> AI\u300b\uff08\u56db\uff09", "comment": "Source: WeChat, Published: 2025-11-20 23:00:48", "summary": "\u4e00\u8d77\u6765\u5b66\u4e60 -\u300a\u5434\u6069\u8fbeAgentic AI\u300b\uff08\u4e8c\uff09\uff1aAgentic AI \u4e2d\u7684\u53cd\u601d\u6a21\u5f0f\uff08Reflection\uff09 \u4e00\u8d77\u6765\u5b66\u4e60 -\u300a\u5434\u6069\u8fbeAgentic AI\u300b\uff08\u4e09\uff09\uff1aAgentic AI \u4e2d\u7684\u5de5\u5177\u8c03\u7528\u6a21\u5f0f\uff08Tool Use\uff09", "AI": {"tldr": "\u4e00\u8d77\u6765\u5b66\u4e60 -\u300a\u5434\u6069\u8fbeAgentic AI\u300b\uff08\u4e8c\uff09\uff1aAgentic AI \u4e2d\u7684\u53cd\u601d\u6a21\u5f0f\uff08Reflection\uff09 \u4e00\u8d77\u6765\u5b66\u4e60 -\u300a\u5434\u6069\u8fbeAgentic AI\u300b\uff08\u4e09\uff09\uff1aAgentic AI \u4e2d\u7684\u5de5\u5177\u8c03\u7528\u6a21\u5f0f\uff08Tool Use\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.29083701", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505061&idx=1&sn=469d315719ad1aba2e2f0b3862299da6&chksm=9a4ddbd2b94693b16bd2a2b13f4b1e422bd190bbae65034a633c8904dcf26fde59f6ee4467ac#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505061&idx=1&sn=469d315719ad1aba2e2f0b3862299da6&chksm=9a4ddbd2b94693b16bd2a2b13f4b1e422bd190bbae65034a633c8904dcf26fde59f6ee4467ac#rd", "authors": ["\u5173\u4e8eNLP\u90a3\u4e9b\u4f60\u4e0d\u77e5\u9053\u7684\u4e8b"], "title": "Google\u53d1\u5e03\uff01\u4e00\u6587\u4e86\u89e321\u79cd<em class=\"highlight\">Agentic</em>\u8bbe\u8ba1\u6a21\u5f0f", "comment": "Source: WeChat, Published: 2025-11-20 23:00:00", "summary": "\u626b\u7801\u56de\u590d\u201c\u667a\u80fd\u4f53\u8bbe\u8ba1\u201d\u514d\u8d39\u9886\u53d6\u539f\u8457&\u4e2d\u6587\u7248PDF\u5982\u679c\u4f60\u60f3\u5199\u5927\u6a21\u578b\u8bba\u6587\uff0c\u4f46\u5374\u6ca1\u6709\u5408\u9002\u7684idea\uff0c\u6211\u6536\u96c6\u6574\u7406\u4e86\u6765\u81eaQS\u524d50\u540d\u6821\u5927\u4f6c\u7684\u5927\u6a21\u578b\u7814\u7a76\u601d\u8def\uff01", "AI": {"tldr": "\u626b\u7801\u56de\u590d\u201c\u667a\u80fd\u4f53\u8bbe\u8ba1\u201d\u514d\u8d39\u9886\u53d6\u539f\u8457&\u4e2d\u6587\u7248PDF\u5982\u679c\u4f60\u60f3\u5199\u5927\u6a21\u578b\u8bba\u6587\uff0c\u4f46\u5374\u6ca1\u6709\u5408\u9002\u7684idea\uff0c\u6211\u6536\u96c6\u6574\u7406\u4e86\u6765\u81eaQS\u524d50\u540d\u6821\u5927\u4f6c\u7684\u5927\u6a21\u578b\u7814\u7a76\u601d\u8def\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
