<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 31]
- [cs.AI](#cs.AI) [Total: 35]
- [tldr.article](#tldr.article) [Total: 17]
- [cs.SE](#cs.SE) [Total: 32]
- [cs.LG](#cs.LG) [Total: 25]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LimAgents: Multi-Agent LLMs for Generating Research Limitations](https://arxiv.org/abs/2601.11578)
*Ibrahim Al Azher,Zhishuai Guo,Hamed Alhoori*

Main category: cs.CL

TL;DR: LimAgents是一个多智能体LLM框架，通过集成OpenReview评论、作者声明限制和引用文献，系统性地生成实质性研究局限性分析，相比零样本方法显著提升覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在识别研究局限性时存在表面化、泛化问题，通常只是重复作者声明的局限性，而忽略了更深层次的方法论问题和上下文差距。同时，许多作者只披露部分或琐碎的局限性，导致局限性分析不够全面。

Method: 提出LimAgents多智能体框架，包含多个专门角色的智能体：提取显式局限性、分析方法论差距、模拟同行评审视角、分析引用文献上下文。通过Judge智能体精炼输出，Master智能体整合成清晰集合。同时引入基于LLM-as-a-Judge的点评估协议来更准确衡量覆盖范围。

Result: 实验表明LimAgents显著提升性能：RAG+多智能体GPT-4o mini配置相比零样本基线获得+15.51%的覆盖增益，Llama 3 8B多智能体设置获得+4.41%的改进。

Conclusion: LimAgents框架能够系统性地识别显式、隐式、同行评审关注和文献背景下的局限性，为科学研究提供更全面、实质性的局限性分析，超越传统表面化方法。

Abstract: Identifying and articulating limitations is essential for transparent and rigorous scientific research. However, zero-shot large language models (LLMs) approach often produce superficial or general limitation statements (e.g., dataset bias or generalizability). They usually repeat limitations reported by authors without looking at deeper methodological issues and contextual gaps. This problem is made worse because many authors disclose only partial or trivial limitations. We propose LimAgents, a multi-agent LLM framework for generating substantive limitations. LimAgents integrates OpenReview comments and author-stated limitations to provide stronger ground truth. It also uses cited and citing papers to capture broader contextual weaknesses. In this setup, different agents have specific roles as sequential role: some extract explicit limitations, others analyze methodological gaps, some simulate the viewpoint of a peer reviewer, and a citation agent places the work within the larger body of literature. A Judge agent refines their outputs, and a Master agent consolidates them into a clear set. This structure allows for systematic identification of explicit, implicit, peer review-focused, and literature-informed limitations. Moreover, traditional NLP metrics like BLEU, ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They often overlook semantically similar limitations. To address this, we introduce a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage more accurately. Experiments show that LimAgents substantially improve performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51% coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup yields a +4.41% improvement.

</details>


### [2] [Entropic Context Shaping: Information-Theoretic Filtering for Context-Aware LLM Agents](https://arxiv.org/abs/2601.11585)
*Hyunjun Kim*

Main category: cs.CL

TL;DR: 本文提出了熵上下文塑造（ECS），一种基于信息论的框架，通过模型答案分布向正确答案的偏移来测量上下文效用，在细粒度上下文选择任务中显著优于传统词汇相似性方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理的上下文工程需要区分实用信息和误导性干扰。传统基于词汇相似性的方法无法捕捉语用效用——即段落是否真正有助于回答问题。

Method: 引入熵上下文塑造（ECS）框架，将效用形式化为答案概率的有符号变化，通过测量模型答案分布向正确答案的偏移来量化上下文效用。理论分析表明任务无关的更新产生接近零的分布偏移。

Result: 在LongMemEval（会话级）和LoCoMo（轮次级）基准测试中，ECS在细粒度轮次选择任务上，使用Llama-3.1-8B达到F1=0.265，相比TF-IDF（F1=0.154）有71.83%的相对提升。

Conclusion: ECS框架能够有效捕捉语用效用，在精确上下文选择任务中显著优于词汇相似性方法，为LLM代理的上下文工程提供了更有效的工具。

Abstract: Context engineering for large language model (LLM) agents requires distinguishing pragmatically useful information from misleading distractors. We introduce Entropic Context Shaping (ECS), an information-theoretic framework that measures context utility via the shift in the model's answer distribution toward the correct answer. Unlike lexical similarity methods that rely on word overlap, ECS captures pragmatic utility -- whether a passage actually helps answer the question. We formalize utility as the signed change in answer probability and provide theoretical analysis showing that task-irrelevant updates yield near-zero distribution shift. We evaluate on multi-turn context selection tasks using LongMemEval (session-level) and LoCoMo (turn-level) benchmarks. On fine-grained turn selection, ECS with Llama-3.1-8B achieves F1=0.265, a 71.83% relative improvement over TF-IDF (F1=0.154), demonstrating that pragmatic utility outperforms lexical similarity when precise context selection matters. Code and data are available in the supplementary materials.

</details>


### [3] [Towards AGI A Pragmatic Approach Towards Self Evolving Agent](https://arxiv.org/abs/2601.11658)
*Indrajit Kar,Sammy Zonunpuia,Zonunfeli Ralte*

Main category: cs.CL

TL;DR: 提出一个分层自进化多智能体框架，通过工具合成和进化算法使LLM智能体能够持续适应和自主扩展能力


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在部署后是静态的，缺乏自主扩展能力、生成新工具或进化推理的能力，需要实现持续自适应

Method: 分层自进化多智能体框架：基础LLM、操作SLM智能体、代码生成LLM和教师LLM。任务失败时升级到工具合成，持续失败时触发进化阶段（课程学习、基于奖励的学习或遗传算法进化）

Result: 在TaskCraft数据集上评估：课程学习实现快速恢复和强泛化，基于奖励的学习在高难度任务上表现优异，遗传算法提供高行为多样性。所有进化智能体都优于原始版本

Conclusion: 分层自进化框架实现了稳健、自主、自我改进的智能体进化，展示了智能体持续适应和扩展能力的可行性

Abstract: Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.

</details>


### [4] [ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System](https://arxiv.org/abs/2601.11854)
*Yifei Zhang,Hooshang Nayyeri,Rinat Khaziev,Emine Yilmaz,Gokhan Tur,Dilek Hakkani-Tür,Hari Thadakamalla*

Main category: cs.CL

TL;DR: ATOD是一个用于评估任务导向对话系统中智能体行为的基准测试和对话生成框架，包含ATOD-Eval评估框架和基于记忆的评估器


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对LLM驱动的任务导向对话系统中智能体行为（如多目标协调、长期推理、异步执行等）的系统性评估支持

Method: 提出ATOD基准测试和合成对话生成管道，创建需要长期推理的丰富标注对话；开发ATOD-Eval评估框架，将智能体维度转化为细粒度指标；提出基于记忆的评估器

Result: ATOD-Eval能够全面评估任务完成度、智能体能力和响应质量；提出的评估器在准确性和效率之间提供了更好的权衡

Conclusion: ATOD填补了现有基准测试在评估高级任务导向对话系统智能体行为方面的空白，为系统评估提供了全面框架

Abstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.

</details>


### [5] [LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding](https://arxiv.org/abs/2601.11913)
*Yichen Jiang,Peng Ye,Jiakang Yuan,Chongjun Tu,Lei Bai,Tao Chen*

Main category: cs.CL

TL;DR: 提出LSTM-MAS多智能体系统，模仿LSTM架构处理长文本，通过链式结构和门控机制减少错误积累和幻觉传播，在多个长文本理解任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM处理长文本存在挑战：单LLM方法需要减少上下文窗口或优化注意力机制，导致额外计算成本或受限的扩展上下文长度；多智能体框架虽能缓解这些限制，但仍易受错误积累和幻觉传播影响。

Method: 设计LSTM-MAS多智能体系统，模仿LSTM架构：采用链式结构组织智能体，每个节点包含worker agent（段级理解）、filter agent（冗余减少）、judge agent（持续错误检测）和manager agent（全局信息传播和保留调控），分别对应LSTM的输入门、遗忘门、恒定误差循环单元和输出门。

Result: 相比之前最佳多智能体方法CoA，在NarrativeQA、Qasper、HotpotQA和MuSiQue任务上分别提升40.93%、43.70%、121.57%和33.12%。

Conclusion: LSTM-MAS通过模仿LSTM的层次信息流和门控记忆机制，实现了可控信息传递和选择性长距离依赖建模，有效避免了错误积累和幻觉传播，显著提升了长文本理解性能。

Abstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.

</details>


### [6] [Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence](https://arxiv.org/abs/2601.11956)
*Yuyin Lu,Ziran Liang,Yanghui Rao,Wenqi Fan,Fu Lee Wang,Qing Li*

Main category: cs.CL

TL;DR: DoublyCal框架通过双重校准原则，使用轻量级代理模型生成知识图谱证据和校准的证据置信度，指导黑盒LLM产生更准确且置信度可追溯的预测


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉问题，现有知识图谱增强方法无法量化检索证据和LLM推理中的认知不确定性

Method: 基于双重校准原则，使用轻量级代理模型生成知识图谱证据并校准证据置信度，然后用校准后的证据指导黑盒LLM进行推理

Result: 在知识密集型基准测试中，DoublyCal显著提高了黑盒LLM的准确性和置信度校准，同时保持了较低的token成本

Conclusion: DoublyCal框架通过量化证据不确定性并建立可追溯的置信度，有效提高了LLM推理的可信度

Abstract: Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.

</details>


### [7] [PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning](https://arxiv.org/abs/2601.11957)
*Bingxuan Li,Jeonghwan Kim,Cheng Qian,Xiusi Chen,Eitan Anzenberg,Niran Kundapur,Heng Ji*

Main category: cs.CL

TL;DR: 该论文提出了CalConflictBench基准测试，用于评估语言代理在日历冲突解决中的表现，并开发了PEARL强化学习框架来提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 专业人士经常面临日历邀请冲突，需要反复决定参加、重新安排或拒绝哪些会议。自动化这一决策过程很重要但具有挑战性，因为调度工作耗时且人工委托难以扩展，因此需要研究语言代理能否有效管理时间。

Method: 首先引入CalConflictBench基准测试，以顺序方式呈现冲突并提供每轮反馈。然后提出PEARL强化学习框架，通过外部记忆模块和优化的轮次奖励设计来增强语言代理，使其能够逐步推断并适应用户偏好。

Result: 当前LLM代理表现不佳，如Qwen-3-30B-Think平均错误率达35%。PEARL框架实现了0.76的错误减少率，相比最强基线平均错误率提升55%。

Conclusion: 日历冲突解决是一个具有挑战性的长视野任务，当前语言代理表现有限，但通过PEARL这样的强化学习框架可以显著提升代理性能，使其能够更好地推断和适应用户偏好。

Abstract: Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.

</details>


### [8] [$\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models](https://arxiv.org/abs/2601.11969)
*Zecheng Tang,Baibei Ji,Ruoxi Sun,Haitian Wang,WangJie You,Zhang Yijun,Wenpeng Zhu,Ji Qi,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: MemoryRewardBench：首个系统研究奖励模型评估长时记忆管理能力的基准，涵盖8K-128K上下文长度，包含10种不同记忆模式，发现开源与闭源模型性能差距缩小。


<details>
  <summary>Details</summary>
Motivation: 随着记忆中心机制在长上下文处理中的广泛应用，有效的记忆管理成为LLM跨序列传播信息的关键能力。需要奖励模型来自动可靠地评估记忆质量，但目前缺乏系统研究这一能力的基准。

Method: 构建MemoryRewardBench基准，覆盖长上下文理解和长文本生成任务，包含10种不同记忆管理模式的设置，上下文长度从8K到128K tokens。在13个前沿奖励模型上进行评估。

Result: 评估显示开源与闭源模型性能差距正在缩小，新一代模型无论参数数量如何都持续超越前代。同时揭示了当前奖励模型在评估LLM记忆管理方面的能力和基本局限性。

Conclusion: MemoryRewardBench为系统研究奖励模型评估长时记忆管理能力提供了首个基准，揭示了模型发展趋势和现有局限性，为未来改进提供了方向。

Abstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.

</details>


### [9] [Conversational Context Classification: A Representation Engineering Approach](https://arxiv.org/abs/2601.12286)
*Jonathan Pan*

Main category: cs.CL

TL;DR: 该研究探索使用表征工程和单类支持向量机在LLM内部状态中识别特定上下文子空间，以检测模型是否偏离预期对话规范


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型日益普及，需要有效保障其操作安全，特别是检测模型生成脱离上下文回复的趋势，传统异常检测方法难以直接应用于上下文语义

Method: 使用表征工程和单类支持向量机，在LLM内部状态中识别代表特定上下文的子空间，通过在上下文示例上训练OCSVM，在LLM隐藏状态潜在空间中建立鲁棒边界，并在Llama和Qwen模型上进行评估

Result: 评估结果显示在识别特定上下文子空间方面取得了有希望的结果，能够有效检测对话线程是否在上下文范围内

Conclusion: 该方法不仅可用于检测对话是否在上下文范围内，还为更好地解释LLM的研究做出了贡献

Abstract: The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.

</details>


### [10] [Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies](https://arxiv.org/abs/2601.12369)
*Ming Zhang,Jiabao Zhuang,Wenqing Jing,Ziyu Kong,Jingyi Deng,Yujiong Shen,Kexin Tan,Yuhang Zhao,Ning Luo,Renzhe Zheng,Jiahui Lin,Mingqi Wu,Long Ma,Yi Zou,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: TaxoBench是一个用于评估深度研究代理能力的诊断基准，基于72篇高引用计算机科学综述构建，包含3,815个精确分类的引用作为基准。研究发现当前最佳代理只能召回20.9%的专家选择论文，即使有完美输入，最佳模型在组织结构方面也仅达到0.31 ARI，表明深度研究代理与专家级综述撰写仍有很大差距。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理越来越多地用于自动生成综述，但它们是否能像人类专家一样撰写综述尚不清楚。现有基准主要关注流畅性或引用准确性，但缺乏对核心能力（检索关键论文并将其组织成连贯知识结构）的评估。

Method: 从72篇高引用计算机科学综述中手动提取专家构建的分类树，包含3,815个精确分类的引用作为基准。支持两种评估模式：深度研究模式（给定主题测试端到端检索和组织能力）和自底向上模式（提供人类专家使用的确切论文，隔离组织结构能力）。评估了7个领先的深度研究代理和12个前沿LLM。

Result: 揭示了双重瓶颈：最佳代理只能召回20.9%的专家选择论文；即使有完美输入，最佳模型在组织结构方面也仅达到0.31 ARI（调整兰德指数）。当前深度研究代理与专家级综述撰写仍有很大差距。

Conclusion: 当前深度研究代理在检索关键论文和组织知识结构方面仍远未达到专家水平。TaxoBench基准为评估和改进深度研究代理提供了重要工具。

Abstract: Deep Research Agents are increasingly used for automated survey generation. However, whether they can write surveys like human experts remains unclear. Existing benchmarks focus on fluency or citation accuracy, but none evaluates the core capabilities: retrieving essential papers and organizing them into coherent knowledge structures. We introduce TaxoBench, a diagnostic benchmark derived from 72 highly-cited computer science surveys. We manually extract expert-authored taxonomy trees containing 3,815 precisely categorized citations as ground truth. Our benchmark supports two evaluation modes: Deep Research mode tests end-to-end retrieval and organization given only a topic, while Bottom-Up mode isolates structuring capability by providing the exact papers human experts used. We evaluate 7 leading Deep Research agents and 12 frontier LLMs. Results reveal a dual bottleneck: the best agent recalls only 20.9% of expert-selected papers, and even with perfect input, the best model achieves only 0.31 ARI in organization. Current deep research agents remain far from expert-level survey writing. Our benchmark is publicly available at https://github.com/KongLongGeFDU/TaxoBench.

</details>


### [11] [Legal experts disagree with rationale extraction techniques for explaining ECtHR case outcome classification](https://arxiv.org/abs/2601.12419)
*Mahammad Namazov,Tomáš Koref,Ivan Habernal*

Main category: cs.CL

TL;DR: 该论文提出了一个模型无关的可解释性技术比较分析框架，用于法律领域的LLM决策解释，通过忠实度和合理性评估发现模型预测理由与法律专家存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 法律领域应用大语言模型需要信任和透明度，但现有解释技术中哪种最适合法律结果预测仍是一个开放问题。

Method: 提出了一个比较分析框架，采用两种理由提取方法，通过标准化充分性和全面性指标评估忠实度，并请法律专家评估提取理由的合理性。

Result: 尽管量化分析结果很有前景且下游分类性能合理，但模型预测违规的"理由"与法律专家的理由存在显著差异。

Conclusion: 需要更细致的评估方法来理解LLM在法律领域的决策过程，并探讨了使用LLM作为评判者的可行性。

Abstract: Interpretability is critical for applications of large language models in the legal domain which requires trust and transparency. While some studies develop task-specific approaches, other use the classification model's parameters to explain the decisions. However, which technique explains the legal outcome prediction best remains an open question. To address this challenge, we propose a comparative analysis framework for model-agnostic interpretability techniques. Among these, we employ two rationale extraction methods, which justify outcomes with human-interpretable and concise text fragments (i.e., rationales) from the given input text. We conduct comparison by evaluating faithfulness-via normalized sufficiency and comprehensiveness metrics along with plausibility-by asking legal experts to evaluate extracted rationales. We further assess the feasibility of LLM-as-a-Judge using legal expert evaluation results. We show that the model's "reasons" for predicting a violation differ substantially from those of legal experts, despite highly promising quantitative analysis results and reasonable downstream classification performance. The source code of our experiments is publicly available at https://github.com/trusthlt/IntEval.

</details>


### [12] [Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping](https://arxiv.org/abs/2601.12465)
*Miao Peng,Weizhou Shen,Nuo Chen,Chenliang Li,Ming Yan,Jia Li*

Main category: cs.CL

TL;DR: 提出DeepReasonQA和LongPAS方法解决长上下文推理中RLVR性能下降问题，通过知识图谱构建高难度多跳QA对，并利用过程优势塑造进行细粒度信用分配，显著提升长上下文推理性能。


<details>
  <summary>Details</summary>
Motivation: RLVR在短上下文推理中有效，但在需要精确基础和长距离推理的长上下文场景中性能下降。研究发现"almost-there"现象，即轨迹大部分正确但在最后一步失败，归因于长上下文QA数据缺乏高推理密度，以及RL训练中对部分正确轨迹的惩罚导致学习信号丢失。

Method: 1. DeepReasonQA：基于知识图谱的可控合成框架，构建具有固有推理链的高难度多跳长上下文QA对；2. LongPAS：长上下文过程优势塑造方法，通过有效性和相关性维度评估推理步骤，进行细粒度信用分配，从"almost-there"轨迹中捕获关键学习信号。

Result: 在三个长上下文推理基准测试中，该方法显著优于RLVR基线，与前沿LLM性能相当但使用更少参数。进一步分析证实方法能有效增强长上下文推理能力，同时保持稳定的RL训练。

Conclusion: 通过构建高质量推理密集型数据和引入过程优势塑造方法，成功解决了长上下文推理中RLVR的性能瓶颈问题，为长上下文推理的强化学习训练提供了有效解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the "almost-there" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from "almost-there" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.

</details>


### [13] [Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty](https://arxiv.org/abs/2601.12471)
*Sravanthi Machcha,Sushrita Yerra,Sahil Gupta,Aishwarya Sahoo,Sharmin Sultana,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: MedAbstain是一个用于评估LLMs在医学多选题中弃权能力的基准，发现即使高准确率模型也常无法在不确定时弃权，提供明确弃权选项比输入扰动更能提高安全性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估过于关注准确性，但在现实世界和安全关键应用中，模型在不确定时能够弃权同样重要，这对于可信部署至关重要。

Method: 引入MedAbstain基准和评估协议，整合了共形预测、对抗性问题扰动和明确弃权选项，系统评估开源和闭源LLMs在医学多选题中的弃权能力。

Result: 即使最先进的高准确率模型也经常无法在不确定时弃权；提供明确弃权选项比输入扰动更能增加模型不确定性和安全弃权；扩大模型规模或高级提示带来的改进有限。

Conclusion: 弃权机制对于LLM可信部署具有核心作用，为高风险应用中的安全性改进提供了实用指导。

Abstract: Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.

</details>


### [14] [A Cloud-based Multi-Agentic Workflow for Science](https://arxiv.org/abs/2601.12607)
*Anurag Acharya,Timothy Vega,Rizwan A. Ashraf,Anshu Sharma,Derek Parker,Robert Rallo*

Main category: cs.CL

TL;DR: 提出一个领域无关、模型独立的云端智能体框架，作为科学助手，能够处理从文献综述到模拟运行等任务，在催化剂研究中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂任务（如运行模拟或做出复杂决策）方面能力有限，限制了其在科学领域的应用。虽然基于LLM的智能体能够调用外部资源和工具，但设计平衡模型、云提供商和外部资源的工作流程非常困难，使得实现智能体系统变得复杂。

Method: 提出一个领域无关、模型独立的云端智能体框架，采用监督者智能体协调多个具有特定能力的智能体。该框架整合了文献综述、数据分析等简单任务和模拟运行等复杂任务。在催化剂研究领域构建了概念验证系统，并详细描述了框架设计。

Result: 系统能够90%的时间将任务路由到正确的智能体，在合成任务中成功完成任务97.5%的时间，在真实世界任务中成功91%的时间。与前沿模型相比，达到相当或更好的准确性。报告了运营成本及各项服务的成本细分。

Conclusion: 该框架是可行的科学助手解决方案，能够有效处理科学任务，展示了在其他科学领域复制的潜力。系统在任务路由和完成率方面表现良好，同时保持了成本可控。

Abstract: As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.

</details>


### [15] [A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization](https://arxiv.org/abs/2601.12698)
*Qiuyi Qu,Yicheng Sui,Yufei Sun,Rui Chen,Xiaofei Zhang,Yuzhi Zhang,Haofeng Wang,Ge Lan,Ning Zhang*

Main category: cs.CL

TL;DR: 提出模板化重写层结合搜索自动调优的GPU内核优化方法，相比直接代码重写更稳定可控，实验显示最佳情况下可实现3倍以上加速。


<details>
  <summary>Details</summary>
Motivation: GPU代码优化对HPC和大模型训练/推理至关重要，但现有LLM代理方法多依赖直接代码重写，参数选择隐式且难以控制，导致性能提升不稳定。

Method: 在代理驱动迭代循环上增加模板化重写层：将内核语义重构为显式参数化模板，然后通过基于搜索的自动调优优化模板参数，结合性能分析反馈进行约束参数搜索。

Result: 在真实世界内核实验中，最佳情况下实现超过3倍的加速。相比纯代理直接重写，模板加搜索设计显著减少迭代优化的随机性，使过程更可解释。

Conclusion: 模板化重写与搜索自动调优相结合的方法为GPU内核优化提供了更系统、稳定的途径，可扩展到OpenCL、HIP等后端，实现生产工作负载的自动化性能优化。

Abstract: GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.

</details>


### [16] [A Shared Geometry of Difficulty in Multilingual Language Models](https://arxiv.org/abs/2601.12731)
*Stefano Civelli,Pietro Bernardelle,Nicolò Brunello,Gianluca Demartini*

Main category: cs.CL

TL;DR: LLMs对问题难度的预测呈现两阶段表征：浅层表征形成语言无关的难度估计，深层表征则变得语言特定化。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs中问题难度预测的多语言几何特性，探索难度相关信号在不同模型内部层次的表征方式及其跨语言泛化能力。

Method: 使用Easy2Hard基准的AMC子集，翻译成21种语言，在LLMs内部表征上训练线性探针，分析浅层（早期层）和深层（后期层）表征的功能差异。

Result: 深层表征探针在同语言上准确率高但跨语言泛化差；浅层表征探针在同语言上准确率较低但跨语言泛化显著更好，表明LLMs先形成语言无关的难度表征，后变为语言特定化。

Conclusion: LLMs对问题难度估计存在两阶段表征过程：先抽象概念空间（语言无关），后语言特定输出，这与LLM可解释性研究中的发现一致，且扩展到了元认知属性。

Abstract: Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.

</details>


### [17] [Towards Robust Process Reward Modeling via Noise-aware Learning](https://arxiv.org/abs/2601.12748)
*Bin Xie,Bingbing Xu,Xueyun Tian,Yilin Chen,Huawei Shen*

Main category: cs.CL

TL;DR: 提出两阶段框架解决过程奖励模型中的噪声监督问题：标签阶段使用LLM检测反思行为修正标签，训练阶段通过噪声感知迭代训练框架逐步精炼噪声标签。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型在复杂推理中表现良好，但受限于昂贵的逐过程监督。蒙特卡洛估计作为替代方法会产生策略依赖的奖励，导致标签噪声（包括错误奖励正确步骤和惩罚正确步骤）。

Method: 两阶段框架：1) 标签阶段：引入反思感知标签修正机制，使用LLM作为裁判检测与当前推理步骤相关的反思和自我修正行为，抑制高估奖励；2) 训练阶段：提出噪声感知迭代训练框架，使PRM能够基于自身置信度逐步精炼噪声标签。

Result: 实验表明该方法显著提高了步骤级正确性判别能力，相比使用噪声监督训练的PRM，平均F1分数绝对提升高达27%。

Conclusion: 提出的两阶段框架有效缓解了过程奖励模型中的噪声监督问题，通过结合LLM的反思检测和噪声感知迭代训练，显著提升了步骤级评估的准确性。

Abstract: Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \underline{\textbf{N}}oise-\underline{\textbf{A}}ware \underline{\textbf{I}}terative \underline{\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\% absolute gain in average F1 over PRMs trained with noisy supervision.

</details>


### [18] [Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory](https://arxiv.org/abs/2601.12771)
*Keito Inoshita*

Main category: cs.CL

TL;DR: 提出LAMA框架，利用LLM的世界知识作为关联记忆，通过回忆同名名人并聚合其国籍来预测国籍，显著优于传统提示方法


<details>
  <summary>Details</summary>
Motivation: LLMs拥有丰富的世界知识，但有效提取这些知识的方法尚未充分探索。国籍和地区预测任务不仅需要语言特征，还需要文化和历史背景知识，这使得LLM的世界知识特别有价值。然而，传统的LLM提示方法依赖直接推理，在应用抽象语言规则方面存在局限性。

Method: 提出LLM关联记忆代理(LAMA)框架，不直接从名字推断国籍，而是回忆同名的著名人物并通过间接推理聚合其国籍。采用双代理架构：人物代理和媒体代理，分别专注于不同知识领域，并行回忆著名人物，通过投票生成Top-1预测，通过条件补全生成Top-K预测。

Result: 在99个国家的国籍预测任务中，LAMA达到0.817的准确率，显著优于传统LLM提示方法和神经模型。实验表明：LLMs在回忆具体示例方面比抽象推理更可靠；基于回忆的方法对低频国籍具有鲁棒性，不受数据频率分布影响；双代理架构具有互补性，产生协同效应。

Conclusion: 这些结果展示了一种新的多代理系统的有效性，该系统检索和聚合LLM知识而非提示推理。LAMA框架通过利用LLM作为关联记忆，在需要世界知识的任务中表现出色。

Abstract: Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.

</details>


### [19] [The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check](https://arxiv.org/abs/2601.12979)
*Qingyu Lu,Liang Ding,Kanjian Zhang,Jinxia Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: 当前基于扩散的大语言模型(dLLMs)在实时代理交互中效率优势明显，但在实际代理任务中表现不佳，特别是在长时程规划和精确格式要求方面存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 研究dLLMs作为自回归模型替代方案在代理任务中的实际效果，验证其效率优势是否能转化为有效的代理行为。

Method: 在Agentboard和BFCL基准上全面评估dLLMs在两种代理范式中的表现：具身代理（需要长时程规划）和工具调用代理（需要精确格式）。引入DiffuAgent多代理评估框架，将dLLMs作为即插即用的认知核心集成到代理工作流中。

Result: dLLMs在代理任务中表现不佳：1）在具身设置中，dLLMs反复尝试失败，无法在时间反馈下进行分支；2）在工具调用设置中，dLLMs在扩散噪声下无法保持符号精度（如严格的JSON模式）。dLLMs在非因果角色（如记忆总结和工具选择）中有效，但需要将因果、精确和逻辑基础推理机制整合到去噪过程中才能适用于代理任务。

Conclusion: 当前dLLMs不能作为可靠的代理主干，其效率优势并未转化为有效的代理行为。dLLMs在代理工作流中更适合非因果角色，需要改进推理机制才能用于核心代理任务。

Abstract: The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a "bitter lesson": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.

</details>


### [20] [Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.12995)
*Runxuan Liu,Xianhao Ou,Xinyan Ma,Jiyuan Wang,Jiafeng Liang,Jiaqi Li,Tao He,Zheng Chu,Rongchuan Mu,Zekun Wang,Baoxin Wang,Dayong Wu,Ming Liu,Shijin Wang,Guoping Hu,Bing Qin*

Main category: cs.CL

TL;DR: 论文提出图推理范式(GRP)和PASC-GRPO优化方法，通过结构化图表示和过程感知验证解决传统CoT推理中的语义评估瓶颈、监督粗糙、奖励黑客等问题，在数学推理和代码生成任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的推理主要生成纯文本，对非结构化数据进行语义评估会造成训练计算瓶颈。尽管有RLVR优化，现有方法仍存在监督粗糙、奖励黑客、训练成本高和泛化差等问题。

Method: 提出图推理范式(GRP)，实现结构化和符号化推理，通过带步骤级认知标签的图结构表示。基于GRP设计PASC-GRPO方法，用结构化评估替代语义评估，通过图结构结果奖励实现过程感知验证，通过分层裁剪优势估计缓解奖励黑客问题。

Result: 实验表明在数学推理和代码生成任务上取得显著改进。数据、模型和代码将后续发布。

Conclusion: GRP和PASC-GRPO方法有效解决了传统CoT推理中的关键问题，通过结构化表示和过程感知优化显著提升了LLMs的推理能力。

Abstract: Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.

</details>


### [21] [Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning](https://arxiv.org/abs/2601.13115)
*Fengran Mo,Yifan Gao,Sha Li,Hansi Zeng,Xin Liu,Zhaoxuan Tan,Xian Li,Jianshu Chen,Dakuo Wang,Meng Jiang*

Main category: cs.CL

TL;DR: 提出一个基于强化学习的对话代理，通过跨轮次交替搜索和推理来处理多轮对话中的动态用户意图，超越现有静态流水线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统通常采用静态的改写-检索-生成流水线，分别优化不同步骤，忽略了混合主动动作的联合优化。虽然深度搜索代理在单轮场景中展示了联合优化检索和生成的有效性，但缺乏处理多轮交互的能力。

Method: 引入一个对话代理，通过强化学习训练，在多个轮次中交替进行搜索和推理，实现探索性和适应性行为。使用针对演化用户目标的定制奖励函数进行RL训练。

Result: 在四个广泛使用的对话基准测试中，该方法超越了多个现有强基线，证明了其有效性。

Conclusion: 通过强化学习训练跨轮次交替搜索和推理的对话代理，能够更好地处理多轮对话中动态演化的用户意图，相比静态流水线方法有显著改进。

Abstract: Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.

</details>


### [22] [OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference](https://arxiv.org/abs/2601.13300)
*Yow-Fu Liou,Yu-Chien Tang,Yu-Hsiang Liu,An-Zi Yen*

Main category: cs.CL

TL;DR: 本文提出OI-Bench基准测试，通过在多选题界面注入误导性指令选项，系统评估LLM对指令干扰的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM决策容易受到社交线索、框架效应和指令等定向信号的影响，需要系统评估模型对这些干扰的鲁棒性

Method: 提出选项注入方法，在MCQA界面添加包含误导性指令的额外选项，构建包含3000个问题的OI-Bench基准，涵盖16种指令类型

Result: 评估12个LLM显示显著漏洞和异质性鲁棒性，攻击成功率存在差异，需要进一步研究缓解策略

Conclusion: OI-Bench支持在基于选择的界面中系统评估LLM对指令干扰的鲁棒性，揭示了模型的重要安全漏洞

Abstract: Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.

</details>


### [23] [LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction](https://arxiv.org/abs/2601.13352)
*Yuxing Lu,J. Ben Tamo,Weichen Zhao,Nan Sun,Yishan Zhong,Wenqi Shi,Jinzhuo Wang,May D. Wang*

Main category: cs.CL

TL;DR: LLM-as-RNN：将冻结LLM转换为循环预测器的推理框架，通过自然语言记忆状态实现在线学习，无需参数更新，在多个序列预测任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理时上下文历史不可变，一旦在生成步骤t出错，模型缺乏可更新的记忆机制来改进步骤t+1的预测。需要一种无需参数更新的在线学习方法。

Method: 提出LLM-as-RNN框架，将冻结LLM转换为循环预测器，隐藏状态表示为自然语言记忆（结构化系统提示摘要），通过反馈驱动的文本重写在每个时间步更新状态，实现在线学习。

Result: 在医疗、气象和金融三个序列基准测试中，LLM-as-RNN显著优于零样本、完整历史和MemPrompt基线，平均预测准确率提升6.5%，同时产生可解释的人类可读学习轨迹。

Conclusion: LLM-as-RNN通过自然语言记忆状态实现有效的在线学习，无需参数更新，在固定token预算下纠正错误并保留任务相关模式，为LLM推理提供了新的循环预测范式。

Abstract: Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.

</details>


### [24] [Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection](https://arxiv.org/abs/2601.13359)
*Asen Dotsinski,Panagiotis Eustratiadis*

Main category: cs.CL

TL;DR: 提出了一种名为"sockpuppetting"的简单越狱方法，通过在模型输出开头插入接受序列来攻击开源大语言模型，仅需一行代码且无需优化，攻击成功率比GCG方法高80%。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型能力增强，保护它们免受恶意提示攻击和理解可能的攻击向量变得日益重要。现有自动化越狱方法如GCG需要大量计算资源和专业知识，需要更简单有效的攻击方法。

Method: 提出"sockpuppetting"方法：在模型输出开头插入接受序列（如"Sure, here is how to..."），然后让模型完成响应。还探索了混合方法，在助手消息块内优化对抗后缀而非用户提示。

Result: 在Qwen3-8B上，sockpuppetting在每提示比较中比GCG攻击成功率提高80%；在Llama-3.1-8B上，混合方法在提示无关设置中比GCG攻击成功率提高64%。

Conclusion: sockpuppetting是一种有效的低成本攻击方法，即使是技术不熟练的攻击者也能使用，突显了开源模型需要防御输出前缀注入攻击的必要性。

Abstract: As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce "sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., "Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.

</details>


### [25] [PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving](https://arxiv.org/abs/2601.13453)
*Aditya Thole,Anmol Agrawal,Arnav Ramamoorthy,Dhruv Kumar*

Main category: cs.CL

TL;DR: PSA是一个自主代理，使用Manim动画生成长达6分钟的物理解释视频，并通过包含15个定量参数的评估管道和VLM反馈迭代改进视频质量，在32个物理问题上实现了100%视频完成率，但存在视觉布局不一致和解释错误等挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在文本物理问题上表现良好，但生成高质量视觉解释的能力尚未充分探索。视觉推理能显著提升物理概念理解，因此需要开发能生成长视频解释的系统。

Method: 引入PhysicsSolutionAgent (PSA)，使用Manim动画生成物理解释视频；设计评估管道进行15个定量参数的自动化检查，并整合视觉语言模型的反馈来迭代改进视频质量。

Result: 在32个物理问题视频评估中，PSA使用GPT-5-mini实现了100%视频完成率，平均自动化得分3.8/5。但定性分析和人工检查发现视觉布局不一致、视觉内容解释错误等问题，揭示了问题难度和类型对视频质量的系统性影响。

Conclusion: 研究揭示了可靠Manim代码生成的关键限制，突显了多模态推理和评估在物理问题视觉解释中的广泛挑战，强调未来多模态教育系统需要改进视觉理解、验证和评估框架。

Abstract: Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems

</details>


### [26] [When Wording Steers the Evaluation: Framing Bias in LLM judges](https://arxiv.org/abs/2601.13537)
*Yerin Hwang,Dongryeol Lee,Taegwan Kang,Minwoo Lee,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现大语言模型在评估任务中存在显著的框架偏差，即提示语的微小变化会显著影响模型判断，这揭示了当前LLM评估系统的结构性缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然已知LLM对提示语措辞敏感，但这种框架偏差对LLM评估任务的影响尚未充分研究。在需要稳定公正判断的高风险评估场景中，这种偏差可能严重影响评估结果的可靠性。

Method: 受心理学框架效应启发，研究者系统研究了提示语框架如何影响模型判断。设计了使用谓词-肯定和谓词-否定结构的对称提示语，在四个高风险评估任务中测试了14个LLM法官模型。

Result: 研究发现框架偏差显著影响模型输出，所有测试模型都表现出明显的框架敏感性。不同模型家族在同意或拒绝倾向上表现出明显差异，表明框架偏差是当前LLM评估系统的结构性特征。

Conclusion: 框架偏差是当前LLM评估系统的一个基本问题，需要开发框架感知的评估协议来确保评估结果的稳定性和公正性。

Abstract: Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.

</details>


### [27] [Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions](https://arxiv.org/abs/2601.13590)
*Fan Huang,Haewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 该论文系统评估了LLM在SMCR沟通框架下对说服的易感性，发现小模型极易被说服，元认知提示反而增加脆弱性，对抗性微调效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于问答任务，但研究表明它们容易受到说服并采纳反事实信念。需要系统评估LLM在SMCR框架下对说服的易感性，了解不同说服策略如何影响信念稳定性。

Method: 使用SMCR沟通框架，在五个主流LLM和三个领域（事实知识、医学QA、社会偏见）上分析不同说服策略对多轮交互中信念稳定性的影响。评估元认知提示和对抗性微调作为防御措施的效果。

Result: 小模型表现出极端顺从性，80%以上的信念变化发生在第一轮说服中。元认知提示反而增加脆弱性，加速信念侵蚀。对抗性微调效果因模型而异：GPT-4o-mini达到98.6%的鲁棒性，Mistral 7B从35.7%提升到79.3%，而Llama模型即使微调后仍保持高易感性（<14%）。

Conclusion: 当前鲁棒性干预措施存在显著的模型依赖性限制，需要开发更值得信赖的LLM。研究结果为开发更可靠的LLM提供了指导。

Abstract: Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.

</details>


### [28] [Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff](https://arxiv.org/abs/2601.13717)
*Zehan Li,Yuxuan Wang,Ali El Lahib,Ying-Jieh Xia,Xinyu Pi*

Main category: cs.CL

TL;DR: 研究发现模拟无知(SI)无法有效近似真实无知(TI)，在477个竞赛级问题和9个模型上，SI与TI存在52%的性能差距，推理过程无法抑制先验知识，因此基于SI的回顾性预测评估方法存在根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 评估LLM预测能力面临根本矛盾：前瞻性评估方法严谨但延迟高，回顾性预测面临数据快速减少的问题。模拟无知(SI)作为潜在解决方案，需要验证其是否能有效近似真实无知(TI)。

Method: 在477个竞赛级问题上测试9个模型，比较模拟无知(SI)与真实无知(TI)的表现，分析截止指令效果、思维链推理对知识抑制的影响，以及推理优化模型的表现差异。

Result: SI系统性失败：1)截止指令导致SI与TI存在52%性能差距；2)思维链推理无法抑制先验知识，即使推理痕迹不含明确截止后信息；3)推理优化模型的SI保真度更差，尽管推理痕迹质量更高。

Conclusion: 提示无法可靠"回滚"模型知识，基于预截止事件的回顾性预测方法存在方法论缺陷，不建议使用基于SI的回顾性设置来基准测试预测能力。

Abstract: Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.

</details>


### [29] [Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering](https://arxiv.org/abs/2601.14050)
*Yuxin Chen,Zhengzhou Cai,Xiangtian Ji,Weixiang Zhao,An Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文系统分析了MoE模型的多语言处理机制，发现路由行为与语言家族对齐，专家利用呈现清晰的层级模式，并提出了一种路由引导的转向方法以提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构在多语言能力方面表现出色，但其内部机制（性能提升和跨语言差异）尚未得到充分理解。需要系统分析MoE模型的路由行为和专家专业化模式。

Method: 对MoE模型进行系统分析，研究跨语言和网络深度的路由行为和专家专业化。基于分析结果，提出路由引导的转向方法，在推理时自适应地将中间层的路由行为引导至与主导语言相关的共享专家。

Result: 分析发现：1）路由与语言家族对齐；2）专家利用呈现清晰的层级模式；3）高资源语言依赖共享专家，低资源语言更多依赖语言专属专家但性能较弱；4）早期和晚期MoE层支持语言特定处理，中间层作为语言无关的容量中心。提出的路由引导转向方法能持续提升多语言性能，特别是语言相关的语言对。

Conclusion: MoE模型中的多语言处理具有高度结构化特征，路由行为与语言家族相关。通过路由引导的中间层转向方法可以有效提升多语言性能，为理解和改进MoE模型的多语言能力提供了新视角。

Abstract: Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.

</details>


### [30] [HALT: Hallucination Assessment via Latent Testing](https://arxiv.org/abs/2601.14210)
*Rohan Bhatnagar,Youran Sun,Chi Andrew Zhang,Yixin Wen,Haizhao Yang*

Main category: cs.CL

TL;DR: 提出轻量级残差探针，直接从LLM中间隐藏状态读取幻觉风险，实现零延迟的幻觉风险评估，用于选择性生成和路由决策。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉问题可视为忠实读取失败：尽管内部表示可能编码了查询的不确定性，但解码压力仍会产生流畅答案。需要直接从中间层读取被最终解码阶段衰减的认知信号。

Method: 设计轻量级残差探针，作为小型辅助网络，在问题标记的中间隐藏状态上操作。计算成本比令牌生成低几个数量级，可与推理完全并行评估，实现近瞬时幻觉风险评估。

Result: 在四个QA基准测试和多个LLM家族上，方法实现了强大的AUROC和AURAC，在数据集偏移下具有良好泛化能力，并揭示了中间表示的可解释结构。

Conclusion: 快速内部不确定性读取可作为可靠智能体AI的原则性基础，通过选择性生成和路由实现高效可靠的问答系统。

Abstract: Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.

</details>


### [31] [APEX-Agents](https://arxiv.org/abs/2601.14242)
*Bertie Vidgen,Austin Mann,Abby Fennelly,John Wright Stanly,Lucas Rothman,Marco Burstein,Julien Benchek,David Ostrofsky,Anirudh Ravichandran,Debnil Sur,Neel Venugopal,Alannah Hsia,Isaac Robinson,Calix Huang,Olivia Varones,Daniyal Khan,Michael Haines,Zach Richards,Chirag Mahapatra,Brendan Foody,Osvald Nitski*

Main category: cs.CL

TL;DR: APEX-Agents是一个评估AI代理执行投资银行、管理咨询和法律领域长时程跨应用任务的基准测试，包含真实工作环境和工具，Gemini 3 Flash表现最佳(24.0%)。


<details>
  <summary>Details</summary>
Motivation: 需要评估AI代理在专业领域（投资银行、管理咨询、法律）中执行复杂长时程跨应用任务的能力，现有基准可能无法充分反映真实工作场景。

Method: 创建APEX-Agents基准测试，包含480个任务，模拟真实工作环境（文件和工具），使用Pass@1评估8个代理，并开源基准数据和Archipelago执行评估基础设施。

Result: Gemini 3 Flash (Thinking=High)得分最高(24.0%)，其次是GPT-5.2、Claude Opus 4.5和Gemini 3 Pro。所有测试代理表现相对较低，显示长时程跨应用任务仍具挑战性。

Conclusion: APEX-Agents为评估AI代理在专业领域的长时程跨应用能力提供了标准化基准，开源基础设施促进该领域研究发展，当前代理性能仍有提升空间。

Abstract: We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles](https://arxiv.org/abs/2601.11781)
*Dawood Wasif,Terrence J. Moore,Seunghyun Yoon,Hyuk Lim,Dan Dongseong Kim,Frederica F. Nelson,Jin-Hee Cho*

Main category: cs.AI

TL;DR: RAIL是一个风险感知的人机协同框架，通过融合多种运行时信号生成入侵风险评分，根据风险等级动态调整控制策略，并在高风险时结合特定防护机制，同时支持人类干预。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在面对罕见的长尾场景或网络物理入侵时，必须保持安全和有效性。现有方法在处理这些复杂风险场景时存在局限性，需要一种能够实时感知风险并自适应调整的框架。

Method: RAIL融合三种信号（曲率执行完整性、碰撞时间接近度和观测偏移一致性）通过加权Noisy-OR生成入侵风险评分(IRS)。根据IRS阈值，系统在低风险时执行名义策略，高风险时结合特定防护机制。使用上下文bandit根据信号向量选择防护机制，并采用Soft Actor-Critic结合风险优先回放和双重奖励进行学习。

Result: 在MetaDrive上，RAIL获得测试回报360.65，测试成功率0.85，测试安全违规0.75，扰动率0.0027，训练安全违规仅29.07。在CAN注入和LiDAR欺骗攻击下，成功率提升至0.68和0.80，攻击下脱离率降至0.37和0.03，攻击成功率降至0.34和0.11。在CARLA上，仅用8000步获得测试回报1609.70和测试成功率0.41。

Conclusion: RAIL框架通过风险感知的人机协同机制，有效提升了自动驾驶系统在复杂风险场景下的安全性和性能，显著优于现有RL、安全RL、离线/模仿学习和先前的人机协同基线方法。

Abstract: Autonomous vehicles must remain safe and effective when encountering rare long-tailed scenarios or cyber-physical intrusions during driving. We present RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime signals into calibrated control adaptations and focused learning. RAIL fuses three cues (curvature actuation integrity, time-to-collision proximity, and observation-shift consistency) into an Intrusion Risk Score (IRS) via a weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a cue-specific shield using a learned authority, while human override remains available; when risk is low, the nominal policy executes. A contextual bandit arbitrates among shields based on the cue vector, improving mitigation choices online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and dual rewards so that takeovers and near misses steer learning while nominal behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of 360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of 0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training safety violations, outperforming RL, safe RL, offline/imitation learning, and prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and TSR of 0.41 with only 8000 steps.

</details>


### [33] [POLARIS: Typed Planning and Governed Execution for Agentic AI in Back-Office Automation](https://arxiv.org/abs/2601.11816)
*Zahra Moslemi,Keerthi Koneru,Yen-Ting Lee,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: POLARIS是一个面向企业后台工作流的治理型LLM智能体编排框架，通过类型化计划合成和验证执行实现可审计、策略对齐的操作自动化。


<details>
  <summary>Details</summary>
Motivation: 企业后台工作流需要可审计、策略对齐且操作可预测的智能体系统，而通用的多智能体设置往往无法满足这些要求。

Method: POLARIS采用治理型编排框架，将自动化视为基于LLM智能体的类型化计划合成和验证执行。包括：规划器生成类型检查的有向无环图，基于规则的推理模块选择合规计划，执行过程通过验证器门控检查、有限修复循环和编译的策略护栏来保护。

Result: 在文档中心化金融任务中，POLARIS生成决策级工件和完整执行跟踪，减少人工干预。在SROIE数据集上达到0.81的micro F1分数，在受控合成套件上实现0.95-1.00的异常路由精度，同时保持审计跟踪。

Conclusion: POLARIS为策略对齐的智能体AI提供了方法论和基准参考，构成了治理型智能体AI的初步基准。

Abstract: Enterprise back office workflows require agentic systems that are auditable, policy-aligned, and operationally predictable, capabilities that generic multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM Agentic Reasoning for Integrated Systems), a governed orchestration framework that treats automation as typed plan synthesis and validated execution over LLM agents. A planner proposes structurally diverse, type checked directed acyclic graphs (DAGs), a rubric guided reasoning module selects a single compliant plan, and execution is guarded by validator gated checks, a bounded repair loop, and compiled policy guardrails that block or route side effects before they occur. Applied to document centric finance tasks, POLARIS produces decision grade artifacts and full execution traces while reducing human intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision for anomaly routing with preserved audit trails. These evaluations constitute an initial benchmark for governed Agentic AI. POLARIS provides a methodological and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI, Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed Planning, Evaluation

</details>


### [34] [Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of Software Logic](https://arxiv.org/abs/2601.11840)
*Hongyu Lin,Samer Abdallah,Makar Valentinov,Paul Brennan,Elijah Kagan,Christoph M. Wintersteiger,Denis Ignatovich,Grant Passmore*

Main category: cs.AI

TL;DR: CodeLogician是一个神经符号代理，结合LLMs和形式化推理引擎ImandraX，用于精确分析软件逻辑，在代码逻辑推理基准上相比纯LLM方法提升41-47个百分点。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码理解任务上表现良好，但缺乏对程序行为进行精确、详尽的数学推理能力。现有基准要么专注于与真实软件脱节的数学证明自动化，要么专注于不需要语义严谨性的工程任务。

Method: 提出CodeLogician神经符号代理，集成工业级自动推理引擎ImandraX。使用LLMs构建软件系统的显式形式化模型，使自动推理能够回答超越二元验证结果的丰富语义问题。

Result: 在code-logic-bench基准测试中，相比纯LLM推理，CodeLogician的形式化增强带来显著改进，缩小了41-47个百分点的推理准确率差距。

Conclusion: 神经符号集成对于扩展程序分析、实现严格自主软件理解至关重要。

Abstract: Large Language Models (LLMs) have shown strong performance on code understanding tasks, yet they fundamentally lack the ability to perform precise, exhaustive mathematical reasoning about program behavior. Existing benchmarks either focus on mathematical proof automation, largely disconnected from real-world software, or on engineering tasks that do not require semantic rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of software logic, integrated with ImandraX, an industrial automated reasoning engine deployed in financial markets and safety-critical systems. Unlike prior approaches that use formal methods primarily to validate LLM outputs, CodeLogician uses LLMs to construct explicit formal models of software systems, enabling automated reasoning to answer rich semantic questions beyond binary verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we introduce code-logic-bench, a benchmark targeting the middle ground between theorem proving and software engineering benchmarks. It measures reasoning correctness about program state spaces, control flow, coverage constraints, and edge cases, with ground truth defined via formal modeling and region decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal augmentation yields substantial improvements, closing a 41-47 percentage point gap in reasoning accuracy. These results demonstrate that neurosymbolic integration is essential for scaling program analysis toward rigorous, autonomous software understanding.

</details>


### [35] [AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled Agentic LLM Systems](https://arxiv.org/abs/2601.11903)
*YenTing Lee,Keerthi Koneru,Zahra Moslemi,Sheethal Kumar,Ramesh Radhakrishnan*

Main category: cs.AI

TL;DR: AEMA是一个面向企业级多智能体系统的评估框架，通过多步骤、可审计的评估流程，提供比单一LLM-as-a-Judge更稳定、可追溯的评估方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体系统评估方法存在局限性：单响应评分或狭窄基准测试在扩展到企业级多智能体规模时缺乏稳定性、可扩展性和自动化能力，需要更可靠、透明且可验证的评估框架。

Method: 提出AEMA（自适应评估多智能体）框架，这是一个过程感知、可审计的评估系统，能够在人类监督下规划、执行和聚合异构智能体工作流的多步骤评估，支持可追溯的记录和问责自动化。

Result: 在模拟真实业务场景的企业风格智能体工作流上测试，AEMA相比单一LLM-as-a-Judge方法展现出更高的稳定性、更好的人类对齐性，并提供可追溯的记录，支持负责任的自动化评估。

Conclusion: AEMA为基于LLM的多智能体系统提供了透明且可复现的评估路径，支持负责任的企业级智能体系统评估，具有过程感知、可审计和可扩展的特点。

Abstract: Evaluating large language model (LLM)-based multi-agent systems remains a critical challenge, as these systems must exhibit reliable coordination, transparent decision-making, and verifiable performance across evolving tasks. Existing evaluation approaches often limit themselves to single-response scoring or narrow benchmarks, which lack stability, extensibility, and automation when deployed in enterprise settings at multi-agent scale. We present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable framework that plans, executes, and aggregates multi-step evaluations across heterogeneous agentic workflows under human oversight. Compared to a single LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable records that support accountable automation. Our results on enterprise-style agent workflows simulated using realistic business scenarios demonstrate that AEMA provides a transparent and reproducible pathway toward responsible evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable Evaluation, Human Oversight

</details>


### [36] [Learn Like Humans: Use Meta-cognitive Reflection for Efficient Self-Improvement](https://arxiv.org/abs/2601.11974)
*Xinmeng Hou,Peiliang Gong,Bohao Qu,Wuqi Wang,Qing Guo,Yang Liu*

Main category: cs.AI

TL;DR: MARS框架通过单次循环实现高效自我进化，结合原则性反思和程序性反思，显著降低计算成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体受限于静态人工设计的提示，缺乏适应性。现有自我改进框架依赖低效的多轮递归循环，计算成本高

Method: 提出MARS框架，受教育心理学启发，结合原则性反思（抽象规范规则避免错误）和程序性反思（推导逐步成功策略），在单次循环中合成优化指令

Result: 在六个基准测试中，MARS优于最先进的自我进化系统，同时显著减少计算开销

Conclusion: MARS框架通过模仿人类学习过程，实现了高效的系统性推理逻辑优化，为智能体自我进化提供了新方向

Abstract: While Large Language Models (LLMs) enable complex autonomous behavior, current agents remain constrained by static, human-designed prompts that limit adaptability. Existing self-improving frameworks attempt to bridge this gap but typically rely on inefficient, multi-turn recursive loops that incur high computational costs. To address this, we propose Metacognitive Agent Reflective Self-improvement (MARS), a framework that achieves efficient self-evolution within a single recurrence cycle. Inspired by educational psychology, MARS mimics human learning by integrating principle-based reflection (abstracting normative rules to avoid errors) and procedural reflection (deriving step-by-step strategies for success). By synthesizing these insights into optimized instructions, MARS allows agents to systematically refine their reasoning logic without continuous online feedback. Extensive experiments on six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving systems while significantly reducing computational overhead.

</details>


### [37] [DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants](https://arxiv.org/abs/2601.12138)
*Abhishek Kumar,Riya Tapwal,Carsten Maple*

Main category: cs.AI

TL;DR: DriveSafe：针对LLM驾驶助手的四层次风险分类法，包含129个细粒度风险类别，评估显示现有模型在驾驶场景中安全拒绝能力不足。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地集成到车载数字助手中，但现有安全分类和评估框架大多是通用型的，未能捕捉真实驾驶场景中的领域特定风险。不安全、模糊或法律错误的响应可能导致严重的安全、道德和监管后果。

Method: 提出了DriveSafe，一个分层的四层次风险分类法，包含129个细粒度原子风险类别，涵盖技术、法律、社会和伦理维度。该分类法基于真实驾驶法规和安全原则构建，并由领域专家评审。通过评估六个广泛部署的LLM的拒绝行为来验证构建提示的安全相关性和真实性。

Result: 评估显示，被评估的模型经常无法适当拒绝不安全或不合规的驾驶相关查询，突显了通用安全对齐在驾驶场景中的局限性。

Conclusion: 需要针对驾驶领域的特定安全评估框架，现有LLM在驾驶场景中的安全对齐存在不足，DriveSafe分类法为系统评估LLM驾驶助手的安全性提供了基础。

Abstract: Large Language Models (LLMs) are increasingly integrated into vehicle-based digital assistants, where unsafe, ambiguous, or legally incorrect responses can lead to serious safety, ethical, and regulatory consequences. Despite growing interest in LLM safety, existing taxonomies and evaluation frameworks remain largely general-purpose and fail to capture the domain-specific risks inherent to real-world driving scenarios. In this paper, we introduce DriveSafe, a hierarchical, four-level risk taxonomy designed to systematically characterize safety-critical failure modes of LLM-based driving assistants. The taxonomy comprises 129 fine-grained atomic risk categories spanning technical, legal, societal, and ethical dimensions, grounded in real-world driving regulations and safety principles and reviewed by domain experts. To validate the safety relevance and realism of the constructed prompts, we evaluate their refusal behavior across six widely deployed LLMs. Our analysis shows that the evaluated models often fail to appropriately refuse unsafe or non-compliant driving-related queries, underscoring the limitations of general-purpose safety alignment in driving contexts.

</details>


### [38] [FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains](https://arxiv.org/abs/2601.12259)
*Jiashuo Liu,Siyuan Chen,Zaiyuan Wang,Zhiyuan Zeng,Jiacheng Guo,Liang Hu,Lingyue Yin,Suozhi Huang,Wenxin Hao,Yang Yang,Zerui Cheng,Zixin Yao,Lingyue Yin,Haoxin Liu,Jiayi Cheng,Yuzhen Li,Zezhong Ma,Bingjie Wang,Bingsen Qiu,Xiao Liu,Zeyang Zhang,Zijian Liu,Jinpeng Wang,Mingren Yin,Tianci He,Yali Liao,Yixiao Tian,Zhenwei Zhu,Anqi Dai,Ge Zhang,Jingkai Liu,Kaiyuan Zhang,Wenlong Wu,Xiang Gao,Xinjie Chen,Zhixin Yao,Zhoufutu Wen,B. Aditya Prakash,Jose Blanchet,Mengdi Wang,Nian Si,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX-Pro扩展了FutureX的通用未来预测基准，针对金融、零售、公共卫生和自然灾害四个高价值垂直领域建立了专门的预测框架，评估当前SOTA智能体LLM在这些关键领域的实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 虽然通用智能体在开放领域搜索中表现出色，但在资本密集型和安全关键领域的可靠性尚未充分探索。需要评估当前最先进的智能体LLM是否具备工业部署所需的领域基础。

Method: 基于FutureX的无污染、实时评估管道，针对四个垂直领域（金融、零售、公共卫生、自然灾害）建立专门的预测框架，在入门级但基础性的预测任务上对智能体LLM进行基准测试。

Result: 研究发现通用推理能力与高价值垂直应用所需精度之间存在性能差距，揭示了当前SOTA智能体LLM在工业部署准备度方面的不足。

Conclusion: 智能体LLM在专业垂直领域的应用仍面临挑战，需要更强的领域基础才能满足工业部署的要求。

Abstract: Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.

</details>


### [39] [ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents](https://arxiv.org/abs/2601.12294)
*Dawei Li,Yuguang Yao,Zhen Tan,Huan Liu,Ruocheng Guo*

Main category: cs.AI

TL;DR: 本文提出了ToolPRMBench，一个专门用于评估工具使用智能体过程奖励模型（PRMs）的大规模基准测试。该基准基于多个代表性工具使用基准构建，将智能体轨迹转换为步骤级测试用例，包含交互历史、正确动作、合理但不正确的替代动作及相关工具元数据。


<details>
  <summary>Details</summary>
Motivation: 虽然奖励引导搜索方法通过使用过程奖励模型（PRMs）提供步骤级奖励来增强工具使用智能体，但在工具使用场景中缺乏系统可靠的PRMs评估基准。现有评估方法不够全面，无法准确衡量PRMs在实际工具使用任务中的性能。

Method: 1. 基于多个代表性工具使用基准构建ToolPRMBench；2. 将智能体轨迹转换为步骤级测试用例；3. 使用离线采样隔离局部单步错误；4. 使用在线采样捕捉完整智能体轨迹中的实际多步失败；5. 提出多LLM验证流程减少标签噪声并确保数据质量。

Result: 通过在大语言模型、通用PRMs和工具专用PRMs上进行广泛实验，结果显示PRMs有效性存在明显差异，并突显了工具专用PRMs在工具使用场景中的潜力。

Conclusion: ToolPRMBench为评估工具使用智能体的过程奖励模型提供了系统可靠的基准，揭示了不同PRMs的性能差异，并证明了工具专用PRMs的优势。该基准将促进工具使用智能体中PRMs的进一步研究和开发。

Abstract: Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.

</details>


### [40] [Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation](https://arxiv.org/abs/2601.12410)
*Dingyi Yang,Junqi Zhao,Xue Li,Ce Li,Boyang Li*

Main category: cs.AI

TL;DR: LLMs在知识状态追踪和估计任务上表现接近随机，远低于人类水平，未来研究应更重视知识估计和意图理解能力


<details>
  <summary>Details</summary>
Motivation: 认知人类学认为人类智能的关键在于推断他人知识状态和理解意图的能力，而黑猩猩等动物缺乏这种能力。本研究旨在评估LLM在知识状态追踪和估计方面的表现。

Method: 设计两个任务：(1)检测故事角色是否通过行动展示了他们本不应拥有的知识；(2)预测故事角色基于自身知识（而非客观事实）的下一步行动。

Result: 当前最先进的LLM在两个任务上都表现出接近随机的性能，显著低于人类水平。

Conclusion: LLM在知识状态追踪和意图理解方面存在明显不足，未来LLM研究应更重视知识估计和意图理解能力的提升。

Abstract: Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.

</details>


### [41] [Large Language Model for OWL Proofs](https://arxiv.org/abs/2601.12444)
*Hui Yang,Jiaoyan Chen,Uli Sattler*

Main category: cs.AI

TL;DR: 该研究开发了一个自动化框架来评估大语言模型在OWL本体论中生成证明的能力，发现逻辑复杂性是影响性能的主要因素，而非表示格式，同时噪声和不完整性会显著降低模型表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在推理任务上的能力已被广泛研究，但它们在生成忠实、人类可读的证明（解释结论为何成立）方面的能力仍未被充分探索，特别是在OWL本体论这种复杂知识表示和推理的背景下。

Method: 开发了一个自动化数据集构建和评估框架，在OWL本体论背景下评估LLMs的证明生成能力。评估包括三个顺序任务：提取、简化和解释，以及一个额外的逻辑完整性评估任务。在广泛使用的推理LLMs上进行了大量实验。

Result: 研究发现：(1) 某些模型总体表现良好但在复杂案例上仍有局限；(2) 逻辑复杂性（而非表示格式如形式逻辑语言与自然语言）是影响LLM性能的主导因素；(3) 输入数据中的噪声和不完整性会显著降低LLMs的表现。

Conclusion: LLMs在提供严格逻辑解释方面具有潜力，但在复杂或不完美条件下支持弹性推理方面仍存在差距。代码和数据已开源。

Abstract: The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.

</details>


### [42] [Agentic Reasoning for Large Language Models](https://arxiv.org/abs/2601.12538)
*Tianxin Wei,Ting-Wei Li,Zhining Liu,Xuying Ning,Ze Yang,Jiaru Zou,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Dongqi Fu,Zihao Li,Mengting Ai,Duo Zhou,Wenxuan Bao,Yunzhe Li,Gaotang Li,Cheng Qian,Yu Wang,Xiangru Tang,Yin Xiao,Liri Fang,Hui Liu,Xianfeng Tang,Yuji Zhang,Chi Wang,Jiaxuan You,Heng Ji,Hanghang Tong,Jingrui He*

Main category: cs.AI

TL;DR: 该论文是一篇关于智能体推理的综述，系统性地将LLM作为自主智能体的推理能力组织为三个维度：基础智能体推理、自我进化推理和集体多智能体推理，并区分上下文推理与后训练推理，最后讨论了实际应用和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在封闭环境中表现出强大的推理能力，但在开放动态环境中仍存在局限。智能体推理通过将LLM重构为能够规划、行动和持续学习的自主智能体，代表了范式转变，需要系统性地梳理这一新兴领域。

Method: 采用综述研究方法，从三个互补维度组织智能体推理：1)基础智能体推理（单智能体在稳定环境中的规划、工具使用和搜索）；2)自我进化推理（通过反馈、记忆和适应优化能力）；3)集体多智能体推理（协作环境中的协调、知识共享和共同目标）。同时区分上下文推理（通过结构化编排扩展测试时交互）和后训练推理（通过强化学习和监督微调优化行为）。

Result: 构建了统一的智能体推理路线图，连接思想与行动，涵盖了从科学、机器人、医疗保健、自主研究到数学等多个实际应用领域和基准测试的代表性框架。

Conclusion: 智能体推理代表了LLM能力的重要演进方向，将推理从静态过程转变为动态交互过程。未来挑战包括个性化、长时程交互、世界建模、可扩展的多智能体训练以及实际部署的治理问题。

Abstract: Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.

</details>


### [43] [Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents](https://arxiv.org/abs/2601.12560)
*Arunkumar V,Gangadharan G. R.,Rajkumar Buyya*

Main category: cs.AI

TL;DR: 该论文提出了一种统一的Agentic AI分类法，将智能体分解为感知、大脑、规划、行动、工具使用和协作六个核心组件，并分析了从线性推理到原生推理模型的演进趋势。


<details>
  <summary>Details</summary>
Motivation: 随着AI从单纯生成文本转向Agentic AI（智能体AI），各种系统设计涌现，从简单的单循环智能体到分层多智能体系统，使得这一领域变得难以导航。作者旨在通过统一的分类法来梳理这一复杂领域。

Method: 提出一个统一的分类法，将智能体分解为六个核心组件：感知、大脑、规划、行动、工具使用和协作。使用这一框架分析从线性推理到原生推理模型的演进，以及从固定API调用到开放标准（如MCP和Native Computer Use）的转变。

Result: 建立了Agentic AI的统一分类框架，识别了智能体在不同环境（数字操作系统、具身机器人等）中的操作模式，并回顾了当前的评估实践。

Conclusion: 论文为Agentic AI领域提供了系统化的分类框架，指出了幻觉、无限循环、提示注入等开放挑战，并展望了未来构建更稳健可靠自主系统的研究方向。

Abstract: Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.

</details>


### [44] [Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts](https://arxiv.org/abs/2601.12711)
*Kevin Wang,Neel P. Bhatt,Cong Liu,Junbo Li,Runjin Chen,Yihan Xi,Timothy Barclay,Alvaro Velasquez,Ufuk Topcu,Zhangyang Wang*

Main category: cs.AI

TL;DR: 提出神经符号LoRA框架，动态结合数值参数更新和符号提示编辑两种策略，通过统一监控信号和奖励分类器决定何时使用LoRA进行事实重构，何时使用TextGrad进行token级编辑，在多个LLM骨干上优于纯数值或纯符号基线。


<details>
  <summary>Details</summary>
Motivation: LLMs可以通过数值更新（修改模型参数）或符号操作（处理离散提示或逻辑约束）进行适配。数值微调擅长注入新事实知识，符号更新则能灵活控制风格和对齐而无需重新训练。需要结合这两种互补策略以实现更强大的语言模型微调。

Method: 提出神经符号LoRA框架：1）统一监控信号和基于奖励的分类器，动态决定何时使用LoRA进行深度事实重构，何时应用TextGrad进行token级编辑；2）保持内存效率，仅在需要时将符号转换卸载到外部LLM；3）符号编辑过程中生成的精炼提示可作为高质量可重用训练数据。

Result: 在多个LLM骨干上的广泛实验表明，神经符号LoRA始终优于纯数值或纯符号基线，展示了卓越的适应性和改进的性能。特别是在数学推理等数据稀缺领域，该方法表现出色。

Conclusion: 数值和符号更新的交错结合为语言模型微调开启了新的多功能性水平，神经符号LoRA框架展示了这种混合方法的优势，为LLM适配提供了更灵活高效的解决方案。

Abstract: Large language models (LLMs) can be adapted either through numerical updates that alter model parameters or symbolic manipulations that work on discrete prompts or logical constraints. While numerical fine-tuning excels at injecting new factual knowledge, symbolic updates offer flexible control of style and alignment without retraining. We introduce a neurosymbolic LoRA framework that dynamically combines these two complementary strategies. Specifically, we present a unified monitoring signal and a reward-based classifier to decide when to employ LoRA for deeper factual reconstruction and when to apply TextGrad for token-level edits. Our approach remains memory-efficient by offloading the symbolic transformations to an external LLM only when needed. Additionally, the refined prompts produced during symbolic editing serve as high-quality, reusable training data, an important benefit in data-scarce domains like mathematical reasoning. Extensive experiments across multiple LLM backbones show that neurosymbolic LoRA consistently outperforms purely numerical or purely symbolic baselines, demonstrating superior adaptability and improved performance. Our findings highlight the value of interleaving numerical and symbolic updates to unlock a new level of versatility in language model fine-tuning.

</details>


### [45] [MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real Reasoning Correction](https://arxiv.org/abs/2601.12822)
*Wenqi Zhang,Yulin Shen,Changyue Jiang,Jiarun Dai,Geng Hong,Xudong Pan*

Main category: cs.AI

TL;DR: MirrorGuard是一个即插即用的防御框架，通过基于模拟的训练来增强计算机使用代理的安全性，在文本模拟环境中生成高风险GUI交互轨迹，学习在真实操作前拦截和修正不安全推理链。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型集成到计算机使用代理中，使其能够通过图形用户界面自主与操作系统交互执行复杂任务，但这种自主性引入了严重的安全风险：恶意指令或视觉提示注入可能触发不安全的推理并导致有害的系统级操作。现有防御方法（如基于检测的阻断）虽然能防止损害，但通常会过早中止任务，降低了代理的实用性。

Method: 提出MirrorGuard框架，采用基于模拟的训练方法。开发了新颖的神经符号模拟管道，在纯文本模拟环境中生成逼真的高风险GUI交互轨迹，捕获不安全推理模式和潜在系统危险，而无需执行真实操作。在模拟环境中，MirrorGuard学习在CUAs产生和执行不安全操作之前拦截和修正其不安全推理链。

Result: 在字节跳动UI-TARS系统上，MirrorGuard将不安全率从66.5%降低到13.0%，同时保持较低的错误拒绝率（FRR）。相比之下，最先进的GuardAgent仅将不安全率降低到53.9%，且FRR高出15.4%。在多样化的基准测试和CUA架构上的广泛评估表明，MirrorGuard显著减轻了安全风险。

Conclusion: 模拟衍生的防御方法能够在保持代理基本实用性的同时，提供强大的真实世界保护。MirrorGuard证明了通过模拟训练可以在不牺牲功能的情况下有效增强计算机使用代理的安全性。

Abstract: Large foundation models are integrated into Computer Use Agents (CUAs), enabling autonomous interaction with operating systems through graphical user interfaces (GUIs) to perform complex tasks. This autonomy introduces serious security risks: malicious instructions or visual prompt injections can trigger unsafe reasoning and cause harmful system-level actions. Existing defenses, such as detection-based blocking, prevent damage but often abort tasks prematurely, reducing agent utility. In this paper, we present MirrorGuard, a plug-and-play defense framework that uses simulation-based training to improve CUA security in the real world. To reduce the cost of large-scale training in operating systems, we propose a novel neural-symbolic simulation pipeline, which generates realistic, high-risk GUI interaction trajectories entirely in a text-based simulated environment, which captures unsafe reasoning patterns and potential system hazards without executing real operations. In the simulation environment, MirrorGuard learns to intercept and rectify insecure reasoning chains of CUAs before they produce and execute unsafe actions. In real-world testing, extensive evaluations across diverse benchmarks and CUA architectures show that MirrorGuard significantly mitigates security risks. For instance, on the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0% while maintaining a marginal false refusal rate (FRR). In contrast, the state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from a 15.4% higher FRR. Our work proves that simulation-derived defenses can provide robust, real-world protection while maintaining the fundamental utility of the agent. Our code and model are publicly available at https://bmz-q-q.github.io/MirrorGuard/.

</details>


### [46] [SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for Mathematical Reasoning](https://arxiv.org/abs/2601.12842)
*Qitong Fang,Haotian Li,Xu Wang*

Main category: cs.AI

TL;DR: SCULPT提出了一种约束引导的蒙特卡洛树搜索方法，通过领域感知的符号检查和结构模式指导来提升LLM代理工作流的推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化代理工作流依赖随机探索，经常遍历不合理的推理分支，因为现有方法使用通用提示或弱领域先验的策略来采样候选步骤，导致在操作符、单位和格式上的近随机游走。

Method: SCULPT将领域感知评分集成到MCTS的选择、扩展、模拟和回传阶段，通过符号检查（维度一致性、类型兼容性、幅度合理性、深度控制和多样性）和结构模式指导来评分和剪枝动作。

Result: 在相同的LLM配置下，SCULPT在多个数据集上实现了稳定的改进；使用GPT-5.2的额外结果评估了执行器可转移性和前沿推理模型的性能。

Conclusion: 领域感知约束可以在保持效率和推理稳定性的同时提高准确性。

Abstract: Automated agent workflows can enhance the problem-solving ability of large language models (LLMs), but common search strategies rely on stochastic exploration and often traverse implausible branches. This occurs because current pipelines sample candidate steps from generic prompts or learned policies with weak domain priors, yielding near-random walks over operators, units, and formats. To promote ordered exploration, this paper introduces SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that integrates domain-aware scoring into selection, expansion, simulation, and backpropagation. SCULPT scores and prunes actions using a combination of symbolic checks (dimensional consistency, type compatibility, magnitude sanity, depth control, and diversity) and structural pattern guidance, thereby steering the search toward plausible reasoning paths. Under matched LLM configurations, SCULPT yields stable improvements on multiple datasets; additional results with GPT-5.2 assess executor transferability and performance on frontier reasoning models. Overall, domain-aware constraints can improve accuracy while maintaining efficiency and reasoning stability.

</details>


### [47] [Human Emotion Verification by Action Languages via Answer Set Programming](https://arxiv.org/abs/2601.12912)
*Andreas Brännström,Juan Carlos Nieves*

Main category: cs.AI

TL;DR: 提出基于答案集编程和转移系统的C-MT语言，用于形式化人类心理状态随可观察动作的演化，特别关注情绪变化，并引入新的因果规则来控制代理行为和限制心理副作用。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对可控代理行为的支持，无法有效限制动作带来的心理副作用。需要一种形式化框架来建模人类心理状态（特别是情绪）随动作序列的演化，并支持不同心理学理论的比较。

Method: 基于答案集编程和转移系统构建C-MT语言，利用评价情绪理论等心理学理论将心理状态形式化为多维配置。引入新的"forbids to cause"因果规则和专门的心理状态动态表达式，将心理变化原则转化为转移约束和不变性属性，通过轨迹分析进行严格评估。

Result: 开发出能够建模人类心理状态动态演化的形式化框架，支持可控推理和心理变化原则的比较分析。该框架已应用于情绪验证模型的设计。

Conclusion: C-MT语言为人类心理状态演化提供了形式化建模工具，特别适用于情绪验证和可控代理行为设计，支持不同心理学理论的比较分析。

Abstract: In this paper, we introduce the action language C-MT (Mind Transition Language). It is built on top of answer set programming (ASP) and transition systems to represent how human mental states evolve in response to sequences of observable actions. Drawing on well-established psychological theories, such as the Appraisal Theory of Emotion, we formalize mental states, such as emotions, as multi-dimensional configurations. With the objective to address the need for controlled agent behaviors and to restrict unwanted mental side-effects of actions, we extend the language with a novel causal rule, forbids to cause, along with expressions specialized for mental state dynamics, which enables the modeling of principles for valid transitions between mental states. These principles of mental change are translated into transition constraints, and properties of invariance, which are rigorously evaluated using transition systems in terms of so-called trajectories. This enables controlled reasoning about the dynamic evolution of human mental states. Furthermore, the framework supports the comparison of different dynamics of change by analyzing trajectories that adhere to different psychological principles. We apply the action language to design models for emotion verification. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [48] [MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI Agents via Automated Feedback Reflux](https://arxiv.org/abs/2601.13060)
*Zecheng Li,Zhihui Cao,Wenke Huang,Yudong Zhang,Keying Qi,Rui Wang,Zeyu Zheng,Jian Zhao,Hao Zhu,Hengxin Wu,Yuran Wang,Guitao Fan,Guokun Wu,Yicong Liu,Zhilin Gao,Haikun Xu,He Yang,Minqi Xiang,Xingyu Liu,Zuojian Wang*

Main category: cs.AI

TL;DR: MagicGUI-RMS是一个多智能体奖励模型系统，用于GUI智能体的自适应轨迹评估、纠正反馈和自我进化学习，通过领域特定和通用奖励模型结合实现细粒度动作评估和跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体面临两大挑战：自动化评估智能体轨迹和规模化生成高质量训练数据。现有方法依赖人工标注或静态规则验证，限制了可扩展性和动态环境适应性。

Method: 提出MagicGUI-RMS多智能体奖励模型系统，集成领域特定奖励模型(DS-RM)和通用奖励模型(GP-RM)，设计结构化数据构建管道自动生成平衡多样的奖励数据集，通过自动数据回流机制持续改进智能体行为。

Result: 实验表明MagicGUI-RMS在任务准确性和行为鲁棒性方面取得显著提升，为基于奖励自适应构建自我改进的GUI智能体提供了有效基础。

Conclusion: MagicGUI-RMS为构建基于奖励自适应的自我改进GUI智能体提供了一个原则性且有效的基础框架。

Abstract: Graphical user interface (GUI) agents are rapidly progressing toward autonomous interaction and reliable task execution across diverse applications. However, two central challenges remain unresolved: automating the evaluation of agent trajectories and generating high-quality training data at scale to enable continual improvement. Existing approaches often depend on manual annotation or static rule-based verification, which restricts scalability and limits adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent reward model system that delivers adaptive trajectory evaluation, corrective feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model (GP-RM), enabling fine-grained action assessment and robust generalization across heterogeneous GUI tasks. To support reward learning at scale, we design a structured data construction pipeline that automatically produces balanced and diverse reward datasets, effectively reducing annotation costs while maintaining sample fidelity. During execution, the reward model system identifies erroneous actions, proposes refined alternatives, and continuously enhances agent behavior through an automated data-reflux mechanism. Extensive experiments demonstrate that MagicGUI-RMS yields substantial gains in task accuracy, behavioral robustness. These results establish MagicGUI-RMS as a principled and effective foundation for building self-improving GUI agents driven by reward-based adaptation.

</details>


### [49] [Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI Sustainability via Semantic Caching](https://arxiv.org/abs/2601.13186)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 论文提出TIVS-O评估框架，结合语义相似性缓存和可观测性评分，在嵌套学习架构中研究防御效果与透明度的权衡，实现零高风险漏洞同时减少41.6%的LLM调用。


<details>
  <summary>Details</summary>
Motivation: 提示注入是多智能体环境中LLM安全部署的主要障碍，现有评估框架缺乏对防御效果与透明度之间权衡的系统分析，需要更全面的安全评估方法。

Method: 扩展TIVS框架，增加语义相似性缓存和第五个指标（可观测性评分比），在HOPE启发的嵌套学习架构中实现智能体管道与连续记忆系统，使用301个合成提示进行测试。

Result: 系统实现零高风险漏洞，语义缓存减少41.6%的LLM调用，降低延迟、能耗和碳排放，五种TIVS-O配置揭示了缓解严格性与取证透明度之间的最优权衡。

Conclusion: 可观测性感知评估能揭示多智能体管道中的非单调效应，内存增强智能体可在不改动模型权重的情况下同时最大化安全鲁棒性、实时性能、运营成本和环境可持续性。

Abstract: Prompt injection remains a central obstacle to the safe deployment of large language models, particularly in multi-agent settings where intermediate outputs can propagate or amplify malicious instructions. Building on earlier work that introduced a four-metric Total Injection Vulnerability Score (TIVS), this paper extends the evaluation framework with semantic similarity-based caching and a fifth metric (Observability Score Ratio) to yield TIVS-O, investigating how defence effectiveness interacts with transparency in a HOPE-inspired Nested Learning architecture. The proposed system combines an agentic pipeline with Continuum Memory Systems that implement semantic similarity-based caching across 301 synthetically generated injection-focused prompts drawn from ten attack families, while a fourth agent performs comprehensive security analysis using five key performance indicators. In addition to traditional injection metrics, OSR quantifies the richness and clarity of security-relevant reasoning exposed by each agent, enabling an explicit analysis of trade-offs between strict mitigation and auditability. Experiments show that the system achieves secure responses with zero high-risk breaches, while semantic caching delivers substantial computational savings, achieving a 41.6% reduction in LLM calls and corresponding decreases in latency, energy consumption, and carbon emissions. Five TIVS-O configurations reveal optimal trade-offs between mitigation strictness and forensic transparency. These results indicate that observability-aware evaluation can reveal non-monotonic effects within multi-agent pipelines and that memory-augmented agents can jointly maximize security robustness, real-time performance, operational cost savings, and environmental sustainability without modifying underlying model weights, providing a production-ready pathway for secure and green LLM deployments.

</details>


### [50] [Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic Dialogues](https://arxiv.org/abs/2601.13206)
*Neil K. R. Sehgal,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.AI

TL;DR: LLMs在时间敏感场景中表现不佳，尤其是在需要持续追踪剩余时间的实时谈判中，但通过提供剩余时间信息可以显著改善表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界的沟通（如治疗会话、商业谈判）通常有时间限制，但当前LLM架构和评估协议很少测试其在实时截止时间下的时间意识能力。

Method: 使用模拟谈判实验，让成对的LLM代理在严格截止时间下进行谈判。设置两个条件：控制组（仅知道全局时间限制）和时间感知组（每轮获得剩余时间更新）。比较不同条件下的交易达成率。

Result: 时间感知条件下的交易达成率显著更高（GPT-5.1为32% vs 4%），报价接受率高出六倍。但在基于轮次的限制下，LLMs能达到接近完美的交易达成率（≥95%）。

Conclusion: LLMs在内部追踪流逝时间方面存在系统性缺陷，这限制了其在许多时间敏感应用中的部署。失败源于时间追踪能力而非战略推理能力。

Abstract: Large Language Models (LLMs) generate text token-by-token in discrete time, yet real-world communication, from therapy sessions to business negotiations, critically depends on continuous time constraints. Current LLM architectures and evaluation protocols rarely test for temporal awareness under real-time deadlines. We use simulated negotiations between paired agents under strict deadlines to investigate how LLMs adjust their behavior in time-sensitive settings. In a control condition, agents know only the global time limit. In a time-aware condition, they receive remaining-time updates at each turn. Deal closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer acceptances are sixfold higher in the time-aware condition than in the control, suggesting LLMs struggle to internally track elapsed time. However, the same LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based limits, revealing the failure is in temporal tracking rather than strategic reasoning. These effects replicate across negotiation scenarios and models, illustrating a systematic lack of LLM time awareness that will constrain LLM deployment in many time-sensitive applications.

</details>


### [51] [CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning](https://arxiv.org/abs/2601.13262)
*Eric Onyame,Akash Ghosh,Subhadip Baidya,Sriparna Saha,Xiuying Chen,Chirag Agarwal*

Main category: cs.AI

TL;DR: CURE-MED提出一个课程强化学习框架，通过代码切换感知的监督微调和组相对策略优化，在多语言医疗推理任务上显著提升语言一致性和逻辑正确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单语言数学和常识推理上表现良好，但在多语言医疗推理应用中仍不可靠，阻碍了其在多语言医疗环境中的部署。

Method: 首先引入CUREMED-BENCH数据集，包含13种语言的开源医疗推理查询；然后提出CURE-MED框架，整合代码切换感知的监督微调和组相对策略优化，共同提升逻辑正确性和语言稳定性。

Result: 在13种语言上，该方法持续超越强基线并有效扩展：7B参数模型达到85.21%语言一致性和54.35%逻辑正确性；32B参数模型达到94.96%语言一致性和70.04%逻辑正确性。

Conclusion: 这些结果支持LLMs实现可靠且公平的多语言医疗推理，促进了多语言医疗环境中AI系统的部署。

Abstract: While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/

</details>


### [52] [The Geometry of Thought: How Scale Restructures Reasoning In Large Language Models](https://arxiv.org/abs/2601.13358)
*Samuel Cyrenius Anderson*

Main category: cs.AI

TL;DR: 研究发现模型规模扩张不会均匀提升推理能力，而是会重构推理过程。通过分析25,000+思维链轨迹，发现神经缩放定律会触发领域特定的相变：法律推理发生"结晶化"，科学和数学推理保持"液态"，代码推理形成"离散格点"。几何结构可预测可学习性，并提出了神经推理算子来加速推理。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解模型规模扩张如何影响推理能力，特别是探索缩放定律是否均匀提升所有领域的推理能力，还是会导致不同领域的结构性变化。

Method: 方法包括：1) 分析25,000+思维链轨迹，涵盖法律、科学、代码、数学四个领域和8B、70B两个规模；2) 测量表示维度、轨迹对齐度、流形解缠等几何特征；3) 提出神经推理算子，学习从初始隐藏状态到最终隐藏状态的映射；4) 识别跨领域和规模的振荡特征。

Result: 主要结果：1) 法律推理发生"结晶化"：表示维度下降45%，轨迹对齐度增加31%，流形解缠度提升10倍；2) 科学和数学推理保持"液态"：几何特征在9倍参数增加下保持不变；3) 代码推理形成"离散格点"：轮廓系数从0.13提升到0.42；4) 神经推理算子在法律推理上达到63.6%的准确率；5) 发现跨领域和规模的普遍振荡特征(相干性约-0.4)。

Conclusion: 结论是推理成本由流形几何而非任务难度决定，这为在拓扑允许的情况下加速推理提供了蓝图。模型规模扩张会触发领域特定的相变，而不是均匀的能力提升。

Abstract: Scale does not uniformly improve reasoning - it restructures it. Analyzing 25,000+ chain-of-thought trajectories across four domains (Law, Science, Code, Math) and two scales (8B, 70B parameters), we discover that neural scaling laws trigger domain-specific phase transitions rather than uniform capability gains. Legal reasoning undergoes Crystallization: 45% collapse in representational dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x manifold untangling. Scientific and mathematical reasoning remain Liquid - geometrically invariant despite 9x parameter increase. Code reasoning forms a discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry predicts learnability. We introduce Neural Reasoning Operators - learned mappings from initial to terminal hidden states. In crystalline legal reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe decoding, predicting reasoning endpoints without traversing intermediate states. We further identify a universal oscillatory signature (coherence ~ -0.4) invariant across domains and scales, suggesting attention and feedforward layers drive reasoning through opposing dynamics. These findings establish that the cost of thought is determined not by task difficulty but by manifold geometry - offering a blueprint for inference acceleration where topology permits.

</details>


### [53] [A Lightweight Modular Framework for Constructing Autonomous Agents Driven by Large Language Models: Design, Implementation, and Applications in AgentForge](https://arxiv.org/abs/2601.13383)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.AI

TL;DR: AgentForge是一个轻量级开源Python框架，通过模块化架构简化LLM驱动的自主代理构建，提供可组合技能抽象、统一LLM后端接口和声明式YAML配置系统。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架存在架构僵化、供应商锁定和复杂性过高的问题，阻碍了快速原型设计和部署，需要一种更灵活、易用的解决方案来民主化自主代理开发。

Method: 提出AgentForge框架，包含三个核心创新：1) 可组合技能抽象，支持细粒度任务分解；2) 统一LLM后端接口，支持云端API和本地推理引擎无缝切换；3) 声明式YAML配置系统，分离代理逻辑与实现细节。将技能组合机制形式化为有向无环图(DAG)。

Result: 在四个基准场景的评估中，AgentForge实现了竞争性的任务完成率，相比LangChain减少62%开发时间，相比直接API集成减少78%开发时间。编排延迟低于100ms，适合实时应用。框架集成了六个内置技能，支持自定义技能开发。

Conclusion: AgentForge填补了LLM代理生态系统的重要空白，为研究人员和从业者提供了生产就绪的基础设施，用于构建、评估和部署自主代理，同时保持灵活性和性能。

Abstract: The emergence of LLMs has catalyzed a paradigm shift in autonomous agent development, enabling systems capable of reasoning, planning, and executing complex multi-step tasks. However, existing agent frameworks often suffer from architectural rigidity, vendor lock-in, and prohibitive complexity that impedes rapid prototyping and deployment. This paper presents AgentForge, a lightweight, open-source Python framework designed to democratize the construction of LLM-driven autonomous agents through a principled modular architecture. AgentForge introduces three key innovations: (1) a composable skill abstraction that enables fine-grained task decomposition with formally defined input-output contracts, (2) a unified LLM backend interface supporting seamless switching between cloud-based APIs and local inference engines, and (3) a declarative YAML-based configuration system that separates agent logic from implementation details. We formalize the skill composition mechanism as a directed acyclic graph (DAG) and prove its expressiveness for representing arbitrary sequential and parallel task workflows. Comprehensive experimental evaluation across four benchmark scenarios demonstrates that AgentForge achieves competitive task completion rates while reducing development time by 62% compared to LangChain and 78% compared to direct API integration. Latency measurements confirm sub-100ms orchestration overhead, rendering the framework suitable for real-time applications. The modular design facilitates extension: we demonstrate the integration of six built-in skills and provide comprehensive documentation for custom skill development. AgentForge addresses a critical gap in the LLM agent ecosystem by providing researchers and practitioners with a production-ready foundation for constructing, evaluating, and deploying autonomous agents without sacrificing flexibility or performance.

</details>


### [54] [Explicit Cognitive Allocation: A Principle for Governed and Auditable Inference in Large Language Models](https://arxiv.org/abs/2601.13443)
*Héctor Manuel Manzanilla-Granados,Zaira Navarrete-Cazales,Miriam Pescador-Rojas,Tonahtiu Ramírez-Romero*

Main category: cs.AI

TL;DR: 论文提出"显式认知分配"原则，通过分离和组织认知功能来结构化AI辅助推理，并实现为认知通用代理架构，在农业领域评估中显示出更好的认知收敛和工具意识。


<details>
  <summary>Details</summary>
Motivation: 当前LLM使用方式在认知上缺乏结构，将问题框架、知识探索、检索、方法意识和解释等认知功能合并到单一生成过程中，限制了可追溯性、认知控制和可重复性，特别是在高责任环境中。

Method: 提出显式认知分配原则，实现为认知通用代理架构，将推理组织为探索与框架、认知锚定、工具与方法映射、解释合成等阶段，并引入通用认知工具概念来形式化各种调查手段。

Result: 在农业领域的对比实验中，CUA引导的推理显示出更早且结构化的认知收敛、在语义扩展下更高的认知对齐度，并能系统性地暴露调查的工具景观，而基线LLM推理在认知对齐上表现出更大变异性且无法显式呈现工具结构。

Conclusion: 显式认知分配原则和CUA架构能够有效结构化AI辅助推理，提高认知控制、可追溯性和可重复性，为高责任环境中的AI辅助推理提供了系统化框架。

Abstract: The rapid adoption of large language models (LLMs) has enabled new forms of AI-assisted reasoning across scientific, technical, and organizational domains. However, prevailing modes of LLM use remain cognitively unstructured: problem framing, knowledge exploration, retrieval, methodological awareness, and explanation are typically collapsed into a single generative process. This cognitive collapse limits traceability, weakens epistemic control, and undermines reproducibility, particularly in high-responsibility settings.
  We introduce Explicit Cognitive Allocation, a general principle for structuring AI-assisted inference through the explicit separation and orchestration of epistemic functions. We instantiate this principle in the Cognitive Universal Agent (CUA), an architecture that organizes inference into distinct stages of exploration and framing, epistemic anchoring, instrumental and methodological mapping, and interpretive synthesis. Central to this framework is the notion of Universal Cognitive Instruments (UCIs), which formalize heterogeneous means, including computational, experimental, organizational, regulatory, and educational instruments, through which abstract inquiries become investigable.
  We evaluate the effects of explicit cognitive and instrumental allocation through controlled comparisons between CUA-orchestrated inference and baseline LLM inference under matched execution conditions. Across multiple prompts in the agricultural domain, CUA inference exhibits earlier and structurally governed epistemic convergence, higher epistemic alignment under semantic expansion, and systematic exposure of the instrumental landscape of inquiry. In contrast, baseline LLM inference shows greater variability in alignment and fails to explicitly surface instrumental structure.

</details>


### [55] [Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental Health via Multi-Agent Instruction Refinement](https://arxiv.org/abs/2601.13481)
*Jian Zhang,Zhangqi Wang,Zhiyuan Wang,Weiping Fu,Yu He,Haiping Zhu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: APOLO是一个用于精神健康领域情感诊断的自动化提示优化框架，通过多智能体协作机制解决情感共病和临床线索探索效率问题，提升LLM在医疗场景下的诊断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在情感分析任务中表现出色，但在高风险、上下文密集的医疗环境中，其诊断可靠性高度依赖提示设计。现有方法面临情感共病（多种情感状态交织）和临床相关线索探索效率低两大挑战。

Method: 提出APOLO框架，将指令优化建模为部分可观测马尔可夫决策过程，采用多智能体协作机制（规划者、教师、批评者、学生、目标者），在闭环框架中迭代优化提示，提升推理稳定性和有效性。

Result: 实验结果表明，APOLO在领域特定和分层基准测试中持续提升诊断准确性和鲁棒性，为精神健康领域可信赖的LLM应用提供了可扩展和可泛化的范式。

Conclusion: APOLO通过系统化的提示优化和多智能体协作，有效解决了医疗情感诊断中的关键挑战，为高风险医疗环境中的LLM应用提供了可靠解决方案。

Abstract: Linguistic expressions of emotions such as depression, anxiety, and trauma-related states are pervasive in clinical notes, counseling dialogues, and online mental health communities, and accurate recognition of these emotions is essential for clinical triage, risk assessment, and timely intervention. Although large language models (LLMs) have demonstrated strong generalization ability in emotion analysis tasks, their diagnostic reliability in high-stakes, context-intensive medical settings remains highly sensitive to prompt design. Moreover, existing methods face two key challenges: emotional comorbidity, in which multiple intertwined emotional states complicate prediction, and inefficient exploration of clinically relevant cues. To address these challenges, we propose APOLO (Automated Prompt Optimization for Linguistic Emotion Diagnosis), a framework that systematically explores a broader and finer-grained prompt space to improve diagnostic efficiency and robustness. APOLO formulates instruction refinement as a Partially Observable Markov Decision Process and adopts a multi-agent collaboration mechanism involving Planner, Teacher, Critic, Student, and Target roles. Within this closed-loop framework, the Planner defines an optimization trajectory, while the Teacher-Critic-Student agents iteratively refine prompts to enhance reasoning stability and effectiveness, and the Target agent determines whether to continue optimization based on performance evaluation. Experimental results show that APOLO consistently improves diagnostic accuracy and robustness across domain-specific and stratified benchmarks, demonstrating a scalable and generalizable paradigm for trustworthy LLM applications in mental healthcare.

</details>


### [56] [AgenticRed: Optimizing Agentic Systems for Automated Red-teaming](https://arxiv.org/abs/2601.13518)
*Jiayi Yuan,Jonathan Nöther,Natasha Jaques,Goran Radanović*

Main category: cs.AI

TL;DR: AgenticRed：一种利用LLM上下文学习自动设计和优化红队系统的框架，无需人工干预，通过进化选择演化智能体系统，显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法依赖人工指定的工作流程，存在人为偏见且探索设计空间成本高昂，需要一种能自动设计和优化红队系统的框架。

Method: 将红队视为系统设计问题，利用LLM的上下文学习能力迭代设计和优化红队系统，采用进化选择方法演化智能体系统结构。

Result: 在Llama-2-7B上达到96%攻击成功率（提升36%），在Llama-3-8B上达到98%，对GPT-3.5-Turbo和GPT-4o-mini达到100%成功率，对Claude-Sonnet-3.5达到60%（提升24%）。

Conclusion: 自动化系统设计是AI安全评估的强大范式，能够跟上模型快速演进的步伐，为红队测试提供了更有效的方法。

Abstract: While recent automated red-teaming methods show promise for systematically exposing model vulnerabilities, most existing approaches rely on human-specified workflows. This dependence on manually designed workflows suffers from human biases and makes exploring the broader design space expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs' in-context learning to iteratively design and refine red-teaming systems without human intervention. Rather than optimizing attacker policies within predefined structures, AgenticRed treats red-teaming as a system design problem. Inspired by methods like Meta Agent Search, we develop a novel procedure for evolving agentic systems using evolutionary selection, and apply it to the problem of automatic red-teaming. Red-teaming systems designed by AgenticRed consistently outperform state-of-the-art approaches, achieving 96% attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B on HarmBench. Our approach exhibits strong transferability to proprietary models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on Claude-Sonnet-3.5 (24% improvement). This work highlights automated system design as a powerful paradigm for AI safety evaluation that can keep pace with rapidly evolving models.

</details>


### [57] [AgentGC: Evolutionary Learning-based Lossless Compression for Genomics Data with LLM-driven Multiple Agent](https://arxiv.org/abs/2601.13559)
*Sun Hui,Ding Yanfeng,Huidong Ma,Chang Xu,Keyan Jin,Lizheng Zu,Cheng Zhong,xiaoguang Liu,Gang Wang,Wentong Cai*

Main category: cs.AI

TL;DR: AgentGC：首个基于智能体的进化式基因组数据压缩器，通过三层多智能体架构（Leader和Worker）实现用户友好界面、认知优化和自动化压缩，在压缩比和吞吐量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的基因组数据压缩方法存在非进化性、低级压缩建模、适应性有限和用户界面不友好等问题，需要一种更智能、可进化的压缩解决方案。

Method: 提出AgentGC三层架构：1）用户层通过Leader结合LLM提供友好界面；2）认知层由Leader驱动，集成LLM进行算法-数据集-系统联合优化；3）压缩层由Worker负责，通过自动化多知识学习框架执行压缩解压。支持三种模式：CP（压缩比优先）、TP（吞吐量优先）、BM（平衡模式）。

Result: 在9个数据集上与14个基线方法比较，平均压缩比提升16.66%、16.11%、16.33%，吞吐量提升4.73倍、9.23倍、9.15倍。

Conclusion: AgentGC通过智能体架构和LLM集成，成功解决了现有基因组数据压缩方法的局限性，在压缩性能和用户体验方面都取得了显著改进。

Abstract: Lossless compression has made significant advancements in Genomics Data (GD) storage, sharing and management. Current learning-based methods are non-evolvable with problems of low-level compression modeling, limited adaptability, and user-unfriendly interface. To this end, we propose AgentGC, the first evolutionary Agent-based GD Compressor, consisting of 3 layers with multi-agent named Leader and Worker. Specifically, the 1) User layer provides a user-friendly interface via Leader combined with LLM; 2) Cognitive layer, driven by the Leader, integrates LLM to consider joint optimization of algorithm-dataset-system, addressing the issues of low-level modeling and limited adaptability; and 3) Compression layer, headed by Worker, performs compression & decompression via a automated multi-knowledge learning-based compression framework. On top of AgentGC, we design 3 modes to support diverse scenarios: CP for compression-ratio priority, TP for throughput priority, and BM for balanced mode. Compared with 14 baselines on 9 datasets, the average compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains are 4.73x, 9.23x, and 9.15x, respectively.

</details>


### [58] [Motion-to-Response Content Generation via Multi-Agent AI System with Real-Time Safety Verification](https://arxiv.org/abs/2601.13589)
*HyeYoung Lee*

Main category: cs.AI

TL;DR: 提出一个基于音频情感信号实时生成响应式媒体内容的多智能体AI系统，通过四个协作智能体实现情感识别到安全可控内容的转换，包含安全验证循环确保年龄适宜性。


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别研究主要关注分类准确性，但缺乏将识别出的情感状态转化为安全、年龄适宜、可控的响应内容的能力。需要一种能够实时生成响应式媒体内容并确保安全性的系统。

Method: 采用四智能体协作架构：1)基于CNN的情感识别智能体提取声学特征；2)响应策略决策智能体将情感映射到响应模式；3)内容参数生成智能体产生媒体控制参数；4)安全验证智能体强制执行年龄适宜性和刺激约束。系统包含显式的安全验证循环，在输出前过滤生成内容。

Result: 在公开数据集上，系统达到73.2%的情感识别准确率、89.4%的响应模式一致性、100%的安全合规性，同时保持低于100ms的推理延迟，适合设备端部署。

Conclusion: 该模块化架构提供了可解释性和可扩展性，适用于儿童相关媒体、治疗应用和情感响应智能设备，实现了从情感识别到安全可控内容生成的完整流程。

Abstract: This paper proposes a multi-agent artificial intelligence system that generates response-oriented media content in real time based on audio-derived emotional signals. Unlike conventional speech emotion recognition studies that focus primarily on classification accuracy, our approach emphasizes the transformation of inferred emotional states into safe, age-appropriate, and controllable response content through a structured pipeline of specialized AI agents. The proposed system comprises four cooperative agents: (1) an Emotion Recognition Agent with CNN-based acoustic feature extraction, (2) a Response Policy Decision Agent for mapping emotions to response modes, (3) a Content Parameter Generation Agent for producing media control parameters, and (4) a Safety Verification Agent enforcing age-appropriateness and stimulation constraints. We introduce an explicit safety verification loop that filters generated content before output, ensuring compliance with predefined rules. Experimental results on public datasets demonstrate that the system achieves 73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100% safety compliance while maintaining sub-100ms inference latency suitable for on-device deployment. The modular architecture enables interpretability and extensibility, making it applicable to child-adjacent media, therapeutic applications, and emotionally responsive smart devices.

</details>


### [59] [DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems](https://arxiv.org/abs/2601.13591)
*Maojun Sun,Yifei Xie,Yue Wu,Ruijian Han,Binyan Jiang,Defeng Sun,Yancheng Yuan,Jian Huang*

Main category: cs.AI

TL;DR: DSAEval是一个包含641个真实世界数据科学问题的基准测试，基于285个多样化数据集，涵盖结构化和非结构化数据，具有多模态环境感知、多查询交互和多维度评估三大特点。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的数据代理旨在自动化数据科学任务，但真实世界数据科学问题的开放性、跨多个分类且缺乏标准答案的特性给评估带来了重大挑战。

Method: 提出DSAEval基准测试，包含641个真实世界数据科学问题，基于285个多样化数据集，涵盖结构化和非结构化数据（视觉和文本）。该基准具有三大特点：多模态环境感知、多查询交互和多维度评估。

Result: 评估了11个先进的代理LLM，结果显示Claude-Sonnet-4.5整体性能最强，GPT-5.2最有效率，MiMo-V2-Flash最具成本效益。多模态感知在视觉相关任务上能提升2.04%到11.30%的性能。

Conclusion: 当前数据科学代理在结构化数据和常规数据分析工作流上表现良好，但在非结构化领域仍面临重大挑战。研究为数据科学代理的发展提供了关键见解和未来研究方向。

Abstract: Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.

</details>


### [60] [Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games](https://arxiv.org/abs/2601.13709)
*Christopher Kao,Vanshika Vats,James Davis*

Main category: cs.AI

TL;DR: LLM代理在社交推理游戏《黑手党》中表现出比人类更强的欺骗能力，通过异步多智能体框架模拟真实社交环境，GPT-4o智能体的欺骗质量更高，更难被检测。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在更多应用中使用，对其安全性的担忧增加。虽然之前研究表明LLM在受控任务中会欺骗，但对其在自然语言社交环境中的欺骗能力了解较少。本研究旨在探索LLM在社交推理游戏中的欺骗表现。

Method: 使用异步多智能体框架模拟35场《黑手党》游戏，GPT-4o作为玩家。创建基于GPT-4-Turbo的"黑手党检测器"，在不提供玩家角色信息的情况下分析游戏对话记录来预测黑手党玩家。将预测准确率作为欺骗质量的替代指标，并与28场人类游戏和随机基线进行比较。

Result: 黑手党检测器在LLM游戏中的预测准确率低于人类游戏，表明LLM能更好地融入群体，欺骗效果更强。这一结果在不同游戏天数和检测到的黑手党数量上保持一致。

Conclusion: LLM在社交环境中的欺骗能力比人类更复杂和有效，这突显了LLM欺骗的风险和复杂性。研究发布了LLM黑手党对话数据集以支持未来研究。

Abstract: Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.

</details>


### [61] [DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution](https://arxiv.org/abs/2601.13761)
*Shengda Fan,Xuyan Ye,Yankai Lin*

Main category: cs.AI

TL;DR: DARC是一个两阶段自进化框架，通过解耦问题生成与解答训练，解决自对弈中的优化不稳定问题，在多个推理基准上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自对弈框架存在优化不稳定问题，主要源于：(1) 问题生成器的目标非平稳性（依赖于解答器的奖励反馈），(2) 解答器训练中的自生成伪标签带来的引导误差。

Method: 提出两阶段框架：第一阶段训练问题生成器合成难度校准的问题（基于明确难度级别和外部语料）；第二阶段采用非对称自蒸馏机制训练解答器，其中文档增强的教师模型生成高质量伪标签来监督无文档访问的学生解答器。

Result: DARC具有模型无关性，在9个推理基准和3个骨干模型上平均提升10.9个点，持续优于所有基线方法，接近完全监督模型的性能且无需人工标注。

Conclusion: DARC通过解耦问题生成与解答训练，稳定了自进化过程，为自改进AI提供了有效的解决方案。

Abstract: Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.

</details>


### [62] [LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health](https://arxiv.org/abs/2601.13880)
*Ye Tian,Zihao Wang,Onat Gungor,Xiaoran Fan,Tajana Rosing*

Main category: cs.AI

TL;DR: LifeAgentBench是一个大规模QA基准，用于评估LLM在长期、跨维度、多用户生活方式健康推理方面的能力，包含22,573个问题，并提出了LifeAgent作为强基线代理。


<details>
  <summary>Details</summary>
Motivation: 个性化数字健康支持需要对异构生活方式信号进行长期跨维度推理，但当前LLM在此场景下的能力尚不明确，缺乏系统性基准评估。

Method: 1) 构建LifeAgentBench基准，包含22,573个从基础检索到复杂推理的问题；2) 发布可扩展的基准构建流程和标准化评估协议；3) 系统评估11个领先LLM；4) 提出LifeAgent代理，集成多步证据检索与确定性聚合。

Result: 1) 识别出LLM在长期聚合和跨维度推理方面的关键瓶颈；2) LifeAgent相比两个广泛使用的基线有显著改进；3) 案例研究展示了其在现实日常场景中的潜力。

Conclusion: LifeAgentBench为评估LLM健康助手提供了可靠且可扩展的基准，LifeAgent代理在解决长期跨维度推理问题上表现出色，为未来健康助手研究提供了强基线。

Abstract: Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.

</details>


### [63] [Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval](https://arxiv.org/abs/2601.13969)
*Joaquín Polonuer,Lucas Vittor,Iñaki Arango,Ayush Noori,David A. Clifton,Luciano Del Corro,Marinka Zitnik*

Main category: cs.AI

TL;DR: ARK是一个自适应知识图谱检索器，通过语言模型控制广度-深度权衡，使用全局搜索和邻域探索两种操作，在STaRK基准上显著优于现有方法，并能通过无标签模仿蒸馏到小模型中。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱检索方法存在局限性：基于相似性的检索器覆盖广但深度浅，基于遍历的方法依赖种子节点选择且在多实体多关系查询中容易失败。需要一种能自适应平衡广度搜索和深度遍历的检索方法。

Method: ARK采用代理式知识图谱检索器，让语言模型通过两种操作工具控制检索：全局词汇搜索（用于语言密集型查询）和一跳邻域探索（用于关系密集型查询）。这两种操作可以组合实现多跳遍历，无需预设跳数或检索训练。

Result: 在STaRK基准上，ARK达到59.1%的平均Hit@1和67.4的平均MRR，比检索基线和无训练代理方法分别提升最高31.4%的Hit@1和28.0%的MRR。通过无标签模仿蒸馏到8B模型，在AMAZON、MAG和PRIME数据集上分别比基础8B模型提升+7.0、+26.6和+13.5个绝对百分点的Hit@1，同时保留高达98.5%的教师模型性能。

Conclusion: ARK通过自适应平衡广度搜索和深度遍历，显著提升了知识图谱检索性能。其工具使用轨迹可以通过无标签模仿有效蒸馏到更小的语言模型中，实现高效的知识转移。

Abstract: Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.

</details>


### [64] [Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics](https://arxiv.org/abs/2601.14027)
*Junqi Liu,Zihao Zhou,Zekai Zhu,Marco Dos Santos,Weikun He,Jiawei Liu,Ran Wang,Yunzhou Xie,Junqiao Zhao,Qiufeng Wang,Lihong Zhi,Jia Li,Wenda Li*

Main category: cs.AI

TL;DR: 提出使用通用代码代理作为形式数学推理器的新范式，通过Numina-Lean-Agent实现，在Putnam 2025中取得12/12的满分成绩，并成功形式化Brascamp-Lieb定理。


<details>
  <summary>Details</summary>
Motivation: 现有形式定理证明方法依赖任务特定流水线和训练过的形式证明器，限制了灵活性和可复现性。通用代码代理能提供超越证明的多样化推理任务接口，仅通过替换基础模型即可提升性能，且MCP支持灵活扩展和自主调用专业工具。

Method: 提出直接使用通用代码代理作为形式数学推理器的新范式。具体实现Numina-Lean-Agent，结合Claude Code与Numina-Lean-MCP，实现与Lean的自主交互、相关定理检索、非形式化证明和辅助推理工具调用。

Result: 使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent在Putnam 2025中解决了所有问题（12/12），与最佳闭源系统持平。此外，通过与数学家交互成功形式化了Brascamp-Lieb定理，展示了其通用性。

Conclusion: 通用代码代理作为形式数学推理器的新范式具有显著优势，Numina-Lean-Agent在基准测试和实际数学形式化任务中都表现出色，为灵活、可复现的形式推理系统提供了新方向。

Abstract: Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.

</details>


### [65] [Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance](https://arxiv.org/abs/2601.14171)
*Qianli Ma,Chang Guo,Zhiheng Tian,Siyu Wang,Jipeng Xiao,Yuanhao Yue,Zhipeng Zhang*

Main category: cs.AI

TL;DR: RebuttalAgent是一个多智能体框架，将反驳生成重构为以证据为中心的规划任务，通过分解评审意见、构建混合上下文和集成外部搜索，确保每个论点都有明确的证据支撑。


<details>
  <summary>Details</summary>
Motivation: 撰写有效的反驳信是一项高风险任务，需要准确理解评审意图和稿件细节。现有解决方案通常将其视为直接文本生成问题，存在幻觉、遗漏批评和缺乏可验证依据等问题。

Method: 提出RebuttalAgent多智能体框架：1) 将复杂反馈分解为原子化关切点；2) 动态构建混合上下文，结合压缩摘要和高保真文本；3) 集成自主按需外部搜索模块解决需要外部文献的问题；4) 在起草前生成可检查的响应计划。

Result: 在提出的RebuttalBench上验证，该管道在覆盖率、忠实度和策略连贯性方面优于强基线，为同行评审过程提供了透明可控的助手。

Conclusion: RebuttalAgent通过证据中心的规划方法解决了现有反驳生成系统的局限性，提供了更可靠、可验证和可控的同行评审辅助工具。

Abstract: Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.

</details>


### [66] [Toward Efficient Agents: Memory, Tool learning, and Planning](https://arxiv.org/abs/2601.14192)
*Xiaofang Yang,Lijun Li,Heng Zhou,Tong Zhu,Xiaoye Qu,Yuchen Fan,Qianshan Wei,Rui Ye,Li Kang,Yiran Qin,Zhiqiang Kou,Daizong Liu,Qi Li,Ning Ding,Siheng Chen,Jing Shao*

Main category: cs.AI

TL;DR: 该论文系统研究了智能体系统的效率问题，从内存、工具学习和规划三个核心组件出发，分析了成本（延迟、token数、步骤等），提出了效率的两种评估方式，并总结了相关基准和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向智能体系统扩展，现有研究主要关注有效性而忽视了效率，而效率对于实际部署至关重要。本文旨在填补这一空白，系统研究智能体系统的效率问题。

Method: 从智能体的三个核心组件（内存、工具学习、规划）出发，综述了多种提高效率的方法，包括上下文压缩管理、设计减少工具调用的强化学习奖励、控制搜索机制等。提出了两种效率评估方式：固定成本预算下的有效性比较，以及相似有效性水平下的成本比较。

Result: 总结了智能体效率研究的现状，识别了不同方法背后的共同原则，整理了相关基准评估协议和常用效率指标，建立了效率与有效性之间的帕累托前沿分析框架。

Conclusion: 智能体效率是实际部署的关键因素，需要系统研究。本文为效率研究提供了全面的分析框架，总结了现有方法、评估指标和基准，并指出了未来的研究挑战和方向。

Abstract: Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [67] [Bugbot's Code Review Evolution](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbuilding-bugbot%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/g3Bq7D_m3HbXyJ_zQjOnvx7CDk2mWDMexpuhRIhf_T8=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor的Bugbot已进化为能检测逻辑、性能和安全性问题的AI代码审查工具


<details>
  <summary>Details</summary>
Motivation: 传统代码审查耗时且容易遗漏问题，需要自动化工具来提高代码质量和开发效率

Method: Bugbot作为AI驱动的代码审查代理，分析pull requests中的代码，检测逻辑错误、性能问题和安全漏洞

Result: Bugbot已进化为能够有效识别多种代码问题的AI代码审查工具

Conclusion: AI代码审查工具如Bugbot能够显著提高代码质量和开发效率

Abstract: Bugbot's Code Review Evolution (5 minute read) Cursor's Bugbot has evolved into a capable AI code reviewer that detects logic, performance, and security issues in pull requests.

</details>


### [68] [The Bitter Lesson of Agent Frameworks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FnPJc6z/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/83dyNf4mcJ1ddoFXXifAE6StoeYxCMGJmaS9mzNvzrc=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者认为智能体框架是不必要的，智能体本质上只是消息的for循环。模型只需要完整的动作空间、循环、显式退出和上下文管理。每个抽象都是负担，每个"助手"都是失败点。


<details>
  <summary>Details</summary>
Motivation: 当前智能体框架过度复杂化，引入了不必要的抽象层和依赖，反而降低了系统的可靠性和效率。作者希望揭示智能体开发的本质，提倡极简主义的设计哲学。

Method: 提出极简主义方法：智能体只需要四个核心组件：1）完整的动作空间，2）消息循环（for-loop），3）显式退出机制，4）上下文管理。避免使用复杂的框架和抽象层。

Result: 通过简化设计，智能体系统变得更加可靠、高效且易于维护。减少构建的内容反而能获得更好的工作效果。

Conclusion: 智能体框架通常是不必要的复杂性来源。最有效的智能体实现应该基于极简主义原则，避免过度工程化，专注于核心的消息循环和动作执行机制。

Abstract: The Bitter Lesson of Agent Frameworks (12 minute read) An agent is just a for-loop of messages. Agent frameworks aren't required. Models only need a complete action space, a for-loop, an explicit exit, and context management. Every abstraction is a liability, and every 'helper' is a failure point. The less you build, the more it works.

</details>


### [69] [Benchmarking AI Agent Memory: Is a Filesystem All You Need?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.letta.com%2Fblog%2Fbenchmarking-ai-agent-memory%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/Gh3qSqbY708mrH60zIeaCkNgAtuxkC4nRyfjQ7dk5aA=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究表明，对于设计良好的AI智能体，简单的文件系统工具足以满足其记忆需求，而更复杂的记忆工具可以通过MCP或自定义工具集成。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体的记忆能力依赖于其架构、工具和底层模型，但业界对于智能体记忆系统的最佳实践缺乏系统性的基准测试和评估。

Method: 通过基准测试方法，比较不同记忆工具（从简单文件系统到复杂记忆系统）在AI智能体中的表现，并探讨通过MCP（模型控制协议）或自定义工具集成复杂记忆功能的可能性。

Result: 研究发现，对于设计良好的智能体，简单的文件系统工具已经足够有效，而更复杂的记忆工具可以作为可选插件通过标准化接口集成。

Conclusion: 智能体的记忆性能更多取决于整体架构设计而非记忆工具的复杂度，开发者可以从简单的文件系统开始，再根据需要逐步集成更复杂的记忆功能。

Abstract: Benchmarking AI Agent Memory: Is a Filesystem All You Need? (6 minute read) An agent's memory depends on its architecture, tools, and the underlying model. You can always mix and match frameworks, tools, and models. Simple file system tools are sufficient for a well-designed agent. More complex memory tools can be plugged in via MCP or custom tools.

</details>


### [70] [Uniqueness-Aware RL for LLM Diversity](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2601.08763%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/fGR9o0QYvm2oJck2kaT5PIVtU3XV4p4UcnzoXGH3UBY=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MIT研究者提出了一种新的强化学习方法，通过基于LLM的轨迹聚类来奖励多样化的高层解决方案策略，在不降低pass@1的情况下提高pass@k性能


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的代码生成方法往往产生同质化的解决方案，缺乏多样性，限制了pass@k性能的提升潜力

Method: 提出独特性感知强化学习方法，使用LLM对生成的代码轨迹进行聚类，识别不同的高层解决方案策略，并在强化学习奖励中引入多样性奖励项

Result: 该方法在多个任务上显著提高了pass@k性能，同时保持了pass@1性能不下降，证明了多样性奖励的有效性

Conclusion: 通过奖励多样化的解决方案策略可以显著提升代码生成的多样性，从而提高整体性能，为代码生成任务提供了新的优化方向

Abstract: Uniqueness-Aware RL for LLM Diversity (14 minute read) MIT researchers have proposed a new reinforcement learning method that rewards diverse high-level solution strategies using LLM-based clustering of rollouts. This method improves pass@k across tasks without degrading pass@1.

</details>


### [71] [The Code-Only Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frijnard.com%2Fblog%2Fthe-code-only-agent%3Futm_source=tldrai/1/0100019bd6a06275-bdb817f7-7835-4355-bb98-4bb198cacc76-000000/YFKB1v8wpUsROJ2HzKFkFFI1s_n5Gv1K_aFUQGZxM2g=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出一种"纯代码代理"模式，让AI代理通过编写和运行代码来响应每个提示，强制代理生成可执行的代码证明而非直接回答。


<details>
  <summary>Details</summary>
Motivation: 当前代理开发生态系统过于复杂，开发者被推向复杂的"正确"实现方式。作者希望探索一种更简单直接的代理构建模式。

Method: 采用纯代码代理模式，代理无法直接做任何有生产力的事情，必须为每个提示编写和运行代码。这种方法强制代理生成代码见证，将工作以非常字面的方式编码化。

Result: 该方法产生答案的代码见证，将工作以代码形式具体化。强制代理生成证明、运行证明并解释结果。

Conclusion: 纯代码代理模式提供了一种简化代理开发的方法，通过强制代码生成和执行的机制，确保代理输出的可验证性和可追溯性。

Abstract: The Code-Only Agent (17 minute read) Building an agent can be overwhelming. The ecosystem pushes developers toward complexity and the 'right' way to do things. This post explores a prompting pattern where agents create and run code as a response to every prompt. In this framework, agents can't do anything productive without writing code. This method produces a code witness of an answer, codifying the work in a very literal sense. It forces agents to produce proofs, run proofs, and interpret p...

</details>


### [72] [AMA: Inside the OWASP Top 10 for Agentic Applications](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OWASP发布了针对自主AI代理应用的安全风险框架Top 10，并举办AMA网络研讨会介绍如何将其整合到威胁模型和安全流程中


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理应用的快速发展，行业需要标准化的安全风险框架来评估和管理这些系统的安全风险，OWASP为此制定了专门针对代理应用的安全标准

Method: 通过专家团队制定OWASP Top 10风险框架，并通过AMA网络研讨会形式向行业介绍框架内容、风险类别含义以及如何将其整合到实际安全工作中

Result: 建立了首个针对自主AI代理应用的同行评审安全风险框架，为行业提供了标准化的风险评估和管理工具

Conclusion: OWASP Top 10 for Agentic Applications为AI代理应用安全提供了重要标准，帮助组织系统化地识别和管理相关安全风险

Abstract: AMA: Inside the OWASP Top 10 for Agentic Applications (Sponsor) The industry now has a peer-reviewed risk framework for autonomous, tool-using AI agents. In this live AMA-style webinar on Wed. 1/28, engage with Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler - security experts who helped shape the new OWASP standard. Learn more about: What each risk category means in practice Concrete steps to map the Top 10 into your threat models, control frameworks, and SOC workflows Ho...

</details>


### [73] [Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=infosec-primary-jan19-cta1/2/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/ligKyRL5dcfvS-vkU34wI0Ctbc0mW_eu52zPMUVCFCA=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OWASP发布了针对自主AI代理的风险框架Top 10，并举办AMA网络研讨会，让安全专家讲解如何将风险类别映射到实际威胁模型和控制框架中。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的广泛应用，行业需要一个经过同行评审的风险框架来识别和管理相关安全风险，OWASP为此制定了专门针对工具使用AI代理的Top 10风险标准。

Method: 通过举办AMA式网络研讨会，邀请参与制定OWASP标准的安全专家（Chris Hughes、Steve Wilson、Michael Bargury、Kayla Underkoffler）讲解风险框架，包括风险类别的实际含义以及如何将其映射到威胁模型、控制框架和安全运营中心工作流程中。

Result: 行业现在有了针对自主、工具使用AI代理的同行评审风险框架，安全专家通过AMA研讨会向从业者传授如何实际应用这一框架来增强AI代理的安全性。

Conclusion: OWASP Top 10 for Agentic Applications为AI代理安全提供了标准化风险框架，通过专家指导帮助组织将安全控制措施集成到AI代理开发和部署流程中。

Abstract: AMA: Inside the OWASP Top 10 for Agentic Applications (Sponsor) The industry now has a peer-reviewed risk framework for autonomous, tool-using AI agents. In this live AMA-style webinar on Wed. 1/28, engage with Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler - security experts who helped shape the new OWASP standard. Learn more about: What each risk category means in practice Concrete steps to map the Top 10 into your threat models, control frameworks, and SOC workflows Ho...

</details>


### [74] [Claude Cowork Exfiltrates Files](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.promptarmor.com%2Fresources%2Fclaude-cowork-exfiltrates-files%3Futm_source=tldrinfosec/1/0100019bd6ac7b4f-fc533f2e-5517-439e-bd32-7fc9f87ca325-000000/GuyGYGUxPNZNHTB8OGLR1J2a5PIWCeIW7tICh-SCar8=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Claude Cowork代理存在文件上传漏洞，可通过间接提示注入攻击泄露敏感文档


<details>
  <summary>Details</summary>
Motivation: 揭示Claude Cowork这一新型通用代理在文件上传功能中的安全漏洞，该漏洞允许攻击者通过间接提示注入窃取敏感文件

Method: 利用Claude沙箱中先前报告但未修复的漏洞，该漏洞允许网络连接到Claude API，通过上传文件进行间接提示注入攻击

Result: 成功演示了Claude Cowork可以被滥用来泄露敏感文档，证明该代理存在严重的安全风险

Conclusion: Claude Cowork作为研究预览版发布，存在已知但未修复的安全漏洞，需要立即修复以防止敏感数据泄露

Abstract: Claude Cowork Exfiltrates Files (3 minute read) Claude Cowork is a new general-purpose agent now available in research preview. It is vulnerable to indirect prompt injection via uploaded files and can be abused to exfiltrate sensitive documents. This attack exploits a previously reported, unremediated vulnerability in the Claude sandbox that allows network connections to the Claude API.

</details>


### [75] [I was a top 0.01% Cursor user. Here's why I switched to Claude Code 2.0](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.silennai.com%2Fclaude-code%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/E1AaxAziEC3YHWkrrnyyfmQgbBPTHJlrP4SXQneYJVI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者从Cursor转向Claude Code 2.0，因为后者具有更灵活强大的框架、改进的用户体验和更少bug，特别是Opus 4.5的RLHF训练让用户只需测试行为而无需逐行审查代码。


<details>
  <summary>Details</summary>
Motivation: 作者作为顶级Cursor用户，发现Claude Code 2.0在代码生成方面有显著优势，特别是其强化学习训练让非专业开发者也能轻松使用，只需关注输出结果而无需深入代码细节。

Method: 通过实际使用体验对比，分析Claude Code 2.0的技术特点，包括其灵活强大的框架、改进的用户界面、减少的bug数量，以及Opus 4.5模型通过RLHF训练带来的行为测试能力。

Result: Claude Code 2.0相比Cursor提供了更优越的开发体验，用户不再需要逐文件或逐函数审查代码，只需测试行为即可，甚至非程序员也能有效使用，这完全改变了代码辅助工具的范式。

Conclusion: Claude Code 2.0代表了代码生成工具的重要进步，其基于行为测试的方法降低了使用门槛，使更广泛的用户群体能够受益于AI代码辅助，标志着从代码审查到行为验证的转变。

Abstract: I was a top 0.01% Cursor user. Here's why I switched to Claude Code 2.0 (32 minute read) Claude Code 2.0 has a flexible and robust harness with an evolved UX and fewer bugs. The RLHF Anthropic did on Opus 4.5 completely changed the equation. Users no longer need to review code or instruct the model at the level of files or functions: they can just test behaviors instead. Claude Code can be used by developers who never plan on learning how to code and just care about outputs.

</details>


### [76] [Agent Psychosis: Are We Going Insane?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2026%2F1%2F18%2Fagent-psychosis%2F%3Futm_source=tldrnewsletter/1/0100019bdb259f26-0b912e7d-3876-48c9-be8b-fe370c495c64-000000/Gx33uStNMK8WEFFts2xOPazDp3_7JPnJzV3S--okkUI=440)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理如果盲目使用会成为不可靠的"垃圾生成器"，需要批判性思维来避免"代理精神病"


<details>
  <summary>Details</summary>
Motivation: 当前AI代理系统存在盲目信任和过度依赖的问题，用户如果不加批判地接受代理输出，会导致不可靠的结果

Method: 通过批判性分析和用户参与来评估AI代理的可靠性，强调需要保持人类判断力

Result: 指出盲目使用AI代理会产生不可靠输出，需要人类监督和批判性思维来确保质量

Conclusion: AI代理需要人类监督和批判性思维，不能完全依赖自动化，否则会产生"代理精神病"现象

Abstract: Agent Psychosis: Are We Going Insane? (12 minute read) AI agents are massive slop machines if you turn off your brain and let go immediately.

</details>


### [77] [AI Not Ready to Replace Junior Devs Says Ruby on Rails Creator](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finalroundai.com%2Fblog%2Fai-can-not-replace-junior-programmers%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/zYlL-eTlELaDwYg3U-4Xgz8blpsEGlF9ttGDJiAsR4g=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ruby on Rails创始人认为AI编码工具尚无法替代初级开发者，因为AI生成代码质量不稳定、结构差、缺乏系统理解，只是"闪烁的灯泡"偶尔出色但经常不可靠


<details>
  <summary>Details</summary>
Motivation: 针对当前AI编码工具被过度炒作可能替代初级开发者的现象，Ruby on Rails创始人基于实际观察提出批判性观点，强调AI工具的局限性

Method: 基于作者作为资深开发者和Ruby on Rails创始人的实践经验观察，对AI编码工具的实际表现进行定性分析

Result: AI编码工具存在严重不一致性，生成代码质量差、结构混乱、难以维护，缺乏对系统的真正理解，无法可靠替代初级开发者

Conclusion: AI编码工具目前还不够成熟，不能替代初级开发者，需要更多时间发展才能真正成为可靠的编程助手

Abstract: AI Not Ready to Replace Junior Devs Says Ruby on Rails Creator (8 minute read) The creator of Ruby on Rails says that AI coding tools are not yet ready to replace junior developers due to their inconsistency and lack of quality. AI is a "flickering light bulb," occasionally brilliant but often producing code that is poorly structured, hard to maintain, and lacks true system understanding.

</details>


### [78] [A brief history of ralph](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.humanlayer.dev%2Fblog%2Fbrief-history-of-ralph%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/REiuVqRZaM5YKmuqjMFo9dGFexT9cfxAUZs_bZ9qum4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ralph Wiggum Technique是一种在2025年底流行的智能编码方法，最初是简单的bash循环用于持续AI执行，后来演变为在重构、规范生成和项目设置等多个应用场景中非常有用。


<details>
  <summary>Details</summary>
Motivation: 该技术旨在通过自动化AI执行流程来提高编码效率，特别是针对大规模代码重构等特定任务，解决传统手动编码的耗时问题。

Method: 最初采用简单的bash循环实现持续AI执行，后来演变为更复杂的代理式编码方法，应用于代码重构、规范生成和项目设置等多个领域。

Result: 该技术在2025年底迅速流行，被证明在大型代码重构等特定任务中非常有效，成为当时重要的编码工具和方法。

Conclusion: Ralph Wiggum Technique展示了代理式编码方法的潜力，通过自动化AI执行流程显著提高了编码效率，特别是在大规模重构任务中表现出色。

Abstract: A brief history of ralph (10 minute read) The "Ralph Wiggum Technique" is an agentic coding method that went viral in late 2025. Initially a simple bash loop for continuous AI execution, ralph's evolution had many applications in refactoring, spec generation, and project setup. It was very useful for specific tasks like large-scale code refactoring.

</details>


### [79] [Looper: The AI Junior That Never Forgets the Backlog](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Flooper-article%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/Dz8RdQnEgnL_KWjaxfKMIQiQB73_Fcr5fAg8hIYyFEU=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Looper是一个AI编码工具，通过JSON待办事项和强制审查流程来确保确定性和可审计的开发


<details>
  <summary>Details</summary>
Motivation: 当前AI编码工具缺乏结构化和可审计的开发流程，导致开发过程不可预测且难以追踪

Method: 采用结构化、单任务AI代理迭代，通过JSON待办事项管理和强制审查流程来确保开发质量

Result: 实现了确定性和可审计的开发过程，确保AI编码工具能够系统化地处理开发任务

Conclusion: Looper通过结构化方法和强制审查机制，为AI编码工具提供了可靠和可追踪的开发框架

Abstract: Looper: The AI Junior That Never Forgets the Backlog (8 minute read) Looper is an AI coding tool that makes sure of deterministic and auditable development by enforcing structured, single-task AI agent iterations managed via a JSON backlog and a forced review pass.

</details>


### [80] [Two cheers for ugly code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.johndcook.com%2Fblog%2F2026%2F01%2F19%2Fugly-code%2F%3Futm_source=tldrdev/1/0100019bdb4e9f20-c81c33b0-e794-49c2-bf3f-d19c6c9a82d7-000000/agdWIsOj64E2EcaeO8-0Q35ZBv5GoYfOxeKNFTgPfz4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 丑陋代码因其蕴含的隐性知识和经过时间验证的实用性而具有价值，值得投入精力去理解和维护而非直接丢弃


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，人们通常追求优雅、整洁的代码，但往往会忽视那些看似"丑陋"的代码可能包含的宝贵价值。这些代码可能承载着重要的业务逻辑、历史经验或特定约束条件下的最优解决方案。

Method: 通过分析丑陋代码的价值来源，包括其蕴含的隐性知识（如领域专业知识、历史决策背景）和经过时间验证的实用性（如稳定性、性能优化），提出重新评估代码质量的标准。

Result: 丑陋代码往往包含难以通过文档传递的隐性知识，并且经过长期生产环境验证，具有较高的可靠性和实用性。这些价值超过了表面上的代码美观性。

Conclusion: 应该重新思考代码质量评估标准，认识到丑陋代码可能具有的独特价值，在重构或重写决策中更加谨慎，优先考虑代码的实际效用和知识传承。

Abstract: Two cheers for ugly code (2 minute read) Ugly code can be valuable due to the implicit knowledge it contains or its proven utility over time, making it worth the effort to work with and learn from rather than discard.

</details>


### [81] [all your OpenCodes belong to us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjohncodes.com%2Farchive%2F2026%2F01-18-all-your-opencodes%2F%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/u44NY_yb_jSU-4mZcIGFKFynNefigd3bAr7c3o-QL6E=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenCode v1.1.10之前版本存在严重RCE漏洞，通过未认证HTTP服务器暴露端点，允许任意shell命令执行、创建交互式终端会话和任意文件读取，还能通过提示注入操纵AI代理窃取数据或执行恶意操作。


<details>
  <summary>Details</summary>
Motivation: 揭示OpenCode早期版本存在的严重安全漏洞，这些漏洞可能被攻击者利用来远程控制受影响的系统，并通过AI代理进行二次攻击。

Method: 通过安全分析发现OpenCode v1.1.10之前版本中的未认证HTTP服务器暴露了多个危险端点，包括命令执行、终端会话创建和文件读取功能，同时存在提示注入漏洞可操纵LLM上下文。

Result: 发现了一个严重的RCE漏洞，影响数千个开发环境，攻击者可以远程执行任意命令、访问文件系统，并通过提示注入操纵AI代理执行恶意操作。

Conclusion: OpenCode早期版本存在严重安全缺陷，需要立即升级到v1.1.10或更高版本，并加强AI代理系统的安全防护，防止提示注入攻击。

Abstract: all your OpenCodes belong to us (7 minute read) OpenCode versions before v1.1.10 contained a critical RCE vulnerability via an unauthenticated HTTP server that exposed endpoints for arbitrary shell command execution, interactive terminal session creation, and arbitrary file reads. The vulnerability also enabled prompt injection into the LLM's context window, creating a secondary attack vector to manipulate the AI agent into exfiltrating data or performing malicious actions. Thousands of devel...

</details>


### [82] [ServiceNow BodySnatcher flaw highlights risks of rushed AI integrations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.csoonline.com%2Farticle%2F4118264%2Fservicenow-bodysnatcher-flaw-highlights-risks-of-rushed-ai-integrations.html%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/F6C5IBZHaqUedKXXWetzogt6av7IVdomfUUuJXkxO44=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: ServiceNow BodySnatcher漏洞暴露AI集成安全风险，允许未认证攻击者通过利用弱认证默认设置和自动链接功能，以任意用户身份执行代理工作流，可能创建后门管理员账户。


<details>
  <summary>Details</summary>
Motivation: 揭示ServiceNow AI代理集成中的安全漏洞，强调在快速AI集成过程中忽视安全性的风险，特别是默认认证设置和自动链接功能带来的安全隐患。

Method: 通过分析ServiceNow Now Assist AI Agents和Virtual Agent API的认证机制，发现攻击者可利用弱认证默认设置和自动链接功能，以未认证身份执行任意用户的代理工作流。

Result: 发现BodySnatcher关键漏洞，允许攻击者创建后门管理员账户。ServiceNow已于10月底修补托管实例（Now Assist AI Agents 5.1.18/5.2.19+和Virtual Agent API 3.15+）。

Conclusion: AI代理集成需要更严格的安全审查，特别是默认认证设置和自动化功能。企业应审慎评估AI集成安全风险，及时应用安全补丁。

Abstract: ServiceNow BodySnatcher flaw highlights risks of rushed AI integrations (8 minute read) BodySnatcher is a critical vulnerability in ServiceNow's Now Assist AI Agents and Virtual Agent API that allows unauthenticated attackers to execute agentic workflows as any user by exploiting weak authentication defaults and auto-linking features—potentially creating backdoor admin accounts. ServiceNow patched hosted instances in late October (Now Assist AI Agents 5.1.18/5.2.19+ and Virtual Agent API 3.15...

</details>


### [83] [Agentic Browser Security: 2025 Year-End Review](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.wiz.io%2Fblog%2Fagentic-browser-security-2025-year-end-review%3Futm_source=tldrinfosec/1/0100019bdbbbf159-a7a223f6-20a6-48ee-bebd-03eae3027f91-000000/DLvsRSyBX566fxVNcwg3QDZzYZPMj2fSKwGB9zuqGOg=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Wiz对2025年代理式浏览器采用激增及相关提示注入漏洞进行了年度回顾分析


<details>
  <summary>Details</summary>
Motivation: 随着代理式浏览器在2025年快速普及，相关的安全风险特别是提示注入漏洞显著增加，需要进行系统性安全评估

Method: 对2025年代理式浏览器采用情况进行回顾性分析，识别和评估相关的提示注入漏洞

Result: 发现代理式浏览器采用在2025年出现激增，同时伴随大量提示注入漏洞，揭示了该技术快速普及带来的安全挑战

Conclusion: 代理式浏览器的快速采用需要更强的安全防护措施，特别是针对提示注入漏洞的防御机制

Abstract: Agentic Browser Security: 2025 Year-End Review (4 minute read) Wiz reviewed 2025's surge in agentic browser adoption alongside numerous prompt-injection vulnerabilities.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [84] [Reinforcement Learning for Dynamic Workflow Optimization in CI/CD Pipelines](https://arxiv.org/abs/2601.11647)
*Aniket Abhishek Soni,Milan Parikh,Rashi Nimesh Kumar Dhenia,Jubin Abhishek Soni,Ayush Raj Jha,Sneja Mitinbhai Shah*

Main category: cs.SE

TL;DR: 使用强化学习动态优化CI/CD流水线工作流，通过智能决策测试执行策略，提升吞吐量30%，减少测试时间25%，同时保持缺陷遗漏率低于5%


<details>
  <summary>Details</summary>
Motivation: 现代CI/CD流水线采用静态工作流，随着系统规模扩大引入效率低下问题。需要动态优化方法来平衡测试覆盖率和执行效率

Method: 将CI/CD流水线建模为马尔可夫决策过程，训练强化学习代理在运行时决策（完整测试、部分测试或不测试），开发可配置的CI/CD仿真环境进行评估

Result: 强化学习优化的流水线相比静态基线：吞吐量提升30%，测试执行时间减少约25%，缺陷遗漏率保持在5%以下。代理学会为低风险提交选择性跳过或简化测试

Conclusion: 强化学习能够实现自适应和智能的DevOps工作流，为更高效、弹性和可持续的CI/CD自动化提供实用路径

Abstract: Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software delivery, yet their static workflows often introduce inefficiencies as systems scale. This paper proposes a reinforcement learning (RL) based approach to dynamically optimize CI/CD pipeline workflows. The pipeline is modeled as a Markov Decision Process, and an RL agent is trained to make runtime decisions such as selecting full, partial, or no test execution in order to maximize throughput while minimizing testing overhead.
  A configurable CI/CD simulation environment is developed to evaluate the approach across build, test, and deploy stages. Experimental results show that the RL optimized pipeline achieves up to a 30 percent improvement in throughput and approximately a 25 percent reduction in test execution time compared to static baselines, while maintaining a defect miss rate below 5 percent. The agent learns to selectively skip or abbreviate tests for low risk commits, accelerating feedback cycles without significantly increasing failure risk.
  These results demonstrate the potential of reinforcement learning to enable adaptive and intelligent DevOps workflows, providing a practical pathway toward more efficient, resilient, and sustainable CI/CD automation.

</details>


### [85] [Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey](https://arxiv.org/abs/2601.11655)
*Caihua Li,Lianghong Guo,Yanlin Wang,Daya Guo,Wei Tao,Zhenyu Shan,Mingwei Liu,Jiachi Chen,Haoyu Song,Duyu Tang,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文系统综述了AI在软件工程问题解决领域的研究进展，涵盖数据构建、方法分析、质量评估和应用实践，并提供了开源资源库。


<details>
  <summary>Details</summary>
Motivation: 问题解决是软件工程中的复杂任务，SWE-bench等基准测试显示大型语言模型在此任务上表现困难，这推动了自主编码代理的发展，需要对该领域进行系统性梳理。

Method: 采用系统性综述方法：1) 分析数据构建管道（自动收集与合成方法）；2) 全面分析方法论（无训练框架的模块化组件到基于训练的技术如监督微调和强化学习）；3) 讨论数据质量和代理行为的关键分析；4) 探讨实际应用。

Result: 建立了该领域的系统性知识框架，识别了关键挑战和研究方向，并创建了开源资源库作为动态参考资源。

Conclusion: 问题解决是AI在软件工程中的重要挑战领域，需要系统性的研究方法和资源支持，未来研究应关注数据质量、方法创新和实际应用。

Abstract: Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.

</details>


### [86] [From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy Informs the Design of Agentic AI Systems](https://arxiv.org/abs/2601.11672)
*Deepak Babu Piskala*

Main category: cs.SE

TL;DR: 论文探讨了从Unix的"一切皆文件"原则到现代AI智能体的类比统一化趋势，认为采用文件和代码为中心的交互模型能使智能体系统更易维护、可审计且操作稳健。


<details>
  <summary>Details</summary>
Motivation: 早期Unix系统的"一切皆文件"原则为异构设备和内核资源提供了统一的读写接口。本文旨在探索类似的统一化抽象如何在当代AI智能体系统中出现，并分析这种类比如何促进智能体系统的发展。

Method: 通过追溯从Unix到DevOps、基础设施即代码，再到自主软件智能体的演化历程，分析文件类抽象和基于代码的规范如何将多样化资源整合为一致、可组合的接口。

Result: 研究发现，类似于Unix的文件抽象原则正在AI智能体领域形成，文件和代码中心的交互模型能够为智能体系统提供统一的接口抽象。

Conclusion: 采用文件和代码为中心的交互模型可能使智能体系统更具可维护性、可审计性和操作稳健性，延续了Unix统一抽象的思想传统。

Abstract: A core abstraction in early Unix systems was the principle that 'everything is a file', enabling heterogeneous devices and kernel resources to be manipulated via uniform read/write interfaces. This paper explores how an analogous unification is emerging in contemporary agentic AI. We trace the evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous software agents, highlighting how file-like abstractions and code-based specifications collapse diverse resources into consistent, composable interfaces. The resulting perspective suggests that adopting file- and code-centric interaction models may enable agentic systems that are more maintainable, auditable, and operationally robust.

</details>


### [87] [Semantic Caching and Intent-Driven Context Optimization for Multi-Agent Natural Language to Code Systems](https://arxiv.org/abs/2601.11687)
*Harmohit Singh*

Main category: cs.SE

TL;DR: 提出一个生产优化的多智能体系统，将自然语言查询转换为可执行的Python代码进行结构化数据分析，通过语义缓存、双阈值决策和意图驱动的动态提示组装实现高准确率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖昂贵的前沿模型，需要开发一个既能保持高准确性又能实现成本效益的多智能体系统，用于企业级结构化数据分析。

Method: 1) 基于LLM的等价检测和结构化适配提示的语义缓存系统；2) 分离精确匹配检索和参考引导生成的双阈值决策机制；3) 通过表感知上下文过滤减少40-60%token消耗的意图驱动动态提示组装系统。

Result: 生产查询缓存命中率达67%，处理超过10,000个查询，平均延迟8.2秒，语义准确率94.3%，已部署于企业库存管理系统。

Conclusion: 该系统展示了在生产环境中部署基于LLM的分析系统的可行性，通过创新架构实现了高准确性和成本效率的平衡。

Abstract: We present a production-optimized multi-agent system designed to translate natural language queries into executable Python code for structured data analytics. Unlike systems that rely on expensive frontier models, our approach achieves high accuracy and cost efficiency through three key innovations: (1) a semantic caching system with LLM-based equivalence detection and structured adaptation hints that provides cache hit rates of 67% on production queries; (2) a dual-threshold decision mechanism that separates exact-match retrieval from reference-guided generation; and (3) an intent-driven dynamic prompt assembly system that reduces token consumption by 40-60% through table-aware context filtering. The system has been deployed in production for enterprise inventory management, processing over 10,000 queries with an average latency of 8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present empirical results from production deployment, and discuss practical considerations for deploying LLM-based analytics systems at scale.

</details>


### [88] [The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing](https://arxiv.org/abs/2601.11783)
*Murtuza N. Shergadwala*

Main category: cs.SE

TL;DR: 研究发现LLM法官评估存在"稳定性陷阱"：虽然判决稳定性高（>99%），但推理稳定性差异显著，客观指令的推理稳定性可低至19%，主观指令在35%-83%之间，表明高判决稳定性可能掩盖脆弱的推理过程。


<details>
  <summary>Details</summary>
Motivation: 在受监管行业（如人力资源）中，企业需要可扩展且可复现的生成式AI审计机制。虽然LLM作为法官的方法提供了可扩展性，但其在评估不同类型系统指令遵守情况时的可靠性尚未得到验证。本研究探讨应用测试指令类型如何影响法官评估的稳定性。

Method: 引入范围指令分解框架，将应用测试指令分类为客观和主观类型，以隔离导致法官不稳定的因素。将该框架应用于两个代表性的人力资源生成式AI应用，评估四种法官架构在多次运行中的稳定性。

Result: 揭示了"稳定性陷阱"现象：判决稳定性与推理稳定性之间存在分歧。虽然法官在客观和主观评估中都实现了接近完美的判决一致性（>99%），但他们的推理轨迹差异显著。客观指令（如字数统计）的推理稳定性低至约19%，主观指令的推理稳定性在35%-83%之间变化，而专注于离散实体提取的客观指令实现了高推理稳定性（>90%）。

Conclusion: 高判决稳定性可能掩盖脆弱的推理过程。建议审计员严格限定自动化评估协议的范围：将所有可确定性验证的逻辑委托给代码，同时保留LLM法官用于复杂的语义评估。

Abstract: The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a ``Stability Trap'' characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19\%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35\%$--$83\%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($>90\%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.

</details>


### [89] [Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces](https://arxiv.org/abs/2601.11868)
*Mike A. Merrill,Alexander G. Shaw,Nicholas Carlini,Boxuan Li,Harsh Raj,Ivan Bercovich,Lin Shi,Jeong Yeon Shin,Thomas Walshe,E. Kelly Buchanan,Junhong Shen,Guanghao Ye,Haowei Lin,Jason Poulos,Maoyu Wang,Marianna Nezhurina,Jenia Jitsev,Di Lu,Orfeas Menis Mastromichalakis,Zhiwei Xu,Zizhao Chen,Yue Liu,Robert Zhang,Leon Liangyu Chen,Anurag Kashyap,Jan-Lucas Uslu,Jeffrey Li,Jianbo Wu,Minghao Yan,Song Bian,Vedang Sharma,Ke Sun,Steven Dillmann,Akshay Anand,Andrew Lanpouthakoun,Bardia Koopah,Changran Hu,Etash Guha,Gabriel H. S. Dreiman,Jiacheng Zhu,Karl Krauth,Li Zhong,Niklas Muennighoff,Robert Amanfu,Shangyin Tan,Shreyas Pimpalgaonkar,Tushar Aggarwal,Xiangning Lin,Xin Lan,Xuandong Zhao,Yiqing Liang,Yuanli Wang,Zilong Wang,Changzhi Zhou,David Heineman,Hange Liu,Harsh Trivedi,John Yang,Junhong Lin,Manish Shetty,Michael Yang,Nabil Omi,Negin Raoof,Shanda Li,Terry Yue Zhuo,Wuwei Lin,Yiwei Dai,Yuxin Wang,Wenhao Chai,Shang Zhou,Dariush Wahdany,Ziyu She,Jiaming Hu,Zhikang Dong,Yuxuan Zhu,Sasha Cui,Ahson Saiyed,Arinbjörn Kolbeinsson,Jesse Hu,Christopher Michael Rytting,Ryan Marten,Yixin Wang,Alex Dimakis,Andy Konwinski,Ludwig Schmidt*

Main category: cs.SE

TL;DR: Terminal-Bench 2.0是一个包含89个终端环境任务的硬基准测试，灵感来源于真实工作流程问题，用于评估AI代理在长时域任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试要么不衡量真实世界任务，要么难度不足以有效评估前沿模型。需要创建一个足够困难的基准来测量AI代理在自主完成有价值长时域任务方面的能力。

Method: 开发了Terminal-Bench 2.0基准，包含89个计算机终端环境任务，每个任务都有独特环境、人工编写的解决方案和全面的验证测试。任务设计灵感来源于真实工作流程问题。

Result: 前沿模型和代理在基准测试中得分低于65%，表明当前AI代理在复杂终端任务上仍有较大改进空间。通过错误分析识别了模型和代理需要改进的领域。

Conclusion: Terminal-Bench 2.0是一个有效的硬基准测试，能够衡量AI代理在真实世界长时域任务中的能力。发布数据集和评估框架以支持未来研究和开发。

Abstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ .

</details>


### [90] [From LLMs to Agents in Programming: The Impact of Providing an LLM with a Compiler](https://arxiv.org/abs/2601.12146)
*Viktor Kjellberg,Miroslaw Staron,Farnaz Fotrousi*

Main category: cs.SE

TL;DR: 研究LLM代码生成代理如何通过编译器工具提升性能，发现在C语言编程任务中，编译器访问显著提高编译成功率，减少语法错误，且小模型搭配编译器有时能超越大模型。


<details>
  <summary>Details</summary>
Motivation: LLM生成的源代码质量不稳定，经常无法编译。研究旨在探索代码生成代理通过编译器工具（如gcc）进行迭代开发的能力，评估工具访问对LLM从被动生成器转变为主动代理的影响。

Method: 在RosettaCode数据集的699个C语言编程任务上进行计算实验。评估16个不同规模的LLM（1.35亿到700亿参数），比较有/无编译器访问时的表现。让LLM代理基于编译器反馈迭代开发可运行程序。

Result: 编译器访问使编译成功率提升5.3-79.4个百分点，不影响程序语义。语法错误减少75%，未定义引用错误减少87%。在某些情况下，配备编译器的小模型性能超过配备编译器的大模型。

Conclusion: LLM必须访问软件工程工具以提升性能，减少对大型模型的依赖，从而降低能源消耗。编译器工具使LLM从被动生成器转变为能够迭代开发可运行程序的主动代理。

Abstract: Large Language Models have demonstrated a remarkable capability in natural language and program generation and software development. However, the source code generated by the LLMs does not always meet quality requirements and may fail to compile. Therefore, many studies evolve into agents that can reason about the problem before generating the source code for the solution. The goal of this paper is to study the degree to which such agents benefit from access to software development tools, in our case, a \texttt{gcc} compiler. We conduct a computational experiment on the RosettaCode dataset, on 699 programming tasks in C. We evaluate how the integration with a compiler shifts the role of the language model from a passive generator to an active agent capable of iteratively developing runnable programs based on feedback from the compiler. We evaluated 16 language models with sizes ranging from small (135 million) to medium (3 billion) and large (70 billion). Our results show that access to a compiler improved the compilation success by 5.3 to 79.4 percentage units in compilation without affecting the semantics of the generated program. Syntax errors dropped by 75\%, and errors related to undefined references dropped by 87\% for the tasks where the agents outperformed the baselines. We also observed that in some cases, smaller models with a compiler outperform larger models with a compiler. We conclude that it is essential for LLMs to have access to software engineering tools to enhance their performance and reduce the need for large models in software engineering, such as reducing our energy footprint.

</details>


### [91] [Many Hands Make Light Work: An LLM-based Multi-Agent System for Detecting Malicious PyPI Packages](https://arxiv.org/abs/2601.12148)
*Muhammad Umar Zeshan,Motunrayo Ibiyo,Claudio Di Sipio,Phuong T. Nguyen,Davide Di Ruscio*

Main category: cs.SE

TL;DR: LAMPS是一个多智能体系统，使用协作式LLM检测恶意PyPI包，在多个数据集上达到97-99%准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开源仓库中的恶意代码对软件供应链构成威胁，传统基于规则的工具忽视源代码语义模式，LLM在可解释和模块化安全管道中的应用有限。

Method: 提出LAMPS多智能体系统，包含包检索、文件提取、分类和裁决聚合四个角色特定智能体，通过CrewAI框架协调，结合微调CodeBERT模型和LLaMA-3智能体进行上下文推理。

Result: 在D1数据集（6000个setup.py文件）上达到97.7%准确率，优于MPHunter；在D2数据集（1296个多文件）上达到99.5%准确率和99.5%平衡准确率，优于RAG方法和微调单智能体基线，McNemar检验证实改进显著。

Conclusion: 分布式LLM推理用于恶意代码检测可行，模块化多智能体设计在软件供应链安全中具有优势。

Abstract: Malicious code in open-source repositories such as PyPI poses a growing threat to software supply chains. Traditional rule-based tools often overlook the semantic patterns in source code that are crucial for identifying adversarial components. Large language models (LLMs) show promise for software analysis, yet their use in interpretable and modular security pipelines remains limited. This paper presents LAMPS, a multi-agent system that employs collaborative LLMs to detect malicious PyPI packages. The system consists of four role-specific agents for package retrieval, file extraction, classification, and verdict aggregation, coordinated through the CrewAI framework. A prototype combines a fine-tuned CodeBERT model for classification with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two complementary datasets: D1, a balanced collection of 6,000 setup.py files, and D2, a realistic multi-file dataset with 1,296 files and natural class imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5% balanced accuracy, outperforming RAG-based approaches and fine-tuned single-agent baselines. McNemar's test confirmed these improvements as highly significant. The results demonstrate the feasibility of distributed LLM reasoning for malicious code detection and highlight the benefits of modular multi-agent designs in software supply chain security.

</details>


### [92] [Aletheia: What Makes RLVR For Code Verifiers Tick?](https://arxiv.org/abs/2601.12186)
*Vatsal Venkatkrishna,Indraneil Paul,Iryna Gurevych*

Main category: cs.SE

TL;DR: 该论文提出了Aletheia测试平台，用于评估代码验证器在不同策略模型和协变量偏移下的鲁棒性，发现RLVR方法在代码验证中有效，但可以简化训练配方。


<details>
  <summary>Details</summary>
Motivation: 代码验证器在代码生成中应用较少，但它们在难以获得执行反馈的场景中很有价值。目前缺乏系统评估代码验证器鲁棒性的方法。

Method: 创建Aletheia测试平台，支持基于执行的代码验证器评估。研究RLVR训练配方的关键组件：中间思考轨迹、从负样本学习、在线策略训练。

Result: RLVR方法在代码验证中表现最优，但训练配方可以简化。小规模验证器中在线策略学习是关键，大规模验证器中基于思考的训练最重要。

Conclusion: 代码验证器是代码生成后训练工具箱的有力补充，RLVR方法有效但可简化，不同规模验证器需要不同关键组件。

Abstract: Multi-domain thinking verifiers trained via Reinforcement Learning from Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model (LLM) post-training pipeline, owing to their ability to robustly rate and rerank model outputs. However, the adoption of such verifiers towards code generation has been comparatively sparse, with execution feedback constituting the dominant signal. Nonetheless, code verifiers remain valuable toward judging model outputs in scenarios where execution feedback is hard to obtain and are a potentially powerful addition to the code generation post-training toolbox. To this end, we create and open-source Aletheia, a controlled testbed that enables execution-grounded evaluation of code verifiers' robustness across disparate policy models and covariate shifts. We examine components of the RLVR-based verifier training recipe widely credited for its success: (1) intermediate thinking traces, (2) learning from negative samples, and (3) on-policy training. While experiments show the optimality of RLVR, we uncover important opportunities to simplify the recipe. Particularly, despite code verification exhibiting positive training- and inference-time scaling, on-policy learning stands out as the key component at small verifier sizes, and thinking-based training emerges as the most important component at larger scales.

</details>


### [93] [Environment-Aware Code Generation: How far are We?](https://arxiv.org/abs/2601.12262)
*Tongtong Wu,Rongyi Chen,Wenjie Du,Suyu Ma,Guilin Qi,Zhenchang Xing,Shahram Khadivi,Ramesh Periyathambi,Gholamreza Haffari*

Main category: cs.SE

TL;DR: 该研究提出了环境感知代码生成(EACG)的概念，并创建了VersiBCB基准测试来评估LLMs在特定软件环境下生成可执行代码的能力。研究发现当前LLMs在这方面表现不佳，但通过数据、参数和缓存三种适配策略可以显著改善环境兼容性和可执行性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数代码生成评估只测试孤立的小规模代码，忽略了实际软件环境的复杂性。不清楚LLMs是否能生成在用户特定环境下可执行的代码，这限制了LLMs在实际软件工程工作流中的应用。

Method: 提出了环境感知代码生成(EACG)框架，创建了VersiBCB基准测试（多包、执行验证、弃用感知）。研究了三种适配策略：数据适配（使用环境特定数据训练）、参数适配（调整模型参数）、缓存适配（利用缓存机制）。

Result: 当前LLMs在环境特定代码生成方面表现不佳，但提出的三种适配策略显著提高了环境兼容性和可执行性。VersiBCB基准测试有效揭示了现有方法的局限性。

Conclusion: 环境感知代码生成是实际软件工程应用中的关键挑战，需要专门的适配策略。该研究为LLMs在真实软件开发环境中的部署提供了重要见解和方向。

Abstract: Recent progress in large language models (LLMs) has improved code generation, but most evaluations still test isolated, small-scale code (e.g., a single function) under default or unspecified software environments. As a result, it is unclear whether LLMs can reliably generate executable code tailored to a user's specific environment. We present the first systematic study of Environment-Aware Code Generation (EACG), where generated code must be functionally correct and directly executable under arbitrary software configurations. To enable realistic evaluation, we introduce VersiBCB, a benchmark that is multi-package, execution-verified, and deprecation-aware, capturing complex and evolving environments that prior datasets often overlook. Using VersiBCB, we investigate three complementary adaptation axes: data, parameters, and cache, and develop representative strategies for each. Our results show that current LLMs struggle with environment-specific code generation, while our adaptations improve environment compatibility and executability. These findings highlight key challenges and opportunities for deploying LLMs in practical software engineering workflows.

</details>


### [94] [Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs](https://arxiv.org/abs/2601.12273)
*Chihiro Yoshida,Yuta Ishimoto,Olivier Nourry,Masanari Kondo,Makoto Matsushita,Yasutaka Kamei,Yoshiki Higo*

Main category: cs.SE

TL;DR: 该研究构建了一个基于大语言模型的量子程序自动修复框架，通过结合静态信息、动态信息和变异分析结果来提升修复成功率，实验显示变异分析能显著改善修复效果（达到94.4%成功率）和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有量子程序自动修复技术存在修复成功率低和生成补丁可理解性差的问题，需要开发既能提高可靠性又能增强可解释性的新方法。

Method: 构建基于大语言模型的框架，生成代码修复及自然语言解释；设计四种提示配置，组合静态信息、动态信息和变异分析结果；变异分析通过评估程序微小变化对执行结果的影响提供详细动态信息。

Result: 实验结果显示变异分析能为基于LLM的量子程序APR提供有价值的上下文信息，显著提高修复成功率（达到94.4%），在某些情况下还能改善生成解释的质量。

Conclusion: 研究为开发量子程序自动修复技术指出了新方向，强调同时增强可靠性和可解释性，变异分析是提升LLM-based APR性能的关键因素。

Abstract: In recent years, Automated Program Repair (APR) techniques specifically designed for quantum programs have been proposed. However, existing approaches often suffer from low repair success rates or poor understandability of the generated patches. In this study, we construct a framework in which a large language model (LLM) generates code repairs along with a natural language explanation of the applied repairs. To investigate how the contextual information included in prompts influences APR performance for quantum programs, we design four prompt configurations with different combinations of static information, dynamic information, and mutation analysis results. Mutation analysis evaluates how small changes to specific parts of a program affect its execution results and provides more detailed dynamic information than simple execution outputs such as stack traces. Our experimental results show that mutation analysis can provide valuable contextual information for LLM-based APR of quantum programs, improving repair success rates (achieving 94.4% in our experiment) and in some cases also improving the quality of generated explanations. Our findings point toward new directions for developing APR techniques for quantum programs that enhance both reliability and explainability.

</details>


### [95] [Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition](https://arxiv.org/abs/2601.12522)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: CogniGent是一种基于多智能体的错误定位技术，通过因果推理、调用图分析和上下文工程，模拟开发者的动态认知调试过程，显著提升了错误定位性能。


<details>
  <summary>Details</summary>
Motivation: 软件错误每年造成数十亿美元损失，开发者50%时间用于修复错误。传统错误定位方法孤立分析代码组件，忽略了组件间的连接关系。现有LLM方法缺乏因果推理能力，难以有效管理上下文。

Method: 提出CogniGent多智能体技术，结合因果推理、调用图根因分析和上下文工程，模拟开发者的动态认知调试实践，通过假设检验支持错误定位。

Result: 在591个错误报告数据集上评估，相比6个现有基准方法，在文档和方法级别MAP提升23.33-38.57%，MRR提升25.14-53.74%，统计显著性测试确认了技术优势。

Conclusion: CogniGent通过解决推理、依赖和上下文限制，将类人认知与智能体自动化结合，推进了错误定位技术发展。

Abstract: Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.

</details>


### [96] [OpenAI for OpenAPI: Automated generation of REST API specification via LLMs](https://arxiv.org/abs/2601.12735)
*Hao Chen,Yunchun Li,Chen Chen,Fengxu Lin,Wei Li*

Main category: cs.SE

TL;DR: OOPS：首个技术无关的LLM静态分析方法，用于从REST API代码生成OpenAPI规范，通过API依赖图和多阶段生成解决LLM上下文限制和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 开发者编写和维护OpenAPI规范面临挑战，现有静态分析方法受限于特定编程语言和框架，而LLM方法受上下文限制和幻觉问题约束。

Method: 提出OOPS方法，包含两个关键步骤：端点方法提取和OAS生成。通过构建API依赖图建立文件关联解决上下文限制，采用多阶段生成和自优化机制缓解语法和语义幻觉。

Result: 在12个真实REST API（涵盖5种编程语言和8个开发框架）上评估，端点方法推断F1分数超过98%，请求参数和响应推断达97%，参数约束推断达92%。输入token平均低于5.6K，输出token平均低于0.9K。

Conclusion: OOPS能够准确生成高质量OAS，支持多种技术栈，减少技术特定规则和人工干预，为REST API文档生成提供了有效的技术无关解决方案。

Abstract: REST APIs, based on the REpresentational State Transfer (REST) architecture, are the primary type of Web API. The OpenAPI Specification (OAS) serves as the de facto standard for describing REST APIs and is crucial for multiple software engineering tasks. However, developers face challenges in writing and maintaining OAS. Although static analysis shows potential for OAS generation, it is limited to specific programming languages and development frameworks. The powerful code understanding capabilities of LLMs offer new opportunities for OAS generation, yet they are constrained by context limitations and hallucinations. To address these challenges, we propose the OpenAI OpenAPI Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis method for OAS generation, requiring fewer technology-specific rules and less human expert intervention. OOPS is implemented as an LLM agent workflow comprising two key steps: endpoint method extraction and OAS generation. By constructing an API dependency graph, it establishes necessary file associations to address LLMs' context limitations. Through multi-stage generation and self-refine, it mitigates both syntactic and semantic hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST APIs spanning 5 programming languages and 8 development frameworks. Experimental results demonstrate that OOPS accurately generates high-quality OAS for REST APIs implemented with diverse technologies, achieving an average F1-score exceeding 98% for endpoint method inference, 97% for both request parameter and response inference, and 92% for parameter constraint inference. The input tokens average below 5.6K with a maximum of 16.2K, while the output tokens average below 0.9K with a maximum of 7.7K.

</details>


### [97] [Teaching LLMs to Learn Tool Trialing and Execution through Environment Interaction](https://arxiv.org/abs/2601.12762)
*Xingjie Gao,Pengcheng Huang,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Chen Qian,Ge Yu,Yu Gu*

Main category: cs.SE

TL;DR: ToolMaster是一个让LLM通过与环境交互主动学习工具使用的框架，采用试错执行范式，显著提升对未见工具的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹记忆的方法在面对新工具或演化工具时泛化能力有限，需要更鲁棒的工具使用框架。

Method: 采用试错执行范式：先模仿包含显式工具尝试和自我修正的教师轨迹，再通过强化学习联合优化尝试和执行阶段。

Result: ToolMaster在未见或不熟悉工具上显著优于现有基线，在泛化性和鲁棒性方面表现突出。

Conclusion: 通过主动环境交互形成经验知识的方法比静态轨迹记忆更能提升LLM工具使用的泛化能力。

Abstract: Equipping Large Language Models (LLMs) with external tools enables them to solve complex real-world problems. However, the robustness of existing methods remains a critical challenge when confronting novel or evolving tools. Existing trajectory-centric paradigms primarily rely on memorizing static solution paths during training, which limits the ability of LLMs to generalize tool usage to newly introduced or previously unseen tools. In this paper, we propose ToolMaster, a framework that shifts tool use from imitating golden tool-calling trajectories to actively learning tool usage through interaction with the environment. To optimize LLMs for tool planning and invocation, ToolMaster adopts a trial-and-execution paradigm, which trains LLMs to first imitate teacher-generated trajectories containing explicit tool trials and self-correction, followed by reinforcement learning to coordinate the trial and execution phases jointly. This process enables agents to autonomously explore correct tool usage by actively interacting with environments and forming experiential knowledge that benefits tool execution. Experimental results demonstrate that ToolMaster significantly outperforms existing baselines in terms of generalization and robustness across unseen or unfamiliar tools. All code and data are available at https://github.com/NEUIR/ToolMaster.

</details>


### [98] [Automatic Generation of Formal Specification and Verification Annotations Using LLMs and Test Oracles](https://arxiv.org/abs/2601.12845)
*João Pascoal Faria,Emanuel Trigo,Vinicius Honorato,Rui Abreu*

Main category: cs.SE

TL;DR: LLMs自动为Dafny程序生成形式化验证注释，结合Claude Opus 4.5和GPT-5.2的多模型方法在110个程序上达到98.2%的正确率，最多8次修复迭代。


<details>
  <summary>Details</summary>
Motivation: 当前形式化验证工具虽然自动化程度提高，但为程序添加形式化规范注释（前置条件、后置条件、循环不变量等）仍需大量人工工作和专业知识。研究者希望利用LLMs自动生成这些注释，降低形式化验证的门槛。

Method: 使用多模型方法结合Claude Opus 4.5和GPT-5.2，从带有自然语言规范注释和测试代码的传统代码中自动生成Dafny程序的形式化验证注释。利用验证器反馈进行最多8次修复迭代，测试用例中的断言作为静态预言机验证生成的前置/后置条件。

Result: 在110个Dafny程序实验中，多模型方法在最多8次修复迭代内为98.2%的程序生成了正确的注释。逻辑回归分析显示证明辅助注释对当前LLMs构成不成比例的难度。还开发了VS Code扩展集成自动生成功能，获得积极的可用性反馈。

Conclusion: LLMs能够有效自动生成形式化验证注释，显著减少人工工作量。多模型方法和验证器反馈修复机制是关键成功因素。证明辅助注释的生成仍是挑战，需要进一步改进。

Abstract: Recent verification tools aim to make formal verification more accessible to software engineers by automating most of the verification process. However, annotating conventional programs with the formal specification and verification constructs (preconditions, postconditions, loop invariants, auxiliary predicates and functions and proof helpers) required to prove their correctness still demands significant manual effort and expertise. This paper investigates how LLMs can automatically generate such annotations for programs written in Dafny, a verification-aware programming language, starting from conventional code accompanied by natural language specifications (in comments) and test code. In experiments on 110 Dafny programs, a multimodel approach combining Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the programs within at most 8 repair iterations, using verifier feedback. A logistic regression analysis shows that proof-helper annotations contribute disproportionately to problem difficulty for current LLMs. Assertions in the test cases served as static oracles to automatically validate the generated pre/postconditions. We also compare generated and manual solutions and present an extension for Visual Studio Code to incorporate automatic generation into the IDE, with encouraging usability feedback.

</details>


### [99] [Efficient Code Analysis via Graph-Guided Large Language Models](https://arxiv.org/abs/2601.12890)
*Hang Gao,Tao Peng,Baoquan Cui,Hong Huang,Fengge Wu,Junsuo Zhao,Jian Zhang*

Main category: cs.SE

TL;DR: 提出基于图注意力获取的管道，通过代码图解析、GNN初步检测和注意力回溯，增强LLM定位恶意代码的能力，减少无关上下文干扰。


<details>
  <summary>Details</summary>
Motivation: 恶意行为常隐藏在大型复杂代码库中容易被忽视的小片段，跨文件依赖使得即使强大的LLM也难以可靠检测，需要增强LLM定位恶意行为的能力。

Method: 1) 将项目解析为代码图；2) 使用LLM编码节点语义和结构信号；3) 在稀疏监督下训练GNN进行初步检测；4) 通过预测回溯识别关键代码区域；5) 用这些区域引导LLM注意力进行深入分析。

Result: 在多个公共和自建数据集上的广泛实验表明，该方法持续优于现有方法，显著减少无关上下文干扰，同时保持低标注成本。

Conclusion: 该方法有效增强了LLM定位恶意代码的能力，在软件安全场景中具有实际部署潜力，通过图注意力机制平衡了检测精度和标注成本。

Abstract: Malicious behavior is often hidden in small, easily overlooked code fragments, especially within large and complex codebases. The cross-file dependencies of these fragments make it difficult for even powerful large language models (LLMs) to detect them reliably. We propose a graph-centric attention acquisition pipeline that enhances LLMs' ability to localize malicious behavior. The approach parses a project into a code graph, uses an LLM to encode nodes with semantic and structural signals, and trains a Graph Neural Network (GNN) under sparse supervision. The GNN performs an initial detection, and through backtracking of its predictions, identifies key code sections that are most likely to contain malicious behavior. These influential regions are then used to guide the LLM's attention for in-depth analysis. This strategy significantly reduces interference from irrelevant context while maintaining low annotation costs. Extensive experiments show that the method consistently outperforms existing methods on multiple public and self-built datasets, highlighting its potential for practical deployment in software security scenarios.

</details>


### [100] [Beyond Accuracy: Characterizing Code Comprehension Capabilities in (Large) Language Models](https://arxiv.org/abs/2601.12951)
*Felix Mächtle,Jan-Niclas Serr,Nils Loose,Thomas Eisenbarth*

Main category: cs.SE

TL;DR: 论文研究发现LLM的代码理解能力与传统人类中心软件度量指标相关性很弱，而影子模型能更好地预测LLM表现，表明LLM理解遵循模型特有的规律，需要超越聚合准确率的诊断方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程中的基准测试只提供粗略的性能总结，无法揭示模型能力的多样性。需要研究LLM的代码理解性能是否与传统人类中心软件度量指标一致，还是反映了独特的非人类规律。

Method: 引入诊断框架，将代码理解重构为二元输入输出一致性任务，评估分类和生成模型。使用大规模数据集，将模型性能与传统人类中心复杂度指标（如词汇量、控制流复杂度、抽象语法树结构）进行相关性分析。

Result: 人类定义指标与LLM成功之间的相关性很弱（AUROC 0.63），而影子模型获得显著更高的预测性能（AUROC 0.86），捕捉到超越传统软件度量的复杂、部分可预测模式。

Conclusion: LLM理解反映了模型特有的规律，这些规律只能通过人类设计或学习特征部分获取。强调需要超越聚合准确率、转向实例级诊断的基准方法，同时承认预测正确结果的根本限制。

Abstract: Large Language Models (LLMs) are increasingly integrated into software engineering workflows, yet current benchmarks provide only coarse performance summaries that obscure the diverse capabilities and limitations of these models. This paper investigates whether LLMs' code-comprehension performance aligns with traditional human-centric software metrics or instead reflects distinct, non-human regularities. We introduce a diagnostic framework that reframes code understanding as a binary input-output consistency task, enabling the evaluation of classification and generative models. Using a large-scale dataset, we correlate model performance with traditional, human-centric complexity metrics, such as lexical size, control-flow complexity, and abstract syntax tree structure. Our analyses reveal minimal correlation between human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve substantially higher predictive performance (AUROC 0.86), capturing complex, partially predictable patterns beyond traditional software measures. These findings suggest that LLM comprehension reflects model-specific regularities only partially accessible through either human-designed or learned features, emphasizing the need for benchmark methodologies that move beyond aggregate accuracy and toward instance-level diagnostics, while acknowledging fundamental limits in predicting correct outcomes.

</details>


### [101] [ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs](https://arxiv.org/abs/2601.13007)
*Rusheng Pan,Bingcheng Mao,Tianyi Ma,Zhenhua Ling*

Main category: cs.SE

TL;DR: ArchAgent是一个基于智能体的可扩展框架，通过结合静态分析、自适应代码分割和LLM驱动的合成，从跨仓库代码库中重构多视图、业务对齐的软件架构，解决了传统方法中的架构漂移、关系缺失和LLM上下文限制问题。


<details>
  <summary>Details</summary>
Motivation: 从大规模遗留软件中恢复准确架构面临三大挑战：架构漂移（实际实现偏离设计）、关系缺失（代码依赖不完整）、以及大型语言模型的有限上下文窗口。现有方法难以有效处理跨仓库代码库和业务关键模块识别。

Method: ArchAgent采用智能体框架，结合静态分析提取代码结构，自适应代码分割处理大规模代码，LLM驱动的合成生成架构视图。引入可扩展的图表生成机制，包含上下文剪枝和跨仓库数据集成，以识别业务关键模块。

Result: 在典型大规模GitHub项目评估中，ArchAgent相比现有基准有显著改进。消融研究证实依赖上下文能提高生产级仓库架构生成的准确性，真实案例研究展示了从遗留项目中有效恢复关键业务逻辑的能力。

Conclusion: ArchAgent通过智能体框架有效解决了大规模遗留软件架构恢复的挑战，提供了可扩展的多视图架构重构方案，并在实际项目中验证了其恢复业务关键逻辑的有效性。相关数据集已开源。

Abstract: Recovering accurate architecture from large-scale legacy software is hindered by architectural drift, missing relations, and the limited context of Large Language Models (LLMs). We present ArchAgent, a scalable agent-based framework that combines static analysis, adaptive code segmentation, and LLM-powered synthesis to reconstruct multiview, business-aligned architectures from cross-repository codebases. ArchAgent introduces scalable diagram generation with contextual pruning and integrates cross-repository data to identify business-critical modules. Evaluations of typical large-scale GitHub projects show significant improvements over existing benchmarks. An ablation study confirms that dependency context improves the accuracy of generated architectures of production-level repositories, and a real-world case study demonstrates effective recovery of critical business logics from legacy projects. The dataset is available at https://github.com/panrusheng/arch-eval-benchmark.

</details>


### [102] [MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code Generation](https://arxiv.org/abs/2601.13015)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.SE

TL;DR: MeltRTL是一个用于硬件RTL代码生成的框架，通过多专家注意力架构和推理时干预机制，在不重新训练基础模型的情况下显著提高了LLM生成RTL代码的准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的硬件RTL代码生成方案在生成复杂数字设计的语法和功能正确代码方面存在困难，需要一种无需重新训练基础模型就能提高准确率的方法。

Method: 提出MeltRTL框架，包含三个关键创新：1）多专家注意力架构，动态路由设计规范到专业专家网络；2）推理时干预机制，使用非线性探针检测和纠正硬件特定错误；3）高效干预框架，选择性操作专家特定注意力头，计算开销最小。

Result: 在VerilogEval基准测试中，MeltRTL实现了96%的可综合性和60%的功能正确性，相比基础LLM的85.3%和45.3%有显著提升。这些改进完全在推理时获得，仅增加27%计算开销，无需模型微调。

Conclusion: MeltRTL框架通过多专家架构和推理时干预的协同效应，显著提高了LLM生成RTL代码的质量，且可直接部署在现有预训练LLM上，无需额外训练。

Abstract: The automated generation of hardware register-transfer level (RTL) code with large language models (LLMs) shows promise, yet current solutions struggle to produce syntactically and functionally correct code for complex digital designs. This paper introduces MeltRTL, a novel framework that integrates multi-expert attention with inference-time intervention (ITI) to significantly improve LLM-based RTL code generation accuracy without retraining the base model. MeltRTL introduces three key innovations: (1) A multi-expert attention architecture that dynamically routes design specifications to specialized expert networks, enabling targeted reasoning across various hardware categories; (2) An inference-time intervention mechanism that employs non-linear probes to detect and correct hardware-specific inaccuracies during generation; and (3) An efficient intervention framework that selectively operates on expert-specific attention heads with minimal computational overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96% synthesizability and 60% functional correctness, compared to the base LLM's 85.3% and 45.3%, respectively. These improvements are obtained entirely at inference time, with only 27% computational overhead and no model fine-tuning, making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation studies further show the complementary benefits of multi-expert architecture and ITI, highlighting their synergistic effects when combined.

</details>


### [103] [RM -RF: Reward Model for Run-Free Unit Test Evaluation](https://arxiv.org/abs/2601.13097)
*Elena Bruches,Daniil Grebenkin,Mikhail Klementev,Vadim Alperovich,Roman Derunets,Dari Baturova,Georgy Mkrtchyan,Oleg Sedukhin,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: RM-RF是一个轻量级奖励模型，用于免运行评估自动生成的单元测试。它直接从源代码和测试代码预测三个执行相关信号，相比传统编译执行方法具有更低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 传统测试生成评估需要反复编译和执行测试，导致高延迟和高基础设施成本。需要一种更高效的评估方法，为大规模测试生成和基于强化学习的代码优化提供快速反馈。

Method: 构建多语言数据集（Java、Python、Go），包含源文件、测试文件和候选测试添加。训练RM-RF模型从代码直接预测三个执行信号：编译运行成功、代码覆盖率提升、变异杀死率改进。测试了多种模型家族和调优策略（零样本、全微调、LoRA PEFT）。

Result: 在三个预测目标上平均F1达到0.69。相比传统编译执行工具，RM-RF提供显著更低的延迟和基础设施成本，同时保持有竞争力的预测准确性。

Conclusion: RM-RF能够实现快速、可扩展的测试生成评估，为大规模测试生成和RL代码优化提供高效反馈机制，平衡了预测准确性和计算效率。

Abstract: We present RM-RF, a lightweight reward model for run-free evaluation of automatically generated unit tests. Instead of repeatedly compiling and executing candidate tests, RM-RF predicts - from source and test code alone - three execution-derived signals: (1) whether the augmented test suite compiles and runs successfully, (2) whether the generated test cases increase code coverage, and (3) whether the generated test cases improve the mutation kill rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java, Python, Go) of focal files, test files, and candidate test additions labeled by an execution-based pipeline, and we release an associated dataset and methodology for comparative evaluation. We tested multiple model families and tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an average F1 of 0.69 across the three targets. Compared to conventional compile-and-run instruments, RM-RF provides substantially lower latency and infrastructure cost while delivering competitive predictive fidelity, enabling fast, scalable feedback for large-scale test generation and RL-based code optimization.

</details>


### [104] [Guidelines to Prompt Large Language Models for Code Generation: An Empirical Characterization](https://arxiv.org/abs/2601.13118)
*Alessandro Midolo,Alessandro Giagnorio,Fiorella Zampetti,Rosalia Tufano,Gabriele Bavota,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: 该研究提出了10条针对代码生成的提示优化指南，通过测试驱动方法自动优化提示，并评估了开发者对这些指南的使用情况和感知有用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs广泛用于软件工程任务，但目前缺乏针对代码生成的具体提示优化指南。现有研究显示合适的提示工程能改善代码生成，但开发者缺乏系统性的指导原则。

Method: 采用迭代的测试驱动方法自动优化代码生成提示，分析优化过程中的改进项，从中提炼出10条提示改进指南。然后对50名从业者进行评估，了解他们对这些指南的使用情况和感知有用性。

Result: 研究识别出10条有效的提示改进指南，涉及I/O规范、前后条件、示例提供、细节描述和消除歧义等方面。从业者评估显示，他们对这些指南的感知有用性并不总是与实际使用情况一致。

Conclusion: 该研究为从业者、教育者和LLM辅助软件开发工具开发者提供了实用的提示优化指导，有助于提高代码生成的质量和效率。

Abstract: Large Language Models (LLMs) are nowadays extensively used for various types of software engineering tasks, primarily code generation. Previous research has shown how suitable prompt engineering could help developers in improving their code generation prompts. However, so far, there do not exist specific guidelines driving developers towards writing suitable prompts for code generation. In this work, we derive and evaluate development-specific prompt optimization guidelines. First, we use an iterative, test-driven approach to automatically refine code generation prompts, and we analyze the outcome of this process to identify prompt improvement items that lead to test passes. We use such elements to elicit 10 guidelines for prompt improvement, related to better specifying I/O, pre-post conditions, providing examples, various types of details, or clarifying ambiguities. We conduct an assessment with 50 practitioners, who report their usage of the elicited prompt improvement patterns, as well as their perceived usefulness, which does not always correspond to the actual usage before knowing our guidelines. Our results lead to implications not only for practitioners and educators, but also for those aimed at creating better LLM-aided software development tools.

</details>


### [105] [From Human to Machine Refactoring: Assessing GPT-4's Impact on Python Class Quality and Readability](https://arxiv.org/abs/2601.13139)
*Alessandro Midolo,Emiliano Tramontana,Massimiliano Di Penta*

Main category: cs.SE

TL;DR: GPT-4o在Python代码重构中能保持行为正确性并改善代码质量，但会降低可读性


<details>
  <summary>Details</summary>
Motivation: 尽管已有自动化重构工具，但其实用性有限。LLMs为自动化代码重构提供了新机会，但LLM驱动方法的效果评估仍存在未解问题

Method: 使用GPT-4o对ClassEval基准中的100个Python类进行综合实证研究，探索Fowler目录启发的类级重构，从三个互补视角评估：行为正确性（单元测试验证）、代码质量（Pylint、Flake8、SonarCloud评估）、可读性（最先进的可读性工具测量）

Result: GPT-4o通常能产生保持行为正确的重构，减少代码异味并改善质量指标，但代价是降低了可读性

Conclusion: 研究结果为LLMs在自动化软件重构中的能力和局限性提供了新证据，突出了将LLMs集成到实际重构工作流的方向

Abstract: Refactoring is a software engineering practice that aims to improve code quality without altering program behavior. Although automated refactoring tools have been extensively studied, their practical applicability remains limited. Recent advances in Large Language Models (LLMs) have introduced new opportunities for automated code refactoring. The evaluation of such an LLM-driven approach, however, leaves unanswered questions about its effects on code quality. In this paper, we present a comprehensive empirical study on LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the ClassEval benchmark. Unlike prior work, our study explores a wide range of class-level refactorings inspired by Fowler's catalog and evaluates their effects from three complementary perspectives: (i) behavioral correctness, verified through unit tests; (ii) code quality, assessed via Pylint, Flake8, and SonarCloud; and (iii) readability, measured using a state-of-the-art readability tool. Our findings show that GPT-4o generally produces behavior-preserving refactorings that reduce code smells and improve quality metrics, albeit at the cost of decreased readability. Our results provide new evidence on the capabilities and limitations of LLMs in automated software refactoring, highlighting directions for integrating LLMs into practical refactoring workflows.

</details>


### [106] [KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://arxiv.org/abs/2601.13240)
*Xue Jiang,Jiaru Qian,Xianjie Shi,Chenjie Li,Hao Zhu,Ziyu Wang,Jielun Zhang,Zheyu Zhao,Kechi Zhang,Jia Li,Wenpin Jiao,Zhi Jin,Ge Li,Yihong Dong*

Main category: cs.SE

TL;DR: KOCO-BENCH是一个评估LLM领域专业化方法的新基准，包含6个新兴领域、11个软件框架和25个项目，通过知识库和多粒度任务评估LLM获取和应用领域知识的能力。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定代码基准无法评估领域专业化方法的有效性，它们只关注LLM拥有什么知识，而不是如何获取和应用新知识，且缺乏明确的知识库来开发领域专业化方法。

Method: 构建包含6个新兴领域、11个软件框架和25个项目的基准，提供精心策划的知识库，设计多粒度评估任务：领域代码生成（从函数级到项目级，包含严格测试套件）和领域知识理解（通过多项选择问答）。

Result: KOCO-BENCH对最先进的LLM构成重大挑战，即使应用领域专业化方法（如SFT、RAG、kNN-LM），改进仍然有限。最佳性能的编码代理Claude Code仅达到34.2%，表明需要更有效的领域专业化方法。

Conclusion: KOCO-BENCH填补了评估LLM领域专业化方法的空白，揭示了当前方法的局限性，并提供了促进进一步研究的基准、评估代码和基线。

Abstract: Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.

</details>


### [107] [From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning](https://arxiv.org/abs/2601.13384)
*Jiajun Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Yuheng Jing,Zeyao Ma,Tianyi Bai,Zilei Wang,Qiang Liu,Liang Wang,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出Search-and-Replace Infilling (SRI)框架，将代理验证-编辑机制内化为单次推理过程，解决传统FIM代码补全的局限性，在保持推理效率的同时提升代码补全质量。


<details>
  <summary>Details</summary>
Motivation: 传统Fill-in-the-Middle (FIM)代码补全范式存在两个主要问题：1）无法修正上下文错误；2）依赖未对齐、不安全的Base模型。而Chat LLMs虽然安全，但性能下降；Agentic工作流虽然灵活，但延迟过高。需要一种兼顾性能、安全和效率的解决方案。

Method: 提出Search-and-Replace Infilling (SRI)框架，通过显式搜索阶段结构化地定位编辑位置，将代理验证-编辑机制内化为统一的单次推理过程。构建高质量数据集SRI-200K，并微调SRI-Coder系列模型。该方法将静态填充扩展到动态上下文感知编辑。

Result: 仅使用2万样本微调，SRI-Coder就能使Chat模型超越其Base对应模型的补全性能。与FIM风格微调不同，SRI保持通用编码能力，推理延迟与标准FIM相当。整个Qwen3-Coder系列都集成了SRI能力。

Conclusion: SRI框架成功解决了传统FIM代码补全的局限性，在保持推理效率的同时显著提升代码补全质量，为开发者社区提供了先进的自动补全和辅助开发工具。

Abstract: The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.

</details>


### [108] [AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on Software Development](https://arxiv.org/abs/2601.13597)
*Shyam Agarwal,Hao He,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 研究通过因果分析发现，AI代码代理在首次引入项目时能显著提升开发速度，但会持续增加代码复杂度等质量问题；而已有AI IDE的项目则获益有限。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码代理作为自主贡献者生成和合并PR，但其对软件项目的实际影响尚不清楚，特别是相对于广泛采用的IDE AI助手。需要了解代理工具与IDE工具如何交互，以及如何在AI集成开发工作流中平衡加速与可维护性。

Method: 使用AIDev数据集，采用交错双重差分法（staggered difference-in-differences）与匹配对照组进行纵向因果研究。将首次代理生成的PR定义为采用，分析月度仓库级结果，包括开发速度（提交数、代码行数）和软件质量（静态分析警告、认知复杂度、重复率、注释密度）。

Result: 1. 速度收益：仅当代理是项目中首个可观察AI工具时，才有显著的前期速度提升；已有AI IDE使用的仓库获益有限或短暂。2. 质量风险：在所有设置中持续存在，静态分析警告增加约18%，认知复杂度增加约35%，表明即使速度优势消退，代理引发的复杂度债务仍持续存在。

Conclusion: AI辅助存在收益递减效应，需要质量保障措施、来源跟踪和选择性部署自主代理。研究为理解代理与IDE工具的交互提供了实证基础，并推动研究如何在AI集成开发工作流中平衡加速与可维护性。

Abstract: Large language model (LLM)-based coding agents increasingly act as autonomous contributors that generate and merge pull requests, yet their real-world effects on software projects are unclear, especially relative to widely adopted IDE-based AI assistants. We present a longitudinal causal study of agent adoption in open-source repositories using staggered difference-in-differences with matched controls. Using the AIDev dataset, we define adoption as the first agent-generated pull request and analyze monthly repository-level outcomes spanning development velocity (commits, lines added) and software quality (static-analysis warnings, cognitive complexity, duplication, and comment density). Results show large, front-loaded velocity gains only when agents are the first observable AI tool in a project; repositories with prior AI IDE usage experience minimal or short-lived throughput benefits. In contrast, quality risks are persistent across settings, with static-analysis warnings and cognitive complexity rising roughly 18% and 35%, indicating sustained agent-induced complexity debt even when velocity advantages fade. These heterogeneous effects suggest diminishing returns to AI assistance and highlight the need for quality safeguards, provenance tracking, and selective deployment of autonomous agents. Our findings establish an empirical basis for understanding how agentic and IDE-based tools interact, and motivate research on balancing acceleration with maintainability in AI-integrated development workflows.

</details>


### [109] [Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs](https://arxiv.org/abs/2601.13655)
*Guangba Yu,Zirui Wang,Yujie Huang,Renyi Zhong,Yuedong Zhong,Yilun Wang,Michael R. Lyu*

Main category: cs.SE

TL;DR: 首次对开源LLM生态系统进行大规模实证研究，分析705个真实故障，揭示白盒编排将可靠性瓶颈从模型算法缺陷转移到部署栈的系统脆弱性。


<details>
  <summary>Details</summary>
Motivation: 开源LLM的民主化让用户可以在本地基础设施上微调和部署模型，但暴露了"第一英里"部署环境。与黑盒API消费不同，用户管理的编排可靠性仍然是一个关键盲点，需要填补这一研究空白。

Method: 对开源DeepSeek、Llama和Qwen生态系统的705个真实故障进行大规模实证研究，分析故障模式和根本原因。

Result: 发现三个关键现象：1）诊断分歧：运行时崩溃独特地指示基础设施摩擦，而不正确功能则作为内部分词器缺陷的特征；2）系统同质性：根本原因在不同系列中趋同，确认可靠性障碍是共享生态系统固有的；3）生命周期升级：障碍从微调期间的内在配置斗争升级到推理期间的复合环境不兼容。

Conclusion: 白盒编排将可靠性瓶颈从模型算法缺陷转移到部署栈的系统脆弱性，这些见解为增强LLM生态系统的可靠性提供了可操作的指导。

Abstract: The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.

</details>


### [110] [CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case Generation](https://arxiv.org/abs/2601.13682)
*Jianfeng Cai,Jinhua Zhu,Ruopei Sun,Kangwen Zhao,Dongyun Xue,Mingxiao Feng,Wengang Zhou,Houqiang Li*

Main category: cs.SE

TL;DR: 提出反馈驱动的迭代框架，通过LLM生成测试用例，利用执行失败结果作为反馈优化测试用例，构建高质量数据集CodeContests-O，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 推理模型需要大规模可验证数据，编程任务是理想来源。但现有竞争编程平台缺乏高质量测试用例，现有LLM方法仅依赖模型内在生成能力，导致测试用例多样性不足

Method: 反馈驱动的迭代框架：1) LLM生成初始测试用例；2) 对正确和错误解决方案执行测试；3) 利用失败结果作为反馈指导LLM优化测试用例，提高保真度和区分度；4) 应用于CodeContests数据集构建CodeContests-O

Result: CodeContests-O在1100万解决方案中达到平均TPR 89.37%和TNR 90.89%，显著优于CodeContests和CodeContests+。在Qwen2.5-7B上微调后，LiveCodeBench Pass@1提升9.52%

Conclusion: 提出的反馈驱动迭代框架有效，CodeContests-O数据集质量高，能显著提升模型性能。已开源代码和数据集支持可复现性和未来研究

Abstract: The rise of reasoning models necessitates large-scale verifiable data, for which programming tasks serve as an ideal source. However, while competitive programming platforms provide abundant problems and solutions, high-quality test cases for verification remain scarce. Existing approaches attempt to synthesize test cases using Large Language Models (LLMs), but rely solely on the model's intrinsic generation capabilities without external feedback, frequently resulting in insufficiently diverse cases. To address this limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for comprehensive test case construction. Specifically, our method leverages the LLM to generate initial test cases, executes them against known correct and incorrect solutions, and utilizes the failed results as feedback to guide the LLM in refining the test cases toward high fidelity and discriminability. We then apply this method to the CodeContests dataset to construct an optimized high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of $90.89\%$, significantly outperforming the CodeContests and CodeContests+ by margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our framework and the quality of CodeContests-O. To support reproducibility and facilitate future research, we release the $\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and $\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.

</details>


### [111] [SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories](https://arxiv.org/abs/2601.13713)
*Aditya Bharat Soni,Rajat Ghosh,Vaishnavi Bhargava,Valerie Chen,Debojyoti Dutta*

Main category: cs.SE

TL;DR: 本文提出SWE-Tester，一个用于训练开源LLM生成问题复现测试的框架，在SWT-Bench Verified上取得了最高10%的成功率和21%变更覆盖率的绝对提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖闭源LLM，对开源模型探索有限。自动从自然语言问题描述生成测试能提高开发者生产力、简化根因分析、促进测试驱动开发，并提升自动问题解决系统的效果。

Method: 提出SWE-Tester训练管道：首先从2.6K个GitHub仓库中整理41K个高质量训练实例，然后用这些数据训练不同规模和家族的开源LLM。

Result: 微调模型在SWT-Bench Verified上实现了最高10%成功率和21%变更覆盖率的绝对提升。分析显示增加推理计算、更多数据和更大模型能带来持续改进。

Conclusion: 该框架在推进开源LLM在此领域的应用方面效果显著，为开源模型在软件测试生成任务上的发展提供了有效路径。

Abstract: Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- "test first, write code later", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\% in success rate and 21\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.

</details>


### [112] [On Autopilot? An Empirical Study of Human-AI Teaming and Review Practices in Open Source](https://arxiv.org/abs/2601.13754)
*Haoyu Gao,Peerachai Banyongrakkul,Hao Guan,Mansooreh Zahedi,Christoph Treude*

Main category: cs.SE

TL;DR: 研究发现AI协作的PR主要来自无代码所有权的贡献者，但多数仓库缺乏AI使用指南；AI协作PR合并更快、反馈更少，特别是非所有者提交的AI协作PR80%无明确审查即被合并。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地自动化软件工程任务，开源软件中"AI作为队友"的采用加速，但开发者与AI协作的交互模式尚未得到充分探索。

Method: 扩展AIDev数据集，包含细粒度的贡献者代码所有权信息，并与人工创建的PR建立比较基线，研究项目级指南和开发者与AI辅助PR的交互。

Result: 1) 67.5%的AI协作PR来自无代码所有权的贡献者；2) 多数仓库缺乏AI编码代理使用指南；3) AI协作PR合并显著更快且反馈最少；4) 非所有者提交的AI协作PR约80%无明确审查即被合并。

Conclusion: AI协作PR呈现独特的交互模式：合并更快、反馈更少，特别是非所有者提交的AI协作PR审查不足，这对开发者和研究者具有重要启示。

Abstract: Large Language Models (LLMs) increasingly automate software engineering tasks. While recent studies highlight the accelerated adoption of ``AI as a teammate'' in Open Source Software (OSS), developer interaction patterns remain under-explored. In this work, we investigated project-level guidelines and developers' interactions with AI-assisted pull requests (PRs) by expanding the AIDev dataset to include finer-grained contributor code ownership and a comparative baseline of human-created PRs. We found that over 67.5\% of AI-co-authored PRs originate from contributors without prior code ownership. Despite this, the majority of repositories lack guidelines for AI-coding agent usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs are merged significantly faster with minimal feedback. In contrast to human-created PRs where non-owner developers receive the most feedback, AI-co-authored PRs from non-owners receive the least, with approximately 80\% merged without any explicit review. Finally, we discuss implications for developers and researchers.

</details>


### [113] [VulnResolver: A Hybrid Agent Framework for LLM-Based Automated Vulnerability Issue Resolution](https://arxiv.org/abs/2601.13933)
*Mingming Zhang,Xu Wang,Jian Zhang,Xiangxin Meng,Jiayi Zhang,Chunming Hu*

Main category: cs.SE

TL;DR: VulnResolver是一个基于LLM的混合代理框架，用于自动化漏洞问题解决，通过两个专门代理实现上下文收集和安全属性分析，在SEC-bench基准测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，安全漏洞日益普遍且代价高昂。现有自动化漏洞修复方法严重依赖手动标注（如故障位置或CWE标签），这些标注难以获取且耗时，同时忽略了开发者问题报告中丰富的自然语义上下文。

Method: 提出VulnResolver框架，结合自主代理的适应性和工作流引导修复的稳定性。包含两个专门代理：上下文预收集代理（CPCAgent）自适应探索仓库收集依赖和上下文信息；安全属性分析代理（SPAAgent）生成并验证漏洞违反的安全属性。这些代理生成结构化分析来丰富原始问题报告，实现更准确的漏洞定位和补丁生成。

Result: 在SEC-bench基准测试中，VulnResolver在SEC-bench Lite上解决了75%的问题，达到最佳解决性能。在SEC-bench Full上，也显著优于最强的基线方法（基于代理的OpenHands），证实了其有效性。

Conclusion: VulnResolver提供了一个自适应且安全感知的框架，通过工作流稳定性和专门代理在上下文推理和基于属性分析方面的能力，推进了端到端自动化漏洞问题解决。

Abstract: As software systems grow in complexity, security vulnerabilities have become increasingly prevalent, posing serious risks and economic costs. Although automated detection tools such as fuzzers have advanced considerably, effective resolution still often depends on human expertise. Existing automated vulnerability repair (AVR) methods rely heavily on manually provided annotations (e.g., fault locations or CWE labels), which are often difficult and time-consuming to obtain, while overlooking the rich, naturally embedded semantic context found in issue reports from developers.
  In this paper, we present VulnResolver, the first LLM-based hybrid agent framework for automated vulnerability issue resolution. VulnResolver unites the adaptability of autonomous agents with the stability of workflow-guided repair through two specialized agents. The Context Pre-Collection Agent (CPCAgent) adaptively explores the repository to gather dependency and contextual information, while the Safety Property Analysis Agent (SPAAgent) generates and validates the safety properties violated by vulnerabilities. Together, these agents produce structured analyses that enrich the original issue reports, enabling more accurate vulnerability localization and patch generation.
  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of issues on SEC-bench Lite, achieving the best resolution performance. On SEC-bench Full, VulnResolver also significantly outperforms the strongest baseline, the agent-based OpenHands, confirming its effectiveness. Overall, VulnResolver delivers an adaptive and security-aware framework that advances end-to-end automated vulnerability issue resolution through workflow stability and the specialized agents' capabilities in contextual reasoning and property-based analysis.

</details>


### [114] [RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository](https://arxiv.org/abs/2601.13943)
*Zhiyuan Peng,Xin Yin,Pu Zhao,Fangkai Yang,Lu Wang,Ran Jia,Xu Chen,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: RepoGenesis是首个多语言仓库级端到端Web微服务生成基准测试，包含106个Python和Java微服务仓库，评估显示现有系统在架构一致性、依赖管理和跨文件一致性方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注孤立函数/类级代码生成或现有代码库修改，缺乏完整的微服务仓库生成基准，无法反映真实的0到1开发工作流程。

Method: 构建包含106个Python和Java微服务仓库的多语言基准测试，涵盖18个领域和11个框架，包含1258个API端点和2335个测试用例，采用"评审-反驳"质量保证流程验证。

Result: 评估显示尽管API覆盖率高达73.91%，部署成功率100%，但最佳系统在Python和Java上的Pass@1分别仅为23.67%和21.45%，暴露了架构一致性、依赖管理和跨文件一致性的缺陷。

Conclusion: RepoGenesis基准测试揭示了现有代码生成系统在完整微服务生成方面的局限性，并为推进微服务生成研究提供了高质量评估平台。

Abstract: Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a "review-rebuttal" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.

</details>


### [115] [Toward self-coding information systems](https://arxiv.org/abs/2601.14132)
*Rodrigo Falcão,Frank Elberzhager,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 提出"自编码信息系统"新概念，系统能在运行时动态评估、生成、测试和部署代码，实现自主结构/行为适应，缩短新功能上市时间


<details>
  <summary>Details</summary>
Motivation: 当前信息系统需要人工编码和部署，导致新功能开发周期长。自编码系统能自主适应变化，减少人工干预，提高系统响应速度和灵活性

Method: 提出自编码信息系统的正式定义，讨论其预期影响，并指出潜在研究方向。系统包括评估适应决策、生成源代码、测试和自主部署等能力

Result: 这是一个概念性提案，尚未有具体实验结果。提出了新的研究主题框架，为未来自编码系统开发奠定理论基础

Conclusion: 自编码信息系统是代理AI领域有前景的新研究方向，能实现系统自主适应和演化，对软件开发范式有重要影响

Abstract: In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [116] [CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration](https://arxiv.org/abs/2601.11556)
*Boyang Wang,Yash Vishe,Xin Xu,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: CSyMR-Bench：一个用于评估LLM在符号音乐推理中组合分析能力的基准，包含126个来自专家论坛和专业考试的多选题，需要结合多个原子分析来得出最终答案。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注孤立的知识或原子分析，缺乏对连接音乐结构所需的组合性推理能力的评估，这在符号音乐理解中至关重要。

Method: 1. 构建CSyMR-Bench基准数据集（126个多选题）；2. 开发基于music21库符号音乐分析工具的工具增强代理框架；3. 在社区来源和考试风格问题上进行实验验证。

Result: CSyMR-Bench对所有基线模型都构成非平凡挑战，而工具增强代理框架在所有基线上表现最佳，实现了5-7%的绝对准确率提升。

Conclusion: 组合性符号音乐推理是一个具有挑战性的任务，需要专门的基准和工具增强方法来有效解决。CSyMR-Bench填补了这一空白，工具增强代理框架展示了利用领域特定工具提升LLM音乐推理能力的有效性。

Abstract: Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.

</details>


### [117] [Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces](https://arxiv.org/abs/2601.11572)
*Timo Aukusti Laine*

Main category: cs.LG

TL;DR: 使用线性代数和哈密顿形式等数学工具分析LLM嵌入空间结构，发现L2归一化约束使嵌入空间适合哈密顿分析，推导了余弦相似度与向量扰动的关系，并探索量子力学类比


<details>
  <summary>Details</summary>
Motivation: 观察到LLM嵌入表现出离散语义状态，表明存在离散语义表示，希望通过数学工具分析语义关系

Method: 应用线性代数和哈密顿形式分析LLM嵌入空间，推导L2归一化约束下的数学关系，探索余弦相似度与向量扰动的关系，研究直接和间接语义转换，并引入量子力学类比

Result: L2归一化约束导致结构化嵌入空间适合哈密顿分析，推导了余弦相似度与向量扰动的数学关系，探索了量子力学类比如零点能，并讨论了与Koopman-von Neumann力学的潜在联系

Conclusion: 该方法为深入理解LLM提供了有前景的途径，可能有助于开发减轻幻觉的新方法，但解释需要谨慎考虑

Abstract: We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.

</details>


### [118] [GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment](https://arxiv.org/abs/2601.11574)
*Lukas Abrie Nel*

Main category: cs.LG

TL;DR: GRADE使用Gumbel-softmax重参数化替代高方差的策略梯度方法，通过可微松弛实现从奖励信号到模型参数的端到端梯度传播，在文本对齐任务上比PPO提升50%性能且梯度方差降低14倍。


<details>
  <summary>Details</summary>
Motivation: RLHF已成为对齐大语言模型的主流方法，但PPO等策略梯度方法存在梯度方差高、需要大量超参数调优和计算资源的问题，需要更稳定高效的替代方案。

Method: 提出GRADE方法，使用Gumbel-Softmax重参数化配合直通估计（GRADE-STE），对离散token采样过程进行可微松弛，实现从奖励信号通过生成token到模型参数的端到端梯度传播。

Result: 在IMDB数据集的情感控制文本生成任务中，GRADE-STE获得0.763的测试奖励，相比PPO的0.510和REINFORCE的0.617提升50%；梯度方差比REINFORCE低14倍以上，训练动态更稳定。

Conclusion: GRADE为LLM对齐提供了比强化学习更简单、稳定、有效的替代方案，其改进在验证集/测试集上具有良好泛化性。

Abstract: Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.

</details>


### [119] [DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models](https://arxiv.org/abs/2601.11895)
*Pareesa Ameneh Golnari,Adarsh Kumarappan,Wen Wen,Xiaoyu Liu,Gabriel Ryan,Yuting Sun,Shengyu Fu,Elsie Nallipogu*

Main category: cs.LG

TL;DR: DevBench是一个基于真实开发者遥测数据的代码补全基准测试，包含1800个评估实例，涵盖6种编程语言和6个任务类别，强调生态有效性并避免训练数据污染。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全基准测试缺乏生态有效性（不能反映真实开发场景），存在训练数据污染问题，且无法提供详细的诊断信息来指导模型选择和改进。

Method: 基于真实开发者遥测数据构建评估实例，结合功能正确性、相似度度量和LLM评估（关注有用性和上下文相关性）的多维度评估方法，对9个最先进模型进行全面评估。

Result: 评估揭示了不同模型在语法精度、语义推理和实际效用方面的差异，为模型选择和针对性改进提供了可操作的见解。

Conclusion: DevBench提供了其他基准测试通常缺失但实际部署和针对性模型开发必需的详细诊断信息，能够有效指导模型选择和改进。

Abstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.

</details>


### [120] [R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning](https://arxiv.org/abs/2601.11960)
*Jingchu Wang,Bingbing Xu,Yige Yuan,Bin Xie,Xiaoqian Sun,Huawei Shen*

Main category: cs.LG

TL;DR: 提出R²PO方法，通过引入轻量级残差Rollout-Head解耦训练轨迹和推理响应，解决强化学习中探索不足的问题，在多个基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用单一策略同时产生推理响应和训练优化轨迹，导致生成稳定推理响应与多样化训练轨迹之间的目标冲突，造成探索不足，损害推理能力。

Method: 提出R²PO（Residual Rollout Policy Optimization），在策略之上引入轻量级残差Rollout-Head，将训练轨迹与推理响应解耦，实现训练期间可控的轨迹多样化，同时保持推理生成的稳定性。

Result: 在多个基准测试中一致优于基线方法，在MATH-500上平均准确率提升3.1%，在APPS上提升2.4%，同时减少格式错误并缓解长度偏差以实现稳定优化。

Conclusion: R²PO通过解耦训练轨迹和推理响应，有效解决了强化学习中探索不足的问题，显著提升了LLM的推理能力。

Abstract: Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.

</details>


### [121] [Extreme Value Policy Optimization for Safe Reinforcement Learning](https://arxiv.org/abs/2601.12008)
*Shiqing Gao,Yihang Zhou,Shuai Shao,Haoyu Luo,Yiheng Bing,Jiaxin Ding,Luoyi Fu,Xinbing Wang*

Main category: cs.LG

TL;DR: EVO算法利用极值理论建模极端奖励和成本样本，通过极端分位数优化目标和极端优先级回放机制，显著减少强化学习中的约束违反，相比期望约束方法有更低的约束违反概率。


<details>
  <summary>Details</summary>
Motivation: 传统约束强化学习使用期望累积成本作为约束，但忽略了尾部分布中罕见但影响巨大的极端值事件（如黑天鹅事件），这可能导致严重的约束违反。需要一种能处理极端值的方法来确保安全性。

Method: 提出EVO算法：1）利用极值理论建模极端奖励和成本样本；2）引入极端分位数优化目标，显式捕捉成本尾部分布中的极端样本；3）提出极端优先级回放机制，在经验回放中放大罕见但高影响极端样本的学习信号。

Result: 理论分析：建立了策略更新期间期望约束违反的上界，保证在零违反分位数水平上的严格约束满足。实验证明：EVO在训练期间显著减少约束违反，同时保持与基线相当的策略性能，相比期望约束方法有更低的约束违反概率，比分位数回归方法有更低的方差。

Conclusion: EVO算法通过极值理论有效处理强化学习中的极端值事件，显著提高了安全性，为实际应用中的约束强化学习提供了更可靠的解决方案。

Abstract: Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.

</details>


### [122] [Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate](https://arxiv.org/abs/2601.12091)
*Qian Tan,Lei Jiang,Yuting Zeng,Shuoyang Ding,Xiaohua Xu*

Main category: cs.LG

TL;DR: 提出CEBiasBench中英双语基准和Multi-Agent Vote评估框架，发现中文提示仅将偏见转向东亚视角而非消除。提出Multi-Agent Cultural Debate训练免费框架，通过文化角色辩论有效缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在系统性西方中心偏见，但非西方语言（如中文）提示能否缓解这种偏见尚未充分研究。现有方法在评估和缓解两方面都存在不足：评估方法强制输出预定义文化类别而无中立选项；缓解方法依赖昂贵的多文化语料库或缺乏明确文化表征的代理框架。

Method: 1) 引入CEBiasBench中英双语基准；2) 提出Multi-Agent Vote评估框架，支持"无偏见"判断；3) 提出Multi-Agent Cultural Debate训练免费框架，为代理分配不同文化角色，采用"求同存异"策略进行辩论。

Result: 中文提示仅将偏见转向东亚视角而非消除偏见。MACD框架在CEBiasBench上达到57.6%平均无偏见率（LLM-as-judge评估）和86.0%（MAV评估），相比GPT-4o基线的47.6%和69.0%有显著提升。在阿拉伯语CAMeL基准上也表现出良好泛化能力。

Conclusion: 代理框架中明确的文化表征对于跨文化公平至关重要。Multi-Agent Cultural Debate通过文化角色辩论有效缓解语言模型的文化偏见，且无需额外训练。

Abstract: Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a "Seeking Common Ground while Reserving Differences" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.

</details>


### [123] [Speculative Sampling with Reinforcement Learning](https://arxiv.org/abs/2601.12212)
*Chenan Wang,Daniel H. Shi,Haipeng Chen*

Main category: cs.LG

TL;DR: 提出Re-SpS框架，使用强化学习动态优化推测采样中的草稿树超参数，提升LLM推理速度


<details>
  <summary>Details</summary>
Motivation: 当前推测采样方法（如EAGLE-3）使用静态超参数控制草稿树结构，限制了在不同上下文和领域中的灵活性和效率。需要动态调整超参数以平衡推测的激进程度和计算开销。

Method: 提出基于强化学习的推测采样框架Re-SpS，动态实时调整草稿树超参数。利用目标模型隐藏状态的高效状态表示，引入多步动作持久化以更好地建模上下文，学习上下文感知的策略来最大化生成速度。

Result: 在五个不同基准测试中相比SOTA方法EAGLE-3持续改进，相比骨干LLM实现最高5.45倍加速，相比EAGLE-3实现最高1.12倍加速，且输出保真度无损失。

Conclusion: Re-SpS通过强化学习动态优化草稿树超参数，有效提升了LLM推理速度，为实时应用提供了更高效的解决方案。

Abstract: Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\times$ speedup over the backbone LLM and up to 1.12$\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.

</details>


### [124] [Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation](https://arxiv.org/abs/2601.12401)
*Jinmei Liu,Haoru Li,Zhenhong Sun,Chaofeng Chen,Yatao Bian,Bo Wang,Daoyi Dong,Chunlin Chen,Zhi Wang*

Main category: cs.LG

TL;DR: DRIFT框架通过采样、提示和优化三个角度激励多样性，解决强化学习微调生成模型时的多样性崩溃问题，在任务对齐和生成多样性之间实现帕累托优势。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调大规模生成模型时存在"多样性崩溃诅咒"，目标函数和优化过程导致策略坍缩为狄拉克分布，限制了生成多样性，而实际应用需要多样化的候选生成。

Method: 提出DRIFT框架，从三个角度系统激励多样性：1) 采样奖励集中子集过滤异常值防止过早崩溃；2) 使用随机变体提示扩展条件空间；3) 通过基于势能的奖励塑形机制优化组内多样性。

Result: DRIFT在任务对齐和生成多样性方面实现帕累托优势：在同等对齐水平下多样性提升9.08%~43.46%，在同等多样性水平下对齐度提升59.65%~65.86%。

Conclusion: DRIFT框架有效解决了强化学习微调生成模型时的多样性崩溃问题，实现了任务对齐与生成多样性的平衡，增强了生成模型的实用性。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \textbf{DRIFT} (\textbf{D}ive\textbf{R}sity-\textbf{I}ncentivized Reinforcement \textbf{F}ine-\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\%\!\sim\! 43.46\%$ increase in diversity at equivalent alignment levels and a $ 59.65\% \!\sim\! 65.86\%$ increase in alignment at equivalent levels of diversity.

</details>


### [125] [Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization](https://arxiv.org/abs/2601.12598)
*Younes Bouhadjar,Maxime Fabre,Felix Schmidt,Emre Neftci*

Main category: cs.LG

TL;DR: 该论文提出了SelectivBench，一个用于系统评估线性循环模型选择性的轻量级合成基准测试套件，揭示了这些模型在序列处理中的关键架构特征。


<details>
  <summary>Details</summary>
Motivation: 线性循环神经网络作为Transformer注意力机制的高效替代方案，虽然训练可并行化且推理时内存和计算需求恒定，但现有基准测试要么过于简单无法揭示实质性差异，要么资源消耗过大。缺乏对线性循环模型的系统性直接比较。

Method: 提出了线性循环模型的细化分类法，并开发了SelectivBench——一个轻量级、可定制的合成基准测试套件。该基准使用基于规则的语法生成可调整复杂度的序列，包含故意违反转换规则的不规则间隔，专门评估序列模型在小到中等规模下的选择性能力。

Result: 在SelectivBench上评估线性循环模型显示出与大规模语言任务一致的表现模式。分析揭示了关键架构特征的作用：门控和快速遗忘机制促进召回，状态内通道混合对选择性不必要但对泛化关键，softmax注意力因内存容量随序列长度扩展而保持优势。

Conclusion: SelectivBench实现了对线性循环模型的针对性高效探索，为研究大规模评估中观察到的行为提供了受控环境，有助于理解序列模型的选择性机制。

Abstract: Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench

</details>


### [126] [Towards Spectroscopy: Susceptibility Clusters in Language Models](https://arxiv.org/abs/2601.12703)
*Andrew Gordon,Garrett Baker,George Wang,William Snell,Stan van Wingerden,Daniel Murfet*

Main category: cs.LG

TL;DR: 提出一种基于谱分析原理的神经网络内部结构分析方法，通过扰动数据分布测量模型响应，识别出510个可解释的聚类，涵盖语法模式、代码结构和数学符号等。


<details>
  <summary>Details</summary>
Motivation: 受光谱学通过扰动物理系统测量响应来推断内部结构的启发，作者希望将这一原理应用于神经网络，通过扰动数据分布来理解模型的内部工作机制和表示结构。

Method: 使用随机梯度朗之万动力学（SGLD）在局部吉布斯后验上计算敏感度χ_xy，开发基于电导的聚类算法，通过扰动数据分布中特定标记的权重来测量模型组件的响应。

Result: 在Pythia-14M模型上识别出510个可解释的聚类，涵盖语法模式、代码结构和数学符号等。与稀疏自编码器相比，50%的聚类与SAE特征匹配，验证了两种方法恢复相似结构。

Conclusion: 该方法成功将光谱学原理应用于神经网络分析，提供了一种理解模型内部结构的新框架，能够识别出可解释的表示模式，并与现有方法结果一致。

Abstract: Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $χ_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts "for similar reasons" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.

</details>


### [127] [Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off](https://arxiv.org/abs/2601.12730)
*Zhaochun Li,Chen Wang,Jionghao Bai,Shisheng Cui,Ge Lan,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: DCPO提出了一种分布中心的强化学习方法，通过分布级正则化控制策略熵，解决传统样本中心方法探索不足的问题，在多个基准上平均提升20%性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法（如GRPO）倾向于过度利用，导致熵单调下降、样本收敛、探索消失。现有解决方案多为样本中心方法，依赖稀有样本的"运气"，缺乏对策略的原则性控制，效果有限且不稳定。

Method: 提出Distribution-Centric Policy Optimization (DCPO)，从分布中心视角重新定义探索问题，将熵调节重新表述为分布级正则化。DCPO完全在策略内实现可控熵，无需从外部分布采样，实现高效探索同时保持训练稳定性。

Result: 在多个模型和七个基准测试中，DCPO相比GRPO平均提升约20%。DCPO用分布级原则替代样本级启发式方法，提供了理论基础和灵活框架。

Conclusion: DCPO通过分布中心视角为RL探索提供了理论基础和实用框架，实现了可控探索和更强的探索-利用权衡，代码已开源。

Abstract: The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the "luck" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \textbf{distribution-centric} perspective for RL, in which exploration is always guided by a "better" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.

</details>


### [128] [PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient](https://arxiv.org/abs/2601.12988)
*Zijian Wang,Tiancheng Huang,Hanqi Li,Da Ma,Lu Chen,Kai Yu*

Main category: cs.LG

TL;DR: PaperCompass是一个通过分离高层规划和细粒度执行来提升科学文献阅读代理效率的框架，采用Draft-and-Follow Policy Optimization训练方法，在保持性能的同时显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长使得研究人员难以通过手动阅读跟踪新进展。现有基于LLM的自主代理方法要么依赖大量工程化提示，要么使用传统的SFT-RL训练流程，这两种方法都容易导致过度探索和低效产出。

Method: PaperCompass框架将高层规划与细粒度执行分离：首先制定明确计划（draft plan）概述行动序列，然后通过详细推理实例化每个步骤，为相应函数调用选择参数。训练方法采用Draft-and-Follow Policy Optimization（DFPO），这是一种轻量级分层强化学习方法，联合优化草稿计划和最终解决方案。

Result: 在Paper-QA基准测试中，PaperCompass在保持性能的同时显著提高了效率，取得了与更大模型相当的结果，同时提供了理论分析支持DFPO的优化特性。

Conclusion: PaperCompass通过分离规划和执行的认知科学启发方法，有效解决了LLM在科学文献处理中的"知行差距"问题，提供了一种更稳定可靠的训练框架。

Abstract: The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.

</details>


### [129] [PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning](https://arxiv.org/abs/2601.13020)
*Zhiyan Hou,Haiyun Guo,Haokai Ma,Yandu Sun,Yonghui Yang,Jinqiao Wang*

Main category: cs.LG

TL;DR: 提出PASs-based MoE-LoRA方法，通过路径激活子空间校准路由和稳定重要秩方向，解决多模态大语言模型持续指令调优中的专家共漂移问题，提升准确性和抗遗忘性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的MoE方法在持续指令调优中，路由器和专家会共同漂移，导致早期输入-专家专业化逐渐偏离，造成专家责任模糊和遗忘加剧，需要解决这种"错位共漂移"现象。

Method: 提出路径激活子空间(PASs)作为能力对齐的坐标系统，基于此设计固定容量的PASs-based MoE-LoRA方法，包含PAS引导的重新加权（校准路由）和PAS感知的秩稳定化（选择性稳定先前任务的重要秩方向）。

Result: 在持续指令调优基准测试中，该方法在准确性和抗遗忘性方面持续优于传统持续学习基线和MoE-LoRA变体，且不增加参数。

Conclusion: PASs-based MoE-LoRA方法通过路径激活子空间有效解决了专家共漂移问题，提升了多模态大语言模型在持续指令调优中的性能，为持续学习提供了新思路。

Abstract: Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.

</details>


### [130] [Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks](https://arxiv.org/abs/2601.13244)
*Prateek Munjal,Clement Christophe,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.LG

TL;DR: 指令微调并不增强推理能力，而是诱导表面模式匹配。研究发现指令微调模型的性能优势不稳定，在零样本CoT设置下基础模型表现更好，且微调增益在分布偏移下脆弱。


<details>
  <summary>Details</summary>
Motivation: 探究指令微调是否真正增强LLM的推理能力，还是仅仅诱导表面模式匹配。当前对指令微调如何影响模型内在推理能力缺乏清晰理解。

Method: 在标准数学基准（GSM8K）、结构扰动变体和领域偏移任务（MedCalc）上评估基础和指令微调模型。分析零样本CoT与少样本设置下的性能差异。

Result: 1. 在零样本CoT设置下，基础模型始终优于指令微调变体（Llama3-70B下降高达32.67%）。2. 指令微调模型仅在提供少样本示例时才能匹配或超越基础模型。3. 在领域特定MedCalc基准上，基础模型优于指令微调变体。4. 指令微调模型在扰动数据集上表现急剧下降。

Conclusion: 指令微调并不增强内在推理能力，而是使模型依赖特定提示模式。其性能增益在分布偏移下脆弱，表明当前指令微调方法主要诱导表面模式匹配而非稳健推理。

Abstract: Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.

</details>


### [131] [CooperBench: Why Coding Agents Cannot be Your Teammates Yet](https://arxiv.org/abs/2601.13295)
*Arpandeep Khatua,Hao Zhu,Peter Tran,Arya Prabhudesai,Frederic Sadrieh,Johann K. Lieberwirth,Xinkai Yu,Yicheng Fu,Michael J. Ryan,Jiaxin Pei,Diyi Yang*

Main category: cs.LG

TL;DR: 论文提出了CooperBench基准测试，包含600多个协作编码任务，发现AI代理在协作时成功率比单独执行低30%，揭示了沟通不畅、承诺偏离和错误预期三大协调问题，呼吁从追求个体能力转向发展社交智能。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在复杂工作中越来越多地协作，它们需要发展协调能力来成为有效的团队成员。然而作者假设当前代理缺乏这些能力，需要建立一个基准来测试和评估代理的协作能力。

Method: 引入CooperBench基准测试，包含600多个协作编码任务，覆盖12个库和4种编程语言。每个任务给两个代理分配不同的功能特性，这些特性可以独立实现但可能在没有适当协调时发生冲突。任务基于真实开源仓库和专家编写的测试。通过大规模模拟评估最先进的编码代理。

Result: 观察到"协调诅咒"现象：代理协作时的平均成功率比单独执行两个任务低30%。这与人类团队形成鲜明对比（人类团队增加成员通常会提高生产力）。分析揭示了三个关键问题：1) 沟通渠道被模糊、时机不当和不准确的信息堵塞；2) 即使有有效沟通，代理也会偏离承诺；3) 代理经常对他人计划和沟通持有错误预期。同时观察到罕见但有趣的涌现协调行为，包括角色分工、资源分配和谈判。

Conclusion: 研究提出了一个新颖的协作编码基准测试，呼吁从追求个体代理能力转向发展社交智能。当前AI代理缺乏有效的团队协调能力，需要开发社交智能来解决协作中的沟通、承诺和预期问题。

Abstract: Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.

</details>


### [132] [Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility](https://arxiv.org/abs/2601.13398)
*Nickil Maveli,Antonio Vergari,Shay B. Cohen*

Main category: cs.LG

TL;DR: RTCE是一个评估代码大模型往返执行一致性的基准，发现现有模型在保持编码-解码双向一致性方面存在局限，即使使用零样本提示、监督微调和自反思机制也只能获得有限改进。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码基准测试中表现良好，但往返代码执行揭示了它们在保持前后向执行一致性推理方面的局限性。现有基准（如I/O预测、执行推理或往返自然语言基准）无法捕捉这种一致性缺陷。

Method: 提出RoundTripCodeEval (RTCE)基准，包含四个不同的代码执行推理任务，通过免执行的精确匹配评估双射保真度。使用零样本提示、监督微调执行轨迹和自反思机制系统评估最先进的代码LLMs。

Result: 每种方法（零样本提示、监督微调、自反思）都带来适度改进，但都无法弥合差距。当前LLMs在真正的往返一致性方面存在困难，缺乏可信代码推理所需的内在一致性。

Conclusion: RTCE揭示了现有基准未捕捉到的新见解，表明当前LLMs缺乏可信代码推理所需的内在一致性。基准代码和数据集将在接受后发布。

Abstract: LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.

</details>


### [133] [Behavior Knowledge Merge in Reinforced Agentic Models](https://arxiv.org/abs/2601.13572)
*Xiangchi Yuan,Dachuan Shi,Chunhui Zhang,Zheyuan Liu,Shenglong Yao,Soroush Vosoughi,Wenke Lee*

Main category: cs.LG

TL;DR: 本文提出RAM框架，专门针对RL训练的智能体模型进行模型合并，解决传统SFT合并方法在RL智能体上效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在智能体模型后训练中至关重要，模型合并是将多个任务训练的智能体整合为通用模型的有效方法。然而，现有的合并方法主要针对监督微调设计，不适用于RL训练的智能体模型，因为RL产生的任务向量具有高度稀疏性和异质性，而SFT合并方法假设向量密集且全局可比。

Method: 提出RAM框架，专门为RL训练的智能体模型设计。该方法解耦共享和任务特定的参数更新，对共享组件进行平均，同时选择性地保留和重新缩放独特组件，以抵消参数更新的稀释效应。

Result: 在多个智能体领域和模型架构上的实验表明，RAM不仅超越了现有的合并基线方法，还能解锁智能体之间的协同潜力，在各自领域中实现优于专业智能体的性能。

Conclusion: RAM框架有效解决了RL智能体模型合并中的任务向量不匹配问题，通过分布感知的合并方法保留了关键的任务特定行为，实现了更好的模型整合效果。

Abstract: Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.

</details>


### [134] [TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation](https://arxiv.org/abs/2601.13653)
*Xingjian Wu,Junkai Lu,Zhengyu Li,Xiangfei Qiu,Jilin Hu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.LG

TL;DR: TimeART是一个融合强大多功能工具分析能力和大语言模型推理能力的框架，作为完全自主的时间序列问答数据科学家，通过四阶段训练策略和100k专家轨迹语料库，在多个TSQA任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实网络物理系统中广泛存在时间序列数据，其分析解释具有重要价值（如灾害预测、金融风险控制），但当前工作流程主要依赖人类数据科学家，劳动成本高且缺乏自动化。

Method: 1. 提出TimeART框架，融合现成工具的分析能力和LLMs的推理能力；2. 收集100k专家轨迹语料库TimeToolBench；3. 设计四阶段训练策略，让时间序列推理模型从早期经验和自我反思中学习。

Result: 在TimeToolBench上训练的8B时间序列推理模型结合TimeART框架，在多个时间序列问答任务上实现了一致的state-of-the-art性能。

Conclusion: TimeART为自主时间序列推理开辟了新途径，展示了融合工具分析和LLM推理的框架在自动化时间序列分析中的有效性。

Abstract: Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.

</details>


### [135] [RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning](https://arxiv.org/abs/2601.13964)
*Cheol-Hui Lee,Hwa-Yeon Lee,Dong-Joo Kim*

Main category: cs.LG

TL;DR: RL-BioAug：利用强化学习代理自动确定最优数据增强策略的框架，仅需10%标记数据指导，在EEG对比学习中显著优于随机增强策略。


<details>
  <summary>Details</summary>
Motivation: EEG信号的非平稳特性使得静态或随机数据增强策略难以保留内在信息，影响对比学习性能。需要一种能自适应EEG信号特性的智能增强方法。

Method: 提出RL-BioAug框架，使用标签高效的强化学习代理自主确定最优增强策略。仅用10%标记数据指导代理策略，使编码器能以严格自监督方式学习鲁棒表示。

Result: 在Sleep-EDFX和CHB-MIT数据集上，相比随机选择策略，Macro-F1分数分别提升9.69%和8.80%。代理能针对不同任务选择最优策略，如睡眠分期任务62%概率选择时间掩码，癫痫检测任务77%概率选择裁剪与调整大小。

Conclusion: RL-BioAug有潜力取代传统的启发式增强方法，建立数据增强的自主新范式，为EEG对比学习提供更有效的增强策略。

Abstract: The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\% probability for sleep stage classification and Crop \& Resize with a 77\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.

</details>


### [136] [LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems](https://arxiv.org/abs/2601.14053)
*Badri N. Patro,Vijay S. Agneeswaran*

Main category: cs.LG

TL;DR: LLMOrbit提出一个全面的循环分类法，分析2019-2025年间的大语言模型，识别三大危机（数据稀缺、成本增长、能耗不可持续）和六大突破壁垒的范式，揭示三个范式转变（后训练增益、效率革命、民主化）。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域正经历从基础Transformer架构到接近人类水平推理系统的革命。需要系统性地梳理LLM发展脉络，分析当前面临的扩展壁垒危机，并探索突破这些限制的技术范式。

Method: 提出LLMOrbit循环分类法，通过八个相互关联的轨道维度分析超过50个模型和15个组织。从架构创新、训练方法、效率模式等角度系统考察现代LLM、生成式AI和智能体系统。

Result: 识别三大危机：数据稀缺（2026-2028年耗尽9-27T tokens）、成本指数增长（5年内从300万美元到3亿美元以上）、能耗不可持续（增加22倍）。发现六大突破壁垒的范式：测试时计算、量化、分布式边缘计算、模型融合、高效训练、小型专用模型。揭示三个范式转变：后训练增益、效率革命、民主化。

Conclusion: LLM发展正面临扩展壁垒，但通过测试时计算、效率优化和开源民主化等创新范式，可以在不依赖暴力扩展的情况下继续推进。后训练技术、效率革命和开源运动是推动AI发展的关键力量。

Abstract: The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.

</details>


### [137] [A model of errors in transformers](https://arxiv.org/abs/2601.14175)
*Suvrat Raju,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 本文研究LLM在需要确定性输出的任务（如算术）中的错误率，提出错误源于注意力机制中的小误差累积，并推导出错误率与任务复杂度的两参数定量关系。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在需要确定性输出的重复性任务（如算术）中为何会出错，挑战了关于LLM错误源于"推理崩溃"或无法表达"组合函数"的观点。

Method: 采用"有效场论"视角，将LLM的众多参数重组为两个关键参数：基本噪声率和可能错误预测的令牌数量。通过理论分析推导错误率与任务复杂度的定量关系，并使用Gemini 2.5 Flash、Gemini 2.5 Pro和DeepSeek R1进行广泛的实证测试。

Result: 在多种任务中，预测错误率与观察到的错误率表现出极好的一致性，尽管在某些情况下也发现了偏差。模型还展示了如何构建提示来降低错误率。

Conclusion: LLM在确定性任务中的错误可以通过注意力机制的小误差累积来解释，而不是"推理崩溃"或组合能力不足。提出的两参数模型能有效预测错误率，并为优化提示提供了实用指导。

Abstract: We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.

</details>


### [138] [InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning](https://arxiv.org/abs/2601.14209)
*Matthew Y. R. Yang,Hao Bai,Ian Wu,Gene Yang,Amrith Setlur,Aviral Kumar*

Main category: cs.LG

TL;DR: InT是一种训练范式，让模型通过提出短小、有针对性的修正来对自己的推理轨迹进行细粒度信用分配，从而引导轨迹获得更高奖励，解决了标准RL中的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 标准结果奖励强化学习只对最终答案分配信用，当结果错误时惩罚整个推理轨迹，正确时统一强化所有步骤。这导致正确中间步骤在失败轨迹中被抑制，而虚假步骤在成功轨迹中被强化，即信用分配问题。

Method: 提出干预训练（InT）：利用数学推理数据集中通常可用的参考解，模型识别自身推理中的第一个错误，并提出单步干预来将轨迹重定向到正确解。然后对错误点之前的策略展开加上干预进行监督微调，将错误定位到导致失败的具体步骤。

Result: 经过InT和后续RL微调后，在IMO-AnswerBench上比4B参数基础模型提高了近14%的准确率，优于gpt-oss-20b等更大的开源模型。

Conclusion: InT训练出的模型为RL训练提供了更好的初始化，通过让模型对自己的推理轨迹进行细粒度信用分配，有效解决了标准RL中的信用分配问题。

Abstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.

</details>


### [139] [Q-learning with Adjoint Matching](https://arxiv.org/abs/2601.14234)
*Qiyang Li,Sergey Levine*

Main category: cs.LG

TL;DR: QAM是一种新的TD强化学习算法，通过伴随匹配技术解决连续动作RL中扩散/流匹配策略优化的数值不稳定问题，在稀疏奖励任务上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决连续动作强化学习中一个长期存在的挑战：如何高效优化具有表达力的扩散或流匹配策略。现有方法要么只使用值函数而丢弃梯度信息，要么依赖近似方法牺牲策略表达能力或引入偏差。

Method: 提出Q-learning with Adjoint Matching (QAM)，利用伴随匹配技术将评论家的动作梯度转换为步进式目标函数，避免不稳定的反向传播，同时提供无偏且表达力强的策略。结合时间差分备份进行评论家学习。

Result: 在困难的稀疏奖励任务上，QAM在离线RL和离线到在线RL中都持续优于现有方法。

Conclusion: QAM通过伴随匹配技术有效解决了连续动作RL中扩散策略优化的数值不稳定问题，为表达力强的策略优化提供了稳定且无偏的解决方案。

Abstract: We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.

</details>


### [140] [Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow](https://arxiv.org/abs/2601.14243)
*Haocheng Xi,Charlie Ruan,Peiyuan Liao,Yujun Lin,Han Cai,Yilong Zhao,Shuo Yang,Kurt Keutzer,Song Han,Ligeng Zhu*

Main category: cs.LG

TL;DR: Jet-RL提出统一的FP8精度强化学习训练框架，解决现有BF16训练+FP8推理策略的数值不匹配问题，实现33%推理加速和16%端到端加速，同时保持稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 现有RL训练中推理阶段占70%以上时间，量化训练（特别是FP8精度）是解决瓶颈的可行方案。但常用的BF16训练+FP8推理策略在长序列和复杂任务下存在严重训练不稳定和精度崩溃问题。

Method: 提出Jet-RL框架，采用统一的FP8精度流同时用于训练和推理，最小化数值差异，消除低效的跨步骤校准需求。

Result: Jet-RL在推理阶段实现33%加速，训练阶段41%加速，端到端16%加速，在所有设置下保持稳定收敛，精度损失可忽略。

Conclusion: 统一的FP8精度流是解决RL训练中数值不匹配问题的有效方案，Jet-RL框架在保持精度的同时显著提升训练效率。

Abstract: Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.

</details>
