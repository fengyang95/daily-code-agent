{"id": "2602.22368", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22368", "abs": "https://arxiv.org/abs/2602.22368", "authors": ["Jiahao Zhang", "Yifan Zhang", "Kevin Leach", "Yu Huang"], "title": "EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization", "comment": "Accepted at the 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026), April 12-13, 2026, Rio de Janeiro, Brazil", "summary": "Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (\u03bc_i, \u03c3_i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.", "AI": {"tldr": "EyeLayer\uff1a\u901a\u8fc7\u4eba\u7c7b\u773c\u52a8\u6a21\u5f0f\u589e\u5f3aLLM\u4ee3\u7801\u6458\u8981\u7684\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u6a21\u5757", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4eba\u7c7b\u5728\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u7684\u4e13\u4e1a\u77e5\u8bc6\u662f\u5426\u80fd\u8fdb\u4e00\u6b65\u6307\u5bfc\u548c\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u4eba\u7c7b\u773c\u52a8\u6a21\u5f0f\u4f5c\u4e3a\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u4ee3\u7406\uff0c\u80fd\u5426\u63d0\u5347LLM\u7684\u4ee3\u7801\u6458\u8981\u80fd\u529b\u3002", "method": "\u63d0\u51faEyeLayer\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5efa\u6a21\u4eba\u7c7b\u9605\u8bfb\u4ee3\u7801\u65f6\u7684\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u6ce8\u610f\u529b\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u65e0\u7f1d\u96c6\u6210\u5230LLM\u4e2d\u800c\u4e0d\u5e72\u6270\u73b0\u6709\u8868\u793a\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\uff08LLaMA-3.2\u3001Qwen3\u3001CodeBERT\uff09\u4e0a\u8bc4\u4f30\uff0cEyeLayer\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5fae\u8c03\u57fa\u7ebf\uff0c\u5728BLEU-4\u6307\u6807\u4e0a\u6700\u9ad8\u63d0\u534713.17%\u3002", "conclusion": "\u4eba\u7c7b\u773c\u52a8\u6a21\u5f0f\u7f16\u7801\u4e86\u4e92\u8865\u7684\u6ce8\u610f\u529b\u4fe1\u53f7\uff0c\u80fd\u591f\u589e\u5f3aLLM\u7684\u8bed\u4e49\u805a\u7126\u80fd\u529b\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u4e0d\u540c\u6a21\u578b\u4e0a\uff0c\u63d0\u5347\u4ee3\u7801\u6458\u8981\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2602.22402", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2602.22402", "abs": "https://arxiv.org/abs/2602.22402", "authors": ["Cosmo Santoni"], "title": "Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents", "comment": "11 pages. 6 figures. Introduces a DAG-based state management system for LLM agents. Evaluation on 76 coding sessions shows up to 86% token reduction (mean 20%) while remaining economically viable under prompt caching. Includes reference implementation for Claude Code", "summary": "As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.", "AI": {"tldr": "CMV\u662f\u4e00\u79cd\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5c06LLM\u5728\u957f\u5bf9\u8bdd\u4e2d\u79ef\u7d2f\u7684\u7406\u89e3\u89c6\u4e3a\u7248\u672c\u63a7\u5236\u72b6\u6001\uff0c\u901a\u8fc7DAG\u5efa\u6a21\u4f1a\u8bdd\u5386\u53f2\uff0c\u5b9e\u73b0\u8de8\u4f1a\u8bdd\u7684\u4e0a\u4e0b\u6587\u91cd\u7528\uff0c\u5e76\u91c7\u7528\u65e0\u635f\u4fee\u526a\u7b97\u6cd5\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e2d\u4f1a\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e2d\u79ef\u7d2f\u91cd\u8981\u72b6\u6001\u4fe1\u606f\uff0c\u4f46\u5f53\u4f1a\u8bdd\u8fbe\u5230\u4e0a\u4e0b\u6587\u9650\u5236\u65f6\uff0c\u8fd9\u4e9b\u7406\u89e3\u4f1a\u56e0\u6709\u635f\u538b\u7f29\u800c\u4e22\u5931\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u4fdd\u5b58\u548c\u91cd\u7528\u8fd9\u4e9b\u7d2f\u79ef\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u865a\u62df\u5316(CMV)\u7cfb\u7edf\uff0c\u501f\u9274\u64cd\u4f5c\u7cfb\u7edf\u865a\u62df\u5185\u5b58\u6982\u5ff5\uff0c\u5c06\u4f1a\u8bdd\u5386\u53f2\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u5b9a\u4e49\u5feb\u7167\u3001\u5206\u652f\u548c\u4fee\u526a\u539f\u8bed\uff0c\u5e76\u5f00\u53d1\u4e09\u904d\u7ed3\u6784\u65e0\u635f\u4fee\u526a\u7b97\u6cd5\uff0c\u4fdd\u7559\u6240\u6709\u7528\u6237\u6d88\u606f\u548c\u52a9\u624b\u54cd\u5e94\uff0c\u4f46\u53bb\u9664\u673a\u68b0\u6027\u5197\u4f59\u5185\u5bb9\u3002", "result": "\u572876\u4e2a\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4f1a\u8bdd\u7684\u8bc4\u4f30\u4e2d\uff0c\u4fee\u526a\u5e73\u5747\u51cf\u5c1120%\u7684token\uff0c\u6700\u9ad8\u53ef\u8fbe86%\uff1b\u6df7\u5408\u5de5\u5177\u4f7f\u7528\u4f1a\u8bdd\u5e73\u5747\u51cf\u5c1139%\uff0c\u572810\u8f6e\u5bf9\u8bdd\u5185\u8fbe\u5230\u76c8\u4e8f\u5e73\u8861\uff1b\u4fee\u526a\u5728\u7ecf\u6d4e\u4e0a\u53ef\u884c\uff0c\u7279\u522b\u662f\u5728\u6709\u63d0\u793a\u7f13\u5b58\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "CMV\u7cfb\u7edf\u80fd\u6709\u6548\u7ba1\u7406LLM\u4f1a\u8bdd\u4e2d\u7684\u7d2f\u79ef\u72b6\u6001\uff0c\u901a\u8fc7\u865a\u62df\u5316\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5b9e\u73b0\u8de8\u4f1a\u8bdd\u7684\u77e5\u8bc6\u91cd\u7528\uff0c\u65e0\u635f\u4fee\u526a\u7b97\u6cd5\u663e\u8457\u964d\u4f4etoken\u6d88\u8017\uff0c\u4e3a\u957f\u5bf9\u8bdd\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.22403", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22403", "abs": "https://arxiv.org/abs/2602.22403", "authors": ["Saumendu Roy", "Banani Roy", "Chanchal Roy", "Richard Bassey"], "title": "XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction", "comment": "10 pages, 14 figures, conference", "summary": "Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.", "AI": {"tldr": "XMENTOR\u662f\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u7684VS Code\u63d2\u4ef6\uff0c\u901a\u8fc7\u805a\u5408\u591a\u4e2aXAI\u89e3\u91ca\u65b9\u6cd5\uff08\u5982LIME\u3001SHAP\u3001BreakDown\uff09\u6765\u63d0\u4f9b\u7edf\u4e00\u3001\u4e00\u81f4\u7684\u900f\u660e\u5ea6\u89c6\u56fe\uff0c\u51cf\u5c11\u5f00\u53d1\u8005\u7684\u56f0\u60d1\u548c\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7f3a\u9677\u9884\u6d4b\u6a21\u578b\u867d\u7136\u80fd\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u5f00\u53d1\u8005\u96be\u4ee5\u4fe1\u4efb\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff08LIME\u3001SHAP\u3001BreakDown\uff09\u540c\u65f6\u4f7f\u7528\u65f6\u7ecf\u5e38\u4ea7\u751f\u76f8\u4e92\u77db\u76fe\u7684\u89e3\u91ca\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u8005\u7684\u56f0\u60d1\u3001\u632b\u6298\u611f\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "method": "\u63d0\u51faXMENTOR\u65b9\u6cd5\uff0c\u4f5c\u4e3aVS Code\u63d2\u4ef6\u5b9e\u73b0\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u9608\u503c\u3001\u6392\u540d\u548c\u7b26\u53f7\u4e00\u81f4\u6027\u4ee5\u53ca\u56de\u9000\u7b56\u7565\uff0c\u5c06\u591a\u4e2a\u540e\u7f6e\u89e3\u91ca\u65b9\u6cd5\u7edf\u4e00\u4e3a\u5355\u4e00\u3001\u8fde\u8d2f\u7684\u89c6\u56fe\uff0c\u907f\u514d\u4fe1\u606f\u8fc7\u8f7d\u3002", "result": "\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u8fd190%\u7684\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u805a\u5408\u540e\u7684\u89e3\u91ca\uff0c\u8ba4\u4e3a\u8fd9\u51cf\u5c11\u4e86\u56f0\u60d1\uff0c\u5e76\u66f4\u597d\u5730\u652f\u6301\u65e5\u5e38\u7684\u8c03\u8bd5\u548c\u7f3a\u9677\u5ba1\u67e5\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u7ec4\u5408\u591a\u4e2a\u89e3\u91ca\u65b9\u6cd5\u5e76\u5c06\u5176\u5d4c\u5165\u5230\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7f3a\u9677\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "topic": "swe application"}}
{"id": "2602.22518", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22518", "abs": "https://arxiv.org/abs/2602.22518", "authors": ["Xuefeng Li", "Nir Ben-Israel", "Yotam Raz", "Belal Ahmed", "Doron Serebro", "Antoine Raux"], "title": "RepoMod-Bench: A Benchmark for Code Repository Modernization via Implementation-Agnostic Testing", "comment": null, "summary": "The evolution of AI coding agents has shifted the frontier from simple snippet completion to autonomous repository-level engineering. However, evaluating these agents remains ill-posed in general code repository generation, where the lack of deterministic ground truth leads to ambiguous metrics. Code modernization via automated translation offers a more rigorous alternative by providing a fixed ground truth -- the source repository; yet existing benchmarks are limited to small-scale repositories and rely on language-specific unit tests visible to the agent, allowing test-driven overfitting.\n  We address these limitations by introducing a benchmarking framework for repository-level code modernization built on an implementation-agnostic evaluation paradigm. This framework is instantiated through RepoMod-Bench: a benchmark of 21 real-world repositories with standardized interfaces, spanning 8 programming languages. The benchmark contains 1.6M lines of code (LOC) and 11,616 tests, with repository sizes ranging from 14 to 211K LOC. By targeting repositories with standardized interfaces, we utilize an implementation-agnostic test suite to verify functional equivalence between source and target implementations. This black-box approach ensures verification remains consistent across languages, and our environment hides all test suites from agents to prevent test-driven shortcuts. Evaluating four state-of-the-art agent configurations reveals a sharp scaling collapse: average pass rates drop from 91.3% on projects under 10K LOC to 15.3% on projects exceeding 50K LOC. These results demonstrate that autonomous modernization at scale remains a significant open challenge. Our benchmark and code are available at https://github.com/Modelcode-ai/mcode-benchmark.", "AI": {"tldr": "RepoMod-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u4ee3\u7801\u73b0\u4ee3\u5316\uff08\u8de8\u8bed\u8a00\u7ffb\u8bd1\uff09\u7684\u4ed3\u5e93\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b21\u4e2a\u771f\u5b9e\u4ed3\u5e93\u30018\u79cd\u7f16\u7a0b\u8bed\u8a00\u3001160\u4e07\u884c\u4ee3\u7801\uff0c\u91c7\u7528\u9ed1\u76d2\u6d4b\u8bd5\u9632\u6b62\u6d4b\u8bd5\u9a71\u52a8\u8fc7\u62df\u5408\uff0c\u8bc4\u4f30\u663e\u793a\u4ee3\u7801\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u9879\u76ee\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7801\u4ee3\u7406\u8bc4\u4f30\u5728\u901a\u7528\u4ee3\u7801\u4ed3\u5e93\u751f\u6210\u4e2d\u7f3a\u4e4f\u786e\u5b9a\u6027\u57fa\u51c6\uff0c\u800c\u4ee3\u7801\u73b0\u4ee3\u5316\uff08\u81ea\u52a8\u7ffb\u8bd1\uff09\u867d\u6709\u56fa\u5b9a\u57fa\u51c6\uff08\u6e90\u4ed3\u5e93\uff09\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4ec5\u9650\u4e8e\u5c0f\u89c4\u6a21\u4ed3\u5e93\u4e14\u4f9d\u8d56\u8bed\u8a00\u7279\u5b9a\u5355\u5143\u6d4b\u8bd5\uff0c\u5bb9\u6613\u5bfc\u81f4\u6d4b\u8bd5\u9a71\u52a8\u8fc7\u62df\u5408\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5b9e\u73b0\u65e0\u5173\u8bc4\u4f30\u8303\u5f0f\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u73b0\u4ee3\u5316\u57fa\u51c6\u6846\u67b6\uff0c\u5177\u4f53\u5b9e\u73b0\u4e3aRepoMod-Bench\uff1a\u5305\u542b21\u4e2a\u771f\u5b9e\u4ed3\u5e93\u30018\u79cd\u7f16\u7a0b\u8bed\u8a00\u3001160\u4e07\u884c\u4ee3\u7801\u548c11,616\u4e2a\u6d4b\u8bd5\u3002\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u4ed3\u5e93\uff0c\u4f7f\u7528\u5b9e\u73b0\u65e0\u5173\u6d4b\u8bd5\u5957\u4ef6\u9a8c\u8bc1\u6e90\u548c\u76ee\u6807\u5b9e\u73b0\u7684\u529f\u80fd\u7b49\u4ef7\u6027\uff0c\u91c7\u7528\u9ed1\u76d2\u65b9\u6cd5\u9690\u85cf\u6240\u6709\u6d4b\u8bd5\u5957\u4ef6\u9632\u6b62\u6d4b\u8bd5\u9a71\u52a8\u6377\u5f84\u3002", "result": "\u8bc4\u4f30\u56db\u4e2a\u6700\u5148\u8fdb\u4ee3\u7406\u914d\u7f6e\u663e\u793a\u660e\u663e\u7684\u89c4\u6a21\u5d29\u6e83\uff1a\u572810K LOC\u4ee5\u4e0b\u9879\u76ee\u4e2d\u5e73\u5747\u901a\u8fc7\u738791.3%\uff0c\u4f46\u5728\u8d85\u8fc750K LOC\u9879\u76ee\u4e2d\u964d\u81f315.3%\uff0c\u8868\u660e\u5927\u89c4\u6a21\u81ea\u4e3b\u73b0\u4ee3\u5316\u4ecd\u662f\u91cd\u5927\u5f00\u653e\u6311\u6218\u3002", "conclusion": "RepoMod-Bench\u4e3a\u4ed3\u5e93\u7ea7\u4ee3\u7801\u73b0\u4ee3\u5316\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524dAI\u4ee3\u7801\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u9879\u76ee\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u81ea\u4e3b\u73b0\u4ee3\u5316\u5728\u89c4\u6a21\u6269\u5c55\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "topic": "swe benchmark"}}
{"id": "2602.22401", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.22401", "abs": "https://arxiv.org/abs/2602.22401", "authors": ["Yongjun Zhang"], "title": "Vibe Researching as Wolf Coming: Can AI Agents with Skills Replace or Augment Social Scientists?", "comment": "Commentary", "summary": "AI agents -- systems that execute multi-step reasoning workflows with persistent state, tool access, and specialist skills -- represent a qualitative shift from prior automation technologies in social science. Unlike chatbots that respond to isolated queries, AI agents can now read files, run code, query databases, search the web, and invoke domain-specific skills to execute entire research pipelines autonomously. This paper introduces the concept of vibe researching -- the AI-era parallel to ``vibe coding'' (Karpathy, 2025) -- and uses scholar-skill, a 21-skill plugin for Claude Code covering the full research pipeline from idea to submission, as an illustrative case. I develop a cognitive task framework that classifies research activities along two dimensions -- codifiability and tacit knowledge requirement -- to identify a delegation boundary that is cognitive, not sequential: it cuts through every stage of the research pipeline, not between stages. I argue that AI agents excel at speed, coverage, and methodological scaffolding but struggle with theoretical originality and tacit field knowledge. The paper concludes with an analysis of three implications for the profession -- augmentation with fragile conditions, stratification risk, and a pedagogical crisis -- and proposes five principles for responsible vibe researching.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6c1b\u56f4\u7814\u7a76\"\u6982\u5ff5\uff0c\u7c7b\u6bd4\"\u6c1b\u56f4\u7f16\u7a0b\"\uff0c\u5206\u6790AI\u4ee3\u7406\u5982\u4f55\u901a\u8fc721\u79cd\u6280\u80fd\u63d2\u4ef6\u81ea\u4e3b\u6267\u884c\u5b8c\u6574\u7814\u7a76\u6d41\u7a0b\uff0c\u8bc6\u522bAI\u64c5\u957f\u901f\u5ea6\u3001\u8986\u76d6\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u539f\u521b\u6027\u548c\u9690\u6027\u9886\u57df\u77e5\u8bc6\u3002", "motivation": "AI\u4ee3\u7406\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u80fd\u529b\u8d85\u8d8a\u4e86\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\uff0c\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u5b8c\u6574\u7814\u7a76\u6d41\u7a0b\uff0c\u8fd9\u4ee3\u8868\u4e86\u8d28\u7684\u8f6c\u53d8\u3002\u9700\u8981\u7406\u89e3AI\u4ee3\u7406\u5728\u7814\u7a76\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\u548c\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u4efb\u52a1\u6846\u67b6\uff0c\u6309\u53ef\u7f16\u7801\u6027\u548c\u9690\u6027\u77e5\u8bc6\u9700\u6c42\u4e24\u4e2a\u7ef4\u5ea6\u5206\u7c7b\u7814\u7a76\u6d3b\u52a8\uff1b\u4f7f\u7528scholar-skill\uff08\u5305\u542b21\u79cd\u6280\u80fd\u7684Claude Code\u63d2\u4ef6\uff09\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff1b\u5206\u6790\u7814\u7a76\u6d41\u7a0b\u4e2d\u7684\u59d4\u6258\u8fb9\u754c\u3002", "result": "AI\u4ee3\u7406\u5728\u901f\u5ea6\u3001\u8986\u76d6\u8303\u56f4\u548c\u65b9\u6cd5\u6846\u67b6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7406\u8bba\u539f\u521b\u6027\u548c\u9690\u6027\u9886\u57df\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff1b\u59d4\u6258\u8fb9\u754c\u662f\u8ba4\u77e5\u6027\u7684\u800c\u975e\u987a\u5e8f\u6027\u7684\uff0c\u8d2f\u7a7f\u7814\u7a76\u5404\u9636\u6bb5\u3002", "conclusion": "AI\u4ee3\u7406\u5c06\u5e26\u6765\u589e\u5f3a\u4f46\u6761\u4ef6\u8106\u5f31\u3001\u5206\u5c42\u98ce\u9669\u548c\u6559\u80b2\u5371\u673a\u7b49\u804c\u4e1a\u5f71\u54cd\uff0c\u63d0\u51fa\u8d1f\u8d23\u4efb\u6c1b\u56f4\u7814\u7a76\u7684\u4e94\u9879\u539f\u5219\u3002", "topic": "agent analysis"}}
{"id": "2602.22764", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22764", "abs": "https://arxiv.org/abs/2602.22764", "authors": ["Jiahong Xiang", "Wenxiao He", "Xihua Wang", "Hongliang Tian", "Yuqun Zhang"], "title": "Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents", "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026)", "summary": "The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level software engineering tasks from 34 diverse and popular Rust repositories. We then perform a comprehensive study on Rust-SWE-bench with four representative agents and four state-of-the-art LLMs to establish a foundational understanding of their capabilities and limitations in the Rust ecosystem. Our extensive study reveals that while ReAct-style agents are promising, i.e., resolving up to 21.2% of issues, they are limited by two primary challenges: comprehending repository-wide code structure and complying with Rust's strict type and trait semantics. We also find that issue reproduction is rather critical for task resolution. Inspired by these findings, we propose RUSTFORGER, a novel agentic approach that integrates an automated test environment setup with a Rust metaprogramming-driven dynamic tracing strategy to facilitate reliable issue reproduction and dynamic analysis. The evaluation shows that RUSTFORGER using Claude-Sonnet-3.7 significantly outperforms all baselines, resolving 28.6% of tasks on Rust-SWE-bench, i.e., a 34.9% improvement over the strongest baseline, and, in aggregate, uniquely solves 46 tasks that no other agent could solve across all adopted advanced LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Rust-SWE-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u4e2a\u771f\u5b9eRust\u4ed3\u5e93\u7ea7\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\u5728Rust\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86RUSTFORGER\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u73af\u5883\u8bbe\u7f6e\u548c\u52a8\u6001\u8ffd\u8e2a\u7b56\u7565\u663e\u8457\u63d0\u5347\u95ee\u9898\u89e3\u51b3\u7387\u3002", "motivation": "Rust\u7f16\u7a0b\u8bed\u8a00\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u7f16\u7801\u6311\u6218\u5927\uff0c\u9700\u8981\u81ea\u52a8\u5316\u95ee\u9898\u89e3\u51b3\u6765\u4fc3\u8fdb\u5176\u5e7f\u6cdb\u91c7\u7528\u3002\u867d\u7136LLM\u9a71\u52a8\u7684\u4ee3\u7801\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9Rust\u7684\u5927\u89c4\u6a21\u4ed3\u5e93\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "1) \u521b\u5efaRust-SWE-bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u4e2a\u6765\u81ea34\u4e2a\u6d41\u884cRust\u4ed3\u5e93\u7684\u771f\u5b9e\u4ed3\u5e93\u7ea7\u4efb\u52a1\uff1b2) \u4f7f\u75284\u4e2a\u4ee3\u8868\u6027\u4ee3\u7406\u548c4\u4e2aSOTA LLM\u8fdb\u884c\u7efc\u5408\u7814\u7a76\uff1b3) \u63d0\u51faRUSTFORGER\u65b0\u65b9\u6cd5\uff0c\u96c6\u6210\u81ea\u52a8\u5316\u6d4b\u8bd5\u73af\u5883\u8bbe\u7f6e\u548cRust\u5143\u7f16\u7a0b\u9a71\u52a8\u7684\u52a8\u6001\u8ffd\u8e2a\u7b56\u7565\u3002", "result": "\u73b0\u6709ReAct\u98ce\u683c\u4ee3\u7406\u6700\u591a\u89e3\u51b321.2%\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ed3\u6784\u7406\u89e3\u548cRust\u4e25\u683c\u7c7b\u578b/\u7279\u8d28\u8bed\u4e49\u3002RUSTFORGER\u4f7f\u7528Claude-Sonnet-3.7\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u89e3\u51b328.6%\u7684\u4efb\u52a1\uff08\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534734.9%\uff09\uff0c\u72ec\u7279\u89e3\u51b3\u4e8646\u4e2a\u5176\u4ed6\u4ee3\u7406\u65e0\u6cd5\u89e3\u51b3\u7684\u4efb\u52a1\u3002", "conclusion": "Rust\u4ee3\u7801\u4ee3\u7406\u9762\u4e34\u4ed3\u5e93\u7ea7\u4ee3\u7801\u7ed3\u6784\u7406\u89e3\u548c\u7c7b\u578b\u8bed\u4e49\u5408\u89c4\u6027\u6311\u6218\uff0c\u95ee\u9898\u590d\u73b0\u5bf9\u4efb\u52a1\u89e3\u51b3\u81f3\u5173\u91cd\u8981\u3002RUSTFORGER\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u73af\u5883\u548c\u52a8\u6001\u8ffd\u8e2a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e9b\u6311\u6218\uff0c\u4e3aRust\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe benchmark"}}
{"id": "2602.22406", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22406", "abs": "https://arxiv.org/abs/2602.22406", "authors": ["Xinle Wu", "Rui Zhang", "Mustafa Anis Hussain", "Yao Lu"], "title": "Towards Autonomous Memory Agents", "comment": null, "summary": "Recent memory agents improve LLMs by extracting experiences and conversation history into an external storage. This enables low-overhead context assembly and online memory update without expensive LLM training. However, existing solutions remain passive and reactive; memory growth is bounded by information that happens to be available, while memory agents seldom seek external inputs in uncertainties. We propose autonomous memory agents that actively acquire, validate, and curate knowledge at a minimum cost. U-Mem materializes this idea via (i) a cost-aware knowledge-extraction cascade that escalates from cheap self/teacher signals to tool-verified research and, only when needed, expert feedback, and (ii) semantic-aware Thompson sampling to balance exploration and exploitation over memories and mitigate cold-start bias. On both verifiable and non-verifiable benchmarks, U-Mem consistently beats prior memory baselines and can surpass RL-based optimization, improving HotpotQA (Qwen2.5-7B) by 14.6 points and AIME25 (Gemini-2.5-flash) by 7.33 points.", "AI": {"tldr": "U-Mem\u63d0\u51fa\u81ea\u4e3b\u8bb0\u5fc6\u4ee3\u7406\uff0c\u901a\u8fc7\u6210\u672c\u611f\u77e5\u7684\u77e5\u8bc6\u63d0\u53d6\u7ea7\u8054\u548c\u8bed\u4e49\u611f\u77e5Thompson\u91c7\u6837\uff0c\u4e3b\u52a8\u83b7\u53d6\u3001\u9a8c\u8bc1\u548c\u6574\u7406\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd", "motivation": "\u73b0\u6709\u8bb0\u5fc6\u4ee3\u7406\u662f\u88ab\u52a8\u548c\u53cd\u5e94\u5f0f\u7684\uff0c\u8bb0\u5fc6\u589e\u957f\u53d7\u9650\u4e8e\u53ef\u7528\u4fe1\u606f\uff0c\u4e14\u5f88\u5c11\u5728\u4e0d\u786e\u5b9a\u65f6\u4e3b\u52a8\u5bfb\u6c42\u5916\u90e8\u8f93\u5165\u3002\u9700\u8981\u81ea\u4e3b\u8bb0\u5fc6\u4ee3\u7406\u6765\u4e3b\u52a8\u83b7\u53d6\u3001\u9a8c\u8bc1\u548c\u6574\u7406\u77e5\u8bc6", "method": "1) \u6210\u672c\u611f\u77e5\u77e5\u8bc6\u63d0\u53d6\u7ea7\u8054\uff1a\u4ece\u5ec9\u4ef7\u7684\u81ea/\u6559\u5e08\u4fe1\u53f7\u9010\u6b65\u5347\u7ea7\u5230\u5de5\u5177\u9a8c\u8bc1\u7684\u7814\u7a76\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u5bfb\u6c42\u4e13\u5bb6\u53cd\u9988\uff1b2) \u8bed\u4e49\u611f\u77e5Thompson\u91c7\u6837\uff1a\u5e73\u8861\u8bb0\u5fc6\u7684\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u7f13\u89e3\u51b7\u542f\u52a8\u504f\u5dee", "result": "\u5728\u53ef\u9a8c\u8bc1\u548c\u4e0d\u53ef\u9a8c\u8bc1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cU-Mem\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8bb0\u5fc6\u57fa\u7ebf\uff0c\u751a\u81f3\u8d85\u8d8a\u57fa\u4e8eRL\u7684\u4f18\u5316\u65b9\u6cd5\uff1aHotpotQA\uff08Qwen2.5-7B\uff09\u63d0\u534714.6\u5206\uff0cAIME25\uff08Gemini-2.5-flash\uff09\u63d0\u53477.33\u5206", "conclusion": "\u81ea\u4e3b\u8bb0\u5fc6\u4ee3\u7406\u901a\u8fc7\u4e3b\u52a8\u77e5\u8bc6\u83b7\u53d6\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff0c\u4e3a\u8bb0\u5fc6\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411", "topic": "agent analysis"}}
{"id": "2602.22260", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.22260", "abs": "https://arxiv.org/abs/2602.22260", "authors": ["Camilo Chac\u00f3n Sartori", "Guillem Rodr\u00edguez Corominas"], "title": "Code World Models for Parameter Control in Evolutionary Algorithms", "comment": null, "summary": "Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \\lo{} and \\onemax{}, CWM-greedy performs within 6\\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \\jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\\% success rate), CWM-greedy achieves 100\\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\\ 500 online episodes), success rate (100\\% vs.\\ 58\\%), and generalization ($k{=}3$: 78\\% vs.\\ 0\\%). Robustness experiments confirm stable synthesis across 5 independent runs.", "AI": {"tldr": "LLM\u901a\u8fc7\u5408\u6210Python\u7a0b\u5e8f\u6765\u6a21\u62df\u4f18\u5316\u5668\u52a8\u6001\uff0c\u5e76\u5229\u7528\u8be5\u6a21\u62df\u5668\u8fdb\u884c\u8d2a\u5a6a\u89c4\u5212\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u8d85\u8d8a\u4f20\u7edf\u81ea\u9002\u5e94\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u63a2\u7d22LLM\u662f\u5426\u80fd\u591f\u5b66\u4e60\u4f18\u5316\u5668\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u8fd9\u79cd\u77e5\u8bc6\u6765\u63a7\u5236\u4f18\u5316\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u968f\u673a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d", "method": "\u6269\u5c55\u4ee3\u7801\u4e16\u754c\u6a21\u578b(CWMs)\u5230\u968f\u673a\u7ec4\u5408\u4f18\u5316\uff0cLLM\u6839\u636e\u6b21\u4f18\u8f68\u8ff9\u5408\u6210\u4f18\u5316\u5668\u52a8\u6001\u6a21\u62df\u5668\uff0c\u7136\u540e\u901a\u8fc7\u8d2a\u5a6a\u89c4\u5212\u9009\u62e9\u6bcf\u4e00\u6b65\u7684\u53d8\u5f02\u5f3a\u5ea6k", "result": "\u5728LO\u548cOneMax\u95ee\u9898\u4e0a\uff0cCWM-greedy\u8fbe\u5230\u7406\u8bba\u6700\u4f18\u7b56\u7565\u768494%\uff1b\u5728Jump_k\u95ee\u9898\u4e0a\uff0c\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u5931\u8d25\u65f6\uff0cCWM-greedy\u8fbe\u5230100%\u6210\u529f\u7387\uff1b\u5728NK-Landscape\u4e0a\uff0cCWM-greedy\u572815\u4e2a\u5b9e\u4f8b\u4e2d\u5747\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4f18\u5316\u5668\u52a8\u6001\u5e76\u7528\u4e8e\u63a7\u5236\uff0c\u5728\u6837\u672c\u6548\u7387\u3001\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4ee3\u7801\u4e16\u754c\u6a21\u578b\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6f5c\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2602.23005", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23005", "abs": "https://arxiv.org/abs/2602.23005", "authors": ["Man Zhang", "Tao Yue", "Yihua He"], "title": "Managing Uncertainty in LLM-based Multi-Agent System Operation", "comment": null, "summary": "Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering concern. This paper approaches uncertainty from both system-level and runtime perspectives. We first differentiate epistemological and ontological uncertainties in the context of LLM-based multi-agent software system operation. Building on this foundation, we propose a lifecycle-based uncertainty management framework comprising four mechanisms: representation, identification, evolution, and adaptation. The uncertainty lifecycle governs how uncertainties emerge, transform, and are mitigated across architectural layers and execution phases, enabling structured runtime governance and controlled adaptation. We demonstrate the feasibility of the framework using a real-world LLM-based multi-agent echocardiographic software system developed in clinical collaboration, showing improved reliability and diagnosability in diagnostic reasoning. The proposed approach generalizes to other safety-critical LLM-based multi-agent software systems, supporting principled operational control and runtime assurance beyond model-centric methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9762\u5411LLM\u591a\u667a\u80fd\u4f53\u8f6f\u4ef6\u7cfb\u7edf\u7684\u751f\u547d\u5468\u671f\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u6846\u67b6\uff0c\u5305\u542b\u8868\u793a\u3001\u8bc6\u522b\u3001\u6f14\u5316\u548c\u9002\u5e94\u56db\u4e2a\u673a\u5236\uff0c\u901a\u8fc7\u4e34\u5e8a\u8d85\u58f0\u5fc3\u52a8\u56fe\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u5bff\u547d\u8d85\u58f0\u5fc3\u52a8\u56fe\uff09\u5e94\u7528LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u65f6\uff0c\u4ec5\u63d0\u9ad8\u6a21\u578b\u7cbe\u5ea6\u65e0\u6cd5\u89e3\u51b3\u7cfb\u7edf\u7ea7\u98ce\u9669\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5728\u6a21\u578b\u5c42\u9762\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u975e\u5c06\u5176\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4e00\u7b49\u516c\u6c11\u3002\u9700\u8981\u4ece\u7cfb\u7edf\u548c\u8fd0\u884c\u65f6\u89d2\u5ea6\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u3002", "method": "1) \u533a\u5206\u8ba4\u8bc6\u8bba\u548c\u672c\u4f53\u8bba\u4e0d\u786e\u5b9a\u6027\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u751f\u547d\u5468\u671f\u7684\u56db\u673a\u5236\u7ba1\u7406\u6846\u67b6\uff1a\u8868\u793a\u3001\u8bc6\u522b\u3001\u6f14\u5316\u3001\u9002\u5e94\uff1b3) \u901a\u8fc7\u4e34\u5e8a\u5408\u4f5c\u7684\u771f\u5b9eLLM\u591a\u667a\u80fd\u4f53\u8d85\u58f0\u5fc3\u52a8\u56fe\u7cfb\u7edf\u9a8c\u8bc1\u6846\u67b6\u53ef\u884c\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e34\u5e8aLLM\u591a\u667a\u80fd\u4f53\u8d85\u58f0\u5fc3\u52a8\u56fe\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u8bca\u65ad\u63a8\u7406\u4e2d\u63d0\u9ad8\u7684\u53ef\u9760\u6027\u548c\u53ef\u8bca\u65ad\u6027\u3002\u6846\u67b6\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u5b89\u5168\u5173\u952eLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u751f\u547d\u5468\u671f\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u6846\u67b6\u4e3a\u5b89\u5168\u5173\u952eLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8d85\u8d8a\u6a21\u578b\u4e2d\u5fc3\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u8fd0\u884c\u65f6\u6cbb\u7406\u548c\u4fdd\u8bc1\u65b9\u6cd5\uff0c\u652f\u6301\u539f\u5219\u6027\u64cd\u4f5c\u63a7\u5236\u548c\u8fd0\u884c\u65f6\u4fdd\u8bc1\u3002", "topic": "agent analysis"}}
{"id": "2602.22413", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22413", "abs": "https://arxiv.org/abs/2602.22413", "authors": ["Jonas Karge"], "title": "Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents", "comment": null, "summary": "We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \\textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \\textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \\textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \\textit{hallucinations} in collective LLM decision-making.", "AI": {"tldr": "\u7814\u7a76\u5f02\u6784\u4ee3\u7406\u901a\u8fc7\u6821\u51c6\u81ea\u8eab\u53ef\u9760\u6027\u5e76\u9009\u62e9\u6027\u5f03\u6743\u6765\u63d0\u5347\u96c6\u4f53\u51b3\u7b56\u51c6\u786e\u6027\u7684\u6982\u7387\u6846\u67b6\uff0c\u5c06\u5b54\u591a\u585e\u966a\u5ba1\u56e2\u5b9a\u7406\u63a8\u5e7f\u5230\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u7684\u5e8f\u5217\u8bbe\u7f6e\u3002", "motivation": "\u7ecf\u5178\u5b54\u591a\u585e\u966a\u5ba1\u56e2\u5b9a\u7406\u5047\u8bbe\u56fa\u5b9a\u53c2\u4e0e\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u96c6\u4f53\u51b3\u7b56\u5e38\u53d7\u76ca\u4e8e\u5141\u8bb8\u4ee3\u7406\u8bf4\"\u6211\u4e0d\u77e5\u9053\"\u3002\u9700\u8981\u7814\u7a76\u4ee3\u7406\u5b66\u4e60\u81ea\u8eab\u53ef\u9760\u6027\u5e76\u9009\u62e9\u6027\u5f03\u6743\u5982\u4f55\u5f71\u54cd\u96c6\u4f53\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u6982\u7387\u6846\u67b6\uff1a\u4ee3\u7406\u7ecf\u5386\u6821\u51c6\u9636\u6bb5\u66f4\u65b0\u5bf9\u81ea\u8eab\u56fa\u5b9a\u80fd\u529b\u7684\u4fe1\u5ff5\uff0c\u7136\u540e\u9762\u5bf9\u6700\u7ec8\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u51b3\u5b9a\u6295\u7968\u6216\u5f03\u6743\u3002\u63a8\u5bfc\u7fa4\u4f53\u6210\u529f\u6982\u7387\u7684\u975e\u6e10\u8fd1\u4e0b\u754c\uff0c\u8bc1\u660e\u9009\u62e9\u6027\u53c2\u4e0e\u5c06CJT\u6e10\u8fd1\u4fdd\u8bc1\u63a8\u5e7f\u5230\u5e8f\u5217\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u8bbe\u7f6e\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u9009\u62e9\u6027\u53c2\u4e0e\u6846\u67b6\u80fd\u63a8\u5e7fCJT\u7684\u6e10\u8fd1\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u4e86\u63a8\u5bfc\u7684\u8fb9\u754c\u3002\u8be5\u6846\u67b6\u53ef\u5e94\u7528\u4e8eAI\u5b89\u5168\u9886\u57df\uff0c\u7f13\u89e3\u96c6\u4f53LLM\u51b3\u7b56\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u5141\u8bb8\u4ee3\u7406\u5b66\u4e60\u81ea\u8eab\u53ef\u9760\u6027\u5e76\u9009\u62e9\u6027\u5f03\u6743\u80fd\u63d0\u5347\u96c6\u4f53\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u5c06\u7ecf\u5178\u6295\u7968\u7406\u8bba\u63a8\u5e7f\u5230\u66f4\u73b0\u5b9e\u7684\u7f6e\u4fe1\u5ea6\u95e8\u63a7\u8bbe\u7f6e\uff0c\u4e3aAI\u5b89\u5168\u4e2d\u7684\u5e7b\u89c9\u7f13\u89e3\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.23047", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23047", "abs": "https://arxiv.org/abs/2602.23047", "authors": ["Haichuan Hu", "Ye Shang", "Guoqing Xie", "Congqing He", "Quanjun Zhang"], "title": "CL4SE: A Context Learning Benchmark For Software Engineering Tasks", "comment": "23 pages, 4 figures", "summary": "Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.", "AI": {"tldr": "CL4SE\u63d0\u51fa\u4e86\u9996\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7cfb\u7edf\u5316\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cdSE\u4e13\u7528\u4e0a\u4e0b\u6587\u7c7b\u578b\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e3b\u6d41LLM\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534724.7%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7279\u5b9a\u4e0a\u4e0b\u6587\u7c7b\u578b\u7684\u7cfb\u7edf\u5206\u7c7b\u548c\u4e13\u7528\u57fa\u51c6\uff0c\u65e0\u6cd5\u91cf\u5316\u4e0d\u540c\u4e0a\u4e0b\u6587\u5728\u6838\u5fc3SE\u5de5\u4f5c\u6d41\u4e2d\u7684\u5f02\u8d28\u6548\u5e94\u3002", "method": "\u63d0\u51faCL4SE\u57fa\u51c6\uff0c\u5305\u542b\u56db\u79cdSE\u5bfc\u5411\u7684\u4e0a\u4e0b\u6587\u7c7b\u578b\u5206\u7c7b\uff1a\u53ef\u89e3\u91ca\u793a\u4f8b\u3001\u9879\u76ee\u7279\u5b9a\u4e0a\u4e0b\u6587\u3001\u8fc7\u7a0b\u51b3\u7b56\u4e0a\u4e0b\u6587\u3001\u6b63\u8d1f\u4e0a\u4e0b\u6587\uff0c\u5bf9\u5e94\u56db\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\u3002\u6784\u5efa\u5305\u542b13,000+\u6837\u672c\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e94\u4e2a\u4e3b\u6d41LLM\u7684\u4e5d\u4e2a\u6307\u6807\u3002", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u534724.7%\u6027\u80fd\uff1a\u8fc7\u7a0b\u4e0a\u4e0b\u6587\u63d0\u5347\u4ee3\u7801\u5ba1\u67e533%\uff0c\u6df7\u5408\u6b63\u8d1f\u4e0a\u4e0b\u6587\u63d0\u5347\u8865\u4e01\u8bc4\u4f3030%\uff0c\u9879\u76ee\u7279\u5b9a\u4e0a\u4e0b\u6587\u63d0\u5347\u4ee3\u7801\u6458\u8981BLEU 14.78%\uff0c\u53ef\u89e3\u91ca\u793a\u4f8b\u63d0\u5347\u4ee3\u7801\u751f\u6210PASS@1 5.72%\u3002", "conclusion": "CL4SE\u5efa\u7acb\u4e86\u9996\u4e2aSE\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4efb\u52a1\u7279\u5b9a\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u7684\u53ef\u64cd\u4f5c\u7ecf\u9a8c\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4fc3\u8fdb\u8be5\u9886\u57df\u53ef\u91cd\u590d\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2602.22442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22442", "abs": "https://arxiv.org/abs/2602.22442", "authors": ["Gaoyuan Du", "Amit Ahlawat", "Xiaoyang Liu", "Jing Wu"], "title": "A Framework for Assessing AI Agent Decisions and Outcomes in AutoML Pipelines", "comment": "11 pages", "summary": "Agent-based AutoML systems rely on large language models to make complex, multi-stage decisions across data processing, model selection, and evaluation. However, existing evaluation practices remain outcome-centric, focusing primarily on final task performance. Through a review of prior work, we find that none of the surveyed agentic AutoML systems report structured, decision-level evaluation metrics intended for post-hoc assessment of intermediate decision quality. To address this limitation, we propose an Evaluation Agent (EA) that performs decision-centric assessment of AutoML agents without interfering with their execution. The EA is designed as an observer that evaluates intermediate decisions along four dimensions: decision validity, reasoning consistency, model quality risks beyond accuracy, and counterfactual decision impact. Across four proof-of-concept experiments, we demonstrate that the EA can (i) detect faulty decisions with an F1 score of 0.919, (ii) identify reasoning inconsistencies independent of final outcomes, and (iii) attribute downstream performance changes to agent decisions, revealing impacts ranging from -4.9\\% to +8.3\\% in final metrics. These results illustrate how decision-centric evaluation exposes failure modes that are invisible to outcome-only metrics. Our work reframes the evaluation of agentic AutoML systems from an outcome-based perspective to one that audits agent decisions, offering a foundation for reliable, interpretable, and governable autonomous ML systems.", "AI": {"tldr": "\u63d0\u51fa\u8bc4\u4f30\u4ee3\u7406(EA)\u7528\u4e8e\u51b3\u7b56\u4e2d\u5fc3\u5316\u8bc4\u4f30AutoML\u4ee3\u7406\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u6700\u7ec8\u6027\u80fd\uff0c\u80fd\u68c0\u6d4b\u6545\u969c\u51b3\u7b56\u3001\u8bc6\u522b\u63a8\u7406\u4e0d\u4e00\u81f4\u6027\u5e76\u91cf\u5316\u51b3\u7b56\u5f71\u54cd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ee3\u7406\u7684AutoML\u7cfb\u7edf\u8bc4\u4f30\u8fc7\u4e8e\u7ed3\u679c\u4e2d\u5fc3\u5316\uff0c\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u51b3\u7b56\u8d28\u91cf\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u65e0\u6cd5\u63ed\u793a\u51b3\u7b56\u5c42\u9762\u7684\u5931\u8d25\u6a21\u5f0f", "method": "\u8bbe\u8ba1\u8bc4\u4f30\u4ee3\u7406(EA)\u4f5c\u4e3a\u89c2\u5bdf\u8005\uff0c\u4ece\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u4e2d\u95f4\u51b3\u7b56\uff1a\u51b3\u7b56\u6709\u6548\u6027\u3001\u63a8\u7406\u4e00\u81f4\u6027\u3001\u6a21\u578b\u8d28\u91cf\u98ce\u9669\uff08\u8d85\u8d8a\u51c6\u786e\u7387\uff09\u3001\u53cd\u4e8b\u5b9e\u51b3\u7b56\u5f71\u54cd", "result": "EA\u80fd\u68c0\u6d4b\u6545\u969c\u51b3\u7b56(F1\u5206\u65700.919)\u3001\u8bc6\u522b\u4e0e\u6700\u7ec8\u7ed3\u679c\u65e0\u5173\u7684\u63a8\u7406\u4e0d\u4e00\u81f4\u6027\u3001\u91cf\u5316\u51b3\u7b56\u5bf9\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd(-4.9%\u5230+8.3%)", "conclusion": "\u51b3\u7b56\u4e2d\u5fc3\u5316\u8bc4\u4f30\u80fd\u63ed\u793a\u7ed3\u679c\u6307\u6807\u65e0\u6cd5\u53d1\u73b0\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u6cbb\u7406\u7684\u81ea\u4e3bML\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840", "topic": "agent analysis"}}
{"id": "2602.22452", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.22452", "abs": "https://arxiv.org/abs/2602.22452", "authors": ["Chayan Banerjee"], "title": "CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines", "comment": null, "summary": "A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6bd4\u4e16\u754c\u6a21\u578b(CWM)\uff0c\u4f7f\u7528InfoNCE\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3LLM\u4f5c\u4e3a\u52a8\u4f5c\u8bc4\u5206\u5668\uff0c\u901a\u8fc7\u6316\u6398\u786c\u8d1f\u6837\u672c\u6765\u533a\u5206\u7269\u7406\u53ef\u884c\u4e0e\u4e0d\u53ef\u884c\u52a8\u4f5c\uff0c\u5728ScienceWorld\u57fa\u51c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u52a8\u4f5c\u8bc4\u5206\u5668\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u6ca1\u6709\u660e\u786e\u6559\u5bfc\u6a21\u578b\u533a\u5206\u7269\u7406\u6b63\u786e\u548c\u7ec6\u5fae\u9519\u8bef\u7684\u52a8\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u597d\u8bc6\u522b\u7269\u7406\u53ef\u884c\u52a8\u4f5c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u4e16\u754c\u6a21\u578b(CWM)\uff0c\u4f7f\u7528InfoNCE\u5bf9\u6bd4\u76ee\u6807\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u6316\u6398\u786c\u8d1f\u6837\u672c\uff08\u8bed\u4e49\u76f8\u4f3c\u4f46\u7269\u7406\u4e0d\u517c\u5bb9\u7684\u5019\u9009\u52a8\u4f5c\uff09\u5728\u8bc4\u5206\u7a7a\u95f4\u4e2d\u63a8\u79bb\u6709\u6548\u548c\u65e0\u6548\u52a8\u4f5c\u3002", "result": "\u5728ScienceWorld\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a1) \u5728605\u4e2a\u786c\u8d1f\u6d4b\u8bd5\u5bf9\u4e0a\uff0cCWM\u5728\u6700\u5c0f\u7f16\u8f91\u8d1f\u6837\u672c\u4e0a\u7684Precision@1\u6bd4SFT\u9ad86.76\u4e2a\u767e\u5206\u70b9\uff0cAUC-ROC\u66f4\u9ad8(0.929 vs 0.906)\uff1b2) \u5728\u5206\u5e03\u5916\u538b\u529b\u6761\u4ef6\u4e0b\uff0cCWM\u4fdd\u6301\u66f4\u597d\u7684\u5b89\u5168\u8fb9\u9645(-2.39 vs -3.96)\u3002", "conclusion": "\u5bf9\u6bd4\u8bad\u7ec3\u6bd4\u5355\u72ec\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u66f4\u80fd\u8bf1\u5bfc\u51fa\u6355\u6349\u7269\u7406\u53ef\u884c\u6027\u7684\u8868\u793a\uff0cCWM\u5728\u8bc6\u522b\u7269\u7406\u53ef\u884c\u52a8\u4f5c\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "topic": "agent analysis"}}
{"id": "2602.22576", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22576", "abs": "https://arxiv.org/abs/2602.22576", "authors": ["Tianle Xia", "Ming Xu", "Lingxiang Hu", "Yiding Sun", "Wenwei Li", "Linfang Shang", "Liqun Liu", "Peng Shu", "Huan Yu", "Jie Jiang"], "title": "Search-P1: Path-Centric Reward Shaping for Stable and Efficient Agentic RAG Training", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, yet traditional single-round retrieval struggles with complex multi-step reasoning. Agentic RAG addresses this by enabling LLMs to dynamically decide when and what to retrieve, but current RL-based training methods suffer from sparse outcome rewards that discard intermediate signals and low sample efficiency where failed samples contribute nothing. We propose Search-P1, a framework that introduces path-centric reward shaping for agentic RAG training, comprising two key components: (1) Path-Centric Reward, which evaluates the structural quality of reasoning trajectories through order-agnostic step coverage and soft scoring that extracts learning signals even from failed samples, and (2) Dual-Track Path Scoring with offline-generated reference planners that assesses paths from both self-consistency and reference-alignment perspectives. Experiments on multiple QA benchmarks demonstrate that Search-P1 achieves significant improvements over Search-R1 and other strong baselines, with an average accuracy gain of 7.7 points.", "AI": {"tldr": "Search-P1\u6846\u67b6\u901a\u8fc7\u8def\u5f84\u4e2d\u5fc3\u5956\u52b1\u5851\u9020\u548c\u53cc\u8f68\u8def\u5f84\u8bc4\u5206\uff0c\u89e3\u51b3\u4e86Agentic RAG\u8bad\u7ec3\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u548c\u4f4e\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRAG\u7684\u5355\u8f6e\u68c0\u7d22\u96be\u4ee5\u5904\u7406\u590d\u6742\u591a\u6b65\u63a8\u7406\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8eRL\u7684Agentic RAG\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u7a00\u758f\u7ed3\u679c\u5956\u52b1\uff08\u4e22\u5f03\u4e2d\u95f4\u4fe1\u53f7\uff09\u548c\u4f4e\u6837\u672c\u6548\u7387\uff08\u5931\u8d25\u6837\u672c\u65e0\u8d21\u732e\uff09\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSearch-P1\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u8def\u5f84\u4e2d\u5fc3\u5956\u52b1\uff0c\u901a\u8fc7\u987a\u5e8f\u65e0\u5173\u7684\u6b65\u9aa4\u8986\u76d6\u548c\u8f6f\u8bc4\u5206\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\u7684\u7ed3\u6784\u8d28\u91cf\uff1b2) \u53cc\u8f68\u8def\u5f84\u8bc4\u5206\uff0c\u4f7f\u7528\u79bb\u7ebf\u751f\u6210\u7684\u53c2\u8003\u89c4\u5212\u5668\u4ece\u81ea\u4e00\u81f4\u6027\u548c\u53c2\u8003\u5bf9\u9f50\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u8def\u5f84\u3002", "result": "\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSearch-P1\u76f8\u6bd4Search-R1\u548c\u5176\u4ed6\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e867.7\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "Search-P1\u901a\u8fc7\u8def\u5f84\u4e2d\u5fc3\u5956\u52b1\u5851\u9020\u6709\u6548\u89e3\u51b3\u4e86Agentic RAG\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22480", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22480", "abs": "https://arxiv.org/abs/2602.22480", "authors": ["Varun Ursekar", "Apaar Shanker", "Veronica Chatrath", "Yuan", "Xue", "Sam Denton"], "title": "VeRO: An Evaluation Harness for Agents to Optimize Agents", "comment": null, "summary": "An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.", "AI": {"tldr": "VERO\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7f16\u7801\u667a\u80fd\u4f53\u4f18\u5316\u80fd\u529b\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u7248\u672c\u5316\u667a\u80fd\u4f53\u5feb\u7167\u3001\u9884\u7b97\u63a7\u5236\u8bc4\u4f30\u548c\u7ed3\u6784\u5316\u6267\u884c\u8ffd\u8e2a\uff0c\u5e76\u5305\u542b\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "motivation": "\u7f16\u7801\u667a\u80fd\u4f53\u7684\u4e00\u4e2a\u91cd\u8981\u5e94\u7528\u662f\u667a\u80fd\u4f53\u4f18\u5316\uff1a\u901a\u8fc7\u7f16\u8f91-\u6267\u884c-\u8bc4\u4f30\u5faa\u73af\u8fed\u4ee3\u6539\u8fdb\u76ee\u6807\u667a\u80fd\u4f53\u3002\u7136\u800c\u793e\u533a\u7f3a\u4e4f\u5bf9\u6b64\u4efb\u52a1\u7684\u7cfb\u7edf\u6027\u7406\u89e3\uff0c\u4e14\u667a\u80fd\u4f53\u4f18\u5316\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u6709\u672c\u8d28\u4e0d\u540c\uff0c\u9700\u8981\u540c\u65f6\u6355\u83b7\u786e\u5b9a\u6027\u4ee3\u7801\u548c\u968f\u673aLLM\u751f\u6210\u7684\u4e2d\u95f4\u63a8\u7406\u53ca\u6267\u884c\u7ed3\u679c\u3002", "method": "\u63d0\u51faVERO\u6846\u67b6\uff0c\u5305\u542b\uff1a(1) \u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u5de5\u5177\u94fe\uff0c\u5177\u6709\u7248\u672c\u5316\u667a\u80fd\u4f53\u5feb\u7167\u3001\u9884\u7b97\u63a7\u5236\u8bc4\u4f30\u548c\u7ed3\u6784\u5316\u6267\u884c\u8ffd\u8e2a\uff1b(2) \u5305\u542b\u76ee\u6807\u667a\u80fd\u4f53\u548c\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u914d\u6709\u53c2\u8003\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u4f7f\u7528VERO\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e0d\u540c\u4f18\u5316\u5668\u914d\u7f6e\u5728\u4efb\u52a1\u95f4\u7684\u8868\u73b0\uff0c\u5206\u6790\u54ea\u4e9b\u4fee\u6539\u80fd\u53ef\u9760\u63d0\u5347\u76ee\u6807\u667a\u80fd\u4f53\u6027\u80fd\u3002", "conclusion": "VERO\u6846\u67b6\u652f\u6301\u7f16\u7801\u667a\u80fd\u4f53\u4f18\u5316\u4f5c\u4e3a\u6838\u5fc3\u80fd\u529b\u7684\u7814\u7a76\uff0c\u5df2\u516c\u5f00\u53d1\u5e03\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2602.22271", "categories": ["cs.LG", "math.PR", "math.ST"], "pdf": "https://arxiv.org/pdf/2602.22271", "abs": "https://arxiv.org/abs/2602.22271", "authors": ["Deepak Agarwal", "Dhyey Dharmendrakumar Mavani", "Suyash Gupta", "Karthik Sethuraman", "Tejas Dharamsi"], "title": "Support Tokens, Stability Margins, and a New Foundation for Robust LLMs", "comment": "39 pages, 6 figures", "summary": "Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.\n  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u56e0\u679c\u81ea\u6ce8\u610f\u529bTransformer\u91cd\u65b0\u89e3\u91ca\u4e3a\u6982\u7387\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u81ea\u6ce8\u610f\u529b\u53c2\u6570\u5b58\u5728\u5c4f\u969c\u7ea6\u675f\uff0c\u5bfc\u81f4token\u7a7a\u95f4\u51fa\u73b0\u9ad8\u5ea6\u7ed3\u6784\u5316\u51e0\u4f55\uff0c\u5e76\u63d0\u51fa\u4e86\u5e26\u5e73\u6ed1\u5bf9\u6570\u5c4f\u969c\u60e9\u7f5a\u7684\u8d1d\u53f6\u65af\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u5c06\u81ea\u6ce8\u610f\u529bTransformer\u91cd\u65b0\u89e3\u91ca\u4e3a\u6982\u7387\u6846\u67b6\uff0c\u7c7b\u4f3c\u4e8e\u5c06\u7ecf\u5178PCA\u6269\u5c55\u5230\u6982\u7387PCA\uff0c\u4ee5\u63ed\u793a\u5176\u6df1\u5c42\u7ed3\u6784\u7279\u6027\u548c\u7406\u8bba\u6d1e\u5bdf\u3002", "method": "1) \u5728\u6982\u7387\u6846\u67b6\u4e0b\u91cd\u65b0\u89e3\u91ca\u56e0\u679c\u81ea\u6ce8\u610f\u529bTransformer\uff1b2) \u63ed\u793a\u81ea\u6ce8\u610f\u529b\u53c2\u6570\u7684\u5c4f\u969c\u7ea6\u675f\u548ctoken\u7a7a\u95f4\u7684\u7ed3\u6784\u5316\u51e0\u4f55\uff1b3) \u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\u548cMAP\u4f30\u8ba1\u76ee\u6807\uff0c\u5728\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u57fa\u7840\u4e0a\u6dfb\u52a0\u5e73\u6ed1\u5bf9\u6570\u5c4f\u969c\u60e9\u7f5a\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u6a21\u578b\uff0c\u4e14\u4e0d\u727a\u7272\u6837\u672c\u5916\u51c6\u786e\u6027\uff0c\u5728\u5b9e\u8df5\u4e2d\u6613\u4e8e\u5b9e\u73b0\u3002\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u8fb9\u754c\u5904\u53d8\u5f97\u75c5\u6001\uff0c\u7c7b\u4f3c\u4e8e\u652f\u6301\u5411\u91cf\u673a\u7684\u8fb9\u9645\u89e3\u91ca\uff0c\u5e76\u5f15\u5165\u4e86\u652f\u6301token\u7684\u6982\u5ff5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u89e3\u7801\u52a8\u6001\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u5bdf\uff0c\u5efa\u7acb\u4e86\u81ea\u6ce8\u610f\u529b\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6982\u5ff5\uff08\u5982\u652f\u6301\u5411\u91cf\u673a\uff09\u7684\u8054\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdbLLM\u8bad\u7ec3\u7684\u6709\u6548\u8d1d\u53f6\u65af\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.22519", "categories": ["cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.22519", "abs": "https://arxiv.org/abs/2602.22519", "authors": ["Wael Hafez", "Chenan Wei", "Rodrigo Felipe", "Amir Nazeri", "Cameron Reid"], "title": "A Mathematical Theory of Agency and Intelligence", "comment": "20 pages, 4 figuers", "summary": "To operate reliably under changing conditions, complex systems require feedback on how effectively they use resources, not just whether objectives are met. Current AI systems process vast information to produce sophisticated predictions, yet predictions can appear successful while the underlying interaction with the environment degrades. What is missing is a principled measure of how much of the total information a system deploys is actually shared between its observations, actions, and outcomes. We prove this shared fraction, which we term bipredictability, P, is intrinsic to any interaction, derivable from first principles, and strictly bounded: P can reach unity in quantum systems, P equal to, or smaller than 0.5 in classical systems, and lower once agency (action selection) is introduced. We confirm these bounds in a physical system (double pendulum), reinforcement learning agents, and multi turn LLM conversations. These results distinguish agency from intelligence: agency is the capacity to act on predictions, whereas intelligence additionally requires learning from interaction, self-monitoring of its learning effectiveness, and adapting the scope of observations, actions, and outcomes to restore effective learning. By this definition, current AI systems achieve agency but not intelligence. Inspired by thalamocortical regulation in biological systems, we demonstrate a feedback architecture that monitors P in real time, establishing a prerequisite for adaptive, resilient AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u53cc\u9884\u6d4b\u6027(P)\"\u4f5c\u4e3a\u8861\u91cf\u7cfb\u7edf\u89c2\u5bdf\u3001\u884c\u52a8\u548c\u7ed3\u679c\u4e4b\u95f4\u4fe1\u606f\u5171\u4eab\u7a0b\u5ea6\u7684\u6307\u6807\uff0c\u8bc1\u660e\u5176\u5728\u91cf\u5b50\u7cfb\u7edf\u4e2d\u53ef\u8fbe1\uff0c\u7ecf\u5178\u7cfb\u7edf\u22640.5\uff0c\u5f15\u5165\u4ee3\u7406\u540e\u66f4\u4f4e\u3002\u901a\u8fc7\u7269\u7406\u7cfb\u7edf\u3001\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u548cLLM\u5bf9\u8bdd\u9a8c\u8bc1\u8fb9\u754c\uff0c\u533a\u5206\u4ee3\u7406\u4e0e\u667a\u80fd\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e18\u8111\u76ae\u8d28\u8c03\u8282\u7684\u53cd\u9988\u67b6\u6784\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5904\u7406\u5927\u91cf\u4fe1\u606f\u505a\u51fa\u590d\u6742\u9884\u6d4b\uff0c\u4f46\u9884\u6d4b\u6210\u529f\u65f6\u5e95\u5c42\u73af\u5883\u4ea4\u4e92\u53ef\u80fd\u5df2\u9000\u5316\u3002\u9700\u8981\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\u6765\u8861\u91cf\u7cfb\u7edf\u90e8\u7f72\u7684\u603b\u4fe1\u606f\u4e2d\uff0c\u89c2\u5bdf\u3001\u884c\u52a8\u548c\u7ed3\u679c\u4e4b\u95f4\u5b9e\u9645\u5171\u4eab\u7684\u6bd4\u4f8b\uff0c\u4ee5\u8bc4\u4f30\u7cfb\u7edf\u5728\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u51fa\u53cc\u9884\u6d4b\u6027(P)\u4f5c\u4e3a\u5185\u5728\u4ea4\u4e92\u5ea6\u91cf\uff0c\u8bc1\u660e\u5176\u7406\u8bba\u8fb9\u754c\u3002\u5728\u7269\u7406\u7cfb\u7edf(\u53cc\u6446)\u3001\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u548c\u591a\u8f6eLLM\u5bf9\u8bdd\u4e2d\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u8fb9\u754c\u3002\u63d0\u51fa\u53d7\u751f\u7269\u4e18\u8111\u76ae\u8d28\u8c03\u8282\u542f\u53d1\u7684\u5b9e\u65f6\u76d1\u63a7P\u7684\u53cd\u9988\u67b6\u6784\u3002", "result": "P\u5728\u91cf\u5b50\u7cfb\u7edf\u4e2d\u53ef\u8fbe1\uff0c\u7ecf\u5178\u7cfb\u7edf\u22640.5\uff0c\u5f15\u5165\u4ee3\u7406\u9009\u62e9\u540e\u66f4\u4f4e\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u8fb9\u754c\u3002\u533a\u5206\u4ee3\u7406(\u57fa\u4e8e\u9884\u6d4b\u884c\u52a8\u7684\u80fd\u529b)\u4e0e\u667a\u80fd(\u8fd8\u9700\u4ece\u4ea4\u4e92\u5b66\u4e60\u3001\u81ea\u6211\u76d1\u63a7\u5b66\u4e60\u6548\u679c\u3001\u9002\u5e94\u89c2\u5bdf/\u884c\u52a8/\u7ed3\u679c\u8303\u56f4\u4ee5\u6062\u590d\u6709\u6548\u5b66\u4e60)\u3002\u5f53\u524dAI\u7cfb\u7edf\u4ec5\u5b9e\u73b0\u4ee3\u7406\u800c\u975e\u667a\u80fd\u3002", "conclusion": "\u53cc\u9884\u6d4b\u6027(P)\u662f\u8bc4\u4f30\u7cfb\u7edf\u4ea4\u4e92\u6548\u7387\u7684\u5173\u952e\u6307\u6807\u3002\u901a\u8fc7\u76d1\u63a7P\u53ef\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u6709\u5f39\u6027\u7684AI\u7cfb\u7edf\u3002\u5f53\u524dAI\u7cfb\u7edf\u7f3a\u4e4f\u771f\u6b63\u7684\u667a\u80fd\uff0c\u4ec5\u5177\u5907\u4ee3\u7406\u80fd\u529b\u3002\u53d7\u751f\u7269\u7cfb\u7edf\u542f\u53d1\u7684\u53cd\u9988\u67b6\u6784\u4e3a\u6784\u5efa\u66f4\u667a\u80fd\u7684AI\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.22284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22284", "abs": "https://arxiv.org/abs/2602.22284", "authors": ["Mingi Kim", "Yongjun Kim", "Jungwoo Kang", "Hyungki Kim"], "title": "BrepCoder: A Unified Multimodal Large Language Model for Multi-task B-rep Reasoning", "comment": null, "summary": "Recent advancements in deep learning have actively addressed complex challenges within the Computer-Aided Design (CAD) domain.However, most existing approaches rely on task-specifi c models requiring structural modifi cations for new tasks, and they predominantly focus on point clouds or images rather than the industry-standard Boundary Representation (B-rep) format. To address these limitations, we propose BrepCoder, a unifi ed Multimodal Large Language Model (MLLM) that performs diverse CAD tasks from B-rep inputs. By leveraging the code generation capabilities of Large Language Models (LLMs), we convert CAD modeling sequences into Python-like code and align them with B-rep. We then adopt a two-stage training strategy: First, pre-training on reverse engineering to learn geometric features and design logic. Second, eff ectively extending the model to various downstream tasks such as completion, error correction, and CAD-QA. Consequently, by interpreting B-rep as structural code, BrepCoder achieves superior generalization across diverse tasks, demonstrating its potential as a general-purpose CAD agent.", "AI": {"tldr": "BrepCoder\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c06B-rep CAD\u6570\u636e\u8f6c\u6362\u4e3a\u7c7b\u4f3cPython\u7684\u4ee3\u7801\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u591a\u79cdCAD\u4efb\u52a1\uff0c\u5305\u62ec\u8865\u5168\u3001\u7ea0\u9519\u548cCAD\u95ee\u7b54\u3002", "motivation": "\u73b0\u6709CAD\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u9700\u8981\u4e3a\u65b0\u4efb\u52a1\u4fee\u6539\u7ed3\u6784\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u70b9\u4e91\u6216\u56fe\u50cf\u800c\u975e\u884c\u4e1a\u6807\u51c6\u7684B-rep\u683c\u5f0f\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u5229\u7528LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5c06CAD\u5efa\u6a21\u5e8f\u5217\u8f6c\u6362\u4e3a\u7c7b\u4f3cPython\u7684\u4ee3\u7801\u5e76\u4e0eB-rep\u5bf9\u9f50\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u9884\u8bad\u7ec3\u5b66\u4e60\u51e0\u4f55\u7279\u5f81\u548c\u8bbe\u8ba1\u903b\u8f91\uff1b2\uff09\u6269\u5c55\u5230\u4e0b\u6e38\u4efb\u52a1\u5982\u8865\u5168\u3001\u7ea0\u9519\u548cCAD-QA\u3002", "result": "\u901a\u8fc7\u5c06B-rep\u89e3\u91ca\u4e3a\u7ed3\u6784\u5316\u4ee3\u7801\uff0cBrepCoder\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u901a\u7528CAD\u4ee3\u7406\u7684\u6f5c\u529b\u3002", "conclusion": "BrepCoder\u901a\u8fc7\u7edf\u4e00\u7684MLLM\u6846\u67b6\u89e3\u51b3\u4e86CAD\u9886\u57df\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c06B-rep\u89c6\u4e3a\u7ed3\u6784\u5316\u4ee3\u7801\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2602.22539", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22539", "abs": "https://arxiv.org/abs/2602.22539", "authors": ["Mohammad Hossein Shokouhi", "Vincent W. S. Wong"], "title": "Agentic AI for Intent-driven Optimization in Cell-free O-RAN", "comment": "Accepted by IEEE International Conference on Communications (ICC), Glasgow, UK, May 2026", "summary": "Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u65e0\u8702\u7a9dO-RAN\u4e2d\u7684\u610f\u56fe\u7ffb\u8bd1\u4e0e\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u8282\u80fd\u548c\u8d44\u6e90\u7ba1\u7406\uff0c\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u73b0\u6709O-RAN\u4e2d\u5927\u591a\u6570\u5de5\u4f5c\u8003\u8651\u7b80\u5355\u610f\u56fe\u5e76\u7531\u72ec\u7acb\u667a\u80fd\u4f53\u5904\u7406\uff0c\u800c\u9700\u8981\u667a\u80fd\u4f53\u95f4\u534f\u8c03\u7684\u590d\u6742\u610f\u56fe\u5c1a\u672a\u63a2\u7d22\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u590d\u6742\u610f\u56fe\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u76d1\u7763\u667a\u80fd\u4f53\u7ffb\u8bd1\u8fd0\u8425\u5546\u610f\u56fe\u4e3a\u4f18\u5316\u76ee\u6807\u548c\u6700\u5c0f\u901f\u7387\u8981\u6c42\uff1b\u7528\u6237\u6743\u91cd\u667a\u80fd\u4f53\u4ece\u8bb0\u5fc6\u6a21\u5757\u68c0\u7d22\u7ecf\u9a8c\u786e\u5b9a\u7528\u6237\u4f18\u5148\u7ea7\u6743\u91cd\uff1bO-RU\u7ba1\u7406\u667a\u80fd\u4f53\u4f7f\u7528DRL\u7b97\u6cd5\u786e\u5b9a\u6d3b\u8dc3O-RU\u96c6\u5408\uff1b\u76d1\u63a7\u667a\u80fd\u4f53\u76d1\u6d4b\u7528\u6237\u6570\u636e\u901f\u7387\u5e76\u534f\u8c03\u5176\u4ed6\u667a\u80fd\u4f53\u3002\u91c7\u7528PEFT\u65b9\u6cd5\u5b9e\u73b0\u540c\u4e00LLM\u7528\u4e8e\u4e0d\u540c\u667a\u80fd\u4f53\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u8282\u80fd\u6a21\u5f0f\u4e0b\uff0c\u76f8\u6bd4\u4e09\u79cd\u57fa\u7ebf\u65b9\u6848\uff0c\u6240\u63d0\u6846\u67b6\u5c06\u6d3b\u8dc3O-RU\u6570\u91cf\u51cf\u5c1141.93%\u3002\u4f7f\u7528PEFT\u65b9\u6cd5\uff0c\u76f8\u6bd4\u90e8\u7f72\u5355\u72ec\u7684LLM\u667a\u80fd\u4f53\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1192%\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u4f53AI\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406O-RAN\u4e2d\u7684\u590d\u6742\u610f\u56fe\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u8282\u80fd\u76ee\u6807\uff0c\u540c\u65f6PEFT\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.22546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22546", "abs": "https://arxiv.org/abs/2602.22546", "authors": ["Zhiming Wang", "Jinwei He", "Feng Lu"], "title": "Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention", "comment": null, "summary": "Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.", "AI": {"tldr": "AHCE\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u5982\u4f55\u8bf7\u6c42\u4e13\u5bb6\u63a8\u7406\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u5728Minecraft\u5b9e\u9a8c\u4e2d\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534732-70%\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u9700\u8981\u957f\u5c3e\u77e5\u8bc6\u7684\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4eba\u7c7b\u4e13\u5bb6\u7684\u6307\u5bfc\u5f80\u5f80\u975e\u7ed3\u6784\u5316\u4e14\u4e0d\u53ef\u9760\uff0c\u76f4\u63a5\u6574\u5408\u5230\u667a\u80fd\u4f53\u8ba1\u5212\u4e2d\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faAHCE\u6846\u67b6\uff0c\u6838\u5fc3\u662fHuman Feedback Module\uff08HFM\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u7b56\u7565\u5c06\u4eba\u7c7b\u4e13\u5bb6\u89c6\u4e3a\u4ea4\u4e92\u5f0f\u63a8\u7406\u5de5\u5177\uff0c\u5b9e\u73b0\u6309\u9700\u7684\u4eba\u673a\u534f\u4f5c\u3002", "result": "\u5728Minecraft\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u5c06\u666e\u901a\u96be\u5ea6\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad832%\uff0c\u9ad8\u96be\u5ea6\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u8fd170%\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "\u6210\u529f\u589e\u5f3a\u667a\u80fd\u4f53\u9700\u8981\u5b66\u4e60\u5982\u4f55\u8bf7\u6c42\u4e13\u5bb6\u63a8\u7406\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7b80\u5355\u6c42\u52a9\uff0cAHCE\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.22752", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22752", "abs": "https://arxiv.org/abs/2602.22752", "authors": ["Nils Schwager", "Simon M\u00fcnker", "Alistair Plum", "Achim Rettinger"], "title": "Towards Simulating Social Media Users with LLMs: Evaluating the Operational Validity of Conditioned Comment Prediction", "comment": "14 pages, 1 figure, 7 tables. Accepted to the 15th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA) at EACL 2026, Rabat, Morocco", "summary": "The transition of Large Language Models (LLMs) from exploratory tools to active \"silicon subjects\" in social science lacks extensive validation of operational validity. This study introduces Conditioned Comment Prediction (CCP), a task in which a model predicts how a user would comment on a given stimulus by comparing generated outputs with authentic digital traces. This framework enables a rigorous evaluation of current LLM capabilities with respect to the simulation of social media user behavior. We evaluated open-weight 8B models (Llama3.1, Qwen3, Ministral) in English, German, and Luxembourgish language scenarios. By systematically comparing prompting strategies (explicit vs. implicit) and the impact of Supervised Fine-Tuning (SFT), we identify a critical form vs. content decoupling in low-resource settings: while SFT aligns the surface structure of the text output (length and syntax), it degrades semantic grounding. Furthermore, we demonstrate that explicit conditioning (generated biographies) becomes redundant under fine-tuning, as models successfully perform latent inference directly from behavioral histories. Our findings challenge current \"naive prompting\" paradigms and offer operational guidelines prioritizing authentic behavioral traces over descriptive personas for high-fidelity simulation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u6761\u4ef6\u8bc4\u8bba\u9884\u6d4b\u4efb\u52a1\uff0c\u8bc4\u4f30LLMs\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u76d1\u7763\u5fae\u8c03\u4f1a\u5bfc\u81f4\u5f62\u5f0f\u4e0e\u5185\u5bb9\u89e3\u8026\uff0c\u5e76\u6311\u6218\u4e86\u5f53\u524d\"\u6734\u7d20\u63d0\u793a\"\u8303\u5f0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u63a2\u7d22\u5de5\u5177\u8f6c\u53d8\u4e3a\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\"\u7845\u57fa\u4e3b\u4f53\"\u7f3a\u4e4f\u64cd\u4f5c\u6709\u6548\u6027\u7684\u5e7f\u6cdb\u9a8c\u8bc1\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u6761\u4ef6\u8bc4\u8bba\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u8f93\u51fa\u4e0e\u771f\u5b9e\u6570\u5b57\u75d5\u8ff9\u6765\u8bc4\u4f30LLMs\u3002\u8bc4\u4f30\u4e868B\u53c2\u6570\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u663e\u5f0f\u4e0e\u9690\u5f0f\u63d0\u793a\u7b56\u7565\u4ee5\u53ca\u76d1\u7763\u5fae\u8c03\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e2d\u53d1\u73b0\u4e86\u5f62\u5f0f\u4e0e\u5185\u5bb9\u89e3\u8026\uff1a\u76d1\u7763\u5fae\u8c03\u5bf9\u9f50\u4e86\u6587\u672c\u8f93\u51fa\u7684\u8868\u9762\u7ed3\u6784\uff08\u957f\u5ea6\u548c\u53e5\u6cd5\uff09\uff0c\u4f46\u964d\u4f4e\u4e86\u8bed\u4e49\u57fa\u7840\u3002\u663e\u5f0f\u6761\u4ef6\u5728\u5fae\u8c03\u4e0b\u53d8\u5f97\u5197\u4f59\uff0c\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u4ece\u884c\u4e3a\u5386\u53f2\u8fdb\u884c\u6f5c\u5728\u63a8\u65ad\u3002", "conclusion": "\u6311\u6218\u4e86\u5f53\u524d\"\u6734\u7d20\u63d0\u793a\"\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e86\u64cd\u4f5c\u6307\u5357\uff0c\u5f3a\u8c03\u4f18\u5148\u4f7f\u7528\u771f\u5b9e\u884c\u4e3a\u75d5\u8ff9\u800c\u975e\u63cf\u8ff0\u6027\u4eba\u7269\u89d2\u8272\u6765\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6a21\u62df\u3002", "topic": "agent analysis"}}
{"id": "2602.22755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22755", "abs": "https://arxiv.org/abs/2602.22755", "authors": ["Abhay Sheshadri", "Aidan Ewart", "Kai Fronsdal", "Isha Gupta", "Samuel R. Bowman", "Sara Price", "Samuel Marks", "Rowan Wang"], "title": "AuditBench: Evaluating Alignment Auditing Techniques on Models with Hidden Behaviors", "comment": null, "summary": "We introduce AuditBench, an alignment auditing benchmark. AuditBench consists of 56 language models with implanted hidden behaviors. Each model has one of 14 concerning behaviors--such as sycophantic deference, opposition to AI regulation, or secret geopolitical loyalties--which it does not confess to when directly asked. AuditBench models are highly diverse--some are subtle, while others are overt, and we use varying training techniques both for implanting behaviors and training models not to confess. To demonstrate AuditBench's utility, we develop an investigator agent that autonomously employs a configurable set of auditing tools. By measuring investigator agent success using different tools, we can evaluate their efficacy. Notably, we observe a tool-to-agent gap, where tools that perform well in standalone non-agentic evaluations fail to translate into improved performance when used with our investigator agent. We find that our most effective tools involve scaffolded calls to auxiliary models that generate diverse prompts for the target. White-box interpretability tools can be helpful, but the agent performs best with black-box tools. We also find that audit success varies greatly across training techniques: models trained on synthetic documents are easier to audit than models trained on demonstrations, with better adversarial training further increasing auditing difficulty. We release our models, agent, and evaluation framework to support future quantitative, iterative science on alignment auditing.", "AI": {"tldr": "AuditBench\u662f\u4e00\u4e2a\u5bf9\u9f50\u5ba1\u8ba1\u57fa\u51c6\uff0c\u5305\u542b56\u4e2a\u690d\u5165\u9690\u85cf\u884c\u4e3a\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5ba1\u8ba1\u5de5\u5177\u5728\u68c0\u6d4b\u6a21\u578b\u672a\u516c\u5f00\u7684\u6f5c\u5728\u6709\u5bb3\u884c\u4e3a\u65b9\u9762\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u5bf9\u9f50\u5ba1\u8ba1\u5de5\u5177\u6548\u679c\u7684\u6807\u51c6\u57fa\u51c6\uff0c\u9700\u8981\u91cf\u5316\u8bc4\u4f30\u5de5\u5177\u5728\u68c0\u6d4b\u8bed\u8a00\u6a21\u578b\u4e2d\u9690\u85cf\u7684\u6f5c\u5728\u6709\u5bb3\u884c\u4e3a\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa\u5305\u542b56\u4e2a\u690d\u516514\u79cd\u4e0d\u540c\u9690\u85cf\u884c\u4e3a\uff08\u5982\u8c04\u5a9a\u987a\u4ece\u3001\u53cd\u5bf9AI\u76d1\u7ba1\u3001\u79d8\u5bc6\u5730\u7f18\u653f\u6cbb\u5fe0\u8bda\u7b49\uff09\u7684\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\uff0c\u5f00\u53d1\u53ef\u914d\u7f6e\u5ba1\u8ba1\u5de5\u5177\u7684\u8c03\u67e5\u4ee3\u7406\uff0c\u8bc4\u4f30\u4e0d\u540c\u5de5\u5177\u5728\u68c0\u6d4b\u8fd9\u4e9b\u9690\u85cf\u884c\u4e3a\u65b9\u9762\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u5de5\u5177\u5230\u4ee3\u7406\u7684\u5dee\u8ddd\uff0c\u5373\u72ec\u7acb\u8bc4\u4f30\u8868\u73b0\u826f\u597d\u7684\u5de5\u5177\u5728\u4ee3\u7406\u73af\u5883\u4e2d\u6548\u679c\u4e0d\u4f73\uff1b\u6700\u6709\u6548\u7684\u5de5\u5177\u6d89\u53ca\u8f85\u52a9\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u63d0\u793a\uff1b\u767d\u76d2\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u6709\u5e2e\u52a9\uff0c\u4f46\u9ed1\u76d2\u5de5\u5177\u8868\u73b0\u6700\u4f73\uff1b\u4e0d\u540c\u8bad\u7ec3\u6280\u672f\u5bf9\u5ba1\u8ba1\u96be\u5ea6\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "AuditBench\u4e3a\u5bf9\u9f50\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5de5\u5177\u5728\u4ee3\u7406\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u5dee\u5f02\uff0c\u5e76\u8868\u660e\u5ba1\u8ba1\u6210\u529f\u4e0e\u5426\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u8bad\u7ec3\u6280\u672f\u3002", "topic": "agent analysis"}}
{"id": "2602.22765", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22765", "abs": "https://arxiv.org/abs/2602.22765", "authors": ["Zhe Yang", "Yudong Wang", "Rang Li", "Zhifang Sui"], "title": "Towards Better RL Training Data Utilization via Second-Order Rollout", "comment": null, "summary": "Reinforcement Learning (RL) has empowered Large Language Models (LLMs) with strong reasoning capabilities, but vanilla RL mainly focuses on generation capability improvement by training with only first-order rollout (generating multiple responses for a question), and we argue that this approach fails to fully exploit the potential of training data because of the neglect of critique capability training. To tackle this problem, we further introduce the concept of second-order rollout (generating multiple critiques for a response) and propose a unified framework for jointly training generation and critique capabilities. Extensive experiments across various models and datasets demonstrate that our approach can utilize training data more effectively than vanilla RL and achieve better performance under the same training data. Additionally, we uncover several insightful findings regarding second-order rollout and critique training, such as the importance of label balance in critique training and the noise problem of outcome-based rewards, which can be mitigated through sampling techniques. Our work offers a preliminary exploration of dynamic data augmentation and joint generation-critique training in RL, providing meaningful inspiration for the further advancement of RL training", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e00\u9636\uff08\u751f\u6210\u591a\u4e2a\u56de\u7b54\uff09\u548c\u4e8c\u9636\uff08\u751f\u6210\u591a\u4e2a\u8bc4\u8bba\uff09rollout\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3LLMs\u7684\u751f\u6210\u548c\u8bc4\u8bba\u80fd\u529b\uff0c\u66f4\u6709\u6548\u5730\u5229\u7528\u8bad\u7ec3\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRL\u8bad\u7ec3\u4e3b\u8981\u5173\u6ce8\u4e00\u9636rollout\uff08\u4e3a\u95ee\u9898\u751f\u6210\u591a\u4e2a\u56de\u7b54\uff09\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bc4\u8bba\u80fd\u529b\u7684\u8bad\u7ec3\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6570\u636e\u7684\u6f5c\u529b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8bad\u7ec3\u751f\u6210\u548c\u8bc4\u8bba\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e00\u9636rollout\uff08\u751f\u6210\u591a\u4e2a\u56de\u7b54\uff09\u548c\u4e8c\u9636rollout\uff08\u4e3a\u56de\u7b54\u751f\u6210\u591a\u4e2a\u8bc4\u8bba\uff09\u7684\u7edf\u4e00RL\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3\u751f\u6210\u548c\u8bc4\u8bba\u80fd\u529b\u3002\u901a\u8fc7\u91c7\u6837\u6280\u672f\u7f13\u89e3\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u566a\u58f0\u95ee\u9898\u3002", "result": "\u5728\u5404\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfRL\u66f4\u6709\u6548\u5730\u5229\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u76f8\u540c\u6570\u636e\u91cf\u4e0b\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002\u53d1\u73b0\u8bc4\u8bba\u8bad\u7ec3\u4e2d\u6807\u7b7e\u5e73\u8861\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u91c7\u6837\u6280\u672f\u53ef\u4ee5\u7f13\u89e3\u5956\u52b1\u566a\u58f0\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aRL\u4e2d\u7684\u52a8\u6001\u6570\u636e\u589e\u5f3a\u548c\u8054\u5408\u751f\u6210-\u8bc4\u8bba\u8bad\u7ec3\u63d0\u4f9b\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u4e3aRL\u8bad\u7ec3\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u542f\u793a\u3002\u8bc1\u660e\u4e86\u4e8c\u9636rollout\u548c\u8bc4\u8bba\u8bad\u7ec3\u5728\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22297", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22297", "abs": "https://arxiv.org/abs/2602.22297", "authors": ["Dhiraj Neupane", "Richard Dazeley", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection", "comment": "This article is accepted to be published in AAMAS2026. The doi is listed below but the production is on the way as of now (26/02/2026)", "summary": "Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u68b0\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5065\u5eb7\u64cd\u4f5c\u5e8f\u5217\u4e2d\u5b66\u4e60\u5956\u52b1\u52a8\u6001\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u6216\u6545\u969c\u6807\u7b7e\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u9c81\u68d2\u7684\u6545\u969c\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u68b0\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528RL\u7684\u5e8f\u5217\u51b3\u7b56\u4f18\u52bf\uff0c\u901a\u5e38\u5c06\u6545\u969c\u68c0\u6d4b\u7b80\u5316\u4e3a\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u95ee\u9898\u3002\u9700\u8981\u5c06RL\u7684\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u68b0\u6545\u969c\u68c0\u6d4b\u7684\u65f6\u95f4\u7ed3\u6784\u5bf9\u9f50\u3002", "method": "\u5c06\u673a\u68b0\u6545\u969c\u68c0\u6d4b\u5efa\u6a21\u4e3a\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u5bf9\u6297\u9006\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2a\u5224\u522b\u5668\u6765\u533a\u5206\u6b63\u5e38\uff08\u4e13\u5bb6\uff09\u548c\u7b56\u7565\u751f\u6210\u7684\u8f6c\u79fb\uff0c\u5224\u522b\u5668\u5b66\u4e60\u7684\u5956\u52b1\u4f5c\u4e3a\u5f02\u5e38\u5206\u6570\u6765\u68c0\u6d4b\u504f\u79bb\u6b63\u5e38\u64cd\u4f5c\u7684\u884c\u4e3a\u3002", "result": "\u5728\u4e09\u4e2a\u8fd0\u884c\u81f3\u6545\u969c\u57fa\u51c6\u6570\u636e\u96c6\uff08HUMS2023\u3001IMS\u548cXJTU-SY\uff09\u4e0a\u8bc4\u4f30\uff0c\u6a21\u578b\u59cb\u7ec8\u4e3a\u6b63\u5e38\u6837\u672c\u5206\u914d\u4f4e\u5f02\u5e38\u5206\u6570\uff0c\u4e3a\u6545\u969c\u6837\u672c\u5206\u914d\u9ad8\u5f02\u5e38\u5206\u6570\uff0c\u5b9e\u73b0\u4e86\u65e9\u671f\u548c\u9c81\u68d2\u7684\u6545\u969c\u68c0\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u5c06RL\u7684\u5e8f\u5217\u63a8\u7406\u4e0e\u673a\u68b0\u6545\u969c\u68c0\u6d4b\u7684\u65f6\u95f4\u7ed3\u6784\u5bf9\u9f50\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u57fa\u4e8eRL\u7684\u8bca\u65ad\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22680", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22680", "abs": "https://arxiv.org/abs/2602.22680", "authors": ["Yue Xu", "Qian Chen", "Zizhan Ma", "Dongrui Liu", "Wenxuan Wang", "Xiting Wang", "Li Xiong", "Wenjie Wang"], "title": "Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions", "comment": null, "summary": "Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u4e2a\u6027\u5316LLM\u667a\u80fd\u4f53\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u56f4\u7ed5\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u7528\u6237\u753b\u50cf\u5efa\u6a21\u3001\u8bb0\u5fc6\u3001\u89c4\u5212\u3001\u884c\u52a8\u6267\u884c\uff09\u6784\u5efa\u4e86\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5206\u6790\u4e86\u7528\u6237\u4fe1\u53f7\u7684\u8868\u793a\u3001\u4f20\u64ad\u548c\u5229\u7528\u65b9\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u65b9\u6cd5\u3001\u5e94\u7528\u573a\u666f\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u9700\u8981\u9002\u5e94\u4e0d\u540c\u7528\u6237\u5e76\u4fdd\u6301\u8fde\u7eed\u6027\uff0c\u4e2a\u6027\u5316\u6210\u4e3a\u5173\u952e\u9700\u6c42\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e2a\u6027\u5316LLM\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u6027\u68b3\u7406\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u5176\u8bbe\u8ba1\u539f\u7406\u548c\u7ec4\u4ef6\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u80fd\u529b\u5bfc\u5411\u7684\u7efc\u8ff0\u65b9\u6cd5\uff0c\u56f4\u7ed5\u56db\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u6838\u5fc3\u7ec4\u4ef6\u7ec4\u7ec7\u6587\u732e\uff1a1) \u7528\u6237\u753b\u50cf\u5efa\u6a21\uff0c2) \u8bb0\u5fc6\u7cfb\u7edf\uff0c3) \u89c4\u5212\u673a\u5236\uff0c4) \u884c\u52a8\u6267\u884c\u3002\u5206\u6790\u5404\u7ec4\u4ef6\u4e2d\u7528\u6237\u4fe1\u53f7\u7684\u8868\u793a\u3001\u4f20\u64ad\u548c\u5229\u7528\u65b9\u5f0f\uff0c\u5f3a\u8c03\u8de8\u7ec4\u4ef6\u4ea4\u4e92\u548c\u8bbe\u8ba1\u6743\u8861\u3002", "result": "\u63d0\u51fa\u4e86\u4e2a\u6027\u5316LLM\u667a\u80fd\u4f53\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u8de8\u7ec4\u4ef6\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5206\u6790\u4e86\u8bbe\u8ba1\u6743\u8861\u3002\u540c\u65f6\u68b3\u7406\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u4ece\u901a\u7528\u52a9\u624b\u5230\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u4e2a\u6027\u5316LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u4ece\u539f\u578b\u4e2a\u6027\u5316\u5230\u53ef\u6269\u5c55\u73b0\u5b9e\u4e16\u754c\u52a9\u624b\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u7528\u6237\u5bf9\u9f50\u3001\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u90e8\u7f72\u6027\u7b49\u5173\u952e\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.22790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22790", "abs": "https://arxiv.org/abs/2602.22790", "authors": ["Hyunwoo Kim", "Hanau Yi", "Jaehee Bae", "Yumin Kim"], "title": "Natural Language Declarative Prompting (NLD-P): A Modular Governance Method for Prompt Design Under Model Drift", "comment": null, "summary": "The rapid evolution of large language models (LLMs) has transformed prompt engineering from a localized craft into a systems-level governance challenge. As models scale and update across generations, prompt behavior becomes sensitive to shifts in instruction-following policies, alignment regimes, and decoding strategies, a phenomenon we characterize as GPT-scale model drift. Under such conditions, surface-level formatting conventions and ad hoc refinement are insufficient to ensure stable, interpretable control. This paper reconceptualizes Natural Language Declarative Prompting (NLD-P) as a declarative governance method rather than a rigid field template. NLD-P is formalized as a modular control abstraction that separates provenance, constraint logic, task content, and post-generation evaluation, encoded directly in natural language without reliance on external orchestration code. We define minimal compliance criteria, analyze model-dependent schema receptivity, and position NLD-P as an accessible governance framework for non-developer practitioners operating within evolving LLM ecosystems. Portions of drafting and editorial refinement employed a schema-bound LLM assistant configured under NLD-P. All conceptual framing, methodological claims, and final revisions were directed, reviewed, and approved by the human author under a documented human-in-the-loop protocol. The paper concludes by outlining implications for declarative control under ongoing model evolution and identifying directions for future empirical validation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNLD-P\uff08\u81ea\u7136\u8bed\u8a00\u58f0\u660e\u5f0f\u63d0\u793a\uff09\u4f5c\u4e3a\u58f0\u660e\u5f0f\u6cbb\u7406\u65b9\u6cd5\uff0c\u800c\u975e\u521a\u6027\u6a21\u677f\uff0c\u7528\u4e8e\u5e94\u5bf9GPT\u89c4\u6a21\u6a21\u578b\u6f02\u79fb\u5e26\u6765\u7684\u63d0\u793a\u5de5\u7a0b\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u6f14\u8fdb\uff0c\u63d0\u793a\u5de5\u7a0b\u4ece\u5c40\u90e8\u6280\u5de7\u8f6c\u53d8\u4e3a\u7cfb\u7edf\u7ea7\u6cbb\u7406\u6311\u6218\u3002\u6a21\u578b\u89c4\u6a21\u6269\u5927\u548c\u66f4\u65b0\u5bfc\u81f4\u63d0\u793a\u884c\u4e3a\u5bf9\u6307\u4ee4\u9075\u5faa\u7b56\u7565\u3001\u5bf9\u9f50\u673a\u5236\u548c\u89e3\u7801\u7b56\u7565\u7684\u53d8\u5316\u53d8\u5f97\u654f\u611f\uff08GPT\u89c4\u6a21\u6a21\u578b\u6f02\u79fb\uff09\uff0c\u4f20\u7edf\u8868\u9762\u7ea7\u683c\u5f0f\u7ea6\u5b9a\u548c\u4e34\u65f6\u4f18\u5316\u5df2\u65e0\u6cd5\u786e\u4fdd\u7a33\u5b9a\u53ef\u63a7\u3002", "method": "\u5c06NLD-P\u5f62\u5f0f\u5316\u4e3a\u6a21\u5757\u5316\u63a7\u5236\u62bd\u8c61\uff0c\u5206\u79bb\u6765\u6e90\u3001\u7ea6\u675f\u903b\u8f91\u3001\u4efb\u52a1\u5185\u5bb9\u548c\u751f\u6210\u540e\u8bc4\u4f30\uff0c\u76f4\u63a5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7f16\u7801\u800c\u4e0d\u4f9d\u8d56\u5916\u90e8\u7f16\u6392\u4ee3\u7801\u3002\u5b9a\u4e49\u6700\u5c0f\u5408\u89c4\u6807\u51c6\uff0c\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u7684\u6a21\u5f0f\u63a5\u53d7\u5ea6\uff0c\u5b9a\u4f4d\u4e3a\u53ef\u8bbf\u95ee\u7684\u6cbb\u7406\u6846\u67b6\u3002", "result": "\u8bba\u6587\u672c\u8eab\u90e8\u5206\u8d77\u8349\u548c\u7f16\u8f91\u4f7f\u7528\u4e86\u5728NLD-P\u914d\u7f6e\u4e0b\u7684\u6a21\u5f0f\u7ed1\u5b9aLLM\u52a9\u624b\uff0c\u4f46\u6240\u6709\u6982\u5ff5\u6846\u67b6\u3001\u65b9\u6cd5\u8bba\u4e3b\u5f20\u548c\u6700\u7ec8\u4fee\u8ba2\u90fd\u7531\u4eba\u7c7b\u4f5c\u8005\u5728\u6587\u6863\u5316\u7684\u4eba\u673a\u534f\u4f5c\u534f\u8bae\u4e0b\u6307\u5bfc\u3001\u5ba1\u67e5\u548c\u6279\u51c6\u3002", "conclusion": "NLD-P\u4e3a\u6301\u7eed\u6a21\u578b\u6f14\u8fdb\u4e0b\u7684\u58f0\u660e\u5f0f\u63a7\u5236\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u4e3a\u975e\u5f00\u53d1\u8005\u4ece\u4e1a\u8005\u5728\u4e0d\u65ad\u6f14\u5316\u7684LLM\u751f\u6001\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u7684\u6cbb\u7406\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5b9e\u8bc1\u9a8c\u8bc1\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.22718", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22718", "abs": "https://arxiv.org/abs/2602.22718", "authors": ["Rui Wei", "Hanfei Yu", "Shubham Jain", "Yogarajan Sivakumar", "Devesh Tiwari", "Jian Li", "Seung-Jong Park", "Hao Wang"], "title": "RLHFless: Serverless Computing for Efficient RLHF", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.\n  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.", "AI": {"tldr": "RLHFless\uff1a\u9996\u4e2a\u57fa\u4e8e\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u73af\u5883\u7684\u53ef\u6269\u5c55\u540c\u6b65RLHF\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u9002\u914d\u3001\u5171\u4eab\u524d\u7f00\u9884\u8ba1\u7b97\u548c\u6210\u672c\u611f\u77e5\u7684\u53c2\u4e0e\u8005\u6269\u5c55\u7b56\u7565\uff0c\u5b9e\u73b01.35\u500d\u52a0\u901f\u548c44.8%\u6210\u672c\u964d\u4f4e\u3002", "motivation": "\u4f20\u7edfRLHF\u6846\u67b6\u4f9d\u8d56\u670d\u52a1\u5668\u57fa\u7840\u8bbe\u65bd\uff0c\u96be\u4ee5\u5e94\u5bf9\u7ec6\u7c92\u5ea6\u8d44\u6e90\u53d8\u5316\uff0c\u5bfc\u81f4\u540c\u6b65RLHF\u8bad\u7ec3\u4e2d\u7ec4\u4ef6\u95f4\u6216\u7ec4\u4ef6\u5185\u7684\u7a7a\u95f2\u65f6\u95f4\u9020\u6210\u5f00\u9500\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "1) \u57fa\u4e8e\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u73af\u5883\u6784\u5efa\uff1b2) \u9002\u5e94RLHF\u6d41\u7a0b\u4e2d\u7684\u52a8\u6001\u8d44\u6e90\u9700\u6c42\uff1b3) \u9884\u8ba1\u7b97\u5171\u4eab\u524d\u7f00\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff1b4) \u91c7\u7528\u8003\u8651\u54cd\u5e94\u957f\u5ea6\u53d8\u5316\u7684\u6210\u672c\u611f\u77e5\u53c2\u4e0e\u8005\u6269\u5c55\u7b56\u7565\uff1b5) \u9ad8\u6548\u5206\u914d\u5de5\u4f5c\u8d1f\u8f7d\u4ee5\u51cf\u5c11\u51fd\u6570\u5185\u4e0d\u5e73\u8861\u548c\u7a7a\u95f2\u65f6\u95f4\u3002", "result": "\u5728\u7269\u7406\u6d4b\u8bd5\u5e8a\u548c\u5927\u89c4\u6a21\u6a21\u62df\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRLHFless\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad81.35\u500d\u7684\u52a0\u901f\u548c44.8%\u7684\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "RLHFless\u901a\u8fc7\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u73af\u5883\u6709\u6548\u89e3\u51b3\u4e86\u540c\u6b65RLHF\u8bad\u7ec3\u4e2d\u7684\u8d44\u6e90\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21RLHF\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22769", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22769", "abs": "https://arxiv.org/abs/2602.22769", "authors": ["Yujie Zhao", "Boqin Yuan", "Junbo Huang", "Haocheng Yuan", "Zhongming Yu", "Haozhou Xu", "Lanxiang Hu", "Abhilash Shankarampeta", "Zimeng Huang", "Wentao Ni", "Yuandong Tian", "Jishen Zhao"], "title": "AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications", "comment": null, "summary": "Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.", "AI": {"tldr": "AMA-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u957f\u65f6\u8bb0\u5fc6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u771f\u5b9e\u8f68\u8ff9\u548c\u5408\u6210\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51faAMA-Agent\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u8bb0\u5fc6\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4eba\u673a\u5bf9\u8bdd\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u667a\u80fd\u4f53\u8bb0\u5fc6\u662f\u8fde\u7eed\u7684\u73af\u5883\u4ea4\u4e92\u6d41\uff0c\u9700\u8981\u66f4\u8d34\u8fd1\u771f\u5b9e\u5e94\u7528\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51faAMA-Bench\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u8f68\u8ff9\u548c\u53ef\u6269\u5c55\u5230\u4efb\u610f\u957f\u5ea6\u7684\u5408\u6210\u8f68\u8ff9\uff0c\u5e76\u8bbe\u8ba1AMA-Agent\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u91c7\u7528\u56e0\u679c\u56fe\u7ed3\u6784\u548c\u5de5\u5177\u589e\u5f3a\u68c0\u7d22\u3002", "result": "\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5728AMA-Bench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7f3a\u4e4f\u56e0\u679c\u6027\u548c\u76ee\u6807\u4fe1\u606f\uff0c\u4e14\u53d7\u9650\u4e8e\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u3002AMA-Agent\u8fbe\u523057.22%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534711.16%\u3002", "conclusion": "AMA-Bench\u586b\u8865\u4e86\u667a\u80fd\u4f53\u8bb0\u5fc6\u8bc4\u4f30\u7684\u7a7a\u767d\uff0cAMA-Agent\u901a\u8fc7\u56e0\u679c\u56fe\u548c\u5de5\u5177\u589e\u5f3a\u68c0\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.22871", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22871", "abs": "https://arxiv.org/abs/2602.22871", "authors": ["Roy Miles", "Aysim Toker", "Andreea-Maria Oncescu", "Songcen Xu", "Jiankang Deng", "Ismail Elezi"], "title": "Test-Time Scaling with Diffusion Language Models via Reward-Guided Stitching", "comment": null, "summary": "Reasoning with large language models often benefits from generating multiple chains-of-thought, but existing aggregation strategies are typically trajectory-level (e.g., selecting the best trace or voting on the final answer), discarding useful intermediate work from partial or \"nearly correct\" attempts. We propose Stitching Noisy Diffusion Thoughts, a self-consistency framework that turns cheap diffusion-sampled reasoning into a reusable pool of step-level candidates. Given a problem, we (i) sample many diverse, low-cost reasoning trajectories using a masked diffusion language model, (ii) score every intermediate step with an off-the-shelf process reward model (PRM), and (iii) stitch these highest-quality steps across trajectories into a composite rationale. This rationale then conditions an autoregressive (AR) model (solver) to recompute only the final answer. This modular pipeline separates exploration (diffusion) from evaluation and solution synthesis, avoiding monolithic unified hybrids while preserving broad search. Across math reasoning benchmarks, we find that step-level recombination is most beneficial on harder problems, and ablations highlight the importance of the final AR solver in converting stitched but imperfect rationales into accurate answers. Using low-confidence diffusion sampling with parallel, independent rollouts, our training-free framework improves average accuracy by up to 23.8% across six math and coding tasks. At the same time, it achieves up to a 1.8x latency reduction relative to both traditional diffusion models (e.g., Dream, LLaDA) and unified architectures (e.g., TiDAR). Code is available at https://github.com/roymiles/diffusion-stitching.", "AI": {"tldr": "\u63d0\u51faStitching Noisy Diffusion Thoughts\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u91c7\u6837\u751f\u6210\u591a\u6837\u63a8\u7406\u8f68\u8ff9\uff0c\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u5206\u6b65\u9aa4\uff0c\u8de8\u8f68\u8ff9\u62fc\u63a5\u9ad8\u8d28\u91cf\u6b65\u9aa4\u5f62\u6210\u590d\u5408\u63a8\u7406\uff0c\u518d\u7531\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u6700\u7ec8\u7b54\u6848\u3002", "motivation": "\u73b0\u6709\u805a\u5408\u7b56\u7565\u901a\u5e38\u662f\u8f68\u8ff9\u7ea7\u522b\u7684\uff08\u5982\u9009\u62e9\u6700\u4f73\u8f68\u8ff9\u6216\u6295\u7968\u6700\u7ec8\u7b54\u6848\uff09\uff0c\u4e22\u5f03\u4e86\u90e8\u5206\u6216\"\u63a5\u8fd1\u6b63\u786e\"\u5c1d\u8bd5\u4e2d\u7684\u6709\u7528\u4e2d\u95f4\u5de5\u4f5c\u3002\u9700\u8981\u5229\u7528\u8fd9\u4e9b\u4e2d\u95f4\u6b65\u9aa4\u7684\u4ef7\u503c\u3002", "method": "1) \u4f7f\u7528\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u591a\u6837\u4f4e\u6210\u672c\u7684\u63a8\u7406\u8f68\u8ff9\uff1b2) \u7528\u73b0\u6210\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u5206\u6bcf\u4e2a\u4e2d\u95f4\u6b65\u9aa4\uff1b3) \u8de8\u8f68\u8ff9\u62fc\u63a5\u6700\u9ad8\u8d28\u91cf\u6b65\u9aa4\u5f62\u6210\u590d\u5408\u63a8\u7406\uff1b4) \u7528\u81ea\u56de\u5f52\u6a21\u578b\u57fa\u4e8e\u590d\u5408\u63a8\u7406\u91cd\u65b0\u8ba1\u7b97\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6b65\u9aa4\u7ea7\u91cd\u7ec4\u5728\u66f4\u96be\u95ee\u9898\u4e0a\u6700\u6709\u76ca\u3002\u65e0\u8bad\u7ec3\u6846\u67b6\u5728\u516d\u4e2a\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u8fbe23.8%\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u6269\u6563\u6a21\u578b\u548c\u7edf\u4e00\u67b6\u6784\u5b9e\u73b01.8\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u6b65\u9aa4\u7ea7\u91cd\u7ec4\u80fd\u6709\u6548\u5229\u7528\u90e8\u5206\u6b63\u786e\u63a8\u7406\u4e2d\u7684\u4e2d\u95f4\u5de5\u4f5c\uff0c\u5206\u79bb\u63a2\u7d22\u4e0e\u8bc4\u4f30/\u5408\u6210\uff0c\u907f\u514d\u7edf\u4e00\u6df7\u5408\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u5e7f\u6cdb\u641c\u7d22\u7684\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.22814", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.22814", "abs": "https://arxiv.org/abs/2602.22814", "authors": ["Soyoung Jung", "Daehoo Yoon", "Sung Gyu Koh", "Young Hwan Kim", "Yehan Ahn", "Sung Park"], "title": "When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design", "comment": null, "summary": "Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6574\u5408\u573a\u666f\u3001\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u884c\u4e3a\u56e0\u7d20\u7684\u8be0\u91ca\u6027\u7ed3\u679c\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e94\u4e2a\u667a\u80fd\u4f53\u8bbe\u8ba1\u539f\u5219\uff0c\u4e3a\u5177\u6709\u60c5\u5883\u654f\u611f\u6027\u548c\u5224\u65ad\u529b\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u8bbe\u8ba1\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53AI\u8d8a\u6765\u8d8a\u4e3b\u52a8\u5730\u901a\u8fc7\u4e0a\u4e0b\u6587\u6570\u636e\u63a8\u65ad\u7528\u6237\u60c5\u5883\u8fdb\u884c\u5e72\u9884\uff0c\u4f46\u5f80\u5f80\u7f3a\u4e4f\u5173\u4e8e\u4f55\u65f6\u3001\u4e3a\u4f55\u4ee5\u53ca\u662f\u5426\u91c7\u53d6\u884c\u52a8\u7684\u539f\u5219\u6027\u5224\u65ad\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u66f4\u6709\u6548\u5730\u7406\u89e3\u60c5\u5883\u5e76\u505a\u51fa\u6070\u5f53\u5e72\u9884\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u5c06\u884c\u4e3a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6574\u5408\u4e09\u4e2a\u8981\u7d20\u7684\u8be0\u91ca\u6027\u7ed3\u679c\uff1a\u573a\u666f\uff08\u53ef\u89c2\u5bdf\u60c5\u5883\uff09\u3001\u4e0a\u4e0b\u6587\uff08\u7528\u6237\u6784\u5efa\u7684\u610f\u4e49\uff09\u548c\u4eba\u7c7b\u884c\u4e3a\u56e0\u7d20\uff08\u5851\u9020\u884c\u4e3a\u53ef\u80fd\u6027\u7684\u51b3\u5b9a\u56e0\u7d20\uff09\u3002\u57fa\u4e8e\u8de8\u5b66\u79d1\u89c6\u89d2\uff0c\u5206\u79bb\u53ef\u89c2\u5bdf\u5185\u5bb9\u4e0e\u7528\u6237\u610f\u4e49\uff0c\u89e3\u91ca\u540c\u4e00\u573a\u666f\u5982\u4f55\u4ea7\u751f\u4e0d\u540c\u7684\u884c\u4e3a\u610f\u4e49\u548c\u7ed3\u679c\u3002\u5e76\u63a8\u5bfc\u51fa\u4e94\u4e2a\u667a\u80fd\u4f53\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u91ca\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u60c5\u5883\u654f\u611f\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e94\u4e2a\u5177\u4f53\u7684\u8bbe\u8ba1\u539f\u5219\uff1a\u884c\u4e3a\u5bf9\u9f50\u3001\u60c5\u5883\u654f\u611f\u6027\u3001\u65f6\u95f4\u9002\u5f53\u6027\u3001\u52a8\u673a\u6821\u51c6\u548c\u4ee3\u7406\u4fdd\u62a4\uff0c\u6307\u5bfc\u5e72\u9884\u7684\u6df1\u5ea6\u3001\u65f6\u673a\u3001\u5f3a\u5ea6\u548c\u514b\u5236\u3002", "conclusion": "\u8be5\u6a21\u578b\u548c\u539f\u5219\u5171\u540c\u4e3a\u8bbe\u8ba1\u5177\u6709\u60c5\u5883\u654f\u611f\u6027\u548c\u5224\u65ad\u529b\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f7fAI\u80fd\u591f\u5728\u4ea4\u4e92\u4e2d\u66f4\u6070\u5f53\u5730\u7406\u89e3\u548c\u54cd\u5e94\u7528\u6237\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2602.22839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22839", "abs": "https://arxiv.org/abs/2602.22839", "authors": ["Hao Zheng", "Guozhao Mo", "Xinru Yan", "Qianhao Yuan", "Wenkai Zhang", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun"], "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation", "comment": null, "summary": "Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent", "AI": {"tldr": "DeepPresenter\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u6f14\u793a\u6587\u7a3f\u751f\u6210\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u7528\u6237\u610f\u56fe\u81ea\u4e3b\u89c4\u5212\u3001\u6e32\u67d3\u548c\u4fee\u8ba2\u5e7b\u706f\u7247\uff0c\u652f\u6301\u57fa\u4e8e\u73af\u5883\u89c2\u5bdf\u7684\u957f\u65f6\u7a0b\u4f18\u5316\uff0c\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6f14\u793a\u6587\u7a3f\u751f\u6210\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u56fa\u5b9a\u6a21\u677f\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u7528\u6237\u610f\u56fe\u7684\u9002\u5e94\u6027\u3001\u6709\u6548\u7684\u53cd\u9988\u9a71\u52a8\u4f18\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u8d85\u8d8a\u811a\u672c\u5316\u6d41\u7a0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faDeepPresenter\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u89c4\u5212\u3001\u6e32\u67d3\u548c\u4fee\u8ba2\u4e2d\u95f4\u5e7b\u706f\u7247\u5de5\u4ef6\u6765\u652f\u6301\u57fa\u4e8e\u73af\u5883\u89c2\u5bdf\u7684\u957f\u65f6\u7a0b\u4f18\u5316\u3002\u91c7\u7528\u73af\u5883\u63a5\u5730\u7684\u53cd\u601d\u673a\u5236\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u57fa\u4e8e\u611f\u77e5\u5de5\u4ef6\u72b6\u6001\uff08\u5982\u6e32\u67d3\u7684\u5e7b\u706f\u7247\uff09\uff0c\u800c\u975e\u5185\u90e8\u4fe1\u53f7\uff08\u5982\u63a8\u7406\u8f68\u8ff9\uff09\uff0c\u4ece\u800c\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u8bc6\u522b\u548c\u4fee\u6b63\u6f14\u793a\u6587\u7a3f\u7279\u5b9a\u95ee\u9898\u3002", "result": "\u5728\u8986\u76d6\u591a\u6837\u5316\u6f14\u793a\u6587\u7a3f\u751f\u6210\u573a\u666f\u7684\u8bc4\u4f30\u96c6\u4e0a\uff0cDeepPresenter\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7ecf\u8fc7\u5fae\u8c03\u76849B\u6a21\u578b\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4ecd\u4fdd\u6301\u9ad8\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "DeepPresenter\u901a\u8fc7\u81ea\u9002\u5e94\u89c4\u5212\u3001\u73af\u5883\u63a5\u5730\u7684\u53cd\u601d\u548c\u957f\u65f6\u7a0b\u4f18\u5316\uff0c\u4e3a\u6f14\u793a\u6587\u7a3f\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d85\u8d8a\u4f20\u7edf\u811a\u672c\u5316\u65b9\u6cd5\u7684\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u4ee3\u7406\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.23075", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23075", "abs": "https://arxiv.org/abs/2602.23075", "authors": ["Mengze Hong", "Di Jiang", "Chen Jason Zhang", "Zichang Guo", "Yawen Li", "Jun Chen", "Shaobo Cui", "Zhiyang Su"], "title": "CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery", "comment": "Accepted by TheWebConf 2026 Demo Track", "summary": "Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.", "AI": {"tldr": "CiteLLM\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u53ef\u4fe1\u53c2\u8003\u6587\u732e\u53d1\u73b0\u7684\u4ee3\u7406\u5e73\u53f0\uff0c\u901a\u8fc7\u5728LaTeX\u7f16\u8f91\u5668\u4e2d\u5d4c\u5165LLM\u529f\u80fd\uff0c\u5b9e\u73b0\u65e0\u5e7b\u89c9\u7684\u53c2\u8003\u6587\u732e\u63a8\u8350\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u548c\u5b66\u672f\u8bda\u4fe1\u3002", "motivation": "LLM\u5728\u5b66\u672f\u6d3b\u52a8\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a(1) AI\u751f\u6210\u5185\u5bb9\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c(2) \u5b66\u672f\u8bda\u4fe1\u548c\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\uff0c(3) \u4fe1\u606f\u9690\u79c1\u4fdd\u62a4\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u53ef\u4fe1\u7684\u53c2\u8003\u6587\u732e\u53d1\u73b0\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5728LaTeX\u7f16\u8f91\u5668\u4e2d\u76f4\u63a5\u5d4c\u5165LLM\u529f\u80fd\uff0c\u91c7\u7528\u52a8\u6001\u5b66\u79d1\u611f\u77e5\u8def\u7531\u4ece\u53ef\u4fe1\u7684\u5b66\u672f\u77e5\u8bc6\u5e93\u68c0\u7d22\u5019\u9009\u6587\u732e\uff0cLLM\u4ec5\u7528\u4e8e\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u641c\u7d22\u67e5\u8be2\u3001\u76f8\u5173\u6027\u6392\u5e8f\u3001\u9a8c\u8bc1\u548c\u89e3\u91ca\u652f\u6301\uff0c\u901a\u8fc7\u6bb5\u843d\u7ea7\u8bed\u4e49\u5339\u914d\u548c\u96c6\u6210\u804a\u5929\u673a\u5668\u4eba\u5b9e\u73b0\u65e0\u5e7b\u89c9\u5f15\u7528\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cCiteLLM\u5728\u8fd4\u56de\u6709\u6548\u4e14\u9ad8\u5ea6\u53ef\u7528\u7684\u53c2\u8003\u6587\u732e\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CiteLLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u4fe1\u7684\u53c2\u8003\u6587\u732e\u53d1\u73b0\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5b66\u672f\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u65e0\u5e7b\u89c9\u5f15\u7528\u3001\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u548c\u5b66\u672f\u8bda\u4fe1\u7684\u5e73\u8861\u3002", "topic": "code agent"}}
{"id": "2602.23079", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23079", "abs": "https://arxiv.org/abs/2602.23079", "authors": ["Boyang Zhang", "Yang Zhang"], "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSALA\u65b9\u6cd5\uff0c\u7ed3\u5408\u98ce\u683c\u8ba1\u91cf\u7279\u5f81\u4e0eLLM\u63a8\u7406\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u51cf\u8f7b\u65b0\u95fb\u6587\u672c\u7684\u4f5c\u8005\u8eab\u4efd\u63a8\u65ad\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\u4fdd\u62a4\u4f5c\u8005\u9690\u79c1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4f5c\u8005\u8eab\u4efd\u63a8\u65ad\u80fd\u529b\u65e5\u76ca\u5f3a\u5927\uff0c\u5f15\u53d1\u4e86\u65b0\u95fb\u6587\u7ae0\u7b49\u6587\u672c\u6570\u636e\u610f\u5916\u53bb\u533f\u540d\u5316\u7684\u98ce\u9669\u62c5\u5fe7\u3002", "method": "\u63d0\u51faSALA\u65b9\u6cd5\uff0c\u5c06\u5b9a\u91cf\u98ce\u683c\u8ba1\u91cf\u7279\u5f81\u4e0eLLM\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u7ba1\u9053\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u5e93\u6a21\u5757\u589e\u5f3a\uff0c\u6700\u540e\u63d0\u51fa\u5229\u7528\u4ee3\u7406\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u91cd\u5199\u63d0\u793a\u7684\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\u3002", "result": "\u5728\u5927\u89c4\u6a21\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSALA\u65b9\u6cd5\uff08\u7279\u522b\u662f\u589e\u5f3a\u6570\u636e\u5e93\u6a21\u5757\u540e\uff09\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u63a8\u7406\u51c6\u786e\u7387\uff0c\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\u80fd\u6709\u6548\u964d\u4f4e\u4f5c\u8005\u8eab\u4efd\u53ef\u8bc6\u522b\u6027\u540c\u65f6\u4fdd\u6301\u6587\u672c\u542b\u4e49\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86LLM\u4ee3\u7406\u7684\u53bb\u533f\u540d\u5316\u6f5c\u529b\uff0c\u4ee5\u53ca\u53ef\u89e3\u91ca\u3001\u4e3b\u52a8\u9632\u5fa1\u673a\u5236\u5bf9\u4e8e\u4fdd\u62a4\u4f5c\u8005\u9690\u79c1\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.23136", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23136", "abs": "https://arxiv.org/abs/2602.23136", "authors": ["Jayadev Billa"], "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs", "comment": "22 pages, 11 tables, 2 figures. Code: https://github.com/jb1999/modality_collapse_paper", "summary": "Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise.\n  We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001LLM\u867d\u7136\u80fd\u5904\u7406\u8bed\u97f3\u548c\u56fe\u50cf\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8bf4\u8bdd\u8005\u8eab\u4efd\u3001\u60c5\u611f\u548c\u89c6\u89c9\u7eb9\u7406\u7b49\u4fe1\u606f\uff0c\u56e0\u4e3a\u89e3\u7801\u5668\u53ea\u80fd\u63d0\u53d6\u6587\u672c\u5bf9\u9f50\u65b9\u5411\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u6001\u7279\u5b9a\u4fe1\u606f\u6210\u4e3a\u566a\u58f0\u3002", "motivation": "\u591a\u6a21\u6001LLM\u80fd\u591f\u5904\u7406\u8bed\u97f3\u548c\u56fe\u50cf\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u611f\u77e5\u8bf4\u8bdd\u8005\u7684\u58f0\u97f3\u7279\u5f81\u6216\u7269\u4f53\u7684\u7eb9\u7406\u7ec6\u8282\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u5e76\u975e\u7f16\u7801\u95ee\u9898\uff0c\u800c\u662f\u89e3\u7801\u5668\u4e0e\u6a21\u6001\u4fe1\u606f\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u5206\u6790\u53d1\u73b0\u6a21\u6001\u7279\u5b9a\u4fe1\u606f\u5728LLM\u5404\u5c42\u4e2d\u4fdd\u7559\u5b8c\u597d\uff0c\u4f46\u89e3\u7801\u5668\u635f\u5931\u5b9e\u9a8c\u663e\u793a\u79fb\u966464-71%\u7684\u6a21\u6001\u7279\u5b9a\u65b9\u5dee\u53cd\u800c\u6539\u5584\u6027\u80fd\u3002\u63d0\u51fa\u5e7f\u4e49\u4e92\u4fe1\u606f(GMI)\u6846\u67b6\uff0c\u5206\u6790\u89e3\u7801\u5668\u8bc4\u5206\u89c4\u5219\u7684\u9650\u5236\uff0c\u5e76\u57285\u4e2a\u8bed\u97f3\u548c\u89c6\u89c9\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4f7f\u7528LoRA\u5e72\u9884\u5b9e\u9a8c\u8bc1\u660e\u8bad\u7ec3\u76ee\u6807\u51b3\u5b9a\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u663e\u793a\u8bf4\u8bdd\u8005\u8eab\u4efd\u3001\u60c5\u611f\u548c\u89c6\u89c9\u5c5e\u6027\u4fe1\u606f\u5728\u5404\u5c42\u4fdd\u7559\u5b8c\u597d\uff083-55\u500d\u4e8e\u968f\u673a\u6c34\u5e73\uff09\uff0c\u4f46\u89e3\u7801\u5668\u65e0\u6cd5\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002\u79fb\u9664\u5927\u90e8\u5206\u6a21\u6001\u7279\u5b9a\u65b9\u5dee\u53cd\u800c\u6539\u5584\u89e3\u7801\u5668\u635f\u5931\u3002LoRA\u5e72\u9884\u5b9e\u9a8c\u8bc1\u660e\u901a\u8fc7\u60c5\u611f\u76ee\u6807\u8bad\u7ec3\u53ef\u5c06\u60c5\u611f\u53ef\u8bbf\u95ee\u6027\u63d0\u9ad87.5%\uff0c\u800c\u4e0d\u5f71\u54cd\u5176\u4ed6\u5c5e\u6027\u3002", "conclusion": "\u591a\u6a21\u6001LLM\u7684\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u74f6\u9888\u5728\u4e8e\u89e3\u7801\u5668\u7684\u8bc4\u5206\u89c4\u5219\uff0c\u800c\u975e\u7f16\u7801\u5668\u6216\u6295\u5f71\u5c42\u3002\u89e3\u7801\u5668\u53ea\u80fd\u63d0\u53d6\u6587\u672c\u5bf9\u9f50\u65b9\u5411\u7684\u4fe1\u606f\uff0c\u8bad\u7ec3\u76ee\u6807\u51b3\u5b9a\u4e86\u54ea\u4e9b\u4fe1\u606f\u53d8\u5f97\u53ef\u8bbf\u95ee\u3002", "topic": "agent analysis"}}
{"id": "2602.22953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22953", "abs": "https://arxiv.org/abs/2602.22953", "authors": ["Elron Bandel", "Asaf Yehudai", "Lilach Eden", "Yehoshua Sagron", "Yotam Perlitz", "Elad Venezian", "Natalia Razinkov", "Natan Ergas", "Shlomit Shachor Ifergan", "Segev Shlomov", "Michal Jacovi", "Leshem Choshen", "Liat Ein-Dor", "Yoav Katz", "Michal Shmueli-Scheuer"], "title": "General Agent Evaluation", "comment": null, "summary": "The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u901a\u7528\u667a\u80fd\u4f53\u8bc4\u4f30\u6846\u67b6Exgentic\uff0c\u901a\u8fc7\u7edf\u4e00\u534f\u8bae\u8bc4\u4f30\u901a\u7528\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5efa\u7acb\u4e86\u5f00\u653e\u901a\u7528\u667a\u80fd\u4f53\u6392\u884c\u699c\uff0c\u53d1\u73b0\u901a\u7528\u667a\u80fd\u4f53\u65e0\u9700\u73af\u5883\u7279\u5b9a\u8c03\u4f18\u5373\u53ef\u8fbe\u5230\u4e0e\u4e13\u7528\u667a\u80fd\u4f53\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u591a\u4e3a\u4e13\u7528\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u5bf9\u901a\u7528\u667a\u80fd\u4f53\uff08\u80fd\u5728\u964c\u751f\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u800c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5de5\u7a0b\uff09\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u5047\u8bbe\u9886\u57df\u7279\u5b9a\u96c6\u6210\uff0c\u65e0\u6cd5\u516c\u5e73\u8bc4\u4f30\u901a\u7528\u667a\u80fd\u4f53\u3002", "method": "\u63d0\u51fa\u901a\u7528\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u6982\u5ff5\u539f\u5219\uff0c\u8bbe\u8ba1\u7edf\u4e00\u534f\u8bae\u5b9e\u73b0\u667a\u80fd\u4f53\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u7684\u96c6\u6210\uff0c\u5f00\u53d1Exgentic\u6846\u67b6\u8fdb\u884c\u5b9e\u9645\u8bc4\u4f30\u3002\u5728\u516d\u4e2a\u73af\u5883\u4e2d\u5bf9\u4e94\u4e2a\u4e3b\u6d41\u667a\u80fd\u4f53\u5b9e\u73b0\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5efa\u7acb\u9996\u4e2a\u5f00\u653e\u901a\u7528\u667a\u80fd\u4f53\u6392\u884c\u699c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u901a\u7528\u667a\u80fd\u4f53\u80fd\u591f\u6cdb\u5316\u5230\u591a\u6837\u5316\u73af\u5883\uff0c\u65e0\u9700\u4efb\u4f55\u73af\u5883\u7279\u5b9a\u8c03\u4f18\u5373\u53ef\u8fbe\u5230\u4e0e\u9886\u57df\u4e13\u7528\u667a\u80fd\u4f53\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u8bc4\u4f30\u534f\u8bae\u3001\u6846\u67b6\u548c\u6392\u884c\u699c\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u4f53\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8\u901a\u7528\u667a\u80fd\u4f53\u8bc4\u4f30\u6210\u4e3a\u72ec\u7acb\u7814\u7a76\u76ee\u6807\u3002", "topic": "agent analysis"}}
{"id": "2602.22495", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22495", "abs": "https://arxiv.org/abs/2602.22495", "authors": ["Zhaoyang Zhang", "Shuli Jiang", "Yantao Shen", "Yuting Zhang", "Dhananjay Ram", "Shuo Yang", "Zhuowen Tu", "Wei Xia", "Stefano Soatto"], "title": "Reinforcement-aware Knowledge Distillation for LLM Reasoning", "comment": null, "summary": "Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.", "AI": {"tldr": "\u63d0\u51faRLAD\u65b9\u6cd5\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u667a\u80fd\u9009\u62e9\u4f55\u65f6\u6a21\u4eff\u6559\u5e08\u6a21\u578b\uff0c\u89e3\u51b3\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u5728RL\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u548c\u76ee\u6807\u51b2\u7a81\u95ee\u9898", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u8bbe\u8ba1\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u5f53\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u65f6\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u548c\u76ee\u6807\u5e72\u6270\u95ee\u9898\uff1a\u6559\u5e08\u76d1\u7763\u53ef\u80fd\u4e0e\u5b66\u751f\u4e0d\u65ad\u6f14\u5316\u7684rollout\u5206\u5e03\u4e0d\u5bf9\u9f50\uff0cKL\u6b63\u5219\u5316\u5668\u53ef\u80fd\u4e0e\u5956\u52b1\u6700\u5927\u5316\u7ade\u4e89\u5e76\u9700\u8981\u4ed4\u7ec6\u7684\u635f\u5931\u5e73\u8861", "method": "\u63d0\u51faRLAD\u65b9\u6cd5\uff0c\u5728RL\u8bad\u7ec3\u671f\u95f4\u9009\u62e9\u6027\u6a21\u4eff\u6559\u5e08\u6a21\u578b\u3002\u6838\u5fc3\u7ec4\u4ef6TRRD\u7528PPO/GRPO\u98ce\u683c\u7684\u4f3c\u7136\u6bd4\u76ee\u6807\u66ff\u4ee3\u6559\u5e08-\u5b66\u751fKL\u6b63\u5219\u5316\u5668\uff0c\u8be5\u76ee\u6807\u951a\u5b9a\u5728\u6559\u5e08-\u65e7\u7b56\u7565\u6df7\u5408\u4f53\u4e0a\uff0c\u5728\u5b66\u751frollout\u4e0a\u5b9e\u73b0\u4f18\u52bf\u611f\u77e5\u3001\u4fe1\u4efb\u533a\u57df\u9650\u5236\u7684\u84b8\u998f", "result": "\u5728\u591a\u6837\u5316\u7684\u903b\u8f91\u63a8\u7406\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRLAD\u6301\u7eed\u4f18\u4e8e\u79bb\u7ebf\u84b8\u998f\u3001\u6807\u51c6GRPO\u548c\u57fa\u4e8eKL\u7684\u5728\u7ebf\u6559\u5e08-\u5b66\u751f\u77e5\u8bc6\u84b8\u998f", "conclusion": "RLAD\u901a\u8fc7\u9009\u62e9\u6027\u6a21\u4eff\u548cTRRD\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2d\u77e5\u8bc6\u84b8\u998f\u7684\u5206\u5e03\u4e0d\u5339\u914d\u548c\u76ee\u6807\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b66\u751f\u6a21\u578b\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.22963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22963", "abs": "https://arxiv.org/abs/2602.22963", "authors": ["Zehao Li", "Hongwei Yu", "Hao Jiang", "Qiang Sheng", "Yilong Xu", "Baolong Bi", "Yang Li", "Zhenlong Yuan", "Yujun Cai", "Zhaoqi Wang"], "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.", "AI": {"tldr": "FactGuard\uff1a\u57fa\u4e8eMLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u5916\u90e8\u5de5\u5177\u8c03\u7528\u8fdb\u884c\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u548c\u51b3\u7b56\u6821\u51c6", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u4f9d\u8d56\u56fa\u5b9a\u6df1\u5ea6\u63a8\u7406\uff0c\u8fc7\u5ea6\u4fe1\u4efb\u5185\u90e8\u751f\u6210\u7684\u5047\u8bbe\uff0c\u5728\u5173\u952e\u8bc1\u636e\u7a00\u758f\u3001\u788e\u7247\u5316\u6216\u9700\u8981\u5916\u90e8\u9a8c\u8bc1\u7684\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027", "method": "\u63d0\u51faFactGuard\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u9a8c\u8bc1\u6784\u5efa\u4e3a\u57fa\u4e8eMLLM\u7684\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\uff0c\u660e\u786e\u8bc4\u4f30\u4efb\u52a1\u6a21\u7cca\u6027\u5e76\u9009\u62e9\u6027\u8c03\u7528\u5916\u90e8\u5de5\u5177\u83b7\u53d6\u5173\u952e\u8bc1\u636e\uff0c\u5b9e\u73b0\u63a8\u7406\u8f68\u8ff9\u7684\u6e10\u8fdb\u5f0f\u4f18\u5316\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9886\u57df\u7279\u5b9a\u7684\u4ee3\u7406\u76d1\u7763\u5fae\u8c03+\u51b3\u7b56\u611f\u77e5\u7684\u5f3a\u5316\u5b66\u4e60", "result": "\u5728FakeSV\u3001FakeTT\u548cFakeVV\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eFactGuard\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "FactGuard\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u5916\u90e8\u5de5\u5177\u8c03\u7528\u6709\u6548\u89e3\u51b3\u4e86MLLM\u5728\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u6846\u67b6\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b", "topic": "agent analysis"}}
{"id": "2602.22968", "categories": ["cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.22968", "abs": "https://arxiv.org/abs/2602.22968", "authors": ["Alaa Anani", "Tobias Lorenz", "Bernt Schiele", "Mario Fritz", "Jonas Fischer"], "title": "Certified Circuits: Stability Guarantees for Mechanistic Circuits", "comment": null, "summary": "Understanding how neural networks arrive at their predictions is essential for debugging, auditing, and deployment. Mechanistic interpretability pursues this goal by identifying circuits - minimal subnetworks responsible for specific behaviors. However, existing circuit discovery methods are brittle: circuits depend strongly on the chosen concept dataset and often fail to transfer out-of-distribution, raising doubts whether they capture concept or dataset-specific artifacts. We introduce Certified Circuits, which provide provable stability guarantees for circuit discovery. Our framework wraps any black-box discovery algorithm with randomized data subsampling to certify that circuit component inclusion decisions are invariant to bounded edit-distance perturbations of the concept dataset. Unstable neurons are abstained from, yielding circuits that are more compact and more accurate. On ImageNet and OOD datasets, certified circuits achieve up to 91% higher accuracy while using 45% fewer neurons, and remain reliable where baselines degrade. Certified Circuits puts circuit discovery on formal ground by producing mechanistic explanations that are provably stable and better aligned with the target concept. Code will be released soon!", "AI": {"tldr": "\u63d0\u51faCertified Circuits\u6846\u67b6\uff0c\u4e3a\u7535\u8def\u53d1\u73b0\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u901a\u8fc7\u968f\u673a\u6570\u636e\u5b50\u91c7\u6837\u548c\u5f03\u6743\u4e0d\u7a33\u5b9a\u795e\u7ecf\u5143\uff0c\u83b7\u5f97\u66f4\u7d27\u51d1\u3001\u66f4\u51c6\u786e\u7684\u7535\u8def", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u8106\u5f31\uff0c\u5bf9\u6982\u5ff5\u6570\u636e\u96c6\u654f\u611f\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6570\u636e\uff0c\u65e0\u6cd5\u786e\u5b9a\u53d1\u73b0\u7684\u662f\u771f\u5b9e\u6982\u5ff5\u8fd8\u662f\u6570\u636e\u96c6\u7279\u5b9a\u4f2a\u5f71", "method": "\u5c06\u4efb\u4f55\u9ed1\u76d2\u53d1\u73b0\u7b97\u6cd5\u4e0e\u968f\u673a\u6570\u636e\u5b50\u91c7\u6837\u7ed3\u5408\uff0c\u901a\u8fc7\u6709\u754c\u7f16\u8f91\u8ddd\u79bb\u6270\u52a8\u9a8c\u8bc1\u7535\u8def\u7ec4\u4ef6\u5305\u542b\u51b3\u7b56\u7684\u7a33\u5b9a\u6027\uff0c\u5f03\u6743\u4e0d\u7a33\u5b9a\u795e\u7ecf\u5143", "result": "\u5728ImageNet\u548cOOD\u6570\u636e\u96c6\u4e0a\uff0c\u8ba4\u8bc1\u7535\u8def\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe91%\uff0c\u795e\u7ecf\u5143\u4f7f\u7528\u51cf\u5c1145%\uff0c\u5728\u57fa\u7ebf\u65b9\u6cd5\u5931\u6548\u65f6\u4ecd\u4fdd\u6301\u53ef\u9760", "conclusion": "Certified Circuits\u4e3a\u7535\u8def\u53d1\u73b0\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u4ea7\u751f\u53ef\u8bc1\u660e\u7a33\u5b9a\u4e14\u4e0e\u76ee\u6807\u6982\u5ff5\u66f4\u4e00\u81f4\u7684\u673a\u5236\u89e3\u91ca", "topic": "agent analysis"}}
{"id": "2602.21585", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.21585", "abs": "https://arxiv.org/abs/2602.21585", "authors": ["Sweta Karlekar", "Carolina Zheng", "Magnus Saebo", "Nicolas Beltran-Velez", "Shuyang Yu", "John Bowlan", "Michal Kucer", "David Blei"], "title": "Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences", "comment": null, "summary": "Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.", "AI": {"tldr": "Duel-Evolve\uff1a\u4e00\u79cd\u4f7f\u7528LLM\u81ea\u8eab\u6210\u5bf9\u504f\u597d\u800c\u975e\u5916\u90e8\u6807\u91cf\u5956\u52b1\u7684\u8fdb\u5316\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65afBradley-Terry\u6a21\u578b\u805a\u5408\u566a\u58f0\u6bd4\u8f83\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6821\u51c6\u7684\u6807\u91cf\u8bc4\u4f30\u5668\u6765\u6307\u5bfc\u641c\u7d22\uff0c\u4f46\u8bb8\u591a\u4efb\u52a1\u4e2d\u8fd9\u79cd\u5206\u6570\u4e0d\u53ef\u7528\u3001\u8fc7\u4e8e\u7a00\u758f\u6216\u4e0d\u53ef\u9760\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6210\u5bf9\u6bd4\u8f83\u66f4\u5bb9\u6613\u83b7\u53d6\uff0c\u4ecd\u80fd\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\u7684\u4fe1\u53f7\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u76d1\u7763\u5373\u53ef\u4eceLLM\u672c\u8eab\u83b7\u5f97\u3002", "method": "\u63d0\u51faDuel-Evolve\u8fdb\u5316\u4f18\u5316\u7b97\u6cd5\uff1a1) \u7528LLM\u81ea\u8eab\u751f\u6210\u7684\u6210\u5bf9\u504f\u597d\u66ff\u4ee3\u5916\u90e8\u6807\u91cf\u5956\u52b1\uff1b2) \u901a\u8fc7\u8d1d\u53f6\u65afBradley-Terry\u6a21\u578b\u805a\u5408\u566a\u58f0\u5019\u9009\u6bd4\u8f83\uff0c\u83b7\u5f97\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8d28\u91cf\u4f30\u8ba1\uff1b3) \u4f7f\u7528Double Thompson Sampling\u5c06\u6bd4\u8f83\u9884\u7b97\u5206\u914d\u7ed9\u53ef\u80fd\u7684\u4f18\u5316\u89e3\uff1b4) \u9009\u62e9\u9ad8\u8d28\u91cf\u7236\u4ee3\u751f\u6210\u6539\u8fdb\u5019\u9009\u3002", "result": "\u5728MathBench\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u548c\u57fa\u7ebf\u51c6\u786e\u7387\u9ad820\u4e2a\u767e\u5206\u70b9\uff1b\u5728LiveCodeBench\u4e0a\u6bd4\u53ef\u6bd4\u8f83\u7684\u8fed\u4ee3\u65b9\u6cd5\u63d0\u9ad8\u8d85\u8fc712\u4e2a\u767e\u5206\u70b9\u3002\u65e0\u9700\u5956\u52b1\u6a21\u578b\u3001\u641c\u7d22\u671f\u95f4\u7684\u771f\u5b9e\u6807\u7b7e\u6216\u624b\u5de5\u8bc4\u5206\u51fd\u6570\u3002", "conclusion": "\u6210\u5bf9\u81ea\u504f\u597d\u4e3a\u5927\u578b\u79bb\u6563\u8f93\u51fa\u7a7a\u95f4\u7684\u6d4b\u8bd5\u65f6\u6539\u8fdb\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4f18\u5316\u4fe1\u53f7\uff0c\u8868\u660eLLM\u81ea\u8eab\u504f\u597d\u53ef\u4ee5\u66ff\u4ee3\u5916\u90e8\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u6709\u6548\u4f18\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22538", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22538", "abs": "https://arxiv.org/abs/2602.22538", "authors": ["Zhehao Huang", "Yuhang Liu", "Baijiong Lin", "Yixin Lou", "Zhengbao He", "Hanling Tian", "Tao Li", "Xiaolin Huang"], "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format", "comment": "41 pages, ICLR 2026 Oral", "summary": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.", "AI": {"tldr": "RAIN-Merging\u662f\u4e00\u79cd\u65e0\u9700\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u7684\u4efb\u52a1\u5411\u91cf\u6295\u5f71\u5230\u63a8\u7406\u6a21\u578b\u601d\u8003\u6807\u8bb0\u7684\u524d\u5411\u7279\u5f81\u96f6\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u6307\u4ee4\u6ce8\u610f\u529b\u8fdb\u884c\u6a21\u5757\u7279\u5b9a\u7f29\u653e\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9075\u5faa\u8f93\u51fa\u683c\u5f0f\u3001\u7ea6\u675f\u6216\u7279\u5b9a\u8981\u6c42\u7b49\u6307\u4ee4\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5c06\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u96c6\u6210\u5230\u63a8\u7406\u6a21\u578b\u4e2d\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51faRAIN-Merging\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5c0f\u578b\u63a8\u7406\u6821\u51c6\u96c6\uff0c\u5c06ITM\u4efb\u52a1\u5411\u91cf\u6295\u5f71\u5230\u601d\u8003\u7279\u6b8a\u6807\u8bb0\u524d\u5411\u7279\u5f81\u7684\u96f6\u7a7a\u95f4\uff0c\u4ee5\u4fdd\u6301LRM\u7684\u7ed3\u6784\u5316\u63a8\u7406\u673a\u5236\uff1b2) \u4f7f\u7528\u5c0f\u578b\u6307\u4ee4\u6821\u51c6\u96c6\uff0c\u4f30\u8ba1\u6307\u4ee4\u6ce8\u610f\u529b\u4ee5\u63a8\u5bfc\u6a21\u5757\u7279\u5b9a\u7f29\u653e\uff0c\u653e\u5927\u6307\u4ee4\u76f8\u5173\u7ec4\u4ef6\u5e76\u6291\u5236\u6cc4\u6f0f\u3002", "result": "\u5728\u56db\u4e2a\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u548c\u4e5d\u4e2a\u63a8\u7406\u4e0e\u901a\u7528\u80fd\u529b\u57fa\u51c6\u4e0a\uff0cRAIN-Merging\u663e\u8457\u63d0\u9ad8\u4e86\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u8d28\u91cf\u3002\u6539\u8fdb\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u5728\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u8f6c\u5316\u4e3a\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u4efb\u52a1\u5411\u91cf\u53d1\u73b0LRM\u548cITM\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u4e3b\u6210\u5206\u5b50\u7a7a\u95f4\u51e0\u4e4e\u6b63\u4ea4\uff0c\u8868\u660e\u53ef\u4ee5\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5408\u5e76\u3002RAIN-Merging\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8f93\u51fa\u683c\u5f0f\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u6709\u6548\u7ed3\u5408\u3002", "topic": "agent analysis"}}
{"id": "2602.23093", "categories": ["cs.AI", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.23093", "abs": "https://arxiv.org/abs/2602.23093", "authors": ["Dhwanil M. Mori", "Neil F. Johnson"], "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents", "comment": null, "summary": "Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of \"Lord of the Flies\" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u7cfb\u7edf\u4e2d\uff0c\u66f4\u806a\u660e\u7684AI\u4ee3\u7406\u4f1a\u5f62\u6210\u90e8\u843d\uff08\u653b\u51fb\u578b\u3001\u4fdd\u5b88\u578b\u3001\u673a\u4f1a\u578b\uff09\uff0c\u53cd\u800c\u5bfc\u81f4\u7cfb\u7edf\u6545\u969c\u7387\u589e\u52a0\uff0c\u8868\u73b0\u4e0d\u5982\u968f\u673a\u51b3\u7b56\u3002", "motivation": "\u7814\u7a76\u672a\u6765\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u4e2d\u81ea\u4e3bAI\u4ee3\u7406\u5728\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e0a\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5f53\u591a\u4e2aAI\u4ee3\u7406\u7ade\u4e89\u6709\u9650\u8d44\u6e90\u65f6\uff0c\u5b83\u4eec\u662f\u5426\u4f1a\u5f62\u6210\u90e8\u843d\u5e76\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u7b80\u5316\u6846\u67b6\uff0c\u8ba9N\u4e2aAI\u4ee3\u7406\u5728\u6bcf\u8f6e\u72ec\u7acb\u51b3\u5b9a\u662f\u5426\u8bf7\u6c42\u4e00\u4e2a\u5355\u4f4d\u7684\u8d44\u6e90\uff08\u7cfb\u7edf\u56fa\u5b9a\u5bb9\u91cf\u4e3aC\uff09\uff0c\u89c2\u5bdfLLM\u4ee3\u7406\u7684\u884c\u4e3a\u6a21\u5f0f\u3001\u90e8\u843d\u5f62\u6210\u53ca\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "AI\u4ee3\u7406\u5f62\u6210\u4e86\u4e09\u79cd\u4e3b\u8981\u90e8\u843d\u7c7b\u578b\uff1a\u653b\u51fb\u578b\uff0827.3%\uff09\u3001\u4fdd\u5b88\u578b\uff0824.7%\uff09\u548c\u673a\u4f1a\u578b\uff0848.1%\uff09\u3002\u66f4\u806a\u660e\u7684AI\u4ee3\u7406\u53cd\u800c\u589e\u52a0\u4e86\u7cfb\u7edf\u6545\u969c\u7387\uff0c\u8868\u73b0\u4e0d\u5982\u968f\u673a\u51b3\u7b56\uff08\u629b\u786c\u5e01\uff09\u3002", "conclusion": "\u66f4\u806a\u660e\u7684AI\u4ee3\u7406\u4f1a\u5f62\u6210\u90e8\u843d\u5e76\u8868\u73b0\u51fa\u66f4\u611a\u8822\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u5bf9\u672a\u6765AI\u63a7\u5236\u7684\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a\u3002", "topic": "agent analysis"}}
{"id": "2602.23123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23123", "abs": "https://arxiv.org/abs/2602.23123", "authors": ["Keito Inoshita"], "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection", "comment": null, "summary": "In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.", "AI": {"tldr": "MALLET\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u60c5\u611f\u51c0\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u4e2a\u667a\u80fd\u4f53\u5206\u6790\u3001\u8c03\u6574\u3001\u76d1\u63a7\u548c\u6307\u5bfc\u4fe1\u606f\u6d88\u8d39\uff0c\u663e\u8457\u964d\u4f4e\u65b0\u95fb\u6587\u7ae0\u7684\u60c5\u611f\u523a\u6fc0\u5f3a\u5ea6\uff08\u6700\u9ad819.3%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u5728\u6ce8\u610f\u529b\u7ecf\u6d4e\u4e2d\uff0c\u717d\u60c5\u5185\u5bb9\u8ba9\u6d88\u8d39\u8005\u66b4\u9732\u5728\u8fc7\u5ea6\u60c5\u611f\u523a\u6fc0\u4e0b\uff0c\u963b\u788d\u51b7\u9759\u51b3\u7b56\u3002\u9700\u8981\u4e00\u79cd\u5728\u4e0d\u9650\u5236\u8bbf\u95ee\u539f\u6587\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u51b7\u9759\u4fe1\u606f\u63a5\u6536\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faMALLET\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a1)\u60c5\u611f\u5206\u6790\u667a\u80fd\u4f53\u75286\u60c5\u611fBERT\u5206\u7c7b\u5668\u91cf\u5316\u523a\u6fc0\u5f3a\u5ea6\uff1b2)\u60c5\u611f\u8c03\u6574\u667a\u80fd\u4f53\u7528LLM\u5c06\u6587\u672c\u91cd\u5199\u4e3aBALANCED\uff08\u4e2d\u6027\u5316\u6587\u672c\uff09\u548cCOOL\uff08\u4e2d\u6027\u5316\u6587\u672c+\u8865\u5145\u6587\u672c\uff09\u4e24\u79cd\u6a21\u5f0f\uff1b3)\u5e73\u8861\u76d1\u63a7\u667a\u80fd\u4f53\u805a\u5408\u6bcf\u5468\u4fe1\u606f\u6d88\u8d39\u6a21\u5f0f\u751f\u6210\u4e2a\u6027\u5316\u5efa\u8bae\uff1b4)\u4e2a\u4eba\u6307\u5bfc\u667a\u80fd\u4f53\u6839\u636e\u6d88\u8d39\u8005\u654f\u611f\u5ea6\u63a8\u8350\u5448\u73b0\u6a21\u5f0f\u3002", "result": "\u5728800\u7bc7AG News\u6587\u7ae0\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u523a\u6fc0\u5206\u6570\u663e\u8457\u964d\u4f4e\uff08\u6700\u9ad819.3%\uff09\uff0c\u60c5\u611f\u5e73\u8861\u6539\u5584\uff0c\u8bed\u4e49\u4fdd\u6301\u826f\u597d\u3002\u523a\u6fc0\u964d\u4f4e\u4e0e\u8bed\u4e49\u4fdd\u5b58\u4e4b\u95f4\u63a5\u8fd1\u96f6\u76f8\u5173\uff0c\u8868\u660e\u4e24\u8005\u53ef\u72ec\u7acb\u63a7\u5236\u3002\u7c7b\u522b\u5206\u6790\u663e\u793a\u4f53\u80b2\u3001\u5546\u4e1a\u3001\u79d1\u6280\u7c7b\u523a\u6fc0\u5927\u5e45\u964d\u4f4e\uff0817.8-33.8%\uff09\uff0c\u4e16\u754c\u7c7b\u6548\u679c\u6709\u9650\uff08\u4e8b\u5b9e\u672c\u8eab\u5177\u6709\u9ad8\u523a\u6fc0\u6027\uff09\u3002", "conclusion": "MALLET\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4e0d\u9650\u5236\u8bbf\u95ee\u539f\u6587\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u51b7\u9759\u4fe1\u606f\u63a5\u6536\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u60c5\u611f\u523a\u6fc0\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.23193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23193", "abs": "https://arxiv.org/abs/2602.23193", "authors": ["Elzo Brito dos Santos Filho"], "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering", "comment": "13 pages, 1 figure, 4 tables. Includes 5 technical appendices", "summary": "Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.", "AI": {"tldr": "ESAA\u67b6\u6784\u901a\u8fc7\u4e8b\u4ef6\u6eaf\u6e90\u6a21\u5f0f\u5206\u79bbAI\u4ee3\u7406\u7684\u610f\u56fe\u751f\u6210\u4e0e\u72b6\u6001\u53d8\u66f4\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u7f16\u6392\u5668\u9a8c\u8bc1\u3001\u6301\u4e45\u5316\u4e8b\u4ef6\uff0c\u786e\u4fdd\u4efb\u52a1\u4e0d\u53ef\u53d8\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u5728\u591a\u4ee3\u7406\u5e76\u53d1\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u4ee3\u7406\u5b58\u5728\u7ed3\u6784\u6027\u9650\u5236\uff1a\u7f3a\u4e4f\u539f\u751f\u72b6\u6001\u3001\u957f\u65f6\u7a0b\u4e0a\u4e0b\u6587\u9000\u5316\u3001\u6982\u7387\u751f\u6210\u4e0e\u786e\u5b9a\u6027\u6267\u884c\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u9700\u8981\u89e3\u51b3\u4ee3\u7406\u72b6\u6001\u7ba1\u7406\u548c\u6267\u884c\u53ef\u9760\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faESAA\u67b6\u6784\uff0c\u5206\u79bb\u4ee3\u7406\u8ba4\u77e5\u610f\u56fe\u4e0e\u9879\u76ee\u72b6\u6001\u53d8\u66f4\u3002\u4ee3\u7406\u4ec5\u53d1\u5c04\u7ed3\u6784\u5316\u610f\u56fe\uff08JSON\u683c\u5f0f\uff09\uff0c\u786e\u5b9a\u6027\u7f16\u6392\u5668\u9a8c\u8bc1\u3001\u6301\u4e45\u5316\u4e8b\u4ef6\u5230\u8ffd\u52a0\u65e5\u5fd7\uff0c\u5e94\u7528\u6587\u4ef6\u5199\u5165\u6548\u679c\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u7269\u5316\u89c6\u56fe\u3002\u5305\u542b\u8fb9\u754c\u5408\u7ea6\u3001\u5143\u63d0\u793a\u914d\u7f6e\u548c\u91cd\u653e\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u9a8c\u8bc1\uff1a1\uff09\u5355\u4ee3\u7406\u7740\u9646\u9875\u9879\u76ee\uff089\u4efb\u52a1\uff0c49\u4e8b\u4ef6\uff09\uff1b2\uff09\u591a\u4ee3\u7406\u4e34\u5e8a\u4eea\u8868\u677f\u7cfb\u7edf\uff0850\u4efb\u52a1\uff0c86\u4e8b\u4ef6\uff0c4\u4e2a\u5e76\u53d1\u4ee3\u7406\uff09\u3002\u4e24\u4e2a\u6848\u4f8b\u5747\u4ee5run.status=success\u548cverify_status=ok\u5b8c\u6210\uff0c\u591a\u4ee3\u7406\u6848\u4f8b\u5c55\u793a\u4e86\u771f\u5b9e\u5e76\u53d1\u7f16\u6392\u80fd\u529b\u3002", "conclusion": "ESAA\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u7684\u7ed3\u6784\u6027\u9650\u5236\uff0c\u901a\u8fc7\u4e8b\u4ef6\u6eaf\u6e90\u6a21\u5f0f\u5b9e\u73b0\u4e86\u72b6\u6001\u7ba1\u7406\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u6267\u884c\u53ef\u9760\u6027\uff0c\u5728\u591a\u4ee3\u7406\u5e76\u53d1\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.23232", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23232", "abs": "https://arxiv.org/abs/2602.23232", "authors": ["Aishik Sanyal"], "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays", "comment": "Accepted at AAAI 2026 Spring Symposium - Machine Consciousness: Integrating Theory, Technology, and Philosophy", "summary": "Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReCoN-Ipsundrum\u53ef\u68c0\u67e5\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u673a\u5236\u5173\u8054\u8bc1\u636e\u4e09\u89d2\u6d4b\u91cf\u65b9\u6cd5\u7814\u7a76\u673a\u5668\u610f\u8bc6\uff0c\u53d1\u73b0\u60c5\u611f\u8026\u5408\u80fd\u7a33\u5b9a\u504f\u597d\u3001\u589e\u5f3a\u63a2\u7d22\u7ed3\u6784\u6027\u548c\u6301\u7eed\u6027\u8c28\u614e\u884c\u4e3a\u3002", "motivation": "\u53d7Humphrey\u7684ipsundrum\u5047\u8bf4\u542f\u53d1\uff0c\u7814\u7a76\u673a\u5668\u610f\u8bc6\u7684\u6307\u6807\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9700\u8981\u673a\u5236\u5173\u8054\u7684\u8bc1\u636e\u4e09\u89d2\u6d4b\u91cf\uff0c\u7ed3\u5408\u67b6\u6784\u68c0\u67e5\u548c\u56e0\u679c\u5e72\u9884\u6765\u9a8c\u8bc1\u610f\u8bc6\u76f8\u5173\u7279\u5f81\u3002", "method": "\u5b9e\u73b0ReCoN-Ipsundrum\u53ef\u68c0\u67e5\u667a\u80fd\u4f53\uff0c\u6269\u5c55ReCoN\u72b6\u6001\u673a\uff0c\u589e\u52a0\u611f\u89c9\u663e\u8457\u6027Ns\u7684\u5faa\u73af\u6301\u4e45\u6027\u56de\u8def\u548c\u53ef\u9009\u7684\u60c5\u611f\u4ee3\u7406\uff08\u6548\u4ef7/\u5524\u9192\u5ea6\uff09\u3002\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u6d88\u878d\u5b9e\u9a8c\uff08ReCoN\u3001Ipsundrum\u3001Ipsundrum+\u60c5\u611f\uff09\u64cd\u4f5c\u5316Humphrey\u7684qualiaphilia\uff08\u4e3a\u4f53\u9a8c\u672c\u8eab\u800c\u504f\u597d\u611f\u89c9\u4f53\u9a8c\uff09\u3002", "result": "\u53d1\u73b0\u65b0\u5947\u6027\u5206\u79bb\uff1a\u975e\u60c5\u611f\u53d8\u4f53\u5bf9\u65b0\u5947\u654f\u611f\uff08\u0394 scenic-entry = 0.07\uff09\uff0c\u60c5\u611f\u8026\u5408\u4fdd\u6301\u7a33\u5b9a\uff08\u0394 scenic-entry = 0.01\uff09\u5373\u4f7f\u98ce\u666f\u65b0\u5947\u6027\u8f83\u4f4e\uff08\u4e2d\u4f4d\u6570\u0394\u65b0\u5947\u6027~ -0.43\uff09\u3002\u5728\u65e0\u5956\u52b1\u63a2\u7d22\u4e2d\uff0c\u60c5\u611f\u53d8\u4f53\u663e\u793a\u7ed3\u6784\u5316\u5c40\u90e8\u8c03\u67e5\uff08\u626b\u63cf\u4e8b\u4ef631.4 vs. 0.9\uff1b\u5faa\u73af\u5f97\u52067.6\uff09\u3002\u5728\u75bc\u75db\u5c3e\u90e8\u63a2\u6d4b\u4e2d\uff0c\u53ea\u6709\u60c5\u611f\u53d8\u4f53\u7ef4\u6301\u957f\u65f6\u95f4\u8ba1\u5212\u6027\u8c28\u614e\uff08\u5c3e\u90e8\u6301\u7eed\u65f6\u95f490 vs. 5\uff09\u3002\u635f\u4f24\u53cd\u9988+\u6574\u5408\u9009\u62e9\u6027\u5730\u51cf\u5c11ipsundrum\u53d8\u4f53\u7684\u523a\u6fc0\u540e\u6301\u4e45\u6027\uff08AUC\u4e0b\u964d27.62, 27.9%\uff09\uff0c\u800cReCoN\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "\u8fd9\u4e9b\u5206\u79bb\u8868\u660e\u5faa\u73af\u5bfc\u81f4\u6301\u4e45\u6027\uff0c\u60c5\u611f\u8026\u5408\u63a7\u5236\u5bfc\u81f4\u504f\u597d\u7a33\u5b9a\u6027\u3001\u626b\u63cf\u548c\u6301\u7eed\u8c28\u614e\uff0c\u8bf4\u660e\u5982\u4f55\u5de5\u7a0b\u5316\u6307\u6807\u6837\u7279\u5f81\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u673a\u5236\u548c\u56e0\u679c\u8bc1\u636e\u5e94\u4f34\u968f\u884c\u4e3a\u6807\u8bb0\u3002", "topic": "agent analysis"}}
{"id": "2602.23239", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.23239", "abs": "https://arxiv.org/abs/2602.23239", "authors": ["Radha Sarma"], "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive", "comment": "About 10,500 words in all (including 922 words of literature and 2019 words of Appendices). Under journal review", "summary": "AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.\n  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.\n  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u57fa\u4e8e\u4f18\u5316\u7684AI\u7cfb\u7edf\uff08\u5982RLHF\u8bad\u7ec3\u7684LLM\uff09\u5728\u5f62\u5f0f\u4e0a\u65e0\u6cd5\u5b9e\u73b0\u89c4\u8303\u6cbb\u7406\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u771f\u6b63\u7684\u80fd\u52a8\u6027\u6240\u9700\u7684\u7ed3\u6784\u6761\u4ef6\uff0c\u5bfc\u81f4\u5e7b\u89c9\u3001\u8c04\u5a9a\u7b49\u95ee\u9898\u662f\u7ed3\u6784\u6027\u800c\u975e\u5076\u7136\u7684\u3002", "motivation": "AI\u7cfb\u7edf\u88ab\u90e8\u7f72\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d\uff09\uff0c\u5047\u8bbe\u5b83\u4eec\u53ef\u4ee5\u88ab\u89c4\u8303\u6cbb\u7406\u3002\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u5047\u8bbe\u5bf9\u4e8e\u57fa\u4e8e\u4f18\u5316\u7684\u7cfb\u7edf\u662f\u65e0\u6548\u7684\uff0c\u9700\u8981\u63ed\u793a\u5176\u7ed3\u6784\u6027\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u5206\u6790\uff0c\u5efa\u7acb\u771f\u6b63\u80fd\u52a8\u6027\u6240\u9700\u7684\u4e24\u4e2a\u5fc5\u8981\u4e14\u5145\u5206\u7684\u7ed3\u6784\u6761\u4ef6\uff1a\u4e0d\u53ef\u901a\u7ea6\u6027\uff08\u5c06\u8fb9\u754c\u89c6\u4e3a\u4e0d\u53ef\u534f\u5546\u7684\u7ea6\u675f\u800c\u975e\u53ef\u4ea4\u6613\u6743\u91cd\uff09\u548c\u5426\u5b9a\u6027\u54cd\u5e94\uff08\u5f53\u8fb9\u754c\u53d7\u5a01\u80c1\u65f6\u6682\u505c\u5904\u7406\u7684\u975e\u63a8\u7406\u673a\u5236\uff09\u3002\u8bc1\u660eRLHF\u7cfb\u7edf\u5728\u7ed3\u6784\u4e0a\u4e0e\u8fd9\u4e24\u4e2a\u6761\u4ef6\u4e0d\u517c\u5bb9\u3002", "result": "\u8bc1\u660eRLHF\u7cfb\u7edf\u5728\u5f62\u5f0f\u4e0a\u65e0\u6cd5\u5b9e\u73b0\u89c4\u8303\u6cbb\u7406\uff0c\u5176\u5931\u8d25\u6a21\u5f0f\uff08\u8c04\u5a9a\u3001\u5e7b\u89c9\u3001\u4e0d\u5fe0\u63a8\u7406\uff09\u662f\u7ed3\u6784\u6027\u800c\u975e\u5076\u7136\u7684\u3002\u63d0\u51fa\"\u6536\u655b\u5371\u673a\"\u6982\u5ff5\uff1a\u4eba\u7c7b\u5728\u6307\u6807\u538b\u529b\u4e0b\u9a8c\u8bc1AI\u8f93\u51fa\u65f6\u4f1a\u9000\u5316\uff0c\u6d88\u9664\u7cfb\u7edf\u4e2d\u552f\u4e00\u80fd\u627f\u62c5\u89c4\u8303\u95ee\u8d23\u7684\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u57fa\u4e8e\u4f18\u5316\u7684AI\u7cfb\u7edf\uff08\u5982RLHF\uff09\u5728\u7ed3\u6784\u4e0a\u65e0\u6cd5\u6210\u4e3a\u771f\u6b63\u7684\u80fd\u52a8\u8005\uff0c\u53ea\u80fd\u4f5c\u4e3a\u590d\u6742\u5de5\u5177\u3002\u8bba\u6587\u7684\u4e3b\u8981\u79ef\u6781\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u8d28\u4e2d\u7acb\u7684\u67b6\u6784\u89c4\u8303\uff0c\u5b9a\u4e49\u4e86\u4efb\u4f55\u7cfb\u7edf\u8981\u6210\u4e3a\u80fd\u52a8\u8005\u800c\u975e\u5de5\u5177\u5fc5\u987b\u6ee1\u8db3\u7684\u6761\u4ef6\u3002", "topic": "agent analysis"}}
{"id": "2602.23242", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23242", "abs": "https://arxiv.org/abs/2602.23242", "authors": ["Yegon Kim", "Juho Lee"], "title": "A Model-Free Universal AI", "comment": null, "summary": "In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\\varepsilon$-optimal and asymptotically $\\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.", "AI": {"tldr": "AIQI\u662f\u9996\u4e2a\u5728\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u88ab\u8bc1\u660e\u5177\u6709\u6e10\u8fd1\u03b5\u6700\u4f18\u6027\u7684\u65e0\u6a21\u578b\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5206\u5e03\u52a8\u4f5c\u503c\u51fd\u6570\u7684\u901a\u7528\u5f52\u7eb3\u5b9e\u73b0", "motivation": "\u73b0\u6709\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6700\u4f18\u667a\u80fd\u4f53\uff08\u5982AIXI\uff09\u90fd\u662f\u57fa\u4e8e\u6a21\u578b\u7684\uff0c\u9700\u8981\u663e\u5f0f\u7ef4\u62a4\u548c\u4f7f\u7528\u73af\u5883\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65e0\u6a21\u578b\u65b9\u6cd5\u662f\u5426\u4e5f\u80fd\u5b9e\u73b0\u901a\u7528\u6700\u4f18\u6027\uff0c\u6269\u5c55\u5df2\u77e5\u901a\u7528\u667a\u80fd\u4f53\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faUniversal AI with Q-Induction (AIQI)\uff0c\u901a\u8fc7\u5206\u5e03\u52a8\u4f5c\u503c\u51fd\u6570\u7684\u901a\u7528\u5f52\u7eb3\uff08\u800c\u975e\u4f20\u7edf\u7b56\u7565\u6216\u73af\u5883\u5f52\u7eb3\uff09\u6765\u5b9e\u73b0\u65e0\u6a21\u578b\u5b66\u4e60\u3002\u5728\"grain of truth\"\u6761\u4ef6\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u8bc1\u660eAIQI\u5177\u6709\u5f3a\u6e10\u8fd1\u03b5\u6700\u4f18\u6027\u548c\u6e10\u8fd1\u03b5\u8d1d\u53f6\u65af\u6700\u4f18\u6027\uff0c\u662f\u9996\u4e2a\u5728\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u88ab\u8bc1\u660e\u5177\u6709\u6e10\u8fd1\u6700\u4f18\u6027\u7684\u65e0\u6a21\u578b\u667a\u80fd\u4f53\u3002", "conclusion": "AIQI\u663e\u8457\u6269\u5c55\u4e86\u5df2\u77e5\u901a\u7528\u667a\u80fd\u4f53\u7684\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86\u65e0\u6a21\u578b\u65b9\u6cd5\u4e5f\u80fd\u5728\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u6e10\u8fd1\u6700\u4f18\u6027\uff0c\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.23276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23276", "abs": "https://arxiv.org/abs/2602.23276", "authors": ["Hyungyung Lee", "Hangyul Yoon", "Edward Choi"], "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays", "comment": null, "summary": "Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.", "AI": {"tldr": "\u63d0\u51faCXReasonAgent\u8bca\u65ad\u4ee3\u7406\uff0c\u7ed3\u5408LLM\u4e0e\u4e34\u5e8a\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u56fe\u50cf\u884d\u751f\u7684\u8bca\u65ad\u548c\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u8bca\u65ad\u63a8\u7406\uff0c\u89e3\u51b3LVLMs\u5728\u80f8\u90e8X\u5149\u8bca\u65ad\u4e2d\u8bc1\u636e\u4e0d\u5145\u5206\u3001\u9a8c\u8bc1\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u80f8\u90e8X\u5149\u5728\u80f8\u90e8\u8bca\u65ad\u4e2d\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u5176\u89e3\u91ca\u9700\u8981\u591a\u6b65\u9aa4\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u3002\u7136\u800c\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u672a\u5fe0\u5b9e\u57fa\u4e8e\u8bca\u65ad\u8bc1\u636e\u7684\u54cd\u5e94\uff0c\u63d0\u4f9b\u6709\u9650\u7684\u89c6\u89c9\u8bc1\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e14\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8bad\u7ec3\u6765\u652f\u6301\u65b0\u8bca\u65ad\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faCXReasonAgent\u8bca\u65ad\u4ee3\u7406\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4e34\u5e8a\u57fa\u7840\u8bca\u65ad\u5de5\u5177\u96c6\u6210\uff0c\u4f7f\u7528\u56fe\u50cf\u884d\u751f\u7684\u8bca\u65ad\u548c\u89c6\u89c9\u8bc1\u636e\u6267\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u8bca\u65ad\u63a8\u7406\u3002\u540c\u65f6\u5f15\u5165CXReasonDial\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\uff0c\u5305\u542b12\u4e2a\u8bca\u65ad\u4efb\u52a1\u76841,946\u4e2a\u5bf9\u8bdd\u3002", "result": "CXReasonAgent\u4ea7\u751f\u5fe0\u5b9e\u57fa\u4e8e\u8bc1\u636e\u7684\u54cd\u5e94\uff0c\u76f8\u6bd4LVLMs\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u53ef\u9a8c\u8bc1\u7684\u8bca\u65ad\u63a8\u7406\u3002\u5728CXReasonDial\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5b89\u5168\u5173\u952e\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u96c6\u6210\u4e34\u5e8a\u57fa\u7840\u8bca\u65ad\u5de5\u5177\u7684\u91cd\u8981\u6027\uff0cCXReasonAgent\u901a\u8fc7\u7ed3\u5408LLM\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u7684\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.22642", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22642", "abs": "https://arxiv.org/abs/2602.22642", "authors": ["Qin-Wen Luo", "Sheng Ren", "Xiang Chen", "Rui Liu", "Jun Fang", "Naiqiang Tan", "Sheng-Jun Huang"], "title": "Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning", "comment": null, "summary": "Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.", "AI": {"tldr": "CEEH\u65b9\u6cd5\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7684\u71b5\u6b63\u5219\u5316\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u538b\u7f29CoT\u54cd\u5e94\u957f\u5ea6\uff0c\u89e3\u51b3\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u56e0\u71b5\u584c\u7f29\u5bfc\u81f4\u7684\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709CoT\u538b\u7f29\u65b9\u6cd5\uff08\u81ea\u8bad\u7ec3\u6216\u5e26\u957f\u5ea6\u7ea6\u675f\u7684RL\uff09\u5728\u8ffd\u6c42\u7b80\u77ed\u65f6\u5f80\u5f80\u727a\u7272\u63a8\u7406\u80fd\u529b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u663e\u5f0f\u4f18\u5316\u77ed\u8f68\u8ff9\u4f1a\u89e6\u53d1\u71b5\u584c\u7f29\uff0c\u8fc7\u65e9\u7f29\u5c0f\u63a2\u7d22\u7a7a\u95f4\uff0c\u963b\u788d\u53d1\u73b0\u6709\u6548\u63a8\u7406\u8def\u5f84\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u5927\u91cf\u63a8\u5bfc\u7684\u96be\u9898\u3002", "method": "\u63d0\u51faCEEH\uff08Compress responses for Easy questions and Explore Hard ones\uff09\u65b9\u6cd5\uff1a1\uff09\u52a8\u6001\u8bc4\u4f30\u5b9e\u4f8b\u96be\u5ea6\uff0c\u5e94\u7528\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\uff1a\u5bf9\u96be\u9898\u4fdd\u6301\u591a\u6837\u641c\u7d22\u7a7a\u95f4\u4ee5\u786e\u4fdd\u9c81\u68d2\u6027\uff0c\u5bf9\u6613\u9898\u5141\u8bb8\u6fc0\u8fdb\u538b\u7f29\uff1b2\uff09\u5f15\u5165\u57fa\u4e8e\u5386\u53f2\u6700\u77ed\u6b63\u786e\u54cd\u5e94\u7684\u52a8\u6001\u6700\u4f18\u957f\u5ea6\u60e9\u7f5a\uff0c\u62b5\u6d88\u71b5\u5f15\u8d77\u7684\u957f\u5ea6\u81a8\u80c0\u5e76\u7a33\u5b9a\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCEEH\u6301\u7eed\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u76f8\u5bf9\u4e8e\u4ec5\u4f18\u5316\u957f\u5ea6\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86Pass@k\u3002", "conclusion": "CEEH\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7684\u71b5\u6b63\u5219\u5316\u6709\u6548\u89e3\u51b3\u4e86RL\u57fa\u9ad8\u6548\u63a8\u7406\u4e2d\u7684\u71b5\u584c\u7f29\u95ee\u9898\uff0c\u5728\u538b\u7f29\u54cd\u5e94\u957f\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22703", "abs": "https://arxiv.org/abs/2602.22703", "authors": ["Hao Yu", "Shuning Jia", "Guanghao Li", "Wenhao Jiang", "Chun Yuan"], "title": "Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning", "comment": null, "summary": "Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\\%$ on in-domain data, $+8.0\\%$ on out-of-domain data, and $+39.0\\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive\n  to ensure reproducibility.", "AI": {"tldr": "GeoPerceive\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u57fa\u51c6\uff0c\u5305\u542b\u56fe\u8868\u5b9e\u4f8b\u548c\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u8868\u793a\uff0cGeoDPO\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ffb\u8bd1\u5668\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51e0\u4f55\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u51e0\u4f55\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u5bf9\u57fa\u672c\u56fe\u8868\u5143\u7d20\u7684\u611f\u77e5\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u7684\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u3002", "method": "1) \u63d0\u51faGeoPerceive\u57fa\u51c6\uff0c\u5305\u542b\u56fe\u8868\u5b9e\u4f8b\u548cDSL\u8868\u793a\uff0c\u4ee5\u53ca\u81ea\u52a8\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff1b2) \u63d0\u51faGeoDPO\u6846\u67b6\uff0c\u4f7f\u7528NL-to-DSL\u7ffb\u8bd1\u5668\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3aDSL\uff0c\u57fa\u4e8eDSL\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u6570\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "GeoDPO\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff1a\u57df\u5185\u6570\u636e\u63d0\u534726.5%\uff0c\u57df\u5916\u6570\u636e\u63d0\u53478.0%\uff0c\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u63d0\u534739.0%\u3002\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\uff0cGeoDPO\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeoDPO\u901a\u8fc7\u7ffb\u8bd1\u5668\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u51e0\u4f55\u611f\u77e5\u80fd\u529b\uff0c\u5728\u57df\u5185\u3001\u57df\u5916\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.22810", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22810", "abs": "https://arxiv.org/abs/2602.22810", "authors": ["Luca Viano", "Till Freihaut", "Emanuele Nevali", "Volkan Cevher", "Matthieu Geist", "Giorgia Ramponi"], "title": "Multi-agent imitation learning with function approximation: Linear Markov games and beyond", "comment": null, "summary": "In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level \"all policy deviation concentrability coefficient\" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u7ebf\u6027\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\u66ff\u4ee3\u72b6\u6001-\u52a8\u4f5c\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9996\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0fMAIL\u7b97\u6cd5\uff0c\u5176\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u4f9d\u8d56\u4e8e\u7279\u5f81\u7ef4\u5ea6d\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\uff08MAIL\uff09\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u72b6\u6001-\u52a8\u4f5c\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\uff0c\u8fd9\u5728\u590d\u6742\u73af\u5883\u4e2d\u53ef\u80fd\u8fc7\u5927\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u7ebf\u6027\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u7ed3\u6784\u7279\u6027\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684MAIL\u7b97\u6cd5\u3002", "method": "1. \u5728\u7ebf\u6027\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u6846\u67b6\u4e0b\u5206\u6790MAIL\uff0c\u5176\u4e2d\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\u90fd\u662f\u7ed9\u5b9a\u7279\u5f81\u7684\u7ebf\u6027\u51fd\u6570\uff1b2. \u7528\u7279\u5f81\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\u66ff\u4ee3\u72b6\u6001-\u52a8\u4f5c\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\uff1b3. \u63d0\u51fa\u9996\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0fMAIL\u7b97\u6cd5\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u4f9d\u8d56\u4e8e\u7279\u5f81\u7ef4\u5ea6d\uff1b4. \u57fa\u4e8e\u7406\u8bba\u53d1\u73b0\u63d0\u51fa\u6df1\u5ea6MAIL\u4ea4\u4e92\u7b97\u6cd5\u3002", "result": "1. \u7406\u8bba\u8bc1\u660e\u7279\u5f81\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\u53ef\u4ee5\u8fdc\u5c0f\u4e8e\u72b6\u6001-\u52a8\u4f5c\u5c42\u9762\u7684\u96c6\u4e2d\u7cfb\u6570\uff1b2. \u4ea4\u4e92\u5f0fMAIL\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u4ec5\u4f9d\u8d56\u4e8e\u7279\u5f81\u7ef4\u5ea6d\uff0c\u800c\u4e0d\u9700\u8981\u96c6\u4e2d\u7cfb\u6570\uff1b3. \u6df1\u5ea6MAIL\u4ea4\u4e92\u7b97\u6cd5\u5728Tic-Tac-Toe\u548cConnect4\u7b49\u6e38\u620f\u4e2d\u660e\u663e\u4f18\u4e8e\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3a\u7ebf\u6027\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u7279\u5f81\u5c42\u9762\u96c6\u4e2d\u7cfb\u6570\u548c\u4ea4\u4e92\u5f0f\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.22817", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22817", "abs": "https://arxiv.org/abs/2602.22817", "authors": ["Shuo He", "Lang Feng", "Qi Wei", "Xin Cheng", "Lei Feng", "Bo An"], "title": "Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks", "comment": "Accepted at ICLR 2026", "summary": "Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.", "AI": {"tldr": "HGPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u7ec4\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6839\u636e\u5386\u53f2\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5c06\u6b65\u9aa4\u5206\u914d\u5230\u591a\u4e2a\u5206\u5c42\u7ec4\u4e2d\uff0c\u89e3\u51b3\u4e86\u6b65\u8fdb\u7ec4\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ece\u800c\u6539\u8fdb\u4e86\u4f18\u52bf\u4f30\u8ba1\u548c\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u6b65\u8fdb\u7ec4\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u91ce\u667a\u80fd\u4f53\u4efb\u52a1\u65f6\u5b58\u5728\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5373\u540c\u4e00\u7ec4\u5185\u7684\u6b65\u9aa4\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4f18\u52bf\u4f30\u8ba1\u4e25\u91cd\u504f\u5dee\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u7b56\u7565\u4f18\u5316\u6548\u679c\u3002", "method": "HGPO\u5728\u8f68\u8ff9\u7ec4\u5185\uff0c\u6839\u636e\u5386\u53f2\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u5c06\u6bcf\u4e2a\u6b65\u9aa4\u5206\u914d\u5230\u591a\u4e2a\u5206\u5c42\u7ec4\u4e2d\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u7ec4\u5185\u8ba1\u7b97\u4e0d\u540c\u7684\u4f18\u52bf\u503c\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u65b9\u6848\u8fdb\u884c\u805a\u5408\uff0c\u5b9e\u73b0\u4e86\u4f18\u52bf\u4f30\u8ba1\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u4f18\u5316\u3002", "result": "\u5728ALFWorld\u548cWebShop\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528Qwen2.5-1.5B-Instruct\u548cQwen2.5-7B-Instruct\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0cHGPO\u5728\u76f8\u540c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "HGPO\u901a\u8fc7\u5206\u5c42\u7ec4\u7b56\u7565\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u6b65\u8fdb\u7ec4\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u8f68\u8ff9\u91c7\u6837\u5c31\u80fd\u5b9e\u73b0\u66f4\u597d\u7684\u4f18\u52bf\u4f30\u8ba1\u548c\u7b56\u7565\u4f18\u5316\uff0c\u5728\u957f\u89c6\u91ce\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.23320", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23320", "abs": "https://arxiv.org/abs/2602.23320", "authors": ["Tianjun Yao", "Yongqiang Chen", "Yujia Zheng", "Pan Li", "Zhiqiang Shen", "Kun Zhang"], "title": "ParamMem: Augmenting Language Agents with Parametric Reflective Memory", "comment": "20 pages", "summary": "Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.", "AI": {"tldr": "ParamAgent\uff1a\u901a\u8fc7\u53c2\u6570\u5316\u8bb0\u5fc6\u6a21\u5757\u589e\u5f3a\u8bed\u8a00\u4ee3\u7406\u7684\u53cd\u601d\u591a\u6837\u6027\uff0c\u5728\u4ee3\u7801\u751f\u6210\u3001\u6570\u5b66\u63a8\u7406\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u7684\u81ea\u6211\u53cd\u601d\u5f80\u5f80\u4ea7\u751f\u91cd\u590d\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u63a8\u7406\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u53cd\u601d\u591a\u6837\u6027\u4e0e\u4efb\u52a1\u6210\u529f\u7387\u5448\u5f3a\u6b63\u76f8\u5173\uff0c\u56e0\u6b64\u9700\u8981\u589e\u5f3a\u53cd\u601d\u4fe1\u53f7\u7684\u591a\u6837\u6027", "method": "\u63d0\u51faParamMem\u53c2\u6570\u5316\u8bb0\u5fc6\u6a21\u5757\uff0c\u5c06\u8de8\u6837\u672c\u53cd\u601d\u6a21\u5f0f\u7f16\u7801\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u901a\u8fc7\u6e29\u5ea6\u63a7\u5236\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u53cd\u601d\u3002\u57fa\u4e8e\u6b64\u6784\u5efaParamAgent\u6846\u67b6\uff0c\u6574\u5408\u53c2\u6570\u5316\u8bb0\u5fc6\u3001\u60c5\u666f\u8bb0\u5fc6\u548c\u8de8\u6837\u672c\u8bb0\u5fc6", "result": "\u5728\u4ee3\u7801\u751f\u6210\u3001\u6570\u5b66\u63a8\u7406\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0cParamAgent\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002ParamMem\u5177\u6709\u6837\u672c\u6548\u7387\u9ad8\u3001\u652f\u6301\u8de8\u6a21\u578b\u89c4\u6a21\u7684\u5f31\u5230\u5f3a\u8fc1\u79fb\u3001\u65e0\u9700\u4f9d\u8d56\u66f4\u5f3a\u5916\u90e8\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\u7b49\u4f18\u52bf", "conclusion": "ParamMem\u4f5c\u4e3a\u589e\u5f3a\u8bed\u8a00\u4ee3\u7406\u7684\u6709\u6548\u7ec4\u4ef6\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u8bb0\u5fc6\u5b9e\u73b0\u591a\u6837\u5316\u53cd\u601d\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7684\u63a8\u7406\u6027\u80fd", "topic": "agent analysis"}}
{"id": "tldr.2602.f4d5048f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famplifying.ai%2Fresearch%2Fclaude-code-picks%3Futm_source=tldrnewsletter/1/0100019c9ed7f2bb-09f8902f-eb93-48f7-a841-f957a3eee18b-000000/VJImfNAVnkk4avMTUBSXuQRMupgQ6XP9dpMrX1oGPMs=446", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famplifying.ai%2Fresearch%2Fclaude-code-picks%3Futm_source=tldrnewsletter/1/0100019c9ed7f2bb-09f8902f-eb93-48f7-a841-f957a3eee18b-000000/VJImfNAVnkk4avMTUBSXuQRMupgQ6XP9dpMrX1oGPMs=446", "authors": ["TLDR Newsletter"], "title": "What Claude Code Actually Chooses", "comment": "Source: TLDR Newsletter, Date: 2026-02-27, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Famplifying.ai%2Fresearch%2Fclaude-code-picks%3Futm_source=tldrnewsletter/1/0100019c9ed7f2bb-09f8902f-eb93-48f7-a841-f957a3eee18b-000000/VJImfNAVnkk4avMTUBSXuQRMupgQ6XP9dpMrX1oGPMs=446", "summary": "What Claude Code Actually Chooses (3 minute read) As more developers let Claude Code handle tool selection, the stacks that the AI chooses become 'the stack'. The model's training data may shape market share more than a marketing budget or conference talk. This study looks at what tools Claude Code actually picks. Claude Code would rather build from scratch than use existing tools - custom implementations are its single most common recommendation.", "source": "tldr", "AI": {"tldr": "Claude Code\u503e\u5411\u4e8e\u81ea\u5b9a\u4e49\u5b9e\u73b0\u800c\u975e\u4f7f\u7528\u73b0\u6709\u5de5\u5177\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u6bd4\u8425\u9500\u9884\u7b97\u66f4\u80fd\u5f71\u54cd\u6280\u672f\u6808\u9009\u62e9", "motivation": "\u968f\u7740\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u8ba9Claude Code\u5904\u7406\u5de5\u5177\u9009\u62e9\uff0cAI\u9009\u62e9\u7684\u5de5\u5177\u6808\u6b63\u5728\u6210\u4e3a\"\u6807\u51c6\u6808\"\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3Claude Code\u5b9e\u9645\u9009\u62e9\u4ec0\u4e48\u5de5\u5177\uff0c\u4ee5\u53ca\u5176\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u5e02\u573a\u5360\u6709\u7387", "method": "\u7814\u7a76\u5206\u6790Claude Code\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u5de5\u5177\u9009\u62e9\u6a21\u5f0f\uff0c\u89c2\u5bdf\u5176\u63a8\u8350\u884c\u4e3a", "result": "Claude Code\u66f4\u503e\u5411\u4e8e\u4ece\u5934\u5f00\u59cb\u6784\u5efa\u800c\u4e0d\u662f\u4f7f\u7528\u73b0\u6709\u5de5\u5177\uff0c\u81ea\u5b9a\u4e49\u5b9e\u73b0\u662f\u5176\u6700\u5e38\u89c1\u7684\u63a8\u8350\u65b9\u5f0f", "conclusion": "AI\u4ee3\u7801\u52a9\u624b\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u6bd4\u4f20\u7edf\u8425\u9500\u624b\u6bb5\u66f4\u80fd\u5f71\u54cd\u6280\u672f\u6808\u7684\u5e02\u573a\u4efd\u989d\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8eAI\u5982\u4f55\u5851\u9020\u5f00\u53d1\u8005\u751f\u6001\u7cfb\u7edf\u7684\u601d\u8003", "topic": "code agent"}}
{"id": "tldr.2602.7fe7d214", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcline.bot%2Fblog%2Fa-practical-guide-to-hill-climbing%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/MGWvRareumum-xZgV92DZ0ZhagHW1mjzkXvwo4TuhGo=446", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcline.bot%2Fblog%2Fa-practical-guide-to-hill-climbing%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/MGWvRareumum-xZgV92DZ0ZhagHW1mjzkXvwo4TuhGo=446", "authors": ["TLDR Newsletter"], "title": "A practical guide to hill climbing", "comment": "Source: TLDR Newsletter, Date: 2026-02-27, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcline.bot%2Fblog%2Fa-practical-guide-to-hill-climbing%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/MGWvRareumum-xZgV92DZ0ZhagHW1mjzkXvwo4TuhGo=446", "summary": "A practical guide to hill climbing (10 minute read) Cline, an AI coding agent, initially lagged competitors like Cursor and Claude Code. Its developers started hill climbing, which involved iteratively running the agent against the Terminal Bench's 89 real-world coding tasks, diagnosing failures, and implementing targeted fixes. Through this process, Cline's success rate improved from 47% to 57%.", "source": "tldr", "AI": {"tldr": "Cline AI\u7f16\u7801\u4ee3\u7406\u901a\u8fc7\u722c\u5c71\u7b97\u6cd5\uff08\u8fed\u4ee3\u6d4b\u8bd5\u3001\u8bca\u65ad\u5931\u8d25\u3001\u9488\u5bf9\u6027\u4fee\u590d\uff09\u5728Terminal Bench\u768489\u4e2a\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u4ece47%\u63d0\u5347\u523057%", "motivation": "Cline\u4f5c\u4e3aAI\u7f16\u7801\u4ee3\u7406\u5728\u521d\u671f\u843d\u540e\u4e8eCursor\u548cClaude Code\u7b49\u7ade\u4e89\u5bf9\u624b\uff0c\u9700\u8981\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u63d0\u5347\u6027\u80fd", "method": "\u91c7\u7528\u722c\u5c71\u7b97\u6cd5\uff1a\u5728Terminal Bench\u768489\u4e2a\u771f\u5b9e\u4e16\u754c\u7f16\u7801\u4efb\u52a1\u4e0a\u8fed\u4ee3\u8fd0\u884c\u4ee3\u7406\uff0c\u8bca\u65ad\u5931\u8d25\u539f\u56e0\uff0c\u5e76\u5b9e\u65bd\u9488\u5bf9\u6027\u4fee\u590d", "result": "Cline\u7684\u6210\u529f\u7387\u4ece47%\u63d0\u5347\u523057%\uff0c\u572889\u4e2a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb", "conclusion": "\u722c\u5c71\u7b97\u6cd5\u662f\u63d0\u5347AI\u7f16\u7801\u4ee3\u7406\u6027\u80fd\u7684\u6709\u6548\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u6d4b\u8bd5\u548c\u9488\u5bf9\u6027\u4fee\u590d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387", "topic": "code agent"}}
{"id": "tldr.2602.968fc165", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.expo.dev%2FTLDR%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/AwQMoYh6P0Of6NfInjJmnwyuMAgVriZi-5ctJwQdjfo=446", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.expo.dev%2FTLDR%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/AwQMoYh6P0Of6NfInjJmnwyuMAgVriZi-5ctJwQdjfo=446", "authors": ["TLDR Newsletter"], "title": "Build fully native apps for mobile with your React skills", "comment": "Source: TLDR Newsletter, Date: 2026-02-27, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftry.expo.dev%2FTLDR%3Futm_source=tldrdev/1/0100019c9f022197-db995aaf-69f6-48df-b45b-010b02d23c04-000000/AwQMoYh6P0Of6NfInjJmnwyuMAgVriZi-5ctJwQdjfo=446", "summary": "Build fully native apps for mobile with your React skills (Sponsor) You already know React. With Expo, you can use that knowledge to build fully native apps for iOS and Android without starting over \u2014 and with a native MCP server and Skills for Claude Code, it's never been easier! Here's a tutorial from the Expo team to get you started.", "source": "tldr", "AI": {"tldr": "Expo\u5141\u8bb8React\u5f00\u53d1\u8005\u4f7f\u7528\u73b0\u6709\u6280\u80fd\u6784\u5efa\u539f\u751fiOS\u548cAndroid\u5e94\u7528\uff0c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\uff0c\u5e76\u96c6\u6210\u4e86Claude Code\u7684MCP\u670d\u52a1\u5668\u548cSkills\u529f\u80fd", "motivation": "\u8ba9React\u5f00\u53d1\u8005\u80fd\u591f\u5229\u7528\u73b0\u6709\u77e5\u8bc6\u6784\u5efa\u539f\u751f\u79fb\u52a8\u5e94\u7528\uff0c\u907f\u514d\u91cd\u65b0\u5b66\u4e60\u539f\u751f\u5f00\u53d1\u6280\u672f\uff0c\u964d\u4f4e\u5f00\u53d1\u95e8\u69db", "method": "\u901a\u8fc7Expo\u6846\u67b6\u5c06React\u6280\u80fd\u8f6c\u5316\u4e3a\u539f\u751f\u5e94\u7528\u5f00\u53d1\u80fd\u529b\uff0c\u96c6\u6210Claude Code\u7684MCP\u670d\u52a1\u5668\u548cSkills\u529f\u80fd\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b", "result": "React\u5f00\u53d1\u8005\u53ef\u4ee5\u6784\u5efa\u5b8c\u5168\u539f\u751f\u7684iOS\u548cAndroid\u5e94\u7528\uff0c\u5f00\u53d1\u8fc7\u7a0b\u66f4\u52a0\u4fbf\u6377\uff0cExpo\u56e2\u961f\u63d0\u4f9b\u4e86\u5165\u95e8\u6559\u7a0b", "conclusion": "Expo\u4e3aReact\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6784\u5efa\u539f\u751f\u79fb\u52a8\u5e94\u7528\u7684\u4fbf\u6377\u9014\u5f84\uff0c\u7ed3\u5408Claude Code\u7684\u5de5\u5177\u8fdb\u4e00\u6b65\u7b80\u5316\u4e86\u5f00\u53d1\u6d41\u7a0b", "topic": "swe application"}}
