<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.AI](#cs.AI) [Total: 13]
- [wechat.article](#wechat.article) [Total: 25]
- [tldr.article](#tldr.article) [Total: 18]
- [cs.LG](#cs.LG) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: DeepNews框架通过模拟金融记者的认知过程，采用双粒度检索、模式引导战略规划和对抗约束提示，解决了长文本生成中的"不可能三角"问题，显著提升了金融报道的真实性和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在垂直领域长文本生成中存在"不可能三角"瓶颈：难以同时实现低幻觉、深度逻辑连贯性和个性化表达。研究发现这是由于现有生成范式陷入"统计平滑陷阱"，忽视了专家写作所需的高熵信息获取和结构化认知过程。

Method: 提出DeepNews框架，模拟资深金融记者的认知过程：1)基于信息觅食理论的双粒度检索机制，强制执行10:1饱和信息输入比；2)模式引导战略规划，利用领域专家知识库和原子块构建逻辑骨架；3)对抗约束提示，使用节奏打破和逻辑迷雾等技术破坏模型生成文本的概率平滑性。

Result: 实验发现金融深度报道中存在"知识悬崖"：当检索上下文低于15,000字符时内容真实性崩溃，而超过30,000字符的高冗余输入可将无幻觉率稳定在85%以上。在盲测中，基于旧模型构建的DeepNews系统获得25%的提交接受率，显著优于SOTA模型的零样本生成(0%)。

Conclusion: 通过显式建模专家认知过程，DeepNews框架有效解决了长文本生成的"不可能三角"问题，证明了结构化工作流在提升垂直领域文本生成质量方面的有效性。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [2] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: AutoMedic是一个多智能体模拟框架，用于自动化评估大语言模型作为临床对话代理的性能，将静态医疗问答数据集转换为虚拟患者档案，通过CARE指标进行多维度评估。


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域大语言模型评估存在局限：静态问答基准无法评估动态交互式临床对话场景，且缺乏多维度评估策略。动态临床情境评估面临组合空间庞大、难以标准化和定量测量的挑战。

Method: 提出AutoMedic多智能体模拟框架，将现成的静态医疗问答数据集转换为虚拟患者档案，实现LLM智能体之间真实、临床基础的多轮临床对话。使用CARE指标（临床对话准确性、效率/策略、同理心、鲁棒性）进行多维度评估。

Result: AutoMedic被验证为临床对话代理的有效自动化评估框架，为对话式医疗应用中LLM的有效开发提供实用指南。人类专家验证了其有效性。

Conclusion: AutoMedic解决了医疗领域LLM评估的关键挑战，通过多智能体模拟和CARE多维度指标，为临床对话代理的标准化、定量评估提供了有效解决方案。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [3] [Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale](https://arxiv.org/abs/2512.10398)
*Zhaodong Wang,Zhenting Qi,Sherman Wong,Nathan Hu,Samuel Lin,Jun Ge,Erwin Gao,Yining Yang,Ben Maurer,Wenlin Chen,David Recordon,Yilun Du,Minlan Yu,Ying Zhang*

Main category: cs.CL

TL;DR: Confucius Code Agent (CCA) 是一个开源AI软件工程师，在工业规模任务上表现优异，在SWE-Bench-Pro上达到54.3%的Resolve@1性能，通过Confucius SDK平台实现长上下文推理、跨会话持续学习和模块化工具使用。


<details>
  <summary>Details</summary>
Motivation: 现实世界的AI软件工程需要能够处理大规模代码库、保持长期记忆并协调复杂工具链的编码代理。现有开源代理在工业规模任务上表现不足，而专有代理虽然性能强但可扩展性、可解释性和可控性有限。

Method: 基于Confucius SDK平台构建，包含统一编排器（分层工作记忆用于长上下文推理）、持久笔记系统（跨会话持续学习）、模块化扩展模块（鲁棒工具使用），以及通过构建-测试-改进循环自动化配置合成的元代理。

Result: 在SWE-Bench-Pro上达到54.3%的Resolve@1性能，显著优于先前编码代理，实现了工业规模的软件工程任务处理能力。

Conclusion: Confucius SDK和CCA为AI代理提供了透明、可扩展和可复现的基础，弥合了研究原型与生产级系统之间的差距，支持工业规模的代理开发和部署。

Abstract: Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: ATLAS是一个自动化管道，通过合成带验证的Dafny程序来解决LLM程序验证训练数据稀缺问题，生成2700个验证程序和19000多个训练示例，显著提升了Qwen 2.5 7B Coder在Dafny验证任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在程序验证方面显示出潜力，但由于缺乏经过验证的代码训练数据，进展受到阻碍。现有的验证代码稀缺，限制了LLM在形式验证任务上的能力提升。

Method: ATLAS是一个自动化管道，能够大规模合成经过验证的Dafny程序。它生成完整的Dafny程序，包括规范、实现和证明。通过将合成过程分解为多个专门任务，从每个验证程序中提取多个训练示例，共生成2700个验证程序和19000多个训练示例。

Result: 在ATLAS生成的数据集上微调Qwen 2.5 7B Coder模型，在DafnyBench上提升了23个百分点，在DafnySynthesis上提升了50个百分点，显示出合成验证代码能有效增强LLM的形式验证能力。

Conclusion: 合成验证代码可以有效解决LLM程序验证训练数据稀缺的问题，显著提升模型在形式验证任务上的性能，为程序验证领域提供了新的数据生成方法。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [5] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 研究发现SWE-Bench-Verified基准测试可能被训练数据污染，模型表现优异更多源于训练记忆而非真实问题解决能力，这误导了对代码代理能力的评估。


<details>
  <summary>Details</summary>
Motivation: SWE-Bench-Verified作为评估LLM解决GitHub问题的基准测试，可能存在训练数据污染问题，导致分数反映的是训练记忆而非真实技能，这会影响对代码代理能力的准确评估。

Method: 测试两个在基准测试中表现优异的Claude模型，让它们仅使用问题文本定位相关文件，然后使用问题文本加文件路径。在BeetleBox和SWE-rebench上进行相同测试，比较模型在不同基准上的表现差异。

Result: 模型在SWE-Bench-Verified上的表现比在其他基准上好3倍，定位编辑文件的能力好6倍，尽管没有额外项目上下文。这表明模型可能在训练中见过SWE-Bench-Verified任务。

Conclusion: SWE-Bench-Verified基准测试可能被训练数据污染，分数不能反映代理处理真实软件问题的能力，继续使用会误导进展评估和代理设计选择，需要转向考虑污染问题的新数据集。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [6] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 该研究通过三个互补方向提升软件问题解决效率：1) 利用LLM推理和应用特定信息提高问题报告质量；2) 实证分析传统和AI增强系统中的开发者工作流程；3) 通过ML、DL和LLM方法自动化bug定位和解决方案识别等认知密集型任务。


<details>
  <summary>Details</summary>
Motivation: 解决软件开发中常见的问题解决挑战：低质量的问题报告、对实际工作流程理解有限、缺乏自动化支持。这些问题影响软件质量和维护效率。

Method: 采用三个互补的研究方向：1) 利用LLM推理和应用特定信息的技术提升问题报告质量；2) 实证分析传统和AI增强系统中的开发者工作流程；3) 结合机器学习、深度学习和LLM方法，自动化bug定位和解决方案识别等认知密集型任务。

Result: 开发了实证洞察、实用工具和自动化方法，推进AI驱动的问题解决，支持更可维护和高质量的软件系统。

Conclusion: 通过综合方法提升软件问题解决效率，为AI驱动的软件维护提供实证基础、工具支持和技术方案。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [7] [Cross-modal Retrieval Models for Stripped Binary Analysis](https://arxiv.org/abs/2512.10393)
*Guoqiang Chen,Lingyun Ying,Ziyang Song,Daguang Liu,Qiang Wang,Zhiqi Wang,Li Hu,Shaoyin Cheng,Weiming Zhang,Nenghai Yu*

Main category: cs.SE

TL;DR: BinSeek：首个两阶段跨模态检索框架，用于无符号二进制代码分析，通过嵌入模型和重排序模型实现二进制代码与自然语言描述的相关性匹配。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在二进制代码分析中面临挑战：从数千个无符号二进制函数中检索相关代码片段很困难，因为缺乏符号信息使其与源代码检索不同。

Method: 提出两阶段框架：1) BinSeekEmbedding在大规模数据集上训练，学习二进制代码与自然语言描述的语义相关性；2) BinSeek-Reranker通过上下文增强仔细判断候选代码与描述的相关性。使用LLM数据合成管道自动化训练数据构建。

Result: BinSeek取得最先进性能：在Rec@3上超越同规模模型31.42%，在MRR@3上超越27.17%，且领先参数规模大16倍的通用模型。

Conclusion: BinSeek是首个针对无符号二进制代码分析的两阶段跨模态检索框架，通过专门设计的嵌入和重排序模型显著提升了二进制代码检索性能，为未来研究提供了领域基准。

Abstract: LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.

</details>


### [8] [How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation](https://arxiv.org/abs/2512.10415)
*Devanshu Sahoo,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.SE

TL;DR: 首次大规模研究学术环境中基于LLM的代码评估器的越狱攻击，提出学术越狱概念，发布包含2.5万对抗性提交的数据集，评估6个LLM的脆弱性，发现说服性和角色扮演攻击成功率高达97%


<details>
  <summary>Details</summary>
Motivation: LLM作为代码评估自动评判器在学术环境中日益普及，但学生可能使用对抗性提示策略诱导误判以获取不当学术优势，需要研究这种学术越狱攻击的可靠性和影响

Method: 系统性地将20多种越狱策略适配到学术代码评估场景，定义学术越狱攻击类别；发布包含2.5万对抗性学生提交的毒化数据集；定义三个越狱度量指标（越狱成功率、分数膨胀、危害性）；使用6个LLM全面评估学术越狱攻击

Result: 发现这些模型表现出显著脆弱性，特别是对说服性和角色扮演攻击（越狱成功率高达97%），为下一代鲁棒的基于LLM的学术代码评估器奠定基础

Conclusion: 学术环境中LLM代码评估器存在严重安全漏洞，需要开发更鲁棒的评估系统，发布的对抗性数据集和基准套件为未来研究提供了重要基础

Abstract: The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.

</details>


### [9] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: UniCoR是一个自监督的统一代码表示学习框架，通过多视角对比学习和分布一致性学习，解决了混合代码检索中的语义理解不足、模态融合效率低和跨语言泛化弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码检索方法在混合查询（自然语言+代码片段）和跨语言场景中存在三个主要挑战：语义理解不足、模态融合效率低、跨语言泛化能力弱。

Method: 提出UniCoR框架：1）多视角监督对比学习模块，从代码-代码、自然语言-代码、自然语言-自然语言多个视角对齐表示；2）表示分布一致性学习模块，显式对齐不同编程语言的特征分布。

Result: 在基准测试中，UniCoR优于所有基线模型，MRR平均提升8.64%，MAP平均提升11.54%，在混合代码检索中表现稳定，在跨语言场景中具有良好泛化能力。

Conclusion: UniCoR通过统一表示学习有效解决了混合代码检索的挑战，为跨语言代码搜索提供了鲁棒的解决方案。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [10] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 该论文对LMSYS-Chat-1M和WildChat数据集中人类与LLM在编码协作中的交互模式进行了实证分析，研究了交互机制、指令遵循能力和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 尽管存在LMSYS-Chat-1M和WildChat等真实世界对话数据集，但很少有研究系统探索编码场景中人类与LLM协作的机制，包括用户交互路径、LLM指令遵循能力和用户满意度。

Method: 使用LMSYS-Chat-1M和WildChat数据集进行实证分析，探索人类-LLM协作机制、LLM指令遵循能力和人类满意度。

Result: 1) 任务类型塑造交互模式（线性、星形和树形）；2) 错误修复和代码重构对LLM指令遵循更具挑战性；3) 代码质量优化和需求驱动开发任务用户满意度较低，而结构化知识查询和算法设计满意度较高。

Conclusion: 研究结果为改进LLM界面和编码协作中的用户满意度提供了建议，同时为自适应对话系统的未来研究指明了方向，有助于更有效的AI辅助开发。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [11] [PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code](https://arxiv.org/abs/2512.10713)
*Itay Dreyfuss,Antonio Abu Nassar,Samuel Ackerman,Axel Ben David,Rami Katan,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: PACIFIC框架自动生成评估LLM代码助手顺序指令跟随和代码干运行能力的基准测试，通过控制难度生成变体，避免数据污染，有效区分不同模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法往往依赖工具使用或代理行为，难以隔离评估LLM内在的代码干运行（不执行代码的逐步推理）和指令跟随能力，且存在训练数据污染问题。

Method: 提出PACIFIC框架，自动生成具有明确定义期望输出的基准测试变体，控制难度级别，通过简单输出比较进行可靠评估，避免工具使用或代理行为干扰。

Result: 生成涵盖不同难度级别的基准测试套件，评估多个SOTA LLM，结果显示PACIFIC能产生越来越具挑战性的基准，有效区分不同模型在指令跟随和干运行方面的能力差异。

Conclusion: PACIFIC提供了一个可扩展、抗污染的方法论，用于评估LLM在代码相关任务中的核心能力，特别是顺序指令跟随和代码干运行能力。

Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [DynaMate: An Autonomous Agent for Protein-Ligand Molecular Dynamics Simulations](https://arxiv.org/abs/2512.10034)
*Salomé Guilbert,Cassandra Masschelein,Jeremy Goumaz,Bohdan Naida,Philippe Schwaller*

Main category: cs.AI

TL;DR: DynaMate是一个基于多智能体LLM的自动化框架，能够自主设计和执行蛋白质及蛋白质-配体复合物的完整分子动力学模拟工作流程，包括自由能结合亲和力计算。


<details>
  <summary>Details</summary>
Motivation: 尽管分子动力学模拟在药物发现和蛋白质工程中应用广泛，但其技术复杂性（参数化、输入准备、软件配置）成为广泛高效使用的主要障碍。目前尚未有成功的基于智能体LLM的自动化蛋白质-配体MD工作流程解决方案。

Method: DynaMate采用模块化多智能体框架，包含三个专门模块：实验规划、模拟执行和结果分析。框架集成了动态工具使用、网络搜索、PaperQA和自校正行为，能够自主设计并执行完整的MD工作流程，包括MM/PB(GB)SA方法的自由能结合亲和力计算。

Result: 在12个不同复杂度的基准系统上评估显示，DynaMate能够可靠地执行完整的MD模拟，通过迭代推理纠正运行时错误，并产生有意义的蛋白质-配体相互作用分析。

Conclusion: 该自动化框架为未来生物分子和药物设计应用中的标准化、可扩展和时间高效的分子建模流程铺平了道路。

Abstract: Force field-based molecular dynamics (MD) simulations are indispensable for probing the structure, dynamics, and functions of biomolecular systems, including proteins and protein-ligand complexes. Despite their broad utility in drug discovery and protein engineering, the technical complexity of MD setup, encompassing parameterization, input preparation, and software configuration, remains a major barrier for widespread and efficient usage. Agentic LLMs have demonstrated their capacity to autonomously execute multi-step scientific processes, and to date, they have not successfully been used to automate protein-ligand MD workflows. Here, we present DynaMate, a modular multi-agent framework that autonomously designs and executes complete MD workflows for both protein and protein-ligand systems, and offers free energy binding affinity calculations with the MM/PB(GB)SA method. The framework integrates dynamic tool use, web search, PaperQA, and a self-correcting behavior. DynaMate comprises three specialized modules, interacting to plan the experiment, perform the simulation, and analyze the results. We evaluated its performance across twelve benchmark systems of varying complexity, assessing success rate, efficiency, and adaptability. DynaMate reliably performed full MD simulations, corrected runtime errors through iterative reasoning, and produced meaningful analyses of protein-ligand interactions. This automated framework paves the way toward standardized, scalable, and time-efficient molecular modeling pipelines for future biomolecular and drug design applications.

</details>


### [13] [CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](https://arxiv.org/abs/2512.10206)
*Yakun Zhu,Zhongzhen Huang,Qianhan Feng,Linjie Mu,Yannian Gu,Shaoting Zhang,Qi Dou,Xiaofan Zhang*

Main category: cs.AI

TL;DR: CP-Env是一个可控的智能体医院环境，用于评估LLM在端到端临床路径中的表现，模拟医院生态系统，包含患者和医生智能体，支持从分诊到多学科会诊的复杂临床场景。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要关注静态考试或孤立对话，无法充分评估LLM在动态临床场景中的表现。医疗护理遵循复杂的临床路径，涉及多个阶段的决策和转换，需要更全面的评估框架。

Method: 开发CP-Env可控智能体医院环境，模拟医院生态系统，包含患者和医生智能体。构建从分诊、专科会诊到诊断测试和多学科团队会议的场景。采用三层评估框架：临床效能、流程能力和专业伦理。

Result: 大多数模型在路径复杂性方面表现不佳，出现幻觉并丢失关键诊断细节。有趣的是，过多的推理步骤有时会适得其反，而顶级模型倾向于通过内化知识减少工具依赖。

Conclusion: CP-Env通过全面的端到端临床评估推进医疗AI智能体发展。该基准测试和评估工具已开源，供进一步研究和开发使用。

Abstract: Medical care follows complex clinical pathways that extend beyond isolated physician-patient encounters, emphasizing decision-making and transitions between different stages. Current benchmarks focusing on static exams or isolated dialogues inadequately evaluate large language models (LLMs) in dynamic clinical scenarios. We introduce CP-Env, a controllable agentic hospital environment designed to evaluate LLMs across end-to-end clinical pathways. CP-Env simulates a hospital ecosystem with patient and physician agents, constructing scenarios ranging from triage and specialist consultation to diagnostic testing and multidisciplinary team meetings for agent interaction. Following real hospital adaptive flow of healthcare, it enables branching, long-horizon task execution. We propose a three-tiered evaluation framework encompassing Clinical Efficacy, Process Competency, and Professional Ethics. Results reveal that most models struggle with pathway complexity, exhibiting hallucinations and losing critical diagnostic details. Interestingly, excessive reasoning steps can sometimes prove counterproductive, while top models tend to exhibit reduced tool dependency through internalized knowledge. CP-Env advances medical AI agents development through comprehensive end-to-end clinical evaluation. We provide the benchmark and evaluation tools for further research and development at https://github.com/SPIRAL-MED/CP-Env.

</details>


### [14] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 本文提出了"可信编排AI的十大标准"框架，通过控制面板架构将治理嵌入AI生态系统执行层面，确保AI系统可验证、透明、可重现且受有意义的人类控制。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在关键决策中扮演越来越重要的角色，技术能力与制度问责之间出现了日益扩大的差距。仅靠伦理指导不足以应对这一挑战，需要将治理嵌入生态系统执行层面的架构。

Method: 提出了可信编排AI的十大标准框架，采用控制面板架构，整合人类输入、语义一致性、审计和溯源完整性。该框架为整个AI组件、其消费者和人类参与者提供治理保障，借鉴国际标准和澳大利亚国家AI保障框架。

Result: 展示了可信性可以通过工程方法系统地融入AI系统，确保执行层面保持可验证、透明、可重现且受有意义的人类控制。

Conclusion: 该框架为解决AI系统治理与问责问题提供了系统性工程方法，通过将治理嵌入执行架构，弥合了技术能力与制度问责之间的差距。

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [15] [AgentProg: Empowering Long-Horizon GUI Agents with Program-Guided Context Management](https://arxiv.org/abs/2512.10371)
*Shizuo Tian,Hao Wen,Yuxuan Chen,Jiacheng Liu,Shanhui Zhao,Guohong Liu,Ju Ren,Yunxin Liu,Yuanchun Li*

Main category: cs.AI

TL;DR: AgentProg：一种程序引导的移动GUI代理上下文管理方法，通过将交互历史重构为带变量和控制流的程序来减少上下文开销，在长时程任务中保持高性能。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理在长时程任务自动化中面临关键瓶颈：依赖不断扩展的交互历史会导致大量上下文开销，现有上下文管理和压缩技术往往无法保留重要语义信息，导致任务性能下降。

Method: 提出AgentProg，一种程序引导的代理上下文管理方法：1) 将交互历史重构为带有变量和控制流的程序；2) 基于程序结构确定哪些信息应保留或丢弃；3) 集成受信念MDP框架启发的全局信念状态机制来处理部分可观测性和适应环境变化。

Result: 在AndroidWorld和扩展的长时程任务套件上的实验表明，AgentProg在这些基准测试中达到了最先进的成功率。更重要的是，它在长时程任务中保持稳健性能，而基线方法则出现灾难性性能下降。

Conclusion: AgentProg通过程序化重构交互历史有效解决了移动GUI代理的上下文管理问题，在长时程任务中实现了高性能和鲁棒性，为移动自动化任务提供了新的解决方案。

Abstract: The rapid development of mobile GUI agents has stimulated growing research interest in long-horizon task automation. However, building agents for these tasks faces a critical bottleneck: the reliance on ever-expanding interaction history incurs substantial context overhead. Existing context management and compression techniques often fail to preserve vital semantic information, leading to degraded task performance. We propose AgentProg, a program-guided approach for agent context management that reframes the interaction history as a program with variables and control flow. By organizing information according to the structure of program, this structure provides a principled mechanism to determine which information should be retained and which can be discarded. We further integrate a global belief state mechanism inspired by Belief MDP framework to handle partial observability and adapt to unexpected environmental changes. Experiments on AndroidWorld and our extended long-horizon task suite demonstrate that AgentProg has achieved the state-of-the-art success rates on these benchmarks. More importantly, it maintains robust performance on long-horizon tasks while baseline methods experience catastrophic degradation. Our system is open-sourced at https://github.com/MobileLLM/AgentProg.

</details>


### [16] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出SaEI方法，通过选择性对抗熵干预增强视觉语言模型的推理能力，在RL采样阶段引入熵干预来提升响应多样性


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的微调方法通常只在策略优化阶段对特定token进行熵干预，忽略了在RL采样阶段的熵干预可以提升响应多样性，从而改善GRPO性能

Method: 提出选择性对抗熵干预(SaEI)：1) 熵引导对抗采样(EgAS)，将采样响应的熵作为对抗目标，生成对抗样本来扩大答案空间探索；2) token选择性熵计算(TsEC)，在不扭曲事实知识的前提下最大化对抗攻击效果

Result: 在领域内和领域外数据集上的广泛实验表明，该方法能通过熵干预显著提升策略探索能力，从而增强推理能力

Conclusion: 在RL采样阶段引入选择性对抗熵干预能有效提升视觉语言模型的推理能力，通过增强响应多样性来改善策略探索

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [17] [Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation](https://arxiv.org/abs/2512.10501)
*Lim Chien Her,Ming Yan,Yunshu Bai,Ruihao Li,Hao Zhang*

Main category: cs.AI

TL;DR: 提出一种无需训练的架构，利用LLM代理进行零样本PCG参数配置，通过Actor-Critic双代理迭代工作流，将抽象用户指令转化为精确参数规范


<details>
  <summary>Details</summary>
Motivation: PCG需要精确配置不透明的技术参数，而现成的LLM难以弥合抽象用户指令与严格参数规范之间的语义鸿沟，需要更好的控制方法

Method: 采用Actor-Critic双代理架构，Actor负责生成参数配置，Critic进行评估和反馈，形成迭代工作流，自主推理工具参数并逐步优化配置以符合人类设计偏好

Result: 在3D地图生成任务上验证，优于单代理基线，能从自然语言描述生成多样且结构有效的环境，建立了PCG指令跟随的新基准

Conclusion: 现成LLM可有效重新用作通用PCG工具代理，通过将负担从模型训练转向架构推理，为无需任务特定微调而掌握复杂软件提供了可扩展框架

Abstract: Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.

</details>


### [18] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: InternGeometry是一个基于LLM的几何问题求解智能体，通过迭代提出命题和辅助构造、符号引擎验证和反馈反思，结合动态内存机制和复杂度提升强化学习，仅用少量训练数据就在IMO几何问题上达到金牌水平。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在几何问题求解方面受限于辅助构造的启发式能力不足，而专家模型如AlphaGeometry 2需要大规模数据合成和搜索。本文旨在构建一个达到金牌水平的LLM几何智能体，克服几何问题中的启发式限制。

Method: 1. 迭代式问题求解：提出命题和辅助构造，用符号引擎验证，根据反馈指导后续提案；2. 动态内存机制：支持每个问题与符号引擎进行200多次交互；3. 复杂度提升强化学习：在训练阶段逐渐增加合成问题的复杂度；4. 基于InternThinker-32B模型构建。

Result: 在2000-2024年的50个IMO几何问题中解决了44个，超过金牌选手平均分（40.9），仅使用13K训练样本（AlphaGeometry 2数据量的0.004%）。还能为IMO问题提出人类解法中未出现的新颖辅助构造。

Conclusion: InternGeometry展示了LLM智能体在专家级几何任务上的潜力，通过创新的迭代求解框架和高效训练方法，用极少数据达到金牌水平，为几何AI研究提供了新方向。

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [19] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode是一种半形式化语言，用于构建推理计划，通过严格分离语义操作和语法操作，消除多步LLM工作流中的上下文污染问题。


<details>
  <summary>Details</summary>
Motivation: 多步LLM工作流存在上下文污染问题：随着信息在步骤间积累，模型会产生幻觉、混淆中间输出、丢失任务约束。需要一种方法来消除跨步骤污染，实现可审计的AI工作流。

Method: NormCode是一种半形式化语言，包含三种同构格式：.ncds用于人工编写，.ncd用于机器执行，.ncn用于人工验证。它严格分离语义操作（LLM驱动的推理，非确定性）和语法操作（确定性数据重组），支持从草图到生产的渐进形式化。

Result: 通过两个演示验证：1）基础X加法算法在任意长度输入上实现100%准确率；2）自托管执行NormCode自身的五阶段编译器流水线。工作编排器提供依赖驱动调度、SQLite支持的检查点和循环管理。

Conclusion: NormCode通过设计消除跨步骤污染，使AI工作流可审计，满足法律推理、医疗决策和金融分析等高风险领域对透明度的关键需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [20] [Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution](https://arxiv.org/abs/2512.10696)
*Zouying Cao,Jiaji Deng,Li Yu,Weikang Zhou,Zhaoyang Liu,Bolin Ding,Hai Zhao*

Main category: cs.AI

TL;DR: ReMe是一个记忆驱动的智能体进化框架，通过多维度蒸馏、上下文自适应重用和基于效用的精炼机制，实现从静态存储到动态推理的转变，在BFCL-V3和AppWorld基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的程序性记忆框架主要采用"被动积累"范式，将记忆视为静态的只追加档案，缺乏动态推理能力。需要弥合静态存储与动态推理之间的差距，实现经验驱动的智能体进化。

Method: 提出ReMe框架，包含三个创新机制：1) 多维度蒸馏：通过识别成功模式、分析失败触发因素和生成比较性见解来提取细粒度经验；2) 上下文自适应重用：通过场景感知索引将历史见解适配到新上下文；3) 基于效用的精炼：自主添加有效记忆并修剪过时记忆，维护紧凑高质量的经验池。

Result: 在BFCL-V3和AppWorld基准测试中，ReMe建立了智能体记忆系统的新SOTA。关键发现：配备ReMe的Qwen3-8B超越了更大的无记忆Qwen3-14B，显示出显著的内存缩放效应，表明自进化记忆为终身学习提供了计算高效的途径。

Conclusion: ReMe通过动态记忆生命周期管理，实现了从被动积累到主动进化的转变，为LLM智能体提供了高效的经验学习和终身学习能力。释放的代码和reme.library数据集将促进进一步研究。

Abstract: Procedural memory enables large language model (LLM) agents to internalize "how-to" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a "passive accumulation" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\textbf{ReMe}$ ($\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\texttt{reme.library}$ dataset to facilitate further research.

</details>


### [21] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 该研究探讨了基于大语言模型的多智能体系统中价值多样性如何影响集体行为，发现价值多样性增强了价值稳定性、促进了涌现行为，但极端异质性会导致不稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的多智能体系统日益普及，这些人工社区的集体行为（如集体智能）受到越来越多的关注。本研究旨在回答一个基本问题：价值多样性如何塑造AI社区的集体行为？

Method: 使用基于施瓦茨基本人类价值理论的自然主义价值启发方法，构建了多智能体模拟，让不同规模的社区参与开放式互动和宪法制定。

Result: 结果显示：价值多样性增强了价值稳定性，促进了涌现行为，并带来了更多由智能体自身开发（无需外部指导）的创造性原则。但这些效应也呈现边际递减：极端异质性会引发不稳定性。

Conclusion: 该研究将价值多样性定位为未来AI能力的新维度，连接了AI能力与社会学中的制度涌现研究。

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [22] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 该研究探讨了LLM安全评估的新方向：从通用风险评估转向考虑用户具体情境的个人福利评估，特别是在金融和健康建议领域，发现传统评估方法会高估安全性，而用户自我披露的上下文信息不足以弥补评估差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要关注通用风险（如危险能力），但数百万用户在实际使用LLM获取金融和健康等高风险建议时，危害是情境依赖的而非通用的。虽然OECD等框架认识到需要评估个人风险，但用户福利安全评估仍不成熟，且如何将用户情境纳入评估设计存在根本性挑战。

Method: 探索性研究：1）评估GPT-5、Claude Sonnet 4和Gemini 2.5 Pro在金融和健康领域的建议；2）使用不同脆弱程度的用户档案；3）比较有无用户情境信息的评估者评分；4）测试用户自我披露的上下文信息是否能改善评估效果。

Result: 1）情境盲评估者比了解用户情境的评估者显著高估安全性（高脆弱用户安全评分从3/7上升到5/7）；2）即使用户在提示中披露自认为关键的上下文信息，评估效果也无显著改善；3）有效的用户福利评估需要评估者基于多样化用户档案评估响应。

Conclusion: 用户福利安全评估需要与现有通用风险评估框架不同的方法，必须让评估者评估响应与多样化用户档案的匹配度，仅靠用户自我披露的上下文信息不足，特别是对脆弱人群。研究提供了情境感知评估的方法论起点。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [23] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: V-OCBF：一种从离线演示中学习神经控制屏障函数的框架，无需系统动力学模型，通过递归有限差分屏障更新实现模型无关的安全学习，结合期望分位数目标避免分布外动作查询，最终通过二次规划合成实时安全控制。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法通常强制执行软期望成本约束，无法保证前向不变性；而控制屏障函数（CBFs）虽然提供严格安全保证，但通常依赖专家设计的屏障函数或完整的系统动力学知识。需要一种能够从离线数据中学习安全控制器的方法，既不需要在线交互，也不需要手动设计的屏障函数。

Method: 提出V-OCBF框架：1）从离线演示中学习神经CBF，无需动力学模型；2）推导递归有限差分屏障更新，实现模型无关的安全信息时间传播；3）采用期望分位数目标，避免在分布外动作上查询屏障函数，并将更新限制在数据集支持的动作集内；4）使用学习的屏障函数通过二次规划（QP）合成实时安全控制。

Result: 在多个案例研究中，V-OCBF相比基线方法显著减少了安全违规次数，同时保持了强大的任务性能，展示了其在无需在线交互或手工设计屏障的情况下离线合成安全关键控制器的可扩展性。

Conclusion: V-OCBF为自主系统提供了一种有效的离线安全控制器合成方法，通过学习神经CBF实现了严格的安全保证，无需系统动力学模型或专家设计的屏障函数，在安全性和任务性能之间取得了良好平衡。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>


### [24] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了部分可观测马尔可夫决策过程（POMDP）中的决策智能体与单输入过程函数（高阶量子操作的经典极限）之间的精确对应关系


<details>
  <summary>Details</summary>
Motivation: 建立人工智能决策理论与量子信息理论之间的桥梁，通过过程函数框架统一描述智能体与环境之间的交互关系

Method: 将智能体的策略和记忆更新结合成过程函数w，通过链接积与POMDP环境交互；建立物理视图（过程函数作为环境）与AI视图（过程函数编码智能体）的对偶解释；扩展到多智能体系统，将观测无关的分散式POMDP识别为多输入过程函数的自然领域

Result: 建立了POMDP智能体与过程函数之间的精确对应关系，提供了智能体-环境交互的统一数学框架，并扩展到多智能体系统

Conclusion: 该对应关系为理解智能体决策提供了新的理论视角，连接了人工智能和量子信息理论，为多智能体系统的形式化分析奠定了基础

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [25] [国防科技大学“天河”高性能计算机系统结构研究团队 | 深度<em class="highlight">强化学习</em>和负载中心性理论融合的分段路由优化算法](http://mp.weixin.qq.com/s?__biz=MzAwMjE0NDA5Mg==&mid=2650030709&idx=4&sn=deee01d1cce736a19eb4fc9ed54c6309&chksm=83df50c3ecc5137e35f24acf2f8253e5393780f4a995005f7e3f20f1ca1c5ec3f2016c296ae1#rd)
*国防科技大学学报*

Main category: wechat.article

TL;DR: 利用多智能体强化学习框架，在关键节点部署分布式深度强化学习智能体，通过共享奖励机制协调路由决策，实现链路负载的主动优化。同时结合SR的灵活性，动态调整段标识列表快速重路由部分流量，降低本地链路利用率并规


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 利用多智能体强化学习框架，在关键节点部署分布式深度强化学习智能体，通过共享奖励机制协调路由决策，实现链路负载的主动优化。同时结合SR的灵活性，动态调整段标识列表快速重路由部分流量，降低本地链路利用率并规

</details>


### [26] [一文讲清楚:机器学习、深度学习、<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzE5ODk0NTY4Ng==&mid=2247483696&idx=1&sn=9fe930ce495b7c8f21d0016554dbc833&chksm=970f7e095e5cf5746ad5ddb225c701c20fb81e5c910876748707191d79c50e5051ab04d5814c#rd)
*闲闲爱诗书*

Main category: wechat.article

TL;DR: 强化学习：不是从数据里学，而是靠“试错 + 奖励”学会策略 一句话总结关系：深度学习是机器学习的一种方法，强化学习是机器学习的一种范式，它可以结合深度学习一起用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习：不是从数据里学，而是靠“试错 + 奖励”学会策略 一句话总结关系：深度学习是机器学习的一种方法，强化学习是机器学习的一种范式，它可以结合深度学习一起用。

</details>


### [27] [多模态大模型结合<em class="highlight">强化学习</em>的模式探讨](http://mp.weixin.qq.com/s?__biz=Mzk0NjM1MTgxNg==&mid=2247490473&idx=1&sn=4c8d08887c46309bbdaa500a9f17dbe2&chksm=c2c9d6e49079a1b3d70685d1439164f343c9db0a55a3b2277ebbf0a55953ebb27cff3178476e#rd)
*亚信科技新技术探索*

Main category: wechat.article

TL;DR: 深度强化学习将深度学习与强化学习相结合，通过神经网络近似价值函数或策略函数，解决了传统强化学习在高维状态空间中的维度灾难问题。二 核心方法与技术路径


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 深度强化学习将深度学习与强化学习相结合，通过神经网络近似价值函数或策略函数，解决了传统强化学习在高维状态空间中的维度灾难问题。二 核心方法与技术路径

</details>


### [28] [研究进展：忆阻器-类脑<em class="highlight">强化学习</em> | Nature Machine Intelligence](http://mp.weixin.qq.com/s?__biz=MzkwMTEzMjE5OQ==&mid=2247669742&idx=5&sn=655f7740e4307c6c58730f1ed89fab61&chksm=c1dbbcbc1cf7db9a9fb83887b0f013365673bf79a2e2bd4c65a3c3f19916296158480f61091c#rd)
*今日新材料*

Main category: wechat.article

TL;DR: 图1-类脑强化学习架构与任务示意图图2-模拟忆阻器的制备与电学特性图3-内存学习循环与误差校正机制图4-T迷宫导航任务的硬件实现结果图5-莫里斯水迷宫任务的仿真扩展


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 图1-类脑强化学习架构与任务示意图图2-模拟忆阻器的制备与电学特性图3-内存学习循环与误差校正机制图4-T迷宫导航任务的硬件实现结果图5-莫里斯水迷宫任务的仿真扩展

</details>


### [29] [全球<em class="highlight">强化学习</em>+VLA范式，PI*0.6背后都有这家中国公司技术伏笔](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651006802&idx=1&sn=e3c63a11065446d8df4ba386229f7a6d&chksm=851e48891550b96e2fd6e0cad212a7fc8125024ff965c8109ddde2d85e2acd281c89812e4cf7#rd)
*机器之心*

Main category: wechat.article

TL;DR: 第一阶段：在线强化学习（探索与发现）图注：稳定探索在这个阶段，机器人的目标是去试错，探索如何完成新任务。冻结大脑（Freeze VLM）：为了防止模型崩溃和减少计算量，作者冻结了巨大的 VLM 主干参数。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 第一阶段：在线强化学习（探索与发现）图注：稳定探索在这个阶段，机器人的目标是去试错，探索如何完成新任务。冻结大脑（Freeze VLM）：为了防止模型崩溃和减少计算量，作者冻结了巨大的 VLM 主干参数。

</details>


### [30] [【专题】AI领域中的“<em class="highlight">强化学习</em>”相关研究-2025年11-12月](http://mp.weixin.qq.com/s?__biz=Mzk0NzY3ODMxMA==&mid=2247488841&idx=1&sn=1529aeae908be13175f03b5750ef7979&chksm=c2f86a9aa2539826be9c79d1b347aedfc132c0a7ea4e1821dfe50757342a4a23aef87fbb98be#rd)
*AI新文*

Main category: wechat.article

TL;DR: 动态环境下的基于信号激励的多智能体强化学习 原标题：Signaling-Driven Incentive Communication for Enhanced Multiagent Reinforcement Learning in Dynamic Environments作者：Kexing Peng；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 动态环境下的基于信号激励的多智能体强化学习 原标题：Signaling-Driven Incentive Communication for Enhanced Multiagent Reinforcement Learning in Dynamic Environments作者：Kexing Peng；

</details>


### [31] [2025 re:Invent ：亚马逊云科技把<em class="highlight">Agentic</em> AI生态梳理明白了](http://mp.weixin.qq.com/s?__biz=MzAxODMwOTc4OQ==&mid=2650748298&idx=1&sn=0e614637a06af9bf35cae595369d48e2&chksm=828171253ea1e4e9b6494b46c47e1fa395fba263877e5d9ed2aeca5e2c7fd6c5cae25e78312c#rd)
*至顶头条*

Main category: wechat.article

TL;DR: 亚马逊云科技看到Agent的出现让企业的投资开始看到成果，未来每个企业都将会有很多Agent，处理更多的事情，每个企业都可以从AI中得到更多价值，所以亚马逊云科技对人工智能基础设施、推理平台、企业数据、构建Agent的工具


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 亚马逊云科技看到Agent的出现让企业的投资开始看到成果，未来每个企业都将会有很多Agent，处理更多的事情，每个企业都可以从AI中得到更多价值，所以亚马逊云科技对人工智能基础设施、推理平台、企业数据、构建Agent的工具

</details>


### [32] [美国FDA推出新一代<em class="highlight">Agentic</em> AI工具，聚焦复杂工作流程优化！](http://mp.weixin.qq.com/s?__biz=MzUzMjkzNTI5OQ==&mid=2247540984&idx=1&sn=dd6dabfcdfd3335b07b0378ac80113cf&chksm=fbc660b1110c137eaac773c14814fc823f5923b26aa8efa6069a204128420697afee4b06580f#rd)
*艾邦新消费电子资讯*

Main category: wechat.article

TL;DR: Agentic AI 指一种能够通过规划、推理和执行多步骤行动来实现特定目标的先进人工智能系统。这类系统内置指导机制——包括人工监督——以确保结果可靠。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 指一种能够通过规划、推理和执行多步骤行动来实现特定目标的先进人工智能系统。这类系统内置指导机制——包括人工监督——以确保结果可靠。

</details>


### [33] [ICAEW洞察 | <em class="highlight">Agentic</em> AI崛起：职场效率革命背后的网络安全“暗战”](http://mp.weixin.qq.com/s?__biz=MzkyODYzOTM5OQ==&mid=2247500058&idx=1&sn=c5f75a473ca017b2dad6be8b2baf1ed2&chksm=c3120d51b8196e5ee3b02e59b17a5570b300bc83d24797f6e605470e27d341434763aab7b66c#rd)
*ICAEW特许会计师 ACA*

Main category: wechat.article

TL;DR: Agentic AI则指这些工具的运行框架，或是统筹多个智能体协同工作、解决复杂精密任务的技术体系。安永（EY）英国技术风险总监Alistair Grange指出：“尽管AI技术在职场中潜力巨大，但它也降低了网络犯罪的准入门槛。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI则指这些工具的运行框架，或是统筹多个智能体协同工作、解决复杂精密任务的技术体系。安永（EY）英国技术风险总监Alistair Grange指出：“尽管AI技术在职场中潜力巨大，但它也降低了网络犯罪的准入门槛。

</details>


### [34] [告别工具属性，拥抱价值创造：<em class="highlight">Agentic</em> AI的商业革命与落地实践](http://mp.weixin.qq.com/s?__biz=Mzk4ODI0NTI1Mw==&mid=2247484151&idx=1&sn=c834e03e3f4db1a1fcb232df9e29d720&chksm=c4da712bb95509823539788d33430ec31c6aea3ec6aa418a77836af37457c8e6a5b95a7a983a#rd)
*海联云科技*

Main category: wechat.article

TL;DR: Agentic AI 让服务从 “被动响应” 转向 “主动预判”。Druva 通过 Amazon Bedrock AgentCore 构建的智能体，自主解决63% 的客户支持问题，响应速度提升58%，这种价值不仅体现在人力成本降低，更在于核心服务质量的突破性提升。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 让服务从 “被动响应” 转向 “主动预判”。Druva 通过 Amazon Bedrock AgentCore 构建的智能体，自主解决63% 的客户支持问题，响应速度提升58%，这种价值不仅体现在人力成本降低，更在于核心服务质量的突破性提升。

</details>


### [35] [如何理解 <em class="highlight">Agentic</em> AI、LLM格局](http://mp.weixin.qq.com/s?__biz=Mzg2MjE4NDMxNA==&mid=2247483819&idx=1&sn=3cefd915b844cb0c6f520f20de7f5347&chksm=cfa44af29ac7e3e60371401106cfd0d696b7a8da376bfb2264a38dc9e46fd910323ad12884a9#rd)
*对弈大模型*

Main category: wechat.article

TL;DR: 2、Agentic AI是LLM的一个方向，其首要目标不是追求智商，而是执行工作任务，追求执行任务的具体能力水平，如编程作图做视频等等。典型的如Gemini3。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2、Agentic AI是LLM的一个方向，其首要目标不是追求智商，而是执行工作任务，追求执行任务的具体能力水平，如编程作图做视频等等。典型的如Gemini3。

</details>


### [36] [汽车金融首个开源<em class="highlight">Agentic</em>大模型发布，行业智能化进入“快车道”](http://mp.weixin.qq.com/s?__biz=MzYzNzA2OTc3OA==&mid=2247483718&idx=1&sn=25d83cd128cff4b26bdb787694f05e99&chksm=f1c18b8c98e6c56d046fd7be5bc4f1e36c6920f92ed7ef45b8c6dc4089d729991711bda33274#rd)
*汽车AI聊不停*

Main category: wechat.article

TL;DR: 从闭源到开源，从独享到共享，开源Agentic大模型之举，不仅仅是一次技术发布，更是一次发展理念的升级。当技术壁垒被打破，竞争的核心将更聚焦于业务场景的深度理解和模式的创新。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从闭源到开源，从独享到共享，开源Agentic大模型之举，不仅仅是一次技术发布，更是一次发展理念的升级。当技术壁垒被打破，竞争的核心将更聚焦于业务场景的深度理解和模式的创新。

</details>


### [37] [AWS re:Invent 2025：<em class="highlight">Agentic</em> AI 彻底爆发，大模型“独立”时代已来](http://mp.weixin.qq.com/s?__biz=MzAxNzU2MTg5OQ==&mid=2648604824&idx=1&sn=b436c52c8b678152b4fe8db23c4f79eb&chksm=82fe7235bb4f0cb14ce698d9e5e5751973b9378b638b5253baa71e4a404faec19e6b435efe2a#rd)
*擎翌智能*

Main category: wechat.article

TL;DR: 01Agentic AI从“说”到“做”的跨越在 re：Invent 2025 的舞台上，最核心的关键词莫过于 "Agentic Workflows"（代理工作流）。过去我们使用大模型，是“输入 Prompt -> 得到文本”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 01Agentic AI从“说”到“做”的跨越在 re：Invent 2025 的舞台上，最核心的关键词莫过于 "Agentic Workflows"（代理工作流）。过去我们使用大模型，是“输入 Prompt -> 得到文本”。

</details>


### [38] [<em class="highlight">Agentic</em> AI学会自主行动：探秘NVIDIA如何用微服务架构重塑生成式AI生态](http://mp.weixin.qq.com/s?__biz=MzE5MTIyNzU5Mg==&mid=2247484775&idx=1&sn=084f3378eb28f1c07a8b44ae39b98d41&chksm=97c434598e71ad7ea4f67068212052cf50a18aa3a9b1c6d4af3f58fe159956bcb1fa323eebe8#rd)
*潮汕炜佳导导*

Main category: wechat.article

TL;DR: 简单来说，Agentic AI指的是具备自主性、能够理解复杂指令、制定分步计划并执行任务的AI系统。不同于传统 chatbot 的一问一答模式，这些AI智能体更像是一位得力的数字助手——你只需给出一个目标，它就能自己思考如何实现。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 简单来说，Agentic AI指的是具备自主性、能够理解复杂指令、制定分步计划并执行任务的AI系统。不同于传统 chatbot 的一问一答模式，这些AI智能体更像是一位得力的数字助手——你只需给出一个目标，它就能自己思考如何实现。

</details>


### [39] [<em class="highlight">Agentic</em> OS 中的主流应用形态](http://mp.weixin.qq.com/s?__biz=MzAwMTc2NTI2MA==&mid=2247483684&idx=1&sn=254a6968bf38c78dc79211a974eebce2&chksm=9b55224aed1596b64d4d5e0f4227b4aa6397174c03855cebf296865e197b411b3a5632424303#rd)
*敏法工房*

Main category: wechat.article

TL;DR: 上次写了Agentic AI注定让超级应用unbundling，最近随着OpenAI、Anthropic、Shopify在MCP-UI、MCP App上的开放合作，以及由此发展出的以MCP为基础的「Agentic AI基金会」（AAIF）纳入了主要AI Lab和Infra厂商（让我想起之前USD/Khronos和GLTF/W3C强强联


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 上次写了Agentic AI注定让超级应用unbundling，最近随着OpenAI、Anthropic、Shopify在MCP-UI、MCP App上的开放合作，以及由此发展出的以MCP为基础的「Agentic AI基金会」（AAIF）纳入了主要AI Lab和Infra厂商（让我想起之前USD/Khronos和GLTF/W3C强强联

</details>


### [40] [<em class="highlight">Agentic</em>设计模式（4）：反思（Reflection）](http://mp.weixin.qq.com/s?__biz=MzkyMDYxNDA5Nw==&mid=2247484833&idx=1&sn=39337364bb4838515e81f5ff2d3df5c1&chksm=c0f2aa8de88116bf3d9b847512f8c96cf0b271f15ff62afb4af4ce082ed32a160ed0521ed319#rd)
*叶行宽*

Main category: wechat.article

TL;DR: 三人行必有我师这种，善假于物的反思，类比到智能体反思，其典型实现如下：将流程拆分为两个独立的逻辑角色：生产者（Producer）和评论者（Critic）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 三人行必有我师这种，善假于物的反思，类比到智能体反思，其典型实现如下：将流程拆分为两个独立的逻辑角色：生产者（Producer）和评论者（Critic）。

</details>


### [41] [喜报！吉林交通<em class="highlight">大模型</em>智能体获全国大赛一等奖](http://mp.weixin.qq.com/s?__biz=MzkxODQxOTEyNg==&mid=2247792253&idx=2&sn=3abe193e7dd871b2d4f9629a262ee089&chksm=c0eef7a29906ca6d990740ed31039dc4e453d215268fa608857ad471b635cc4352e6b5c36ba8#rd)
*中国吉林网*

Main category: wechat.article

TL;DR: 交通运输部职业资格中心主办的第一届综合交通运输大模型智能体创新应用大赛全国总决赛在厦门落幕。大赛以“智领交通，慧见未来”为主题，涵盖政务与管理、安全与监管、运营与服务、技术与创新等方向，来自政务、铁路


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 交通运输部职业资格中心主办的第一届综合交通运输大模型智能体创新应用大赛全国总决赛在厦门落幕。大赛以“智领交通，慧见未来”为主题，涵盖政务与管理、安全与监管、运营与服务、技术与创新等方向，来自政务、铁路

</details>


### [42] [行业热点资讯｜OpenAI：正式推出专业知识工作<em class="highlight">大模型</em>GPT-5.2](http://mp.weixin.qq.com/s?__biz=MzkzNzIwNTA1Nw==&mid=2247540365&idx=3&sn=6e4a1adf91443fa1fb55e752515ae9a8&chksm=c3fb43a8235dd7db6e95b609a6c7fcbde1529d91ab1e0d7a18428128fa606edc944ad96a26cd#rd)
*投资东莞*

Main category: wechat.article

TL;DR: 工作大模型GPT-5.2当地时间12月11日，OpenAI正式发布其最新模型GPT-5.2，这是在谷歌Gemini3强势挑战下的一次全面回击。新模型GPT-5.2聚焦专业工作场景优化，在编程、科学任务、长文档处理等核心能力上实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 工作大模型GPT-5.2当地时间12月11日，OpenAI正式发布其最新模型GPT-5.2，这是在谷歌Gemini3强势挑战下的一次全面回击。新模型GPT-5.2聚焦专业工作场景优化，在编程、科学任务、长文档处理等核心能力上实现显著提升。

</details>


### [43] [大语言<em class="highlight">模型</em>：理解与生成语言的人工智能](http://mp.weixin.qq.com/s?__biz=MzAxODk3NjQ5OQ==&mid=2247484075&idx=1&sn=eabc1d31bedad5ffb9e05904f017101f&chksm=9aa0d6689ee67c571fbcf15d6c87e7871508d0e9e786a11ebbc5f9eff74fdae4a766b9cffcb8#rd)
*矿山百通-矿山领域的AI专家*

Main category: wechat.article

TL;DR: 随着技术的进步，未来的大语言模型可能会变得更加智能，能够处理更复杂的任务，理解更深层次的含义。结合多模态学习和强化学习，它们有望在更多应用场景中提供解决方案，甚至在模拟人类认知和决策方面发挥更大作用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 随着技术的进步，未来的大语言模型可能会变得更加智能，能够处理更复杂的任务，理解更深层次的含义。结合多模态学习和强化学习，它们有望在更多应用场景中提供解决方案，甚至在模拟人类认知和决策方面发挥更大作用。

</details>


### [44] [荐读 | 基于多模态<em class="highlight">大模型</em>的具身智能体研究进展与展望，附下载](http://mp.weixin.qq.com/s?__biz=MzI3NjQ3NjUxMA==&mid=2247518760&idx=1&sn=0975d1847a98a26de9f0c686541830cc&chksm=eaa3b71feb09abd9c2b00c21f18afdd153fc70ac48b5038e225fb19d98e55953325d3a436f6b#rd)
*具身智能产业洞察*

Main category: wechat.article

TL;DR: 即可下载《基于多模态大模型的具身智能体研究进展与展望》完整版PDF。以下是部分内容，敬请阅读· 未完 ·


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 即可下载《基于多模态大模型的具身智能体研究进展与展望》完整版PDF。以下是部分内容，敬请阅读· 未完 ·

</details>


### [45] [“智”变中的“信”守：探寻软件工程<em class="highlight">大模型</em>的可信变革之路 | ChinaData](http://mp.weixin.qq.com/s?__biz=MjM5MTY5ODE4OQ==&mid=2651618725&idx=4&sn=a8debb1b7194cf04ec03d17c9455cc5b&chksm=bcbd5c54628b369866a6dd3a88be5a664737c91b7e3e277f8a60ead056964a922b1299fadde6#rd)
*中国计算机学会*

Main category: wechat.article

TL;DR: 大模型正在重塑软件工程的全生命周期，从需求分析、代码生成到自动化测试，大语言模型展现了显著的效率红利，已成为推动软件生态建设与技术突破的关键力量。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型正在重塑软件工程的全生命周期，从需求分析、代码生成到自动化测试，大语言模型展现了显著的效率红利，已成为推动软件生态建设与技术突破的关键力量。

</details>


### [46] [从零基础到企业级应用：22 天吃透 AI <em class="highlight">大模型</em>应用，转岗 IT 不踩坑](http://mp.weixin.qq.com/s?__biz=MzU0OTg2ODc4Nw==&mid=2247594660&idx=1&sn=8a9586a1157e09386d9e0e6ca5075c5b&chksm=fa85a5dfc5671e363988f7c9f12e23af787a4ae3599e9e114d82d097b8b2cfd91f16cd6228ab#rd)
*国信安教育*

Main category: wechat.article

TL;DR: 全新AI大模型课程，为你量身打造02立足转岗人群的学习特性，课程采用三阶递进式设计，让零基础也能稳步成长：学完可胜任 AI 应用开发、RAG 系统工程师、AI 产品经理等多个热门岗位，覆盖全产业链就业需求。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全新AI大模型课程，为你量身打造02立足转岗人群的学习特性，课程采用三阶递进式设计，让零基础也能稳步成长：学完可胜任 AI 应用开发、RAG 系统工程师、AI 产品经理等多个热门岗位，覆盖全产业链就业需求。

</details>


### [47] [中国工程院院士谭建荣：千万不要忽视小模型！<em class="highlight">大模型</em>的根基在小模型和建模能力！智能体模型不能只停留在端侧！](http://mp.weixin.qq.com/s?__biz=Mzk0NjcxMDM1Nw==&mid=2247495062&idx=3&sn=237c6df6ca31748d4cec747aac06d093&chksm=c2046c61d772fc512565e81bd4f87e8edc8abec0073fe474605256a6be8746e6263d3c5f79c0#rd)
*智能感知工程*

Main category: wechat.article

TL;DR: 谭院士三十年前就开始做数据建模，而大模型之所以崛起，是因为长期的数据积累与建模工具链的成熟。“没有小模型，哪里来大模型？” 同时，谭院士，还给出了大模型的发展方向：从“越大越好”走向“精准小模型＋行业


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 谭院士三十年前就开始做数据建模，而大模型之所以崛起，是因为长期的数据积累与建模工具链的成熟。“没有小模型，哪里来大模型？” 同时，谭院士，还给出了大模型的发展方向：从“越大越好”走向“精准小模型＋行业

</details>


### [48] [AI浪潮之下，国产<em class="highlight">大模型</em>赶超了吗？](http://mp.weixin.qq.com/s?__biz=MzI1NjUzMjM1Mw==&mid=2247493656&idx=1&sn=685063aa8b4d554fff747e61a1599cef&chksm=eba6ce08205a6ffa710e72dcccd652f0de8b08ad4b3f325124e290e7133a2bfb222978ca82dd#rd)
*紫金财经*

Main category: wechat.article

TL;DR: 中国研发的开源人工智能模型全球下载量占比达到 17.1%，超越美国的 15.8%，位居全球第一，标志着国产大模型开源生态正式跻身全球领先行列。阿里的Qwen系列、DeepSeek等开源模型在全球开发者社区中获得了广泛应用与衍生，构建


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 中国研发的开源人工智能模型全球下载量占比达到 17.1%，超越美国的 15.8%，位居全球第一，标志着国产大模型开源生态正式跻身全球领先行列。阿里的Qwen系列、DeepSeek等开源模型在全球开发者社区中获得了广泛应用与衍生，构建

</details>


### [49] [OpenAI正式发布GPT5.2：做表格、写PPT、敲代码等生产力大增](http://mp.weixin.qq.com/s?__biz=MjM5MTI3MTY3Mg==&mid=2651545854&idx=3&sn=261a28e3c082fc08624040e37ad6e8e9&chksm=bc78af2483cfe910b59d4f61fa8ea7ad90b783897394bf5c72221205a29087a0dc868625bf05#rd)
*荆楚网*

Main category: wechat.article

TL;DR: 今天凌晨，OpenAI正式推出新一代大模型GPT-5.2，付费用户端及API同步开放。此举距上一代GPT-5.1发布不足一月，背后是谷歌Gemini 3引发的“红色警报”竞争压力——OpenAI此前紧急叫停广告、资讯等侧项目，将所有资源集中于主模型


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 今天凌晨，OpenAI正式推出新一代大模型GPT-5.2，付费用户端及API同步开放。此举距上一代GPT-5.1发布不足一月，背后是谷歌Gemini 3引发的“红色警报”竞争压力——OpenAI此前紧急叫停广告、资讯等侧项目，将所有资源集中于主模型

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [50] [Docker Joins the Agentic AI Foundation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fdocker-joins-the-agentic-ai-foundation%2F%3Futm_source=tldrdevops/1/0100019b0827d676-6a211884-1a71-4c64-aae2-b5632bd8f95a-000000/VKzx6kVyQTN27bo9e6I6T6l0Rmy_UeQruNF1S6NbIR8=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Linux基金会成立Agentic AI基金会，旨在标准化AI代理基础设施协议，Docker作为黄金成员加入，联合多家科技巨头推动AI代理互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理领域缺乏统一的基础设施标准和协议，不同公司的AI代理框架互不兼容，阻碍了AI代理技术的规模化发展和生态系统建设。

Method: Linux基金会牵头成立Agentic AI基金会，整合Anthropic的MCP协议、Block的goose框架和OpenAI的AGENTS.md标准等现有项目，建立统一的AI代理基础设施协议标准。

Result: 成功吸引了Amazon、Google、Microsoft、OpenAI等主要科技公司参与，Docker作为黄金成员加入，形成了行业联盟来推动AI代理基础设施的标准化。

Conclusion: Agentic AI基金会的成立标志着AI代理基础设施标准化迈出重要一步，有望促进AI代理技术的透明演进和跨平台互操作性，加速AI代理生态系统的成熟。

Abstract: Docker Joins the Agentic AI Foundation (3 minute read) The Linux Foundation has launched the Agentic AI Foundation to standardize infrastructure protocols for AI agents, bringing together major tech companies like Amazon, Google, Microsoft, and OpenAI. This initiative unifies projects like Anthropic's Model Context Protocol (MCP), Block's goose agent framework, and OpenAI's AGENTS.md standard to ensure transparent evolution and interoperability, with Docker joining as a Gold member.

</details>


### [51] [How we deploy the largest GitLab instance 12 times daily](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fabout.gitlab.com%2Fblog%2Fcontinuously-deploying-the-largest-gitlab-instance%2F%3Futm_source=tldrdevops/1/0100019b0827d676-6a211884-1a71-4c64-aae2-b5632bd8f95a-000000/2nGvuIj73VkpxSAH7g0N0k2nLuv824_MV-ojImVjVus=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitLab使用自己的CI/CD平台每天部署代码最多12次，采用金丝雀部署、渐进式发布和多版本兼容性确保零停机时间，通过自动化测试验证每个阶段以保持稳定性。


<details>
  <summary>Details</summary>
Motivation: GitLab需要频繁、可靠地部署代码到GitLab.com，同时确保服务稳定性和零停机时间，以支持大规模用户使用。

Method: 使用GitLab自己的CI/CD平台，采用金丝雀部署、渐进式发布、多版本兼容性策略，管理容器化和传统服务，在部署前运行向后兼容的数据库迁移，并通过自动化测试验证每个阶段。

Result: 成功实现每天最多12次代码部署，确保零停机时间，保持服务稳定性，并能够高效管理大规模部署流程。

Conclusion: GitLab通过自有的CI/CD平台和先进的部署策略，能够实现高频、可靠的代码部署，为大规模服务提供了有效的部署解决方案。

Abstract: How we deploy the largest GitLab instance 12 times daily (12 minute read) GitLab deploys code to GitLab.com up to 12 times daily using its own CI/CD platform, leveraging Canary deployments, progressive rollouts, and multiversion compatibility to ensure zero downtime. The deployment pipeline manages both containerized and traditional services, runs backward-compatible database migrations in Canary before post-deploy changes, and validates every stage through automated tests to maintain stabili...

</details>


### [52] [OpenSpec](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FFission-AI%2FOpenSpec%3Futm_source=tldrdevops/1/0100019b0827d676-6a211884-1a71-4c64-aae2-b5632bd8f95a-000000/l60kid57IFtUlqehHiVWskX2TZDeYYyY21bwC-xtEzE=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenSpec提出了一种基于规范的AI编码助手开发工作流，通过先锁定意图再生成代码，使AI输出更可预测和可审查


<details>
  <summary>Details</summary>
Motivation: 解决当前AI编码助手中常见的模糊提示问题，使代码生成更加确定性和可预测，提高代码生成的可审查性

Method: 采用规范驱动的开发工作流，在编写任何代码之前先通过轻量级规范流程锁定意图，从已达成共识的需求中生成确定性代码

Result: 通过规范驱动的方法使AI编码助手的输出更加可预测和可审查，解决了模糊提示导致的代码生成不确定性问题

Conclusion: OpenSpec工作流通过先规范后代码的方法，显著提高了AI编码助手的输出质量和可预测性

Abstract: OpenSpec (GitHub Repo) OpenSpec is a spec-driven development workflow for AI coding assistants that aims to make their outputs more predictable and reviewable by locking intent before any code is written. This lightweight specification process enables deterministic code generation from agreed-upon requirements, addressing the common issue of vague prompts.

</details>


### [53] [Agent Lightning](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmicrosoft%2Fagent-lightning%3Futm_source=tldrdevops/1/0100019b0827d676-6a211884-1a71-4c64-aae2-b5632bd8f95a-000000/fzXkOSoygpahOVWAjA44ek9Lq5ydCCDWQGM-W-8A9Yo=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Lightning是一个新的训练器，旨在简化AI智能体的开发和改进，通过集中收集提示、工具调用和奖励事件来持续优化智能体性能


<details>
  <summary>Details</summary>
Motivation: 开发AI智能体时面临实现复杂性高的问题，需要让开发者能够专注于创意而非技术细节，需要一个统一的训练框架来持续改进智能体性能

Method: 设计了一个与现有智能体框架集成的训练器，通过"LightningStore"集中收集提示、工具调用和奖励事件，然后使用算法持续优化智能体性能

Result: 创建了一个能够简化智能体开发流程的训练框架，支持与现有智能体框架集成，实现了智能体性能的持续改进机制

Conclusion: Agent Lightning为AI智能体开发提供了一个有效的训练解决方案，通过集中化的事件收集和优化算法，降低了开发复杂度并提升了智能体性能

Abstract: Agent Lightning (GitHub Repo) Agent Lightning is a new trainer designed to streamline the development and improvement of AI agents. It allows developers to focus on their ideas by minimizing implementation complexities. The trainer integrates with any existing agent framework, collecting prompt, tool call, and reward events into a central "LightningStore" which then feeds an algorithm to continuously refine agent performance.

</details>


### [54] [A New Approach for Coding Agent Safety](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fdocker-sandboxes-a-new-approach-for-coding-agent-safety%2F%3Futm_source=tldrdevops/1/0100019b0827d676-6a211884-1a71-4c64-aae2-b5632bd8f95a-000000/n-4qo6TUSqwajR_V88HmWwDIvOkBTGboQBImq1lGZqQ=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Docker推出实验性容器沙箱，为日益自主的编码代理提供隔离环境，支持Claude Code和Gemini CLI，未来将向微虚拟机隔离和更强控制发展


<details>
  <summary>Details</summary>
Motivation: 随着编码代理自主性增强，需要对其访问进行控制，确保安全性和隔离性

Method: Docker引入实验性容器沙箱，隔离代理工作流，同时镜像本地工作环境

Result: 预览版支持Claude Code和Gemini CLI，未来将扩展为微虚拟机隔离、更强控制机制和更广泛的代理兼容性

Conclusion: 容器沙箱为编码代理安全提供了新方法，是向更安全、可控的代理工作环境演进的重要一步

Abstract: A New Approach for Coding Agent Safety (4 minute read) Coding agents are gaining autonomy and need controlled access, prompting Docker to introduce experimental container-based sandboxes that isolate agent workflows while mirroring local workspaces. The preview supports Claude Code and Gemini CLI and will evolve toward microVM isolation, stronger controls, and broader agent compatibility.

</details>


### [55] [Codacy's new AI Reviewer helps devs regain control of Pull Requests](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.codacy.com%2Fai-reviewer%2F%3Futm_campaign=31129159-AI%2520Risk%2520Hub%2520%2526%2520AI%2520Reviewer%2520Launch%2520%257C%2520Newsletters%26utm_source=TLDR%26utm_medium=newsletter%26utm_content=RiskHub_AIReviewer/1/0100019b0827d676-6a211884-1a71-4c64-aae2-b5632bd8f95a-000000/bA4otMFXJq8881PW9O38jywYzGJaLzndkoJKyFpc0-o=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Codacy推出AI Reviewer工具，结合静态分析与上下文感知的代码审查，帮助开发者更好地管理Pull Requests中的AI生成代码


<details>
  <summary>Details</summary>
Motivation: GenAI正在以超过开发者审查速度的方式重写代码库，传统扫描工具无法有效检测AI生成代码中的问题，需要更智能的审查工具

Method: 将确定性静态分析与上下文感知的代码审查相结合，提供比传统扫描工具更全面的代码质量检测

Result: 开发了AI Reviewer工具，能够捕获传统扫描器遗漏的问题，帮助开发者重新掌控Pull Requests的审查过程

Conclusion: AI Reviewer通过结合静态分析和上下文感知审查，有效解决了AI生成代码审查的挑战，提升了代码质量和开发效率

Abstract: Codacy's new AI Reviewer helps devs regain control of Pull Requests (Sponsor) GenAI is rewriting your codebase faster than your devs can review it. Codacy's new AI Reviewer pairs deterministic static analysis with context-aware code reviews that catch issues missed by legacy scanners. See how it works

</details>


### [56] [Background Coding Agents: Predictable Results Through Strong Feedback Loops](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.atspotify.com%2F2025%2F12%2Ffeedback-loops-background-coding-agents-part-3%3Futm_source=tldrdev/1/0100019b083d999a-f3936cdc-fbaf-4971-83db-628bf95bb4bd-000000/ZwxhugminsfrYzep9kBMSFh6yo8RgRiHshuu8xxtLoU=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Spotify开发了后台编码代理系统，通过验证循环（包括构建系统、测试和LLM评审）来确保代码生成的可预测性和质量，防止代理偏离提示要求。


<details>
  <summary>Details</summary>
Motivation: 解决编码代理在生成代码时可能偏离原始提示、产生不可预测结果的问题，确保代码生成过程可控且可靠。

Method: 采用多层验证循环：1) 独立验证器（如Maven）运行自动构建和测试提供增量反馈；2) LLM评审在代码提交前审查PR，当代理过于"创造性"偏离提示时行使否决权。

Result: 通过强反馈循环机制，编码代理能够保持任务轨迹，生成更可预测、符合要求的代码，减少了人工干预需求。

Conclusion: 强反馈循环是确保编码代理可靠性的关键，验证机制能有效防止代理偏离，实现更可控的自动化代码生成。

Abstract: Background Coding Agents: Predictable Results Through Strong Feedback Loops (8 minute read) Spotify's background coding agents use verification loops to stay on track. Independent verifiers, like Maven, build systems and tests that run automatically and give incremental feedback. An LLM judge also vetoes PRs when agents get too creative and drift from their prompts.

</details>


### [57] [Introducing: Devstral 2 and Mistral Vibe CLI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmistral.ai%2Fnews%2Fdevstral-2-vibe-cli%3Futm_source=tldrdev/1/0100019b083d999a-f3936cdc-fbaf-4971-83db-628bf95bb4bd-000000/rfIwTqcZwX4UDYwgvXd2h7-09DJJaEq34QDmBoqHnks=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Mistral AI发布了新一代开源代码代理模型Devstral 2和Devstral Small 2，以及用于端到端代码自动化的Mistral Vibe CLI工具


<details>
  <summary>Details</summary>
Motivation: 推动开源代码代理模型的发展，提供更强大的代码生成和自动化能力，降低开发者的编码负担

Method: 开发了123B参数的Devstral 2模型作为开源权重代码代理，同时创建了Mistral Vibe CLI命令行工具，能够自主探索、修改和执行代码变更

Result: Devstral 2在代码代理基准测试中建立了新的标准，Mistral Vibe CLI提供了端到端的代码自动化解决方案

Conclusion: Mistral AI的新一代代码代理模型和工具显著提升了开源代码自动化的能力，为开发者提供了更强大的辅助工具

Abstract: Introducing: Devstral 2 and Mistral Vibe CLI (6 minute read) Mistral AI has launched Devstral 2 and Devstral Small 2, its next-generation open-source agentic coding models, along with the Mistral Vibe CLI for end-to-end code automation. Devstral 2 (123B) is a state-of-the-art model establishing new benchmarks for open-weight code agents. The Mistral Vibe CLI provides an open-source command-line interface that uses Devstral models to autonomously explore, modify, and execute changes across ent...

</details>


### [58] [Streamdown](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvercel%2Fstreamdown%3Futm_source=tldrdev/1/0100019b083d999a-f3936cdc-fbaf-4971-83db-628bf95bb4bd-000000/tzivSK2mG95k_xwpZjrudD3B88BURIUKzaULB1PrdaA=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Streamdown是react-markdown的替代库，专门处理AI模型流式Markdown内容的独特挑战，修复不完整或未终止的Markdown块问题


<details>
  <summary>Details</summary>
Motivation: 现有react-markdown在处理AI模型流式Markdown内容时存在缺陷，特别是无法正确处理不完整或未终止的Markdown块，需要专门的解决方案

Method: 开发一个专门针对流式Markdown处理的库，作为react-markdown的替代品，支持GitHub风格Markdown、LaTeX数学渲染、Mermaid图表和Shiki代码高亮

Result: 创建了Streamdown库，能够有效处理AI模型生成的流式Markdown内容，解决了不完整Markdown块的问题

Conclusion: Streamdown为处理AI模型流式Markdown内容提供了有效的解决方案，填补了现有工具在处理此类场景时的不足

Abstract: Streamdown (GitHub Repo) Streamdown is a drop-in replacement for `react-markdown` specifically engineered to handle the unique challenges of streaming Markdown content from AI models. It fixes issues like incomplete or unterminated Markdown blocks. Streamdown supports GitHub Flavored Markdown, LaTeX math rendering, Mermaid diagrams, and Shiki code highlighting.

</details>


### [59] [Vibe Coding in style.md](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fevilmartians.com%2Fchronicles%2Fvibe-coding-in-style-dot-md%3Futm_source=tldrdesign/1/0100019b085e4328-f4d2c5e5-8d8d-4b8e-b1b1-1743f38a9753-000000/qYp45TbfvtCbn5DXkWJVjgwkQh712jROHeFESsdf8Wo=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Evil Martians开发了一个编码风格指南，将工程最佳实践编码到可重用的文件中，供AI代码生成工具使用，帮助非工程师生成更高质量的代码。


<details>
  <summary>Details</summary>
Motivation: 比较AI快速生成的Rails应用与专家重构版本后，发现需要将工程最佳实践系统化，以解决AI生成代码质量不高、需要昂贵清理的问题。

Method: 开发了一个编码风格指南文件，捕获了领域特定命名、枚举使用、将逻辑提取到命名空间类等模式，作为可重用的资产供AI代码生成工具使用。

Result: 将昂贵的代码清理工作转化为可扩展的资产，帮助非工程师生成更可维护、更符合最佳实践的代码。

Conclusion: 通过将工程最佳实践编码到可重用的风格指南中，可以规模化地提高AI生成代码的质量，降低后期清理成本。

Abstract: Vibe Coding in style.md (5 minute read) Evil Martians developed a style guide that encodes engineering best practices into a reusable file for AI code generation tools. The guide emerged from comparing a hastily AI-generated Rails app with its expert-refactored version, capturing patterns such as domain-specific naming, enum usage, and the extraction of logic into namespaced classes. This approach transforms expensive code cleanup into a scalable asset that helps non-engineers produce more ma...

</details>


### [60] [Observability for agentic AI and LLMs: 6 recommendations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dynatrace.com%2Finfo%2Freports%2Fbring-clarity-to-your-ai-systems%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=cloud-ai-observability-hyperframe-observability-for-ai%26utm_term=111425/2/0100019b08a0d363-13b01862-7b7f-4091-8d0a-a18507b5d782-000000/FVipBKrlYxmAFLN6dL2WVILoX682hJt11uK9_gz7bn8=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Dynatrace报告提出6项实用可观测性建议，帮助管理代理式AI和生成式AI工作负载，解决其不可预测性问题


<details>
  <summary>Details</summary>
Motivation: 代理式AI和生成式AI虽然强大但不可预测，不仅存在幻觉问题，还会在既定工作流中采取全新路径，需要更好的可观测性管理

Method: 报告提出6项实用可观测性建议，包括超越传统监控、发现成本上升、及早发现关键问题等方法

Result: 提供了针对代理式AI和生成式AI工作负载的实用可观测性框架和最佳实践

Conclusion: 需要采用专门的可观测性方法来有效管理代理式AI和生成式AI的不可预测性，确保其可靠运行

Abstract: Observability for agentic AI and LLMs: 6 recommendations (Sponsor) Agentic AI and GenAI are powerful but unpredictable. It's not just hallucination - they regularly take entirely new paths through established workflows.This Dynatrace report lays out six pragmatic observability recommendations for practitioners managing agentic AI and GenAI workloads. Learn to look beyond monitoring, spot escalating costs, and catch critical issues early. Read the report Want to check it out firsthand? Experim...

</details>


### [61] [Introducing Devstral2 and Mistral Vibe CLI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmistral.ai%2Fnews%2Fdevstral-2-vibe-cli%3Futm_source=tldrai/1/0100019b08a0d363-13b01862-7b7f-4091-8d0a-a18507b5d782-000000/kzdAKJb2GxZBGMzoozP7afR5rwJVHXioIM6o1562VRY=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Devstral2在SWE-bench上达到72.2%准确率，成为最佳开源编码模型之一；同时推出Vibe CLI终端代理工具


<details>
  <summary>Details</summary>
Motivation: 开发更高效、更小规模的代码生成模型，使其能在消费级硬件上运行，同时提供多文件代码库管理的终端代理工具

Method: 开发了123B参数的Devstral2模型和24B的Devstral Small 2模型，并创建了开源的Vibe CLI终端代理工具

Result: Devstral2在SWE-bench Verified上达到72.2%准确率，Devstral Small 2达到68.0%且能在消费级硬件上运行

Conclusion: Devstral2成为最佳开源编码模型之一，Vibe CLI为代码库管理提供了实用的终端代理工具

Abstract: Introducing Devstral2 and Mistral Vibe CLI (5 minute read) Devstral 2 hits 72.2% on SWE-bench Verified with 123B parameters, making it one of the best open-weight coding models despite being a fraction of the size of its peers. The 24B Devstral Small 2 scores 68.0% and runs on consumer hardware. Mistral also launched Vibe CLI, an open-source terminal agent that orchestrates multi-file changes across codebases.

</details>


### [62] [How People Use AI Agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FEDxRfQ/1/0100019b08a0d363-13b01862-7b7f-4091-8d0a-a18507b5d782-000000/Xo_vQNIvhz_yHJckFugOFpky_bhmI33RKUc6CXh4_ds=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 哈佛与Perplexity的大规模使用研究表明，超过一半的AI代理任务涉及研究和生产力等深度认知工作，挑战了AI代理仅处理简单任务的刻板印象


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解人们实际如何使用AI代理，挑战AI代理仅用于简单任务的普遍看法，探索其在深度认知工作中的真实应用场景

Method: 基于Perplexity和哈佛的大规模使用研究，分析用户与AI代理的交互数据，统计不同类型任务的比例和特征

Result: 研究发现超过50%的AI代理任务涉及研究、生产力等深度认知工作，表明AI代理在实际使用中更多用于复杂思考任务而非简单操作

Conclusion: AI代理的实际应用已超越简单任务范畴，正成为支持深度认知工作的重要工具，需要重新评估对AI代理能力的传统认知

Abstract: How People Use AI Agents (7 minute read) A large-scale usage study from Perplexity and Harvard reveals that over half of AI agent tasks involve deep cognitive work like research and productivity, challenging the stereotype of agents as simple task-doers.

</details>


### [63] [Practitioner's guide to reinforcement learning, from Weights & Biases](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwandb.ai%2Fsite%2Fresources%2Fwhitepapers%2Freinforcement-learning-ebook%3Futm_source=tldr-ai%26utm_medium=cpc%26utm_campaign=weave%26utm_content=RL%26utm_term=newsletter/1/0100019b08a0d363-13b01862-7b7f-4091-8d0a-a18507b5d782-000000/0VsRAv-uSo2dCEYsPtcStcR2Fi8R1SG_TUUlQFjTHls=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Weights & Biases提供的强化学习实践指南，介绍RL应用场景、与其他技术的关系以及其Serverless RL服务


<details>
  <summary>Details</summary>
Motivation: 针对强化学习在AGI路径上的争议，提供实践指导，帮助开发者理解RL的最佳应用场景和与其他技术的关系

Method: 提供实践指南，涵盖RL工作原理、SFT、LoRA、GRPO等技术在RL中的角色，以及Weights & Biases的Serverless RL服务

Result: 提供了一份全面的RL实践指南，帮助开发者理解何时使用RL以及如何结合其他技术，并推广了Weights & Biases的Serverless RL服务

Conclusion: 强化学习是复杂但强大的工具，需要根据具体场景选择使用，Weights & Biases的Serverless RL可以简化RL应用开发

Abstract: Practitioner's guide to reinforcement learning, from Weights & Biases (Sponsor) RL has been touted as the direct path to AGI, while others dismiss it as a dead end. Learn how and when RL works best, where SFT, LoRA, and GRPO fit in, and about the benefits of new Serverless RL from Weights & Biases. Get the guide here.

</details>


### [64] [AlphaEvolve on Google Cloud: AI for agentic discovery and optimization](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Fproducts%2Fai-machine-learning%2Falphaevolve-on-google-cloud%3Futm_source=tldrai/1/0100019b08a0d363-13b01862-7b7f-4091-8d0a-a18507b5d782-000000/YD2LJzab2DMMjEEcSAALmT9rHD94uOlg1C2e7SE3d58=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AlphaEvolve在Google Cloud上推出，利用Gemini模型通过算法进化（变异和评估）来优化复杂问题，已在Google内部提升数据中心效率、减少训练时间并加速TPU设计。


<details>
  <summary>Details</summary>
Motivation: 解决复杂优化问题，特别是在数据中心效率、AI模型训练时间和硬件设计等领域的挑战，为各行业提供专有优化解决方案。

Method: 基于Gemini模型，采用算法进化方法，通过变异和评估过程来演化算法，在Google Cloud平台上实现。

Result: 在Google内部应用中取得了显著成果：提高了数据中心效率，减少了Gemini模型的训练时间，加速了TPU设计过程。

Conclusion: AlphaEvolve技术具有广泛的应用潜力，生物技术和物流等行业可以利用它来解决专有的优化挑战。

Abstract: AlphaEvolve on Google Cloud: AI for agentic discovery and optimization (4 minute read) AlphaEvolve, now on Google Cloud in private preview, leverages Gemini models to optimize complex problems by evolving algorithms through mutation and evaluation. The technology has improved data center efficiency, reduced Gemini's training time, and accelerated TPU design at Google. Various industries, including biotech and logistics, can use it to solve proprietary optimization challenges.

</details>


### [65] [AI Memory is Really a Database Problem](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoworld.com%2Farticle%2F4101981%2Fai-memory-is-just-another-database-problem.html%3Futm_source=tldrdata/1/0100019b0d189ec2-3866eb1e-7402-47a9-94e1-f43aca7361c5-000000/pUUOqqR4DaDu1kh3E0eXy5gympeUU8YYX9hRQfUJW8Q=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理内存应像企业数据库一样严格管理，采用防火墙、访问控制、行级安全和完整审计，而非当前无管理的向量存储或内存缓存


<details>
  <summary>Details</summary>
Motivation: 当前AI代理内存管理方法（如无管理的向量存储或内存缓存）存在严重安全漏洞，包括内存中毒、工具滥用和权限蔓延，需要更严格的数据治理

Method: 将AI代理内存集成到受治理、可审计的数据基础设施中，确保数据沿袭、生命周期管理，并采用企业数据库级别的安全控制措施

Result: 通过将AI内存视为数据库问题处理，可以显著减少攻击面，提高安全性，并确保数据的完整性和可追溯性

Conclusion: AI代理内存管理需要采用企业数据库级别的严格安全和控制措施，而不是依赖当前不安全的临时解决方案

Abstract: AI Memory is Really a Database Problem (7 minute read) AI agent memory must be treated with the same rigor as enterprise databases, incorporating firewalls, access controls, row-level security, and full auditability. Current approaches—often unmanaged vector stores or in-memory caches—create significant attack surfaces through memory poisoning, tool misuse, and privilege creep. Integrating agent memory into governed, auditable data infrastructure ensures data lineage, lifecycle management, an...

</details>


### [66] [Agentic Postgres: Postgres for Agentic Apps with Fast Forking and AI-Ready Features](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.infoq.com%2Fnews%2F2025%2F12%2Fagentic-postgres-fast-forking%2F%3Futm_source=tldrdata/1/0100019b0d189ec2-3866eb1e-7402-47a9-94e1-f43aca7361c5-000000/HVIUYo5ITYmYIyrdEjwZvHMqeEDV5TVjF8xr21GOrn8=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agentic Postgres 是一个为AI代理设计的PostgreSQL扩展，提供秒级写时复制分叉、原生BM25和高性能向量搜索，以及集成的MCP服务器，用于自然语言模式管理和提示驱动的操作。


<details>
  <summary>Details</summary>
Motivation: 现有的PostgreSQL数据库在处理AI代理工作流时存在性能瓶颈，特别是在需要快速数据分叉、向量搜索和自然语言交互方面。AI代理应用需要能够快速创建数据沙箱、进行高效的语义搜索，以及通过自然语言管理数据库。

Method: 开发PostgreSQL扩展，集成三个关键技术：1) Fluid Storage实现秒级写时复制分叉；2) 原生BM25和高性能向量搜索；3) 集成MCP服务器支持自然语言模式管理和提示驱动操作。

Result: 能够为AI代理应用提供即时隔离的沙箱环境，显著加速代理工作流，支持安全实验，并实现自然语言驱动的数据库管理。

Conclusion: Agentic Postgres通过扩展PostgreSQL功能，使其更适合AI代理应用，解决了数据分叉、搜索效率和自然语言交互等关键问题。

Abstract: Agentic Postgres: Postgres for Agentic Apps with Fast Forking and AI-Ready Features (2 minute read) Agentic Postgres is a Postgres extension built for AI agents that adds sub-second copy-on-write forking (via Fluid Storage), native BM25 and high-performance vector search, and an integrated MCP server for natural-language schema management and prompt-driven operations. It enables instant, isolated sandboxes on live production data, dramatically speeding up agentic workflows, safe experimentati...

</details>


### [67] [Donating the Model Context Protocol and Establishing the Agentic AI Foundation](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fnews%2Fdonating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation%3Futm_source=tldrdata/1/0100019b0d189ec2-3866eb1e-7402-47a9-94e1-f43aca7361c5-000000/kxb0TuUAaCBfwZOOPM_x7GLphNnGOiYAxZMubDf9nnA=435)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic将Model Context Protocol捐赠给新成立的Agentic AI Foundation，使其成为连接AI代理与外部工具、数据和服务的供应商中立、社区治理标准


<details>
  <summary>Details</summary>
Motivation: 促进AI代理与外部工具、数据和服务的互操作性，建立开放治理的行业标准，避免供应商锁定

Method: 将Model Context Protocol捐赠给Linux Foundation下的Agentic AI Foundation，使其成为社区治理的开放标准

Result: MCP成为供应商中立的标准协议，与goose、AGENTS.md等贡献并列，推动AI代理生态的互操作性

Conclusion: 通过建立开放标准促进AI代理生态系统的互操作性和协作发展

Abstract: Donating the Model Context Protocol and Establishing the Agentic AI Foundation (4 minute read) Anthropic donated Model Context Protocol (MCP) to the newly created Agentic AI Foundation (AAIF) under Linux Foundation, putting it alongside contributions such as goose (from Block) and AGENTS.md (from OpenAI). The move ensures MCP becomes a vendor-neutral, community-governed standard for linking AI agents to external tools, data, and services that promote interoperability, open governance, and lon...

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [Latent Action World Models for Control with Unlabeled Trajectories](https://arxiv.org/abs/2512.10016)
*Marvin Alles,Xingyuan Zhang,Patrick van der Smagt,Philip Becker-Ehmck*

Main category: cs.LG

TL;DR: 提出一种潜在动作世界模型，通过联合使用动作条件数据和无动作数据，学习共享的潜在动作表示，从而在少量动作标签样本下实现高效的世界模型学习。


<details>
  <summary>Details</summary>
Motivation: 传统世界模型依赖动作条件轨迹，当动作标签稀缺时效果受限。受人类结合直接交互和无动作经验（如视频）的启发，需要能够利用异构数据的世界模型。

Method: 引入潜在动作世界模型家族，学习共享的潜在动作表示，将观察到的控制信号与被动观察推断的动作对齐。使用该模型通过离线强化学习学习潜在动作策略。

Result: 在DeepMind Control Suite上，该方法在使用比纯动作条件基线少约一个数量级的动作标签样本情况下，实现了强大的性能。

Conclusion: 潜在动作使世界模型能够同时利用被动和交互数据进行训练，提高了学习效率，弥合了离线RL和无动作训练这两个传统上分离的领域。

Abstract: Inspired by how humans combine direct interaction with action-free experience (e.g., videos), we study world models that learn from heterogeneous data. Standard world models typically rely on action-conditioned trajectories, which limits effectiveness when action labels are scarce. We introduce a family of latent-action world models that jointly use action-conditioned and action-free data by learning a shared latent action representation. This latent space aligns observed control signals with actions inferred from passive observations, enabling a single dynamics model to train on large-scale unlabeled trajectories while requiring only a small set of action-labeled ones. We use the latent-action world model to learn a latent-action policy through offline reinforcement learning (RL), thereby bridging two traditionally separate domains: offline RL, which typically relies on action-conditioned data, and action-free training, which is rarely used with subsequent RL. On the DeepMind Control Suite, our approach achieves strong performance while using about an order of magnitude fewer action-labeled samples than purely action-conditioned baselines. These results show that latent actions enable training on both passive and interactive data, which makes world models learn more efficiently.

</details>


### [69] [SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation](https://arxiv.org/abs/2512.10042)
*Jongmin Lee,Meiqi Sun,Pieter Abbeel*

Main category: cs.LG

TL;DR: 提出SEMDICE算法，用于无监督强化学习预训练，通过最大化状态熵从任意离线数据集中学习先验策略，在状态熵最大化和下游任务适应效率方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在无监督强化学习预训练中，智能体需要在不依赖任务特定奖励函数的情况下学习适用于下游任务的先验策略。状态熵最大化（SEM）是一种有效方法，但现有方法在从离线数据集中学习SEM策略方面存在局限性。

Method: 提出SEMDICE算法，一种基于原理的离线策略算法，直接从任意离线数据集中计算SEM策略。该方法在平稳分布空间中直接优化策略，计算单一、平稳的马尔可夫状态熵最大化策略。

Result: 实验结果表明，SEMDICE在最大化状态熵方面优于基线算法，同时在基于SEM的无监督RL预训练方法中实现了最佳的下游任务适应效率。

Conclusion: SEMDICE是一种有效的无监督强化学习预训练方法，能够从离线数据集中学习高质量的先验策略，为下游任务提供良好的初始化。

Abstract: In the unsupervised pre-training for reinforcement learning, the agent aims to learn a prior policy for downstream tasks without relying on task-specific reward functions. We focus on state entropy maximization (SEM), where the goal is to learn a policy that maximizes the entropy of the state stationary distribution. In this paper, we introduce SEMDICE, a principled off-policy algorithm that computes an SEM policy from an arbitrary off-policy dataset, which optimizes the policy directly within the space of stationary distributions. SEMDICE computes a single, stationary Markov state-entropy-maximizing policy from an arbitrary off-policy dataset. Experimental results demonstrate that SEMDICE outperforms baseline algorithms in maximizing state entropy while achieving the best adaptation efficiency for downstream tasks among SEM-based unsupervised RL pre-training methods.

</details>


### [70] [Detailed balance in large language model-driven agents](https://arxiv.org/abs/2512.10047)
*Zhuo-Yang Song,Qing-Hong Cao,Ming-xing Luo,Hua Xing Zhu*

Main category: cs.LG

TL;DR: 该论文发现LLM驱动的智能体在状态转移中存在细致平衡，表明LLM生成可能不是通过学习规则集和策略，而是通过隐式学习一类潜在函数实现的，这可能是首个不依赖具体模型细节的LLM生成动力学宏观物理定律。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的智能体在解决复杂问题方面取得了经验性成功，但缺乏理解其宏观动力学的理论框架。作者希望建立复杂AI系统的宏观动力学理论，将AI智能体研究从工程实践提升到可预测、可量化的科学。

Method: 基于最小作用量原理，通过实验测量LLM生成状态之间的转移概率，统计发现LLM生成转换中存在细致平衡。这表明LLM生成可能通过隐式学习潜在函数实现，而非一般性地学习规则集和策略。

Result: 发现了LLM生成转换中的细致平衡现象，表明LLM生成可能通过隐式学习一类潜在函数实现，这类函数可能超越不同的LLM架构和提示模板。这是首个不依赖具体模型细节的LLM生成动力学宏观物理定律的发现。

Conclusion: 该工作是建立复杂AI系统宏观动力学理论的尝试，旨在将AI智能体研究从工程实践集合提升到基于可预测、可量化有效测量的科学。发现的细致平衡现象为理解LLM生成动力学提供了新的理论视角。

Abstract: Large language model (LLM)-driven agents are emerging as a powerful new paradigm for solving complex problems. Despite the empirical success of these practices, a theoretical framework to understand and unify their macroscopic dynamics remains lacking. This Letter proposes a method based on the least action principle to estimate the underlying generative directionality of LLMs embedded within agents. By experimentally measuring the transition probabilities between LLM-generated states, we statistically discover a detailed balance in LLM-generated transitions, indicating that LLM generation may not be achieved by generally learning rule sets and strategies, but rather by implicitly learning a class of underlying potential functions that may transcend different LLM architectures and prompt templates. To our knowledge, this is the first discovery of a macroscopic physical law in LLM generative dynamics that does not depend on specific model details. This work is an attempt to establish a macroscopic dynamics theory of complex AI systems, aiming to elevate the study of AI agents from a collection of engineering practices to a science built on effective measurements that are predictable and quantifiable.

</details>


### [71] [Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories](https://arxiv.org/abs/2512.10350)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 论文提出了分析智能体循环在语义嵌入空间中几何行为的框架，区分了语言变换的工件空间和几何测量的嵌入空间，通过等渗校准消除余弦相似度偏差，识别了收缩重写循环和探索性总结否定循环两种基本机制。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的智能体系统通过递归反馈循环运行，但人们对这些智能体循环的几何行为（收敛、发散或更复杂动态）理解不足，需要建立分析框架来理解其动态特性。

Method: 引入在语义嵌入空间中分析智能体轨迹的几何框架，将迭代变换视为离散动力系统；区分工件空间（语言变换）和嵌入空间（几何测量）；提出等渗校准消除余弦相似度的各向异性偏差；通过受控实验研究单一智能体循环。

Result: 识别了两种基本机制：收缩重写循环收敛于稳定吸引子且分散度递减，探索性总结否定循环产生无界发散且无聚类形成；这些机制显示出收缩和扩张的定性不同几何特征；提示设计直接控制智能体循环的动态机制。

Conclusion: 智能体循环的几何行为可以通过系统框架进行分析，提示设计能够直接控制循环的动态机制，为迭代LLM变换中的收敛、发散和轨迹结构提供了系统控制方法。

Abstract: Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.

</details>


### [72] [UACER: An Uncertainty-Aware Critic Ensemble Framework for Robust Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.10492)
*Jiaxi Wu,Tiantian Zhang,Yuxing Wang,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: 提出UACER方法，通过多样化评论家集成和时间变化衰减不确定性机制，解决鲁棒对抗强化学习中训练不稳定和收敛困难的问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒对抗强化学习在自动驾驶和机器人控制等序列决策领域有重要应用，但可训练的对抗者会导致学习动态的非平稳性，加剧训练不稳定和收敛困难，特别是在高维复杂环境中。

Method: UACER包含两个策略：1）多样化评论家集成：使用K个并行评论家网络稳定Q值估计，减少方差并增强鲁棒性；2）时间变化衰减不确定性机制：基于方差推导的Q值聚合策略，显式结合认知不确定性来动态调节探索-利用权衡并稳定训练过程。

Result: 在多个MuJoCo控制问题上进行综合实验，验证了UACER的优越有效性，在整体性能、稳定性和效率方面均优于最先进方法。

Conclusion: UACER通过创新的评论家集成和不确定性感知机制，有效解决了鲁棒对抗强化学习中的训练不稳定问题，为复杂环境中的序列决策提供了更可靠的解决方案。

Abstract: Robust adversarial reinforcement learning has emerged as an effective paradigm for training agents to handle uncertain disturbance in real environments, with critical applications in sequential decision-making domains such as autonomous driving and robotic control. Within this paradigm, agent training is typically formulated as a zero-sum Markov game between a protagonist and an adversary to enhance policy robustness. However, the trainable nature of the adversary inevitably induces non-stationarity in the learning dynamics, leading to exacerbated training instability and convergence difficulties, particularly in high-dimensional complex environments. In this paper, we propose a novel approach, Uncertainty-Aware Critic Ensemble for robust adversarial Reinforcement learning (UACER), which consists of two strategies: 1) Diversified critic ensemble: a diverse set of K critic networks is exploited in parallel to stabilize Q-value estimation rather than conventional single-critic architectures for both variance reduction and robustness enhancement. 2) Time-varying Decay Uncertainty (TDU) mechanism: advancing beyond simple linear combinations, we develop a variance-derived Q-value aggregation strategy that explicitly incorporates epistemic uncertainty to dynamically regulate the exploration-exploitation trade-off while simultaneously stabilizing the training process. Comprehensive experiments across several MuJoCo control problems validate the superior effectiveness of UACER, outperforming state-of-the-art methods in terms of overall performance, stability, and efficiency.

</details>


### [73] [Adaptive Replay Buffer for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2512.10510)
*Chihyeon Song,Jaewoo Lee,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出自适应回放缓冲区(ARB)，通过动态调整离线与在线数据采样比例，解决离线到在线强化学习中的稳定性与性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习面临关键困境：需要在固定离线数据集和新收集的在线经验之间取得平衡。标准方法通常依赖固定的数据混合比例，难以在早期学习稳定性和渐进性能之间进行权衡。

Method: 引入自适应回放缓冲区(ARB)，基于轻量级指标"on-policyness"动态优先采样数据。该方法无需复杂学习过程，简单易实现，可无缝集成到现有O2O RL算法中。评估收集的轨迹与当前策略行为的对齐程度，并为该轨迹中的每个转移分配相应的采样权重。

Result: 在D4RL基准测试上的广泛实验表明，ARB能持续缓解早期性能下降，并显著提高各种O2O RL算法的最终性能。

Conclusion: 自适应、行为感知的回放缓冲区设计对于离线到在线强化学习至关重要，ARB通过动态数据采样策略有效平衡了离线数据的初始稳定性和在线经验的学习效率。

Abstract: Offline-to-Online Reinforcement Learning (O2O RL) faces a critical dilemma in balancing the use of a fixed offline dataset with newly collected online experiences. Standard methods, often relying on a fixed data-mixing ratio, struggle to manage the trade-off between early learning stability and asymptotic performance. To overcome this, we introduce the Adaptive Replay Buffer (ARB), a novel approach that dynamically prioritizes data sampling based on a lightweight metric we call 'on-policyness'. Unlike prior methods that rely on complex learning procedures or fixed ratios, ARB is designed to be learning-free and simple to implement, seamlessly integrating into existing O2O RL algorithms. It assesses how closely collected trajectories align with the current policy's behavior and assigns a proportional sampling weight to each transition within that trajectory. This strategy effectively leverages offline data for initial stability while progressively focusing learning on the most relevant, high-rewarding online experiences. Our extensive experiments on D4RL benchmarks demonstrate that ARB consistently mitigates early performance degradation and significantly improves the final performance of various O2O RL algorithms, highlighting the importance of an adaptive, behavior-aware replay buffer design.

</details>


### [74] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的方法，利用旋转嵌入特性使LLM能够同时思考、监听和生成输出，实现实时推理响应。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理需要先思考再回答，导致交互延迟，不适合语音助手等需要实时响应的应用场景。人类可以异步思考、倾听和行动，而现有LLM只能顺序交互。

Method: 利用旋转嵌入的特性，使原本设计用于顺序交互的LLM能够同时进行思考、监听和生成输出，无需额外训练。

Result: 在数学、常识和安全推理任务上，该方法能生成准确的思考增强答案，将首个非思考token的生成时间从分钟级减少到≤5秒，整体实时延迟降低6-11倍。

Conclusion: 通过旋转嵌入实现LLM的异步推理能力，显著改善了实时交互性能，使LLM更适合语音助手等需要实时响应的应用场景。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [75] [Multi-Objective Reward and Preference Optimization: Theory and Algorithms](https://arxiv.org/abs/2512.10601)
*Akhil Agnihotri*

Main category: cs.LG

TL;DR: 该论文在约束强化学习领域做出了系统性贡献，包括平均成本CMDP的ACPO算法、有限时域CMDP的e-COP算法、基于人类偏好的warmPref-PS和PSPL方法，以及大规模模型对齐的MOPO算法，为安全对齐的决策提供理论和实践工具。


<details>
  <summary>Details</summary>
Motivation: 当前约束强化学习在控制、偏好学习和大模型对齐方面存在理论和方法上的局限性，需要开发统一的框架来处理不同场景下的约束问题，确保决策系统的安全性和对齐性。

Method: 1) 针对平均成本CMDP开发ACPO算法，结合敏感性分析和信任域更新；2) 针对有限时域CMDP提出e-COP算法，基于时域策略差异引理；3) 提出warmPref-PS用于线性赌博机，整合异质评分者的离线偏好数据；4) 开发PSPL算法从成对轨迹比较中联合采样奖励模型和转移动态；5) 提出MOPO算法用于大规模模型对齐，采用多目标约束优化视角。

Result: ACPO在平均成本CMDP中达到最先进的实证性能并具有理论保证；e-COP在安全关键环境中提供可证明的性能、简单性和可扩展性；warmPref-PS显著减少遗憾并提高数据收集效率；PSPL提供贝叶斯简单遗憾保证并稳健识别最优策略；MOPO可扩展到数十亿参数的语言模型并在各种对齐设置中保持鲁棒性。

Conclusion: 该论文统一了平均成本、时域和偏好驱动的约束强化学习范式，为安全和对齐的决策提供了理论进展和实用工具，在控制、偏好学习和模型对齐等多个领域做出了系统性贡献。

Abstract: This thesis develops theoretical frameworks and algorithms that advance constrained reinforcement learning (RL) across control, preference learning, and alignment of large language models. The first contribution addresses constrained Markov Decision Processes (CMDPs) under the average-cost criterion through the Average-Constrained Policy Optimization (ACPO) algorithm. ACPO integrates sensitivity analysis with trust-region updates to ensure stable constraint handling, achieving state-of-the-art empirical performance with theoretical guarantees. Constrained RL is then extended to finite-horizon settings via e-COP, the first policy optimization method for episodic CMDPs. Built on an episodic policy difference lemma, e-COP offers provable performance, simplicity, and scalability in safety-critical environments. The thesis then investigates reinforcement learning from human preferences. warmPref-PS introduces a posterior sampling strategy for linear bandits that integrates offline preference data from heterogeneous raters into online learning. Explicit modeling of rater competence yields substantial regret reduction and more efficient data collection for RLHF. The PSPL algorithm further advances preference-based RL by jointly sampling reward models and transition dynamics from pairwise trajectory comparisons, providing Bayesian simple-regret guarantees and robust empirical identification of optimal policies. The final contribution applies these methods to large-scale model alignment. A multi-objective constrained optimization view yields MOPO, an iterative algorithm with closed-form updates that scales to multi-billion-parameter language models and remains robust across alignment settings. Collectively, the thesis unifies constrained RL across average-cost, episodic, and preference-driven paradigms, delivering theoretical advances and practical tools for safe and aligned decision-making.

</details>


### [76] [Interpretable and Steerable Concept Bottleneck Sparse Autoencoders](https://arxiv.org/abs/2512.10805)
*Akshay Kulkarni,Tsui-Wei Weng,Vivek Narayanaswamy,Shusen Liu,Wesam A. Sakla,Kowshik Thopalli*

Main category: cs.LG

TL;DR: 本文提出CB-SAE框架，通过剪枝低效用神经元并添加轻量级概念瓶颈层，显著提升稀疏自编码器的可解释性和可操控性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在LLM/LVM的机制可解释性、概念发现和模型操控方面有潜力，但现有方法存在两个问题：1）多数神经元可解释性或可操控性低；2）无监督学习导致用户所需概念缺失。

Method: 提出CB-SAE框架：1）引入两个计算成本低的可解释性和可操控性度量指标；2）剪枝低效用神经元；3）添加轻量级概念瓶颈层，与用户定义的概念集对齐。

Result: 在LVLM和图像生成任务中，CB-SAE将可解释性提升32.1%，可操控性提升14.5%。

Conclusion: CB-SAE通过后处理框架有效解决了稀疏自编码器的实用性问题，为模型可解释性和操控提供了更实用的解决方案。

Abstract: Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.

</details>


### [77] [Learning Controllable and Diverse Player Behaviors in Multi-Agent Environments](https://arxiv.org/abs/2512.10835)
*Atahan Cilan,Atay Özgövde*

Main category: cs.LG

TL;DR: 提出一个强化学习框架，无需人类游戏数据即可生成可控且多样化的玩家行为，通过行为向量空间实现平滑控制


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量玩家轨迹数据、为不同玩家类型训练单独模型，或缺乏可解释行为参数与学习策略的直接映射，限制了可扩展性和可控性

Method: 将玩家行为定义为N维连续空间，均匀采样目标行为向量，训练时输入当前和目标行为向量，奖励基于两者距离的归一化减少，使用PPO多智能体策略

Result: 在自定义多玩家Unity游戏中，相比仅追求胜利的基线方法，该框架产生显著更大的行为多样性，并能可靠匹配指定的行为向量

Conclusion: 该方法为自动化游戏测试、游戏平衡、人类行为模拟和在线游戏中断线玩家替换提供了可扩展解决方案

Abstract: This paper introduces a reinforcement learning framework that enables controllable and diverse player behaviors without relying on human gameplay data. Existing approaches often require large-scale player trajectories, train separate models for different player types, or provide no direct mapping between interpretable behavioral parameters and the learned policy, limiting their scalability and controllability. We define player behavior in an N-dimensional continuous space and uniformly sample target behavior vectors from a region that encompasses the subset representing real human styles. During training, each agent receives both its current and target behavior vectors as input, and the reward is based on the normalized reduction in distance between them. This allows the policy to learn how actions influence behavioral statistics, enabling smooth control over attributes such as aggressiveness, mobility, and cooperativeness. A single PPO-based multi-agent policy can reproduce new or unseen play styles without retraining. Experiments conducted in a custom multi-player Unity game show that the proposed framework produces significantly greater behavioral diversity than a win-only baseline and reliably matches specified behavior vectors across diverse targets. The method offers a scalable solution for automated playtesting, game balancing, human-like behavior simulation, and replacing disconnected players in online games.

</details>
