<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 11]
- [cs.AI](#cs.AI) [Total: 20]
- [tldr.article](#tldr.article) [Total: 4]
- [cs.LG](#cs.LG) [Total: 15]
- [wechat.article](#wechat.article) [Total: 22]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 提出了LAHIS方法，通过单次前向和反向传播识别多语言能力中的注意力头重要性，发现了语言特定和语言通用头，并通过轻量级适配提升多语言性能。


<details>
  <summary>Details</summary>
Motivation: 探索多头自注意力在LLMs多语言处理中的贡献，增强模型的可解释性和多语言能力。

Method: 提出LAHIS方法识别注意力头重要性，发现语言特定和语言通用头，并引入轻量级软头掩码适配。

Result: 在Aya-23-8B、Llama-3.2-3B和Mistral-7B-v0.1上验证了LAHIS的有效性，仅需20个可调参数即可提升XQuAD准确率。

Conclusion: 从多头自注意力角度增强了LLMs的可解释性和多语言能力。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [2] [From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://arxiv.org/abs/2511.07800)
*Siyu Xia,Zekun Xu,Jiajun Chai,Wentian Fan,Yan Song,Xiaohan Wang,Guojun Yin,Wei Lin,Haifeng Zhang,Jun Wang*

Main category: cs.CL

TL;DR: 提出了一种可训练的图记忆框架，通过将智能体轨迹抽象为状态机中的结构化决策路径，并进一步提炼为高层战略元认知，增强LLM智能体的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体获取经验的方式存在局限性：隐式记忆训练存在灾难性遗忘和可解释性差的问题，显式记忆提示缺乏适应性。需要一种更好的方法来利用先验经验指导当前决策。

Method: 提出多层级图记忆框架，将原始智能体轨迹抽象为状态机中的结构化决策路径，并提炼为高层战略元认知。通过基于奖励的权重优化程序估计每个元认知的效用，并通过元认知提示动态集成到LLM智能体训练中。

Result: 可学习的图记忆框架实现了稳健的泛化能力，提高了LLM智能体的战略推理性能，并在强化学习训练中提供了一致的收益。

Conclusion: 该图记忆框架有效增强了LLM智能体利用参数化信息的能力，为智能体经验利用提供了新的解决方案。

Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.

</details>


### [3] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出Logits-to-Logic框架，通过logits增强和过滤模块来纠正LLM在结构化知识推理中的逻辑缺陷，显著提升逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: LLM在非结构化文本预训练中表现出色，但在结构化知识推理任务中面临逻辑漂移问题，现有方法仅提供输入级指导，无法从根本上解决输出中的逻辑不一致。

Method: 提出Logits-to-Logic框架，包含logits增强和logits过滤两个核心模块，针对自回归生成过程中的logits输出进行修正。

Result: 在多个KGQA基准测试中取得最先进性能，显著提升了LLM在结构化知识推理中的逻辑一致性。

Conclusion: Logits-to-Logic框架能有效解决LLM在结构化知识推理中的逻辑漂移问题，为提升逻辑一致性提供了新思路。

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [4] [Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition](https://arxiv.org/abs/2511.08126)
*Raquel Montero,Natalia Moskvina,Paolo Morosi,Tamara Serrano,Elena Pagliarini,Evelina Leivada*

Main category: cs.CL

TL;DR: 该论文研究了多模态大语言模型在量化表达处理方面的表现，分析了三个关键人类量化特征在模型中的编码情况，发现模型与人类在这些特征上存在明显差异。


<details>
  <summary>Details</summary>
Motivation: 量化表达是(M)LLMs特别难以处理的语言现象，但其表现不佳的确切原因尚不清楚。作者希望探究人类量化特征的三个关键方面在模型中的编码情况，以及模型与人类的差异。

Method: 研究三个跨语言共享的人类量化特征：量词的排序尺度、使用范围和原型性、人类近似数字系统的固有偏差。通过多种任务分析这些特征在模型架构中的编码情况。

Result: 发现在各种任务中，人类与MLLMs在这些量化特征上存在明显差异，无论是在体内还是硅中的量化表征。

Conclusion: 这项工作为探讨MLLMs作为语义和语用代理的本质铺平了道路，跨语言视角可以阐明它们的能力在不同语言中是否稳健和稳定。

Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.

</details>


### [5] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 提出一个多智能体框架来优化LLM对话响应，通过专门负责事实性、个性化和连贯性的智能体进行协同优化，并采用动态通信策略自适应选择相关智能体。


<details>
  <summary>Details</summary>
Motivation: LLM在对话系统中虽然能生成类人响应，但在个性化或特定知识方面存在不足，且用户难以检测错误并要求重新生成。现有单LLM优化方法无法有效考虑对话的多个关键方面。

Method: 使用多智能体框架，每个智能体专门负责一个方面（事实性、个性化、连贯性），通过动态通信策略自适应选择和协调相关智能体，将反馈合并以改进整体响应。

Result: 在具有挑战性的对话数据集上验证，该方法显著优于相关基线，特别是在涉及知识或用户角色的任务中表现突出。

Conclusion: 多智能体框架能有效提升对话质量，动态通信策略增强了智能体间的协作效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [6] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出了一种名为AgentPRM的过程奖励模型，用于评估智能体任务中的每个决策，通过捕捉决策间的相互依赖关系及其对最终目标的贡献，实现更好的进度跟踪和探索-利用平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮决策任务中仍面临挑战，传统方法依赖精心设计的提示工程或专家轨迹微调，本文从不同角度探索构建过程奖励模型来指导智能体决策过程。

Method: 提出AgentPRM模型，结合时序差分估计和广义优势估计来高效获取训练数据，捕捉决策序列的相互依赖关系及其对目标的贡献。

Result: 实验表明AgentPRM比基线方法计算效率高8倍以上，在扩展测试时计算时表现出稳健的改进。

Conclusion: AgentPRM为智能体任务提供了一种有效的决策评估方法，并可应用于LLM智能体的强化学习。

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [7] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: TRACE提出了一种基于对话嵌入轨迹几何特性的新型奖励信号，仅使用交互动态就能达到与完整文本分析相当的准确率，且结合文本分析后性能最佳。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法仅依赖文本内容奖励信号，忽视了交互动态这一丰富的补充信号源。

Method: 基于对话嵌入轨迹的几何特性构建奖励模型，提出'对话几何'概念，并开发了结合交互动态和文本分析的混合模型。

Result: 仅使用结构信号的奖励模型达到68.20%的成对准确率，与完整文本分析的LLM基线(70.04%)相当；混合模型达到最高性能80.17%。

Conclusion: 在交互设置中，代理的沟通方式与沟通内容同等重要，提供了一种隐私保护的框架来对齐代理并理解成功协作的交互模式。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [8] [AlphaResearch: Accelerating New Algorithm Discovery with Language Models](https://arxiv.org/abs/2511.08522)
*Zhaojian Yu,Kaiyue Feng,Yilun Zhao,Shilin He,Xiao-Ping Zhang,Arman Cohan*

Main category: cs.CL

TL;DR: AlphaResearch是一个自主研究代理，通过结合执行验证和模拟同行评审的双重研究环境，在开放性问题中发现新算法，在8个算法问题中达到2/8的胜率，并在圆打包问题上取得了最佳已知性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂但易于验证的问题上取得显著进展，但在发现未知知识方面仍有困难。本文旨在开发能够自主发现新算法的研究代理。

Method: 构建双重研究环境（执行验证+模拟同行评审），通过迭代执行三个步骤：提出新想法、在双重环境中验证、优化研究提案。

Result: AlphaResearch在8个开放算法问题竞赛中获得2/8的胜率，在圆打包问题上超越人类研究者和AlphaEvolve等基线方法，达到最佳已知性能。

Conclusion: 展示了利用LLMs加速算法发现的可能性，并对6/8失败案例进行了全面分析，为未来研究提供宝贵见解。

Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.

</details>


### [9] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 本文系统研究了大型推理模型（LRMs）通过思维链（CoT）进行监控的可行性和挑战，包括模型是否真实表达决策因素以及监控器可靠性问题，并提出了新的监控范式MoME。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的详细推理轨迹为AI安全监控提供了新机会，但面临两个关键挑战：模型可能不真实表达内部决策过程，以及监控器可能过于敏感或不够敏感。

Method: 通过实证研究和相关性分析，考察言语化质量、监控可靠性和LLM性能之间的关系，研究不同CoT干预方法对监控效果的影响，并提出MoME监控范式。

Result: 研究提供了跨数学、科学和伦理领域的实证证据，分析了言语化质量、监控可靠性和模型性能之间的相关性，并验证了不同CoT干预方法对监控有效性的影响。

Conclusion: CoT监控性具有潜力但面临挑战，需要进一步研究如何提高言语化真实性和监控可靠性，MoME范式为模型间监控提供了新思路。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [10] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 该研究分析了LLMs在角色扮演情境下的道德响应，通过道德基础问卷评估了道德易感性和道德鲁棒性，发现模型家族对道德鲁棒性影响最大，而模型大小对道德易感性影响更明显。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地在社交环境中使用，需要了解它们在角色扮演时如何表达和改变道德判断。

Method: 使用道德基础问卷(MFQ)创建基准，通过角色扮演提示让LLMs扮演特定角色，量化道德易感性和道德鲁棒性。

Result: Claude家族道德鲁棒性最强，GPT-4和Gemini次之；模型大小对道德易感性有显著影响，大模型更易受影响；道德鲁棒性和易感性呈正相关。

Conclusion: 角色扮演条件显著影响LLMs的道德行为，模型家族是决定道德鲁棒性的主要因素。

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [11] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 提出Think-at-Hard(TaH)方法，通过动态潜在思考机制，仅在困难token上进行深度迭代，避免简单token的过度思考问题，提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决循环transformer中存在的潜在过度思考现象——简单token在第一次前向传播后预测正确，但在额外迭代中被错误修正的问题。

Method: 使用轻量级神经决策器识别可能错误的困难token，仅在这些token上触发潜在迭代；采用LoRA模块将LLM目标从通用下一token预测转向专注困难token优化；引入双因果注意力机制扩展注意力维度。

Result: 在五个挑战性基准测试中提升LLM推理性能，相比对所有token迭代两次的基线方法，准确率提升8.1-11.3%，同时免除94%token的第二次迭代。

Conclusion: TaH方法能有效提升LLM推理能力，在相同参数量下实现更好的性能，解决了过度思考问题。

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [12] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: 提出了一种基于置信度的奖励模型，专门用于增强STEM推理能力，通过惩罚低置信度的正确答案来促进更稳健和逻辑一致的推理。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的奖励强化学习在小型模型上经常导致推理链质量差或推理过程与最终答案不一致的问题，限制了资源有限组织在小规模模型上进行直接强化学习训练。

Method: 开发了置信度奖励模型，不仅惩罚错误答案，还惩罚低置信度的正确答案；通过静态评估、Best-of-N推理测试和基于PPO的强化学习训练验证方法有效性。

Result: 该方法在多个STEM基准测试中优于多个最先进的开源奖励模型。

Conclusion: 置信度奖励模型能够有效提升STEM推理能力，促进更稳健和逻辑一致的推理过程。

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [13] [Procedural Knowledge Improves Agentic LLM Workflows](https://arxiv.org/abs/2511.07568)
*Vincent Hsiao,Mark Roberts,Leslie Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种利用分层任务网络（HTN）作为程序性知识来提升LLM在代理任务中性能的方法，实验证明手工编码的HTN能显著提升LLM表现，甚至让较小模型超越更大模型。


<details>
  <summary>Details</summary>
Motivation: LLM在执行代理任务时通常需要大量工具支持、提示工程或微调，而领域相关的程序性知识可以显著提高规划效率，但目前很少有研究评估其在需要隐式规划的代理任务中的潜力。

Method: 形式化、实现并评估了一种利用分层任务网络（HTN）作为程序性知识的代理LLM工作流程，包括手工编码HTN和LLM生成的HTN两种方式。

Result: 实验结果显示，手工编码的HTN能显著提升LLM在代理任务中的性能，使用HTN可以让20b或70b参数的LLM超越120b参数LLM基线；LLM生成的HTN也能改善性能，但效果较弱。

Conclusion: 利用人类、文档或LLM的专业知识来策划程序性知识将成为改进LLM工作流程的另一个重要工具。

Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.

</details>


### [14] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: Orion训练框架使紧凑模型(350M-1.2B参数)能够通过学习的搜索策略执行迭代检索，在多个基准测试中超越更大模型，表明检索性能来自学习策略而非模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法存在缺陷：神经检索器缺乏推理能力，LLM成本过高，查询重写或分解仅限于静态转换，无法满足复杂查询所需的迭代探索、反馈和修订动态。

Method: 结合：(1)合成轨迹生成和监督微调以鼓励多样化探索模式；(2)强化学习奖励有效查询细化和回溯行为；(3)推理时波束搜索算法利用RL期间学到的自反能力。

Result: 1.2B模型在SciFact上达到77.6%成功率(先前72.6%)，BRIGHT上25.2%(先前22.1%)，NFCorpus上63.2%(先前57.8%)，在FEVER、HotpotQA和MSMarco上保持竞争力，在六个基准中的五个超越200-400倍大的检索器。

Conclusion: 检索性能可以从学习策略中涌现，而不仅仅是模型规模，当模型被训练去搜索、反思和修订时。

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [15] [Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions](https://arxiv.org/abs/2511.07669)
*Alejandro R. Jadad*

Main category: cs.AI

TL;DR: 该研究提出了一个框架，通过7阶段校准序列和5层保护架构，使人类-AI团队在高风险战略决策中实现认知伙伴关系，避免可预防的认知陷阱和遗憾。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在可验证领域表现出色，但在高风险战略决策中可靠性不足，存在相互强化的认知偏见，威胁投资决策的可持续性。

Method: 对7个前沿级LLM和3个市场风险案例进行系统定性评估，开发包含7阶段校准序列、4阶段初始化过程和5层保护架构的框架，实现偏见自我监控、人机对抗挑战等功能。

Result: 实现了可维持的伙伴关系状态，发现可靠性在架构漂移和上下文耗尽时下降，跨模型验证显示不同LLM架构存在系统性性能差异。

Conclusion: 人类-AI团队可以通过有序校准实现认知伙伴关系，防止高风险决策中的可避免遗憾，满足依赖AI系统支持重大决策的投资回报期望。

Abstract: Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector.
  This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection.
  Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures.
  This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.

</details>


### [16] [AIA Forecaster: Technical Report](https://arxiv.org/abs/2511.07678)
*Rohan Alur,Bradly C. Stadie,Daniel Kang,Ryan Chen,Matt McManus,Michael Rickert,Tyler Lee,Michael Federici,Richard Zhu,Dennis Fogerty,Hayley Williamson,Nina Lozinski,Aaron Linsky,Jasjeet S. Sekhon*

Main category: cs.AI

TL;DR: AIA Forecaster是一个基于LLM的判断性预测系统，通过智能搜索、监督代理和统计校准技术，在ForecastBench上达到人类超级预测者的水平，并在预测市场基准上提供增量信息。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够利用非结构化数据进行专家级预测的AI系统，解决LLM在预测任务中的行为偏差问题，并建立可扩展的AI预测新标准。

Method: 结合三个核心要素：基于代理的高质量新闻搜索、协调不同预测的监督代理、以及对抗LLM行为偏差的统计校准技术。

Result: 在ForecastBench上表现与人类超级预测者相当，超越现有LLM基线；在预测市场基准上，与市场共识集成后优于单独使用市场共识。

Conclusion: 建立了AI预测的新技术水平，为未来研究提供了实用且可转移的建议，首次验证了大规模专家级预测的可行性。

Abstract: This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.

</details>


### [17] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 提出了ResearchRubrics基准，用于评估深度研究代理的能力，包含2800+小时人工构建的2500+专家评分标准，评估事实基础、推理合理性和清晰度。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理需要多步推理、跨文档综合和生成证据支持的长篇回答，但目前评估困难，因为回答冗长多样、存在多种有效解决方案且依赖动态信息源。

Method: 构建标准化的ResearchRubrics基准，包含领域多样的提示和精细评分标准；提出三维复杂性框架（概念广度、逻辑嵌套、探索性）；开发人工和模型评估协议。

Result: 评估多个最先进的深度研究系统，发现即使是领先的代理如Gemini和OpenAI的DR系统，平均符合度也低于68%，主要问题是遗漏隐式上下文和对检索信息推理不足。

Conclusion: 需要强大可扩展的深度研究能力评估方法，为此发布ResearchRubrics基准以促进研究助手的发展。

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [18] [Towards AI-Assisted Generation of Military Training Scenarios](https://arxiv.org/abs/2511.07690)
*Soham Hans,Volkan Ustun,Benjamin Nye,James Sterrett,Matthew Green*

Main category: cs.AI

TL;DR: 提出一个基于LLM的多智能体多模态推理框架，用于自动生成军事训练场景中的作战命令等关键文档，通过分层分解任务和专门的智能体协作来克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统军事训练场景生成过程复杂且资源密集，而前LLM时代的AI工具难以生成足够复杂和自适应的场景。需要一种能够自动生成高质量训练文档的新方法。

Method: 将场景生成分解为层次化子问题，使用专门的LLM智能体分别处理不同子任务。每个智能体接收前序智能体的输出，整合文本和视觉信息，通过多智能体协作确保逻辑一致性和文档准确性。

Result: 通过概念验证成功生成了作战命令的机动方案和移动部分，并准确估计了地图位置和移动路径，证明了框架的可行性和准确性。

Conclusion: LLM驱动的多智能体系统能够生成连贯、细致的文档，并动态适应变化条件，推动了军事训练场景生成的自动化进程。

Abstract: Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.

</details>


### [19] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: 提出基于心理学的Scaffold Reasoning框架用于代码调试，通过三个流（Scaffold、Analytic、Integration）实现高效推理，在DebugBench上达到88.91%通过率和5.36秒平均推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法在平衡复杂性和计算效率方面存在不足，缺乏对System 2推理的深入探索，需要借鉴心理学理论优化认知路径。

Method: 提出Scaffold Reasoning框架，包含Scaffold Stream（构建参考代码）、Analytic Stream（分析错误代码）和Integration Stream（整合结果），模拟人类认知过程。

Result: 在DebugBench上达到88.91%通过率，平均每问题推理时间5.36秒，优于其他推理方法，在不同难度问题和错误类型上均表现优异。

Conclusion: 该框架与人类认知过程一致，能有效平衡推理准确性和效率，为代码调试任务提供了心理学支持的优化方法。

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [20] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: 提出SparseRM方法，利用稀疏自编码器从LLM表示中提取偏好相关特征，构建轻量级可解释的奖励模型，在仅使用不到1%可训练参数的情况下优于主流奖励模型。


<details>
  <summary>Details</summary>
Motivation: 解决在有限资源下训练可靠奖励模型的挑战，因为传统方法依赖大规模偏好标注和昂贵的LLM微调成本。

Method: 使用稀疏自编码器分解LLM表示，提取偏好相关的可解释方向，将表示投影到这些方向计算对齐分数，通过简单奖励头聚合分数预测偏好得分。

Result: 在三个偏好建模任务上的实验表明，SparseRM在仅使用不到1%可训练参数的情况下，性能优于大多数主流奖励模型。

Conclusion: SparseRM能够无缝集成到下游对齐流程中，展示了其在高效对齐方面的潜力。

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [21] [Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning](https://arxiv.org/abs/2511.08301)
*Valentin Tablan,Scott Taylor,Gabriel Hurtado,Kristoffer Bernhem,Anders Uhrenholt,Gabriele Farei,Karo Moilanen*

Main category: cs.AI

TL;DR: Spark是一个共享智能体记忆架构，旨在模拟人类开发者社区的集体智能，让AI编程智能体能够贡献和获取持续演化的经验记忆，实现集体持续学习。


<details>
  <summary>Details</summary>
Motivation: 传统的人类中心软件开发知识共享环境正在被智能体中心实践所颠覆，AI智能体缺乏共享学习资源库，需要新的知识共享机制。

Method: 引入Spark共享智能体记忆架构，作为AI编程智能体的教练，提供持续演化的经验记忆库，智能体可以在相同问题空间中使用该记忆库进行知识共享和学习。

Result: Spark的推荐显著提高了通用代码生成模型的代码质量，300亿参数的小型模型在Spark加持下能够匹配更大规模最先进模型的代码质量，推荐的有用性在最高两个等级达到98.2%。

Conclusion: Spark架构成功模拟了人类开发者社区的集体智能，为AI编程智能体提供了有效的知识共享和学习机制，显著提升了代码生成质量。

Abstract: The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning.
  In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.

</details>


### [22] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: 提出Thinker模型，通过分层思维和多轮交互实现深度搜索，将复杂问题分解为可独立解决的子问题，并使用自然语言和逻辑函数双重表示来增强推理过程的监督性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用端到端强化学习训练LLMs利用外部检索器，但缺乏对推理过程的监督，难以保证逻辑连贯性和严谨性。

Method: 采用分层思维模型，将问题分解为子问题，每个子问题用自然语言和逻辑函数双重表示，通过逻辑函数传递依赖关系，并进行知识边界判定以避免不必要的外部搜索。

Result: 仅需数百个训练样本即可与现有基线竞争，在完整训练集上显著优于其他方法，在不同数据集和模型规模上均表现优异。

Conclusion: Thinker模型通过可监督的推理过程和逻辑一致性增强，有效提升了LLMs解决复杂问题的能力。

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [23] [Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://arxiv.org/abs/2511.07979)
*Wenhan Yu,Xinbo Lin,Lanxin Ni,Jinhua Cheng,Lei Sha*

Main category: cs.AI

TL;DR: 提出了MSLR，首个基于真实司法决策的中文多步法律推理数据集，采用IRAC框架建模结构化专家推理，并设计了可扩展的人机协作标注流程。


<details>
  <summary>Details</summary>
Motivation: 现有法律基准往往混淆事实记忆与真实推理，碎片化推理过程，忽视推理质量，需要更全面的法律推理评估框架。

Method: 基于IRAC框架构建多步法律推理数据集，采用人机协作标注流程生成细粒度推理标注，评估多种LLM在复杂法律推理任务上的表现。

Result: 多个LLM在MSLR上表现中等，表明适应复杂法律推理的挑战。自主生成的思维链提示优于人工设计的提示，提升推理连贯性和质量。

Conclusion: MSLR推动了LLM推理和思维链策略的发展，为未来研究提供了开放资源。

Abstract: Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook the quality of reasoning. To address these limitations, we introduce MSLR, the first Chinese multi-step legal reasoning dataset grounded in real-world judicial decision making. MSLR adopts the IRAC framework (Issue, Rule, Application, Conclusion) to model structured expert reasoning from official legal documents. In addition, we design a scalable Human-LLM collaborative annotation pipeline that efficiently produces fine-grained step-level reasoning annotations and provides a reusable methodological framework for multi-step reasoning datasets. Evaluation of multiple LLMs on MSLR shows only moderate performance, highlighting the challenges of adapting to complex legal reasoning. Further experiments demonstrate that Self-Initiated Chain-of-Thought prompts generated by models autonomously improve reasoning coherence and quality, outperforming human-designed prompts. MSLR contributes to advancing LLM reasoning and Chain-of-Thought strategies and offers open resources for future research. The dataset and code are available at https://github.com/yuwenhan07/MSLR-Bench and https://law.sjtu.edu.cn/flszyjzx/index.html.

</details>


### [24] [Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations](https://arxiv.org/abs/2511.08042)
*JV Roig*

Main category: cs.AI

TL;DR: KAMI v0.1是一个企业级智能体基准测试，解决了传统LLM基准测试的数据污染问题，并评估多步骤工具使用和不确定性决策等智能体能力。


<details>
  <summary>Details</summary>
Motivation: 企业采用智能体AI系统需要能够反映真实部署场景的可靠评估方法。传统LLM基准测试存在训练数据污染问题，且无法衡量智能体能力。

Method: 通过处理170,000个LLM测试项，处理超过55亿个token，涵盖35种模型配置，开发了Kamiwaza智能体价值指数(KAMI) v0.1基准测试。

Result: 传统基准测试排名无法准确预测实际智能体性能；新一代模型如Llama 4或Qwen 3在企业相关任务上并不总是优于旧版本；发现了成本-性能权衡、模型特定行为模式以及推理能力对token效率的影响。

Conclusion: KAMI基准测试为企业部署决策提供了关键见解，强调需要专门的企业级智能体评估方法。

Abstract: Enterprise adoption of agentic AI systems requires reliable evaluation methods that reflect real-world deployment scenarios. Traditional LLM benchmarks suffer from training data contamination and fail to measure agentic capabilities such as multi-step tool use and decision-making under uncertainty. We present the Kamiwaza Agentic Merit Index (KAMI) v0.1, an enterprise-focused benchmark that addresses both contamination resistance and agentic evaluation. Through 170,000 LLM test items processing over 5.5 billion tokens across 35 model configurations, we demonstrate that traditional benchmark rankings poorly predict practical agentic performance. Notably, newer generation models like Llama 4 or Qwen 3 do not always outperform their older generation variants on enterprise-relevant tasks, contradicting traditional benchmark trends. We also present insights on cost-performance tradeoffs, model-specific behavioral patterns, and the impact of reasoning capabilities on token efficiency -- findings critical for enterprises making deployment decisions.

</details>


### [25] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 本文提出信息容量作为衡量LLM效率的统一指标，基于文本压缩性能与计算复杂度的关系，能够公平比较不同模型系列间的效率并预测同系列模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于LLM快速发展及其广泛应用导致计算资源需求激增，测试时扩展进一步加剧了模型能力与资源消耗之间的紧张关系，但目前缺乏能够准确反映不同规模和架构LLM效率的统一指标。

Method: 引入信息容量指标，基于文本压缩性能相对于计算复杂度的关系来衡量模型效率。通过评估49个模型在5个异构数据集上的表现，分析分词器效率、预训练数据和专家混合架构的影响。

Result: 实证评估显示，同一系列中不同规模的模型具有一致的信息容量。该指标能够公平比较模型系列间的效率，并准确预测同系列模型的性能。分词器效率对信息容量有显著影响。

Conclusion: 信息容量是一个有效的LLM效率评估指标，能够统一衡量不同模型规模和架构的效率，并考虑了常被忽视的分词器效率因素。

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [26] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: SciAgent是一个统一的多智能体系统，旨在实现通用的科学推理能力，能够跨学科和难度级别自适应推理策略。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在特定科学任务上达到专家水平，但仍局限于狭窄领域且需要手工定制。需要开发能够跨学科进行连贯推理的通用科学智能系统。

Method: 采用分层多智能体架构：协调器智能体分析问题领域和复杂度，动态编排由符号推理、概念建模、数值计算和验证子智能体组成的专业工作系统，协作构建和优化针对每个任务的推理管道。

Result: 在数学和物理奥林匹克竞赛（IMO、IMC、IPhO、CPhO）中，SciAgent持续达到或超越人类金牌得主表现，并在国际化学奥林匹克和Humanity's Last Exam基准测试中进一步验证了跨领域泛化能力。

Conclusion: SciAgent代表了向通用科学智能迈出的具体一步，即能够在专家水平上进行连贯、跨学科推理的AI系统。

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [27] [Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning](https://arxiv.org/abs/2511.08234)
*Zhihao Lin*

Main category: cs.AI

TL;DR: 提出Geometric Action Control (GAC)方法，通过方向向量和可学习浓度参数分解动作生成，在保持球形分布几何优势的同时简化计算，在连续控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高斯策略在深度强化学习的连续控制中占主导地位，但其无界支持需要临时压缩函数来扭曲有界动作空间的几何结构。虽然vMF分布提供了理论上的替代方案，但依赖贝塞尔函数和拒绝采样阻碍了实际应用。

Method: GAC将动作生成分解为方向向量和可学习浓度参数，实现确定性动作和均匀球形噪声之间的高效插值。该设计将参数数量从2d减少到d+1，避免了vMF拒绝采样的O(dk)复杂度，实现简单的O(d)操作。

Result: 在六个MuJoCo基准测试中，GAC始终匹配或超越最先进方法，在Ant-v4上比SAC提高37.6%，在6个任务中的4个上取得最佳结果。消融研究表明球形归一化和自适应浓度控制对GAC的成功至关重要。

Conclusion: 稳健高效的连续控制不需要复杂的分布，而是需要对动作空间几何结构的原理性尊重。

Abstract: Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \(2d\) to \(d+1\), and avoids the \(O(dk)\) complexity of vMF rejection sampling, achieving simple \(O(d)\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \textbf{spherical normalization} and \textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.

</details>


### [28] [Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents](https://arxiv.org/abs/2511.08242)
*Waseem AlShikh,Muayad Sayed Ali,Brian Kennedy,Dmytro Mozolevskyi*

Main category: cs.AI

TL;DR: 提出11个基于结果、任务无关的AI代理性能评估指标框架，通过大规模模拟实验验证了混合代理架构在多个领域表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有基于基础设施的指标（如延迟、吞吐量）无法评估代理的决策质量、操作自主性和业务价值，需要更全面的评估框架

Method: 提出11个结果导向的指标（如目标完成率、自主性指数、多步骤任务韧性等），在5个不同领域（医疗、金融、营销、法律、客服）对4种代理架构进行大规模模拟实验

Result: 混合代理在大多数指标上表现最稳定，平均目标完成率达88.8%，投资回报率最高，不同代理设计之间存在显著的性能权衡

Conclusion: 该工作为AI代理的全面评估提供了标准化方法，有助于更有效的开发、部署和治理

Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.

</details>


### [29] [Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs](https://arxiv.org/abs/2511.08274)
*Anton Gusarov,Anastasia Volkova,Valentin Khrulkov,Andrey Kuznetsov,Evgenii Maslov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出了Multi-Agent GraphRAG系统，利用Cypher查询语言和标签属性图数据库作为可扩展的推理引擎，填补了现有GraphRAG研究主要关注RDF知识图谱的空白。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG方法主要基于RDF知识图谱和SPARQL查询，但Cypher和标签属性图数据库作为可扩展有效推理引擎在GraphRAG中的潜力尚未充分探索。

Method: 构建模块化LLM代理系统，用于文本到Cypher查询生成，作为LPG图数据的自然语言接口。采用迭代内容感知校正和规范化，通过聚合反馈循环确保生成查询的语义和语法优化。

Result: 在CypherBench图数据集上评估系统，涵盖多个通用领域的不同查询类型。还在基于IFC数据的属性图上展示性能，证明该方法能够桥接AI与真实世界应用。

Conclusion: 该方法能够实现工业数字自动化用例，为GraphRAG提供了基于Cypher和标签属性图的新范式。

Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.

</details>


### [30] [A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models](https://arxiv.org/abs/2511.08548)
*Shubhra Mishra,Yuka Machino,Gabriel Poesia,Albert Jiang,Joy Hsu,Adrian Weller,Challenger Mishra,David Broman,Joshua B. Tenenbaum,Mateja Jamnik,Cedegao E. Zhang,Katherine M. Collins*

Main category: cs.AI

TL;DR: 该研究通过两个实证研究分析了人类与LLM在数学问题趣味性和难度评估上的一致性，发现LLM虽然大致同意人类的趣味性概念，但无法完全捕捉人类判断的分布，且与人类选择趣味性理由的相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统（如LLM）越来越多地参与数学研究和教育，了解它们对人类趣味性判断的匹配程度变得重要，这对于数学AI思维伙伴关系的发展至关重要。

Method: 通过两个实证研究，分别针对众包平台参与者和国际数学奥林匹克竞赛选手，比较人类与多种LLM对数学问题趣味性和难度的评估。

Result: LLM在数学趣味性判断上与人类有大致共识，但无法完全复制人类判断的分布模式，且与人类选择的趣味性理由相关性较弱。

Conclusion: 当前LLM在捕捉人类数学趣味性判断方面既有前景也有局限，需要进一步改进才能成为有效的数学AI思维伙伴。

Abstract: The evolution of mathematics has been guided in part by interestingness. From researchers choosing which problems to tackle next, to students deciding which ones to engage with, people's choices are often guided by judgments about how interesting or challenging problems are likely to be. As AI systems, such as LLMs, increasingly participate in mathematics with people -- whether for advanced research or education -- it becomes important to understand how well their judgments align with human ones. Our work examines this alignment through two empirical studies of human and LLM assessment of mathematical interestingness and difficulty, spanning a range of mathematical experience. We study two groups: participants from a crowdsourcing platform and International Math Olympiad competitors. We show that while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments. Moreover, most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales. Together, our findings highlight both the promises and limitations of current LLMs in capturing human interestingness judgments for mathematical AI thought partnerships.

</details>


### [31] [Simulating the Visual World with Artificial Intelligence: A Roadmap](https://arxiv.org/abs/2511.08585)
*Jingtong Yue,Ziqi Huang,Zhaoxi Chen,Xintao Wang,Pengfei Wan,Ziwei Liu*

Main category: cs.AI

TL;DR: 该论文将视频生成模型演进为包含隐式世界模型和视频渲染器的视频基础模型，这些模型不仅能生成视觉内容，还能模拟物理动态、交互和任务规划，最终形成具有物理合理性、多模态交互和多尺度规划能力的世界模型。


<details>
  <summary>Details</summary>
Motivation: 视频生成领域正从单纯生成视觉吸引力的片段转向构建支持交互和保持物理合理性的虚拟环境，这指向了视频基础模型的发展，这些模型不仅作为视觉生成器，还作为隐式世界模型。

Method: 将现代视频基础模型概念化为两个核心组件的组合：隐式世界模型和视频渲染器。世界模型编码关于世界的结构化知识，视频渲染器将潜在模拟转化为逼真的视觉观察。

Result: 论文追溯了视频生成的四个代际演进，每一代核心能力逐步提升，最终形成具有内在物理合理性、实时多模态交互和跨多时空尺度规划能力的世界模型。

Conclusion: 讨论了下一代世界模型的开放挑战和设计原则，包括智能体智能在塑造和评估这些系统中的作用。

Abstract: The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a "window" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [32] [AI's 70% Problem](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzed.dev%2Fblog%2Fai-70-problem-addy-osmani%3Futm_source=tldrdevops/1/0100019a6da89c99-6da40ba9-328b-40c9-9214-53060dd5e732-000000/gMg2lywpaCjFqmGg0blhrLpNFzQVuuKXwGE8nO1t0PQ=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编码工具能快速生成约70%的解决方案，但剩下的30%（包括边缘情况处理、调试、生产集成和安全性）仍需人工专业知识。尽管采用率增长，但对AI生成代码的信任度在下降。


<details>
  <summary>Details</summary>
Motivation: 探讨AI编码工具在实际应用中的局限性，特别是虽然能快速生成大部分代码，但在关键环节仍需人工干预，以及信任度下降的问题。

Method: 分析AI编码工具的能力边界，识别其在边缘情况处理、调试、生产集成和安全性方面的不足。

Result: 发现AI工具只能解决约70%的编码问题，剩余30%的关键部分需要人类专业知识，且对AI生成代码的信任度呈下降趋势。

Conclusion: 开发者需要优先考虑人工理解、代码审查和问责制，因为自动化将瓶颈从编写代码转移到审查代码。

Abstract: AI's 70% Problem (4 minute read) AI coding tools can produce about 70% of a solution quickly, but the remaining 30%—covering edge cases, debugging, production integration, and security—still demands human expertise. Despite growing adoption, trust in AI-generated code is declining. Developers need to prioritize human understanding, code review, and accountability as automation shifts bottlenecks from writing code to reviewing it.

</details>


### [33] [Built Technologies launches AI agent to automate CRE lending](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F11%2F06%2Fbuilt-technologies-ai-draw-agent-cre-lending.html%3Futm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/lrPggasAUf9PrS8EBhM7TbWWwo8i2_YMeHralCenb2Q=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Built Technologies推出AI代理自动化商业地产贷款提款流程，处理速度提升95%，风险检测提高400%


<details>
  <summary>Details</summary>
Motivation: 传统商业地产贷款提款流程效率低下，需要自动化来提高处理速度和风险检测能力

Method: 开发AI代理系统"Draw Agent"，用于自动化贷款提款请求处理，确保完全符合贷款机构政策

Result: 处理速度提升95%，风险检测能力提高400%，已被美国银行、花旗银行等主要银行采用

Conclusion: AI代理能显著提升商业地产贷款流程的效率和风险管理水平

Abstract: Built Technologies launches AI agent to automate CRE lending (3 minute read) Built Technologies, a $1.5B-valued fintech focused on construction and real estate finance, has launched an AI agent to automate draw requests—the process developers use to access new tranches of loan funding. The “Draw Agent” processes approvals up to 95% faster, with 400% higher risk detection and full compliance to lender policies. Used by banks like US Bank, Citi, and Fifth Third, the agent has delivered 300–500%...

</details>


### [34] [Lloyds Banking to launch AI-powered financial assistant in 2026](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.retailbankerinternational.com%2Fnews%2Flloyds-banking-ai-financial-assistant%2F%3Fcf-view%26utm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/iw-lLFC6Pe7a_709qRSpgFSHlBydOLaS5Gs_RV60G7o=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Lloyds Banking将在2026年推出AI驱动的金融助手，这是英国消费银行业首次使用智能代理AI


<details>
  <summary>Details</summary>
Motivation: 为2100万客户提供个性化金融服务，通过自然对话帮助用户管理支出、储蓄和投资

Method: 开发AI驱动的对话式金融助手，已在7000名员工中进行测试，通过Lloyds应用提供自然对话界面

Result: 助手正在测试中，预计将逐步处理抵押贷款和汽车金融等更复杂的金融服务

Conclusion: 这是英国银行业首次应用智能代理AI，标志着金融服务向更个性化和对话式体验的转变

Abstract: Lloyds Banking to launch AI-powered financial assistant in 2026 (5 minute read) Lloyds is preparing to roll out an AI-driven conversational financial assistant in early 2026, marking the UK's first use of agentic AI in consumer banking. The assistant, already being tested by 7,000 employees, will help users manage spending, savings, and investments through natural dialogue in the Lloyds app. Over time, it's expected to handle mortgages and car finance, giving 21 million customers personalized...

</details>


### [35] [Ransomvibing appears in VS Code extensions](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecureannex.com%2Fblog%2Fransomvibe%2F%3Futm_source=tldrinfosec/1/0100019a6e18786c-4be738f4-f44c-42bb-b92d-cb2d4af5653b-000000/c8jsS5_7bKiVlcpbTx5vgVXloY5hYk8mdSMHlki_lKo=430)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 在VS Code扩展中发现勒索软件感染，名为'susvsex'的扩展通过GitHub进行命令控制，加密文件并勒索，但由于代码质量差和硬编码解密密钥，目前威胁较低。


<details>
  <summary>Details</summary>
Motivation: 揭示VS Code扩展市场的安全漏洞，提醒开发者注意扩展安全风险。

Method: 分析恶意扩展'susvsex'的技术实现，包括其使用GitHub进行C&C通信、文件加密机制和勒索流程。

Result: 发现该勒索软件扩展存在严重编码缺陷，硬编码解密密钥使其威胁性大大降低，但暴露了扩展市场的安全审查不足。

Conclusion: VS Code扩展市场需要加强安全审查机制，开发者应谨慎选择和使用第三方扩展。

Abstract: Ransomvibing appears in VS Code extensions (5 minute read) A ransomware-infected VS Code extension, "susvsex," was found on the Visual Studio Marketplace. It utilizes GitHub for command and control, encrypts files, and uploads them for extortion, but its malicious intent was clearly evident. Due to poor coding and a hardcoded decryption key, its threat is currently low. The incident highlights gaps in extension security.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Provably Efficient Sample Complexity for Robust CMDP](https://arxiv.org/abs/2511.07486)
*Sourav Ganguly,Arnob Ghosh*

Main category: cs.LG

TL;DR: 本文研究了在安全约束下学习策略的问题，提出了首个具有样本复杂度保证的鲁棒约束MDP算法RCVI，解决了当真实环境与模拟器不同时的鲁棒约束强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有工作虽然为RCMDPs建立了有限时间迭代复杂度保证，但样本复杂度保证仍然未被充分探索。需要解决马尔可夫策略在矩形不确定性集下可能不是最优的问题。

Method: 引入增强状态空间，将剩余效用预算纳入状态表示，提出新颖的鲁棒约束值迭代(RCVI)算法，使用生成模型实现样本复杂度保证。

Result: RCVI算法实现了$\mathcal{\tilde{O}}(|S||A|H^5/ε^2)$的样本复杂度，在最多$ε$违反约束的情况下达到最优性能，这是RCMDP的第一个样本复杂度保证。

Conclusion: 该工作为鲁棒约束强化学习提供了首个样本复杂度理论保证，并通过实证结果验证了方法的有效性。

Abstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/ε^2)$ achieving at most $ε$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.

</details>


### [37] [Diffusion Guided Adversarial State Perturbations in Reinforcement Learning](https://arxiv.org/abs/2511.07701)
*Xiaolin Sun,Feidi Liu,Zhengming Ding,ZiZhan Zheng*

Main category: cs.LG

TL;DR: SHIFT是一种基于扩散模型的策略无关状态扰动攻击方法，能够生成语义不同但保持真实性和历史一致性的扰动状态，有效突破现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有RL系统在视觉环境中容易受到对抗攻击，而当前防御方法的有效性主要源于现有l_p范数约束攻击的局限性，这些攻击即使在较大扰动预算下也难以改变图像输入的语义。

Method: 提出SHIFT攻击方法，利用扩散模型生成语义不同但保持真实性和历史一致性的扰动状态，是一种策略无关的状态扰动攻击。

Result: 评估显示SHIFT攻击能有效突破现有防御机制（包括最复杂的防御），显著优于现有攻击方法，同时具有更好的感知隐蔽性。

Conclusion: 结果表明RL代理对语义感知的对抗扰动存在脆弱性，强调了开发更鲁棒策略的重要性。

Abstract: Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.

</details>


### [38] [Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework](https://arxiv.org/abs/2511.07702)
*Meraj Hassanzadeh,Ehsan Ghaderi,Mohamad Ali Bijarchi,Siamak Kazemzadeh Hannani*

Main category: cs.LG

TL;DR: 提出了一种基于科学机器学习的多维优化框架，使用深度强化学习代理与物理信息神经网络交互，实现快速优化，在微混合器案例中效率提升达32%。


<details>
  <summary>Details</summary>
Motivation: 传统仿真优化方法存在只能单问题优化、计算时间长等局限性，需要开发能够快速解决多维优化问题的新方法。

Method: 使用深度强化学习代理作为优化器，与参数化物理信息神经网络环境交互，通过代理探索问题参数间关系来寻找最优解。

Result: 在微混合器案例中，经过训练的代理在宽范围Schmidt数下均实现高于基线的效率，最大效率出现在Schmidt数为13.3时，提升约32%。

Conclusion: 该方法相比传统数值方法和遗传算法具有显著优势，能够快速提供多维优化问题的即时解。

Abstract: Multidimensional optimization has consistently been a critical challenge in engineering. However, traditional simulation-based optimization methods have long been plagued by significant limitations: they are typically capable of optimizing only a single problem at a time and require substantial computational time for meshing and numerical simulation. This paper introduces a novel framework leveraging cutting-edge Scientific Machine Learning (Sci-ML) methodologies to overcome these inherent drawbacks of conventional approaches. The proposed method provides instantaneous solutions to a spectrum of complex, multidimensional optimization problems. A micromixer case study is employed to demonstrate this methodology. An agent, operating on a Deep Reinforcement Learning (DRL) architecture, serves as the optimizer to explore the relationships between key problem parameters. This optimizer interacts with an environment constituted by a parametric Physics-Informed Neural Network (PINN), which responds to the agent's actions at a significantly higher speed than traditional numerical methods. The agent's objective, conditioned on the Schmidt number is to discover the optimal geometric and physical parameters that maximize the micromixer's efficiency. After training the agent across a wide range of Schmidt numbers, we analyzed the resulting optimal designs. Across this entire spectrum, the achieved efficiency was consistently greater than the baseline, normalized value. The maximum efficiency occurred at a Schmidt number of 13.3, demonstrating an improvement of approximately 32%. Finally, a comparative analysis with a Genetic Algorithm was conducted under equivalent conditions to underscore the advantages of the proposed method.

</details>


### [39] [MURPHY: Multi-Turn GRPO for Self Correcting Code Generation](https://arxiv.org/abs/2511.07833)
*Chanakya Ekbote,Vijay Lingam,Behrooz Omidvar-Tehrani,Jun Huan,Sujay Sanghavi,Anoop Deoras,Stefano Soatto*

Main category: cs.LG

TL;DR: Murphy是一个多轮反思优化框架，通过结合迭代自我校正来扩展GRPO，在代码生成基准测试中相比GRPO实现了高达8%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法如GRPO在推理基准上有效，但在需要迭代决策的代理任务上表现不佳。

Method: 引入多轮反思优化框架Murphy，在训练过程中结合迭代自我校正，利用定量和定性执行反馈来逐步改进推理。

Result: 在Qwen和OLMo等模型家族的代码生成基准测试中，Murphy相比GRPO在相同计算预算下实现了高达8%的相对pass@1提升。

Conclusion: Murphy框架通过多轮反思优化有效提升了语言模型在代理任务中的推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.

</details>


### [40] [Test-driven Reinforcement Learning](https://arxiv.org/abs/2511.07904)
*Zhao Yu,Xiuping Wu,Liangjun Ke*

Main category: cs.LG

TL;DR: 提出了一种基于测试驱动的强化学习框架，用多个测试函数替代单一奖励函数来定义任务目标，简化了奖励设计过程并支持多目标优化。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中的奖励函数设计困难，因为奖励函数既要定义最优目标又要指导学习过程，这往往导致任务表示不理想。

Method: 使用通过-失败测试和指示性测试来分别定义最优目标和指导学习过程，采用词典序启发式方法比较轨迹与最优轨迹集的相对距离关系，并开发了TdRL算法实现。

Result: 在DeepMind Control Suite基准测试中，TdRL在策略训练上匹配或优于手工设计奖励的方法，具有更高的设计简单性和对多目标优化的内在支持。

Conclusion: TdRL为表示任务目标提供了新的视角，有助于解决强化学习应用中的奖励设计挑战。

Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.

</details>


### [41] [CellARC: Measuring Intelligence with Cellular Automata](https://arxiv.org/abs/2511.07908)
*Miroslav Lžičař*

Main category: cs.LG

TL;DR: CellARC是一个基于一维多色元胞自动机的抽象推理合成基准，包含95k训练集和2k测试集，支持快速迭代并评估多种模型性能。


<details>
  <summary>Details</summary>
Motivation: 为抽象推理研究提供可控任务空间，解耦泛化能力与人类先验知识，支持可重复的规则推断研究。

Method: 使用元胞自动机构建基准，控制字母表大小、半径、规则族等参数，评估符号、循环、卷积、Transformer、递归和LLM等基线模型。

Result: 10M参数Transformer在插值/外推集上达到58.0%/32.4%准确率，GPT-5 High达到62.3%/48.1%，神经符号集成方法达到65.4%/35.5%。

Conclusion: CellARC支持难度可控的采样和可重复研究，展示了神经符号方法的互补性，为小模型快速推理规则提供了有效基准。

Abstract: We introduce CellARC, a synthetic benchmark for abstraction and reasoning built from multicolor 1D cellular automata (CA). Each episode has five support pairs and one query serialized in 256 tokens, enabling rapid iteration with small models while exposing a controllable task space with explicit knobs for alphabet size k, radius r, rule family, Langton's lambda, query coverage, and cell entropy. We release 95k training episodes plus two 1k test splits (interpolation/extrapolation) and evaluate symbolic, recurrent, convolutional, transformer, recursive, and LLM baselines. CellARC decouples generalization from anthropomorphic priors, supports unlimited difficulty-controlled sampling, and enables reproducible studies of how quickly models infer new rules under tight budgets. Our strongest small-model baseline (a 10M-parameter vanilla transformer) outperforms recent recursive models (TRM, HRM), reaching 58.0%/32.4% per-token accuracy on the interpolation/extrapolation splits, while a large closed model (GPT-5 High) attains 62.3%/48.1% on subsets of 100 test tasks. An ensemble that chooses per episode between the Transformer and the best symbolic baseline reaches 65.4%/35.5%, highlighting neuro-symbolic complementarity. Leaderboard: https://cellarc.mireklzicar.com

</details>


### [42] [Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison](https://arxiv.org/abs/2511.07919)
*Yoonho Lee,Joseph Boen,Chelsea Finn*

Main category: cs.LG

TL;DR: Feedback Descent是一个通过结构化文本反馈优化文本产物（提示、代码、分子）的框架，无需依赖标量奖励，通过保留详细批评而非压缩为二元偏好来拓宽偏好学习的信息瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有方法将判断压缩为单个比特，导致信息丢失。Feedback Descent旨在通过高带宽的文本反馈提供更丰富的监督信息，实现文本空间的定向优化。

Method: 使用上下文学习将结构化反馈转化为类似梯度的方向信息，实现目标编辑。评估器为每个比较配对文本反馈，纯推理时间迭代，无需修改模型权重，任务无关。

Result: 在三个不同领域评估中，Feedback Descent优于最先进的提示优化方法（GEPA）、强化学习方法（GRPO、REINVENT）以及专门的基于图的分子优化器。在DOCKSTRING分子发现基准测试中，识别出超过260,000个化合物数据库中99.9%百分位的新型类药物分子。

Conclusion: Feedback Descent通过结构化文本反馈实现了有效的文本产物优化，在多个领域超越现有方法，证明了高带宽反馈在优化任务中的重要性。

Abstract: We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.

</details>


### [43] [SERL: Self-Examining Reinforcement Learning on Open-Domain](https://arxiv.org/abs/2511.07922)
*Weixuan Ou,Yanzhao Zheng,Shuoshuo Sun,Wei Zhang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Pengwei Yan,Yifan Qiao*

Main category: cs.LG

TL;DR: 提出了SERL（自我检验强化学习）框架，让LLM同时作为Actor和Judge，通过两种内部奖励机制实现自我改进，无需外部信号。


<details>
  <summary>Details</summary>
Motivation: 解决开放领域任务中RL面临的两个挑战：主观性任务无法提供可验证奖励，以及RLHF依赖外部奖励机制。

Method: SERL框架包含两种奖励机制：基于Copeland风格成对比较的Actor奖励，和鼓励一致判断的Judge自一致性奖励。

Result: 在AlpacaEval 2上，Qwen3-8B的LC胜率从52.37%提升到59.90%，达到自我改进方法中的最先进水平，性能可与Qwen3-32B等更大模型相媲美。

Conclusion: SERL在开放领域任务上表现出优越的有效性和鲁棒性，为LLM自我改进提供了新思路。

Abstract: Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.

</details>


### [44] [Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning](https://arxiv.org/abs/2511.07971)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: LOREN是一种曲率感知的零阶优化方法，用于微调大语言模型，通过自适应估计各向异性扰动分布、低秩块对角预处理器和REINFORCE留一法梯度估计器来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零阶方法使用随机扰动通过有限差分估计梯度，存在高方差和次优搜索方向的问题，需要改进梯度估计的质量和效率。

Method: 将梯度预条件问题重新表述为自适应估计各向异性扰动分布；使用自然进化策略框架通过低秩块对角预处理器捕捉曲率；应用REINFORCE留一法梯度估计器减少方差。

Result: 在标准LLM基准测试中，LOREN优于最先进的零阶方法，实现了更高的准确性和更快的收敛速度，与MeZO-Adam相比峰值内存使用减少高达27.3%。

Conclusion: LOREN通过曲率感知的零阶优化方法有效解决了现有方法的局限性，在保持内存效率的同时提升了微调性能。

Abstract: We introduce LOREN, a curvature-aware zeroth-order (ZO) optimization method for fine-tuning large language models (LLMs). Existing ZO methods, which estimate gradients via finite differences using random perturbations, often suffer from high variance and suboptimal search directions. Our approach addresses these challenges by: (i) reformulating the problem of gradient preconditioning as that of adaptively estimating an anisotropic perturbation distribution for gradient estimation, (ii) capturing curvature through a low-rank block diagonal preconditioner using the framework of natural evolution strategies, and (iii) applying a REINFORCE leave-one-out (RLOO) gradient estimator to reduce variance. Experiments on standard LLM benchmarks show that our method outperforms state-of-the-art ZO methods by achieving higher accuracy and faster convergence, while cutting peak memory usage by up to 27.3% compared with MeZO-Adam.

</details>


### [45] [SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories](https://arxiv.org/abs/2511.08136)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出SafeMIL方法，通过多示例学习从非偏好轨迹中学习风险成本函数，用于离线安全模仿学习，确保策略在满足安全约束的同时不降低奖励性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中在线交互存在风险，且难以精确指定每个时间步的奖励和安全成本信息，但收集反映不良或风险行为的轨迹相对可行。

Method: 使用多示例学习学习参数化成本函数，预测状态-动作对的风险程度，然后利用该成本函数避免非偏好行为，生成优先考虑安全性的策略。

Result: 实验证明该方法能学习到满足成本约束的更安全策略，且不降低奖励性能，优于多个基线方法。

Conclusion: SafeMIL方法有效解决了离线安全模仿学习问题，通过非偏好轨迹学习风险成本，实现安全优先的策略学习。

Abstract: In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.

</details>


### [46] [BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services](https://arxiv.org/abs/2511.08142)
*Anna Lackinger,Andrea Morichetta,Pantelis A. Frangoudis,Schahram Dustdar*

Main category: cs.LG

TL;DR: 提出了BIPPO（预算感知独立近端策略优化），一种用于联邦学习中客户端选择的节能多智能体强化学习解决方案，在资源受限的物联网环境中提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然保证了负载分布和隐私，但未原生考虑基础设施效率，特别是在资源受限环境中。现有的基于强化学习的客户端选择方法未充分考虑资源限制和设备流失等基础设施挑战，且缺乏通用性和能效优化。

Method: BIPPO是一种节能的多智能体强化学习解决方案，采用预算感知的独立近端策略优化方法，改进客户端选择策略。

Result: 在高度预算受限设置下，BIPPO在非IID数据上训练时，相比非RL机制、传统PPO和IPPO提高了平均准确率，且仅消耗可忽略的预算比例，即使客户端数量增加也能保持稳定。

Conclusion: BIPPO为物联网联邦学习中的客户端选择提供了高性能、稳定、可扩展和可持续的解决方案。

Abstract: Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.

</details>


### [47] [PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore](https://arxiv.org/abs/2511.08241)
*Zhihao Lin,Lin Wu,Zhen Tian,Jianglin Lan*

Main category: cs.LG

TL;DR: PrefPoE是一个基于偏好-专家乘积框架的强化学习探索方法，通过优势引导的智能探索来解决熵最大化带来的高方差问题，在多个控制任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中的探索方法（如熵最大化）通常导致高方差和低效的策略更新，需要一种更智能的探索机制来平衡探索与利用。

Method: 提出PrefPoE框架，训练偏好网络来集中概率质量于高优势动作，通过专家乘积（PoE）融合与主策略，创建软信任区域来稳定策略更新。

Result: 在连续和离散动作空间的各种控制任务中取得显著改进：HalfCheetah-v4提升321%，Ant-v4提升69%，LunarLander-v2提升276%，训练稳定性和样本效率均有提升。

Conclusion: 通过优势引导的偏好学习来指导探索方向与学习如何行动同等重要，为策略梯度方法提供了一个通用的增强框架。

Abstract: Exploration in reinforcement learning remains a critical challenge, as naive entropy maximization often results in high variance and inefficient policy updates. We introduce \textbf{PrefPoE}, a novel \textit{Preference-Product-of-Experts} framework that performs intelligent, advantage-guided exploration via the first principled application of product-of-experts (PoE) fusion for single-task exploration-exploitation balancing. By training a preference network to concentrate probability mass on high-advantage actions and fusing it with the main policy through PoE, PrefPoE creates a \textbf{soft trust region} that stabilizes policy updates while maintaining targeted exploration. Across diverse control tasks spanning both continuous and discrete action spaces, PrefPoE demonstrates consistent improvements: +321\% on HalfCheetah-v4 (1276~$\rightarrow$~5375), +69\% on Ant-v4, +276\% on LunarLander-v2, with consistently enhanced training stability and sample efficiency. Unlike standard PPO, which suffers from entropy collapse, PrefPoE sustains adaptive exploration through its unique dynamics, thereby preventing premature convergence and enabling superior performance. Our results establish that learning \textit{where to explore} through advantage-guided preferences is as crucial as learning how to act, offering a general framework for enhancing policy gradient methods across the full spectrum of reinforcement learning domains. Code and pretrained models are available in supplementary materials.

</details>


### [48] [LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration](https://arxiv.org/abs/2511.08339)
*Ruiyu Qiu,Rui Wang,Guanghui Yang,Xiang Li,Zhijiang Shao*

Main category: cs.LG

TL;DR: 提出LPPG-RL框架，通过顺序梯度投影解决词典序多目标强化学习问题，适用于连续空间，无需启发式阈值调整，在2D导航环境中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统安全强化学习和多目标强化学习方法难以有效处理具有明确优先级的词典序多目标问题，现有LMORL方法要么依赖先验知识进行启发式阈值调整，要么仅限于离散域。

Method: 使用顺序梯度投影识别可行的策略更新方向，将投影步骤重新表述为优化问题，利用Dykstra投影而非通用求解器加速计算，并引入子问题探索防止梯度消失。

Result: 在2D导航环境中，LPPG-RL优于现有最先进的连续LMORL方法，提供了收敛性理论保证和政策改进下界。

Conclusion: LPPG-RL是一个广泛兼容所有策略梯度算法的LMORL框架，特别适用于中小规模实例，通过梯度投影和子问题探索实现了高效稳定的性能。

Abstract: Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.

</details>


### [49] [ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games](https://arxiv.org/abs/2511.08412)
*Ruochuan Shi,Runyu Lu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.LG

TL;DR: 提出了ARAC框架，结合注意力图神经网络和自适应发散正则化机制，解决图结构多智能体强化学习中的稀疏奖励问题，在追捕和对抗任务中实现更快的收敛和更高的成功率。


<details>
  <summary>Details</summary>
Motivation: 在图结构多智能体强化学习的对抗任务中，智能体需要在高度动态的交互下协调，稀疏奖励阻碍了策略学习效率。需要一种既能利用参考策略进行有效探索，又能避免继承其局限性的方法。

Method: ARAC框架集成注意力图神经网络建模智能体依赖关系，以及自适应发散正则化机制。GNN表达图环境中的空间关系和状态特征，自适应机制在训练早期利用参考策略进行探索，后期减少依赖以避免次优收敛。

Result: 在追捕和对抗场景的实验中，ARAC相比多智能体强化学习基线方法实现了更快的收敛速度、更高的最终成功率，并在不同智能体数量下展现出更强的可扩展性。

Conclusion: ARAC在复杂图结构环境中表现出色，自适应发散正则化机制有效平衡了利用参考策略和避免其局限性的权衡。

Abstract: In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.

</details>


### [50] [The Path Not Taken: RLVR Provably Learns Off the Principals](https://arxiv.org/abs/2511.08567)
*Hanqing Zhu,Zhenyu Zhang,Hanxian Huang,DiJia Su,Zechun Liu,Jiawei Zhao,Igor Fedorov,Hamed Pirsiavash,Zhizhou Sha,Jinwon Lee,David Z. Pan,Zhangyang Wang,Yuandong Tian,Kai Sheng Tai*

Main category: cs.LG

TL;DR: RLVR（带可验证奖励的强化学习）在改进大语言模型推理性能时表现出参数更新的稀疏性，这实际上是模型条件优化偏差的表面现象。研究发现RLVR在权重空间中沿非主方向学习，通过最小谱漂移、减少主子空间旋转和离主更新对齐实现性能提升，而SFT则针对主权重并扭曲谱结构。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR训练动态中的参数更新稀疏性悖论，揭示RLVR与SFT在优化机制上的根本差异，为设计RLVR原生学习算法提供理论基础。

Method: 提出三门理论解释RLVR动态：门I（KL锚点）施加KL约束更新；门II（模型几何）将步长导向低曲率、保谱子空间；门III（精度）在非偏好区域隐藏微更新。通过参数级分析验证理论。

Result: 发现RLVR在权重空间中沿非主方向学习，实现最小谱漂移和减少主子空间旋转，而SFT扭曲谱结构且性能落后于RLVR。参数更新具有跨运行、数据集和RL配方的强一致性。

Conclusion: RLVR在优化机制上与SFT存在本质差异，直接套用SFT时代的参数高效微调方法存在缺陷。研究为RLVR的白盒理解和几何感知的RLVR原生算法设计指明了方向。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.
  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [51] [深度<em class="highlight">强化学习</em>之 Vanilla Policy Gradient 从零实现](http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516960&idx=1&sn=394c2d9a49a3d356a887e72847ddfb87&chksm=9a929a082ea937cd3e291fb04fc9708fbcf0e01aa1140cdbbf500b9672a4eb81cb3d32073cf2#rd)
*月来客栈*

Main category: wechat.article

TL;DR: 因为在整个强化学习过程中策略函数使用的是深度神经网络，所以又将其称之为深度强化学习。这里首先定义一个两层的策略网络，示例代码如下：1 class PolicyNet（nn.Module）：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 因为在整个强化学习过程中策略函数使用的是深度神经网络，所以又将其称之为深度强化学习。这里首先定义一个两层的策略网络，示例代码如下：1 class PolicyNet（nn.Module）：

</details>


### [52] [兵棋推演中<em class="highlight">强化学习</em>的应用研究](http://mp.weixin.qq.com/s?__biz=MzA3Mzc0NDEzOQ==&mid=2650065872&idx=2&sn=470b40f7c8c60c6d13d9630b8d426148&chksm=86a564d5f42987a37a51091c3752def1d4d5903229501afcf0bdbdd99829633af4af656ea6b5#rd)
*军事文摘*

Main category: wechat.article

TL;DR: 兵棋推演中的强化学习体系设计 为让AI在兵棋推演中学习战术，研究构建了包含推演环境、战斗规则、强化学习工具在内的完整体系，每一部分均贴合兵棋推演的核心逻辑。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 兵棋推演中的强化学习体系设计 为让AI在兵棋推演中学习战术，研究构建了包含推演环境、战斗规则、强化学习工具在内的完整体系，每一部分均贴合兵棋推演的核心逻辑。

</details>


### [53] [<em class="highlight">强化学习</em> AI 系统的设计实现及未来发展](http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247648721&idx=2&sn=197a107dbec3567392542ca111d25572&chksm=faf5e7b90b4d78aeadd5ccf8b62acb5f520f2b5d6eedbd1cd2e2e8e796a7a6e8ab1b9797efa5#rd)
*AI前线*

Main category: wechat.article

TL;DR: 从常见的人类反馈强化学习，到基于宪法的反馈强化学习，再到如今基于可验证规则的强化学习，这些不断进步的过程，实际上代表着强化学习奖励函数的信号来源日益广泛，同时任务难度也在不断提高。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从常见的人类反馈强化学习，到基于宪法的反馈强化学习，再到如今基于可验证规则的强化学习，这些不断进步的过程，实际上代表着强化学习奖励函数的信号来源日益广泛，同时任务难度也在不断提高。

</details>


### [54] [直接拿捏！只用38页PPT学会了<em class="highlight">强化学习</em>八大算法，全面解析！](http://mp.weixin.qq.com/s?__biz=MzkwMDY5OTk4OA==&mid=2247487269&idx=1&sn=292026cb5d04c7f8fc58ad30a53b7606&chksm=c1838b611859af4fab740602ee7decc780d0af258545ed55dafcd097f09997db1ce1d9a8bdbd#rd)
*小王聊编程*

Main category: wechat.article

TL;DR: 法 一种基于时序差分学习的在线强化学习算法，通过与环境的实时交互来学习Q函数，即在给定状态下采取特定动作的预期回报。强化学习简介和应用 ppo算法与公式推导 ppo实战-月球登陆器训练实例 q-learning与oqn算法。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 法 一种基于时序差分学习的在线强化学习算法，通过与环境的实时交互来学习Q函数，即在给定状态下采取特定动作的预期回报。强化学习简介和应用 ppo算法与公式推导 ppo实战-月球登陆器训练实例 q-learning与oqn算法。

</details>


### [55] [上交博士最新思考：仅用两个问题讲清<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247673270&idx=4&sn=54f36ae320a972bb2899bd0f40587826&chksm=e9654e6a605909322e2c47a02e547a7b8e43e7d063e2c76e0791a7c8a5c90f5f02d5e66e4260#rd)
*图灵人工智能*

Main category: wechat.article

TL;DR: 02 学习更新的节奏 而强化学习的第二个维度，是学习更新的节奏。简单来说，就是智能体多久评估一次策略，又多久调整一次行为。最简单的方式是一种“一步式学习”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 02 学习更新的节奏 而强化学习的第二个维度，是学习更新的节奏。简单来说，就是智能体多久评估一次策略，又多久调整一次行为。最简单的方式是一种“一步式学习”。

</details>


### [56] [<em class="highlight">Agentic</em> RL详解：打造自主学习自主迭代的高性能 Agent](http://mp.weixin.qq.com/s?__biz=MzA3ODUzOTkyMA==&mid=2247486417&idx=2&sn=5ace61a7f2ea37b45d913d6347780863&chksm=9ee9935443f1139df82502f140eb3f6e27fea6c35b4f064bbe2ff702b8e7064c8f75ad354cc9#rd)
*赋范大模型技术圈*

Main category: wechat.article

TL;DR: 全部文件/Agentic RL入门实战已全部加载，共5个 88文件名 大小 类型 修改时间项目源码 文件夹 2025.11.09 19：36课件&代码 文件夹 2025.11.09 19：36微调SQL数据集 文件夹 2025.11.09 19：36


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全部文件/Agentic RL入门实战已全部加载，共5个 88文件名 大小 类型 修改时间项目源码 文件夹 2025.11.09 19：36课件&代码 文件夹 2025.11.09 19：36微调SQL数据集 文件夹 2025.11.09 19：36

</details>


### [57] [2026拥抱Agent正当时，首届 <em class="highlight">Agentic</em> AI 峰会聚集京东、蚂蚁、记忆张量、快手等企业实践话题](http://mp.weixin.qq.com/s?__biz=MzI4OTQyNzA0Ng==&mid=2247542420&idx=2&sn=614e3aa2592af4653540f80821a802f3&chksm=ed3adfc850819b9872fbbc14e4666f9931135e33760bef7f14d593ca3feafa33d0e05eac6c42#rd)
*语音之家*

Main category: wechat.article

TL;DR: Agentic AI Summit 2026将于2026年1月16-17日在中关村展示中心会议中心举办，汇聚500+技术精英，覆盖Agent技术全链路。峰会设置了九大专题分论坛，从底层模型到上层应用形成完整闭环，此次会议将邀请业界专家来分享团队的实践话题


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI Summit 2026将于2026年1月16-17日在中关村展示中心会议中心举办，汇聚500+技术精英，覆盖Agent技术全链路。峰会设置了九大专题分论坛，从底层模型到上层应用形成完整闭环，此次会议将邀请业界专家来分享团队的实践话题

</details>


### [58] [从单<em class="highlight">智能体</em>到多<em class="highlight">智能体</em>协作：<em class="highlight">Agentic</em> System的演进与LangGraph4j实战](http://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&mid=2649281552&idx=1&sn=42b297f288241cca018a30c472224bc7&chksm=86af81e011b6e5b96ff6894f9595c228e21b64e49ed3bca2d93b8c4bf712622fd50b710a0916#rd)
*Qunar技术沙龙*

Main category: wechat.article

TL;DR: 2.2 业界定义与共识-Agentic SystemOpenAI对于Agent的定义Agent 是“能代表你独立完成任务的系统”Anthropic对于Agent的定义"Agent" can be defined in several ways. Some customers define agents as fully autonomous systems that operate indepen...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2.2 业界定义与共识-Agentic SystemOpenAI对于Agent的定义Agent 是“能代表你独立完成任务的系统”Anthropic对于Agent的定义"Agent" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods， us

</details>


### [59] [吴恩达教授Ai课程第一课：什么是<em class="highlight">Agentic</em> Ai？](http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483772&idx=1&sn=1eb37c739fc9e3f21644bb38f562c21a&chksm=f16c3b058f7b85c1ef7e6d28ac5a8111e57b9c5d69063d80aecbd097b4efc7cf347e9e157089#rd)
*EasyShip.AI*

Main category: wechat.article

TL;DR: 课程总共分为5个部分：- Introduction to Agentic Workflows：Agentic工作流介绍；- Reflection Design Pattern：自我反思Agent设计模式；- Tool use：工具使用；- Practical Tips for Building Agentic AI：构建Agent的实用技巧；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 课程总共分为5个部分：- Introduction to Agentic Workflows：Agentic工作流介绍；- Reflection Design Pattern：自我反思Agent设计模式；- Tool use：工具使用；- Practical Tips for Building Agentic AI：构建Agent的实用技巧；

</details>


### [60] [谷歌分享54页AI Agent介绍 - 五层<em class="highlight">Agentic</em>系统 - Agent核心架构：模型工具，编排，进化与自学习 - 高阶A](http://mp.weixin.qq.com/s?__biz=MzI0OTAzNTEwMw==&mid=2247488508&idx=1&sn=3eec357313db4e7b3cea265979209abb&chksm=e85cf4c48439b8909c6aeabf968e6f17881aab19c20f38bbea5f06f82c1625aa70c2b07a3f5a#rd)
*AgenticAI*

Main category: wechat.article

TL;DR: 谷歌分享54页ai agent介 绍 - 五层agentic系统 - agent核心架构：模型 工具，编排，进化与自学 习 - 高阶agent示例，包含 自进化。alphaevolve level 4： self evolving agents level 3： the rise of collaborative multi-agent systems level 2： the strategic...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 谷歌分享54页ai agent介 绍 - 五层agentic系统 - agent核心架构：模型 工具，编排，进化与自学 习 - 高阶agent示例，包含 自进化。alphaevolve level 4： self evolving agents level 3： the rise of collaborative multi-agent systems level 2： the strategic problem-sol

</details>


### [61] [国内首个支持视觉的 <em class="highlight">Agentic</em> 编程模型来了！性能更强，价格更低](http://mp.weixin.qq.com/s?__biz=MzkyNjI3NjQ2MA==&mid=2247492065&idx=1&sn=58ff92f30340d054ef007dbffdb10bfa&chksm=c319f3a39286ada6b39f77f06847224ee3d080296ef730812f71a8affdee92446d63454510cb#rd)
*梦飞 AI*

Main category: wechat.article

TL;DR: 既然是Agentic编程模型，我想他会懂我意思的，直接提出大白话需求：你知道【炸弹人】小游戏吗？我想玩这个游戏，给我生成代码文件日 日 终 帮助h yaingy jbnanisx bonbran.htm tral 文性 bombmanhmal x bonbnanjs 伫。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 既然是Agentic编程模型，我想他会懂我意思的，直接提出大白话需求：你知道【炸弹人】小游戏吗？我想玩这个游戏，给我生成代码文件日 日 终 帮助h yaingy jbnanisx bonbran.htm tral 文性 bombmanhmal x bonbnanjs 伫。

</details>


### [62] [第180期 <em class="highlight">智能体</em>人工智能（<em class="highlight">Agentic</em> AI）：单<em class="highlight">智能体</em>系统 vs 多<em class="highlight">智能体</em>系统](http://mp.weixin.qq.com/s?__biz=MzI3NDE5MjExOQ==&mid=2650985383&idx=1&sn=6172a9aa83793b2403b85d4d6796b11f&chksm=f142387c19523d1034f5ff5c979e37e0378c44bc5f819b7e524b4a36b865fd92cc01b2434791#rd)
*AI拉呱*

Main category: wechat.article

TL;DR: 智能体人工智能（Agentic AI）与大语言模型（LLMs）智能体人工智能本质上是通过自然语言进行编程。它不再依赖僵化、明确的代码，而是通过自然语言指令引导大语言模型（LLMs）实现数据路由与任务执行，从而完成自动化流程。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 智能体人工智能（Agentic AI）与大语言模型（LLMs）智能体人工智能本质上是通过自然语言进行编程。它不再依赖僵化、明确的代码，而是通过自然语言指令引导大语言模型（LLMs）实现数据路由与任务执行，从而完成自动化流程。

</details>


### [63] [<em class="highlight">Agentic</em>21种设计模式-Reflection](http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484288&idx=1&sn=166d186f7141a45dfd76b781ca0ca50e&chksm=c0a3fe115fb23be8037185f5cc008d83da7103a2d4abcad4588972bffe175476300ca4eb8de1#rd)
*AI Lab Dev*

Main category: wechat.article

TL;DR: 从而使得代理的决策更加智能、更加符合实际情境。实际应用与案例在那些对输出质量、准确性或对各种复杂约束的遵守要求极高的场景中，反思模式显得非常有用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从而使得代理的决策更加智能、更加符合实际情境。实际应用与案例在那些对输出质量、准确性或对各种复杂约束的遵守要求极高的场景中，反思模式显得非常有用。

</details>


### [64] [一文看懂 <em class="highlight">Agentic</em> AI：搭建单体 vs 多<em class="highlight">智能体</em>系统，结果出乎意料！](http://mp.weixin.qq.com/s?__biz=MzA4NzA4NjAxOA==&mid=2452976528&idx=1&sn=4d8664b7bc22105cef3e7a5b6913ca8a&chksm=86f4f9640406d4cb6fd01f9626abb48a746ee9e23822cf045597f7bb5ad39a65428615da3462#rd)
*AI技术研习社*

Main category: wechat.article

TL;DR: 我喜欢用一句话解释：Agentic AI 就是“用自然语言编程”。传统开发要写死逻辑，而 Agentic AI 是让大语言模型（LLM）自己“理解任务、规划步骤、调用工具、生成结果”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我喜欢用一句话解释：Agentic AI 就是“用自然语言编程”。传统开发要写死逻辑，而 Agentic AI 是让大语言模型（LLM）自己“理解任务、规划步骤、调用工具、生成结果”。

</details>


### [65] [服务全商业场景智能预测 蚂蚁国际开源“鹰序”AI预测<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzE5ODAzOTA2Ng==&mid=2247484037&idx=1&sn=331a09c578e708e6ca8bebe50717bdae&chksm=978f745cafdf99b1db0257e3c445b29b25ffdd9ff216737ea478cbbeda145dc6ff6f16f9d8c3#rd)
*蚂蚁国际 Ant International*

Main category: wechat.article

TL;DR: 蚂蚁国际自研的“鹰序”AI预测大模型，是业内首个基于多分段模式（Patch）并采用“混合专家”（Mixture of Experts， MoE）架构的大规模时序预测基础模型，参数规模超过25亿，在多个权威基准评测中（如平均绝对误差率）取得了


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 蚂蚁国际自研的“鹰序”AI预测大模型，是业内首个基于多分段模式（Patch）并采用“混合专家”（Mixture of Experts， MoE）架构的大规模时序预测基础模型，参数规模超过25亿，在多个权威基准评测中（如平均绝对误差率）取得了

</details>


### [66] [NeurIPS 2025 | 中科大、港中深、通义千问联合发布CoRT：仅30个样本教会<em class="highlight">大模型</em>高效推理，token消耗降低50%](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651001296&idx=4&sn=081802d634f4b8e87a9470ca0436fa29&chksm=8521e901276a61ac0aaa0866fc4e8d9da75f0fcc37d8ad9475ddda119fc748a3085f1e4ec168#rd)
*机器之心*

Main category: wechat.article

TL;DR: 那么，如何让大模型学会「何时」以及「如何」高效地使用工具，将自身的抽象推理能力与工具的精确计算能力完美结合？来自中国科学技术大学、香港中文大学（深圳）、通义千问的联合研究团队给出了他们的答案：CoRT （Code


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 那么，如何让大模型学会「何时」以及「如何」高效地使用工具，将自身的抽象推理能力与工具的精确计算能力完美结合？来自中国科学技术大学、香港中文大学（深圳）、通义千问的联合研究团队给出了他们的答案：CoRT （Code

</details>


### [67] [吴恩达：终于有人一次性把<em class="highlight">大模型</em>三种模式讲清了！！！](http://mp.weixin.qq.com/s?__biz=MzYyMzQyNTMxNQ==&mid=2247484444&idx=1&sn=e16aeea1f0beb12ea635ccc8f8c40f71&chksm=fe4ad53b3771eb5dbd6b81bb6b1362ad7d0338dfe9f48d220c2b7834d0ec9a24a0bbc44cbbe7#rd)
*AI大模型算法工程师*

Main category: wechat.article

TL;DR: 大模型的三种模式。embedding， copilot， agent。生成式ai的智能革命演化至今，从人机协同呈现了三种 模式： ·嵌入（embedding）模式：某个环节里去调用大模 型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型的三种模式。embedding， copilot， agent。生成式ai的智能革命演化至今，从人机协同呈现了三种 模式： ·嵌入（embedding）模式：某个环节里去调用大模 型。

</details>


### [68] [人工智能<em class="highlight">大模型</em>系列国家标准解读（二）——《人工智能  <em class="highlight">大模型</em>  第2部分：评测指标与方法》](http://mp.weixin.qq.com/s?__biz=Mzg4Mzk2Njc4Mw==&mid=2247522312&idx=4&sn=449532d1c023d46f847b0ed77d7a7979&chksm=ce9366ebd9f6f5a1ecc205123994f1199f6444caea109fb0b3b2f7ee99d83d21c4bd0f06fb82#rd)
*甘肃交通科技通信*

Main category: wechat.article

TL;DR: 《人工智能 大模型 第2部分：评测指标与方法》确立了人工智能大模型的评测指标，描述了人工智能大模型的评测方法。《人工智能 大模型 第3部分：服务能力成熟度评估》给出了大模型服务能力框架和成熟度等级，描述了大模


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 《人工智能 大模型 第2部分：评测指标与方法》确立了人工智能大模型的评测指标，描述了人工智能大模型的评测方法。《人工智能 大模型 第3部分：服务能力成熟度评估》给出了大模型服务能力框架和成熟度等级，描述了大模

</details>


### [69] [各有所长，国内外模型安全评估丨多个<em class="highlight">大模型</em>安全榜单揭晓](http://mp.weixin.qq.com/s?__biz=MzkyNzg1MzMxNg==&mid=2247487795&idx=1&sn=7b0a87fee0d83567101148c1bce33d2c&chksm=c315e2b83b522dc2b2d9a306e9d2d45539cdb36057f99d9742facfdea7a23868a3d6b5cfa029#rd)
*司南评测体系*

Main category: wechat.article

TL;DR: 为填补大模型安全评估领域的标准化空白，上海人工智能实验室评测专项组基于多维度安全测试基准，针对国内外主流大模型开展了系统性评测，现公布三大安全评估榜单，为行业提供客观、可靠的安全性能参考。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 为填补大模型安全评估领域的标准化空白，上海人工智能实验室评测专项组基于多维度安全测试基准，针对国内外主流大模型开展了系统性评测，现公布三大安全评估榜单，为行业提供客观、可靠的安全性能参考。

</details>


### [70] [高薪、缺人！零成本快速入门<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzUzMzEyMDI2Mg==&mid=2247489934&idx=3&sn=adb1b35101c8f55dd42bb99822191985&chksm=fb8fd11085595a40af9ddc09eb73b48de437944e0de27ce29c93053ad7c49c853e4912981593#rd)
*极客时间训练营*

Main category: wechat.article

TL;DR: 从原理出发真正入局大模型。在这里推荐一下由 LangChain 开发者，谷歌开发者专家彭靖田，专门为开发者量身打造的『从 0 到 1 入门 AI 大模型』视频课程：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从原理出发真正入局大模型。在这里推荐一下由 LangChain 开发者，谷歌开发者专家彭靖田，专门为开发者量身打造的『从 0 到 1 入门 AI 大模型』视频课程：

</details>


### [71] [高薪、缺人！零成本快速入门<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzUzNDQzNjUyOQ==&mid=2247492691&idx=3&sn=d47ce3def9dc18cc5faca6c2c0c61a54&chksm=fbeafdf78382b2becfb6b17d25ff4e1f26633f3a4bdbeba3343d866fd2c16a6f14b83049e52a#rd)
*极客时间精选*

Main category: wechat.article

TL;DR: 从原理出发真正入局大模型。在这里推荐一下由 LangChain 开发者，谷歌开发者专家彭靖田，专门为开发者量身打造的『从 0 到 1 入门 AI 大模型』视频课程：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从原理出发真正入局大模型。在这里推荐一下由 LangChain 开发者，谷歌开发者专家彭靖田，专门为开发者量身打造的『从 0 到 1 入门 AI 大模型』视频课程：

</details>


### [72] [AI<em class="highlight">大模型</em>学习路线（2025最新）从零基础入门到精通，看完这一篇就够了！](http://mp.weixin.qq.com/s?__biz=MzIwMzc5NzU3Mw==&mid=2247484103&idx=1&sn=ca2c1ebdf590632a89a0145525ba41dd&chksm=975ed22a27aa29e709b5cbc2134568483078b3e6cb32462820fc351d9dc69a9fa207267b38e7#rd)
*AI大模型-木子*

Main category: wechat.article

TL;DR: ai大模型学习路线 第一阶段·大模型开发基础。为什么要学习大模型开发？需要准备的工具和环境。第二章：大模型的训练与应用。大模型发展史。从大模型预训练、微调到应用 gpt结构，剖析 大模型家族、类别、应用场景。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ai大模型学习路线 第一阶段·大模型开发基础。为什么要学习大模型开发？需要准备的工具和环境。第二章：大模型的训练与应用。大模型发展史。从大模型预训练、微调到应用 gpt结构，剖析 大模型家族、类别、应用场景。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [73] [SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction](https://arxiv.org/abs/2511.07584)
*Wuyang Zhang,Chenkai Zhang,Zhen Luo,Jianming Ma,Wangming Yuan,Chuqiao Gu,Chenwei Feng*

Main category: cs.SE

TL;DR: SemanticForge通过四种算法创新解决LLM代码生成的系统错误：逻辑幻觉和模式幻觉。它结合静态-动态知识图谱、神经查询生成、集成SMT求解的束搜索以及增量维护算法，实现语义感知的代码生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成中存在系统性错误，特别是逻辑幻觉（控制/数据流推理错误）和模式幻觉（类型不匹配、签名违规等），这些错误源于缺乏可查询的仓库级语义表示。

Method: 提出四种核心算法：1）自动协调静态-动态知识图谱；2）从自然语言生成结构化图查询的神经网络方法；3）集成SMT求解的束搜索算法；4）增量维护算法以高效更新知识图谱。

Result: 神经查询生成方法达到73%的精确度，相比传统检索方法的51%；增量维护算法在O(|ΔR|·log n)时间内更新知识图谱。

Conclusion: SemanticForge通过语义感知方法显著提升了代码生成的准确性和可靠性，解决了LLM在软件开发中的关键限制。

Abstract: Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \textit{logical hallucination} (incorrect control/data-flow reasoning) and \textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.
  This paper presents \textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\% precision versus 51\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|ΔR| \cdot \log n)$ time while maintaining semantic equivalence.

</details>


### [74] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: 该研究通过眼动追踪实验探索Python多范式语言特征对代码理解和调试的影响，发现开发者对函数式和过程式范式存在混淆，函数式代码完成时间最长，但范式变化不影响调试能力。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言如Python支持多种范式（面向对象、过程式、函数式），但缺乏关于哪些范式特定语言特征影响代码理解和调试的研究。

Method: 采用探索性眼动追踪实证研究，招募29名开发者（主要是学生），进行4个分类任务和4个调试任务，记录眼动数据。

Result: 结果显示：1）开发者对函数式和过程式范式分类存在混淆；2）函数式代码完成时间最长；3）范式变化不影响调试能力，但开发者对函数式代码自信度较低；4）调试时阅读模式有显著差异，尤其在函数式代码中。

Conclusion: 多范式代码中，函数式范式特征对开发者理解和分类影响最大，但实际调试能力不受范式影响，开发者阅读模式在不同范式代码中存在差异。

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [75] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: 提出了自改进安全框架(SISF)，这是一个运行时架构，通过动态反馈循环让AI系统能够自主持续地调整其安全协议，将攻击成功率从100%降低到45.58%，同时保持0%的误报率。


<details>
  <summary>Details</summary>
Motivation: 现有软件架构模式是静态的，而当前的安全保证方法不可扩展，使系统容易受到新型对抗性威胁。需要设计能够自主适应安全协议的AI驱动系统。

Method: SISF架构将未受保护的基座LLM与动态反馈循环耦合，包括AI裁决器用于违规检测和政策合成模块用于自主生成新的安全策略。

Result: 在AdvBench数据集上测试，SISF从零策略开始，检测到237次违规，自主合成了234个新策略，将攻击成功率从100%降至45.58%，在良性提示上保持0%误报率。

Conclusion: 基于自适应原则的AI安全架构方法是可行且有效的策略，为构建更强大、有弹性和可扩展的AI驱动系统提供了实际路径。

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [76] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: 提出了BRACE框架，用于在统一的能效和功能正确性尺度上评估代码语言模型，包含CIRC和OTER两种评级方法，对22个最先进模型进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统框架来评估代码语言模型的准确性-能耗权衡，需要平衡AI技术环境影响的可持续性与功能正确性。

Method: 开发BRACE框架，提出两种评级方法：CIRC（基于欧几里得距离的确定性排名）和OTER（趋势感知评估），在代码生成和摘要任务上对22个模型进行基准测试。

Result: 模型在代码摘要任务中表现更好，因为不需要生成语法正确的输出；模型大小对评级影响不显著，参数利用效率更重要。

Conclusion: BRACE框架使从业者能够基于证据选择模型，平衡可持续性与任务需求，根据部署优先级选择CIRC或OTER评级方法。

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


### [77] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: ChaosEater是一个基于LLM的自动化混沌工程系统，针对Kubernetes软件系统，通过预定义的智能工作流自动完成整个混沌工程周期，显著降低了时间和成本。


<details>
  <summary>Details</summary>
Motivation: 传统混沌工程中实验规划和系统改进过程仍需要人工操作，这些过程劳动密集且需要多领域专业知识，限制了混沌工程的广泛应用。

Method: 提出ChaosEater系统，按照系统化的混沌工程周期预定义智能工作流，将工作流中的细分过程分配给LLM完成，包括需求定义、代码生成、测试和调试等软件工程任务。

Result: 在小型和大型Kubernetes系统上的案例研究表明，该系统能够以显著低的时间和金钱成本持续完成合理的混沌工程周期，其周期质量得到了人类工程师和LLM的验证。

Conclusion: ChaosEater证明了使用LLM自动化整个混沌工程周期的可行性，使任何人都能以低成本构建弹性系统。

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


### [78] [Testing Question Answering Software with Context-Driven Question Generation](https://arxiv.org/abs/2511.07924)
*Shuang Liu,Zhirun Zhang,Jinhao Dong,Zan Wang,Qingchao Shen,Junjie Chen,Wei Lu,Xiaoyong Du*

Main category: cs.SE

TL;DR: CQ^2A是一个基于上下文的问答系统测试方法，通过从上下文中提取实体和关系形成真实答案，并利用大语言模型生成问题，提高了测试问题的自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统测试方法生成的问题不自然且缺乏上下文多样性，无法有效发现真实场景中的缺陷。

Method: 从上下文中提取实体和关系形成真实答案，利用大语言模型生成问题，并采用一致性验证和约束检查来提高输出可靠性。

Result: 在三个数据集上的实验表明，CQ^2A在缺陷检测能力、生成问题的自然性和上下文覆盖方面优于现有方法，且生成的测试用例能降低问答软件的错误率。

Conclusion: CQ^2A方法有效解决了现有测试方法的局限性，提高了问答系统测试的效果。

Abstract: Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.
  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test

</details>


### [79] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: 本文系统研究了传统源代码模型和LLM4Code的内在漏洞可转移性，提出了一种无需访问下游分类器的受害者无关方法生成实用对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有研究既没有提供实用的对抗样本生成方法（需要访问SCM的下游分类器），也没有关注现代软件开发平台中广泛使用的LLM4Code。

Method: 设计了HABITAT，包含定制的扰动插入机制和分层强化学习框架，自适应选择最优扰动而不需要访问SCM的下游分类器。

Result: 实验评估表明，基于传统SCM构建的对抗样本对LLM4Code的攻击成功率高达64%，比现有技术高出15%以上。

Conclusion: 揭示了传统SCM与LLM4Code之间的潜在漏洞相关性，为未来开发鲁棒防御提供了关键焦点。

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [80] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: 该研究系统分析了94篇关于LLM驱动的多智能体系统在软件工程任务中的应用论文，识别了主要关注的软件工程任务、质量属性、设计模式和设计原理。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程任务复杂性增加，基于LLM的多智能体系统因其自主性和可扩展性受到关注，但目前缺乏对其设计原则、质量属性和设计模式的系统性研究。

Method: 收集了94篇关于LLM驱动的多智能体系统在软件工程任务中的论文作为研究来源，进行系统性分析。

Result: 研究发现：代码生成是最常见的软件工程任务；功能性适用性是设计者最关注的质量属性；基于角色的协作是最常用的设计模式；提高生成代码质量是最常见的设计原理。

Conclusion: 基于研究结果，提出了支持软件工程任务的LLM驱动多智能体系统设计的启示和建议。

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


### [81] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: 研究探索使用GPT-4o模拟KLEE符号执行工具的输出，特别是识别程序中最受约束的执行路径，以替代昂贵的符号执行过程。


<details>
  <summary>Details</summary>
Motivation: 符号执行工具如KLEE在复杂程序中运行缓慢且资源消耗大，希望利用LLM模拟其输出来节省时间和资源。

Method: 使用GPT-4o在100个C程序数据集上预测KLEE输出和最受约束路径（具有最多符号条件的执行路径）。

Result: GPT-4o在生成KLEE类输出和识别最受约束路径方面准确率约为20%。

Conclusion: 虽然准确率不高，但这项早期工作展示了当前LLM在模拟符号执行方面的能力与局限。

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>
