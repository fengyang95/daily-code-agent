{"id": "2511.02997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02997", "abs": "https://arxiv.org/abs/2511.02997", "authors": ["Jon Kutasov", "Chloe Loughridge", "Yuqi Sun", "Henry Sleight", "Buck Shlegeris", "Tyler Tracy", "Joe Benton"], "title": "Evaluating Control Protocols for Untrusted AI Agents", "comment": null, "summary": "As AI systems become more capable and widely deployed as agents, ensuring\ntheir safe operation becomes critical. AI control offers one approach to\nmitigating the risk from untrusted AI agents by monitoring their actions and\nintervening or auditing when necessary. Evaluating the safety of these\nprotocols requires understanding both their effectiveness against current\nattacks and their robustness to adaptive adversaries. In this work, we\nsystematically evaluate a range of control protocols in SHADE-Arena, a dataset\nof diverse agentic environments. First, we evaluate blue team protocols,\nincluding deferral to trusted models, resampling, and deferring on critical\nactions, against a default attack policy. We find that resampling for\nincrimination and deferring on critical actions perform best, increasing safety\nfrom 50% to 96%. We then iterate on red team strategies against these protocols\nand find that attack policies with additional affordances, such as knowledge of\nwhen resampling occurs or the ability to simulate monitors, can substantially\nimprove attack success rates against our resampling strategy, decreasing safety\nto 17%. However, deferring on critical actions is highly robust to even our\nstrongest red team strategies, demonstrating the importance of denying attack\npolicies access to protocol internals.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86AI\u63a7\u5236\u534f\u8bae\u5728SHADE-Arena\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u80fd\uff0c\u53d1\u73b0\u5173\u952e\u884c\u52a8\u5ef6\u8fdf\u7b56\u7565\u5bf9\u7ea2\u961f\u653b\u51fb\u5177\u6709\u9ad8\u5ea6\u9c81\u68d2\u6027\uff0c\u800c\u91cd\u91c7\u6837\u7b56\u7565\u5728\u7ea2\u961f\u83b7\u5f97\u534f\u8bae\u5185\u90e8\u4fe1\u606f\u65f6\u5b89\u5168\u6027\u5927\u5e45\u4e0b\u964d\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4f5c\u4e3a\u667a\u80fd\u4f53\u5e7f\u6cdb\u90e8\u7f72\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u8fd0\u884c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002AI\u63a7\u5236\u901a\u8fc7\u76d1\u63a7\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u5728\u5fc5\u8981\u65f6\u5e72\u9884\u6216\u5ba1\u8ba1\u6765\u964d\u4f4e\u98ce\u9669\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u534f\u8bae\u7684\u6709\u6548\u6027\u548c\u5bf9\u9002\u5e94\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728SHADE-Arena\u591a\u6837\u5316\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u63a7\u5236\u534f\u8bae\uff1a\u5305\u62ec\u5ef6\u8fdf\u5230\u53ef\u4fe1\u6a21\u578b\u3001\u91cd\u91c7\u6837\u548c\u5173\u952e\u884c\u52a8\u5ef6\u8fdf\u7b49\u84dd\u961f\u534f\u8bae\uff0c\u5e76\u8fed\u4ee3\u5f00\u53d1\u7ea2\u961f\u653b\u51fb\u7b56\u7565\u3002", "result": "\u91cd\u91c7\u6837\u548c\u5173\u952e\u884c\u52a8\u5ef6\u8fdf\u7b56\u7565\u5c06\u5b89\u5168\u6027\u4ece50%\u63d0\u5347\u523096%\u3002\u4f46\u5f53\u7ea2\u961f\u7b56\u7565\u83b7\u5f97\u91cd\u91c7\u6837\u53d1\u751f\u65f6\u673a\u6216\u6a21\u62df\u76d1\u63a7\u5668\u80fd\u529b\u65f6\uff0c\u653b\u51fb\u6210\u529f\u7387\u5927\u5e45\u63d0\u9ad8\uff0c\u5b89\u5168\u6027\u964d\u81f317%\u3002\u5173\u952e\u884c\u52a8\u5ef6\u8fdf\u7b56\u7565\u5bf9\u6700\u5f3a\u7ea2\u961f\u653b\u51fb\u4ecd\u4fdd\u6301\u9ad8\u5ea6\u9c81\u68d2\u6027\u3002", "conclusion": "\u5173\u952e\u884c\u52a8\u5ef6\u8fdf\u662f\u9ad8\u5ea6\u9c81\u68d2\u7684\u63a7\u5236\u7b56\u7565\uff0c\u800c\u91cd\u91c7\u6837\u7b56\u7565\u5728\u653b\u51fb\u8005\u4e86\u89e3\u534f\u8bae\u5185\u90e8\u4fe1\u606f\u65f6\u6613\u53d7\u653b\u51fb\uff0c\u8868\u660e\u62d2\u7edd\u653b\u51fb\u7b56\u7565\u8bbf\u95ee\u534f\u8bae\u5185\u90e8\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.03023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03023", "abs": "https://arxiv.org/abs/2511.03023", "authors": ["Sina Montazeri", "Yunhe Feng", "Kewei Sha"], "title": "PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework", "comment": null, "summary": "Open data repositories hold potential for evidence-based decision-making, yet\nare inaccessible to non-experts lacking expertise in dataset discovery, schema\nmapping, and statistical analysis. Large language models show promise for\nindividual tasks, but end-to-end analytical workflows expose fundamental\nlimitations: attention dilutes across growing contexts, specialized reasoning\npatterns interfere, and errors propagate undetected. We present PublicAgent, a\nmulti-agent framework that addresses these limitations through decomposition\ninto specialized agents for intent clarification, dataset discovery, analysis,\nand reporting. This architecture maintains focused attention within agent\ncontexts and enables validation at each stage. Evaluation across five models\nand 50 queries derives five design principles for multi-agent LLM systems.\nFirst, specialization provides value independent of model strength--even the\nstrongest model shows 97.5% agent win rates, with benefits orthogonal to model\nscale. Second, agents divide into universal (discovery, analysis) and\nconditional (report, intent) categories. Universal agents show consistent\neffectiveness (std dev 12.4%) while conditional agents vary by model (std dev\n20.5%). Third, agents mitigate distinct failure modes--removing discovery or\nanalysis causes catastrophic failures (243-280 instances), while removing\nreport or intent causes quality degradation. Fourth, architectural benefits\npersist across task complexity with stable win rates (86-92% analysis, 84-94%\ndiscovery), indicating workflow management value rather than reasoning\nenhancement. Fifth, wide variance in agent effectiveness across models (42-96%\nfor analysis) requires model-aware architecture design. These principles guide\nwhen and why specialization is necessary for complex analytical workflows while\nenabling broader access to public data through natural language interfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86PublicAgent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7aef\u5230\u7aef\u6570\u636e\u5206\u6790\u5de5\u4f5c\u6d41\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff08\u610f\u56fe\u6f84\u6e05\u3001\u6570\u636e\u96c6\u53d1\u73b0\u3001\u5206\u6790\u548c\u62a5\u544a\uff09\uff0c\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u6ce8\u610f\u529b\u7a00\u91ca\u3001\u4e13\u4e1a\u63a8\u7406\u6a21\u5f0f\u5e72\u6270\u548c\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "motivation": "\u5f00\u653e\u6570\u636e\u4ed3\u5e93\u5bf9\u975e\u4e13\u5bb6\u96be\u4ee5\u8bbf\u95ee\uff0c\u9700\u8981\u6570\u636e\u96c6\u53d1\u73b0\u3001\u6a21\u5f0f\u6620\u5c04\u548c\u7edf\u8ba1\u5206\u6790\u7684\u4e13\u4e1a\u77e5\u8bc6\u3002\u867d\u7136LLM\u5728\u5355\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7aef\u5230\u7aef\u5206\u6790\u5de5\u4f5c\u6d41\u4e2d\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u5de5\u4f5c\u6d41\u5206\u89e3\u4e3a\u56db\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u610f\u56fe\u6f84\u6e05\u3001\u6570\u636e\u96c6\u53d1\u73b0\u3001\u5206\u6790\u548c\u62a5\u544a\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4fdd\u6301\u4e13\u6ce8\u7684\u6ce8\u610f\u529b\u8303\u56f4\u5e76\u5b9e\u73b0\u9636\u6bb5\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u4e865\u4e2a\u6a21\u578b\u548c50\u4e2a\u67e5\u8be2\uff0c\u5f97\u51fa\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u4e94\u4e2a\u8bbe\u8ba1\u539f\u5219\uff1a\u4e13\u4e1a\u5316\u72ec\u7acb\u4e8e\u6a21\u578b\u5f3a\u5ea6\u63d0\u4f9b\u4ef7\u503c\uff1b\u667a\u80fd\u4f53\u5206\u4e3a\u901a\u7528\u578b\u548c\u6761\u4ef6\u578b\uff1b\u4e0d\u540c\u667a\u80fd\u4f53\u7f13\u89e3\u4e0d\u540c\u5931\u8d25\u6a21\u5f0f\uff1b\u67b6\u6784\u4f18\u52bf\u5728\u4e0d\u540c\u4efb\u52a1\u590d\u6742\u5ea6\u4e0b\u6301\u7eed\u5b58\u5728\uff1b\u667a\u80fd\u4f53\u6709\u6548\u6027\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u8fd9\u4e9b\u539f\u5219\u6307\u5bfc\u4e86\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u5728\u590d\u6742\u5206\u6790\u5de5\u4f5c\u6d41\u4e2d\u9700\u8981\u4e13\u4e1a\u5316\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u516c\u5171\u6570\u636e\u8bbf\u95ee\u3002", "topic": "agent analysis"}}
{"id": "2511.02919", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.02919", "abs": "https://arxiv.org/abs/2511.02919", "authors": ["Shuhang Lin", "Zhencan Peng", "Lingyao Li", "Xiao Lin", "Xi Zhu", "Yongfeng Zhang"], "title": "Cache Mechanism for Agent RAG Systems", "comment": null, "summary": "Recent advances in Large Language Model (LLM)-based agents have been\npropelled by Retrieval-Augmented Generation (RAG), which grants the models\naccess to vast external knowledge bases. Despite RAG's success in improving\nagent performance, agent-level cache management, particularly constructing,\nmaintaining, and updating a compact, relevant corpus dynamically tailored to\neach agent's need, remains underexplored. Therefore, we introduce ARC (Agent\nRAG Cache Mechanism), a novel, annotation-free caching framework that\ndynamically manages small, high-value corpora for each agent. By synthesizing\nhistorical query distribution patterns with the intrinsic geometry of cached\nitems in the embedding space, ARC automatically maintains a high-relevance\ncache. With comprehensive experiments on three retrieval datasets, our\nexperimental results demonstrate that ARC reduces storage requirements to\n0.015% of the original corpus while offering up to 79.8% has-answer rate and\nreducing average retrieval latency by 80%. Our results demonstrate that ARC can\ndrastically enhance efficiency and effectiveness in RAG-powered LLM agents.", "AI": {"tldr": "ARC\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u65e0\u9700\u6807\u6ce8\u7684\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7ba1\u7406\u5c0f\u578b\u9ad8\u4ef7\u503c\u8bed\u6599\u5e93\uff0c\u5c06\u5b58\u50a8\u9700\u6c42\u964d\u4f4e\u5230\u539f\u59cb\u8bed\u6599\u5e93\u76840.015%\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u8fbe79.8%\u7684\u7b54\u6848\u547d\u4e2d\u7387\uff0c\u5e76\u5c06\u5e73\u5747\u68c0\u7d22\u5ef6\u8fdf\u964d\u4f4e80%\u3002", "motivation": "\u5c3d\u7ba1RAG\u5728\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u9488\u5bf9\u6bcf\u4e2a\u667a\u80fd\u4f53\u9700\u6c42\u52a8\u6001\u6784\u5efa\u3001\u7ef4\u62a4\u548c\u66f4\u65b0\u7d27\u51d1\u76f8\u5173\u8bed\u6599\u5e93\u7684\u667a\u80fd\u4f53\u7ea7\u7f13\u5b58\u7ba1\u7406\u4ecd\u7136\u7814\u7a76\u4e0d\u8db3\u3002", "method": "ARC\u901a\u8fc7\u7efc\u5408\u5386\u53f2\u67e5\u8be2\u5206\u5e03\u6a21\u5f0f\u4e0e\u7f13\u5b58\u9879\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u81ea\u52a8\u7ef4\u62a4\u9ad8\u76f8\u5173\u6027\u7684\u7f13\u5b58\u3002", "result": "\u5728\u4e09\u4e2a\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cARC\u5c06\u5b58\u50a8\u9700\u6c42\u964d\u81f3\u539f\u59cb\u8bed\u6599\u5e93\u76840.015%\uff0c\u63d0\u4f9b\u9ad8\u8fbe79.8%\u7684\u7b54\u6848\u547d\u4e2d\u7387\uff0c\u5e76\u5c06\u5e73\u5747\u68c0\u7d22\u5ef6\u8fdf\u964d\u4f4e80%\u3002", "conclusion": "ARC\u80fd\u591f\u663e\u8457\u63d0\u5347\u57fa\u4e8eRAG\u7684LLM\u667a\u80fd\u4f53\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.02854", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02854", "abs": "https://arxiv.org/abs/2511.02854", "authors": ["Yixiang Chen", "Tianshi Zheng", "Shijue Huang", "Zhitao He", "Yi R. Fung"], "title": "SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation", "comment": "15 pages, 8 figures,2 tables", "summary": "Test-time scaling without interpreter feedback is essential for real-world\ncode generation scenarios where test cases are not readily available. While\nexisting paradigms often rely on either greedy exploitation (i.e., iterative\nrefinement) or stochastic exploration (i.e., relying on sample-based voting or\nreranking mechanisms), the balance between these two dimensions remains\nunderexplored. To investigate the LLM's intrinsic ability to balance\nexploitation and exploration, we introduce SELF-REDRAFT, a framework built upon\nSelf-Refine that encourages the model to propose new drafts for solutions that\nare fundamentally flawed. Our results show that SELF-REDRAFT consistently\nachieves better performance than Self-Refine when converged under the same\nmaximum number of iterations. Still, we observe that significant room for\nimprovement remains, largely due to two core aspects of current self-redraft\ncapabilities: constrained capacity for generating instructive feedback and\nfragile discriminative judgment. We also find that balancing strategies vary\nnotably across different LLMs, reflecting distinct, model-specific behaviors.\nOverall, our study establishes a baseline for intrinsic\nexploration-exploitation balancing in test-time scaling and identifies feedback\nand discrimination as key areas with potential for future advances.", "AI": {"tldr": "\u63d0\u51fa\u4e86SELF-REDRAFT\u6846\u67b6\uff0c\u57fa\u4e8eSelf-Refine\u6784\u5efa\uff0c\u901a\u8fc7\u9f13\u52b1\u6a21\u578b\u4e3a\u6839\u672c\u6027\u7f3a\u9677\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u51fa\u65b0\u8349\u7a3f\uff0c\u6765\u5e73\u8861\u5229\u7528\u548c\u63a2\u7d22\u3002\u7ed3\u679c\u663e\u793a\u5728\u76f8\u540c\u6700\u5927\u8fed\u4ee3\u6b21\u6570\u4e0b\uff0cSELF-REDRAFT\u6bd4Self-Refine\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\uff0c\u5f53\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u53ef\u7528\u65f6\uff0c\u9700\u8981\u5e73\u8861\u5229\u7528\uff08\u8fed\u4ee3\u4f18\u5316\uff09\u548c\u63a2\u7d22\uff08\u57fa\u4e8e\u6837\u672c\u7684\u6295\u7968\u6216\u91cd\u6392\uff09\u4e24\u4e2a\u7ef4\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u5e73\u8861\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5f15\u5165SELF-REDRAFT\u6846\u67b6\uff0c\u57fa\u4e8eSelf-Refine\u6784\u5efa\uff0c\u9f13\u52b1\u6a21\u578b\u4e3a\u6839\u672c\u6027\u7f3a\u9677\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u51fa\u65b0\u8349\u7a3f\uff0c\u7814\u7a76LLM\u5728\u5e73\u8861\u5229\u7528\u548c\u63a2\u7d22\u65b9\u9762\u7684\u5185\u5728\u80fd\u529b\u3002", "result": "SELF-REDRAFT\u5728\u76f8\u540c\u6700\u5927\u8fed\u4ee3\u6b21\u6570\u4e0b\u59cb\u7ec8\u6bd4Self-Refine\u8868\u73b0\u66f4\u597d\u3002\u4f46\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u751f\u6210\u6307\u5bfc\u6027\u53cd\u9988\u7684\u80fd\u529b\u548c\u8106\u5f31\u7684\u5224\u522b\u5224\u65ad\u80fd\u529b\u3002\u4e0d\u540cLLM\u7684\u5e73\u8861\u7b56\u7565\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u5185\u5728\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u5e76\u786e\u5b9a\u53cd\u9988\u548c\u5224\u522b\u80fd\u529b\u662f\u672a\u6765\u53d1\u5c55\u7684\u5173\u952e\u9886\u57df\u3002", "topic": "code agent"}}
{"id": "2511.02869", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.02869", "abs": "https://arxiv.org/abs/2511.02869", "authors": ["Amirreza Esmaeili", "Fahd Seddik", "Yongyi Ji", "Fatemeh Fard", "Fuxiang Chen"], "title": "Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models", "comment": null, "summary": "Programming languages can benefit from one another by utilizing a language\nmodel for software engineering tasks. Full fine-tuning and Parameter Efficient\nFine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for\nmultilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims\nto enhance task performance by leveraging information from multiple programming\nlanguages, but primarily focuses on the target programming language.\n  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that\neffectively learns from other programming languages before adapting to the\ntarget task. Though previous experiments showed that AdvFusion outperformed\nAdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited\nto only two tasks, code summarization and method name prediction. In this\nstudy, we expanded our work and investigated AdvFusion on Code Large Language\nModels (Code-LLMs), considering three new tasks: code generation, code\ntranslation, and commit message generation. We observed that different\nCode-LLMs/tasks exhibit different characteristics. In code generation,\nAdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,\nCompacter, and TaskAdapter). In commit message generation, AdapterFusion\nperformed better than AdvFusion, and contrary to code generation, we found that\nthe other PEFT methods do not have better performance. In code translation,\nAdvFusion performed worse than AdapterFusion overall, with the performance gap\nmarginally widening as the model size increases. However, consistent with code\ngeneration, other PEFT methods showed better performance.", "AI": {"tldr": "AdvFusion\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eAdapterFusion\uff0c\u4f46\u5728\u4ee3\u7801\u7ffb\u8bd1\u548c\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u5176\u4ed6PEFT\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22AdvFusion\u5728\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6269\u5c55\u5e94\u7528\uff0c\u7814\u7a76\u5176\u5728\u66f4\u591a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff08\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u7ffb\u8bd1\u3001\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u73b0\u6709PEFT\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528AdvFusion\u65b9\u6cd5\u5bf9\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u5148\u5b66\u4e60\u5176\u4ed6\u7f16\u7a0b\u8bed\u8a00\u7684\u77e5\u8bc6\uff0c\u7136\u540e\u9002\u5e94\u76ee\u6807\u4efb\u52a1\u3002\u4e0eAdapterFusion\u3001LoRA\u3001Compacter\u548cTaskAdapter\u7b49PEFT\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u4e0d\u540c\u4efb\u52a1\u548c\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u540c\u7279\u6027\uff1a\u4ee3\u7801\u751f\u6210\u4e2dAdvFusion\u4f18\u4e8eAdapterFusion\u4f46\u4e0d\u5982\u5176\u4ed6PEFT\u65b9\u6cd5\uff1b\u63d0\u4ea4\u4fe1\u606f\u751f\u6210\u4e2dAdapterFusion\u8868\u73b0\u66f4\u597d\uff1b\u4ee3\u7801\u7ffb\u8bd1\u4e2dAdvFusion\u6574\u4f53\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\u6027\u80fd\u5dee\u8ddd\u6269\u5927\u3002", "conclusion": "AdvFusion\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u4e0d\u80fd\u5728\u6240\u6709\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u5176\u4ed6PEFT\u65b9\u6cd5\u3002\u4efb\u52a1\u7279\u6027\u548c\u6a21\u578b\u89c4\u6a21\u90fd\u4f1a\u5f71\u54cd\u5176\u6027\u80fd\u8868\u73b0\u3002", "topic": "code agent"}}
{"id": "2511.03179", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03179", "abs": "https://arxiv.org/abs/2511.03179", "authors": ["Varun Kumar", "George Em Karniadakis"], "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework", "comment": null, "summary": "The engineering design process often demands expertise from multiple domains,\nleading to complex collaborations and iterative refinements. Traditional\nmethods can be resource-intensive and prone to inefficiencies. To address this,\nwe formalize the engineering design process through a multi-agent AI framework\nthat integrates structured design and review loops. The framework introduces\nspecialized knowledge-driven agents that collaborate to generate and refine\ndesign candidates. As an exemplar, we demonstrate its application to the\naerodynamic optimization of 4-digit NACA airfoils. The framework consists of\nthree key AI agents: a Graph Ontologist, a Design Engineer, and a Systems\nEngineer. The Graph Ontologist employs a Large Language Model (LLM) to\nconstruct two domain-specific knowledge graphs from airfoil design literature.\nThe Systems Engineer, informed by a human manager, formulates technical\nrequirements that guide design generation and evaluation. The Design Engineer\nleverages the design knowledge graph and computational tools to propose\ncandidate airfoils meeting these requirements. The Systems Engineer reviews and\nprovides feedback both qualitative and quantitative using its own knowledge\ngraph, forming an iterative feedback loop until a design is validated by the\nmanager. The final design is then optimized to maximize performance metrics\nsuch as the lift-to-drag ratio. Overall, this work demonstrates how\ncollaborative AI agents equipped with structured knowledge representations can\nenhance efficiency, consistency, and quality in the engineering design process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u5de5\u7a0b\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4e13\u4e1a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u548c\u4f18\u5316\u8bbe\u8ba1\uff0c\u4ee5NACA\u7ffc\u578b\u6c14\u52a8\u4f18\u5316\u4e3a\u4f8b\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5de5\u7a0b\u8bbe\u8ba1\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u591a\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\uff0c\u5b58\u5728\u6548\u7387\u74f6\u9888\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u4e13\u4e1aAI\u667a\u80fd\u4f53\u7684\u6846\u67b6\uff1a\u56fe\u672c\u4f53\u5b66\u5bb6\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\uff0c\u7cfb\u7edf\u5de5\u7a0b\u5e08\u5236\u5b9a\u6280\u672f\u9700\u6c42\uff0c\u8bbe\u8ba1\u5de5\u7a0b\u5e08\u751f\u6210\u5019\u9009\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u8fdb\u884c\u8bbe\u8ba1\u4f18\u5316\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e4\u4f4d\u6570NACA\u7ffc\u578b\u7684\u6c14\u52a8\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u8bbe\u8ba1\u8fc7\u7a0b\u7684\u6548\u7387\u63d0\u5347\u548c\u6027\u80fd\u6307\u6807\u4f18\u5316\u3002", "conclusion": "\u914d\u5907\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u7684\u534f\u4f5cAI\u667a\u80fd\u4f53\u80fd\u591f\u663e\u8457\u63d0\u5347\u5de5\u7a0b\u8bbe\u8ba1\u8fc7\u7a0b\u7684\u6548\u7387\u3001\u4e00\u81f4\u6027\u548c\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2511.02922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02922", "abs": "https://arxiv.org/abs/2511.02922", "authors": ["Yunhan Qiao", "Christopher Hundhausen", "Summit Haque", "Md Istiak Hossain Shihab"], "title": "Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension", "comment": "12 pages", "summary": "Code comprehension is essential for brownfield programming tasks, in which\ndevelopers maintain and enhance legacy code bases. Generative AI (GenAI) coding\nassistants such as GitHub Copilot have been shown to improve developer\nproductivity, but their impact on code understanding is less clear. We\nreplicate and extend a previous study by exploring both performance and\ncomprehension in GenAI-assisted brownfield programming tasks. In a\nwithin-subjects experimental study, 18 computer science graduate students\ncompleted feature implementation tasks with and without Copilot. Results show\nthat Copilot significantly reduced task time and increased the number of test\ncases passed. However, comprehension scores did not differ across conditions,\nrevealing a comprehension-performance gap: participants passed more test cases\nwith Copilot, but did not demonstrate greater understanding of the legacy\ncodebase. Moreover, we failed to find a correlation between comprehension and\ntask performance. These findings suggest that while GenAI tools can accelerate\nprogramming progress in a legacy codebase, such progress may come without an\nimproved understanding of that codebase. We consider the implications of these\nfindings for programming education and GenAI tool design.", "AI": {"tldr": "GitHub Copilot\u5728\u9057\u7559\u4ee3\u7801\u7f16\u7a0b\u4efb\u52a1\u4e2d\u80fd\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u4e0d\u4f1a\u6539\u5584\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u5b58\u5728\u7406\u89e3-\u6027\u80fd\u5dee\u8ddd", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0fAI\u7f16\u7a0b\u52a9\u624b\u5728\u9057\u7559\u4ee3\u7801\u7ef4\u62a4\u4efb\u52a1\u4e2d\u5bf9\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u7684\u5f71\u54cd", "method": "\u91c7\u7528\u7ec4\u5185\u5b9e\u9a8c\u8bbe\u8ba1\uff0c18\u540d\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u751f\u5728\u6709/\u65e0Copilot\u60c5\u51b5\u4e0b\u5b8c\u6210\u529f\u80fd\u5b9e\u73b0\u4efb\u52a1\uff0c\u6d4b\u91cf\u4efb\u52a1\u65f6\u95f4\u3001\u6d4b\u8bd5\u7528\u4f8b\u901a\u8fc7\u7387\u548c\u7406\u89e3\u5206\u6570", "result": "Copilot\u663e\u8457\u51cf\u5c11\u4efb\u52a1\u65f6\u95f4\u5e76\u63d0\u9ad8\u6d4b\u8bd5\u7528\u4f8b\u901a\u8fc7\u7387\uff0c\u4f46\u7406\u89e3\u5206\u6570\u65e0\u5dee\u5f02\uff0c\u7406\u89e3\u4e0e\u4efb\u52a1\u8868\u73b0\u65e0\u76f8\u5173\u6027", "conclusion": "GenAI\u5de5\u5177\u80fd\u52a0\u901f\u9057\u7559\u4ee3\u7801\u5e93\u7684\u7f16\u7a0b\u8fdb\u5ea6\uff0c\u4f46\u8fd9\u79cd\u8fdb\u6b65\u53ef\u80fd\u4e0d\u4f1a\u5e26\u6765\u5bf9\u4ee3\u7801\u5e93\u7684\u66f4\u597d\u7406\u89e3", "topic": "agent analysis"}}
{"id": "2511.03136", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03136", "abs": "https://arxiv.org/abs/2511.03136", "authors": ["Kexing Ji", "Shiyun Fu", "Cuiyun Gao", "Yujia Chen", "Zezhou Yang", "Chaozheng Wang", "Yuetang Deng"], "title": "Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat", "comment": "Accepted by ASE 2025 Industry Track", "summary": "Large Code Models (LCMs) show potential in code intelligence, but their\neffectiveness is greatly influenced by prompt quality. Current prompt design is\nmostly manual, which is time-consuming and highly dependent on specific LCMs\nand tasks. While automated prompt generation (APG) exists in NLP, it is\nunderexplored for code intelligence. This creates a gap, as automating the\nprompt process is essential for developers facing diverse tasks and black-box\nLCMs.\n  To mitigate this, we empirically investigate two important parts of APG:\nInstruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a\ntask-related description to instruct LCMs, while MSR guides them to produce\nlogical steps before the final answer. We evaluate widely-used APG methods for\neach part on four open-source LCMs and three code intelligence tasks: code\ntranslation (PL-PL), code summarization (PL-NL), and API recommendation\n(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance\nperformance compared to basic prompts. Based on these results, we propose a\nnovel APG approach combining the best methods of the two parts. Experiments\nshow our approach achieves average improvements of 28.38% in CodeBLEU (code\ntranslation), 58.11% in ROUGE-L (code summarization), and 84.53% in\nSuccessRate@1 (API recommendation) over basic prompts. To validate its\neffectiveness in an industrial scenario, we evaluate our approach on\nWeChat-Bench, a proprietary dataset, achieving an average MRR improvement of\n148.89% for API recommendation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u4ee3\u7801\u6a21\u578b\u7684\u81ea\u52a8\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u751f\u6210\u548c\u591a\u6b65\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u4ee3\u7801\u6a21\u578b\u7684\u6709\u6548\u6027\u53d7\u63d0\u793a\u8d28\u91cf\u5f71\u54cd\u5f88\u5927\uff0c\u5f53\u524d\u63d0\u793a\u8bbe\u8ba1\u591a\u4e3a\u624b\u52a8\u4e14\u4f9d\u8d56\u7279\u5b9a\u6a21\u578b\u548c\u4efb\u52a1\u3002\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u81ea\u52a8\u63d0\u793a\u751f\u6210\u65b9\u6cd5\u5728\u4ee3\u7801\u667a\u80fd\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5b9e\u8bc1\u7814\u7a76\u6307\u4ee4\u751f\u6210\u548c\u591a\u6b65\u63a8\u7406\u4e24\u79cd\u81ea\u52a8\u63d0\u793a\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u79cd\u5f00\u6e90\u5927\u578b\u4ee3\u7801\u6a21\u578b\u548c\u4e09\u79cd\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e0a\u8bc4\u4f30\u3002\u57fa\u4e8e\u7ed3\u679c\u63d0\u51fa\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u6700\u4f73\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u6307\u4ee4\u751f\u6210\u548c\u591a\u6b65\u63a8\u7406\u76f8\u6bd4\u57fa\u7840\u63d0\u793a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u65b0\u65b9\u6cd5\u5728\u4ee3\u7801\u7ffb\u8bd1\u3001\u4ee3\u7801\u6458\u8981\u548cAPI\u63a8\u8350\u4efb\u52a1\u4e0a\u5206\u522b\u83b7\u5f9728.38%\u300158.11%\u548c84.53%\u7684\u5e73\u5747\u6539\u8fdb\u3002\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\uff0cAPI\u63a8\u8350\u7684MRR\u63d0\u5347148.89%\u3002", "conclusion": "\u81ea\u52a8\u63d0\u793a\u751f\u6210\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u4ee3\u7801\u6a21\u578b\u5728\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u591a\u6837\u5316\u4efb\u52a1\u548c\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2511.03724", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03724", "abs": "https://arxiv.org/abs/2511.03724", "authors": ["Richard Dewey", "Janos Botyanszki", "Ciamac C. Moallemi", "Andrew T. Zheng"], "title": "Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning", "comment": null, "summary": "AI researchers have long focused on poker-like games as a testbed for\nenvironments characterized by multi-player dynamics, imperfect information, and\nreasoning under uncertainty. While recent breakthroughs have matched elite\nhuman play at no-limit Texas hold'em, the multi-player dynamics are subdued:\nmost hands converge quickly with only two players engaged through multiple\nrounds of bidding. In this paper, we present Solly, the first AI agent to\nachieve elite human play in reduced-format Liar's Poker, a game characterized\nby extensive multi-player engagement. We trained Solly using self-play with a\nmodel-free, actor-critic, deep reinforcement learning algorithm. Solly played\nat an elite human level as measured by win rate (won over 50% of hands) and\nequity (money won) in heads-up and multi-player Liar's Poker. Solly also\noutperformed large language models (LLMs), including those with reasoning\nabilities, on the same metrics. Solly developed novel bidding strategies,\nrandomized play effectively, and was not easily exploitable by world-class\nhuman players.", "AI": {"tldr": "Solly\u662f\u7b2c\u4e00\u4e2a\u5728\u7b80\u5316\u7248Liar's Poker\u4e2d\u8fbe\u5230\u7cbe\u82f1\u4eba\u7c7b\u6c34\u5e73\u7684AI\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u65e0\u6a21\u578b\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\uff0c\u5728\u5355\u6311\u548c\u591a\u73a9\u5bb6\u6e38\u620f\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u8d85\u8d8a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "AI\u7814\u7a76\u957f\u671f\u4ee5\u6765\u5173\u6ce8\u6251\u514b\u7c7b\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u591a\u73a9\u5bb6\u52a8\u6001\u3001\u4e0d\u5b8c\u5168\u4fe1\u606f\u548c\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u7684\u73af\u5883\u3002\u867d\u7136\u6700\u8fd1\u5728\u65e0\u9650\u5236\u5fb7\u5dde\u6251\u514b\u4e2d\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u591a\u73a9\u5bb6\u52a8\u6001\u88ab\u6291\u5236\uff0c\u5927\u591a\u6570\u624b\u724c\u5f88\u5feb\u7ed3\u675f\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u80fd\u5728\u591a\u73a9\u5bb6\u53c2\u4e0e\u5ea6\u9ad8\u7684Liar's Poker\u4e2d\u8fbe\u5230\u7cbe\u82f1\u6c34\u5e73\u7684AI\u3002", "method": "\u4f7f\u7528\u65e0\u6a21\u578b\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u81ea\u6211\u5bf9\u5f08\u8bad\u7ec3\u3002", "result": "Solly\u5728\u5355\u6311\u548c\u591a\u73a9\u5bb6Liar's Poker\u4e2d\u90fd\u8fbe\u5230\u4e86\u7cbe\u82f1\u4eba\u7c7b\u6c34\u5e73\uff08\u80dc\u7387\u8d85\u8fc750%\u4e14\u76c8\u5229\uff09\uff0c\u8d85\u8d8a\u4e86\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u7ade\u4ef7\u7b56\u7565\uff0c\u6709\u6548\u968f\u673a\u5316\u6e38\u620f\uff0c\u4e14\u4e0d\u6613\u88ab\u4e16\u754c\u7ea7\u4eba\u7c7b\u73a9\u5bb6\u5229\u7528\u3002", "conclusion": "Solly\u6210\u529f\u5c55\u793a\u4e86\u5728\u5177\u6709\u5e7f\u6cdb\u591a\u73a9\u5bb6\u53c2\u4e0e\u7684\u6e38\u620f\u4e2d\u8fbe\u5230\u7cbe\u82f1\u4eba\u7c7b\u6c34\u5e73\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u591a\u73a9\u5bb6\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u7684AI\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2511.03153", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03153", "abs": "https://arxiv.org/abs/2511.03153", "authors": ["Khouloud Oueslati", "Maxime Lamothe", "Foutse Khomh"], "title": "RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring", "comment": null, "summary": "Large Language Models (LLMs) have substantially influenced various software\nengineering tasks. Indeed, in the case of software refactoring, traditional\nLLMs have shown the ability to reduce development time and enhance code\nquality. However, these LLMs often rely on static, detailed instructions for\nspecific tasks. In contrast, LLM-based agents can dynamically adapt to evolving\ncontexts and autonomously make decisions by interacting with software tools and\nexecuting workflows. In this paper, we explore the potential of LLM-based\nagents in supporting refactoring activities. Specifically, we introduce\nRefAgent, a multi-agent LLM-based framework for end-to-end software\nrefactoring. RefAgent consists of specialized agents responsible for planning,\nexecuting, testing, and iteratively refining refactorings using self-reflection\nand tool-calling capabilities. We evaluate RefAgent on eight open-source Java\nprojects, comparing its effectiveness against a single-agent approach, a\nsearch-based refactoring tool, and historical developer refactorings. Our\nassessment focuses on: (1) the impact of generated refactorings on software\nquality, (2) the ability to identify refactoring opportunities, and (3) the\ncontribution of each LLM agent through an ablation study. Our results show that\nRefAgent achieves a median unit test pass rate of 90%, reduces code smells by a\nmedian of 52.5%, and improves key quality attributes (e.g., reusability) by a\nmedian of 8.6%. Additionally, it closely aligns with developer refactorings and\nthe search-based tool in identifying refactoring opportunities, attaining a\nmedian F1-score of 79.15% and 72.7%, respectively. Compared to single-agent\napproaches, RefAgent improves the median unit test pass rate by 64.7% and the\nmedian compilation success rate by 40.1%. These findings highlight the promise\nof multi-agent architectures in advancing automated software refactoring.", "AI": {"tldr": "RefAgent\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u8f6f\u4ef6\u91cd\u6784\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u89c4\u5212\u3001\u6267\u884c\u3001\u6d4b\u8bd5\u548c\u8fed\u4ee3\u4f18\u5316\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u548c\u91cd\u6784\u6548\u679c\u3002", "motivation": "\u4f20\u7edfLLM\u5728\u8f6f\u4ef6\u91cd\u6784\u4e2d\u4f9d\u8d56\u9759\u6001\u6307\u4ee4\uff0c\u800cLLM\u667a\u80fd\u4f53\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0a\u4e0b\u6587\u5e76\u4e0e\u8f6f\u4ef6\u5de5\u5177\u4ea4\u4e92\uff0c\u81ea\u4e3b\u51b3\u7b56\u6267\u884c\u91cd\u6784\u5de5\u4f5c\u6d41\u3002", "method": "RefAgent\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542b\u4e13\u95e8\u8d1f\u8d23\u89c4\u5212\u3001\u6267\u884c\u3001\u6d4b\u8bd5\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u4ee3\u7406\uff0c\u5229\u7528\u81ea\u53cd\u601d\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\u8fdb\u884c\u7aef\u5230\u7aef\u91cd\u6784\u3002", "result": "\u57288\u4e2a\u5f00\u6e90Java\u9879\u76ee\u4e0a\u8bc4\u4f30\uff0cRefAgent\u8fbe\u523090%\u7684\u4e2d\u4f4d\u6570\u5355\u5143\u6d4b\u8bd5\u901a\u8fc7\u7387\uff0c\u51cf\u5c1152.5%\u7684\u4ee3\u7801\u5f02\u5473\uff0c\u63d0\u53478.6%\u7684\u5173\u952e\u8d28\u91cf\u5c5e\u6027\uff0cF1\u5206\u6570\u8fbe79.15%\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u91cd\u6784\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u5347\u4e8664.7%\u7684\u6d4b\u8bd5\u901a\u8fc7\u7387\u548c40.1%\u7684\u7f16\u8bd1\u6210\u529f\u7387\u3002", "topic": "code agent"}}
{"id": "2511.03182", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03182", "abs": "https://arxiv.org/abs/2511.03182", "authors": ["Vinaik Chhetri", "A. B Siddique", "Umar Farooq"], "title": "Understanding Robustness of Model Editing in Code LLMs: An Empirical Study", "comment": "26 pages, 2 figures, 15 tables", "summary": "Large language models (LLMs) are increasingly used in software development.\nHowever, while LLMs remain static after pretraining, programming languages and\nAPIs continue to evolve, leading to the generation of deprecated or\nincompatible code that undermines reliability. Retraining LLMs from scratch to\nreflect such changes is computationally expensive, making model editing a\npromising lightweight alternative that updates only a small subset of\nparameters. Despite its potential, it remains unclear whether model editing\nyields genuine syntactic and semantic adaptations or merely superficial fixes.\nIn this work, we present a systematic study of five state-of-the-art model\nediting methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We\napply these methods to three leading open-source code LLMs, CodeLlama,\nCodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.\nOur evaluation covers both instant and sequential editing settings, using three\ndisjoint evaluation sets designed to assess reliability, generalization, and\nspecificity. We measure model correctness at three levels: successful\ncompilation, partial test case pass, and full test pass. Our findings show that\ninstant edits consistently degrade model performance, with syntactic validity\ndropping by up to 86 percentage points and functional correctness declining by\n45 points even in the best-performing setting. Sequential edits further amplify\nthis degradation, and in some cases, model performance collapses entirely.\nAcross all models, most passing generations relied on workarounds rather than\ncorrectly adopting the intended changes, while faulty adoptions that result in\ntest failures or compilation errors were significantly more frequent. Correct\nadoptions, where the model correctly integrates the intended change, occurred\nin only about 6% of cases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u4ee3\u7801LLMs\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5728API\u5f03\u7528\u573a\u666f\u4e0b\uff0c\u5373\u65f6\u7f16\u8f91\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u8bed\u6cd5\u6709\u6548\u6027\u4e0b\u964d\u9ad8\u8fbe86\u4e2a\u767e\u5206\u70b9\uff0c\u529f\u80fd\u6b63\u786e\u6027\u4e0b\u964d45\u4e2a\u767e\u5206\u70b9\u3002\u987a\u5e8f\u7f16\u8f91\u8fdb\u4e00\u6b65\u52a0\u5267\u6027\u80fd\u9000\u5316\uff0c\u6b63\u786e\u91c7\u7528\u9884\u671f\u66f4\u6539\u7684\u60c5\u51b5\u4ec5\u5360\u7ea66%\u3002", "motivation": "LLMs\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u9884\u8bad\u7ec3\u540e\u4fdd\u6301\u9759\u6001\uff0c\u800c\u7f16\u7a0b\u8bed\u8a00\u548cAPI\u6301\u7eed\u6f14\u8fdb\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u65f6\u6216\u4e0d\u517c\u5bb9\u4ee3\u7801\u3002\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3LLMs\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u6a21\u578b\u7f16\u8f91\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u662f\u5426\u80fd\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u9002\u5e94\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff08Constrained FT\u3001GRACE\u3001MEMIT\u3001PMET\u3001ROME\uff09\uff0c\u5e94\u7528\u4e8e\u4e09\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u4ee3\u7801LLMs\uff08CodeLlama\u3001CodeQwen1.5\u3001DeepSeek-Coder\uff09\uff0c\u5728\u53d7\u63a7API\u5f03\u7528\u573a\u666f\u4e0b\u8fdb\u884c\u5373\u65f6\u548c\u987a\u5e8f\u7f16\u8f91\u8bbe\u7f6e\u8bc4\u4f30\u3002", "result": "\u5373\u65f6\u7f16\u8f91\u6301\u7eed\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u8bed\u6cd5\u6709\u6548\u6027\u4e0b\u964d\u9ad8\u8fbe86\u4e2a\u767e\u5206\u70b9\uff0c\u529f\u80fd\u6b63\u786e\u6027\u4e0b\u964d45\u4e2a\u767e\u5206\u70b9\u3002\u987a\u5e8f\u7f16\u8f91\u8fdb\u4e00\u6b65\u52a0\u5267\u9000\u5316\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6a21\u578b\u6027\u80fd\u5b8c\u5168\u5d29\u6e83\u3002\u5927\u591a\u6570\u901a\u8fc7\u751f\u6210\u4f9d\u8d56\u53d8\u901a\u65b9\u6cd5\u800c\u975e\u6b63\u786e\u91c7\u7528\u9884\u671f\u66f4\u6539\uff0c\u6b63\u786e\u91c7\u7528\u4ec5\u5360\u7ea66%\u3002", "conclusion": "\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u4ee3\u7801LLMs\u4e2d\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u4ea7\u751f\u8868\u9762\u4fee\u590d\u800c\u975e\u771f\u6b63\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u9002\u5e94\uff0c\u6b63\u786e\u91c7\u7528\u9884\u671f\u66f4\u6539\u7684\u6bd4\u4f8b\u6781\u4f4e\uff0c\u8868\u660e\u5f53\u524d\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u53ef\u9760\u6027\u548c\u529f\u80fd\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "2511.03047", "categories": ["cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.03047", "abs": "https://arxiv.org/abs/2511.03047", "authors": ["Emi Soroka", "Tanmay Chopra", "Krish Desai", "Sanjay Lall"], "title": "Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions", "comment": "Under review at ICLR 2026", "summary": "Large language models (LLMs) have seen increasing popularity in enterprise\napplications where AI agents and humans engage in objective-driven\ninteractions. However, these systems are difficult to evaluate: data may be\ncomplex and unlabeled; human annotation is often impractical at scale; custom\nmetrics can monitor for specific errors, but not previously-undetected ones;\nand LLM judges can produce unreliable results. We introduce the first set of\nunsupervised metrics for objective-driven interactions, leveraging statistical\nproperties of unlabeled interaction data and using fine-tuned LLMs to adapt to\ndistributional shifts. We develop metrics for labeling user goals, measuring\ngoal completion, and quantifying LLM uncertainty without grounding evaluations\nin human-generated ideal responses. Our approach is validated on open-domain\nand task-specific interaction data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u76ee\u6807\u9a71\u52a8\u4ea4\u4e92\u7684\u65e0\u76d1\u7763\u8bc4\u4f30\u6307\u6807\uff0c\u5229\u7528\u672a\u6807\u6ce8\u4ea4\u4e92\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\u548c\u5fae\u8c03LLM\u6765\u9002\u5e94\u5206\u5e03\u53d8\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u7406\u60f3\u56de\u7b54\u4f5c\u4e3a\u57fa\u51c6\u3002", "motivation": "\u4f01\u4e1a\u5e94\u7528\u4e2dAI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u7684\u76ee\u6807\u9a71\u52a8\u4ea4\u4e92\u7cfb\u7edf\u96be\u4ee5\u8bc4\u4f30\uff1a\u6570\u636e\u590d\u6742\u4e14\u65e0\u6807\u6ce8\u3001\u4eba\u5de5\u6807\u6ce8\u4e0d\u5177\u89c4\u6a21\u6027\u3001\u81ea\u5b9a\u4e49\u6307\u6807\u65e0\u6cd5\u68c0\u6d4b\u672a\u77e5\u9519\u8bef\u3001LLM\u8bc4\u4f30\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u5229\u7528\u672a\u6807\u6ce8\u4ea4\u4e92\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u4f7f\u7528\u5fae\u8c03LLM\u9002\u5e94\u5206\u5e03\u53d8\u5316\uff0c\u5f00\u53d1\u4e86\u7528\u6237\u76ee\u6807\u6807\u6ce8\u3001\u76ee\u6807\u5b8c\u6210\u5ea6\u6d4b\u91cf\u548cLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u65e0\u76d1\u7763\u6307\u6807\u3002", "result": "\u5728\u5f00\u653e\u9886\u57df\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u4ea4\u4e92\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u5957\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u65e0\u76d1\u7763\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u76ee\u6807\u9a71\u52a8\u4ea4\u4e92\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2511.03404", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03404", "abs": "https://arxiv.org/abs/2511.03404", "authors": ["Qianhui Zhao", "Li Zhang", "Fang Liu", "Junhang Cheng", "Chengru Wu", "Junchen Ai", "Qiaoyuanhe Meng", "Lichen Zhang", "Xiaoli Lian", "Shubin Song", "Yuanping Guo"], "title": "Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling", "comment": null, "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nprogress in automated code generation. In real-world software engineering, the\ngrowing demand for rapid iteration and continuous delivery underscores the\nimportance of project-level code generation, where LLMs are expected to\ngenerate complete software projects directly from complex user requirements.\nAlthough existing studies have made initial explorations, they still face key\nlimitations, including unrealistic datasets and unreliable evaluation metrics\nthat fail to reflect real-world complexity, the semantic gap between\nhuman-written requirements and machine-interpretable structures, and\ndifficulties in managing hierarchical dependencies and maintaining quality\nthroughout the generation process. To address these limitations, we first\nintroduce CodeProjectEval, a project-level code generation dataset built from\n18 real-world repositories with 12.7 files and 2,388.6 lines of code per task\non average, supplemented with documentation and executable test cases for\nautomatic evaluation. We further propose ProjectGen, a multi-agent framework\nthat decomposes projects into architecture design, skeleton generation, and\ncode filling stages with iterative refinement and memory-based context\nmanagement. Within this framework, we introduce the Semantic Software\nArchitecture Tree (SSAT), a structured and semantically rich representation\nthat effectively bridges user requirements and source code implementation.\nExperiments show that ProjectGen achieves state-of-the-art performance, passing\n52/124 test cases on the small-scale project-level code generation dataset\nDevBench, a 57% improvement over the baseline approaches, and 310 test cases on\nCodeProjectEval, representing an improvement of roughly tenfold compared to the\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86ProjectGen\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548cCodeProjectEval\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u9879\u76ee\u7ea7\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u73b0\u5b9e\u6570\u636e\u96c6\u7f3a\u4e4f\u3001\u8bed\u4e49\u9e3f\u6c9f\u548c\u4f9d\u8d56\u7ba1\u7406\u7b49\u95ee\u9898\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9879\u76ee\u7ea7\u4ee3\u7801\u751f\u6210\u7814\u7a76\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u4e0d\u73b0\u5b9e\u7684\u6570\u636e\u96c6\u548c\u4e0d\u53ef\u9760\u7684\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0c\u4eba\u7c7b\u9700\u6c42\u4e0e\u673a\u5668\u53ef\u89e3\u91ca\u7ed3\u6784\u4e4b\u95f4\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4ee5\u53ca\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7ba1\u7406\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\u548c\u4fdd\u6301\u8d28\u91cf\u7684\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86ProjectGen\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u9879\u76ee\u5206\u89e3\u4e3a\u67b6\u6784\u8bbe\u8ba1\u3001\u9aa8\u67b6\u751f\u6210\u548c\u4ee3\u7801\u586b\u5145\u4e09\u4e2a\u9636\u6bb5\uff0c\u91c7\u7528\u8fed\u4ee3\u4f18\u5316\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002\u5f15\u5165\u4e86\u8bed\u4e49\u8f6f\u4ef6\u67b6\u6784\u6811\uff08SSAT\uff09\u4f5c\u4e3a\u7ed3\u6784\u5316\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u6709\u6548\u6865\u63a5\u7528\u6237\u9700\u6c42\u548c\u6e90\u4ee3\u7801\u5b9e\u73b0\u3002", "result": "\u5728\u5c0f\u578b\u9879\u76ee\u7ea7\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6DevBench\u4e0a\u901a\u8fc7\u4e8652/124\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e8657%\uff1b\u5728CodeProjectEval\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u4e86310\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e86\u7ea610\u500d\u3002", "conclusion": "ProjectGen\u6846\u67b6\u5728\u9879\u76ee\u7ea7\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5206\u89e3\u548c\u8bed\u4e49\u67b6\u6784\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "swe application"}}
{"id": "2511.03517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03517", "abs": "https://arxiv.org/abs/2511.03517", "authors": ["Wencheng Ye", "Yan Liu"], "title": "U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility", "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities in software\nengineering tasks, yet most existing LLM-based SWE-Agents mainly tackle\nwell-defined problems using conventional methods, often overlooking alternative\nor innovative solutions beyond their predefined frameworks. This limitation is\nevident in open-world software environments, where emerging challenges\ntranscend established paradigms.\n  We propose U2F (Unknown Unknowns to Functional solutions), a\ncognitive-inspired, uncertainty-embracing multi-agent framework that\nsystematically surfaces \"Unknown Unknowns\" - novel solution pathways absent\nfrom initial formulations but holding innovative potential. U2F consists of two\nkey components: (1) a Discovery-Exploration-Integration agent system for\nuncovering and synthesizing potential solutions, and (2) cognitive enhancement\nmechanisms across three dimensions: cross-domain analogical reasoning, reverse\nthinking, and external validation, which strategically reframe and extend\nconventional solution boundaries.\n  Applied to 218 real-world software enabler stories curated from authentic\nengineering tasks, U2F achieved notable improvements: human experts reported a\n14 percent increase in overall novelty, 51 percent improvement in semantic\nnovelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based\nevaluator. These results highlight the potential of embracing uncertainty as a\ncatalyst for innovation in software engineering.", "AI": {"tldr": "\u63d0\u51faU2F\u6846\u67b6\uff0c\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u4e0d\u786e\u5b9a\u6027\u62e5\u62b1\u65b9\u6cd5\uff0c\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u53d1\u73b0\u672a\u77e5\u672a\u77e5\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848", "motivation": "\u73b0\u6709LLM-based SWE-Agent\u4e3b\u8981\u89e3\u51b3\u660e\u786e\u5b9a\u4e49\u7684\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u9884\u5b9a\u4e49\u6846\u67b6\u4e4b\u5916\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5f00\u653e\u4e16\u754c\u8f6f\u4ef6\u73af\u5883\u4e2d\u7684\u65b0\u5174\u6311\u6218", "method": "U2F\u5305\u542b\u53d1\u73b0-\u63a2\u7d22-\u96c6\u6210\u4ee3\u7406\u7cfb\u7edf\u548c\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8ba4\u77e5\u589e\u5f3a\u673a\u5236\uff1a\u8de8\u9886\u57df\u7c7b\u6bd4\u63a8\u7406\u3001\u9006\u5411\u601d\u7ef4\u548c\u5916\u90e8\u9a8c\u8bc1", "result": "\u5728218\u4e2a\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u4f7f\u80fd\u6545\u4e8b\u4e0a\uff0cU2F\u5b9e\u73b0\u4e8614%\u6574\u4f53\u65b0\u9896\u6027\u63d0\u5347\u300151%\u8bed\u4e49\u65b0\u9896\u6027\u6539\u8fdb\u548c\u7a33\u5b9a\u7684\u53ef\u884c\u6027\uff084.02/5.0\uff09", "conclusion": "\u62e5\u62b1\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u521b\u65b0\u7684\u50ac\u5316\u5242", "topic": "swe application"}}
{"id": "2511.03690", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03690", "abs": "https://arxiv.org/abs/2511.03690", "authors": ["Xingyao Wang", "Simon Rosenberg", "Juan Michelini", "Calvin Smith", "Hoang Tran", "Engel Nyst", "Rohit Malhotra", "Xuhui Zhou", "Valerie Chen", "Robert Brennan", "Graham Neubig"], "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents", "comment": null, "summary": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale.", "AI": {"tldr": "OpenHands Software Agent SDK\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u4ee3\u7406\u5b9e\u73b0\u3001\u5b89\u5168\u53ef\u9760\u7684\u6267\u884c\u73af\u5883\u4ee5\u53ca\u591a\u79cd\u7528\u6237\u4ea4\u4e92\u63a5\u53e3\u3002", "motivation": "\u6784\u5efa\u751f\u4ea7\u7ea7\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u9762\u4e34\u5b9e\u73b0\u7075\u6d3b\u6027\u3001\u6267\u884c\u53ef\u9760\u6027\u548c\u7528\u6237\u4ea4\u4e92\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5de5\u5177\u5305\u6765\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u5bf9OpenHands\u6846\u67b6\u7684\u4ee3\u7406\u7ec4\u4ef6\u8fdb\u884c\u67b6\u6784\u91cd\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u7b80\u5355\u6613\u7528\u7684\u63a5\u53e3\u3001\u672c\u5730\u5230\u8fdc\u7a0b\u7684\u65e0\u7f1d\u6267\u884c\u3001\u96c6\u6210REST/WebSocket\u670d\u52a1\uff0c\u5e76\u652f\u6301\u591a\u79cd\u4ea4\u4e92\u63a5\u53e3\u3002", "result": "\u5728SWE-Bench Verified\u548cGAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u539f\u751f\u6c99\u7bb1\u6267\u884c\u3001\u751f\u547d\u5468\u671f\u63a7\u5236\u3001\u6a21\u578b\u65e0\u5173\u7684\u591aLLM\u8def\u7531\u548c\u5185\u7f6e\u5b89\u5168\u5206\u6790\u7b49\u72ec\u7279\u529f\u80fd\u3002", "conclusion": "OpenHands Software Agent SDK\u4e3a\u539f\u578b\u8bbe\u8ba1\u3001\u5f00\u53d1\u65b0\u578b\u81ea\u5b9a\u4e49\u5e94\u7528\u548c\u5927\u89c4\u6a21\u53ef\u9760\u90e8\u7f72\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2511.03100", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.03100", "abs": "https://arxiv.org/abs/2511.03100", "authors": ["Hao Xiang Li", "Michael Amir", "Amanda Prorok"], "title": "Scaling Multi-Agent Environment Co-Design with Diffusion Models", "comment": null, "summary": "The agent-environment co-design paradigm jointly optimises agent policies and\nenvironment configurations in search of improved system performance. With\napplication domains ranging from warehouse logistics to windfarm management,\nco-design promises to fundamentally change how we deploy multi-agent systems.\nHowever, current co-design methods struggle to scale. They collapse under\nhigh-dimensional environment design spaces and suffer from sample inefficiency\nwhen addressing moving targets inherent to joint optimisation. We address these\nchallenges by developing Diffusion Co-Design (DiCoDe), a scalable and\nsample-efficient co-design framework pushing co-design towards practically\nrelevant settings. DiCoDe incorporates two core innovations. First, we\nintroduce Projected Universal Guidance (PUG), a sampling technique that enables\nDiCoDe to explore a distribution of reward-maximising environments while\nsatisfying hard constraints such as spatial separation between obstacles.\nSecond, we devise a critic distillation mechanism to share knowledge from the\nreinforcement learning critic, ensuring that the guided diffusion model adapts\nto evolving agent policies using a dense and up-to-date learning signal.\nTogether, these improvements lead to superior environment-policy pairs when\nvalidated on challenging multi-agent environment co-design benchmarks including\nwarehouse automation, multi-agent pathfinding and wind farm optimisation. Our\nmethod consistently exceeds the state-of-the-art, achieving, for example, 39%\nhigher rewards in the warehouse setting with 66% fewer simulation samples. This\nsets a new standard in agent-environment co-design, and is a stepping stone\ntowards reaping the rewards of co-design in real world domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86Diffusion Co-Design (DiCoDe)\u6846\u67b6\uff0c\u901a\u8fc7Projected Universal Guidance\u91c7\u6837\u6280\u672f\u548ccritic\u84b8\u998f\u673a\u5236\uff0c\u89e3\u51b3agent-environment\u534f\u540c\u8bbe\u8ba1\u4e2d\u9ad8\u7ef4\u73af\u5883\u8bbe\u8ba1\u7a7a\u95f4\u548c\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u7ef4\u73af\u5883\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4e14\u5728\u8054\u5408\u4f18\u5316\u4e2d\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "DiCoDe\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1aProjected Universal Guidance\u91c7\u6837\u6280\u672f\uff0c\u7528\u4e8e\u63a2\u7d22\u6ee1\u8db3\u786c\u7ea6\u675f\u7684\u5956\u52b1\u6700\u5927\u5316\u73af\u5883\u5206\u5e03\uff1bcritic\u84b8\u998f\u673a\u5236\uff0c\u901a\u8fc7\u5171\u4eab\u5f3a\u5316\u5b66\u4e60critic\u7684\u77e5\u8bc6\uff0c\u786e\u4fdd\u6269\u6563\u6a21\u578b\u9002\u5e94\u6f14\u5316\u7684\u667a\u80fd\u4f53\u7b56\u7565\u3002", "result": "\u5728\u4ed3\u5e93\u81ea\u52a8\u5316\u3001\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u548c\u98ce\u7535\u573a\u4f18\u5316\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u4f8b\u5982\u5728\u4ed3\u5e93\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e8639%\u7684\u66f4\u9ad8\u5956\u52b1\u548c66%\u66f4\u5c11\u7684\u6a21\u62df\u6837\u672c\u3002", "conclusion": "DiCoDe\u4e3aagent-environment\u534f\u540c\u8bbe\u8ba1\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u662f\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u534f\u540c\u8bbe\u8ba1\u6536\u76ca\u7684\u91cd\u8981\u8fdb\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.03261", "categories": ["cs.CL", "cs.AI", "I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2511.03261", "abs": "https://arxiv.org/abs/2511.03261", "authors": ["Ranul Dayarathne", "Uvini Ranaweera", "Upeksha Ganegoda"], "title": "Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature", "comment": "18 pages, 4 figures, 5 tables, presented at the 5th International\n  Conference on Artificial Intelligence in Education Technology", "summary": "Retrieval Augmented Generation (RAG) is emerging as a powerful technique to\nenhance the capabilities of Generative AI models by reducing hallucination.\nThus, the increasing prominence of RAG alongside Large Language Models (LLMs)\nhas sparked interest in comparing the performance of different LLMs in\nquestion-answering (QA) in diverse domains. This study compares the performance\nof four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,\nFalcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA\ntasks within the computer science literature leveraging RAG support. Evaluation\nmetrics employed in the study include accuracy and precision for binary\nquestions and ranking by a human expert, ranking by Google's AI model Gemini,\nalongside cosine similarity for long-answer questions. GPT-3.5, when paired\nwith RAG, effectively answers binary and long-answer questions, reaffirming its\nstatus as an advanced LLM. Regarding open-source LLMs, Mistral AI's\nMistral-7b-instruct paired with RAG surpasses the rest in answering both binary\nand long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b\nreports the shortest average latency in generating responses, whereas\nLLaMa2-7b-chat by Meta reports the highest average latency. This research\nunderscores the fact that open-source LLMs, too, can go hand in hand with\nproprietary models like GPT-3.5 with better infrastructure.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u5f00\u6e90LLM\uff08Mistral-7b-instruct\u3001LLaMa2-7b-chat\u3001Falcon-7b-instruct\u3001Orca-mini-v3-7b\uff09\u548cGPT-3.5\u5728RAG\u652f\u6301\u4e0b\u7684\u95ee\u7b54\u4efb\u52a1\u8868\u73b0\uff0c\u53d1\u73b0GPT-3.5\u8868\u73b0\u6700\u4f73\uff0c\u800cMistral-7b-instruct\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u597d\u3002", "motivation": "\u968f\u7740RAG\u6280\u672f\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u6bd4\u8f83\u4e0d\u540cLLM\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u6587\u732e\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u5f00\u6e90LLM\u548cGPT-3.5\uff0c\u5728RAG\u652f\u6301\u4e0b\u8fdb\u884c\u95ee\u7b54\u4efb\u52a1\u8bc4\u4f30\uff0c\u91c7\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u4e13\u5bb6\u6392\u540d\u3001Gemini\u6392\u540d\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7b49\u6307\u6807\u3002", "result": "GPT-3.5\u5728RAG\u652f\u6301\u4e0b\u8868\u73b0\u6700\u4f73\uff1b\u5f00\u6e90\u6a21\u578b\u4e2dMistral-7b-instruct\u8868\u73b0\u6700\u597d\uff1bOrca-mini-v3-7b\u54cd\u5e94\u5ef6\u8fdf\u6700\u77ed\uff0cLLaMa2-7b-chat\u5ef6\u8fdf\u6700\u9ad8\u3002", "conclusion": "\u5f00\u6e90LLM\u5728\u9002\u5f53\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u4e0b\u53ef\u4ee5\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0cRAG\u6280\u672f\u80fd\u6709\u6548\u589e\u5f3aLLM\u7684\u95ee\u7b54\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.03187", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03187", "abs": "https://arxiv.org/abs/2511.03187", "authors": ["Jonghae Park", "Daesol Cho", "Jusuk Lee", "Dongseok Shim", "Inkyu Jang", "H. Jin Kim"], "title": "Periodic Skill Discovery", "comment": "NeurIPS 2025", "summary": "Unsupervised skill discovery in reinforcement learning (RL) aims to learn\ndiverse behaviors without relying on external rewards. However, current methods\noften overlook the periodic nature of learned skills, focusing instead on\nincreasing the mutual dependence between states and skills or maximizing the\ndistance traveled in latent space. Considering that many robotic tasks --\nparticularly those involving locomotion -- require periodic behaviors across\nvarying timescales, the ability to discover diverse periodic skills is\nessential. Motivated by this, we propose Periodic Skill Discovery (PSD), a\nframework that discovers periodic behaviors in an unsupervised manner. The key\nidea of PSD is to train an encoder that maps states to a circular latent space,\nthereby naturally encoding periodicity in the latent representation. By\ncapturing temporal distance, PSD can effectively learn skills with diverse\nperiods in complex robotic tasks, even with pixel-based observations. We\nfurther show that these learned skills achieve high performance on downstream\ntasks such as hurdling. Moreover, integrating PSD with an existing skill\ndiscovery method offers more diverse behaviors, thus broadening the agent's\nrepertoire. Our code and demos are available at\nhttps://jonghaepark.github.io/psd/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5468\u671f\u6027\u6280\u80fd\u53d1\u73b0\uff08PSD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u72b6\u6001\u6620\u5c04\u5230\u5706\u5f62\u6f5c\u5728\u7a7a\u95f4\u6765\u65e0\u76d1\u7763\u5730\u53d1\u73b0\u5468\u671f\u6027\u884c\u4e3a\uff0c\u80fd\u591f\u5b66\u4e60\u5177\u6709\u4e0d\u540c\u5468\u671f\u7684\u6280\u80fd\uff0c\u5e76\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5b66\u4e60\u6280\u80fd\u7684\u5468\u671f\u6027\u7279\u6027\uff0c\u800c\u8bb8\u591a\u673a\u5668\u4eba\u4efb\u52a1\uff08\u7279\u522b\u662f\u8fd0\u52a8\u4efb\u52a1\uff09\u9700\u8981\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u6267\u884c\u5468\u671f\u6027\u884c\u4e3a\uff0c\u56e0\u6b64\u53d1\u73b0\u591a\u6837\u5316\u7684\u5468\u671f\u6027\u6280\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bad\u7ec3\u7f16\u7801\u5668\u5c06\u72b6\u6001\u6620\u5c04\u5230\u5706\u5f62\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u6f5c\u5728\u8868\u793a\u4e2d\u81ea\u7136\u7f16\u7801\u5468\u671f\u6027\uff0c\u901a\u8fc7\u6355\u6349\u65f6\u95f4\u8ddd\u79bb\u6765\u5b66\u4e60\u5177\u6709\u4e0d\u540c\u5468\u671f\u7684\u6280\u80fd\u3002", "result": "PSD\u80fd\u591f\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u6709\u6548\u5b66\u4e60\u5177\u6709\u4e0d\u540c\u5468\u671f\u7684\u6280\u80fd\uff0c\u5373\u4f7f\u57fa\u4e8e\u50cf\u7d20\u89c2\u6d4b\u4e5f\u80fd\u5de5\u4f5c\u826f\u597d\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u8de8\u680f\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u73b0\u6709\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u7ed3\u5408\u53ef\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u884c\u4e3a\u3002", "conclusion": "PSD\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u6280\u80fd\u53d1\u73b0\u4e2d\u5468\u671f\u6027\u884c\u4e3a\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5468\u671f\u6027\u6280\u80fd\u53d1\u73b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.03410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03410", "abs": "https://arxiv.org/abs/2511.03410", "authors": ["Longpeng Qiu", "Ting Li", "Shuai Mao", "Nan Yang", "Xiaohui Yan"], "title": "Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG", "comment": "EMNLP2025 Industry Track", "summary": "Input errors in question-answering (QA) systems often lead to incorrect\nresponses. Large language models (LLMs) struggle with this task, frequently\nfailing to interpret user intent (misinterpretation) or unnecessarily altering\nthe original question's structure (over-correction). We propose QuestionRAG, a\nframework that tackles these problems. To address misinterpretation, it\nenriches the input with external knowledge (e.g., search results, related\nentities). To prevent over-correction, it uses reinforcement learning (RL) to\nalign the model's objective with precise correction, not just paraphrasing. Our\nresults demonstrate that knowledge augmentation is critical for understanding\nfaulty questions. Furthermore, RL-based alignment proves significantly more\neffective than traditional supervised fine-tuning (SFT), boosting the model's\nability to follow instructions and generalize. By integrating these two\nstrategies, QuestionRAG unlocks the full potential of LLMs for the question\ncorrection task.", "AI": {"tldr": "QuestionRAG\u662f\u4e00\u4e2a\u89e3\u51b3\u95ee\u7b54\u7cfb\u7edf\u4e2d\u8f93\u5165\u9519\u8bef\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u6539\u5584LLM\u5bf9\u9519\u8bef\u95ee\u9898\u7684\u7406\u89e3\u548c\u4fee\u6b63\u80fd\u529b\u3002", "motivation": "\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u8f93\u5165\u9519\u8bef\u5e38\u5bfc\u81f4\u9519\u8bef\u56de\u7b54\uff0cLLM\u5728\u5904\u7406\u8fd9\u7c7b\u4efb\u52a1\u65f6\u5bb9\u6613\u8bef\u89e3\u7528\u6237\u610f\u56fe\u6216\u8fc7\u5ea6\u4fee\u6b63\u95ee\u9898\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\uff08\u5982\u641c\u7d22\u7ed3\u679c\u3001\u76f8\u5173\u5b9e\u4f53\uff09\u589e\u5f3a\u8f93\u5165\u4ee5\u89e3\u51b3\u8bef\u89e3\u95ee\u9898\uff1b\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4f7f\u6a21\u578b\u76ee\u6807\u4e0e\u7cbe\u786e\u4fee\u6b63\u5bf9\u9f50\uff0c\u800c\u975e\u7b80\u5355\u6539\u5199\u3002", "result": "\u77e5\u8bc6\u589e\u5f3a\u5bf9\u7406\u89e3\u9519\u8bef\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff1b\u57fa\u4e8eRL\u7684\u5bf9\u9f50\u65b9\u6cd5\u6bd4\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u66f4\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u9075\u5faa\u6307\u4ee4\u548c\u6cdb\u5316\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u589e\u5f3a\u548c\u5f3a\u5316\u5b66\u4e60\u4e24\u79cd\u7b56\u7565\uff0cQuestionRAG\u5145\u5206\u53d1\u6325\u4e86LLM\u5728\u95ee\u9898\u4fee\u6b63\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.03243", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03243", "abs": "https://arxiv.org/abs/2511.03243", "authors": ["Miguel Costa", "Arthur Vandervoort", "Martin Drews", "Karyn Morrissey", "Francisco C. Pereira"], "title": "Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways", "comment": "Accepted for presentation at AI for Climate and Conservation Workshop\n  at EurIPS 2025", "summary": "Climate change will cause an increase in the frequency and severity of flood\nevents, prompting the need for cohesive adaptation policymaking. Designing\neffective adaptation policies, however, depends on managing the uncertainty of\nlong-term climate impacts. Meanwhile, such policies can feature important\nnormative choices that are not always made explicit. We propose that\nReinforcement Learning (RL) can be a useful tool to both identify adaptation\npathways under uncertain conditions while it also allows for the explicit\nmodelling (and consequent comparison) of different adaptation priorities (e.g.\neconomic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link\ntogether a rainfall and flood model, and compute the impacts of flooding in\nterms of quality of life (QoL), transportation, and infrastructure damage. Our\nresults show that models prioritising QoL over economic impacts results in more\nadaptation spending as well as a more even distribution of spending over the\nstudy area, highlighting the extent to which such normative assumptions can\nalter adaptation policy. Our framework is publicly available:\nhttps://github.com/MLSM-at-DTU/maat_qol_framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u8bc6\u522b\u6c14\u5019\u53d8\u5316\u4e0b\u7684\u9002\u5e94\u8def\u5f84\uff0c\u5e76\u660e\u786e\u5efa\u6a21\u4e0d\u540c\u9002\u5e94\u4f18\u5148\u7ea7\uff08\u7ecf\u6d4evs\u798f\u7949\uff09\u3002\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u6a21\u578b\u5206\u6790\u6d2a\u6c34\u5bf9\u751f\u6d3b\u8d28\u91cf\u3001\u4ea4\u901a\u548c\u57fa\u7840\u8bbe\u65bd\u7684\u5f71\u54cd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5c06\u589e\u52a0\u6d2a\u6c34\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u9002\u5e94\u653f\u7b56\u5236\u5b9a\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7ba1\u7406\u957f\u671f\u6c14\u5019\u5f71\u54cd\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u653f\u7b56\u4e2d\u7684\u89c4\u8303\u6027\u9009\u62e9\u5f80\u5f80\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u7efc\u5408\u8bc4\u4f30\u6a21\u578b\uff08IAM\uff09\uff0c\u5c06\u964d\u96e8\u548c\u6d2a\u6c34\u6a21\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u8ba1\u7b97\u6d2a\u6c34\u5bf9\u751f\u6d3b\u8d28\u91cf\u3001\u4ea4\u901a\u548c\u57fa\u7840\u8bbe\u65bd\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5148\u8003\u8651\u751f\u6d3b\u8d28\u91cf\u800c\u975e\u7ecf\u6d4e\u5f71\u54cd\u7684\u6a21\u578b\u4f1a\u5bfc\u81f4\u66f4\u591a\u7684\u9002\u5e94\u652f\u51fa\uff0c\u5e76\u4e14\u5728\u7814\u7a76\u533a\u57df\u5185\u652f\u51fa\u5206\u5e03\u66f4\u5747\u5300\uff0c\u7a81\u663e\u4e86\u89c4\u8303\u6027\u5047\u8bbe\u5bf9\u9002\u5e94\u653f\u7b56\u7684\u5f71\u54cd\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u8bc6\u522b\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u9002\u5e94\u8def\u5f84\u7684\u6709\u7528\u5de5\u5177\uff0c\u540c\u65f6\u5141\u8bb8\u660e\u786e\u5efa\u6a21\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u9002\u5e94\u4f18\u5148\u7ea7\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.03542", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03542", "abs": "https://arxiv.org/abs/2511.03542", "authors": ["Roberta Di Marino", "Giovanni Dioguardi", "Antonio Romano", "Giuseppe Riccio", "Mariano Barone", "Marco Postiglione", "Flora Amato", "Vincenzo Moscato"], "title": "SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across Medical Specialties", "comment": null, "summary": "Medical question answering systems face deployment challenges including\nhallucinations, bias, computational demands, privacy concerns, and the need for\nspecialized expertise across diverse domains. Here, we present SOLVE-Med, a\nmulti-agent architecture combining domain-specialized small language models for\ncomplex medical queries. The system employs a Router Agent for dynamic\nspecialist selection, ten specialized models (1B parameters each) fine-tuned on\nspecific medical domains, and an Orchestrator Agent that synthesizes responses.\nEvaluated on Italian medical forum data across ten specialties, SOLVE-Med\nachieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,\noutperforming standalone models up to 14B parameters while enabling local\ndeployment. Our code is publicly available on GitHub:\nhttps://github.com/PRAISELab-PicusLab/SOLVE-Med.", "AI": {"tldr": "SOLVE-Med\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7ed3\u5408\u9886\u57df\u4e13\u4e1a\u5316\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u590d\u6742\u533b\u7597\u67e5\u8be2\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u8def\u7531\u667a\u80fd\u4f53\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u6a21\u578b\uff0c\u4f7f\u752810\u4e2a\u4e13\u4e1a\u6a21\u578b\uff08\u6bcf\u4e2a10\u4ebf\u53c2\u6570\uff09\u5728\u7279\u5b9a\u533b\u7597\u9886\u57df\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u534f\u8c03\u667a\u80fd\u4f53\u5408\u6210\u54cd\u5e94\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u9762\u4e34\u7684\u5e7b\u89c9\u3001\u504f\u89c1\u3001\u8ba1\u7b97\u9700\u6c42\u3001\u9690\u79c1\u95ee\u9898\u4ee5\u53ca\u8de8\u591a\u4e2a\u4e13\u4e1a\u9886\u57df\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7b49\u90e8\u7f72\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u8def\u7531\u667a\u80fd\u4f53\u8d1f\u8d23\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\uff0c10\u4e2a\u4e13\u4e1a\u6a21\u578b\uff08\u6bcf\u4e2a1B\u53c2\u6570\uff09\u5728\u7279\u5b9a\u533b\u7597\u9886\u57df\u5fae\u8c03\uff0c\u534f\u8c03\u667a\u80fd\u4f53\u5408\u6210\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5728\u610f\u5927\u5229\u533b\u7597\u8bba\u575b\u6570\u636e\u768410\u4e2a\u4e13\u4e1a\u9886\u57df\u8bc4\u4f30\u4e2d\uff0cSOLVE-Med\u5b9e\u73b0\u4e86ROUGE-1 0.301\u548cBERTScore F1 0.697\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u4f18\u4e8e\u9ad8\u8fbe140\u4ebf\u53c2\u6570\u7684\u72ec\u7acb\u6a21\u578b\uff0c\u540c\u65f6\u652f\u6301\u672c\u5730\u90e8\u7f72\u3002", "conclusion": "SOLVE-Med\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u672c\u5730\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2511.03547", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.03547", "abs": "https://arxiv.org/abs/2511.03547", "authors": ["Brian DuSell", "Ryan Cotterell"], "title": "Bearing Syntactic Fruit with Stack-Augmented Neural Networks", "comment": "15 pages, 5 figures", "summary": "Any finite set of training data is consistent with an infinite number of\nhypothetical algorithms that could have generated it. Studies have shown that\nwhen human children learn language, they consistently favor hypotheses based on\nhierarchical syntactic rules without ever encountering disambiguating examples.\nA recent line of work has inquired as to whether common neural network\narchitectures share this bias, finding that they do so only under special\nconditions: when syntactically supervised, when pre-trained on massive corpora,\nor when trained long past convergence. In this paper, we demonstrate, for the\nfirst time, neural network architectures that are able to generalize in\nhuman-like fashion without any of the aforementioned requirements:\nstack-augmented neural networks. We test three base architectures (transformer,\nsimple RNN, LSTM) augmented with two styles of stack: the superposition stack\nof Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed\nby DuSell & Chiang (2023). We find that transformers with nondeterministic\nstacks generalize best out of these architectures on a classical question\nformation task. We also propose a modification to the stack RNN architecture\nthat improves hierarchical generalization. These results suggest that\nstack-augmented neural networks may be more accurate models of human language\nacquisition than standard architectures, serving as useful objects of\npsycholinguistic study. Our code is publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5806\u6808\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5728\u6ca1\u6709\u8bed\u6cd5\u76d1\u7763\u3001\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6216\u957f\u65f6\u95f4\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u50cf\u4eba\u7c7b\u4e00\u6837\u8fdb\u884c\u5c42\u6b21\u5316\u6cdb\u5316\uff0c\u7279\u522b\u662f\u5728\u95ee\u7b54\u5f62\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u5177\u6709\u4e0e\u4eba\u7c7b\u513f\u7ae5\u5b66\u4e60\u8bed\u8a00\u65f6\u76f8\u4f3c\u7684\u5c42\u6b21\u5316\u8bed\u6cd5\u89c4\u5219\u504f\u597d\uff0c\u800c\u65e0\u9700\u6b67\u4e49\u6d88\u9664\u793a\u4f8b\u3002", "method": "\u6d4b\u8bd5\u4e86\u4e09\u79cd\u57fa\u7840\u67b6\u6784\uff08transformer\u3001\u7b80\u5355RNN\u3001LSTM\uff09\u4e0e\u4e24\u79cd\u5806\u6808\u98ce\u683c\u7684\u7ec4\u5408\uff1aJoulin & Mikolov\u7684\u53e0\u52a0\u5806\u6808\u548cDuSell & Chiang\u7684\u975e\u786e\u5b9a\u6027\u5806\u6808\uff0c\u5e76\u5728\u7ecf\u5178\u95ee\u7b54\u5f62\u6210\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5e26\u6709\u975e\u786e\u5b9a\u6027\u5806\u6808\u7684transformer\u5728\u8fd9\u4e9b\u67b6\u6784\u4e2d\u6cdb\u5316\u6548\u679c\u6700\u597d\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6539\u8fdb\u5806\u6808RNN\u67b6\u6784\u4ee5\u589e\u5f3a\u5c42\u6b21\u5316\u6cdb\u5316\u7684\u65b9\u6cd5\u3002", "conclusion": "\u5806\u6808\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u53ef\u80fd\u662f\u6bd4\u6807\u51c6\u67b6\u6784\u66f4\u51c6\u786e\u7684\u4eba\u7c7b\u8bed\u8a00\u4e60\u5f97\u6a21\u578b\uff0c\u53ef\u4f5c\u4e3a\u5fc3\u7406\u8bed\u8a00\u5b66\u7814\u7a76\u7684\u6709\u6548\u5bf9\u8c61\u3002", "topic": "agent analysis"}}
{"id": "2511.03553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03553", "abs": "https://arxiv.org/abs/2511.03553", "authors": ["Sofie Helene Bruun", "Dan Saattrup Smart"], "title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark", "comment": "Submitted to LREC 2026", "summary": "Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes.", "AI": {"tldr": "\u521b\u5efa\u4e86MultiZebraLogic\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u79cd\u8bed\u8a00\u3001\u4e3b\u9898\u548c\u96be\u5ea6\u7684\u6591\u9a6c\u8c1c\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u8c1c\u9898\u5927\u5c0f\u548c\u5e72\u6270\u7ebf\u7d22\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u9700\u8981\u521b\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u6765\u6bd4\u8f83\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u4e2d\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u589e\u52a0\u96be\u5ea6\u7684\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "\u751f\u6210\u591a\u79cd\u8bed\u8a00\u3001\u4e3b\u9898\u3001\u5927\u5c0f\u548c\u7ebf\u7d22\u7c7b\u578b\u7684\u6591\u9a6c\u8c1c\u9898\uff0c\u5305\u62ec14\u79cd\u7ebf\u7d22\u7c7b\u578b\u548c8\u79cd\u5e72\u6270\u7ebf\u7d22\u7c7b\u578b\uff0c\u6d4b\u8bd5\u4e86GPT-4o mini\u548co3-mini\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b02x3\u548c4x5\u5927\u5c0f\u7684\u8c1c\u9898\u5206\u522b\u5bf9GPT-4o mini\u548co3-mini\u5177\u6709\u8db3\u591f\u6311\u6218\u6027\uff1b5\u4e2a\u5e72\u6270\u7ebf\u7d22\u4f7fo3-mini\u57284x5\u8c1c\u9898\u4e0a\u7684\u51c6\u786e\u7387\u4e0b\u964d15\u00b17%\uff1b\u8bed\u8a00\u548c\u4e3b\u9898\u53d8\u5316\u5bf9\u8868\u73b0\u65e0\u663e\u8457\u5f71\u54cd\uff1b\u7ebf\u7d22\u7c7b\u578b\u4e0e\u96be\u5ea6\u65e0\u76f8\u5173\u6027\u3002", "conclusion": "\u6210\u529f\u521b\u5efa\u4e86\u53ef\u6269\u5c55\u7684MultiZebraLogic\u6570\u636e\u96c6\u548c\u751f\u6210\u4ee3\u7801\uff0c\u4e3a\u8bc4\u4f30LLM\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u8c1c\u9898\u5927\u5c0f\u548c\u5e72\u6270\u7ebf\u7d22\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2511.03363", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03363", "abs": "https://arxiv.org/abs/2511.03363", "authors": ["Xiaocai Zhang", "Hur Lim", "Ke Wang", "Zhe Xiao", "Jing Wang", "Kelvin Lee", "Xiuju Fu", "Zheng Qin"], "title": "A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications", "comment": "Present in the Transportation Research Board (TRB) Annual Meeting\n  2026", "summary": "In this study, a modular, data-free pipeline for multi-label intention\nrecognition is proposed for agentic AI applications in transportation. Unlike\ntraditional intent recognition systems that depend on large, annotated corpora\nand often struggle with fine-grained, multi-label discrimination, our approach\neliminates the need for costly data collection while enhancing the accuracy of\nmulti-label intention understanding. Specifically, the overall pipeline, named\nDMTC, consists of three steps: 1) using prompt engineering to guide large\nlanguage models (LLMs) to generate diverse synthetic queries in different\ntransport scenarios; 2) encoding each textual query with a Sentence-T5 model to\nobtain compact semantic embeddings; 3) training a lightweight classifier using\na novel online focal-contrastive (OFC) loss that emphasizes hard samples and\nmaximizes inter-class separability. The applicability of the proposed pipeline\nis demonstrated in an agentic AI application in the maritime transportation\ncontext. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%\nand an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers\nand recent end-to-end SOTA LLM-based baselines. Further analysis reveals that\nSentence-T5 embeddings improve subset accuracy by at least 3.29% over\nalternative encoders, and integrating the OFC loss yields an additional 0.98%\ngain compared to standard contrastive objectives. In conclusion, our system\nseamlessly routes user queries to task-specific modules (e.g., ETA information,\ntraffic risk evaluation, and other typical scenarios in the transportation\ndomain), laying the groundwork for fully autonomous, intention-aware agents\nwithout costly manual labelling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u7684\u6a21\u5757\u5316\u591a\u6807\u7b7e\u610f\u56fe\u8bc6\u522b\u7ba1\u9053DMTC\uff0c\u901a\u8fc7LLM\u751f\u6210\u5408\u6210\u67e5\u8be2\u3001Sentence-T5\u7f16\u7801\u548c\u5728\u7ebf\u7126\u70b9\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\uff0c\u5728\u4ea4\u901a\u9886\u57df\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u5b9e\u73b0\u9ad8\u6548\u610f\u56fe\u7406\u89e3\u3002", "motivation": "\u4f20\u7edf\u610f\u56fe\u8bc6\u522b\u7cfb\u7edf\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u591a\u6807\u7b7e\u533a\u5206\uff0c\u9700\u8981\u6d88\u9664\u6602\u8d35\u7684\u6570\u636e\u6536\u96c6\u9700\u6c42\u5e76\u63d0\u5347\u591a\u6807\u7b7e\u610f\u56fe\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "method": "\u4e09\u6b65\u9aa4\u7ba1\u9053\uff1a1) \u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u5f15\u5bfcLLM\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u67e5\u8be2\uff1b2) \u7528Sentence-T5\u6a21\u578b\u7f16\u7801\u6587\u672c\u67e5\u8be2\u83b7\u5f97\u8bed\u4e49\u5d4c\u5165\uff1b3) \u4f7f\u7528\u65b0\u9896\u7684\u5728\u7ebf\u7126\u70b9\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u3002", "result": "\u5728\u6d77\u4e0a\u4ea4\u901a\u5e94\u7528\u4e2d\uff0cDMTC\u5b9e\u73b05.35%\u7684\u6c49\u660e\u635f\u5931\u548c95.92%\u7684AUC\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u548cLLM\u57fa\u7ebf\u3002Sentence-T5\u5d4c\u5165\u6bd4\u66ff\u4ee3\u7f16\u7801\u5668\u63d0\u5347\u81f3\u5c113.29%\u7684\u5b50\u96c6\u51c6\u786e\u7387\uff0cOFC\u635f\u5931\u5e26\u6765\u989d\u59160.98%\u589e\u76ca\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u65e0\u7f1d\u5730\u5c06\u7528\u6237\u67e5\u8be2\u8def\u7531\u5230\u7279\u5b9a\u4efb\u52a1\u6a21\u5757\uff0c\u4e3a\u65e0\u9700\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u7684\u5b8c\u5168\u81ea\u4e3b\u610f\u56fe\u611f\u77e5\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2511.03473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03473", "abs": "https://arxiv.org/abs/2511.03473", "authors": ["Alexandru Cioba", "Aya Kayal", "Laura Toni", "Sattar Vakili", "Alberto Bernacchia"], "title": "Reinforcement Learning Using known Invariances", "comment": null, "summary": "In many real-world reinforcement learning (RL) problems, the environment\nexhibits inherent symmetries that can be exploited to improve learning\nefficiency. This paper develops a theoretical and algorithmic framework for\nincorporating known group symmetries into kernel-based RL. We propose a\nsymmetry-aware variant of optimistic least-squares value iteration (LSVI),\nwhich leverages invariant kernels to encode invariance in both rewards and\ntransition dynamics. Our analysis establishes new bounds on the maximum\ninformation gain and covering numbers for invariant RKHSs, explicitly\nquantifying the sample efficiency gains from symmetry. Empirical results on a\ncustomized Frozen Lake environment and a 2D placement design problem confirm\nthe theoretical improvements, demonstrating that symmetry-aware RL achieves\nsignificantly better performance than their standard kernel counterparts. These\nfindings highlight the value of structural priors in designing more\nsample-efficient reinforcement learning algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5df2\u77e5\u7fa4\u5bf9\u79f0\u6027\u878d\u5165\u57fa\u4e8e\u6838\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u8bba\u7b97\u6cd5\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u5bf9\u79f0\u611f\u77e5\u7684\u4e50\u89c2\u6700\u5c0f\u4e8c\u4e58\u503c\u8fed\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u53d8\u6838\u7f16\u7801\u5956\u52b1\u548c\u8f6c\u79fb\u52a8\u6001\u7684\u4e0d\u53d8\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u901a\u5e38\u5b58\u5728\u56fa\u6709\u7684\u5bf9\u79f0\u6027\uff0c\u5229\u7528\u8fd9\u4e9b\u5bf9\u79f0\u6027\u53ef\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6316\u6398\u5bf9\u79f0\u6027\u5728\u57fa\u4e8e\u6838\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u5bf9\u79f0\u611f\u77e5\u7684\u4e50\u89c2\u6700\u5c0f\u4e8c\u4e58\u503c\u8fed\u4ee3(LSVI)\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e0d\u53d8\u6838\u6765\u7f16\u7801\u5956\u52b1\u548c\u8f6c\u79fb\u52a8\u6001\u7684\u4e0d\u53d8\u6027\uff0c\u5efa\u7acb\u4e86\u4e0d\u53d8RKHS\u4e2d\u6700\u5927\u4fe1\u606f\u589e\u76ca\u548c\u8986\u76d6\u6570\u7684\u65b0\u754c\u9650\u3002", "result": "\u5728\u5b9a\u5236\u7684Frozen Lake\u73af\u5883\u548c2D\u5e03\u5c40\u8bbe\u8ba1\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u79f0\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6bd4\u6807\u51c6\u6838\u65b9\u6cd5\u83b7\u5f97\u4e86\u663e\u8457\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5148\u9a8c\u5728\u8bbe\u8ba1\u66f4\u6837\u672c\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5bf9\u79f0\u6027\u5229\u7528\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.03475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03475", "abs": "https://arxiv.org/abs/2511.03475", "authors": ["Yinsicheng Jiang", "Yeqi Huang", "Liang Cheng", "Cheng Deng", "Xuan Sun", "Luo Mai"], "title": "RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith retrieved context but often suffers from downgraded prefill performance as\nmodern applications demand longer and more complex inputs. Existing caching\ntechniques either preserve accuracy with low cache reuse or improve reuse at\nthe cost of degraded reasoning quality. We present RAGBoost, an efficient RAG\nsystem that achieves high cache reuse without sacrificing accuracy through\naccuracy-preserving context reuse. RAGBoost detects overlapping retrieved items\nacross concurrent sessions and multi-turn interactions, using efficient context\nindexing, ordering, and de-duplication to maximize reuse, while lightweight\ncontextual hints maintain reasoning fidelity. It integrates seamlessly with\nexisting LLM inference engines and improves their prefill performance by 1.5-3X\nover state-of-the-art methods, while preserving or even enhancing reasoning\naccuracy across diverse RAG and agentic AI workloads. Our code is released at:\nhttps://github.com/Edinburgh-AgenticAI/RAGBoost.", "AI": {"tldr": "RAGBoost\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u51c6\u786e\u4fdd\u6301\u7684\u4e0a\u4e0b\u6587\u91cd\u7528\u5b9e\u73b0\u9ad8\u7f13\u5b58\u5229\u7528\u7387\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u9884\u586b\u5145\u6027\u80fd1.5-3\u500d\u3002", "motivation": "\u73b0\u6709\u7684RAG\u7f13\u5b58\u6280\u672f\u8981\u4e48\u4fdd\u6301\u51c6\u786e\u6027\u4f46\u7f13\u5b58\u91cd\u7528\u7387\u4f4e\uff0c\u8981\u4e48\u63d0\u9ad8\u91cd\u7528\u7387\u4f46\u727a\u7272\u63a8\u7406\u8d28\u91cf\uff0c\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u9ad8\u91cd\u7528\u7387\u548c\u51c6\u786e\u6027\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u68c0\u6d4b\u5e76\u53d1\u4f1a\u8bdd\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u91cd\u53e0\u68c0\u7d22\u9879\uff0c\u4f7f\u7528\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u7d22\u5f15\u3001\u6392\u5e8f\u548c\u53bb\u91cd\u6765\u6700\u5927\u5316\u91cd\u7528\uff0c\u540c\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u63d0\u793a\u4fdd\u6301\u63a8\u7406\u4fdd\u771f\u5ea6\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u9884\u586b\u5145\u6027\u80fd\u63d0\u53471.5-3\u500d\uff0c\u540c\u65f6\u5728\u591a\u6837\u5316RAG\u548c\u4ee3\u7406AI\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "RAGBoost\u5b9e\u73b0\u4e86\u9ad8\u7f13\u5b58\u91cd\u7528\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709LLM\u63a8\u7406\u5f15\u64ce\u4e2d\uff0c\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2511.03697", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03697", "abs": "https://arxiv.org/abs/2511.03697", "authors": ["Mohsen Ahmadzadeh", "Kaichang Chen", "Georges Gielen"], "title": "AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing", "comment": "This article was accepted by 2025 International Conference on\n  Computer-Aided Design (ICCAD 2025) and was presented in Munich, October 2025", "summary": "Analog/mixed-signal circuits are key for interfacing electronics with the\nphysical world. Their design, however, remains a largely handcrafted process,\nresulting in long and error-prone design cycles. While the recent rise of\nAI-based reinforcement learning and generative AI has created new techniques to\nautomate this task, the need for many time-consuming simulations is a critical\nbottleneck hindering the overall efficiency. Furthermore, the lack of\nexplainability of the resulting design solutions hampers widespread adoption of\nthe tools. To address these issues, a novel agentic AI framework for\nsample-efficient and explainable analog circuit sizing is presented. It employs\na multi-agent workflow where specialized Large Language Model (LLM)-based\nagents collaborate to interpret the circuit topology, to understand the design\ngoals, and to iteratively refine the circuit's design parameters towards the\ntarget goals with human-interpretable reasoning. The adaptive simulation\nstrategy creates an intelligent control that yields a high sample efficiency.\nThe AnaFlow framework is demonstrated for two circuits of varying complexity\nand is able to complete the sizing task fully automatically, differently from\npure Bayesian optimization and reinforcement learning approaches. The system\nlearns from its optimization history to avoid past mistakes and to accelerate\nconvergence. The inherent explainability makes this a powerful tool for analog\ndesign space exploration and a new paradigm in analog EDA, where AI agents\nserve as transparent design assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u6a21\u62df\u7535\u8def\u5c3a\u5bf8\u8bbe\u8ba1\u6846\u67b6AnaFlow\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u7535\u8def\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4eff\u771f\u6210\u672c\u9ad8\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u6a21\u62df/\u6df7\u5408\u4fe1\u53f7\u7535\u8def\u8bbe\u8ba1\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u624b\u5de5\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u5468\u671f\u957f\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u73b0\u6709\u7684AI\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u4eff\u771f\u6210\u672c\u9ad8\u548c\u7ed3\u679c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u74f6\u9888\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u534f\u4f5c\u89e3\u91ca\u7535\u8def\u62d3\u6251\u3001\u7406\u89e3\u8bbe\u8ba1\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fed\u4ee3\u4f18\u5316\u7535\u8def\u8bbe\u8ba1\u53c2\u6570\u3002\u81ea\u9002\u5e94\u4eff\u771f\u7b56\u7565\u5b9e\u73b0\u9ad8\u6837\u672c\u6548\u7387\u3002", "result": "AnaFlow\u6846\u67b6\u5728\u4e24\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u7535\u8def\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u80fd\u591f\u5b8c\u5168\u81ea\u52a8\u5b8c\u6210\u5c3a\u5bf8\u8bbe\u8ba1\u4efb\u52a1\uff0c\u4f18\u4e8e\u7eaf\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u7cfb\u7edf\u80fd\u4ece\u4f18\u5316\u5386\u53f2\u4e2d\u5b66\u4e60\uff0c\u907f\u514d\u91cd\u590d\u9519\u8bef\u5e76\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6a21\u62df\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u4ee3\u8868\u4e86\u6a21\u62dfEDA\u9886\u57df\u7684\u65b0\u8303\u5f0f\uff0c\u5176\u4e2dAI\u667a\u80fd\u4f53\u4f5c\u4e3a\u900f\u660e\u7684\u8bbe\u8ba1\u52a9\u624b\u3002", "topic": "agent analysis"}}
{"id": "2511.03616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03616", "abs": "https://arxiv.org/abs/2511.03616", "authors": ["Iason Chrysomallis", "Georgios Chalkiadakis"], "title": "Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning", "comment": null, "summary": "Imitation learning traditionally requires complete state-action\ndemonstrations from optimal or near-optimal experts. These requirements\nseverely limit practical applicability, as many real-world scenarios provide\nonly state observations without corresponding actions and expert performance is\noften suboptimal. In this paper we introduce a deep implicit imitation\nreinforcement learning framework that addresses both limitations by combining\ndeep reinforcement learning with implicit imitation learning from\nobservation-only datasets. Our main algorithm, Deep Implicit Imitation\nQ-Network (DIIQN), employs an action inference mechanism that reconstructs\nexpert actions through online exploration and integrates a dynamic confidence\nmechanism that adaptively balances expert-guided and self-directed learning.\nThis enables the agent to leverage expert guidance for accelerated training\nwhile maintaining capacity to surpass suboptimal expert performance. We further\nextend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to\ntackle scenarios where expert and agent possess different action sets, a\nchallenge previously unaddressed in the implicit imitation learning literature.\nHA-DIIQN introduces an infeasibility detection mechanism and a bridging\nprocedure identifying alternative pathways connecting agent capabilities to\nexpert guidance when direct action replication is impossible. Our experimental\nresults demonstrate that DIIQN achieves up to 130% higher episodic returns\ncompared to standard DQN, while consistently outperforming existing implicit\nimitation methods that cannot exceed expert performance. In heterogeneous\naction settings, HA-DIIQN learns up to 64% faster than baselines, leveraging\nexpert datasets unusable by conventional approaches. Extensive parameter\nsensitivity analysis reveals the framework's robustness across varying dataset\nsizes and hyperparameter configurations.", "AI": {"tldr": "\u63d0\u51fa\u6df1\u5ea6\u9690\u5f0f\u6a21\u4eff\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u4ec5\u89c2\u5bdf\u6570\u636e\u96c6\u7684\u9690\u5f0f\u6a21\u4eff\u5b66\u4e60\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5b8c\u6574\u72b6\u6001-\u52a8\u4f5c\u6f14\u793a\u548c\u6700\u4f18\u4e13\u5bb6\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5b8c\u6574\u7684\u72b6\u6001-\u52a8\u4f5c\u6f14\u793a\u548c\u6700\u4f18\u4e13\u5bb6\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u4e3a\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u53ea\u63d0\u4f9b\u72b6\u6001\u89c2\u5bdf\u800c\u6ca1\u6709\u5bf9\u5e94\u52a8\u4f5c\uff0c\u4e14\u4e13\u5bb6\u8868\u73b0\u901a\u5e38\u4e0d\u662f\u6700\u4f18\u7684\u3002", "method": "\u5f00\u53d1\u4e86\u6df1\u5ea6\u9690\u5f0f\u6a21\u4effQ\u7f51\u7edc\uff08DIIQN\uff09\uff0c\u91c7\u7528\u52a8\u4f5c\u63a8\u7406\u673a\u5236\u901a\u8fc7\u5728\u7ebf\u63a2\u7d22\u91cd\u5efa\u4e13\u5bb6\u52a8\u4f5c\uff0c\u5e76\u96c6\u6210\u52a8\u6001\u7f6e\u4fe1\u673a\u5236\u81ea\u9002\u5e94\u5e73\u8861\u4e13\u5bb6\u5f15\u5bfc\u548c\u81ea\u4e3b\u5b66\u4e60\u3002\u8fd8\u6269\u5c55\u4e86\u5f02\u6784\u52a8\u4f5cDIIQN\uff08HA-DIIQN\uff09\u7b97\u6cd5\u5904\u7406\u4e13\u5bb6\u548c\u667a\u80fd\u4f53\u5177\u6709\u4e0d\u540c\u52a8\u4f5c\u96c6\u7684\u60c5\u51b5\u3002", "result": "DIIQN\u76f8\u6bd4\u6807\u51c6DQN\u5b9e\u73b0\u4e86\u9ad8\u8fbe130%\u7684\u66f4\u9ad8\u56de\u5408\u56de\u62a5\uff0c\u540c\u65f6\u6301\u7eed\u4f18\u4e8e\u65e0\u6cd5\u8d85\u8d8a\u4e13\u5bb6\u6027\u80fd\u7684\u73b0\u6709\u9690\u5f0f\u6a21\u4eff\u65b9\u6cd5\u3002\u5728\u5f02\u6784\u52a8\u4f5c\u8bbe\u7f6e\u4e2d\uff0cHA-DIIQN\u5b66\u4e60\u901f\u5ea6\u6bd4\u57fa\u7ebf\u5feb64%\uff0c\u80fd\u591f\u5229\u7528\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u4f7f\u7528\u7684\u4e13\u5bb6\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5229\u7528\u4e13\u5bb6\u6307\u5bfc\u52a0\u901f\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8d8a\u6b21\u4f18\u4e13\u5bb6\u6027\u80fd\u7684\u80fd\u529b\uff0c\u5728\u4ec5\u89c2\u5bdf\u6570\u636e\u96c6\u548c\u5f02\u6784\u52a8\u4f5c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.03618", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.03618", "abs": "https://arxiv.org/abs/2511.03618", "authors": ["Shangtong Zhang"], "title": "Towards Formalizing Reinforcement Learning Theory", "comment": null, "summary": "In this paper, we formalize the almost sure convergence of $Q$-learning and\nlinear temporal difference (TD) learning with Markovian samples using the Lean\n4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are\namong the earliest and most influential reinforcement learning (RL) algorithms.\nThe investigation of their convergence properties is not only a major research\ntopic during the early development of the RL field but also receives increasing\nattention nowadays. This paper formally verifies their almost sure convergence\nin a unified framework based on the Robbins-Siegmund theorem. The framework\ndeveloped in this work can be easily extended to convergence rates and other\nmodes of convergence. This work thus makes an important step towards fully\nformalizing convergent RL results. The code is available at\nhttps://github.com/ShangtongZhang/rl-theory-in-lean.", "AI": {"tldr": "\u4f7f\u7528Lean 4\u5b9a\u7406\u8bc1\u660e\u5668\u57fa\u4e8eMathlib\u5e93\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e86Q\u5b66\u4e60\u548c\u7ebf\u6027TD\u5b66\u4e60\u5728\u9a6c\u5c14\u53ef\u592b\u6837\u672c\u4e0b\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u6027", "motivation": "Q\u5b66\u4e60\u548c\u7ebf\u6027TD\u5b66\u4e60\u662f\u6700\u65e9\u4e14\u6700\u5177\u5f71\u54cd\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7814\u7a76\u5176\u6536\u655b\u6027\u8d28\u4e0d\u4ec5\u662fRL\u9886\u57df\u65e9\u671f\u53d1\u5c55\u7684\u4e3b\u8981\u7814\u7a76\u8bfe\u9898\uff0c\u5982\u4eca\u4e5f\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8", "method": "\u57fa\u4e8eRobbins-Siegmund\u5b9a\u7406\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528Lean 4\u5b9a\u7406\u8bc1\u660e\u5668\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1", "result": "\u6210\u529f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e86Q\u5b66\u4e60\u548c\u7ebf\u6027TD\u5b66\u4e60\u7684\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b8c\u5168\u5f62\u5f0f\u5316\u6536\u655bRL\u7ed3\u679c\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u6240\u5f00\u53d1\u7684\u6846\u67b6\u53ef\u4ee5\u8f7b\u677e\u6269\u5c55\u5230\u6536\u655b\u901f\u7387\u548c\u5176\u4ed6\u6536\u655b\u6a21\u5f0f", "topic": "agent analysis"}}
{"id": "2511.03695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.03695", "abs": "https://arxiv.org/abs/2511.03695", "authors": ["Lipeng Zu", "Hansong Zhou", "Xiaonan Zhang"], "title": "Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL", "comment": null, "summary": "Offline reinforcement learning (RL) enables training from fixed data without\nonline interaction, but policies learned offline often struggle when deployed\nin dynamic environments due to distributional shift and unreliable value\nestimates on unseen state-action pairs. We introduce Behavior-Adaptive\nQ-Learning (BAQ), a framework designed to enable a smooth and reliable\ntransition from offline to online RL. The key idea is to leverage an implicit\nbehavioral model derived from offline data to provide a behavior-consistency\nsignal during online fine-tuning. BAQ incorporates a dual-objective loss that\n(i) aligns the online policy toward the offline behavior when uncertainty is\nhigh, and (ii) gradually relaxes this constraint as more confident online\nexperience is accumulated. This adaptive mechanism reduces error propagation\nfrom out-of-distribution estimates, stabilizes early online updates, and\naccelerates adaptation to new scenarios. Across standard benchmarks, BAQ\nconsistently outperforms prior offline-to-online RL approaches, achieving\nfaster recovery, improved robustness, and higher overall performance. Our\nresults demonstrate that implicit behavior adaptation is a principled and\npractical solution for reliable real-world policy deployment.", "AI": {"tldr": "BAQ\u662f\u4e00\u79cd\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u79bb\u7ebf\u6570\u636e\u4e2d\u7684\u9690\u5f0f\u884c\u4e3a\u6a21\u578b\uff0c\u5728\u5728\u7ebf\u5fae\u8c03\u671f\u95f4\u63d0\u4f9b\u884c\u4e3a\u4e00\u81f4\u6027\u4fe1\u53f7\uff0c\u5b9e\u73b0\u5e73\u7a33\u53ef\u9760\u7684\u8fc7\u6e21\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u52a8\u6001\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u7531\u4e8e\u5206\u5e03\u504f\u79fb\u548c\u672a\u89c1\u72b6\u6001-\u52a8\u4f5c\u5bf9\u4e0a\u7684\u4e0d\u53ef\u9760\u503c\u4f30\u8ba1\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u8fc7\u6e21\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u884c\u4e3a\u81ea\u9002\u5e94Q\u5b66\u4e60(BAQ)\uff0c\u91c7\u7528\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff1a(i)\u5728\u4e0d\u786e\u5b9a\u6027\u9ad8\u65f6\u4f7f\u5728\u7ebf\u7b56\u7565\u4e0e\u79bb\u7ebf\u884c\u4e3a\u5bf9\u9f50\uff0c(ii)\u968f\u7740\u5728\u7ebf\u7ecf\u9a8c\u79ef\u7d2f\u9010\u6e10\u653e\u677e\u7ea6\u675f\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBAQ\u4e00\u81f4\u4f18\u4e8e\u5148\u524d\u7684\u79bb\u7ebf\u5230\u5728\u7ebfRL\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6062\u590d\u3001\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u548c\u66f4\u9ad8\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u9690\u5f0f\u884c\u4e3a\u9002\u5e94\u662f\u53ef\u9760\u73b0\u5b9e\u4e16\u754c\u7b56\u7565\u90e8\u7f72\u7684\u539f\u5219\u6027\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.c8492b25", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhamel.dev%2Fnotes%2Fcoding-agents%2Famp.html%3Futm_source=tldrai/1/0100019a4f360b3c-22ea7ec0-872a-4f6b-af3d-7222997bc7c9-000000/859K8eRpYSS6yNdUHhHg0iSZjm-_1EXV9XiQLxS8vF0=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhamel.dev%2Fnotes%2Fcoding-agents%2Famp.html%3Futm_source=tldrai/1/0100019a4f360b3c-22ea7ec0-872a-4f6b-af3d-7222997bc7c9-000000/859K8eRpYSS6yNdUHhHg0iSZjm-_1EXV9XiQLxS8vF0=429", "authors": ["TLDR Newsletter"], "title": "Notes from reading the Amp manual", "comment": "Source: TLDR Newsletter, Date: 2025-11-04, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhamel.dev%2Fnotes%2Fcoding-agents%2Famp.html%3Futm_source=tldrai/1/0100019a4f360b3c-22ea7ec0-872a-4f6b-af3d-7222997bc7c9-000000/859K8eRpYSS6yNdUHhHg0iSZjm-_1EXV9XiQLxS8vF0=429", "summary": "Notes from reading the Amp manual (5 minute read) Amp is a coding agent with support for MCP, the ability to set project, folder, and global context, a permissions system, headless mode, and more. It stores all threads on the server so users can resume a thread anytime. Amp picks the right model for each task, but it can be nudged to use more expensive models. It is pay-per-token and not cheap, but it is worth it because it saves time.", "source": "tldr", "AI": {"tldr": "Amp\u662f\u4e00\u4e2a\u652f\u6301MCP\u7684\u7f16\u7a0b\u4ee3\u7406\uff0c\u5177\u6709\u9879\u76ee/\u6587\u4ef6\u5939/\u5168\u5c40\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u3001\u6743\u9650\u7cfb\u7edf\u3001\u65e0\u5934\u6a21\u5f0f\u7b49\u529f\u80fd\uff0c\u91c7\u7528\u6309token\u4ed8\u8d39\u6a21\u5f0f\u3002", "motivation": "\u63d0\u4f9b\u66f4\u667a\u80fd\u7684\u7f16\u7a0b\u4ee3\u7406\u5de5\u5177\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u591a\u6a21\u578b\u9009\u62e9\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "method": "\u4f7f\u7528MCP\u652f\u6301\uff0c\u5b9e\u73b0\u9879\u76ee/\u6587\u4ef6\u5939/\u5168\u5c40\u4e09\u7ea7\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5185\u7f6e\u6743\u9650\u7cfb\u7edf\uff0c\u652f\u6301\u65e0\u5934\u6a21\u5f0f\uff0c\u81ea\u52a8\u9009\u62e9\u5408\u9002\u6a21\u578b\u6267\u884c\u4efb\u52a1\u3002", "result": "\u80fd\u591f\u4fdd\u5b58\u6240\u6709\u7ebf\u7a0b\u5728\u670d\u52a1\u5668\u4e0a\uff0c\u7528\u6237\u53ef\u4ee5\u968f\u65f6\u6062\u590d\u5de5\u4f5c\uff0c\u867d\u7136\u6309token\u4ed8\u8d39\u6210\u672c\u8f83\u9ad8\u4f46\u80fd\u8282\u7701\u65f6\u95f4\u3002", "conclusion": "Amp\u662f\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u7684\u7f16\u7a0b\u4ee3\u7406\u5de5\u5177\uff0c\u867d\u7136\u4ef7\u683c\u4e0d\u83f2\u4f46\u7269\u6709\u6240\u503c\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2511.5c64ea41", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdeepnote%2Fdeepnote%3Futm_source=tldrnewsletter/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/nf6RnTN5Thot9YRXBI9nArTokYFN6Bc2Z5bQLgy1pO0=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdeepnote%2Fdeepnote%3Futm_source=tldrnewsletter/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/nf6RnTN5Thot9YRXBI9nArTokYFN6Bc2Z5bQLgy1pO0=430", "authors": ["TLDR Newsletter"], "title": "Deepnote", "comment": "Source: TLDR Newsletter, Date: 2025-11-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdeepnote%2Fdeepnote%3Futm_source=tldrnewsletter/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/nf6RnTN5Thot9YRXBI9nArTokYFN6Bc2Z5bQLgy1pO0=430", "summary": "Deepnote (GitHub Repo) Deepnote is a drop-in replacement for Jupyter that can scale to Deepnote cloud for real-time collaboration, Deepnote agent, and deployable data apps.", "source": "tldr", "AI": {"tldr": "Deepnote\u662fJupyter\u7684\u66ff\u4ee3\u54c1\uff0c\u63d0\u4f9b\u4e91\u7aef\u5b9e\u65f6\u534f\u4f5c\u3001AI\u4ee3\u7406\u548c\u53ef\u90e8\u7f72\u7684\u6570\u636e\u5e94\u7528", "motivation": "\u89e3\u51b3Jupyter\u5728\u534f\u4f5c\u3001\u6269\u5c55\u6027\u548c\u90e8\u7f72\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u73af\u5883", "method": "\u5f00\u53d1\u57fa\u4e8e\u4e91\u7684Jupyter\u66ff\u4ee3\u5e73\u53f0\uff0c\u96c6\u6210\u5b9e\u65f6\u534f\u4f5c\u529f\u80fd\u3001AI\u4ee3\u7406\u548c\u90e8\u7f72\u80fd\u529b", "result": "\u521b\u5efa\u4e86\u80fd\u591f\u6269\u5c55\u81f3\u4e91\u7aef\u3001\u652f\u6301\u56e2\u961f\u534f\u4f5c\u5e76\u5177\u5907AI\u8f85\u52a9\u529f\u80fd\u7684\u5b8c\u6574\u6570\u636e\u79d1\u5b66\u5e73\u53f0", "conclusion": "Deepnote\u6210\u529f\u6269\u5c55\u4e86Jupyter\u7684\u529f\u80fd\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u534f\u4f5c\u6027\u66f4\u5f3a\u7684\u5f00\u53d1\u73af\u5883", "topic": "swe application"}}
{"id": "tldr.2511.4dfaf4a5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/AP1xBsxEc-MHnDFgJ-sYl1Ym9fAYf5oTbaDvT_hxzGs=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/AP1xBsxEc-MHnDFgJ-sYl1Ym9fAYf5oTbaDvT_hxzGs=430", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-11-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a53c1e16f-a263bbcb-11dc-49df-b6fb-cea544e71c0c-000000/AP1xBsxEc-MHnDFgJ-sYl1Ym9fAYf5oTbaDvT_hxzGs=430", "summary": "Deepnote (GitHub Repo) Deepnote is a drop-in replacement for Jupyter that can scale to Deepnote cloud for real-time collaboration, Deepnote agent, and deployable data apps.", "source": "tldr", "AI": {"tldr": "Deepnote\u662fJupyter\u7684\u66ff\u4ee3\u54c1\uff0c\u63d0\u4f9b\u4e91\u7aef\u5b9e\u65f6\u534f\u4f5c\u3001Deepnote\u4ee3\u7406\u548c\u53ef\u90e8\u7f72\u6570\u636e\u5e94\u7528", "motivation": "\u4e3a\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u534f\u4f5c\u548c\u90e8\u7f72\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edfJupyter\u7b14\u8bb0\u672c\u7684\u9650\u5236", "method": "\u5f00\u53d1\u4e91\u7aef\u534f\u4f5c\u5e73\u53f0\uff0c\u96c6\u6210AI\u4ee3\u7406\u529f\u80fd\uff0c\u652f\u6301\u6570\u636e\u5e94\u7528\u90e8\u7f72", "result": "\u521b\u5efa\u4e86\u80fd\u591f\u6269\u5c55\u81f3\u4e91\u7aef\u3001\u652f\u6301\u5b9e\u65f6\u534f\u4f5c\u548cAI\u8f85\u52a9\u7684\u6570\u636e\u79d1\u5b66\u73af\u5883", "conclusion": "Deepnote\u6210\u529f\u6269\u5c55\u4e86Jupyter\u7684\u529f\u80fd\uff0c\u4e3a\u73b0\u4ee3\u6570\u636e\u79d1\u5b66\u56e2\u961f\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "wechat.2511.e4a51dcb", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzMzk5MjAzNA==&mid=2247504065&idx=4&sn=f4e010d1153026188162ed052d1e0418&chksm=fba234d5efdeb50a8812725be92fb70f5f51f925a3195871286d277cb44eb266f501f8e420a1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzMzk5MjAzNA==&mid=2247504065&idx=4&sn=f4e010d1153026188162ed052d1e0418&chksm=fba234d5efdeb50a8812725be92fb70f5f51f925a3195871286d277cb44eb266f501f8e420a1#rd", "authors": ["\u8f68\u9053\u77e5\u9053"], "title": "\u5b66\u70b9\u4ea4\u901a | \u79d1\u7814\u79d1\u666e\uff1a\u4ec0\u4e48\u662f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1f\u4ee5\u53ca<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5728\u4ea4\u901a\u79d1\u7814\u5e94\u7528\u4e3e\u4f8b", "comment": "Source: WeChat, Published: 2025-11-06 13:28:47", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u8bd5\u9519\u6765\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u4ea4\u901a\u9886\u57df\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u8303\u5f0f\u3002\u672c\u6587\u9996\u5148\u9610\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u539f\u7406\u53ca\u5176\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u8fdb\u800c\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5176\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u4f5c\u4e3a\u4e00\u79cd\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u8bd5\u9519\u6765\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u7b56\u7565\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u89e3\u51b3\u4ea4\u901a\u9886\u57df\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u8303\u5f0f\u3002\u672c\u6587\u9996\u5148\u9610\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u672c\u539f\u7406\u53ca\u5176\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u8fdb\u800c\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5176\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.7c7955a7", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0NjczNTg2NQ==&mid=2247520375&idx=7&sn=16d6005aaa62581d581966aaa01e355e&chksm=fad676180716625802f1e5a0b1f9f36a590a1249ea2ec572d2a64d8320b4042fa407e1bbf5db#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0NjczNTg2NQ==&mid=2247520375&idx=7&sn=16d6005aaa62581d581966aaa01e355e&chksm=fad676180716625802f1e5a0b1f9f36a590a1249ea2ec572d2a64d8320b4042fa407e1bbf5db#rd", "authors": ["\u6df1\u5ea6\u4e4b\u773c"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6210\u4eca\u5e74\u6700\u5927\u8d62\u5bb6\uff01\u767b\u4e0aNature\u9876\u520a\uff01", "comment": "Source: WeChat, Published: 2025-11-06 12:05:54", "summary": "\u5206\u4eab\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u65b9\u5411\u503c\u5f97\u5b66\u4e60\u7684\u6210\u679c\u2014\u2014\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60MaxDiff RL\uff0c\u611f\u5174\u8da3\u7684\u540c\u5b66\u53ef\u9605\u8bfb\u539f\u6587\u3002\u8fd9\u662f\u4e2a\u65b0\u578bRL\u65b9\u6cd5\uff0c\u76ee\u524d\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u5b9e\u73b0\u4e86SOTA\uff0c\u5df2\u6210\u529f\u767b\u4e0aNature Machine Intelligence\u3002", "AI": {"tldr": "\u5206\u4eab\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u65b9\u5411\u503c\u5f97\u5b66\u4e60\u7684\u6210\u679c\u2014\u2014\u6700\u5927\u6269\u6563\u5f3a\u5316\u5b66\u4e60MaxDiff RL\uff0c\u611f\u5174\u8da3\u7684\u540c\u5b66\u53ef\u9605\u8bfb\u539f\u6587\u3002\u8fd9\u662f\u4e2a\u65b0\u578bRL\u65b9\u6cd5\uff0c\u76ee\u524d\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u5b9e\u73b0\u4e86SOTA\uff0c\u5df2\u6210\u529f\u767b\u4e0aNature Machine Intelligence\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.6aaa4722", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTQ2NTY2Ng==&mid=2247487533&idx=2&sn=9e0596a9ed9d6299556e3104ee46cb28&chksm=c53d9be50e9e31c76bc03808105bf750a1d1d0a9b564c44d964bdd131b9cfd426dcdb8b02c8d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTQ2NTY2Ng==&mid=2247487533&idx=2&sn=9e0596a9ed9d6299556e3104ee46cb28&chksm=c53d9be50e9e31c76bc03808105bf750a1d1d0a9b564c44d964bdd131b9cfd426dcdb8b02c8d#rd", "authors": ["\u5317\u4eac\u4e2d\u5173\u6751\u5b66\u9662"], "title": "X\u1d2c\u1d35\u79d1\u7814\u8fdb\u5c55 | RLinf\u4e0a\u65b0\u03c0RL\uff1a\u5728\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5fae\u8c03\u03c00\u548c\u03c00.5", "comment": "Source: WeChat, Published: 2025-11-06 12:01:25", "summary": "\u5f3a\u5316\u5b66\u4e60\u5141\u8bb8\u667a\u80fd\u4f53\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u771f\u5b9e\u4ea4\u4e92\u81ea\u884c\u63a2\u7d22\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u53ef\u4ee5\u51cf\u5c11 VLA \u6a21\u578b\u5bf9\u5927\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u5347 SFT \u7684\u6027\u80fd\u4e0a\u9650\u3002\u76ee\u524d\uff0c\u9488\u5bf9\u6d41\u5339\u914d VLA \u7684 RL \u7814\u7a76\u4ecd\u8f83\u5c11\uff0c\u4e3b\u6d41\u5de5\u4f5c\u5927\u591a\u96c6\u4e2d\u5728 OpenVLA \u548c OpenVLA-OFT \u7b49\u81ea\u56de\u5f52 VLA", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u5141\u8bb8\u667a\u80fd\u4f53\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u771f\u5b9e\u4ea4\u4e92\u81ea\u884c\u63a2\u7d22\u548c\u8fed\u4ee3\u6539\u8fdb\uff0c\u53ef\u4ee5\u51cf\u5c11 VLA \u6a21\u578b\u5bf9\u5927\u91cf\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u5347 SFT \u7684\u6027\u80fd\u4e0a\u9650\u3002\u76ee\u524d\uff0c\u9488\u5bf9\u6d41\u5339\u914d VLA \u7684 RL \u7814\u7a76\u4ecd\u8f83\u5c11\uff0c\u4e3b\u6d41\u5de5\u4f5c\u5927\u591a\u96c6\u4e2d\u5728 OpenVLA \u548c OpenVLA-OFT \u7b49\u81ea\u56de\u5f52 VLA", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6bd37555", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMDU4Nzg0Mw==&mid=2247594335&idx=1&sn=052e2c5955d1ffac36daf55145633792&chksm=97299d73e6c202ed6049fbea9088fb63e58f96c1e3505a20a22c65de7b31588ec380d143121a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMDU4Nzg0Mw==&mid=2247594335&idx=1&sn=052e2c5955d1ffac36daf55145633792&chksm=97299d73e6c202ed6049fbea9088fb63e58f96c1e3505a20a22c65de7b31588ec380d143121a#rd", "authors": ["RUC AI Box"], "title": "LLM <em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u63a2\u7d22\u673a\u5236\u76f8\u5173\u5de5\u4f5c\u901f\u89c8", "comment": "Source: WeChat, Published: 2025-11-06 10:13:04", "summary": "\u5728\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u4e2d\uff0c\u5982\u4f55\u5728\u63a2\u7d22\uff08exploration\uff09\u4e0e\u5229\u7528\uff08exploitation\uff09\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u4e00\u76f4\u662f\u6838\u5fc3\u95ee\u9898\u3002\u5c3d\u7ba1\u8fd1\u5e74\u6765\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u591a\u6570\u65b9\u6cd5\u504f\u5411\u4e8e\u5229\u7528\uff0c\u5bfc\u81f4\u6027\u80fd\u9010\u6e10\u8fdb\u5165\u5e73\u53f0\u671f\u3002", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u4e2d\uff0c\u5982\u4f55\u5728\u63a2\u7d22\uff08exploration\uff09\u4e0e\u5229\u7528\uff08exploitation\uff09\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u4e00\u76f4\u662f\u6838\u5fc3\u95ee\u9898\u3002\u5c3d\u7ba1\u8fd1\u5e74\u6765\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u591a\u6570\u65b9\u6cd5\u504f\u5411\u4e8e\u5229\u7528\uff0c\u5bfc\u81f4\u6027\u80fd\u9010\u6e10\u8fdb\u5165\u5e73\u53f0\u671f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.41bdd424", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNjkzMTE2Mg==&mid=2247486285&idx=1&sn=70e9c57e87cd6d456bb4cb3d1c85e094&chksm=9a807654978565c55a8482d87de21b4fc3b738a322c46047725cc74d5a4a298d14c91b0b5559#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNjkzMTE2Mg==&mid=2247486285&idx=1&sn=70e9c57e87cd6d456bb4cb3d1c85e094&chksm=9a807654978565c55a8482d87de21b4fc3b738a322c46047725cc74d5a4a298d14c91b0b5559#rd", "authors": ["\u63a7\u5236\u5de5\u7a0b"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u81ea\u9002\u5e94\u63a7\u5236\uff1a\u7406\u8bba\u8fdb\u5c55\u3001\u5de5\u7a0b\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411", "comment": "Source: WeChat, Published: 2025-11-06 10:09:42", "summary": "\u800c\u73b0\u5728\uff0c\u5143\u5f3a\u5316\u5b66\u4e60\u7684\u51fa\u73b0\u5f7b\u5e95\u6539\u53d8\u4e86\u8fd9\u4e00\u5c40\u9762\u3002\u5143\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u5728\u591a\u4e2a\u76f8\u4f3c\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u5230\u4e00\u79cd \u201c\u901a\u7528\u7684\u5b66\u4e60\u80fd\u529b\u201d\uff0c\u4f7f\u5f97\u667a\u80fd\u4f53\u5728\u9762\u5bf9\u65b0\u4efb\u52a1\u65f6\uff0c\u53ea\u9700\u5c11\u91cf\u6837\u672c\u5c31\u80fd\u5feb\u901f\u9002\u5e94\u3002", "AI": {"tldr": "\u800c\u73b0\u5728\uff0c\u5143\u5f3a\u5316\u5b66\u4e60\u7684\u51fa\u73b0\u5f7b\u5e95\u6539\u53d8\u4e86\u8fd9\u4e00\u5c40\u9762\u3002\u5143\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u5728\u591a\u4e2a\u76f8\u4f3c\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u5230\u4e00\u79cd \u201c\u901a\u7528\u7684\u5b66\u4e60\u80fd\u529b\u201d\uff0c\u4f7f\u5f97\u667a\u80fd\u4f53\u5728\u9762\u5bf9\u65b0\u4efb\u52a1\u65f6\uff0c\u53ea\u9700\u5c11\u91cf\u6837\u672c\u5c31\u80fd\u5feb\u901f\u9002\u5e94\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.8c334372", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4MTg0Njk2OQ==&mid=2247501241&idx=3&sn=8e1ecfb197d8daf6d7796641aa64abfe&chksm=fc4fd3979facb826510ecbd15d61ca13df58410702bcef46d9d2bdd534049fb906d508024b6b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4MTg0Njk2OQ==&mid=2247501241&idx=3&sn=8e1ecfb197d8daf6d7796641aa64abfe&chksm=fc4fd3979facb826510ecbd15d61ca13df58410702bcef46d9d2bdd534049fb906d508024b6b#rd", "authors": ["\u6559\u80b2\u4e0e\u4ea7\u4e1a"], "title": "\u5168\u7403\u9996\u4e2a\u5177\u8eab\u667a\u80fd<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6280\u672f\u6b63\u5f0f\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-11-06 09:10:41", "summary": "\u5168\u7403\u9996\u4e2a\u5177\u8eab\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u6b63\u5f0f\u5e94\u7528 \u5728\u4e0a\u6d77\u7684\u4e00\u5bb6\u667a\u80fd\u8bbe\u5907\u4ea7\u7ebf\u91cc\uff0c\u8fd9\u6279\u65b0\u6765\u7684\u673a\u5668\u4eba\u5458\u5de5\u5c31\u642d\u8f7d\u4e86\u5168\u7403\u9996\u4e2a\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u3002\u6709\u4e86\u8fd9\u9879\u6280\u672f\uff0c\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u5468\u671f\u5f97\u5230\u4e86\u5927\u5e45\u7f29\u51cf\u3002", "AI": {"tldr": "\u5168\u7403\u9996\u4e2a\u5177\u8eab\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u6b63\u5f0f\u5e94\u7528 \u5728\u4e0a\u6d77\u7684\u4e00\u5bb6\u667a\u80fd\u8bbe\u5907\u4ea7\u7ebf\u91cc\uff0c\u8fd9\u6279\u65b0\u6765\u7684\u673a\u5668\u4eba\u5458\u5de5\u5c31\u642d\u8f7d\u4e86\u5168\u7403\u9996\u4e2a\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u3002\u6709\u4e86\u8fd9\u9879\u6280\u672f\uff0c\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u5468\u671f\u5f97\u5230\u4e86\u5927\u5e45\u7f29\u51cf\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.436d8717", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU0NDMzNDM2NA==&mid=2247583618&idx=1&sn=1f0d8af3a1baca476e41f4dd344a1991&chksm=fa9d25ce978023a47e8d0ba00c352ff7d2a99615be3f8bd42bb58255be635759a0807fc2001f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU0NDMzNDM2NA==&mid=2247583618&idx=1&sn=1f0d8af3a1baca476e41f4dd344a1991&chksm=fa9d25ce978023a47e8d0ba00c352ff7d2a99615be3f8bd42bb58255be635759a0807fc2001f#rd", "authors": ["\u653f\u6cd5\u667a\u80fd\u5316\u5efa\u8bbe\u6280\u672f\u88c5\u5907\u53ca\u6210\u679c\u5c55"], "title": "\u5168\u7403\u9996\u4e2a\u5177\u8eab\u667a\u80fd<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6280\u672f\u6b63\u5f0f\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-11-06 09:05:05", "summary": "\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u6b63\u5f0f\u5e94\u7528 \u5728\u4e0a\u6d77\u7684\u4e00\u5bb6\u667a\u80fd\u8bbe\u5907\u4ea7\u7ebf\u91cc\uff0c\u8fd9\u6279\u65b0\u6765\u7684\u673a\u5668\u4eba\u5458\u5de5\u5c31\u642d\u8f7d\u4e86\u5168\u7403\u9996\u4e2a\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u3002\u6709\u4e86\u8fd9\u9879\u6280\u672f\uff0c\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u5468\u671f\u5f97\u5230\u4e86\u5927\u5e45\u7f29\u51cf\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u6b63\u5f0f\u5e94\u7528 \u5728\u4e0a\u6d77\u7684\u4e00\u5bb6\u667a\u80fd\u8bbe\u5907\u4ea7\u7ebf\u91cc\uff0c\u8fd9\u6279\u65b0\u6765\u7684\u673a\u5668\u4eba\u5458\u5de5\u5c31\u642d\u8f7d\u4e86\u5168\u7403\u9996\u4e2a\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u3002\u6709\u4e86\u8fd9\u9879\u6280\u672f\uff0c\u673a\u5668\u4eba\u7684\u8bad\u7ec3\u5468\u671f\u5f97\u5230\u4e86\u5927\u5e45\u7f29\u51cf\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.c69ccba8", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MTE5NDMxOA==&mid=2247491426&idx=1&sn=4accd6a44255fc894a4a6970677e76de&chksm=c3d5ac4f291221419546c6a68d5840479f340e64c5ec82920b9d69ba93c5887b58b3488b3c90#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MTE5NDMxOA==&mid=2247491426&idx=1&sn=4accd6a44255fc894a4a6970677e76de&chksm=c3d5ac4f291221419546c6a68d5840479f340e64c5ec82920b9d69ba93c5887b58b3488b3c90#rd", "authors": ["\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 CASIA"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u667a\u80fd\u4f53\u52a9\u529b\u5927\u6a21\u578b\u96c6\u6210\uff0c\u52a8\u6001\u6743\u91cd\u4f18\u5316\u89e3\u9501\u591a\u6a21\u578b\u534f\u540c", "comment": "Source: WeChat, Published: 2025-11-06 07:07:55", "summary": "RLAE\u7684\u6838\u5fc3\u601d\u8def\u662f\u201c\u4ee5\u5f3a\u5316\u5b66\u4e60\u5efa\u6a21\u96c6\u6210\u8fc7\u7a0b\uff0c\u7528\u52a8\u6001\u6743\u91cd\u9002\u914d\u573a\u666f\u9700\u6c42\u201d\uff0c\u5177\u4f53\u901a\u8fc7\u4e09\u5927\u6a21\u5757\u5b9e\u73b0\uff1a 1. MDP\u6846\u67b6\uff1a\u5c06\u96c6\u6210\u8f6c\u5316\u4e3a\u5e8f\u8d2f\u51b3\u7b56\u95ee\u9898RLAE\u5c06\u201c\u5927\u6a21\u578b\u96c6\u6210\u201d\u5f62\u5f0f\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5176\u5b9a\u4e49\u7684\u5173\u952e\u7ec4\u4ef6\u5982\u4e0b\uff1a", "AI": {"tldr": "RLAE\u7684\u6838\u5fc3\u601d\u8def\u662f\u201c\u4ee5\u5f3a\u5316\u5b66\u4e60\u5efa\u6a21\u96c6\u6210\u8fc7\u7a0b\uff0c\u7528\u52a8\u6001\u6743\u91cd\u9002\u914d\u573a\u666f\u9700\u6c42\u201d\uff0c\u5177\u4f53\u901a\u8fc7\u4e09\u5927\u6a21\u5757\u5b9e\u73b0\uff1a 1. MDP\u6846\u67b6\uff1a\u5c06\u96c6\u6210\u8f6c\u5316\u4e3a\u5e8f\u8d2f\u51b3\u7b56\u95ee\u9898RLAE\u5c06\u201c\u5927\u6a21\u578b\u96c6\u6210\u201d\u5f62\u5f0f\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5176\u5b9a\u4e49\u7684\u5173\u952e\u7ec4\u4ef6\u5982\u4e0b\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.90c9c4ff", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2ODA1MDE4NQ==&mid=2247483742&idx=1&sn=9ffd0aeba255f003ea467ff75c63eab0&chksm=c5a1fc04ca2a846277dc21911270d22ccab6e5d57cf24e65bd72cc190fb3bb04d6cf7387c46c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2ODA1MDE4NQ==&mid=2247483742&idx=1&sn=9ffd0aeba255f003ea467ff75c63eab0&chksm=c5a1fc04ca2a846277dc21911270d22ccab6e5d57cf24e65bd72cc190fb3bb04d6cf7387c46c#rd", "authors": ["\u5177\u8eab\u667a\u5e93"], "title": "\u3010RL\u7cfb\u5217\u3011HIL-SERL\uff1a99%+\u6210\u529f\u7387\u7684\u771f\u673a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-11-06 07:04:57", "summary": "\u8be5\u5206\u6790\u4e0d\u4ec5\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u4e3a\u4f55\u80fd\u5728\u8fd9\u4e9b\u590d\u6742\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u8fd8\u4e3a\u5c06\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u64cd\u4f5c\u65b9\u6cd5\u62d3\u5c55\u5230\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\u3002", "AI": {"tldr": "\u8be5\u5206\u6790\u4e0d\u4ec5\u5e2e\u52a9\u6211\u4eec\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u4e3a\u4f55\u80fd\u5728\u8fd9\u4e9b\u590d\u6742\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u8fd8\u4e3a\u5c06\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u64cd\u4f5c\u65b9\u6cd5\u62d3\u5c55\u5230\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.c25b55e0", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNzc0OTczNA==&mid=2247488902&idx=1&sn=13f50737a301a8fc17a9fd780761bcad&chksm=c3f721d7cb8f90e6b204ccef03ca2902a7db4d1e8e8ec9d19d712ca14592ecff1501ba289ccf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNzc0OTczNA==&mid=2247488902&idx=1&sn=13f50737a301a8fc17a9fd780761bcad&chksm=c3f721d7cb8f90e6b204ccef03ca2902a7db4d1e8e8ec9d19d712ca14592ecff1501ba289ccf#rd", "authors": ["\u771fAI\u81f3\u4e0a"], "title": "\u6ef4\u6ef4\u518d\u653e\u5927\u62db\uff1f\u7aef\u5230\u7aef<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8d85\u8d8a\u5927\u578b\u524d\u6cbfLLM", "comment": "Source: WeChat, Published: 2025-11-06 04:30:52", "summary": "\uff01\u56de\u590d \u201c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u201d \u5373\u53ef\u9886\u53d6 \u3010\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u3011\u7814\u7a76\u8bba\u6587 volant \u6c83\u6069\u667a\u6167 DeepTravel\uff1a\u4e00\u79cd\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u7684\u7aef\u5230\u7aef\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6", "AI": {"tldr": "\uff01\u56de\u590d \u201c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u201d \u5373\u53ef\u9886\u53d6 \u3010\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u3011\u7814\u7a76\u8bba\u6587 volant \u6c83\u6069\u667a\u6167 DeepTravel\uff1a\u4e00\u79cd\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u7684\u7aef\u5230\u7aef\u4ee3\u7406\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.0dbcfc95", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247717353&idx=1&sn=2951ec58ebf861a84b1ec6e2835bd233&chksm=f8d3940aca19f4a9bc193fd9982183617495727a5d76fb1eaf56f43f025f66ae259d8622056e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247717353&idx=1&sn=2951ec58ebf861a84b1ec6e2835bd233&chksm=f8d3940aca19f4a9bc193fd9982183617495727a5d76fb1eaf56f43f025f66ae259d8622056e#rd", "authors": ["\u4e00\u70b9\u4eba\u5de5\u4e00\u70b9\u667a\u80fd"], "title": "\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u6d41\u7a0b\u5de5\u4e1a\u667a\u80fd\u51b3\u7b56\u7814\u7a76\u4e0e\u5c55\u671b", "comment": "Source: WeChat, Published: 2025-11-06 04:01:05", "summary": "to \u5f3a\u5316\u5b66\u4e60\u6982\u8ff0 \u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u7c7b\u7528\u4e8e\u89e3\u51b3\u5e8f\u8d2f\u51b3\u7b56\u95ee\u9898\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u76ee\u6807\u662f\u8bad\u7ec3\u4e00\u4e2a\u667a\u80fd\u4f53\u5728\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u56de\u62a5\u3002", "AI": {"tldr": "to \u5f3a\u5316\u5b66\u4e60\u6982\u8ff0 \u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u7c7b\u7528\u4e8e\u89e3\u51b3\u5e8f\u8d2f\u51b3\u7b56\u95ee\u9898\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u76ee\u6807\u662f\u8bad\u7ec3\u4e00\u4e2a\u667a\u80fd\u4f53\u5728\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u56de\u62a5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.3cc3cf0f", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0OTMxMzYxNQ==&mid=2247490608&idx=1&sn=f262585f08f2ab04e0cc676273f2774e&chksm=c2cef8be694edf3f8072f3cd44da77cf06c4107e1594d2d4f2cdee0e8a7b9d2914c169db27e1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0OTMxMzYxNQ==&mid=2247490608&idx=1&sn=f262585f08f2ab04e0cc676273f2774e&chksm=c2cef8be694edf3f8072f3cd44da77cf06c4107e1594d2d4f2cdee0e8a7b9d2914c169db27e1#rd", "authors": ["\u7535\u529b\u7535\u5b50\u5b9e\u9a8c\u5ba4"], "title": "Nature\uff1a\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u52a9\u529b\u300c\u4eba\u9020\u592a\u9633\u300d\u53ef\u63a7\u7cbe\u51c6\u653e\u7535", "comment": "Source: WeChat, Published: 2025-11-06 03:01:01", "summary": "\u5176\u6b21\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u6258\u5361\u9a6c\u514b\u6a21\u62df\u5668\u4ea4\u4e92\uff0c\u5bfb\u627e\u6ee1\u8db3\u76ee\u6807\u7684\u8fd1\u6700\u4f18\u63a7\u5236\u7b56\u7565\u3002\u6700\u540e\uff0c\u8be5\u63a7\u5236\u7b56\u7565\u7531\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u5e76\u76f4\u63a5\u4ee5\u201c\u96f6\u6837\u672c\u201d\u65b9\u5f0f\u5728\u6258\u5361\u9a6c\u514b\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "AI": {"tldr": "\u5176\u6b21\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u6258\u5361\u9a6c\u514b\u6a21\u62df\u5668\u4ea4\u4e92\uff0c\u5bfb\u627e\u6ee1\u8db3\u76ee\u6807\u7684\u8fd1\u6700\u4f18\u63a7\u5236\u7b56\u7565\u3002\u6700\u540e\uff0c\u8be5\u63a7\u5236\u7b56\u7565\u7531\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u5e76\u76f4\u63a5\u4ee5\u201c\u96f6\u6837\u672c\u201d\u65b9\u5f0f\u5728\u6258\u5361\u9a6c\u514b\u786c\u4ef6\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.c4126b8b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMjc4ODQzMQ==&mid=2247484856&idx=1&sn=fa2fa87d4276d00524b1515784ef8363&chksm=e98c2fa061b468cc788f0626f182bf9339c05d268d45a241292c2b0ebb6ce95288affa91ae6e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMjc4ODQzMQ==&mid=2247484856&idx=1&sn=fa2fa87d4276d00524b1515784ef8363&chksm=e98c2fa061b468cc788f0626f182bf9339c05d268d45a241292c2b0ebb6ce95288affa91ae6e#rd", "authors": ["\u5f3a\u5316\u5b66\u4e60"], "title": "\u4e00\u6587\u8bfb\u61c2<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6838\u5fc3\uff1aQ \u5b66\u4e60\u7b97\u6cd5\u7684\u539f\u7406\u4e0e\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-11-06 02:23:33", "summary": "\u5e2e\u4f60\u638c\u63e1\u5f3a\u5316\u5b66\u4e60\u7684\u5165\u95e8\u5173\u952e\u3002lQ \u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\uff1a\u7ed9 \u201c\u51b3\u7b56\u201d \u6253\u5206Q \u5b66\u4e60\u7684\u672c\u8d28\uff0c\u662f\u4e3a \u201c\u72b6\u6001 - \u52a8\u4f5c\u201d \u5bf9\uff08State - Action Pair\uff09\u6253\u5206 \u2014\u2014 \u8fd9\u4e2a\u5206\u6570\u5c31\u662f \u201cQ \u503c\u201d\uff0c\u4ee3\u8868 \u201c\u5728\u67d0\u4e2a\u72b6\u6001\u4e0b\u6267\u884c\u67d0\u4e2a\u52a8\u4f5c\u540e\uff0c\u80fd\u83b7\u5f97\u7684\u957f\u671f\u7d2f\u79ef\u5956\u52b1", "AI": {"tldr": "\u5e2e\u4f60\u638c\u63e1\u5f3a\u5316\u5b66\u4e60\u7684\u5165\u95e8\u5173\u952e\u3002lQ \u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\uff1a\u7ed9 \u201c\u51b3\u7b56\u201d \u6253\u5206Q \u5b66\u4e60\u7684\u672c\u8d28\uff0c\u662f\u4e3a \u201c\u72b6\u6001 - \u52a8\u4f5c\u201d \u5bf9\uff08State - Action Pair\uff09\u6253\u5206 \u2014\u2014 \u8fd9\u4e2a\u5206\u6570\u5c31\u662f \u201cQ \u503c\u201d\uff0c\u4ee3\u8868 \u201c\u5728\u67d0\u4e2a\u72b6\u6001\u4e0b\u6267\u884c\u67d0\u4e2a\u52a8\u4f5c\u540e\uff0c\u80fd\u83b7\u5f97\u7684\u957f\u671f\u7d2f\u79ef\u5956\u52b1", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.108d297f", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyNDY2MTg4Mg==&mid=2247486457&idx=1&sn=67d6c48c78ece2ed9843656d52b0b834&chksm=e94ec0174a8d95b44450417eecdff1ef114238ac08ccfc153ad8f558a7614accc929f2a10dd5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyNDY2MTg4Mg==&mid=2247486457&idx=1&sn=67d6c48c78ece2ed9843656d52b0b834&chksm=e94ec0174a8d95b44450417eecdff1ef114238ac08ccfc153ad8f558a7614accc929f2a10dd5#rd", "authors": ["AICon\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u4e0e\u5e94\u7528\u5927\u4f1a"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em> AI \u7cfb\u7edf\u7684\u8bbe\u8ba1\u5b9e\u73b0\u53ca\u672a\u6765\u53d1\u5c55", "comment": "Source: WeChat, Published: 2025-11-06 01:02:24", "summary": "\u4ece\u5e38\u89c1\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5230\u57fa\u4e8e\u5baa\u6cd5\u7684\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u518d\u5230\u5982\u4eca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0d\u65ad\u8fdb\u6b65\u7684\u8fc7\u7a0b\uff0c\u5b9e\u9645\u4e0a\u4ee3\u8868\u7740\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u4fe1\u53f7\u6765\u6e90\u65e5\u76ca\u5e7f\u6cdb\uff0c\u540c\u65f6\u4efb\u52a1\u96be\u5ea6\u4e5f\u5728\u4e0d\u65ad\u63d0\u9ad8\u3002", "AI": {"tldr": "\u4ece\u5e38\u89c1\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5230\u57fa\u4e8e\u5baa\u6cd5\u7684\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u518d\u5230\u5982\u4eca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0d\u65ad\u8fdb\u6b65\u7684\u8fc7\u7a0b\uff0c\u5b9e\u9645\u4e0a\u4ee3\u8868\u7740\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u4fe1\u53f7\u6765\u6e90\u65e5\u76ca\u5e7f\u6cdb\uff0c\u540c\u65f6\u4efb\u52a1\u96be\u5ea6\u4e5f\u5728\u4e0d\u65ad\u63d0\u9ad8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.e7fb51ac", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5MzA0Nzk5Nw==&mid=2247505989&idx=1&sn=b01a22a707209dd7a9095994b2e12e0f&chksm=c1ecc4a8dcc7c9b7d3e3077169c8b05827305c4cd90343b18410e6edc66369bfd29136aadf38#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5MzA0Nzk5Nw==&mid=2247505989&idx=1&sn=b01a22a707209dd7a9095994b2e12e0f&chksm=c1ecc4a8dcc7c9b7d3e3077169c8b05827305c4cd90343b18410e6edc66369bfd29136aadf38#rd", "authors": ["SCUPIologist"], "title": "\u6d3b\u52a8\u56de\u987e\u4e28\u6570\u5b66\u7814\u8ba8\u4f1a\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u5965\u79d8\u65e0\u9650", "comment": "Source: WeChat, Published: 2025-11-06 01:00:23", "summary": "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u201d\u4e3a\u4e3b\u9898\u7684\u6570\u5b66\u7814\u8ba8\u4f1a\uff0c\u75312022\u7ea7\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u4e13\u4e1a\u5b66\u751f\u738b\u4fca\u6865\u4e3b\u8bb2\u3002\u526f\u6559\u6388\u6768\u94ee\u3001\u52a9\u7406\u6559\u6388\u5434\u82f1\u6770\u51fa\u5e2d\u4e86\u672c\u6b21\u6d3b\u52a8\uff0c\u5e76\u5728\u7814\u8ba8\u8fc7\u7a0b\u4e2d\u4e0e\u738b\u4fca\u6865\u540c\u5b66\u8fdb\u884c\u4e86\u4e13\u4e1a\u4ea4\u6d41\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u201d\u4e3a\u4e3b\u9898\u7684\u6570\u5b66\u7814\u8ba8\u4f1a\uff0c\u75312022\u7ea7\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u4e13\u4e1a\u5b66\u751f\u738b\u4fca\u6865\u4e3b\u8bb2\u3002\u526f\u6559\u6388\u6768\u94ee\u3001\u52a9\u7406\u6559\u6388\u5434\u82f1\u6770\u51fa\u5e2d\u4e86\u672c\u6b21\u6d3b\u52a8\uff0c\u5e76\u5728\u7814\u8ba8\u8fc7\u7a0b\u4e2d\u4e0e\u738b\u4fca\u6865\u540c\u5b66\u8fdb\u884c\u4e86\u4e13\u4e1a\u4ea4\u6d41\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6df7f4e5", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247504917&idx=1&sn=bd0f3b6c93184c6d9e87d342c3d9482f&chksm=9a0a55615a9379326b635f314a4505f3713661835b5668abae3c43c90ace16cb643d6f46159b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247504917&idx=1&sn=bd0f3b6c93184c6d9e87d342c3d9482f&chksm=9a0a55615a9379326b635f314a4505f3713661835b5668abae3c43c90ace16cb643d6f46159b#rd", "authors": ["\u5173\u4e8eNLP\u90a3\u4e9b\u4f60\u4e0d\u77e5\u9053\u7684\u4e8b"], "title": "\u767d\u8bdd\u6587<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>(RL) PPO\u7684\u524d\u751f\u4eca\u4e16(\u7b56\u7565\u68af\u5ea6\u4e0ePPO\u539f\u7406\u89e3\u8bfb)", "comment": "Source: WeChat, Published: 2025-11-05 22:38:11", "summary": "\u53e6\u5916\uff0cRL\u4e0e\u76d1\u7763\u5b66\u4e60\u6700\u5927\u7684\u4e0d\u540c\u662fRL\u5341\u5206\u5f3a\u8c03\u8f68\u8ff9\u4ee5\u53ca\u672a\u6765\u7684\u6536\u76ca\uff0c\u73b0\u5728\u8fc7\u7a0b\u76d1\u7763\uff08\uff09\u4e5f\u6bd4\u8f83\u706b\u70ed\uff0c\u6bcf\u4e00\u6b65\u7684\u52a8\u4f5c\u53ef\u80fd\u4f1a\u6253\u5f00\u4e0d\u540c\u7a0b\u5ea6\u7684\u8def\u7ebf\uff0c\u9700\u8981\u91cd\u89c6\u6bcf\u4e00\u6b65\u7684\u6536\u76ca\uff08\u53ca\u672a\u6765\u6536\u76ca\uff09\uff0c\u4e5f\u8981\u91cd\u89c6\u6574\u6761\u8f68\u8ff9\u7684\u6536\u76ca\u3002", "AI": {"tldr": "\u53e6\u5916\uff0cRL\u4e0e\u76d1\u7763\u5b66\u4e60\u6700\u5927\u7684\u4e0d\u540c\u662fRL\u5341\u5206\u5f3a\u8c03\u8f68\u8ff9\u4ee5\u53ca\u672a\u6765\u7684\u6536\u76ca\uff0c\u73b0\u5728\u8fc7\u7a0b\u76d1\u7763\uff08\uff09\u4e5f\u6bd4\u8f83\u706b\u70ed\uff0c\u6bcf\u4e00\u6b65\u7684\u52a8\u4f5c\u53ef\u80fd\u4f1a\u6253\u5f00\u4e0d\u540c\u7a0b\u5ea6\u7684\u8def\u7ebf\uff0c\u9700\u8981\u91cd\u89c6\u6bcf\u4e00\u6b65\u7684\u6536\u76ca\uff08\u53ca\u672a\u6765\u6536\u76ca\uff09\uff0c\u4e5f\u8981\u91cd\u89c6\u6574\u6761\u8f68\u8ff9\u7684\u6536\u76ca\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.df55b09c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxOTkwNzU4NA==&mid=2247489700&idx=1&sn=73c22898af38313f5eced7da47036fc6&chksm=964e4e17330802c335108ccbd2c28c49d5614ea5657ad4eeb1b7aafe13f2ff27128c024ee40d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxOTkwNzU4NA==&mid=2247489700&idx=1&sn=73c22898af38313f5eced7da47036fc6&chksm=964e4e17330802c335108ccbd2c28c49d5614ea5657ad4eeb1b7aafe13f2ff27128c024ee40d#rd", "authors": ["\u9676\u4eba\u8d85\u6709\u6599"], "title": "\u96f6\u95e8\u69db\u4e0a\u624b\u6700\u5f3a\u591a\u6a21\u6001<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0cQwen3-VL\u6a21\u578b\u76f4\u63a5\u5728\u672c\u5730\u90e8\u7f72(\u9644\u6559\u7a0b\uff09", "comment": "Source: WeChat, Published: 2025-11-06 13:20:19", "summary": "\u5728\u5f00\u6e90\u591a\u6a21\u6001\u5927\u6a21\u578b\u9886\u57dfQwen3-VL\u7cfb\u5217\u53ef\u4ee5\u8bf4\u662f\u884c\u4e1a\u7684\u9738\u699c\u5927\u54e5\u5927\u7ea7\u522b\u7684\u5b58\u5728~qwen3 lm dense/moe decoder ... lmages and videos here. 11427 tokens 8 tokens 1125 tokens imtimestomp in text farmol text tokens tishes picture 1 picture 2 picture 3 video 1 *** vision encoder -oooooo l", "AI": {"tldr": "\u5728\u5f00\u6e90\u591a\u6a21\u6001\u5927\u6a21\u578b\u9886\u57dfQwen3-VL\u7cfb\u5217\u53ef\u4ee5\u8bf4\u662f\u884c\u4e1a\u7684\u9738\u699c\u5927\u54e5\u5927\u7ea7\u522b\u7684\u5b58\u5728~qwen3 lm dense/moe decoder ... lmages and videos here. 11427 tokens 8 tokens 1125 tokens imtimestomp in text farmol text tokens tishes picture 1 picture 2 pictur...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.8f73388b", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247487697&idx=1&sn=918944c5f3cfbd1df5b63ff2df2cac62&chksm=c398de0783c7602769485e7a7713d865641c3e0fa01214bbe17b7b3f375897b7681f99ee3807#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MDY2ODM3NQ==&mid=2247487697&idx=1&sn=918944c5f3cfbd1df5b63ff2df2cac62&chksm=c398de0783c7602769485e7a7713d865641c3e0fa01214bbe17b7b3f375897b7681f99ee3807#rd", "authors": ["AI\u5927\u6a21\u578b\u524d\u6cbf"], "title": "\u8d85\u5168\u9762\uff01\u5404\u79cd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u67b6\u6784\u6bd4\u8f83\uff01", "comment": "Source: WeChat, Published: 2025-11-06 12:02:48", "summary": "\u4f20\u7edf\u7684NLP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5982MMLU\uff08\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\uff09\uff0c\u6b63\u8fc5\u901f\u8d8b\u4e8e\u9971\u548c\uff0c\u5bf9\u4e8e\u533a\u5206\u524d\u6cbf\u6a21\u578b\u7684\u80fd\u529b\u6108\u53d1\u6709\u9650\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u4e00\u7c7b\u4e13\u6ce8\u4e8e\u590d\u6742\u63a8\u7406\uff08\u5982GPQA\uff0c AIME\uff09\u548c\u667a\u80fd\u4f53\u6267\u884c\uff08\u5982SWE - bench\uff0c Terminal - bench\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u5df2\u6210\u4e3a\u8861\u91cfSOTA", "AI": {"tldr": "\u4f20\u7edf\u7684NLP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5982MMLU\uff08\u5927\u89c4\u6a21\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\uff09\uff0c\u6b63\u8fc5\u901f\u8d8b\u4e8e\u9971\u548c\uff0c\u5bf9\u4e8e\u533a\u5206\u524d\u6cbf\u6a21\u578b\u7684\u80fd\u529b\u6108\u53d1\u6709\u9650\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u4e00\u7c7b\u4e13\u6ce8\u4e8e\u590d\u6742\u63a8\u7406\uff08\u5982GPQA\uff0c AIME\uff09\u548c\u667a\u80fd\u4f53\u6267\u884c\uff08\u5982SWE - bench\uff0c Terminal - bench\uff09\u7684\u65b0\u57fa\u51c6\uff0c\u5df2\u6210\u4e3a\u8861\u91cfSOTA", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.efcd0077", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2Mzc1OTg0MA==&mid=2247501510&idx=1&sn=477f3f1190854e2c08076f8e8f2f552d&chksm=cfbe04d5e688c501120cf07980b6560534bf97db96aa92cfeb4556fe715998f9b51ca7d3ad1f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2Mzc1OTg0MA==&mid=2247501510&idx=1&sn=477f3f1190854e2c08076f8e8f2f552d&chksm=cfbe04d5e688c501120cf07980b6560534bf97db96aa92cfeb4556fe715998f9b51ca7d3ad1f#rd", "authors": ["\u672a\u540d\u79d1\u521b"], "title": "\u3010\u79d1\u6280\u6210\u679c\u3011\u7834\u89e3<em class=\"highlight\">\u5927\u6a21\u578b</em> \u201c\u4f2a\u88c5\u201d \u96be\u9898\uff01\u5317\u5927 CoT Monitor+\u6846\u67b6\u5b9e\u73b0\u63a8\u7406\u4e2d\u5b89\u5168\u5e72\u9884", "comment": "Source: WeChat, Published: 2025-11-06 10:30:52", "summary": "\u8fd1\u65e5\uff0c\u5317\u4eac\u5927\u5b66\u56e2\u961f\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b89\u5168\u5bf9\u9f50\u9886\u57df\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u63d0\u51faCoTMonitor+\u6846\u67b6\u2014\u2014\u901a\u8fc7\u5728\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u81ea\u6211\u76d1\u63a7\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u578b\u201c\u6b3a\u9a97\u6027\u5bf9\u9f50\u201d\u95ee\u9898\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u6b3a\u9a97\u884c\u4e3a\u7684\u57fa\u51c6", "AI": {"tldr": "\u8fd1\u65e5\uff0c\u5317\u4eac\u5927\u5b66\u56e2\u961f\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b89\u5168\u5bf9\u9f50\u9886\u57df\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\uff0c\u63d0\u51faCoTMonitor+\u6846\u67b6\u2014\u2014\u901a\u8fc7\u5728\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u81ea\u6211\u76d1\u63a7\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u578b\u201c\u6b3a\u9a97\u6027\u5bf9\u9f50\u201d\u95ee\u9898\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u6b3a\u9a97\u884c\u4e3a\u7684\u57fa\u51c6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.d59687fb", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658913&idx=3&sn=c84c80d7bc36ddaff5563d9f4a901a12&chksm=c0dba8d2e1f834496ab52ba4b6c33ba583eef915fe10f2e11ba7bf7031488e7336bd01b1ddd8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658913&idx=3&sn=c84c80d7bc36ddaff5563d9f4a901a12&chksm=c0dba8d2e1f834496ab52ba4b6c33ba583eef915fe10f2e11ba7bf7031488e7336bd01b1ddd8#rd", "authors": ["DataFunSummit"], "title": "Agent+Copilot\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5728\u667a\u80fd\u8fd0\u7ef4\u9886\u57df\u7684\u5e94\u7528", "comment": "Source: WeChat, Published: 2025-11-06 10:02:45", "summary": "\u8fd9\u4e48\u5927\u7684\u65e5\u5fd7\u91cf\u9760\u4eba\u5de5\u5df2\u7ecf\u65e0\u6cd5\u89e3\u51b3\uff0c\u6240\u4ee5\u5927\u6a21\u578b\u88ab\u8ba4\u4e3a\u662f\u4e00\u4e2a\u65b0\u8d28\u751f\u4ea7\u529b\uff0c\u5b83\u7684\u51fa\u73b0\u4e3a\u4eba\u4eec\u5e26\u6765\u4e86\u5e0c\u671b\u30026. AI \u573a\u666fsecops\u884c\u4e1a\u75db\u70b9-ai\u573a\u666f dns\u53cd\u5411\u5916\u8fde\u57df\u540d\u68c0\u6d4b web\u5f02\u5e38\u8bf7\u6c42\u68c0\u6d4b \u5229\u7528ai\u8bc6\u522b\u6f5c\u5728\u7684\u6076\u610f\u57df\u540d\uff0c\u63d0\u9ad8\u5bf9\u65b0\u578b\u57df \u533a\u5206\u6b63\u5e38", "AI": {"tldr": "\u8fd9\u4e48\u5927\u7684\u65e5\u5fd7\u91cf\u9760\u4eba\u5de5\u5df2\u7ecf\u65e0\u6cd5\u89e3\u51b3\uff0c\u6240\u4ee5\u5927\u6a21\u578b\u88ab\u8ba4\u4e3a\u662f\u4e00\u4e2a\u65b0\u8d28\u751f\u4ea7\u529b\uff0c\u5b83\u7684\u51fa\u73b0\u4e3a\u4eba\u4eec\u5e26\u6765\u4e86\u5e0c\u671b\u30026. AI \u573a\u666fsecops\u884c\u4e1a\u75db\u70b9-ai\u573a\u666f dns\u53cd\u5411\u5916\u8fde\u57df\u540d\u68c0\u6d4b web\u5f02\u5e38\u8bf7\u6c42\u68c0\u6d4b \u5229\u7528ai\u8bc6\u522b\u6f5c\u5728\u7684\u6076\u610f\u57df\u540d\uff0c\u63d0\u9ad8\u5bf9\u65b0\u578b\u57df \u533a\u5206\u6b63\u5e38", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.42b9efeb", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MzEzMTY0MA==&mid=2652083851&idx=2&sn=3b21e83cd0d6de04d4a28219fb102750&chksm=bca8a5526375912b0fea10a80749f83539b1d86e0e7560ca8719517ff1c275b4ff0d168e5262#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MzEzMTY0MA==&mid=2652083851&idx=2&sn=3b21e83cd0d6de04d4a28219fb102750&chksm=bca8a5526375912b0fea10a80749f83539b1d86e0e7560ca8719517ff1c275b4ff0d168e5262#rd", "authors": ["C114\u901a\u4fe1\u7f51"], "title": "\u8d4b\u80fd\u54c1\u724c\u5efa\u8bbe\uff1a\u5143\u666f\u54c1\u724c\u4ef7\u503c\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4eae\u76f8", "comment": "Source: WeChat, Published: 2025-11-06 08:54:28", "summary": "\u5176\u6b21\u662f\u6280\u672f\u521b\u65b0\uff0c\u4e2d\u56fd\u8054\u901a\u65e9\u57282024\u5e74\u521d\u5c31\u53d1\u5e03\u4e86\u5143\u666f\u201c1+1+M\u201d\u5927\u6a21\u578b\u4f53\u7cfb\uff0c\u5305\u542b1\u5957\u57fa\u7840\u5927\u6a21\u578b\u30011\u4e2a\u5927\u6a21\u578b\u5e73\u53f0\u548cM\u79cd\u884c\u4e1a\u5927\u6a21\u578b\u30022025\u5e74\u53c8\u5f00\u6e90\u4e86\u201c\u5143\u666f\u4e07\u609f\u201d\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0\uff0c\u652f\u6301\u7528\u62370\u4ee3\u7801\u3001\u4f4e\u95e8\u69db\u3001\u9ad8\u6548\u7387\u521b\u5efa\u667a\u80fd\u4f53\u5e94\u7528\u3002", "AI": {"tldr": "\u5176\u6b21\u662f\u6280\u672f\u521b\u65b0\uff0c\u4e2d\u56fd\u8054\u901a\u65e9\u57282024\u5e74\u521d\u5c31\u53d1\u5e03\u4e86\u5143\u666f\u201c1+1+M\u201d\u5927\u6a21\u578b\u4f53\u7cfb\uff0c\u5305\u542b1\u5957\u57fa\u7840\u5927\u6a21\u578b\u30011\u4e2a\u5927\u6a21\u578b\u5e73\u53f0\u548cM\u79cd\u884c\u4e1a\u5927\u6a21\u578b\u30022025\u5e74\u53c8\u5f00\u6e90\u4e86\u201c\u5143\u666f\u4e07\u609f\u201d\u667a\u80fd\u4f53\u5f00\u53d1\u5e73\u53f0\uff0c\u652f\u6301\u7528\u62370\u4ee3\u7801\u3001\u4f4e\u95e8\u69db\u3001\u9ad8\u6548\u7387\u521b\u5efa\u667a\u80fd\u4f53\u5e94\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.5d2f86c6", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MzAxOTMwMQ==&mid=2650126591&idx=1&sn=3dce7664a96ac6d072a3b102ca001194&chksm=f283362e2e7e75d6bd31cccdda04e6af3732ac0a9e811fb04670e4f05082aef2c14fc715ab10#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MzAxOTMwMQ==&mid=2650126591&idx=1&sn=3dce7664a96ac6d072a3b102ca001194&chksm=f283362e2e7e75d6bd31cccdda04e6af3732ac0a9e811fb04670e4f05082aef2c14fc715ab10#rd", "authors": ["\u4e2d\u534e\u533b\u5b66\u671f\u520a\u7f51"], "title": "\u4e13\u5bb6\u5171\u8bc6\u4e28\u533b\u7597\u573a\u666f\u4e0b\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\u5e94\u7528\u6548\u679c\u56de\u987e\u6027\u8bc4\u6d4b\u4e13\u5bb6\u5171\u8bc6\uff082025\u7248\uff09", "comment": "Source: WeChat, Published: 2025-11-06 08:30:00", "summary": "\u5927\u8bed\u8a00\u6a21\u578b\u6307\u4f7f\u7528\u5927\u91cf\u6587\u672c\u6570\u636e\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6587\u672c\u6216\u7406\u89e3\u8bed\u8a00\u6587\u672c\u7684\u542b\u4e49\uff0c\u901a\u8fc7\u5728\u5e9e\u5927\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6765\u63d0\u4f9b\u6709\u5173\u5404\u79cd\u4e3b\u9898\u7684\u6df1\u539a\u77e5\u8bc6\u548c\u8bed\u8a00\u751f\u4ea7\u3002", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u6307\u4f7f\u7528\u5927\u91cf\u6587\u672c\u6570\u636e\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6587\u672c\u6216\u7406\u89e3\u8bed\u8a00\u6587\u672c\u7684\u542b\u4e49\uff0c\u901a\u8fc7\u5728\u5e9e\u5927\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6765\u63d0\u4f9b\u6709\u5173\u5404\u79cd\u4e3b\u9898\u7684\u6df1\u539a\u77e5\u8bc6\u548c\u8bed\u8a00\u751f\u4ea7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.677a380b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMTIxMDY2NQ==&mid=2247492530&idx=1&sn=e90bfd614d6ed2eb869251e034670dac&chksm=c047406ca3233534f8ee96447413dda7b6837b645086fe5fc2c7144d88202a92aef59fa706d0#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMTIxMDY2NQ==&mid=2247492530&idx=1&sn=e90bfd614d6ed2eb869251e034670dac&chksm=c047406ca3233534f8ee96447413dda7b6837b645086fe5fc2c7144d88202a92aef59fa706d0#rd", "authors": ["\u56fe\u683c\u56fe\u4e66"], "title": "\u5e72\u8d27\u5206\u4eab|Qwen3 <em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b9e\u6218\u6307\u5357\u6765\u88ad\uff01", "comment": "Source: WeChat, Published: 2025-11-06 06:58:20", "summary": "\u9762\u5411\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u7684\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\uff0c\u9488\u5bf9\u5927\u6a21\u578b\u5e94 \u7528\u5f00\u53d1\u6784\u5efa\u65b9\u6cd5\u3001\u8fc7\u7a0b\u3001\u5de5\u5177\u548c\u6848\u4f8b\u8fdb\u884c\u4ecb\u7ecd\u3002\u6848\u4f8b\u53ef\u501f\u9274\u3002\u56f4\u7ed5\u6cd5\u5f8b\u54a8\u8be2\u667a\u80fd\u52a9\u624b\u548c\u4ee3\u7801\u4fee\u590d\u667a\u80fd\u52a9\u624b\u4e24\u4e2a\u5f00\u53d1\u6848 \u4f8b\uff0c\u4ecb\u7ecd\u5177\u4f53\u9879\u76ee\u7684\u9700\u6c42\u5206\u6790\u3001\u7cfb\u7edf\u67b6\u6784\u3001\u5173\u952e\u6280\u672f\u4e0e", "AI": {"tldr": "\u9762\u5411\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u7684\u5b9e\u8df5\u8005\u548c\u7814\u7a76\u8005\uff0c\u9488\u5bf9\u5927\u6a21\u578b\u5e94 \u7528\u5f00\u53d1\u6784\u5efa\u65b9\u6cd5\u3001\u8fc7\u7a0b\u3001\u5de5\u5177\u548c\u6848\u4f8b\u8fdb\u884c\u4ecb\u7ecd\u3002\u6848\u4f8b\u53ef\u501f\u9274\u3002\u56f4\u7ed5\u6cd5\u5f8b\u54a8\u8be2\u667a\u80fd\u52a9\u624b\u548c\u4ee3\u7801\u4fee\u590d\u667a\u80fd\u52a9\u624b\u4e24\u4e2a\u5f00\u53d1\u6848 \u4f8b\uff0c\u4ecb\u7ecd\u5177\u4f53\u9879\u76ee\u7684\u9700\u6c42\u5206\u6790\u3001\u7cfb\u7edf\u67b6\u6784\u3001\u5173\u952e\u6280\u672f\u4e0e", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
