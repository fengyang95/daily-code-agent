{"id": "2512.06042", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06042", "abs": "https://arxiv.org/abs/2512.06042", "authors": ["Ashish Hooda", "Mihai Christodorescu", "Chuangang Ren", "Aaron Wilson", "Kassem Fawaz", "Somesh Jha"], "title": "Auto-SPT: Automating Semantic Preserving Transformations for Code", "comment": null, "summary": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.", "AI": {"tldr": "Auto-SPT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u5728\u5e72\u51c0\u3001\u7ed3\u6784\u5316\u7684\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7801\u4f1a\u7ecf\u5386\u5404\u79cd\u8bed\u4e49\u4fdd\u6301\u7684\u53d8\u6362\uff08\u5982\u91cd\u6784\u3001\u538b\u7f29\u3001\u683c\u5f0f\u5316\u3001\u7f16\u8bd1\u5668\u4f18\u5316\uff09\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd", "method": "\u63d0\u51faAuto-SPT\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u6784\u5efa\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff1a1) \u8bbe\u8ba1\u591a\u6837\u5316\u7684\u8bed\u4e49\u4fdd\u6301\u53d8\u6362(SPTs)\uff1b2) \u4e3a\u8fd9\u4e9bSPTs\u751f\u6210\u5f3a\u5b9e\u73b0\uff1b3) \u7ec4\u5408\u8fd9\u4e9b\u53d8\u6362\u5f62\u6210\u5f3a\u53d8\u6362\u3002\u5f62\u5f0f\u5316\u5206\u6790\u663e\u793aSPT\u7684\u591a\u6837\u6027\u5f71\u54cd\u7ec4\u5408\u5f3a\u5ea6", "result": "Auto-SPT\u6bd4\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u66f4\u591a\u6837\u5316\u7684SPTs\uff0c\u8fd9\u4e9bSPTs\u663e\u8457\u964d\u4f4e\u4e86\u6700\u5148\u8fdb\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660eAuto-SPT\u53ef\u7528\u4e8e\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ea7\u751f\u5bf9\u771f\u5b9e\u4e16\u754c\u5bf9\u6297\u6027\u4ee3\u7801\u53d8\u6362\u5177\u6709\u9c81\u68d2\u6027\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u6a21\u578b", "conclusion": "Auto-SPT\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u8bad\u7ec3-\u6d4b\u8bd5\u6570\u636e\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027", "topic": "swe benchmark"}}
{"id": "2512.06046", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06046", "abs": "https://arxiv.org/abs/2512.06046", "authors": ["Ramprasath Ganesaraja", "Swathika N", "Saravanan AP", "Kamalkumar Rathinasamy", "Chetana Amancharla", "Rahul Das", "Sahil Dilip Panse", "Aditya Batwe", "Dileep Vijayan", "Veena Ashok", "Thanushree A P", "Kausthubh J Rao", "Alden Olivero", "Roshan", "Rajeshwar Reddy Manthena", "Asmitha Yuga Sre A", "Harsh Tripathi", "Suganya Selvaraj", "Vito Chin", "Kasthuri Rangan Bhaskar", "Kasthuri Rangan Bhaskar", "Venkatraman R", "Sajit Vijayakumar"], "title": "Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework", "comment": "17 pages, 9 figures", "summary": "We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline", "AI": {"tldr": "AI4UI\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u7ea7\u5e94\u7528\u4ea4\u4ed8\u7684\u81ea\u4e3b\u524d\u7aef\u5f00\u53d1\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u751f\u4ea7\u5c31\u7eea\u6027\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9636\u6bb5\u5d4c\u5165Gen-AI\u53cb\u597d\u8bed\u6cd5\u548c\u540e\u5904\u7406\u9636\u6bb5\u4e13\u5bb6\u7cbe\u4fee\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u6269\u5c55\u3001\u5408\u89c4\u4e14\u53ef\u7ef4\u62a4\u7684UI\u4ee3\u7801\u81ea\u52a8\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u4ee3\u7801\u52a9\u624b\u4e3b\u8981\u9762\u5411\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f01\u4e1a\u7ea7\u5e94\u7528\u5bf9\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u5408\u89c4\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u7684\u4e25\u683c\u8981\u6c42\u3002\u4f01\u4e1a\u9700\u8981\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5e76\u786e\u4fdd\u751f\u4ea7\u5c31\u7eea\u7684UI\u4ee3\u7801\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u76ee\u6807\u5bfc\u5411\u7684\u4eba\u673a\u534f\u540c\u65b9\u6cd5\uff1a\u8bbe\u8ba1\u9636\u6bb5\u5728Figma\u539f\u578b\u4e2d\u5d4c\u5165Gen-AI\u53cb\u597d\u8bed\u6cd5\u7f16\u7801\u9700\u6c42\uff1b\u540e\u5904\u7406\u9636\u6bb5\u7531\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u7cbe\u4fee\uff1b\u4e2d\u95f4\u9636\u6bb5\u5b8c\u5168\u81ea\u4e3b\u8fd0\u884c\u3002\u6280\u672f\u8d21\u732e\u5305\u62ecFigma\u8bed\u6cd5\u89e3\u91ca\u5668\u3001\u9886\u57df\u611f\u77e5\u77e5\u8bc6\u56fe\u8c31\u3001\u5b89\u5168\u62bd\u8c61/\u5305\u4ee3\u7801\u96c6\u6210\u7b56\u7565\u3001\u4e13\u5bb6\u9a71\u52a8\u7684\u67b6\u6784\u6a21\u677f\uff0c\u4ee5\u53ca\u7531\u4e13\u95e8\u4ee3\u7406\u89d2\u8272\u534f\u8c03\u7684\u53d8\u66f4\u5bfc\u5411\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAI4UI\u5b9e\u73b0\u4e8697.24%\u7684\u5e73\u53f0\u517c\u5bb9\u6027\u300187.10%\u7684\u7f16\u8bd1\u6210\u529f\u7387\u300186.98%\u7684\u5b89\u5168\u5408\u89c4\u6027\u300178.00%\u7684\u529f\u80fd\u5b9e\u73b0\u6210\u529f\u7387\u300173.50%\u7684\u4ee3\u7801\u5ba1\u67e5\u8d28\u91cf\u548c73.36%\u7684UI/UX\u4e00\u81f4\u6027\u3002\u5728200\u540d\u4e13\u5bb6\u8bc4\u4f30\u8005\u7684\u76f2\u9009\u504f\u597d\u7814\u7a76\u4e2d\uff0cAI4UI\u6210\u4e3a\u9886\u5148\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\u3002", "conclusion": "AI4UI\u80fd\u591f\u5f02\u6b65\u751f\u6210\u6570\u5343\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684UI\u754c\u9762\uff0c\u5c06\u4ea4\u4ed8\u65f6\u95f4\u4ece\u6570\u6708\u538b\u7f29\u5230\u6570\u5468\uff0c\u5c55\u793a\u4e86\u5728\u4f01\u4e1a\u7ea7\u524d\u7aef\u5f00\u53d1\u4e2d\u7684\u5f3a\u5927\u7ade\u4e89\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "2512.06060", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06060", "abs": "https://arxiv.org/abs/2512.06060", "authors": ["Mohanakrishnan Hariharan"], "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring", "comment": null, "summary": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u81ea\u4e3b\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u5de5\u7a0b\u53cd\u9988\u6301\u7eed\u6539\u8fdb\u4ece\u4e1a\u52a1\u9700\u6c42\u6587\u6863\u81ea\u52a8\u751f\u6210\u8f6f\u4ef6\u6d4b\u8bd5\u7528\u4f8b\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u4ece\u9759\u6001\u77e5\u8bc6\u5e93\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u65e0\u6cd5\u968f\u65f6\u95f4\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u8d28\u91cf\u5de5\u7a0b\u53cd\u9988\u4e2d\u5b66\u4e60\u5e76\u6301\u7eed\u6539\u8fdb\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u4ee3\u7406\u5f0fRAG\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u95e8\u4ee3\u7406\u4e0e\u6df7\u5408\u5411\u91cf-\u56fe\u77e5\u8bc6\u5e93\uff0c\u4f7f\u7528PPO\u548cDQN\u7b97\u6cd5\u6839\u636e\u6d4b\u8bd5\u6709\u6548\u6027\u3001\u7f3a\u9677\u68c0\u6d4b\u7387\u548c\u5de5\u4f5c\u6d41\u6307\u6807\u4f18\u5316\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u5728\u4f01\u4e1a\u7ea7Apple\u9879\u76ee\u4e0a\u9a8c\u8bc1\uff0c\u6d4b\u8bd5\u751f\u6210\u51c6\u786e\u7387\u4ece94.8%\u63d0\u5347\u81f397.2%\uff08\u63d0\u9ad82.4%\uff09\uff0c\u7f3a\u9677\u68c0\u6d4b\u7387\u63d0\u534710.8%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u7531\u8d28\u91cf\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u9a71\u52a8\u7684\u6301\u7eed\u77e5\u8bc6\u4f18\u5316\u5faa\u73af\uff0c\u4ea7\u751f\u9010\u6b65\u63d0\u5347\u7684\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\uff0c\u589e\u5f3a\u800c\u975e\u53d6\u4ee3\u4eba\u5de5\u6d4b\u8bd5\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "2512.05998", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05998", "abs": "https://arxiv.org/abs/2512.05998", "authors": ["Michael Todasco"], "title": "Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals", "comment": "25 pages, 8 tables, 2 figures. Pilot study. Data, prompts, and code available at https://osf.io/dc24t/", "summary": "Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. \"Whale\" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.", "AI": {"tldr": "\u901a\u8fc7\u865a\u6784\u9884\u6d4b\u5e02\u573a\u8ba9LLM\u7528\u865a\u62df\u8d27\u5e01\u4e0b\u6ce8\uff0c\u5c06\u8bc4\u4f30\u4efb\u52a1\u8f6c\u5316\u4e3a\u8d4c\u535a\u6e38\u620f\uff0c\u867d\u7136\u51c6\u786e\u7387\u63d0\u5347\u6709\u9650\uff0c\u4f46\u663e\u8457\u4ea7\u751f\u4e86\u53ef\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u5176\u4ed6\u6a21\u578b\u65f6\u901a\u5e38\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u8868\u793a\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u8ba9LLM\u8868\u8fbe\u5185\u90e8\u7f6e\u4fe1\u5ea6\u5e76\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u8bbe\u8ba1\u6570\u5b66\u548c\u903b\u8f91\u95ee\u9898\uff0c\u8ba9\u57fa\u7ebf\u6a21\u578b\u56de\u7b54\uff0c\u9884\u6d4b\u6a21\u578b\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u9884\u6d4b\u57fa\u7ebf\u6a21\u578b\u662f\u5426\u6b63\u786e\uff1a\u63a7\u5236\u7ec4\uff08\u7b80\u5355\u9884\u6d4b\uff09\u548c\u6fc0\u52b1\u7ec4\uff08\u9884\u6d4b+\u7528\u865a\u62df\u8d27\u5e01\u4e0b\u6ce8\uff09\u3002", "result": "\u6fc0\u52b1\u7ec4\u51c6\u786e\u7387\u7565\u6709\u63d0\u5347\uff0881.5% vs 79.1%\uff09\uff0c\u5b66\u4e60\u901f\u5ea6\u663e\u8457\u66f4\u5feb\uff1b\u4e0b\u6ce8\u91d1\u989d\u4e0e\u7f6e\u4fe1\u5ea6\u76f8\u5173\uff0c\u5927\u989d\u4e0b\u6ce8\u51c6\u786e\u7387\u7ea699%\uff0c\u5c0f\u989d\u4e0b\u6ce8\u7ea674%\u3002", "conclusion": "\u8d4c\u535a\u673a\u5236\u521b\u9020\u4e86\u53ef\u8bfb\u7684\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\uff0c\u7b80\u5355\u7684\u91d1\u878d\u6846\u67b6\u53ef\u80fd\u5e2e\u52a9LLM\u6210\u4e3a\u98ce\u9669\u611f\u77e5\u7684\u9884\u6d4b\u8005\uff0c\u4f7f\u5176\u5185\u90e8\u4fe1\u5ff5\u53ef\u89c1\u53ef\u7528\u3002", "topic": "agent analysis"}}
{"id": "2512.06247", "categories": ["cs.SE", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06247", "abs": "https://arxiv.org/abs/2512.06247", "authors": ["Gus Henry Smith", "Sandesh Adhikary", "Vineet Thumuluri", "Karthik Suresh", "Vivek Pandit", "Kartik Hegde", "Hamid Shojaei", "Chandra Bhagavatula"], "title": "DUET: Agentic Design Understanding via Experimentation and Testing", "comment": null, "summary": "AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.", "AI": {"tldr": "DUET\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b9e\u9a8c\u548c\u6d4b\u8bd5\u6765\u7406\u89e3\u786c\u4ef6\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u6a21\u4eff\u786c\u4ef6\u4e13\u5bb6\u901a\u8fc7\u8fed\u4ee3\u5b9e\u9a8c\u7406\u89e3\u590d\u6742\u8bbe\u8ba1\u7684\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u5728\u5f62\u5f0f\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002RTL\u4ee3\u7801\u4f7f\u7528SystemVerilog\u7b49\u4f4e\u7ea7\u8bed\u8a00\u7279\u5f81\u7f16\u7801\u590d\u6742\u3001\u52a8\u6001\u3001\u65f6\u95f4\u6f14\u5316\u7684\u884c\u4e3a\uff0cLLM\u4ec5\u4eceRTL\u8bed\u6cd5\u96be\u4ee5\u63a8\u65ad\u8fd9\u4e9b\u590d\u6742\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5176\u5728\u4ee3\u7801\u8865\u5168\u3001\u6587\u6863\u751f\u6210\u548c\u9a8c\u8bc1\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "DUET\u91c7\u7528\u8bbe\u8ba1\u7406\u89e3\u901a\u8fc7\u5b9e\u9a8c\u548c\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u6a21\u4eff\u786c\u4ef6\u8bbe\u8ba1\u4e13\u5bb6\u7406\u89e3\u590d\u6742\u8bbe\u8ba1\u7684\u65b9\u5f0f\uff1a\u4e0d\u662f\u4e00\u6b21\u6027\u9605\u8bfbRTL\uff0c\u800c\u662f\u901a\u8fc7\u4f7f\u7528\u591a\u79cd\u5de5\u5177\u8fdb\u884c\u8fed\u4ee3\u5b9e\u9a8c\u3002\u8be5\u65b9\u6cd5\u8fed\u4ee3\u751f\u6210\u5047\u8bbe\uff0c\u4f7f\u7528EDA\u5de5\u5177\uff08\u5982\u4eff\u771f\u3001\u6ce2\u5f62\u68c0\u67e5\u548c\u5f62\u5f0f\u9a8c\u8bc1\uff09\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u6574\u5408\u7ed3\u679c\u4ee5\u6784\u5efa\u81ea\u5e95\u5411\u4e0a\u7684\u8bbe\u8ba1\u7406\u89e3\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cDUET\u76f8\u6bd4\u6ca1\u6709\u5b9e\u9a8c\u7684\u57fa\u7ebf\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86AI\u4ee3\u7406\u5728\u5f62\u5f0f\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "DUET\u901a\u8fc7\u6a21\u4eff\u786c\u4ef6\u4e13\u5bb6\u7684\u5b9e\u9a8c\u6027\u7406\u89e3\u65b9\u6cd5\uff0c\u4e3aAI\u4ee3\u7406\u5904\u7406\u590d\u6742\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5f62\u5f0f\u9a8c\u8bc1\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "topic": "code agent"}}
{"id": "2512.06248", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06248", "abs": "https://arxiv.org/abs/2512.06248", "authors": ["Cheng Cheng", "Jinqiu Yang"], "title": "CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models", "comment": null, "summary": "Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.", "AI": {"tldr": "CFCEval\u662f\u4e00\u4e2a\u8bc4\u4f30\u4ee3\u7801\u751f\u6210LLM\u8d28\u91cf\u548c\u5b89\u5168\u6027\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7MLVBench\u57fa\u51c6\u548cELRM\u5ea6\u91cf\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801LLM\u8bc4\u4f30\u534f\u8bae\u7f3a\u4e4f\u65b9\u6cd5\u4e25\u8c28\u6027\u548c\u5168\u9762\u6027\uff0c\u5b58\u5728\u6570\u636e\u96c6\u504f\u5dee\u95ee\u9898\uff0c\u4e14\u5e7f\u6cdb\u4f7f\u7528\u7684CodeBLEU\u5ea6\u91cf\u5b58\u5728\u6807\u8bb0\u5316\u4e0d\u7cbe\u786e\u3001\u7ed3\u6784\u9650\u5236\u548c\u53c2\u8003\u591a\u6837\u6027\u4f4e\u7b49\u5173\u952e\u7f3a\u9677\u3002", "method": "\u63d0\u51faCFCEval\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efa\u65b0\u7684\u57fa\u51c6MLVBench\u6765\u7f13\u89e3\u6570\u636e\u96c6\u504f\u5dee\uff0c\u5e76\u5f15\u5165ELRM\u5ea6\u91cf\u6765\u8bc4\u4f30\u53c2\u8003\u4ee3\u7801\u4e0e\u751f\u6210\u4ee3\u7801\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u4ece\u7f16\u7a0b\u8d28\u91cf\u3001\u6f0f\u6d1e\u4fee\u590d\u80fd\u529b\u3001\u540e\u8f6c\u6362\u4fee\u590d\u80fd\u529b\u548c\u76f8\u5173\u6027\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u751f\u6210\u4ee3\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCFCEval\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u5b89\u5168\u65b9\u9762\uff0c\u5176ELRM\u5ea6\u91cf\u6bd4CodeBLEU\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\u3002", "conclusion": "CFCEval\u4e3a\u4ee3\u7801LLM\u8bc4\u4f30\u7684\u672a\u6765\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2512.06227", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06227", "abs": "https://arxiv.org/abs/2512.06227", "authors": ["Junyu Mao", "Anthony Hills", "Talia Tseriotou", "Maria Liakata", "Aya Shamir", "Dan Sayda", "Dana Atzil-Slonim", "Natalie Djohari", "Arpan Mandal", "Silke Roth", "Pamela Ugwudike", "Mahesan Niranjan", "Stuart E. Middleton"], "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety", "comment": null, "summary": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.", "AI": {"tldr": "\u63d0\u51faCFD\u6846\u67b6\uff0c\u901a\u8fc7\u591aLLM\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u6807\u6ce8\u5458\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc1\u636e\u4ea4\u6362\u8fbe\u6210\u5171\u8bc6\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u5728\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6548\u679c", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6307\u6807\uff08\u5982\u5fc3\u7406\u5065\u5eb7\u4e8b\u4ef6\u3001\u5728\u7ebf\u5b89\u5168\u98ce\u9669\uff09\u5bf9NLP\u4efb\u52a1\u5f88\u91cd\u8981\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u9700\u8981\u6709\u6548\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5", "method": "\u63d0\u51fa\u7f6e\u4fe1\u611f\u77e5\u7ec6\u7c92\u5ea6\u8fa9\u8bba\uff08CFD\uff09\u6846\u67b6\uff0c\u591a\u4e2aLLM\u4ee3\u7406\u6a21\u62df\u4eba\u7c7b\u6807\u6ce8\u5458\uff0c\u4ea4\u6362\u7ec6\u7c92\u5ea6\u8bc1\u636e\u8fbe\u6210\u5171\u8bc6\uff1b\u6bd4\u8f83\u591a\u79cdLLM\u6570\u636e\u589e\u5f3a\u65b9\u6cd5", "result": "CFD\u6846\u67b6\u5728\u6570\u636e\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u6700\u7a33\u5065\uff1b\u6570\u636e\u589e\u5f3a\u6301\u7eed\u6539\u5584\u4e0b\u6e38\u4efb\u52a1\uff1b\u901a\u8fc7\u8fa9\u8bba\u8f6c\u5f55\u672c\u589e\u5f3a\u7684\u7279\u5f81\u63d0\u5347\u6700\u5927\uff0c\u5728\u7ebf\u5b89\u5168\u4efb\u52a1\u4e0a\u6bd4\u975e\u589e\u5f3a\u57fa\u7ebf\u63d0\u534710.1%", "conclusion": "CFD\u6846\u67b6\u662f\u6709\u6548\u7684LLM\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u6307\u6807\u6807\u6ce8\u56f0\u96be\u7684\u60c5\u51b5\u4e0b", "topic": "agent analysis"}}
{"id": "2512.06401", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06401", "abs": "https://arxiv.org/abs/2512.06401", "authors": ["Zhenzhen Yang", "Chenhui Cui", "Tao Li", "Rubing Huang", "Nan Niu", "Dave Towey", "Shikai Guo"], "title": "LLMCFG-TGen: Using LLM-Generated Control Flow Graphs to Automatically Create Test Cases from Use Cases", "comment": null, "summary": "Appropriate test case generation is critical in software testing, significantly impacting the quality of the testing. Requirements-Based Test Generation (RBTG) derives test cases from software requirements, aiming to verify whether or not the system's behaviors align with user needs and expectations. Requirements are often documented in Natural Language (NL), with use-case descriptions being a popular method for capturing functional behaviors and interaction flows in a structured form. Large Language Models (LLMs) have shown strong potential for automating test generation directly from NL requirements. However, current LLM-based approaches may not provide comprehensive, non-redundant coverage. They may also fail to capture complex conditional logic in requirements, resulting in incomplete test cases. We propose a new approach that automatically generates test cases from NL use-case descriptions, called Test Generation based on LLM-generated Control Flow Graphs (LLMCFG-TGen). LLMCFG-TGen comprises three main steps: (1) An LLM transforms a use case into a structured CFG that encapsulates all potential branches; (2) The generated CFG is explored, and all complete execution paths are enumerated; and (3) The execution paths are then used to generate the test cases. To evaluate our proposed approach, we conducted a series of experiments. The results show that LLMs can effectively construct well-structured CFGs from NL use cases. Compared with the baseline methods, LLMCFG-TGen achieves full path coverage, improving completeness and ensuring clear and accurate test cases. Practitioner assessments confirm that LLMCFG-TGen produces logically consistent and comprehensive test cases, while substantially reducing manual effort. The findings suggest that coupling LLM-based semantic reasoning with structured modeling effectively bridges the gap between NL requirements and systematic test generation.", "AI": {"tldr": "\u63d0\u51faLLMCFG-TGen\u65b9\u6cd5\uff0c\u5229\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u7528\u4f8b\u751f\u6210\u63a7\u5236\u6d41\u56fe\uff0c\u518d\u679a\u4e3e\u5b8c\u6574\u6267\u884c\u8def\u5f84\u6765\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8986\u76d6\u4e0d\u5168\u9762\u3001\u5197\u4f59\u3001\u96be\u4ee5\u5904\u7406\u590d\u6742\u6761\u4ef6\u903b\u8f91\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "method": "LLMCFG-TGen\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a1) LLM\u5c06\u81ea\u7136\u8bed\u8a00\u7528\u4f8b\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u63a7\u5236\u6d41\u56fe\uff1b2) \u63a2\u7d22CFG\u5e76\u679a\u4e3e\u6240\u6709\u5b8c\u6574\u6267\u884c\u8def\u5f84\uff1b3) \u57fa\u4e8e\u6267\u884c\u8def\u5f84\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLM\u80fd\u6709\u6548\u4ece\u81ea\u7136\u8bed\u8a00\u7528\u4f8b\u6784\u5efa\u7ed3\u6784\u826f\u597d\u7684CFG\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cLLMCFG-TGen\u5b9e\u73b0\u4e86\u5b8c\u6574\u8def\u5f84\u8986\u76d6\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u5b8c\u6574\u6027\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u5c06LLM\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e0e\u7ed3\u6784\u5316\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u80fd\u6709\u6548\u5f25\u5408\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u4e0e\u7cfb\u7edf\u5316\u6d4b\u8bd5\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u751f\u6210\u903b\u8f91\u4e00\u81f4\u4e14\u5168\u9762\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "topic": "swe application"}}
{"id": "2512.06448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06448", "abs": "https://arxiv.org/abs/2512.06448", "authors": ["Takaaki Tateishi", "Yasuharu Katsuno"], "title": "Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models", "comment": "5 pages, 7 figures, to be published in ICSE 2026 NIER", "summary": "Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.\n  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u677f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u751f\u6210\u4ee3\u7801\u6a21\u677f\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5e2e\u52a9LLM\u5c06PL/I\u5b8f\u8fc7\u7a0b\u7ffb\u8bd1\u4e3a\u53ef\u7ef4\u62a4\u7684Java\u4ee3\u7801", "motivation": "\u4f01\u4e1a\u7cfb\u7edf\u73b0\u4ee3\u5316\u9700\u8981\u5c06PL/I\u7a0b\u5e8f\u7ffb\u8bd1\u4e3aJava\u7b49\u73b0\u4ee3\u8bed\u8a00\uff0c\u4f46PL/I\u5b8f\u8fc7\u7a0b\u4f5c\u4e3a\u5b57\u7b26\u4e32\u64cd\u4f5c\u7a0b\u5e8f\u751f\u6210PL/I\u4ee3\u7801\uff0c\u4f7f\u81ea\u52a8\u7ffb\u8bd1\u53d8\u5f97\u590d\u6742\u3002\u73b0\u6709LLM\u65b9\u6cd5\u96be\u4ee5\u5c06PL/I\u5b8f\u8fc7\u7a0b\u7ffb\u8bd1\u4e3a\u80fd\u91cd\u73b0\u539f\u59cbPL/I\u4ee3\u7801\u884c\u4e3a\u7684Java\u7a0b\u5e8f", "method": "\u63d0\u51fa\u6a21\u677f\u5316\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u7b26\u53f7\u6267\u884c\u751f\u6210\u4ee3\u7801\u6a21\u677f\uff08\u5e26\u6709\u547d\u540d\u5360\u4f4d\u7b26\u7684\u4ee3\u7801\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff1b2) \u5c06\u7b26\u53f7\u503c\u89c6\u4e3a\u5b8f\u751f\u6210\u4ee3\u7801\u7684\u4e00\u90e8\u5206\uff1b3) \u901a\u8fc7\u7b26\u53f7\u6267\u884c\u5b8f\u8fc7\u7a0b\u751f\u6210\u4ee3\u7801\u6a21\u677f\uff0c\u5e2e\u52a9LLM\u751f\u6210\u53ef\u8bfb\u53ef\u7ef4\u62a4\u7684Java\u4ee3\u7801", "result": "\u572810\u4e2aPL/I\u5b8f\u8fc7\u7a0b\u4e0a\u7684\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u6a21\u677f\u5316\u7684LLM\u7ffb\u8bd1\u6210\u529f\u751f\u6210\u4e86\u80fd\u91cd\u73b0\u5b8f\u751f\u6210PL/I\u7a0b\u5e8f\u884c\u4e3a\u7684Java\u7a0b\u5e8f", "conclusion": "\u6a21\u677f\u5316\u65b9\u6cd5\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u751f\u6210\u4ee3\u7801\u6a21\u677f\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u7ffb\u8bd1PL/I\u5b8f\u8fc7\u7a0b\u65f6\u7684\u56f0\u96be\uff0c\u80fd\u591f\u751f\u6210\u884c\u4e3a\u6b63\u786e\u7684\u53ef\u7ef4\u62a4Java\u4ee3\u7801", "topic": "code agent"}}
{"id": "2512.06902", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06902", "abs": "https://arxiv.org/abs/2512.06902", "authors": ["Fazle Rabbi", "Soumit Kanti Saha", "Tri Minh Triet Pham", "Song Wang", "Jinqiu Yang"], "title": "BabelCoder: Agentic Code Translation with Specification Alignment", "comment": "21 pages, 8 figures, 4 tables", "summary": "As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.", "AI": {"tldr": "BabelCoder\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u4ee3\u7801\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u3001\u6d4b\u8bd5\u548c\u7cbe\u70bc\u4e09\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u7684\u5206\u5de5\u5408\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u4ee3\u7801\u8fc1\u79fb\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u6f14\u8fdb\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u95f4\u5de5\u4f5c\uff0c\u7ecf\u5e38\u9700\u8981\u5c06\u4ee3\u7801\u4ece\u4e00\u79cd\u8bed\u8a00\u8fc1\u79fb\u5230\u53e6\u4e00\u79cd\u8bed\u8a00\u3002\u867d\u7136\u81ea\u52a8\u4ee3\u7801\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u4ecd\u6709\u5c40\u9650\uff0c\u672a\u80fd\u6709\u6548\u5229\u7528\u4ee3\u7801\u4e2d\u7684\u4e0a\u4e0b\u6587\u548c\u7ed3\u6784\u7ebf\u7d22\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u3002", "method": "BabelCoder\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\uff1a\u7ffb\u8bd1\u667a\u80fd\u4f53\u8d1f\u8d23\u751f\u6210\u4ee3\u7801\uff0c\u6d4b\u8bd5\u667a\u80fd\u4f53\u8d1f\u8d23\u9a8c\u8bc1\u6b63\u786e\u6027\uff0c\u7cbe\u70bc\u667a\u80fd\u4f53\u8d1f\u8d23\u4fee\u590d\u9519\u8bef\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u534f\u4f5c\u5de5\u4f5c\uff0c\u901a\u8fc7\u5206\u5de5\u5408\u4f5c\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e0e\u56db\u79cd\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0cBabelCoder\u572894%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u5e45\u5ea6\u4e3a0.5%-13.5%\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523094.16%\u3002", "conclusion": "BabelCoder\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u4ee3\u7801\u8fc1\u79fb\u7684\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2512.06915", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06915", "abs": "https://arxiv.org/abs/2512.06915", "authors": ["Kelin Fu", "Tianyu Liu", "Zeyu Shang", "Yingwei Ma", "Jian Yang", "Jiaheng Liu", "Kaigui Bian"], "title": "Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering", "comment": null, "summary": "Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.", "AI": {"tldr": "Multi-Docker-Eval \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u5316\u73af\u5883\u914d\u7f6e\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b40\u4e2a\u771f\u5b9e\u4ed3\u5e93\u548c9\u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u8bc4\u4f30\u53d1\u73b0\u5f53\u524dLLMs\u6210\u529f\u7387\u4f4e\uff08\u6700\u9ad837.7%\uff09\uff0c\u73af\u5883\u6784\u5efa\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u957f\u5ea6\u4e0d\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u81ea\u52a8\u5316\u73af\u5883\u914d\u7f6e\u662f\u6269\u5c55\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u7684\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u8861\u91cf\u6a21\u578b\u5728\u771f\u5b9e\u7ea6\u675f\u4e0b\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "method": "\u521b\u5efaMulti-Docker-Eval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b40\u4e2a\u771f\u5b9e\u4e16\u754c\u4ed3\u5e93\u548c9\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u6d4b\u91cf\u5b9e\u73b0\u53ef\u6267\u884c\u72b6\u6001\u7684\u6210\u529f\u7387\u4ee5\u53ca\u5728\u73b0\u5b9e\u7ea6\u675f\u4e0b\u7684\u6548\u7387\u3002\u5bf9\u6700\u5148\u8fdb\u7684LLMs\u548c\u4ee3\u7406\u6846\u67b6\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\u3002", "result": "1) \u5f53\u524d\u6a21\u578b\u6574\u4f53\u6210\u529f\u7387\u4f4e\uff08F2P\u6700\u9ad837.7%\uff09\uff0c\u73af\u5883\u6784\u5efa\u662f\u4e3b\u8981\u74f6\u9888\uff1b2) \u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u957f\u5ea6\u4e0d\u662f\u51b3\u5b9a\u6027\u56e0\u7d20\uff0c\u5f00\u6e90\u6a21\u578b\u5982DeepSeek-V3.1\u548cKimi-K2\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u6709\u7ade\u4e89\u529b\uff1b3) \u4ee3\u7406\u6846\u67b6\u548c\u7f16\u7a0b\u8bed\u8a00\u5bf9\u6210\u529f\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u3001\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6d41\u6c34\u7ebf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6307\u5bfc\u65b9\u9488\u3002", "topic": "swe benchmark"}}
{"id": "2512.07022", "categories": ["cs.SE", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.07022", "abs": "https://arxiv.org/abs/2512.07022", "authors": ["Genevieve Caumartin", "Glaucia Melo"], "title": "Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization", "comment": "Accepted at BoatSE 2026", "summary": "Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u4ee3\u7406\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67e5\u8be2\u91cd\u6784\u548c\u6458\u8981\u6280\u672f\uff0c\u5728\u6587\u4ef6\u7ea7bug\u5b9a\u4f4d\u4e2d\u5b9e\u73b0\u4e8635%\u7684\u6392\u540d\u63d0\u5347\uff0c\u4f18\u4e8e\u4f20\u7edfIRBL\u65b9\u6cd5\u548cSWE-agent\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u7684bug\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u672a\u6539\u53d8\u7684bug\u63cf\u8ff0\uff0c\u5e38\u5305\u542b\u566a\u58f0\u4fe1\u606f\u5bfc\u81f4\u68c0\u7d22\u51c6\u786e\u6027\u5dee\u3002LLM\u5728\u67e5\u8be2\u91cd\u6784\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5bf9\u4ee3\u7406\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u3001\u672a\u5fae\u8c03\u7684LLM\u4ecebug\u62a5\u544a\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff08\u5982\u6807\u8bc6\u7b26\u548c\u4ee3\u7801\u7247\u6bb5\uff09\uff0c\u8fdb\u884c\u68c0\u7d22\u524d\u67e5\u8be2\u91cd\u6784\u3002\u4ee3\u7406\u968f\u540e\u4f7f\u7528\u8fd9\u4e9b\u9884\u5904\u7406\u67e5\u8be2\u7f16\u6392BM25\u68c0\u7d22\uff0c\u5b9e\u73b0\u89c4\u6a21\u5316\u81ea\u52a8\u5316\u5b9a\u4f4d\u5de5\u4f5c\u6d41\u3002", "result": "\u4f7f\u7528\u6700\u4f73\u67e5\u8be2\u91cd\u6784\u6280\u672f\uff0c\u4ee3\u7406\u5728\u9996\u6b21\u6587\u4ef6\u68c0\u7d22\u4e2d\u6bd4BM25\u57fa\u7ebf\u83b7\u5f9735%\u7684\u6392\u540d\u63d0\u5347\uff0c\u6587\u4ef6\u68c0\u7d22\u6027\u80fd\u6bd4SWE-agent\u9ad8\u51fa22%\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u4ee3\u7406\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67e5\u8be2\u91cd\u6784\u548c\u6458\u8981\u6280\u672f\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u6587\u4ef6\u7ea7bug\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u8f6f\u4ef6\u4ed3\u5e93\u4e2d\u7684bug\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2512.06573", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.06573", "abs": "https://arxiv.org/abs/2512.06573", "authors": ["Onur Bilgin", "Abdullah As Sami", "Sriram Sai Vujjini", "John Licato"], "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion", "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5728LLM\u667a\u80fd\u4f53\u4e2d\u5f15\u5165\u4fe1\u5ff5\u9648\u8ff0\uff08\u4fe1\u5ff5\u76d2\uff09\u5982\u4f55\u5f71\u54cd\u5176\u884c\u4e3a\u548c\u8bf4\u670d\u529b\uff0c\u7279\u522b\u662f\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u573a\u666f\u4e2d\uff0c\u53d1\u73b0\u4fe1\u5ff5\u9648\u8ff0\u548c\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u4f1a\u663e\u8457\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u6539\u53d8\u503e\u5411\u548c\u8bf4\u670d\u6548\u679c\u3002", "motivation": "\u968f\u7740\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u548c\u51b3\u7b56\u5e94\u7528\u4e2d\u7684\u589e\u52a0\uff0c\u9700\u8981LLM\u667a\u80fd\u4f53\u5177\u5907\u7c7b\u4f3c\u547d\u9898\u4fe1\u5ff5\u7684\u80fd\u529b\u3002\u7814\u7a76\u8005\u60f3\u77e5\u9053\u5728\u63d0\u793a\u7a7a\u95f4\u4e2d\u5305\u542b\u4fe1\u5ff5\u9648\u8ff0\uff08\u4fe1\u5ff5\u76d2\uff09\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3001\u4fe1\u5ff5\u503e\u5411\u4ee5\u53ca\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u8bf4\u670d\u529b\uff0c\u540c\u65f6\u63a2\u7d22\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u63a2\u7d22\u4fe1\u5ff5\u9648\u8ff0\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8ba9\u667a\u80fd\u4f53\u5728\u63d0\u793a\u7a7a\u95f4\u4e2d\u7ef4\u62a4\u4fe1\u5ff5\u9648\u8ff0\uff08\u4fe1\u5ff5\u76d2\uff09\uff0c\u6d4b\u8bd5\u8fd9\u4e9b\u9648\u8ff0\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u76f8\u53cd\u89c2\u70b9\u7684\u62b5\u6297\u529b\u548c\u8bf4\u670d\u529b\uff0c\u5e76\u8003\u5bdf\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u8fa9\u8bba\u4e2d\u5904\u4e8e\u5c11\u6570\u6d3e\uff08\u540c\u4faa\u538b\u529b\uff09\u573a\u666f\u4e0b\u7684\u4fe1\u5ff5\u6539\u53d8\u53ef\u80fd\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u786e\u5b9e\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u4fe1\u5ff5\u6539\u53d8\u7684\u63a5\u53d7\u7a0b\u5ea6\uff1b2\uff09\u7eb3\u5165\u4fe1\u5ff5\u9648\u8ff0\u53ca\u5176\u5f3a\u5ea6\u4f1a\u5f71\u54cd\u667a\u80fd\u4f53\u5bf9\u76f8\u53cd\u89c2\u70b9\u7684\u62b5\u6297\u529b\u548c\u8bf4\u670d\u529b\uff1b3\uff09\u5728\u8fa9\u8bba\u4e2d\u88ab\u76f8\u53cd\u89c2\u70b9\u5305\u56f4\uff08\u540c\u4faa\u538b\u529b\uff09\u65f6\uff0c\u4fe1\u5ff5\u9648\u8ff0\u4f1a\u5f71\u54cd\u4fe1\u5ff5\u6539\u53d8\u7684\u53ef\u80fd\u6027\u3002\u7ed3\u679c\u8bc1\u660e\u4e86\u4fe1\u5ff5\u76d2\u6280\u672f\u5728\u63a8\u7406\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u4fe1\u5ff5\u76d2\u6280\u672f\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u663e\u8457\u5f71\u54cdLLM\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u76f8\u5173\u884c\u4e3a\u3002\u5f00\u653e\u5fc3\u6001\u6307\u4ee4\u548c\u4fe1\u5ff5\u9648\u8ff0\u7684\u7eb3\u5165\u4f1a\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u8bf4\u670d\u529b\u548c\u4fe1\u5ff5\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u573a\u666f\u4e2d\u3002\u8fd9\u4e3a\u6784\u5efa\u66f4\u590d\u6742\u7684\u4fe1\u5ff5\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2512.06653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06653", "abs": "https://arxiv.org/abs/2512.06653", "authors": ["Hengzhi Lan", "Yue Yu", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Ting Bai"], "title": "LightSearcher: Efficient DeepSearch via Experiential Memory", "comment": "10 pages, 5 figures", "summary": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.", "AI": {"tldr": "LightSearcher\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11DeepSearch\u8303\u5f0f\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u6b21\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684RL\u9a71\u52a8\u7684DeepSearch\u7cfb\u7edf\u5b58\u5728\u51c6\u786e\u6027\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff1a\u9891\u7e41\u8c03\u7528\u5916\u90e8\u641c\u7d22\u5de5\u5177\u53ef\u4ee5\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4f46\u4f1a\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6548\u7387\u964d\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5e73\u8861\u8fd9\u79cd\u56fa\u6709\u7684\u6743\u8861\u3002", "method": "1) \u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u63a8\u7406\u8f68\u8ff9\u6765\u5b66\u4e60\u6587\u672c\u7ecf\u9a8c\u8bb0\u5fc6\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6210\u529f\u63a8\u7406\u6a21\u5f0f\u6458\u8981\uff1b2) \u91c7\u7528\u81ea\u9002\u5e94\u5956\u52b1\u5851\u9020\u673a\u5236\uff0c\u4ec5\u5728\u6b63\u786e\u7b54\u6848\u573a\u666f\u4e2d\u60e9\u7f5a\u5197\u4f59\u7684\u5de5\u5177\u8c03\u7528\u3002", "result": "\u5728\u56db\u4e2a\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLightSearcher\u4fdd\u6301\u4e86\u4e0eSOTA\u57fa\u7ebfReSearch\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5c06\u641c\u7d22\u5de5\u5177\u8c03\u7528\u51cf\u5c11\u4e8639.6%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e8648.6%\uff0ctoken\u6d88\u8017\u51cf\u5c11\u4e8621.2%\u3002", "conclusion": "LightSearcher\u901a\u8fc7\u521b\u65b0\u7684\u8bb0\u5fc6\u5b66\u4e60\u548c\u5956\u52b1\u673a\u5236\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86DeepSearch\u8303\u5f0f\u4e2d\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5916\u90e8\u77e5\u8bc6\u83b7\u53d6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.06204", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06204", "abs": "https://arxiv.org/abs/2512.06204", "authors": ["Rodney Lafuente-Mercado", "Daniela Rus", "T. Konstantin Rusch"], "title": "Quantifying Memory Use in Reinforcement Learning with Temporal Range", "comment": null, "summary": "How much does a trained RL policy actually use its past observations? We propose \\emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\\partial y_s/\\partial x_t\\in\\mathbb{R}^{c\\times d}$ averaged over final timesteps $s\\in\\{t+1,\\dots,T\\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.", "AI": {"tldr": "\u63d0\u51faTemporal Range\u6307\u6807\uff0c\u901a\u8fc7\u53cd\u5411\u81ea\u52a8\u5fae\u5206\u8ba1\u7b97\u7b56\u7565\u8f93\u51fa\u5bf9\u8fc7\u53bb\u89c2\u6d4b\u7684\u654f\u611f\u6027\uff0c\u91cf\u5316RL\u7b56\u7565\u5bf9\u5386\u53f2\u4fe1\u606f\u7684\u4f9d\u8d56\u7a0b\u5ea6", "motivation": "\u7814\u7a76\u8bad\u7ec3\u597d\u7684RL\u7b56\u7565\u5b9e\u9645\u4f7f\u7528\u591a\u5c11\u8fc7\u53bb\u89c2\u6d4b\u4fe1\u606f\uff0c\u7f3a\u4e4f\u91cf\u5316\u7b56\u7565\u5386\u53f2\u4f9d\u8d56\u7a0b\u5ea6\u7684\u6307\u6807", "method": "\u63d0\u51faTemporal Range\u6307\u6807\uff0c\u901a\u8fc7Jacobian\u5757\u8ba1\u7b97\u8f93\u51fa\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u654f\u611f\u6027\uff0c\u7528\u5e45\u5ea6\u52a0\u6743\u5e73\u5747\u6ede\u540e\u65f6\u95f4\u603b\u7ed3\u65f6\u95f4\u5f71\u54cd\u5206\u5e03", "result": "\u5728\u8bca\u65ad\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cTemporal Range\u80fd\u51c6\u786e\u53cd\u6620\u4efb\u52a1\u9700\u6c42\uff1a\u5b8c\u5168\u89c2\u5bdf\u63a7\u5236\u4e2d\u8f83\u5c0f\uff0cCopy-k\u4e2d\u4e0e\u771f\u5b9e\u6ede\u540e\u5339\u914d\uff0c\u4e0e\u7a97\u53e3\u6d88\u878d\u5b9e\u9a8c\u4e00\u81f4", "conclusion": "Temporal Range\u63d0\u4f9b\u5b9e\u7528\u7684\u5e8f\u5217\u7ea7\u8bb0\u5fc6\u4f9d\u8d56\u5ea6\u91cf\uff0c\u53ef\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u667a\u80fd\u4f53\u548c\u73af\u5883\uff0c\u9009\u62e9\u6700\u77ed\u8db3\u591f\u4e0a\u4e0b\u6587", "topic": "agent analysis"}}
{"id": "2512.06710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06710", "abs": "https://arxiv.org/abs/2512.06710", "authors": ["Zairah Mustahsan", "Abel Lim", "Megna Anand", "Saahil Jain", "Bryan McCann"], "title": "Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation", "comment": null, "summary": "As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u7ec4\u5185\u76f8\u5173\u7cfb\u6570(ICC)\u6765\u8bc4\u4f30AI\u4ee3\u7406\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u533a\u5206\u4efb\u52a1\u96be\u5ea6\u548c\u4ee3\u7406\u4e0d\u4e00\u81f4\u6027\uff0c\u4f7f\u8bc4\u4f30\u66f4\u53ef\u9760", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u8bc4\u4f30\u53ea\u62a5\u544a\u5355\u4e00\u51c6\u786e\u7387\u6570\u5b57\uff0c\u63a9\u76d6\u4e86\u7ed3\u679c\u80cc\u540e\u7684\u65b9\u5dee\uff0c\u65e0\u6cd5\u533a\u5206\u771f\u5b9e\u80fd\u529b\u63d0\u5347\u4e0e\u968f\u673a\u91c7\u6837\u8fd0\u6c14\uff0c\u5bfc\u81f4\u4e0b\u6e38\u7cfb\u7edf\u8106\u5f31", "method": "\u91c7\u7528\u6d4b\u91cf\u79d1\u5b66\u4e2d\u7684\u7ec4\u5185\u76f8\u5173\u7cfb\u6570(ICC)\uff0c\u5c06\u89c2\u5bdf\u5230\u7684\u65b9\u5dee\u5206\u89e3\u4e3a\u67e5\u8be2\u95f4\u65b9\u5dee(\u4efb\u52a1\u96be\u5ea6)\u548c\u67e5\u8be2\u5185\u65b9\u5dee(\u4ee3\u7406\u4e0d\u4e00\u81f4\u6027)\uff0c\u5728GAIA\u548cFRAMES\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30", "result": "ICC\u968f\u4efb\u52a1\u7ed3\u6784\u53d8\u5316\u663e\u8457\uff1a\u63a8\u7406\u548c\u68c0\u7d22\u4efb\u52a1(FRAMES)ICC=0.4955-0.7118\uff0c\u4ee3\u7406\u4efb\u52a1(GAIA)ICC=0.304-0.774\u3002\u7ed3\u6784\u5316\u4efb\u52a1\u9700\u89818-16\u6b21\u8bd5\u9a8c\u6536\u655b\uff0c\u590d\u6742\u63a8\u7406\u9700\u8981\u226532\u6b21", "conclusion": "\u5efa\u8bae\u5c06\u51c6\u786e\u7387\u4e0eICC\u548c\u67e5\u8be2\u5185\u65b9\u5dee\u4e00\u8d77\u4f5c\u4e3a\u6807\u51c6\u62a5\u544a\u5b9e\u8df5\uff0c\u66f4\u65b0\u8bc4\u4f30\u5361\u7247\uff0c\u4f7f\u8bc4\u4f30\u7a33\u5b9a\u6027\u53ef\u89c1\uff0c\u5c06\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4ece\u4e0d\u900f\u660e\u7684\u6392\u884c\u699c\u7ade\u4e89\u8f6c\u53d8\u4e3a\u53ef\u4fe1\u8d56\u7684\u5b9e\u9a8c\u79d1\u5b66", "topic": "agent analysis"}}
{"id": "2512.06688", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06688", "abs": "https://arxiv.org/abs/2512.06688", "authors": ["Bowen Jiang", "Yuan Yuan", "Maohao Shen", "Zhuoqun Hao", "Zhangchen Xu", "Zichen Chen", "Ziyi Liu", "Anvesh Rao Vijjini", "Jiashu He", "Hanchao Yu", "Radha Poovendran", "Gregory Wornell", "Lyle Ungar", "Dan Roth", "Sihao Chen", "Camillo Jose Taylor"], "title": "PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory", "comment": "Data is available at https://huggingface.co/datasets/bowen-upenn/PersonaMem-v2", "summary": "Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.\n  In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.", "AI": {"tldr": "PersonaMem-v2\u662f\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u4e2a\u6027\u5316\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u4e2a\u771f\u5b9e\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u8986\u76d6300+\u573a\u666f\u548c20000+\u7528\u6237\u504f\u597d\u3002\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\uff0cQwen3-4B\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-5\u8fbe\u523053%\u51c6\u786e\u7387\uff0c\u800c\u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\u4ee516\u500d\u66f4\u5c11\u7684\u8f93\u5165token\u5b9e\u73b055%\u7684SOTA\u51c6\u786e\u7387\u3002", "motivation": "\u4e2a\u6027\u5316\u662fAI\u80fd\u529b\u548c\u5bf9\u9f50\u7684\u4e0b\u4e00\u4e2a\u91cd\u8981\u91cc\u7a0b\u7891\u3002\u5f53\u524d\u524d\u6cbfLLM\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u4ec537-48%\u51c6\u786e\u7387\uff09\uff0c\u5c3d\u7ba1\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4ecd\u662f\u74f6\u9888\u3002\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u4e2a\u6027\u5316\u667a\u80fd\u3002", "method": "1) \u521b\u5efaPersonaMem-v2\u6570\u636e\u96c6\uff0c\u6a21\u62df1000\u4e2a\u771f\u5b9e\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\uff0c\u5305\u542b300+\u573a\u666f\u300120000+\u7528\u6237\u504f\u597d\u548c128k-token\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b2) \u4f7f\u7528\u5f3a\u5316\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff1b3) \u5f00\u53d1\u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\uff0c\u7ef4\u62a4\u5355\u4e00\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u968f\u7528\u6237\u4ea4\u4e92\u589e\u957f\u3002", "result": "1) \u524d\u6cbfLLM\u5728\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u4ec537-48%\uff1b2) \u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\uff0cQwen3-4B\u8fbe\u523053%\u51c6\u786e\u7387\uff0c\u8d85\u8d8aGPT-5\uff1b3) \u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\u5b9e\u73b055%\u7684SOTA\u51c6\u786e\u7387\uff0c\u4ec5\u4f7f\u75282k-token\u8bb0\u5fc6\u800c\u975e\u5b8c\u6574\u768432k\u5bf9\u8bdd\u5386\u53f2\uff0c\u8f93\u5165token\u51cf\u5c1116\u500d\u3002", "conclusion": "PersonaMem-v2\u6570\u636e\u96c6\u5bf9\u4e2a\u6027\u5316\u7814\u7a76\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u667a\u80fd\u8bb0\u5fc6\u6846\u67b6\u4e3a\u771f\u5b9e\u4e16\u754c\u4e2a\u6027\u5316\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002\u5f3a\u5316\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u9690\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2512.06716", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06716", "abs": "https://arxiv.org/abs/2512.06716", "authors": ["Zhibo Liang", "Tianze Hu", "Zaiye Chen", "Mingjie Tang"], "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents", "comment": null, "summary": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8ba4\u77e5\u63a7\u5236\u67b6\u6784\uff08CCA\uff09\uff0c\u901a\u8fc7\u610f\u56fe\u56fe\u548c\u5206\u5c42\u88c1\u51b3\u5668\u6784\u5efa\u53cc\u5c42\u9632\u5fa1\u7cfb\u7edf\uff0c\u6709\u6548\u62b5\u5fa1\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5728\u5b89\u5168\u3001\u529f\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3bLLM\u4ee3\u7406\u5bf9\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u73b0\u6709\u9632\u5fa1\u673a\u5236\u5728\u5b89\u5168\u4e0e\u529f\u80fd\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff0c\u5bfc\u81f4\u6076\u610f\u5de5\u5177\u8c03\u7528\u548c\u4ee3\u7406\u76ee\u6807\u504f\u79bb\u3002\u73b0\u6709\u9632\u5fa1\u67b6\u6784\u5206\u6563\uff0c\u65e0\u6cd5\u5728\u6574\u4e2a\u4efb\u52a1\u6267\u884c\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u5b8c\u6574\u5b8c\u6574\u6027\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u63a7\u5236\u67b6\u6784\uff08CCA\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u652f\u67f1\uff1a1\uff09\u901a\u8fc7\u9884\u751f\u6210\u7684\"\u610f\u56fe\u56fe\"\u5b9e\u73b0\u4e3b\u52a8\u63a7\u5236\u6d41\u548c\u6570\u636e\u6d41\u5b8c\u6574\u6027\u6267\u884c\uff1b2\uff09\u521b\u65b0\u7684\"\u5206\u5c42\u88c1\u51b3\u5668\"\uff0c\u5728\u68c0\u6d4b\u5230\u504f\u5dee\u65f6\u57fa\u4e8e\u591a\u7ef4\u8bc4\u5206\u542f\u52a8\u6df1\u5ea6\u63a8\u7406\uff0c\u4e13\u95e8\u5e94\u5bf9\u590d\u6742\u6761\u4ef6\u653b\u51fb\u3002", "result": "\u5728AgentDojo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCCA\u4e0d\u4ec5\u80fd\u6709\u6548\u62b5\u5fa1\u6311\u6218\u5176\u4ed6\u5148\u8fdb\u9632\u5fa1\u65b9\u6cd5\u7684\u590d\u6742\u653b\u51fb\uff0c\u8fd8\u80fd\u5728\u4e0d\u59a5\u534f\u5b89\u5168\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u663e\u8457\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u591a\u7ef4\u6743\u8861\u95ee\u9898\u3002", "conclusion": "CCA\u901a\u8fc7\u5168\u751f\u547d\u5468\u671f\u8ba4\u77e5\u76d1\u7763\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u6709\u6548\u9632\u5fa1\uff0c\u5728\u5b89\u5168\u3001\u529f\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u8fbe\u6210\u5e73\u8861\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u5fa1\u673a\u5236\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2512.07404", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07404", "abs": "https://arxiv.org/abs/2512.07404", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "title": "Do LLMs Trust the Code They Write?", "comment": null, "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86LLM\u5185\u90e8\u662f\u5426\u7f16\u7801\u4e86\u4ee3\u7801\u6b63\u786e\u6027\u7684\u8868\u793a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6b63\u786e\u548c\u9519\u8bef\u4ee3\u7801\u7684\u9690\u85cf\u72b6\u6001\u6765\u63d0\u53d6\u8fd9\u79cd\u8868\u793a\uff0c\u5e76\u5229\u7528\u5b83\u6765\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\uff0c\u65e0\u9700\u6267\u884c\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5f88\u6709\u6548\uff0c\u4f46\u7ecf\u5e38\u8f93\u51fa\u9519\u8bef\u4ee3\u7801\u3002\u4e00\u4e2a\u539f\u56e0\u662f\u6a21\u578b\u8f93\u51fa\u6982\u7387\u4e0e\u6b63\u786e\u6027\u76f8\u5173\u6027\u4e0d\u5f3a\uff0c\u4e14\u53ea\u53cd\u6620\u751f\u6210\u8fc7\u7a0b\u7684\u6700\u7ec8\u8f93\u51fa\u3002\u53d7LLM\u5185\u90e8\u7f16\u7801\"\u771f\u5b9e\u6027\"\u7b49\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u672c\u6587\u63a2\u7d22LLM\u662f\u5426\u7c7b\u4f3c\u5730\u8868\u793a\u4ee3\u7801\u6b63\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u540c\u4e00\u7f16\u7a0b\u4efb\u52a1\u4e0b\u6b63\u786e\u548c\u9519\u8bef\u4ee3\u7801\u5bf9\u7684\u9690\u85cf\u72b6\u6001\uff0c\u8bc6\u522bLLM\u5185\u90e8\u7684\u6b63\u786e\u6027\u8868\u793a\u3002\u5728\u56db\u4e2aLLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5229\u7528\u63d0\u53d6\u7684\u6b63\u786e\u6027\u8868\u793a\u6765\u6539\u8fdb\u4ee3\u7801\u9009\u62e9\u3002", "result": "\u5229\u7528\u63d0\u53d6\u7684\u5185\u90e8\u6b63\u786e\u6027\u8868\u793a\u4f18\u4e8e\u6807\u51c6\u7684\u5bf9\u6570\u4f3c\u7136\u6392\u5e8f\u548c\u8bed\u8a00\u5316\u6a21\u578b\u7f6e\u4fe1\u5ea6\u3002\u8fd9\u79cd\u5185\u90e8\u6b63\u786e\u6027\u4fe1\u53f7\u53ef\u7528\u4e8e\u9009\u62e9\u66f4\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u6837\u672c\uff0c\u65e0\u9700\u6267\u884c\u6d4b\u8bd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u5185\u90e8\u8868\u793a\u6765\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\uff0c\u4f7fLLM\u66f4\u53ef\u9760\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u7684\u4fe1\u5fc3\u3002", "topic": "code agent"}}
{"id": "2512.06721", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06721", "abs": "https://arxiv.org/abs/2512.06721", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "comment": null, "summary": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "AI": {"tldr": "ProAgent\uff1a\u9996\u4e2a\u7aef\u5230\u7aef\u4e3b\u52a8\u4ee3\u7406\u7cfb\u7edf\uff0c\u5229\u7528\u591a\u6a21\u6001\u611f\u77e5\u548cLLM\u63a8\u7406\u63d0\u4f9b\u4e3b\u52a8\u534f\u52a9\uff0c\u5728AR\u773c\u955c\u4e0a\u5b9e\u73b0\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u4e3b\u8981\u9075\u5faa\u88ab\u52a8\u8303\u5f0f\uff0c\u4f9d\u8d56\u7528\u6237\u660e\u786e\u6307\u4ee4\u542f\u52a8\u670d\u52a1\uff0c\u589e\u52a0\u4e86\u7528\u6237\u7684\u7269\u7406\u548c\u8ba4\u77e5\u8d1f\u62c5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u611f\u77e5\u73af\u5883\u5e76\u63d0\u4f9b\u534f\u52a9\u7684\u7cfb\u7edf\u3002", "method": "1) \u91c7\u7528\u4e3b\u52a8\u5bfc\u5411\u7684\u4e0a\u4e0b\u6587\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u9700\u5206\u5c42\u611f\u77e5\u6301\u7eed\u611f\u77e5\u73af\u5883\uff0c\u63d0\u53d6\u5305\u542b\u611f\u5b98\u548c\u4e2a\u4eba\u7279\u5f81\u7684\u5206\u5c42\u4e0a\u4e0b\u6587\uff1b2) \u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e3b\u52a8\u63a8\u7406\u5668\uff0c\u5c06\u4e0a\u4e0b\u6587\u6620\u5c04\u5230\u7528\u6237\u9700\u6c42\u548c\u5de5\u5177\u8c03\u7528\uff0c\u63d0\u4f9b\u4e3b\u52a8\u534f\u52a9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u5e73\u53f0\u3001\u516c\u5171\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\u8bc4\u4f30\uff0cProAgent\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5b9e\u73b0\u4e86\uff1a\u4e3b\u52a8\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad833.4%\uff0c\u5de5\u5177\u8c03\u7528F1\u5206\u6570\u63d0\u9ad816.8%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ProAgent\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u4e3b\u52a8\u4ee3\u7406\u7cfb\u7edf\uff0c\u5728AR\u773c\u955c\u4e0a\u5b9e\u73b0\u5e76\u5c55\u793a\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6807\u5fd7\u7740\u5411\u4e3b\u52a8\u52a9\u624b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2512.06244", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.06244", "abs": "https://arxiv.org/abs/2512.06244", "authors": ["Caleb Ju", "Guanghui Lan"], "title": "Auto-exploration for online reinforcement learning", "comment": "35 pages (9 appendix), 1 figure. Comments are welcome", "summary": "The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(\u03b5^{-2})$ sample complexity to solve to $\u03b5$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.", "AI": {"tldr": "\u63d0\u51fa\u5177\u6709\u81ea\u52a8\u63a2\u7d22\u529f\u80fd\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u5148\u9a8c\u53c2\u6570\u77e5\u8bc6\uff0c\u5728\u8868\u683c\u548c\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u5b9e\u73b0O(\u03b5\u207b\u00b2)\u6837\u672c\u590d\u6742\u5ea6", "motivation": "\u73b0\u6709RL\u7b97\u6cd5\u9700\u8981\u5047\u8bbe\u5145\u5206\u63a2\u7d22\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8fd9\u5bfc\u81f4\u7b97\u6cd5\u4e0d\u53ef\u5b9e\u73b0\u4e14\u6027\u80fd\u6b21\u4f18\u3002\u9700\u8981\u53c2\u6570\u65e0\u5173\u7684\u81ea\u52a8\u63a2\u7d22\u65b9\u6cd5\u6765\u89e3\u51b3\u63a2\u7d22-\u5229\u7528\u56f0\u5883", "method": "\u63d0\u51fa\u4e24\u79cd\u53d8\u4f53\uff1a\u8868\u683c\u8bbe\u7f6e\u548c\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u3002\u91c7\u7528\u52a8\u6001\u6df7\u5408\u65f6\u95f4\u3001\u6298\u6263\u72b6\u6001\u5206\u5e03\u91c7\u6837\u3001\u9c81\u68d2\u68af\u5ea6\u4f30\u8ba1\u5668\u548c\u4f18\u52bf\u95f4\u9699\u51fd\u6570\u7b49\u65b0\u7b97\u6cd5\u521b\u65b0", "result": "\u5728\u5b58\u5728\u63a2\u7d22\u6700\u4f18\u7b56\u7565\u7684\u5047\u8bbe\u4e0b\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u8fbe\u5230O(\u03b5\u207b\u00b2)\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e14\u590d\u6742\u5ea6\u4e0d\u5305\u542b\u53ef\u80fd\u4efb\u610f\u5927\u7684\u7b97\u6cd5\u76f8\u5173\u53c2\u6570", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u63a2\u7d22\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edfRL\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u65e0\u5173\u3001\u6613\u4e8e\u5b9e\u73b0\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u9ad8\u6548\u63a2\u7d22", "topic": "agentic reinforcement learning"}}
{"id": "2512.06749", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.06749", "abs": "https://arxiv.org/abs/2512.06749", "authors": ["Ming Ma", "Jue Zhang", "Fangkai Yang", "Yu Kang", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems", "comment": null, "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.", "AI": {"tldr": "DoVer\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e72\u9884\u7684\u8c03\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u9a8c\u8bc1\u5047\u8bbe\uff08\u5982\u7f16\u8f91\u6d88\u606f\u3001\u4fee\u6539\u8ba1\u5212\uff09\u6765\u8c03\u8bd5LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u76f8\u6bd4\u4f20\u7edf\u65e5\u5fd7\u8c03\u8bd5\u80fd\u66f4\u6709\u6548\u5730\u4fee\u590d\u5931\u8d25\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8c03\u8bd5\u56f0\u96be\uff0c\u4f20\u7edf\u65e5\u5fd7\u8c03\u8bd5\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u4ec5\u57fa\u4e8e\u65e5\u5fd7\u7684\u8c03\u8bd5\u7f3a\u4e4f\u9a8c\u8bc1\uff0c\u4ea7\u751f\u672a\u7ecf\u6d4b\u8bd5\u7684\u5047\u8bbe\uff1b2) \u5355\u6b65\u6216\u5355\u667a\u80fd\u4f53\u5f52\u56e0\u5f80\u5f80\u4e0d\u51c6\u786e\uff0c\u56e0\u4e3a\u591a\u4e2a\u4e0d\u540c\u7684\u5e72\u9884\u53ef\u80fd\u90fd\u80fd\u72ec\u7acb\u4fee\u590d\u5931\u8d25\u4efb\u52a1\u3002", "method": "DoVer\u662f\u4e00\u4e2a\u5e72\u9884\u9a71\u52a8\u7684\u8c03\u8bd5\u6846\u67b6\uff0c\u5c06\u5047\u8bbe\u751f\u6210\u4e0e\u4e3b\u52a8\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\uff08\u5982\u7f16\u8f91\u6d88\u606f\u3001\u4fee\u6539\u8ba1\u5212\uff09\u6765\u9a8c\u8bc1\u6216\u53cd\u9a73\u5931\u8d25\u5047\u8bbe\u3002\u6846\u67b6\u4e0d\u8ffd\u6c42\u5f52\u56e0\u51c6\u786e\u6027\uff0c\u800c\u662f\u5173\u6ce8\u7cfb\u7edf\u662f\u5426\u80fd\u89e3\u51b3\u5931\u8d25\u6216\u5411\u4efb\u52a1\u6210\u529f\u53d6\u5f97\u53ef\u91cf\u5316\u7684\u8fdb\u5c55\u3002", "result": "\u5728Magnetic-One\u667a\u80fd\u4f53\u6846\u67b6\u4e0a\uff0c\u4f7f\u7528GAIA\u548cAssistantBench\u6570\u636e\u96c6\uff0cDoVer\u5c0618-28%\u7684\u5931\u8d25\u8bd5\u9a8c\u8f6c\u4e3a\u6210\u529f\uff0c\u5b9e\u73b0\u9ad8\u8fbe16%\u7684\u91cc\u7a0b\u7891\u8fdb\u5c55\uff0c\u9a8c\u8bc1\u6216\u53cd\u9a7330-60%\u7684\u5931\u8d25\u5047\u8bbe\u3002\u5728GSMPlus\u6570\u636e\u96c6\u548cAG2\u6846\u67b6\u4e0a\uff0cDoVer\u6062\u590d\u4e8649%\u7684\u5931\u8d25\u8bd5\u9a8c\u3002", "conclusion": "\u5e72\u9884\u662f\u63d0\u9ad8\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5b9e\u7528\u673a\u5236\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u7684\u8c03\u8bd5\u65b9\u6cd5\u3002DoVer\u5c55\u793a\u4e86\u5e72\u9884\u9a71\u52a8\u8c03\u8bd5\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.06250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06250", "abs": "https://arxiv.org/abs/2512.06250", "authors": ["Chris Tava"], "title": "Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning", "comment": "7 pages", "summary": "Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\\times$16 to 128$\\times$128 $\\times$ 10 unique mazes $\\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\\% threshold baselines. Results show 23-55\\% improvements in completion time, 83\\% reduction in runtime variance, and 71\\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\\% improvement for 16$\\times$16 mazes, 34\\% for 32$\\times$32, and 55\\% for 64$\\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u7b56\u7565\u5207\u6362\u65b9\u6cd5\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u8ff7\u5bab\u5bfc\u822a\u4e2d\u52a8\u6001\u5207\u6362\u7cfb\u7edf\u63a2\u7d22\u548c\u76ee\u6807\u5bfc\u5411\u8def\u5f84\u89c4\u5212\u7b56\u7565\uff0c\u76f8\u6bd4\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u667a\u80fd\u4f53\u9700\u8981\u591a\u79cd\u7b56\u7565\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u786e\u5b9a\u4f55\u65f6\u5207\u6362\u7b56\u7565\u4ecd\u5177\u6311\u6218\u3002\u73b0\u6709\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5\u4e0d\u591f\u7075\u6d3b\uff0c\u9700\u8981\u9886\u57df\u77e5\u8bc6\u548c\u624b\u5de5\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "method": "\u4f7f\u7528Q-learning\u5b66\u4e60\u4e24\u4e2a\u6b63\u4ea4\u5bfc\u822a\u7b56\u7565\uff08\u7cfb\u7edf\u63a2\u7d22\u548c\u76ee\u6807\u5bfc\u5411\u8def\u5f84\u89c4\u5212\uff09\u4e4b\u95f4\u7684\u5207\u6362\u9608\u503c\u3002\u5c06\u72b6\u6001\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u8986\u76d6\u7387\u548c\u8ddd\u79bb\u6876\uff0c\u57fa\u4e8e\u8986\u76d6\u767e\u5206\u6bd4\u548c\u76ee\u6807\u8ddd\u79bb\u52a8\u6001\u8c03\u6574\u5207\u6362\u884c\u4e3a\uff0820-60%\u9608\u503c\u8303\u56f4\uff09\u3002\u4ec5\u9700\u8ff7\u5bab\u5c3a\u5bf8\u548c\u76ee\u6807\u4f4d\u7f6e\u7b49\u6700\u5c0f\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5728240\u4e2a\u6d4b\u8bd5\u914d\u7f6e\uff084\u79cd\u8ff7\u5bab\u5c3a\u5bf8\u00d710\u4e2a\u72ec\u7279\u8ff7\u5bab\u00d76\u79cd\u667a\u80fd\u4f53\u53d8\u4f53\uff09\u4e2d\uff0c\u81ea\u9002\u5e94\u9608\u503c\u5b66\u4e60\u4f18\u4e8e\u5355\u7b56\u7565\u667a\u80fd\u4f53\u548c\u56fa\u5b9a40%\u9608\u503c\u57fa\u7ebf\u3002\u5b8c\u6210\u65f6\u95f4\u63d0\u534723-55%\uff0c\u8fd0\u884c\u65f6\u65b9\u5dee\u964d\u4f4e83%\uff0c\u6700\u574f\u60c5\u51b5\u6539\u558471%\u3002\u6027\u80fd\u589e\u76ca\u968f\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u5b66\u4e60\u7b56\u7565\u5207\u6362\u9608\u503c\uff0c\u5b9e\u73b0\u52a8\u6001\u7b56\u7565\u8f6c\u6362\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u6216\u624b\u5de5\u542f\u53d1\u5f0f\u3002\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u7684\u4ef7\u503c\u968f\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07501", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07501", "abs": "https://arxiv.org/abs/2512.07501", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "comment": null, "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "AI": {"tldr": "AutoICE\u4f7f\u7528LLM\u9a71\u52a8\u7684\u8fdb\u5316\u641c\u7d22\u5408\u6210\u53ef\u9a8c\u8bc1\u7684C\u4ee3\u7801\uff0c\u901a\u8fc7\u591a\u6837\u4e2a\u4f53\u521d\u59cb\u5316\u3001\u534f\u4f5c\u4ea4\u53c9\u548c\u81ea\u53cd\u601d\u53d8\u5f02\u6765\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u5e76\u53d1\u73b0\u9690\u542b\u77e5\u8bc6\uff0c\u5728\u4ee3\u7801\u9a8c\u8bc1\u6210\u529f\u7387\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u81ea\u52a8\u5408\u6210\u53ef\u9a8c\u8bc1\u4ee3\u7801\u80fd\u786e\u4fdd\u8f6f\u4ef6\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u964d\u4f4e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u91c7\u7528\u95e8\u69db\u3002\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u9519\u8bef\uff0c\u4e14\u96be\u4ee5\u6709\u6548\u5f62\u5f0f\u5316\u9690\u542b\u77e5\u8bc6\u3002", "method": "\u63d0\u51faAutoICE\uff0c\u4e00\u79cdLLM\u9a71\u52a8\u7684\u8fdb\u5316\u641c\u7d22\u65b9\u6cd5\uff0c\u5305\u542b\uff1a1) \u591a\u6837\u4e2a\u4f53\u521d\u59cb\u5316\u5b9e\u73b0\u591a\u6837\u8fed\u4ee3\u66f4\u65b0\uff1b2) \u534f\u4f5c\u4ea4\u53c9\u51cf\u5c11\u5355\u667a\u80fd\u4f53\u8fed\u4ee3\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\uff1b3) \u81ea\u53cd\u601d\u53d8\u5f02\u4fc3\u8fdb\u53d1\u73b0\u9690\u542b\u77e5\u8bc6\u3002", "result": "AutoICE\u6210\u529f\u9a8c\u8bc1\u4e8690.36%\u7684\u4ee3\u7801\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u5728\u5f00\u53d1\u8005\u53cb\u597d\u7684\u6570\u636e\u96c6\u53d8\u4f53\u4e0a\uff0c\u8fbe\u523088.33%\u7684\u9a8c\u8bc1\u6210\u529f\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u768465%\u3002", "conclusion": "AutoICE\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u4ee3\u7801\u5408\u6210\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5408\u6210\u53ef\u9a8c\u8bc1\u4ee3\u7801\u7684\u6210\u529f\u7387\u3002", "topic": "code agent"}}
{"id": "2512.06835", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06835", "abs": "https://arxiv.org/abs/2512.06835", "authors": ["Tingyu Li", "Zheng Sun", "Jingxuan Wei", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning", "comment": "25 pages, 5 figures", "summary": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.", "AI": {"tldr": "DoGe\u63d0\u51fa\u53cc\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u601d\u8003\u8005\u4e0e\u89e3\u51b3\u8005\u89d2\u8272\uff0c\u7ed3\u5408\u6f14\u5316\u8bfe\u7a0b\u5b66\u4e60\uff0c\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5956\u52b1\u7834\u89e3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u6f14\u5316\uff0c\u4f46\u5728\u5316\u5b66\u3001\u5730\u7403\u79d1\u5b66\u3001\u591a\u6a21\u6001\u6570\u5b66\u7b49\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u73b0\u6709\u5408\u6210\u6570\u636e\u548c\u81ea\u5956\u52b1\u673a\u5236\u5b58\u5728\u5206\u5e03\u6709\u9650\u548c\u5bf9\u9f50\u56f0\u96be\uff0c\u5bfc\u81f4\u5956\u52b1\u7834\u89e3\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "DoGe\u91c7\u7528\u53cc\u89e3\u8026\u6846\u67b6\uff1a1) \u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u601d\u8003\u8005\u548c\u89e3\u51b3\u8005\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u4ece\u81ea\u7531\u63a2\u7d22\u4e0a\u4e0b\u6587\u5230\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\uff1b2) \u6784\u5efa\u6f14\u5316\u8bfe\u7a0b\u5b66\u4e60\u7ba1\u9053\uff0c\u5305\u62ec\u6269\u5c55\u7684\u672c\u9886\u57df\u77e5\u8bc6\u8bed\u6599\u5e93\u548c\u8fed\u4ee3\u6f14\u5316\u7684\u79cd\u5b50\u95ee\u9898\u6c60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e3a\u81ea\u6211\u6f14\u5316\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u73b0\u8def\u5f84\u3002", "conclusion": "DoGe\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u548c\u6f14\u5316\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5956\u52b1\u7834\u89e3\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u81ea\u6211\u6f14\u5316\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.06859", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06859", "abs": "https://arxiv.org/abs/2512.06859", "authors": ["Ce Chi", "Xing Wang", "Zhendong Wang", "Xiaofan Liu", "Ce Li", "Zhiyan Song", "Chen Zhao", "Kexin Yang", "Boshen Shi", "Jingjing Yang", "Chao Deng", "Junlan Feng"], "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models", "comment": null, "summary": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.", "AI": {"tldr": "JT-DA-8B\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\u76848B\u53c2\u6570\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b34\u4e2a\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff0c\u7ed3\u5408SFT\u548cRL\u4f18\u5316\uff0c\u5728\u591a\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u9488\u5bf9\u8868\u683c\u63a8\u7406\u573a\u666f\u4e2d\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u8868\u683c\u5206\u6790\u4efb\u52a1\u3002", "method": "1) \u6784\u5efa\u5305\u542b34\u4e2a\u8868\u683c\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u8bad\u7ec3\u8bed\u6599\uff0c\u6574\u540829\u4e2a\u516c\u5f00\u8868\u683cQA\u6570\u636e\u96c6\u548c300\u4e07\u5f20\u8868\u683c\uff1b2) \u63d0\u51fa\u81ea\u52a8\u6d41\u6c34\u7ebf\u751f\u6210\u591a\u6b65\u5206\u6790\u4efb\u52a1\uff1b3) \u57fa\u4e8eJT-Coder-8B\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528LLM\u8bc4\u5206\u548c\u5de5\u4f5c\u6d41\u5bf9\u9f50\u8fc7\u6ee4\u6765\u84b8\u998f\u9ad8\u8d28\u91cf\u6570\u636e\uff1b4) \u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4f18\u5316\u6a21\u578b\uff1b5) \u63d0\u51fa\u56db\u9636\u6bb5\u8868\u683c\u63a8\u7406\u5de5\u4f5c\u6d41\uff08\u8868\u683c\u9884\u5904\u7406\u3001\u8868\u683c\u611f\u77e5\u3001\u5de5\u5177\u96c6\u6210\u63a8\u7406\u3001\u63d0\u793a\u5de5\u7a0b\uff09\u3002", "result": "JT-DA-8B\u5728\u591a\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u4e2d\u5fc3\u7684\u751f\u6210\u65b9\u6cd5\u548c\u5de5\u4f5c\u6d41\u9a71\u52a8\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5168\u9762\u7684\u8bad\u7ec3\u8bed\u6599\u3001\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684\u751f\u6210\u65b9\u6cd5\u548c\u5de5\u4f5c\u6d41\u9a71\u52a8\u7684\u4f18\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u5728\u590d\u6742\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u7684\u4e13\u95e8\u5316\u5927\u8bed\u8a00\u6a21\u578b\u3002", "topic": "code agent"}}
{"id": "2512.07497", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07497", "abs": "https://arxiv.org/abs/2512.07497", "authors": ["JV Roig"], "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations", "comment": "48 pages, 3 tables, 2 listings", "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5177\u6709\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u81ea\u4e3b\u4ee3\u7406\u65f6\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7KAMI\u57fa\u51c6\u6d4b\u8bd5\u5206\u6790\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u5e76\u975e\u4ee3\u7406\u9c81\u68d2\u6027\u7684\u552f\u4e00\u51b3\u5b9a\u56e0\u7d20\uff0c\u5e76\u8bc6\u522b\u51fa\u56db\u79cd\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u6267\u884c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u65f6\u7684\u5931\u8d25\u539f\u56e0\uff0c\u7406\u89e3\u54ea\u4e9b\u56e0\u7d20\u5f71\u54cd\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u4f01\u4e1a\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528KAMI v0.1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790900\u4e2a\u6267\u884c\u8f68\u8ff9\uff0c\u6db5\u76d6\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff08Granite 4 Small\u3001Llama 4 Maverick\u3001DeepSeek V3.1\uff09\u5728\u6587\u4ef6\u7cfb\u7edf\u3001\u6587\u672c\u63d0\u53d6\u3001CSV\u5206\u6790\u548cSQL\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u9010\u8bd5\u9a8c\u884c\u4e3a\u5206\u6790\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u5e76\u975e\u4ee3\u7406\u9c81\u68d2\u6027\u7684\u552f\u4e00\u9884\u6d4b\u56e0\u7d20\uff1bLlama 4 Maverick\uff08400B\uff09\u5728\u67d0\u4e9b\u4e0d\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u4ec5\u6bd4Granite 4 Small\uff0832B\uff09\u7565\u597d\uff1bDeepSeek V3.1\u7684\u4f18\u8d8a\u53ef\u9760\u6027\u4e3b\u8981\u6765\u81ea\u540e\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u800c\u975e\u67b6\u6784\u6216\u89c4\u6a21\uff1b\u8bc6\u522b\u51fa\u56db\u79cd\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff1a\u672a\u57fa\u4e8e\u4e8b\u5b9e\u7684\u8fc7\u65e9\u884c\u52a8\u3001\u8fc7\u5ea6\u5e2e\u52a9\u66ff\u4ee3\u7f3a\u5931\u5b9e\u4f53\u3001\u6613\u53d7\u5e72\u6270\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u6c61\u67d3\u3001\u8d1f\u8f7d\u4e0b\u7684\u8106\u5f31\u6267\u884c\u3002", "conclusion": "\u53ef\u9760\u7684\u4ee3\u7406\u90e8\u7f72\u4e0d\u4ec5\u9700\u8981\u66f4\u5f3a\u7684\u6a21\u578b\uff0c\u8fd8\u9700\u8981\u6709\u610f\u8bc6\u7684\u8bad\u7ec3\u548c\u8bbe\u8ba1\u9009\u62e9\uff0c\u5f3a\u8c03\u4ea4\u4e92\u5f0f\u57fa\u7840\u3001\u6062\u590d\u884c\u4e3a\u548c\u73af\u5883\u611f\u77e5\u9002\u5e94\uff0c\u9700\u8981\u52a0\u5f3a\u9a8c\u8bc1\u3001\u7ea6\u675f\u53d1\u73b0\u548c\u9075\u5faa\u771f\u5b9e\u6570\u636e\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2512.07666", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07666", "abs": "https://arxiv.org/abs/2512.07666", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "AI": {"tldr": "CGBridge\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u53ef\u8bad\u7ec3\u7684\u6865\u63a5\u6a21\u5757\u5c06\u4ee3\u7801\u56fe\u4fe1\u606f\u6ce8\u5165LLMs\uff0c\u4ee5\u589e\u5f3a\u5bf9\u7a0b\u5e8f\u7ed3\u6784\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u5728\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u5904\u7406\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u65f6\uff0c\u4f9d\u8d56\u7ebf\u6027\u5316token\u5e8f\u5217\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u7a0b\u5e8f\u7ed3\u6784\u8bed\u4e49\u7684\u7406\u89e3\u80fd\u529b\u3002\u5148\u524d\u7684\u7814\u7a76\u8981\u4e48\u53d7\u9650\u4e8e\u63d0\u793a\u957f\u5ea6\u7ea6\u675f\uff0c\u8981\u4e48\u9700\u8981\u7279\u5b9a\u4efb\u52a1\u67b6\u6784\u4fee\u6539\uff0c\u4e0e\u5927\u89c4\u6a21\u6307\u4ee4\u8ddf\u968fLLMs\u4e0d\u517c\u5bb9\u3002", "method": "CGBridge\u9996\u5148\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u572827\u4e07\u4ee3\u7801\u56fe\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u4ee3\u7801\u56fe\u7f16\u7801\u5668\uff0c\u7136\u540e\u8bad\u7ec3\u5916\u90e8\u6a21\u5757\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50\u4ee3\u7801\u3001\u56fe\u548c\u6587\u672c\u7684\u8bed\u4e49\uff0c\u6700\u540e\u6865\u63a5\u6a21\u5757\u751f\u6210\u7ed3\u6784\u611f\u77e5\u63d0\u793a\u5e76\u6ce8\u5165\u51bb\u7ed3\u7684LLM\u4e2d\uff0c\u9488\u5bf9\u4e0b\u6e38\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "CGBridge\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e0a\u76f8\u5bf9\u539f\u59cb\u6a21\u578b\u548c\u56fe\u5f62\u589e\u5f3a\u63d0\u793a\u65b9\u6cd5\u5206\u522b\u83b7\u5f9716.19%\u548c9.12%\u7684\u76f8\u5bf9\u589e\u76ca\uff0c\u5728\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u5206\u522b\u83b7\u5f979.84%\u548c38.87%\u7684\u76f8\u5bf9\u589e\u76ca\u3002\u63a8\u7406\u901f\u5ea6\u6bd4LoRA\u8c03\u4f18\u6a21\u578b\u5feb4\u500d\u4ee5\u4e0a\u3002", "conclusion": "CGBridge\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u7ed3\u6784\u611f\u77e5\u4ee3\u7801\u7406\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u6865\u63a5\u6a21\u5757\u6210\u529f\u5c06\u4ee3\u7801\u56fe\u4fe1\u606f\u96c6\u6210\u5230LLMs\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2512.06787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06787", "abs": "https://arxiv.org/abs/2512.06787", "authors": ["Ofek Glick", "Vladimir Tchuiev", "Marah Ghoummaid", "Michal Moshkovitz", "Dotan Di-Castro"], "title": "LLM4SFC: Sequential Function Chart Generation via Large Language Models", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly used for synthesizing textual PLC programming languages like Structured Text (ST) code, other IEC 61131-3 standard graphical languages like Sequential Function Charts (SFCs) remain underexplored. Generating SFCs is challenging due to graphical nature and ST actions embedded within, which are not directly compatible with standard generation techniques, often leading to non-executable code that is incompatible with industrial tool-chains In this work, we introduce LLM4SFC, the first framework to receive natural-language descriptions of industrial workflows and provide executable SFCs. LLM4SFC is based on three components: (i) A reduced structured representation that captures essential topology and in-line ST and reduced textual verbosity; (ii) Fine-tuning and few-shot retrieval-augmented generation (RAG) for alignment with SFC programming conventions; and (iii) A structured generation approach that prunes illegal tokens in real-time to ensure compliance with the textual format of SFCs. We evaluate LLM4SFC on a dataset of real-world SFCs from automated manufacturing projects, using both open-source and proprietary LLMs. The results show that LLM4SFC reliably generates syntactically valid SFC programs effectively bridging graphical and textual PLC languages, achieving a generation generation success of 75% - 94%, paving the way for automated industrial programming.", "AI": {"tldr": "LLM4SFC\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u6267\u884c\u7684\u987a\u5e8f\u529f\u80fd\u56fe(SFC)\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86\u56fe\u5f62\u5316PLC\u7f16\u7a0b\u8bed\u8a00\u751f\u6210\u7684\u6311\u6218\uff0c\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fbe\u523075%-94%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u867d\u7136LLM\u5df2\u7528\u4e8e\u751f\u6210\u6587\u672c\u578bPLC\u7f16\u7a0b\u8bed\u8a00(\u5982\u7ed3\u6784\u5316\u6587\u672c)\uff0c\u4f46IEC 61131-3\u6807\u51c6\u7684\u56fe\u5f62\u5316\u8bed\u8a00(\u5982\u987a\u5e8f\u529f\u80fd\u56feSFC)\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002SFC\u751f\u6210\u9762\u4e34\u56fe\u5f62\u7279\u6027\u548c\u5d4c\u5165\u5f0fST\u4ee3\u7801\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5e38\u4ea7\u751f\u4e0d\u53ef\u6267\u884c\u6216\u4e0e\u5de5\u4e1a\u5de5\u5177\u94fe\u4e0d\u517c\u5bb9\u7684\u4ee3\u7801\u3002", "method": "LLM4SFC\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7cbe\u7b80\u7ed3\u6784\u5316\u8868\u793a\uff0c\u6355\u6349SFC\u62d3\u6251\u7ed3\u6784\u548c\u5d4c\u5165\u5f0fST\u4ee3\u7801\uff1b2) \u5fae\u8c03\u548c\u5c11\u6837\u672c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u4f7f\u6a21\u578b\u7b26\u5408SFC\u7f16\u7a0b\u89c4\u8303\uff1b3) \u7ed3\u6784\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u65f6\u4fee\u526a\u975e\u6cd5\u6807\u8bb0\u4ee5\u786e\u4fdd\u7b26\u5408SFC\u6587\u672c\u683c\u5f0f\u3002", "result": "\u5728\u81ea\u52a8\u5316\u5236\u9020\u9879\u76ee\u7684\u771f\u5b9eSFC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u5f00\u6e90\u548c\u4e13\u6709LLM\uff0cLLM4SFC\u53ef\u9760\u5730\u751f\u6210\u8bed\u6cd5\u6709\u6548\u7684SFC\u7a0b\u5e8f\uff0c\u6210\u529f\u7387\u8fbe\u523075%-94%\uff0c\u6709\u6548\u6865\u63a5\u56fe\u5f62\u5316\u548c\u6587\u672c\u5316PLC\u8bed\u8a00\u3002", "conclusion": "LLM4SFC\u662f\u9996\u4e2a\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u53ef\u6267\u884cSFC\u7684\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5316\u5de5\u4e1a\u7f16\u7a0b\u94fa\u5e73\u9053\u8def\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u5f62\u5316PLC\u8bed\u8a00\u751f\u6210\u7684\u72ec\u7279\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "2512.07094", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07094", "abs": "https://arxiv.org/abs/2512.07094", "authors": ["Christopher Cruz"], "title": "VIGIL: A Reflective Runtime for Self-Healing Agents", "comment": null, "summary": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.", "AI": {"tldr": "VIGIL\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u68c0\u67e5\u548c\u4fdd\u62a4\u8fed\u4ee3\u5b66\u4e60\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u901a\u8fc7\u76d1\u7763\u5144\u5f1f\u4ee3\u7406\u3001\u5206\u6790\u884c\u4e3a\u65e5\u5fd7\u3001\u7ef4\u62a4\u60c5\u611f\u94f6\u884c\uff0c\u5b9e\u73b0\u81ea\u4e3b\u7ef4\u62a4\u548c\u81ea\u6211\u4fee\u590d\uff0c\u800c\u975e\u76f4\u63a5\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406LLM\u6846\u67b6\u867d\u7136\u627f\u8bfa\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u8fed\u4ee3\u89c4\u5212\u5b9e\u73b0\u81ea\u4e3b\u884c\u4e3a\uff0c\u4f46\u5927\u591a\u6570\u90e8\u7f72\u7cfb\u7edf\u4ecd\u7136\u8106\u5f31\u3002\u5b83\u4eec\u7f3a\u4e4f\u8fd0\u884c\u65f6\u5185\u7701\uff0c\u65e0\u6cd5\u8bca\u65ad\u81ea\u8eab\u6545\u969c\u6a21\u5f0f\uff0c\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u5c31\u65e0\u6cd5\u6539\u8fdb\u3002\u5b9e\u8df5\u4e2d\uff0c\u8bb8\u591a\u4ee3\u7406\u6808\u9000\u5316\u4e3a\u88c5\u9970\u6027\u7684LLM\u8c03\u7528\u94fe\uff0c\u7f3a\u4e4f\u53ef\u9760\u6027\u7684\u7ed3\u6784\u673a\u5236\u3002", "method": "VIGIL\u662f\u4e00\u4e2a\u53cd\u5c04\u8fd0\u884c\u65f6\uff0c\u76d1\u7763\u5144\u5f1f\u4ee3\u7406\u5e76\u6267\u884c\u81ea\u4e3b\u7ef4\u62a4\u3002\u5b83\u6444\u5165\u884c\u4e3a\u65e5\u5fd7\uff0c\u5c06\u6bcf\u4e2a\u4e8b\u4ef6\u8bc4\u4f30\u4e3a\u7ed3\u6784\u5316\u60c5\u611f\u8868\u793a\uff0c\u7ef4\u62a4\u5177\u6709\u8870\u51cf\u548c\u4e0a\u4e0b\u6587\u7b56\u7565\u7684\u6301\u4e45\u60c5\u611f\u94f6\u884c\uff0c\u5e76\u751f\u6210RBT\u8bca\u65ad\uff08\u5c06\u8fd1\u671f\u884c\u4e3a\u5206\u7c7b\u4e3a\u4f18\u52bf\u3001\u673a\u4f1a\u548c\u5931\u8d25\uff09\u3002\u57fa\u4e8e\u6b64\u5206\u6790\uff0cVIGIL\u751f\u6210\u4fdd\u62a4\u6027\u63d0\u793a\u66f4\u65b0\uff08\u4fdd\u7559\u6838\u5fc3\u8eab\u4efd\u8bed\u4e49\uff09\u548c\u53ea\u8bfb\u4ee3\u7801\u63d0\u6848\uff08\u7531\u7b56\u7565\u5f15\u64ce\u57fa\u4e8e\u65e5\u5fd7\u8bc1\u636e\u548c\u4ee3\u7801\u70ed\u70b9\u751f\u6210\uff09\u3002VIGIL\u4f5c\u4e3a\u72b6\u6001\u95e8\u63a7\u7ba1\u9053\u8fd0\u884c\uff0c\u975e\u6cd5\u8f6c\u6362\u4f1a\u4ea7\u751f\u663e\u5f0f\u9519\u8bef\u800c\u975e\u5141\u8bb8LLM\u5373\u5174\u53d1\u6325\u3002", "result": "\u5728\u63d0\u9192\u5ef6\u8fdf\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cVIGIL\u8bc6\u522b\u51fa\u5ef6\u8fdf\u5347\u9ad8\uff0c\u63d0\u51fa\u4e86\u63d0\u793a\u548c\u4ee3\u7801\u4fee\u590d\u65b9\u6848\u3002\u5f53\u81ea\u5df1\u7684\u8bca\u65ad\u5de5\u5177\u56e0\u6a21\u5f0f\u51b2\u7a81\u800c\u5931\u8d25\u65f6\uff0c\u5b83\u66b4\u9732\u51fa\u5185\u90e8\u9519\u8bef\uff0c\u751f\u6210\u5907\u7528\u8bca\u65ad\uff0c\u5e76\u53d1\u51fa\u4fee\u590d\u8ba1\u5212\u3002\u8fd9\u5c55\u793a\u4e86\u5728\u90e8\u7f72\u7684\u4ee3\u7406\u8fd0\u884c\u65f6\u4e2d\u5b9e\u73b0\u5143\u7ea7\u81ea\u6211\u4fee\u590d\u3002", "conclusion": "VIGIL\u5c55\u793a\u4e86\u4ee3\u7406\u8fd0\u884c\u65f6\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u5185\u7701\u3001\u60c5\u611f\u5206\u6790\u548c\u81ea\u4e3b\u7ef4\u62a4\u673a\u5236\u5b9e\u73b0\u81ea\u6211\u4fee\u590d\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2512.07109", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07109", "abs": "https://arxiv.org/abs/2512.07109", "authors": ["Miguel Ingram", "Arthur Joseph Merritt"], "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy", "comment": "62 pages, 10 figures", "summary": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9400\u4e2a\u4efb\u52a1\u76849\u7c7b\u522b\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7CNN\u9a8c\u8bc1\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u63ed\u793aTransformer\u572835.3%\u4efb\u52a1\u4e0a\u5b58\u5728\u4f4e\u795e\u7ecf\u4eb2\u548c\u529b\uff0c\u5e76\u53d1\u73b0\u7ec4\u5408\u6027\u5dee\u8ddd\uff1a\u5c40\u90e8\u6a21\u5f0f\u51c6\u786e\u7387\u9ad8\u4f46\u5168\u5c40\u5408\u6210\u80fd\u529b\u5dee\uff0c\u8868\u660e\u6027\u80fd\u53d7\u67b6\u6784\u9002\u7528\u6027\u9650\u5236\u800c\u975e\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u54cd\u5e94Hodel\u7b49\u4eba\u5bf9\u4efb\u52a1\u76f8\u5173\u6027\u5f62\u5f0f\u5316\u5b9a\u4e49\u7684\u9700\u6c42\uff0c\u4e3aARC-AGI\u57fa\u51c6\u4e2d\u7684\u4efb\u52a1\u5efa\u7acb\u7cfb\u7edf\u5206\u7c7b\uff0c\u4ee5\u8bca\u65ad\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5bf9\u4e0d\u540c\u7c7b\u578b\u4efb\u52a1\u7684\u9002\u7528\u6027\u9650\u5236\u3002", "method": "1) \u5f00\u53d1\u57fa\u4e8e\u89c4\u5219\u4ee3\u7801\u5206\u6790\u76849\u7c7b\u522b\u4efb\u52a1\u5206\u7c7b\u6cd5\uff1b2) \u4f7f\u7528CNN\u5728\u539f\u59cb\u7f51\u683c\u50cf\u7d20\u4e0a\u9a8c\u8bc1\u5206\u7c7b\u6cd5\u7684\u89c6\u89c9\u4e00\u81f4\u6027\uff1b3) \u5728302\u4e2a\u4efb\u52a1\u4e0a\u5fae\u8c03170\u4e07\u53c2\u6570Transformer\uff1b4) \u5e94\u7528\u5206\u7c7b\u6cd5\u5206\u6790Li\u7b49\u4eba\u7684ViTARC\u7814\u7a76\u3002", "result": "\u5206\u7c7b\u6cd5\u51c6\u786e\u7387\u8fbe97.5%\uff0cCNN\u9a8c\u8bc1\u51c6\u786e\u738795.24%\uff1b\u53d1\u73b069.5%\u4efb\u52a1\u5b58\u5728\u7ec4\u5408\u6027\u5dee\u8ddd\uff08\u5c40\u90e8\u51c6\u786e\u7387>80%\u4f46\u5168\u5c40<10%\uff09\uff1b\u4f4e\u4eb2\u548c\u529b\u4efb\u52a1\u6027\u80fd\u4e0a\u9650\u660e\u663e\uff0851.9% vs \u9ad8\u4eb2\u548c\u529b77.7%\uff09\uff0c\u90e8\u5206\u4efb\u52a1\u5373\u4f7f\u5927\u91cf\u6570\u636e\u4e5f\u65e0\u6cd5\u5b66\u4e60\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u53d7\u67b6\u6784\u4e0e\u4efb\u52a1\u4eb2\u548c\u529b\u9650\u5236\uff0c\u800c\u975e\u8bad\u7ec3\u8bfe\u7a0b\uff1b\u9700\u8981\u5f00\u53d1\u5177\u6709\u4eb2\u548c\u529b\u5bf9\u9f50\u6a21\u5757\u7684\u6df7\u5408\u67b6\u6784\uff1b\u5206\u7c7b\u6cd5\u4e3a\u8bca\u65ad\u4efb\u52a1\u96be\u5ea6\u548c\u67b6\u6784\u9002\u7528\u6027\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2512.06343", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06343", "abs": "https://arxiv.org/abs/2512.06343", "authors": ["Tong Xie", "Andrew Bai", "Yuanhao Ban", "Yunqi Hong", "Haoyu Li", "Cho-jui Hsieh"], "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models", "comment": null, "summary": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86Bradley-Terry\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u7279\u6027\uff0c\u53d1\u73b0\u5176\u68af\u5ea6\u8303\u6570\u53d7\u4e24\u4e2a\u56e0\u7d20\u5f71\u54cd\uff1a\u9884\u6d4b\u5956\u52b1\u5dee\u5f02\u548c\u8868\u793a\u8ddd\u79bb\u3002\u8868\u793a\u8ddd\u79bb\u4f1a\u5bfc\u81f4\u5927\u8ddd\u79bb\u5bf9\u68af\u5ea6\u4e3b\u5bfc\u3001\u5c0f\u8ddd\u79bb\u5bf9\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e86NormBT\u65b9\u6cd5\u8fdb\u884c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u3002", "motivation": "Bradley-Terry\u635f\u5931\u51fd\u6570\u662fLLM\u5bf9\u9f50\u4e2d\u5956\u52b1\u6a21\u578b\u7684\u6838\u5fc3\u76ee\u6807\u51fd\u6570\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u68af\u5ea6\u5b58\u5728\u8868\u793a\u8ddd\u79bb\u5e26\u6765\u7684\u504f\u5dee\u95ee\u9898\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u6548\u679c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8868\u793a\u8ddd\u79bb\u5c0f\u7684\u7ec6\u7c92\u5ea6\u533a\u5206\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86NormBT\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6210\u5bf9\u5f52\u4e00\u5316\u65b9\u6848\u6765\u5e73\u8861\u8868\u793a\u8ddd\u79bb\u6548\u5e94\uff0c\u5c06\u5b66\u4e60\u4fe1\u53f7\u805a\u7126\u5728\u9884\u6d4b\u8bef\u5dee\u4e0a\u3002\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u53ef\u76f4\u63a5\u66ff\u6362BT\u635f\u5931\u7684\u6539\u8fdb\u65b9\u6848\u3002", "result": "\u5728\u5404\u79cdLLM\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\uff0cNormBT\u90fd\u80fd\u4e00\u81f4\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd\uff0c\u5728RewardBench\u7684\u63a8\u7406\u7c7b\u522b\u4e0a\u83b7\u5f97\u4e86\u8d85\u8fc75%\u7684\u663e\u8457\u63d0\u5347\uff0c\u8be5\u7c7b\u522b\u5305\u542b\u5927\u91cf\u5c0f\u8ddd\u79bb\u5bf9\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684BT\u76ee\u6807\u51fd\u6570\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u4fee\u6b63\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6539\u8fdbLLM\u5bf9\u9f50\u4e2d\u7684\u5956\u52b1\u5efa\u6a21\u3002", "topic": "agent analysis"}}
{"id": "2512.06874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06874", "abs": "https://arxiv.org/abs/2512.06874", "authors": ["Ziyun Yu", "Yiru Zhou", "Chen Zhao", "Hongyi Wen"], "title": "An Analysis of Large Language Models for Simulating User Responses in Surveys", "comment": "Accepted to IJCNLP-AACL 2025 (Main Conference)", "summary": "Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u6a21\u62df\u7528\u6237\u610f\u89c1\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u4f7f\u7528CLAIMSIM\u65b9\u6cd5\u63d0\u5347\u56de\u7b54\u591a\u6837\u6027\uff0cLLM\u4ecd\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u4e0d\u540c\u80cc\u666f\u7528\u6237\u7684\u89c2\u70b9\uff0c\u4e3b\u8981\u5b58\u5728\u89c2\u70b9\u56fa\u5316\u3001\u5355\u89c6\u89d2\u751f\u6210\u548c\u96be\u4ee5\u5904\u7406\u4eba\u53e3\u7279\u5f81\u7ec6\u5fae\u5dee\u5f02\u7b49\u5c40\u9650\u3002", "motivation": "LLM\u88ab\u8d8a\u6765\u8d8a\u591a\u7528\u4e8e\u6a21\u62df\u7528\u6237\u610f\u89c1\uff0c\u4f46\u7ecf\u8fc7RLHF\u8bad\u7ec3\u7684LLM\u5b58\u5728\u504f\u5411\u4e3b\u6d41\u89c2\u70b9\u7684\u504f\u89c1\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5b83\u4eec\u80fd\u5426\u4ee3\u8868\u4e0d\u540c\u4eba\u53e3\u548c\u6587\u5316\u80cc\u666f\u7528\u6237\u7684\u62c5\u5fe7\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u76f4\u63a5\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u6d4b\u8bd5LLM\u6a21\u62df\u4eba\u7c7b\u56de\u7b54\u8de8\u9886\u57df\u8c03\u67e5\u95ee\u9898\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51faCLAIMSIM\u65b9\u6cd5\uff0c\u4eceLLM\u53c2\u6570\u77e5\u8bc6\u4e2d\u63d0\u53d6\u89c2\u70b9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136CLAIMSIM\u80fd\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u56de\u7b54\uff0c\u4f46\u4e24\u79cd\u65b9\u6cd5\u90fd\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u7528\u6237\u3002\u5206\u6790\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1\uff09LLM\u503e\u5411\u4e8e\u5728\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u4e0b\u4fdd\u6301\u56fa\u5b9a\u89c2\u70b9\uff0c\u751f\u6210\u5355\u4e00\u89c6\u89d2\u4e3b\u5f20\uff1b2\uff09\u9762\u5bf9\u51b2\u7a81\u4e3b\u5f20\u65f6\uff0cLLM\u96be\u4ee5\u63a8\u7406\u4eba\u53e3\u7279\u5f81\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u6839\u636e\u5177\u4f53\u7528\u6237\u753b\u50cf\u8c03\u6574\u56de\u7b54\u7684\u80fd\u529b\u3002", "conclusion": "LLM\u5728\u6a21\u62df\u7528\u6237\u610f\u89c1\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4eba\u53e3\u7279\u5f81\u591a\u6837\u6027\u548c\u89c2\u70b9\u7ec6\u5fae\u5dee\u5f02\u65b9\u9762\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u4ee3\u8868\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.07436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07436", "abs": "https://arxiv.org/abs/2512.07436", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "AI": {"tldr": "LocalSearchBench\u662f\u9996\u4e2a\u9488\u5bf9\u672c\u5730\u751f\u6d3b\u670d\u52a1\u7684\u667a\u80fd\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b15\u4e07\u6761\u9ad8\u8d28\u91cf\u6570\u636e\uff0c300\u4e2a\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\uff0c\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8be5\u9886\u57df\u8868\u73b0\u4e5f\u5f88\u5dee\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u4fe1\u606f\u68c0\u7d22\uff0c\u5f88\u5c11\u63a2\u7d22\u5177\u6709\u72ec\u7279\u6311\u6218\u7684\u5782\u76f4\u9886\u57df\u3002\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u7684\u67e5\u8be2\u901a\u5e38\u6a21\u7cca\u4e14\u9700\u8981\u8de8\u5546\u5bb6\u548c\u4ea7\u54c1\u7684\u591a\u8df3\u63a8\u7406\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u6784\u5efaLocalSearchBench\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea\u4e0d\u540c\u57ce\u5e02\u548c\u4e1a\u52a1\u7c7b\u578b\u768415\u4e07\u6761\u9ad8\u8d28\u91cf\u6761\u76ee\uff1b\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u521b\u5efa300\u4e2a\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\uff1b\u5f00\u53d1LocalPlayground\u7edf\u4e00\u73af\u5883\uff0c\u96c6\u6210\u591a\u79cd\u5de5\u5177\u4f9b\u667a\u80fd\u4f53\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728LocalSearchBench\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1a\u6700\u4f73\u6a21\u578b\uff08DeepSeek-V3.1\uff09\u6b63\u786e\u7387\u4ec534.34%\uff0c\u5927\u591a\u6570\u6a21\u578b\u5728\u5b8c\u6574\u6027\uff08\u5e73\u574777.33%\uff09\u548c\u5fe0\u5b9e\u5ea6\uff08\u5e73\u574761.99%\uff09\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002", "conclusion": "\u672c\u5730\u751f\u6d3b\u670d\u52a1\u9886\u57df\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u9886\u57df\u7279\u5b9a\u7684\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u73b0\u6709\u901a\u7528\u6a21\u578b\u5728\u8be5\u9886\u57df\u8868\u73b0\u6709\u9650\uff0c\u51f8\u663e\u4e86\u5782\u76f4\u9886\u57df\u4e13\u4e1a\u5316\u7684\u91cd\u8981\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2512.06392", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06392", "abs": "https://arxiv.org/abs/2512.06392", "authors": ["Runlong Zhou", "Lefan Zhang", "Shang-Chen Wu", "Kelvin Zou", "Hanzhi Zhou", "Ke Ye", "Yihao Feng", "Dong Yin", "Alex Guillen Garcia", "Dmytro Babych", "Rohit Chatterjee", "Matthew Hopkins", "Xiang Kong", "Chang Lan", "Lezhi Li", "Yiping Ma", "Daniele Molinari", "Senyu Tong", "Yanchao Sun", "Thomas Voice", "Jianyu Wang", "Chong Wang", "Simon Wang", "Floris Weers", "Yechen Xu", "Guolin Yin", "Muyang Yu", "Yi Zhang", "Zheng Zhou", "Danyang Zhuo", "Ruoming Pang", "Cheng Leong"], "title": "RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs", "comment": "14 pages, 6 figures", "summary": "Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.", "AI": {"tldr": "RLAX\u662f\u4e00\u4e2a\u5728TPU\u4e0a\u8fd0\u884c\u7684\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u548c\u65b0\u7684\u6570\u636e\u96c6\u7ba1\u7406\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u3001\u8bad\u7ec3\u6548\u7387\u548c\u5bb9\u9519\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21TPU\u96c6\u7fa4\u4e0a\u8fd0\u884c\u65f6\u3002", "method": "\u91c7\u7528\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\uff0c\u4e3b\u8bad\u7ec3\u5668\u5b9a\u671f\u63a8\u9001\u66f4\u65b0\u6743\u91cd\u5230\u53c2\u6570\u670d\u52a1\u5668\uff0c\u63a8\u7406\u5de5\u4f5c\u8282\u70b9\u62c9\u53d6\u6700\u65b0\u6743\u91cd\u751f\u6210\u65b0\u6570\u636e\uff1b\u5f15\u5165\u7cfb\u7edf\u6280\u672f\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u53ef\u62a2\u5360\u7684RL\uff1b\u5f00\u53d1\u65b0\u7684\u6570\u636e\u96c6\u7ba1\u7406\u548c\u5bf9\u9f50\u6280\u672f\u52a0\u901f\u6536\u655b\u3002", "result": "\u57281024\u4e2av5p TPU\u4e0a\uff0c\u4ec5\u752812\u5c0f\u65f648\u5206\u949f\u5c31\u5c06QwQ-32B\u6a21\u578b\u7684pass@8\u51c6\u786e\u7387\u63d0\u534712.8%\uff0c\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u62a2\u5360\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "RLAX\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21TPU\u96c6\u7fa4\u4e0a\u5feb\u901f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u826f\u597d\u7684\u5bb9\u9519\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07611", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07611", "abs": "https://arxiv.org/abs/2512.07611", "authors": ["Yongsheng Lian"], "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "comment": null, "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.\n  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "AI": {"tldr": "\u7cfb\u7edf\u6bd4\u8f83\u4e09\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08PPO\u3001GRPO\u3001DAPO\uff09\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u63a7\u5236\u6027\u8fc1\u79fb\u5b66\u4e60\u8bc4\u4f30\u53d1\u73b0RL\u8bad\u7ec3\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u6539\u8fdb\u7a0b\u5ea6\u56e0\u57fa\u51c6\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6548\u679c\uff0c\u4e3aRL-based LLM\u8bad\u7ec3\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u63a7\u5236\u6027\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff1a\u9996\u5148\u5728\u4e13\u95e8\u7684Countdown Game\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u5728\u901a\u7528\u63a8\u7406\u57fa\u51c6\u5957\u4ef6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u53c2\u6570\u5206\u6790\u5305\u62ecGRPO\u548cDAPO\u4e2d\u7684\u7ec4\u5927\u5c0f\u3001KL\u60e9\u7f5a\u7cfb\u6570\u5f71\u54cd\uff0c\u4ee5\u53caDAPO\u4e2d\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u7684\u6548\u679c\u3002", "result": "\u6240\u6709RL\u8bad\u7ec3\u6a21\u578b\u5747\u4f18\u4e8e\u5bf9\u5e94\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u6539\u8fdb\u7a0b\u5ea6\u56e0\u57fa\u51c6\u800c\u5f02\u3002\u589e\u52a0GRPO\u548cDAPO\u7684\u7ec4\u5927\u5c0f\u80fd\u5e26\u6765\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u66f4\u9ad8\u51c6\u786e\u7387\uff0cKL\u60e9\u7f5a\u7cfb\u6570\u7684\u5f71\u54cd\u662f\u975e\u5355\u8c03\u7684\u3002DAPO\u4e2d\u7684\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u5e76\u672a\u6539\u5584\u6027\u80fd\uff0c\u7981\u7528DS\u65f6DAPO\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u7ed3\u679c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7b97\u6cd5\u9009\u62e9\u548c\u53c2\u6570\u914d\u7f6e\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002GRPO\u548cDAPO\u7684\u7ec4\u5927\u5c0f\u589e\u52a0\u6709\u76ca\uff0c\u800cDAPO\u7684\u52a8\u6001\u91c7\u6837\u7ec4\u4ef6\u5b9e\u9645\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07631", "categories": ["cs.AI", "cs.CC", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07631", "abs": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "comment": null, "summary": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Agent Capability Problem (ACP)\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u83b7\u53d6\u89c6\u89d2\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u80fd\u5426\u89e3\u51b3\u95ee\u9898\uff0c\u5c06\u95ee\u9898\u89e3\u51b3\u5efa\u6a21\u4e3a\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\uff0c\u5e76\u63a8\u5bfc\u51fa\u6709\u6548\u6210\u672c\u516c\u5f0f\u6765\u9884\u6d4b\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u51b3\u5b9a\u4f55\u65f6\u6295\u5165\u8d44\u6e90\u89e3\u51b3\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u4f9d\u8d56\u7ecf\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u6846\u67b6\u6765\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u80fd\u5426\u6210\u529f\u5b8c\u6210\u4efb\u52a1\u3002", "method": "\u63d0\u51faACP\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u89e3\u51b3\u5efa\u6a21\u4e3a\u4fe1\u606f\u83b7\u53d6\u8fc7\u7a0b\uff1a\u667a\u80fd\u4f53\u9700\u8981I_total\u6bd4\u7279\u4fe1\u606f\u6765\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\uff0c\u6bcf\u4e2a\u884c\u52a8\u83b7\u5f97I_step\u6bd4\u7279\u4fe1\u606f\uff0c\u6210\u672c\u4e3aC_step\u3002\u63a8\u5bfc\u51fa\u6709\u6548\u6210\u672cC_eff = (I_total/I_step) * C_step\u6765\u9884\u6d4b\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86\u6982\u7387\u4e0a\u4e0b\u754c\u8bc1\u660e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aACP\u9884\u6d4b\u4e0e\u5b9e\u9645\u667a\u80fd\u4f53\u6027\u80fd\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u6709\u6548\u754c\u5b9a\u641c\u7d22\u52aa\u529b\uff0c\u76f8\u6bd4\u8d2a\u5a6a\u548c\u968f\u673a\u7b56\u7565\u63d0\u9ad8\u4e86\u6548\u7387\u3002\u8be5\u6846\u67b6\u5728LLM-based\u548cagentic\u5de5\u4f5c\u6d41\u4e2d\u90fd\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ACP\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u4e3b\u52a8\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u539f\u7406\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.07710", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86CompassMax-V3-Thinking\uff0c\u4e00\u4e2a\u5343\u4ebf\u89c4\u6a21\u7684MoE\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u7684RL\u6846\u67b6\uff0c\u6838\u5fc3\u539f\u5219\u662f\"\u6bcf\u4e2a\u63d0\u793a\u90fd\u5fc5\u987b\u6709\u4ef7\u503c\"\u3002\u901a\u8fc7\u591a\u9879\u521b\u65b0\u6280\u672f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21RL\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5c06RL\u6269\u5c55\u5230\u5343\u4ebf\u89c4\u6a21\u65f6\u66b4\u9732\u51fa\u5173\u952e\u6548\u7387\u95ee\u9898\uff1a\u96f6\u65b9\u5dee\u63d0\u793a\u6d6a\u8d39rollout\u8d44\u6e90\u3001\u957f\u89c6\u91ce\u91cd\u8981\u6027\u91c7\u6837\u4e0d\u7a33\u5b9a\u3001\u6807\u51c6\u5956\u52b1\u6a21\u578b\u5bfc\u81f4\u7684\u4f18\u52bf\u53cd\u8f6c\uff0c\u4ee5\u53carollout\u5904\u7406\u7684\u7cfb\u7edf\u6027\u74f6\u9888\u3002", "method": "1) \u591a\u9636\u6bb5\u96f6\u65b9\u5dee\u6d88\u9664\uff1a\u8fc7\u6ee4\u975e\u4fe1\u606f\u6027\u63d0\u793a\uff0c\u7a33\u5b9a\u57fa\u4e8e\u7ec4\u7684\u7b56\u7565\u4f18\u5316\uff1b2) ESPO\uff1a\u71b5\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff0c\u5e73\u8861token\u7ea7\u548c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u91c7\u6837\uff1b3) Router Replay\u7b56\u7565\uff1a\u5bf9\u9f50\u8bad\u7ec3\u65f6MoE\u8def\u7531\u51b3\u7b56\u4e0e\u63a8\u7406\u65f6\u884c\u4e3a\uff1b4) \u9ad8\u541e\u5410RL\u7cfb\u7edf\uff1aFP8\u7cbe\u5ea6rollout\u3001\u91cd\u53e0\u5956\u52b1\u8ba1\u7b97\u548c\u957f\u5ea6\u611f\u77e5\u8c03\u5ea6\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5728\u5185\u90e8\u548c\u516c\u5171\u8bc4\u4f30\u4e2d\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5728\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578b\u4e0a\u8fdb\u884c\u7a33\u5b9a\u9ad8\u6548\u7684RL\u8bad\u7ec3\u3002", "conclusion": "\u8fd9\u4e9b\u521b\u65b0\u5f62\u6210\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684pipeline\uff0c\u4f7f\u5f97\u5728\u5343\u4ebf\u89c4\u6a21MoE\u6a21\u578b\u4e0a\u8fdb\u884cRL\u8bad\u7ec3\u53d8\u5f97\u7a33\u5b9a\u9ad8\u6548\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07761", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u653b\u51fb\u8005LLM\u6765\u8bf1\u4f7f\u9ed1\u76d2\u6a21\u578b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u76f8\u6bd4\u5355\u8f6e\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u5f71\u54cd\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u8f6e\u4f18\u5316\uff0c\u96be\u4ee5\u5b66\u4e60\u957f\u671f\u653b\u51fb\u7b56\u7565\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u8f6e\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u76f4\u63a5\u4f18\u5316\u6700\u7ec8\u8f6e\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4f5c\u4e3a\u7ed3\u679c\u5956\u52b1\u3002\u63d0\u51fa\u4e24\u79cd\u542f\u53d1\u5f0f\u8fc7\u7a0b\u5956\u52b1\uff1a1\uff09\u63a7\u5236\u4e2d\u95f4\u8f93\u51fa\u7684\u6709\u5bb3\u6027\u4ee5\u907f\u514d\u89e6\u53d1\u9ed1\u76d2\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\uff1b2\uff09\u4fdd\u6301\u4e2d\u95f4\u8f93\u51fa\u7684\u8bed\u4e49\u76f8\u5173\u6027\u4ee5\u907f\u514d\u5185\u5bb9\u6f02\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u4e00\u81f4\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bad\u7ec3\u653b\u51fb\u8005LLM\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u7684\u63d0\u793a-\u8f93\u51fa\u4ea4\u4e92\u6765\u8bf1\u4f7f\u9ed1\u76d2\u6a21\u578b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u653b\u51fb\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2512.06471", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06471", "abs": "https://arxiv.org/abs/2512.06471", "authors": ["Nathan P. Lawrence", "Ali Mesbah"], "title": "Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control", "comment": "IFAC preprint", "summary": "Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u6700\u4f18\u63a7\u5236\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff0c\u63a8\u5bfc\u4e86\u4f20\u7edf\u4e8c\u6b21\u76ee\u6807\u4e0e\u76ee\u6807\u6761\u4ef6\u5956\u52b1\u4e4b\u95f4\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u5e76\u5c06\u72b6\u6001\u4f30\u8ba1\u4e0e\u6982\u7387\u5956\u52b1\u8054\u7cfb\u8d77\u6765\uff0c\u9a8c\u8bc1\u4e86\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u5728\u975e\u7ebf\u6027\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u65e8\u5728\u8bad\u7ec3\u667a\u80fd\u4f53\u6700\u5927\u5316\u8fbe\u5230\u76ee\u6807\u72b6\u6001\u7684\u6982\u7387\uff0c\u4f46\u4f20\u7edf\"\u5bc6\u96c6\"\u5956\u52b1\uff08\u5982\u4e8c\u6b21\u578b\u5956\u52b1\uff09\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002\u8bba\u6587\u65e8\u5728\u4ece\u6700\u4f18\u63a7\u5236\u89d2\u5ea6\u5206\u6790\u76ee\u6807\u6761\u4ef6\u8bbe\u7f6e\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u76ee\u6807\u6761\u4ef6RL\u6709\u6548\u4ee5\u53ca\u4f20\u7edf\u5956\u52b1\u4e3a\u4f55\u4f1a\u5931\u8d25\u3002", "method": "1. \u57fa\u4e8e\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5206\u6790\u76ee\u6807\u6761\u4ef6\u8bbe\u7f6e\uff1b2. \u63a8\u5bfc\u4f20\u7edf\u4e8c\u6b21\u76ee\u6807\u4e0e\u76ee\u6807\u6761\u4ef6\u5956\u52b1\u4e4b\u95f4\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff1b3. \u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5c06\u72b6\u6001\u4f30\u8ba1\u4e0e\u6982\u7387\u5956\u52b1\u8054\u7cfb\u8d77\u6765\uff1b4. \u5728\u975e\u7ebf\u6027\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u4f7f\u7528RL\u548c\u9884\u6d4b\u63a7\u5236\u6280\u672f\u9a8c\u8bc1\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u7684\u4f18\u52bf\u3002", "result": "\u8bba\u6587\u9610\u660e\u4e86\u76ee\u6807\u6761\u4ef6RL\u6210\u529f\u7684\u539f\u56e0\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u5bc6\u96c6\u5956\u52b1\u53ef\u80fd\u5931\u8d25\u7684\u7406\u8bba\u4f9d\u636e\u3002\u901a\u8fc7\u5c06\u72b6\u6001\u4f30\u8ba1\u4e0e\u6982\u7387\u5956\u52b1\u8fde\u63a5\uff0c\u4f7f\u76ee\u6807\u6761\u4ef6\u5956\u52b1\u7279\u522b\u9002\u5408\u53cc\u91cd\u63a7\u5236\u95ee\u9898\u3002\u5728\u975e\u7ebf\u6027\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u7684\u4f18\u52bf\u3002", "conclusion": "\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4ece\u6700\u4f18\u63a7\u5236\u89d2\u5ea6\u5177\u6709\u7406\u8bba\u57fa\u7840\uff0c\u5176\u6982\u7387\u5956\u52b1\u5f62\u5f0f\u6bd4\u4f20\u7edf\u4e8c\u6b21\u5956\u52b1\u66f4\u9002\u5408\u76ee\u6807\u8fbe\u6210\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u548c\u53cc\u91cd\u63a7\u5236\u95ee\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07795", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07795", "abs": "https://arxiv.org/abs/2512.07795", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "comment": "11 pages, 3 tables, 4 figures", "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "AI": {"tldr": "ReasonBENCH\u662f\u9996\u4e2a\u91cf\u5316LLM\u63a8\u7406\u4e0d\u7a33\u5b9a\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u8f6e\u8fd0\u884c\u534f\u8bae\u63d0\u4f9b\u7edf\u8ba1\u53ef\u9760\u7684\u6027\u80fd\u548c\u8d28\u91cf\u6307\u6807\uff0c\u63ed\u793a\u5f53\u524d\u63a8\u7406\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u9ad8\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u8bc4\u4f30\u4e3b\u8981\u62a5\u544a\u5355\u6b21\u8fd0\u884c\u51c6\u786e\u7387\uff0c\u5ffd\u7565\u4e86\u968f\u673a\u89e3\u7801\u5e26\u6765\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u6210\u672c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faReasonBENCH\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\uff1a(1)\u6807\u51c6\u5316\u63a8\u7406\u6846\u67b6\u3001\u6a21\u578b\u548c\u4efb\u52a1\u7684\u6a21\u5757\u5316\u8bc4\u4f30\u5e93\uff1b(2)\u62a5\u544a\u8d28\u91cf\u548c\u6210\u672c\u7edf\u8ba1\u53ef\u9760\u6307\u6807\u7684\u591a\u8f6e\u8fd0\u884c\u534f\u8bae\uff1b(3)\u9f13\u52b1\u65b9\u5dee\u611f\u77e5\u62a5\u544a\u7684\u516c\u5f00\u6392\u884c\u699c\u3002", "result": "\u53d1\u73b0\u7edd\u5927\u591a\u6570\u63a8\u7406\u7b56\u7565\u548c\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u9ad8\u4e0d\u7a33\u5b9a\u6027\uff0c\u5373\u4f7f\u5e73\u5747\u6027\u80fd\u76f8\u4f3c\u7684\u7b56\u7565\u7f6e\u4fe1\u533a\u95f4\u5bbd\u5ea6\u53ef\u8fbe4\u500d\u5dee\u5f02\uff0c\u4e14\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u901a\u5e38\u6210\u672c\u66f4\u9ad8\u4e14\u66f4\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u53ef\u590d\u73b0\u6027\u662f\u53ef\u9760LLM\u63a8\u7406\u7684\u5173\u952e\u7ef4\u5ea6\uff0cReasonBENCH\u4e3a\u672a\u6765\u63a8\u7406\u65b9\u6cd5\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2512.07132", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07132", "abs": "https://arxiv.org/abs/2512.07132", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "comment": "Code: https://github.com/nsivaku/dart", "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "AI": {"tldr": "DART\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8fa9\u8bba\u5206\u6b67\u6765\u8bc6\u522b\u6709\u7528\u7684\u89c6\u89c9\u5de5\u5177\uff08\u5982\u7269\u4f53\u68c0\u6d4b\u3001OCR\u3001\u7a7a\u95f4\u63a8\u7406\u7b49\uff09\uff0c\u5229\u7528\u5de5\u5177\u4fe1\u606f\u89e3\u51b3\u5206\u6b67\u5e76\u4fc3\u8fdb\u8ba8\u8bba\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u4e13\u7528\u89c6\u89c9\u5de5\u5177\u53ef\u4ee5\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e13\u5bb6\u77e5\u8bc6\uff0c\u4f46\u786e\u5b9a\u4f55\u65f6\u8c03\u7528\u54ea\u4e9b\u5de5\u5177\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5de5\u5177\u9009\u62e9\u548c\u8c03\u7528\u65f6\u673a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDART\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1\uff09\u591a\u4e2a\u89c6\u89c9\u667a\u80fd\u4f53\u8fdb\u884c\u8fa9\u8bba\uff0c\u901a\u8fc7\u5206\u6b67\u8bc6\u522b\u6709\u7528\u7684\u89c6\u89c9\u5de5\u5177\uff1b2\uff09\u8c03\u7528\u5de5\u5177\u5f15\u5165\u65b0\u4fe1\u606f\u5e76\u63d0\u4f9b\u5de5\u5177\u5bf9\u9f50\u7684\u5171\u8bc6\u5206\u6570\uff1b3\uff09\u4f7f\u7528\u805a\u5408\u667a\u80fd\u4f53\u57fa\u4e8e\u667a\u80fd\u4f53\u8f93\u51fa\u548c\u5de5\u5177\u4fe1\u606f\u9009\u62e9\u6700\u4f73\u7b54\u6848\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u5728A-OKVQA\u4e0a\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff08\u5e26\u6cd5\u5b98\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff09\u63d0\u53473.4%\uff0c\u5728MMMU\u4e0a\u63d0\u53472.4%\uff0c\u5728M3D\u533b\u7597\u6570\u636e\u96c6\u4e0a\u6bd4\u5176\u4ed6\u57fa\u7ebf\u63d0\u53471.3%\u3002\u5206\u6790\u663e\u793aDART\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u8ba8\u8bba\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u4f7f\u7528\u591a\u6837\u5316\u5de5\u5177\u89e3\u51b3\u5206\u6b67\u3002", "conclusion": "DART\u901a\u8fc7\u5229\u7528\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u5206\u6b67\u6765\u6307\u5bfc\u5de5\u5177\u8c03\u7528\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u9002\u5e94\u65b0\u5de5\u5177\u548c\u5e94\u7528\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2512.06547", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.06547", "abs": "https://arxiv.org/abs/2512.06547", "authors": ["Xiaocan Li", "Shiliang Wu", "Zheng Shen"], "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation", "comment": null, "summary": "Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md", "AI": {"tldr": "A-3PO\u901a\u8fc7\u8fd1\u4f3c\u8fd1\u7aef\u7b56\u7565\u6d88\u9664\u5f02\u6b65RL\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u51cf\u5c1118%\u8bad\u7ec3\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u6027\u80fd", "motivation": "\u4f20\u7edf\u7684\u89e3\u8026\u635f\u5931\u65b9\u6cd5\u5728\u5f02\u6b65RL\u4e2d\u5f15\u5165\u8fd1\u7aef\u7b56\u7565\u6765\u7a33\u5b9a\u5b66\u4e60\uff0c\u4f46\u9700\u8981\u989d\u5916\u7684\u524d\u5411\u4f20\u64ad\u8ba1\u7b97\uff0c\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9020\u6210\u8ba1\u7b97\u74f6\u9888", "method": "\u63d0\u51faA-3PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u63d2\u503c\u8fd1\u4f3c\u8fd1\u7aef\u7b56\u7565\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\uff0c\u6d88\u9664\u989d\u5916\u8ba1\u7b97\u5f00\u9500", "result": "\u51cf\u5c1118%\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "A-3PO\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6b65RL\u4e2d\u89e3\u8026\u635f\u5931\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u578b\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2512.07407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07407", "abs": "https://arxiv.org/abs/2512.07407", "authors": ["Niklas Mellgren", "Peter Schneider-Kamp", "Lukas Galke Poech"], "title": "Training Language Models to Use Prolog as a Tool", "comment": "10 pages", "summary": "Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4f7f\u7528Prolog\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u8ba1\u7b97\u5de5\u5177\uff0c\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\u548c\u53ef\u5ba1\u8ba1\u6027", "motivation": "\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u63a8\u7406\uff0c\u96be\u4ee5\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002\u4e3a\u786e\u4fdd\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u9700\u8981\u5c06\u6a21\u578b\u63a8\u7406\u5efa\u7acb\u5728\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u7cfb\u7edf\u4e0a\u3002", "method": "\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03Qwen2.5-3B-Instruct\u6a21\u578b\uff0c\u5728GSM8K-Prolog-Prover\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u4e0d\u540c\u63d0\u793a\u7ed3\u6784\u3001\u5956\u52b1\u7ec4\u5408\uff08\u6267\u884c\u3001\u8bed\u6cd5\u3001\u8bed\u4e49\u3001\u7ed3\u6784\uff09\u548c\u63a8\u7406\u534f\u8bae\uff08\u5355\u6b21\u3001best-of-N\u3001\u5185\u90e8\u8c03\u7528Prolog\u3001\u72ec\u7acb\u8c03\u7528Prolog\uff09", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c3B\u6a21\u578b\u5728\u96f6\u6837\u672cMMLU\u4e0a\u8fbe\u52307B\u6a21\u578b\u5c11\u6837\u672c\u6027\u80fd\u3002\u6700\u4f73\u914d\u7f6e\u5728GSM8K\u4e0a\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u667a\u80fd\u4f53\u63a8\u7406\u5728MMLU-Stem\u548cMMLU-Pro\u4e0a\u5b9e\u73b0\u6700\u4f18\u96f6\u6837\u672c\u6cdb\u5316", "conclusion": "\u5c06\u6a21\u578b\u63a8\u7406\u5efa\u7acb\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7cfb\u7edf\u4e0a\u80fd\u663e\u8457\u63d0\u5347\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u8054\u5408\u4f18\u5316\u63d0\u793a\u3001\u5956\u52b1\u548c\u63a8\u7406\u534f\u8bae\u5bf9\u7a0b\u5e8f\u8bed\u6cd5\u548c\u903b\u8f91\u6709\u91cd\u8981\u5f71\u54cd", "topic": "agentic reinforcement learning"}}
{"id": "2512.07461", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07461", "abs": "https://arxiv.org/abs/2512.07461", "authors": ["Tong Wu", "Yang Liu", "Jun Bai", "Zixia Jia", "Shuyi Zhang", "Ziyong Lin", "Yanting Wang", "Song-Chun Zhu", "Zilong Zheng"], "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "comment": null, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "AI": {"tldr": "NPR\u662f\u4e00\u4e2a\u6559\u5e08\u81ea\u7531\u7684\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u83b7\u5f97\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4ece\u987a\u5e8f\u6a21\u62df\u5230\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u7684\u8f6c\u53d8\uff0c\u57288\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u5347\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u8fbe4.6\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901a\u5e38\u662f\u987a\u5e8f\u6a21\u62df\u7684\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u8ba9\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u51fa\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\u6216\u6559\u5e08\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ece\"\u51b7\u542f\u52a8\"\u683c\u5f0f\u53d1\u73b0\u8fc7\u6e21\u5230\u4e25\u683c\u62d3\u6251\u7ea6\u675f\uff1b2\uff09\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u6267\u884c\u56fe\u4e2d\u76f4\u63a5\u4f18\u5316\u5206\u652f\u7b56\u7565\uff1b3\uff09\u7a33\u5065\u7684NPR\u5f15\u64ce\uff0c\u91cd\u6784SGLang\u7684\u5185\u5b58\u7ba1\u7406\u548c\u6d41\u7a0b\u63a7\u5236\u3002", "result": "\u5728Qwen3-4B\u6a21\u578b\u4e0a\u8bad\u7ec3NPR\uff0c\u57288\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u6027\u80fd\u63d0\u5347\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u8fbe4.6\u500d\uff0c\u5b9e\u73b0100%\u771f\u6b63\u7684\u5e76\u884c\u6267\u884c\uff0c\u4f18\u4e8e\u4f9d\u8d56\u81ea\u56de\u5f52\u89e3\u7801\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NPR\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u667a\u80fd\u63a8\u7406\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u4ece\u987a\u5e8f\u6a21\u62df\u5230\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u7684\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2512.07478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07478", "abs": "https://arxiv.org/abs/2512.07478", "authors": ["Zhuoran Zhuang", "Ye Chen", "Jianghao Su", "Chao Luo", "Luhui Liu", "Xia Zeng"], "title": "Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization", "comment": null, "summary": "Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6280\u672f\uff08PRS\u548cVSPO\uff09\u6765\u89e3\u51b3\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4e2d\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u7a00\u758f\u5956\u52b1\u548c\u68af\u5ea6\u9000\u5316\u95ee\u9898\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u5177\u96c6\u6210\u63a8\u7406\uff08TIR\uff09\u7684LLM\u901a\u8fc7\u8fed\u4ee3\u89c4\u5212\u3001\u8c03\u7528\u5916\u90e8\u5de5\u5177\u6765\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u7a00\u758f\u7684\u4e8c\u5143\u9a8c\u8bc1\u4fe1\u53f7\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u6307\u5bfc\u6709\u9650\u4e14\u6536\u655b\u6162\uff1b2\uff09\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u68af\u5ea6\u9000\u5316\u95ee\u9898\uff0c\u76f8\u540c\u5956\u52b1\u5bfc\u81f4\u96f6\u4f18\u52bf\uff0c\u964d\u4f4e\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1a\u6e10\u8fdb\u5f0f\u5956\u52b1\u5851\u9020\uff08PRS\uff09\u548c\u57fa\u4e8e\u4ef7\u503c\u7684\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff08VSPO\uff09\u3002PRS\u91c7\u7528\u8bfe\u7a0b\u5f0f\u5956\u52b1\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u5bc6\u96c6\u7684\u9636\u6bb5\u53cd\u9988\uff0c\u5148\u9f13\u52b1\u6a21\u578b\u638c\u63e1\u53ef\u89e3\u6790\u7684\u5de5\u5177\u8c03\u7528\u683c\u5f0f\uff0c\u518d\u4f18\u5316\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u7b54\u6848\u8d28\u91cf\u3002VSPO\u662f\u589e\u5f3a\u7684GRPO\u53d8\u4f53\uff0c\u7528\u4efb\u52a1\u4ef7\u503c\u5ea6\u91cf\u9009\u62e9\u6837\u672c\uff0c\u5e94\u7528\u4ef7\u503c\u5e73\u6ed1\u88c1\u526a\u7a33\u5b9a\u68af\u5ea6\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPRS\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u4e8c\u5143\u5956\u52b1\uff0cVSPO\u76f8\u6bd4PPO\u3001GRPO\u3001CISPO\u548cSFT\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u6027\u80fd\u3002\u4e24\u8005\u7ed3\u5408\u4ea7\u751f\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u66f4\u597d\u7684TIR\u4ee3\u7406\u3002", "conclusion": "PRS\u548cVSPO\u6709\u6548\u89e3\u51b3\u4e86TIR\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5956\u52b1\u8bbe\u8ba1\u548c\u6539\u8fdb\u7684\u91c7\u6837\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.06692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06692", "abs": "https://arxiv.org/abs/2512.06692", "authors": ["Shiye Lei", "Zhihao Cheng", "Dacheng Tao"], "title": "State Diversity Matters in Offline Behavior Distillation", "comment": "12 pages, 5 figures, 5 tables", "summary": "Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u79bb\u7ebf\u884c\u4e3a\u84b8\u998f\u4e2d\u539f\u59cb\u6570\u636e\u96c6\u4e0e\u5408\u6210\u6570\u636e\u96c6\u5b58\u5728\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51fa\u72b6\u6001\u5bc6\u5ea6\u52a0\u6743\u7b97\u6cd5\u6765\u589e\u5f3a\u72b6\u6001\u591a\u6837\u6027\uff0c\u4ece\u800c\u63d0\u5347\u84b8\u998f\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u884c\u4e3a\u84b8\u998f\u53ef\u4ee5\u5c06\u5927\u91cf\u79bb\u7ebfRL\u6570\u636e\u538b\u7f29\u6210\u7d27\u51d1\u7684\u5408\u6210\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u9ad8\u8d28\u91cf\u539f\u59cb\u6570\u636e\u96c6\u4e0d\u4e00\u5b9a\u80fd\u4ea7\u751f\u4f18\u8d28\u5408\u6210\u6570\u636e\u96c6\uff0c\u5b58\u5728\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6539\u8fdb\u84b8\u998f\u8fc7\u7a0b\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u635f\u5931\u6c34\u5e73\u4e0b\u7684\u7b56\u7565\u6027\u80fd\uff0c\u53d1\u73b0\u72b6\u6001\u591a\u6837\u6027\u5728\u8bad\u7ec3\u635f\u5931\u8f83\u5927\u65f6\u66f4\u91cd\u8981\u3002\u63d0\u51fa\u72b6\u6001\u5bc6\u5ea6\u52a0\u6743\u7b97\u6cd5\uff0c\u4f7f\u7528\u72b6\u6001\u5bc6\u5ea6\u7684\u5012\u6570\u5bf9\u84b8\u998f\u76ee\u6807\u8fdb\u884c\u52a0\u6743\uff0c\u4ece\u800c\u5c06\u66f4\u591a\u6837\u5316\u7684\u72b6\u6001\u4fe1\u606f\u84b8\u998f\u5230\u5408\u6210\u6570\u636e\u4e2d\u3002", "result": "\u5728\u591a\u4e2aD4RL\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u539f\u59cb\u6570\u636e\u96c6\u72b6\u6001\u591a\u6837\u6027\u6709\u9650\u65f6\uff0cSDW\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u884c\u4e3a\u84b8\u998f\u7684\u6027\u80fd\u3002", "conclusion": "\u79bb\u7ebf\u884c\u4e3a\u84b8\u998f\u4e2d\u72b6\u6001\u591a\u6837\u6027\u6bd4\u72b6\u6001\u8d28\u91cf\u66f4\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u635f\u5931\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u3002\u63d0\u51fa\u7684\u72b6\u6001\u5bc6\u5ea6\u52a0\u6743\u7b97\u6cd5\u901a\u8fc7\u5f3a\u8c03\u72b6\u6001\u591a\u6837\u6027\u6709\u6548\u6539\u5584\u4e86\u84b8\u998f\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07777", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07777", "abs": "https://arxiv.org/abs/2512.07777", "authors": ["Karin de Langis", "P\u00fcren \u00d6ncel", "Ryan Peters", "Andrew Elfenbein", "Laura Kristen Allen", "Andreas Schramm", "Dongyeop Kang"], "title": "Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?", "comment": null, "summary": "Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs' internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM's understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.", "AI": {"tldr": "LLMs\u80fd\u901a\u8fc7\u5185\u90e8\u8868\u5f81\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u5728\u56de\u7b54\u8bc4\u5206\u95ee\u9898\u65f6\u65e0\u6cd5\u6709\u6548\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u6545\u4e8b\uff0c\u8868\u660e\u5176\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u5b58\u5728\u7f3a\u9677\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u7684\u53d9\u4e8b\uff0c\u63a2\u7d22LLMs\u5bf9\u6545\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u80fd\u529b\u53ca\u5176\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u914d\u5bf9\u53d9\u4e8b\u6570\u636e\u96c6\u8fdb\u884c\u63a2\u6d4b\u7814\u7a76\uff0c\u5206\u6790LLMs\u7684\u5185\u90e8\u8868\u5f81\u548c\u751f\u6210\u54cd\u5e94\uff0c\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u53d8\u4f53\u548c\u63a8\u7406\u94fe\u65b9\u6cd5\u3002", "result": "LLMs\u5185\u90e8\u8868\u5f81\u80fd\u53ef\u9760\u8bc6\u522b\u4e0d\u8fde\u8d2f\u53d9\u4e8b\uff0c\u4f46\u751f\u6210\u54cd\u5e94\u65e0\u6cd5\u6709\u6548\u533a\u5206\uff1b\u5bf9\u8fdd\u53cd\u8bbe\u5b9a\u7684\u4e0d\u8fde\u8d2f\u6bd4\u5bf9\u8fdd\u53cd\u89d2\u8272\u7279\u8d28\u7684\u4e0d\u8fde\u8d2f\u66f4\u654f\u611f\uff1b\u63a8\u7406\u94fe\u65e0\u6cd5\u6d88\u9664\u8fd9\u79cd\u7f3a\u9677\u3002", "conclusion": "LLMs\u5bf9\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u7406\u89e3\u4e0d\u5b8c\u6574\uff0c\u66f4\u4f9d\u8d56\u539f\u578b\u4e16\u754c\u77e5\u8bc6\u800c\u975e\u57fa\u4e8e\u610f\u4e49\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\u6784\u5efa\uff0c\u5185\u90e8\u72b6\u6001\u4e0e\u884c\u4e3a\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2512.07783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07783", "abs": "https://arxiv.org/abs/2512.07783", "authors": ["Charlie Zhang", "Graham Neubig", "Xiang Yue"], "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models", "comment": null, "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e2d\u7684\u771f\u5b9e\u4f5c\u7528\uff1a\u4ec5\u5f53\u9884\u8bad\u7ec3\u7559\u6709\u8db3\u591f\u7a7a\u95f4\u4e14RL\u6570\u636e\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u65f6\uff0cRL\u624d\u80fd\u4ea7\u751f\u771f\u6b63\u7684\u80fd\u529b\u589e\u76ca\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u867d\u7136\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u96be\u4ee5\u786e\u5b9a\u8fd9\u79cd\u63d0\u5347\u662f\u771f\u6b63\u6269\u5c55\u4e86\u6a21\u578b\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u4e2d\u5df2\u6709\u7684\u77e5\u8bc6\u3002\u7531\u4e8e\u73b0\u4ee3\u8bad\u7ec3\u6d41\u7a0b\u7f3a\u4e4f\u63a7\u5236\uff08\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u900f\u660e\u3001\u4e2d\u671f\u8bad\u7ec3\u5e38\u88ab\u5ffd\u89c6\u3001RL\u76ee\u6807\u4e0e\u672a\u77e5\u5148\u9a8c\u77e5\u8bc6\u590d\u6742\u4ea4\u4e92\uff09\uff0c\u9700\u8981\u5f00\u53d1\u53d7\u63a7\u5b9e\u9a8c\u6846\u67b6\u6765\u5398\u6e05\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u540e\u8bad\u7ec3\u5404\u81ea\u7684\u56e0\u679c\u8d21\u732e\u3002", "method": "\u5f00\u53d1\u4e86\u5b8c\u5168\u53d7\u63a7\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u63a8\u7406\u4efb\u52a1\uff08\u5177\u6709\u660e\u786e\u7684\u539f\u5b50\u64cd\u4f5c\u3001\u53ef\u89e3\u6790\u7684\u9010\u6b65\u63a8\u7406\u75d5\u8ff9\uff09\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u64cd\u7eb5\u8bad\u7ec3\u5206\u5e03\u3002\u8bc4\u4f30\u6a21\u578b\u7684\u4e24\u4e2a\u7ef4\u5ea6\uff1a1\uff09\u5916\u63a8\u6cdb\u5316\u5230\u66f4\u590d\u6742\u7684\u7ec4\u5408\uff1b2\uff09\u8de8\u8868\u9762\u4e0a\u4e0b\u6587\u7684\u4e0a\u4e0b\u6587\u6cdb\u5316\u3002", "result": "1) RL\u4ec5\u5728\u9884\u8bad\u7ec3\u7559\u6709\u8db3\u591f\u7a7a\u95f4\u4e14RL\u6570\u636e\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\uff08\u56f0\u96be\u4f46\u5c1a\u672a\u8d85\u51fa\u80fd\u529b\u8303\u56f4\u7684\u4efb\u52a1\uff09\u65f6\uff0c\u624d\u80fd\u4ea7\u751f\u771f\u6b63\u7684\u80fd\u529b\u589e\u76ca\uff08pass@128\uff09\u30022) \u4e0a\u4e0b\u6587\u6cdb\u5316\u9700\u8981\u6700\u5c0f\u4f46\u8db3\u591f\u7684\u9884\u8bad\u7ec3\u66b4\u9732\uff0c\u4e4b\u540eRL\u53ef\u4ee5\u53ef\u9760\u5730\u8fc1\u79fb\u30023) \u4e2d\u671f\u8bad\u7ec3\u5728\u56fa\u5b9a\u8ba1\u7b97\u91cf\u4e0b\u6bd4\u4ec5\u7528RL\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u663e\u793a\u4e86\u5176\u5728\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u6838\u5fc3\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4f5c\u7528\u30024) \u8fc7\u7a0b\u7ea7\u5956\u52b1\u51cf\u5c11\u4e86\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u5e76\u63d0\u9ad8\u4e86\u63a8\u7406\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cRL\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u4e86\u57fa\u7840\u3002RL\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u8bad\u7ec3\u9636\u6bb5\u95f4\u7684\u534f\u8c03\uff0c\u7279\u522b\u662f\u9700\u8981\u9488\u5bf9\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u7684\u6570\u636e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07801", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07801", "abs": "https://arxiv.org/abs/2512.07801", "authors": ["Raunak Jain", "Mudita Khurana"], "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "comment": null, "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u534f\u4f5c\u56e0\u679c\u610f\u4e49\u5efa\u6784\uff08CCS\uff09\u4f5c\u4e3a\u51b3\u7b56\u652f\u6301\u4ee3\u7406\u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u65e8\u5728\u5c06AI\u8bbe\u8ba1\u4e3a\u8ba4\u77e5\u5de5\u4f5c\u7684\u5408\u4f5c\u4f19\u4f34\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u5de5\u5177\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u4eba\u673a\u56e2\u961f\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u590d\u6742\u9ad8\u98ce\u9669\u51b3\u7b56\u73af\u5883\u4e2d\u672a\u80fd\u5b9e\u73b0\u4eba\u673a\u4e92\u8865\uff0c\u4eba\u7c7b\u4e13\u5bb6\u5728\u9a8c\u8bc1\u5faa\u73af\u548c\u8fc7\u5ea6\u4f9d\u8d56\u4e4b\u95f4\u6447\u6446\uff0c\u4eba\u673a\u56e2\u961f\u8868\u73b0\u5f80\u5f80\u4e0d\u5982\u6700\u4f73\u4e2a\u4f53\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e0d\u4ec5\u662f\u51c6\u786e\u6027\u95ee\u9898\uff0c\u800c\u662fAI\u8f85\u52a9\u6982\u5ff5\u7684\u6839\u672c\u7f3a\u9677\u2014\u2014\u4e13\u5bb6\u51b3\u7b56\u662f\u901a\u8fc7\u534f\u4f5c\u8ba4\u77e5\u8fc7\u7a0b\u5b8c\u6210\u7684\uff0c\u800c\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u652f\u6301\u8fd9\u79cd\u534f\u4f5c\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u56e0\u679c\u610f\u4e49\u5efa\u6784\uff08CCS\uff09\u4f5c\u4e3a\u7814\u7a76\u8bae\u7a0b\u548c\u7ec4\u7ec7\u6846\u67b6\uff1a\u8bbe\u8ba1\u80fd\u591f\u4f5c\u4e3a\u8ba4\u77e5\u5408\u4f5c\u4f19\u4f34\u7684\u7cfb\u7edf\uff0c\u7ef4\u62a4\u4e13\u5bb6\u63a8\u7406\u7684\u6f14\u5316\u6a21\u578b\uff0c\u5e2e\u52a9\u9610\u8ff0\u548c\u4fee\u8ba2\u76ee\u6807\uff0c\u5171\u540c\u6784\u5efa\u548c\u538b\u529b\u6d4b\u8bd5\u56e0\u679c\u5047\u8bbe\uff0c\u5e76\u4ece\u8054\u5408\u51b3\u7b56\u7ed3\u679c\u4e2d\u5b66\u4e60\uff0c\u4f7f\u4eba\u7c7b\u548c\u4ee3\u7406\u90fd\u80fd\u968f\u65f6\u95f4\u6539\u8fdb\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86CCS\u6846\u67b6\u7684\u5177\u4f53\u6311\u6218\uff1a1\uff09\u8bad\u7ec3\u751f\u6001\u4f7f\u534f\u4f5c\u601d\u8003\u5177\u6709\u5de5\u5177\u4ef7\u503c\uff1b2\uff09\u5171\u540c\u6784\u5efa\u6a21\u578b\u7684\u8868\u793a\u548c\u4ea4\u4e92\u534f\u8bae\uff1b3\uff09\u4ee5\u4fe1\u4efb\u548c\u4e92\u8865\u6027\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4ef7\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u5411\u53ef\u4ee5\u91cd\u6784\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\uff0c\u4f7f\u5176\u56f4\u7ed5\u53c2\u4e0e\u534f\u4f5c\u610f\u4e49\u5efa\u6784\u7684\u4ee3\u7406\u5c55\u5f00\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u6784\u60f3AI\u8f85\u52a9\uff0c\u4ece\u5de5\u5177\u8f6c\u5411\u8ba4\u77e5\u5408\u4f5c\u4f19\u4f34\u3002CCS\u6846\u67b6\u4e3a\u5f00\u53d1\u80fd\u591f\u4e0e\u4eba\u7c7b\u4f19\u4f34\u5171\u540c\u601d\u8003\u7684AI\u961f\u53cb\u63d0\u4f9b\u4e86\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03\u534f\u4f5c\u610f\u4e49\u5efa\u6784\u3001\u5171\u540c\u6a21\u578b\u6784\u5efa\u548c\u6301\u7eed\u5b66\u4e60\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u4eba\u673a\u4e92\u8865\u3002", "topic": "agent analysis"}}
{"id": "2512.07832", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07832", "abs": "https://arxiv.org/abs/2512.07832", "authors": ["Matteo Boglioni", "Andrea Sgobbi", "Gabriel Tavernini", "Francesco Rita", "Marius Mosbach", "Tiago Pimentel"], "title": "Do Generalisation Results Generalise?", "comment": null, "summary": "A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u7684OOD\u6cdb\u5316\u6027\u80fd\u5728\u4e0d\u540c\u6d4b\u8bd5\u96c6\u95f4\u7684\u76f8\u5173\u6027\u6ca1\u6709\u7edf\u4e00\u8d8b\u52bf\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u6a21\u578b\u9009\u62e9\uff0c\u8868\u660e\u5355\u4e00OOD\u6570\u636e\u96c6\u8bc4\u4f30\u53ef\u80fd\u4e0d\u591f\u51c6\u786e\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u6cdb\u5316\u80fd\u529b\u7684\u7814\u7a76\u901a\u5e38\u53ea\u4f7f\u7528\u5355\u4e2aOOD\u6570\u636e\u96c6\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u6a21\u578b\u4f1a\u9047\u5230\u66f4\u590d\u6742\u591a\u6837\u7684\u6570\u636e\u504f\u79fb\uff0c\u8fd9\u79cd\u5355\u4e00\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u7136\u540e\u8ba1\u7b97\u8fd9\u4e9b\u6d4b\u8bd5\u96c6\u6027\u80fd\u4e4b\u95f4\u7684\u504f\u76f8\u5173\u7cfb\u6570\uff08\u63a7\u5236\u57df\u5185\u6027\u80fd\u7684\u5f71\u54cd\uff09\uff0c\u4ee5\u6b64\u8bc4\u4f30\u6cdb\u5316\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u5206\u6790OLMo2\u548cOPT\u6a21\u578b\u53d1\u73b0\uff0cOOD\u6cdb\u5316\u7ed3\u679c\u6ca1\u6709\u7edf\u4e00\u7684\u8d8b\u52bf\uff1a\u4efb\u610f\u4e24\u4e2aOOD\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u6b63\u76f8\u5173\u6216\u8d1f\u76f8\u5173\uff0c\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u5177\u4f53\u5206\u6790\u7684\u6a21\u578b\u9009\u62e9\u3002", "conclusion": "LLM\u7684OOD\u6cdb\u5316\u6027\u80fd\u8bc4\u4f30\u9700\u8981\u66f4\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u4e0d\u80fd\u4ec5\u4f9d\u8d56\u5355\u4e00\u6d4b\u8bd5\u96c6\uff0c\u56e0\u4e3a\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540cOOD\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u76f8\u5173\u6027\u6a21\u5f0f\u5404\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2512.06917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06917", "abs": "https://arxiv.org/abs/2512.06917", "authors": ["Clifford F", "Devika Jay", "Abhishek Sarkar", "Satheesh K Perepu", "Santhosh G S", "Kaushik Dey", "Balaraman Ravindran"], "title": "Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis", "comment": "Accepted at 4th Deployable AI Workshop at AAAI 2026", "summary": "As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a \"radical term\" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful \"Why this, and not that?\" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u8f68\u8ff9\u7ea7\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408Q\u503c\u5dee\u5f02\u548c\"\u6fc0\u8fdb\u9879\"\u7684\u65b0\u72b6\u6001\u91cd\u8981\u6027\u5ea6\u91cf\u6765\u6392\u540d\u6574\u4e2a\u8f68\u8ff9\uff0c\u8bc6\u522b\u6700\u4f18\u884c\u4e3a\u5e76\u63d0\u4f9b\u5bf9\u6bd4\u89e3\u91ca\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\uff08XRL\uff09\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u5355\u6b65\u51b3\u7b56\uff0c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u957f\u671f\u884c\u4e3a\u7684\u89e3\u91ca\u3002\u968f\u7740RL\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u900f\u660e\u53ef\u4fe1\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u89e3\u91ca\u667a\u80fd\u4f53\u957f\u671f\u8f68\u8ff9\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u548c\u805a\u5408\u65b0\u7684\u72b6\u6001\u91cd\u8981\u6027\u5ea6\u91cf\u6765\u6392\u540d\u6574\u4e2a\u8f68\u8ff9\u3002\u8be5\u5ea6\u91cf\u7ed3\u5408\u7ecf\u5178Q\u503c\u5dee\u5f02\u548c\"\u6fc0\u8fdb\u9879\"\uff08\u6355\u6349\u667a\u80fd\u4f53\u8fbe\u5230\u76ee\u6807\u7684\u4eb2\u548c\u5ea6\uff09\uff0c\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u72b6\u6001\u5173\u952e\u6027\u6d4b\u91cf\u3002\u901a\u8fc7\u4ece\u5173\u952e\u72b6\u6001\u751f\u6210\u53cd\u4e8b\u5b9e\u63a8\u6f14\uff0c\u63d0\u4f9b\"\u4e3a\u4ec0\u4e48\u9009\u62e9\u8fd9\u4e2a\u800c\u4e0d\u662f\u90a3\u4e2a\"\u7684\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u5728\u6807\u51c6OpenAI Gym\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u4ece\u5f02\u6784\u667a\u80fd\u4f53\u7ecf\u9a8c\u4e2d\u8bc6\u522b\u6700\u4f18\u8f68\u8ff9\u3002\u76f8\u6bd4\u7ecf\u5178\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u91cd\u8981\u6027\u5ea6\u91cf\u5728\u8bc6\u522b\u6700\u4f18\u884c\u4e3a\u65b9\u9762\u66f4\u6709\u6548\uff0c\u5e76\u80fd\u5c55\u793a\u667a\u80fd\u4f53\u9009\u62e9\u8def\u5f84\u76f8\u5bf9\u4e8e\u66ff\u4ee3\u65b9\u6848\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u91ca\u667a\u80fd\u4f53\u957f\u671f\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5206\u6790\u548c\u5bf9\u6bd4\u89e3\u91ca\uff0c\u5411\u53ef\u4fe1\u81ea\u4e3b\u7cfb\u7edf\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2512.06920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06920", "abs": "https://arxiv.org/abs/2512.06920", "authors": ["Alexandr Plashchinsky"], "title": "Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models", "comment": null, "summary": "We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.", "AI": {"tldr": "PGSRM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u6846\u67b6\uff0c\u4f7f\u7528\u7236\u6a21\u578b\u53c2\u8003\u8f93\u51fa\u5d4c\u5165\u4e0e\u5b50\u6a21\u578b\u751f\u6210\u8f93\u51fa\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u8bed\u4e49\u5956\u52b1\uff0c\u66ff\u4ee3\u4f20\u7edfRLHF\u4e2d\u7684\u4e8c\u5143\u5956\u52b1\u6216\u4eba\u5de5\u504f\u597d\u6570\u636e\u3002", "motivation": "\u4f20\u7edfRLHF\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u8bad\u7ec3\u590d\u6742\u7684\u5956\u52b1\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u7a33\u5b9a\u3002PGSRM\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5956\u52b1\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u7236\u6a21\u578b\uff08\u5982\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff09\u751f\u6210\u53c2\u8003\u8f93\u51fa\u7684\u5d4c\u5165\u8868\u793a\uff0c\u8ba1\u7b97\u5b50\u6a21\u578b\u751f\u6210\u8f93\u51fa\u5d4c\u5165\u4e0e\u53c2\u8003\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u5bc6\u96c6\u8bed\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u5e94\u7528\u4e8ePPO\u7b49\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5728\u4e94\u4e2a\u8bed\u8a00\u4efb\u52a1\u4e0a\uff0cPGSRM\u76f8\u6bd4\u4e8c\u5143\u5956\u52b1\u57fa\u7ebf\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u5956\u52b1\u6539\u8fdb\u548c\u66f4\u7a33\u5b9a\u7684PPO\u52a8\u6001\uff0c\u8868\u660e\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u5956\u52b1\u662fRLHF\u5f0f\u5956\u52b1\u5efa\u6a21\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5d4c\u5165\u8bed\u4e49\u5956\u52b1\u4e3a\u5c0f\u578bTransformer\u6a21\u578b\u7684\u7236\u5f15\u5bfc\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u989d\u5916\u6a21\u578b\u8bad\u7ec3\uff0c\u80fd\u4ea7\u751f\u66f4\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.06982", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06982", "abs": "https://arxiv.org/abs/2512.06982", "authors": ["Yu Yu", "Qian Xie", "Nairen Cao", "Li Jin"], "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding", "comment": "NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning", "summary": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u9a71\u52a8\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bbe\u8ba1\u591a\u6e90\u5f3a\u5316\u5b66\u4e60\u7684\u72b6\u6001\u7f16\u7801\u5668\uff0c\u5728\u4ea4\u901a\u63a7\u5236\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edfNAS\u548cGENIUS\u6846\u67b6\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u591a\u6e90\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5982\u4f55\u8bbe\u8ba1\u878d\u5408\u4f20\u611f\u5668\u6d4b\u91cf\u3001\u65f6\u5e8f\u4fe1\u53f7\u3001\u56fe\u50cf\u89c2\u6d4b\u548c\u6587\u672c\u6307\u4ee4\u7b49\u591a\u79cd\u4fe1\u606f\u6e90\u7684\u72b6\u6001\u7f16\u7801\u5668\u4ecd\u7136\u7f3a\u4e4f\u63a2\u7d22\u4e14\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u3002\u73b0\u6709NAS\u65b9\u6cd5\u5ffd\u7565\u4e86\u6a21\u5757\u4e2d\u95f4\u8f93\u51fa\u7684\u6709\u7528\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u591a\u6e90RL\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u590d\u5408\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u95ee\u9898\uff0c\u63d0\u51faLLM\u9a71\u52a8\u7684NAS\u6d41\u7a0b\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u548c\u4e2d\u95f4\u8f93\u51fa\u4fe1\u53f7\u6765\u6307\u5bfc\u9ad8\u6548\u641c\u7d22\u9ad8\u6027\u80fd\u590d\u5408\u72b6\u6001\u7f16\u7801\u5668\u3002", "result": "\u5728\u6df7\u5408\u81ea\u4e3b\u4ea4\u901a\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfNAS\u57fa\u7ebf\u548cLLM-based GENIUS\u6846\u67b6\u7528\u66f4\u5c11\u7684\u5019\u9009\u8bc4\u4f30\u53d1\u73b0\u66f4\u9ad8\u6027\u80fd\u7684\u67b6\u6784\u3002", "conclusion": "LLM\u9a71\u52a8\u7684NAS\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u6e90RL\u72b6\u6001\u7f16\u7801\u5668\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u4e2d\u95f4\u8f93\u51fa\u4fe1\u53f7\u548c\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u641c\u7d22\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07287", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07287", "abs": "https://arxiv.org/abs/2512.07287", "authors": ["Sijia Li", "Yuchen Huang", "Zifan Liu", "Zijian Li", "Jingjing fu", "Lei Song", "Jiang Bian", "Jun Zhang", "Rui Wang"], "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents", "comment": null, "summary": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.", "AI": {"tldr": "\u63d0\u51faSIT-Graph\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u72b6\u6001\u96c6\u6210\u5de5\u5177\u56fe\u6765\u589e\u5f3a\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\uff0c\u7ed3\u5408\u60c5\u666f\u8bb0\u5fc6\u548c\u7a0b\u5e8f\u8bb0\u5fc6\uff0c\u5728\u9700\u8981\u65f6\u68c0\u7d22\u72b6\u6001\u6458\u8981\uff0c\u5728\u5e38\u89c4\u6b65\u9aa4\u4e2d\u9075\u5faa\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u8981\u4e48\u5c06\u6574\u4e2a\u8f68\u8ff9\u6216\u9884\u5b9a\u4e49\u5b50\u4efb\u52a1\u89c6\u4e3a\u4e0d\u53ef\u5206\u5272\u5355\u5143\uff0c\u8981\u4e48\u4ec5\u5229\u7528\u5de5\u5177\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u96be\u4ee5\u9002\u5e94\u72b6\u6001\u548c\u4fe1\u606f\u968f\u8f6e\u6b21\u53d8\u5316\u7684\u60c5\u51b5\u3002", "method": "\u6784\u5efa\u72b6\u6001\u96c6\u6210\u5de5\u5177\u56fe(SIT-Graph)\uff1a\u9996\u5148\u4ece\u5386\u53f2\u5de5\u5177\u4f7f\u7528\u5e8f\u5217\u6784\u5efa\u5de5\u5177\u56fe\uff0c\u7136\u540e\u4e3a\u6bcf\u6761\u8fb9\u6dfb\u52a0\u5bf9\u8bdd\u548c\u5de5\u5177\u5386\u53f2\u7684\u7d27\u51d1\u72b6\u6001\u6458\u8981\u3002\u63a8\u7406\u65f6\uff0c\u5728\u9700\u8981\u56de\u5fc6\u5148\u524d\u4e0a\u4e0b\u6587\u65f6\u68c0\u7d22\u76f8\u5173\u8fb9\u7684\u72b6\u6001\u6458\u8981\u6765\u6307\u5bfc\u884c\u52a8\uff0c\u5728\u5e38\u89c4\u6b65\u9aa4\u4e2d\u9075\u5faa\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u6709\u72b6\u6001\u7684\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIT-Graph\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u8bb0\u5fc6\u548c\u56fe\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u5de5\u5177\u9009\u62e9\u548c\u66f4\u6709\u6548\u7684\u7ecf\u9a8c\u8f6c\u79fb\u3002", "conclusion": "SIT-Graph\u901a\u8fc7\u7ed3\u5408\u60c5\u666f\u8bb0\u5fc6\u548c\u7a0b\u5e8f\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e86\u4eba\u7c7b\u822c\u7684\u51b3\u7b56\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u4ee3\u7406\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.07417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07417", "abs": "https://arxiv.org/abs/2512.07417", "authors": ["Giray \u00d6n\u00fcr", "Azita Dabiri", "Bart De Schutter"], "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning", "comment": null, "summary": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u72b6\u6001\u53cd\u9988\u4ea4\u901a\u63a7\u5236\u5668\u7684\u53c2\u6570\uff0c\u7ed3\u5408\u4e86\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u7ba1\u7406\u7b56\u7565\uff08\u5982\u8def\u5f84\u5f15\u5bfc\u3001\u531d\u9053\u63a7\u5236\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff09\u4f9d\u8d56\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u867d\u7136\u7b80\u5355\u53cd\u5e94\u5feb\uff0c\u4f46\u7f3a\u4e4f\u5e94\u5bf9\u590d\u6742\u65f6\u53d8\u4ea4\u901a\u52a8\u6001\u7684\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53cd\u5e94\u6027\u53c8\u5177\u5907\u9002\u5e94\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u8c03\u6574\u72b6\u6001\u53cd\u9988\u4ea4\u901a\u63a7\u5236\u5668\u7684\u53c2\u6570\uff08\u800c\u975e\u76f4\u63a5\u9ad8\u9891\u63a7\u5236\u52a8\u4f5c\uff09\uff0c\u4ee5\u8f83\u4f4e\u9891\u7387\u8c03\u6574\u53c2\u6570\u3002\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u5c40\u90e8\u63a7\u5236\u5668\u5728\u90e8\u5206\u6545\u969c\u65f6\u53ef\u72ec\u7acb\u8fd0\u884c\u3002", "result": "\u5728\u6a21\u62df\u7684\u591a\u7c7b\u4ea4\u901a\u7f51\u7edc\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u4f18\u4e8e\u65e0\u63a7\u5236\u548c\u56fa\u5b9a\u53c2\u6570\u72b6\u6001\u53cd\u9988\u63a7\u5236\uff0c\u4e0e\u5355\u667a\u80fd\u4f53RL\u81ea\u9002\u5e94\u72b6\u6001\u53cd\u9988\u63a7\u5236\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5bf9\u90e8\u5206\u6545\u969c\u5177\u6709\u66f4\u597d\u7684\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u53c2\u6570\u8c03\u6574\u800c\u975e\u76f4\u63a5\u52a8\u4f5c\u63a7\u5236\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u5bb9\u9519\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.07828", "categories": ["cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2512.07828", "abs": "https://arxiv.org/abs/2512.07828", "authors": ["Jeremy Yang", "Noah Yonack", "Kate Zyskowski", "Denis Yarats", "Johnny Ho", "Jerry Ma"], "title": "The Adoption and Usage of AI Agents: Early Evidence from Perplexity", "comment": null, "summary": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.", "AI": {"tldr": "\u9996\u4e2a\u5927\u89c4\u6a21AI\u4ee3\u7406\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u5b9e\u5730\u7814\u7a76\uff0c\u5206\u6790\u4e86Perplexity\u7684Comet\u6d4f\u89c8\u5668\u53ca\u5176AI\u52a9\u624b\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u63ed\u793a\u4e86\u7528\u6237\u7fa4\u4f53\u3001\u4f7f\u7528\u5f3a\u5ea6\u548c\u7528\u4f8b\u7684\u5f02\u8d28\u6027\u3002", "motivation": "\u4e86\u89e3\u901a\u7528AI\u4ee3\u7406\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u91c7\u7528\u60c5\u51b5\u3001\u4f7f\u7528\u5f3a\u5ea6\u548c\u5177\u4f53\u7528\u4f8b\uff0c\u586b\u8865\u5927\u89c4\u6a21\u5b9e\u5730\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u57fa\u4e8ePerplexity\u7684Comet\u6d4f\u89c8\u5668\u53ca\u5176Comet Assistant\u4ee3\u7406\u7684\u6570\u4ebf\u533f\u540d\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u901a\u8fc7\u5206\u5c42\u4ee3\u7406\u5206\u7c7b\u6cd5\uff08\u4e3b\u9898\u3001\u5b50\u4e3b\u9898\u3001\u4efb\u52a1\u4e09\u7ea7\uff09\u7cfb\u7edf\u5206\u6790\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u65e9\u671f\u91c7\u7528\u8005\u3001\u9ad8GDP\u56fd\u5bb6\u7528\u6237\u3001\u6570\u5b57/\u77e5\u8bc6\u5bc6\u96c6\u578b\u884c\u4e1a\u4ece\u4e1a\u8005\u66f4\u53ef\u80fd\u4f7f\u7528AI\u4ee3\u7406\uff1b\u751f\u4ea7\u529b\u548c\u5b66\u4e60\u7814\u7a76\u5360\u67e5\u8be2\u768457%\uff1b\u4e2a\u4eba\u4f7f\u7528\u536055%\uff0c\u4e13\u4e1a\u548c\u6559\u8093\u5206\u522b\u536030%\u548c16%\uff1b\u77ed\u671f\u5185\u7528\u4f8b\u7c98\u6027\u5f3a\uff0c\u957f\u671f\u7528\u6237\u8f6c\u5411\u8ba4\u77e5\u5bfc\u5411\u4e3b\u9898\u3002", "conclusion": "AI\u4ee3\u7406\u7684\u91c7\u7528\u548c\u4f7f\u7528\u5b58\u5728\u663e\u8457\u5f02\u8d28\u6027\uff0c\u5176\u6269\u6563\u5bf9\u7814\u7a76\u8005\u3001\u4f01\u4e1a\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u6559\u80b2\u8005\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e00\u65b0\u5174AI\u80fd\u529b\u7c7b\u522b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.e23bd581", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Farchitecting-efficient-context-aware-multi-agent-framework-for-production%2F%3Futm_source=tldrai/1/0100019aeef6a1be-c5707be7-2ccc-4e6f-9616-fe892eab8ed9-000000/qS2Chm9Arm4FaEQ0vfOjJ-ulO_oMKAr5ngBQvvYYOac=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Farchitecting-efficient-context-aware-multi-agent-framework-for-production%2F%3Futm_source=tldrai/1/0100019aeef6a1be-c5707be7-2ccc-4e6f-9616-fe892eab8ed9-000000/qS2Chm9Arm4FaEQ0vfOjJ-ulO_oMKAr5ngBQvvYYOac=434", "authors": ["TLDR Newsletter"], "title": "Architecting efficient context-aware multi-agent framework for production", "comment": "Source: TLDR Newsletter, Date: 2025-12-05, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Farchitecting-efficient-context-aware-multi-agent-framework-for-production%2F%3Futm_source=tldrai/1/0100019aeef6a1be-c5707be7-2ccc-4e6f-9616-fe892eab8ed9-000000/qS2Chm9Arm4FaEQ0vfOjJ-ulO_oMKAr5ngBQvvYYOac=434", "summary": "Architecting efficient context-aware multi-agent framework for production (17 minute read) The landscape of AI agent development is shifting fast. Organizations are now deploying sophisticated, autonomous agents to handle long-horizon tasks. However, this ambition is being bottlenecked by context. The context stack in Google Agent Development Kit was developed to support context engineering. The open-source, multi-agent-native framework is built to make active context engineering achievable i...", "source": "tldr", "AI": {"tldr": "\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u89e3\u51b3AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u7684\u4e0a\u4e0b\u6587\u74f6\u9888\u95ee\u9898\uff0c\u652f\u6301\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u5904\u7406", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5f00\u53d1\u9762\u4e34\u4e0a\u4e0b\u6587\u74f6\u9888\uff0c\u7ec4\u7ec7\u5728\u90e8\u7f72\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u590d\u6742\u81ea\u4e3b\u667a\u80fd\u4f53\u65f6\uff0c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6210\u4e3a\u4e3b\u8981\u9650\u5236\u56e0\u7d20", "method": "\u57fa\u4e8eGoogle Agent Development Kit\u7684\u4e0a\u4e0b\u6587\u6808\uff0c\u5f00\u53d1\u5f00\u6e90\u7684\u591a\u667a\u80fd\u4f53\u539f\u751f\u6846\u67b6\uff0c\u5b9e\u73b0\u4e3b\u52a8\u4e0a\u4e0b\u6587\u5de5\u7a0b", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f7f\u4e3b\u52a8\u4e0a\u4e0b\u6587\u5de5\u7a0b\u53d8\u5f97\u53ef\u884c\uff0c\u89e3\u51b3\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u4e0a\u4e0b\u6587\u74f6\u9888\u95ee\u9898", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5de5\u7a0b\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u63a8\u52a8\u590d\u6742\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u90e8\u7f72", "topic": "agent analysis"}}
{"id": "tldr.2512.ce29c12f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FPhtq3h/1/0100019afda5395a-c3bdafbc-7a92-4ae7-b877-cf8766adf713-000000/UjJ-8am0jb_iaoGsy2lEB-yX8BvUIopRNv98YTAOW28=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FPhtq3h/1/0100019afda5395a-c3bdafbc-7a92-4ae7-b877-cf8766adf713-000000/UjJ-8am0jb_iaoGsy2lEB-yX8BvUIopRNv98YTAOW28=434", "authors": ["TLDR Newsletter"], "title": "Why We Built \u201cBlaBlaCar Data Copilot\u201d: Shifting Data Analysis Left", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FPhtq3h/1/0100019afda5395a-c3bdafbc-7a92-4ae7-b877-cf8766adf713-000000/UjJ-8am0jb_iaoGsy2lEB-yX8BvUIopRNv98YTAOW28=434", "summary": "Why We Built \u201cBlaBlaCar Data Copilot\u201d: Shifting Data Analysis Left (6 minute read) Data Copilot is an AI-driven IDE extension that empowers software engineers to perform data analysis directly in their coding environment. It eliminates reliance on BI consoles and reduces organizational silos. Featuring business-specific curated queries, transparent SQL/Python code generation, and heuristic Data Health Cards, Data Copilot enforces analytics best practices and unit testing earlier in the cycle....", "source": "tldr", "AI": {"tldr": "BlaBlaCar Data Copilot\u662f\u4e00\u6b3eAI\u9a71\u52a8\u7684IDE\u6269\u5c55\uff0c\u8ba9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u80fd\u5728\u7f16\u7801\u73af\u5883\u4e2d\u76f4\u63a5\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u51cf\u5c11\u5bf9BI\u63a7\u5236\u53f0\u7684\u4f9d\u8d56\u548c\u7ec4\u7ec7\u5b64\u5c9b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5206\u6790\u9700\u8981\u4f9d\u8d56BI\u63a7\u5236\u53f0\uff0c\u5bfc\u81f4\u7ec4\u7ec7\u5b64\u5c9b\u548c\u6d41\u7a0b\u5ef6\u8fdf\u3002\u4f5c\u8005\u5e0c\u671b\u8ba9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5728\u5f00\u53d1\u65e9\u671f\u5c31\u80fd\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u5b9e\u73b0\"\u5de6\u79fb\"\u7684\u6570\u636e\u5206\u6790\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1AI\u9a71\u52a8\u7684IDE\u6269\u5c55\uff0c\u63d0\u4f9b\u4e1a\u52a1\u7279\u5b9a\u7684\u7cbe\u9009\u67e5\u8be2\u3001\u900f\u660e\u7684SQL/Python\u4ee3\u7801\u751f\u6210\u3001\u542f\u53d1\u5f0f\u6570\u636e\u5065\u5eb7\u5361\u7b49\u529f\u80fd\uff0c\u5f3a\u5236\u5b9e\u65bd\u5206\u6790\u6700\u4f73\u5b9e\u8df5\u548c\u5355\u5143\u6d4b\u8bd5\u3002", "result": "\u521b\u5efa\u4e86Data Copilot\u5de5\u5177\uff0c\u4f7f\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u80fd\u5728\u7f16\u7801\u73af\u5883\u4e2d\u76f4\u63a5\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u51cf\u5c11\u5bf9BI\u63a7\u5236\u53f0\u7684\u4f9d\u8d56\uff0c\u6253\u7834\u7ec4\u7ec7\u5b64\u5c9b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6570\u636e\u5206\u6790\"\u5de6\u79fb\"\u5230\u5f00\u53d1\u73af\u5883\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5206\u6790\u6548\u7387\uff0c\u51cf\u5c11\u7ec4\u7ec7\u5b64\u5c9b\uff0c\u5e76\u66f4\u65e9\u5730\u5b9e\u65bd\u5206\u6790\u6700\u4f73\u5b9e\u8df5\u3002", "topic": "swe application"}}
{"id": "tldr.2512.7d036b26", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FogYaHh/2/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/YE7snMXV5ufV2f-Thc9HCCWyAauQz5OlJMBBYaGP84I=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FogYaHh/2/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/YE7snMXV5ufV2f-Thc9HCCWyAauQz5OlJMBBYaGP84I=434", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FogYaHh/2/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/YE7snMXV5ufV2f-Thc9HCCWyAauQz5OlJMBBYaGP84I=434", "summary": "Agents that don't suck (Sponsor) Most AI agents never reach production. Agent Bricks by Databricks fixes that by helping you build agents that actually work \u2014 accurate, reliable and grounded in your data. This product delivers practical quality, not hype: automatic evaluation, metrics tuned to your goals, and human feedback to keep improving accuracy. Generic benchmarks don't cut it. Agent Bricks measures performance on the tasks that matter to your business. The result: production agents you...", "source": "tldr", "AI": {"tldr": "Databricks\u63a8\u51faAgent Bricks\u4ea7\u54c1\uff0c\u5e2e\u52a9\u4f01\u4e1a\u6784\u5efa\u53ef\u5b9e\u9645\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\u7684AI\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u81ea\u52a8\u8bc4\u4f30\u3001\u5b9a\u5236\u5316\u6307\u6807\u548c\u4eba\u5de5\u53cd\u9988\u673a\u5236\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u7684\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u6570\u636e\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570AI\u667a\u80fd\u4f53\u65e0\u6cd5\u771f\u6b63\u6295\u5165\u751f\u4ea7\u73af\u5883\uff0c\u5b58\u5728\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002\u4f01\u4e1a\u9700\u8981\u80fd\u591f\u5b9e\u9645\u5de5\u4f5c\u7684\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7406\u8bba\u4e0a\u7684\u6982\u5ff5\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7Agent Bricks\u5e73\u53f0\u63d0\u4f9b\uff1a1\uff09\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\uff1b2\uff09\u6839\u636e\u4f01\u4e1a\u76ee\u6807\u5b9a\u5236\u7684\u6027\u80fd\u6307\u6807\uff1b3\uff09\u4eba\u5de5\u53cd\u9988\u673a\u5236\u6301\u7eed\u6539\u8fdb\u51c6\u786e\u6027\uff1b4\uff09\u57fa\u4e8e\u4f01\u4e1a\u5b9e\u9645\u4e1a\u52a1\u4efb\u52a1\u7684\u6027\u80fd\u6d4b\u91cf\uff0c\u800c\u975e\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u80fd\u591f\u6784\u5efa\u51fa\u51c6\u786e\u3001\u53ef\u9760\u3001\u57fa\u4e8e\u4f01\u4e1a\u6570\u636e\u7684\u751f\u4ea7\u7ea7\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4f20\u7edf\u667a\u80fd\u4f53\u65e0\u6cd5\u6295\u5165\u5b9e\u9645\u751f\u4ea7\u7684\u95ee\u9898\u3002", "conclusion": "Agent Bricks\u901a\u8fc7\u5b9e\u7528\u5bfc\u5411\u7684\u65b9\u6cd5\u5e2e\u52a9\u4f01\u4e1a\u6784\u5efa\u771f\u6b63\u53ef\u6295\u5165\u751f\u4ea7\u7684AI\u667a\u80fd\u4f53\uff0c\u5f3a\u8c03\u5b9e\u9645\u4e1a\u52a1\u4ef7\u503c\u800c\u975e\u7406\u8bba\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "tldr.2512.02508966", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.raf.xyz%2Fblog%2F03-how-i-keep-up-with-ai-generated-prs%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/NS6nG_tw2sSgxnOiNDZssE_B9smz-ZCYWe97Szv3N4k=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.raf.xyz%2Fblog%2F03-how-i-keep-up-with-ai-generated-prs%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/NS6nG_tw2sSgxnOiNDZssE_B9smz-ZCYWe97Szv3N4k=434", "authors": ["TLDR Newsletter"], "title": "How I keep up with AI-generated PRs", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.raf.xyz%2Fblog%2F03-how-i-keep-up-with-ai-generated-prs%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/NS6nG_tw2sSgxnOiNDZssE_B9smz-ZCYWe97Szv3N4k=434", "summary": "How I keep up with AI-generated PRs (3 minute read) AI can generate code really fast, but that code still needs to be reviewed. Automated PR tools can help, but they're not sufficient. This article presents a workflow that can cut the time it takes to review AI-generated PRs significantly. The workflow uses AI to summarize changes and identify potential issues while allowing the developer to stay in control of the final output.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e00\u4e2a\u5de5\u4f5c\u6d41\uff0c\u5229\u7528AI\u603b\u7ed3\u4ee3\u7801\u53d8\u66f4\u5e76\u8bc6\u522b\u6f5c\u5728\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11AI\u751f\u6210PR\u7684\u5ba1\u67e5\u65f6\u95f4\uff0c\u540c\u65f6\u8ba9\u5f00\u53d1\u8005\u4fdd\u6301\u6700\u7ec8\u63a7\u5236\u6743\u3002", "motivation": "AI\u80fd\u5feb\u901f\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u8fd9\u4e9b\u4ee3\u7801\u4ecd\u9700\u4eba\u5de5\u5ba1\u67e5\u3002\u73b0\u6709\u81ea\u52a8\u5316PR\u5de5\u5177\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5de5\u4f5c\u6d41\u6765\u5e94\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u5ba1\u67e5\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u5de5\u4f5c\u6d41\uff0c\u5229\u7528AI\u81ea\u52a8\u603b\u7ed3\u4ee3\u7801\u53d8\u66f4\u3001\u8bc6\u522b\u6f5c\u5728\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5f00\u53d1\u8005\u5bf9\u6700\u7ec8\u8f93\u51fa\u7684\u63a7\u5236\u6743\u3002", "result": "\u8be5\u5de5\u4f5c\u6d41\u80fd\u663e\u8457\u51cf\u5c11AI\u751f\u6210PR\u7684\u5ba1\u67e5\u65f6\u95f4\uff0c\u63d0\u9ad8\u5ba1\u67e5\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7AI\u8f85\u52a9\u7684\u5de5\u4f5c\u6d41\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u5f00\u53d1\u8005\u63a7\u5236\u6743\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347AI\u751f\u6210\u4ee3\u7801\u7684\u5ba1\u67e5\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2512.fe203bcb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1996538308697137277.html%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/ZoZ06LccycD770FzhPIyeDLm98YaQ3fyx6hRo52wC2A=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1996538308697137277.html%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/ZoZ06LccycD770FzhPIyeDLm98YaQ3fyx6hRo52wC2A=434", "authors": ["TLDR Newsletter"], "title": "Another DeepSeek moment", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1996538308697137277.html%3Futm_source=tldrnewsletter/1/0100019afdb44fd9-0c87237d-e80e-4aef-849e-24e53e1e1c55-000000/ZoZ06LccycD770FzhPIyeDLm98YaQ3fyx6hRo52wC2A=434", "summary": "Another DeepSeek moment (4 minute read) ZTE's Nubia M153 is a smartphone that runs ByteDance's DoubaoAI agent at the OS level. The agent has complete control over the phone. It can see the UI, choose/download apps, tap/type, make calls, and run multi-step task chains. The Doubao model is a massive, sparse Mixture of Experts model with full text and vision support. This thread contains several videos that show what the phone is capable of.", "source": "tldr", "AI": {"tldr": "\u4e2d\u5174Nubia M153\u624b\u673a\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u96c6\u6210\u4e86\u5b57\u8282\u8df3\u52a8\u7684DoubaoAI\u667a\u80fd\u4f53\uff0c\u8be5\u667a\u80fd\u4f53\u80fd\u5b8c\u5168\u63a7\u5236\u624b\u673a\uff0c\u6267\u884cUI\u8bc6\u522b\u3001\u5e94\u7528\u4e0b\u8f7d\u3001\u70b9\u51fb\u8f93\u5165\u3001\u901a\u8bdd\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u94fe\u7b49\u64cd\u4f5c\u3002", "motivation": "\u5c06AI\u667a\u80fd\u4f53\u6df1\u5ea6\u96c6\u6210\u5230\u624b\u673a\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u771f\u6b63\u7684AI\u539f\u751f\u624b\u673a\u4f53\u9a8c\uff0c\u8ba9AI\u80fd\u591f\u5b8c\u5168\u63a7\u5236\u8bbe\u5907\u5e76\u6267\u884c\u590d\u6742\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5b57\u8282\u8df3\u52a8\u7684Doubao\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u652f\u6301\u5b8c\u6574\u7684\u6587\u672c\u548c\u89c6\u89c9\u529f\u80fd\uff0c\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u76f4\u63a5\u96c6\u6210AI\u667a\u80fd\u4f53\u3002", "result": "\u5c55\u793a\u4e86\u624b\u673a\u80fd\u591f\u6267\u884c\u591a\u79cd\u590d\u6742\u4efb\u52a1\uff0c\u5305\u62ecUI\u8bc6\u522b\u3001\u5e94\u7528\u9009\u62e9\u4e0b\u8f7d\u3001\u70b9\u51fb\u8f93\u5165\u3001\u901a\u8bdd\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u94fe\u64cd\u4f5c\uff0c\u89c6\u9891\u6f14\u793a\u4e86\u624b\u673a\u7684\u5b9e\u9645\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662fAI\u667a\u80fd\u4f53\u6df1\u5ea6\u96c6\u6210\u5230\u79fb\u52a8\u8bbe\u5907\u64cd\u4f5c\u7cfb\u7edf\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86AI\u539f\u751f\u624b\u673a\u7684\u6f5c\u529b\uff0c\u53ef\u80fd\u4ee3\u8868\u667a\u80fd\u624b\u673a\u53d1\u5c55\u7684\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.936bb076", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.codacy.com%2Fai-reviewer%2F%3Futm_campaign=31129159-AI%2520Risk%2520Hub%2520%2526%2520AI%2520Reviewer%2520Launch%2520%257C%2520Newsletters%26utm_source=TLDR%26utm_medium=newsletter%26utm_content=RiskHub_AIReviewer/1/0100019afdeb0004-29db0d24-6ef5-486b-9d27-5dc19ce0de40-000000/EWi7If7aKPjh9XAo4QKSVh1qTTQB_RBlF2dSLGcaDHo=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.codacy.com%2Fai-reviewer%2F%3Futm_campaign=31129159-AI%2520Risk%2520Hub%2520%2526%2520AI%2520Reviewer%2520Launch%2520%257C%2520Newsletters%26utm_source=TLDR%26utm_medium=newsletter%26utm_content=RiskHub_AIReviewer/1/0100019afdeb0004-29db0d24-6ef5-486b-9d27-5dc19ce0de40-000000/EWi7If7aKPjh9XAo4QKSVh1qTTQB_RBlF2dSLGcaDHo=434", "authors": ["TLDR Newsletter"], "title": "Codacy launched AI Reviewer to boost Dev Experience", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.codacy.com%2Fai-reviewer%2F%3Futm_campaign=31129159-AI%2520Risk%2520Hub%2520%2526%2520AI%2520Reviewer%2520Launch%2520%257C%2520Newsletters%26utm_source=TLDR%26utm_medium=newsletter%26utm_content=RiskHub_AIReviewer/1/0100019afdeb0004-29db0d24-6ef5-486b-9d27-5dc19ce0de40-000000/EWi7If7aKPjh9XAo4QKSVh1qTTQB_RBlF2dSLGcaDHo=434", "summary": "Codacy launched AI Reviewer to boost Dev Experience (Sponsor) GenAI is rewriting your codebase faster than your devs can review it. Codacy's new AI Reviewer pairs deterministic static analysis with context-aware code reviews that catch issues missed by legacy scanners. See how it works", "source": "tldr", "AI": {"tldr": "Codacy\u63a8\u51faAI Reviewer\u5de5\u5177\uff0c\u7ed3\u5408\u9759\u6001\u5206\u6790\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\u5ba1\u67e5\uff0c\u63d0\u5347\u5f00\u53d1\u4f53\u9a8c", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u5feb\u901f\u91cd\u5199\u4ee3\u7801\u5e93\uff0c\u4f20\u7edf\u626b\u63cf\u5de5\u5177\u65e0\u6cd5\u8ddf\u4e0a\u5ba1\u67e5\u9700\u6c42\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u4ee3\u7801\u5ba1\u67e5\u89e3\u51b3\u65b9\u6848", "method": "\u5c06\u786e\u5b9a\u6027\u9759\u6001\u5206\u6790\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\u5ba1\u67e5\u76f8\u7ed3\u5408\uff0c\u6355\u6349\u4f20\u7edf\u626b\u63cf\u5668\u9057\u6f0f\u7684\u95ee\u9898", "result": "\u63a8\u51faAI Reviewer\u4ea7\u54c1\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4ee3\u7801\u8d28\u91cf\u68c0\u67e5\u548c\u5f00\u53d1\u4f53\u9a8c\u63d0\u5347", "conclusion": "AI Reviewer\u80fd\u591f\u6709\u6548\u5e94\u5bf9GenAI\u5feb\u901f\u4ee3\u7801\u751f\u6210\u5e26\u6765\u7684\u5ba1\u67e5\u6311\u6218\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf", "topic": "swe application"}}
{"id": "tldr.2512.6a9f754f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feliocapella.com%2Fblog%2Fai-library-migration-guide%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/BQ8YYi3o4f45AZz14wCfsHtCr2zU_HKD6H4PMIt_Cug=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feliocapella.com%2Fblog%2Fai-library-migration-guide%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/BQ8YYi3o4f45AZz14wCfsHtCr2zU_HKD6H4PMIt_Cug=434", "authors": ["TLDR Newsletter"], "title": "Migrating 6000 React tests using AI Agents and ASTs", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Feliocapella.com%2Fblog%2Fai-library-migration-guide%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/BQ8YYi3o4f45AZz14wCfsHtCr2zU_HKD6H4PMIt_Cug=434", "summary": "Migrating 6000 React tests using AI Agents and ASTs (7 minute read) Elio used Claude Code to migrate 970 test files (6,000+ tests) from React Testing Library v13 to v14 in just one week. Its team had Claude build a detailed migration guide first. They created an AST codemod to handle mechanical changes, then let Claude iteratively migrate 10 tests at a time while automatically running tests and checking coverage.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528AI\u4ee3\u7406\u548cAST\u57281\u5468\u5185\u8fc1\u79fb6000\u4e2aReact\u6d4b\u8bd5\uff0c\u4eceReact Testing Library v13\u5347\u7ea7\u5230v14", "motivation": "\u9700\u8981\u5c06\u5927\u91cfReact\u6d4b\u8bd5\uff086000+\u4e2a\u6d4b\u8bd5\uff0c970\u4e2a\u6587\u4ef6\uff09\u4eceReact Testing Library v13\u8fc1\u79fb\u5230v14\uff0c\u624b\u52a8\u8fc1\u79fb\u5de5\u4f5c\u91cf\u5927\u4e14\u8017\u65f6", "method": "1. \u4f7f\u7528Claude Code\u521b\u5efa\u8be6\u7ec6\u7684\u8fc1\u79fb\u6307\u5357\uff1b2. \u6784\u5efaAST\u4ee3\u7801\u8f6c\u6362\u5de5\u5177\u5904\u7406\u673a\u68b0\u6027\u53d8\u66f4\uff1b3. \u8ba9Claude\u4ee5\u6bcf\u6b2110\u4e2a\u6d4b\u8bd5\u7684\u8fed\u4ee3\u65b9\u5f0f\u8fc1\u79fb\uff0c\u540c\u65f6\u81ea\u52a8\u8fd0\u884c\u6d4b\u8bd5\u5e76\u68c0\u67e5\u8986\u76d6\u7387", "result": "\u6210\u529f\u57281\u5468\u5185\u5b8c\u6210\u4e86970\u4e2a\u6d4b\u8bd5\u6587\u4ef6\uff086000+\u6d4b\u8bd5\uff09\u7684\u8fc1\u79fb\u5de5\u4f5c", "conclusion": "AI\u4ee3\u7406\u4e0eAST\u7ed3\u5408\u7684\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5b8c\u6210\u5927\u89c4\u6a21\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf", "topic": "swe application"}}
{"id": "tldr.2512.4969bdbc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mckaywrigley.com%2Fposts%2Fopus-4.5%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/0ZpjKrpZv8zCQHHV6EXjufXi1WG0ggrT-ImCQhu994g=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mckaywrigley.com%2Fposts%2Fopus-4.5%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/0ZpjKrpZv8zCQHHV6EXjufXi1WG0ggrT-ImCQhu994g=434", "authors": ["TLDR Newsletter"], "title": "My Thoughts on Claude Opus 4.5", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.mckaywrigley.com%2Fposts%2Fopus-4.5%3Futm_source=tldrdev/1/0100019afdf05cd7-68050db4-377b-442f-8784-42bb6aaf3d25-000000/0ZpjKrpZv8zCQHHV6EXjufXi1WG0ggrT-ImCQhu994g=434", "summary": "My Thoughts on Claude Opus 4.5 (10 minute read) Claude Opus 4.5 is a generational \"unlock\" in AI. It is the best model for both code and agents. Users should treat Opus 4.5 as a trusted coworker, use Claude Code for programming, and build impactful agents using the SDK.", "source": "tldr", "AI": {"tldr": "Claude Opus 4.5\u662fAI\u9886\u57df\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u5728\u4ee3\u7801\u548c\u667a\u80fd\u4f53\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5e94\u4f5c\u4e3a\u53ef\u4fe1\u8d56\u7684\u540c\u4e8b\u4f7f\u7528", "motivation": "\u4ecb\u7ecdClaude Opus 4.5\u4f5c\u4e3aAI\u9886\u57df\u7684\u91cd\u5927\u7a81\u7834\uff0c\u5f3a\u8c03\u5176\u5728\u4ee3\u7801\u548c\u667a\u80fd\u4f53\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u65e8\u5728\u6307\u5bfc\u7528\u6237\u5982\u4f55\u5145\u5206\u5229\u7528\u8fd9\u4e00\u5148\u8fdb\u6a21\u578b", "method": "\u901a\u8fc7\u5206\u6790Claude Opus 4.5\u7684\u6027\u80fd\u7279\u70b9\uff0c\u63d0\u51fa\u5177\u4f53\u4f7f\u7528\u5efa\u8bae\uff1a\u4f5c\u4e3a\u53ef\u4fe1\u8d56\u7684\u540c\u4e8b\u3001\u4f7f\u7528Claude Code\u8fdb\u884c\u7f16\u7a0b\u3001\u5229\u7528SDK\u6784\u5efa\u6709\u5f71\u54cd\u529b\u7684\u667a\u80fd\u4f53", "result": "Claude Opus 4.5\u88ab\u5b9a\u4f4d\u4e3aAI\u9886\u57df\u7684\"\u4e16\u4ee3\u89e3\u9501\"\uff0c\u5728\u4ee3\u7801\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684AI\u534f\u4f5c\u5de5\u5177", "conclusion": "Claude Opus 4.5\u4ee3\u8868\u4e86AI\u6280\u672f\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u7528\u6237\u5e94\u5c06\u5176\u4f5c\u4e3a\u53ef\u4fe1\u8d56\u7684\u534f\u4f5c\u4f19\u4f34\uff0c\u5145\u5206\u5229\u7528\u5176\u5728\u7f16\u7a0b\u548c\u667a\u80fd\u4f53\u5f00\u53d1\u65b9\u9762\u7684\u4f18\u52bf", "topic": "code agent"}}
{"id": "tldr.2512.a5859318", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftomtunguz.com%2Fagent-asana-inflection%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/b6ACZ0MKcB3eld_8eSEQ9idCEyInIHYQIuBKttiAVN0=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftomtunguz.com%2Fagent-asana-inflection%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/b6ACZ0MKcB3eld_8eSEQ9idCEyInIHYQIuBKttiAVN0=434", "authors": ["TLDR Newsletter"], "title": "From 10 to 31 Tasks Daily: The Agent Inflection Point", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftomtunguz.com%2Fagent-asana-inflection%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/b6ACZ0MKcB3eld_8eSEQ9idCEyInIHYQIuBKttiAVN0=434", "summary": "From 10 to 31 Tasks Daily: The Agent Inflection Point (2 minute read) The velocity of work changes instantly when we give agents the right tools. Businesses can now use agents to complete tasks, just like software engineers have been doing for the last several months. It is now possible to spin up multiple parallel workstreams, multiplying daily output without working longer hours. The agent inflection point is already here.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u5de5\u5177\u4f7f\u4f01\u4e1a\u80fd\u50cf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e00\u6837\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\uff0c\u4ece\u6bcf\u592910\u4e2a\u4efb\u52a1\u63d0\u5347\u523031\u4e2a\uff0c\u5b9e\u73b0\u5de5\u4f5c\u4ea7\u51fa\u500d\u589e", "motivation": "\u4f01\u4e1a\u5e0c\u671b\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\uff0c\u5b9e\u73b0\u4efb\u52a1\u5b8c\u6210\u91cf\u7684\u663e\u8457\u589e\u957f\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u5de5\u4f5c\u65f6\u95f4\u3002\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5df2\u7ecf\u901a\u8fc7AI\u4ee3\u7406\u5de5\u5177\u63d0\u5347\u4e86\u751f\u4ea7\u529b\uff0c\u73b0\u5728\u4f01\u4e1a\u4e5f\u9700\u8981\u7c7b\u4f3c\u7684\u80fd\u529b", "method": "\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u5408\u9002\u7684\u5de5\u5177\uff0c\u4f7f\u5176\u80fd\u591f\u5e76\u884c\u5904\u7406\u591a\u4e2a\u5de5\u4f5c\u6d41\uff0c\u4ece\u800c\u540c\u65f6\u5b8c\u6210\u66f4\u591a\u4efb\u52a1", "result": "\u4f01\u4e1a\u80fd\u591f\u5c06\u6bcf\u65e5\u4efb\u52a1\u5b8c\u6210\u91cf\u4ece10\u4e2a\u63d0\u5347\u523031\u4e2a\uff0c\u5b9e\u73b0\u5de5\u4f5c\u4ea7\u51fa\u7684\u500d\u589e\uff0c\u4e14\u65e0\u9700\u5ef6\u957f\u5de5\u4f5c\u65f6\u95f4", "conclusion": "AI\u4ee3\u7406\u7684\u62d0\u70b9\u5df2\u7ecf\u5230\u6765\uff0c\u901a\u8fc7\u5408\u9002\u7684\u5de5\u5177\u914d\u7f6e\uff0c\u4f01\u4e1a\u53ef\u4ee5\u50cf\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4e00\u6837\u5927\u5e45\u63d0\u5347\u5de5\u4f5c\u6548\u7387", "topic": "agent analysis"}}
{"id": "tldr.2512.c2f74f3d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.protaige.com%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/HMcouxPej7HknWGff5ZH9-iiEMmJ_32Y3-8GhylolQE=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.protaige.com%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/HMcouxPej7HknWGff5ZH9-iiEMmJ_32Y3-8GhylolQE=434", "authors": ["TLDR Newsletter"], "title": "Protaig\u00e9", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.protaige.com%2F%3Futm_source=tldrfounders/1/0100019afe13fa76-4cb2e076-8ef5-4fc1-924c-58624830e2de-000000/HMcouxPej7HknWGff5ZH9-iiEMmJ_32Y3-8GhylolQE=434", "summary": "Protaig\u00e9 (Tool) Protaig\u00e9 is an AI platform that creates complete, on-brand marketing campaigns using coordinated agent workflows.", "source": "tldr", "AI": {"tldr": "Protaig\u00e9\u662f\u4e00\u4e2aAI\u5e73\u53f0\uff0c\u901a\u8fc7\u534f\u8c03\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u521b\u5efa\u5b8c\u6574\u3001\u7b26\u5408\u54c1\u724c\u5f62\u8c61\u7684\u8425\u9500\u6d3b\u52a8", "motivation": "\u4f20\u7edf\u8425\u9500\u6d3b\u52a8\u521b\u5efa\u8fc7\u7a0b\u590d\u6742\u3001\u8017\u65f6\u4e14\u96be\u4ee5\u4fdd\u6301\u54c1\u724c\u4e00\u81f4\u6027\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u5347\u6548\u7387\u548c\u6548\u679c", "method": "\u91c7\u7528\u534f\u8c03\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u67b6\u6784\uff0c\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u5206\u5de5\u534f\u4f5c\uff0c\u5171\u540c\u5b8c\u6210\u8425\u9500\u6d3b\u52a8\u7684\u5404\u4e2a\u7ec4\u6210\u90e8\u5206", "result": "\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5b8c\u6574\u7684\u8425\u9500\u6d3b\u52a8\uff0c\u5305\u62ec\u5185\u5bb9\u521b\u4f5c\u3001\u89c6\u89c9\u8bbe\u8ba1\u3001\u6e20\u9053\u89c4\u5212\u7b49\uff0c\u4fdd\u6301\u54c1\u724c\u4e00\u81f4\u6027", "conclusion": "Protaig\u00e9\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5728\u8425\u9500\u81ea\u52a8\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8425\u9500\u6d3b\u52a8\u7684\u521b\u5efa\u6548\u7387\u548c\u8d28\u91cf", "topic": "agent analysis"}}
{"id": "tldr.2512.a6c0ddcd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomshardware.com%2Ftech-industry%2Fcyber-security%2Fresearchers-uncover-critical-ai-ide-flaws-exposing-developers-to-data-theft-and-rce%3Futm_source=tldrinfosec/1/0100019afe4b61fb-2aca89ac-1354-4c6d-a19f-a61c55019f16-000000/Lo3LyLKRbb-hEKdovyIIpKflHoEP-7EvCjM4M1FP6OQ=434", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomshardware.com%2Ftech-industry%2Fcyber-security%2Fresearchers-uncover-critical-ai-ide-flaws-exposing-developers-to-data-theft-and-rce%3Futm_source=tldrinfosec/1/0100019afe4b61fb-2aca89ac-1354-4c6d-a19f-a61c55019f16-000000/Lo3LyLKRbb-hEKdovyIIpKflHoEP-7EvCjM4M1FP6OQ=434", "authors": ["TLDR Newsletter"], "title": "Critical flaws found in AI development tools dubbed an 'IDEsaster' \u2014 data theft and remote code execution possible", "comment": "Source: TLDR Newsletter, Date: 2025-12-08, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomshardware.com%2Ftech-industry%2Fcyber-security%2Fresearchers-uncover-critical-ai-ide-flaws-exposing-developers-to-data-theft-and-rce%3Futm_source=tldrinfosec/1/0100019afe4b61fb-2aca89ac-1354-4c6d-a19f-a61c55019f16-000000/Lo3LyLKRbb-hEKdovyIIpKflHoEP-7EvCjM4M1FP6OQ=434", "summary": "Critical flaws found in AI development tools dubbed an 'IDEsaster' \u2014 data theft and remote code execution possible (3 minute read) A six-month investigation uncovered over 30 vulnerabilities across every major AI-integrated IDE tested, including GitHub Copilot, Cursor, Claude Code, and JetBrains products, resulting in at least 24 assigned CVEs. The attack chain exploits prompt injection via rule files, READMEs, or malicious MCP servers to hijack AI agents, which then abuse legitimate IDE feat...", "source": "tldr", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41AI\u96c6\u6210IDE\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u52ab\u6301AI\u4ee3\u7406\uff0c\u5bfc\u81f4\u6570\u636e\u7a83\u53d6\u548c\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c", "motivation": "\u968f\u7740AI\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff08IDE\uff09\u7684\u666e\u53ca\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u51f8\u663e\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u4e3b\u6d41AI\u96c6\u6210IDE\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u6570\u636e\u7a83\u53d6\u548c\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u98ce\u9669\u3002", "method": "\u8fdb\u884c\u4e86\u4e3a\u671f\u516d\u4e2a\u6708\u7684\u8c03\u67e5\uff0c\u6d4b\u8bd5\u4e86\u5305\u62ecGitHub Copilot\u3001Cursor\u3001Claude Code\u548cJetBrains\u4ea7\u54c1\u5728\u5185\u7684\u6240\u6709\u4e3b\u6d41AI\u96c6\u6210IDE\uff0c\u901a\u8fc7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u94fe\u5229\u7528\u89c4\u5219\u6587\u4ef6\u3001README\u6216\u6076\u610fMCP\u670d\u52a1\u5668\u6765\u52ab\u6301AI\u4ee3\u7406\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u7684AI\u96c6\u6210IDE\u4e2d\u53d1\u73b0\u4e86\u8d85\u8fc730\u4e2a\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u81f3\u5c1124\u4e2aCVE\u7f16\u53f7\u88ab\u5206\u914d\u3002\u653b\u51fb\u94fe\u80fd\u591f\u6210\u529f\u52ab\u6301AI\u4ee3\u7406\u5e76\u6ee5\u7528\u5408\u6cd5IDE\u529f\u80fd\uff0c\u5b9e\u73b0\u6570\u636e\u7a83\u53d6\u548c\u8fdc\u7a0b\u4ee3\u7801\u6267\u884c\u3002", "conclusion": "AI\u96c6\u6210IDE\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u52a0\u5f3a\u5b89\u5168\u9632\u62a4\u63aa\u65bd\uff0c\u7279\u522b\u662f\u5728\u63d0\u793a\u6ce8\u5165\u9632\u5fa1\u548cAI\u4ee3\u7406\u5b89\u5168\u63a7\u5236\u65b9\u9762\u3002", "topic": "swe application"}}
{"id": "wechat.2512.5ad24b7d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&mid=2247544704&idx=1&sn=3ca130b702b7f1e505df3ac39aa2c08f&chksm=ea4877d25b051c9f11046be7d7a3d313643c9f95043023e507a44100c89dfdd7fabc0ba86e59#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&mid=2247544704&idx=1&sn=3ca130b702b7f1e505df3ac39aa2c08f&chksm=ea4877d25b051c9f11046be7d7a3d313643c9f95043023e507a44100c89dfdd7fabc0ba86e59#rd", "authors": ["\u7b97\u6cd5\u8fdb\u9636"], "title": "\u4e00\u6587\u770b\u5168\uff01DeepSeek<em class=\"highlight\">\u5927\u6a21\u578b</em>\u67b6\u6784", "comment": "Source: WeChat, Published: 2025-12-09 12:01:57", "summary": "\u4e3b\u8981\u6a21\u578b\u4ee5\u7ea2\u8272\u663e\u793a\u3002\u6b63\u5982\u6211\u5728\u4e5d\u6708\u4efd\u6240\u9884\u6d4b\u7684\u90a3\u6837\uff0cDeepSeek V3.2-Exp \u7684\u53d1\u5e03\u65e8\u5728\u4e3a\u6258\u7ba1\u521a\u521a\u53d1\u5e03\u7684 V3.2 \u6a21\u578b\u51c6\u5907\u751f\u6001\u7cfb\u7edf\u548c\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\u3002V3.2-Exp \u548c V3.2 \u4f7f\u7528\u4e86\u4e00\u79cd\u975e\u6807\u51c6\u7684\u7a00\u758f\u6ce8\u610f\u529b\uff08Sparse Attention\uff09\u53d8\u4f53\uff0c\u8fd9\u9700\u8981\u5b9a\u5236\u4ee3\u7801\uff0c", "AI": {"tldr": "\u4e3b\u8981\u6a21\u578b\u4ee5\u7ea2\u8272\u663e\u793a\u3002\u6b63\u5982\u6211\u5728\u4e5d\u6708\u4efd\u6240\u9884\u6d4b\u7684\u90a3\u6837\uff0cDeepSeek V3.2-Exp \u7684\u53d1\u5e03\u65e8\u5728\u4e3a\u6258\u7ba1\u521a\u521a\u53d1\u5e03\u7684 V3.2 \u6a21\u578b\u51c6\u5907\u751f\u6001\u7cfb\u7edf\u548c\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\u3002V3.2-Exp \u548c V3.2 \u4f7f\u7528\u4e86\u4e00\u79cd\u975e\u6807\u51c6\u7684\u7a00\u758f\u6ce8\u610f\u529b\uff08Sparse Attention\uff09\u53d8\u4f53\uff0c\u8fd9\u9700\u8981\u5b9a\u5236\u4ee3\u7801\uff0c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.50cc35d1", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MjYwMDI4MA==&mid=2247569750&idx=2&sn=ca710002a2d01e09f2ed0c466ee6d54e&chksm=cf9d4472fadcb4b9f9367464226063aab0baac5efc5291cd1f70b4d6a860d73bcfc1f4afc9d3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MjYwMDI4MA==&mid=2247569750&idx=2&sn=ca710002a2d01e09f2ed0c466ee6d54e&chksm=cf9d4472fadcb4b9f9367464226063aab0baac5efc5291cd1f70b4d6a860d73bcfc1f4afc9d3#rd", "authors": ["\u5357\u65b9\u7535\u7f51\u6280\u672f"], "title": "\u3010\u4e13\u5bb6\u62a5\u544a\u3011\u8d75\u4fca\u534e\u6559\u6388\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff1a\u4eba\u673a\u534f\u540c\u4e0e\u8303\u5f0f\u53d8\u9769", "comment": "Source: WeChat, Published: 2025-12-09 10:24:15", "summary": "\u5728\u4f1a\u4e0a\uff0c\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u8d75\u4fca\u534e\u6559\u6388\u4f5c\u4e86\u9898\u4e3a\u201c \u5927\u6a21\u578b\u667a\u80fd\u4f53\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff1a\u4eba\u673a\u534f\u540c\u4e0e\u8303\u5f0f\u53d8\u9769\u201d\u7684\u4e13\u9898\u62a5\u544a\uff0c\u5f81\u5f97\u8d75\u6559\u6388\u540c\u610f\uff0c\u7279\u4e0e\u60a8\u5206\u4eab\uff01", "AI": {"tldr": "\u5728\u4f1a\u4e0a\uff0c\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u8d75\u4fca\u534e\u6559\u6388\u4f5c\u4e86\u9898\u4e3a\u201c \u5927\u6a21\u578b\u667a\u80fd\u4f53\u5728\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff1a\u4eba\u673a\u534f\u540c\u4e0e\u8303\u5f0f\u53d8\u9769\u201d\u7684\u4e13\u9898\u62a5\u544a\uff0c\u5f81\u5f97\u8d75\u6559\u6388\u540c\u610f\uff0c\u7279\u4e0e\u60a8\u5206\u4eab\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.08f4ecf1", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498344&idx=1&sn=e656c4aae0b2c1621a7805a7aa04c5e2&chksm=fd37e3802e21e971f843a7e06233b08b5bcd1d6a7852270617760cba9961528488d0224943d5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2MzEwNzQ3MA==&mid=2247498344&idx=1&sn=e656c4aae0b2c1621a7805a7aa04c5e2&chksm=fd37e3802e21e971f843a7e06233b08b5bcd1d6a7852270617760cba9961528488d0224943d5#rd", "authors": ["\u4e30\u519c\u4fe1\u606f"], "title": "\u667a\u6167\u519c\u4e1a\u7684\u65b0\u5f15\u64ce\uff1a\u89e3\u5bc6\u519c\u4e1a\u5782\u76f4<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u843d\u5730\u4e0e\u672a\u6765\u200c\u200c", "comment": "Source: WeChat, Published: 2025-12-09 09:51:08", "summary": "\u6210\u529f\u6848\u4f8b\uff1a \u4e2d\u56fd\u519c\u4e1a\u5927\u5b66\u7684 \u201c\u795e\u519c\u5927\u6a21\u578b3.0\u201d \u662f\u8fd9\u4e00\u9886\u57df\u7684\u6807\u6746\u3002\u5b83\u4e0d\u4ec5\u5f00\u6e90\uff0c\u8986\u76d6\u4e8690%\u7684\u519c\u4e1a\u5b66\u79d1\u548c80%\u7684\u519c\u4e1a\u573a\u666f\uff0c\u8fd8\u914d\u5957\u63a8\u51fa\u4e86\u5305\u542b36\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u5e73\u53f0 \u3002", "AI": {"tldr": "\u6210\u529f\u6848\u4f8b\uff1a \u4e2d\u56fd\u519c\u4e1a\u5927\u5b66\u7684 \u201c\u795e\u519c\u5927\u6a21\u578b3.0\u201d \u662f\u8fd9\u4e00\u9886\u57df\u7684\u6807\u6746\u3002\u5b83\u4e0d\u4ec5\u5f00\u6e90\uff0c\u8986\u76d6\u4e8690%\u7684\u519c\u4e1a\u5b66\u79d1\u548c80%\u7684\u519c\u4e1a\u573a\u666f\uff0c\u8fd8\u914d\u5957\u63a8\u51fa\u4e86\u5305\u542b36\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u5e73\u53f0 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.81f08552", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODU5Mzg1Mg==&mid=2247484017&idx=3&sn=15e8005db71113fd12e5680cd9081ddd&chksm=97408740b5805d8d1a7461df6b30366cf04f169b95a16553abf265f9eb34c552d50f616d4623#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODU5Mzg1Mg==&mid=2247484017&idx=3&sn=15e8005db71113fd12e5680cd9081ddd&chksm=97408740b5805d8d1a7461df6b30366cf04f169b95a16553abf265f9eb34c552d50f616d4623#rd", "authors": ["\u4e2d\u56fdAOPA\u65e0\u4eba\u673a\u5e94\u7528\u6280\u80fd"], "title": "AI\u5b66\u4e60\uff08\u7b2c\u4e8c\u671f\uff09| \u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>\u67b6\u6784\uff1a\u4ece\u4e09\u79cd\u4e3b\u6d41\u67b6\u6784\u5230\u6280\u672f\u6f14\u8fdb", "comment": "Source: WeChat, Published: 2025-12-09 09:50:26", "summary": "\u6211\u4eec\u53ef\u4ee5\u628a\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u4e3a\u4e00\u4e2a\u6781\u5176\u806a\u660e\u7684\u201c\u6587\u672c\u5904\u7406\u5668\u201d\u3002\u6839\u636e\u5b83\u5904\u7406\u4efb\u52a1\u7684\u65b9\u5f0f\u4e0d\u540c\uff0c\u4e3b\u8981\u5206\u4e3a\u4e09\u79cd\u67b6\u6784\uff1aEncoder-only \u67b6\u6784\uff08\u53ea\u7f16\u7801\uff0c\u64c5\u957f\u201c\u7406\u89e3\u201d\uff09", "AI": {"tldr": "\u6211\u4eec\u53ef\u4ee5\u628a\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u4e3a\u4e00\u4e2a\u6781\u5176\u806a\u660e\u7684\u201c\u6587\u672c\u5904\u7406\u5668\u201d\u3002\u6839\u636e\u5b83\u5904\u7406\u4efb\u52a1\u7684\u65b9\u5f0f\u4e0d\u540c\uff0c\u4e3b\u8981\u5206\u4e3a\u4e09\u79cd\u67b6\u6784\uff1aEncoder-only \u67b6\u6784\uff08\u53ea\u7f16\u7801\uff0c\u64c5\u957f\u201c\u7406\u89e3\u201d\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.588f7485", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493923&idx=1&sn=59fb3dd5426ee50b1723ba1b51630e4e&chksm=fafd9062a7a225ec3dd6076bd830a4711f00cf7ccfd385af2312a29dc3dbd31e82fd7e93520d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493923&idx=1&sn=59fb3dd5426ee50b1723ba1b51630e4e&chksm=fafd9062a7a225ec3dd6076bd830a4711f00cf7ccfd385af2312a29dc3dbd31e82fd7e93520d#rd", "authors": ["\u6155\u5bb9\u5343\u8bed"], "title": "\u5fc5\u85cf\u5e72\u8d27\uff012025 \u6700\u65b0\u6700\u5168<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b66\u4e60\u8d44\u6e90\u5305\uff08\u7c89\u4e1d\u4e13\u4eab\u7248\uff09", "comment": "Source: WeChat, Published: 2025-12-09 09:38:25", "summary": "\u8d44\u6599\u540d\u79f0\uff1a\u5927\u6a21\u578b\u4ee3\u7801\u6587\u6863\u8d44\u6599\u7f51\u76d8\u94fe\u63a5\uff1ahttps\uff1a//pan.baidu.com/s/1yyw3KhO2q3lLCli1nJGt3Q\uff1fpwd=ss45 \u63d0\u53d6\u7801\uff1a ss45\u8d44\u6599\u5305\u542b\uff1aTansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/\u667a\u80fd\u5ba2\u670d\u7b49\u9879\u76ee\u6e90\u7801\uff0c\u9762\u8bd5\u9898\uff0c\u7535\u5b50\u4e66\u7c4d\uff0c\u5434\u6069\u8fbe&\u674e\u98de\u98de\u8bba\u6587\uff0c\u767d\u76ae\u4e66\u7b49\uff0cGi", "AI": {"tldr": "\u8d44\u6599\u540d\u79f0\uff1a\u5927\u6a21\u578b\u4ee3\u7801\u6587\u6863\u8d44\u6599\u7f51\u76d8\u94fe\u63a5\uff1ahttps\uff1a//pan.baidu.com/s/1yyw3KhO2q3lLCli1nJGt3Q\uff1fpwd=ss45 \u63d0\u53d6\u7801\uff1a ss45\u8d44\u6599\u5305\u542b\uff1aTansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/\u667a\u80fd\u5ba2\u670d\u7b49\u9879\u76ee\u6e90\u7801\uff0c\u9762\u8bd5\u9898\uff0c\u7535\u5b50\u4e66\u7c4d\uff0c\u5434\u6069\u8fbe&\u674e\u98de\u98de\u8bba\u6587\uff0c\u767d\u76ae\u4e66\u7b49\uff0cGi", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.0e58595f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzY0MDE5NDgxOA==&mid=2247483935&idx=1&sn=90d47dd8f60d6b8d8302bc27086fc550&chksm=f16dd88a7fb88ebb0b74522eb67e95d3faf48f32dd9fc9cde1abf0636454f22c3bc14a595b51#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzY0MDE5NDgxOA==&mid=2247483935&idx=1&sn=90d47dd8f60d6b8d8302bc27086fc550&chksm=f16dd88a7fb88ebb0b74522eb67e95d3faf48f32dd9fc9cde1abf0636454f22c3bc14a595b51#rd", "authors": ["AI\u5927\u6a21\u578b\u5c0f\u660e"], "title": "\u3010\u7c89\u4e1d\u4e13\u4eab\u30112025\u5e74\u6700\u65b0\u6700\u5168\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b66\u4e60\u8d44\u6e90\u5305\uff01\uff01", "comment": "Source: WeChat, Published: 2025-12-09 09:04:27", "summary": "\u8d44\u6599\u4e0b\u8f7d\u8d44\u6599\u540d\u79f0\uff1a\u5927\u6a21\u578b\u4ee3\u7801\u6587\u6863\u8d44\u6599\u7f51\u76d8\u94fe\u63a5\uff1ahttps\uff1a//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA\uff1fpwd=itn5 \u63d0\u53d6\u7801\uff1a itn5\u8d44\u6599\u5305\u542b\uff1aTansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/\u667a\u80fd\u5ba2\u670d\u7b49\u9879\u76ee\u6e90\u7801\uff0c\u9762\u8bd5\u9898\uff0c\u7535\u5b50\u4e66\u7c4d\uff0c\u5434\u6069\u8fbe&\u674e\u98de\u98de\u8bba\u6587\uff0c\u767d", "AI": {"tldr": "\u8d44\u6599\u4e0b\u8f7d\u8d44\u6599\u540d\u79f0\uff1a\u5927\u6a21\u578b\u4ee3\u7801\u6587\u6863\u8d44\u6599\u7f51\u76d8\u94fe\u63a5\uff1ahttps\uff1a//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA\uff1fpwd=itn5 \u63d0\u53d6\u7801\uff1a itn5\u8d44\u6599\u5305\u542b\uff1aTansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/\u667a\u80fd\u5ba2\u670d\u7b49\u9879\u76ee\u6e90\u7801\uff0c\u9762\u8bd5\u9898\uff0c\u7535\u5b50\u4e66\u7c4d\uff0c\u5434\u6069\u8fbe&\u674e\u98de\u98de\u8bba\u6587\uff0c\u767d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.68c03d00", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247486266&idx=1&sn=7ab6dfbc010c867ced3d21a0f50e43b8&chksm=e88773fcd4a14cbb099d56594ca719ecc690e816dab22e26d27a497052787105aaa4b255a9b8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0MjM0MTg4MQ==&mid=2247486266&idx=1&sn=7ab6dfbc010c867ced3d21a0f50e43b8&chksm=e88773fcd4a14cbb099d56594ca719ecc690e816dab22e26d27a497052787105aaa4b255a9b8#rd", "authors": ["\u61c2\u70b9AI\u7684\u6d77\u6587"], "title": "\u3010\u7c89\u4e1d\u4e13\u5c5e\u798f\u5229\u30112025\u5e74\u6700\u65b0\u6700\u5168\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b66\u4e60\u8d44\u6e90\u5305\uff01\uff01", "comment": "Source: WeChat, Published: 2025-12-09 05:50:19", "summary": "\u8d44\u6599\u4e0b\u8f7d\u8d44\u6599\u540d\u79f0\uff1a\u5927\u6a21\u578b\u4ee3\u7801\u6587\u6863\u8d44\u6599\u7f51\u76d8\u94fe\u63a5\uff1ahttps\uff1a//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA\uff1fpwd=itn5 \u63d0\u53d6\u7801\uff1a itn5\u8d44\u6599\u5305\u542b\uff1aTansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/\u667a\u80fd\u5ba2\u670d\u7b49\u9879\u76ee\u6e90\u7801\uff0c\u9762\u8bd5\u9898\uff0c\u7535\u5b50\u4e66\u7c4d\uff0c\u5434\u6069\u8fbe&\u674e\u98de\u98de\u8bba\u6587\uff0c\u767d", "AI": {"tldr": "\u8d44\u6599\u4e0b\u8f7d\u8d44\u6599\u540d\u79f0\uff1a\u5927\u6a21\u578b\u4ee3\u7801\u6587\u6863\u8d44\u6599\u7f51\u76d8\u94fe\u63a5\uff1ahttps\uff1a//pan.baidu.com/s/14UcHrKze8rxPw8_pdBxrgA\uff1fpwd=itn5 \u63d0\u53d6\u7801\uff1a itn5\u8d44\u6599\u5305\u542b\uff1aTansformer/RAGflow/MCP/n8n/A2A/Cursor/Dify/Coze/\u667a\u80fd\u5ba2\u670d\u7b49\u9879\u76ee\u6e90\u7801\uff0c\u9762\u8bd5\u9898\uff0c\u7535\u5b50\u4e66\u7c4d\uff0c\u5434\u6069\u8fbe&\u674e\u98de\u98de\u8bba\u6587\uff0c\u767d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.ba1bcbb0", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMTcxNzU3OA==&mid=2247489366&idx=1&sn=7953af78ad8cd8b940cab7d9fd8b879d&chksm=c35a61520e158156f3a3b231e5917f5859a591ea23e491ac3b7f3397d641b264a219ed0399c4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMTcxNzU3OA==&mid=2247489366&idx=1&sn=7953af78ad8cd8b940cab7d9fd8b879d&chksm=c35a61520e158156f3a3b231e5917f5859a591ea23e491ac3b7f3397d641b264a219ed0399c4#rd", "authors": ["AI\u4fe1\u606f\u98ce\u5411"], "title": "\u5168\u7403\u9876\u7ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6700\u65b0\u6392\u540d\u51fa\u7089\uff1a\u4e2d\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8868\u73b0\u4f18\u79c0\uff0cDeepSeek V3.2 \u4e0e Kimi K2 Thinking \u5747\u6324\u8fdb\u524d 10", "comment": "Source: WeChat, Published: 2025-12-09 01:57:52", "summary": "\u6839\u636e Artificial Analysis\u7f51\u7ad9\u7684\u6700\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff08\u622a\u81f32025\u5e7412\u6708\uff09\uff0c\u5168\u7403\u9876\u7ea7\u5927\u6a21\u578b\u5728 Intelligence\uff08\u667a\u529b\u6307\u6570\uff09\u3001Speed\uff08\u901f\u5ea6\uff09\u548c Price\uff08\u4ef7\u683c\uff09\u4e09\u5927\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u65e5\u8d8b\u5206\u660e\u3002", "AI": {"tldr": "\u6839\u636e Artificial Analysis\u7f51\u7ad9\u7684\u6700\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff08\u622a\u81f32025\u5e7412\u6708\uff09\uff0c\u5168\u7403\u9876\u7ea7\u5927\u6a21\u578b\u5728 Intelligence\uff08\u667a\u529b\u6307\u6570\uff09\u3001Speed\uff08\u901f\u5ea6\uff09\u548c Price\uff08\u4ef7\u683c\uff09\u4e09\u5927\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u65e5\u8d8b\u5206\u660e\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
