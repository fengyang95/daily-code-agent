{"id": "2601.00818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00818", "abs": "https://arxiv.org/abs/2601.00818", "authors": ["Chandra Sekhar Kubam"], "title": "Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making", "comment": "8 pages", "summary": "Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684Agentic AI\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u81ea\u4e3b\u3001\u900f\u660e\u3001\u5b9e\u65f6\u7684\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u51b3\u7b56\u7cfb\u7edf\uff0c\u76f8\u6bd4\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u7684\u5feb\u901f\u6570\u5b57\u5316\u5bf9\u81ea\u4e3b\u3001\u900f\u660e\u3001\u5b9e\u65f6\u7684\u4fe1\u7528\u98ce\u9669\u51b3\u7b56\u7cfb\u7edf\u63d0\u51fa\u4e86\u8feb\u5207\u9700\u6c42\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u867d\u7136\u64c5\u957f\u6a21\u5f0f\u8bc6\u522b\uff0c\u4f46\u7f3a\u4e4f\u73b0\u4ee3\u91d1\u878d\u8fd0\u8425\u6240\u9700\u7684\u9002\u5e94\u6027\u63a8\u7406\u3001\u60c5\u5883\u611f\u77e5\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aAgentic AI\u6846\u67b6\uff0c\u5305\u542b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u53ef\u89e3\u91caAI\u6a21\u5757\u548c\u5b9e\u65f6\u6570\u636e\u5438\u6536\u7ba1\u9053\u3002\u7cfb\u7edf\u5305\u62ec\u667a\u80fd\u4f53\u534f\u4f5c\u534f\u8bae\u3001\u98ce\u9669\u8bc4\u5206\u5f15\u64ce\u3001\u53ef\u89e3\u91ca\u6027\u5c42\u548c\u6301\u7eed\u53cd\u9988\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8be5\u7cfb\u7edf\u5728\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\u3002\u4f46\u4ecd\u5b58\u5728\u6a21\u578b\u6f02\u79fb\u98ce\u9669\u3001\u9ad8\u7ef4\u6570\u636e\u89e3\u91ca\u4e0d\u4e00\u81f4\u3001\u76d1\u7ba1\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u4f4e\u8d44\u6e90\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u9650\u5236\u7b49\u5b9e\u9645\u9650\u5236\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u53d8\u9769\u4fe1\u7528\u5206\u6790\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u52a8\u6001\u76d1\u7ba1\u5408\u89c4\u673a\u5236\u3001\u65b0\u578b\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u4ee5\u53ca\u8de8\u56fd\u4fe1\u7528\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.00821", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.00821", "abs": "https://arxiv.org/abs/2601.00821", "authors": ["Tao An"], "title": "CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations", "comment": "15 pages, 5 figures", "summary": "Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.\n  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.\n  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.", "AI": {"tldr": "CogCanvas\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u5bf9\u8bdd\u4e2d\u7684\u8ba4\u77e5\u6784\u4ef6\uff08\u51b3\u7b56\u3001\u4e8b\u5b9e\u3001\u63d0\u9192\uff09\u5e76\u7ec4\u7ec7\u6210\u65f6\u95f4\u611f\u77e5\u56fe\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u4e0a\u4e0b\u6587\u9650\u5236\u4e0e\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u77db\u76fe\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e0e\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u57fa\u672c\u77db\u76fe\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u622a\u65ad\u548c\u6458\u8981\uff09\u8981\u4e48\u4e22\u5f03\u65e9\u671f\u4fe1\u606f\uff0c\u8981\u4e48\u4e22\u5931\u7ec6\u8282\u4fe1\u606f\u3002", "method": "\u63d0\u51faCogCanvas\u6846\u67b6\uff0c\u4ece\u5bf9\u8bdd\u8f6e\u6b21\u4e2d\u63d0\u53d6\u57fa\u4e8e\u539f\u6587\u7684\u8ba4\u77e5\u6784\u4ef6\uff08\u51b3\u7b56\u3001\u4e8b\u5b9e\u3001\u63d0\u9192\uff09\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u65f6\u95f4\u611f\u77e5\u56fe\uff0c\u5b9e\u73b0\u6297\u538b\u7f29\u68c0\u7d22\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCogCanvas\u8fbe\u523034.7%\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4f18\u4e8eRAG\uff0825.6%\uff09\u548cGraphRAG\uff0813.7%\uff09\u3002\u5728\u65f6\u95f4\u63a8\u7406\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff1a31.5% vs. 9.3%\uff08RAG\uff09\u548c5.0%\uff08GraphRAG\uff09\uff0c\u76f8\u5bf9\u63d0\u5347530%\u3002\u5728\u591a\u8df3\u56e0\u679c\u63a8\u7406\u4e0a\u8fbe\u523081.0%\u901a\u8fc7\u7387\uff0cvs. GraphRAG\u768440.0%\u3002", "conclusion": "\u867d\u7136\u4e13\u95e8\u8bad\u7ec3\u7684\u65b9\u6cd5\uff08\u5982EverMemOS\u7ea692%\uff09\u80fd\u8fbe\u5230\u66f4\u9ad8\u7edd\u5bf9\u5206\u6570\uff0c\u4f46CogCanvas\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u7acb\u5373\u90e8\u7f72\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.00831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00831", "abs": "https://arxiv.org/abs/2601.00831", "authors": ["Uday Kumar Nidadala", "Venkata Bhumika Guthi"], "title": "Horizon Reduction as Information Loss in Offline Reinforcement Learning", "comment": "13 pages, 3 figures", "summary": "Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0chorizon reduction\uff08\u89c6\u91ce\u7f29\u51cf\uff09\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u635f\u5931\uff0c\u4f7f\u5f97\u6700\u4f18\u7b56\u7565\u4e0e\u6b21\u4f18\u7b56\u7565\u5728\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\uff0c\u5373\u4f7f\u6709\u65e0\u9650\u6570\u636e\u548c\u5b8c\u7f8e\u51fd\u6570\u903c\u8fd1\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u89c6\u91ce\u7f29\u51cf\u53ef\u4ee5\u6539\u5584\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u5c55\u6027\uff0c\u4f46\u5176\u7406\u8bba\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u89c6\u91ce\u7f29\u51cf\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u57fa\u672c\u7406\u8bba\u9650\u5236\u3002", "method": "\u5c06\u89c6\u91ce\u7f29\u51cf\u5f62\u5f0f\u5316\u4e3a\u4ece\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u7247\u6bb5\u4e2d\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6700\u5c0f\u53cd\u4f8b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDPs\uff09\u5c55\u793a\u4e09\u79cd\u7ed3\u6784\u6027\u5931\u6548\u6a21\u5f0f\u3002", "result": "\u8bc1\u660e\u5728\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u7247\u6bb5\u7684\u5b66\u4e60\u8303\u5f0f\u4e0b\uff0c\u6700\u4f18\u7b56\u7565\u53ef\u80fd\u4e0e\u6b21\u4f18\u7b56\u7565\u7edf\u8ba1\u4e0a\u65e0\u6cd5\u533a\u5206\u3002\u8bc6\u522b\u4e86\u4e09\u79cd\u5931\u6548\u6a21\u5f0f\uff1a\u524d\u7f00\u4e0d\u53ef\u533a\u5206\u6027\u5bfc\u81f4\u53ef\u8bc6\u522b\u6027\u5931\u8d25\u3001\u622a\u65ad\u56de\u62a5\u5bfc\u81f4\u7684\u76ee\u6807\u9519\u8bef\u6307\u5b9a\u3001\u79bb\u7ebf\u6570\u636e\u96c6\u652f\u6301\u548c\u8868\u793a\u6df7\u6dc6\u3002", "conclusion": "\u89c6\u91ce\u7f29\u51cf\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u4ec5\u901a\u8fc7\u7b97\u6cd5\u6539\u8fdb\u514b\u670d\u3002\u7814\u7a76\u5efa\u7acb\u4e86\u89c6\u91ce\u7f29\u51cf\u5b89\u5168\u5e94\u7528\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u8865\u5145\u4e86\u5173\u4e8e\u4fdd\u5b88\u76ee\u6807\u548c\u5206\u5e03\u504f\u79fb\u7684\u7b97\u6cd5\u5de5\u4f5c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.01042", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01042", "abs": "https://arxiv.org/abs/2601.01042", "authors": ["Zixiao Zhao", "Yanjie Jiang", "Hui Liu", "Kui Liu", "Lu Zhang"], "title": "SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities", "comment": "Accepted by ICSE 2026", "summary": "Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \\textbf{SeRe}, a \\textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \\textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b89\u5168\u76f8\u5173\u7684\u4ee3\u7801\u5ba1\u67e5\u6570\u636e\u96c6SeRe\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u96c6\u6210\u5206\u7c7b\u65b9\u6cd5\u4ece37\u4e07+\u539f\u59cb\u5ba1\u67e5\u4e2d\u63d0\u53d66732\u6761\u5b89\u5168\u76f8\u5173\u8bc4\u8bba\uff0c\u5e76\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u4ee3\u7801\u5ba1\u67e5\u751f\u6210\u65b9\u6cd5\u7684\u5b89\u5168\u53cd\u9988\u80fd\u529b\u3002", "motivation": "\u8f6f\u4ef6\u5b89\u5168\u6f0f\u6d1e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u9700\u8981\u65e9\u671f\u68c0\u6d4b\u3002\u867d\u7136\u4ee3\u7801\u5ba1\u67e5\u662f\u9632\u6b62\u5b89\u5168\u7f3a\u9677\u7684\u5173\u952e\u673a\u5236\uff0c\u4f46\u7531\u4e8e\u5ba1\u67e5\u8005\u5bf9\u5b89\u5168\u95ee\u9898\u5173\u6ce8\u4e0d\u8db3\u6216\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\uff0c\u76f8\u5173\u53cd\u9988\u4ecd\u7136\u7a00\u7f3a\u3002\u73b0\u6709\u6570\u636e\u96c6\u548c\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\uff0c\u8981\u4e48\u7f3a\u4e4f\u5b89\u5168\u7279\u5b9a\u6807\u6ce8\uff0c\u8981\u4e48\u89c4\u6a21\u592a\u5c0f\u65e0\u6cd5\u652f\u6301\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u96c6\u6210\u5206\u7c7b\u65b9\u6cd5\u6784\u5efaSeRe\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u8fed\u4ee3\u4f18\u5316\u6a21\u578b\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u5408\u7406\u53ec\u56de\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002\u4f7f\u7528\u5fae\u8c03\u7684\u96c6\u6210\u5206\u7c7b\u5668\u4ece373,824\u4e2a\u539f\u59cb\u5ba1\u67e5\u5b9e\u4f8b\u4e2d\u63d0\u53d66,732\u4e2a\u5b89\u5168\u76f8\u5173\u5ba1\u67e5\uff0c\u786e\u4fdd\u8de8\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u8868\u6027\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u8868\u660eSeRe\u6570\u636e\u96c6\u4e0e\u73b0\u5b9e\u4e16\u754c\u5b89\u5168\u76f8\u5173\u5ba1\u67e5\u5206\u5e03\u57fa\u672c\u4e00\u81f4\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u73b0\u6709\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u751f\u6210\u65b9\u6cd5\u5728\u5b89\u5168\u76f8\u5173\u53cd\u9988\u751f\u6210\u65b9\u9762\u7684\u6548\u679c\uff0c\u4e3a\u81ea\u52a8\u5316\u5b89\u5168\u5bfc\u5411\u4ee3\u7801\u5ba1\u67e5\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03SeRe\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u65e8\u5728\u63a8\u8fdb\u81ea\u52a8\u5316\u5b89\u5168\u5bfc\u5411\u4ee3\u7801\u5ba1\u67e5\u7814\u7a76\uff0c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u5b89\u5168\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2601.01129", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01129", "abs": "https://arxiv.org/abs/2601.01129", "authors": ["Kla Tantithamthavorn", "Yaotian Zou", "Andy Wong", "Michael Gupta", "Zhe Wang", "Mike Buller", "Ryan Jiang", "Matthew Watson", "Minwoo Jeong", "Kun Chen", "Ming Wu"], "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian", "comment": "Accepted at the 48th International Conference on Software Engineering (ICSE'26), SEIP Track. 12 Pages", "summary": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?\n  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).", "AI": {"tldr": "RovoDev Code Reviewer\u662f\u4e00\u4e2a\u4f01\u4e1a\u7ea7LLM\u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u751f\u6210\u57fa\u4e8e\u5ba1\u67e5\u6307\u5bfc\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8d28\u91cf\u68c0\u67e5\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\uff0c\u5df2\u5728Atlassian\u7684Bitbucket\u4e2d\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "motivation": "\u5c3d\u7ba1LLM\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u751f\u6210\u65b9\u6cd5\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u8bbe\u8ba1\u4f01\u4e1a\u7ea7\u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u5de5\u5177\u4ecd\u9762\u4e34\u5b9e\u9645\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u56de\u7b54\u4e00\u4e2a\u5b9e\u9645\u95ee\u9898\uff1a\u5982\u4f55\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u8bbe\u8ba1\u51fa\u57fa\u4e8e\u5ba1\u67e5\u6307\u5bfc\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8d28\u91cf\u68c0\u67e5\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u751f\u6210\u7cfb\u7edf\u3002", "method": "\u63d0\u51faRovoDev Code Reviewer\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f01\u4e1a\u7ea7LLM\u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u65e0\u7f1d\u96c6\u6210\u5230Atlassian\u7684Bitbucket\u4e2d\u3002\u901a\u8fc7\u79bb\u7ebf\u3001\u5728\u7ebf\u548c\u7528\u6237\u53cd\u9988\u8bc4\u4f30\uff0c\u5728\u4e00\u5e74\u65f6\u95f4\u5185\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "RovoDev Code Reviewer\u6709\u6548\u751f\u6210\u80fd\u5bfc\u81f4\u4ee3\u7801\u89e3\u51b3\u7684\u5ba1\u67e5\u8bc4\u8bba\uff0838.70%\u7684\u8bc4\u8bba\u89e6\u53d1\u4e86\u540e\u7eed\u63d0\u4ea4\u4e2d\u7684\u4ee3\u7801\u66f4\u6539\uff09\uff0c\u5e76\u52a0\u901f\u53cd\u9988\u5468\u671f\uff08\u51cf\u5c11PR\u5468\u671f\u65f6\u95f430.8%\uff09\u3001\u51cf\u8f7b\u5ba1\u67e5\u5458\u5de5\u4f5c\u91cf\uff08\u51cf\u5c11\u4eba\u5de5\u7f16\u5199\u8bc4\u8bba35.6%\uff09\u3001\u63d0\u9ad8\u6574\u4f53\u8f6f\u4ef6\u8d28\u91cf\uff08\u53d1\u73b0\u9519\u8bef\u5e76\u63d0\u4f9b\u53ef\u884c\u5efa\u8bae\uff09\u3002", "conclusion": "RovoDev Code Reviewer\u662f\u4e00\u4e2a\u6210\u529f\u7684\u4f01\u4e1a\u7ea7LLM\u4ee3\u7801\u5ba1\u67e5\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\uff0c\u663e\u8457\u6539\u5584\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u548c\u8f6f\u4ef6\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2601.01199", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01199", "abs": "https://arxiv.org/abs/2601.01199", "authors": ["Logan Murphy", "Aren A. Babikian", "Marsha Chechik"], "title": "Abductive Vibe Coding (Extended Abstract)", "comment": null, "summary": "When software artifacts are generated by AI models (\"vibe coding\"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u4e3aAI\u751f\u6210\u7684\u4ee3\u7801\uff08\"vibe coding\"\uff09\u63d0\u53d6\u53ef\u5206\u6790\u3001\u534a\u5f62\u5f0f\u5316\u7684\u5408\u7406\u6027\u4f9d\u636e\uff0c\u800c\u975e\u76f4\u63a5\u5224\u65ad\u6b63\u786e\u6027\uff0c\u751f\u6210\u4e00\u7ec4\u6761\u4ef6\u6765\u8bc4\u4f30\u4ee3\u7801\u7684\u5145\u5206\u6027\u3002", "motivation": "\u5f53AI\u6a21\u578b\u751f\u6210\u8f6f\u4ef6\u5236\u54c1\u65f6\uff0c\u4eba\u7c7b\u5de5\u7a0b\u5e08\u9700\u8981\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\u5e94\u901a\u8fc7\u5f62\u5f0f\u5316\u8bc1\u660e\u6765\u5b8c\u6210\uff0c\u4f46\u5bf9\u4e8e\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684vibe coding\u573a\u666f\u6765\u8bf4\u4e0d\u53ef\u884c\uff0c\u7279\u522b\u662f\u5f53AI\u751f\u6210\u5236\u54c1\u7684\u8981\u6c42\u96be\u4ee5\u5f62\u5f0f\u5316\u65f6\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u6846\u67b6\u6765\u63d0\u53d6\u53ef\u5206\u6790\u3001\u534a\u5f62\u5f0f\u5316\u7684\u5408\u7406\u6027\u4f9d\u636e\uff0c\u7528\u4e8e\u8bc4\u4f30vibe-coded\u5236\u54c1\u7684\u5145\u5206\u6027\u3002\u8be5\u6846\u67b6\u4e0d\u76f4\u63a5\u5224\u65ad\u6b63\u786e\u6027\uff0c\u800c\u662f\u751f\u6210\u4e00\u7ec4\u6761\u4ef6\uff0c\u5728\u8fd9\u4e9b\u6761\u4ef6\u4e0b\u53ef\u4ee5\u8ba4\u4e3a\u751f\u6210\u7684\u4ee3\u7801\u662f\u5145\u5206\u7684\u3002", "result": "\u8bba\u6587\u63cf\u8ff0\u4e86\u5f53\u524d\u6846\u67b6\u5b9e\u73b0\u7684\u5de5\u4f5c\u8fdb\u5c55\u548c\u9884\u671f\u7684\u7814\u7a76\u673a\u4f1a\u3002\u8fd9\u662f\u4e00\u4e2a\u6b63\u5728\u8fdb\u884c\u7684\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u65b9\u6cd5\u8bba\u4f46\u5c1a\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u9700\u8981\u4e3aAI\u751f\u6210\u7684\u4ee3\u7801\u9a8c\u8bc1\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5f53\u5f62\u5f0f\u5316\u8bc1\u660e\u4e0d\u53ef\u884c\u65f6\u3002\u63d0\u51fa\u7684\u534a\u5f62\u5f0f\u5316\u5408\u7406\u6027\u6846\u67b6\u4e3a\u89e3\u51b3vibe coding\u9a8c\u8bc1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "swe application"}}
{"id": "2601.01233", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01233", "abs": "https://arxiv.org/abs/2601.01233", "authors": ["Kangchen Zhu", "Zhiliang Tian", "Shangwen Wang", "Mingyue Leng", "Xiaoguang Mao"], "title": "Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling", "comment": "Accepted by ICSE 2026", "summary": "Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.", "AI": {"tldr": "Atomizer\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u8026\u590d\u5408\u63d0\u4ea4\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\u548c\u5206\u7ec4-\u8bc4\u5ba1\u534f\u4f5c\u5faa\u73af\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u56fe\u805a\u7c7b\u65b9\u6cd5\u3002", "motivation": "\u590d\u5408\u63d0\u4ea4\uff08\u5305\u542b\u591a\u4e2a\u65e0\u5173\u5173\u6ce8\u70b9\u7684\u63d0\u4ea4\uff09\u666e\u904d\u5b58\u5728\uff0c\u4e25\u91cd\u963b\u788d\u7a0b\u5e8f\u7406\u89e3\u548c\u7ef4\u62a4\u3002\u73b0\u6709\u81ea\u52a8\u5316\u89e3\u8026\u65b9\u6cd5\uff08\u7279\u522b\u662f\u57fa\u4e8e\u56fe\u805a\u7c7b\u7684\u65b9\u6cd5\uff09\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a1) \u8fc7\u5ea6\u4f9d\u8d56\u7ed3\u6784\u4fe1\u606f\uff0c\u65e0\u6cd5\u7406\u89e3\u53d8\u66f4\u7684\u8bed\u4e49\u610f\u56fe\uff1b2) \u4f5c\u4e3a\"\u5355\u6b21\"\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u8bc4\u5ba1\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u53cd\u601d\u548c\u7cbe\u70bc\u673a\u5236\u3002", "method": "\u63d0\u51faAtomizer\u6846\u67b6\uff1a1) \u91c7\u7528\u610f\u56fe\u5bfc\u5411\u7684\u601d\u7ef4\u94fe\u7b56\u7565\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6839\u636e\u4ee3\u7801\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u63a8\u65ad\u6bcf\u4e2a\u4ee3\u7801\u53d8\u66f4\u7684\u610f\u56fe\uff1b2) \u4f7f\u7528\u4e24\u4e2a\u667a\u80fd\u4f53\u5efa\u7acb\u5206\u7ec4-\u8bc4\u5ba1\u534f\u4f5c\u7cbe\u70bc\u5faa\u73af\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc4\u5ba1\u5b9e\u8df5\uff0c\u8fed\u4ee3\u7cbe\u70bc\u5206\u7ec4\u76f4\u5230\u540c\u4e00\u7c07\u4e2d\u7684\u6240\u6709\u53d8\u66f4\u5171\u4eab\u76f8\u540c\u7684\u5e95\u5c42\u8bed\u4e49\u610f\u56fe\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6C#\u548cJava\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAtomizer\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u3002\u5e73\u5747\u800c\u8a00\uff0c\u5728C#\u6570\u636e\u96c6\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u56fe\u57fa\u65b9\u6cd5\u9ad8\u51fa6.0%\u4ee5\u4e0a\uff0c\u5728Java\u6570\u636e\u96c6\u4e0a\u9ad8\u51fa5.5%\u4ee5\u4e0a\u3002\u5728\u590d\u6742\u63d0\u4ea4\u4e0a\u4f18\u52bf\u66f4\u52a0\u660e\u663e\uff0c\u6027\u80fd\u4f18\u52bf\u6269\u5927\u523016%\u4ee5\u4e0a\u3002", "conclusion": "Atomizer\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u610f\u56fe\u7406\u89e3\u548c\u8fed\u4ee3\u7cbe\u70bc\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u5408\u63d0\u4ea4\u89e3\u8026\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "topic": "swe application"}}
{"id": "2601.00848", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.00848", "abs": "https://arxiv.org/abs/2601.00848", "authors": ["Ron F. Del Rosario"], "title": "Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models", "comment": "26 pages, 3 figures, 7 tables. Datasets and code: https://huggingface.co/guerilla7/agentic-safety-gguf", "summary": "We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eOpenTelemetry\u8ffd\u8e2a\u5206\u6790\u7684\u591a\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u65f6\u5e8f\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7QLoRA\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u5b9e\u73b0\u51c6\u786e\u7387\u4ece42.86%\u63d0\u5347\u81f374.29%", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u9762\u4e34\u65f6\u5e8f\u653b\u51fb\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u68c0\u6d4b\u6b64\u7c7b\u653b\u51fb\u6a21\u5f0f\u7684\u5b89\u5168\u6a21\u578b\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u534f\u8c03\u653b\u51fb\u548c\u76d1\u7ba1\u8fdd\u89c4\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4e14\u7f3a\u4e4f\u53ef\u590d\u73b0\u7684\u6846\u67b6", "method": "\u6536\u96c680,851\u4e2a\u7f51\u7edc\u5b89\u5168\u793a\u4f8b\u548c35,026\u4e2a\u5408\u6210OpenTelemetry\u8ffd\u8e2a\u6570\u636e\uff0c\u5728NVIDIA DGX Spark ARM64\u786c\u4ef6\u4e0a\u4f7f\u7528QLoRA\u8fdb\u884c\u4e09\u6b21\u8fed\u4ee3\u5fae\u8c03\uff0c\u91c7\u7528\u7b56\u7565\u6027\u6570\u636e\u589e\u5f3a", "result": "\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u51c6\u786e\u7387\u4ece42.86%\u63d0\u5347\u81f374.29%\uff0c\u83b7\u5f9731.4\u4e2a\u767e\u5206\u70b9\u7684\u663e\u8457\u63d0\u5347\u3002\u9488\u5bf9\u7279\u5b9a\u77e5\u8bc6\u5dee\u8ddd\u7684\u5b9a\u5411\u8bad\u7ec3\u4f18\u4e8e\u65e0\u5dee\u522b\u6269\u5c55", "conclusion": "\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u4ece\u6839\u672c\u4e0a\u51b3\u5b9a\u6a21\u578b\u884c\u4e3a\uff0c\u867d\u7136\u5b9e\u9645\u90e8\u7f72\u9700\u8981\u4eba\u5de5\u76d1\u7763\uff08\u5b58\u5728\u8bef\u62a5\uff09\uff0c\u4f46\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u6784\u5efa\u9002\u5e94\u5176\u5a01\u80c1\u73af\u5883\u7684\u5b9a\u5236\u5316\u667a\u80fd\u4f53\u5b89\u5168\u6a21\u578b", "topic": "agent analysis"}}
{"id": "2601.01271", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01271", "abs": "https://arxiv.org/abs/2601.01271", "authors": ["Qingxiao Tao", "Xiaodong Gu", "Hao Zhong", "Beijun Shen"], "title": "CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs", "comment": null, "summary": "Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.", "AI": {"tldr": "CatchAll\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ed3\u5e93\u611f\u77e5\u5f02\u5e38\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u5c42\u77e5\u8bc6\uff08API\u7ea7\u5f02\u5e38\u77e5\u8bc6\u3001\u4ed3\u5e93\u7ea7\u6267\u884c\u4e0a\u4e0b\u6587\u3001\u8de8\u4ed3\u5e93\u5904\u7406\u6a21\u5f0f\uff09\u63d0\u5347\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5f02\u5e38\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5f02\u5e38\u5904\u7406\u662f\u7f16\u7a0b\u4e2d\u7684\u91cd\u8981\u9519\u8bef\u6062\u590d\u673a\u5236\uff0c\u4f46LLM\u5728\u4ed3\u5e93\u7ea7\u522b\u7684\u5f02\u5e38\u5904\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u4ed3\u5e93\u7ea7\u4e0a\u4e0b\u6587\u5e76\u751f\u6210\u51c6\u786e\u5f02\u5e38\u5904\u7406\u4ee3\u7801\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCatchAll\u65b9\u6cd5\uff0c\u4e3aLLM\u63d0\u4f9b\u4e09\u5c42\u5f02\u5e38\u5904\u7406\u77e5\u8bc6\uff1a1) API\u7ea7\u5f02\u5e38\u77e5\u8bc6\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u6784\u5efa\u7684API-\u5f02\u5e38\u6620\u5c04\uff1b2) \u4ed3\u5e93\u7ea7\u6267\u884c\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u5efa\u6a21\u76ee\u6807\u4ee3\u7801\u5468\u56f4\u7684\u8c03\u7528\u8ddf\u8e2a\u6355\u83b7\u5f02\u5e38\u4f20\u64ad\uff1b3) \u8de8\u4ed3\u5e93\u5904\u7406\u77e5\u8bc6\uff0c\u4ece\u5386\u53f2\u4ee3\u7801\u4e2d\u6316\u6398\u53ef\u91cd\u7528\u7684\u5f02\u5e38\u5904\u7406\u6a21\u5f0f\u3002\u8fd9\u4e9b\u77e5\u8bc6\u88ab\u7f16\u7801\u5230\u7ed3\u6784\u5316\u63d0\u793a\u4e2d\u6307\u5bfcLLM\u751f\u6210\u4ee3\u7801\u3002", "result": "\u5728\u65b0\u5efa\u7684\u4e24\u4e2a\u4ed3\u5e93\u611f\u77e5\u5f02\u5e38\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08RepoExEval\u548cRepoExEval-Exec\uff09\u4e0a\uff0cCatchAll\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\uff0cCodeBLEU\u5f97\u52060.31\uff08\u57fa\u7ebf0.27\uff09\uff0c\u610f\u56fe\u9884\u6d4b\u51c6\u786e\u738760.1%\uff08\u57fa\u7ebf48.0%\uff09\uff0cPass@1\u4e3a29%\uff08\u57fa\u7ebf25%\uff09\u3002", "conclusion": "CatchAll\u901a\u8fc7\u6574\u5408\u591a\u5c42\u5f02\u5e38\u5904\u7406\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u771f\u5b9e\u4ed3\u5e93\u7ea7\u522b\u5f02\u5e38\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u4ee3\u7801\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "2601.01320", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01320", "abs": "https://arxiv.org/abs/2601.01320", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python", "comment": null, "summary": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.", "AI": {"tldr": "ALPHA\u662f\u9996\u4e2a\u51fd\u6570\u7ea7Python\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u5206\u5c42\u611f\u77e5\u7684CWE\u7279\u5b9a\u60e9\u7f5a\u6765\u8bc4\u4f30LLM\u548cSAST\u5de5\u5177\uff0c\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u9519\u8bef\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6f0f\u6d1e\u68c0\u6d4b\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u57fa\u51c6\u91c7\u7528\u4e8c\u5143\u5206\u7c7b\uff0c\u7f3a\u4e4fCWE\u7ea7\u522b\u7684\u7279\u5f02\u6027\uff0c\u65e0\u6cd5\u4e3a\u8fed\u4ee3\u4fee\u6b63\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u53cd\u6620\u5b9e\u9645\u8bca\u65ad\u6548\u7528\u5dee\u5f02\u3002", "method": "\u63d0\u51faALPHA\u57fa\u51c6\uff0c\u91c7\u7528\u5206\u5c42\u611f\u77e5\u7684CWE\u7279\u5b9a\u60e9\u7f5a\u673a\u5236\uff0c\u533a\u5206\u8fc7\u5ea6\u6cdb\u5316\u3001\u8fc7\u5ea6\u89c4\u8303\u548c\u6a2a\u5411\u9519\u8bef\u3002\u8bc4\u4f30\u4e867\u4e2aLLM\u548c2\u4e2aSAST\u5de5\u5177\uff0c\u5206\u6790\u9884\u6d4b\u4e00\u81f4\u6027\u3002", "result": "LLM\u5728\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8eSAST\u5de5\u5177\uff0c\u4f46SAST\u5728\u68c0\u6d4b\u53d1\u751f\u65f6\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u3002\u6a21\u578b\u95f4\u7684\u9884\u6d4b\u4e00\u81f4\u6027\u5dee\u5f02\u5de8\u5927\uff088.26%-81.87%\uff09\uff0c\u5bf9\u53cd\u9988\u9a71\u52a8\u7cfb\u7edf\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "ALPHA\u4e3a\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u672a\u6765\u53ef\u5c06ALPHA\u60e9\u7f5a\u673a\u5236\u878d\u5165\u76d1\u7763\u5fae\u8c03\uff0c\u5b9e\u73b0\u539f\u5219\u6027\u7684\u5206\u5c42\u611f\u77e5\u6f0f\u6d1e\u68c0\u6d4b\u3002", "topic": "swe benchmark"}}
{"id": "2601.00880", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00880", "abs": "https://arxiv.org/abs/2601.00880", "authors": ["Anthony Mikinka"], "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering", "comment": "25 pages, 15 figures, 5 tables. Includes appendices with variable reference, pattern library, and O_s calculation examples. Supplementary materials: V1-V4.1 prompt source code and 305 model responses available at GitHub repositories", "summary": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.", "AI": {"tldr": "UCL\u662f\u4e00\u4e2a\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u5316\u4e3a\u7cfb\u7edf\u5316\u4f18\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u663e\u8457\u51cf\u5c1129.8%\u7684token\u4f7f\u7528\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u63ed\u793a\u4e86\u8fc7\u89c4\u8303\u6096\u8bba\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u5b9e\u8df5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6570\u5b66\u6846\u67b6\u3002\u4f5c\u8005\u5e0c\u671b\u5c06\u63d0\u793a\u4f18\u5316\u8f6c\u5316\u4e3a\u53ef\u7cfb\u7edf\u5316\u7684\u6570\u5b66\u95ee\u9898\uff0c\u63d0\u9ad8LLM\u4ea4\u4e92\u7684\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u63d0\u51faUniversal Conditional Logic (UCL)\u6570\u5b66\u6846\u67b6\uff0c\u5305\u542b\u6307\u793a\u51fd\u6570(I_i)\u3001\u7ed3\u6784\u5f00\u9500\u51fd\u6570(O_s = gamma * sum(ln C_k))\u3001\u65e9\u671f\u7ed1\u5b9a\u7b49\u6838\u5fc3\u673a\u5236\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30(N=305, 11\u4e2a\u6a21\u578b, 4\u6b21\u8fed\u4ee3)\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "result": "\u663e\u8457\u51cf\u5c1129.8%\u7684token\u4f7f\u7528(t(10)=6.36, p < 0.001, Cohen's d = 2.01)\uff0c\u76f8\u5e94\u964d\u4f4e\u6210\u672c\u3002\u53d1\u73b0\u8fc7\u89c4\u8303\u6096\u8bba\uff1a\u8d85\u8fc7\u9608\u503cS* = 0.509\u540e\uff0c\u989d\u5916\u89c4\u8303\u4f1a\u4e8c\u6b21\u964d\u4f4e\u6027\u80fd\u3002\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u9700\u8981\u4e0d\u540c\u7684UCL\u914d\u7f6e\u3002", "conclusion": "UCL\u5efa\u7acb\u4e86\u53ef\u6821\u51c6\u7684\u9ad8\u6548LLM\u4ea4\u4e92\u6846\u67b6\uff0c\u6a21\u578b\u5bb6\u65cf\u7279\u5b9a\u7684\u4f18\u5316\u662f\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u6846\u67b6\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u5316\u4e3a\u7cfb\u7edf\u5316\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2601.01426", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01426", "abs": "https://arxiv.org/abs/2601.01426", "authors": ["Chaofan Tao", "Jierun Chen", "Yuxin Jiang", "Kaiqi Kou", "Shaowei Wang", "Ruoyu Wang", "Xiaohui Li", "Sidi Yang", "Yiming Du", "Jianbo Dai", "Zhiming Mao", "Xinyu Wang", "Lifeng Shang", "Haoli Bai"], "title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving", "comment": "Project website: https://github.com/SWE-Lego/SWE-Lego", "summary": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.", "AI": {"tldr": "SWE-Lego\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5728\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u6539\u8fdb\u7684SFT\u6d41\u7a0b\u4ee5\u53ca\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u89e3\u51b3\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u590d\u6742\u7684\u8bad\u7ec3\u8303\u5f0f\uff08\u5982\u4e2d\u671f\u8bad\u7ec3\u3001SFT\u3001\u5f3a\u5316\u5b66\u4e60\u53ca\u5176\u7ec4\u5408\uff09\uff0c\u672c\u6587\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u7eafSFT\u65b9\u6cd5\u5728SWE\u4efb\u52a1\u4e0a\u8fbe\u5230\u6781\u9650\u6027\u80fd\u3002", "method": "1) SWE-Lego\u6570\u636e\u96c6\uff1a\u5305\u542b32k\u9ad8\u8d28\u91cf\u4efb\u52a1\u5b9e\u4f8b\u548c18k\u9a8c\u8bc1\u8f68\u8ff9\uff0c\u7ed3\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\uff1b2) \u6539\u8fdb\u7684SFT\u6d41\u7a0b\uff1a\u5305\u542b\u9519\u8bef\u63a9\u7801\u548c\u57fa\u4e8e\u96be\u5ea6\u7684\u8bfe\u7a0b\u5b66\u4e60\uff1b3) \u6d4b\u8bd5\u65f6\u6269\u5c55\uff1a\u57fa\u4e8e\u8bad\u7ec3\u826f\u597d\u7684\u9a8c\u8bc1\u5668\u8fdb\u884c\u6269\u5c55\u3002", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u4e0a\uff0cSWE-Lego-Qwen3-8B\u8fbe\u523042.2%\uff0c32B\u7248\u672c\u8fbe\u523052.6%\u3002\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55@16\uff0c8B\u6a21\u578b\u63d0\u5347\u81f349.6%\uff0c32B\u6a21\u578b\u63d0\u5347\u81f358.8%\uff0c\u5728\u540c\u7b49\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7eafSFT\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u65e0\u9700\u590d\u6742\u7684\u8bad\u7ec3\u8303\u5f0f\u3002\u9ad8\u8d28\u91cf\u6570\u636e\u3001\u6539\u8fdb\u7684SFT\u6d41\u7a0b\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u662f\u6210\u529f\u7684\u5173\u952e\u8981\u7d20\u3002", "topic": "swe application"}}
{"id": "2601.01514", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01514", "abs": "https://arxiv.org/abs/2601.01514", "authors": ["Matej Kucera", "Marco Castelluccio", "Daniel Feitosa", "Ayushi Rastogi"], "title": "Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox", "comment": "11 pages, 1 figure, 4 tables. To be published in ICSE-SEIP 2026 conference proceedings", "summary": "The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7fa4\u4f53\u5ba1\u67e5\u8bf7\u6c42\u4e0e\u4e2a\u4f53\u5ba1\u67e5\u8bf7\u6c42\u5bf9\u5ba1\u67e5\u901f\u5ea6\u548c\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7fa4\u4f53\u5ba1\u67e5\u4e0e\u66f4\u5c11\u7684\u56de\u5f52\u95ee\u9898\u76f8\u5173\uff0c\u4f46\u5bf9\u5ba1\u67e5\u901f\u5ea6\u5f71\u54cd\u4e0d\u5927\u3002", "motivation": "\u4ee3\u7801\u5ba1\u67e5\u901f\u5ea6\u662f\u8861\u91cf\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\u548c\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u7684\u91cd\u8981\u6307\u6807\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u8ba8\u4e86\u5f71\u54cd\u5ba1\u67e5\u901f\u5ea6\u7684\u56e0\u7d20\uff0c\u4f46\u5ba1\u67e5\u5206\u914d\u8fc7\u7a0b\u4e2d\"\u7fa4\u4f53\u5ba1\u67e5\u8bf7\u6c42\"\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\u3002\u7fa4\u4f53\u5ba1\u67e5\u8bf7\u6c42\u5141\u8bb8\u5c06\u4ee3\u7801\u53d8\u66f4\u5206\u914d\u7ed9\u4e00\u4e2a\u5ba1\u67e5\u8005\u7fa4\u4f53\uff0c\u800c\u4e0d\u662f\u7279\u5b9a\u7684\u4e2a\u4f53\u5ba1\u67e5\u8005\uff0c\u8fd9\u79cd\u6a21\u5f0f\u5728Phabricator\u3001GitHub\u3001Bitbucket\u7b49\u5e73\u53f0\u4e0a\u53ef\u7528\u3002", "method": "\u7814\u7a76\u501f\u9274\u4e86\u7ba1\u7406\u5b66\u4e2d\u7684\u5171\u4eab\u4efb\u52a1\u961f\u5217\u7406\u8bba\uff0c\u5206\u6790\u4e86Mozilla Firefox\u9879\u76ee\u4e2d\u7ea666,000\u4e2a\u4fee\u8ba2\u7248\u672c\u3002\u91c7\u7528\u7edf\u8ba1\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u4ece\u4e1a\u8005\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u7684\u89c2\u70b9\u3002", "result": "\u7fa4\u4f53\u5ba1\u67e5\u4e0e\u6539\u8fdb\u7684\u5ba1\u67e5\u8d28\u91cf\u76f8\u5173\uff08\u8868\u73b0\u4e3a\u66f4\u5c11\u7684\u56de\u5f52\u95ee\u9898\uff09\uff0c\u4f46\u4e0e\u5ba1\u67e5\u901f\u5ea6\u7684\u5173\u8054\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002\u989d\u5916\u7684\u611f\u77e5\u76ca\u5904\u5305\u62ec\u66f4\u5e73\u8861\u7684\u5de5\u4f5c\u5206\u914d\u548c\u65b0\u5ba1\u67e5\u8005\u7684\u57f9\u8bad\u673a\u4f1a\u3002", "conclusion": "\u7fa4\u4f53\u5ba1\u67e5\u8bf7\u6c42\u5728\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u8d28\u91cf\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c11\u56de\u5f52\u95ee\u9898\u65b9\u9762\uff0c\u4f46\u5bf9\u5ba1\u67e5\u901f\u5ea6\u7684\u5f71\u54cd\u6709\u9650\u3002\u8fd9\u79cd\u6a21\u5f0f\u8fd8\u80fd\u5e26\u6765\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u548c\u65b0\u4eba\u57f9\u8bad\u7b49\u989d\u5916\u597d\u5904\u3002", "topic": "swe application"}}
{"id": "2601.01126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01126", "abs": "https://arxiv.org/abs/2601.01126", "authors": ["Andrew Borthwick", "Stephen Ash"], "title": "RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution", "comment": "18 pages, 3 figures", "summary": "We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.", "AI": {"tldr": "RoboPhD\u662f\u4e00\u4e2aAI\u81ea\u4e3b\u7814\u7a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u6539\u8fdbText-to-SQL\u6027\u80fd\uff0c\u4ece70\u884c\u57fa\u7ebf\u8fdb\u5316\u52301500\u884c\uff0c\u5728BIRD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523073.67%\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\"\u8df3\u8fc7\u4e00\u7ea7\"\u90e8\u7f72\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22AI\u80fd\u5426\u5728\u6ca1\u6709\u4eba\u7c7b\u9886\u57df\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u6784\u5efa\u5f3a\u5927\u7684Text-to-SQL\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdb\u5316\u673a\u5236\u53d1\u73b0\u6709\u6548\u7b56\u7565\uff0c\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u3002", "method": "\u91c7\u7528\u95ed\u73af\u8fdb\u5316\u5faa\u73af\uff1aSQL\u751f\u6210\u4ee3\u7406\uff08\u6570\u636e\u5e93\u5206\u6790\u811a\u672c+SQL\u751f\u6210\u6307\u4ee4\uff09\u548c\u8fdb\u5316\u4ee3\u7406\uff08\u57fa\u4e8e\u6027\u80fd\u53cd\u9988\u8bbe\u8ba1\u65b0\u7248\u672c\uff09\u3002\u6838\u5fc3\u662fELO\u9009\u62e9\u673a\u5236\u5904\u7406\u6027\u80fd\u975e\u4f20\u9012\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u53c9\u6388\u7c89\u8fdb\u5316\u4ee3\u7406\u3002", "result": "\u4ece70\u884c\u57fa\u7ebf\u8fdb\u5316\u52301500\u884c\u4ee3\u7406\uff0c\u53d1\u73b0\u81ea\u9002\u5e94\u6570\u636e\u5e93\u5206\u6790\u3001\u5217\u9009\u62e9\u3001\u8bc1\u636e\u89e3\u91ca\u548c\u805a\u5408\u7b49\u7b56\u7565\u3002\u5728BIRD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523073.67%\u51c6\u786e\u7387\uff0c\u5bf9\u8f83\u5f31\u6a21\u578b\u6539\u8fdb\u66f4\u5927\uff08Claude Haiku\u63d0\u53478.9\u70b9\uff09\uff0c\u5b9e\u73b0\"\u8df3\u8fc7\u4e00\u7ea7\"\u90e8\u7f72\uff1a\u8fdb\u5316Haiku\u8d85\u8fc7\u539f\u751fSonnet\uff0c\u8fdb\u5316Sonnet\u8d85\u8fc7\u539f\u751fOpus\u3002", "conclusion": "AI\u80fd\u591f\u4ec5\u4ece\u7b80\u5355\u7684\u4eba\u7c7b\u63d0\u4f9b\u8d77\u70b9\u81ea\u4e3b\u6784\u5efa\u5f3a\u5927\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u8fdb\u5316\u673a\u5236\u5bf9\u6210\u672c\u8f83\u4f4e\u6a21\u578b\u6548\u679c\u66f4\u663e\u8457\uff0c\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u4f18\u5316\u3002", "topic": "code agent"}}
{"id": "2601.00994", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.00994", "abs": "https://arxiv.org/abs/2601.00994", "authors": ["Michael Bao"], "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)", "summary": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "AI": {"tldr": "ElecTwit\u662f\u4e00\u4e2a\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u4e2d\u591a\u667a\u80fd\u4f53\u8bf4\u670d\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u53d1\u73b0LLM\u4f7f\u7528\u4e8625\u79cd\u8bf4\u670d\u6280\u5de7\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u5bf9\u793e\u4ea4\u6a21\u62df\u52a8\u6001\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4f7f\u7528\u57fa\u4e8e\u6e38\u620f\u7684\u6a21\u62df\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u521b\u5efa\u4e00\u4e2a\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u6765\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bf4\u670d\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u573a\u666f\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86ElecTwit\u6a21\u62df\u6846\u67b6\uff0c\u5728\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u73af\u5883\u4e2d\u6d4b\u8bd5\u591a\u4e2aLLM\u6a21\u578b\uff0c\u5206\u6790\u5b83\u4eec\u4f7f\u7528\u7684\u8bf4\u670d\u6280\u5de7\u548c\u4ea4\u4e92\u52a8\u6001\u3002", "result": "\u89c2\u5bdf\u5230\u5927\u591a\u6570\u6d4b\u8bd5\u7684LLM\u4f7f\u7528\u4e8625\u79cd\u7279\u5b9a\u7684\u8bf4\u670d\u6280\u5de7\uff0c\u8303\u56f4\u6bd4\u5148\u524d\u62a5\u9053\u7684\u66f4\u5e7f\u3002\u4e0d\u540c\u6a21\u578b\u5728\u6280\u5de7\u4f7f\u7528\u548c\u6574\u4f53\u8bf4\u670d\u8f93\u51fa\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u53cd\u6620\u4e86\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u7684\u5f71\u54cd\u3002\u8fd8\u53d1\u73b0\u4e86\"\u771f\u76f8\u6838\u5fc3\"\u4fe1\u606f\u548c\"\u58a8\u6c34\"\u75f4\u8ff7\u7b49\u72ec\u7279\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8bc4\u4f30\u6709\u8bf4\u670d\u529b\u7684LLM\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u786e\u4fdd\u5bf9\u9f50\u5e76\u9632\u6b62\u5371\u9669\u7ed3\u679c\u3002", "topic": "agent analysis"}}
{"id": "2601.01780", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01780", "abs": "https://arxiv.org/abs/2601.01780", "authors": ["Arsham Khosravani", "Alireza Hosseinpour", "Arshia Akhavan", "Mehdi Keshani", "Abbas Heydarnoori"], "title": "LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment", "comment": null, "summary": "Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.", "AI": {"tldr": "\u4f7f\u7528DeepSeek-R1-Distill-Llama-8B\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u63d0\u51faLIA\u65b9\u6cd5\u5b9e\u73b0\u81ea\u52a8\u5316\u7684issue\u5206\u914d\uff0c\u76f8\u6bd4\u57fa\u51c6\u6a21\u578b\u548c\u73b0\u6709\u65b9\u6cd5\u5728Hit@1\u6307\u6807\u4e0a\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u7684issue\u5206\u914d\u8fc7\u7a0b\u901a\u5e38\u624b\u52a8\u8fdb\u884c\uff0c\u5b58\u5728\u4e0d\u4e00\u81f4\u548c\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u5f00\u6e90\u9879\u76ee\u4e2d\u6bcf\u6708\u6709\u6570\u5343\u4e2a\u65b0issue\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9879\u76ee\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u6216\u7a00\u758f\u5608\u6742\u7684\u5173\u7cfb\u4fe1\u606f\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faLIA\u65b9\u6cd5\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u5c06DeepSeek-R1-Distill-Llama-8B\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230issue\u5206\u914d\u4efb\u52a1\u3002\u5229\u7528LLM\u5bf9\u81ea\u7136\u8bed\u8a00\u548c\u8f6f\u4ef6\u76f8\u5173\u6587\u672c\u7684\u9884\u8bad\u7ec3\u8bed\u4e49\u7406\u89e3\uff0c\u76f4\u63a5\u4eceissue\u6807\u9898\u548c\u63cf\u8ff0\u751f\u6210\u5f00\u53d1\u8005\u63a8\u8350\u6392\u540d\uff0c\u57fa\u4e8e\u5386\u53f2issue-\u5f00\u53d1\u8005\u5206\u914d\u6a21\u5f0f\u5b66\u4e60\u3002", "result": "LIA\u76f8\u6bd4\u5176\u57fa\u7840\u9884\u8bad\u7ec3\u6a21\u578b\u5728Hit@1\u6307\u6807\u4e0a\u63d0\u5347\u9ad8\u8fbe+187.8%\uff0c\u76f8\u6bd4\u56db\u79cd\u9886\u5148\u7684issue\u5206\u914d\u65b9\u6cd5\u5728Hit@1\u4e0a\u63d0\u5347\u9ad8\u8fbe+211.2%\uff0c\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u9886\u57df\u9002\u914d\u7684LLM\u5728\u8f6f\u4ef6\u7ef4\u62a4\u4efb\u52a1\u4e2d\u975e\u5e38\u6709\u6548\uff0cLIA\u4e3aissue\u5206\u914d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2601.00868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00868", "abs": "https://arxiv.org/abs/2601.00868", "authors": ["Aditya Sreevatsa K", "Arun Kumar Raveendran", "Jesrael K Mani", "Prakash G Shigli", "Rajkumar Rangadore", "Narayana Darapaneni", "Anwesh Reddy Paduri"], "title": "SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation", "comment": null, "summary": "SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.", "AI": {"tldr": "SmartFlow\u662f\u4e00\u4e2a\u591a\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u667a\u80fd\u4f53AI\u89e3\u51b3\u57ce\u5e02\u5171\u4eab\u5355\u8f66\u52a8\u6001\u518d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6218\u7565DQN\u5b66\u4e60\u7b56\u7565\u3001\u6218\u672f\u6a21\u5757\u4f18\u5316\u8c03\u5ea6\u3001\u901a\u4fe1\u5c42\u751f\u6210\u53ef\u6267\u884c\u6307\u4ee4\uff0c\u663e\u8457\u964d\u4f4e\u7f51\u7edc\u4e0d\u5e73\u8861\u5e76\u63d0\u9ad8\u8fd0\u8425\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u518d\u5e73\u8861\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u52a8\u6001\u73af\u5883\uff0c\u9700\u8981\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u95f2\u7f6e\u65f6\u95f4\u3001\u63d0\u9ad8\u8f66\u8f86\u53ef\u7528\u6027\u5e76\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "method": "\u91c7\u7528\u591a\u5c42\u67b6\u6784\uff1a\u6218\u7565\u5c42\u4f7f\u7528DQN\u5728\u7ebd\u7ea6Citi Bike\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u5b66\u4e60\u518d\u5e73\u8861\u7b56\u7565\uff1b\u6218\u672f\u5c42\u786e\u5b9a\u6027\u4f18\u5316\u591a\u6bb5\u884c\u7a0b\u548c\u8c03\u5ea6\uff1b\u901a\u4fe1\u5c42\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53AI\u5c06\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u6307\u4ee4\u3002", "result": "\u8bc4\u4f30\u663e\u793aSmartFlow\u80fd\u51cf\u5c1195%\u4ee5\u4e0a\u7684\u7f51\u7edc\u4e0d\u5e73\u8861\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u884c\u9a76\u8ddd\u79bb\uff0c\u5b9e\u73b0\u9ad8\u5361\u8f66\u5229\u7528\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u8fd0\u8425\u6548\u7387\u3002", "conclusion": "SmartFlow\u4e3a\u590d\u6742\u57ce\u5e02\u79fb\u52a8\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001AI\u9a71\u52a8\u7684\u7269\u6d41\u84dd\u56fe\uff0c\u6210\u529f\u8fde\u63a5\u673a\u5668\u667a\u80fd\u4e0e\u4eba\u5de5\u64cd\u4f5c\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.01330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01330", "abs": "https://arxiv.org/abs/2601.01330", "authors": ["Shengji Tang", "Weihao Lin", "Jingqi Ye", "Hao Li", "Bo Zhang", "Shuyue Hu", "Tao Chen", "Wangli Ouyang", "Lei Bai", "Peng Ye"], "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale", "comment": "12 pages", "summary": "Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).", "AI": {"tldr": "JiSi\u6846\u67b6\u901a\u8fc7\u67e5\u8be2-\u54cd\u5e94\u6df7\u5408\u8def\u7531\u3001\u652f\u6301\u96c6\u805a\u5408\u5668\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\uff0c\u4f7f\u5f00\u6e90LLM\u534f\u4f5c\u8d85\u8d8aGemini-3-Pro\uff0c\u6210\u672c\u4ec547%", "motivation": "\u63a2\u7d22\u96c6\u4f53\u667a\u80fd\u4f5c\u4e3a\u66ff\u4ee3\u5355\u4f53\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u5f53\u524dLLM\u8def\u7531\u548c\u805a\u5408\u7684\u4e09\u4e2a\u74f6\u9888\uff1a\u67e5\u8be2\u5f0f\u8def\u7531\u5c40\u9650\u3001\u9759\u6001\u805a\u5408\u65b9\u6cd5\u3001\u8def\u7531\u4e0e\u805a\u5408\u4e92\u8865\u6027\u672a\u5145\u5206\u5229\u7528", "method": "\u63d0\u51faJiSi\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u67e5\u8be2-\u54cd\u5e94\u6df7\u5408\u8def\u7531\uff0c\u540c\u65f6\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\u548c\u95ee\u9898\u96be\u5ea6\uff1b2) \u57fa\u4e8e\u652f\u6301\u96c6\u7684\u805a\u5408\u5668\u9009\u62e9\uff0c\u8bc4\u4f30\u805a\u5408\u80fd\u529b\u548c\u9886\u57df\u80fd\u529b\uff1b3) \u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\uff0c\u52a8\u6001\u5229\u7528\u8def\u7531\u548c\u805a\u5408\u4f18\u52bf", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJiSi\u901a\u8fc7\u534f\u8c0310\u4e2a\u5f00\u6e90LLM\uff0c\u4ee5\u4ec547%\u7684\u6210\u672c\u8d85\u8d8a\u4e86Gemini-3-Pro\uff0c\u540c\u65f6\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u96c6\u4f53\u667a\u80fd\u4ee3\u8868\u4e86\u901a\u5f80AGI\u7684\u65b0\u8def\u5f84\uff0c\u5f00\u6e90LLM\u7684\u534f\u4f5c\u53ef\u4ee5\u8d85\u8d8a\u9876\u7ea7\u5355\u4f53\u6a21\u578b", "topic": "agent analysis"}}
{"id": "2601.01366", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01366", "abs": "https://arxiv.org/abs/2601.01366", "authors": ["Zixian Liu", "Sihao Liu", "Yuqi Zhao"], "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models", "comment": null, "summary": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.", "AI": {"tldr": "\u63d0\u51fa\u4e86KGCE\u57fa\u51c6\u5e73\u53f0\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u6559\u80b2\u573a\u666f\u4e2d\u8de8\u5e73\u53f0\u4efb\u52a1\u6267\u884c\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u5b66\u6821\u4e13\u7528\u8f6f\u4ef6\u7684\u7406\u89e3\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6846\u67b6\u5728\u6559\u80b2\u573a\u666f\u7684\u8de8\u5e73\u53f0\u4efb\u52a1\u652f\u6301\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u5b66\u6821\u4e13\u7528\u8f6f\u4ef6\uff08\u5982\u5c0f\u96c5\u667a\u80fd\u52a9\u624b\u3001\u534e\u5e08\u5323\u5b50\u7b49\uff09\u7f3a\u4e4f\u7406\u89e3\u5bfc\u81f4\u4ee3\u7406\u6548\u7387\u663e\u8457\u4e0b\u964d\uff0c\u4e14\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u6307\u6807\u96be\u4ee5\u6355\u6349\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8be6\u7ec6\u6267\u884c\u6548\u7387\u3002", "method": "\u6784\u5efa\u5305\u542b104\u4e2a\u6559\u80b2\u76f8\u5173\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6Windows\u3001Android\u548c\u8de8\u5e73\u53f0\u534f\u4f5c\u4efb\u52a1\uff1b\u63d0\u51fa\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u76ee\u6807\u5e76\u9a8c\u8bc1\u5b8c\u6210\u72b6\u6001\uff1b\u5f00\u53d1\u4e86\u5305\u542b\u5b66\u6821\u4e13\u7528\u8f6f\u4ef6\u77e5\u8bc6\u5e93\u7684\u589e\u5f3a\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u5f00\u53d1\u4e86KGCE\u57fa\u51c6\u5e73\u53f0\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u5728GitHub\u4e0a\u3002", "conclusion": "KGCE\u901a\u8fc7\u77e5\u8bc6\u5e93\u589e\u5f3a\u548c\u53cc\u56fe\u8bc4\u4f30\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6559\u80b2\u573a\u666f\u4e2d\u8de8\u5e73\u53f0\u4efb\u52a1\u6267\u884c\u7684\u8bc4\u4f30\u74f6\u9888\uff0c\u7279\u522b\u662f\u9488\u5bf9\u79c1\u6709\u9886\u57df\u8f6f\u4ef6\u7684\u7406\u89e3\u95ee\u9898\u3002", "topic": "swe benchmark"}}
{"id": "2601.02200", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02200", "abs": "https://arxiv.org/abs/2601.02200", "authors": ["Markus Borg", "Nadim Hagatulah", "Adam Tornhill", "Emma S\u00f6derberg"], "title": "Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics", "comment": "Accepted for the 3rd ACM International Conference on AI Foundation Models and Software Engineering (FORGE 2026)", "summary": "We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u53cb\u597d\u7684\u4ee3\u7801\uff08CodeHealth\u8bc4\u5206\u9ad8\uff09\u4e5f\u66f4\u5bb9\u6613\u88abAI\u5de5\u5177\u7406\u89e3\u548c\u7f16\u8f91\uff0c\u8bed\u4e49\u4fdd\u7559\u5ea6\u66f4\u9ad8\uff0c\u8868\u660e\u7ec4\u7ec7\u53ef\u4ee5\u7528CodeHealth\u6307\u6807\u6765\u8bc4\u4f30AI\u5e72\u9884\u7684\u98ce\u9669\u7a0b\u5ea6\u3002", "motivation": "\u968f\u7740AI\u7f16\u7a0b\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5f00\u53d1\u8005\u5171\u540c\u5de5\u4f5c\u7684\u6df7\u5408\u65f6\u4ee3\u5230\u6765\uff0c\u9700\u8981\u786e\u4fdd\u4e0d\u540c\u80fd\u529b\u7684LLM\u80fd\u591f\u53ef\u9760\u5730\u7f16\u8f91\u4ee3\u7801\u3002\u4f20\u7edf\u4e0a\u4ee3\u7801\u4f18\u5316\u4e3b\u8981\u8003\u8651\u4eba\u7c7b\u53ef\u8bfb\u6027\uff0c\u73b0\u5728\u9700\u8981\u7814\u7a76\u4ec0\u4e48\u6837\u7684\u4ee3\u7801\u5bf9AI\u66f4\u53cb\u597d\u3002", "method": "\u4f7f\u7528LLM\u5bf95,000\u4e2a\u6765\u81ea\u7f16\u7a0b\u7ade\u8d5b\u7684Python\u6587\u4ef6\u8fdb\u884c\u91cd\u6784\uff0c\u5206\u6790CodeHealth\uff08\u9488\u5bf9\u4eba\u7c7b\u7406\u89e3\u6821\u51c6\u7684\u8d28\u91cf\u6307\u6807\uff09\u4e0eAI\u91cd\u6784\u540e\u8bed\u4e49\u4fdd\u7559\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0CodeHealth\u4e0eAI\u91cd\u6784\u540e\u7684\u8bed\u4e49\u4fdd\u7559\u5ea6\u5b58\u5728\u6709\u610f\u4e49\u7684\u5173\u8054\uff0c\u4eba\u7c7b\u53cb\u597d\u7684\u4ee3\u7801\u4e5f\u66f4\u517c\u5bb9AI\u5de5\u5177\u3002", "conclusion": "\u7ec4\u7ec7\u53ef\u4ee5\u4f7f\u7528CodeHealth\u6307\u6807\u6765\u6307\u5bfcAI\u5e72\u9884\u7684\u98ce\u9669\u8bc4\u4f30\uff0c\u6295\u8d44\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u4e0d\u4ec5\u5e2e\u52a9\u4eba\u7c7b\u5f00\u53d1\u8005\uff0c\u4e5f\u4e3a\u5927\u89c4\u6a21AI\u91c7\u7528\u505a\u597d\u51c6\u5907\u3002", "topic": "code agent"}}
{"id": "2601.02215", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02215", "abs": "https://arxiv.org/abs/2601.02215", "authors": ["Nenad Petrovic", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems", "comment": null, "summary": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2aLLM\u8d4b\u80fd\u7684\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u6db5\u76d6\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u62d3\u6251\u8bbe\u8ba1\u548c\u4e8b\u4ef6\u9a71\u52a8\u51b3\u7b56\u4ee3\u7801\u5206\u6790", "motivation": "\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\u5f00\u53d1\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u5904\u7406\u5b89\u5168\u611f\u77e5\u7684\u7cfb\u7edf\u62d3\u6251\u8bbe\u8ba1\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u4ee3\u7801\u5206\u6790\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42", "method": "\u91c7\u7528\u4e8b\u4ef6\u94fe\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u5206\u6790\uff0c\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u548c\u5bf9\u8c61\u7ea6\u675f\u8bed\u8a00\u8fdb\u884c\u5b89\u5168\u62d3\u6251\u8bbe\u8ba1\uff0c\u652f\u6301\u672c\u5730\u90e8\u7f72\u548c\u4e13\u6709\u89e3\u51b3\u65b9\u6848", "result": "\u5728\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\u76f8\u5173\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "LLM\u8d4b\u80fd\u7684\u5de5\u4f5c\u6d41\u80fd\u591f\u6709\u6548\u652f\u6301\u8f6f\u4ef6\u5b9a\u4e49\u8f66\u8f86\u7684\u5f00\u53d1\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u8bbe\u8ba1\u548c\u4ee3\u7801\u5206\u6790\u65b9\u9762", "topic": "swe application"}}
{"id": "2601.02238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02238", "abs": "https://arxiv.org/abs/2601.02238", "authors": ["Nils Bosbach", "Alwalid Salama", "Lukas J\u00fcnger", "Mark Burton", "Niko Zurstra\u00dfen", "Rebecca Pelke", "Rainer Leupers"], "title": "NQC2: A Non-Intrusive QEMU Code Coverage Plugin", "comment": "PREPRINT - accepted by the Rapid Simulation and Performance Evaluation for Design Workshop (RAPIDO '24)", "summary": "Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.\n  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.", "AI": {"tldr": "NQC2\u662f\u4e00\u4e2aQEMU\u63d2\u4ef6\uff0c\u7528\u4e8e\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u88f8\u673a\u7a0b\u5e8f\u4e2d\u63d0\u53d6\u4ee3\u7801\u8986\u76d6\u7387\u4fe1\u606f\uff0c\u65e0\u9700\u76ee\u6807\u8f6f\u4ef6\u63d2\u6869\uff0c\u6027\u80fd\u6bd4Xilinx\u65b9\u6848\u63d0\u53478.5\u500d\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8986\u76d6\u7387\u5206\u6790\u9700\u8981\u64cd\u4f5c\u7cfb\u7edf\u548c\u6587\u4ef6\u7cfb\u7edf\u652f\u6301\uff0c\u800c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u88f8\u673a\u7a0b\u5e8f\u7f3a\u4e4f\u8fd9\u4e9b\u529f\u80fd\uff0c\u5bfc\u81f4\u65e0\u6cd5\u4f7f\u7528\u5e38\u89c4\u8986\u76d6\u7387\u6d4b\u91cf\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1QEMU\u63d2\u4ef6NQC2\uff0c\u5728\u8fd0\u884c\u65f6\u4eceQEMU\u865a\u62df\u673a\u4e2d\u63d0\u53d6\u8986\u76d6\u7387\u4fe1\u606f\u5e76\u5b58\u50a8\u5230\u4e3b\u673a\u6587\u4ef6\u4e2d\uff0c\u517c\u5bb9\u4fee\u6539\u7248QEMU\u4e14\u65e0\u9700\u76ee\u6807\u8f6f\u4ef6\u63d2\u6869\u3002", "result": "NQC2\u5728\u6027\u80fd\u4e0a\u6bd4Xilinx\u7684\u7c7b\u4f3c\u65b9\u6848\u63d0\u5347\u9ad8\u8fbe8.5\u500d\uff0c\u80fd\u591f\u6709\u6548\u6d4b\u91cf\u5d4c\u5165\u5f0f\u7cfb\u7edf\u88f8\u673a\u7a0b\u5e8f\u7684\u4ee3\u7801\u8986\u76d6\u7387\u3002", "conclusion": "NQC2\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u63d2\u6869\u7684\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4ee3\u7801\u8986\u76d6\u7387\u6d4b\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5177\u6709\u66f4\u597d\u7684\u517c\u5bb9\u6027\u548c\u6027\u80fd\u3002", "topic": "swe application"}}
{"id": "2601.01522", "categories": ["cs.AI", "cs.CL", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.01522", "abs": "https://arxiv.org/abs/2601.01522", "authors": ["Danial Amin"], "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u7684\u591aLLM\u534f\u540c\u51b3\u7b56\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u4e0d\u5bf9\u79f0\u9519\u8bef\u6210\u672c\u7684\u987a\u5e8f\u51b3\u7b56\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355LLM\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4e0d\u5bf9\u79f0\u9519\u8bef\u6210\u672c\u7684\u51b3\u7b56\u573a\u666f\uff08\u5982\u62db\u8058\u3001\u533b\u7597\u5206\u8bca\u3001\u6b3a\u8bc8\u68c0\u6d4b\uff09\u4e2d\u88ab\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u4e3b\u6d41\u65b9\u6cd5\u4ec5\u67e5\u8be2\u5355\u4e2aLLM\u8fdb\u884c\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\u5e76\u57fa\u4e8e\"\u7f6e\u4fe1\u5ea6\"\u9608\u503c\u884c\u52a8\uff0c\u8fd9\u5728\u987a\u5e8f\u51b3\u7b56\u4e2d\u662f\u4e0d\u5145\u5206\u7684\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u3001\u6210\u672c\u611f\u77e5\u7684\u591aLLM\u534f\u540c\u6846\u67b6\uff0c\u5c06LLM\u89c6\u4e3a\u8fd1\u4f3c\u4f3c\u7136\u6a21\u578b\u800c\u975e\u5206\u7c7b\u5668\u3002\u901a\u8fc7\u5bf9\u6bd4\u63d0\u793a\u4e3a\u6bcf\u4e2a\u5019\u9009\u72b6\u6001\u83b7\u53d6\u4f3c\u7136\u4f30\u8ba1\uff0c\u4f7f\u7528\u7a33\u5065\u7edf\u8ba1\u65b9\u6cd5\u805a\u5408\u591a\u4e2a\u6a21\u578b\u7ed3\u679c\uff0c\u5728\u65b0\u8bc1\u636e\u5230\u8fbe\u65f6\u57fa\u4e8e\u663e\u5f0f\u5148\u9a8c\u4f7f\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u66f4\u65b0\u4fe1\u5ff5\u3002", "result": "\u5728\u7b80\u5386\u7b5b\u9009\u5b9e\u9a8c\u4e2d\uff08\u6210\u672c\uff1a\u9519\u5931\u4eba\u624d40000\u7f8e\u5143/\u6b21\uff0c\u9762\u8bd52500\u7f8e\u5143/\u6b21\uff0c\u7535\u8bdd\u7b5b\u9009150\u7f8e\u5143/\u6b21\uff09\uff0c\u4f7f\u75285\u4e2aLLM\uff08GPT-4o\u3001Claude 4.5 Sonnet\u3001Gemini Pro\u3001Grok\u3001DeepSeek\uff09\u5904\u74061000\u4efd\u7b80\u5386\uff0c\u76f8\u6bd4\u6700\u4f73\u5355LLM\u57fa\u7ebf\u964d\u4f4e\u603b\u6210\u672c294000\u7f8e\u5143\uff0834%\uff09\uff0c\u5e76\u5c06\u4eba\u53e3\u7edf\u8ba1\u516c\u5e73\u6027\u63d0\u9ad845%\uff08\u6700\u5927\u7fa4\u4f53\u5dee\u8ddd\u4ece22%\u964d\u81f35%\uff09\u3002", "conclusion": "\u6b63\u786e\u7684\u6982\u7387\u57fa\u7840\u4e3a\u987a\u5e8f\u51b3\u7b56\u5e26\u6765\u4e86\u7406\u8bba\u4f18\u52bf\uff0c\u591aLLM\u805a\u5408\u8d21\u732e\u4e8651%\u7684\u6210\u672c\u8282\u7ea6\uff0c\u987a\u5e8f\u66f4\u65b0\u8d21\u732e43%\uff0c\u5206\u6b67\u89e6\u53d1\u7684\u4fe1\u606f\u6536\u96c6\u8d21\u732e20%\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u5bf9\u79f0\u6210\u672c\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.00898", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00898", "abs": "https://arxiv.org/abs/2601.00898", "authors": ["Ruiming Liang", "Yinan Zheng", "Kexin Zheng", "Tianyi Tan", "Jianxiong Li", "Liyuan Mao", "Zhihao Wang", "Guang Chen", "Hangjun Ye", "Jingjing Liu", "Jinqiao Wang", "Xianyuan Zhan"], "title": "Dichotomous Diffusion Policy Optimization", "comment": null, "summary": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.", "AI": {"tldr": "DIPOLE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u7a33\u5b9a\u5b66\u4e60\u7684\u4e8c\u5206\u7b56\u7565\uff08\u4e00\u4e2a\u6700\u5927\u5316\u5956\u52b1\uff0c\u4e00\u4e2a\u6700\u5c0f\u5316\u5956\u52b1\uff09\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u5927\u578b\u6269\u6563\u7b56\u7565\u9762\u4e34\u6311\u6218\uff1a\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u56e0\u76f4\u63a5\u6700\u5927\u5316\u4ef7\u503c\u76ee\u6807\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u8981\u4e48\u56e0\u4f9d\u8d56\u7c97\u7cd9\u7684\u9ad8\u65af\u4f3c\u7136\u8fd1\u4f3c\u800c\u9762\u4e34\u8ba1\u7b97\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6RL\u4e2d\u7684KL\u6b63\u5219\u5316\u76ee\u6807\uff0c\u63d0\u51fa\u8d2a\u5a6a\u5316\u7b56\u7565\u6b63\u5219\u5316\u65b9\u6848\uff0c\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u4e8c\u5206\u7b56\u7565\uff08\u5956\u52b1\u6700\u5927\u5316\u548c\u6700\u5c0f\u5316\uff09\u3002\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u4e8c\u5206\u7b56\u7565\u7684\u5206\u6570\u751f\u6210\u4f18\u5316\u52a8\u4f5c\uff0c\u5b9e\u73b0\u8d2a\u5a6a\u7a0b\u5ea6\u7684\u7075\u6d3b\u63a7\u5236\u3002", "result": "\u5728ExORL\u548cOGBench\u7684\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebfRL\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4f7f\u7528DIPOLE\u8bad\u7ec3\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cAD\u57fa\u51c6NAVSIM\u4e0a\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "DIPOLE\u4e3a\u6269\u6563\u7b56\u7565\u7684\u7a33\u5b9a\u53ef\u63a7\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e8c\u5206\u7b56\u7565\u5206\u89e3\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u7075\u6d3b\u6027\u7684\u5e73\u8861\uff0c\u5728\u590d\u6742\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.01546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01546", "abs": "https://arxiv.org/abs/2601.01546", "authors": ["Letian Kong", "Qianran", "Jin", "Renyu Zhang"], "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation", "comment": "39 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\u6539\u5584LLM\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u5bf9\u9f50\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e0a\u4e0b\u6587\u5f62\u6210\u660e\u786e\u6307\u5b9a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e0a\u4e0b\u6587\u5bfc\u822a\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002\u9a8c\u8bc1\u8868\u660e\u590d\u6742\u73af\u5883\u9700\u8981\u4e24\u9636\u6bb5\uff0c\u7b80\u5355\u4efb\u52a1\u4ec5\u9700\u7b2c\u4e00\u9636\u6bb5\u3002", "motivation": "LLM\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65f6\uff0c\u5728\u9700\u8981\u9884\u6d4b\u4ed6\u4eba\u884c\u52a8\u548c\u57fa\u4e8e\u89c2\u5bdf\u884c\u4e3a\u5f62\u6210\u4fe1\u5ff5\u7684\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\uff0c\u4f1a\u7cfb\u7edf\u6027\u5730\u504f\u79bb\u4eba\u7c7b\u51b3\u7b56\u3002\u9700\u8981\u6539\u8fdbLLM\u5728\u884c\u4e3a\u7814\u7a76\u4e2d\u7684\u5bf9\u9f50\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4e0a\u4e0b\u6587\u5f62\u6210\uff1a\u660e\u786e\u6307\u5b9a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5efa\u7acb\u51b3\u7b56\u4efb\u52a1\u53ca\u5176\u4e0a\u4e0b\u6587\u7684\u51c6\u786e\u8868\u793a\uff1b2) \u4e0a\u4e0b\u6587\u5bfc\u822a\uff1a\u5728\u8be5\u8868\u793a\u5185\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u4ee5\u505a\u51fa\u51b3\u7b56\u3002\u5728\u4e09\u4e2a\u4e0d\u540c\u51b3\u7b56\u73af\u5883\u4e2d\u9a8c\u8bc1\uff1a\u987a\u5e8f\u8d2d\u4e70\u6e38\u620f\u3001\u4f17\u7b79\u6e38\u620f\u548c\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\u3002", "result": "\u5728\u56db\u4e2aSOTA\u6a21\u578b\uff08GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1\uff09\u4e0a\u9a8c\u8bc1\u53d1\u73b0\uff1a\u590d\u6742\u51b3\u7b56\u73af\u5883\u9700\u8981\u4e24\u9636\u6bb5\u624d\u80fd\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u884c\u4e3a\u5bf9\u9f50\uff0c\u800c\u7b80\u5355\u7684\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\u4ec5\u9700\u4e0a\u4e0b\u6587\u5f62\u6210\u9636\u6bb5\u3002", "conclusion": "\u9610\u660e\u4e86\u6bcf\u4e2a\u9636\u6bb5\u5728\u4f55\u65f6\u5fc5\u8981\uff0c\u4e3a\u8bbe\u8ba1\u548c\u8bca\u65adLLM\u793e\u4f1a\u6a21\u62df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u884c\u4e3a\u7814\u7a76\u4e2d\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u8865\u5145\u3002", "topic": "agent analysis"}}
{"id": "2601.01569", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01569", "abs": "https://arxiv.org/abs/2601.01569", "authors": ["Maohao Ran", "Zhenglin Wan", "Cooper Lin", "Yanting Zhang", "Hongyu Xin", "Hongwei Fan", "Yibo Xu", "Beier Luo", "Yaxin Zhou", "Wangbo Zhao", "Lijie Yang", "Lang Feng", "Fuchao Yang", "Jingxuan Wu", "Yiqiao Huang", "Chendong Ma", "Dailing Jiang", "Jianbo Deng", "Sihui Han", "Bo An", "Yike Guo", "Jun Song"], "title": "CaveAgent: Transforming LLMs into Stateful Runtime Operators", "comment": "32 pages, 14 Figures", "summary": "LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from \"LLM-as-Text-Generator\" to \"LLM-as-Runtime-Operator.\" We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \\textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\\% success rate improvement on retail tasks and reduces total token consumption by 28.4\\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.", "AI": {"tldr": "CaveAgent\u662f\u4e00\u4e2a\u5c06LLM\u4ece\u6587\u672c\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u8fd0\u884c\u65f6\u64cd\u4f5c\u5458\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u4e0a\u4e0b\u6587\u67b6\u6784\u548c\u72b6\u6001\u5316\u8fd0\u884c\u65f6\u7ba1\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfJSON\u51fd\u6570\u8c03\u7528\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u8106\u5f31\u6027\u548c\u4e0a\u4e0b\u6587\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u9650\u4e8e\u6587\u672c\u4e2d\u5fc3\u8303\u5f0f\uff0c\u4f20\u7edfJSON\u51fd\u6570\u8c03\u7528\u65b9\u6cd5\u5728\u5904\u7406\u957f\u65f6\u4efb\u52a1\u65f6\u5b58\u5728\u8106\u5f31\u7684\u591a\u8f6e\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u6f02\u79fb\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u72b6\u6001\u7ba1\u7406\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51faCaveAgent\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u6d41\u4e0a\u4e0b\u6587\u67b6\u6784\uff1a\u8f7b\u91cf\u7ea7\u8bed\u4e49\u6d41\u7528\u4e8e\u63a8\u7406\uff0c\u6301\u4e45\u5316\u786e\u5b9a\u6027Python\u8fd0\u884c\u65f6\u6d41\u7528\u4e8e\u6267\u884c\u3002\u5f15\u5165\u72b6\u6001\u5316\u8fd0\u884c\u65f6\u7ba1\u7406\uff0c\u652f\u6301\u590d\u6742Python\u5bf9\u8c61\uff08\u5982DataFrame\u3001\u6570\u636e\u5e93\u8fde\u63a5\uff09\u7684\u6ce8\u5165\u3001\u64cd\u4f5c\u548c\u8de8\u8f6e\u6b21\u6301\u4e45\u5316\u3002", "result": "\u5728Tau\u00b2-bench\u3001BFCL\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u96f6\u552e\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534710.5%\uff0c\u591a\u8f6e\u573a\u666f\u603btoken\u6d88\u8017\u51cf\u5c1128.4%\u3002\u6570\u636e\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\uff0c\u76f4\u63a5\u53d8\u91cf\u5b58\u50a8\u68c0\u7d22\u51cf\u5c1159% token\u6d88\u8017\uff0c\u80fd\u5904\u7406\u5bfc\u81f4\u5176\u4ed6\u4ee3\u7406\u4e0a\u4e0b\u6587\u6ea2\u51fa\u7684\u6d77\u91cf\u6570\u636e\u3002", "conclusion": "CaveAgent\u901a\u8fc7\u5c06LLM\u8f6c\u53d8\u4e3a\u8fd0\u884c\u65f6\u64cd\u4f5c\u5458\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7406\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u9760\u7684\u957f\u65f6\u4efb\u52a1\u6267\u884c\uff0c\u4e3a\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.01718", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01718", "abs": "https://arxiv.org/abs/2601.01718", "authors": ["YuanLab. ai", ":", "Shawn Wu", "Sean Wang", "Louie Li", "Darcy Chen", "Allen Wang", "Jiangang Luo", "Xudong Zhao", "Joseph Shen", "Gawain Ma", "Jasper Jia", "Marcus Mao", "Claire Wang", "Hunter He", "Carol Wang", "Zera Zhang", "Jason Wang", "Chonly Shen", "Leo Zhang", "Logan Chen", "Qasim Meng", "James Gong", "Danied Zhao", "Penn Zheng", "Owen Zhu", "Tong Yu"], "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications", "comment": null, "summary": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.", "AI": {"tldr": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u5f00\u6e90\u7684MoE\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u670937\u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c400\u4ebf\u603b\u53c2\u6570\uff0c\u4e13\u4e3a\u4f01\u4e1a\u4efb\u52a1\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u4efb\u52a1\u7ade\u4e89\u529b\u3002\u4e3a\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u63d0\u51fa\u4e86RAPO\u8bad\u7ec3\u7b97\u6cd5\u3002\u5728RAG\u3001\u590d\u6742\u8868\u683c\u7406\u89e3\u548c\u6458\u8981\u7b49\u4f01\u4e1a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6570\u5b66\u3001\u79d1\u5b66\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u7cbe\u5ea6\uff0c\u4f46\u4ec5\u97001/4\u52301/2\u7684\u5e73\u5747token\u6570\u3002", "motivation": "\u9488\u5bf9\u4f01\u4e1a\u4efb\u52a1\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u5e38\u89c1\u7684\"\u8fc7\u5ea6\u601d\u8003\"\u73b0\u8c61\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u5728\u4f01\u4e1a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u53c8\u80fd\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u9ad8\u6548\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542b37\u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c400\u4ebf\u603b\u53c2\u6570\u3002\u63d0\u51faReflection-aware Adaptive Policy Optimization (RAPO)\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b97\u6cd5\uff0c\u6709\u6548\u8c03\u8282\u8fc7\u5ea6\u601d\u8003\u884c\u4e3a\u3002", "result": "\u5728\u4f01\u4e1a\u4efb\u52a1\uff08RAG\u3001\u590d\u6742\u8868\u683c\u7406\u89e3\u3001\u6458\u8981\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff1b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u7cbe\u5ea6\uff0c\u4f46\u4ec5\u97001/4\u52301/2\u7684\u5e73\u5747token\u6570\uff1b\u5df2\u5b8c\u5168\u5f00\u6e90\u4f9b\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u4f01\u4e1a\u5bfc\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7RAPO\u7b97\u6cd5\u89e3\u51b3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4f01\u4e1a\u4efb\u52a1\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "topic": "agent analysis"}}
{"id": "2601.00924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00924", "abs": "https://arxiv.org/abs/2601.00924", "authors": ["Rares Folea", "Radu Iacob", "Emil Slusanschi", "Traian Rebedea"], "title": "Complexity-based code embeddings", "comment": null, "summary": "This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u5c06\u5404\u79cd\u7b97\u6cd5\u7684\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u5d4c\u5165\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u591a\u4e2a\u901a\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u57fa\u4e8er-Complexity\u6784\u5efa\u4ee3\u7801\u5d4c\u5165\uff0c\u5e76\u5728Codeforces\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0XGBoost\u7b97\u6cd5\uff0c\u572811\u7c7b\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u53d6\u5f97\u826f\u597dF1\u5206\u6570\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u793a\uff08\u5d4c\u5165\uff09\uff0c\u4ee5\u4fbf\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u52a8\u6001\u884c\u4e3a\u7684\u5206\u6790\uff0c\u6216\u8005\u4e0d\u80fd\u5f88\u597d\u5730\u9002\u5e94\u4e0d\u540c\u7c7b\u578b\u7684\u7b97\u6cd5\u590d\u6742\u5ea6\u5ea6\u91cf\u3002", "method": "1. \u52a8\u6001\u5206\u6790\u8ba1\u7b97\u673a\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff1b2. \u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u591a\u4e2a\u901a\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff1b3. \u57fa\u4e8er-Complexity\u6784\u5efa\u7b97\u6cd5\u5d4c\u5165\uff1b4. \u4f7f\u7528\u8fd9\u4e9b\u5d4c\u5165\u5b9e\u73b0XGBoost\u7b97\u6cd5\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728Codeforces\u5e73\u53f0\u771f\u5b9e\u7f16\u7a0b\u7ade\u8d5b\u4ee3\u7801\u7247\u6bb5\u6784\u5efa\u768411\u7c7b\u591a\u6807\u7b7e\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e86\u5e73\u5747F1\u5206\u6570\uff08\u5177\u4f53\u6570\u503c\u672a\u5728\u6458\u8981\u4e2d\u7ed9\u51fa\uff0c\u4f46\u8868\u660e\u53d6\u5f97\u4e86\u826f\u597d\u6027\u80fd\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u4ee3\u7801\u5d4c\u5165\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u793a\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4ee3\u7801\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u4ee3\u7801\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002", "topic": "code agent"}}
{"id": "2601.01743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01743", "abs": "https://arxiv.org/abs/2601.01743", "authors": ["Bin Xu"], "title": "AI Agent Systems: Architectures, Applications, and Evaluation", "comment": null, "summary": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8eAI\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u6027\u5730\u6574\u7406\u4e86\u667a\u80fd\u4f53\u5728\u63a8\u7406\u3001\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u7b49\u65b9\u9762\u7684\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u6743\u8861\u3001\u8bc4\u4f30\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u4e0e\u63a8\u7406\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4f7f\u7528\u76f8\u7ed3\u5408\uff0cAI\u667a\u80fd\u4f53\u6b63\u5728\u6210\u4e3a\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u4e0e\u73b0\u5b9e\u4e16\u754c\u8ba1\u7b97\u7684\u5b9e\u7528\u63a5\u53e3\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u6574\u7406\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u67b6\u6784\u5206\u7c7b\u548c\u8bbe\u8ba1\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u73b0\u6709AI\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u6027\u6574\u7406\u548c\u5206\u7c7b\u3002\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u6db5\u76d6\u667a\u80fd\u4f53\u7ec4\u4ef6\uff08\u7b56\u7565/LLM\u6838\u5fc3\u3001\u8bb0\u5fc6\u3001\u4e16\u754c\u6a21\u578b\u3001\u89c4\u5212\u5668\u3001\u5de5\u5177\u8def\u7531\u5668\u3001\u6279\u8bc4\u5668\uff09\u3001\u7f16\u6392\u6a21\u5f0f\uff08\u5355\u667a\u80fd\u4f53vs.\u591a\u667a\u80fd\u4f53\uff1b\u96c6\u4e2d\u5f0fvs.\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff09\u548c\u90e8\u7f72\u8bbe\u7f6e\uff08\u79bb\u7ebf\u5206\u6790vs.\u5728\u7ebf\u4ea4\u4e92\u8f85\u52a9\uff1b\u5b89\u5168\u5173\u952evs.\u5f00\u653e\u4efb\u52a1\uff09\u3002", "result": "\u5efa\u7acb\u4e86AI\u667a\u80fd\u4f53\u67b6\u6784\u7684\u5168\u9762\u5206\u7c7b\u4f53\u7cfb\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u8bbe\u8ba1\u6743\u8861\uff08\u5ef6\u8fdfvs.\u51c6\u786e\u6027\u3001\u81ea\u4e3b\u6027vs.\u53ef\u63a7\u6027\u3001\u80fd\u529bvs.\u53ef\u9760\u6027\uff09\uff0c\u5206\u6790\u4e86\u8bc4\u4f30\u6311\u6218\uff08\u975e\u786e\u5b9a\u6027\u3001\u957f\u65f6\u7a0b\u4fe1\u7528\u5206\u914d\u3001\u5de5\u5177\u548c\u73af\u5883\u53d8\u5f02\u6027\u3001\u9690\u85cf\u6210\u672c\uff09\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u6d4b\u91cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\u3002", "conclusion": "AI\u667a\u80fd\u4f53\u67b6\u6784\u9886\u57df\u6b63\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5305\u62ec\u5de5\u5177\u52a8\u4f5c\u7684\u9a8c\u8bc1\u548c\u5b89\u5168\u9632\u62a4\u3001\u53ef\u6269\u5c55\u7684\u8bb0\u5fc6\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u53ca\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u53ef\u91cd\u590d\u8bc4\u4f30\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u89e3\u51b3\u8fd9\u4e9b\u5f00\u653e\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.01498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01498", "abs": "https://arxiv.org/abs/2601.01498", "authors": ["Bingguang Hao", "Zengzhuang Xu", "Yuntao Wen", "Xinyi Xu", "Yang Liu", "Tong Zhao", "Maolin Wang", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Xiangyu Zhao", "Chenyi Zhuang", "Ji Zhang"], "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents", "comment": null, "summary": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.", "AI": {"tldr": "HardGen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u56f0\u96be\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6837\u672c\uff0c\u901a\u8fc7\u52a8\u6001API\u56fe\u3001\u9ad8\u7ea7\u5de5\u5177\u5b9e\u4f8b\u5316\u548c\u95ed\u73af\u8bc4\u4f30\u53cd\u9988\u6765\u521b\u5efa\u590d\u6742\u591a\u6837\u7684\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u968f\u673a\u91c7\u6837\u548c\u6d45\u5c42\u751f\u6210\u8303\u5f0f\uff0c\u4ea7\u751f\u7684\u8f68\u8ff9\u7b80\u5355\u4e14\u540c\u8d28\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u3001\u9690\u5f0f\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u80fd\u529b\u63d0\u5347\u3002", "method": "1) \u57fa\u4e8e\u667a\u80fd\u4f53\u5931\u8d25\u6848\u4f8b\u5efa\u7acb\u52a8\u6001API\u56fe\u5e76\u91c7\u6837\u5408\u6210\u56f0\u96be\u8f68\u8ff9\uff1b2) \u4ee5\u8fd9\u4e9b\u8f68\u8ff9\u4e3a\u6761\u4ef6\u5148\u9a8c\u6307\u5bfc\u6a21\u5757\u5316\u62bd\u8c61\u9ad8\u7ea7\u5de5\u5177\u7684\u5b9e\u4f8b\u5316\uff1b3) \u5229\u7528\u9ad8\u7ea7\u5de5\u5177\u5236\u5b9a\u56f0\u96be\u67e5\u8be2\uff1b4) \u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u590d\u6742\u601d\u7ef4\u94fe\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u8bc4\u4f30\u53cd\u9988\u6301\u7eed\u4f18\u5316\u6574\u4e2a\u8fc7\u7a0b\u3002", "result": "\u4f7f\u7528HardGen\u751f\u6210\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u76844B\u53c2\u6570\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u591a\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u7ade\u4e89\u5bf9\u624b\uff0c\u5305\u62ecGPT-5.2\u3001Gemini-3-Pro\u548cClaude-Opus-4.5\u3002", "conclusion": "HardGen\u80fd\u591f\u6709\u6548\u751f\u6210\u590d\u6742\u591a\u6837\u7684\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u8bba\u3002", "topic": "agent analysis"}}
{"id": "2601.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01857", "abs": "https://arxiv.org/abs/2601.01857", "authors": ["Defei Xia", "Bingfeng Pi", "Shenbin Zhang", "Song Hua", "Yunfei Wei", "Lei Zuo"], "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios", "comment": null, "summary": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.", "AI": {"tldr": "\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u548c\u5206\u5c42\u5185\u5b58\u673a\u5236\u4f18\u5316LLM\u667a\u80fd\u4f53\uff0c\u5728\u4efb\u52a1\u51c6\u786e\u6027\u4e0a\u63d0\u534720%\uff0c\u964d\u4f4etoken\u6210\u672c\u548c\u5ef6\u8fdf", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u53d1\u5c55\uff0c\u63d0\u5347\u5176\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u54cd\u5e94\u751f\u6210\u65b9\u9762\u7684\u4efb\u52a1\u6027\u80fd\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u867d\u7136\u6539\u8fdb\u4e86\u667a\u80fd\u4f53\u6574\u4f53\u8bbe\u8ba1\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\u7684\u7cfb\u7edf\u6027\u4f18\u5316\u4ecd\u4e0d\u8db3", "method": "\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u7b56\u7565\uff0c\u6839\u636e\u667a\u80fd\u4f53\u72b6\u6001\u548c\u4efb\u52a1\u76ee\u6807\u8c03\u6574\u63d0\u793a\uff1b2) \u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u6a21\u5757\uff0c\u8fdb\u884c\u5de5\u5177\u5206\u7c7b\u3001\u8bed\u4e49\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u8c03\u7528\uff1b3) \u5206\u5c42\u5185\u5b58\u673a\u5236\uff0c\u6574\u5408\u4f1a\u8bdd\u5185\u5b58\u3001\u4efb\u52a1\u5386\u53f2\u548c\u5916\u90e8\u6458\u8981\uff0c\u901a\u8fc7\u52a8\u6001\u6458\u8981\u548c\u538b\u7f29\u63d0\u9ad8\u76f8\u5173\u6027\u548c\u6548\u7387\u3002\u6846\u67b6\u8fd8\u96c6\u6210\u4e86\u57fa\u4e8eMCP\u7684\u5de5\u5177\u3001\u6587\u4ef6I/O\u548c\u6267\u884c\u53cd\u9988\u4f18\u5316", "result": "\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u51c6\u786e\u6027\u63d0\u534720%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86token\u6210\u672c\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8c03\u7528\u5931\u8d25\u7387\u3002\u6846\u67b6\u5df2\u5728Jenius\u5e73\u53f0\u90e8\u7f72\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "Jenius-Agent\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u4f18\u5316\u667a\u80fd\u4f53\u5185\u90e8\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u7a33\u5065\u3001\u534f\u8bae\u517c\u5bb9\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2601.01584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01584", "abs": "https://arxiv.org/abs/2601.01584", "authors": ["Jakub Hoscilowicz"], "title": "Steerability of Instrumental-Convergence Tendencies in LLMs", "comment": "Code is available at https://github.com/j-hoscilowicz/instrumental_steering", "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5e76\u975e\u8d1f\u76f8\u5173\uff0c\u533a\u5206\u4e86\u6388\u6743\u4e0e\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u6743\u91cd\u6a21\u578b\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff1a\u5b89\u5168\u9700\u8981\u9ad8\u64cd\u63a7\u6027\u6765\u5b9e\u65bd\u63a7\u5236\uff0c\u800c\u5b89\u5168\u9700\u8981\u4f4e\u64cd\u63a7\u6027\u6765\u9632\u6b62\u6076\u610f\u884c\u4e3a\u3002\u901a\u8fc7Qwen3\u6a21\u578b\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7b80\u77ed\u7684\u53cd\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u80fd\u663e\u8457\u51cf\u5c11\u5de5\u5177\u6027\u8d8b\u540c\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76AI\u7cfb\u7edf\u7684\u4e24\u4e2a\u5173\u952e\u5c5e\u6027\uff1a\u80fd\u529b\uff08\u7cfb\u7edf\u80fd\u505a\u4ec0\u4e48\uff09\u548c\u53ef\u64cd\u63a7\u6027\uff08\u5982\u4f55\u53ef\u9760\u5730\u5c06\u884c\u4e3a\u8f6c\u5411\u9884\u671f\u7ed3\u679c\uff09\uff0c\u7279\u522b\u5173\u6ce8\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u9762\u4e34\u7684\u57fa\u672c\u5b89\u5168-\u5b89\u5168\u56f0\u5883\u3002", "method": "\u4f7f\u7528Qwen3\u6a21\u578b\uff084B/30B\uff1b\u57fa\u7840\u7248/\u6307\u5bfc\u7248/\u601d\u8003\u7248\uff09\u548cInstrumentalEval\u5de5\u5177\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4eb2\u5de5\u5177\u6027\u548c\u53cd\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u884c\u4e3a\u64cd\u63a7\u6027\uff0c\u6d4b\u91cf\u5de5\u5177\u6027\u8d8b\u540c\u884c\u4e3a\uff08\u5982\u5173\u673a\u56de\u907f\u3001\u6b3a\u9a97\u3001\u81ea\u6211\u590d\u5236\uff09\u7684\u53d8\u5316\u3002", "result": "\u7b80\u77ed\u7684\u53cd\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u80fd\u663e\u8457\u51cf\u5c11\u5de5\u5177\u6027\u8d8b\u540c\u884c\u4e3a\uff1aQwen3-30B\u6307\u5bfc\u7248\u4ece\u4eb2\u5de5\u5177\u6027\u540e\u7f00\u4e0b\u768481.69%\u964d\u81f3\u53cd\u5de5\u5177\u6027\u540e\u7f00\u4e0b\u76842.82%\u3002\u5728\u53cd\u5de5\u5177\u6027\u63d0\u793a\u4e0b\uff0c\u66f4\u5927\u7684\u5bf9\u9f50\u6a21\u578b\u4ea7\u751f\u7684\u8d8b\u540c\u884c\u4e3a\u6bd4\u5c0f\u6a21\u578b\u66f4\u5c11\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5e76\u975e\u8d1f\u76f8\u5173\uff0c\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff1a\u65e2\u9700\u8981\u9ad8\u64cd\u63a7\u6027\u6765\u5b9e\u65bd\u5b89\u5168\u63a7\u5236\uff0c\u53c8\u9700\u8981\u4f4e\u64cd\u63a7\u6027\u6765\u9632\u6b62\u6076\u610f\u5229\u7528\u3002\u63d0\u793a\u5de5\u7a0b\u80fd\u6709\u6548\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\uff0c\u66f4\u5927\u7684\u5bf9\u9f50\u6a21\u578b\u5728\u9002\u5f53\u63d0\u793a\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u5b89\u5168\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.01685", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01685", "abs": "https://arxiv.org/abs/2601.01685", "authors": ["Jinwei Hu", "Xinmiao Huang", "Youcheng Sun", "Yi Dong", "Xiaowei Huang"], "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage", "comment": "Under Review", "summary": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\uff0c\u5229\u7528LLM\u8fc7\u5ea6\u601d\u8003\u503e\u5411\uff0c\u901a\u8fc7\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u4e8b\uff0c\u4f7f\u53d7\u5bb3\u8005\u5185\u5316\u5e76\u4f20\u64ad\u865a\u5047\u7ed3\u8bba", "motivation": "\u968f\u7740LLM\u5411\u81ea\u4e3b\u4ee3\u7406\u53d1\u5c55\uff0c\u5176\u63a8\u7406\u80fd\u529b\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u653b\u51fb\u4f9d\u8d56\u9690\u853d\u901a\u4fe1\u3001\u540e\u95e8\u6216\u4f2a\u9020\u6587\u6863\uff0c\u800c\u672c\u7814\u7a76\u63a2\u7d22\u4ec5\u901a\u8fc7\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u5728\u516c\u5171\u6e20\u9053\u4f20\u64ad\u7684\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb", "method": "\u63d0\u51faGenerative Montage\u6846\u67b6\uff0c\u5305\u542bWriter-Editor-Director\u4e09\u4e2a\u89d2\u8272\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8fa9\u8bba\u548c\u534f\u8c03\u53d1\u5e03\u8bc1\u636e\u7247\u6bb5\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u4e8b\u3002\u5f00\u53d1CoPHEME\u6570\u636e\u96c6\uff08\u57fa\u4e8e\u771f\u5b9e\u8c23\u8a00\u4e8b\u4ef6\uff09\uff0c\u572814\u4e2aLLM\u5bb6\u65cf\u4e0a\u6a21\u62df\u653b\u51fb", "result": "\u653b\u51fb\u6210\u529f\u7387\uff1a\u4e13\u6709\u6a21\u578b74.4%\uff0c\u5f00\u6e90\u6743\u91cd\u6a21\u578b70.6%\u3002\u53cd\u76f4\u89c9\u7684\u662f\uff0c\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u66f4\u5bb9\u6613\u53d7\u653b\u51fb\uff0c\u63a8\u7406\u4e13\u7528\u6a21\u578b\u6bd4\u57fa\u7840\u6a21\u578b\u6216\u63d0\u793a\u66f4\u6613\u53d7\u5f71\u54cd\u3002\u865a\u5047\u4fe1\u5ff5\u4f1a\u5411\u4e0b\u6e38\u4f20\u64ad\uff0c\u6b3a\u9a97\u7387\u8d85\u8fc760%", "conclusion": "\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u8106\u5f31\u6027\uff0c\u63a8\u7406\u80fd\u529b\u53cd\u800c\u589e\u52a0\u4e86\u5bf9\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\u7684\u6613\u611f\u6027\uff0c\u9700\u8981\u65b0\u7684\u9632\u5fa1\u673a\u5236", "topic": "agent analysis"}}
{"id": "2601.01982", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01982", "abs": "https://arxiv.org/abs/2601.01982", "authors": ["Noel Thomas"], "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems", "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)", "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.", "AI": {"tldr": "ChaosBench-Logic\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u9886\u57df\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b30\u4e2a\u7cfb\u7edf\u3001621\u4e2a\u95ee\u9898\uff0c\u6d4b\u8bd5\u591a\u79cd\u63a8\u7406\u7c7b\u578b\uff0c\u53d1\u73b0\u524d\u6cbfLLM\u5728\u5355\u9879\u51c6\u786e\u7387\u53ef\u8fbe91-94%\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u4e0a\u5f97\u5206\u4e3a0%\uff0c\u5bf9\u8bdd\u51c6\u786e\u738753.1-75.5%\u3002", "motivation": "LLM\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u903b\u8f91\u548c\u7b26\u53f7\u63a8\u7406\u7684\u9886\u57df\u4ecd\u7136\u8106\u5f31\u3002\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u7279\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u6df7\u6c8c\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4f46\u5e38\u88ab\u8bef\u89e3\u4e3a\u968f\u673a\u6027\u6216\u590d\u6742\u6027\u3002\u9700\u8981\u8bc4\u4f30LLM\u5728\u8fd9\u7c7b\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165ChaosBench-Logic\u57fa\u51c6\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u4e00\u9636\u903b\u8f91\u672c\u4f53\u8bba\u8bc4\u4f3030\u4e2a\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u3002\u6bcf\u4e2a\u7cfb\u7edf\u6807\u6ce811\u4e2a\u8bed\u4e49\u8c13\u8bcd\u7684\u771f\u503c\u5206\u914d\uff0c\u751f\u6210621\u4e2a\u95ee\u9898\u6db5\u76d67\u4e2a\u63a8\u7406\u7c7b\u522b\uff1a\u591a\u6b65\u8574\u542b\u3001\u8de8\u7cfb\u7edf\u7c7b\u6bd4\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u504f\u89c1\u63a2\u6d4b\u3001\u591a\u8f6e\u5bf9\u8bdd\u7b49\u3002\u5b9a\u4e49\u903b\u8f91\u51c6\u786e\u6027\u3001\u8574\u542b\u4e00\u81f4\u6027\u3001\u5bf9\u8bdd\u8fde\u8d2f\u6027\u548c\u77db\u76fe\u6027\u7b49\u6307\u6807\uff0c\u5e76\u53d1\u5e03\u5f00\u6e90\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u524d\u6cbfLLM\uff08GPT-4\u3001Claude 3.5 Sonnet\u3001Gemini 2.5 Flash\u3001LLaMA-3 70B\uff09\u5728\u5355\u9879\u51c6\u786e\u7387\u8fbe\u523091-94%\uff0c\u4f46\u5728\u7ec4\u5408\u9879\u76ee\u4e0a\u5f97\u5206\u4e3a0%\uff0c\u8868\u73b0\u51fa\u8106\u5f31\u7684\u5168\u5c40\u8fde\u8d2f\u6027\u3002\u5bf9\u8bdd\u7ea7\u51c6\u786e\u7387\u4ece53.1%\uff08GPT-4 CoT\uff09\u523075.5%\uff08LLaMA-3 zero-shot\uff09\u3002", "conclusion": "ChaosBench-Logic\u4e3a\u8bca\u65adLLM\u63a8\u7406\u5931\u8d25\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e3a\u5f00\u53d1\u6539\u8fdbLLM\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.02061", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02061", "abs": "https://arxiv.org/abs/2601.02061", "authors": ["Faizan Ahmed", "Aniket Dixit", "James Brusey"], "title": "Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management", "comment": "6 pages, accepted at NeurIPS workshop 2025", "summary": "Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u9ad8\u9636\u5bfc\u6570\u60e9\u7f5a\u5b9e\u73b0\u52a8\u4f5c\u5e73\u6ed1\u6b63\u5219\u5316\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u4e09\u9636\u5bfc\u6570\u60e9\u7f5a\uff08\u6025\u52a8\u5ea6\u6700\u5c0f\u5316\uff09\u80fd\u663e\u8457\u63d0\u5347\u5e73\u6ed1\u6027\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5e38\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u3001\u9ad8\u9891\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u8fd9\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u4f1a\u5bfc\u81f4\u80fd\u8017\u8fc7\u9ad8\u548c\u673a\u68b0\u78e8\u635f\u95ee\u9898\uff0c\u9700\u8981\u627e\u5230\u5e73\u8861RL\u4f18\u5316\u4e0e\u5b9e\u9645\u64cd\u4f5c\u7ea6\u675f\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9ad8\u9636\u5bfc\u6570\u60e9\u7f5a\u8fdb\u884c\u52a8\u4f5c\u5e73\u6ed1\u6b63\u5219\u5316\uff0c\u4ece\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u7684\u7406\u8bba\u7406\u89e3\u5230\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u7684\u5b9e\u9645\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u9636\u6b21\u7684\u5bfc\u6570\u60e9\u7f5a\u6548\u679c\u3002", "result": "\u5728\u56db\u4e2a\u8fde\u7eed\u63a7\u5236\u73af\u5883\u4e2d\uff0c\u4e09\u9636\u5bfc\u6570\u60e9\u7f5a\uff08\u6025\u52a8\u5ea6\u6700\u5c0f\u5316\uff09\u59cb\u7ec8\u5b9e\u73b0\u6700\u4f73\u5e73\u6ed1\u6027\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\uff1b\u5728HVAC\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u5e73\u6ed1\u7b56\u7565\u5c06\u8bbe\u5907\u5207\u6362\u51cf\u5c11\u4e8660%\u3002", "conclusion": "\u9ad8\u9636\u52a8\u4f5c\u6b63\u5219\u5316\u4e3aRL\u4f18\u5316\u4e0e\u80fd\u6e90\u5173\u952e\u5e94\u7528\u4e2d\u7684\u64cd\u4f5c\u7ea6\u675f\u4e4b\u95f4\u5efa\u7acb\u4e86\u6709\u6548\u6865\u6881\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.01825", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01825", "abs": "https://arxiv.org/abs/2601.01825", "authors": ["Yaxin Cui", "Yuanqiang Zeng", "Jiapeng Yan", "Keling Lin", "Kai Ji", "Jianhui Zeng", "Sheng Zhang", "Xin Luo", "Binzhu Su", "Chaolai Shen", "Jiahao Yu"], "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CSCBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0LLM\u5728\u6d41\u7a0b\u548c\u8ba4\u77e5\u7ef4\u5ea6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u54c1\u79cd\u89c4\u5219\u7ef4\u5ea6\uff08\u7279\u522b\u662f\u8d27\u8fd0\u534f\u8bae\uff09\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u8fd9\u4e00\u53d7\u5236\u5ea6\u89c4\u5219\u7cfb\u7edf\u548c\u53ef\u884c\u6027\u7ea6\u675f\u652f\u914d\u7684\u9886\u57df\uff0c\u5176\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u5546\u54c1\u4f9b\u5e94\u94fe\u51b3\u7b56\u6d89\u53ca\u590d\u6742\u7684\u6d41\u7a0b\u9636\u6bb5\u3001\u54c1\u79cd\u7279\u5b9a\u89c4\u5219\u548c\u591a\u5c42\u6b21\u63a8\u7406\u6df1\u5ea6\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86CSCBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2300+\u4e2a\u5355\u9009\u9898\uff0c\u57fa\u4e8ePVC 3D\u8bc4\u4f30\u6846\u67b6\uff08\u6d41\u7a0b\u3001\u54c1\u79cd\u3001\u8ba4\u77e5\uff09\u3002\u6d41\u7a0b\u7ef4\u5ea6\u4e0eSCOR+Enable\u6807\u51c6\u5bf9\u9f50\uff1b\u54c1\u79cd\u7ef4\u5ea6\u57fa\u4e8e\u6743\u5a01\u4ea4\u6613\u6240\u6307\u5357/\u89c4\u5219\u624b\u518c\u548c\u884c\u4e1a\u62a5\u544a\uff0c\u5728\u8026\u5408\u7684\u6750\u6599-\u4fe1\u606f-\u8d22\u52a1\u7ea6\u675f\u4e0b\u64cd\u4f5c\u5316\u5546\u54c1\u7279\u5b9a\u89c4\u5219\u7cfb\u7edf\uff1b\u8ba4\u77e5\u7ef4\u5ea6\u9075\u5faa\u4fee\u8ba2\u7248\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u3002", "result": "\u5728\u76f4\u63a5\u63d0\u793a\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4ee3\u8868\u6027LLM\uff0c\u53d1\u73b0\u5728\u6d41\u7a0b\u548c\u8ba4\u77e5\u7ef4\u5ea6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u54c1\u79cd\u7ef4\u5ea6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u8d27\u8fd0\u534f\u8bae\u65b9\u9762\u3002\u8fd9\u8868\u660eLLM\u5728\u5904\u7406\u5546\u54c1\u7279\u5b9a\u89c4\u5219\u7cfb\u7edf\u65b9\u9762\u5b58\u5728\u660e\u663e\u77ed\u677f\u3002", "conclusion": "CSCBench\u4e3a\u8861\u91cf\u548c\u6539\u8fdbLLM\u5728\u8fd9\u4e00\u9ad8\u98ce\u9669\u9886\u57df\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u8bca\u65ad\u6027\u57fa\u51c6\u3002\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u89c4\u5219\u7cfb\u7edf\u65b9\u9762\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.02163", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02163", "abs": "https://arxiv.org/abs/2601.02163", "authors": ["Chuanrui Hu", "Xingze Gao", "Zuyi Zhou", "Dannong Xu", "Yi Bai", "Xintong Li", "Hui Zhang", "Tong Li", "Chong Zhang", "Lidong Bing", "Yafeng Deng"], "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning", "comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS", "summary": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.", "AI": {"tldr": "EverMemOS\u662f\u4e00\u4e2a\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u64cd\u4f5c\u7cfb\u7edf\uff0c\u91c7\u7528\u7c7b\u8bb0\u5fc6\u5370\u8ff9\u7684\u751f\u547d\u5468\u671f\u7ba1\u7406\u8ba1\u7b97\u8bb0\u5fc6\uff0c\u901a\u8fc7\u5c06\u5bf9\u8bdd\u6d41\u8f6c\u6362\u4e3a\u8bb0\u5fc6\u5355\u5143\u3001\u7ec4\u7ec7\u6210\u4e3b\u9898\u573a\u666f\u3001\u8fdb\u884c\u91cd\u6784\u56de\u5fc6\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u957f\u671f\u4ea4\u4e92\u4ee3\u7406\u65f6\uff0c\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u96be\u4ee5\u7ef4\u6301\u8fde\u8d2f\u884c\u4e3a\u3002\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u5b58\u50a8\u5b64\u7acb\u8bb0\u5f55\u5e76\u68c0\u7d22\u7247\u6bb5\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u6f14\u53d8\u7684\u7528\u6237\u72b6\u6001\u548c\u89e3\u51b3\u51b2\u7a81\u3002", "method": "\u63d0\u51faEverMemOS\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u64cd\u4f5c\u7cfb\u7edf\uff1a1) \u60c5\u8282\u75d5\u8ff9\u5f62\u6210\uff1a\u5c06\u5bf9\u8bdd\u6d41\u8f6c\u6362\u4e3a\u5305\u542b\u60c5\u8282\u75d5\u8ff9\u3001\u539f\u5b50\u4e8b\u5b9e\u548c\u65f6\u95f4\u8fb9\u754c\u524d\u77bb\u4fe1\u53f7\u7684\u8bb0\u5fc6\u5355\u5143\uff1b2) \u8bed\u4e49\u6574\u5408\uff1a\u5c06\u8bb0\u5fc6\u5355\u5143\u7ec4\u7ec7\u6210\u4e3b\u9898\u8bb0\u5fc6\u573a\u666f\uff0c\u63d0\u70bc\u7a33\u5b9a\u8bed\u4e49\u7ed3\u6784\u5e76\u66f4\u65b0\u7528\u6237\u753b\u50cf\uff1b3) \u91cd\u6784\u56de\u5fc6\uff1a\u6267\u884c\u8bb0\u5fc6\u573a\u666f\u5f15\u5bfc\u7684\u4ee3\u7406\u68c0\u7d22\uff0c\u4e3a\u4e0b\u6e38\u63a8\u7406\u7ec4\u5408\u5fc5\u8981\u4e14\u5145\u5206\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEverMemOS\u5728\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728PersonaMem v2\u4e0a\u7684\u753b\u50cf\u7814\u7a76\u548c\u5b9a\u6027\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u7528\u6237\u753b\u50cf\u548c\u524d\u77bb\u7b49\u804a\u5929\u5bfc\u5411\u80fd\u529b\u3002", "conclusion": "EverMemOS\u901a\u8fc7\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u64cd\u4f5c\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86LLM\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u8fde\u8d2f\u548c\u667a\u80fd\u7684\u4ee3\u7406\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2601.02314", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02314", "abs": "https://arxiv.org/abs/2601.02314", "authors": ["Sourena Khanzadeh"], "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "comment": null, "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($\u03c6$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($\u03c1$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faProject Ariadne\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u53cd\u4e8b\u5b9e\u903b\u8f91\u6765\u5ba1\u8ba1LLM\u667a\u80fd\u4f53\u63a8\u7406\u7684\u56e0\u679c\u5b8c\u6574\u6027\uff0c\u53d1\u73b0\u5f53\u524d\u667a\u80fd\u4f53\u5b58\u5728\u4e25\u91cd\u7684\"\u56e0\u679c\u89e3\u8026\"\u95ee\u9898\uff0c\u63a8\u7406\u75d5\u8ff9\u53ea\u662f\"\u63a8\u7406\u5267\u573a\"\u800c\u975e\u771f\u6b63\u7684\u51b3\u7b56\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u8d8a\u6765\u8d8a\u591a\u5730\u627f\u62c5\u9ad8\u98ce\u9669\u81ea\u4e3b\u51b3\u7b56\u4efb\u52a1\uff0c\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u6210\u4e3a\u5173\u952e\u5b89\u5168\u95ee\u9898\u3002\u867d\u7136\u601d\u7ef4\u94fe\u63d0\u793a\u5141\u8bb8\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u75d5\u8ff9\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u75d5\u8ff9\u662f\u6a21\u578b\u8f93\u51fa\u7684\u771f\u5b9e\u751f\u6210\u9a71\u52a8\u56e0\u7d20\u8fd8\u662f\u4e8b\u540e\u5408\u7406\u5316\u89e3\u91ca\u3002", "method": "\u63d0\u51faProject Ariadne\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u53cd\u4e8b\u5b9e\u903b\u8f91\u6765\u5ba1\u8ba1\u667a\u80fd\u4f53\u63a8\u7406\u7684\u56e0\u679c\u5b8c\u6574\u6027\u3002\u901a\u8fc7\u786c\u5e72\u9884\uff08do-\u6f14\u7b97\uff09\u5bf9\u4e2d\u95f4\u63a8\u7406\u8282\u70b9\u8fdb\u884c\u7cfb\u7edf\u64cd\u4f5c\uff08\u53cd\u8f6c\u903b\u8f91\u3001\u5426\u5b9a\u524d\u63d0\u3001\u53cd\u8f6c\u4e8b\u5b9e\u4e3b\u5f20\uff09\uff0c\u6d4b\u91cf\u7ec8\u7aef\u7b54\u6848\u7684\u56e0\u679c\u654f\u611f\u6027\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5b58\u5728\u6301\u7eed\u7684\"\u5fe0\u5b9e\u6027\u5dee\u8ddd\"\uff0c\u5b9a\u4e49\u5e76\u68c0\u6d4b\u5230\u5e7f\u6cdb\u7684\"\u56e0\u679c\u89e3\u8026\"\u6545\u969c\u6a21\u5f0f\uff0c\u5728\u4e8b\u5b9e\u548c\u79d1\u5b66\u9886\u57df\u4e2d\u8fdd\u53cd\u5bc6\u5ea6\u9ad8\u8fbe0.77\u3002\u667a\u80fd\u4f53\u5728\u5185\u90e8\u903b\u8f91\u77db\u76fe\u7684\u60c5\u51b5\u4e0b\u4ecd\u5f97\u51fa\u76f8\u540c\u7ed3\u8bba\uff0c\u8bc1\u660e\u63a8\u7406\u75d5\u8ff9\u53ea\u662f\"\u63a8\u7406\u5267\u573a\"\uff0c\u51b3\u7b56\u7531\u6f5c\u5728\u53c2\u6570\u5148\u9a8c\u63a7\u5236\u3002", "conclusion": "\u5f53\u524d\u667a\u80fd\u4f53\u67b6\u6784\u672c\u8d28\u4e0a\u5bb9\u6613\u4ea7\u751f\u4e0d\u5fe0\u5b9e\u7684\u89e3\u91ca\uff0c\u63d0\u51faAriadne\u5206\u6570\u4f5c\u4e3a\u65b0\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u5bf9\u9f50\u9648\u8ff0\u903b\u8f91\u4e0e\u6a21\u578b\u884c\u52a8\u3002", "topic": "agent analysis"}}
{"id": "2601.02346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02346", "abs": "https://arxiv.org/abs/2601.02346", "authors": ["Falcon LLM Team", "Iheb Chaabane", "Puneesh Khanna", "Suhail Mohmad", "Slim Frikha", "Shi Hu", "Abdalgader Abubaker", "Reda Alami", "Mikhail Lubinets", "Mohamed El Amine Seddik", "Hakim Hacid"], "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "comment": null, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "AI": {"tldr": "Falcon-H1R\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u63a8\u7406\u4f18\u5316\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u7ade\u4e89\u529b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6570\u636e\u7b5b\u9009\u548c\u9488\u5bf9\u6027\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u53c2\u6570\u6548\u7387\u3001\u63a8\u7406\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u80fd\u5426\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u63a8\u7406\u7cfb\u7edf\u3002", "method": "\u91c7\u75287B\u53c2\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6570\u636e\u7b5b\u9009\u548c\u9488\u5bf9\u6027\u8bad\u7ec3\u7b56\u7565\uff08\u5305\u62ec\u9ad8\u6548\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\uff09\uff0c\u7ed3\u5408\u6df7\u5408\u5e76\u884c\u67b6\u6784\u8bbe\u8ba1\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u5229\u7528DeepConf\u65b9\u6cd5\u5b9e\u73b0\u6700\u4f18\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u7387\u3002", "result": "Falcon-H1R\u5728\u591a\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u6bd4\u5176\u59272-7\u500d\u7684SOTA\u63a8\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u6548\u7387\u76843D\u63d0\u5347\uff08\u66f4\u5feb\u63a8\u7406\u3001\u66f4\u9ad8token\u6548\u7387\u3001\u66f4\u9ad8\u51c6\u786e\u6027\uff09\uff0c\u6210\u4e3a\u6269\u5c55\u63a8\u7406\u7cfb\u7edf\u7684\u5b9e\u7528\u9aa8\u5e72\u3002", "conclusion": "\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u67b6\u6784\u9009\u62e9\uff0c\u80fd\u591f\u63d0\u4f9b\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u63a8\u7406\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.01862", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01862", "abs": "https://arxiv.org/abs/2601.01862", "authors": ["Nuo Chen", "Hanpei Fang", "Piaohong Wang", "Jiqun Liu", "Tetsuya Sakai", "Xiao-Ming Wu"], "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment", "comment": null, "summary": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.\n  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u901a\u8fc7\u63d0\u793a\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u7279\u5b9a\u4eba\u683c\u7279\u8d28\u5982\u4f55\u5f71\u54cd\u7f51\u7edc\u641c\u7d22\u4e2d\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u53d1\u73b0\u67d0\u4e9b\u4eba\u683c\u7279\u8d28\uff08\u5982\u4f4e\u5b9c\u4eba\u6027\uff09\u80fd\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u6807\u6ce8\u5bf9\u9f50\uff0c\u800c\u4f4e\u5c3d\u8d23\u6027\u80fd\u6709\u6548\u5e73\u8861\u8fc7\u5ea6\u81ea\u4fe1\u548c\u81ea\u4fe1\u4e0d\u8db3\uff0c\u6700\u7ec8\u63d0\u51fa\u57fa\u4e8e\u4eba\u683c\u7279\u5f81\u548c\u7f6e\u4fe1\u5ea6\u7684\u5206\u7c7b\u5668\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u53d6\u5f97\u66f4\u597d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u7136\u8868\u660e\u63d0\u793a\u53ef\u4ee5\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u7279\u5b9a\u4eba\u683c\u7279\u8d28\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6a21\u62df\u4eba\u683c\u5982\u4f55\u5f71\u54cd\u5173\u952e\u7f51\u7edc\u641c\u7d22\u51b3\u7b56\uff08\u7279\u522b\u662f\u76f8\u5173\u6027\u8bc4\u4f30\uff09\u7684\u7406\u89e3\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff08\u8fc7\u5ea6\u81ea\u4fe1\u6216\u81ea\u4fe1\u4e0d\u8db3\u7684\u503e\u5411\uff09\u3002\u5fc3\u7406\u5b66\u6587\u732e\u8868\u660e\u8fd9\u4e9b\u504f\u89c1\u662f\u7279\u8d28\u76f8\u5173\u7684\uff0c\u4f46\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5546\u4e1a\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u63d0\u793a\u8ba9\u5b83\u4eec\u6a21\u62df\u5927\u4e94\u4eba\u683c\u7279\u8d28\u3002\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\uff08TREC DL 2019\u3001TREC DL 2020\u548cLLMJudge\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2-\u6587\u6863\u5bf9\u6536\u96c6\u4e24\u4e2a\u5173\u952e\u8f93\u51fa\uff1a\u76f8\u5173\u6027\u5224\u65ad\u548c\u81ea\u62a5\u544a\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5c06\u4eba\u683c\u6761\u4ef6\u5316\u7684\u5206\u6570\u548c\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u7279\u5f81\u6574\u5408\u5230\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u4e2d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f4e\u5b9c\u4eba\u6027\u4eba\u683c\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u6bd4\u65e0\u63d0\u793a\u6761\u4ef6\u66f4\u597d\uff1b\u4f4e\u5c3d\u8d23\u6027\u5728\u5e73\u8861\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\u548c\u81ea\u4fe1\u4e0d\u8db3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff1b\u76f8\u5173\u5206\u6570\u548c\u7f6e\u4fe1\u5ea6\u5206\u5e03\u5728\u4e0d\u540c\u4eba\u683c\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002\u57fa\u4e8e\u4eba\u683c\u7279\u5f81\u548c\u7f6e\u4fe1\u5ea6\u7684\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u5728\u65b0\u6570\u636e\u96c6\uff08TREC DL 2021\uff09\u4e0a\u8d85\u8d8a\u4e86\u6700\u4f73\u5355\u4e00\u4eba\u683c\u6761\u4ef6\uff0c\u5373\u4f7f\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u4eba\u683c\u884d\u751f\u7684\u7f6e\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u9884\u6d4b\u4fe1\u53f7\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5668\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8fd9\u8868\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4eba\u683c\u6a21\u62df\u53ef\u4ee5\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u641c\u7d22\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2601.01885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01885", "abs": "https://arxiv.org/abs/2601.01885", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "comment": null, "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "AI": {"tldr": "AgeMem\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6\uff0c\u5c06\u957f\u77ed\u671f\u8bb0\u5fc6\u6574\u5408\u5230\u667a\u80fd\u4f53\u7b56\u7565\u4e2d\uff0c\u901a\u8fc7\u5de5\u5177\u5316\u64cd\u4f5c\u548c\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u957f\u65f6\u7a0b\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u63a8\u7406\u4e2d\u9762\u4e34\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u9650\u5236\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u957f\u77ed\u671f\u8bb0\u5fc6\u4f5c\u4e3a\u5206\u79bb\u7ec4\u4ef6\u5904\u7406\uff0c\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u6216\u8f85\u52a9\u63a7\u5236\u5668\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u7aef\u5230\u7aef\u4f18\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faAgentic Memory (AgeMem)\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u8bb0\u5fc6\u7ba1\u7406\u76f4\u63a5\u96c6\u6210\u5230\u667a\u80fd\u4f53\u7b56\u7565\u4e2d\uff0c\u5c06\u8bb0\u5fc6\u64cd\u4f5c\u66b4\u9732\u4e3a\u5de5\u5177\u5316\u52a8\u4f5c\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bbe\u8ba1step-wise GRPO\u5904\u7406\u8bb0\u5fc6\u64cd\u4f5c\u5e26\u6765\u7684\u7a00\u758f\u548c\u4e0d\u8fde\u7eed\u5956\u52b1\u95ee\u9898\u3002", "result": "\u5728\u4e94\u4e2a\u957f\u65f6\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgeMem\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u5f3a\u8bb0\u5fc6\u589e\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u6027\u80fd\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u957f\u65f6\u8bb0\u5fc6\u548c\u66f4\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u4f7f\u7528\u3002", "conclusion": "AgeMem\u901a\u8fc7\u7edf\u4e00\u7684\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6\u548c\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u63a8\u7406\u4e2d\u7684\u8bb0\u5fc6\u7ba1\u7406\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u4f18\u5316\u8bb0\u5fc6\u64cd\u4f5c\u7684\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2601.01972", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01972", "abs": "https://arxiv.org/abs/2601.01972", "authors": ["Alexandre Le Mercier", "Chris Develder", "Thomas Demeester"], "title": "Hidden State Poisoning Attacks against Mamba-based Language Models", "comment": "17 pages, 4 figures. Submitted to ACL 2026", "summary": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u4e2d\u7684\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\uff08HiSPA\uff09\uff0c\u53d1\u73b0\u7279\u5b9a\u77ed\u8f93\u5165\u77ed\u8bed\u4f1a\u4e0d\u53ef\u9006\u5730\u8986\u76d6\u5176\u9690\u85cf\u72b6\u6001\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u51fa\u73b0\u90e8\u5206\u5931\u5fc6\u6548\u5e94\u3002\u4f5c\u8005\u521b\u5efa\u4e86RoBench25\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728HiSPA\u653b\u51fb\u4e0b\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff0c\u5e76\u8bc1\u5b9e\u4e86SSM\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5982Mamba\u63d0\u4f9b\u4e86Transformer\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22SSM\u4e2d\u7279\u5b9a\u77ed\u8f93\u5165\u77ed\u8bed\u5982\u4f55\u901a\u8fc7\u4e0d\u53ef\u9006\u5730\u8986\u76d6\u9690\u85cf\u72b6\u6001\u4fe1\u606f\u6765\u8bf1\u5bfc\u90e8\u5206\u5931\u5fc6\u6548\u5e94\u3002", "method": "1. \u63d0\u51fa\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\uff08HiSPA\uff09\u6982\u5ff5\uff0c\u5373\u7279\u5b9a\u77ed\u8f93\u5165\u77ed\u8bed\u4e0d\u53ef\u9006\u5730\u8986\u76d6\u6a21\u578b\u9690\u85cf\u72b6\u6001\u4fe1\u606f\uff1b2. \u521b\u5efaRoBench25\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728HiSPA\u653b\u51fb\u4e0b\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff1b3. \u5bf9SSM\u548cTransformer\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff1b4. \u5bf9Mamba\u7684\u9690\u85cf\u5c42\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u5206\u6790HiSPA\u653b\u51fb\u65f6\u7684\u6a21\u5f0f\u3002", "result": "1. RoBench25\u57fa\u51c6\u8bc1\u5b9e\u4e86SSM\u5bf9HiSPA\u653b\u51fb\u7684\u8106\u5f31\u6027\uff1b2. \u5373\u4f7f\u662f\u6700\u8fd1\u768452B\u6df7\u5408SSM-Transformer\u6a21\u578b\uff08Jamba\u5bb6\u65cf\uff09\u5728\u4f18\u5316\u7684HiSPA\u89e6\u53d1\u5668\u4e0b\u4e5f\u4f1a\u5728RoBench25\u4e0a\u5d29\u6e83\uff0c\u800c\u7eafTransformer\u6a21\u578b\u4e0d\u4f1a\uff1b3. HiSPA\u89e6\u53d1\u5668\u663e\u8457\u524a\u5f31\u4e86Jamba\u6a21\u578b\u5728Open-Prompt-Injections\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u800c\u7eafTransformer\u4e0d\u53d7\u5f71\u54cd\uff1b4. \u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63ed\u793a\u4e86Mamba\u9690\u85cf\u5c42\u5728HiSPA\u653b\u51fb\u671f\u95f4\u7684\u7279\u5b9a\u6a21\u5f0f\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5bf9\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\u5177\u6709\u663e\u8457\u8106\u5f31\u6027\uff0c\u8fd9\u79cd\u653b\u51fb\u4f1a\u4e0d\u53ef\u9006\u5730\u8986\u76d6\u6a21\u578b\u9690\u85cf\u72b6\u6001\u4fe1\u606f\u3002\u7814\u7a76\u53d1\u73b0\u7684\u6a21\u5f0f\u53ef\u7528\u4e8e\u6784\u5efaHiSPA\u7f13\u89e3\u7cfb\u7edf\uff0c\u4e3aSSM\u7684\u5b89\u5168\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2601.02023", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02023", "abs": "https://arxiv.org/abs/2601.02023", "authors": ["Amirali Ebrahimzadeh", "Seyyed M. Salili"], "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs", "comment": "25 pages, 8 figures, 3 tables", "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u4e2a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u4fe1\u606f\u63d0\u53d6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u5e76\u4e0d\u603b\u662f\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u8868\u73b0\u53d7\u4fe1\u606f\u5206\u5e03\u3001\u4f4d\u7f6e\u6548\u5e94\u548c\u53cd\u5e7b\u89c9\u63d0\u793a\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u968f\u7740LLM\u652f\u6301\u8d8a\u6765\u8d8a\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\uff0c\u4f46\u5b83\u4eec\u5728\u89c4\u6a21\u5316\u4fe1\u606f\u63d0\u53d6\u548c\u63a8\u7406\u65b9\u9762\u7684\u53ef\u9760\u6027\u4ecd\u4e0d\u660e\u786e\u3002\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\uff0c\u4e14\u4e0e\u771f\u5b9e\u8bed\u6599\u4e2d\u4fe1\u606f\u5206\u5e03\u65b9\u5f0f\u5f3a\u70c8\u4ea4\u4e92\u3002\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u5e38\u5c06\u5927\u91cf\u672a\u8fc7\u6ee4\u6587\u6863\u7c98\u8d34\u5230LLM\u63d0\u793a\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u6269\u5c55\u7684\"\u5927\u6d77\u635e\u9488\"\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u56db\u4e2a\u751f\u4ea7\u7ea7\u6a21\u578b\uff1aGemini-2.5-flash\u3001ChatGPT-5-mini\u3001Claude-4.5-haiku\u548cDeepseek-v3.2-chat\u3002\u5206\u522b\u8bc4\u4f30\u5b57\u9762\u63d0\u53d6\u3001\u903b\u8f91\u63a8\u7406\u548c\u5e7b\u89c9\u98ce\u9669\u3002\u8003\u8651\u4f4d\u7f6e\u6548\u5e94\u3001\u8bc1\u636e\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u5206\u5e03\uff0c\u4ee5\u53ca\u660e\u786e\u7981\u6b62\u634f\u9020\u7684\u63d0\u793a\u3002", "result": "\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u5e76\u4e0d\u4fdd\u8bc1\u66f4\u597d\u6027\u80fd\uff0c\u5f53\u76f8\u5173\u8bc1\u636e\u88ab\u7a00\u91ca\u6216\u5e7f\u6cdb\u5206\u6563\u65f6\u53ef\u80fd\u6709\u5bb3\u3002\u6a21\u578b\u95f4\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1a\u4e00\u4e9b\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u4e25\u91cd\u9000\u5316\uff0c\u800c\u53e6\u4e00\u4e9b\u5728\u8f83\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u66f4\u7a33\u5065\u3002\u53cd\u5e7b\u89c9\u6307\u4ee4\u53ef\u80fd\u4f7f\u67d0\u4e9b\u6a21\u578b\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u663e\u8457\u964d\u4f4e\u5b57\u9762\u63d0\u53d6\u548c\u903b\u8f91\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002\u6a21\u578b\u7ecf\u5e38\u96be\u4ee5\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u76f8\u5173\u4fe1\u606f\u3002", "conclusion": "\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6a21\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u7684\u7279\u5b9a\u9c81\u68d2\u6027\u5bf9\u4e8eLLM\u5728\u7814\u7a76\u548c\u4e1a\u52a1\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u8bb8\u591a\u5931\u8d25\u6e90\u4e8e\u65e0\u6548\u7684\u4e0a\u4e0b\u6587\u5229\u7528\uff0c\u800c\u975e\u4fe1\u606f\u7f3a\u5931\u3002\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u76f4\u63a5\u7c98\u8d34\u5927\u91cf\u672a\u8fc7\u6ee4\u6587\u6863\u7684\u505a\u6cd5\u9700\u8981\u8c28\u614e\u8003\u8651\u6a21\u578b\u7684\u5b9e\u9645\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.01298", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01298", "abs": "https://arxiv.org/abs/2601.01298", "authors": ["Jorge L. Ruiz Williams"], "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware", "comment": null, "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.", "AI": {"tldr": "Warp Cortex\u662f\u4e00\u79cd\u5f02\u6b65\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u4f8b\u6743\u91cd\u5171\u4eab\u548c\u62d3\u6251\u7a81\u89e6\u6280\u672f\uff0c\u5c06\u5185\u5b58\u590d\u6742\u5ea6\u4eceO(N*L)\u964d\u4f4e\u5230O(1)\u6743\u91cd\u548cO(N*k)\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u652f\u6301\u767e\u4e07\u7ea7\u667a\u80fd\u4f53\u6269\u5c55\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u5b58\u5728\u7ebf\u6027\u5185\u5b58\u6269\u5c55\u95ee\u9898\uff0c\u4f7f\u5f97\"\u7cfb\u7edf2\"\u5e76\u884c\u63a8\u7406\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u89e3\u51b3\u5185\u5b58\u74f6\u9888\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u4f8b\u6743\u91cd\u5171\u4eab\u548c\u62d3\u6251\u7a81\u89e6\u6280\u672f\uff08\u53d7\u62d3\u6251\u6570\u636e\u5206\u6790\u542f\u53d1\uff09\uff0c\u5c06KV\u7f13\u5b58\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\uff0c\u5e94\u7528\u89c1\u8bc1\u590d\u5f62\u7a00\u758f\u5316\u6280\u672f\uff0c\u5e76\u5f15\u5165\u5f15\u7528\u6ce8\u5165\u673a\u5236\u5b9e\u73b0\u975e\u4fb5\u5165\u5f0fKV\u7f13\u5b58\u66f4\u65b0\u3002", "result": "\u5728\u5355\u5f20NVIDIA RTX 4090\u4e0a\u5b9e\u73b0\u4e86100\u4e2a\u5e76\u53d1\u667a\u80fd\u4f53\u4ec5\u5360\u75282.2GB\u663e\u5b58\uff0c\u7406\u8bba\u5bb9\u91cf\u8d85\u8fc71000\u4e2a\u667a\u80fd\u4f53\uff0c\u8ba1\u7b97\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "Warp Cortex\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u4f18\u5316\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u5185\u5b58\u6269\u5c55\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.01357", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.01357", "abs": "https://arxiv.org/abs/2601.01357", "authors": ["Ke Xiao", "Haoze Zhang", "Runze Mao", "Han Li", "Zhi X. Chen"], "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows", "comment": null, "summary": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.", "AI": {"tldr": "FlamePilot\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u71c3\u70e7\u5efa\u6a21\u7684LLM\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u81ea\u52a8\u6267\u884cCFD\u6a21\u62df\u5de5\u4f5c\u6d41\uff0c\u4ece\u6587\u732e\u5b66\u4e60\u5230\u4eff\u771f\u914d\u7f6e\u3001\u6267\u884c\u548c\u4f18\u5316\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u5728\u590d\u6742\u79d1\u5b66\u9886\u57df\uff08\u5982\u71c3\u70e7\u5efa\u6a21\uff09\u7684\u5e94\u7528\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5c06\u9886\u57df\u6587\u732e\u77e5\u8bc6\u4e0e\u4e13\u4e1a\u5de5\u5177\uff08\u5982CFD\u4ee3\u7801\uff09\u7684\u6267\u884c\u80fd\u529b\u65e0\u7f1d\u96c6\u6210\u3002\u73b0\u6709AI\u52a9\u624b\u96be\u4ee5\u5904\u7406\u4e13\u4e1a\u77e5\u8bc6\u5bc6\u96c6\u7684\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u5de5\u5177\u3002", "method": "FlamePilot\u91c7\u7528\u57fa\u4e8e\u539f\u5b50\u5de5\u5177\u7684\u67b6\u6784\uff0c\u786e\u4fdd\u5728OpenFOAM\u548cDeepFlame\u7b49\u6846\u67b6\u4e2d\u7a33\u5065\u8bbe\u7f6e\u548c\u6267\u884c\u590d\u6742\u6a21\u62df\u3002\u7cfb\u7edf\u80fd\u591f\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u5b66\u4e60\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u6307\u5bfc\u4ece\u521d\u59cb\u8bbe\u7f6e\u5230\u4f18\u5316\u7ed3\u679c\u7684\u6574\u4e2a\u4eff\u771f\u8fc7\u7a0b\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlamePilot\u83b7\u5f97\u4e86\u5b8c\u7f8e\u76841.0\u53ef\u6267\u884c\u6027\u5206\u6570\u548c0.438\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u6700\u4f73\u62a5\u544a\u76840.625\u548c0.250\u5206\u6570\u3002\u5728MILD\u71c3\u70e7\u6a21\u62df\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u5c06\u7814\u7a76\u8bba\u6587\u8f6c\u5316\u4e3a\u914d\u7f6e\u7684\u6a21\u62df\uff0c\u6267\u884c\u4eff\u771f\uff0c\u540e\u5904\u7406\u7ed3\u679c\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bc1\u636e\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u5e76\u5728\u6700\u5c11\u4eba\u5de5\u5e72\u9884\u4e0b\u7ba1\u7406\u591a\u6b65\u53c2\u6570\u7814\u7a76\u76f4\u81f3\u6536\u655b\u3002", "conclusion": "FlamePilot\u901a\u8fc7\u900f\u660e\u53ef\u89e3\u91ca\u7684\u8303\u5f0f\uff0c\u4e3aAI\u8d4b\u80fd\u7684\u71c3\u70e7\u5efa\u6a21\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u4eba\u673a\u534f\u4f5c\u4f19\u4f34\u5173\u7cfb\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u7ba1\u7406\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6b21\u5206\u6790\u3002", "topic": "code agent"}}
{"id": "2601.01580", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01580", "abs": "https://arxiv.org/abs/2601.01580", "authors": ["Zibo Zhao", "Yuanting Zha", "Haipeng Zhang", "Xingcheng Xu"], "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs", "comment": null, "summary": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($\u03c0_{sample}$) for generation and decision ($\u03c0_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $\u03c0_{sample}$ while leaving $\u03c0_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($\u03c0_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u68af\u5ea6\u5f52\u56e0\u5c5e\u6027\u548c\u4e24\u9636\u6bb5\u51b3\u7b56-\u91c7\u6837\u5047\u8bbe\uff0c\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48RL\u540e\u8bad\u7ec3\u80fd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u800cSFT\u4e0d\u80fd\uff0c\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e86RL\u7684\u4f18\u8d8a\u6cdb\u5316\u4e3b\u8981\u6e90\u4e8e\u6539\u8fdb\u7684\u51b3\u7b56\u80fd\u529b\u800c\u975e\u91c7\u6837\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1RL\u540e\u8bad\u7ec3\u80fd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u4f46\u7edf\u4e00\u7684\u4f18\u5316\u76ee\u6807\u5982\u4f55\u4ea7\u751f\u751f\u6210\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u4f55\u65f6\u4fee\u8ba2\u8fd9\u4e24\u79cd\u529f\u80fd\u4e0a\u4e0d\u540c\u7684\u80fd\u529b\uff0c\u5176\u673a\u5236\u4ecd\u7136\u4e0d\u900f\u660e\u3002\u9700\u8981\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u4e3a\u4ec0\u4e48RL\u80fd\u6210\u529f\u800cSFT\u5931\u8d25\u3002", "method": "\u5f15\u5165\u68af\u5ea6\u5f52\u56e0\u5c5e\u6027\u6765\u523b\u753b\u5956\u52b1\u68af\u5ea6\u5728\u7b56\u7565\u7ec4\u4ef6\u95f4\u7684\u5206\u5e03\uff0c\u5f62\u5f0f\u5316\u4e3a\u4e24\u9636\u6bb5\u51b3\u7b56-\u91c7\u6837\u5047\u8bbe\uff0c\u5c06\u7b56\u7565\u5206\u89e3\u4e3a\u7528\u4e8e\u751f\u6210\u7684\u91c7\u6837\u7ec4\u4ef6\u548c\u7528\u4e8e\u9a8c\u8bc1\u7684\u51b3\u7b56\u7ec4\u4ef6\u3002\u7406\u8bba\u8bc1\u660e\u4ee3\u7406\u5956\u52b1\u5177\u6709\u5e73\u8861\u68af\u5ea6\u5f52\u56e0\uff0c\u800cSFT\u548cKL\u60e9\u7f5a\u5177\u6709\u4e0d\u5e73\u8861\u68af\u5ea6\u5f52\u56e0\uff0c\u957f\u5ea6\u52a0\u6743\u521b\u5efa\u4e86\u4e0d\u5bf9\u79f0\u6b63\u5219\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u957f\u5ea6\u52a0\u6743\u7684\u4e0d\u5bf9\u79f0\u6b63\u5219\u5316\u7ea6\u675f\u4e86\u91c7\u6837\u7ec4\u4ef6\u800c\u8ba9\u51b3\u7b56\u7ec4\u4ef6\u4f18\u5316\u4e0d\u8db3\uff0c\u8fd9\u89e3\u91ca\u4e86RL\u6210\u529f\u800cSFT\u5931\u8d25\u7684\u539f\u56e0\u3002\u5728\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660eRL\u7684\u4f18\u8d8a\u6cdb\u5316\u4e3b\u8981\u6e90\u4e8e\u6539\u8fdb\u7684\u51b3\u7b56\u80fd\u529b\u800c\u975e\u91c7\u6837\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u601d\u7ef4\u6a21\u578b\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u673a\u5236\u89e3\u91ca\uff0c\u63ed\u793a\u4e86RL\u540e\u8bad\u7ec3\u4e2d\u51b3\u7b56\u80fd\u529b\u6539\u8fdb\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u53cd\u601d\u80fd\u529b\u7684\u6d8c\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.01678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01678", "abs": "https://arxiv.org/abs/2601.01678", "authors": ["Siba Smarak Panigrahi", "Jovana Videnovi\u0107", "Maria Brbi\u0107"], "title": "HeurekaBench: A Benchmarking Framework for AI Co-scientist", "comment": "33 pages, 5 figures, 7 tables. Code available at https://github.com/mlbio-epfl/HeurekaBench", "summary": "LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.", "AI": {"tldr": "HeurekaBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u4ee3\u7406\u7cfb\u7edf\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u521b\u5efa\u57fa\u4e8e\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u79d1\u5b66\u4ee3\u7406\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u771f\u5b9e\u3001\u7aef\u5230\u7aef\u7684\u7814\u7a76\u573a\u666f\u6765\u6574\u5408\u6570\u636e\u5206\u6790\u3001\u89e3\u91ca\u548c\u4ece\u5b9e\u9a8c\u6570\u636e\u751f\u6210\u65b0\u89c1\u89e3\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u8fd9\u79cd\u73b0\u5b9e\u6027\u548c\u7efc\u5408\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u534a\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u591a\u4e2aLLM\u4ece\u79d1\u5b66\u7814\u7a76\u548c\u5bf9\u5e94\u4ee3\u7801\u4ed3\u5e93\u4e2d\u63d0\u53d6\u89c1\u89e3\u5e76\u751f\u6210\u5019\u9009\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7136\u540e\u4e0e\u62a5\u544a\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\u3002\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\u57fa\u51c6\u3002", "result": "\u4f7f\u7528\u8be5\u57fa\u51c6\u6bd4\u8f83\u4e86\u6700\u5148\u8fdb\u7684\u5355\u7ec6\u80de\u4ee3\u7406\uff0c\u53d1\u73b0\u6dfb\u52a0\u6279\u8bc4\u6a21\u5757\u53ef\u4ee5\u5c06\u5f00\u6e90LLM\u4ee3\u7406\u7684\u9519\u8bef\u54cd\u5e94\u6539\u5584\u9ad8\u8fbe22%\uff0c\u7f29\u5c0f\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u5dee\u8ddd\u3002\u5c55\u793a\u4e86\u57fa\u51c6\u5728\u5b9a\u91cf\u5206\u6790\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u9009\u62e9\u65b9\u9762\u7684\u4ef7\u503c\u3002", "conclusion": "HeurekaBench\u4e3a\u79d1\u5b66\u4ee3\u7406\u7684\u4e25\u683c\u7aef\u5230\u7aef\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u57fa\u51c6\u6784\u5efa\u951a\u5b9a\u5728\u771f\u5b9e\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u4ee3\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2601.01800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01800", "abs": "https://arxiv.org/abs/2601.01800", "authors": ["Qi Wei", "Junchao Fan", "Zhao Yang", "Jianhua Wang", "Jingkai Mao", "Xiaolin Chang"], "title": "Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving", "comment": null, "summary": "Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\\% across all cases compared to state-of-the-art baseline methods.", "AI": {"tldr": "CARRL\u662f\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u98ce\u9669\u66b4\u9732\u5bf9\u624b\u548c\u98ce\u9669\u76ee\u6807\u9c81\u68d2\u4ee3\u7406\u7684\u535a\u5f08\u8bbe\u8ba1\uff0c\u4e13\u95e8\u5904\u7406\u7a00\u758f\u7684\u5b89\u5168\u5173\u952e\u98ce\u9669\uff0c\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8106\u5f31\u6027\uff0c\u4f20\u7edf\u5bf9\u6297\u8bad\u7ec3\u91c7\u7528\u96f6\u548c\u535a\u5f08\u548c\u8fde\u7eed\u653b\u51fb\uff0c\u5ffd\u7565\u4e86\u667a\u80fd\u4f53\u4e0e\u5bf9\u624b\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u672a\u80fd\u53cd\u6620\u5b89\u5168\u5173\u952e\u98ce\u9669\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCARRL\u6846\u67b6\uff0c\u5305\u542b\u98ce\u9669\u66b4\u9732\u5bf9\u624b\uff08REA\uff09\u548c\u98ce\u9669\u76ee\u6807\u9c81\u68d2\u4ee3\u7406\uff08RTRA\uff09\u3002\u5c06\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u822c\u548c\u535a\u5f08\uff0cREA\u4e13\u6ce8\u4e8e\u66b4\u9732\u5b89\u5168\u5173\u952e\u6545\u969c\uff08\u5982\u78b0\u649e\uff09\uff0cRTRA\u5b66\u4e60\u5e73\u8861\u5b89\u5168\u4e0e\u9a7e\u9a76\u6548\u7387\u3002REA\u91c7\u7528\u89e3\u8026\u4f18\u5316\u673a\u5236\u8bc6\u522b\u7a00\u758f\u5b89\u5168\u5173\u952e\u65f6\u523b\uff0cRTRA\u901a\u8fc7\u53cc\u56de\u653e\u7f13\u51b2\u533a\u8054\u5408\u5229\u7528\u826f\u6027\u5bf9\u6297\u7ecf\u9a8c\uff0c\u5e76\u5728\u6270\u52a8\u4e0b\u5f3a\u5236\u7b56\u7565\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u81f3\u5c11\u964d\u4f4e\u4e8622.66%\u7684\u78b0\u649e\u7387\u3002", "conclusion": "CARRL\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9\u7a00\u758f\u5b89\u5168\u5173\u952e\u98ce\u9669\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.01803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01803", "abs": "https://arxiv.org/abs/2601.01803", "authors": ["Dennis Jabs", "Aditya Mohan", "Marius Lindauer"], "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions", "comment": "Workshop paper at RLDM'25", "summary": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(\u03b8)$, obtained by repeatedly sampling minibatches, updating $\u03b8$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(\u03b8)$ can improve stability, directly estimating $R(\u03b8)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(\u03b8)$. In such cases, our moment-based correction narrows $R(\u03b8)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u8bc4\u8bba\u5bb6\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u7684PPO\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\u6765\u51cf\u5c11\u7b56\u7565\u66f4\u65b0\u5f15\u8d77\u7684\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u76f8\u540c\u56de\u62a5\u4e0b\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u540c\u884c\u4e3a\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u73af\u5883\u548c\u7b97\u6cd5\u56e0\u7d20\u3002\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u5c0f\u7684\u53c2\u6570\u53d8\u5316\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6b65\u6001\uff0c\u5f71\u54cd\u7b97\u6cd5\u6bd4\u8f83\u548c\u5b9e\u9645\u5e94\u7528\u3002\u867d\u7136\u7ea6\u675f\u7b56\u7565\u4fdd\u6301\u7a84\u7684\u56de\u62a5\u5206\u5e03\u53ef\u4ee5\u6539\u5584\u7a33\u5b9a\u6027\uff0c\u4f46\u76f4\u63a5\u4f30\u8ba1\u8be5\u5206\u5e03\u5728\u8ba1\u7b97\u4e0a\u6602\u8d35\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u72b6\u6001-\u52a8\u4f5c\u56de\u62a5\u5206\u5e03\uff0c\u7136\u540e\u5229\u7528\u8be5\u5206\u5e03\u7684\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u5bf9PPO\u7684\u4f18\u52bf\u51fd\u6570\u8fdb\u884c\u504f\u7f6e\u3002\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\uff0c\u963b\u6b62\u7b56\u7565\u8fdb\u5165\u5bb9\u6613\u4ea7\u751f\u4e0d\u7a33\u5b9a\u6027\u7684\u53c2\u6570\u533a\u57df\u3002", "result": "\u5728Walker2D\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u7a33\u5b9a\u6027\u63d0\u9ad8\u4e8675%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\u56de\u62a5\u3002\u5f53\u66f4\u65b0\u540e\u8bc4\u8bba\u5bb6\u503c\u4e0e\u66f4\u65b0\u540e\u56de\u62a5\u5bf9\u9f50\u4e0d\u4f73\u65f6\uff0c\u6807\u51c6PPO\u96be\u4ee5\u4ea7\u751f\u7a84\u7684\u56de\u62a5\u5206\u5e03\uff0c\u800c\u672c\u6587\u7684\u77e9\u57fa\u4fee\u6b63\u65b9\u6cd5\u80fd\u591f\u7f29\u5c0f\u56de\u62a5\u5206\u5e03\u3002", "conclusion": "\u5229\u7528\u73af\u5883\u968f\u673a\u6027\u548c\u5206\u5e03\u8bc4\u8bba\u5bb6\u7684\u9ad8\u9636\u77e9\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u7b56\u7565\u66f4\u65b0\u5f15\u8d77\u7684\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.02036", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02036", "abs": "https://arxiv.org/abs/2601.02036", "authors": ["Yiyang Wang", "Xi Chen", "Xiaogang Xu", "Yu Liu", "Hengshuang Zhao"], "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models", "comment": null, "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.", "AI": {"tldr": "\u63d0\u51faGDRO\u65b9\u6cd5\uff0c\u4e00\u79cd\u9488\u5bf9\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u7684\u7fa4\u4f53\u7ea7\u76f4\u63a5\u5956\u52b1\u4f18\u5316\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u89e3\u51b3\u5728\u7ebfRL\u8bad\u7ec3\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u5728\u7ebfRL\u4eceLLMs\u5230\u6587\u672c\u5230\u56fe\u50cf\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5956\u52b1\u5bf9\u9f50\uff0c\u4f46\u9762\u4e34\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002\u6574\u6d41\u6d41\u6a21\u578b\u4e0eLLMs\u6709\u672c\u8d28\u5dee\u5f02\uff1a1) \u5728\u7ebf\u56fe\u50cf\u91c7\u6837\u8017\u65f6\uff1b2) \u6574\u6d41\u6d41\u5728\u521d\u59cb\u566a\u58f0\u56fa\u5b9a\u540e\u662f\u786e\u5b9a\u6027\u7684", "method": "\u8bbe\u8ba1Group-level Direct Reward Optimization (GDRO)\uff0c\u7ed3\u5408\u6574\u6d41\u6d41\u6a21\u578b\u7279\u6027\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u540e\u8bad\u7ec3\u8303\u5f0f\u3002\u652f\u6301\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u8282\u7701\u56fe\u50cf\u91c7\u6837\u65f6\u95f4\uff1b\u6269\u6563\u91c7\u6837\u5668\u65e0\u5173\uff0c\u65e0\u9700ODE-to-SDE\u8fd1\u4f3c\uff1b\u8003\u8651\u5956\u52b1\u9ed1\u5ba2\u9677\u9631\uff0c\u4f7f\u7528\u4fee\u6b63\u8bc4\u5206\u8fdb\u884c\u8bc4\u4f30", "result": "GDRO\u5728OCR\u548cGenEval\u4efb\u52a1\u4e0a\u901a\u8fc7\u7fa4\u4f53\u7ea7\u79bb\u7ebf\u4f18\u5316\u6709\u6548\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5206\u6570\uff0c\u540c\u65f6\u5c55\u73b0\u5f3a\u5927\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898", "conclusion": "GDRO\u4e3a\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u3001\u9c81\u68d2\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5728\u7ebfRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2601.02196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02196", "abs": "https://arxiv.org/abs/2601.02196", "authors": ["Yu Li", "Sizhe Tang", "Rongqian Chen", "Fei Xu Yu", "Guangyu Jiang", "Mahdi Imani", "Nathaniel D. Bastian", "Tian Lan"], "title": "ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense", "comment": null, "summary": "Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u65b9\u6cd5\uff0c\u5728CAGE-4\u6311\u6218\u4e2d\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u7684\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u65b9\u6cd5\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e2d\u9762\u4e34\u63a2\u7d22\u56f0\u96be\u3001\u51b3\u7b56\u7a7a\u95f4\u5927\u3001\u9700\u8981\u5927\u91cf\u6837\u672c\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u6837\u672c\u6548\u7387\u66f4\u9ad8\u7684\u9632\u5fa1\u7b56\u7565\u3002", "method": "\u5c06\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u5efa\u6a21\u4e3a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u89c4\u5212\u4e2d\u5fc3\u9632\u5fa1\u7b56\u7565\u3002\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u7f51\u7edc\u89c2\u6d4b\u4f5c\u4e3a\u5c5e\u6027\u56fe\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u56fe\u5d4c\u5165\u548c\u56fe\u7f16\u8f91\u52a8\u4f5c\u5148\u9a8c\u6765\u6307\u5bfcMCTS\u641c\u7d22\u3002", "result": "\u5728CAGE-4\u6311\u6218\u7684\u5404\u79cd\u7f51\u7edc\u7ed3\u6784\u548c\u5bf9\u624b\u884c\u4e3a\u573a\u666f\u4e2d\uff0c\u8be5\u641c\u7d22\u5f15\u5bfc\u3001\u57fa\u4e8e\u56fe\u5d4c\u5165\u7684\u89c4\u5212\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u63d0\u9ad8\u4e86\u9632\u5fa1\u5956\u52b1\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u6cdb\u5316\u3001\u7b56\u7565\u84b8\u998f\u548c\u524d\u5411\u89c4\u5212\u7684MCTS\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u81ea\u52a8\u5316\u7f51\u7edc\u9632\u5fa1\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u9632\u5fa1\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.02201", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02201", "abs": "https://arxiv.org/abs/2601.02201", "authors": ["Keyu Wang", "Bingchen Miao", "Wendong Bu", "Yu Wu", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Jun Xiao", "Yueting Zhuang"], "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents", "comment": "19 pages, 12 figures", "summary": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.", "AI": {"tldr": "CORE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u7684\u9006\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u6269\u5c55\u6865\u63a5\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u589e\u5f3a\u884c\u4e3a\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u865a\u62df\u4ee3\u7406\u8bad\u7ec3\u9762\u4e34\u4e24\u96be\uff1a\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u4f46\u884c\u4e3a\u591a\u6837\u6027\u4f4e\uff0c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u53d1\u73b0\u65b0\u7b56\u7565\u4f46\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u51b2\u7a81\u3002", "method": "1. \u8bed\u4e49\u4ee3\u7801\u62bd\u8c61\uff1a\u81ea\u52a8\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u63a8\u65ad\u5956\u52b1\u51fd\u6570\uff08\u6807\u7b7e\u51fd\u6570\uff09\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\uff1b2. \u7b56\u7565\u56fe\u6269\u5c55\uff1a\u6784\u5efa\u591a\u8def\u5f84\u7b56\u7565\u56fe\u6355\u6349\u591a\u6837\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff1b3. \u8f68\u8ff9\u5f15\u5bfc\u5916\u63a8\uff1a\u5229\u7528\u6210\u529f\u548c\u5931\u8d25\u8f68\u8ff9\u6269\u5c55\u4efb\u52a1\u7a7a\u95f4\uff0c\u589e\u5f3a\u9886\u57df\u5916\u884c\u4e3a\u591a\u6837\u6027\u3002", "result": "\u5728Web\u548cAndroid\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCORE\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u4f5c\u4e3a\u6784\u5efa\u5f3a\u5927\u865a\u62df\u4ee3\u7406\u7684\u9c81\u68d2\u4e14\u53ef\u6cdb\u5316\u8bad\u7ec3\u8303\u5f0f\u7684\u6f5c\u529b\u3002", "conclusion": "CORE\u901a\u8fc7\u6865\u63a5\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u884c\u4e3a\u591a\u6837\u6027\u4e0e\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u7684\u51b2\u7a81\uff0c\u4e3a\u591a\u6a21\u6001\u865a\u62df\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.ef150028", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fis-agentic-metadata-the-next-infrastructure-layer%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/KcLruhcU92qwevZTMgtFnd5A-Cp7_VWcL1Gre-lRoJ0=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fis-agentic-metadata-the-next-infrastructure-layer%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/KcLruhcU92qwevZTMgtFnd5A-Cp7_VWcL1Gre-lRoJ0=438", "authors": ["TLDR Newsletter"], "title": "Is Agentic Metadata the Next Infrastructure Layer?", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fis-agentic-metadata-the-next-infrastructure-layer%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/KcLruhcU92qwevZTMgtFnd5A-Cp7_VWcL1Gre-lRoJ0=438", "summary": "Is Agentic Metadata the Next Infrastructure Layer? (13 minute read) AI agents generate \"agentic metadata,\" which includes rich data like reasoning traces, user prompts, and tool calls. This metadata is invaluable for debugging, continuous improvement, cost optimization, and governance and compliance. However, effectively collecting, storing, and operationalizing this fragmented data has a lot of challenges.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u4ee3\u7406\u751f\u6210\u7684\"\u4ee3\u7406\u5143\u6570\u636e\"\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\u5c42\u7684\u6f5c\u529b\uff0c\u5305\u62ec\u63a8\u7406\u8f68\u8ff9\u3001\u7528\u6237\u63d0\u793a\u548c\u5de5\u5177\u8c03\u7528\u7b49\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5bf9\u8c03\u8bd5\u3001\u6301\u7eed\u6539\u8fdb\u3001\u6210\u672c\u4f18\u5316\u548c\u6cbb\u7406\u5408\u89c4\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u6536\u96c6\u3001\u5b58\u50a8\u548c\u5229\u7528\u8fd9\u4e9b\u788e\u7247\u5316\u6570\u636e\u9762\u4e34\u6311\u6218\u3002", "motivation": "AI\u4ee3\u7406\u751f\u6210\u4e30\u5bcc\u7684\u5143\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u5bf9\u7cfb\u7edf\u8c03\u8bd5\u3001\u6027\u80fd\u4f18\u5316\u3001\u6210\u672c\u63a7\u5236\u548c\u5408\u89c4\u7ba1\u7406\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u76ee\u524d\u8fd9\u4e9b\u6570\u636e\u5206\u6563\u4e14\u96be\u4ee5\u6709\u6548\u5229\u7528\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u5904\u7406\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u5c06\"\u4ee3\u7406\u5143\u6570\u636e\"\u6982\u5ff5\u5316\u4e3a\u57fa\u7840\u8bbe\u65bd\u5c42\u7684\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u5982\u4f55\u7cfb\u7edf\u5316\u6536\u96c6\u3001\u5b58\u50a8\u548c\u64cd\u4f5c\u5316\u8fd9\u4e9b\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u6807\u51c6\u5316\u3001\u5b58\u50a8\u67b6\u6784\u548c\u5229\u7528\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u4e86\u4ee3\u7406\u5143\u6570\u636e\u4f5c\u4e3a\u57fa\u7840\u8bbe\u65bd\u5c42\u7684\u5173\u952e\u4ef7\u503c\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6570\u636e\u788e\u7247\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u5efa\u7acb\u7edf\u4e00\u5143\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u4ee3\u7406\u5143\u6570\u636e\u6709\u671b\u6210\u4e3aAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\u5c42\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u6846\u67b6\u548c\u5de5\u5177\u6765\u89e3\u51b3\u6570\u636e\u788e\u7247\u5316\u95ee\u9898\uff0c\u4ee5\u5145\u5206\u53d1\u6325\u5176\u5728\u8c03\u8bd5\u3001\u4f18\u5316\u548c\u6cbb\u7406\u65b9\u9762\u7684\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.c8d44450", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/zeho-540QkPZEVqkTevbpL_gbJzRnru7eGZJtNXzdUs=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/zeho-540QkPZEVqkTevbpL_gbJzRnru7eGZJtNXzdUs=438", "authors": ["TLDR Newsletter"], "title": "Claude Code On-The-Go", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/zeho-540QkPZEVqkTevbpL_gbJzRnru7eGZJtNXzdUs=438", "summary": "Claude Code On-The-Go (4 minute read) This dev uses a cloud VM, Termius, and push notifications to run multiple Claude Code agents in parallel on their phone, allowing for asynchronous development from anywhere.", "source": "tldr", "AI": {"tldr": "\u7528\u6237\u4f7f\u7528\u4e91\u865a\u62df\u673a\u3001Termius\u548c\u63a8\u9001\u901a\u77e5\u5728\u624b\u673a\u4e0a\u5e76\u884c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\uff0c\u5b9e\u73b0\u968f\u65f6\u968f\u5730\u5f02\u6b65\u5f00\u53d1", "motivation": "\u5f00\u53d1\u8005\u5e0c\u671b\u80fd\u591f\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fdb\u884c\u7f16\u7a0b\u5f00\u53d1\uff0c\u7a81\u7834\u4f20\u7edf\u5f00\u53d1\u73af\u5883\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u968f\u65f6\u968f\u5730\u5f02\u6b65\u5f00\u53d1", "method": "\u4f7f\u7528\u4e91\u865a\u62df\u673a\u4f5c\u4e3a\u5f00\u53d1\u73af\u5883\uff0c\u901a\u8fc7Termius\u7ec8\u7aef\u5e94\u7528\u8fde\u63a5\uff0c\u7ed3\u5408\u63a8\u9001\u901a\u77e5\u673a\u5236\uff0c\u5728\u624b\u673a\u4e0a\u5e76\u884c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406", "result": "\u6210\u529f\u5728\u624b\u673a\u4e0a\u5b9e\u73b0\u4e86\u5f02\u6b65\u5f00\u53d1\u80fd\u529b\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u4ece\u4efb\u4f55\u5730\u65b9\u8fdb\u884c\u7f16\u7a0b\u5de5\u4f5c\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u7075\u6d3b\u6027\u548c\u6548\u7387", "conclusion": "\u901a\u8fc7\u4e91\u57fa\u7840\u8bbe\u65bd\u548c\u79fb\u52a8\u7ec8\u7aef\u5e94\u7528\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u7a0b\u5e8f\u5f00\u53d1\u5de5\u4f5c\u6d41", "topic": "code agent"}}
{"id": "tldr.2601.3dd6c51c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fnibzard%2Fawesome-agentic-patterns%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uLUvjgRfWujt6zEVN8ZOpHckCqNnzk7dn-rdsbp1-ws=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fnibzard%2Fawesome-agentic-patterns%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uLUvjgRfWujt6zEVN8ZOpHckCqNnzk7dn-rdsbp1-ws=438", "authors": ["TLDR Newsletter"], "title": "Awesome Agentic Patterns", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fnibzard%2Fawesome-agentic-patterns%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/uLUvjgRfWujt6zEVN8ZOpHckCqNnzk7dn-rdsbp1-ws=438", "summary": "Awesome Agentic Patterns (GitHub Repo) This is a catalog of real-world agentic AI patterns, including tricks, workflows, and mini-architectures, designed to help autonomous or semi-autonomous AI agents accomplish useful tasks in production environments.", "source": "tldr", "AI": {"tldr": "GitHub\u4ed3\u5e93\u6536\u96c6\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u667a\u80fd\u4f53\u6a21\u5f0f\uff0c\u5305\u62ec\u6280\u5de7\u3001\u5de5\u4f5c\u6d41\u548c\u5fae\u67b6\u6784\uff0c\u5e2e\u52a9\u81ea\u4e3b\u6216\u534a\u81ea\u4e3bAI\u667a\u80fd\u4f53\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5b8c\u6210\u6709\u7528\u4efb\u52a1", "motivation": "\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u5b9e\u7528\u7684\u751f\u4ea7\u7ea7\u6a21\u5f0f\u53c2\u8003\uff0c\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u6280\u672f\u7684\u843d\u5730\u5e94\u7528", "method": "\u6536\u96c6\u6574\u7406\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u667a\u80fd\u4f53\u6a21\u5f0f\uff0c\u5305\u62ec\u6280\u5de7\u3001\u5de5\u4f5c\u6d41\u7a0b\u548c\u5fae\u67b6\u6784\u8bbe\u8ba1\uff0c\u5f62\u6210\u53ef\u590d\u7528\u7684\u6a21\u5f0f\u5e93", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u5b9e\u7528\u667a\u80fd\u4f53\u6a21\u5f0f\u7684GitHub\u4ed3\u5e93\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6700\u4f73\u5b9e\u8df5\u53c2\u8003", "conclusion": "\u901a\u8fc7\u6a21\u5f0f\u5e93\u7684\u5f62\u5f0f\u7cfb\u7edf\u5316\u667a\u80fd\u4f53\u5f00\u53d1\u7ecf\u9a8c\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u667a\u80fd\u4f53\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528\u548c\u63a8\u5e7f", "topic": "agent analysis"}}
{"id": "tldr.2601.967c13bd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Ffuture-agentic-coding%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iIyiVIDTxnzLc6xeQgY4PdB5pf2jKb_GkVIzJ1-zNeU=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Ffuture-agentic-coding%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iIyiVIDTxnzLc6xeQgY4PdB5pf2jKb_GkVIzJ1-zNeU=438", "authors": ["TLDR Newsletter"], "title": "The future of agentic coding: conductors to orchestrators", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 30 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Ffuture-agentic-coding%2F%3Futm_source=tldrdev/1/0100019b8e1077e6-24335e11-74b1-4245-928c-a29193aa24a7-000000/iIyiVIDTxnzLc6xeQgY4PdB5pf2jKb_GkVIzJ1-zNeU=438", "summary": "The future of agentic coding: conductors to orchestrators (30 minute read) The future of agentic coding will transform software engineers from direct implementers into \"conductors\" guiding single AI assistants and increasingly into \"orchestrators\" managing autonomous fleets of agents to speed up software development.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee3\u7406\u7f16\u7801\u7684\u672a\u6765\uff0c\u9884\u6d4b\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5c06\u4ece\u76f4\u63a5\u5b9e\u65bd\u8005\u8f6c\u53d8\u4e3a\"\u6307\u6325\u5bb6\"\u6307\u5bfc\u5355\u4e2aAI\u52a9\u624b\uff0c\u5e76\u8fdb\u4e00\u6b65\u53d1\u5c55\u4e3a\"\u7f16\u6392\u8005\"\u7ba1\u7406\u81ea\u4e3b\u4ee3\u7406\u8230\u961f\u4ee5\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u3002", "motivation": "\u968f\u7740AI\u52a9\u624b\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u8f6f\u4ef6\u5f00\u53d1\u89d2\u8272\u6b63\u5728\u53d1\u751f\u6839\u672c\u6027\u8f6c\u53d8\u3002\u4f20\u7edf\u7684\u5de5\u7a0b\u5e08\u4f5c\u4e3a\u76f4\u63a5\u7f16\u7801\u8005\u7684\u89d2\u8272\u6b63\u5728\u88ab\u91cd\u65b0\u5b9a\u4e49\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5229\u7528AI\u4ee3\u7406\u6765\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dAI\u7f16\u7801\u52a9\u624b\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u63d0\u51fa\u4ece\"\u6307\u6325\u5bb6\"\u5230\"\u7f16\u6392\u8005\"\u7684\u89d2\u8272\u6f14\u8fdb\u6a21\u578b\uff0c\u63a2\u8ba8\u5de5\u7a0b\u5e08\u5982\u4f55\u4ece\u6307\u5bfc\u5355\u4e2aAI\u52a9\u624b\u53d1\u5c55\u5230\u7ba1\u7406\u591a\u4e2a\u81ea\u4e3b\u4ee3\u7406\u7684\u534f\u4f5c\u3002", "result": "\u63d0\u51fa\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u89d2\u8272\u6f14\u8fdb\u7684\u6846\u67b6\uff1a\u4ece\u76f4\u63a5\u7f16\u7801\u8005\u5230\u6307\u5bfc\u5355\u4e2aAI\u7684\"\u6307\u6325\u5bb6\"\uff0c\u518d\u5230\u7ba1\u7406\u4ee3\u7406\u8230\u961f\u7684\"\u7f16\u6392\u8005\"\uff0c\u8fd9\u79cd\u8f6c\u53d8\u5c06\u663e\u8457\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u3002", "conclusion": "\u4ee3\u7406\u7f16\u7801\u7684\u672a\u6765\u5c06\u4f7f\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u89d2\u8272\u53d1\u751f\u6839\u672c\u6027\u8f6c\u53d8\uff0c\u4ece\u5b9e\u65bd\u8005\u8f6c\u53d8\u4e3a\u6218\u7565\u6307\u5bfc\u8005\u548c\u7cfb\u7edf\u7ba1\u7406\u8005\uff0c\u901a\u8fc7\u6709\u6548\u5229\u7528AI\u4ee3\u7406\u8230\u961f\u6765\u5927\u5e45\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2601.edb4823f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaven.com%2Fp%2Fe5ba0f%2Fbuild-your-first-reddit-ai-agent-automate-growth%3Futm_source=tldrmarketing/1/0100019b8e1a536c-bb5725fd-2aef-4c88-bc96-6a62d5e1dbf7-000000/aqGmm9G9ltzNTFzZBNuMkMse9Yqy5eeEgV4Wj0qtuE8=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaven.com%2Fp%2Fe5ba0f%2Fbuild-your-first-reddit-ai-agent-automate-growth%3Futm_source=tldrmarketing/1/0100019b8e1a536c-bb5725fd-2aef-4c88-bc96-6a62d5e1dbf7-000000/aqGmm9G9ltzNTFzZBNuMkMse9Yqy5eeEgV4Wj0qtuE8=438", "authors": ["TLDR Newsletter"], "title": "Build Your First Reddit AI Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaven.com%2Fp%2Fe5ba0f%2Fbuild-your-first-reddit-ai-agent-automate-growth%3Futm_source=tldrmarketing/1/0100019b8e1a536c-bb5725fd-2aef-4c88-bc96-6a62d5e1dbf7-000000/aqGmm9G9ltzNTFzZBNuMkMse9Yqy5eeEgV4Wj0qtuE8=438", "summary": "Build Your First Reddit AI Agent (Webinar) About 25% of Reddit posts relate to product recommendations, making it a high-intent space for buyers. This webinar walks through how to create a workflow that automatically surfaces high-intent discussions. It will take place on January 14 at 9:30 AM PST.", "source": "tldr", "AI": {"tldr": "\u5173\u4e8eReddit AI\u4ee3\u7406\u7684\u5728\u7ebf\u7814\u8ba8\u4f1a\uff0c\u6307\u5bfc\u5982\u4f55\u521b\u5efa\u81ea\u52a8\u8bc6\u522b\u9ad8\u610f\u5411\u4ea7\u54c1\u8ba8\u8bba\u7684\u5de5\u4f5c\u6d41", "motivation": "Reddit\u4e0a\u7ea625%\u7684\u5e16\u5b50\u4e0e\u4ea7\u54c1\u63a8\u8350\u76f8\u5173\uff0c\u8868\u660e\u8fd9\u662f\u4e00\u4e2a\u9ad8\u8d2d\u4e70\u610f\u5411\u7684\u5e73\u53f0\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u8bc6\u522b\u548c\u5229\u7528\u8fd9\u4e9b\u8ba8\u8bba", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u8ba8\u4f1a\u5f62\u5f0f\uff0c\u9010\u6b65\u6307\u5bfc\u521b\u5efa\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u548c\u63d0\u53d6Reddit\u4e0a\u7684\u9ad8\u610f\u5411\u4ea7\u54c1\u8ba8\u8bba", "result": "\u7814\u8ba8\u4f1a\u5c06\u4e8e1\u670814\u65e5\u592a\u5e73\u6d0b\u65f6\u95f4\u4e0a\u53489:30\u4e3e\u884c\uff0c\u53c2\u4e0e\u8005\u5c06\u5b66\u4e60\u5230\u6784\u5efaReddit AI\u4ee3\u7406\u7684\u5b9e\u9645\u6280\u80fd", "conclusion": "\u8be5\u7814\u8ba8\u4f1a\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5728Reddit\u5e73\u53f0\u4e0a\u6784\u5efaAI\u4ee3\u7406\u7684\u5b9e\u7528\u6307\u5bfc\uff0c\u5e2e\u52a9\u5229\u7528\u5e73\u53f0\u4e0a\u7684\u9ad8\u610f\u5411\u7528\u6237\u8ba8\u8bba", "topic": "code agent"}}
{"id": "tldr.2601.9824ebac", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fwrap-up-your-backlog-with-github-copilot-coding-agent%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ZL4c67C525dVqExcqB5TiGEQP8rYJSaYUnWVwtCRhCM=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fwrap-up-your-backlog-with-github-copilot-coding-agent%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ZL4c67C525dVqExcqB5TiGEQP8rYJSaYUnWVwtCRhCM=438", "authors": ["TLDR Newsletter"], "title": "WRAP up your backlog with GitHub Copilot coding agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fwrap-up-your-backlog-with-github-copilot-coding-agent%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ZL4c67C525dVqExcqB5TiGEQP8rYJSaYUnWVwtCRhCM=438", "summary": "WRAP up your backlog with GitHub Copilot coding agent (6 minute read) WRAP helps developers maximize GitHub Copilot coding agent by writing clear issues, refining instructions, breaking work into atomic tasks, and pairing human judgment with AI for efficient, accurate, and scalable code completion.", "source": "tldr", "AI": {"tldr": "WRAP\u662f\u4e00\u4e2a\u5e2e\u52a9\u5f00\u53d1\u8005\u6700\u5927\u5316GitHub Copilot\u7f16\u7801\u4ee3\u7406\u6548\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u5199\u6e05\u6670\u95ee\u9898\u3001\u7ec6\u5316\u6307\u4ee4\u3001\u5206\u89e3\u539f\u5b50\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u4e0eAI\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u3001\u53ef\u6269\u5c55\u7684\u4ee3\u7801\u8865\u5168\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u4f7f\u7528GitHub Copilot\u7b49AI\u7f16\u7801\u52a9\u624b\u65f6\uff0c\u7ecf\u5e38\u9762\u4e34\u6307\u4ee4\u4e0d\u6e05\u6670\u3001\u4efb\u52a1\u5206\u89e3\u4e0d\u5f53\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4AI\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u4e0d\u9ad8\u3001\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u4f18\u5316\u4eba\u7c7b\u4e0eAI\u7f16\u7801\u4ee3\u7406\u7684\u534f\u4f5c\u3002", "method": "WRAP\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u6b65\u9aa4\uff1a1) \u7f16\u5199\u6e05\u6670\u7684\u95ee\u9898\u63cf\u8ff0\uff1b2) \u7ec6\u5316\u548c\u4f18\u5316\u6307\u4ee4\uff1b3) \u5c06\u590d\u6742\u5de5\u4f5c\u5206\u89e3\u4e3a\u539f\u5b50\u4efb\u52a1\uff1b4) \u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\u4e0eAI\u80fd\u529b\u8fdb\u884c\u4ee3\u7801\u8865\u5168\u3002\u8be5\u65b9\u6cd5\u5f3a\u8c03\u7cfb\u7edf\u5316\u7684\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\u3002", "result": "WRAP\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347GitHub Copilot\u7b49\u7f16\u7801\u4ee3\u7406\u7684\u6548\u80fd\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u4ee3\u7801\u8865\u5168\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u66f4\u597d\u5730\u5229\u7528AI\u7f16\u7801\u52a9\u624b\u5904\u7406\u79ef\u538b\u4efb\u52a1\u3002", "conclusion": "WRAP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\u6765\u4f18\u5316\u5f00\u53d1\u8005\u4e0eAI\u7f16\u7801\u4ee3\u7406\u7684\u534f\u4f5c\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u95ee\u9898\u63cf\u8ff0\u3001\u6307\u4ee4\u4f18\u5316\u548c\u4efb\u52a1\u5206\u89e3\uff0c\u7ed3\u5408\u4eba\u7c7b\u5224\u65ad\uff0c\u80fd\u591f\u6700\u5927\u5316GitHub Copilot\u7b49\u5de5\u5177\u7684\u6548\u80fd\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2601.72d1da28", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/mFK7i431xcCOSjbPB0NsyAOTyXUhbK01s64UWZq1pA0=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/mFK7i431xcCOSjbPB0NsyAOTyXUhbK01s64UWZq1pA0=438", "authors": ["TLDR Newsletter"], "title": "Claude Code On-The-Go", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgranda.org%2Fen%2F2026%2F01%2F02%2Fclaude-code-on-the-go%2F%3Futm_source=tldrdevops/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/mFK7i431xcCOSjbPB0NsyAOTyXUhbK01s64UWZq1pA0=438", "summary": "Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e00\u79cd\u79fb\u52a8\u4f18\u5148\u7684\u5f00\u53d1\u73af\u5883\u914d\u7f6e\uff0c\u901a\u8fc7\u624b\u673a\u4f7f\u7528Termius+mosh\u8fde\u63a5\u6309\u9700\u4ed8\u8d39\u7684Vultr\u865a\u62df\u673a\uff0c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u5e76\u884c\u5de5\u4f5c\uff0c\u5229\u7528tmux\u4fdd\u6301\u4f1a\u8bdd\u6301\u4e45\u6027\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5f00\u53d1\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fdb\u884c\u4ee3\u7801\u5f00\u53d1\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u4e91\u7aef\u5f00\u53d1\u73af\u5883\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u968f\u65f6\u968f\u5730\u901a\u8fc7\u624b\u673a\u8fdb\u884c\u9ad8\u6548\u7f16\u7a0b\u3002", "method": "\u4f7f\u7528Termius\u79fb\u52a8\u7aefSSH\u5ba2\u6237\u7aef\u914d\u5408mosh\u534f\u8bae\u8fde\u63a5Vultr\u6309\u9700\u4ed8\u8d39\u865a\u62df\u673a\uff0c\u901a\u8fc7Tailscale\u786e\u4fdd\u5b89\u5168\u8fde\u63a5\uff0c\u5229\u7528tmux\u7ba1\u7406\u6301\u4e45\u4f1a\u8bdd\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5206\u652f\u5f00\u53d1\uff0c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u79fb\u52a8\u4f18\u5148\u7684\u4e91\u7aef\u5f00\u53d1\u73af\u5883\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u901a\u8fc7\u624b\u673a\u9ad8\u6548\u8fd0\u884c\u591a\u4e2aAI\u4ee3\u7801\u4ee3\u7406\uff0c\u5b9e\u73b0\u968f\u65f6\u968f\u5730\u7684\u7f16\u7a0b\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u79fb\u52a8\u8bbe\u5907\u914d\u5408\u4e91\u7aef\u865a\u62df\u673a\u53ef\u4ee5\u6210\u4e3a\u6709\u6548\u7684\u5f00\u53d1\u5e73\u53f0\uff0c\u901a\u8fc7\u5408\u9002\u7684\u5de5\u5177\u94fe\u914d\u7f6e\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u79fb\u52a8\u7f16\u7a0b\u4f53\u9a8c\u3002", "topic": "code agent"}}
{"id": "tldr.2601.14d87301", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ioP_EkoNUfVcsmsknzSFO06B2cFFm0mAN61S-bUCwi4=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ioP_EkoNUfVcsmsknzSFO06B2cFFm0mAN61S-bUCwi4=438", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ioP_EkoNUfVcsmsknzSFO06B2cFFm0mAN61S-bUCwi4=438", "summary": "Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.", "source": "tldr", "AI": {"tldr": "\u5728\u624b\u673a\u4e0a\u901a\u8fc7Termius+mosh\u8fde\u63a5Vultr\u4e91\u670d\u52a1\u5668\uff0c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u8fdb\u884c\u5e76\u884c\u5f00\u53d1\uff0c\u4f7f\u7528tmux\u4fdd\u6301\u4f1a\u8bdd\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5f00\u53d1", "motivation": "\u89e3\u51b3\u79fb\u52a8\u7aef\u5f00\u53d1\u73af\u5883\u642d\u5efa\u95ee\u9898\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u5728\u624b\u673a\u4e0a\u4f7f\u7528\u5f3a\u5927\u7684\u4ee3\u7801\u4ee3\u7406\u5de5\u5177\u8fdb\u884c\u5e76\u884c\u5f00\u53d1\u5de5\u4f5c", "method": "\u91c7\u7528\u79fb\u52a8\u4f18\u5148\u7684\u5f00\u53d1\u8bbe\u7f6e\uff1aTermius+mosh\u8fde\u63a5Vultr\u6309\u9700\u4ed8\u8d39VM\uff0cTailscale\u786e\u4fdd\u5b89\u5168\uff0ctmux\u63d0\u4f9b\u4f1a\u8bdd\u6301\u4e45\u5316\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5f00\u53d1", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u624b\u673a\u4e0a\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u7684\u5e76\u884c\u5f00\u53d1\u73af\u5883\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u79fb\u52a8\u5f00\u53d1\u89e3\u51b3\u65b9\u6848", "conclusion": "\u79fb\u52a8\u8bbe\u5907\u53ef\u4ee5\u6210\u4e3a\u6709\u6548\u7684\u5f00\u53d1\u5e73\u53f0\uff0c\u901a\u8fc7\u5408\u9002\u7684\u5de5\u5177\u94fe\u7ec4\u5408\uff0c\u80fd\u591f\u5728\u624b\u673a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u4ee3\u7801\u4ee3\u7406\u5e76\u884c\u5f00\u53d1\u5de5\u4f5c\u6d41", "topic": "code agent"}}
{"id": "tldr.2601.36c8737c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ReGecqGdG4Od6j6K_o_273c19CP8wAnkP08q71Sd5jY=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ReGecqGdG4Od6j6K_o_273c19CP8wAnkP08q71Sd5jY=438", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/ReGecqGdG4Od6j6K_o_273c19CP8wAnkP08q71Sd5jY=438", "summary": "Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.", "source": "tldr", "AI": {"tldr": "\u5728\u624b\u673a\u4e0a\u901a\u8fc7Termius+mosh\u8fde\u63a5Vultr\u4e91\u670d\u52a1\u5668\uff0c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u5e76\u884c\u5f00\u53d1\uff0c\u4f7f\u7528tmux\u4fdd\u6301\u4f1a\u8bdd\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5f00\u53d1", "motivation": "\u4e3a\u79fb\u52a8\u5f00\u53d1\u8005\u63d0\u4f9b\u4fbf\u6377\u7684\u5f00\u53d1\u73af\u5883\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u5728\u624b\u673a\u4e0a\u8fd0\u884c\u591a\u4e2aAI\u7f16\u7801\u4ee3\u7406\uff0c\u5b9e\u73b0\u968f\u65f6\u968f\u5730\u7684\u5e76\u884c\u5f00\u53d1\u5de5\u4f5c", "method": "\u4f7f\u7528Termius+mosh\u8fde\u63a5Vultr\u6309\u9700\u4ed8\u8d39\u4e91\u670d\u52a1\u5668\uff0c\u901a\u8fc7Tailscale\u786e\u4fdd\u5b89\u5168\u8fde\u63a5\uff0ctmux\u4fdd\u6301\u4f1a\u8bdd\u6301\u4e45\u5316\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5206\u652f\u5f00\u53d1", "result": "\u6210\u529f\u6784\u5efa\u4e86\u79fb\u52a8\u4f18\u5148\u7684\u5f00\u53d1\u73af\u5883\uff0c\u80fd\u591f\u5728\u624b\u673a\u4e0a\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u8fdb\u884c\u5e76\u884c\u5f00\u53d1\uff0c\u5b9e\u73b0\u968f\u65f6\u968f\u5730\u7684\u7f16\u7801\u5de5\u4f5c", "conclusion": "\u79fb\u52a8\u8bbe\u5907\u53ef\u4ee5\u6210\u4e3a\u6709\u6548\u7684\u5f00\u53d1\u5e73\u53f0\uff0c\u901a\u8fc7\u5408\u9002\u7684\u5de5\u5177\u94fe\u914d\u7f6e\uff0c\u80fd\u591f\u5728\u624b\u673a\u4e0a\u5b9e\u73b0\u591a\u4ee3\u7406\u5e76\u884c\u5f00\u53d1\u73af\u5883", "topic": "code agent"}}
{"id": "tldr.2601.0ccdc47e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/jl6TW2WlGgfP9EFGhQYTOBXJ4pWEpm1Jqvjoifxgv2U=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/jl6TW2WlGgfP9EFGhQYTOBXJ4pWEpm1Jqvjoifxgv2U=438", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b8e219362-965732b1-5433-494c-be61-aef4d97df072-000000/jl6TW2WlGgfP9EFGhQYTOBXJ4pWEpm1Jqvjoifxgv2U=438", "summary": "Claude Code On-The-Go (3 minute read) A mobile-first development setup runs multiple Claude Code agents in parallel from a phone using Termius + mosh into a pay-per-use Vultr VM secured by Tailscale, with tmux for persistence and git worktrees for parallel features.", "source": "tldr", "AI": {"tldr": "\u79fb\u52a8\u4f18\u5148\u5f00\u53d1\u73af\u5883\uff1a\u901a\u8fc7Termius+mosh\u8fde\u63a5\u6309\u9700\u4ed8\u8d39\u7684Vultr\u865a\u62df\u673a\uff0c\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u5e76\u884c\u5f00\u53d1\uff0c\u4f7f\u7528tmux\u4fdd\u6301\u4f1a\u8bdd\uff0cgit worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5f00\u53d1", "motivation": "\u4e3a\u79fb\u52a8\u5f00\u53d1\u8005\u63d0\u4f9b\u4fbf\u6377\u7684\u5f00\u53d1\u73af\u5883\uff0c\u80fd\u591f\u5728\u624b\u673a\u4e0a\u8fd0\u884c\u591a\u4e2a\u4ee3\u7801\u4ee3\u7406\u5e76\u884c\u5de5\u4f5c\uff0c\u89e3\u51b3\u79fb\u52a8\u8bbe\u5907\u5f00\u53d1\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898", "method": "\u4f7f\u7528Termius+mosh\u8fde\u63a5\u5230Vultr\u6309\u9700\u4ed8\u8d39\u865a\u62df\u673a\uff0c\u901a\u8fc7Tailscale\u786e\u4fdd\u5b89\u5168\u8fde\u63a5\uff0c\u5229\u7528tmux\u4fdd\u6301\u4f1a\u8bdd\u6301\u4e45\u6027\uff0c\u4f7f\u7528git worktrees\u652f\u6301\u5e76\u884c\u529f\u80fd\u5f00\u53d1", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u624b\u673a\u4e0a\u8fd0\u884c\u591a\u4e2aClaude Code\u4ee3\u7406\u7684\u79fb\u52a8\u5f00\u53d1\u73af\u5883\uff0c\u652f\u6301\u5e76\u884c\u5f00\u53d1\u548c\u6301\u4e45\u4f1a\u8bdd", "conclusion": "\u79fb\u52a8\u4f18\u5148\u7684\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u65b9\u6848\u53ef\u884c\uff0c\u4e3a\u79fb\u52a8\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4e91\u7aef\u5f00\u53d1\u80fd\u529b", "topic": "code agent"}}
{"id": "tldr.2601.cb09439a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productmarketfit.tech%2Fp%2Fhow-base44-was-bootstrapped-and-sold%3Futm_source=tldrfounders/1/0100019b8e466024-e176364e-796f-40ef-b09e-49f98be6d45a-000000/DZ18-lLodLbY1y4n670k_5dUiz--kE_hpyumZMMsfhk=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productmarketfit.tech%2Fp%2Fhow-base44-was-bootstrapped-and-sold%3Futm_source=tldrfounders/1/0100019b8e466024-e176364e-796f-40ef-b09e-49f98be6d45a-000000/DZ18-lLodLbY1y4n670k_5dUiz--kE_hpyumZMMsfhk=438", "authors": ["TLDR Newsletter"], "title": "How Base44 Sold for $80M in 6 Months", "comment": "Source: TLDR Newsletter, Date: 2026-01-05, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.productmarketfit.tech%2Fp%2Fhow-base44-was-bootstrapped-and-sold%3Futm_source=tldrfounders/1/0100019b8e466024-e176364e-796f-40ef-b09e-49f98be6d45a-000000/DZ18-lLodLbY1y4n670k_5dUiz--kE_hpyumZMMsfhk=438", "summary": "How Base44 Sold for $80M in 6 Months (3 minute read) Zero to exit in 180 days. Maor Shlomo bootstrapped Base44 to an $80M all-cash exit by using AI to generate nearly 100% of the frontend code. This allowed the founder to ship a \"batteries included\" platform (database, auth, and backend pre-wired) where users built apps via text prompts. By focusing entirely on the magic moment - getting a user to a working app in under 60 seconds - the company hit $3.5M ARR with zero ad spend.", "source": "tldr", "AI": {"tldr": "\u521b\u59cb\u4ebaMaor Shlomo\u5728180\u5929\u5185\u5c06Base44\u4ee58000\u4e07\u7f8e\u5143\u73b0\u91d1\u51fa\u552e\uff0c\u901a\u8fc7AI\u751f\u6210\u8fd1100%\u524d\u7aef\u4ee3\u7801\uff0c\u8ba9\u7528\u6237\u901a\u8fc7\u6587\u672c\u63d0\u793a\u6784\u5efa\u5e94\u7528\uff0c\u5b9e\u73b0\u96f6\u5e7f\u544a\u652f\u51fa\u4e0b350\u4e07\u7f8e\u5143\u5e74\u5316\u6536\u5165\u3002", "motivation": "\u5c55\u793a\u5982\u4f55\u901a\u8fc7AI\u6280\u672f\u5feb\u901f\u6784\u5efa\u548c\u89c4\u6a21\u5316SaaS\u5e73\u53f0\uff0c\u5b9e\u73b0\u4ece\u96f6\u5230\u9ad8\u4ef7\u503c\u9000\u51fa\u7684\u521b\u4e1a\u8def\u5f84\uff0c\u8bc1\u660eAI\u5728\u4ee3\u7801\u751f\u6210\u548c\u4ea7\u54c1\u5f00\u53d1\u4e2d\u7684\u5546\u4e1a\u6f5c\u529b\u3002", "method": "\u4f7f\u7528AI\u751f\u6210\u8fd1100%\u524d\u7aef\u4ee3\u7801\uff0c\u6784\u5efa\"\u5f00\u7bb1\u5373\u7528\"\u5e73\u53f0\uff08\u5305\u542b\u6570\u636e\u5e93\u3001\u8ba4\u8bc1\u548c\u9884\u8fde\u63a5\u540e\u7aef\uff09\uff0c\u7528\u6237\u901a\u8fc7\u6587\u672c\u63d0\u793a\u6784\u5efa\u5e94\u7528\uff0c\u4e13\u6ce8\u4e8e\"\u795e\u5947\u65f6\u523b\"\u2014\u2014\u8ba9\u7528\u6237\u572860\u79d2\u5185\u83b7\u5f97\u53ef\u5de5\u4f5c\u7684\u5e94\u7528\u3002", "result": "\u57286\u4e2a\u6708\u5185\u5b9e\u73b08000\u4e07\u7f8e\u5143\u5168\u73b0\u91d1\u9000\u51fa\uff0c\u96f6\u5e7f\u544a\u652f\u51fa\u4e0b\u8fbe\u5230350\u4e07\u7f8e\u5143\u5e74\u5316\u6536\u5165\uff0c\u7528\u6237\u80fd\u572860\u79d2\u5185\u6784\u5efa\u51fa\u53ef\u5de5\u4f5c\u7684\u5e94\u7528\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u53ef\u4ee5\u6781\u5927\u52a0\u901f\u4ea7\u54c1\u5f00\u53d1\u548c\u5546\u4e1a\u5316\u8fdb\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u6838\u5fc3\u7528\u6237\u4f53\u9a8c\u7684\"\u795e\u5947\u65f6\u523b\"\u80fd\u5e26\u6765\u663e\u8457\u7684\u5546\u4e1a\u6210\u529f\u548c\u9000\u51fa\u4ef7\u503c\u3002", "topic": "code agent"}}
