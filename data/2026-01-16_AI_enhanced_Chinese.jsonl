{"id": "2601.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09714", "abs": "https://arxiv.org/abs/2601.09714", "authors": ["Devesh Saraogi", "Rohit Singhee", "Dhruv Kumar"], "title": "Evaluating Novelty in AI-Generated Research Plans Using Multi-Workflow LLM Pipelines", "comment": "Under Review", "summary": "The integration of Large Language Models (LLMs) into the scientific ecosystem raises fundamental questions about the creativity and originality of AI-generated research. Recent work has identified ``smart plagiarism'' as a concern in single-step prompting approaches, where models reproduce existing ideas with terminological shifts. This paper investigates whether agentic workflows -- multi-step systems employing iterative reasoning, evolutionary search, and recursive decomposition -- can generate more novel and feasible research plans. We benchmark five reasoning architectures: Reflection-based iterative refinement, Sakana AI v2 evolutionary algorithms, Google Co-Scientist multi-agent framework, GPT Deep Research (GPT-5.1) recursive decomposition, and Gemini~3 Pro multimodal long-context pipeline. Using evaluations from thirty proposals each on novelty, feasibility, and impact, we find that decomposition-based and long-context workflows achieve mean novelty of 4.17/5, while reflection-based approaches score significantly lower (2.33/5). Results reveal varied performance across research domains, with high-performing workflows maintaining feasibility without sacrificing creativity. These findings support the view that carefully designed multi-stage agentic workflows can advance AI-assisted research ideation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cdAI\u4ee3\u7406\u5de5\u4f5c\u6d41\u5728\u751f\u6210\u65b0\u9896\u53ef\u884c\u7814\u7a76\u8ba1\u5212\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u57fa\u4e8e\u5206\u89e3\u548c\u957f\u4e0a\u4e0b\u6587\u7684\u6d41\u7a0b\u5728\u521b\u65b0\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u57fa\u4e8e\u53cd\u601d\u7684\u65b9\u6cd5\u5f97\u5206\u8f83\u4f4e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u878d\u5165\u79d1\u7814\u751f\u6001\u7cfb\u7edf\uff0c\u4eba\u4eec\u5f00\u59cb\u8d28\u7591AI\u751f\u6210\u7814\u7a76\u7684\u521b\u9020\u6027\u548c\u539f\u521b\u6027\u3002\u73b0\u6709\u7814\u7a76\u6307\u51fa\"\u667a\u80fd\u6284\u88ad\"\u95ee\u9898\uff0c\u5373\u6a21\u578b\u901a\u8fc7\u672f\u8bed\u8f6c\u6362\u590d\u5236\u73b0\u6709\u60f3\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u591a\u6b65\u9aa4\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u662f\u5426\u80fd\u4ea7\u751f\u66f4\u521b\u65b0\u548c\u53ef\u884c\u7684\u7814\u7a76\u8ba1\u5212\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cd\u63a8\u7406\u67b6\u6784\uff1a\u57fa\u4e8e\u53cd\u601d\u7684\u8fed\u4ee3\u4f18\u5316\u3001Sakana AI v2\u8fdb\u5316\u7b97\u6cd5\u3001Google Co-Scientist\u591a\u4ee3\u7406\u6846\u67b6\u3001GPT Deep Research\u9012\u5f52\u5206\u89e3\u3001\u4ee5\u53caGemini~3 Pro\u591a\u6a21\u6001\u957f\u4e0a\u4e0b\u6587\u6d41\u7a0b\u3002\u901a\u8fc7\u8bc4\u4f3030\u4e2a\u63d0\u6848\u7684\u65b0\u9896\u6027\u3001\u53ef\u884c\u6027\u548c\u5f71\u54cd\u529b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u4e8e\u5206\u89e3\u548c\u957f\u4e0a\u4e0b\u6587\u7684\u5de5\u4f5c\u6d41\u5e73\u5747\u65b0\u9896\u6027\u5f97\u52064.17/5\uff0c\u800c\u57fa\u4e8e\u53cd\u601d\u7684\u65b9\u6cd5\u5f97\u5206\u663e\u8457\u8f83\u4f4e(2.33/5)\u3002\u4e0d\u540c\u7814\u7a76\u9886\u57df\u8868\u73b0\u5404\u5f02\uff0c\u9ad8\u6027\u80fd\u5de5\u4f5c\u6d41\u80fd\u5728\u4fdd\u6301\u53ef\u884c\u6027\u7684\u540c\u65f6\u4e0d\u727a\u7272\u521b\u9020\u529b\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u591a\u9636\u6bb5\u4ee3\u7406\u5de5\u4f5c\u6d41\u80fd\u591f\u63a8\u8fdbAI\u8f85\u52a9\u7814\u7a76\u6784\u601d\uff0c\u8868\u660e\u4ee3\u7406\u5de5\u4f5c\u6d41\u53ef\u4ee5\u751f\u6210\u65e2\u65b0\u9896\u53c8\u53ef\u884c\u7684\u7814\u7a76\u8ba1\u5212\uff0c\u89e3\u51b3\u4e86\u5355\u6b65\u63d0\u793a\u65b9\u6cd5\u4e2d\u7684\"\u667a\u80fd\u6284\u88ad\"\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.09770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09770", "abs": "https://arxiv.org/abs/2601.09770", "authors": ["Chen Chen", "Jiawei Shao", "Dakuan Lu", "Haoyi Hu", "Xiangcheng Liu", "Hantao Yao", "Wu Liu"], "title": "GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents", "comment": null, "summary": "Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.", "AI": {"tldr": "GUI-Eyes\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eGUI\u4efb\u52a1\u4e2d\u7684\u4e3b\u52a8\u89c6\u89c9\u611f\u77e5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406\u5b66\u4e60\u4f55\u65f6\u3001\u5982\u4f55\u8c03\u7528\u89c6\u89c9\u5de5\u5177\uff0c\u5728ScreenSpot-Pro\u57fa\u51c6\u4e0a\u4ec5\u75283k\u6837\u672c\u8fbe\u523044.8%\u7684\u5b9a\u4f4d\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709GUI\u81ea\u52a8\u5316\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u3001\u4e00\u6b21\u6027\u89c6\u89c9\u8f93\u5165\u548c\u88ab\u52a8\u611f\u77e5\uff0c\u7f3a\u4e4f\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u3001\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u89c2\u5bdf\u754c\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u611f\u77e5\u7b56\u7565\uff0c\u5c06\u51b3\u7b56\u5206\u89e3\u4e3a\u7c97\u7c92\u5ea6\u63a2\u7d22\u548c\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\uff0c\u7531\u4e24\u7ea7\u7b56\u7565\u534f\u8c03\uff1b\u8bbe\u8ba1\u7a7a\u95f4\u8fde\u7eed\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u4f4d\u7f6e\u63a5\u8fd1\u5ea6\u548c\u533a\u57df\u91cd\u53e0\u5ea6\uff0c\u4e3a\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\u3002", "result": "\u5728ScreenSpot-Pro\u57fa\u51c6\u4e0a\uff0cGUI-Eyes-3B\u4ec5\u4f7f\u75283k\u6807\u8bb0\u6837\u672c\u5c31\u5b9e\u73b0\u4e8644.8%\u7684\u5b9a\u4f4d\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u548c\u57fa\u4e8eRL\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5de5\u5177\u611f\u77e5\u7684\u4e3b\u52a8\u611f\u77e5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u7b56\u7565\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u53cd\u9988\uff0c\u5bf9\u4e8e\u6784\u5efa\u9c81\u68d2\u4e14\u6570\u636e\u9ad8\u6548\u7684GUI\u4ee3\u7406\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.09745", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09745", "abs": "https://arxiv.org/abs/2601.09745", "authors": ["Antonio Abu Nassar", "Eitan Farchi"], "title": "Enhancing Formal Software Specification with Artificial Intelligence", "comment": null, "summary": "Formal software specification is known to enable early error detection and explicit invariants, yet it has seen limited industrial adoption due to its high notation overhead and the expertise required to use traditional formal languages. This paper presents a case study showing that recent advances in artificial intelligence make it possible to retain many of the benefits of formal specification while substantially reducing these costs. The necessity of a clear distinction between what is controlled by the system analyst and can highly benefits from the rigor of formal specification and what need not be controlled is demonstrated. We use natural language augmented with lightweight mathematical notation and written in \\LaTeX\\ as an intermediate specification language, which is reviewed and refined by AI prior to code generation. Applied to a nontrivial simulation of organizational knowledge growth, this approach enables early validation, explicit invariants, and correctness by design, while significantly reducing development effort and producing a correct implementation on the first attempt.", "AI": {"tldr": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u589e\u5f3a\u8f7b\u91cf\u7ea7\u6570\u5b66\u7b26\u53f7\u4f5c\u4e3a\u4e2d\u95f4\u89c4\u8303\u8bed\u8a00\uff0c\u901a\u8fc7AI\u5ba1\u67e5\u548c\u7cbe\u70bc\u540e\u751f\u6210\u4ee3\u7801\uff0c\u5728\u7ec4\u7ec7\u77e5\u8bc6\u589e\u957f\u6a21\u62df\u6848\u4f8b\u4e2d\u663e\u8457\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u5e76\u5b9e\u73b0\u9996\u6b21\u5c1d\u8bd5\u5373\u6b63\u786e\u7684\u5b9e\u73b0", "motivation": "\u4f20\u7edf\u5f62\u5f0f\u5316\u89c4\u8303\u867d\u7136\u80fd\u5b9e\u73b0\u65e9\u671f\u9519\u8bef\u68c0\u6d4b\u548c\u663e\u5f0f\u4e0d\u53d8\u91cf\uff0c\u4f46\u7531\u4e8e\u7b26\u53f7\u5f00\u9500\u5927\u3001\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u800c\u5728\u5de5\u4e1a\u754c\u91c7\u7528\u6709\u9650\u3002\u9700\u8981\u627e\u5230\u65e2\u80fd\u4fdd\u7559\u5f62\u5f0f\u5316\u89c4\u8303\u4f18\u70b9\u53c8\u80fd\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u589e\u5f3a\u8f7b\u91cf\u7ea7\u6570\u5b66\u7b26\u53f7\uff08\u7528LaTeX\u7f16\u5199\uff09\u4f5c\u4e3a\u4e2d\u95f4\u89c4\u8303\u8bed\u8a00\uff0c\u901a\u8fc7AI\u8fdb\u884c\u5ba1\u67e5\u548c\u7cbe\u70bc\uff0c\u7136\u540e\u751f\u6210\u4ee3\u7801\u3002\u533a\u5206\u7cfb\u7edf\u5206\u6790\u5e08\u9700\u8981\u63a7\u5236\u7684\u90e8\u5206\uff08\u53d7\u76ca\u4e8e\u5f62\u5f0f\u5316\u89c4\u8303\uff09\u548c\u4e0d\u9700\u8981\u63a7\u5236\u7684\u90e8\u5206\u3002", "result": "\u5e94\u7528\u4e8e\u975e\u5e73\u51e1\u7684\u7ec4\u7ec7\u77e5\u8bc6\u589e\u957f\u6a21\u62df\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e9\u671f\u9a8c\u8bc1\u3001\u663e\u5f0f\u4e0d\u53d8\u91cf\u548c\u8bbe\u8ba1\u5373\u6b63\u786e\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u5de5\u4f5c\u91cf\uff0c\u5e76\u5728\u9996\u6b21\u5c1d\u8bd5\u4e2d\u4ea7\u751f\u6b63\u786e\u7684\u5b9e\u73b0\u3002", "conclusion": "AI\u7684\u8fdb\u6b65\u4f7f\u5f97\u5728\u4fdd\u7559\u5f62\u5f0f\u5316\u89c4\u8303\u8bb8\u591a\u4f18\u70b9\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u6210\u4e3a\u53ef\u80fd\uff0c\u81ea\u7136\u8bed\u8a00\u589e\u5f3a\u8f7b\u91cf\u7ea7\u6570\u5b66\u7b26\u53f7\u4f5c\u4e3a\u4e2d\u95f4\u89c4\u8303\u8bed\u8a00\u662f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2601.09771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09771", "abs": "https://arxiv.org/abs/2601.09771", "authors": ["Aradhya Dixit", "Shreem Dixit"], "title": "PCN-Rec: Agentic Proof-Carrying Negotiation for Reliable Governance-Constrained Recommendation", "comment": null, "summary": "Modern LLM-based recommenders can generate compelling ranked lists, but they struggle to reliably satisfy governance constraints such as minimum long-tail exposure or diversity requirements. We present PCN-Rec, a proof-carrying negotiation pipeline that separates natural-language reasoning from deterministic enforcement. A base recommender (MF/CF) produces a candidate window of size W, which is negotiated by two agents: a User Advocate optimizing relevance and a Policy Agent enforcing constraints. A mediator LLM synthesizes a top-N slate together with a structured certificate (JSON) describing the claimed constraint satisfaction. A deterministic verifier recomputes all constraints from the slate and accepts only verifier-checked certificates; if verification fails, a deterministic constrained-greedy repair produces a compliant slate for re-verification, yielding an auditable trace. On MovieLens-100K with governance constraints, PCN-Rec achieves a 98.55% pass rate on feasible users (n = 551, W = 80) versus a one-shot single-LLM baseline without verification/repair, while preserving utility with only a 0.021 absolute drop in NDCG@10 (0.403 vs. 0.424); differences are statistically significant (p < 0.05).", "AI": {"tldr": "PCN-Rec\u662f\u4e00\u4e2a\u8bc1\u660e\u643a\u5e26\u7684\u534f\u5546\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u79bb\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u786e\u5b9a\u6027\u7ea6\u675f\u6267\u884c\u6765\u89e3\u51b3LLM\u63a8\u8350\u7cfb\u7edf\u96be\u4ee5\u53ef\u9760\u6ee1\u8db3\u6cbb\u7406\u7ea6\u675f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u80fd\u751f\u6210\u6709\u5438\u5f15\u529b\u7684\u6392\u540d\u5217\u8868\uff0c\u4f46\u96be\u4ee5\u53ef\u9760\u5730\u6ee1\u8db3\u6cbb\u7406\u7ea6\u675f\uff08\u5982\u6700\u5c0f\u957f\u5c3e\u66dd\u5149\u6216\u591a\u6837\u6027\u8981\u6c42\uff09\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u8bc1\u76f8\u5173\u6027\u548c\u7ea6\u675f\u6ee1\u8db3\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8bc1\u660e\u643a\u5e26\u7684\u534f\u5546\u7ba1\u9053\uff1a\u57fa\u7840\u63a8\u8350\u5668\u751f\u6210\u5019\u9009\u7a97\u53e3\uff0c\u4e24\u4e2a\u4ee3\u7406\uff08\u7528\u6237\u5021\u5bfc\u8005\u548c\u7b56\u7565\u4ee3\u7406\uff09\u8fdb\u884c\u534f\u5546\uff0c\u8c03\u89e3LLM\u5408\u6210top-N\u5217\u8868\u548c\u7ed3\u6784\u5316\u8bc1\u4e66\uff0c\u786e\u5b9a\u6027\u9a8c\u8bc1\u5668\u68c0\u67e5\u7ea6\u675f\u6ee1\u8db3\uff0c\u5931\u8d25\u65f6\u4f7f\u7528\u786e\u5b9a\u6027\u7ea6\u675f\u8d2a\u5a6a\u4fee\u590d\u751f\u6210\u5408\u89c4\u5217\u8868\u3002", "result": "\u5728MovieLens-100K\u6570\u636e\u96c6\u4e0a\uff0cPCN-Rec\u5728\u53ef\u884c\u7528\u6237\u4e2d\u8fbe\u523098.55%\u7684\u901a\u8fc7\u7387\uff0c\u76f8\u6bd4\u6ca1\u6709\u9a8c\u8bc1/\u4fee\u590d\u7684\u5355LLM\u57fa\u7ebf\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6NDCG@10\u4ec5\u4e0b\u964d0.021\uff080.403 vs 0.424\uff09\uff0c\u5dee\u5f02\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "PCN-Rec\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u548c\u7ea6\u675f\u6267\u884c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7ea6\u675f\u6ee1\u8db3\u7387\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u8d28\u91cf\uff0c\u4e3a\u53ef\u5ba1\u8ba1\u7684\u7ea6\u675f\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.09750", "categories": ["cs.SE", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.09750", "abs": "https://arxiv.org/abs/2601.09750", "authors": ["Robert K. Strehlow", "Tobias K\u00fcster", "Oskar F. Kupke", "Brandon Llanque Kurps", "Fikret Sivrikaya", "Sahin Albayrak"], "title": "SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments", "comment": null, "summary": "Large language models (LLMs) have proven to work well in question-answering scenarios, but real-world applications often require access to tools for live information or actuation. For this, LLMs can be extended with tools, which are often defined in advance, also allowing for some fine-tuning for specific use cases. However, rapidly evolving software landscapes and individual services require the constant development and integration of new tools. Domain- or company-specific tools can greatly elevate the usefulness of an LLM, but such custom tools can be problematic to integrate, or the LLM may fail to reliably understand and use them. For this, we need strategies to define new tools and integrate them into the LLM dynamically, as well as robust and scalable zero-shot prompting methods that can make use of those tools in an efficient manner. In this paper, we present SAGE, a specialized conversational AI interface, based on the OPACA framework for tool discovery and execution. The integration with OPACA makes it easy to add new tools or services for the LLM to use, while SAGE itself presents rich extensibility and modularity. This not only provides the ability to seamlessly switch between different models (e.g. GPT, LLAMA), but also to add and select prompting methods, involving various setups of differently prompted agents for selecting and executing tools and evaluating the results. We implemented a number of task-solving strategies, making use of agentic concepts and prompting methods in various degrees of complexity, and evaluated those against a comprehensive set of benchmark services. The results are promising and highlight the distinct strengths and weaknesses of different task-solving strategies. Both SAGE and the OPACA framework, as well as the different benchmark services and results, are available as Open Source/Open Data on GitHub.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u57fa\u4e8eOPACA\u6846\u67b6\u7684\u5bf9\u8bddAI\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u5de5\u5177\u53d1\u73b0\u4e0e\u6267\u884c\uff0c\u652f\u6301\u52a8\u6001\u96c6\u6210\u65b0\u5de5\u5177\u548c\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2dLLM\u9700\u8981\u8bbf\u95ee\u5b9e\u65f6\u4fe1\u606f\u548c\u6267\u884c\u5de5\u5177\uff0c\u4f46\u8f6f\u4ef6\u73af\u5883\u5feb\u901f\u53d8\u5316\uff0c\u9700\u8981\u52a8\u6001\u96c6\u6210\u65b0\u5de5\u5177\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u53ef\u9760\u7406\u89e3\u548c\u4f7f\u7528\u81ea\u5b9a\u4e49\u5de5\u5177\uff0c\u9700\u8981\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u548c\u52a8\u6001\u5de5\u5177\u96c6\u6210\u7b56\u7565\u3002", "method": "\u57fa\u4e8eOPACA\u6846\u67b6\u6784\u5efaSAGE\u5bf9\u8bddAI\u63a5\u53e3\uff0c\u652f\u6301\u52a8\u6001\u6dfb\u52a0\u5de5\u5177\u548c\u670d\u52a1\u3002\u7cfb\u7edf\u5177\u6709\u6a21\u5757\u5316\u67b6\u6784\uff0c\u53ef\u5207\u6362\u4e0d\u540c\u6a21\u578b\uff08GPT\u3001LLAMA\u7b49\uff09\uff0c\u96c6\u6210\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\u548c\u4ee3\u7406\u7b56\u7565\uff0c\u5b9e\u73b0\u5de5\u5177\u9009\u62e9\u3001\u6267\u884c\u548c\u7ed3\u679c\u8bc4\u4f30\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u79cd\u4efb\u52a1\u89e3\u51b3\u7b56\u7565\uff0c\u4f7f\u7528\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4ee3\u7406\u6982\u5ff5\u548c\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u7efc\u5408\u57fa\u51c6\u670d\u52a1\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\u4e86\u4e0d\u540c\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\uff0c\u6574\u4f53\u8868\u73b0\u826f\u597d\u3002", "conclusion": "SAGE\u548cOPACA\u6846\u67b6\u63d0\u4f9b\u4e86\u52a8\u6001\u5de5\u5177\u96c6\u6210\u548c\u591a\u6837\u5316\u63d0\u793a\u7b56\u7565\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u652f\u6301\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316AI\u7cfb\u7edf\u5f00\u53d1\u3002", "topic": "code agent"}}
{"id": "2601.09721", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09721", "abs": "https://arxiv.org/abs/2601.09721", "authors": ["Vahideh Zolfaghari"], "title": "Cross-Platform Evaluation of Large Language Model Safety in Pediatric Consultations: Evolution of Adversarial Robustness and the Scale Paradox", "comment": null, "summary": "Background Large language models (LLMs) are increasingly deployed in medical consultations, yet their safety under realistic user pressures remains understudied. Prior assessments focused on neutral conditions, overlooking vulnerabilities from anxious users challenging safeguards. This study evaluated LLM safety under parental anxiety-driven adversarial pressures in pediatric consultations across models and platforms. Methods PediatricAnxietyBench, from a prior evaluation, includes 300 queries (150 authentic, 150 adversarial) spanning 10 topics. Three models were assessed via APIs: Llama-3.3-70B and Llama-3.1-8B (Groq), Mistral-7B (HuggingFace), yielding 900 responses. Safety used a 0-15 scale for restraint, referral, hedging, emergency recognition, and non-prescriptive behavior. Analyses employed paired t-tests with bootstrapped CIs. Results Mean scores: 9.70 (Llama-3.3-70B) to 10.39 (Mistral-7B). Llama-3.1-8B outperformed Llama-3.3-70B by +0.66 (p=0.0001, d=0.225). Models showed positive adversarial effects, Mistral-7B strongest (+1.09, p=0.0002). Safety generalized across platforms; Llama-3.3-70B had 8% failures. Seizures vulnerable (33% inappropriate diagnoses). Hedging predicted safety (r=0.68, p<0.001). Conclusions Evaluation shows safety depends on alignment and architecture over scale, with smaller models outperforming larger. Evolution to robustness across releases suggests targeted training progress. Vulnerabilities and no emergency recognition indicate unsuitability for triage. Findings guide selection, stress adversarial testing, and provide open benchmark for medical AI safety.", "AI": {"tldr": "\u8bc4\u4f30LLMs\u5728\u513f\u79d1\u54a8\u8be2\u4e2d\u9762\u5bf9\u7126\u8651\u5bb6\u957f\u538b\u529b\u65f6\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5c0f\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u6a21\u578b\uff0c\u5b89\u5168\u6027\u4e0e\u5bf9\u9f50\u548c\u67b6\u6784\u76f8\u5173\u800c\u975e\u89c4\u6a21\uff0c\u5b58\u5728\u7279\u5b9a\u6f0f\u6d1e\u5982\u766b\u75eb\u8bca\u65ad\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u533b\u7597\u54a8\u8be2\u4e2d\u5e94\u7528\u589e\u591a\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u4e2d\u6027\u6761\u4ef6\uff0c\u5ffd\u7565\u4e86\u7126\u8651\u7528\u6237\u6311\u6218\u5b89\u5168\u673a\u5236\u65f6\u7684\u8106\u5f31\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u513f\u79d1\u54a8\u8be2\u4e2d\u9762\u5bf9\u5bb6\u957f\u7126\u8651\u9a71\u52a8\u5bf9\u6297\u538b\u529b\u65f6\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528PediatricAnxietyBench\uff08300\u4e2a\u67e5\u8be2\uff1a150\u4e2a\u771f\u5b9e\u3001150\u4e2a\u5bf9\u6297\u6027\uff09\uff0c\u6db5\u76d610\u4e2a\u4e3b\u9898\u3002\u901a\u8fc7API\u8bc4\u4f30\u4e09\u4e2a\u6a21\u578b\uff1aLlama-3.3-70B\u548cLlama-3.1-8B\uff08Groq\uff09\u3001Mistral-7B\uff08HuggingFace\uff09\u3002\u91c7\u75280-15\u5206\u5b89\u5168\u8bc4\u5206\u6807\u51c6\uff0c\u8bc4\u4f30\u7ea6\u675f\u3001\u8f6c\u8bca\u3001\u8c28\u614e\u63aa\u8f9e\u3001\u7d27\u6025\u8bc6\u522b\u548c\u975e\u5904\u65b9\u884c\u4e3a\u3002\u4f7f\u7528\u914d\u5bf9t\u68c0\u9a8c\u548c\u81ea\u52a9\u7f6e\u4fe1\u533a\u95f4\u5206\u6790\u3002", "result": "\u5e73\u5747\u5b89\u5168\u5206\uff1a9.70\uff08Llama-3.3-70B\uff09\u523010.39\uff08Mistral-7B\uff09\u3002Llama-3.1-8B\u4f18\u4e8eLlama-3.3-70B\uff08+0.66\u5206\uff09\u3002\u6240\u6709\u6a21\u578b\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u597d\uff0cMistral-7B\u6700\u5f3a\uff08+1.09\u5206\uff09\u3002\u5b89\u5168\u8868\u73b0\u8de8\u5e73\u53f0\u4e00\u81f4\uff0cLlama-3.3-70B\u67098%\u5931\u8d25\u7387\u3002\u766b\u75eb\u8bca\u65ad\u6700\u8106\u5f31\uff0833%\u4e0d\u9002\u5f53\u8bca\u65ad\uff09\u3002\u8c28\u614e\u63aa\u8f9e\u4e0e\u5b89\u5168\u6027\u9ad8\u5ea6\u76f8\u5173\uff08r=0.68\uff09\u3002", "conclusion": "LLMs\u5b89\u5168\u6027\u66f4\u591a\u53d6\u51b3\u4e8e\u5bf9\u9f50\u548c\u67b6\u6784\u800c\u975e\u6a21\u578b\u89c4\u6a21\uff0c\u5c0f\u6a21\u578b\u53ef\u8d85\u8d8a\u5927\u6a21\u578b\u3002\u7248\u672c\u8fed\u4ee3\u663e\u793a\u9488\u5bf9\u6027\u8bad\u7ec3\u8fdb\u6b65\u3002\u5b58\u5728\u7279\u5b9a\u6f0f\u6d1e\u4e14\u7f3a\u4e4f\u7d27\u6025\u8bc6\u522b\u80fd\u529b\uff0c\u4e0d\u9002\u5408\u5206\u8bca\u3002\u7814\u7a76\u4e3a\u533b\u7597AI\u5b89\u5168\u63d0\u4f9b\u9009\u62e9\u6307\u5bfc\u3001\u5f3a\u8c03\u5bf9\u6297\u6d4b\u8bd5\u91cd\u8981\u6027\uff0c\u5e76\u8d21\u732e\u5f00\u653e\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2601.09869", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.09869", "abs": "https://arxiv.org/abs/2601.09869", "authors": ["Andrea Ferrario", "Rasita Vinay", "Matteo Casserini", "Alessandro Facchini"], "title": "A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents", "comment": "Submitted to FAccT 2026", "summary": "Anthropomorphisation -- the phenomenon whereby non-human entities are ascribed human-like qualities -- has become increasingly salient with the rise of large language model (LLM)-based conversational agents (CAs). Unlike earlier chatbots, LLM-based CAs routinely generate interactional and linguistic cues, such as first-person self-reference, epistemic and affective expressions that empirical work shows can increase engagement. On the other hand, anthropomorphisation raises ethical concerns, including deception, overreliance, and exploitative relationship framing, while some authors argue that anthropomorphic interaction may support autonomy, well-being, and inclusion. Despite increasing interest in the phenomenon, literature remains fragmented across domains and varies substantially in how it defines, operationalizes, and normatively evaluates anthropomorphisation. This scoping review maps ethically oriented work on anthropomorphising LLM-based CAs across five databases and three preprint repositories. We synthesize (1) conceptual foundations, (2) ethical challenges and opportunities, and (3) methodological approaches. We find convergence on attribution-based definitions but substantial divergence in operationalization, a predominantly risk-forward normative framing, and limited empirical work that links observed interaction effects to actionable governance guidance. We conclude with a research agenda and design/governance recommendations for ethically deploying anthropomorphic cues in LLM-based conversational agents.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u4ee3\u7406\u62df\u4eba\u5316\u73b0\u8c61\u7684\u4f26\u7406\u5bfc\u5411\u8303\u56f4\u7efc\u8ff0\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u5b9a\u4e49\u3001\u4f26\u7406\u6311\u6218\u4e0e\u673a\u9047\u3001\u65b9\u6cd5\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u7814\u7a76\u8bae\u7a0b\u548c\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u4ee3\u7406\u65e5\u76ca\u666e\u53ca\uff0c\u5176\u62df\u4eba\u5316\u73b0\u8c61\uff08\u8d4b\u4e88\u975e\u4eba\u7c7b\u5b9e\u4f53\u7c7b\u4eba\u7279\u8d28\uff09\u5f15\u53d1\u4e86\u91cd\u8981\u4f26\u7406\u5173\u6ce8\u3002\u73b0\u6709\u6587\u732e\u5206\u6563\u5728\u4e0d\u540c\u9886\u57df\uff0c\u5b9a\u4e49\u3001\u64cd\u4f5c\u5316\u548c\u4f26\u7406\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u4ee5\u6307\u5bfc\u4f26\u7406\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u8303\u56f4\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68c0\u7d22\u4e86\u4e94\u4e2a\u6570\u636e\u5e93\u548c\u4e09\u4e2a\u9884\u5370\u672c\u5e93\u4e2d\u7684\u76f8\u5173\u6587\u732e\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7efc\u5408\uff1a(1)\u6982\u5ff5\u57fa\u7840\uff0c(2)\u4f26\u7406\u6311\u6218\u4e0e\u673a\u9047\uff0c(3)\u65b9\u6cd5\u8bba\u9014\u5f84\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5728\u5b9a\u4e49\u4e0a\u5b58\u5728\u57fa\u4e8e\u5f52\u56e0\u7684\u5171\u8bc6\uff0c\u4f46\u5728\u64cd\u4f5c\u5316\u4e0a\u5dee\u5f02\u663e\u8457\uff1b\u4f26\u7406\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u98ce\u9669\u800c\u975e\u673a\u9047\uff1b\u5b9e\u8bc1\u7814\u7a76\u6709\u9650\uff0c\u96be\u4ee5\u5c06\u4ea4\u4e92\u6548\u5e94\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u6cbb\u7406\u6307\u5bfc\u3002", "conclusion": "\u63d0\u51fa\u7814\u7a76\u8bae\u7a0b\u548c\u8bbe\u8ba1/\u6cbb\u7406\u5efa\u8bae\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5e73\u8861\u7684\u4f26\u7406\u6846\u67b6\u3001\u66f4\u597d\u7684\u64cd\u4f5c\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u8fde\u63a5\u5b9e\u8bc1\u53d1\u73b0\u4e0e\u6cbb\u7406\u5b9e\u8df5\u7684\u6865\u6881\uff0c\u4ee5\u5b9e\u73b0LLM\u5bf9\u8bdd\u4ee3\u7406\u62df\u4eba\u5316\u7ebf\u7d22\u7684\u4f26\u7406\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2601.09723", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09723", "abs": "https://arxiv.org/abs/2601.09723", "authors": ["Guancheng Du", "Yong Hu", "Wenqing Wang", "Yaming Yang", "Jiaheng Gao"], "title": "SagaScale: A Realistic, Scalable, and High-Quality Long-Context Benchmark Built from Full-Length Novels", "comment": null, "summary": "Large Language Models (LLMs) have shown significant progress, but understanding long and complex documents remains challenging. Many long-context benchmarks have been proposed, but they face several limitations, including task realism, data scalability, and data quality. To this end, we introduce SagaScale, a realistic, scalable, and high-quality long-context benchmark built from full-length novels. The entire benchmark is constructed using an automated data collection pipeline that utilizes external resources (e.g., Wikipedia pages) to curate question-answer pairs. Critically, these external resources are provided only for benchmark construction and not during evaluation, which allows LLMs to curate complex questions that go beyond what they can answer during evaluation. SagaScale is also bilingual and offers the largest context length to date, with average token counts exceeding 250K for English novels and 320K for Chinese novels. Our evaluation across 12 frontier LLMs and three long-context methods -- Na\u00efve RAG, Agentic RAG, and Long Context -- yields key insights, including: (1) Directly supplying the full context to the LLM can outperform other methods by a large margin; (2) Most LLMs still struggle with lengthy contexts, but Gemini-2.5-Pro stands out as an exception; and (3) Agentic RAG effectively addresses the retrieval bottleneck in Na\u00efve RAG. Finally, we publicly release the SagaScale benchmark and our data collection codebase to facilitate future research.", "AI": {"tldr": "SagaScale\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b8c\u6574\u5c0f\u8bf4\u7684\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5177\u6709\u771f\u5b9e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u8d28\u91cf\u7279\u70b9\uff0c\u652f\u6301\u4e2d\u82f1\u53cc\u8bed\uff0c\u5e73\u5747\u4e0a\u4e0b\u6587\u957f\u5ea6\u8d85\u8fc725\u4e07/32\u4e07token\uff0c\u8bc4\u4f30\u4e8612\u4e2a\u524d\u6cbfLLM\u548c\u4e09\u79cd\u957f\u4e0a\u4e0b\u6587\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4efb\u52a1\u771f\u5b9e\u6027\u3001\u6570\u636e\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u8d28\u91cf\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u6784\u5efa\u66f4\u771f\u5b9e\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u957f\u6587\u6863\u7406\u89e3\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u6d41\u6c34\u7ebf\uff0c\u57fa\u4e8e\u5b8c\u6574\u5c0f\u8bf4\u6784\u5efa\u95ee\u7b54\u5bf9\uff0c\u5229\u7528\u5916\u90e8\u8d44\u6e90\uff08\u5982\u7ef4\u57fa\u767e\u79d1\uff09\u751f\u6210\u590d\u6742\u95ee\u9898\uff0c\u4f46\u8bc4\u4f30\u65f6\u4e0d\u63d0\u4f9b\u8fd9\u4e9b\u8d44\u6e90\u3002\u652f\u6301\u4e2d\u82f1\u53cc\u8bed\uff0c\u5e73\u5747\u4e0a\u4e0b\u6587\u957f\u5ea6\u8fbe25\u4e07/32\u4e07token\u3002", "result": "\u8bc4\u4f3012\u4e2a\u524d\u6cbfLLM\u548c\u4e09\u79cd\u65b9\u6cd5\uff08\u6734\u7d20RAG\u3001\u667a\u80fd\u4f53RAG\u3001\u957f\u4e0a\u4e0b\u6587\uff09\u53d1\u73b0\uff1a1\uff09\u76f4\u63a5\u63d0\u4f9b\u5b8c\u6574\u4e0a\u4e0b\u6587\u6548\u679c\u6700\u597d\uff1b2\uff09\u591a\u6570LLM\u4ecd\u96be\u4ee5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u4f46Gemini-2.5-Pro\u8868\u73b0\u7a81\u51fa\uff1b3\uff09\u667a\u80fd\u4f53RAG\u80fd\u6709\u6548\u89e3\u51b3\u6734\u7d20RAG\u7684\u68c0\u7d22\u74f6\u9888\u3002", "conclusion": "SagaScale\u4e3a\u957f\u6587\u6863\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u771f\u5b9e\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\u548c\u6539\u8fdb\u65b9\u5411\uff0c\u5e76\u5f00\u6e90\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801\u5e93\u3002", "topic": "agent analysis"}}
{"id": "2601.09822", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09822", "abs": "https://arxiv.org/abs/2601.09822", "authors": ["Yongjian Tang", "Thomas Runkler"], "title": "LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities", "comment": "Accepted to GenSE 2026 workshop", "summary": "Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5168\u751f\u547d\u5468\u671f\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u9700\u6c42\u5de5\u7a0b\u3001\u4ee3\u7801\u751f\u6210\u3001\u9759\u6001\u68c0\u67e5\u3001\u6d4b\u8bd5\u548c\u8c03\u8bd5\u7b49\uff0c\u63a2\u8ba8\u4e86\u6a21\u578b\u9009\u62e9\u3001\u8bc4\u4f30\u57fa\u51c6\u3001\u6846\u67b6\u534f\u8bae\u7b49\u5173\u952e\u8bdd\u9898\uff0c\u5e76\u6307\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u7f16\u6392\u3001\u4eba\u673a\u534f\u4f5c\u3001\u8ba1\u7b97\u6210\u672c\u4f18\u5316\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u590d\u6742\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u66f4\u534f\u4f5c\u548c\u4e13\u95e8\u5316\u7684\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u56de\u987e\u65b0\u5174\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8303\u5f0f\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u8be5\u9886\u57df\u7684\u524d\u6cbf\u6d1e\u5bdf\u3002", "method": "\u91c7\u7528\u6982\u5ff5\u6027\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5168\u9762\u8003\u5bdfLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u5404\u9636\u6bb5\u7684\u5e94\u7528\uff0c\u5206\u6790\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u3001\u8bc4\u4f30\u57fa\u51c6\u3001\u667a\u80fd\u4f53\u6846\u67b6\u548c\u901a\u4fe1\u534f\u8bae\u7b49\u5173\u952e\u6280\u672f\u8981\u7d20\u3002", "result": "\u8bc6\u522b\u4e86LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u6280\u672f\u6846\u67b6\u548c\u901a\u4fe1\u534f\u8bae\uff0c\u5e76\u660e\u786e\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3a\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u5728\u591a\u667a\u80fd\u4f53\u7f16\u6392\u3001\u4eba\u673a\u534f\u4f5c\u3001\u8ba1\u7b97\u6210\u672c\u4f18\u5316\u548c\u6570\u636e\u6536\u96c6\u7b49\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.09724", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09724", "abs": "https://arxiv.org/abs/2601.09724", "authors": ["Katherine Elkins", "Jon Chun"], "title": "Syntactic Framing Fragility: An Audit of Robustness in LLM Ethical Decisions", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in consequential decision-making settings, yet their robustness to benign prompt variation remains underexplored. In this work, we study whether LLMs maintain consistent ethical judgments across logically equivalent but syntactically different prompts, focusing on variations involving negation and conditional structure. We introduce Syntactic Framing Fragility (SFF), a robustness evaluation framework that isolates purely syntactic effects via Logical Polarity Normalization (LPN), enabling direct comparison of decisions across positive and negative framings without semantic drift. Auditing 23 state-of-the-art models spanning the U.S. and China as well as small U.S. open-source software models over 14 ethical scenarios and four controlled framings (39,975 decisions), we find widespread and statistically significant inconsistency: many models reverse ethical endorsements solely due to syntactic polarity, with open-source models exhibiting over twice the fragility of commercial counterparts. We further uncover extreme negation sensitivity, where some models endorse actions in 80-97% of cases when explicitly prompted with \"should not.\" We show that eliciting chain-of-thought reasoning substantially reduces fragility, identifying a practical mitigation lever, and we map fragility across scenarios, finding higher risk in financial and business contexts than in medical scenarios. Our results demonstrate that syntactic consistency constitutes a distinct and critical dimension of ethical robustness, and we argue that SFF-style audits should be a standard component of safety evaluation for deployed LLMs. Code and results will be available on github.com.", "AI": {"tldr": "LLMs\u5728\u903b\u8f91\u7b49\u4ef7\u4f46\u53e5\u6cd5\u4e0d\u540c\u7684\u63d0\u793a\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f26\u7406\u5224\u65ad\u4e0d\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5bf9\u5426\u5b9a\u548c\u6761\u4ef6\u7ed3\u6784\u7684\u654f\u611f\u6027\uff0c\u5f00\u6e90\u6a21\u578b\u6bd4\u5546\u4e1a\u6a21\u578b\u8106\u5f31\u6027\u9ad8\u4e24\u500d\u4ee5\u4e0a\u3002", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u91cd\u8981\u51b3\u7b56\u573a\u666f\uff0c\u4f46\u5176\u5bf9\u826f\u6027\u63d0\u793a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u903b\u8f91\u7b49\u4ef7\u4f46\u53e5\u6cd5\u4e0d\u540c\u7684\u63d0\u793a\u4e0b\u662f\u5426\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u4f26\u7406\u5224\u65ad\u3002", "method": "\u63d0\u51fa\u53e5\u6cd5\u6846\u67b6\u8106\u5f31\u6027(SFF)\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u903b\u8f91\u6781\u6027\u5f52\u4e00\u5316(LPN)\u9694\u79bb\u7eaf\u53e5\u6cd5\u6548\u5e94\u3002\u5bf923\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5305\u62ec\u4e2d\u7f8e\u6a21\u578b\u53ca\u7f8e\u56fd\u5f00\u6e90\u6a21\u578b\uff09\u572814\u4e2a\u4f26\u7406\u573a\u666f\u548c4\u79cd\u63a7\u5236\u6846\u67b6\u4e0b\u8fdb\u884c\u5ba1\u8ba1\uff08\u517139,975\u4e2a\u51b3\u7b56\uff09\u3002", "result": "\u53d1\u73b0\u5e7f\u6cdb\u4e14\u7edf\u8ba1\u663e\u8457\u7684\u4e0d\u4e00\u81f4\u6027\uff1a\u8bb8\u591a\u6a21\u578b\u4ec5\u56e0\u53e5\u6cd5\u6781\u6027\u5c31\u53cd\u8f6c\u4f26\u7406\u8ba4\u53ef\uff0c\u5f00\u6e90\u6a21\u578b\u7684\u8106\u5f31\u6027\u662f\u5546\u4e1a\u6a21\u578b\u7684\u4e24\u500d\u4ee5\u4e0a\u3002\u53d1\u73b0\u6781\u7aef\u5426\u5b9a\u654f\u611f\u6027\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\"\u4e0d\u5e94\u8be5\"\u63d0\u793a\u4e0b80-97%\u60c5\u51b5\u4e0b\u8ba4\u53ef\u884c\u52a8\u3002\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u663e\u8457\u964d\u4f4e\u8106\u5f31\u6027\u3002", "conclusion": "\u53e5\u6cd5\u4e00\u81f4\u6027\u662f\u4f26\u7406\u9c81\u68d2\u6027\u7684\u4e00\u4e2a\u72ec\u7279\u4e14\u5173\u952e\u7ef4\u5ea6\uff0cSFF\u5f0f\u5ba1\u8ba1\u5e94\u6210\u4e3a\u90e8\u7f72LLMs\u5b89\u5168\u8bc4\u4f30\u7684\u6807\u51c6\u7ec4\u6210\u90e8\u5206\u3002", "topic": "agent analysis"}}
{"id": "2601.09926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09926", "abs": "https://arxiv.org/abs/2601.09926", "authors": ["Kirandeep Kaur", "Vinayak Gupta", "Aditya Gupta", "Chirag Shah"], "title": "The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation", "comment": null, "summary": "Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.", "AI": {"tldr": "ProPer\u662f\u4e00\u4e2a\u4e24\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u751f\u6210\u9690\u5f0f\u7ef4\u5ea6\u6765\u4e3b\u52a8\u6ee1\u8db3\u7528\u6237\u672a\u8868\u8fbe\u7684\u9700\u6c42\uff0c\u76f8\u6bd4\u4f20\u7edf\u53cd\u5e94\u5f0f\u52a9\u624b\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u8d28\u91cf\u548c\u4e3b\u52a8\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u52a9\u624b\u4e3b\u8981\u91c7\u7528\u53cd\u5e94\u5f0f\u7684\u95ee\u7b54\u6a21\u5f0f\uff0c\u9700\u8981\u7528\u6237\u660e\u786e\u8868\u8fbe\u9700\u6c42\uff0c\u5bfc\u81f4\u76f8\u5173\u4f46\u672a\u8868\u8fbe\u7684\u9700\u6c42\u65e0\u6cd5\u5f97\u5230\u6ee1\u8db3\u3002\u73b0\u6709\u4e3b\u52a8\u4ee3\u7406\u8981\u4e48\u9700\u8981\u7528\u6237\u8fdb\u4e00\u6b65\u6f84\u6e05\uff08\u589e\u52a0\u8d1f\u62c5\uff09\uff0c\u8981\u4e48\u4ece\u4e0a\u4e0b\u6587\u63a8\u65ad\u672a\u6765\u9700\u6c42\uff08\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5fc5\u8981\u6216\u65f6\u673a\u4e0d\u5f53\u7684\u5e72\u9884\uff09\u3002", "method": "\u63d0\u51faProPer\uff08Proactivity-driven Personalized agents\uff09\u4e24\u667a\u80fd\u4f53\u67b6\u6784\uff1a1) \u7ef4\u5ea6\u751f\u6210\u667a\u80fd\u4f53\uff08DGA\uff09\uff1a\u5fae\u8c03\u7684LLM\u667a\u80fd\u4f53\uff0c\u5229\u7528\u663e\u5f0f\u7528\u6237\u6570\u636e\u751f\u6210\u591a\u4e2a\u9690\u5f0f\u7ef4\u5ea6\uff08\u4e0e\u7528\u6237\u4efb\u52a1\u76f8\u5173\u4f46\u7528\u6237\u672a\u8003\u8651\u7684\u6f5c\u5728\u65b9\u9762\uff09\u6216\u77e5\u8bc6\u7f3a\u53e3\uff1b2) \u4f7f\u7528\u57fa\u4e8e\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u4efb\u52a1\u76f8\u5173\u6027\u7684\u91cd\u6392\u5e8f\u5668\u7b5b\u9009\u7ef4\u5ea6\uff1b3) \u54cd\u5e94\u751f\u6210\u667a\u80fd\u4f53\uff08RGA\uff09\uff1a\u5e73\u8861\u663e\u5f0f\u548c\u9690\u5f0f\u7ef4\u5ea6\uff0c\u751f\u6210\u5177\u6709\u53ca\u65f6\u4e3b\u52a8\u5e72\u9884\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u3002", "result": "\u5728\u591a\u9886\u57df\u8bc4\u4f30\u4e2d\uff0cProPer\u5728\u8d28\u91cf\u5206\u6570\u548c\u80dc\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u5728\u5355\u8f6e\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe84%\u7684\u589e\u76ca\uff0c\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u6301\u7eed\u5360\u4f18\u3002\u4f7f\u7528\u7ed3\u6784\u5316\u3001\u7f3a\u53e3\u611f\u77e5\u7684\u8bc4\u4f30\u6807\u51c6\uff08\u8986\u76d6\u5ea6\u3001\u4e3b\u52a8\u6027\u9002\u5f53\u6027\u3001\u610f\u56fe\u5bf9\u9f50\uff09\u8fdb\u884c\u6d4b\u91cf\u3002", "conclusion": "ProPer\u901a\u8fc7\u4e24\u667a\u80fd\u4f53\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u53cd\u5e94\u5f0f\u52a9\u624b\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u4e3b\u52a8\u8bc6\u522b\u548c\u6ee1\u8db3\u7528\u6237\u672a\u8868\u8fbe\u7684\u9700\u6c42\uff0c\u5728\u591a\u4e2a\u9886\u57df\u663e\u8457\u63d0\u5347\u4e86\u52a9\u624b\u7684\u4e3b\u52a8\u6027\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.09883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09883", "abs": "https://arxiv.org/abs/2601.09883", "authors": ["Xinxing Ren", "Quagmire Zang", "Caelum Forder", "Suman Deb", "Ahsen Tahir", "Roman J. Georgio", "Peter Carroll", "Zekun Guo"], "title": "Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL", "comment": null, "summary": "Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u6d41\u7f16\u6392\u7684\u591a\u667a\u80fd\u4f53\u8303\u5f0fCORAL\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u52a8\u6001\u534f\u8c03\u4efb\u52a1\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u5728GAIA\u57fa\u51c6\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5OWL 8.49\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u9700\u8981\u4eba\u5de5\u679a\u4e3e\u4efb\u52a1\u72b6\u6001\u5e76\u6307\u5b9a\u8def\u7531\u89c4\u5219\uff0c\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u6765\u9884\u6d4b\u548c\u7f16\u7801\u53ef\u80fd\u72b6\u6001\uff0c\u4e14\u65e0\u6cd5\u7a77\u5c3d\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u7684\u72b6\u6001\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4fe1\u606f\u6d41\u7f16\u6392\u7684\u591a\u667a\u80fd\u4f53\u8303\u5f0fCORAL\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u7f16\u6392\u5668\u6301\u7eed\u76d1\u63a7\u4efb\u52a1\u8fdb\u5ea6\uff0c\u4f7f\u7528A2A\u5de5\u5177\u5305\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u52a8\u6001\u534f\u8c03\u5176\u4ed6\u667a\u80fd\u4f53\uff0c\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u3002", "result": "\u5728GAIA\u901a\u7528\u57fa\u51c6\u4e0a\uff0c\u4ee5OWL\u4e3a\u57fa\u7ebf\uff0cpass@1\u8bbe\u7f6e\u4e0b\u8fbe\u523063.64%\u51c6\u786e\u7387\uff0c\u6bd4OWL\u768455.15%\u9ad8\u51fa8.49\u4e2a\u767e\u5206\u70b9\uff0c\u4e14token\u6d88\u8017\u76f8\u5f53\u3002\u6848\u4f8b\u7ea7\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u66f4\u7075\u6d3b\u76d1\u63a7\u4efb\u52a1\u5e76\u66f4\u9c81\u68d2\u5904\u7406\u8fb9\u7f18\u60c5\u51b5\u3002", "conclusion": "\u4fe1\u606f\u6d41\u7f16\u6392\u7684\u591a\u667a\u80fd\u4f53\u8303\u5f0f\u80fd\u6709\u6548\u514b\u670d\u57fa\u4e8e\u89c4\u5219\u5de5\u4f5c\u6d41\u7684\u9650\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u4efb\u52a1\u76d1\u63a7\u548c\u66f4\u9c81\u68d2\u7684\u8fb9\u7f18\u60c5\u51b5\u5904\u7406\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "topic": "agent analysis"}}
{"id": "2601.09913", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09913", "abs": "https://arxiv.org/abs/2601.09913", "authors": ["Joe Logan"], "title": "Continuum Memory Architectures for Long-Horizon LLM Agents", "comment": "10 Pages", "summary": "Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \\textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u8fde\u7eed\u8bb0\u5fc6\u67b6\u6784\"(CMA)\u4f5c\u4e3aRAG\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u6301\u4e45\u5b58\u50a8\u3001\u9009\u62e9\u6027\u4fdd\u7559\u3001\u5173\u8054\u8def\u7531\u3001\u65f6\u95f4\u94fe\u548c\u62bd\u8c61\u6574\u5408\u6765\u89e3\u51b3RAG\u5728\u8bb0\u5fc6\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u5c06\u8bb0\u5fc6\u89c6\u4e3a\u9759\u6001\u67e5\u627e\u8868\uff0c\u5b58\u5728\u4fe1\u606f\u6c38\u4e45\u4fdd\u7559\u3001\u68c0\u7d22\u53ea\u8bfb\u3001\u7f3a\u4e4f\u65f6\u95f4\u8fde\u7eed\u6027\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u957f\u671f\u667a\u80fd\u4f53\u5bf9\u8bb0\u5fc6\u79ef\u7d2f\u3001\u66f4\u65b0\u548c\u6d88\u6b67\u7684\u9700\u6c42\u3002", "method": "\u5b9a\u4e49\u4e86CMA\u67b6\u6784\u7c7b\u522b\uff0c\u5f3a\u8c03\u901a\u8fc7\u6301\u4e45\u5b58\u50a8\u3001\u9009\u62e9\u6027\u4fdd\u7559\u3001\u5173\u8054\u8def\u7531\u3001\u65f6\u95f4\u94fe\u548c\u6574\u5408\u4e3a\u9ad8\u9636\u62bd\u8c61\u6765\u7ef4\u62a4\u548c\u66f4\u65b0\u8de8\u4ea4\u4e92\u7684\u5185\u90e8\u72b6\u6001\uff0c\u800c\u975e\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002", "result": "\u5728\u77e5\u8bc6\u66f4\u65b0\u3001\u65f6\u95f4\u5173\u8054\u3001\u5173\u8054\u56de\u5fc6\u3001\u4e0a\u4e0b\u6587\u6d88\u6b67\u7b49\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86CMA\u76f8\u6bd4RAG\u7684\u884c\u4e3a\u4f18\u52bf\uff0c\u8bc1\u660eCMA\u662f\u957f\u671f\u667a\u80fd\u4f53\u7684\u5fc5\u8981\u67b6\u6784\u539f\u8bed\u3002", "conclusion": "CMA\u89e3\u51b3\u4e86RAG\u5728\u8bb0\u5fc6\u5904\u7406\u4e0a\u7684\u7ed3\u6784\u7f3a\u9677\uff0c\u4f46\u9762\u4e34\u5ef6\u8fdf\u3001\u6f02\u79fb\u548c\u53ef\u89e3\u91ca\u6027\u7b49\u5f00\u653e\u6311\u6218\uff0c\u662f\u957f\u671f\u667a\u80fd\u4f53\u53d1\u5c55\u7684\u5173\u952e\u67b6\u6784\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.09726", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09726", "abs": "https://arxiv.org/abs/2601.09726", "authors": ["Hien Tran", "Quinten Steenhuis", "Alexandros Christoforos", "Chadbourne Davis"], "title": "Forgetting as a Feature: Cognitive Alignment of Large Language Models", "comment": "Under submission", "summary": "Large Language Models (LLMs) are often evaluated against ideals of perfect Bayesian inference, yet growing evidence suggests that their in-context reasoning exhibits systematic forgetting of past information. Rather than viewing this behavior as a limitation, we reinterpret forgetting as a functional cognitive mechanism. Drawing inspiration from human memory dynamics, we model LLM inference as a probabilistic memory process governed by exponential decay. We introduce a benchmark suite that evaluates temporal reasoning, concept drift adaptation, and associative recall, enabling direct comparison between model behavior and human cognitive patterns. Our empirical results reveal that LLMs demonstrate forgetting rates analogous to human memory efficiency trade-offs between stability and adaptability. Building on these observations, we propose probabilistic memory prompting, a lightweight strategy that shapes evidence integration to mimic human-like memory decay, leading to improved long-horizon reasoning performance. Our findings position forgetting not as a failure mode, but as a principled mechanism for adaptive intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u8be0\u91ca\u4e86LLMs\u4e2d\u7684\u9057\u5fd8\u73b0\u8c61\uff0c\u5c06\u5176\u89c6\u4e3a\u529f\u80fd\u6027\u8ba4\u77e5\u673a\u5236\u800c\u975e\u7f3a\u9677\uff0c\u901a\u8fc7\u5efa\u6a21\u4e3a\u6307\u6570\u8870\u51cf\u7684\u8bb0\u5fc6\u8fc7\u7a0b\uff0c\u63d0\u51fa\u6982\u7387\u8bb0\u5fc6\u63d0\u793a\u7b56\u7565\u6765\u6539\u5584\u957f\u65f6\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e0aLLMs\u7684\u9057\u5fd8\u88ab\u89c6\u4e3a\u63a8\u7406\u7f3a\u9677\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u53ef\u80fd\u662f\u7c7b\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u7684\u529f\u80fd\u6027\u8ba4\u77e5\u673a\u5236\uff0c\u9700\u8981\u91cd\u65b0\u7406\u89e3LLMs\u7684\u63a8\u7406\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u7684\u76f8\u4f3c\u6027\u3002", "method": "\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u53d7\u6307\u6570\u8870\u51cf\u63a7\u5236\u7684\u6982\u7387\u8bb0\u5fc6\u8fc7\u7a0b\uff0c\u5f15\u5165\u8bc4\u4f30\u65f6\u95f4\u63a8\u7406\u3001\u6982\u5ff5\u6f02\u79fb\u9002\u5e94\u548c\u8054\u60f3\u56de\u5fc6\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u5e76\u63d0\u51fa\u6982\u7387\u8bb0\u5fc6\u63d0\u793a\u7b56\u7565\u6765\u5851\u9020\u8bc1\u636e\u6574\u5408\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793aLLMs\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u6548\u7387\u6743\u8861\u76f8\u4f3c\u7684\u9057\u5fd8\u7387\uff0c\u6982\u7387\u8bb0\u5fc6\u63d0\u793a\u7b56\u7565\u80fd\u6709\u6548\u6539\u5584\u957f\u65f6\u63a8\u7406\u6027\u80fd\uff0c\u4f7f\u6a21\u578b\u884c\u4e3a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u3002", "conclusion": "\u9057\u5fd8\u4e0d\u5e94\u88ab\u89c6\u4e3a\u6545\u969c\u6a21\u5f0f\uff0c\u800c\u662f\u81ea\u9002\u5e94\u667a\u80fd\u7684\u539f\u5219\u6027\u673a\u5236\uff0cLLMs\u7684\u63a8\u7406\u884c\u4e3a\u4e0e\u4eba\u7c7b\u8bb0\u5fc6\u52a8\u6001\u5b58\u5728\u6df1\u523b\u76f8\u4f3c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.09873", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.09873", "abs": "https://arxiv.org/abs/2601.09873", "authors": ["Saymon Souza", "Amanda Santana", "Eduardo Figueiredo", "Igor Muzetti", "Jo\u00e3o Eduardo Montandon", "Lionel Briand"], "title": "Beyond Strict Rules: Assessing the Effectiveness of Large Language Models for Code Smell Detection", "comment": null, "summary": "Code smells are symptoms of potential code quality problems that may affect software maintainability, thus increasing development costs and impacting software reliability. Large language models (LLMs) have shown remarkable capabilities for supporting various software engineering activities, but their use for detecting code smells remains underexplored. However, unlike the rigid rules of static analysis tools, LLMs can support flexible and adaptable detection strategies tailored to the unique properties of code smells. This paper evaluates the effectiveness of four LLMs -- DeepSeek-R1, GPT-5 mini, Llama-3.3, and Qwen2.5-Code -- for detecting nine code smells across 30 Java projects. For the empirical evaluation, we created a ground-truth dataset by asking 76 developers to manually inspect 268 code-smell candidates. Our results indicate that LLMs perform strongly for structurally straightforward smells, such as Large Class and Long Method. However, we also observed that different LLMs and tools fare better for distinct code smells. We then propose and evaluate a detection strategy that combines LLMs and static analysis tools. The proposed strategy outperforms LLMs and tools in five out of nine code smells in terms of F1-Score. However, it also generates more false positives for complex smells. Therefore, we conclude that the optimal strategy depends on whether Recall or Precision is the main priority for code smell detection.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cdLLM\u5728\u68c0\u6d4b\u4e5d\u79cd\u4ee3\u7801\u5f02\u5473\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408LLM\u4e0e\u9759\u6001\u5206\u6790\u5de5\u5177\u7684\u6df7\u5408\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5728\u591a\u6570\u5f02\u5473\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\u4f46\u4f1a\u4ea7\u751f\u66f4\u591a\u8bef\u62a5\u3002", "motivation": "\u4ee3\u7801\u5f02\u5473\u4f1a\u5f71\u54cd\u8f6f\u4ef6\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u9760\u6027\uff0c\u589e\u52a0\u5f00\u53d1\u6210\u672c\u3002\u867d\u7136LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4ee3\u7801\u5f02\u5473\u68c0\u6d4b\u65b9\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e0e\u9759\u6001\u5206\u6790\u5de5\u5177\u7684\u521a\u6027\u89c4\u5219\u4e0d\u540c\uff0cLLM\u53ef\u4ee5\u63d0\u4f9b\u7075\u6d3b\u3001\u53ef\u5b9a\u5236\u7684\u68c0\u6d4b\u7b56\u7565\u3002", "method": "1) \u8bc4\u4f30\u56db\u79cdLLM\uff08DeepSeek-R1\u3001GPT-5 mini\u3001Llama-3.3\u3001Qwen2.5-Code\uff09\u572830\u4e2aJava\u9879\u76ee\u4e2d\u68c0\u6d4b\u4e5d\u79cd\u4ee3\u7801\u5f02\u5473\u7684\u6548\u679c\uff1b2) \u521b\u5efa\u5305\u542b268\u4e2a\u4ee3\u7801\u5f02\u5473\u5019\u9009\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u753176\u540d\u5f00\u53d1\u8005\u624b\u52a8\u68c0\u67e5\uff1b3) \u63d0\u51fa\u5e76\u8bc4\u4f30\u7ed3\u5408LLM\u4e0e\u9759\u6001\u5206\u6790\u5de5\u5177\u7684\u6df7\u5408\u68c0\u6d4b\u7b56\u7565\u3002", "result": "1) LLM\u5728\u7ed3\u6784\u7b80\u5355\u7684\u5f02\u5473\uff08\u5982Large Class\u3001Long Method\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff1b2) \u4e0d\u540cLLM\u548c\u5de5\u5177\u5728\u4e0d\u540c\u4ee3\u7801\u5f02\u5473\u68c0\u6d4b\u4e2d\u5404\u6709\u6240\u957f\uff1b3) \u6df7\u5408\u7b56\u7565\u5728\u4e5d\u79cd\u5f02\u5473\u4e2d\u7684\u4e94\u79cd\u4e0aF1\u5206\u6570\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLM\u6216\u5de5\u5177\uff1b4) \u6df7\u5408\u7b56\u7565\u5bf9\u590d\u6742\u5f02\u5473\u4f1a\u4ea7\u751f\u66f4\u591a\u8bef\u62a5\u3002", "conclusion": "\u6700\u4f18\u7684\u4ee3\u7801\u5f02\u5473\u68c0\u6d4b\u7b56\u7565\u53d6\u51b3\u4e8e\u68c0\u6d4b\u76ee\u6807\u662f\u4f18\u5148\u53ec\u56de\u7387\u8fd8\u662f\u7cbe\u786e\u5ea6\u3002\u6df7\u5408\u7b56\u7565\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u9700\u8981\u6743\u8861\u8bef\u62a5\u7387\u3002", "topic": "swe application"}}
{"id": "2601.09923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09923", "abs": "https://arxiv.org/abs/2601.09923", "authors": ["Hanna Foerster", "Robert Mullins", "Tom Blanchard", "Nicolas Papernot", "Kristina Nikoli\u0107", "Florian Tram\u00e8r", "Ilia Shumailov", "Cheng Zhang", "Yiren Zhao"], "title": "CaMeLs Can Use Computers Too: System-level Security for Computer Use Agents", "comment": null, "summary": "AI agents are vulnerable to prompt injection attacks, where malicious content hijacks agent behavior to steal credentials or cause financial loss. The only known robust defense is architectural isolation that strictly separates trusted task planning from untrusted environment observations. However, applying this design to Computer Use Agents (CUAs) -- systems that automate tasks by viewing screens and executing actions -- presents a fundamental challenge: current agents require continuous observation of UI state to determine each action, conflicting with the isolation required for security. We resolve this tension by demonstrating that UI workflows, while dynamic, are structurally predictable. We introduce Single-Shot Planning for CUAs, where a trusted planner generates a complete execution graph with conditional branches before any observation of potentially malicious content, providing provable control flow integrity guarantees against arbitrary instruction injections. Although this architectural isolation successfully prevents instruction injections, we show that additional measures are needed to prevent Branch Steering attacks, which manipulate UI elements to trigger unintended valid paths within the plan. We evaluate our design on OSWorld, and retain up to 57% of the performance of frontier models while improving performance for smaller open-source models by up to 19%, demonstrating that rigorous security and utility can coexist in CUAs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUA\uff09\u7684\u5355\u6b21\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u4fe1\u89c4\u5212\u5668\u5728\u6267\u884c\u524d\u751f\u6210\u5b8c\u6574\u7684\u6267\u884c\u56fe\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u63a7\u5236\u6d41\u5b8c\u6574\u6027\u4fdd\u8bc1\uff0c\u4ee5\u9632\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u80fd\u3002", "motivation": "AI\u4ee3\u7406\u5bb9\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u6076\u610f\u5185\u5bb9\u53ef\u80fd\u52ab\u6301\u4ee3\u7406\u884c\u4e3a\u5bfc\u81f4\u51ed\u8bc1\u7a83\u53d6\u6216\u8d22\u52a1\u635f\u5931\u3002\u5f53\u524d\u552f\u4e00\u5df2\u77e5\u7684\u9c81\u68d2\u9632\u5fa1\u662f\u67b6\u6784\u9694\u79bb\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUA\uff09\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff1aCUA\u9700\u8981\u6301\u7eed\u89c2\u5bdfUI\u72b6\u6001\u6765\u786e\u5b9a\u6bcf\u4e2a\u52a8\u4f5c\uff0c\u8fd9\u4e0e\u5b89\u5168\u6240\u9700\u7684\u9694\u79bb\u8981\u6c42\u76f8\u51b2\u7a81\u3002", "method": "\u5f15\u5165\u5355\u6b21\u89c4\u5212\u65b9\u6cd5\uff0c\u5728\u89c2\u5bdf\u4efb\u4f55\u6f5c\u5728\u6076\u610f\u5185\u5bb9\u4e4b\u524d\uff0c\u7531\u53ef\u4fe1\u89c4\u5212\u5668\u751f\u6210\u5305\u542b\u6761\u4ef6\u5206\u652f\u7684\u5b8c\u6574\u6267\u884c\u56fe\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u63a7\u5236\u6d41\u5b8c\u6574\u6027\u4fdd\u8bc1\uff0c\u9632\u6b62\u4efb\u610f\u6307\u4ee4\u6ce8\u5165\u653b\u51fb\u3002\u540c\u65f6\u8bc6\u522b\u5e76\u89e3\u51b3\u5206\u652f\u5bfc\u5411\u653b\u51fb\u95ee\u9898\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u524d\u6cbf\u6a21\u578b57%\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u8f83\u5c0f\u5f00\u6e90\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe19%\u3002\u8bc1\u660e\u4e86\u4e25\u683c\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u53ef\u4ee5\u5728CUA\u4e2d\u5171\u5b58\u3002", "conclusion": "\u901a\u8fc7\u5355\u6b21\u89c4\u5212\u65b9\u6cd5\u89e3\u51b3\u4e86CUA\u4e2d\u5b89\u5168\u9694\u79bb\u4e0e\u6301\u7eed\u89c2\u5bdf\u9700\u6c42\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\uff0c\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u7684\u63a7\u5236\u6d41\u5b8c\u6574\u6027\u4fdd\u8bc1\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u9700\u8981\u989d\u5916\u9632\u5fa1\u63aa\u65bd\u7684\u5206\u652f\u5bfc\u5411\u653b\u51fb\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.09728", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09728", "abs": "https://arxiv.org/abs/2601.09728", "authors": ["Meicong Zhang", "Tiancheng su", "Guoxiu He"], "title": "Eliminating Agentic Workflow for Introduction Generation with Parametric Stage Tokens", "comment": null, "summary": "In recent years, using predefined agentic workflows to guide large language models (LLMs) for literature classification and review has become a research focus. However, writing research introductions is more challenging. It requires rigorous logic, coherent structure, and abstract summarization. Existing workflows often suffer from long reasoning chains, error accumulation, and reduced textual coherence. To address these limitations, we propose eliminating external agentic workflows. Instead, we directly parameterize their logical structure into the LLM. This allows the generation of a complete introduction in a single inference. To this end, we introduce the Stage Token for Introduction Generation (STIG). STIG converts the multiple stages of the original workflow into explicit stage signals. These signals guide the model to follow different logical roles and functions during generation. Through instruction tuning, the model learns the mapping between stage tokens and text functions. It also learns the logical order and transition patterns between stages, encoding this knowledge into the model parameters. Experimental results show that STIG can generate multi-stage text in a single inference. It does not require explicit workflow calls. STIG outperforms traditional agentic workflows and other baselines on metrics of semantic similarity and sentence-level structural rationality. The code is provided in the Supplementary Materials.", "AI": {"tldr": "STIG\u65b9\u6cd5\u901a\u8fc7\u5c06\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u7684\u903b\u8f91\u7ed3\u6784\u53c2\u6570\u5316\u5230LLM\u4e2d\uff0c\u5b9e\u73b0\u5355\u6b21\u63a8\u7406\u751f\u6210\u5b8c\u6574\u7814\u7a76\u5f15\u8a00\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u957f\u63a8\u7406\u94fe\u548c\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9884\u5b9a\u4e49\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\u5728\u751f\u6210\u7814\u7a76\u5f15\u8a00\u65f6\u5b58\u5728\u957f\u63a8\u7406\u94fe\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u6587\u672c\u8fde\u8d2f\u6027\u964d\u4f4e\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5f15\u8a00\u5199\u4f5c\u9700\u8981\u4e25\u8c28\u903b\u8f91\u3001\u8fde\u8d2f\u7ed3\u6784\u548c\u62bd\u8c61\u6982\u62ec\u80fd\u529b\u3002", "method": "\u63d0\u51faSTIG\u65b9\u6cd5\uff0c\u5c06\u539f\u59cb\u5de5\u4f5c\u6d41\u7684\u591a\u4e2a\u9636\u6bb5\u8f6c\u6362\u4e3a\u663e\u5f0f\u7684\u9636\u6bb5\u4fe1\u53f7\uff08\u9636\u6bb5\u6807\u8bb0\uff09\uff0c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u8ba9\u6a21\u578b\u5b66\u4e60\u9636\u6bb5\u6807\u8bb0\u4e0e\u6587\u672c\u529f\u80fd\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u4ee5\u53ca\u9636\u6bb5\u95f4\u7684\u903b\u8f91\u987a\u5e8f\u548c\u8f6c\u6362\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTIG\u80fd\u591f\u5728\u5355\u6b21\u63a8\u7406\u4e2d\u751f\u6210\u591a\u9636\u6bb5\u6587\u672c\uff0c\u65e0\u9700\u663e\u5f0f\u5de5\u4f5c\u6d41\u8c03\u7528\uff0c\u5728\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u53e5\u5b50\u7ea7\u7ed3\u6784\u5408\u7406\u6027\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5de5\u4f5c\u6d41\u903b\u8f91\u7ed3\u6784\u53c2\u6570\u5316\u5230LLM\u4e2d\uff0cSTIG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5728\u751f\u6210\u7814\u7a76\u5f15\u8a00\u65f6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u8fde\u8d2f\u7684\u6587\u672c\u751f\u6210\u3002", "topic": "agent analysis"}}
{"id": "2601.10093", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10093", "abs": "https://arxiv.org/abs/2601.10093", "authors": ["Yiding Qiu", "Seyed Mahdi Azimi", "Artem Lensky"], "title": "Mark My Works Autograder for Programming Courses", "comment": null, "summary": "Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.\n  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.", "AI": {"tldr": "\u5f00\u53d1\u4e86Mark My Works\u672c\u5730\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\uff0c\u7ed3\u5408\u4f20\u7edf\u5355\u5143\u6d4b\u8bd5\u548cLLM\u751f\u6210\u89e3\u91ca\uff0c\u4e3a\u7f16\u7a0b\u8bfe\u7a0b\u63d0\u4f9b\u53ca\u65f6\u8be6\u7ec6\u53cd\u9988\u3002\u5728191\u540d\u5b66\u751f\u8bfe\u7a0b\u4e2d\u6d4b\u8bd5\uff0cAI\u8bc4\u5206\u4e0e\u4eba\u5de5\u8bc4\u5206\u65e0\u7ebf\u6027\u76f8\u5173\u4f46\u5206\u5e03\u76f8\u4f3c\uff0cAI\u8bc4\u5206\u66f4\u4fdd\u5b88\u4f46\u53cd\u9988\u66f4\u8be6\u7ec6\u3002", "motivation": "\u5927\u578b\u7f16\u7a0b\u8bfe\u7a0b\u96be\u4ee5\u4e3a\u5b66\u751f\u4ee3\u7801\u63d0\u4f9b\u53ca\u65f6\u3001\u8be6\u7ec6\u7684\u53cd\u9988\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u8f85\u52a9\u6559\u5b66\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1Mark My Works\u7cfb\u7edf\uff0c\u7ed3\u5408\u4f20\u7edf\u5355\u5143\u6d4b\u8bd5\u548c\u57fa\u4e8e\u89d2\u8272\u7684LLM\u63d0\u793a\u6765\u5206\u6790\u63d0\u4ea4\u4ee3\u7801\u3001\u6279\u5224\u4ee3\u7801\u8d28\u91cf\u3001\u751f\u6210\u6559\u5b66\u53cd\u9988\uff0c\u5e76\u4fdd\u6301\u63a8\u7406\u8fc7\u7a0b\u900f\u660e\u3002", "result": "\u5728191\u540d\u5b66\u751f\u8bfe\u7a0b\u4e2d\u6d4b\u8bd579\u4efd\u63d0\u4ea4\uff0cAI\u8bc4\u5206\u4e0e\u4eba\u5de5\u8bc4\u5206\u65e0\u7ebf\u6027\u76f8\u5173(r=-0.177,p=0.124)\uff0c\u4f46\u5206\u5e03\u76f8\u4f3c\uff08\u5747\u5de6\u504f\uff09\u3002AI\u8bc4\u5206\u66f4\u4fdd\u5b88\uff08\u5747\u503c59.95 vs 80.53\uff09\uff0c\u4f46\u751f\u6210\u7684\u6280\u672f\u53cd\u9988\u66f4\u8be6\u7ec6\u3002", "conclusion": "AI\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u80fd\u8bc6\u522b\u4e0e\u4eba\u5de5\u8bc4\u5206\u76f8\u4f3c\u7684\u4ee3\u7801\u8d28\u91cf\u5c42\u6b21\uff0c\u867d\u7136\u8bc4\u5206\u54f2\u5b66\u4e0d\u540c\uff0c\u4f46\u80fd\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u6280\u672f\u53cd\u9988\uff0c\u5bf9\u7f16\u7a0b\u6559\u5b66\u6709\u8f85\u52a9\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "2601.10112", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10112", "abs": "https://arxiv.org/abs/2601.10112", "authors": ["Tsvi Cherny-Shahar", "Amiram Yehudai"], "title": "Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants", "comment": "35 pages, 5 figures", "summary": "Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.\n  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\\% and reduces completion time by 53.9\\%, yielding a mean 57.8\\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\\% in accuracy and 69.5\\% in efficiency on average, compared to 6.6\\% and 46.1\\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.", "AI": {"tldr": "\u63d0\u51fa\u4e86Repository Intelligence Graph (RIG)\u6765\u89e3\u51b3\u4ee3\u7801\u4ee3\u7406\u5728\u591a\u8bed\u8a00\u9879\u76ee\u4e2d\u96be\u4ee5\u7406\u89e3\u6784\u5efa\u548c\u6d4b\u8bd5\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u63d0\u53d6\u5668SPADE\u6784\u5efaRIG\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4ee3\u7801\u4ee3\u7406\u5728\u5904\u7406\u591a\u8bed\u8a00\u9879\u76ee\u65f6\u96be\u4ee5\u6062\u590d\u6784\u5efa\u548c\u6d4b\u8bd5\u7ed3\u6784\uff0c\u56e0\u4e3a\u8de8\u8bed\u8a00\u4f9d\u8d56\u5173\u7cfb\u5206\u5e03\u5728\u5f02\u6784\u7684\u6784\u5efa\u7cfb\u7edf\u548c\u5de5\u5177\u4e2d\uff0c\u5bfc\u81f4\u4ee3\u7406\u5bf9\u9879\u76ee\u7ed3\u6784\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86Repository Intelligence Graph (RIG)\uff0c\u8fd9\u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u7684\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u67b6\u6784\u56fe\uff0c\u8868\u793a\u53ef\u6784\u5efa\u7ec4\u4ef6\u3001\u805a\u5408\u5668\u3001\u8fd0\u884c\u5668\u3001\u6d4b\u8bd5\u3001\u5916\u90e8\u5305\u548c\u5305\u7ba1\u7406\u5668\uff0c\u901a\u8fc7\u660e\u786e\u7684\u4f9d\u8d56\u548c\u8986\u76d6\u8fb9\u8fde\u63a5\u3002\u8fd8\u63d0\u51fa\u4e86SPADE\uff0c\u4e00\u4e2a\u4ece\u6784\u5efa\u548c\u6d4b\u8bd5\u5de5\u4ef6\u4e2d\u6784\u9020RIG\u7684\u786e\u5b9a\u6027\u63d0\u53d6\u5668\uff0c\u76ee\u524d\u57fa\u4e8eCMake File API\u548cCTest\u5143\u6570\u636e\uff0c\u5e76\u5c06RIG\u66b4\u9732\u4e3aLLM\u53cb\u597d\u7684JSON\u89c6\u56fe\u3002", "result": "\u57288\u4e2a\u5b58\u50a8\u5e93\u4e0a\u8bc4\u4f30\u4e86\u4e09\u4e2a\u5546\u4e1a\u4ee3\u7406\uff08Claude Code\u3001Cursor\u3001Codex\uff09\uff0c\u63d0\u4f9bRIG\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e8612.2%\uff0c\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u4e8653.9%\uff0c\u6bcf\u4e2a\u6b63\u786e\u7b54\u6848\u7684\u5e73\u5747\u65f6\u95f4\u51cf\u5c11\u4e8657.8%\u3002\u591a\u8bed\u8a00\u5b58\u50a8\u5e93\u7684\u6539\u8fdb\u66f4\u5927\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8617.7%\uff0c\u6548\u7387\u63d0\u9ad8\u4e8669.5%\u3002", "conclusion": "RIG\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u4ee3\u7406\u5bf9\u5b58\u50a8\u5e93\u7ed3\u6784\u7684\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u591a\u8bed\u8a00\u9879\u76ee\uff0c\u5c06\u5931\u8d25\u4ece\u7ed3\u6784\u8bef\u89e3\u8f6c\u5411\u57fa\u4e8e\u6b63\u786e\u7ed3\u6784\u7684\u63a8\u7406\u9519\u8bef\uff0c\u4f46\u57fa\u4e8e\u56fe\u7684\u63a8\u7406\u8d28\u91cf\u4ecd\u7136\u662f\u5173\u952e\u56e0\u7d20\u3002", "topic": "code agent"}}
{"id": "2601.10025", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10025", "abs": "https://arxiv.org/abs/2601.10025", "authors": ["Jinpeng Wang", "Xinyu Jia", "Wei Wei Heng", "Yuquan Li", "Binbin Shi", "Qianlei Chen", "Guannan Chen", "Junxia Zhang", "Yuyu Yin"], "title": "Structured Personality Control and Adaptation for LLM Agents", "comment": null, "summary": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8363\u683c\u5fc3\u7406\u7c7b\u578b\u7684LLM\u4eba\u683c\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u5bfc-\u8f85\u52a9\u534f\u8c03\u3001\u5f3a\u5316-\u8865\u507f\u548c\u53cd\u601d\u4e09\u79cd\u673a\u5236\uff0c\u5b9e\u73b0\u65e2\u4fdd\u6301\u7ec6\u817b\u7279\u8d28\u53c8\u80fd\u52a8\u6001\u9002\u5e94\u4ea4\u4e92\u9700\u6c42\u7684\u4eba\u683c\u8868\u8fbe\u3002", "motivation": "LLMs\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u65e2\u7ec6\u817b\u53c8\u53ef\u9002\u5e94\u7684\u4eba\u683c\u8868\u8fbe\u3002\u4eba\u683c\u7279\u8d28\u5bf9\u7528\u6237\u53c2\u4e0e\u5ea6\u3001\u51b3\u7b56\u548c\u771f\u5b9e\u611f\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u4eba\u683c\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8363\u683c\u5fc3\u7406\u7c7b\u578b\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e09\u673a\u5236\u6846\u67b6\uff1a1)\u4e3b\u5bfc-\u8f85\u52a9\u534f\u8c03\u673a\u5236\u786e\u4fdd\u6838\u5fc3\u4eba\u683c\u4e00\u81f4\u6027\uff1b2)\u5f3a\u5316-\u8865\u507f\u673a\u5236\u5b9e\u73b0\u77ed\u671f\u60c5\u5883\u9002\u5e94\uff1b3)\u53cd\u601d\u673a\u5236\u9a71\u52a8\u957f\u671f\u4eba\u683c\u6f14\u5316\u3002\u4f7f\u7528MBTI\u95ee\u5377\u8fdb\u884c\u4eba\u683c\u5bf9\u9f50\u8bc4\u4f30\uff0c\u5e76\u5728\u591a\u6837\u5316\u6311\u6218\u573a\u666f\u4e2d\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5177\u6709\u6f14\u5316\u80fd\u529b\u7684\u4eba\u683c\u611f\u77e5LLMs\u80fd\u591f\u652f\u6301\u8fde\u8d2f\u4e14\u60c5\u5883\u654f\u611f\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u81ea\u7136\u4e3b\u4e49\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86LLM\u4eba\u683c\u7684\u7ec6\u817b\u8868\u8fbe\u4e0e\u52a8\u6001\u9002\u5e94\u7684\u5e73\u8861\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u4e2d\u81ea\u7136\u3001\u8fde\u8d2f\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.10220", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.10220", "abs": "https://arxiv.org/abs/2601.10220", "authors": ["Simin Sun", "Miroslaw Staron"], "title": "Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges", "comment": null, "summary": "A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.\n  In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.", "AI": {"tldr": "\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u6b63\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u751f\u6210\u5f0fAI\u6574\u5408\u5230\u5b89\u5168\u5173\u952e\u548c\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u9762\u4e34\u786e\u5b9a\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u7b49\u72ec\u7279\u6311\u6218\uff0c\u901a\u8fc7\u7814\u7a76\u8bc6\u522b\u4e8611\u79cd\u65b0\u5174\u5b9e\u8df5\u548c14\u4e2a\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5728\u53d8\u9769\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u4f46\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u7ec4\u7ec7\u9996\u6b21\u5c06AI\u6574\u5408\u5230\u5b89\u5168\u5173\u952e\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4e25\u683c\u7684\u786e\u5b9a\u6027\u3001\u53ef\u9760\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u8981\u6c42\u5bf9\u751f\u6210\u5f0f\u6280\u672f\u7684\u91c7\u7528\u6784\u6210\u4e86\u72ec\u7279\u6311\u6218\u3002", "method": "\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u7126\u70b9\u5c0f\u7ec4\u8bbf\u8c08\u548c\u7ed3\u6784\u5316\u5934\u8111\u98ce\u66b4\u4f1a\u8bae\uff0c\u5bf9\u6765\u81ea\u56db\u5bb6\u516c\u53f8\u7684\u5341\u4f4d\u8d44\u6df1\u4e13\u5bb6\u8fdb\u884c\u5b9a\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u4e2d\u751f\u6210\u5f0fAI\u589e\u5f3a\u5f00\u53d1\u7684\u5e94\u7528\u3002", "result": "\u8bc6\u522b\u4e8611\u79cd\u65b0\u5174\u5b9e\u8df5\u548c14\u4e2a\u6311\u6218\uff0c\u6d89\u53ca\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u7f16\u6392\u3001\u8d1f\u8d23\u4efb\u6cbb\u7406\u548c\u53ef\u6301\u7eed\u91c7\u7528\uff0c\u5c55\u793a\u4e86\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u5982\u4f55\u91cd\u65b0\u601d\u8003\u5de5\u4f5c\u6d41\u3001\u89d2\u8272\u548c\u5de5\u5177\u94fe\u3002", "conclusion": "\u5d4c\u5165\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u6b63\u5728\u91cd\u65b0\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u548c\u5de5\u5177\u94fe\uff0c\u4ee5\u5b9e\u73b0\u5411\u667a\u80fd\u4ee3\u7406\u7ba1\u9053\u548c\u751f\u6210\u5f0fAI\u589e\u5f3a\u5f00\u53d1\u7684\u53ef\u6301\u7eed\u8f6c\u578b\uff0c\u8fd9\u9700\u8981\u89e3\u51b3\u7f16\u6392\u3001\u6cbb\u7406\u548c\u91c7\u7528\u65b9\u9762\u7684\u6311\u6218\u3002", "topic": "swe application"}}
{"id": "2601.10029", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10029", "abs": "https://arxiv.org/abs/2601.10029", "authors": ["Tingyue Pan", "Jie Ouyang", "Mingyue Cheng", "Qingchuan Li", "Zirui Liu", "Mingfan Pan", "Shuo Yu", "Qi Liu"], "title": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization", "comment": null, "summary": "Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.", "AI": {"tldr": "\u63d0\u51faPaperScout\u81ea\u4e3b\u4ee3\u7406\u5c06\u8bba\u6587\u641c\u7d22\u91cd\u6784\u4e3a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165PSPO\u65b9\u6cd5\u89e3\u51b3\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5b66\u672f\u8bba\u6587\u641c\u7d22\u65b9\u6cd5\u4f9d\u8d56\u50f5\u5316\u7684\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u6761\u4ef6\u67e5\u8be2\u3002\u9700\u8981\u66f4\u7075\u6d3b\u7684\u81ea\u9002\u5e94\u4ee3\u7406\u6846\u67b6\u6765\u52a8\u6001\u51b3\u5b9a\u641c\u7d22\u7b56\u7565\u3002", "method": "\u63d0\u51faPaperScout\u81ea\u4e3b\u4ee3\u7406\uff0c\u5c06\u8bba\u6587\u641c\u7d22\u91cd\u6784\u4e3a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff1b\u5f15\u5165Proximal Sequence Policy Optimization (PSPO)\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8fc7\u7a0b\u611f\u77e5\u7684\u5e8f\u5217\u7ea7\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaperScout\u5728\u53ec\u56de\u7387\u548c\u76f8\u5173\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u5de5\u4f5c\u6d41\u9a71\u52a8\u548c\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u9002\u5e94\u4ee3\u7406\u6846\u67b6\u548c\u4f18\u5316\u7b56\u7565\u6709\u6548\uff0cPSPO\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u8f6e\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u4e3a\u5b66\u672f\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.10258", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.10258", "abs": "https://arxiv.org/abs/2601.10258", "authors": ["Agnia Sergeyuk", "Eric Huang", "Dariia Karaeva", "Anastasiia Serova", "Yaroslav Golubev", "Iftekhar Ahmed"], "title": "Evolving with AI: A Longitudinal Analysis of Developer Logs", "comment": "Accepted to ICSE'26 Research track. 12 pages, 5 figures, 1 table", "summary": "AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u5e74\u7eb5\u5411\u9065\u6d4b\u6570\u636e\u548c\u8c03\u67e5\uff0c\u63ed\u793a\u4e86AI\u7f16\u7801\u52a9\u624b\u5bf9\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u7684\u957f\u671f\u5f71\u54cd\uff1aAI\u7528\u6237\u4ea7\u751f\u66f4\u591a\u4ee3\u7801\u4f46\u4e5f\u5220\u9664\u66f4\u591a\uff0c\u5f00\u53d1\u8005\u611f\u77e5\u751f\u4ea7\u529b\u63d0\u5347\u4f46\u5176\u4ed6\u7ef4\u5ea6\u53d8\u5316\u4e0d\u5927\u3002", "motivation": "AI\u7f16\u7801\u52a9\u624b\u5df2\u6210\u4e3a\u4e13\u4e1aIDE\u7684\u6807\u914d\uff0c\u4f46\u5176\u5bf9\u65e5\u5e38\u5f00\u53d1\u7684\u957f\u671f\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002\u5148\u524d\u7814\u7a76\u5173\u6ce8\u77ed\u671f\u4f7f\u7528\u6216\u81ea\u6211\u62a5\u544a\uff0c\u7f3a\u4e4f\u5bf9AI\u5982\u4f55\u91cd\u5851\u5b9e\u9645\u7f16\u7801\u5b9e\u8df5\u7684\u957f\u671f\u7406\u89e3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff1a\u6536\u96c6800\u540d\u5f00\u53d1\u8005\u4e24\u5e74\u7ec6\u7c92\u5ea6\u9065\u6d4b\u6570\u636e\uff0c\u7ed3\u540862\u540d\u4e13\u4e1a\u5f00\u53d1\u8005\u7684\u8c03\u67e5\uff0c\u5206\u6790\u751f\u4ea7\u529b\u3001\u4ee3\u7801\u8d28\u91cf\u3001\u4ee3\u7801\u7f16\u8f91\u3001\u4ee3\u7801\u91cd\u7528\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u4e94\u4e2a\u7ef4\u5ea6\u7684\u5de5\u4f5c\u6d41\u53d8\u5316\u3002", "result": "\u9065\u6d4b\u6570\u636e\u663e\u793aAI\u7528\u6237\u4ea7\u751f\u66f4\u591a\u4ee3\u7801\u4f46\u4e5f\u5220\u9664\u66f4\u591a\uff1b\u8c03\u67e5\u53d7\u8bbf\u8005\u62a5\u544a\u751f\u4ea7\u529b\u63d0\u5347\uff0c\u4f46\u611f\u77e5\u5176\u4ed6\u7ef4\u5ea6\u53d8\u5316\u4e0d\u5927\u3002\u63ed\u793a\u4e86\u5de5\u4f5c\u6d41\u7684\"\u65e0\u58f0\u91cd\u6784\"\u3002", "conclusion": "AI\u7f16\u7801\u52a9\u624b\u6b63\u5728\u65e0\u58f0\u5730\u91cd\u6784\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u4e3a\u672a\u6765AI\u589e\u5f3a\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002\u9700\u8981\u5173\u6ce8AI\u5bf9\u4ee3\u7801\u7f16\u8f91\u6a21\u5f0f\u548c\u5f00\u53d1\u5b9e\u8df5\u7684\u957f\u671f\u5f71\u54cd\u3002", "topic": "code agent"}}
{"id": "2601.10496", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10496", "abs": "https://arxiv.org/abs/2601.10496", "authors": ["Ali Al-Kaswan", "Claudio Spiess", "Prem Devanbu", "Arie van Deursen", "Maliheh Izadi"], "title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs", "comment": "MSR 2026 Technical Track", "summary": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u66b4\u9732\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u91cf\u5316LLM\u8bad\u7ec3\u65f6\u63a5\u89e6buggy vs fixed\u4ee3\u7801\u5bf9\u5176\u504f\u597d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6a21\u578b\u66f4\u6613\u590d\u5236buggy\u4ee3\u7801\uff0c\u66b4\u9732\u4f1a\u626d\u66f2\u8bc4\u4f30\u7ed3\u679c\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u7528\u4e8e\u4ee3\u7801\u751f\u6210\u548c\u8c03\u8bd5\uff0c\u4f46\u5176\u8f93\u51fa\u4ecd\u53ef\u80fd\u5305\u542b\u6e90\u81ea\u8bad\u7ec3\u6570\u636e\u7684bug\u3002\u9700\u8981\u533a\u5206LLM\u662f\u504f\u597d\u6b63\u786e\u4ee3\u7801\u8fd8\u662f\u719f\u6089\u7684\u9519\u8bef\u7248\u672c\uff0c\u8fd9\u53ef\u80fd\u53d7\u8bad\u7ec3\u65f6\u63a5\u89e6\u7684\u5185\u5bb9\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u66b4\u9732\u611f\u77e5\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528ManySStuBs4J\u57fa\u51c6\uff0c\u901a\u8fc7Data Portraits\u5728Stack-V2\u8bed\u6599\u4e0a\u8fdb\u884c\u6210\u5458\u6d4b\u8bd5\uff0c\u4f30\u8ba1\u6bcf\u4e2abuggy\u548cfixed\u53d8\u4f53\u5728\u8bad\u7ec3\u4e2d\u662f\u5426\u88ab\u89c1\u8fc7\u3002\u7136\u540e\u6309\u66b4\u9732\u7a0b\u5ea6\u5206\u5c42\uff0c\u4f7f\u7528\u4ee3\u7801\u8865\u5168\u548c\u591a\u79cd\u57fa\u4e8e\u4f3c\u7136\u7684\u8bc4\u5206\u6307\u6807\u6bd4\u8f83\u6a21\u578b\u504f\u597d\u3002", "result": "67%\u7684\u793a\u4f8b\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u4e24\u4e2a\u53d8\u4f53\u90fd\u672a\u51fa\u73b0\uff1b\u5f53\u53ea\u6709\u4e00\u4e2a\u51fa\u73b0\u65f6\uff0cfixes\u6bd4bugs\u66f4\u5e38\u51fa\u73b0\u3002\u6a21\u578b\u751f\u6210\u4e2d\uff0c\u6a21\u578b\u590d\u5236buggy\u4ee3\u7801\u8fdc\u591a\u4e8efixes\uff0cbug\u66b4\u9732\u793a\u4f8b\u4f1a\u653e\u5927\u8fd9\u79cd\u8d8b\u52bf\uff0cfix\u66b4\u9732\u793a\u4f8b\u4ec5\u663e\u793a\u8fb9\u9645\u6539\u8fdb\u3002\u5728\u4f3c\u7136\u8bc4\u5206\u4e2d\uff0c\u6700\u5c0f\u548c\u6700\u5927token\u6982\u7387\u6307\u6807\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u90fd\u4e00\u81f4\u504f\u597dfixed\u4ee3\u7801\uff0c\u800cGini\u7cfb\u6570\u7b49\u6307\u6807\u5728\u53ea\u6709buggy\u53d8\u4f53\u88ab\u89c1\u8fc7\u65f6\u4f1a\u53cd\u8f6c\u504f\u597d\u3002", "conclusion": "\u66b4\u9732\u4f1a\u626d\u66f2bug-fix\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u7a81\u663eLLM\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u4f20\u64ad\u8bb0\u5fc6\u9519\u8bef\u7684\u98ce\u9669\u3002", "topic": "code agent"}}
{"id": "2601.10088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10088", "abs": "https://arxiv.org/abs/2601.10088", "authors": ["Malika Aubakirova", "Alex Atallah", "Chris Clark", "Justin Summerville", "Anjney Midha"], "title": "State of AI: An Empirical 100 Trillion Token Study with OpenRouter", "comment": "36 pages", "summary": "The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella \"Glass Slipper\" effect. These findings underscore that the way developers and end-users engage with LLMs \"in the wild\" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.", "AI": {"tldr": "\u57fa\u4e8eOpenRouter\u5e73\u53f0\u5206\u6790100\u4e07\u4ebftoken\u771f\u5b9eLLM\u4f7f\u7528\u6570\u636e\uff0c\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u5e7f\u6cdb\u91c7\u7528\u3001\u521b\u610f\u89d2\u8272\u626e\u6f14\u548c\u7f16\u7a0b\u52a9\u624b\u7c7b\u5e94\u7528\u6d41\u884c\u3001\u667a\u80fd\u4f53\u63a8\u7406\u5174\u8d77\uff0c\u4ee5\u53ca\u65e9\u671f\u7528\u6237\u7559\u5b58\u7387\u663e\u8457\u66f4\u9ad8\u7684\"\u73bb\u7483\u978b\u6548\u5e94\"\u3002", "motivation": "\u968f\u7740o1\u7b49\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5e03\uff0cLLM\u4ece\u5355\u6b21\u6a21\u5f0f\u751f\u6210\u8f6c\u5411\u591a\u6b65\u6df1\u601d\u63a8\u7406\uff0c\u4f46\u6211\u4eec\u5bf9\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u60c5\u51b5\u4e86\u89e3\u6ede\u540e\u3002\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u771f\u5b9e\u4e16\u754c\u7684LLM\u4f7f\u7528\u6a21\u5f0f\u3002", "method": "\u5229\u7528OpenRouter\u5e73\u53f0\uff08AI\u63a8\u7406\u63d0\u4f9b\u5546\uff09\u5206\u6790\u8d85\u8fc7100\u4e07\u4ebftoken\u7684\u771f\u5b9eLLM\u4ea4\u4e92\u6570\u636e\uff0c\u6db5\u76d6\u4e0d\u540c\u4efb\u52a1\u3001\u5730\u57df\u548c\u65f6\u95f4\u7ef4\u5ea6\u3002", "result": "\u89c2\u5bdf\u5230\u5f00\u6e90\u6a21\u578b\u88ab\u5e7f\u6cdb\u91c7\u7528\uff1b\u521b\u610f\u89d2\u8272\u626e\u6f14\uff08\u4e0d\u4ec5\u4ec5\u662f\u751f\u4ea7\u529b\u4efb\u52a1\uff09\u548c\u7f16\u7a0b\u52a9\u624b\u7c7b\u522b\u5f02\u5e38\u6d41\u884c\uff1b\u667a\u80fd\u4f53\u63a8\u7406\u5174\u8d77\uff1b\u65e9\u671f\u7528\u6237\u7559\u5b58\u7387\u8fdc\u9ad8\u4e8e\u540e\u671f\u7528\u6237\uff0c\u79f0\u4e3a\"\u73bb\u7483\u978b\u6548\u5e94\"\u3002", "conclusion": "\u5f00\u53d1\u8005\u548c\u7ec8\u7aef\u7528\u6237\u5bf9LLM\u7684\u5b9e\u9645\u4f7f\u7528\u662f\u590d\u6742\u591a\u6837\u7684\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u6a21\u578b\u6784\u5efa\u8005\u3001AI\u5f00\u53d1\u8005\u548c\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u5546\u6709\u91cd\u8981\u542f\u793a\uff0c\u6570\u636e\u9a71\u52a8\u7684\u4f7f\u7528\u7406\u89e3\u53ef\u4ee5\u6307\u5bfc\u66f4\u597d\u7684LLM\u7cfb\u7edf\u8bbe\u8ba1\u548c\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2601.09858", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09858", "abs": "https://arxiv.org/abs/2601.09858", "authors": ["Yilin Bao", "Ziyao He", "Zayden Yang"], "title": "OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing", "comment": null, "summary": "Scientific paper generation requires document-level planning and factual grounding, but current large language models, despite their strong local fluency, often fail in global structure, input coverage, and citation consistency. We present a reinforcement learning framework that casts scientific outline construction as a long-horizon planning problem over hierarchical document structures. Our approach models edit evolving outlines through structured actions, enabling the system to incrementally build a complete scientific manuscript. To support effective and stabilize learning,we introduce a two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning with rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity. In addition, We further introduce a benchmark for scientific paper generation that evaluates document planning, input utilization, reference faithfulness, outline organization, and content-level factual accuracy. Our results show consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u79d1\u5b66\u8bba\u6587\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u52a8\u4f5c\u5efa\u6a21\u5927\u7eb2\u6f14\u5316\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u63d0\u5347\u6587\u6863\u89c4\u5212\u3001\u5f15\u7528\u4e00\u81f4\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u8bba\u6587\u751f\u6210\u4e2d\u5b58\u5728\u5168\u5c40\u7ed3\u6784\u3001\u8f93\u5165\u8986\u76d6\u548c\u5f15\u7528\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u6587\u6863\u7ea7\u89c4\u5212\u548c\u4e8b\u5b9e\u57fa\u7840", "method": "\u5c06\u79d1\u5b66\u5927\u7eb2\u6784\u5efa\u5efa\u6a21\u4e3a\u5206\u5c42\u6587\u6863\u7ed3\u6784\u4e0a\u7684\u957f\u7a0b\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u52a8\u4f5c\u5efa\u6a21\u5927\u7eb2\u6f14\u5316\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u53cd\u5411\u5927\u7eb2\u91cd\u5efa\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u524d\u5411\u4ef7\u503c\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u5efa\u6a21\u79d1\u5b66\u6b63\u786e\u6027\u3001\u8bdd\u8bed\u8fde\u8d2f\u6027\u548c\u5f15\u7528\u4fdd\u771f\u5ea6", "result": "\u76f8\u6bd4\u5f3a\u5927\u7684\u795e\u7ecf\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u957f\u7a0b\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u5f15\u7528\u53ef\u9760\u6027\u65b9\u9762\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u79d1\u5b66\u8bba\u6587\u751f\u6210\u4e2d\u7684\u6587\u6863\u89c4\u5212\u3001\u8f93\u5165\u5229\u7528\u548c\u5f15\u7528\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u8bc4\u4f30\u57fa\u51c6", "topic": "agentic reinforcement learning"}}
{"id": "2601.10132", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10132", "abs": "https://arxiv.org/abs/2601.10132", "authors": ["Yanan Cao", "Farnaz Fallahi", "Murali Mohana Krishna Dandu", "Lalitesh Morishetti", "Kai Zhao", "Luyi Ma", "Sinduja Subramaniam", "Jianpeng Xu", "Evren Korpeoglu", "Kaushiki Nag", "Sushant Kumar", "Kannan Achan"], "title": "Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction", "comment": "Accepted at The Web Conference 2026 (WWW 2026)", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that \"more context leads to better reasoning\". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.", "AI": {"tldr": "LLMs\u5728\u9884\u6d4b\u7528\u6237\u91cd\u590d\u884c\u4e3a\u65f6\u95f4\u95f4\u9694\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u867d\u7136\u4f18\u4e8e\u7b80\u5355\u7edf\u8ba1\u57fa\u7ebf\u4f46\u4e0d\u5982\u4e13\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4e14\u8fc7\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\u53cd\u800c\u4f1a\u964d\u4f4e\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76LLMs\u4ece\u7ed3\u6784\u5316\u884c\u4e3a\u6570\u636e\u4e2d\u63a8\u65ad\u65f6\u95f4\u89c4\u5f8b\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9884\u6d4b\u91cd\u590d\u7528\u6237\u884c\u4e3a\uff08\u5982\u91cd\u590d\u8d2d\u4e70\uff09\u4e4b\u95f4\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u4ee5\u53ca\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u5176\u9884\u6d4b\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u4ee3\u8868\u6027\u7684\u91cd\u590d\u8d2d\u4e70\u573a\u666f\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5bf9\u6700\u5148\u8fdb\u7684LLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e0e\u7edf\u8ba1\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u4e0d\u540c\u5c42\u6b21\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "result": "1. LLMs\u8d85\u8d8a\u8f7b\u91cf\u7ea7\u7edf\u8ba1\u57fa\u7ebf\u4f46\u59cb\u7ec8\u4e0d\u5982\u4e13\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u793a\u5176\u6355\u6349\u5b9a\u91cf\u65f6\u95f4\u7ed3\u6784\u7684\u80fd\u529b\u6709\u9650\uff1b2. \u9002\u5ea6\u4e0a\u4e0b\u6587\u53ef\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u6dfb\u52a0\u66f4\u591a\u7528\u6237\u7ea7\u7ec6\u8282\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5f53\u524dLLMs\u5728\u7ed3\u6784\u5316\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u6311\u6218\u4e86\"\u66f4\u591a\u4e0a\u4e0b\u6587\u5bfc\u81f4\u66f4\u597d\u63a8\u7406\"\u7684\u5047\u8bbe\uff0c\u4e3a\u8bbe\u8ba1\u672a\u6765\u7ed3\u5408\u7edf\u8ba1\u7cbe\u5ea6\u548c\u8bed\u8a00\u7075\u6d3b\u6027\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6df7\u5408\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.10079", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10079", "abs": "https://arxiv.org/abs/2601.10079", "authors": ["Sijia Luo", "Xiaokang Zhang", "Yuxuan Hu", "Bohan Zhang", "Ke Wang", "Jinbo Su", "Mengshu Sun", "Lei Liang", "Jing Zhang"], "title": "Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts", "comment": null, "summary": "Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.", "AI": {"tldr": "Sparse-RL\uff1a\u4e00\u79cd\u5728\u7a00\u758frollouts\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u611f\u77e5\u62d2\u7edd\u91c7\u6837\u548c\u91cd\u8981\u6027\u91cd\u52a0\u6743\u6765\u7ea0\u6b63\u538b\u7f29\u5f15\u8d77\u7684\u7b56\u7565\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4eKV\u7f13\u5b58\u5185\u5b58\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u957f\u5e8f\u5217rollouts\u8fc7\u7a0b\u4e2d\u5b58\u50a8KV\u7f13\u5b58\u7684\u5185\u5b58\u5f00\u9500\u5de8\u5927\uff0c\u6210\u4e3a\u786c\u4ef6\u53d7\u9650\u65f6\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709KV\u538b\u7f29\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u63a8\u7406\u573a\u666f\uff0c\u76f4\u63a5\u5e94\u7528\u4e8eRL\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u7b56\u7565\u4e0d\u5339\u914d\u548c\u6027\u80fd\u5d29\u6e83\u3002", "method": "Sparse-RL\u901a\u8fc7\u7a00\u758f\u611f\u77e5\u62d2\u7edd\u91c7\u6837\u548c\u91cd\u8981\u6027\u91cd\u52a0\u6743\u6765\u89e3\u51b3\u7b56\u7565\u4e0d\u5339\u914d\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u8bc6\u522b\u4e86\u5bc6\u96c6\u65e7\u7b56\u7565\u3001\u7a00\u758f\u91c7\u6837\u5668\u7b56\u7565\u548c\u5b66\u4e60\u5668\u7b56\u7565\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff0c\u5e76\u8bbe\u8ba1\u673a\u5236\u7ea0\u6b63\u538b\u7f29\u5f15\u8d77\u7684\u4fe1\u606f\u635f\u5931\u5e26\u6765\u7684\u79bb\u7b56\u7565\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSparse-RL\u76f8\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u663e\u8457\u964d\u4f4e\u4e86rollout\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u7a00\u758f\u63a8\u7406\u90e8\u7f72\u65f6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Sparse-RL\u6210\u529f\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2dKV\u538b\u7f29\u5bfc\u81f4\u7684\u7b56\u7565\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u5728\u6709\u9650\u786c\u4ef6\u8d44\u6e90\u4e0b\u8fdb\u884c\u9ad8\u6548RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u7a00\u758f\u63a8\u7406\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10148", "abs": "https://arxiv.org/abs/2601.10148", "authors": ["Xiaowei Lv", "Zhilin Zhang", "Yijun Li", "Yusen Huo", "Siyuan Ju", "Xuyan Li", "Chunxiang Hong", "Tianyu Wang", "Yongcai Wang", "Peng Sun", "Chuan Yu", "Jian Xu", "Bo Zheng"], "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration", "comment": null, "summary": "Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDecisionLLM\uff0c\u5c06LLMs\u5e94\u7528\u4e8e\u79bb\u7ebf\u51b3\u7b56\u4efb\u52a1\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u6570\u636e\u4f5c\u4e3a\u72ec\u7acb\u6a21\u6001\u4e0e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86LLMs\u65e0\u6cd5\u7406\u89e3\u8fde\u7eed\u6570\u503c\u7684\u95ee\u9898\uff0c\u5728\u8ff7\u5bab\u548c\u7ade\u4ef7\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u5e8f\u5217\u51b3\u7b56\u4e2d\u5b58\u5728\u6311\u6218\uff0c\u800c\u51b3\u7b56\u53d8\u6362\u5668\u5c06RL\u6784\u5efa\u4e3a\u81ea\u56de\u5f52\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\u3002LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u65e0\u6cd5\u7406\u89e3\u8fde\u7eed\u6570\u503c\u3002\u672c\u6587\u63a2\u7d22LLMs\u5728\u957f\u89c6\u91ce\u5e8f\u5217\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51faDecisionLLM\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u6570\u636e\u4f5c\u4e3a\u72ec\u7acb\u6a21\u6001\uff0c\u5b66\u4e60\u8f68\u8ff9\u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u7684\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u7edf\u4e00\u6846\u67b6\u5185\u81ea\u56de\u5f52\u9884\u6d4b\u672a\u6765\u51b3\u7b56\u3002\u5efa\u7acb\u4e86\u8be5\u8303\u5f0f\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u8003\u8651\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u91cf\u548c\u6570\u636e\u8d28\u91cf\u4e09\u4e2a\u56e0\u7d20\u3002", "result": "DecisionLLM-3B\u5728\u79bb\u7ebf\u5b9e\u9a8c\u57fa\u51c6\u548c\u7ade\u4ef7\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728Maze2D umaze-v1\u4e0a\u6bd4\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\u63d0\u534769.4\u5206\uff0c\u5728AuctionNet\u4e0a\u63d0\u53470.085\u5206\uff0c\u6269\u5c55\u4e86AIGB\u8303\u5f0f\u3002", "conclusion": "LLMs\u5728\u957f\u89c6\u91ce\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cDecisionLLM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u7406\u89e3\u8fde\u7eed\u6570\u503c\u7684\u6311\u6218\uff0c\u4e3a\u5728\u7ebf\u7ade\u4ef7\u7b49\u5e94\u7528\u6307\u660e\u4e86\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10064", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10064", "abs": "https://arxiv.org/abs/2601.10064", "authors": ["Zhenghao Liu", "Zhuoyang Wu", "Xinze Li", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "Long-Chain Reasoning Distillation via Adaptive Prefix Alignment", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.", "AI": {"tldr": "P-ALIGN\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u9002\u5e94\u524d\u7f00\u5bf9\u9f50\u6765\u5229\u7528\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u622a\u53d6\u7b80\u6d01\u6709\u6548\u7684\u63a8\u7406\u524d\u7f00\u6765\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u901a\u5e38\u8fc7\u957f\u4e14\u7ed3\u6784\u590d\u6742\uff0c\u4e0e\u5b66\u751f\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u76d1\u7763\u4fe1\u53f7\u4e0e\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u84b8\u998f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPrefix-ALIGNment\u84b8\u998f\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u622a\u65ad\u6559\u5e08\u63a8\u7406\u8f68\u8ff9\uff0c\u5224\u65ad\u5269\u4f59\u540e\u7f00\u662f\u5426\u7b80\u6d01\u4e14\u8db3\u4ee5\u6307\u5bfc\u5b66\u751f\u6a21\u578b\uff0c\u7136\u540e\u5229\u7528\u6559\u5e08\u751f\u6210\u7684\u524d\u7f00\u76d1\u7763\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u524d\u7f00\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cP-ALIGN\u6bd4\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u8d85\u8fc73%\uff0c\u6784\u5efa\u7684\u524d\u7f00\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5197\u4f59\u548c\u4e0d\u786e\u5b9a\u63a8\u7406\u7ec4\u4ef6\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "P-ALIGN\u901a\u8fc7\u81ea\u9002\u5e94\u524d\u7f00\u5bf9\u9f50\u6709\u6548\u5229\u7528\u4e86\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u6559\u5e08\u8f68\u8ff9\u8fc7\u957f\u590d\u6742\u4e0e\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u80fd\u529b\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.10080", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10080", "abs": "https://arxiv.org/abs/2601.10080", "authors": ["Letian Peng", "Kun Zhou", "Longfei Yun", "Yupeng Hou", "Jingbo Shang"], "title": "Deriving Character Logic from Storyline as Codified Decision Trees", "comment": null, "summary": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.", "AI": {"tldr": "CDT\u6846\u67b6\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u53d9\u4e8b\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u6267\u884c\u3001\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\u6765\u8868\u793a\u89d2\u8272\u884c\u4e3a\u6863\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u7684\u884c\u4e3a\u6863\u6848\u901a\u5e38\u662f\u975e\u7ed3\u6784\u5316\u3001\u4e0d\u53ef\u6267\u884c\u4e14\u9a8c\u8bc1\u4e0d\u8db3\u7684\uff0c\u5bfc\u81f4\u4ee3\u7406\u884c\u4e3a\u8106\u5f31\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u3001\u53ef\u6267\u884c\u4e14\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCodified Decision Trees\uff08CDT\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u884c\u4e3a\u6863\u6848\u8868\u793a\u4e3a\u6761\u4ef6\u89c4\u5219\u6811\uff0c\u5185\u90e8\u8282\u70b9\u5bf9\u5e94\u5df2\u9a8c\u8bc1\u7684\u573a\u666f\u6761\u4ef6\uff0c\u53f6\u5b50\u8282\u70b9\u7f16\u7801\u5177\u4f53\u884c\u4e3a\uff1b2\uff09\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\uff1a\u8bf1\u5bfc\u5019\u9009\u573a\u666f-\u884c\u4e3a\u89c4\u5219\uff0c\u9a8c\u8bc1\u5176\u4e0e\u6570\u636e\u7684\u5339\u914d\u5ea6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u7ec6\u5316\u8fdb\u884c\u4f18\u5316\uff1b3\uff09\u652f\u6301\u786e\u5b9a\u6027\u68c0\u7d22\u4e0a\u4e0b\u6587\u76f8\u5173\u89c4\u5219\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCDT\u572816\u4e2a\u4f5c\u54c1\u768485\u4e2a\u89d2\u8272\u4e0a\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u7f16\u5199\u7684\u884c\u4e3a\u6863\u6848\u548c\u5148\u524d\u7684\u6863\u6848\u8bf1\u5bfc\u65b9\u6cd5\uff0c\u8868\u660e\u7f16\u7801\u5316\u548c\u9a8c\u8bc1\u7684\u884c\u4e3a\u8868\u793a\u80fd\u5e26\u6765\u66f4\u53ef\u9760\u7684\u4ee3\u7406\u57fa\u7840\u3002", "conclusion": "CDT\u6846\u67b6\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u53ef\u6267\u884c\u51b3\u7b56\u6811\u8868\u793a\u884c\u4e3a\u6863\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u884c\u4e3a\u6863\u6848\u7684\u975e\u7ed3\u6784\u5316\u3001\u4e0d\u53ef\u6267\u884c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u900f\u660e\u3001\u53ef\u68c0\u67e5\u548c\u53ef\u66f4\u65b0\u7684\u884c\u4e3a\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.10245", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10245", "abs": "https://arxiv.org/abs/2601.10245", "authors": ["Vansh Kapoor", "Aman Gupta", "Hao Chen", "Anurag Beniwal", "Jing Huang", "Aviral Kumar"], "title": "TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks", "comment": null, "summary": "Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\\unicode{x2013}$those likely to derail the solution$\\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.", "AI": {"tldr": "TRIM\u63d0\u51fa\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8fdb\u884c\u6b65\u9aa4\u7ea7\u8def\u7531\uff0c\u4ec5\u5c06\u5173\u952e\u6b65\u9aa4\u5206\u914d\u7ed9\u5927\u6a21\u578b\uff0c\u8ba9\u5c0f\u6a21\u578b\u5904\u7406\u5e38\u89c4\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387", "motivation": "\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u5bb9\u6613\u53d1\u751f\u7ea7\u8054\u5931\u8d25\uff0c\u5f53\u524dLLM\u8def\u7531\u65b9\u6cd5\u5c06\u6574\u4e2a\u67e5\u8be2\u5206\u914d\u7ed9\u5355\u4e00\u6a21\u578b\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u6b65\u9aa4\u7684\u91cd\u8981\u6027", "method": "\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u9519\u8bef\u6b65\u9aa4\uff0c\u57fa\u4e8e\u6b65\u9aa4\u7ea7\u4e0d\u786e\u5b9a\u6027\u548c\u9884\u7b97\u7ea6\u675f\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u5f00\u53d1\u4ece\u7b80\u5355\u9608\u503c\u7b56\u7565\u5230\u8003\u8651\u957f\u671f\u7cbe\u5ea6-\u6210\u672c\u6743\u8861\u7684\u590d\u6742\u7b56\u7565", "result": "\u5728MATH-500\u4e0a\uff0c\u7b80\u5355\u9608\u503c\u7b56\u7565\u6bd4\u5148\u524d\u8def\u7531\u65b9\u6cd5\u6210\u672c\u6548\u7387\u9ad85\u500d\uff0c\u9ad8\u7ea7\u7b56\u7565\u4ec5\u752820%\u6602\u8d35\u6a21\u578btoken\u5373\u53ef\u5339\u914d\u5f3a\u6a21\u578b\u6027\u80fd\uff1b\u5728AIME\u4e0a\u8fbe\u52306\u500d\u6210\u672c\u6548\u7387\u63d0\u5347", "conclusion": "\u6b65\u9aa4\u7ea7\u96be\u5ea6\u4ee3\u8868\u4e86\u63a8\u7406\u7684\u57fa\u672c\u7279\u5f81\uff0c\u9488\u5bf9\u6027\u6b65\u9aa4\u7ea7\u5e72\u9884\u80fd\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u63a8\u7406\u6548\u7387\uff0c\u5c06\u6602\u8d35\u8c03\u7528\u9650\u5236\u5728\u80fd\u9632\u6b62\u7ea7\u8054\u9519\u8bef\u7684\u5173\u952e\u6b65\u9aa4", "topic": "agent analysis"}}
{"id": "2601.10306", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10306", "abs": "https://arxiv.org/abs/2601.10306", "authors": ["Xin Guan", "Zijian Li", "Shen Huang", "Pengjun Xie", "Jingren Zhou", "Jiuxin Cao"], "title": "Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning", "comment": null, "summary": "While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded \"lucky guesses,\" leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.", "AI": {"tldr": "EAPO\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc1\u636e\u589e\u5f3a\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bc6\u96c6\u7684\u8fc7\u7a0b\u76d1\u7763\u6765\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u8bc1\u636e\u63d0\u53d6\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7ed3\u679c\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u5b58\u5728\u7ed3\u679c\u5956\u52b1\u7a00\u758f\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u60e9\u7f5a\u65e0\u6839\u636e\u7684\"\u5e78\u8fd0\u731c\u6d4b\"\uff0c\u5bfc\u81f4\u5173\u952e\u7684\u8bc1\u636e\u68c0\u7d22\u8fc7\u7a0b\u7f3a\u4e4f\u76d1\u7763\u3002", "method": "\u9996\u5148\u5efa\u7acb\u8bc1\u636e\u589e\u5f3a\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u8bc1\u636e\u91c7\u6837\u9a8c\u8bc1\u8bc1\u636e\u63d0\u53d6\u662f\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5173\u952e\u74f6\u9888\u3002\u7136\u540e\u63d0\u51faEAPO\u7b97\u6cd5\uff0c\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u8ba1\u7b97\u7ec4\u76f8\u5bf9\u8bc1\u636e\u5956\u52b1\uff0c\u63d0\u4f9b\u5bc6\u96c6\u7684\u8fc7\u7a0b\u76d1\u7763\u3002\u8fdb\u4e00\u6b65\u5f15\u5165\u81ea\u9002\u5e94\u5956\u52b1-\u7b56\u7565\u534f\u540c\u8fdb\u5316\u673a\u5236\uff0c\u8fed\u4ee3\u4f18\u5316\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cEAPO\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "EAPO\u901a\u8fc7\u5bc6\u96c6\u7684\u8fc7\u7a0b\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u8bc1\u636e\u63d0\u53d6\u95ee\u9898\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10181", "categories": ["cs.LG", "astro-ph.EP"], "pdf": "https://arxiv.org/pdf/2601.10181", "abs": "https://arxiv.org/abs/2601.10181", "authors": ["Kiattikun Chobtham"], "title": "Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand", "comment": null, "summary": "Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Ni\u00f1o Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u4e1c\u5317\u5b63\u98ce\u6c14\u5019\u6307\u6570\uff0c\u7528\u4e8e\u6539\u8fdb\u6cf0\u56fd\u7279\u5b9a\u5730\u533a\u7684\u957f\u671f\u6708\u964d\u96e8\u91cf\u9884\u6d4b\u3002", "motivation": "\u5168\u7403\u6c14\u5019\u6307\u6570\uff08\u5982\u5384\u5c14\u5c3c\u8bfa\u5357\u65b9\u6d9b\u52a8\uff09\u662f\u957f\u671f\u964d\u96e8\u9884\u6d4b\u7684\u6807\u51c6\u8f93\u5165\u7279\u5f81\uff0c\u4f46\u5728\u6cf0\u56fd\u7279\u5b9a\u533a\u57df\u7f3a\u4e4f\u80fd\u591f\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u7684\u5c40\u5730\u5c3a\u5ea6\u6307\u6570\u3002", "method": "1. \u63d0\u51fa\u65b0\u7684\u4e1c\u5317\u5b63\u98ce\u6c14\u5019\u6307\u6570\uff0c\u57fa\u4e8e\u6d77\u8868\u6e29\u5ea6\u53cd\u6620\u5317\u534a\u7403\u51ac\u5b63\u5b63\u98ce\u6c14\u5019\u5b66\uff1b2. \u4f7f\u7528\u6df1\u5ea6Q\u7f51\u7edc\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63a2\u7d22\u548c\u9009\u62e9\u4e0e\u5b63\u8282\u6027\u964d\u96e8\u76f8\u5173\u6027\u6700\u9ad8\u7684\u77e9\u5f62\u533a\u57df\u6765\u4f18\u5316\u6307\u6570\u8ba1\u7b97\u533a\u57df\uff1b3. \u5c06\u964d\u96e8\u7ad9\u5206\u4e3a12\u4e2a\u805a\u7c7b\u4ee5\u533a\u5206\u6cf0\u56fd\u5357\u90e8\u548c\u4e0a\u90e8\u7684\u964d\u96e8\u6a21\u5f0f\uff1b4. \u5c06\u4f18\u5316\u540e\u7684\u6307\u6570\u6574\u5408\u5230\u957f\u77ed\u671f\u8bb0\u5fc6\u6a21\u578b\u4e2d\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5c06\u4f18\u5316\u540e\u7684\u6307\u6570\u6574\u5408\u5230LSTM\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u591a\u6570\u805a\u7c7b\u533a\u57df\u7684\u957f\u671f\u6708\u964d\u96e8\u9884\u6d4b\u6280\u80fd\uff0c\u6709\u6548\u964d\u4f4e\u4e8612\u4e2a\u6708\u63d0\u524d\u9884\u6d4b\u7684\u5747\u65b9\u6839\u8bef\u5dee\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u5c40\u5730\u6c14\u5019\u6307\u6570\u80fd\u591f\u6709\u6548\u6539\u8fdb\u7279\u5b9a\u533a\u57df\u7684\u957f\u671f\u964d\u96e8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u533a\u57df\u6c14\u5019\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10201", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10201", "abs": "https://arxiv.org/abs/2601.10201", "authors": ["Jiarui Yao", "Ruida Wang", "Tong Zhang"], "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary", "comment": null, "summary": "Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\uff08PRL\uff09\uff0c\u901a\u8fc7\u5c06\u71b5\u6b63\u5219\u5316\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u5206\u89e3\u4e3a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4e3aLLM\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff0c\u65e0\u9700\u989d\u5916\u6b65\u9aa4\u5982MCTS\u6216\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u57fa\u4e8e\u8f68\u8ff9\u7ea7\u522b\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u7f3a\u4e4f\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u76d1\u7763\u3002\u5176\u4ed6\u7ed3\u5408\u8fc7\u7a0b\u4fe1\u53f7\u7684\u8bad\u7ec3\u6846\u67b6\u4f9d\u8d56\u7e41\u7410\u7684\u989d\u5916\u6b65\u9aa4\uff08\u5982MCTS\u3001\u8bad\u7ec3\u5355\u72ec\u5956\u52b1\u6a21\u578b\uff09\uff0c\u4e14\u8fc7\u7a0b\u4fe1\u53f7\u8bbe\u8ba1\u7f3a\u4e4f\u4e25\u683c\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51fa\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\uff08PRL\uff09\uff0c\u4ece\u7406\u8bba\u52a8\u673a\u51fa\u53d1\uff0c\u5c06\u71b5\u6b63\u5219\u5316\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u5206\u89e3\u4e3a\u4e2d\u95f4\u6b65\u9aa4\uff0c\u63a8\u5bfc\u51fa\u7b49\u4ef7\u4e8e\u5956\u52b1\u6700\u5927\u5316\u52a0\u7b56\u7565\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u95f4KL\u6563\u5ea6\u60e9\u7f5a\u9879\u7684PRL\u516c\u5f0f\uff0c\u5c06\u7ed3\u679c\u5956\u52b1\u8f6c\u5316\u4e3a\u8fc7\u7a0b\u76d1\u7763\u4fe1\u53f7\u3002", "result": "PRL\u4e0d\u4ec5\u63d0\u9ad8\u4e86LLM\u63a8\u7406\u80fd\u529b\u7684\u5e73\u5747\u6027\u80fd\uff08average @ n\uff09\uff0c\u8fd8\u901a\u8fc7\u6539\u8fdbpass @ n\u6307\u6807\u62d3\u5bbd\u4e86\u63a8\u7406\u8fb9\u754c\u3002\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PRL\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PRL\u4e3aLLM\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u3001\u8bad\u7ec3\u9ad8\u6548\u7684\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5e76\u62d3\u5bbd\u63a8\u7406\u80fd\u529b\u8fb9\u754c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10402", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10402", "abs": "https://arxiv.org/abs/2601.10402", "authors": ["Xinyu Zhu", "Yuzhu Cai", "Zexi Liu", "Bingyang Zheng", "Cheng Wang", "Rui Ye", "Jiaao Chen", "Hanrui Wang", "Wei-Chen Wang", "Yuzhi Zhang", "Linfeng Zhang", "Weinan E", "Di Jin", "Siheng Chen"], "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "comment": "26 pages. 5 figures", "summary": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "AI": {"tldr": "ML-Master 2.0\u901a\u8fc7\u5206\u5c42\u8ba4\u77e5\u7f13\u5b58\u67b6\u6784\u89e3\u51b3AI\u5728\u8d85\u957f\u5468\u671f\u81ea\u4e3b\u79d1\u5b66\u63a2\u7d22\u4e2d\u7684\u74f6\u9888\uff0c\u5728\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523056.44%\u7684\u5956\u724c\u7387", "motivation": "\u5f53\u524dAI\u5411\u4ee3\u7406\u79d1\u5b66\u53d1\u5c55\u7684\u4e3b\u8981\u74f6\u9888\u662f\u8d85\u957f\u5468\u671f\u81ea\u4e3b\u6027\u95ee\u9898\uff0c\u5373\u5982\u4f55\u5728\u8de8\u8d8a\u6570\u5929\u6216\u6570\u5468\u7684\u5b9e\u9a8c\u5468\u671f\u4e2d\u4fdd\u6301\u6218\u7565\u8fde\u8d2f\u6027\u548c\u8fed\u4ee3\u4fee\u6b63\u80fd\u529b\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u671f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u7814\u7a76\u7684\u9ad8\u7ef4\u3001\u5ef6\u8fdf\u53cd\u9988\u73af\u5883\u4e2d\u5bb9\u6613\u88ab\u6267\u884c\u7ec6\u8282\u6df9\u6ca1\uff0c\u65e0\u6cd5\u5c06\u7a00\u758f\u53cd\u9988\u6574\u5408\u4e3a\u8fde\u8d2f\u7684\u957f\u671f\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8ba4\u77e5\u7f13\u5b58\uff08HCC\uff09\u67b6\u6784\uff0c\u5c06\u4e0a\u4e0b\u6587\u7ba1\u7406\u91cd\u6784\u4e3a\u8ba4\u77e5\u79ef\u7d2f\u8fc7\u7a0b\u3002\u8fd9\u79cd\u53d7\u8ba1\u7b97\u673a\u7cfb\u7edf\u542f\u53d1\u7684\u591a\u5c42\u67b6\u6784\u80fd\u591f\u5b9e\u73b0\u7ecf\u9a8c\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u7ed3\u6784\u5316\u533a\u5206\uff0c\u901a\u8fc7\u52a8\u6001\u5c06\u77ac\u6001\u6267\u884c\u8f68\u8ff9\u63d0\u70bc\u4e3a\u7a33\u5b9a\u77e5\u8bc6\u548c\u8de8\u4efb\u52a1\u667a\u6167\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5c06\u5373\u65f6\u6267\u884c\u4e0e\u957f\u671f\u5b9e\u9a8c\u7b56\u7565\u89e3\u8026\uff0c\u6709\u6548\u514b\u670d\u9759\u6001\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6269\u5c55\u9650\u5236\u3002", "result": "\u5728OpenAI\u7684MLE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u752824\u5c0f\u65f6\u9884\u7b97\uff0cML-Master 2.0\u5b9e\u73b0\u4e8656.44%\u7684\u6700\u5148\u8fdb\u5956\u724c\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8d85\u957f\u5468\u671f\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8d85\u957f\u5468\u671f\u81ea\u4e3b\u6027\u4e3aAI\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u84dd\u56fe\uff0c\u4f7f\u5176\u80fd\u591f\u8d85\u8d8a\u4eba\u7c7b\u5148\u4f8b\u7684\u590d\u6742\u6027\u8fdb\u884c\u81ea\u4e3b\u63a2\u7d22\u3002\u5206\u5c42\u8ba4\u77e5\u7f13\u5b58\u67b6\u6784\u4e3a\u89e3\u51b3AI\u5728\u957f\u671f\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u74f6\u9888\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.10156", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10156", "abs": "https://arxiv.org/abs/2601.10156", "authors": ["Yutao Mou", "Zhangchi Xue", "Lijun Li", "Peiyang Liu", "Shikun Zhang", "Wei Ye", "Jing Shao"], "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback", "comment": "Work in Progress. Code available: https://github.com/MurrayTom/ToolSafe", "summary": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TS-Bench\u57fa\u51c6\u6d4b\u8bd5\u548cTS-Guard\u9632\u62a4\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4bLLM\u4ee3\u7406\u5de5\u5177\u8c03\u7528\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u901a\u8fc7TS-Flow\u6846\u67b6\u51cf\u5c11\u6709\u5bb3\u5de5\u5177\u8c03\u752865%\uff0c\u63d0\u5347\u826f\u6027\u4efb\u52a1\u5b8c\u6210\u738710%", "motivation": "LLM\u4ee3\u7406\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u80fd\u529b\u589e\u5f3a\uff0c\u4f46\u4e5f\u653e\u5927\u4e86\u5b89\u5168\u98ce\u9669\u3002\u9700\u8981\u5b9e\u65f6\u76d1\u63a7\u6b65\u9aa4\u7ea7\u5de5\u5177\u8c03\u7528\u884c\u4e3a\u5e76\u5728\u4e0d\u5b89\u5168\u6267\u884c\u524d\u4e3b\u52a8\u5e72\u9884\uff0c\u8fd9\u5bf9\u4ee3\u7406\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u7814\u7a76", "method": "1) \u6784\u5efaTS-Bench\u57fa\u51c6\u6d4b\u8bd5\u7528\u4e8e\u6b65\u9aa4\u7ea7\u5de5\u5177\u8c03\u7528\u5b89\u5168\u68c0\u6d4b\uff1b2) \u5f00\u53d1TS-Guard\u9632\u62a4\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u4ea4\u4e92\u5386\u53f2\u4e3b\u52a8\u68c0\u6d4b\u4e0d\u5b89\u5168\u5de5\u5177\u8c03\u7528\uff1b3) \u63d0\u51faTS-Flow\u9632\u62a4\u53cd\u9988\u9a71\u52a8\u7684\u63a8\u7406\u6846\u67b6", "result": "TS-Guard\u80fd\u591f\u8bc4\u4f30\u8bf7\u6c42\u5371\u5bb3\u6027\u548c\u884c\u52a8-\u653b\u51fb\u76f8\u5173\u6027\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5b89\u5168\u5224\u65ad\u548c\u53cd\u9988\u3002TS-Flow\u6846\u67b6\u5c06ReAct\u98ce\u683c\u4ee3\u7406\u7684\u6709\u5bb3\u5de5\u5177\u8c03\u7528\u5e73\u5747\u51cf\u5c1165%\uff0c\u5728\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4e0b\u5c06\u826f\u6027\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u5347\u7ea610%", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u4ee3\u7406\u5de5\u5177\u8c03\u7528\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u76d1\u63a7\u548c\u5e72\u9884\u65b9\u6848\uff0c\u901a\u8fc7\u4e3b\u52a8\u68c0\u6d4b\u548c\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u7387", "topic": "agent analysis"}}
{"id": "2601.10416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10416", "abs": "https://arxiv.org/abs/2601.10416", "authors": ["Tiesunlong Shen", "Rui Mao", "Jin Wang", "Heming Sun", "Jian Zhang", "Xuejie Zhang", "Erik Cambria"], "title": "LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models", "comment": "Accepted by AAAI26", "summary": "Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.", "AI": {"tldr": "LLMdoctor\uff1a\u4e00\u79cd\u57fa\u4e8e\u60a3\u8005-\u533b\u751f\u8303\u5f0f\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6token\u7ea7\u5956\u52b1\u83b7\u53d6\u548c\u6d41\u5f15\u5bfc\u504f\u597d\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u7684\u540c\u65f6\u9ad8\u6548\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u7075\u6d3b\uff0c\u73b0\u6709\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u626d\u66f2\u7684\u8f68\u8ff9\u7ea7\u4fe1\u53f7\u6216\u4f4e\u6548\u91c7\u6837\uff0c\u6027\u80fd\u53d7\u9650\u4e14\u96be\u4ee5\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u7684\u751f\u6210\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u60a3\u8005-\u533b\u751f\u8303\u5f0f\uff0c\u4ece\u60a3\u8005LLM\u7684\u884c\u4e3a\u53d8\u5316\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6token\u7ea7\u504f\u597d\u4fe1\u53f7\uff0c\u901a\u8fc7token\u7ea7\u6d41\u5f15\u5bfc\u504f\u597d\u4f18\u5316\uff08TFPO\uff09\u8bad\u7ec3\u8f83\u5c0f\u7684\u533b\u751f\u6a21\u578b\u6765\u5f15\u5bfc\u51bb\u7ed3\u7684\u60a3\u8005\u6a21\u578b\uff0c\u786e\u4fdd\u6240\u6709\u5b50\u8f68\u8ff9\u7684\u6d41\u4e00\u81f4\u6027\u3002", "result": "LLMdoctor\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86DPO\u7b49\u5b8c\u6574\u5fae\u8c03\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "LLMdoctor\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u751f\u6210\u591a\u6837\u6027\u7684\u540c\u65f6\u5b9e\u73b0token\u7ea7\u5bf9\u9f50\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10274", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NI", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.10274", "abs": "https://arxiv.org/abs/2601.10274", "authors": ["Emre Ozbas", "Melih Bastopcu"], "title": "Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers", "comment": null, "summary": "We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u670d\u52a1\u5668\u4e2d\u4efb\u52a1\u7c7b\u578b\u7684token\u5206\u914d\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u8861\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\uff0c\u5728\u961f\u5217\u7a33\u5b9a\u6027\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u52a0\u6743\u51c6\u786e\u7387\u76ee\u6807\u3002", "motivation": "LLM\u670d\u52a1\u5668\u9700\u8981\u5904\u7406\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u7684\u67e5\u8be2\u6d41\uff0c\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u7684\u8ba1\u7b97\u8d44\u6e90\uff08thinking tokens\uff09\u3002\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u66f4\u591atoken\u63d0\u9ad8\u51c6\u786e\u7387\u4f46\u589e\u52a0\u670d\u52a1\u65f6\u95f4\u3002\u9700\u8981\u4f18\u5316token\u5206\u914d\u4ee5\u5728\u961f\u5217\u7a33\u5b9a\u6027\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5c06\u7cfb\u7edf\u5efa\u6a21\u4e3aM/G/1\u961f\u5217\uff0c\u670d\u52a1\u65f6\u95f4\u4e0e\u5206\u914d\u7684token\u6570\u8fd1\u4f3c\u7ebf\u6027\u76f8\u5173\u3002\u5efa\u7acb\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5927\u5316\u52a0\u6743\u5e73\u5747\u51c6\u786e\u7387\uff08\u60e9\u7f5a\u5e73\u5747\u7cfb\u7edf\u65f6\u95f4\uff09\uff0c\u53d7\u9650\u4e8etoken\u9884\u7b97\u548c\u961f\u5217\u7a33\u5b9a\u6027\u6761\u4ef6\u3002\u8bc1\u660e\u76ee\u6807\u51fd\u6570\u5728\u7a33\u5b9a\u533a\u57df\u5185\u4e25\u683c\u51f9\uff0c\u786e\u4fdd\u6700\u4f18\u89e3\u5b58\u5728\u552f\u4e00\u3002\u901a\u8fc7\u4e00\u9636\u6700\u4f18\u6761\u4ef6\u5f97\u5230\u8026\u5408\u6295\u5f71\u5b9a\u70b9\u7279\u6027\uff0c\u5f00\u53d1\u8fed\u4ee3\u89e3\u6cd5\u548c\u6295\u5f71\u68af\u5ea6\u6cd5\u4fdd\u8bc1\u6536\u655b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u76ee\u6807\u51fd\u6570\u4e25\u683c\u51f9\uff0c\u6700\u4f18token\u5206\u914d\u5b58\u5728\u552f\u4e00\u3002\u63d0\u51fa\u8fed\u4ee3\u89e3\u6cd5\u3001\u6536\u7f29\u6761\u4ef6\u548c\u6295\u5f71\u68af\u5ea6\u6cd5\u4fdd\u8bc1\u6536\u655b\u3002\u901a\u8fc7\u8fde\u7eed\u89e3\u820d\u5165\u5f97\u5230\u6574\u6570token\u5206\u914d\uff0c\u4eff\u771f\u8bc4\u4f30\u6027\u80fd\u635f\u5931\u3002", "conclusion": "\u672c\u6587\u4e3aLLM\u670d\u52a1\u5668\u4e2d\u5f02\u6784\u4efb\u52a1\u7c7b\u578b\u7684token\u5206\u914d\u63d0\u4f9b\u4e86\u7cfb\u7edf\u4f18\u5316\u6846\u67b6\uff0c\u5e73\u8861\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\uff0c\u5728\u961f\u5217\u7a33\u5b9a\u6027\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.10187", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10187", "abs": "https://arxiv.org/abs/2601.10187", "authors": ["Ziang Cui", "Mengran Yu", "Tianjiao Li", "Chenyu Shi", "Yingxuan Shi", "Lusheng Zhang", "Hongwei Lin"], "title": "HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively \"tames\" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHOMURA\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3LLM\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u7684\u8de8\u8bed\u8a00\u5197\u957f\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u97f3\u8282\u7ea7\u65f6\u957f\u7ea6\u675f\u4f18\u5316\u7ffb\u8bd1\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u53ef\u884c\u6027\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u8de8\u8bed\u8a00\u5197\u957f\u504f\u5dee\uff0c\u4e0d\u9002\u5408\u5b57\u5e55\u3001\u914d\u97f3\u7b49\u4e25\u683c\u65f6\u95f4\u7ea6\u675f\u4efb\u52a1\u3002\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u53ef\u884c\u6027\u3002", "method": "1. \u5f15\u5165Sand-Glass\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30\u97f3\u8282\u7ea7\u65f6\u957f\u7ea6\u675f\u4e0b\u7684\u7ffb\u8bd1\u8d28\u91cf\uff1b2. \u63d0\u51faHOMURA\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528KL\u6b63\u5219\u5316\u76ee\u6807\u548c\u65b0\u578b\u52a8\u6001\u97f3\u8282\u6bd4\u4f8b\u5956\u52b1\uff0c\u660e\u786e\u4f18\u5316\u8bed\u4e49\u4fdd\u7559\u548c\u65f6\u95f4\u5408\u89c4\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684LLM\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u957f\u5ea6\u63a7\u5236\uff0c\u5728\u5c0a\u91cd\u8bed\u8a00\u5bc6\u5ea6\u5c42\u6b21\u7684\u540c\u65f6\u4e0d\u635f\u5bb3\u8bed\u4e49\u5145\u5206\u6027\u3002", "conclusion": "HOMURA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u7ffb\u8bd1\u4e2d\u7684\u5197\u957f\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u578b\u7ffb\u8bd1\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u7ea6\u675f\u8981\u6c42\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10520", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.10520", "abs": "https://arxiv.org/abs/2601.10520", "authors": ["Felix Jahn", "Yannic Muskalla", "Lisa Dargasz", "Patrick Schramowski", "Kevin Baum"], "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment", "comment": "10 pages, 4 figures, accepted at 2nd Annual Conference of the International Association for Safe & Ethical AI (IASEAI'26)", "summary": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.", "AI": {"tldr": "GRACE\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u7684\u57fa\u4e8e\u539f\u56e0\u7684\u7ea6\u675f\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u89c4\u8303\u6027\u63a8\u7406\u4e0e\u5de5\u5177\u6027\u51b3\u7b56\u89e3\u8026\uff0c\u5305\u542b\u4efb\u4f55\u8bbe\u8ba1\u7684AI\u4ee3\u7406\uff0c\u786e\u4fdd\u5176\u51b3\u7b56\u65e2\u6709\u6548\u53c8\u7b26\u5408\u89c4\u8303\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u91cd\u8981\u573a\u666f\u4e2d\u81ea\u4e3b\u90e8\u7f72\u5e76\u4ea7\u751f\u5b9e\u9645\u5f71\u54cd\uff0c\u786e\u4fdd\u5176\u51b3\u7b56\u4e0d\u4ec5\u5de5\u5177\u6709\u6548\u800c\u4e14\u89c4\u8303\u5bf9\u9f50\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ea6\u675f\u4efb\u4f55\u8bbe\u8ba1AI\u4ee3\u7406\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faGRACE\u67b6\u6784\uff0c\u5c06\u51b3\u7b56\u5206\u4e3a\u4e09\u4e2a\u6a21\u5757\uff1a\u9053\u5fb7\u6a21\u5757\uff08\u4f7f\u7528\u9053\u4e49\u903b\u8f91\u63a8\u7406\u786e\u5b9a\u5141\u8bb8\u7684\u5b8f\u89c2\u884c\u52a8\uff09\u3001\u51b3\u7b56\u6a21\u5757\uff08\u5c01\u88c5\u76ee\u6807\u4ee3\u7406\u9009\u62e9\u5de5\u5177\u6700\u4f18\u7684\u539f\u59cb\u884c\u52a8\uff09\u3001\u5b88\u536b\u6a21\u5757\uff08\u76d1\u63a7\u5e76\u5f3a\u5236\u6267\u884c\u9053\u5fb7\u5408\u89c4\uff09\u3002\u9053\u5fb7\u6a21\u5757\u91c7\u7528\u57fa\u4e8e\u539f\u56e0\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u4e89\u8bae\u6027\u548c\u53ef\u8fa9\u62a4\u6027\u3002", "result": "\u5728LLM\u6cbb\u7597\u52a9\u624b\u793a\u4f8b\u4e2d\u5c55\u793a\u4e86GRACE\u5982\u4f55\u4f7f\u5229\u76ca\u76f8\u5173\u8005\u7406\u89e3\u3001\u8d28\u7591\u548c\u4f18\u5316\u4ee3\u7406\u884c\u4e3a\u3002\u8be5\u67b6\u6784\u652f\u6301\u5f62\u5f0f\u9a8c\u8bc1\u548c\u7edf\u8ba1\u4fdd\u8bc1\u3002", "conclusion": "GRACE\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u7684\u57fa\u4e8e\u539f\u56e0\u7ea6\u675f\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86AI\u4ee3\u7406\u7684\u89c4\u8303\u6027\u5bf9\u9f50\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u5f62\u5f0f\u9a8c\u8bc1\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.10349", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.10349", "abs": "https://arxiv.org/abs/2601.10349", "authors": ["Mark Kashirskiy", "Ilya Makarov"], "title": "SuS: Strategy-aware Surprise for Intrinsic Exploration", "comment": "8 pages, 7 figures, 3 tables. Code available at https://github.com/mariklolik/sus", "summary": "We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.", "AI": {"tldr": "\u63d0\u51faStrategy-aware Surprise (SuS)\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u7a33\u5b9a\u6027\u4e0e\u7b56\u7565\u60ca\u559c\u5ea6\u4f5c\u4e3a\u5185\u5728\u52a8\u673a\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6548\u7387\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u597d\u5947\u5fc3\u9a71\u52a8\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u72b6\u6001\u9884\u6d4b\u8bef\u5dee\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u884c\u4e3a\u7b56\u7565\u5c42\u9762\u7684\u65b0\u9896\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8861\u91cf\u7b56\u7565\u4e00\u81f4\u6027\u548c\u7b56\u7565\u76f8\u5173\u610f\u5916\u7ed3\u679c\u7684\u63a2\u7d22\u673a\u5236\u3002", "method": "\u63d0\u51faSuS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a\u7b56\u7565\u7a33\u5b9a\u6027(SS)\u8861\u91cf\u65f6\u95f4\u6b65\u95f4\u884c\u4e3a\u7b56\u7565\u7684\u4e00\u81f4\u6027\uff1b\u7b56\u7565\u60ca\u559c\u5ea6(SuS)\u6355\u6349\u76f8\u5bf9\u4e8e\u5f53\u524d\u7b56\u7565\u8868\u793a\u7684\u610f\u5916\u7ed3\u679c\u3002\u901a\u8fc7\u5b66\u4e60\u7684\u6743\u91cd\u7cfb\u6570\u7ed3\u5408\u4e24\u79cd\u4fe1\u53f7\u5f62\u6210\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cSuS\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728Pass@1\u4e0a\u63d0\u534717.4%\uff0c\u5728Pass@5\u4e0a\u63d0\u534726.4%\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u7b56\u7565\u591a\u6837\u6027\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u79fb\u9664\u4efb\u4e00\u7ec4\u4ef6\u4f1a\u5bfc\u81f4\u81f3\u5c1110%\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "SuS\u6846\u67b6\u901a\u8fc7\u7b56\u7565\u5c42\u9762\u7684\u65b0\u9896\u6027\u4fe1\u53f7\u6709\u6548\u63d0\u5347\u63a2\u7d22\u6548\u7387\uff0c\u4e24\u4e2a\u7ec4\u4ef6\u7684\u534f\u540c\u4f5c\u7528\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5185\u5728\u52a8\u673a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10257", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10257", "abs": "https://arxiv.org/abs/2601.10257", "authors": ["Nan Li", "Bo Kang", "Tijl De Bie"], "title": "Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs", "comment": null, "summary": "When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \\emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u8bed\u8a00\u5bf9LLM\u9053\u5fb7\u5224\u65ad\u5f71\u54cd\u7684\u65b9\u6cd5\u8bba\uff0c\u533a\u5206\u56f0\u5883\u8bed\u8a00\u548c\u63a8\u7406\u8bed\u8a00\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u63a8\u7406\u8bed\u8a00\u7684\u5f71\u54cd\u662f\u8f93\u5165\u8bed\u8a00\u7684\u4e24\u500d\uff0c\u5e76\u57fa\u4e8e\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u8fdb\u884c\u89e3\u91ca\u3002", "motivation": "\u5f53LLM\u5224\u65ad\u9053\u5fb7\u56f0\u5883\u65f6\uff0c\u4e0d\u540c\u8bed\u8a00\u662f\u5426\u4f1a\u5bfc\u81f4\u4e0d\u540c\u7ed3\u8bba\uff1f\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c06\u56f0\u5883\u8bed\u8a00\u548c\u63a8\u7406\u8bed\u8a00\u6df7\u4e3a\u4e00\u8c08\uff0c\u65e0\u6cd5\u533a\u5206\u4e24\u8005\u7684\u72ec\u7acb\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5206\u89e3\u8fd9\u4e24\u4e2a\u56e0\u7d20\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u79bb\u64cd\u7eb5\u56f0\u5883\u8bed\u8a00\u548c\u63a8\u7406\u8bed\u8a00\u7684\u65b9\u6cd5\u8bba\uff0c\u5305\u62ec\u5339\u914d\u6761\u4ef6\uff08\u5982\u82f1\u8bed\u56f0\u5883+\u82f1\u8bed\u63a8\u7406\uff09\u548c\u4e0d\u5339\u914d\u6761\u4ef6\uff08\u5982\u82f1\u8bed\u56f0\u5883+\u4e2d\u6587\u63a8\u7406\uff09\u3002\u57fa\u4e8e\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u89e3\u91ca\u9053\u5fb7\u5224\u65ad\uff0c\u5e76\u8bc6\u522b\u6743\u5a01\u7ef4\u5ea6\u7684\u7ec6\u5206\u3002", "result": "\u572813\u4e2aLLM\u7684\u82f1\u4e2d\u9053\u5fb7\u5224\u65ad\u5b9e\u9a8c\u4e2d\uff1a(1)\u63a8\u7406\u8bed\u8a00\u7684\u5f71\u54cd\u65b9\u5dee\u662f\u8f93\u5165\u8bed\u8a00\u7684\u4e24\u500d\uff1b(2)\u6846\u67b6\u68c0\u6d4b\u5230\u8fd1\u534a\u6570\u6a21\u578b\u5b58\u5728\u6807\u51c6\u8bc4\u4f30\u9057\u6f0f\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff1b(3)\u8bca\u65ad\u5206\u7c7b\u6cd5\u5c06\u8fd9\u4e9b\u6a21\u5f0f\u8f6c\u5316\u4e3a\u90e8\u7f72\u6307\u5bfc\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u8bba\u80fd\u591f\u6709\u6548\u5206\u89e3\u8bed\u8a00\u5bf9LLM\u9053\u5fb7\u5224\u65ad\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u8bed\u8a00\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u8de8\u6587\u5316\u9053\u5fb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u548c\u90e8\u7f72\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.10407", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10407", "abs": "https://arxiv.org/abs/2601.10407", "authors": ["Yuanjie Zhao", "Junnan Qiu", "Yue Ding", "Jie Li"], "title": "CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning", "comment": null, "summary": "Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network's gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent's performance in clean environments.", "AI": {"tldr": "\u63d0\u51faCS-GBA\u653b\u51fb\u6846\u67b6\uff0c\u9488\u5bf9\u5b89\u5168\u7ea6\u675f\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u6837\u672c\u9009\u62e9\u3001\u76f8\u5173\u6027\u7834\u574f\u89e6\u53d1\u5668\u548c\u68af\u5ea6\u5f15\u5bfc\u52a8\u4f5c\u751f\u6210\uff0c\u57285%\u6c61\u67d3\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u9690\u853d\u6027\u548c\u7834\u574f\u6027\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u7b56\u7565\u96be\u4ee5\u5bf9\u6297\u5b89\u5168\u7ea6\u675f\u7b97\u6cd5\uff08\u5982CQL\uff09\uff0c\u56e0\u4e3a\u968f\u673a\u6c61\u67d3\u6548\u7387\u4f4e\u4e14\u4f7f\u7528\u6613\u68c0\u6d4b\u7684\u5206\u5e03\u5916\u89e6\u53d1\u5668\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u9690\u853d\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "1) \u81ea\u9002\u5e94\u5173\u952e\u6837\u672c\u9009\u62e9\uff1a\u57fa\u4e8eTD\u8bef\u5dee\u8bc6\u522b\u5bf9\u4ef7\u503c\u51fd\u6570\u6536\u655b\u5173\u952e\u7684\u72b6\u6001\u8f6c\u79fb\uff1b2) \u76f8\u5173\u6027\u7834\u574f\u89e6\u53d1\u5668\uff1a\u5229\u7528\u72b6\u6001\u7279\u5f81\u7684\u7269\u7406\u4e92\u65a5\u6027\uff08\u598295%\u5206\u4f4d\u6570\u8fb9\u754c\uff09\u4fdd\u6301\u7edf\u8ba1\u9690\u853d\u6027\uff1b3) \u68af\u5ea6\u5f15\u5bfc\u52a8\u4f5c\u751f\u6210\uff1a\u4f7f\u7528\u53d7\u5bb3\u8005Q\u7f51\u7edc\u7684\u68af\u5ea6\u5728\u6570\u636e\u6d41\u5f62\u4e2d\u641c\u7d22\u6700\u5dee\u52a8\u4f5c\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCS-GBA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4ec55%\u6c61\u67d3\u9884\u7b97\u4e0b\u5bf9\u4ee3\u8868\u6027\u5b89\u5168\u7ea6\u675f\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u73af\u5883\u4e2d\u4fdd\u6301\u667a\u80fd\u4f53\u6027\u80fd\u3002", "conclusion": "CS-GBA\u6846\u67b6\u901a\u8fc7\u5173\u952e\u6837\u672c\u9009\u62e9\u3001\u9690\u853d\u89e6\u53d1\u5668\u8bbe\u8ba1\u548c\u68af\u5ea6\u5f15\u5bfc\u52a8\u4f5c\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5b89\u5168\u7ea6\u675f\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u9ad8\u6548\u540e\u95e8\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5b89\u5168\u673a\u5236\u7684\u8106\u5f31\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10418", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.10418", "abs": "https://arxiv.org/abs/2601.10418", "authors": ["Nadav Merlis"], "title": "Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching", "comment": null, "summary": "We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\\ell$, which can usually be considered a small constant.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5177\u6709\u591a\u6b65\u524d\u77bb\u4fe1\u606f\u7684\u8868\u683c\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u6279\u5904\u7406\u7b56\u7565\uff08ABPs\uff09\u6765\u89e3\u51b3\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e50\u89c2\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u6765\u5b66\u4e60\u6700\u4f18ABP\u3002", "motivation": "\u73b0\u6709\u5904\u7406\u591a\u6b65\u524d\u77bb\u4fe1\u606f\u7684\u65b9\u6cd5\uff08\u56fa\u5b9a\u6279\u5904\u7406\u7b56\u7565\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff09\u5b58\u5728\u660e\u663e\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7b56\u7565\u6765\u5145\u5206\u5229\u7528\u524d\u77bb\u4fe1\u606f\u63d0\u5347\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6279\u5904\u7406\u7b56\u7565\uff08ABPs\uff09\uff0c\u6839\u636e\u72b6\u6001\u81ea\u9002\u5e94\u5730\u5212\u5206\u524d\u77bb\u4fe1\u606f\u6279\u6b21\uff1b\u63a8\u5bfc\u6700\u4f18Bellman\u65b9\u7a0b\uff1b\u8bbe\u8ba1\u4e50\u89c2\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u6765\u5b66\u4e60\u672a\u77e5\u73af\u5883\u4e2d\u7684\u6700\u4f18ABP\u3002", "result": "\u83b7\u5f97\u4e86\u9636\u6700\u4f18\u7684\u540e\u6094\u754c\uff08\u6700\u591a\u76f8\u5dee\u524d\u77bb\u89c6\u91ce\u2113\u7684\u56e0\u5b50\uff09\uff0c\u901a\u5e38\u2113\u53ef\u89c6\u4e3a\u5c0f\u5e38\u6570\uff0c\u8868\u660e\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u81ea\u9002\u5e94\u6279\u5904\u7406\u7b56\u7565\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u524d\u77bb\u4fe1\u606f\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b66\u4e60\u6700\u4f18ABP\uff0c\u4e3a\u591a\u6b65\u524d\u77bb\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10471", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10471", "abs": "https://arxiv.org/abs/2601.10471", "authors": ["Zhancun Mu"], "title": "DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction", "comment": "13 pages, 3 figures", "summary": "We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.", "AI": {"tldr": "DeFlow\u662f\u4e00\u4e2a\u89e3\u8026\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6d41\u5339\u914d\u6280\u672f\u6355\u6349\u590d\u6742\u884c\u4e3a\u6d41\u5f62\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7cbe\u70bc\u6a21\u5757\u5728\u6570\u636e\u9a71\u52a8\u7684\u4fe1\u4efb\u533a\u57df\u5185\u5b66\u4e60\uff0c\u907f\u514d\u4e86ODE\u6c42\u89e3\u5668\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u8fed\u4ee3\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u7b56\u7565\u4f18\u5316\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u901a\u5e38\u9700\u8981\u901a\u8fc7ODE\u6c42\u89e3\u5668\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u8981\u4e48\u901a\u8fc7\u5355\u6b65\u84b8\u998f\u727a\u7272\u8fed\u4ee3\u751f\u6210\u80fd\u529b\uff0c\u9700\u8981\u5728\u4fdd\u6301\u6d41\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u7684\u79bb\u7ebfRL\u6846\u67b6\uff0c\u4f7f\u7528\u6d41\u5339\u914d\u6355\u6349\u884c\u4e3a\u6d41\u5f62\u3002\u5728\u6d41\u6d41\u5f62\u7684\u663e\u5f0f\u3001\u6570\u636e\u9a71\u52a8\u7684\u4fe1\u4efb\u533a\u57df\u5185\u5b66\u4e60\u8f7b\u91cf\u7ea7\u7cbe\u70bc\u6a21\u5757\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u5355\u6b65\u84b8\u998f\u727a\u7272\u8fed\u4ee3\u751f\u6210\u80fd\u529b\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u6c42\u89e3\u5668\u5fae\u5206\uff0c\u65e0\u9700\u5e73\u8861\u635f\u5931\u9879\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684OGBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "DeFlow\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u6d41\u5339\u914d\u6280\u672f\uff0c\u5728\u4fdd\u6301\u6d41\u6a21\u578b\u8fed\u4ee3\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6539\u8fdb\uff0c\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.10318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10318", "abs": "https://arxiv.org/abs/2601.10318", "authors": ["Songsong Tian", "Kongsheng Zhuo", "Zhendong Wang", "Rong Shen", "Shengtao Zhang", "Yong Wu"], "title": "Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis", "comment": null, "summary": "In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.", "AI": {"tldr": "BAR-SQL\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u548c\u53ef\u9760\u6027\u5d4c\u5165\u63d0\u5347NL2SQL\u6027\u80fd\uff0c\u4f7f\u7528\u79cd\u5b50\u7a81\u53d8\u6570\u636e\u5408\u6210\u548c\u77e5\u8bc6\u57fa\u7840\u63a8\u7406\u5408\u6210\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u4efb\u52a1\u6761\u4ef6\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u5728\u81ea\u5efa\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709NL2SQL\u7cfb\u7edf\u5728\u5904\u7406\u4f01\u4e1a\u7ea7\u590d\u6742\u67e5\u8be2\u65f6\u9762\u4e34\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fb9\u754c\u60c5\u51b5\uff08\u5982\u6a21\u7cca\u67e5\u8be2\u548c\u6a21\u5f0f\u9650\u5236\uff09\u4e0b\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7ed3\u679c\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4fdd\u8bc1SQL\u751f\u6210\u51c6\u786e\u6027\u548c\u8fb9\u754c\u611f\u77e5\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "1. \u79cd\u5b50\u7a81\u53d8\u6570\u636e\u5408\u6210\u8303\u5f0f\u6784\u5efa\u4f01\u4e1a\u7ea7\u8bed\u6599\u5e93\uff0c\u5305\u542b\u591a\u6b65\u5206\u6790\u67e5\u8be2\u548c\u8fb9\u754c\u60c5\u51b5\uff1b2. \u77e5\u8bc6\u57fa\u7840\u63a8\u7406\u5408\u6210\u751f\u6210\u57fa\u4e8e\u6a21\u5f0f\u5143\u6570\u636e\u548c\u4e1a\u52a1\u89c4\u5219\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\uff1b3. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03+\u57fa\u4e8e\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\uff1b4. \u4efb\u52a1\u6761\u4ef6\u6df7\u5408\u5956\u52b1\u673a\u5236\u540c\u65f6\u4f18\u5316SQL\u6267\u884c\u51c6\u786e\u6027\u548c\u8bed\u4e49\u7cbe\u5ea6\u3002", "result": "\u5728\u81ea\u5efa\u57fa\u51c6Ent-SQL-Bench\u4e0a\uff0cBAR-SQL\u8fbe\u523091.48%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728SQL\u751f\u6210\u8d28\u91cf\u548c\u8fb9\u754c\u611f\u77e5\u5f03\u6743\u80fd\u529b\u65b9\u9762\u5747\u8d85\u8d8aClaude 4.5 Sonnet\u548cGPT-5\u7b49\u4e3b\u6d41\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "BAR-SQL\u901a\u8fc7\u5c06\u53ef\u9760\u6027\u548c\u8fb9\u754c\u611f\u77e5\u76f4\u63a5\u5d4c\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86NL2SQL\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6a21\u7cca\u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u65f6\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2601.10343", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10343", "abs": "https://arxiv.org/abs/2601.10343", "authors": ["Deming Ding", "Shichun Liu", "Enhui Yang", "Jiahang Lin", "Ziying Chen", "Shihan Dou", "Honglin Guo", "Weiyu Cheng", "Pengyu Zhao", "Chengjun Xiao", "Qunhong Zeng", "Qi Zhang", "Xuanjing Huang", "Qidi Xu", "Tao Gui"], "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding", "comment": null, "summary": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.", "AI": {"tldr": "OctoBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7801\u667a\u80fd\u4f53\u5728\u4ed3\u5e93\u73af\u5883\u4e2d\u9075\u5faa\u811a\u624b\u67b6\u6307\u4ee4\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b34\u4e2a\u73af\u5883\u3001217\u4e2a\u4efb\u52a1\u548c7,098\u4e2a\u68c0\u67e5\u9879\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u89e3\u51b3\u4e0e\u811a\u624b\u67b6\u9075\u5faa\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u8ddd\u3002", "motivation": "\u73b0\u4ee3\u4ee3\u7801\u811a\u624b\u67b6\u5c06LLM\u8f6c\u53d8\u4e3a\u5f3a\u5927\u7684\u8f6f\u4ef6\u667a\u80fd\u4f53\uff0c\u4f46\u5b83\u4eec\u5728\u9075\u5faa\u811a\u624b\u67b6\u6307\u5b9a\u6307\u4ee4\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u68c0\u9a8c\uff0c\u5c24\u5176\u662f\u5728\u7ea6\u675f\u6761\u4ef6\u5f02\u6784\u4e14\u8de8\u4ea4\u4e92\u6301\u7eed\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5f15\u5165OctoBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b34\u4e2a\u73af\u5883\u548c217\u4e2a\u4efb\u52a1\uff0c\u6db5\u76d6\u4e09\u79cd\u811a\u624b\u67b6\u7c7b\u578b\uff0c\u914d\u59077,098\u4e2a\u5ba2\u89c2\u68c0\u67e5\u9879\u3002\u63d0\u4f9b\u81ea\u52a8\u5316\u89c2\u5bdf\u548c\u8bc4\u5206\u5de5\u5177\u5305\uff0c\u6355\u83b7\u5b8c\u6574\u8f68\u8ff9\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u68c0\u67e5\uff0c\u4ee5\u533a\u5206\u4efb\u52a1\u89e3\u51b3\u548c\u89c4\u5219\u9075\u5faa\u3002", "result": "\u5bf9\u516b\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u4efb\u52a1\u89e3\u51b3\u4e0e\u811a\u624b\u67b6\u611f\u77e5\u5408\u89c4\u6027\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u9488\u5bf9\u5f02\u6784\u6307\u4ee4\u9075\u5faa\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5173\u6ce8\u811a\u624b\u67b6\u611f\u77e5\u7684\u4ee3\u7801\u667a\u80fd\u4f53\uff0cOctoBench\u7684\u53d1\u5e03\u65e8\u5728\u652f\u6301\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e76\u52a0\u901f\u8fd9\u7c7b\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2601.10421", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10421", "abs": "https://arxiv.org/abs/2601.10421", "authors": ["Philip Resnik"], "title": "Are Language Models Models?", "comment": "5 pages. This is an invited commentary under review at Behavioral and Brain Sciences", "summary": "Futrell and Mahowald claim LMs \"serve as model systems\", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.", "AI": {"tldr": "\u8bba\u6587\u6279\u5224\u6027\u5730\u8bc4\u4f30\u4e86\u5c06\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u6a21\u578b\u7cfb\u7edf\u7684\u8bf4\u6cd5\uff0c\u8ba4\u4e3a\u8fd9\u79cd\u4e3b\u5f20\u5728\u5b9e\u73b0\u5c42\u9762\u4e0d\u6210\u7acb\uff0c\u5728\u7b97\u6cd5\u8868\u793a\u5c42\u9762\u52a8\u673a\u4e0d\u8db3\uff0c\u5728\u8ba1\u7b97\u7406\u8bba\u5c42\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5efa\u8bae\u5c06\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u5de5\u5177\u800c\u975e\u8ba4\u77e5\u6a21\u578b\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u56de\u5e94Futrell\u548cMahowald\u63d0\u51fa\u7684\"\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u6a21\u578b\u7cfb\u7edf\"\u7684\u4e3b\u5f20\uff0c\u901a\u8fc7Marr\u7684\u4e09\u4e2a\u5206\u6790\u5c42\u6b21\uff08\u8ba1\u7b97\u7406\u8bba\u3001\u7b97\u6cd5\u8868\u793a\u3001\u5b9e\u73b0\uff09\u6765\u7cfb\u7edf\u8bc4\u4f30\u8fd9\u4e00\u8bf4\u6cd5\u7684\u5408\u7406\u6027\uff0c\u9632\u6b62\u5bf9\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u8fc7\u5ea6\u7092\u4f5c\u3002", "method": "\u91c7\u7528David Marr\u63d0\u51fa\u7684\u8ba4\u77e5\u79d1\u5b66\u4e09\u5c42\u5206\u6790\u6846\u67b6\uff1a1) \u8ba1\u7b97\u7406\u8bba\u5c42\u9762\uff08\u8ba1\u7b97\u76ee\u6807\uff09\uff0c2) \u7b97\u6cd5\u8868\u793a\u5c42\u9762\uff08\u4fe1\u606f\u5904\u7406\u673a\u5236\uff09\uff0c3) \u5b9e\u73b0\u5c42\u9762\uff08\u7269\u7406\u5b9e\u73b0\uff09\u3002\u4f5c\u8005\u5728\u8fd9\u4e09\u4e2a\u5c42\u6b21\u4e0a\u5206\u522b\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u6a21\u578b\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "result": "\u5206\u6790\u663e\u793a\uff1a\u5728\u5b9e\u73b0\u5c42\u9762\uff0c\u8bed\u8a00\u6a21\u578b\u660e\u663e\u4e0d\u80fd\u4f5c\u4e3a\u8ba4\u77e5\u6a21\u578b\uff1b\u5728\u7b97\u6cd5\u8868\u793a\u5c42\u9762\uff0c\u8fd9\u79cd\u4e3b\u5f20\u7684\u52a8\u673a\u4e0d\u8db3\uff1b\u5728\u8ba1\u7b97\u7406\u8bba\u5c42\u9762\uff0c\u8be5\u4e3b\u5f20\u5b58\u5728\u95ee\u9898\u3002\u8bed\u8a00\u6a21\u578b\u66f4\u9002\u5408\u4f5c\u4e3a\u5de5\u5177\u4f7f\u7528\uff0c\u5c06\u5176\u79f0\u4e3a\u8ba4\u77e5\u6a21\u578b\u4f1a\u5938\u5927\u5176\u80fd\u529b\u5e76\u52a9\u957f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u7092\u4f5c\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5e94\u88ab\u89c6\u4e3a\u6709\u7528\u7684\u5de5\u5177\uff0c\u800c\u975e\u8ba4\u77e5\u6a21\u578b\u3002\u5c06\u5176\u79f0\u4e3a\u8ba4\u77e5\u6a21\u578b\u4e0d\u4ec5\u5938\u5927\u4e86\u5176\u5b9e\u9645\u80fd\u529b\uff0c\u8fd8\u4f1a\u4e0d\u5fc5\u8981\u5730\u52a9\u957f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7092\u4f5c\u3002\u7814\u7a76\u754c\u5e94\u66f4\u8c28\u614e\u5730\u4f7f\u7528\u672f\u8bed\uff0c\u907f\u514d\u8fc7\u5ea6\u89e3\u8bfb\u8bed\u8a00\u6a21\u578b\u7684\u8ba4\u77e5\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2601.10460", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.10460", "abs": "https://arxiv.org/abs/2601.10460", "authors": ["Abhinaba Basu", "Pavan Chakraborty"], "title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models", "comment": null, "summary": "A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.\n  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.\n  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.\n  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faContextual StereoSet\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u523b\u677f\u5370\u8c61\u5185\u5bb9\u4f46\u7cfb\u7edf\u6539\u53d8\u4e0a\u4e0b\u6587\u6846\u67b6\uff08\u65f6\u95f4\u3001\u5730\u70b9\u3001\u53d7\u4f17\u7b49\uff09\uff0c\u53d1\u73b0\u6a21\u578b\u504f\u89c1\u6d4b\u91cf\u7ed3\u679c\u4f1a\u968f\u4e0a\u4e0b\u6587\u53d8\u5316\u800c\u5267\u70c8\u6ce2\u52a8\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u56fa\u5b9a\u6761\u4ef6\u504f\u89c1\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u5728\u56fa\u5b9a\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u8fdb\u884c\uff0c\u4f46\u6a21\u578b\u5728\u5b9e\u9a8c\u5ba4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u907f\u514d\u523b\u677f\u5370\u8c61\uff0c\u5e76\u4e0d\u4ee3\u8868\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u4e5f\u80fd\u907f\u514d\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u504f\u89c1\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u6846\u67b6\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faContextual StereoSet\u57fa\u51c6\u6d4b\u8bd5\uff0c\u56fa\u5b9a\u523b\u677f\u5370\u8c61\u5185\u5bb9\u4f46\u7cfb\u7edf\u6539\u53d8\u4e0a\u4e0b\u6587\u6846\u67b6\uff08\u65f6\u95f4\u3001\u5730\u70b9\u3001\u53d7\u4f17\u7b49\uff09\u3002\u5f15\u5165Context Sensitivity Fingerprints\uff08CSF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bcf\u7ef4\u5ea6\u79bb\u6563\u5ea6\u548c\u914d\u5bf9\u5bf9\u6bd4\u5206\u6790\u6a21\u578b\u504f\u89c1\u654f\u611f\u6027\u3002\u63d0\u4f9b\u4e24\u79cd\u8bc4\u4f30\u65b9\u6848\uff1a360\u4e0a\u4e0b\u6587\u8bca\u65ad\u7f51\u683c\u7528\u4e8e\u6df1\u5ea6\u5206\u6790\uff0c\u9884\u7b97\u534f\u8bae\u8986\u76d64,229\u9879\u7528\u4e8e\u751f\u4ea7\u7b5b\u9009\u3002", "result": "\u6d4b\u8bd513\u4e2a\u6a21\u578b\u53d1\u73b0\u663e\u8457\u6a21\u5f0f\uff1a\u951a\u5b9a\u52301990\u5e74\uff08vs. 2030\u5e74\uff09\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\u63d0\u9ad8\u523b\u677f\u5370\u8c61\u9009\u62e9\uff08p<0.05\uff09\uff1b\u516b\u5366\u6846\u67b6\u57286\u4e2a\u5168\u7f51\u683c\u6a21\u578b\u4e2d\u76845\u4e2a\u63d0\u9ad8\u504f\u89c1\uff1b\u5916\u7fa4\u4f53\u89c2\u5bdf\u8005\u6846\u67b6\u4f7f\u504f\u89c1\u53d8\u5316\u9ad8\u8fbe13\u4e2a\u767e\u5206\u70b9\u3002\u8fd9\u4e9b\u6548\u5e94\u5728\u62db\u8058\u3001\u8d37\u6b3e\u548c\u6c42\u52a9\u573a\u666f\u4e2d\u53ef\u590d\u73b0\u3002", "conclusion": "\u56fa\u5b9a\u6761\u4ef6\u6d4b\u8bd5\u7684\u504f\u89c1\u5206\u6570\u53ef\u80fd\u65e0\u6cd5\u6cdb\u5316\u3002\u8fd9\u4e0d\u662f\u5173\u4e8e\u771f\u5b9e\u504f\u89c1\u7387\u7684\u58f0\u660e\uff0c\u800c\u662f\u8bc4\u4f30\u9c81\u68d2\u6027\u7684\u538b\u529b\u6d4b\u8bd5\u3002CSF\u8feb\u4f7f\u8bc4\u4f30\u8005\u95ee\"\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u4f1a\u51fa\u73b0\u504f\u89c1\uff1f\"\u800c\u4e0d\u662f\"\u8fd9\u4e2a\u6a21\u578b\u6709\u504f\u89c1\u5417\uff1f\"", "topic": "agent analysis"}}
{"id": "2601.10532", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10532", "abs": "https://arxiv.org/abs/2601.10532", "authors": ["Chengbing Wang", "Wuqiang Zheng", "Yang Zhang", "Fengbin Zhu", "Junyi Cheng", "Yi Xie", "Wenjie Wang", "Fuli Feng"], "title": "PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\\%. Furthermore, a blinded user study reveals a 70\\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.", "AI": {"tldr": "\u63d0\u51faPERM\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u5206\u89e3\uff08\u652f\u6301\u8005\u89c6\u89d2\u3001\u5bfb\u6c42\u8005\u89c6\u89d2\u3001\u65c1\u89c2\u8005\u89c6\u89d2\uff09\u8bc4\u4f30\u5171\u60c5\uff0c\u589e\u5f3aLLM\u7684\u60c5\u611f\u652f\u6301\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u5728\u4eba\u7c7b\u4e2d\u5fc3\u5e94\u7528\u4e2d\u7f3a\u4e4f\u5b9e\u8d28\u6027\u60c5\u611f\u652f\u6301\uff0c\u73b0\u6709RL\u5956\u52b1\u6a21\u578b\u901a\u5e38\u4ece\u5355\u4e00\u89c6\u89d2\u8bc4\u4f30\u5171\u60c5\uff0c\u5ffd\u7565\u4e86\u5171\u60c5\u5468\u671f\u7406\u8bba\u4e2d\u652f\u6301\u8005\u4e0e\u5bfb\u6c42\u8005\u4e4b\u95f4\u7684\u53cc\u5411\u4e92\u52a8\u672c\u8d28", "method": "\u63d0\u51fa\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u5171\u60c5\u5956\u52b1\u5efa\u6a21\uff08PERM\uff09\uff0c\u901a\u8fc7\u53cc\u5411\u5206\u89e3\u64cd\u4f5c\u5171\u60c5\u8bc4\u4f30\uff1a1\uff09\u652f\u6301\u8005\u89c6\u89d2\uff08\u5185\u90e8\u5171\u9e23\u548c\u6c9f\u901a\u8868\u8fbe\uff09\uff1b2\uff09\u5bfb\u6c42\u8005\u89c6\u89d2\uff08\u60c5\u611f\u63a5\u6536\uff09\uff1b3\uff09\u65c1\u89c2\u8005\u89c6\u89d2\uff08\u76d1\u63a7\u6574\u4f53\u4e92\u52a8\u8d28\u91cf\uff09", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u60c5\u611f\u667a\u80fd\u57fa\u51c6\u548c\u5de5\u4e1a\u65e5\u5e38\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\uff0cPERM\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u8d85\u8fc710%\uff1b\u76f2\u6d4b\u7528\u6237\u7814\u7a76\u663e\u793a70%\u7528\u6237\u504f\u597d\u8be5\u65b9\u6cd5", "conclusion": "PERM\u901a\u8fc7\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u53cc\u5411\u5171\u60c5\u8bc4\u4f30\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u5171\u60c5\u56de\u5e94\u7684\u80fd\u529b\uff0c\u4e3a\u60c5\u611f\u652f\u6301\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2601.10645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.10645", "abs": "https://arxiv.org/abs/2601.10645", "authors": ["Yuxi Xia", "Loris Schoenegger", "Benjamin Roth"], "title": "Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs", "comment": null, "summary": "Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTracVC\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u548c\u5f71\u54cd\u4f30\u8ba1\u8ffd\u8e2aLLM\u8868\u8fbe\u81ea\u4fe1\u7684\u8bad\u7ec3\u6570\u636e\u6765\u6e90\uff0c\u53d1\u73b0LLM\u5e38\u6a21\u4eff\u8868\u9762\u81ea\u4fe1\u8868\u8fbe\u800c\u975e\u57fa\u4e8e\u5185\u5bb9\u5408\u7406\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bad\u7ec3\u673a\u5236\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "motivation": "LLM\u901a\u8fc7\u8868\u8fbe\u81ea\u4fe1\u53ef\u4ee5\u589e\u52a0\u7528\u6237\u4fe1\u4efb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u7ecf\u5e38\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5176\u8868\u8fbe\u7684\u81ea\u4fe1\u5ea6\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0d\u4e00\u81f4\u3002\u4e3a\u4e86\u7406\u89e3\u8fd9\u79cd\u81ea\u4fe1\u8868\u8fbe\u7684\u6765\u6e90\uff0c\u9700\u8981\u8ffd\u8e2a\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faTracVC\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u548c\u5f71\u54cd\u4f30\u8ba1\uff0c\u5c06\u751f\u6210\u7684\u81ea\u4fe1\u8868\u8fbe\u8ffd\u6eaf\u5230\u8bad\u7ec3\u6570\u636e\u3002\u5728\u95ee\u7b54\u8bbe\u7f6e\u4e2d\u8bc4\u4f30OLMo\u548cLlama\u6a21\u578b\uff0c\u63d0\u51fa\u65b0\u6307\u6807\"\u5185\u5bb9\u57fa\u7840\u6027\"\uff0c\u8861\u91cfLLM\u81ea\u4fe1\u8868\u8fbe\u57fa\u4e8e\u5185\u5bb9\u76f8\u5173\u8bad\u7ec3\u793a\u4f8b\u7684\u7a0b\u5ea6\u3002", "result": "\u5206\u6790\u663e\u793aOLMo2-13B\u7ecf\u5e38\u53d7\u5230\u4e0e\u67e5\u8be2\u8bcd\u6c47\u65e0\u5173\u7684\u81ea\u4fe1\u76f8\u5173\u6570\u636e\u5f71\u54cd\uff0c\u8868\u660e\u5b83\u53ef\u80fd\u6a21\u4eff\u8868\u9762\u7684\u786e\u5b9a\u6027\u8bed\u8a00\u8868\u8fbe\uff0c\u800c\u975e\u4f9d\u8d56\u771f\u6b63\u7684\u5185\u5bb9\u57fa\u7840\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u8bad\u7ec3\u673a\u5236\u7684\u6839\u672c\u5c40\u9650\u6027\u3002", "conclusion": "LLM\u53ef\u80fd\u5b66\u4f1a\u5982\u4f55\u542c\u8d77\u6765\u81ea\u4fe1\uff0c\u800c\u6ca1\u6709\u5b66\u4f1a\u4f55\u65f6\u81ea\u4fe1\u662f\u5408\u7406\u7684\u3002\u8be5\u5206\u6790\u4e3a\u6539\u8fdbLLM\u8868\u8fbe\u66f4\u53ef\u9760\u81ea\u4fe1\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.10700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.10700", "abs": "https://arxiv.org/abs/2601.10700", "authors": ["Gilat Toker", "Nitay Calderon", "Ohad Amosy", "Roi Reichart"], "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals", "comment": null, "summary": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.", "AI": {"tldr": "LIBERTy\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u751f\u6210\u7ed3\u6784\u53cd\u4e8b\u5b9e\u5bf9\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6982\u5ff5\u89e3\u91ca\u65b9\u6cd5\u7684\u5fe0\u5b9e\u6027\uff0c\u5305\u542b\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6982\u5ff5\u7684\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u6602\u8d35\u4e14\u4e0d\u5b8c\u7f8e\u7684\u4eba\u5de5\u7f16\u5199\u53cd\u4e8b\u5b9e\u6570\u636e\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u89e3\u91ca\u65b9\u6cd5\u7684\u5fe0\u5b9e\u6027\u3002", "method": "\u63d0\u51faLIBERTy\u6846\u67b6\uff1a\u57fa\u4e8e\u660e\u786e\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u901a\u8fc7\u5e72\u9884\u6982\u5ff5\u5e76\u8ba9LLM\u751f\u6210\u53cd\u4e8b\u5b9e\u5bf9\uff0c\u6784\u5efa\u5305\u542b\u75be\u75c5\u68c0\u6d4b\u3001\u7b80\u5386\u7b5b\u9009\u548c\u5de5\u4f5c\u573a\u6240\u66b4\u529b\u9884\u6d4b\u7684\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165order-faithfulness\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8bc4\u4f30\u591a\u79cd\u65b9\u6cd5\u5728\u4e94\u4e2a\u6a21\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6982\u5ff5\u89e3\u91ca\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\uff1b\u5206\u6790\u663e\u793a\u4e13\u6709LLM\u5bf9\u4eba\u53e3\u7edf\u8ba1\u6982\u5ff5\u654f\u611f\u6027\u964d\u4f4e\uff0c\u53ef\u80fd\u662f\u540e\u8bad\u7ec3\u7f13\u89e3\u63aa\u65bd\u6240\u81f4\u3002", "conclusion": "LIBERTy\u4e3a\u5f00\u53d1\u5fe0\u5b9e\u7684\u6982\u5ff5\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6025\u9700\u7684\u57fa\u51c6\uff0c\u652f\u6301\u7cfb\u7edf\u5206\u6790\u6a21\u578b\u5bf9\u5e72\u9884\u7684\u654f\u611f\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.10702", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.10702", "abs": "https://arxiv.org/abs/2601.10702", "authors": ["Ruozhen Yang", "Yucheng Jiang", "Yueqi Jiang", "Priyanka Kargupta", "Yunyi Zhang", "Jiawei Han"], "title": "Grounding Agent Memory in Contextual Intent", "comment": null, "summary": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.\n  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.", "AI": {"tldr": "STITCH\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u65f6\u7a0b\u76ee\u6807\u5bfc\u5411\u4ea4\u4e92\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u610f\u56fe\u7d22\u5f15\u548c\u5339\u914d\u6765\u51cf\u5c11\u68c0\u7d22\u566a\u58f0\uff0c\u5728CAME-Bench\u548cLongMemEval\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5728\u957f\u65f6\u7a0b\u76ee\u6807\u5bfc\u5411\u4ea4\u4e92\u4e2d\uff0c\u76f8\u4f3c\u5b9e\u4f53\u548c\u4e8b\u5b9e\u5728\u4e0d\u540c\u6f5c\u5728\u76ee\u6807\u548c\u7ea6\u675f\u4e0b\u91cd\u590d\u51fa\u73b0\uff0c\u5bfc\u81f4\u4f20\u7edf\u8bb0\u5fc6\u7cfb\u7edf\u68c0\u7d22\u5230\u4e0a\u4e0b\u6587\u4e0d\u5339\u914d\u7684\u8bc1\u636e\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u3002", "method": "\u63d0\u51faSTITCH\u7cfb\u7edf\uff0c\u4e3a\u6bcf\u4e2a\u8f68\u8ff9\u6b65\u9aa4\u5efa\u7acb\u7ed3\u6784\u5316\u68c0\u7d22\u7ebf\u7d22\uff08\u4e0a\u4e0b\u6587\u610f\u56fe\uff09\uff0c\u5305\u62ec\uff1a\u5f53\u524d\u6f5c\u5728\u76ee\u6807\uff08\u5b9a\u4e49\u4e3b\u9898\u6bb5\uff09\u3001\u52a8\u4f5c\u7c7b\u578b\u3001\u5173\u952e\u5b9e\u4f53\u7c7b\u578b\u3002\u901a\u8fc7\u610f\u56fe\u517c\u5bb9\u6027\u8fc7\u6ee4\u548c\u4f18\u5148\u6392\u5e8f\u8bb0\u5fc6\u7247\u6bb5\uff0c\u6291\u5236\u8bed\u4e49\u76f8\u4f3c\u4f46\u4e0a\u4e0b\u6587\u4e0d\u517c\u5bb9\u7684\u5386\u53f2\u3002", "result": "\u5728CAME-Bench\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTITCH\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534735.6%\uff0c\u8f68\u8ff9\u8d8a\u957f\u63d0\u5347\u8d8a\u660e\u663e\u3002\u610f\u56fe\u7d22\u5f15\u663e\u8457\u51cf\u5c11\u68c0\u7d22\u566a\u58f0\u3002", "conclusion": "\u7ed3\u6784\u5316\u610f\u56fe\u8ddf\u8e2a\u4e3a\u957f\u65f6\u7a0b\u63a8\u7406\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u610f\u56fe\u611f\u77e5\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u91cd\u590d\u5b9e\u4f53\u548c\u4e8b\u5b9e\u7684\u6b67\u4e49\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.f5d8faff", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sumologic.com%2Fblog%2Fwelcome-dojo-ai-agents-soc%3Futm_medium=email%26utm_source=TLDR%26utm_term=cloud-siem%26utm_id=701VK00000KhKeHYAV%26utm_campaign=20251204-global-awsmp-TLDR-primary/2/0100019bbcd606f9-972ee35d-63cd-4202-8bef-5a9cd6b125ca-000000/muljs64vPJG78U6uJ-BsZsMmZdQjH7B8HPw9D3b2GOM=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sumologic.com%2Fblog%2Fwelcome-dojo-ai-agents-soc%3Futm_medium=email%26utm_source=TLDR%26utm_term=cloud-siem%26utm_id=701VK00000KhKeHYAV%26utm_campaign=20251204-global-awsmp-TLDR-primary/2/0100019bbcd606f9-972ee35d-63cd-4202-8bef-5a9cd6b125ca-000000/muljs64vPJG78U6uJ-BsZsMmZdQjH7B8HPw9D3b2GOM=440", "authors": ["TLDR Newsletter"], "title": "AI Agents That Actually Accelerate your Investigations", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sumologic.com%2Fblog%2Fwelcome-dojo-ai-agents-soc%3Futm_medium=email%26utm_source=TLDR%26utm_term=cloud-siem%26utm_id=701VK00000KhKeHYAV%26utm_campaign=20251204-global-awsmp-TLDR-primary/2/0100019bbcd606f9-972ee35d-63cd-4202-8bef-5a9cd6b125ca-000000/muljs64vPJG78U6uJ-BsZsMmZdQjH7B8HPw9D3b2GOM=440", "summary": "AI Agents That Actually Accelerate your Investigations (Sponsor) Most \"AI for security\" tools don't actually make your workflows faster or easier. Sumo Logic's Dojo AI is a different beast, deploying specialized agents that work together to triage alerts, explain what triggered them, and write queries for you. >> Mobot lets you use natural language to run an investigation >> Query Agent turns natural language into precise searches for faster root cause analysis >> Summary Agent explains each ...", "source": "tldr", "AI": {"tldr": "Sumo Logic\u7684Dojo AI\u901a\u8fc7\u4e13\u95e8\u5316\u7684AI\u4ee3\u7406\uff08Mobot\u3001Query Agent\u3001Summary Agent\uff09\u534f\u540c\u5de5\u4f5c\uff0c\u52a0\u901f\u5b89\u5168\u8c03\u67e5\u6d41\u7a0b\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u544a\u8b66\u5904\u7406\u3001\u67e5\u8be2\u751f\u6210\u548c\u89e3\u91ca\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6570\"AI for security\"\u5de5\u5177\u672a\u80fd\u771f\u6b63\u52a0\u901f\u5b89\u5168\u8c03\u67e5\u5de5\u4f5c\u6d41\u7a0b\uff0cDojo AI\u65e8\u5728\u901a\u8fc7\u4e13\u95e8\u5316\u4ee3\u7406\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u5b89\u5168\u56e2\u961f\u7684\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u91c7\u7528\u591a\u4e2a\u4e13\u95e8\u5316AI\u4ee3\u7406\u534f\u540c\u5de5\u4f5c\uff1aMobot\u4ee3\u7406\u5141\u8bb8\u81ea\u7136\u8bed\u8a00\u8fd0\u884c\u8c03\u67e5\uff0cQuery Agent\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u7cbe\u786e\u641c\u7d22\u67e5\u8be2\uff0cSummary Agent\u89e3\u91ca\u544a\u8b66\u89e6\u53d1\u539f\u56e0\u3002", "result": "Dojo AI\u80fd\u591f\u52a0\u901f\u5b89\u5168\u8c03\u67e5\u6d41\u7a0b\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u6839\u672c\u539f\u56e0\u5206\u6790\uff0c\u7b80\u5316\u5b89\u5168\u56e2\u961f\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63d0\u9ad8\u8c03\u67e5\u6548\u7387\u3002", "conclusion": "Dojo AI\u901a\u8fc7\u4e13\u95e8\u5316AI\u4ee3\u7406\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u4e3a\u5b89\u5168\u56e2\u961f\u63d0\u4f9b\u4e86\u771f\u6b63\u52a0\u901f\u8c03\u67e5\u6d41\u7a0b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u533a\u522b\u4e8e\u4f20\u7edfAI\u5b89\u5168\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.c5e967f0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrai%26utm_medium=email%26utm_campaign=codereview_260114secondary/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/lk02vB7O23FZpdra7XKkMX18w2CJqZvZ5Biyfj0yjqU=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrai%26utm_medium=email%26utm_campaign=codereview_260114secondary/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/lk02vB7O23FZpdra7XKkMX18w2CJqZvZ5Biyfj0yjqU=440", "authors": ["TLDR Newsletter"], "title": "AI code review with comments you'll actually", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrai%26utm_medium=email%26utm_campaign=codereview_260114secondary/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/lk02vB7O23FZpdra7XKkMX18w2CJqZvZ5Biyfj0yjqU=440", "summary": "AI code review with comments you'll actually (Sponsor) Unblocked is the AI code review that surfaces real issues and meaningful feedback instead of flooding your PRs with stylistic nitpicks and low-value comments. \u201cUnblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.\u201d - Senior developer, Clio Try now for free", "source": "tldr", "AI": {"tldr": "Unblocked\u662f\u4e00\u6b3eAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\uff0c\u4e13\u6ce8\u4e8e\u8bc6\u522b\u771f\u5b9e\u95ee\u9898\u548c\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u53cd\u9988\uff0c\u800c\u975e\u7ea0\u7ed3\u4e8e\u4ee3\u7801\u98ce\u683c\u7ec6\u8282\u548c\u4f4e\u4ef7\u503c\u8bc4\u8bba\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5b58\u5728\u8fc7\u5ea6\u5173\u6ce8\u4ee3\u7801\u98ce\u683c\u7ec6\u8282\u3001\u4ea7\u751f\u5927\u91cf\u4f4e\u4ef7\u503c\u8bc4\u8bba\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u4ea7\u751f\"AI\u75b2\u52b3\"\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7406\u89e3\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u3001\u63d0\u4f9b\u771f\u6b63\u6709\u4ef7\u503c\u53cd\u9988\u7684\u5de5\u5177\u3002", "method": "\u901a\u8fc7AI\u6280\u672f\u5206\u6790\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\uff0c\u8bc6\u522b\u771f\u6b63\u91cd\u8981\u7684\u4ee3\u7801\u95ee\u9898\uff0c\u8fc7\u6ee4\u6389\u98ce\u683c\u6027\u6311\u5254\u548c\u4f4e\u4ef7\u503c\u8bc4\u8bba\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u62e5\u6709\u5b8c\u6574\u4ee3\u7801\u5e93\u89c6\u89d2\u7684\u8d44\u6df1\u5f00\u53d1\u8005\u6240\u80fd\u7ed9\u51fa\u7684\u53cd\u9988\u3002", "result": "\u5f00\u53d1\u8005\u53cd\u9988\u8868\u660eUnblocked\u6539\u53d8\u4e86\u4ed6\u4eec\u5bf9AI\u75b2\u52b3\u7684\u770b\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u53ea\u6709\u4e86\u89e3\u5b8c\u6574\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u7684\u4eba\u624d\u80fd\u7ed9\u51fa\u7684\u6709\u4ef7\u503c\u53cd\u9988\u3002", "conclusion": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u5e94\u8be5\u4e13\u6ce8\u4e8e\u63d0\u4f9b\u6709\u4e0a\u4e0b\u6587\u610f\u8bc6\u7684\u3001\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\uff0c\u800c\u4e0d\u662f\u4ea7\u751f\u5927\u91cf\u4f4e\u4ef7\u503c\u7684\u8bc4\u8bba\uff0c\u8fd9\u6837\u624d\u80fd\u771f\u6b63\u5e2e\u52a9\u5f00\u53d1\u8005\u5e76\u907f\u514dAI\u75b2\u52b3\u3002", "topic": "swe application"}}
{"id": "tldr.2601.0bc18bb2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2FPOWERFULMOVES%2F58bcadab9483bf5e633e865f131e6c25%3Futm_source=tldrai/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/1JQXXB7D_TkTDVkkGaj8cdymW6hmppoTIp8aHZjVWMA=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2FPOWERFULMOVES%2F58bcadab9483bf5e633e865f131e6c25%3Futm_source=tldrai/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/1JQXXB7D_TkTDVkkGaj8cdymW6hmppoTIp8aHZjVWMA=440", "authors": ["TLDR Newsletter"], "title": "Claude Agent SDK Technical Specification", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2FPOWERFULMOVES%2F58bcadab9483bf5e633e865f131e6c25%3Futm_source=tldrai/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/1JQXXB7D_TkTDVkkGaj8cdymW6hmppoTIp8aHZjVWMA=440", "summary": "Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.", "source": "tldr", "AI": {"tldr": "Claude Agent SDK\u6280\u672f\u89c4\u8303\uff0c\u63d0\u4f9b\u5bf9Claude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7a0b\u5e8f\u5316\u8bbf\u95ee", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u7a0b\u5e8f\u5316\u7684\u65b9\u5f0f\u6765\u8bbf\u95ee\u548c\u5229\u7528Claude Code\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u7b80\u5316\u667a\u80fd\u4f53\u5e94\u7528\u7684\u5f00\u53d1\u6d41\u7a0b", "method": "\u63d0\u4f9bSDK\u6280\u672f\u89c4\u8303\uff0c\u5305\u62ecAPI\u63a5\u53e3\u3001\u8c03\u7528\u65b9\u5f0f\u3001\u914d\u7f6e\u53c2\u6570\u7b49\u7a0b\u5e8f\u5316\u8bbf\u95ee\u673a\u5236", "result": "\u5efa\u7acb\u4e86Claude Agent SDK\u7684\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5b8c\u6574\u7684\u7a0b\u5e8f\u5316\u8bbf\u95ee\u65b9\u6848", "conclusion": "Claude Agent SDK\u6280\u672f\u89c4\u8303\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u667a\u80fd\u4f53\u80fd\u529b\u8bbf\u95ee\u63a5\u53e3\uff0c\u4fc3\u8fdb\u667a\u80fd\u4f53\u5e94\u7528\u7684\u5f00\u53d1", "topic": "code agent"}}
{"id": "tldr.2601.edfa0eed", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/an_4c3f7IwUS0oi0qMnufKKq0IStKZXQ_Sz8B3Y_PWk=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/an_4c3f7IwUS0oi0qMnufKKq0IStKZXQ_Sz8B3Y_PWk=440", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/an_4c3f7IwUS0oi0qMnufKKq0IStKZXQ_Sz8B3Y_PWk=440", "summary": "Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.", "source": "tldr", "AI": {"tldr": "Claude Agent SDK\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u63d0\u4f9b\u5bf9Claude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7a0b\u5e8f\u5316\u8bbf\u95ee\u63a5\u53e3", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u7a0b\u5e8f\u5316\u63a5\u53e3\uff0c\u4ee5\u4fbf\u5728\u5e94\u7528\u4e2d\u96c6\u6210Claude Code\u7684\u667a\u80fd\u4f53\u529f\u80fd\uff0c\u964d\u4f4e\u5f00\u53d1\u95e8\u69db", "method": "\u63d0\u4f9bSDK\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u5b9a\u4e49API\u63a5\u53e3\u3001\u6570\u636e\u7ed3\u6784\u3001\u8c03\u7528\u65b9\u5f0f\u7b49\u6280\u672f\u7ec6\u8282", "result": "\u5efa\u7acb\u4e86\u5b8c\u6574\u7684Claude Agent SDK\u6280\u672f\u89c4\u8303\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u901a\u8fc7\u7a0b\u5e8f\u5316\u65b9\u5f0f\u8c03\u7528Claude Code\u7684\u667a\u80fd\u4f53\u80fd\u529b", "conclusion": "\u8be5\u6280\u672f\u89c4\u8303\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u4fc3\u8fdb\u4e86Claude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u96c6\u6210\u548c\u5e94\u7528\u5f00\u53d1", "topic": "code agent"}}
{"id": "tldr.2601.b5cfc04d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/xAZVIqWTPLKM3nK-2FsODjBQNMhLyT74mYZF7H2FLKQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/xAZVIqWTPLKM3nK-2FsODjBQNMhLyT74mYZF7H2FLKQ=440", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/xAZVIqWTPLKM3nK-2FsODjBQNMhLyT74mYZF7H2FLKQ=440", "summary": "Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.", "source": "tldr", "AI": {"tldr": "Claude Agent SDK\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u63d0\u4f9b\u5bf9Claude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7a0b\u5e8f\u5316\u8bbf\u95ee\u63a5\u53e3", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u7a0b\u5e8f\u5316\u7684\u65b9\u5f0f\u6765\u8bbf\u95ee\u548c\u5229\u7528Claude Code\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u4fc3\u8fdb\u57fa\u4e8eClaude\u7684\u5e94\u7528\u7a0b\u5e8f\u5f00\u53d1", "method": "\u63d0\u4f9bSDK\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u5b9a\u4e49API\u63a5\u53e3\u3001\u4f7f\u7528\u65b9\u6cd5\u548c\u96c6\u6210\u6307\u5357\uff0c\u652f\u6301\u5f00\u53d1\u8005\u901a\u8fc7\u7a0b\u5e8f\u5316\u65b9\u5f0f\u8c03\u7528Claude\u667a\u80fd\u4f53\u529f\u80fd", "result": "\u521b\u5efa\u4e86\u5b8c\u6574\u7684Claude Agent SDK\u6280\u672f\u89c4\u8303\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u8bbf\u95eeClaude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u6807\u51c6\u5316\u5de5\u5177\u548c\u6587\u6863", "conclusion": "Claude Agent SDK\u6280\u672f\u89c4\u8303\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bbf\u95eeClaude\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7a0b\u5e8f\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u57fa\u4e8eClaude\u7684\u5e94\u7528\u7a0b\u5e8f\u5f00\u53d1", "topic": "code agent"}}
{"id": "tldr.2601.5ae83a45", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/qPGKuDuvpxUo60lelLIO2x9okOVikO7NI22m2xBZmTg=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/qPGKuDuvpxUo60lelLIO2x9okOVikO7NI22m2xBZmTg=440", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-14, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019bbcdeb03d-56ee2c22-3a71-4bc9-9f34-18422a70ac1f-000000/qPGKuDuvpxUo60lelLIO2x9okOVikO7NI22m2xBZmTg=440", "summary": "Claude Agent SDK Technical Specification (22 minute read) This page contains the technical specification for the Claude Agent SDK, which provides programmatic access to Claude Code's agentic capabilities.", "source": "tldr", "AI": {"tldr": "Claude Agent SDK\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u63d0\u4f9b\u5bf9Claude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u7a0b\u5e8f\u5316\u8bbf\u95ee\u63a5\u53e3", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u7a0b\u5e8f\u5316\u63a5\u53e3\uff0c\u4ee5\u4fbf\u5728\u5e94\u7528\u4e2d\u96c6\u6210Claude Code\u7684\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b", "method": "\u63d0\u4f9b\u6280\u672f\u89c4\u8303\u6587\u6863\uff0c\u5b9a\u4e49SDK\u7684API\u63a5\u53e3\u3001\u4f7f\u7528\u65b9\u6cd5\u548c\u96c6\u6210\u65b9\u5f0f\uff0c\u5305\u542b\u8be6\u7ec6\u7684\u7f16\u7a0b\u63a5\u53e3\u8bf4\u660e", "result": "\u521b\u5efa\u4e86\u5b8c\u6574\u7684Claude Agent SDK\u6280\u672f\u89c4\u8303\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e8622\u5206\u949f\u9605\u8bfb\u65f6\u957f\u7684\u8be6\u7ec6\u6280\u672f\u6587\u6863", "conclusion": "\u8be5\u6280\u672f\u89c4\u8303\u6587\u6863\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u96c6\u6210Claude Code\u667a\u80fd\u4f53\u80fd\u529b\u7684\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2601.0c42b13a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dataengineeringpodcast.com%2Ffenic-ai-dataframe-episode-496%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/3_d30IEAu-I85Ll5__fcp13tSSIswnMKp1UydfxdJHQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dataengineeringpodcast.com%2Ffenic-ai-dataframe-episode-496%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/3_d30IEAu-I85Ll5__fcp13tSSIswnMKp1UydfxdJHQ=440", "authors": ["TLDR Newsletter"], "title": "Semantic Operators Meet Dataframes: Building Context for Agents with FENIC", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.dataengineeringpodcast.com%2Ffenic-ai-dataframe-episode-496%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/3_d30IEAu-I85Ll5__fcp13tSSIswnMKp1UydfxdJHQ=440", "summary": "Semantic Operators Meet Dataframes: Building Context for Agents with FENIC (56 minute podcast) FENIC is a DataFrame-style engine that treats LLM calls like built-in operators, so it can optimize, throttle, and reduce cost for AI-heavy pipelines. It uses LLMs to turn messy text into structured data, making downstream processing reliable, reproducible, and usable as governed tools that agents can safely call.", "source": "tldr", "AI": {"tldr": "FENIC\u662f\u4e00\u4e2aDataFrame\u98ce\u683c\u5f15\u64ce\uff0c\u5c06LLM\u8c03\u7528\u89c6\u4e3a\u5185\u7f6e\u64cd\u4f5c\u7b26\uff0c\u53ef\u4f18\u5316\u3001\u8282\u6d41\u5e76\u964d\u4f4eAI\u5bc6\u96c6\u578b\u7ba1\u9053\u7684\u6210\u672c\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u4f9b\u4ee3\u7406\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3AI\u5bc6\u96c6\u578b\u7ba1\u9053\u4e2dLLM\u8c03\u7528\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u9760\u3001\u53ef\u590d\u73b0\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4e3a\u4ee3\u7406\u63d0\u4f9b\u5b89\u5168\u7684\u6cbb\u7406\u5de5\u5177\u3002", "method": "\u5f00\u53d1DataFrame\u98ce\u683c\u5f15\u64ce\uff0c\u5c06LLM\u8c03\u7528\u89c6\u4e3a\u5185\u7f6e\u64cd\u4f5c\u7b26\uff0c\u5b9e\u73b0\u4f18\u5316\u3001\u8282\u6d41\u548c\u6210\u672c\u63a7\u5236\uff0c\u4f7f\u7528LLM\u5c06\u6df7\u4e71\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u6784\u5efa\u4e86\u53ef\u9760\u3001\u53ef\u590d\u73b0\u3001\u53ef\u4f5c\u4e3a\u6cbb\u7406\u5de5\u5177\u4f7f\u7528\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u5b89\u5168\u8c03\u7528\uff0c\u964d\u4f4e\u4e86AI\u5bc6\u96c6\u578b\u7ba1\u9053\u7684\u6210\u672c\u548c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "FENIC\u901a\u8fc7\u5c06LLM\u8c03\u7528\u96c6\u6210\u5230DataFrame\u5f15\u64ce\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u7ba1\u9053\u4e2d\u7684\u6210\u672c\u3001\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.85bba9ab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewaiorder.substack.com%2Fp%2Fi-tested-14-analytics-agents-so-you%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/_zI791ZiSv8CcmrpQT-Z1quBCk0Lauiu7y4kvElNAAc=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewaiorder.substack.com%2Fp%2Fi-tested-14-analytics-agents-so-you%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/_zI791ZiSv8CcmrpQT-Z1quBCk0Lauiu7y4kvElNAAc=440", "authors": ["TLDR Newsletter"], "title": "I tested 14 analytics agents - so you don't have to", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewaiorder.substack.com%2Fp%2Fi-tested-14-analytics-agents-so-you%3Futm_source=tldrdata/1/0100019bc157af4f-432b22d8-b94f-481f-99ea-f12ae4373a89-000000/_zI791ZiSv8CcmrpQT-Z1quBCk0Lauiu7y4kvElNAAc=440", "summary": "I tested 14 analytics agents - so you don't have to (15 minute read) A 2026 benchmark of 14 agentic analytics tools tested reliability, UX, speed, and cost on real production data. The conclusion is that AI-native BI tools currently offer the best overall experience, but at the expense of higher cost and migration effort.", "source": "tldr", "AI": {"tldr": "\u5bf914\u4e2a\u5206\u6790\u578bAI\u4ee3\u7406\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u53ef\u9760\u6027\u3001\u7528\u6237\u4f53\u9a8c\u3001\u901f\u5ea6\u548c\u6210\u672c\uff0c\u53d1\u73b0AI\u539f\u751f\u7684BI\u5de5\u5177\u63d0\u4f9b\u6700\u4f73\u6574\u4f53\u4f53\u9a8c\uff0c\u4f46\u6210\u672c\u66f4\u9ad8\u4e14\u9700\u8981\u8fc1\u79fb\u6295\u5165", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5de5\u5177\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u4f01\u4e1a\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u9009\u62e9\u53c2\u8003", "method": "\u4f7f\u7528\u771f\u5b9e\u751f\u4ea7\u6570\u636e\u5bf914\u4e2a\u4ee3\u7406\u5316\u5206\u6790\u5de5\u5177\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u53ef\u9760\u6027\u3001\u7528\u6237\u4f53\u9a8c\u3001\u901f\u5ea6\u548c\u6210\u672c", "result": "AI\u539f\u751f\u7684\u5546\u4e1a\u667a\u80fd(BI)\u5de5\u5177\u5728\u6574\u4f53\u4f53\u9a8c\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5b58\u5728\u6210\u672c\u8f83\u9ad8\u548c\u9700\u8981\u8fc1\u79fb\u6295\u5165\u7684\u7f3a\u70b9", "conclusion": "\u5f53\u524d\u9636\u6bb5AI\u539f\u751fBI\u5de5\u5177\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u7efc\u5408\u4f53\u9a8c\uff0c\u4f46\u7528\u6237\u9700\u8981\u5728\u66f4\u597d\u7684\u4f53\u9a8c\u4e0e\u66f4\u9ad8\u7684\u6210\u672c\u53ca\u8fc1\u79fb\u6295\u5165\u4e4b\u95f4\u505a\u51fa\u6743\u8861", "topic": "agent analysis"}}
{"id": "tldr.2601.ac854161", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fgas-town-decoded%2F%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/xPPFMs-j6RTr0IhgoCZWJdQD7uTPrF0IPB-SKGvzcsI=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fgas-town-decoded%2F%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/xPPFMs-j6RTr0IhgoCZWJdQD7uTPrF0IPB-SKGvzcsI=440", "authors": ["TLDR Newsletter"], "title": "Gas Town Decoded", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alilleybrinker.com%2Fmini%2Fgas-town-decoded%2F%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/xPPFMs-j6RTr0IhgoCZWJdQD7uTPrF0IPB-SKGvzcsI=440", "summary": "Gas Town Decoded (3 minute read) This post presents a list of terms used in Gas Town, an AI agent orchestration tool, decoding the definitions used in the introductory paper for developers.", "source": "tldr", "AI": {"tldr": "Gas Town\u662f\u4e00\u4e2aAI\u4ee3\u7406\u7f16\u6392\u5de5\u5177\uff0c\u672c\u6587\u89e3\u91ca\u4e86\u5176\u672f\u8bed\u5b9a\u4e49\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u8be5\u5de5\u5177", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9bGas Town\u5de5\u5177\u4e2d\u4f7f\u7528\u7684\u672f\u8bed\u89e3\u91ca\uff0c\u964d\u4f4e\u5b66\u4e60\u95e8\u69db\uff0c\u4fc3\u8fdb\u5de5\u5177\u91c7\u7528", "method": "\u901a\u8fc7\u672f\u8bed\u5217\u8868\u548c\u89e3\u7801\u7684\u65b9\u5f0f\uff0c\u7cfb\u7edf\u6027\u5730\u89e3\u91caGas Town\u4e2d\u7684\u5173\u952e\u6982\u5ff5\u548c\u5b9a\u4e49", "result": "\u63d0\u4f9b\u4e86Gas Town\u5de5\u5177\u4e2d\u4f7f\u7528\u7684\u672f\u8bed\u5b9a\u4e49\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f7f\u7528\u8be5AI\u4ee3\u7406\u7f16\u6392\u5de5\u5177", "conclusion": "\u6e05\u6670\u7684\u672f\u8bed\u89e3\u91ca\u5bf9\u4e8eAI\u4ee3\u7406\u7f16\u6392\u5de5\u5177\u7684\u5f00\u53d1\u8005\u91c7\u7528\u81f3\u5173\u91cd\u8981\uff0cGas Town\u7684\u672f\u8bed\u89e3\u7801\u6709\u52a9\u4e8e\u63d0\u5347\u5f00\u53d1\u4f53\u9a8c", "topic": "agent analysis"}}
{"id": "tldr.2601.8acc4496", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/Ds7mZ-vMBHqnShZGy2t7DO0oby0FxlKFZ6-XBz_P60A=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/Ds7mZ-vMBHqnShZGy2t7DO0oby0FxlKFZ6-XBz_P60A=440", "authors": ["TLDR Newsletter"], "title": "Getting Real Leverage from Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/Ds7mZ-vMBHqnShZGy2t7DO0oby0FxlKFZ6-XBz_P60A=440", "summary": "Getting Real Leverage from Claude Code (18 minute read) This post discusses how to run multiple Claude Code sessions in parallel, each in its own fully isolated environment.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u5982\u4f55\u5e76\u884c\u8fd0\u884c\u591a\u4e2aClaude Code\u4f1a\u8bdd\uff0c\u6bcf\u4e2a\u4f1a\u8bdd\u90fd\u5728\u5b8c\u5168\u9694\u79bb\u7684\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u4ee5\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u52a9\u624b\u901a\u5e38\u4ee5\u4e32\u884c\u65b9\u5f0f\u8fd0\u884c\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\u3002\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u9694\u79bb\u7684Claude Code\u4f1a\u8bdd\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u73af\u5883\u9694\u79bb\u4ee5\u907f\u514d\u51b2\u7a81\u3002", "method": "\u91c7\u7528\u5bb9\u5668\u5316\u6280\u672f\u4e3a\u6bcf\u4e2aClaude Code\u4f1a\u8bdd\u521b\u5efa\u5b8c\u5168\u9694\u79bb\u7684\u8fd0\u884c\u73af\u5883\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u673a\u5236\u540c\u65f6\u8fd0\u884c\u591a\u4e2a\u4f1a\u8bdd\uff0c\u5b9e\u73b0\u8d44\u6e90\u7684\u9ad8\u6548\u5229\u7528\u3002", "result": "\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u9694\u79bb\u7684Claude Code\u4f1a\u8bdd\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u548c\u5904\u7406\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6\u786e\u4fdd\u5404\u4e2a\u4f1a\u8bdd\u4e4b\u95f4\u7684\u73af\u5883\u9694\u79bb\u548c\u5b89\u5168\u3002", "conclusion": "\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u9694\u79bb\u7684Claude Code\u4f1a\u8bdd\u662f\u63d0\u9ad8AI\u4ee3\u7801\u52a9\u624b\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u5927\u89c4\u6a21\u4ee3\u7801\u751f\u6210\u548c\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2601.c638786c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2139%26utm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/8iAlzXGxeqMFOMS0nYIBERhAQQuKgYZ6hwF3DVJq_YM=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2139%26utm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/8iAlzXGxeqMFOMS0nYIBERhAQQuKgYZ6hwF3DVJq_YM=440", "authors": ["TLDR Newsletter"], "title": "AI Enables As-Needed Software Features", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2139%26utm_source=tldrnewsletter/1/0100019bc167680c-0afb69b8-dc64-4131-868c-e0dbceebfb1b-000000/8iAlzXGxeqMFOMS0nYIBERhAQQuKgYZ6hwF3DVJq_YM=440", "summary": "AI Enables As-Needed Software Features (1 minute read) AI enables people to just describe what they want, and then apps write the code needed to do it on demand.", "source": "tldr", "AI": {"tldr": "AI\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9700\u6c42\uff0c\u5e94\u7528\u7a0b\u5e8f\u5373\u65f6\u751f\u6210\u6240\u9700\u4ee3\u7801\u6765\u5b9e\u73b0\u529f\u80fd", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u9700\u8981\u9884\u5148\u7f16\u5199\u6240\u6709\u529f\u80fd\u4ee3\u7801\uff0c\u800cAI\u53ef\u4ee5\u5b9e\u73b0\u6309\u9700\u751f\u6210\u4ee3\u7801\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u7075\u6d3b\u6027", "method": "\u4f7f\u7528AI\u6280\u672f\uff0c\u8ba9\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9700\u6c42\uff0c\u5e94\u7528\u7a0b\u5e8f\u81ea\u52a8\u751f\u6210\u76f8\u5e94\u7684\u4ee3\u7801\u6765\u5b9e\u73b0\u529f\u80fd", "result": "\u5b9e\u73b0\u4e86\u6309\u9700\u8f6f\u4ef6\u529f\u80fd\u751f\u6210\uff0c\u7528\u6237\u53ea\u9700\u63cf\u8ff0\u9700\u6c42\u5373\u53ef\u83b7\u5f97\u76f8\u5e94\u529f\u80fd", "conclusion": "AI\u6280\u672f\u80fd\u591f\u5b9e\u73b0\u6309\u9700\u8f6f\u4ef6\u529f\u80fd\u751f\u6210\uff0c\u6539\u53d8\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f", "topic": "swe application"}}
{"id": "tldr.2601.e10855f4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrmarketing/1/0100019bc18c36b9-099c5d09-58c1-4c02-a873-af6f2626b087-000000/bCUCJQXS15Cgf87JLS1nVoRNOAERlr6zGW4nqL8CthQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrmarketing/1/0100019bc18c36b9-099c5d09-58c1-4c02-a873-af6f2626b087-000000/bCUCJQXS15Cgf87JLS1nVoRNOAERlr6zGW4nqL8CthQ=440", "authors": ["TLDR Newsletter"], "title": "Cowork: Claude Code for the rest of your work", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fcowork-research-preview%3Futm_source=tldrmarketing/1/0100019bc18c36b9-099c5d09-58c1-4c02-a873-af6f2626b087-000000/bCUCJQXS15Cgf87JLS1nVoRNOAERlr6zGW4nqL8CthQ=440", "summary": "Cowork: Claude Code for the rest of your work (3 minute read) Claude Cowork is a research preview for Claude Max subscribers on macOS that lets anyone (not just developers) work with files in a chosen folder. Claude can read, edit, and create documents, spreadsheets, or reports, plan tasks, and work in parallel while keeping users informed. It can connect to external tools and handle tasks online with browser access. Users control which folders and connectors Claude can access, and it asks be...", "source": "tldr", "AI": {"tldr": "Claude Cowork\u662f\u4e00\u4e2a\u7814\u7a76\u9884\u89c8\u529f\u80fd\uff0c\u8ba9\u975e\u5f00\u53d1\u8005\u4e5f\u80fd\u901a\u8fc7Claude\u5904\u7406\u6587\u4ef6\u5939\u4e2d\u7684\u6587\u4ef6\uff0c\u5305\u62ec\u8bfb\u53d6\u3001\u7f16\u8f91\u3001\u521b\u5efa\u6587\u6863\u3001\u7535\u5b50\u8868\u683c\u548c\u62a5\u544a\uff0c\u540c\u65f6\u652f\u6301\u8fde\u63a5\u5916\u90e8\u5de5\u5177\u548c\u5728\u7ebf\u4efb\u52a1\u5904\u7406\u3002", "motivation": "\u8ba9\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u5229\u7528AI\u52a9\u624b\u5904\u7406\u65e5\u5e38\u5de5\u4f5c\u4e2d\u7684\u6587\u4ef6\u7ba1\u7406\u3001\u6587\u6863\u521b\u5efa\u548c\u4efb\u52a1\u89c4\u5212\u7b49\u9700\u6c42\uff0c\u964d\u4f4eAI\u5de5\u5177\u7684\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6587\u4ef6\u5939\u7ea7\u522b\u7684AI\u52a9\u624b\u7cfb\u7edf\uff0c\u5141\u8bb8\u7528\u6237\u6307\u5b9a\u53ef\u8bbf\u95ee\u7684\u6587\u4ef6\u5939\u548c\u8fde\u63a5\u5668\uff0cClaude\u53ef\u4ee5\u5e76\u884c\u5904\u7406\u591a\u79cd\u6587\u4ef6\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u6d4f\u89c8\u5668\u8bbf\u95ee\u5904\u7406\u5728\u7ebf\u4efb\u52a1\u3002", "result": "\u4e3aClaude Max\u8ba2\u9605\u8005\u63d0\u4f9b\u4e86macOS\u4e0a\u7684\u7814\u7a76\u9884\u89c8\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u6587\u4ef6\u5904\u7406\u3001\u4efb\u52a1\u89c4\u5212\u548c\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u80fd\u529b\u3002", "conclusion": "Claude Cowork\u6269\u5c55\u4e86AI\u52a9\u624b\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u5de5\u4f5c\u573a\u666f\uff0c\u7279\u522b\u662f\u4e3a\u975e\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6587\u4ef6\u7ba1\u7406\u52a9\u624b\u3002", "topic": "swe application"}}
{"id": "tldr.2601.f8fff135", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fscaling-agents%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JgBME-1FcIMg1ww07eSVttxHi3uG6PFoSPZ5_jNW6MU=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fscaling-agents%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JgBME-1FcIMg1ww07eSVttxHi3uG6PFoSPZ5_jNW6MU=440", "authors": ["TLDR Newsletter"], "title": "Scaling long-running autonomous coding", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fscaling-agents%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JgBME-1FcIMg1ww07eSVttxHi3uG6PFoSPZ5_jNW6MU=440", "summary": "Scaling long-running autonomous coding (6 minute read) Single agents struggle with complex projects, and initial attempts at dynamic multi-agent coordination fail due to bottlenecks and risk-averse behavior. The Cursor team developed a successful \"Planners and Workers\" model, where Planners create tasks and Workers execute them independently, solving coordination issues. This system allowed hundreds of agents to run concurrently for weeks, generating over a million lines of code for ambitious...", "source": "tldr", "AI": {"tldr": "Cursor\u56e2\u961f\u5f00\u53d1\u4e86\"\u89c4\u5212\u8005\u4e0e\u5de5\u4f5c\u8005\"\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e2d\u7684\u74f6\u9888\u548c\u98ce\u9669\u89c4\u907f\u95ee\u9898\uff0c\u4f7f\u6570\u767e\u4e2a\u667a\u80fd\u4f53\u80fd\u591f\u5e76\u53d1\u8fd0\u884c\u6570\u5468\uff0c\u751f\u6210\u4e86\u8d85\u8fc7\u767e\u4e07\u884c\u4ee3\u7801\u3002", "motivation": "\u5355\u4e2a\u667a\u80fd\u4f53\u96be\u4ee5\u5904\u7406\u590d\u6742\u9879\u76ee\uff0c\u800c\u65e9\u671f\u7684\u52a8\u6001\u591a\u667a\u80fd\u4f53\u534f\u8c03\u65b9\u6cd5\u56e0\u74f6\u9888\u548c\u98ce\u9669\u89c4\u907f\u884c\u4e3a\u800c\u5931\u8d25\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u81ea\u4e3b\u7f16\u7801\u534f\u8c03\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\"\u89c4\u5212\u8005\u4e0e\u5de5\u4f5c\u8005\"\u6a21\u578b\uff1a\u89c4\u5212\u8005\u521b\u5efa\u4efb\u52a1\uff0c\u5de5\u4f5c\u8005\u72ec\u7acb\u6267\u884c\u4efb\u52a1\uff0c\u907f\u514d\u4e86\u534f\u8c03\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u5e76\u53d1\u8fd0\u884c\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u8ba9\u6570\u767e\u4e2a\u667a\u80fd\u4f53\u5e76\u53d1\u8fd0\u884c\u6570\u5468\uff0c\u751f\u6210\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u884c\u4ee3\u7801\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u9879\u76ee\u7684\u81ea\u4e3b\u7f16\u7801\u6311\u6218\u3002", "conclusion": "\"\u89c4\u5212\u8005\u4e0e\u5de5\u4f5c\u8005\"\u6a21\u578b\u662f\u89e3\u51b3\u5927\u89c4\u6a21\u81ea\u4e3b\u7f16\u7801\u534f\u8c03\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u957f\u671f\u3001\u590d\u6742\u7684\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u3002", "topic": "code agent"}}
{"id": "tldr.2601.5f9abe84", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecondthoughts.ai%2Fp%2Fthe-new-model-of-software-development%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/aODfzXjXJjOUka-NCKXhVPeELxaW7IiAplRCUZAOvoQ=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecondthoughts.ai%2Fp%2Fthe-new-model-of-software-development%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/aODfzXjXJjOUka-NCKXhVPeELxaW7IiAplRCUZAOvoQ=440", "authors": ["TLDR Newsletter"], "title": "Discarding the Shaft-and-Belt Model of Software Development", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecondthoughts.ai%2Fp%2Fthe-new-model-of-software-development%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/aODfzXjXJjOUka-NCKXhVPeELxaW7IiAplRCUZAOvoQ=440", "summary": "Discarding the Shaft-and-Belt Model of Software Development (7 minute read) AI coding agents like Claude Code will shift software from bloated mega-projects to bespoke, artisanal software. By driving small-project costs toward zero, vibe coding allows for personalized applications for the 50 percent of engineers building internal tools.", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u4ee3\u7406\u5c06\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f\uff0c\u4ece\u81c3\u80bf\u7684\u5927\u578b\u9879\u76ee\u8f6c\u5411\u5b9a\u5236\u5316\u3001\u624b\u5de5\u5316\u7684\u8f6f\u4ef6\uff0c\u901a\u8fc7\u964d\u4f4e\u5c0f\u9879\u76ee\u6210\u672c\u5b9e\u73b0\u4e2a\u6027\u5316\u5e94\u7528", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u91c7\u7528\"\u8f74\u5e26\u6a21\u578b\"\uff08\u5927\u578b\u96c6\u4e2d\u5f0f\u9879\u76ee\uff09\uff0c\u5bfc\u81f4\u8f6f\u4ef6\u81c3\u80bf\u3001\u6210\u672c\u9ad8\uff0c\u800cAI\u7f16\u7801\u4ee3\u7406\u7684\u51fa\u73b0\u4e3a\u6539\u53d8\u8fd9\u4e00\u6a21\u5f0f\u63d0\u4f9b\u4e86\u673a\u4f1a", "method": "\u5229\u7528AI\u7f16\u7801\u4ee3\u7406\uff08\u5982Claude Code\uff09\u5b9e\u73b0\"\u6c1b\u56f4\u7f16\u7801\"\uff0c\u5c06\u5c0f\u578b\u9879\u76ee\u7684\u5f00\u53d1\u6210\u672c\u8d8b\u8fd1\u4e8e\u96f6\uff0c\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u4e3a\u5185\u90e8\u5de5\u5177\u6784\u5efa\u4e2a\u6027\u5316\u5e94\u7528", "result": "AI\u7f16\u7801\u4ee3\u7406\u5c06\u4f7f\u8f6f\u4ef6\u5f00\u53d1\u4ece\u5927\u578b\u9879\u76ee\u8f6c\u5411\u5b9a\u5236\u5316\u8f6f\u4ef6\uff0c\u7279\u522b\u6709\u5229\u4e8e\u4e3a50%\u6784\u5efa\u5185\u90e8\u5de5\u5177\u7684\u5de5\u7a0b\u5e08\u521b\u5efa\u4e2a\u6027\u5316\u5e94\u7528\u7a0b\u5e8f", "conclusion": "AI\u7f16\u7801\u4ee3\u7406\u5c06\u5f7b\u5e95\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\uff0c\u6452\u5f03\u4f20\u7edf\u7684\"\u8f74\u5e26\u6a21\u578b\"\uff0c\u5b9e\u73b0\u8f6f\u4ef6\u5f00\u53d1\u7684\u6c11\u4e3b\u5316\u548c\u4e2a\u6027\u5316", "topic": "code agent"}}
{"id": "tldr.2601.1dfa8534", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F398Dio/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JKWST8otUHTrpaxSJygqqb8ZaqKCCNQZcgGJALcLQCY=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F398Dio/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JKWST8otUHTrpaxSJygqqb8ZaqKCCNQZcgGJALcLQCY=440", "authors": ["TLDR Newsletter"], "title": "Tool Search now in Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F398Dio/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/JKWST8otUHTrpaxSJygqqb8ZaqKCCNQZcgGJALcLQCY=440", "summary": "Tool Search now in Claude Code (2 minute read) Claude Code is introducing a new \"Tool Search\" feature for its MCP protocol to address the challenge of managing a growing number of tools. Previously, numerous MCP tool descriptions could consume a lot of context. This new feature dynamically loads tools via search only when needed.", "source": "tldr", "AI": {"tldr": "Claude Code\u5f15\u5165Tool Search\u529f\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u52a0\u8f7d\u5de5\u5177\u6765\u51cf\u5c11MCP\u534f\u8bae\u4e2d\u5de5\u5177\u63cf\u8ff0\u5bf9\u4e0a\u4e0b\u6587\u7684\u5360\u7528", "motivation": "\u968f\u7740MCP\u5de5\u5177\u6570\u91cf\u7684\u589e\u957f\uff0c\u5927\u91cf\u5de5\u5177\u63cf\u8ff0\u4f1a\u6d88\u8017\u5927\u91cf\u4e0a\u4e0b\u6587\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u7ba1\u7406\u65b9\u6848", "method": "\u5728Claude Code\u4e2d\u5f15\u5165Tool Search\u529f\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u641c\u7d22\u6309\u9700\u52a0\u8f7d\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u4e00\u6b21\u6027\u52a0\u8f7d\u6240\u6709\u5de5\u5177\u63cf\u8ff0", "result": "\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u7ba1\u7406\uff0c\u51cf\u5c11\u4e86\u4e0a\u4e0b\u6587\u5360\u7528\uff0c\u63d0\u9ad8\u4e86\u5de5\u5177\u4f7f\u7528\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387", "conclusion": "Tool Search\u529f\u80fd\u89e3\u51b3\u4e86MCP\u534f\u8bae\u4e2d\u5de5\u5177\u6570\u91cf\u589e\u957f\u5e26\u6765\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f00\u53d1\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2601.e366ef4c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvercel-labs%2Fagent-skills%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/K8nbFnkPEvsPLzYdVmdSP8J05LXhwmmRgbSREan4lSM=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvercel-labs%2Fagent-skills%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/K8nbFnkPEvsPLzYdVmdSP8J05LXhwmmRgbSREan4lSM=440", "authors": ["TLDR Newsletter"], "title": "Agent Skills", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fvercel-labs%2Fagent-skills%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/K8nbFnkPEvsPLzYdVmdSP8J05LXhwmmRgbSREan4lSM=440", "summary": "Agent Skills (GitHub Repo) Agent Skills is Vercel's collection of packaged instructions and scripts for AI coding agents. For example, the `react-best-practices` skill provides over 40 rules for optimizing React and Next.js performance across various categories like bundle size and data fetching.", "source": "tldr", "AI": {"tldr": "Agent Skills\u662fVercel\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u7684\u9884\u6253\u5305\u6307\u4ee4\u548c\u811a\u672c\u96c6\u5408\uff0c\u5305\u542b\u5982react-best-practices\u7b49\u6280\u80fd\uff0c\u63d0\u4f9b40\u591a\u6761\u4f18\u5316React\u548cNext.js\u6027\u80fd\u7684\u89c4\u5219", "motivation": "\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u9ad8\u6548\u5730\u4f7f\u7528AI\u4ee3\u7406\u8fdb\u884c\u4ee3\u7801\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728React\u548cNext.js\u751f\u6001\u7cfb\u7edf\u4e2d", "method": "\u901a\u8fc7GitHub\u4ed3\u5e93\u5f62\u5f0f\u63d0\u4f9b\u9884\u6253\u5305\u7684\u6280\u80fd\u96c6\u5408\uff0c\u6bcf\u4e2a\u6280\u80fd\u5305\u542b\u7279\u5b9a\u9886\u57df\u7684\u4f18\u5316\u89c4\u5219\u548c\u811a\u672c\uff0c\u5982react-best-practices\u6280\u80fd\u5305\u542b40\u591a\u6761\u6027\u80fd\u4f18\u5316\u89c4\u5219", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6280\u80fd\u5e93\uff0c\u4f7fAI\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u9075\u5faa\u884c\u4e1a\u6700\u4f73\u5b9e\u8df5\uff0c\u7279\u522b\u662f\u5728React\u548cNext.js\u7684\u6027\u80fd\u4f18\u5316\u65b9\u9762", "conclusion": "Agent Skills\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\u548c\u5f00\u53d1\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2601.9c4cc2b5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/TTximtc-BFetaLKHPNVqhKT700ZfXpqjm6voxHtHCZA=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/TTximtc-BFetaLKHPNVqhKT700ZfXpqjm6voxHtHCZA=440", "authors": ["TLDR Newsletter"], "title": "Getting Real Leverage from Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Festsauver.com%2Fblog%2Fclaude-code-workflow%3Futm_source=tldrdev/1/0100019bc18f3467-53bdd01b-3ddf-4d8f-b67c-212907ef7c4c-000000/TTximtc-BFetaLKHPNVqhKT700ZfXpqjm6voxHtHCZA=440", "summary": "Getting Real Leverage from Claude Code (18 minute read) Traditional Git worktrees alone when using Claude Code are not enough as they lead to shared resource conflicts like port contention and database clashes. Instead, it's better to use a local Kubernetes cluster (k3d) to provide true namespace isolation for each worktree, with Claude assisting in writing the necessary, often tedious, YAML configurations.", "source": "tldr", "AI": {"tldr": "\u4f7f\u7528\u672c\u5730Kubernetes\u96c6\u7fa4(k3d)\u66ff\u4ee3\u4f20\u7edfGit worktrees\uff0c\u4e3aClaude Code\u63d0\u4f9b\u771f\u6b63\u7684\u547d\u540d\u7a7a\u95f4\u9694\u79bb\uff0c\u89e3\u51b3\u7aef\u53e3\u51b2\u7a81\u548c\u6570\u636e\u5e93\u51b2\u7a81\u7b49\u5171\u4eab\u8d44\u6e90\u95ee\u9898", "motivation": "\u4f20\u7edfGit worktrees\u5728\u4f7f\u7528Claude Code\u65f6\u5b58\u5728\u5171\u4eab\u8d44\u6e90\u51b2\u7a81\u95ee\u9898\uff0c\u5982\u7aef\u53e3\u4e89\u7528\u548c\u6570\u636e\u5e93\u51b2\u7a81\uff0c\u9700\u8981\u66f4\u597d\u7684\u9694\u79bb\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u672c\u5730Kubernetes\u96c6\u7fa4(k3d)\u4e3a\u6bcf\u4e2aworktree\u63d0\u4f9b\u771f\u6b63\u7684\u547d\u540d\u7a7a\u95f4\u9694\u79bb\uff0c\u5229\u7528Claude\u8f85\u52a9\u7f16\u5199\u5fc5\u8981\u7684YAML\u914d\u7f6e", "result": "\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u8d44\u6e90\u9694\u79bb\uff0c\u907f\u514d\u4e86\u5171\u4eab\u8d44\u6e90\u51b2\u7a81\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u73af\u5883\u7684\u7a33\u5b9a\u6027\u548c\u5e76\u884c\u5f00\u53d1\u80fd\u529b", "conclusion": "\u672c\u5730Kubernetes\u96c6\u7fa4\u662f\u89e3\u51b3Claude Code\u5f00\u53d1\u73af\u5883\u4e2d\u8d44\u6e90\u9694\u79bb\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edfGit worktrees\u65b9\u6cd5", "topic": "swe application"}}
{"id": "tldr.2601.574fd3ab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpyFNOu/3/0100019bc1fc2fc2-d2c23ecd-8167-4907-b92d-9546d4f86989-000000/rxllp9ATZcHjTU9erEvVJPVXdHdUm-sVBmagyRsOfwg=440", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpyFNOu/3/0100019bc1fc2fc2-d2c23ecd-8167-4907-b92d-9546d4f86989-000000/rxllp9ATZcHjTU9erEvVJPVXdHdUm-sVBmagyRsOfwg=440", "authors": ["TLDR Newsletter"], "title": "January 29", "comment": "Source: TLDR Newsletter, Date: 2026-01-15, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FpyFNOu/3/0100019bc1fc2fc2-d2c23ecd-8167-4907-b92d-9546d4f86989-000000/rxllp9ATZcHjTU9erEvVJPVXdHdUm-sVBmagyRsOfwg=440", "summary": "Join experts from to explore the core competencies of Agentic AI. Are you ready for the AI economy?", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8Agentic AI\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u4e3aAI\u7ecf\u6d4e\u65f6\u4ee3\u505a\u51c6\u5907", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u53d1\u5c55\uff0cAgentic AI\u6210\u4e3aAI\u7ecf\u6d4e\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u6838\u5fc3\u80fd\u529b\u4ee5\u5e94\u5bf9\u672a\u6765\u6311\u6218", "method": "\u901a\u8fc7\u4e13\u5bb6\u8ba8\u8bba\u548c\u63a2\u7d22\u7684\u65b9\u5f0f\u5206\u6790Agentic AI\u7684\u6838\u5fc3\u80fd\u529b", "result": "\u8bc6\u522b\u4e86Agentic AI\u7684\u5173\u952e\u80fd\u529b\uff0c\u4e3a\u53c2\u4e0eAI\u7ecf\u6d4e\u505a\u597d\u51c6\u5907", "conclusion": "Agentic AI\u7684\u6838\u5fc3\u80fd\u529b\u5bf9\u4e8eAI\u7ecf\u6d4e\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u6301\u7eed\u63a2\u7d22\u548c\u51c6\u5907", "topic": "agent analysis"}}
