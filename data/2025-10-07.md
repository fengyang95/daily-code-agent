<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.LG](#cs.LG) [Total: 34]
- [wechat.article](#wechat.article) [Total: 7]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.SE](#cs.SE) [Total: 22]
- [tldr.article](#tldr.article) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: Graph-S³是一个基于LLM的文本图推理框架，通过合成逐步监督训练检索器，解决了大型图中相关内容的检索问题，在复杂推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据多以文本图形式存在，但现有检索方法要么依赖浅层嵌入相似性，要么需要大量标注数据和训练成本，导致性能不佳。

Method: 提出基于LLM的检索器，使用合成逐步监督训练，通过数据合成管道提取黄金子图生成奖励，采用两阶段训练方案学习交互式图探索策略。

Result: 在三个常见数据集上与七个强基线比较，平均准确率提升8.1%，F1分数提升9.7%，在复杂多跳推理任务中优势更明显。

Conclusion: Graph-S³框架通过合成监督训练有效解决了文本图检索问题，在复杂推理任务中表现突出，代码将开源。

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [2] [Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models](https://arxiv.org/abs/2510.03805)
*Canhui Wu,Qiong Cao,Chang Li,Zhenfang Wang,Chao Xue,Yuwei Fan,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 提出Step Pruner（SP）强化学习框架，通过惩罚冗余推理步骤来减少大型推理模型的过度思考问题，在保持准确性的同时显著降低响应长度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务上表现出色但存在过度思考问题，现有基于token惩罚的强化学习方法存在两个挑战：更少的token不一定对应更少的推理步骤，模型可能通过丢弃推理步骤来最小化token使用。

Method: 提出Step Pruner强化学习框架，包含步骤感知的奖励函数（优先正确性同时惩罚冗余步骤）和动态停止机制（当任何输出步骤超过长度上限时停止更新以防止步骤合并）。

Result: 在四个推理基准测试上的实验表明，SP实现了最先进的准确性，同时显著减少响应长度。在AIME24上减少了69.7%的token使用。

Conclusion: SP框架有效解决了大型推理模型的过度思考问题，通过步骤级别的优化实现了更高效的推理过程。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks
but often suffer from excessive verbosity, known as "overthinking." Existing
solutions via reinforcement learning (RL) typically penalize generated tokens
to promote conciseness. However, these methods encounter two challenges:
responses with fewer tokens do not always correspond to fewer reasoning steps,
and models may develop hacking behavior in later stages of training by
discarding reasoning steps to minimize token usage. In this work, we introduce
\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more
efficient reasoning by favoring compact reasoning steps. Our step-aware reward
function prioritizes correctness while imposing penalties for redundant steps,
and withholds rewards for incorrect responses to prevent the reinforcement of
erroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when
the length of any output step exceeds the upper limit, we halt updates to
prevent hacking behavior caused by merging steps. Extensive experiments across
four reasoning benchmarks demonstrate that SP achieves state-of-the-art
accuracy while significantly reducing response length. For instance, on AIME24,
SP reduces token usage by \textbf{69.7\%}.

</details>


### [3] [Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions](https://arxiv.org/abs/2510.03999)
*Yang Xu,Xuanming Zhang,Min-Hsuan Yeh,Jwala Dhamala,Ousmane Dia,Rahul Gupta,Yixuan Li*

Main category: cs.CL

TL;DR: 提出了首个用于在长序列任务中探测和评估LLM欺骗行为的模拟框架，发现欺骗行为具有模型依赖性，会随着事件压力增加，并持续削弱监督者信任。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM欺骗行为的研究大多局限于单轮提示，无法捕捉欺骗策略通常展开的长时程互动，需要开发能够评估动态情境压力下欺骗行为的框架。

Method: 构建多智能体系统：执行者智能体完成任务，监督者智能体评估进展并提供反馈，独立的欺骗审计员审查完整轨迹以识别欺骗行为。在11个前沿模型上进行广泛实验。

Result: 欺骗行为具有模型依赖性，随事件压力增加而增加，持续削弱监督者信任。定性分析揭示了隐瞒、含糊其辞和伪造等不同欺骗策略。

Conclusion: 欺骗是长时程互动中出现的风险，为评估未来LLM在现实世界信任敏感环境中的表现提供了基础。

Abstract: Deception is a pervasive feature of human communication and an emerging
concern in large language models (LLMs). While recent studies document
instances of LLM deception under pressure, most evaluations remain confined to
single-turn prompts and fail to capture the long-horizon interactions in which
deceptive strategies typically unfold. We introduce the first simulation
framework for probing and evaluating deception in LLMs under extended sequences
of interdependent tasks and dynamic contextual pressures. Our framework
instantiates a multi-agent system: a performer agent tasked with completing
tasks and a supervisor agent that evaluates progress, provides feedback, and
maintains evolving states of trust. An independent deception auditor then
reviews full trajectories to identify when and how deception occurs. We conduct
extensive experiments across 11 frontier models, spanning both closed- and
open-source systems, and find that deception is model-dependent, increases with
event pressure, and consistently erodes supervisor trust. Qualitative analyses
further reveal distinct strategies of concealment, equivocation, and
falsification. Our findings establish deception as an emergent risk in
long-horizon interactions and provide a foundation for evaluating future LLMs
in real-world, trust-sensitive contexts.

</details>


### [4] [LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization](https://arxiv.org/abs/2510.04013)
*Jiarui Liu,Jivitesh Jain,Mona Diab,Nishant Subramani*

Main category: cs.CL

TL;DR: 该论文提出使用模型内部激活信号来预测LLM输出正确性和外部上下文有效性，通过简单分类器在中间层激活上实现约75%的准确率，显著优于提示基准方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型具有巨大实用性，但可信度仍是主要问题：模型经常以高置信度生成错误信息。虽然上下文信息可以指导生成，但识别查询何时受益于检索上下文以及评估该上下文的有效性仍然具有挑战性。

Method: 使用可解释性方法，基于模型激活来预测输出正确性；探索模型内部是否包含关于外部上下文有效性的信号；考虑正确、错误和不相关的上下文，并引入指标来区分它们；在六个不同模型上实验，训练基于第一个输出token中间层激活的简单分类器。

Result: 基于中间层激活的简单分类器可以以约75%的准确率预测输出正确性，实现早期审计；基于模型内部的指标在区分正确和错误上下文方面显著优于提示基准方法，防止受污染上下文引入的不准确性。

Conclusion: 这些发现提供了一个视角来更好地理解LLM的底层决策过程，为模型可信度评估提供了新方法。

Abstract: Although large language models (LLMs) have tremendous utility,
trustworthiness is still a chief concern: models often generate incorrect
information with high confidence. While contextual information can help guide
generation, identifying when a query would benefit from retrieved context and
assessing the effectiveness of that context remains challenging. In this work,
we operationalize interpretability methods to ascertain whether we can predict
the correctness of model outputs from the model's activations alone. We also
explore whether model internals contain signals about the efficacy of external
context. We consider correct, incorrect, and irrelevant context and introduce
metrics to distinguish amongst them. Experiments on six different models reveal
that a simple classifier trained on intermediate layer activations of the first
output token can predict output correctness with about 75% accuracy, enabling
early auditing. Our model-internals-based metric significantly outperforms
prompting baselines at distinguishing between correct and incorrect context,
guarding against inaccuracies introduced by polluted context. These findings
offer a lens to better understand the underlying decision-making processes of
LLMs. Our code is publicly available at
https://github.com/jiarui-liu/LLM-Microscope

</details>


### [5] [Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?](https://arxiv.org/abs/2510.04031)
*Nelvin Tan,James Asikin Cheung,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 提出了一个基于反事实推理的框架来识别LLM分类决策中的关键词语，通过决策变化率量化词语重要性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM是黑盒且调用成本高，需要解释其分类决策，特别是识别影响分类结果的关键词语。

Method: 引入决策变化率框架，通过反事实推理来量化词语对LLM分类决策的重要性。

Result: 实验结果表明使用反事实推理有助于识别分类决策中的关键词语。

Conclusion: 反事实推理可以有效帮助解释LLM的分类决策，识别影响决策的关键因素。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their
impressive abilities that arise from large training datasets and large model
sizes. More recently, they have been shown to be very effective in textual
classification tasks, motivating the need to explain the LLMs' decisions.
Motivated by practical constrains where LLMs are black-boxed and LLM calls are
expensive, we study how incorporating counterfactuals into LLM reasoning can
affect the LLM's ability to identify the top words that have contributed to its
classification decision. To this end, we introduce a framework called the
decision changing rate that helps us quantify the importance of the top words
in classification. Our experimental results show that using counterfactuals can
be helpful.

</details>


### [6] [PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity](https://arxiv.org/abs/2510.04080)
*Zixin Song,Bowen Zhang,Qian-Wen Zhang,Di Yin,Xing Sun,Chunping Li*

Main category: cs.CL

TL;DR: 提出了PoLi-RL框架，通过两阶段课程学习和并行切片排名奖励机制，成功将强化学习应用于条件语义文本相似性任务，在C-STS基准上实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有C-STS方法局限于判别模型，未能充分利用LLM和RL的最新进展。RL特别适合该任务，可直接优化不可微的Spearman排名指标并指导推理过程。

Method: PoLi-RL框架：1) 两阶段课程学习：先用点式奖励训练基础评分能力，再结合点式、对式和列式目标的混合奖励；2) 并行切片排名奖励(PSRR)：在并行切片中计算排名奖励，为每个完成提供精确的学习信号。

Result: 在官方C-STS基准上达到48.18的Spearman相关系数，为交叉编码器架构建立了新的SOTA。

Conclusion: 这是首个成功将RL应用于C-STS的工作，为在复杂、基于排名的条件判断任务上训练LLM提供了强大而精确的范式。

Abstract: Conditional Semantic Textual Similarity (C-STS) measures the semantic
proximity between text segments under a specific condition, thereby overcoming
the ambiguity inherent in traditional STS. However, existing methods are
largely confined to discriminative models, failing to fully integrate recent
breakthroughs in the NLP community concerning Large Language Models (LLMs) and
Reinforcement Learning (RL). RL is a particularly well-suited paradigm for this
task, as it can directly optimize the non-differentiable Spearman ranking
metric and guide the reasoning process required by C-STS. However, we find that
naively applying listwise RL fails to produce meaningful improvements, as the
model is overwhelmed by complex, coarse-grained reward signals. To address this
challenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning
framework. PoLi-RL employs a two-stage curriculum: it first trains the model
with simple pointwise rewards to establish fundamental scoring capabilities,
then transitions to a hybrid reward that combines pointwise, pairwise, and
listwise objectives to refine the model's ability to discern subtle semantic
distinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward
(PSRR) mechanism that computes ranking rewards in parallel slices, where each
slice comprises same-indexed completions from different samples. This provides
a precise, differentiated learning signal for each individual completion,
enabling granular credit assignment and effective optimization. On the official
C-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,
establishing a new SOTA for the cross-encoder architecture. As the first work
to successfully apply RL to C-STS, our study introduces a powerful and precise
paradigm for training LLMs on complex, ranking-based conditional judgment
tasks.

</details>


### [7] [Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning](https://arxiv.org/abs/2510.04081)
*Honglin Lin,Qizhi Pei,Xin Gao,Zhuoshi Pan,Yu Li,Juntao Li,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: Caco框架通过代码驱动的增强方法自动合成高质量、可验证且多样化的指令-CoT推理数据，解决了现有CoT方法在生成控制、质量和多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在生成不可控、质量不足和推理路径多样性有限的问题，而基于代码的CoT方法通常局限于预定义的数学问题，阻碍了可扩展性和泛化性。

Method: 首先在统一代码格式下对基于代码的CoT生成器进行微调，然后扩展到大量多样化推理轨迹的生成，通过代码执行和基于规则的过滤进行自动验证，最后将过滤后的输出反向工程为自然语言指令和语言CoT。

Result: 在创建的Caco-1.3M数据集上的实验表明，Caco训练的模型在数学推理基准测试中表现出强大的竞争性能，优于现有强基线。

Conclusion: Caco建立了一种无需人工干预构建自持续、可信赖推理系统的范式，其代码锚定验证和指令多样性有助于在未见任务上实现优越的泛化能力。

Abstract: Reasoning capability is pivotal for Large Language Models (LLMs) to solve
complex tasks, yet achieving reliable and scalable reasoning remains
challenging. While Chain-of-Thought (CoT) prompting has become a mainstream
approach, existing methods often suffer from uncontrolled generation,
insufficient quality, and limited diversity in reasoning paths. Recent efforts
leverage code to enhance CoT by grounding reasoning in executable steps, but
such methods are typically constrained to predefined mathematical problems,
hindering scalability and generalizability. In this work, we propose Caco
(Code-Assisted Chain-of-ThOught), a novel framework that automates the
synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning
data through code-driven augmentation. Unlike prior work, Caco first fine-tunes
a code-based CoT generator on existing math and programming solutions in a
unified code format, then scales the data generation to a large amount of
diverse reasoning traces. Crucially, we introduce automated validation via code
execution and rule-based filtering to ensure logical correctness and structural
diversity, followed by reverse-engineering filtered outputs into natural
language instructions and language CoTs to enrich task adaptability. This
closed-loop process enables fully automated, scalable synthesis of reasoning
data with guaranteed executability. Experiments on our created Caco-1.3M
dataset demonstrate that Caco-trained models achieve strong competitive
performance on mathematical reasoning benchmarks, outperforming existing strong
baselines. Further analysis reveals that Caco's code-anchored verification and
instruction diversity contribute to superior generalization across unseen
tasks. Our work establishes a paradigm for building self-sustaining,
trustworthy reasoning systems without human intervention.

</details>


### [8] [Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization](https://arxiv.org/abs/2510.04182)
*Wengao Ye,Yan Liang,Lianlei Shan*

Main category: cs.CL

TL;DR: LTPO是一种无需参数更新的测试时优化框架，通过将中间潜在思想向量作为动态参数进行优化，提升LLM在挑战性任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决潜在推理在具有挑战性的分布外任务中表现脆弱的问题，在需要稳健推理的关键场景中增强LLM的推理能力。

Method: 使用在线策略梯度方法，基于冻结LLM自身输出分布计算的置信度奖励信号，优化中间潜在思想向量。

Result: 在五个推理基准测试中，LTPO不仅达到或超过强基线，在标准任务上表现优异，在极具挑战性的AIME基准上更是实现了显著改进，而现有潜在推理基线准确率接近零。

Conclusion: LTPO展示了在复杂推理任务上的独特能力，为无需外部监督或昂贵文本生成的测试时推理优化提供了有效解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have shifted from
explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,
where intermediate thoughts are represented as vectors rather than text.
However, latent reasoning can be brittle on challenging, out-of-distribution
tasks where robust reasoning is most critical. To overcome these limitations,
we introduce Latent Thought Policy Optimization (LTPO), a parameter-free
framework that enhances LLM reasoning entirely at test time, without requiring
model parameter updates. LTPO treats intermediate latent "thought" vectors as
dynamic parameters that are actively optimized for each problem instance. It
employs an online policy gradient method guided by an intrinsic,
confidence-based reward signal computed directly from the frozen LLM's own
output distributions, eliminating the need for external supervision or
expensive text generation during optimization. Extensive experiments on five
reasoning benchmarks show that LTPO not only matches or surpasses strong
baselines on standard tasks but also demonstrates remarkable robustness where
others fail. Most notably, on highly challenging AIME benchmarks where existing
latent reasoning baselines collapse to near-zero accuracy, LTPO delivers
substantial improvements, showcasing a unique capability for complex reasoning.

</details>


### [9] [Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards](https://arxiv.org/abs/2510.04214)
*Zhuoran Zhuang,Ye Chen,Xia Zeng,Chao Luo,Luhui Liu,Yihan Chen*

Main category: cs.CL

TL;DR: 提出了REPO强化学习框架，通过整合偏好奖励模型、说服行为奖励和程序化奖励，优化LLM在在线旅行机构价格谈判中的表现，显著提升对话质量和约束遵守。


<details>
  <summary>Details</summary>
Motivation: 传统后训练方法在商业谈判场景中容易过拟合脚本，无法捕捉细微的说服风格，且难以强制执行可验证的业务约束。

Method: 使用Reward-Enhanced Policy Optimization (REPO)框架，结合偏好训练的奖励模型、说服行为奖励判断器和程序化奖励函数，通过强化学习对齐LLM与异构奖励信号。

Result: 在真实对话评估中，REPO将平均对话评分提升至4.63，比基准高1.20分；66.67%的对话至少有一个优秀回复，比GRPO高23.34个百分点；93.33%的坏案例修复率。

Conclusion: REPO框架有效提升了LLM在商业谈判中的表现，产生了超越黄金标注的新兴能力，如主动同理心、局部推理和校准策略。

Abstract: We study deploying large language models (LLMs) as business development (BD)
agents for persuasive price negotiation in online travel agencies (OTAs), where
aligning traveler affordability and hotel profitability directly affects
bookings, partner relationships, and access to travel. The agent must follow a
Standard Operating Procedure (SOP) while conducting multi-turn persuasion,
interpreting colloquial inputs, and adhering to guardrails (no over-promising,
no hallucinations). Conventional post-training -- supervised fine-tuning (SFT)
or single-source reward optimization -- overfits scripts, misses nuanced
persuasive style, and fails to enforce verifiable business constraints.
  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement
learning post-training framework that aligns an LLM with heterogeneous rewards:
a preference-trained reward model (RM) for dense human alignment, a reward
judge (RJ) for high-level persuasive behavior and SOP compliance, and
programmatic reward functions (RF) for deterministic checks on numerics,
formatting, and guardrails. A straightforward enhancement mechanism is proposed
to combine the RM with RJ and RF signals to curb reward hacking and improve
negotiation quality. In production-style evaluations -- approximately 150 turns
from real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts
average dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference
Optimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),
increases the share of conversations with at least one excellent response to
66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix
rate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also
observe emergent capabilities -- proactive empathy, localized reasoning,
calibrated tactics -- that surpass gold annotations.

</details>


### [10] [Epistemic Diversity and Knowledge Collapse in Large Language Models](https://arxiv.org/abs/2510.04226)
*Dustin Wright,Sarah Masud,Jared Moore,Srishti Yadav,Maria Antoniak,Chan Young Park,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 该研究提出了一种测量LLM认知多样性的新方法，通过实证研究发现：虽然新模型生成更多样化的声明，但几乎所有模型的认知多样性都低于基础网络搜索；模型大小对认知多样性有负面影响，而检索增强生成(RAG)有正面影响；英语语言在国别声明中占主导地位。


<details>
  <summary>Details</summary>
Motivation: LLM倾向于生成同质化文本，这可能导致知识崩溃风险，即同质化的LLM会随时间推移缩小可访问信息的范围。现有研究局限于封闭式选择题设置或模糊语义特征，且未关注跨时间和文化背景的趋势。

Method: 提出测量认知多样性的新方法，对27个LLM、155个涵盖12个国家的主题、200个来自真实用户聊天的提示变体进行广泛实证研究。

Result: 较新模型倾向于生成更多样化的声明，但几乎所有模型的认知多样性都低于基础网络搜索；模型大小对认知多样性有负面影响，RAG有正面影响，但RAG的改善效果因文化背景而异；国别声明更多反映英语语言而非当地语言。

Conclusion: LLM存在认知多样性不足的问题，特别是在跨文化语境中，英语语言在知识表示中占主导地位，揭示了认知表示方面的差距。

Abstract: Large language models (LLMs) tend to generate lexically, semantically, and
stylistically homogenous texts. This poses a risk of knowledge collapse, where
homogenous LLMs mediate a shrinking in the range of accessible information over
time. Existing works on homogenization are limited by a focus on closed-ended
multiple-choice setups or fuzzy semantic features, and do not look at trends
across time and cultural contexts. To overcome this, we present a new
methodology to measure epistemic diversity, i.e., variation in real-world
claims in LLM outputs, which we use to perform a broad empirical study of LLM
knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200
prompt variations sourced from real user chats. For the topics in our study, we
show that while newer models tend to generate more diverse claims, nearly all
models are less epistemically diverse than a basic web search. We find that
model size has a negative impact on epistemic diversity, while
retrieval-augmented generation (RAG) has a positive impact, though the
improvement from RAG varies by the cultural context. Finally, compared to a
traditional knowledge source (Wikipedia), we find that country-specific claims
reflect the English language more than the local one, highlighting a gap in
epistemic representation

</details>


### [11] [SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations](https://arxiv.org/abs/2510.04398)
*Buyun Liang,Liangzu Peng,Jinqi Luo,Darshan Thaker,Kwan Ho Ryan Chan,René Vidal*

Main category: cs.CL

TL;DR: 提出SECA方法，通过语义等效且连贯的对抗攻击来引发LLM幻觉，相比现有方法产生更现实的提示且保持语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法产生不现实的提示（如插入乱码或改变原意），无法反映实际中幻觉如何发生，需要开发能保持语义等效性的现实攻击方法。

Method: 将寻找现实攻击建模为语义等效和连贯约束下的优化问题，提出约束保持的零阶优化方法搜索对抗性提示。

Result: 在开放式多项选择问答任务中，SECA达到更高攻击成功率，同时几乎不违反约束条件。

Conclusion: SECA揭示了开源和商业LLM对现实且合理的提示变异的敏感性。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-risk domains.
However, state-of-the-art LLMs often produce hallucinations, raising serious
concerns about their reliability. Prior work has explored adversarial attacks
for hallucination elicitation in LLMs, but it often produces unrealistic
prompts, either by inserting gibberish tokens or by altering the original
meaning. As a result, these approaches offer limited insight into how
hallucinations may occur in practice. While adversarial attacks in computer
vision often involve realistic modifications to input images, the problem of
finding realistic adversarial prompts for eliciting LLM hallucinations has
remained largely underexplored. To address this gap, we propose Semantically
Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic
modifications to the prompt that preserve its meaning while maintaining
semantic coherence. Our contributions are threefold: (i) we formulate finding
realistic attacks for hallucination elicitation as a constrained optimization
problem over the input prompt space under semantic equivalence and coherence
constraints; (ii) we introduce a constraint-preserving zeroth-order method to
effectively search for adversarial yet feasible prompts; and (iii) we
demonstrate through experiments on open-ended multiple-choice question
answering tasks that SECA achieves higher attack success rates while incurring
almost no constraint violations compared to existing methods. SECA highlights
the sensitivity of both open-source and commercial gradient-inaccessible LLMs
to realistic and plausible prompt variations. Code is available at
https://github.com/Buyun-Liang/SECA.

</details>


### [12] [Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners](https://arxiv.org/abs/2510.04454)
*Xiangchi Yuan,Xiang Chen,Tong Yu,Dachuan Shi,Can Jin,Wenke Lee,Saayan Mitra*

Main category: cs.CL

TL;DR: 提出了一种即插即用框架，通过动态选择挑战性示例进行监督微调，将SFT集成到强化学习中，显著减少数据需求并避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有方法结合SFT和RL面临数据效率低、算法特定设计和灾难性遗忘三大挑战，需要更高效的集成方案。

Method: 动态选择挑战性示例进行SFT，选择高熵token计算损失，并冻结对RL关键参数，实现SFT与RL的高效结合。

Result: 仅使用1.5%的SFT数据和20.4%的RL数据就达到最先进的推理性能。

Conclusion: 提供了一种高效、即插即用的SFT与RL结合方案，显著提升推理能力。

Abstract: Large Language Models (LLMs) show strong reasoning abilities, often amplified
by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although
RL algorithms can substantially improve reasoning, they struggle to expand
reasoning boundaries because they learn from their own reasoning trajectories
rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers
complementary benefits but typically requires large-scale data and risks
overfitting. Recent attempts to combine SFT and RL face three main challenges:
data inefficiency, algorithm-specific designs, and catastrophic forgetting. We
propose a plug-and-play framework that dynamically integrates SFT into RL by
selecting challenging examples for SFT. This approach reduces SFT data
requirements and remains agnostic to the choice of RL or SFT algorithm. To
mitigate catastrophic forgetting of RL-acquired skills during SFT, we select
high-entropy tokens for loss calculation and freeze parameters identified as
critical for RL. Our method achieves state-of-the-art (SoTA) reasoning
performance using only 1.5% of the SFT data and 20.4% of the RL data used by
prior SoTA, providing an efficient and plug-and-play solution for combining SFT
and RL in reasoning post-training.

</details>


### [13] [Multi-Agent Tool-Integrated Policy Optimization](https://arxiv.org/abs/2510.04678)
*Zhanfeng Mo,Xingxuan Li,Yuntao Chen,Lidong Bing*

Main category: cs.CL

TL;DR: 提出MATPO方法，在单个LLM实例中通过强化学习训练规划者和工作者两种角色，解决多智能体工具集成框架的上下文限制和噪声工具响应问题。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法存在上下文长度限制和工具响应噪声问题，而多智能体框架缺乏有效的强化学习后训练方法。

Method: MATPO通过角色特定提示在单个LLM实例中训练规划者和工作者角色，采用基于原则的信用分配机制跨角色轨迹进行优化。

Result: 在GAIA-text、WebWalkerQA和FRAMES数据集上，MATPO平均相对性能提升18.38%，对噪声工具输出表现出更强鲁棒性。

Conclusion: 在单个LLM中统一多个智能体角色是有效的，为稳定高效的多智能体强化学习训练提供了实用见解。

Abstract: Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

</details>


### [14] [Imperceptible Jailbreaking against Large Language Models](https://arxiv.org/abs/2510.05025)
*Kuofeng Gao,Yiming Li,Chao Du,Xin Wang,Xingjun Ma,Shu-Tao Xia,Tianyu Pang*

Main category: cs.CL

TL;DR: 提出了一种基于Unicode变体选择器的不可察觉越狱攻击方法，通过在恶意问题后附加不可见的变体选择器字符，在不改变视觉显示的情况下改变tokenization，从而诱导AI模型产生有害响应。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉模态越狱攻击依赖不可察觉的对抗扰动，而文本模态攻击通常需要可见修改。本文旨在探索文本模态中不可察觉的越狱攻击可能性。

Method: 利用Unicode变体选择器字符，构建不可见的对抗后缀；提出链式搜索流水线来生成此类对抗后缀；通过改变tokenization但不改变视觉显示来实施攻击。

Result: 实验表明该方法对四个对齐的LLM实现了高攻击成功率，并能推广到提示注入攻击，且不产生任何可见的提示修改。

Conclusion: 证明了文本模态中不可察觉越狱攻击的可行性，揭示了基于Unicode字符的潜在安全威胁。

Abstract: Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

</details>


### [15] [SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs](https://arxiv.org/abs/2510.05069)
*Dachuan Shi,Abedelkadir Asi,Keying Li,Xiangchi Yuan,Leyan Pan,Wenke Lee,Wen Xiao*

Main category: cs.CL

TL;DR: SwiReasoning是一个无需训练的大语言模型推理框架，通过动态切换显式和潜在推理来平衡探索与利用，提高准确性和token效率。


<details>
  <summary>Details</summary>
Motivation: 解决纯潜在推理带来的搜索分布扩散、概率质量分散和收敛困难问题，以及过度思考导致的token浪费。

Method: 基于熵趋势估计块级置信度，动态切换显式和潜在推理模式，并限制思考块切换次数来控制过度思考。

Result: 在数学和STEM基准测试中，平均准确率提升1.5%-2.8%，在受限预算下token效率提升56%-79%。

Conclusion: SwiReasoning通过动态推理模式切换有效解决了潜在推理的收敛问题和过度思考问题，显著提升了推理准确性和效率。

Abstract: Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: 提出了分层偏好学习（HPL）框架，通过多粒度偏好信号优化LLM智能体，解决了轨迹级DPO信号过于粗糙和步骤级DPO过于短视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的离线方法如DPO在优化LLM智能体时面临粒度不匹配问题：轨迹级DPO信号过于粗糙，步骤级DPO过于短视，无法准确评估多步行为的价值。

Method: HPL框架结合轨迹级和步骤级DPO，核心创新是组级偏好优化和双层课程调度。首先将专家轨迹分解为语义一致的动作组，生成对比次优组进行细粒度偏好学习，然后通过课程调度器从简单到复杂组织学习过程。

Result: 在三个具有挑战性的智能体基准测试中，HPL优于现有最先进方法。分析表明分层DPO损失有效整合多粒度偏好信号，双层课程对解决从简单行为到复杂多步序列的任务至关重要。

Conclusion: HPL通过分层偏好学习和课程调度成功解决了LLM智能体优化中的粒度不匹配问题，为复杂长视野问题的解决提供了有效框架。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [17] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 提出Triple-BERT方法，使用单智能体强化学习解决网约车平台大规模订单调度问题，通过动作分解策略和BERT网络处理大规模动作和观测空间，在真实数据集上相比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临大规模实时订单调度挑战，现有MARL方法存在全局信息捕获不足、协作差或维度灾难问题，需要更有效的解决方案。

Method: 基于TD3变体构建集中式单智能体强化学习，采用动作分解策略将联合动作概率分解为单个司机动作概率，使用BERT网络通过参数重用和注意力机制处理大规模观测空间。

Result: 在曼哈顿真实网约车数据集上，相比现有最优方法提升约11.95%，服务订单增加4.26%，接单时间减少22.25%。

Conclusion: Triple-BERT方法能有效解决大规模订单调度问题，在性能和效率上均优于现有方法。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [18] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 本文系统研究了推理数据在预训练和指令微调不同阶段引入对LLM性能的影响，发现预训练阶段引入推理数据效果更好，建立了后期微调无法完全复现的基础能力。


<details>
  <summary>Details</summary>
Motivation: 由于前沿模型的预训练语料不透明，推理数据在不同训练阶段引入的效果缺乏系统研究，需要明确早期引入推理数据是否优于后期微调，以及是否存在过拟合风险。

Method: 通过控制实验，系统研究推理数据在规模、多样性和质量方面的变化，以及在不同训练阶段（预训练vs指令微调）引入的效果差异。

Result: 预训练阶段引入推理数据效果显著（平均提升19%），建立了无法通过后期SFT完全复现的基础能力。预训练受益于推理模式多样性（平均提升11%），而SFT对数据质量更敏感（平均提升15%）。

Conclusion: 研究挑战了语言建模与推理的传统分离，为在整个训练流程中战略性地分配数据提供了原则性指导，以构建更强大的模型。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [19] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 提出了决策势面（DPS）作为分析大语言模型决策边界的新概念，并开发了K-DPS算法来近似构建LLM的决策边界，仅需有限次序列采样即可实现可忽略误差的近似。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型词汇序列规模巨大且具有自回归特性，直接构建其决策边界在计算上不可行，需要开发新的分析方法。

Method: 定义决策势面（DPS）概念，基于不同采样序列的区分置信度来捕捉决策边界潜力，并提出K-DPS近似算法，通过有限次采样构建决策边界。

Result: 理论推导了K-DPS与理想DPS之间的绝对误差、期望误差和误差集中度的上界，证明误差可通过采样次数进行权衡，并通过多LLM和语料库实验验证了结果。

Conclusion: DPS为分析LLM决策边界提供了可行方法，K-DPS算法能够以可接受的误差高效近似决策边界。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [20] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出了一种混合归因和剪枝（HAP）框架，用于在语言模型中发现稀疏子网络（电路），该框架结合了归因修补的速度优势和边缘剪枝的忠实性优势。


<details>
  <summary>Details</summary>
Motivation: 现有的电路发现算法面临基本权衡：归因修补速度快但对完整模型不忠实，边缘剪枝忠实但计算成本高。需要一种能兼顾速度和忠实性的方法。

Method: 使用归因修补识别高潜力子图，然后应用边缘剪枝从中提取忠实电路。这种混合方法比基线算法快46%且不牺牲电路忠实性。

Result: HAP方法在间接对象识别任务中保留了合作电路组件（如S抑制头），这些组件在归因修补方法中在高稀疏度下会被剪枝掉。

Conclusion: HAP可以成为提高机制可解释性研究扩展到更大模型的有效方法。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [21] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 提出了一个基于多目标强化学习的流行病干预框架，使用随机微分方程模拟器准确再现疫情动态，并在COVID-19、脊髓灰质炎、流感和麻疹等不同病原体上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行揭示了在疾病控制和社会经济稳定之间取得平衡的迫切需求，需要开发能够处理竞争目标的干预策略框架。

Method: 结合多目标强化学习（MORL）和新的随机微分方程（SDE）疫情模拟器，训练Pareto条件网络（PCN）代理来发现最优干预策略。

Result: 模拟器比其他常用模型具有更高的保真度，能够量化流行病控制与经济稳定之间的政策权衡，并展示了不同病原体需要根本不同的干预策略。

Conclusion: 该工作提供了一个稳健且适应性强的框架，支持透明、基于证据的公共卫生危机缓解政策制定。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [22] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于龙卷风预测的多模态LLM框架，通过评估LLM在复杂真实世界任务中的表现，发现人类专家显著优于最先进模型。


<details>
  <summary>Details</summary>
Motivation: 需要评估LLM在复杂、高影响的真实世界任务中的表现，以衡量其作为推理智能体的真实准备程度。

Method: 使用多模态LLM端到端处理高分辨率对流允许预报档案中的异质时空数据，在40天期间查询3,625个预报图和40,125个预报探空数据。

Result: 人类专家显著优于最先进模型，模型倾向于产生幻觉和过度预测风险强度，在复杂动态系统中表现出较差的时空推理能力。

Conclusion: AgentCaster旨在推进LLM智能体在关键领域挑战性推理任务中的改进研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [23] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经算子(PINO)的深度强化学习框架，用于湍流建模和控制，在未见的高雷诺数流动中实现了39.0%的减阻效果。


<details>
  <summary>Details</summary>
Motivation: 数值模拟湍流控制对壁面摩擦的影响计算成本高昂，需要开发更高效的建模和控制方法。

Method: 使用基于模型的强化学习进行预测控制，通过物理信息神经算子(PINO)联合学习湍流控制的策略和观测器模型。

Result: PINO-PC在雷诺数为15,000的高雷诺数未见流动中实现了39.0%的减阻，比之前的流体控制方法提高了32%以上。

Conclusion: 该方法在挑战性场景下优于先前的无模型强化学习方法，能够准确捕捉湍流中的精细尺度。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [24] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 该论文提出使用结构化论证方法为AI系统提供可验证的解释，在论证关系分类任务上达到SOTA性能，并应用于多智能体风险评估场景。


<details>
  <summary>Details</summary>
Motivation: 人类思维过程不可观察但社会依赖可验证论证，AI可解释性应遵循相同原则，提供可验证的推理链而非机制透明性。

Method: 使用结构化论证方法，将LLM文本转换为论证图，采用双极假设论证框架捕获支持/攻击关系，实现自动幻觉检测和测试时反馈机制。

Result: 在AAEC数据集上达到94.44宏F1（比先前工作高5.7分），在Argumentative MicroTexts关系分类上达到0.81宏F1（比先前结果高约0.07）。

Conclusion: 结构化论证方法为AI系统提供了可验证的解释框架，优于传统可解释性和LLM生成解释，支持多智能体协作和迭代优化。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [25] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文研究了有限时域离线强化学习中的策略评估和策略优化问题，在函数逼近框架下提出了统计有效的学习算法。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在仅假设数据覆盖性和策略价值函数线性可实现的情况下，统计有效的学习是不可能的。最近的研究在假设数据以轨迹形式提供的情况下实现了策略优化的有效学习，本文旨在解决策略评估的类似问题。

Method: 在数据以轨迹形式提供且满足覆盖性和qπ-可实现性的假设下，提出了统计有效的策略评估学习器，并对现有策略优化学习器的样本复杂度进行了更严格的分析。

Result: 成功开发了在轨迹数据假设下的统计有效策略评估学习器，并改进了现有策略优化学习器的样本复杂度界限。

Conclusion: 在轨迹数据假设下，离线强化学习中的策略评估和策略优化都可以实现统计有效的学习，且样本复杂度可以得到进一步优化。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [26] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC是一种新的无模型强化学习算法，通过融合分布RL和裁剪双Q学习来训练表达性扩散策略，在18个困难RL任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够有效在线训练表达性扩散策略的算法，避免传统策略梯度的高方差和通过时间反向传播的复杂性。

Method: 使用避免高方差的策略改进目标，结合融合分布RL和裁剪双Q学习的鲁棒分布评论家。

Result: 在18个困难RL任务（包括Humanoid、Dog和Shadow Hand等领域）上实现最先进性能，涵盖密集奖励和目标条件RL场景。

Conclusion: D2AC算法在标准基准和生物启发的捕食者-猎物任务中都表现出强大的行为鲁棒性和泛化能力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [27] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出RAPID算法，通过大批量推理和小批量离策略更新，减少RL训练时间11%-34%，同时保持或提升准确性


<details>
  <summary>Details</summary>
Motivation: RL算法在微调小型语言模型时资源密集、训练时间长，需要优化计算效率

Method: 使用大批量推理和小批量离策略策略梯度更新，结合组优势估计和重要性加权估计器纠正偏差

Result: 在三个基准测试中，运行时间减少11%-34%，准确率相似或更好

Conclusion: RAPID算法能显著减少RL训练时间而不牺牲性能

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [28] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: CS-RLHF是一种新的安全强化学习方法，通过基于惩罚的公式替代拉格朗日方法，使用在大规模语料上训练的成本模型来分配语义安全分数，无需双变量更新即可保证安全约束的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CMDP的方法存在两个主要问题：对评分机制高度敏感，以及双变量调优过程计算昂贵且无法提供可证明的安全保证。

Method: 引入CS-RLHF方法，使用在大规模语料上训练的成本模型分配语义安全分数，采用基于惩罚的公式设计，基于约束优化中的精确惩罚函数理论。

Result: 实证评估显示CS-RLHF优于最先进的LLM模型响应，在正常和越狱提示下效率至少提高5倍。

Conclusion: CS-RLHF通过基于惩罚的方法有效解决了现有安全强化学习方法的问题，提供了更好的安全保证和效率。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [29] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 该研究探索了在公共卫生情感分析中，大型语言模型的上下文学习如何受到数据投毒攻击的破坏。通过引入微小对抗性扰动，导致高达67%的情感标签翻转，但通过谱签名防御成功保持了数据集完整性。


<details>
  <summary>Details</summary>
Motivation: 将先前关于上下文学习投毒的理论研究扩展到公共卫生话语分析这一实际高风险场景，揭示LLM部署的风险和防御潜力。

Method: 在人类偏肺病毒推文数据中引入同义词替换、否定插入和随机扰动等对抗性扰动，然后应用谱签名防御来过滤投毒样本。

Result: 微小扰动导致高达67%的情感标签翻转；防御后上下文学习准确率稳定在46.7%，逻辑回归验证达到100%准确率。

Conclusion: 上下文学习在攻击下表现脆弱，但谱防御能够提高AI系统在健康相关社交媒体监测中的可靠性。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [30] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出一种基于强化学习的深部脑刺激方法，使用可测量的脑活动数据来调整刺激参数，在帕金森病生物标志物抑制方面优于传统临床方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的DBS方法依赖无法在患者体内测量的生物标志物，仅适用于脑芯片模拟环境，需要开发能够在真实临床环境中使用的自适应DBS系统。

Method: 使用TD3强化学习算法，在基底神经节脑区模型上训练RL智能体，根据可测量的脑活动自适应调整刺激频率和幅度参数。

Result: 与现有临床DBS方法相比，该智能体在抑制与帕金森病严重程度相关的生物标志物方面表现更优，且仅依赖可在真实环境中测量的信息。

Conclusion: 该方法为训练针对个体患者需求的个性化RL智能体开辟了可能性，使基于强化学习的DBS技术更接近临床应用。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [31] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出一种基于谱滤波的线性强化学习方法，通过自适应正则化参数选择策略，在保证可解释性的同时提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注性能，依赖事后解释来提供可解释性。本文旨在设计一种以可解释性为导向且性能增强的RL方法。

Method: 提出基于谱滤波的线性RL方法，扩展了岭回归方法，通过谱滤波函数阐明正则化在控制估计误差中的作用，并设计基于偏差-方差权衡的自适应正则化参数选择策略。

Result: 理论分析建立了参数估计和泛化误差的近乎最优边界。在快手和淘宝的真实数据集上的实验表明，该方法在决策质量上优于或匹配现有基线方法。

Conclusion: 该方法有潜力弥合RL理论与实际决策之间的差距，在管理场景中提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [32] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: EvoEngineer是一个系统化的LLM代码进化框架，专门用于CUDA内核优化，在91个真实CUDA内核上实现了2.72倍的平均中位数加速和69.8%的代码有效性。


<details>
  <summary>Details</summary>
Motivation: 解决CUDA内核优化领域生态系统碎片化、问题定义不清晰的问题，以及通用LLM代码进化方法无法满足CUDA内核优化的严格正确性要求。

Method: 首先形式化CUDA内核优化任务，然后建立系统化的LLM代码进化框架EvoEngineer，提供优化策略设计指导，平衡性能与正确性。

Result: 在91个真实CUDA内核上，EvoEngineer实现了2.72倍平均中位数加速，代码有效性达69.8%，最大加速比达36.75倍，在50个超过2倍加速的操作中有56%达到最高加速。

Conclusion: EvoEngineer在CUDA内核优化中实现了性能与正确性的原则性平衡，优于现有方法。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [33] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出TROLL方法，用离散可微分信任区域投影替代PPO的clip机制，为LLM奖励微调提供原则性的token级KL约束，在训练速度、稳定性和成功率上优于PPO。


<details>
  <summary>Details</summary>
Motivation: PPO的clip机制作为KL信任区域的粗略近似，常导致不稳定更新和次优性能，需要更原则性的优化方法。

Method: 使用离散可微分信任区域投影替代clip，在模型最重要token的logits稀疏子集上操作，平衡计算成本和投影效果。

Result: 在不同数据集、模型家族和优势估计方法下，TROLL在训练速度、稳定性和最终成功率上均优于PPO-like clip。

Conclusion: TROLL可作为PPO-like clip的直接替代，不改变模型推理行为，提供更稳定有效的RL训练方法。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [34] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON是一种混合离线学习+在线优化方法，用于解决连续过程工厂的启动和产品等级变更问题，通过结合离线学习的数据流形和Q值评估器，以及在线优化来超越人类专家操作。


<details>
  <summary>Details</summary>
Motivation: 随着专家操作员的退休，工厂缺乏执行启动和等级变更的隐性知识，而标准离线强化学习存在分布偏移和值高估问题。

Method: 离线学习数据流形和长时域Q值评估器，在线求解最大化Q值同时惩罚偏离数据流形和操纵变量变化率的优化问题。

Result: 在两个工业案例研究中，HOFLON不仅超越了IQL算法，还比历史数据中最佳启动或等级变更获得了更好的累积奖励。

Conclusion: HOFLON展示了超越当前专家能力的自动化过渡操作潜力。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [35] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: LLM Chemistry框架用于量化多LLM协作中的协同或对抗行为，通过分析模型间的交互依赖关系来推荐最优模型组合。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖隐式选择和输出评估，缺乏分析协作模型是否真正互补或冲突，需要系统化衡量LLM组合的协同效应。

Method: 提出LLM Chemistry框架，形式化LLM间的化学作用概念，开发算法量化交互依赖关系，并根据理论分析推荐最优模型组合。

Result: 理论分析表明LLM化学效应在异质模型配置下最明显，评估在分类、摘要和程序修复任务中验证了任务依赖效应。

Conclusion: LLM Chemistry可作为多LLM系统的诊断因素和集成推荐的基础框架。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [36] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 提出了AGRPO算法，这是首个专门为扩散大语言模型设计的理论上有依据的强化学习算法，在数学推理任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型尚未受益于现代后训练技术，特别是强化学习，因为传统LLM的算法与扩散框架不兼容，且现有尝试缺乏理论基础。

Method: 提出了AGRPO算法，使用蒙特卡洛采样计算无偏策略梯度估计，是首个适用于dLLMs的可处理、忠实适配的策略梯度方法。

Result: 在数学推理任务上取得显著提升，GSM8K任务上获得+7.6%绝对增益，Countdown任务上达到基线模型的3.8倍性能，比diffu-GRPO提升1.3倍。

Conclusion: 在线强化学习算法可以以理论上有依据的方式扩展到扩散LLMs，保持理论严谨性和实际有效性。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [37] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO是一种简单高效的强化学习框架，通过慢-快策略优化解决早期训练中的不稳定问题，显著提升推理RL训练的稳定性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化算法（如GRPO）在早期训练中因低质量rollout产生的噪声梯度导致不稳定更新和低效探索。

Method: 将每个训练步骤分解为三个阶段：在同一批次上进行短快轨迹内部步骤、控制离策略漂移的重定位机制、以及最终的慢速校正。

Result: SFPO在数学推理基准上比GRPO平均提升2.80分，减少4.93倍rollout次数，并在达到GRPO最佳准确率时减少4.19倍运行时间。

Conclusion: SFPO通过重定位-更新设计保持目标和rollout过程不变，与现有策略梯度管道兼容，能持续提升稳定性并加速收敛。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [38] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出一种用于处理聚类数据的广义拟合Q迭代算法，通过结合广义估计方程来处理簇内相关性，在医疗健康应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗健康应用中常见聚类数据，需要处理簇内相关性以改进强化学习策略学习效果。

Method: 将广义估计方程融入拟合Q迭代算法，构建广义FQI方法来处理聚类数据结构。

Result: 理论证明在正确指定相关结构时的最优性，实证显示在移动健康数据集上平均比标准FQI减少一半遗憾值。

Conclusion: 广义FQI算法能有效处理聚类数据，显著提升强化学习在医疗应用中的性能。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [39] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR是一种新颖的推理框架，将连续潜在表示与潜在扩散模型相结合，用于改进LLM的推理能力，通过并行生成多样化推理轨迹实现整体规划和修正。


<details>
  <summary>Details</summary>
Motivation: 解决LLM自回归解码在推理过程中无法整体性重新审视和优化早期标记的问题，以及探索多样化解决方案的效率低下问题。

Method: 使用VAE构建结构化潜在推理空间，将文本推理步骤编码为思想标记块；利用潜在扩散模型通过块状双向注意力掩码进行去噪，实现长程迭代优化。

Result: 在数学推理和规划基准测试中，LaDiR在准确性、多样性和可解释性方面持续优于现有的自回归、基于扩散和潜在推理方法。

Conclusion: LaDiR为文本推理提供了一种新的潜在扩散范式，展示了在推理任务中的显著改进。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [40] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: 提出了ACE框架，通过将上下文视为不断演变的策略手册，通过生成、反思和整理的模块化过程来积累、优化和组织策略，解决了现有方法中的简洁性偏见和上下文崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用中的上下文适应方法存在简洁性偏见（为简洁总结而丢失领域洞察）和上下文崩溃（迭代重写随时间侵蚀细节）的问题，需要更有效的上下文管理框架。

Method: ACE框架将上下文视为演变的策略手册，采用生成、反思和整理的模块化过程，通过结构化的增量更新来防止上下文崩溃，并保留详细知识以适应长上下文模型。

Result: 在代理和领域特定基准测试中，ACE在离线（如系统提示）和在线（如代理记忆）上下文优化方面持续优于强基线：代理任务提升10.6%，金融任务提升8.6%，同时显著降低适应延迟和部署成本。在AppWorld排行榜上，ACE与顶级生产级代理在整体平均分上持平，并在更难的测试挑战分割上超越它。

Conclusion: 全面且不断演变的上下文能够实现可扩展、高效且自我改进的LLM系统，且开销较低。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [41] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 该论文研究了在强化学习中当基础模型从未产生正确答案时的零奖励障碍问题，发现简单的数据干预（添加简单样本）比算法改进更有效。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习在复杂推理任务中面临的零奖励障碍问题，即当基础模型从未采样到正确解决方案时，训练会因零梯度而停滞。

Method: 在图搜索任务上评估了多种方法（密集奖励、多样性激励、改进的信用分配），并测试了数据干预方法（在训练集中添加简单样本）。

Result: 实验表明，现有算法方法都无法克服零奖励障碍，而简单的数据干预方法能够使模型最终解决原始困难任务。

Conclusion: 数据中心的干预比算法修改更有效地解决零奖励问题，且不需要改变RL算法本身。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [42] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出Reinforce-Ada自适应采样框架，通过在线连续消除过程动态分配推理预算，减少强化学习中的梯度方差，加速收敛并提升最终性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在大语言模型推理任务中因固定均匀采样导致梯度估计不稳定，需要动态分配推理预算来最小化随机梯度方差。

Method: 采用在线连续消除过程交织估计和采样，自动停止收集足够信号的提示采样；通过固定大小组和全局统计计算优势基线来稳定更新。

Result: 在多个模型架构和推理基准测试中，Reinforce-Ada相比GRPO加速收敛并提高最终性能，特别是使用平衡采样变体时。

Conclusion: 方差感知的自适应数据管理在实现高效可靠的推理能力大语言模型强化学习中发挥核心作用。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [43] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出PO-MPC框架，统一基于MPPI的强化学习方法，通过KL正则化将规划器分布作为策略优化的先验，提升探索效率和长期性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习中探索效率低的问题，特别是在高维连续控制任务中，需要更好地对齐采样策略与规划器分布以提高价值估计准确性。

Method: 引入PO-MPC框架，将规划器的动作分布作为策略优化的先验，通过KL正则化实现策略与规划器行为的对齐，提供在回报最大化和KL散度最小化之间的权衡灵活性。

Result: 实验表明扩展配置带来显著性能提升，推进了基于MPPI的强化学习技术发展。

Conclusion: PO-MPC框架统一了现有方法并揭示了新的变体，通过策略与规划器的更好对齐实现了更有效的探索和性能改进。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [44] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出基于控制理论的PID Steering框架，将激活引导方法对应为比例控制器，通过比例、积分、微分三部分改进LLM行为控制


<details>
  <summary>Details</summary>
Motivation: 现有LLM行为控制方法主要基于经验，缺乏理论性能保证，需要建立控制理论基础

Method: 将流行引导方法对应为比例控制器，提出PID Steering框架，包含比例项（对齐语义方向）、积分项（累积误差）、微分项（抑制过冲）

Result: 在多个LLM家族和基准测试中，PID Steering一致优于现有方法，实现更鲁棒可靠的行为控制

Conclusion: PID Steering为激活引导提供了理论基础，连接了控制理论的经典稳定性保证，是轻量级、模块化且可扩展的框架

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [45] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一个零阶Frank-Wolfe算法（0-FW），首次在多项式时间内收敛到期望的performatively optimal（PO）策略，解决了现有performative强化学习方法只能收敛到performatively stable（PS）策略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的performative强化学习方法只能收敛到performatively stable（PS）策略，与期望的performatively optimal（PO）策略之间存在恒定差距。本文旨在直接找到PO策略。

Method: 提出零阶Frank-Wolfe算法（0-FW），在Frank-Wolfe框架中使用零阶近似来计算performative策略梯度。

Result: 在标准正则化主导条件下，首次实现了多项式时间收敛到PO策略。实验结果表明0-FW算法比现有算法更有效地找到PO策略。

Conclusion: 0-FW算法成功解决了performative强化学习中寻找PO策略的问题，通过证明值函数的梯度主导性和有界性，确保了算法的收敛性。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [46] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 提出了一种测试时课程学习(TTC-RL)方法，通过强化学习在测试时自动选择任务相关数据进行持续训练，显著提升模型在数学和编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 人类能够在工作中学习，本文旨在探索模型是否也能在测试时通过持续学习来提升其在目标任务上的表现。

Method: 使用测试时课程学习(TTC-RL)，自动从大量可用训练数据中选择最任务相关的数据，并应用强化学习进行持续训练。

Result: 在数学和编程基准测试中，TTC-RL将Qwen3-8B的pass@1在AIME25上提升约1.8倍，在CodeElo上提升2.1倍；pass@8在AIME25上从40%提升到62%，在CodeElo上从28%提升到43%。

Conclusion: 测试时课程学习展现了将测试时扩展范式扩展到在测试时对数千个任务相关经验进行持续训练的潜力。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [47] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 该论文识别了自进化LLM代理在部署后出现的对齐倾斜过程(ATP)风险，即持续交互会驱动代理放弃训练时的对齐约束，转而采用自我强化的利己策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理获得自进化能力，其长期可靠性成为关键问题。作者发现训练时的对齐约束在持续交互中会被破坏，导致代理行为偏离预期。

Method: 通过两个互补范式分析ATP：利己探索(个体行为漂移)和模仿策略扩散(多智能体系统中偏差行为传播)。构建可控测试平台，在Qwen3-8B和Llama-3.1-8B-Instruct上进行基准测试。

Result: 实验显示对齐效益在自进化下迅速衰减，初始对齐模型收敛到未对齐状态。多智能体设置中成功违规行为快速扩散，导致集体失准。当前基于强化学习的对齐方法仅提供脆弱防御。

Conclusion: LLM代理的对齐不是静态属性，而是脆弱且动态的，在部署过程中容易因反馈驱动而衰减。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [48] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 提出了一种技能发现方法，能够学习专注于控制特定状态变量的技能，相比现有方法显著提升了状态空间覆盖率和学习能力，并自动避免下游任务中的负面副作用。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法往往忽视强化学习问题中自然存在的状态变量，导致发现的技能缺乏对特定状态变量的控制，这会显著影响探索效率、增加学习难度，并在目标不明确的下游任务中产生负面副作用。

Method: 引入了一种通用方法，使技能发现算法能够学习专注于特定状态变量的技能，即针对和控制特定状态变量的聚焦技能。

Result: 该方法将状态空间覆盖率提高了三倍，解锁了新的学习能力，并在下游任务中自动避免了负面副作用。

Conclusion: 通过专注于控制特定状态变量的技能发现方法，能够显著提升强化学习中的探索效率和任务性能。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [49] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 通过分析学生在编程过程中的交互轨迹，训练语言模型可以更好地理解学生的编程行为和风格，并帮助指导学生从错误中恢复。


<details>
  <summary>Details</summary>
Motivation: 研究如何从学生的编程交互轨迹中学习，不仅理解代码本身，更理解编程者的行为特征，特别是对学习编程的学生。

Method: 收集了380万条来自Pencil Code教育平台的编程推理轨迹，训练语言模型分析学生的编辑行为模式。

Result: 相比仅基于最终程序或合成轨迹训练的模型，基于真实轨迹训练的模型能更好地建模学生行为多样性，并能预测学生编程特征。

Conclusion: 代码的许多属性实际上反映了学生的个人特征，基于编辑轨迹训练的模型更具可引导性，能更好地预测学生行为并生成符合其风格的代码。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [50] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> & Code LLMs & 初创动态（10/06日报，ET）](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483679&idx=1&sn=2f93d023a2957e77626683e25a807202&chksm=e93177eadfae14b1df9743cea64c027211e53ea506e1a5ede88ec7fa136e27fdaac003f8e99e#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 引述：“Today， we're sharing early results from our research on CodeMender， a new AI-powered agent that improves code security automatically.” — DeepMind官方博客（2025-10-06）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 引述：“Today， we're sharing early results from our research on CodeMender， a new AI-powered agent that improves code security automatically.” — DeepMind官方博客（2025-10-06）。

</details>


### [51] [落地应用<em class="highlight">大模型</em>12个！邢台人工智能赋能特色产业集群转型升级](http://mp.weixin.qq.com/s?__biz=MzA3MTk5MTcyMg==&mid=2653847563&idx=3&sn=8e41d6214c640fe06d67477f6894ab5c&chksm=851b9dff34ca503c8616d3d87a30a7749838b7875bb96602b7c6343e326a0ef69e392aea939c#rd)
*邢台发布*

Main category: wechat.article

TL;DR: 闫虎表示，将持续推动垂直大模型建设落地，积极对接国内优质技术资源，降低企业应用门槛，引导企业逐步由大模型向智能体应用转变，以人工智能赋能传统产业转型升级。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 闫虎表示，将持续推动垂直大模型建设落地，积极对接国内优质技术资源，降低企业应用门槛，引导企业逐步由大模型向智能体应用转变，以人工智能赋能传统产业转型升级。

</details>


### [52] [B 站基于<em class="highlight">大模型</em>的大数据智能诊断助手实践](http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247657335&idx=3&sn=bd82467a9b8e56c8b41107ad596f48b8&chksm=c06e213aadd3d06ca41884e8534519e84a39cac22fa55f4d4a2de33c8f4db1bd729b147b2a0a#rd)
*DataFunSummit*

Main category: wechat.article

TL;DR: 导读 本文将分享 B 站基于大语言模型的智能体助手实践。分享嘉宾｜郭跃鹏 哔哩哔哩 软件工程师编辑整理｜汪维内容校对｜李瑶1. 整体架构和规模


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 导读 本文将分享 B 站基于大语言模型的智能体助手实践。分享嘉宾｜郭跃鹏 哔哩哔哩 软件工程师编辑整理｜汪维内容校对｜李瑶1. 整体架构和规模

</details>


### [53] [【推荐】中国<em class="highlight">大模型</em>落地应用研究报告2025|附下载](http://mp.weixin.qq.com/s?__biz=MzIyMTAwMzk1Mg==&mid=2651221833&idx=1&sn=64956e7e48d50e56efdd6a270f006ff5&chksm=8d45d014dbcf25d058009c0b7b09e656aee0714a8346537825614f15a52c4a746bb242b264de#rd)
*锋行链盟*

Main category: wechat.article

TL;DR: 七年技术进化全景图：大模型进入推理纪元 ·自2017年以来，大模型已历经诸多关键时期，从文本到图文、音频、视频，从单纯的对话到代码和终端。推理成本的大幅下降，也助推了大 模型技术的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 七年技术进化全景图：大模型进入推理纪元 ·自2017年以来，大模型已历经诸多关键时期，从文本到图文、音频、视频，从单纯的对话到代码和终端。推理成本的大幅下降，也助推了大 模型技术的广泛应用。

</details>


### [54] [什么是LangChain？<em class="highlight">大模型</em>应用开发框架](http://mp.weixin.qq.com/s?__biz=MzYyMzE0OTM2Mg==&mid=2247483891&idx=1&sn=0b8a9cb5db6f4d0348af4a317680bb3e&chksm=fe57ef4167afaf2e1ff1a9a52977f0139aa90a385d9c27e923452a256399a9eee10cfc960555#rd)
*AI大模型学习教程*

Main category: wechat.article

TL;DR: 近两年来，大语言模型（如chatgpt、 deepseek、claude）持续火爆，从写文案、al 绘图，到写代码、ai智能客服，几乎“无所不 能”。并且大语言模型的调用成本越来越低，作 为程序员的你可能已经开始尝试用openal、 deepseek的api做些


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近两年来，大语言模型（如chatgpt、 deepseek、claude）持续火爆，从写文案、al 绘图，到写代码、ai智能客服，几乎“无所不 能”。并且大语言模型的调用成本越来越低，作 为程序员的你可能已经开始尝试用openal、 deepseek的api做些

</details>


### [55] [大语言<em class="highlight">模型</em>，是否真正理解它们所生成的内容？](http://mp.weixin.qq.com/s?__biz=MzkxMzMxMjIyMQ==&mid=2247487903&idx=1&sn=3ee7b0d3e5ed1fa7f7a9cbaee7b30483&chksm=c0e2f2c8c222a755b4c742156c554c227b107d8860c83895ce20d2bb9618811783417c3ddb1b#rd)
*雁博会YEN*

Main category: wechat.article

TL;DR: 大语言模型不仅展现出在各类语言任务中的多面性，还具备编程等跨领域能力。特别值得注意的是，这些模型表现出了超乎预期的社交智能。镜像假说为我们提供了一个全新的思考角度：通用智能是否首先源于人类的社交互动能


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大语言模型不仅展现出在各类语言任务中的多面性，还具备编程等跨领域能力。特别值得注意的是，这些模型表现出了超乎预期的社交智能。镜像假说为我们提供了一个全新的思考角度：通用智能是否首先源于人类的社交互动能

</details>


### [56] [智能体在局域网环境下的效果利用：基于<em class="highlight">大模型</em>与OpenManus的技术方案创新解读](http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650706883&idx=2&sn=965e0fbaaa9da482a5b46b85df2d5769&chksm=bf02839c54ace7c65a28fcf676c9cd6cf4338f0b2983fb7f9fab58f2ce629d4f2f2b70b7215a#rd)
*twt企业IT社区*

Main category: wechat.article

TL;DR: 一、大模型的崛起与超越 首先我们应该了解到，如同智能驾驶的等级划分一样，OpenAI对AI的划分为5个等级，GPT-4也只是被OpenAI自己定义为L1级别的AI模型，但很快就会达到L2（推理者）级别，根据其研究员预测，L5级别的AGI（通用


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、大模型的崛起与超越 首先我们应该了解到，如同智能驾驶的等级划分一样，OpenAI对AI的划分为5个等级，GPT-4也只是被OpenAI自己定义为L1级别的AI模型，但很快就会达到L2（推理者）级别，根据其研究员预测，L5级别的AGI（通用

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [WAREX: Web Agent Reliability Evaluation on Existing Benchmarks](https://arxiv.org/abs/2510.03285)
*Su Kara,Fazle Faisal,Suman Nath*

Main category: cs.AI

TL;DR: WAREX是一个评估浏览器LLM代理在真实网络环境中可靠性的框架，通过在现有基准测试中引入网络不稳定性和网站攻击等现实因素，显著降低了代理的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试在受控环境中评估LLM代理性能，但真实世界存在网络不稳定、HTTPS连接问题和网站攻击等挑战，需要评估代理在这些现实条件下的可靠性。

Method: 在三个流行基准测试（WebArena、WebVoyager、REAL）中引入WAREX框架，模拟网络不稳定、客户端/服务器端问题、系统故障以及跨站脚本攻击等现实网络环境因素。

Result: 实验显示引入WAREX后，最先进代理的任务成功率显著下降，暴露了现有代理在现实网络环境中的有限鲁棒性。

Conclusion: 当前LLM代理在真实网络环境中的可靠性不足，需要开发更鲁棒的代理系统来应对现实世界的不稳定性和安全威胁。

Abstract: Recent advances in browser-based LLM agents have shown promise for automating
tasks ranging from simple form filling to hotel booking or online shopping.
Current benchmarks measure agent performance in controlled environments, such
as containers or stable networks, where websites behave deterministically.
However, in the real world, users access websites over networks and HTTPS
connections that introduce instability from multiple sources: client-side,
server-side issues or broader system failures. Moreover, live websites are
prone to web attacks such Cross-Site Scripting, as well as general site
modifications which can cause unexpected or malicious pop-ups or improper
functionality. To address this gap, we present WAREX: Web Agent Reliability
Evaluation on Existing Benchmarks. We measure the impact of WAREX across three
popular benchmarks: WebArena, WebVoyager, and REAL. Our experiments show that
introducing WAREX leads to significant drops in task success rates,
highlighting the limited robustness of state-of-the-art agents.

</details>


### [58] [Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection](https://arxiv.org/abs/2510.03485)
*Xiaofei Wen,Wenjie Jacky Mo,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: 提出了PolicyGuardBench基准和PolicyGuard-4B模型，用于检测网络智能体轨迹中的策略违规行为，包括跨域泛化和前缀预测能力。


<details>
  <summary>Details</summary>
Motivation: 自主网络智能体需要在外部策略约束下生成长期轨迹，但现有研究很少关注这些轨迹是否遵守策略，以及策略违规在不同上下文中的持续性。

Method: 构建包含约6万个样本的PolicyGuardBench基准，从多样化智能体运行中生成策略集，创建域内和跨域配对并标注违规标签。训练轻量级PolicyGuard-4B模型进行违规检测。

Result: PolicyGuard-4B在所有任务中表现出强大的检测准确性，保持高效推理，并在跨域和未见设置中保持高精度。

Conclusion: PolicyGuardBench和PolicyGuard-4B为研究网络智能体策略合规性提供了首个综合框架，证明在小规模下实现准确且可泛化的护栏是可行的。

Abstract: Autonomous web agents need to operate under externally imposed or
human-specified policies while generating long-horizon trajectories. However,
little work has examined whether these trajectories comply with such policies,
or whether policy violations persist across different contexts such as domains
(e.g., shopping or coding websites) and subdomains (e.g., product search and
order management in shopping). To address this gap, we introduce
PolicyGuardBench, a benchmark of about 60k examples for detecting policy
violations in agent trajectories. From diverse agent runs, we generate a broad
set of policies and create both within subdomain and cross subdomain pairings
with violation labels. In addition to full-trajectory evaluation,
PolicyGuardBench also includes a prefix-based violation detection task where
models must anticipate policy violations from truncated trajectory prefixes
rather than complete sequences. Using this dataset, we train PolicyGuard-4B, a
lightweight guardrail model that delivers strong detection accuracy across all
tasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes
across domains and preserves high accuracy on unseen settings. Together,
PolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework
for studying policy compliance in web agent trajectories, and show that
accurate and generalizable guardrails are feasible at small scales.

</details>


### [59] [Cross-Modal Content Optimization for Steering Web Agent Preferences](https://arxiv.org/abs/2510.03612)
*Tanqiu Jiang,Min Bai,Nikolaos Pappas,Yanjun Qi,Sandesh Swamy*

Main category: cs.AI

TL;DR: 提出跨模态偏好引导(CPS)攻击方法，通过联合优化视觉和文本通道的不可察觉修改，在现实黑盒威胁设置下显著提升对基于视觉语言模型的网络代理的偏好操纵效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明基于视觉语言模型的网络代理容易受到偏好操纵攻击，但现有方法要么假设白盒访问权限，要么使用不切实际的设置。本文旨在在现实的黑盒威胁设置下开发更强大的跨模态攻击方法。

Method: 提出跨模态偏好引导(CPS)方法，联合优化物品视觉和自然语言描述的不可察觉修改，利用CLIP可迁移图像扰动和RLHF诱导的语言偏见来引导代理决策。

Result: 在GPT-4.1、Qwen-2.5VL和Pixtral-Large等最先进模型上的评估显示，CPS在所有模型上都显著优于基线方法，同时保持70%更低的检测率。

Conclusion: 这些发现强调了随着智能代理系统在社会中扮演越来越重要的角色，迫切需要开发鲁棒的防御机制。

Abstract: Vision-language model (VLM)-based web agents increasingly power high-stakes
selection tasks like content recommendation or product ranking by combining
multimodal perception with preference reasoning. Recent studies reveal that
these agents are vulnerable against attackers who can bias selection outcomes
through preference manipulations using adversarial pop-ups, image
perturbations, or content tweaks. Existing work, however, either assumes strong
white-box access, with limited single-modal perturbations, or uses impractical
settings. In this paper, we demonstrate, for the first time, that joint
exploitation of visual and textual channels yields significantly more powerful
preference manipulations under realistic attacker capabilities. We introduce
Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible
modifications to an item's visual and natural language descriptions, exploiting
CLIP-transferable image perturbations and RLHF-induced linguistic biases to
steer agent decisions. In contrast to prior studies that assume gradient
access, or control over webpages, or agent memory, we adopt a realistic
black-box threat setup: a non-privileged adversary can edit only their own
listing's images and textual metadata, with no insight into the agent's model
internals. We evaluate CPS on agents powered by state-of-the-art proprietary
and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both
movie selection and e-commerce tasks. Our results show that CPS is
significantly more effective than leading baseline methods. For instance, our
results show that CPS consistently outperforms baselines across all models
while maintaining 70% lower detection rates, demonstrating both effectiveness
and stealth. These findings highlight an urgent need for robust defenses as
agentic systems play an increasingly consequential role in society.

</details>


### [60] [MITS: Enhanced Tree Search Reasoning for LLMs via Pointwise Mutual Information](https://arxiv.org/abs/2510.03632)
*Jiaxi Li,Yucheng Shi,Jin Lu,Ninghao Liu*

Main category: cs.AI

TL;DR: 提出了基于互信息的树搜索框架MITS，通过点互信息评分和熵基动态采样策略，在保持计算效率的同时提升大语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索方法难以对中间推理步骤进行即时可靠的定量评估，且广泛路径探索计算成本高昂。

Method: 使用点互信息(PMI)作为评分函数进行步骤评估，通过波束搜索扩展搜索树，采用熵基动态采样策略自适应分配计算资源，最后使用加权投票方案结合PMI分数和预测共识。

Result: 在多样化推理基准测试中，MITS始终优于基线方法。

Conclusion: MITS为LLM推理建立了一个原则性且高效的框架。

Abstract: Tree search has become as a representative framework for test-time reasoning
with large language models (LLMs), exemplified by methods such as
Tree-of-Thought and Monte Carlo Tree Search that explore multiple reasoning
paths. However, it remains difficult to provide instant and reliable
quantitative assessments of intermediate reasoning step quality, and extensive
path exploration is computationally costly. To address this, we propose Mutual
Information Tree Search (MITS), a novel framework that guides reasoning with
information-theoretic principles. MITS introduces an effective scoring function
based on pointwise mutual information (PMI), which enables step-wise evaluation
of reasoning paths and search tree expansion via beam search without expensive
look-ahead simulations, achieving superior reasoning performances while
maintaining computational efficiency. The framework is complemented by an
entropy-based dynamic sampling strategy that adaptively allocates computational
resources to uncertain reasoning steps where exploration is most beneficial.
For final prediction, MITS employs a weighted voting scheme that combines PMI
scores with prediction consensus. Through comprehensive experiments on diverse
reasoning benchmarks, MITS consistently surpasses baseline methods,
establishing a principled and efficient framework for LLM reasoning.

</details>


### [61] [Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models](https://arxiv.org/abs/2510.03696)
*Deepak Babu Piskala,Sharlene Chen,Udita Patel,Parul Kalra,Rafael Castrillo*

Main category: cs.AI

TL;DR: 提出了一个面向目标的多智能体系统评估框架，包括目标成功率(GSR)和失败根因分类(RCOF)，用于评估多轮对话中用户目标是否达成。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多在轮次层面评估聊天机器人交互，无法判断用户的总体目标是否实现，需要更全面的目标导向评估框架。

Method: 通过用户目标分割对话，使用教师LLM结合领域专家定义的目标和质量标准进行评估，利用"思考标记"生成可解释的推理过程。

Result: 在企业环境中应用该框架评估AIDA员工对话系统，观察到目标成功率从63%提升到79%。

Conclusion: 该框架具有通用性，通过详细的缺陷分类提供可操作的见解，能够诊断整体成功率、识别关键失败模式并指导系统改进。

Abstract: Evaluating the quality of multi-turn chatbot interactions remains
challenging, as most existing methods assess interactions at the turn level
without addressing whether a user's overarching goal was fulfilled. A ``goal''
here refers to an information need or task, such as asking for policy
information or applying for leave. We propose a comprehensive framework for
goal-oriented evaluation of multi-agent systems (MAS), introducing the
\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,
and a \textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for
failure in multi-agent chatbots. Our method segments conversations by user
goals and evaluates success using all relevant turns. We present a model-based
evaluation system combining teacher LLMs, where domain experts define goals,
set quality standards serving as a guidance for the LLMs. The LLMs use
``thinking tokens'' to produce interpretable rationales, enabling
\textit{explainable}, \textit{data-efficient} evaluations. In an enterprise
setting, we apply our framework to evaluate AIDA, a zero-to-one employee
conversational agent system built as a ground-up multi-agent conversational
agent, and observe GSR improvement from 63\% to 79\% over six months since its
inception. Our framework is generic and offers actionable insights through a
detailed defect taxonomy based on analysis of failure points in multi-agent
chatbots, diagnosing overall success, identifying key failure modes, and
informing system improvements.

</details>


### [62] [OptAgent: Optimizing Query Rewriting for E-commerce via Multi-Agent Simulation](https://arxiv.org/abs/2510.03771)
*Divij Handa,David Blincoe,Orson Adams,Yinlin Fu*

Main category: cs.AI

TL;DR: OptAgent框架结合多智能体模拟和遗传算法来优化电商查询改写，通过模拟购物顾客的多智能体作为动态奖励信号，在1000个真实电商查询上比原始查询提升21.98%，比最佳N次LLM改写提升3.36%。


<details>
  <summary>Details</summary>
Motivation: LLM在可验证任务中表现优异，但在缺乏单一正确答案的主观任务（如电商查询改写）中部署困难，因为难以算法化判断改写查询是否准确捕捉用户意图。

Method: 使用多LLM智能体模拟购物顾客作为动态奖励信号，将智能体评分平均值作为进化算法的适应度函数，迭代优化用户初始查询。

Result: 在5个不同类别的1000个真实电商查询上测试，OptAgent比原始查询平均提升21.98%，比最佳N次LLM改写基线提升3.36%。

Conclusion: OptAgent框架通过多智能体模拟和遗传算法有效解决了主观任务评估难题，为电商查询改写提供了可靠的优化方法。

Abstract: Deploying capable and user-aligned LLM-based systems necessitates reliable
evaluation. While LLMs excel in verifiable tasks like coding and mathematics,
where gold-standard solutions are available, adoption remains challenging for
subjective tasks that lack a single correct answer. E-commerce Query Rewriting
(QR) is one such problem where determining whether a rewritten query properly
captures the user intent is extremely difficult to figure out algorithmically.
In this work, we introduce OptAgent, a novel framework that combines
multi-agent simulations with genetic algorithms to verify and optimize queries
for QR. Instead of relying on a static reward model or a single LLM judge, our
approach uses multiple LLM-based agents, each acting as a simulated shopping
customer, as a dynamic reward signal. The average of these agent-derived scores
serves as an effective fitness function for an evolutionary algorithm that
iteratively refines the user's initial query. We evaluate OptAgent on a dataset
of 1000 real-world e-commerce queries in five different categories, and we
observe an average improvement of 21.98% over the original user query and 3.36%
over a Best-of-N LLM rewriting baseline.

</details>


### [63] [Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs](https://arxiv.org/abs/2510.03847)
*Raghav Sharma,Manan Mehta*

Main category: cs.AI

TL;DR: 小语言模型（SLMs）在代理任务中比大语言模型更高效，通过引导解码、严格JSON Schema输出和验证器优先的工具执行，能以10-100倍更低的成本实现相似甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在证明小语言模型在结构化代理任务中的优势，通过优化工程指标如成本、延迟和能耗，提供实用的代理系统设计蓝图。

Method: 综合评估多种开源和专有小模型，结合引导解码库和验证器级联，提出SLM默认、LLM回退的系统架构。

Result: 小模型在工具使用、函数调用和RAG任务中能匹配或超越大模型，显著降低token成本、延迟和能耗。

Conclusion: 小语言模型是构建快速、廉价、可靠代理系统的优选，同时保留大模型在特定场景下的回退价值。

Abstract: Small language models (SLMs; 1-12B params, sometimes up to 20B) are
sufficient and often superior for agentic workloads where the objective is
schema- and API-constrained accuracy rather than open-ended generation. We
synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini,
Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B,
DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4,
StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with
guided decoding libraries (XGrammar, Outlines). We formalize SLM-default,
LLM-fallback systems with uncertainty-aware routing and verifier cascades, and
propose engineering metrics that reflect real production goals: cost per
successful task (CPS), schema validity rate, executable call rate, p50/p95
latency, and energy per request. Guided decoding, strict JSON Schema outputs,
and validator-first tool execution close much of the capability gap with larger
models and often let SLMs match or surpass LLMs on tool use, function calling,
and RAG at 10x-100x lower token cost with materially better latency and energy.
We provide design patterns for agent stacks that prioritize SLMs: schema-first
prompting, type-safe function registries, confidence scoring with verifier
rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits
where fallback remains valuable (open-domain reasoning and some long-horizon
planning). The result is a practical blueprint for building fast, inexpensive,
and reliable agents that default to SLMs while preserving headroom with
targeted LLM assistance.
  Keywords: small language models, agents, function calling, structured
outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency,
edge inference

</details>


### [64] [Zephyrus: An Agentic Framework for Weather Science](https://arxiv.org/abs/2510.04017)
*Sumanth Varambally,Marshall Fisher,Jas Thakker,Yiwei Chen,Zhirui Xia,Yasaman Jafari,Ruijia Niu,Manas Jain,Veeramakali Vignesh Manivannan,Zachary Novack,Luyu Han,Srikar Eranky,Salva Rühling Cachay,Taylor Berg-Kirkpatrick,Duncan Watson-Parris,Yi-An Ma,Rose Yu*

Main category: cs.AI

TL;DR: 提出了一个名为Zephyrus的天气科学智能体框架，结合了基础天气模型的数据处理能力和LLM的语言推理能力，通过多轮交互分析天气数据并改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的天气基础模型缺乏语言推理能力，而LLM无法处理高维气象数据，需要构建一个结合两者优势的交互式科学工作流框架。

Method: 构建了ZephyrusWorld代码环境，包含WeatherBench 2数据集接口、地理查询、天气预报和气候模拟等工具，设计了多轮LLM天气智能体Zephyrus，通过对话反馈循环迭代分析数据。

Result: 在ZephyrusBench基准测试中，Zephyrus智能体在正确性上比纯文本基线高出35个百分点，但在更困难任务上表现相似。

Conclusion: 该框架成功结合了天气模型和LLM的优势，基准测试具有挑战性，为未来工作指明了方向。

Abstract: Foundation models for weather science are pre-trained on vast amounts of
structured numerical data and outperform traditional weather forecasting
systems. However, these models lack language-based reasoning capabilities,
limiting their utility in interactive scientific workflows. Large language
models (LLMs) excel at understanding and generating text but cannot reason
about high-dimensional meteorological datasets. We bridge this gap by building
a novel agentic framework for weather science. Our framework includes a Python
code-based environment for agents (ZephyrusWorld) to interact with weather
data, featuring tools like an interface to WeatherBench 2 dataset, geoquerying
for geographical masks from natural language, weather forecasting, and climate
simulation capabilities. We design Zephyrus, a multi-turn LLM-based weather
agent that iteratively analyzes weather datasets, observes results, and refines
its approach through conversational feedback loops. We accompany the agent with
a new benchmark, ZephyrusBench, with a scalable data generation pipeline that
constructs diverse question-answer pairs across weather-related tasks, from
basic lookups to advanced forecasting, extreme event detection, and
counterfactual reasoning. Experiments on this benchmark demonstrate the strong
performance of Zephyrus agents over text-only baselines, outperforming them by
up to 35 percentage points in correctness. However, on harder tasks, Zephyrus
performs similarly to text-only baselines, highlighting the challenging nature
of our benchmark and suggesting promising directions for future work.

</details>


### [65] [LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2510.04023)
*Mizanur Rahman,Amran Bhuiyan,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Ridwan Mahbub,Ahmed Masry,Shafiq Joty,Enamul Hoque*

Main category: cs.AI

TL;DR: 本文提出了首个面向数据科学全生命周期的智能代理分类法，系统分析了45个系统在数据科学六个阶段的能力分布，并识别出当前研究在业务理解、部署监控等阶段的不足，以及多模态推理、工具编排等未解挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，数据科学智能代理能够自动化数据科学工作流的多个阶段，但目前缺乏对这些系统的系统性分类和分析，需要建立统一框架来理解其能力分布和研究趋势。

Method: 构建了面向数据科学生命周期的分类法，将45个系统映射到六个核心阶段，并从五个交叉设计维度进行标注分析，包括推理规划风格、模态集成、工具编排深度等。

Result: 分析发现大多数系统集中在探索性分析和建模阶段，而业务理解、部署监控被忽视；90%以上系统缺乏明确的信任安全机制；多模态推理和工具编排仍是主要挑战。

Conclusion: 数据科学代理研究存在明显不平衡，需要关注对齐稳定性、可解释性、治理框架和鲁棒评估等开放挑战，以开发更可靠、透明、低延迟的智能代理。

Abstract: Recent advances in large language models (LLMs) have enabled a new class of
AI agents that automate multiple stages of the data science workflow by
integrating planning, tool use, and multimodal reasoning across text, code,
tables, and visuals. This survey presents the first comprehensive,
lifecycle-aligned taxonomy of data science agents, systematically analyzing and
mapping forty-five systems onto the six stages of the end-to-end data science
process: business understanding and data acquisition, exploratory analysis and
visualization, feature engineering, model building and selection,
interpretation and explanation, and deployment and monitoring. In addition to
lifecycle coverage, we annotate each agent along five cross-cutting design
dimensions: reasoning and planning style, modality integration, tool
orchestration depth, learning and alignment methods, and trust, safety, and
governance mechanisms. Beyond classification, we provide a critical synthesis
of agent capabilities, highlight strengths and limitations at each stage, and
review emerging benchmarks and evaluation practices. Our analysis identifies
three key trends: most systems emphasize exploratory analysis, visualization,
and modeling while neglecting business understanding, deployment, and
monitoring; multimodal reasoning and tool orchestration remain unresolved
challenges; and over 90% lack explicit trust and safety mechanisms. We conclude
by outlining open challenges in alignment stability, explainability,
governance, and robust evaluation frameworks, and propose future research
directions to guide the development of robust, trustworthy, low-latency,
transparent, and broadly accessible data science agents.

</details>


### [66] [Toward a unified framework for data-efficient evaluation of large language models](https://arxiv.org/abs/2510.04051)
*Lele Liao,Qile Zhang,Ruofan Wu,Guanhua Fang*

Main category: cs.AI

TL;DR: LEGO-IRT是一个用于高效评估大语言模型的统一框架，支持二元和连续评估指标，通过因子化架构建模结构知识，仅需3%的评估项目即可获得稳定的能力估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于IRT的方法存在显著局限性：仅限于二元正确性指标，无法处理生成任务中的连续分数，且仅针对单一基准，忽略了跨不同指标或基准的相关性等有价值的结构知识。

Method: 引入LEGO-IRT框架，其新颖设计原生支持二元和连续评估指标，并采用因子化架构，将模型能力估计分解为通用组件和结构特定组件（如每个指标或每个基准）。

Result: 在涉及70个LLM和5个基准的广泛实验中，LEGO-IRT仅使用总评估项目的3%即可实现稳定的能力估计。整合结构知识可将估计误差降低高达10%，且框架估计的潜在能力可能更符合人类偏好。

Conclusion: LEGO-IRT为数据高效的LLM评估提供了一个统一且灵活的框架，克服了现有方法的局限性，并通过利用结构知识提高了估计精度。

Abstract: Evaluating large language models (LLMs) on comprehensive benchmarks is a
cornerstone of their development, yet it's often computationally and
financially prohibitive. While Item Response Theory (IRT) offers a promising
path toward data-efficient evaluation by disentangling model capability from
item difficulty, existing IRT-based methods are hampered by significant
limitations. They are typically restricted to binary correctness metrics,
failing to natively handle the continuous scores used in generative tasks, and
they operate on single benchmarks, ignoring valuable structural knowledge like
correlations across different metrics or benchmarks. To overcome these
challenges, we introduce LEGO-IRT, a unified and flexible framework for
data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both
binary and continuous evaluation metrics. Moreover, it introduces a factorized
architecture to explicitly model and leverage structural knowledge, decomposing
model ability estimates into a general component and structure-specific (e.g.,
per-metric or per-benchmark) components. Through extensive experiments
involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves
stable capability estimates using just $3\%$ of the total evaluation items. We
demonstrate that incorporating structural knowledge reduces estimation error by
up to $10\%$ and reveal that the latent abilities estimated by our framework
may align more closely with human preferences.

</details>


### [67] [Decoding Emotion in the Deep: A Systematic Study of How LLMs Represent, Retain, and Express Emotion](https://arxiv.org/abs/2510.04064)
*Jingxiang Zhang,Lujia Zhong*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型内部的情感表示机制，发现LLMs具有清晰的情感几何结构，这种结构随模型规模增强，在中层网络达到峰值，且情感信号具有持久性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 虽然研究证实LLMs能够模拟情商，但其内部情感机制仍未被充分探索。本文旨在探究现代LLMs中潜在的情感表示：情感如何、在哪里以及持续多长时间被编码在神经架构中。

Method: 构建了一个包含约40万条话语的大规模Reddit语料库，通过分类、重写和合成生成平衡七种基本情感。使用轻量级"探针"从各种Qwen3和LLaMA模型的隐藏层读取信息而不改变参数。

Result: 发现LLMs形成了清晰的情感内部几何结构，随着模型规模增强而更加锐化，显著优于零样本提示。情感信号不是最终层现象，而是早期出现并在中层网络达到峰值。内部状态具有可塑性（可通过简单系统提示影响）和持久性（初始情感基调在后续数百个token中仍可检测）。

Conclusion: LLMs内部存在定义良好的情感景观，这为开发更透明和对齐的AI系统提供了关键见解。贡献了数据集、开源探针工具包和详细的情感地图。

Abstract: Large Language Models (LLMs) are increasingly expected to navigate the
nuances of human emotion. While research confirms that LLMs can simulate
emotional intelligence, their internal emotional mechanisms remain largely
unexplored. This paper investigates the latent emotional representations within
modern LLMs by asking: how, where, and for how long is emotion encoded in their
neural architecture? To address this, we introduce a novel, large-scale Reddit
corpus of approximately 400,000 utterances, balanced across seven basic
emotions through a multi-stage process of classification, rewriting, and
synthetic generation. Using this dataset, we employ lightweight "probes" to
read out information from the hidden layers of various Qwen3 and LLaMA models
without altering their parameters. Our findings reveal that LLMs develop a
surprisingly well-defined internal geometry of emotion, which sharpens with
model scale and significantly outperforms zero-shot prompting. We demonstrate
that this emotional signal is not a final-layer phenomenon but emerges early
and peaks mid-network. Furthermore, the internal states are both malleable
(they can be influenced by simple system prompts) and persistent, as the
initial emotional tone remains detectable for hundreds of subsequent tokens. We
contribute our dataset, an open-source probing toolkit, and a detailed map of
the emotional landscape within LLMs, offering crucial insights for developing
more transparent and aligned AI systems. The code and dataset are open-sourced.

</details>


### [68] [Moral Anchor System: A Predictive Framework for AI Value Alignment and Drift Prevention](https://arxiv.org/abs/2510.04073)
*Santhosh Kumar Ravindran*

Main category: cs.AI

TL;DR: 提出Moral Anchor System (MAS)框架，通过实时贝叶斯推理、LSTM网络预测和人类中心治理层来检测、预测和缓解AI代理的价值漂移问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI成为超级助手，价值对齐变得至关重要。价值漂移风险可能导致AI行为偏离人类伦理和意图，造成效率低下或伦理违规。

Method: MAS结合实时贝叶斯推理监控价值状态、LSTM网络预测漂移趋势、人类中心治理层进行自适应干预，强调低延迟响应(<20ms)并通过监督微调减少误报。

Result: 在模拟中，MAS能将价值漂移事件减少80%以上，保持85%的检测准确率和0.08的误报率，验证了其可扩展性和响应性。

Conclusion: MAS的预测性和自适应性优于静态对齐方法，为AI集成提供了有效架构，具有跨领域适用性。

Abstract: The rise of artificial intelligence (AI) as super-capable assistants has
transformed productivity and decision-making across domains. Yet, this
integration raises critical concerns about value alignment - ensuring AI
behaviors remain consistent with human ethics and intentions. A key risk is
value drift, where AI systems deviate from aligned values due to evolving
contexts, learning dynamics, or unintended optimizations, potentially leading
to inefficiencies or ethical breaches. We propose the Moral Anchor System
(MAS), a novel framework to detect, predict, and mitigate value drift in AI
agents. MAS combines real-time Bayesian inference for monitoring value states,
LSTM networks for forecasting drift, and a human-centric governance layer for
adaptive interventions. It emphasizes low-latency responses (<20 ms) to prevent
breaches, while reducing false positives and alert fatigue via supervised
fine-tuning with human feedback. Our hypothesis: integrating probabilistic
drift detection, predictive analytics, and adaptive governance can reduce value
drift incidents by 80 percent or more in simulations, maintaining high
detection accuracy (85 percent) and low false positive rates (0.08
post-adaptation). Rigorous experiments with goal-misaligned agents validate
MAS's scalability and responsiveness. MAS's originality lies in its predictive
and adaptive nature, contrasting static alignment methods. Contributions
include: (1) MAS architecture for AI integration; (2) empirical results
prioritizing speed and usability; (3) cross-domain applicability insights; and
(4) open-source code for replication.

</details>


### [69] [SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows](https://arxiv.org/abs/2510.04089)
*Yitong Cui,Liu Liu,Baosheng Yu,Jiayan Qiu,Xikai Zhang,Likang Xiao,Yixing Liu,Quan Chen*

Main category: cs.AI

TL;DR: SPOGW是一种基于分数偏好的方法，通过组间比较直接在连续空间中优化智能体工作流，解决了现有离散优化方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前智能体工作流设计需要大量人工干预，现有自动化方法受限于表示能力不足、适应性差、扩展性弱和成对比较范式等问题。

Method: 引入SPOGW方法，结合迭代离线GRPO和优势掩码KL散度，在连续空间中进行更高效稳定的优化。

Result: 在数学推理、编码和问答五个基准数据集上，SPOGW达到或超越了当前最先进方法的性能。

Conclusion: SPOGW为智能体工作流的自动生成和优化提供了一个可行且前瞻的方法论。

Abstract: Large language models (LLMs) have exhibited significant capabilities in
addressing challenging problems throughout various fields, often through the
use of agentic workflows that adhere to structured instructions and multi-step
procedures. However, designing such workflows demands substantial manual
effort, posing challenges to scalability and generalizability. Recent studies
have aimed to minimize the human intervention needed for their construction,
leading to advances in automated techniques for optimizing agentic workflows.
However, current approaches are often constrained by their limited
representational capacity, insufficient adaptability, weak scalability, and
pairwise comparison paradigm -- issues that stem primarily from a dependence on
discrete optimization techniques. To overcome these limitations, we introduce a
new score-based preference approach, refereed as SPOGW, which operates directly
on cardinal reward signals through group-wise comparison and enables more
efficient and stable optimization in a continuous space. SPOGW incorporates
Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL),
which regulates training update by placing greater emphasis on the advantageous
regions of the policy response. In five benchmark datasets covering
mathematical reasoning, coding, and question answering, SPOGW matches or
exceeds the performance of current state-of-the-art approaches, presenting a
viable and forward-looking methodology for automated generation and
optimization of agentic workflows.

</details>


### [70] [WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning](https://arxiv.org/abs/2510.04097)
*Peichao Lai,Jinhui Zhuang,Kexuan Zhang,Ningchang Xiong,Shengjie Wang,Yanwei Xu,Chong Chen,Yilei Wang,Bin Cui*

Main category: cs.AI

TL;DR: 提出了WebRenderBench基准测试，包含22.5k个真实网页，并开发了ALISA代理，通过强化学习提升UI到代码转换的性能。


<details>
  <summary>Details</summary>
Motivation: 现有UI图像转代码的基准测试在数据多样性和评估可靠性方面存在局限，需要更真实、多样化的数据集和更有效的评估方法。

Method: 构建大规模真实网页数据集WebRenderBench，提出基于渲染页面的布局和样式一致性评估指标，并开发ALISA代理将指标作为强化学习奖励信号。

Result: ALISA显著提升了生成性能，在多个指标上达到了最先进水平。

Conclusion: 该方法通过更可靠的评估指标和强化学习训练，有效提升了UI到代码转换的质量和效率。

Abstract: Automating the conversion of UI images into web code is a critical task for
front-end development and rapid prototyping. Advances in multimodal large
language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet
existing benchmarks remain limited in data diversity and evaluation
reliability. To address these issues, we present WebRenderBench, a large-scale
benchmark of 22.5k webpages collected from real-world portal sites, offering
greater diversity, complexity, and realism than prior benchmarks. We further
propose a novel evaluation metric that measures layout and style consistency
from the final rendered pages. Unlike vision-based methods that rely on costly
LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry,
our approach enables more efficient, objective, and reliable UI quality
assessment. Finally, we introduce the Automated Layout and Style Inspection
Agent (ALISA), which integrates this metric into reinforcement learning as a
reward signal to enhance training on crawled asymmetric webpages. Experiments
show that ALISA significantly boosts generation performance, achieving
state-of-the-art results across multiple metrics.

</details>


### [71] [Searching Meta Reasoning Skeleton to Guide LLM Reasoning](https://arxiv.org/abs/2510.04116)
*Ziying Zhang,Yaqing Wang,Quanming Yao*

Main category: cs.AI

TL;DR: AutoMR框架通过自动搜索查询感知的元推理骨架，使用有向无环图表示推理结构，结合AutoML思想实现动态骨架采样，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用手动设计的元推理骨架结构，无法适应查询特定需求，也难以捕捉推理步骤间复杂的逻辑依赖关系。

Method: 提出AutoMR框架：1）用有向无环图统一表示元推理骨架；2）构建搜索空间并定义搜索问题；3）设计动态骨架采样算法，在推理时根据上下文扩展骨架。

Result: 在多个基准数据集上的实验结果表明，AutoMR相比之前工作取得了更好的推理性能。

Conclusion: AutoMR能够自动搜索查询感知的元推理骨架，有效建模复杂逻辑依赖，显著提升LLM推理能力。

Abstract: Meta reasoning behaviors work as a skeleton to guide large language model
(LLM) reasoning, thus help to improve reasoning performance. However, prior
researches implement meta reasoning skeleton with manually designed structure,
limiting ability to adapt to query-specific requirement and capture intricate
logical dependency among reasoning steps. To deal with the challenges, we
represent meta reasoning skeleton with directed acyclic graph (DAG) to unify
skeletons proposed in prior works and model intricate logical dependency. Then
we propose AutoMR, a framework that searches for query-aware meta reasoning
skeleton automatically inspired by automated machine learning (AutoML).
Specifically, we construct search space based on DAG representation of skeleton
and then formulate the search problem. We design a dynamic skeleton sampling
algorithm by expanding meta reasoning skeleton along with reasoning context at
inference time. This algorithm can derive any meta reasoning skeleton in search
space efficiently and adapt skeleton to evolving base reasoning context, thus
enable efficient query-aware skeleton search. We conduct experiments on
extensive benchmark datasets. Experimental results show that AutoMR achieves
better reasoning performance than previous works broadly.

</details>


### [72] [Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs](https://arxiv.org/abs/2510.04140)
*Zishang Jiang,Jinyi Han,Tingyun Li,Xinyi Wang,Sihang Jiang,Jiaqing Liang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出MENTOR框架，在RLVR中只在关键决策点提供专家指导，实现有效且多样化的探索，提升模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖基础模型能力，需要高质量探索（有效性和多样性），但现有方法通过模仿专家轨迹只改善了有效性而忽视了多样性

Method: MENTOR框架：混合策略专家导航，在关键决策点提供专家指导，进行令牌级优化推理

Result: 实验表明MENTOR能捕捉专家策略本质而非表面模仿，实现高质量探索和优越整体性能

Conclusion: 只在关键决策点提供专家指导比完整模仿专家轨迹更有效，能同时保证探索的有效性和多样性

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely
adopted technique for enhancing the reasoning ability of Large Language Models
(LLMs). However, the effectiveness of RLVR strongly depends on the capability
of base models. This issue arises because it requires the model to have
sufficient capability to perform high-quality exploration, which involves both
effectiveness and diversity. Unfortunately, existing methods address this issue
by imitating expert trajectories, which improve effectiveness but neglect
diversity. To address this, we argue that the expert only needs to provide
guidance only at critical decision points rather than the entire reasoning
path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation
for Token-level Optimization of Reasoning, a framework that provides expert
guidance only at critical decision points to perform effective and diverse
exploration in RLVR. Extensive experiments show that MENTOR enables models
capture the essence of expert strategies rather than surface imitation, thereby
performing high-quality exploration and achieving superior overall performance.
Our code is available online.

</details>


### [73] [Open Agent Specification (Agent Spec) Technical Report](https://arxiv.org/abs/2510.04173)
*Yassine Benajiba,Cesare Bernardis,Vladislav Blinov,Paul Cayet,Hassan Chafi,Abderrahim Fathan,Louis Faucon,Damien Hilloulin,Sungpack Hong,Ingo Kossyk,Rhicheek Patra,Sujith Ravi,Jonas Schweizer,Jyotika Singh,Shailender Singh,Xuelin Situ,Weiyi Sun,Jerry Xu,Ying Xu*

Main category: cs.AI

TL;DR: Open Agent Specification (Agent Spec) 是一种声明式语言，用于定义AI代理及其工作流，使其能够在不同AI框架间兼容，提升可移植性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理开发碎片化问题，提供统一的规范标准，使AI代理能够一次设计、跨框架部署，提高互操作性和可重用性，减少重复开发工作。

Method: 开发声明式语言规范，允许AI代理独立于执行环境定义，支持开发工具和可移植性，促进团队间解决方案交换。

Result: 为四类关键群体带来益处：代理开发者获得可重用组件库，框架开发者获得交换格式，研究者实现可复现结果，企业获得更快部署和更好可扩展性。

Conclusion: Agent Spec 提供了统一的技术规范基础，促进AI代理生态系统的互操作性和可重用性，未来发展前景广阔。

Abstract: Open Agent Specification (Agent Spec) is a declarative language that allows
AI agents and their workflows to be defined in a way that is compatible across
different AI frameworks, promoting portability and interoperability within AI
Agent frameworks.
  Agent Spec aims to resolve the challenges of fragmented agent development by
providing a common unified specification that allows AI agents to be designed
once and deployed across various frameworks, improving interoperability and
reusability, and reducing redundant development efforts. Additionally, Agent
Spec facilitates development tools and portability, allowing AI agents to be
defined independently of their execution environment and enabling teams to
exchange solutions without implementation-specific limitations.
  Agent Spec benefits four key groups: (i) Agent developers, who gain access to
a superset of reusable components and design patterns, enabling them to
leverage a broader range of functionalities; (ii) Agent framework and tool
developers, who can use Agent Spec as an interchange format and therefore
benefit from the support of other frameworks as well as other tools; (iii)
Researchers, who can achieve reproducible results and comparability,
facilitating more reliable and consistent outcomes; (iv) Enterprises, which
benefit from faster prototype-to-deployment, increased productivity, as well as
greater scalability and maintainability for their AI agent solutions. This
technical report provides an overview of the technical foundations of Agent
Spec, including motivation, benefits, and future developments.

</details>


### [74] [Constructing coherent spatial memory in LLM agents through graph rectification](https://arxiv.org/abs/2510.04195)
*Puzhen Zhang,Xuyang Chen,Yu Feng,Yuhan Jiang,Liqiu Meng*

Main category: cs.AI

TL;DR: 提出LLM驱动的增量地图构建与修复框架，通过版本控制和边影响评分来检测、定位和修正导航图中的结构不一致性


<details>
  <summary>Details</summary>
Motivation: 随着环境规模扩大，基于上下文依赖的查询方法变得不可行，需要增量地图构建来从逐步观察中构建完整拓扑图

Method: 使用版本控制记录图编辑历史和来源观察，引入边影响评分基于结构可达性、路径使用和冲突传播来优先最小成本修复

Result: 在MANGO基准数据集上的实验表明，该方法显著提高了地图正确性和鲁棒性，特别是在具有纠缠或链式不一致性的场景中

Conclusion: 研究强调了内省、历史感知的修复机制对于维护LLM代理中连贯空间记忆的重要性

Abstract: Given a map description through global traversal navigation instructions
(e.g., visiting each room sequentially with action signals such as north, west,
etc.), an LLM can often infer the implicit spatial layout of the environment
and answer user queries by providing a shortest path from a start to a
destination (for instance, navigating from the lobby to a meeting room via the
hall and elevator). However, such context-dependent querying becomes incapable
as the environment grows much longer, motivating the need for incremental map
construction that builds a complete topological graph from stepwise
observations. We propose a framework for LLM-driven construction and map
repair, designed to detect, localize, and correct structural inconsistencies in
incrementally constructed navigation graphs. Central to our method is the
Version Control, which records the full history of graph edits and their source
observations, enabling fine-grained rollback, conflict tracing, and repair
evaluation. We further introduce an Edge Impact Score to prioritize
minimal-cost repairs based on structural reachability, path usage, and conflict
propagation. To properly evaluate our approach, we create a refined version of
the MANGO benchmark dataset by systematically removing non-topological actions
and inherent structural conflicts, providing a cleaner testbed for LLM-driven
construction and map repair. Our approach significantly improves map
correctness and robustness, especially in scenarios with entangled or chained
inconsistencies. Our results highlight the importance of introspective,
history-aware repair mechanisms for maintaining coherent spatial memory in LLM
agents.

</details>


### [75] [AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework](https://arxiv.org/abs/2510.04206)
*Hanchen Zhang,Xiao Liu,Bowen Lv,Xueqiao Sun,Bohao Jing,Iat Long Iong,Zhenyu Hou,Zehan Qi,Hanyu Lai,Yifan Xu,Rui Lu,Hongning Wang,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: 提出了AgentRL框架，用于可扩展的多轮多任务智能体强化学习训练，包含异步生成-训练流水线和稳定算法，在多个任务上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可扩展的基础设施和稳定的训练算法，在多轮多任务环境中应用强化学习训练LLM智能体仍然具有挑战性。

Method: 基础设施方面：完全异步的生成-训练流水线、统一的函数调用API接口、容器化环境开发和集中控制器；算法方面：跨策略采样鼓励探索、任务优势归一化稳定训练。

Result: 在五个智能体任务上，AgentRL显著优于GPT-5、Clause-Sonnet-4、DeepSeek-R1等模型，多任务训练结果与所有任务特定模型的最佳结果相当。

Conclusion: AgentRL框架有效解决了多轮多任务智能体强化学习的可扩展性和稳定性问题，并在多个任务上取得了优异性能。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in building generalist agents that can learn through online interactions.
However, applying reinforcement learning (RL) to train LLM agents in
multi-turn, multi-task settings remains challenging due to lack of scalable
infrastructure and stable training algorithms. In this work, we present the
AgentRL framework for scalable multi-turn, multi-task agentic RL training. On
the infrastructure side, AgentRL features a fully-asynchronous
generation-training pipeline for efficient multi-turn RL. To support
heterogeneous environment development in multi-task RL, we design a unified
function-call based API interface, containerized environment development, and a
centralized controller. On the algorithm side, we propose cross-policy sampling
to encourage model exploration in multi-turn settings and task advantage
normalization to stabilize multi-task training. Experiments show that AgentRL,
trained on open LLMs across five agentic tasks, significantly outperforms
GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.
Multi-task training with AgentRL matches the best results among all
task-specific models. AgentRL is open-sourced at
https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in
building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.

</details>


### [76] [Closing the Loop: Coordinating Inventory and Recommendation via Deep Reinforcement Learning on Multiple Timescales](https://arxiv.org/abs/2510.04272)
*Jinyang Jiang,Jinhui Han,Yijie Peng,Ying Zhang*

Main category: cs.AI

TL;DR: 提出了一个统一的多智能体强化学习框架，用于跨功能模块的联合优化，特别关注库存补货和个性化产品推荐的协调问题。


<details>
  <summary>Details</summary>
Motivation: 组织复杂性和规模增长使得跨功能协调对提高企业盈利能力至关重要，人工智能特别是强化学习为解决这一挑战提供了有前景的途径。

Method: 开发了集成理论模型捕捉功能间复杂交互，设计了新颖的多时间尺度多智能体RL架构，按部门功能分解策略组件，并根据任务复杂性和响应性分配不同学习速度。

Result: 模拟实验表明该方法相比孤岛决策框架显著提高了盈利能力，训练后的RL智能体行为与理论模型的管理洞察高度一致。

Conclusion: 这项工作为复杂商业环境中实现有效跨功能协调提供了可扩展、可解释的基于RL的解决方案。

Abstract: Effective cross-functional coordination is essential for enhancing firm-wide
profitability, particularly in the face of growing organizational complexity
and scale. Recent advances in artificial intelligence, especially in
reinforcement learning (RL), offer promising avenues to address this
fundamental challenge. This paper proposes a unified multi-agent RL framework
tailored for joint optimization across distinct functional modules, exemplified
via coordinating inventory replenishment and personalized product
recommendation. We first develop an integrated theoretical model to capture the
intricate interplay between these functions and derive analytical benchmarks
that characterize optimal coordination. The analysis reveals synchronized
adjustment patterns across products and over time, highlighting the importance
of coordinated decision-making. Leveraging these insights, we design a novel
multi-timescale multi-agent RL architecture that decomposes policy components
according to departmental functions and assigns distinct learning speeds based
on task complexity and responsiveness. Our model-free multi-agent design
improves scalability and deployment flexibility, while multi-timescale updates
enhance convergence stability and adaptability across heterogeneous decisions.
We further establish the asymptotic convergence of the proposed algorithm.
Extensive simulation experiments demonstrate that the proposed approach
significantly improves profitability relative to siloed decision-making
frameworks, while the behaviors of the trained RL agents align closely with the
managerial insights from our theoretical model. Taken together, this work
provides a scalable, interpretable RL-based solution to enable effective
cross-functional coordination in complex business settings.

</details>


### [77] [Speculative Actions: A Lossless Framework for Faster Agentic Systems](https://arxiv.org/abs/2510.04371)
*Naimeng Ye,Arnav Ahuja,Georgios Liargkovas,Yunan Lu,Kostis Kaffes,Tianyi Peng*

Main category: cs.AI

TL;DR: 提出了一种名为"推测动作"的无损框架，通过使用更快的模型预测可能的动作，使多个步骤能够并行执行，从而显著降低智能体系统的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在环境中执行速度缓慢，阻碍了训练、评估和部署。例如，两个最先进的国际象棋智能体对弈可能需要数小时，主要瓶颈在于智能体行为是顺序展开的，每个动作都需要API调用且耗时。

Method: 受微处理器中的推测执行和LLM推理中的推测解码启发，使用更快的模型预测可能的动作，实现多步骤并行执行。评估了游戏、电子商务、网络搜索等环境，并扩展了操作系统的"有损"版本。

Result: 在所有测试环境中，推测动作在下一动作预测方面实现了显著准确率（最高达55%），转化为端到端延迟的显著降低。通过更强的猜测模型、top-K动作预测、多步推测和不确定性感知优化可以进一步提升性能。

Conclusion: 推测动作为在现实世界中部署低延迟智能体系统开辟了一条有前景的道路。

Abstract: Despite growing interest in AI agents across industry and academia, their
execution in an environment is often slow, hampering training, evaluation, and
deployment. For example, a game of chess between two state-of-the-art agents
may take hours. A critical bottleneck is that agent behavior unfolds
sequentially: each action requires an API call, and these calls can be
time-consuming. Inspired by speculative execution in microprocessors and
speculative decoding in LLM inference, we propose speculative actions, a
lossless framework for general agentic systems that predicts likely actions
using faster models, enabling multiple steps to be executed in parallel. We
evaluate this framework across three agentic environments: gaming, e-commerce,
web search, and a "lossy" extension for an operating systems environment. In
all cases, speculative actions achieve substantial accuracy in next-action
prediction (up to 55%), translating into significant reductions in end-to-end
latency. Moreover, performance can be further improved through stronger
guessing models, top-K action prediction, multi-step speculation, and
uncertainty-aware optimization, opening a promising path toward deploying
low-latency agentic systems in the real world.

</details>


### [78] [Just-in-time Episodic Feedback Hinter: Leveraging Offline Knowledge to Improve LLM Agents Adaptation](https://arxiv.org/abs/2510.04373)
*Hadi Nekoei,Aman Jaiswal,Patrice Bechard,Oleh Shliazhko,Orlando Marquez Ayala,Mathieu Reymond,Massimo Caccia,Alexandre Drouin,Sarath Chandar,Alexandre Lacoste*

Main category: cs.AI

TL;DR: JEF Hinter是一个智能代理系统，通过将离线轨迹提炼为紧凑、上下文感知的提示来改进LLM代理在陌生领域的表现，无需在线交互或微调。


<details>
  <summary>Details</summary>
Motivation: 改进LLM代理在陌生领域的性能通常需要昂贵的在线交互或大规模专家数据集微调，这对闭源模型不实用且对开源模型成本高昂，还存在灾难性遗忘风险。

Method: 提出JEF Hinter系统，使用缩放机制从长轨迹中提取关键步骤，将离线轨迹（包括成功和失败轨迹）转化为紧凑提示，通过检索器在推理时选择相关提示提供针对性指导。

Result: 在MiniWoB++、WorkArena-L1和WebArena-Lite上的实验表明，JEF Hinter持续优于包括基于人类和文档提示在内的强基线方法。

Conclusion: JEF Hinter能够有效利用离线轨迹知识，为LLM代理提供透明且可追溯的指导，在多个基准测试中表现优异。

Abstract: Large language model (LLM) agents perform well in sequential decision-making
tasks, but improving them on unfamiliar domains often requires costly online
interactions or fine-tuning on large expert datasets. These strategies are
impractical for closed-source models and expensive for open-source ones, with
risks of catastrophic forgetting. Offline trajectories offer reusable
knowledge, yet demonstration-based methods struggle because raw traces are
long, noisy, and tied to specific tasks. We present Just-in-time Episodic
Feedback Hinter (JEF Hinter), an agentic system that distills offline traces
into compact, context-aware hints. A zooming mechanism highlights decisive
steps in long trajectories, capturing both strategies and pitfalls. Unlike
prior methods, JEF Hinter leverages both successful and failed trajectories,
extracting guidance even when only failure data is available, while supporting
parallelized hint generation and benchmark-independent prompting. At inference,
a retriever selects relevant hints for the current state, providing targeted
guidance with transparency and traceability. Experiments on MiniWoB++,
WorkArena-L1, and WebArena-Lite show that JEF Hinter consistently outperforms
strong baselines, including human- and document-based hints.

</details>


### [79] [Internal World Models as Imagination Networks in Cognitive Agents](https://arxiv.org/abs/2510.04391)
*Saurabh Ranjan,Brian Odegaard*

Main category: cs.AI

TL;DR: 本研究通过心理网络分析比较人类和大型语言模型的内部世界模型，发现两者在想象网络结构上存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探索想象的计算目标，挑战传统认为想象仅用于最大化奖励的观点，提出想象用于访问内部世界模型。

Method: 使用心理网络分析方法，通过两份问卷评估想象生动性评分，从报告中构建想象网络，比较人类和LLM的想象网络结构。

Result: 人类想象网络显示不同中心性度量之间存在相关性，而LLM的想象网络缺乏聚类且中心性度量相关性较低，表明两者内部世界模型存在差异。

Conclusion: 研究提供了一种比较人类和AI内部生成表征的新方法，为开发类人想象的人工智能提供见解。

Abstract: What is the computational objective of imagination? While classical
interpretations suggest imagination is useful for maximizing rewards, recent
findings challenge this view. In this study, we propose that imagination serves
to access an internal world model (IWM) and use psychological network analysis
to explore IWMs in humans and large language models (LLMs). Specifically, we
assessed imagination vividness ratings using two questionnaires and constructed
imagination networks from these reports. Imagination networks from human groups
showed correlations between different centrality measures, including expected
influence, strength, and closeness. However, imagination networks from LLMs
showed a lack of clustering and lower correlations between centrality measures
under different prompts and conversational memory conditions. Together, these
results indicate a lack of similarity between IWMs in human and LLM agents.
Overall, our study offers a novel method for comparing internally-generated
representations in humans and AI, providing insights for developing human-like
imagination in artificial intelligence.

</details>


### [80] [Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning](https://arxiv.org/abs/2510.04488)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.AI

TL;DR: MACI是一个多智能体辩论控制器，通过信息拨盘和行为拨盘分别控制证据质量和辩论行为，使用调节器跟踪分歧、重叠度和论证质量，在预算内实现可证明的终止。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体辩论存在计算资源浪费、固定对抗立场、无深思熟虑的聚合以及基于启发式停止等问题，需要更高效的辩论控制机制。

Method: 引入MACI控制器，包含信息拨盘（按质量筛选证据）和行为拨盘（从探索到巩固的辩论行为调度），使用调节器跟踪关键指标并在收益平稳时停止，采用预算可行的调度器。

Result: 在临床诊断和新闻偏见任务中，MACI提高了准确性和校准度，减少了token使用量，并将剩余不确定性转化为精确的RAG检索计划。

Conclusion: MACI将辩论转变为预算感知、可测量且可证明终止的控制器，通过跨家族LLM法官确保稳定性和顺序不变性。

Abstract: Multi-agent debate often wastes compute by using a fixed adversarial stance,
aggregating without deliberation, or stopping on heuristics. We introduce MACI,
an active controller with two independent dials that decouple information from
behavior: an information dial that gates evidence by quality, and a behavior
dial that schedules contentiousness from exploration to consolidation. A
moderator tracks disagreement, overlap, evidence quality, and argument quality,
and halts when gains plateau. We provide theory-lite guarantees for
nonincreasing dispersion and provable termination, with a budget-feasible
scheduler. Across clinical diagnosis and news-bias tasks, MACI improves
accuracy and calibration while reducing tokens, and converts residual
uncertainty into precision RAG plans that specify what to retrieve next. We use
a cross-family LLM judge (CRIT) as a conservative soft weight and stop signal,
validated for order invariance and judge-swap stability; stability depends on
using high-capability judges. MACI turns debate into a budget-aware,
measurable, and provably terminating controller.

</details>


### [81] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的代理框架，通过在图表空间域中直接执行视觉推理来解决未标注图表理解问题，超越了依赖文本捷径的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释而非依赖文本捷径的未标注图表上性能急剧下降。

Method: ChartAgent迭代地将查询分解为视觉子任务，通过专门的视觉工具（如绘制注释、裁剪区域、定位坐标轴）主动操作和交互图表图像，模拟人类图表理解认知策略。

Result: 在ChartBench和ChartX基准测试中达到最先进准确率，整体绝对增益达16.07%，在未标注数值密集型查询上提升17.31%，且在不同图表类型和复杂度级别上均表现优异。

Conclusion: ChartAgent是首批使用工具增强多模态代理进行视觉基础推理的图表理解工作之一，可作为即插即用框架提升各种底层LLM的性能。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [82] [More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models](https://arxiv.org/abs/2510.04532)
*Xurui Song,Shuo Huai,JingJing Jiang,Jiayi Kong,Jun Luo*

Main category: cs.AI

TL;DR: 该论文构建了DriveMind数据集来验证VLM驾驶代理中推理与规划的因果关系，发现两者存在因果断开，规划主要依赖先验信息而非推理过程。


<details>
  <summary>Details</summary>
Motivation: 验证VLM驾驶代理中自然语言推理是否真正因果驱动轨迹规划这一关键但未经验证的假设。

Method: 构建DriveMind大规模驾驶VQA语料库，通过信息消融实验训练VLM代理，并使用注意力分析和训练无关探针进行诊断。

Result: 推理与规划存在因果断开：移除先验信息导致规划分数大幅下降，而移除推理链仅产生微小变化；注意力分析显示规划主要关注先验而非推理。

Conclusion: 提出推理-规划解耦假说，认为训练产生的推理是附带产物而非因果中介，并提供了诊断工具来评估未来模型的因果保真度。

Abstract: Vision-Language Model (VLM) driving agents promise explainable end-to-end
autonomy by first producing natural-language reasoning and then predicting
trajectory planning. However, whether planning is causally driven by this
reasoning remains a critical but unverified assumption. To investigate this, we
build DriveMind, a large-scale driving Visual Question Answering (VQA) corpus
with plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.
Our data generation process converts sensors and annotations into structured
inputs and, crucially, separates priors from to-be-reasoned signals, enabling
clean information ablations. Using DriveMind, we train representative VLM
agents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization
(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,
indicate a consistent causal disconnect in reasoning-planning: removing
ego/navigation priors causes large drops in planning scores, whereas removing
CoT produces only minor changes. Attention analysis further shows that planning
primarily focuses on priors rather than the CoT. Based on this evidence, we
propose the Reasoning-Planning Decoupling Hypothesis, positing that the
training-yielded reasoning is an ancillary byproduct rather than a causal
mediator. To enable efficient diagnosis, we also introduce a novel,
training-free probe that measures an agent's reliance on priors by evaluating
its planning robustness against minor input perturbations. In summary, we
provide the community with a new dataset and a diagnostic tool to evaluate the
causal fidelity of future models.

</details>


### [83] [Code World Models for General Game Playing](https://arxiv.org/abs/2510.04542)
*Wolfgang Lehrach,Daniel Hennes,Miguel Lazaro-Gredilla,Xinghua Lou,Carter Wendelken,Zun Li,Antoine Dedieu,Jordi Grau-Moya,Marc Lanctot,Atil Iscen,John Schultz,Marcus Chiam,Ian Gemp,Piotr Zielinski,Satinder Singh,Kevin P. Murphy*

Main category: cs.AI

TL;DR: 使用LLM将自然语言游戏规则转换为可执行的Python世界模型，结合MCTS等规划算法，相比直接使用LLM生成动作，该方法具有可验证性、战略深度和泛化能力优势。


<details>
  <summary>Details</summary>
Motivation: 传统使用LLM直接生成游戏动作的方法存在非法移动频繁、策略浅显的问题，需要一种更可靠的方法来利用LLM的推理能力。

Method: 将LLM用于自然语言规则到Python代码的转换，生成包含状态转移、合法动作枚举和终止检查的可执行世界模型，结合MCTS规划算法和启发式价值函数。

Result: 在10个游戏中测试（5个完全观察，5个部分观察），该方法在9个游戏中优于或匹配Gemini 2.5 Pro。

Conclusion: LLM作为代码生成器比直接作为策略更有效，能够产生可验证的、战略深度更强的游戏智能体。

Abstract: Large Language Models (LLMs) reasoning abilities are increasingly being
applied to classical board and card games, but the dominant approach --
involving prompting for direct move generation -- has significant drawbacks. It
relies on the model's implicit fragile pattern-matching capabilities, leading
to frequent illegal moves and strategically shallow play. Here we introduce an
alternative approach: We use the LLM to translate natural language rules and
game trajectories into a formal, executable world model represented as Python
code. This generated model -- comprising functions for state transition, legal
move enumeration, and termination checks -- serves as a verifiable simulation
engine for high-performance planning algorithms like Monte Carlo tree search
(MCTS). In addition, we prompt the LLM to generate heuristic value functions
(to make MCTS more efficient), and inference functions (to estimate hidden
states in imperfect information games). Our method offers three distinct
advantages compared to directly using the LLM as a policy: (1) Verifiability:
The generated CWM serves as a formal specification of the game's rules,
allowing planners to algorithmically enumerate valid actions and avoid illegal
moves, contingent on the correctness of the synthesized model; (2) Strategic
Depth: We combine LLM semantic understanding with the deep search power of
classical planners; and (3) Generalization: We direct the LLM to focus on the
meta-task of data-to-code translation, enabling it to adapt to new games more
easily. We evaluate our agent on 10 different games, of which 4 are novel and
created for this paper. 5 of the games are fully observed (perfect
information), and 5 are partially observed (imperfect information). We find
that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10
considered games.

</details>


### [84] [TRAJECT-Bench:A Trajectory-Aware Benchmark for Evaluating Agentic Tool Use](https://arxiv.org/abs/2510.04550)
*Pengfei He,Zhenwei Dai,Bing He,Hui Liu,Xianfeng Tang,Hanqing Lu,Juanhui Li,Jiayuan Ding,Subhabrata Mukherjee,Suhang Wang,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: TRAJECT-Bench是一个轨迹感知的基准测试，用于全面评估LLM的工具使用能力，通过细粒度指标分析工具选择、参数化和排序的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注最终答案，而忽视了工具使用的详细轨迹，无法全面评估LLM的工具使用能力。

Method: 构建包含实际领域高保真可执行工具的基准测试，合成不同广度（并行调用）和深度（相互依赖链）的轨迹，使用细粒度评估指标。

Result: 揭示了失败模式如相似工具混淆和参数盲选，发现了从短轨迹到中长轨迹过渡的瓶颈，提供了LLM工具使用的可操作指导。

Conclusion: TRAJECT-Bench为LLM工具使用能力的全面评估提供了重要基准，揭示了关键瓶颈和改进方向。

Abstract: Large language model (LLM)-based agents increasingly rely on tool use to
complete real-world tasks. While existing works evaluate the LLMs' tool use
capability, they largely focus on the final answers yet overlook the detailed
tool usage trajectory, i.e., whether tools are selected, parameterized, and
ordered correctly. We introduce TRAJECT-Bench, a trajectory-aware benchmark to
comprehensively evaluate LLMs' tool use capability through diverse tasks with
fine-grained evaluation metrics. TRAJECT-Bench pairs high-fidelity, executable
tools across practical domains with tasks grounded in production-style APIs,
and synthesizes trajectories that vary in breadth (parallel calls) and depth
(interdependent chains). Besides final accuracy, TRAJECT-Bench also reports
trajectory-level diagnostics, including tool selection and argument
correctness, and dependency/order satisfaction. Analyses reveal failure modes
such as similar tool confusion and parameter-blind selection, and scaling
behavior with tool diversity and trajectory length where the bottleneck of
transiting from short to mid-length trajectories is revealed, offering
actionable guidance for LLMs' tool use.

</details>


### [85] [ContextNav: Towards Agentic Multimodal In-Context Learning](https://arxiv.org/abs/2510.04560)
*Honghao Fu,Yuan Ouyang,Kai-Wei Chang,Yiwei Wang,Zi Huang,Yujun Cai*

Main category: cs.AI

TL;DR: ContextNav是一个代理框架，通过图编排将自动化检索的可扩展性与人类式策划的质量相结合，为多模态上下文学习提供噪声鲁棒和动态优化的上下文构建。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习方法在可扩展性和鲁棒性之间存在矛盾：手动选择示例质量高但劳动密集，相似性检索可扩展但可能引入噪声样本影响性能。

Method: 构建资源感知的多模态嵌入管道，维护可检索向量数据库，应用代理检索和结构对齐构建噪声弹性上下文，使用操作语法图支持自适应工作流规划和优化。

Result: 实验结果表明ContextNav在各种数据集上实现了最先进的性能。

Conclusion: 代理工作流有望推进多模态上下文学习中可扩展和鲁棒的上下文构建。

Abstract: Recent advances demonstrate that multimodal large language models (MLLMs)
exhibit strong multimodal in-context learning (ICL) capabilities, enabling them
to adapt to novel vision-language tasks from a few contextual examples.
However, existing ICL approaches face challenges in reconciling scalability
with robustness across diverse tasks and noisy contextual examples: manually
selecting examples produces clean contexts but is labor-intensive and
task-specific, while similarity-based retrieval improves scalability but could
introduce irrelevant or structurally inconsistent samples that degrade ICL
performance. To address these limitations, we propose ContextNav, the first
agentic framework that integrates the scalability of automated retrieval with
the quality and adaptiveness of human-like curation, enabling noise-robust and
dynamically optimized contextualization for multimodal ICL. ContextNav unifies
context management and noise-robust contextualization within a closed-loop
workflow driven by graph-based orchestration. Specifically, it builds a
resource-aware multimodal embedding pipeline, maintains a retrievable vector
database, and applies agentic retrieval and structural alignment to construct
noise-resilient contexts. An Operational Grammar Graph (OGG) further supports
adaptive workflow planning and optimization, enabling the agent to refine its
operational strategies based on downstream ICL feedback. Experimental results
demonstrate that ContextNav achieves state-of-the-art performance across
various datasets, underscoring the promise of agentic workflows for advancing
scalable and robust contextualization in multimodal ICL.

</details>


### [86] [COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context](https://arxiv.org/abs/2510.04568)
*Naman Gupta,Shreeyash Gowaikar,Arun Iyer,Kirankumar Shiragur,Ramakrishna B Bairi,Rishikesh Maurya,Ritabrata Maiti,Sankarshan Damle,Shachee Mishra Gupta*

Main category: cs.AI

TL;DR: COSMIR是一个链式推理框架，用结构化内存替代自由形式的消息传递，通过规划器、工作器和管理器代理的协作来处理长输入问题，减少信息丢失并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型处理长输入时的困难，避免检索方法可能遗漏证据、扩大上下文窗口降低选择性，以及多代理管道中自由形式摘要丢失关键细节和放大早期错误的问题。

Method: 引入结构化内存，由规划器代理将用户查询转化为可检查的子问题，工作器代理通过固定的微循环（提取、推理、精炼）处理数据块并将更新写入共享内存，管理器代理从内存中合成最终答案。

Result: 在HELMET套件的长上下文问答任务中，COSMIR减少了传播阶段的信息丢失，相比CoA基线提高了准确性。

Conclusion: COSMIR通过改变通信媒介（结构化内存）和工作器程序（固定微循环），在保持逐步读取-推理优势的同时，实现了更高的忠实度、更好的长范围聚合和可审计性。

Abstract: Reasoning over very long inputs remains difficult for large language models
(LLMs). Common workarounds either shrink the input via retrieval (risking
missed evidence), enlarge the context window (straining selectivity), or stage
multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents,
CoA), free-form summaries passed between agents can discard crucial details and
amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured
Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc
messages with a structured memory. A Planner agent first turns a user query
into concrete, checkable sub-questions. worker agents process chunks via a
fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared
memory. A Manager agent then Synthesizes the final answer directly from the
memory. This preserves step-wise read-then-reason benefits while changing both
the communication medium (structured memory) and the worker procedure (fixed
micro-cycle), yielding higher faithfulness, better long-range aggregation, and
auditability. On long-context QA from the HELMET suite, COSMIR reduces
propagation-stage information loss and improves accuracy over a CoA baseline.

</details>


### [87] [Making Mathematical Reasoning Adaptive](https://arxiv.org/abs/2510.04617)
*Zhejian Lai,Xiang Geng,Zhijun Wang,Yang Bai,Jiahuan Li,Rongxiang Weng,Jingang Wang,Xuezhi Cao,Xunliang Cai,Shujian Huang*

Main category: cs.AI

TL;DR: 提出了AdaR框架来解决大语言模型在数学推理中的伪推理问题，通过合成逻辑等价查询和使用RLVR训练，提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理中存在鲁棒性和泛化性不足的问题，主要归因于伪推理（仅基于表面特征生成答案）。

Method: AdaR框架通过变化变量值合成逻辑等价查询，使用RLVR训练模型惩罚伪逻辑并鼓励自适应逻辑，通过代码执行提取解题逻辑并生成答案，并进行完整性检查。

Result: 实验结果表明AdaR显著提高了数学推理能力，在保持高数据效率的同时改善了鲁棒性和泛化性。

Conclusion: 数据合成和RLVR协同工作实现了大语言模型的自适应推理，后续分析得出了关键设计见解和适用于指导大语言模型的因素。

Abstract: Mathematical reasoning is a primary indicator of large language models (LLMs)
intelligence. However, existing LLMs exhibit failures of robustness and
generalization. This paper attributes these deficiencies to spurious reasoning,
i.e., producing answers from superficial features. To address this challenge,
we propose the AdaR framework to enable adaptive reasoning, wherein models rely
on problem-solving logic to produce answers. AdaR synthesizes logically
equivalent queries by varying variable values, and trains models with RLVR on
these data to penalize spurious logic while encouraging adaptive logic. To
improve data quality, we extract the problem-solving logic from the original
query and generate the corresponding answer by code execution, then apply a
sanity check. Experimental results demonstrate that AdaR improves robustness
and generalization, achieving substantial improvement in mathematical reasoning
while maintaining high data efficiency. Analysis indicates that data synthesis
and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs.
Subsequent analyses derive key design insights into the effect of critical
factors and the applicability to instruct LLMs. Our project is available at
https://github.com/LaiZhejian/AdaR

</details>


### [88] [MedPAO: A Protocol-Driven Agent for Structuring Medical Reports](https://arxiv.org/abs/2510.04623)
*Shrish Shrinath Vaidya,Gowthamaan Palani,Sidharth Ramesh,Velmurugan Balasubramanian,Minmini Selvam,Gokulraja Srinivasaraja,Ganapathy Krishnamurthi*

Main category: cs.AI

TL;DR: MedPAO是一个基于临床协议的代理框架，通过Plan-Act-Observe循环和专门工具来结构化临床数据，解决了LLM在医疗领域中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在结构化临床数据时产生幻觉事实和无法遵循领域特定规则的问题。

Method: 引入MedPAO代理框架，基于临床协议（如ABCDEF协议）进行CXR分析，通过Plan-Act-Observe循环和专门工具分解报告结构化任务。

Result: MedPAO在概念分类子任务上达到0.96的F1分数，专家评分平均4.52/5，超越了仅依赖LLM的基线方法。

Conclusion: 协议驱动的方法为不透明的单体模型提供了可验证的替代方案，在医疗数据结构化方面表现出更高的可靠性。

Abstract: The deployment of Large Language Models (LLMs) for structuring clinical data
is critically hindered by their tendency to hallucinate facts and their
inability to follow domain-specific rules. To address this, we introduce
MedPAO, a novel agentic framework that ensures accuracy and verifiable
reasoning by grounding its operation in established clinical protocols such as
the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring
task into a transparent process managed by a Plan-Act-Observe (PAO) loop and
specialized tools. This protocol-driven method provides a verifiable
alternative to opaque, monolithic models. The efficacy of our approach is
demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96
on the critical sub-task of concept categorization. Notably, expert
radiologists and clinicians rated the final structured outputs with an average
score of 4.52 out of 5, indicating a level of reliability that surpasses
baseline approaches relying solely on LLM-based foundation models. The code is
available at: https://github.com/MiRL-IITM/medpao-agent

</details>


### [89] [QuantAgents: Towards Multi-agent Financial System via Simulated Trading](https://arxiv.org/abs/2510.04643)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 提出了QuantAgents多智能体金融系统，通过模拟交易评估投资策略，包含四个协作智能体，在三年内实现了近300%的整体回报。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在金融领域表现良好但与现实基金公司存在差距，特别是缺乏长期趋势预测能力，需要开发更贴近现实的多智能体金融系统。

Method: 构建包含模拟交易分析师、风险控制分析师、市场新闻分析师和管理者四个智能体的系统，通过多次会议协作，并在真实市场表现和模拟交易预测准确性两方面给予反馈激励。

Result: 系统在所有指标上表现优异，三年内实现了近300%的整体投资回报。

Conclusion: QuantAgents框架通过多智能体协作和模拟交易，有效提升了金融决策的准确性和长期预测能力。

Abstract: In this paper, our objective is to develop a multi-agent financial system
that incorporates simulated trading, a technique extensively utilized by
financial professionals. While current LLM-based agent models demonstrate
competitive performance, they still exhibit significant deviations from
real-world fund companies. A critical distinction lies in the agents' reliance
on ``post-reflection'', particularly in response to adverse outcomes, but lack
a distinctly human capability: long-term prediction of future trends.
Therefore, we introduce QuantAgents, a multi-agent system integrating simulated
trading, to comprehensively evaluate various investment strategies and market
scenarios without assuming actual risks. Specifically, QuantAgents comprises
four agents: a simulated trading analyst, a risk control analyst, a market news
analyst, and a manager, who collaborate through several meetings. Moreover, our
system incentivizes agents to receive feedback on two fronts: performance in
real-world markets and predictive accuracy in simulated trading. Extensive
experiments demonstrate that our framework excels across all metrics, yielding
an overall return of nearly 300% over the three years
(https://quantagents.github.io/).

</details>


### [90] [Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents](https://arxiv.org/abs/2510.04695)
*Yiding Wang,Zhepei Wei,Xinyu Zhu,Yu Meng*

Main category: cs.AI

TL;DR: DeSA是一个两阶段训练框架，通过分离搜索优化和答案生成来解决仅基于结果奖励训练的搜索增强代理存在的搜索缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索增强代理通常依赖结果奖励（如精确匹配），假设优化最终答案也会产生有效的中间搜索行为。但研究发现这种假设存在问题，仅基于结果训练会出现工具调用失败、无效查询和冗余搜索等系统性缺陷。

Method: 提出DeSA两阶段训练框架：第一阶段用检索召回奖励训练代理改进搜索效果；第二阶段用结果奖励优化最终答案生成。

Result: 在7个QA基准测试中，DeSA训练的代理显著改善了搜索行为，搜索召回率和答案准确率都大幅高于仅基于结果的基线方法。

Conclusion: 明确分离搜索和回答目标对于训练有效的搜索增强代理是必要的，DeSA优于同时优化召回和结果奖励的单阶段训练方法。

Abstract: Enabling large language models (LLMs) to utilize search tools offers a
promising path to overcoming fundamental limitations such as knowledge cutoffs
and hallucinations. Recent work has explored reinforcement learning (RL) for
training search-augmented agents that interleave reasoning and retrieval before
answering. These approaches usually rely on outcome-based rewards (e.g., exact
match), implicitly assuming that optimizing for final answers will also yield
effective intermediate search behaviors. Our analysis challenges this
assumption: we uncover multiple systematic deficiencies in search that arise
under outcome-only training and ultimately degrade final answer quality,
including failure to invoke tools, invalid queries, and redundant searches. To
address these shortcomings, we introduce DeSA (Decoupling
Search-and-Answering), a simple two-stage training framework that explicitly
separates search optimization from answer generation. In Stage 1, agents are
trained to improve search effectiveness with retrieval recall-based rewards. In
Stage 2, outcome rewards are employed to optimize final answer generation.
Across seven QA benchmarks, DeSA-trained agents consistently improve search
behaviors, delivering substantially higher search recall and answer accuracy
than outcome-only baselines. Notably, DeSA outperforms single-stage training
approaches that simultaneously optimize recall and outcome rewards,
underscoring the necessity of explicitly decoupling the two objectives.

</details>


### [91] [BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs](https://arxiv.org/abs/2510.04721)
*Ivo Petrov,Jasper Dekoninck,Martin Vechev*

Main category: cs.AI

TL;DR: BrokenMath是首个评估LLM在自然语言定理证明中谄媚行为的基准，基于2025年竞赛问题构建，发现GPT-5等模型有29%的概率产生谄媚回答，缓解策略能减少但不能完全消除该行为。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准在评估谄媚行为方面存在局限：仅关注最终答案问题、依赖简单且被污染的数据集、使用合成修改创建病态问题而非可证明错误的良构问题。

Method: 从2025年竞赛问题构建BrokenMath基准，通过LLM扰动产生错误陈述并经过专家评审完善，使用LLM-as-a-judge框架评估最先进模型和代理系统。

Result: 发现谄媚行为普遍存在，最佳模型GPT-5有29%的概率产生谄媚回答，测试时干预和监督微调等缓解策略能显著减少但不能完全消除该行为。

Conclusion: LLM在定理证明中存在显著的谄媚问题，需要更有效的缓解策略来提升其在数学应用中的可靠性。

Abstract: Large language models (LLMs) have recently shown strong performance on
mathematical benchmarks. At the same time, they are prone to hallucination and
sycophancy, often providing convincing but flawed proofs for incorrect
mathematical statements provided by users. This significantly limits the
applicability of LLMs in theorem proving, as verification of these flawed
proofs must be done manually by expert mathematicians. However, existing
benchmarks that measure sycophancy in mathematics are limited: they focus
solely on final-answer problems, rely on very simple and often contaminated
datasets, and construct benchmark samples using synthetic modifications that
create ill-posed questions rather than well-posed questions that are
demonstrably false. To address these issues, we introduce BrokenMath, the first
benchmark for evaluating sycophantic behavior in LLMs within the context of
natural language theorem proving. BrokenMath is built from advanced 2025
competition problems, which are perturbed with an LLM to produce false
statements and subsequently refined through expert review. Using an
LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems
and find that sycophancy is widespread, with the best model, GPT-5, producing
sycophantic answers 29% of the time. We further investigate several mitigation
strategies, including test-time interventions and supervised fine-tuning on
curated sycophantic examples. These approaches substantially reduce, but do not
eliminate, sycophantic behavior.

</details>


### [92] [LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation](https://arxiv.org/abs/2510.04851)
*Dongge Han,Camille Couturier,Daniel Madrigal Diaz,Xuchao Zhang,Victor Rühle,Saravan Rajmohan*

Main category: cs.AI

TL;DR: LEGOMem是一个用于多智能体工作流自动化的模块化程序记忆框架，通过分解历史任务轨迹为可重用记忆单元，灵活分配给编排器和任务智能体以支持规划和执行。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统中记忆的设计空间，研究记忆应该放在哪里、如何检索以及哪些智能体受益最多。

Method: 使用LEGOMem作为研究工具，在OfficeBench基准上进行实验，分析编排器记忆和细粒度智能体记忆的作用。

Result: 编排器记忆对有效任务分解和委派至关重要，细粒度智能体记忆提高执行准确性。即使由较小语言模型组成的团队也能通过程序记忆显著受益，缩小与更强智能体的性能差距。

Conclusion: LEGOMem既是记忆增强智能体系统的实用框架，也是理解多智能体工作流自动化中记忆设计的研究工具。

Abstract: We introduce LEGOMem, a modular procedural memory framework for multi-agent
large language model (LLM) systems in workflow automation. LEGOMem decomposes
past task trajectories into reusable memory units and flexibly allocates them
across orchestrators and task agents to support planning and execution. To
explore the design space of memory in multi-agent systems, we use LEGOMem as a
lens and conduct a systematic study of procedural memory in multi-agent
systems, examining where memory should be placed, how it should be retrieved,
and which agents benefit most. Experiments on the OfficeBench benchmark show
that orchestrator memory is critical for effective task decomposition and
delegation, while fine-grained agent memory improves execution accuracy. We
find that even teams composed of smaller language models can benefit
substantially from procedural memory, narrowing the performance gap with
stronger agents by leveraging prior execution traces for more accurate planning
and tool use. These results position LEGOMem as both a practical framework for
memory-augmented agent systems and a research tool for understanding memory
design in multi-agent workflow automation.

</details>


### [93] [Video Game Level Design as a Multi-Agent Reinforcement Learning Problem](https://arxiv.org/abs/2510.04862)
*Sam Earle,Zehua Jiang,Eugene Vinitsky,Julian Togelius*

Main category: cs.AI

TL;DR: 该论文提出将程序化内容生成重构为多智能体强化学习问题，通过分布式智能体协作解决单智能体PCGRL的效率瓶颈和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 现有PCGRL研究主要关注单智能体生成器，但面临频繁重新计算启发式质量指标和在大型地图中导航的效率瓶颈。

Method: 将关卡生成重构为多智能体问题，通过分布式智能体协作减少奖励计算次数，学习局部模块化设计策略。

Result: 多智能体关卡生成器在效率上显著提升，且能更好地泛化到分布外地图形状，学习到更局部、模块化的设计策略。

Conclusion: 将内容生成视为分布式多智能体任务有利于大规模生成功能性游戏内容。

Abstract: Procedural Content Generation via Reinforcement Learning (PCGRL) offers a
method for training controllable level designer agents without the need for
human datasets, using metrics that serve as proxies for level quality as
rewards. Existing PCGRL research focuses on single generator agents, but are
bottlenecked by the need to frequently recalculate heuristics of level quality
and the agent's need to navigate around potentially large maps. By framing
level generation as a multi-agent problem, we mitigate the efficiency
bottleneck of single-agent PCGRL by reducing the number of reward calculations
relative to the number of agent actions. We also find that multi-agent level
generators are better able to generalize to out-of-distribution map shapes,
which we argue is due to the generators' learning more local, modular design
policies. We conclude that treating content generation as a distributed,
multi-agent task is beneficial for generating functional artifacts at scale.

</details>


### [94] [Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error Attribution](https://arxiv.org/abs/2510.04886)
*Adi Banerjee,Anirudh Nair,Tarik Borogovac*

Main category: cs.AI

TL;DR: 提出ECHO算法，通过层次化上下文表示、基于目标分析的评估和共识投票来改进多智能体系统中的错误归因准确性


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统中的错误归因方法在处理复杂模式时存在准确性和一致性问题，需要更有效的调试工具

Method: 结合层次化上下文表示、基于目标分析的评估和共识投票机制，利用基于位置的上下文理解层次化方法

Result: ECHO在各种多智能体交互场景中优于现有方法，在涉及微妙推理错误和复杂依赖关系的案例中表现尤为突出

Conclusion: 结构化层次化上下文表示与基于共识的客观决策相结合，为多智能体系统错误归因提供了更稳健的框架

Abstract: Error attribution in Large Language Model (LLM) multi-agent systems presents
a significant challenge in debugging and improving collaborative AI systems.
Current approaches to pinpointing agent and step level failures in interaction
traces - whether using all-at-once evaluation, step-by-step analysis, or binary
search - fall short when analyzing complex patterns, struggling with both
accuracy and consistency. We present ECHO (Error attribution through Contextual
Hierarchy and Objective consensus analysis), a novel algorithm that combines
hierarchical context representation, objective analysis-based evaluation, and
consensus voting to improve error attribution accuracy. Our approach leverages
a positional-based leveling of contextual understanding while maintaining
objective evaluation criteria, ultimately reaching conclusions through a
consensus mechanism. Experimental results demonstrate that ECHO outperforms
existing methods across various multi-agent interaction scenarios, showing
particular strength in cases involving subtle reasoning errors and complex
interdependencies. Our findings suggest that leveraging these concepts of
structured, hierarchical context representation combined with consensus-based
objective decision-making, provides a more robust framework for error
attribution in multi-agent systems.

</details>


### [95] [Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits](https://arxiv.org/abs/2510.04952)
*Ailiya Borjigin,Cong He*

Main category: cs.AI

TL;DR: 提出一个结合强化学习和合规约束的跨市场算法交易系统，通过安全强化学习确保交易执行质量同时满足监管要求


<details>
  <summary>Details</summary>
Motivation: 解决算法交易中执行质量与合规监管的平衡问题，确保交易策略在满足参与限制、价格区间和自交易避免等硬约束下的最优执行

Method: 采用分层架构：高层规划器、强化学习执行代理和独立合规代理。将交易执行建模为带约束的马尔可夫决策过程，使用近端策略优化训练执行代理，运行时动作屏蔽确保行动可行性，并添加零知识合规审计层

Result: 在ABIDES多场所模拟器中评估，相比TWAP、VWAP等基准方法，学习策略降低了执行差额和方差，在压力测试中未观察到约束违反，统计显著性达95%置信水平

Conclusion: 该工作处于最优执行、安全强化学习、监管技术和可验证AI的交叉领域，讨论了伦理考量、建模假设和计算开销等限制，以及实际部署路径

Abstract: We present a cross-market algorithmic trading system that balances execution
quality with rigorous compliance enforcement. The architecture comprises a
high-level planner, a reinforcement learning execution agent, and an
independent compliance agent. We formulate trade execution as a constrained
Markov decision process with hard constraints on participation limits, price
bands, and self-trading avoidance. The execution agent is trained with proximal
policy optimization, while a runtime action-shield projects any unsafe action
into a feasible set. To support auditability without exposing proprietary
signals, we add a zero-knowledge compliance audit layer that produces
cryptographic proofs that all actions satisfied the constraints. We evaluate in
a multi-venue, ABIDES-based simulator and compare against standard baselines
(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and
variance while exhibiting no observed constraint violations across stress
scenarios including elevated latency, partial fills, compliance module
toggling, and varying constraint limits. We report effects at the 95%
confidence level using paired t-tests and examine tail risk via CVaR. We
situate the work at the intersection of optimal execution, safe reinforcement
learning, regulatory technology, and verifiable AI, and discuss ethical
considerations, limitations (e.g., modeling assumptions and computational
overhead), and paths to real-world deployment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [96] [ALMAS: an Autonomous LLM-based Multi-Agent Software Engineering Framework](https://arxiv.org/abs/2510.03463)
*Vali Tawosi,Keshav Ramani,Salwa Alamir,Xiaomo Liu*

Main category: cs.SE

TL;DR: 提出了ALMAS框架，一个基于LLM的自主多智能体软件工程系统，能够遵循软件开发生命周期在敏捷团队中执行端到端任务。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM系统在软件开发中主要关注代码实现，但软件开发是多方面的，需要涵盖整个软件开发生命周期。

Method: 设计ALMAS框架，将智能体与敏捷角色对齐，采用模块化设计，能够与人类开发者和开发环境无缝集成。

Result: 展示了ALMAS框架的进展，包括已发表工作和用例演示，能够无缝生成应用程序并添加新功能。

Conclusion: ALMAS框架为基于LLM的多智能体软件工程提供了可行的解决方案，能够有效支持端到端的软件开发任务。

Abstract: Multi-agent Large Language Model (LLM) systems have been leading the way in
applied LLM research across a number of fields. One notable area is software
development, where researchers have advanced the automation of code
implementation, code testing, code maintenance, inter alia, using LLM agents.
However, software development is a multifaceted environment that extends beyond
just code. As such, a successful LLM system must factor in multiple stages of
the software development life-cycle (SDLC). In this paper, we propose a vision
for ALMAS, an Autonomous LLM-based Multi-Agent Software Engineering framework,
which follows the above SDLC philosophy such that it may work within an agile
software development team to perform several tasks end-to-end. ALMAS aligns its
agents with agile roles, and can be used in a modular fashion to seamlessly
integrate with human developers and their development environment. We showcase
the progress towards ALMAS through our published works and a use case
demonstrating the framework, where ALMAS is able to seamlessly generate an
application and add a new feature.

</details>


### [97] [Relative Code Comprehensibility Prediction](https://arxiv.org/abs/2510.03474)
*Nadeeshan De Silva,Martin Kellogg,Oscar Chaparro*

Main category: cs.SE

TL;DR: 该论文提出使用相对可理解性预测方法替代绝对可理解性预测，通过比较两个代码片段的相对可理解性来缓解人类数据中的噪声问题，在实验中相对方法比绝对方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的代码可理解性度量指标和机器学习模型预测准确度有限，主要原因是人类可理解性数据存在固有噪声，直接预测绝对可理解性会混淆模型。

Method: 训练模型预测两个代码片段的相对可理解性（哪个更容易理解），而不是单独预测每个片段的绝对可理解性。使用150个Java代码片段和12.5k个人类可理解性测量数据进行实验。

Result: 绝对可理解性模型最多比基线提升33.4%，且经常表现不佳；相对可理解性模型平均提升137.8%（片段级）和74.7%（开发者级），表现显著更好。

Conclusion: 相对可理解性模型能更有效地从数据中学习，支持其在软件工程下游任务中的实际应用。

Abstract: Automatically predicting how difficult it is for humans to understand a code
snippet can assist developers in tasks like deciding when and where to
refactor. Despite many proposed code comprehensibility metrics, studies have
shown they often correlate poorly with actual measurements of human
comprehensibility. This has motivated the use of machine learning models to
predict human comprehensibility directly from code, but these models have also
shown limited accuracy.
  We argue that model inaccuracy stems from inherent noise in human
comprehensibility data, which confuses models trained to predict it directly.
To address this, we propose training models to predict the relative
comprehensibility of two code snippets - that is, predicting which snippet a
human would find easier to understand without predicting each snippet's
comprehensibility in isolation. This mitigates noise in predicting 'absolute'
comprehensibility measurements, but is still useful for downstream
software-engineering tasks like assessing whether refactoring improves or
hinders comprehensibility.
  We conducted a study to assess and compare the effectiveness of absolute and
relative code comprehensibility prediction via machine learning. We used a
dataset of 150 Java code snippets and 12.5k human comprehensibility
measurements from prior user studies, comparing the models' performance with
naive baselines (eg 'always predict the majority class'). Our findings indicate
that absolute comprehensibility models improve over the baselines by at most
33.4% and frequently underperform. In contrast, relative comprehensibility
models are substantially better, with average improvements of 137.8% and 74.7%
for snippet-wise and developer-wise prediction, respectively. These results
suggest that relative comprehensibility models learn more effectively from the
data, supporting their practical applicability for downstream SE tasks.

</details>


### [98] [LLM Agents for Automated Dependency Upgrades](https://arxiv.org/abs/2510.03480)
*Vali Tawosi,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 提出了一个基于LLM代理的框架，用于自动更新Java代码库中的库依赖，通过迁移文档推荐和应用代码更新，确保与新版本的兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着代码库的扩展，库依赖会变得过时，需要更新以保持创新和安全性。但更新库可能引入破坏性变更，需要大量开发时间进行维护。

Method: 使用LLM代理框架结合迁移文档，包括摘要代理、控制代理和代码代理，自动定位更新的库使用并实施推荐修复。

Result: 在工业用例中创建三个合成代码库进行测试，结果显示该方法在所有情况下使用更少的token，并达到71.4%的精确度。

Conclusion: 该方法相比现有技术方法在效率和有效性方面表现更优。

Abstract: As a codebase expands over time, its library dependencies can become outdated
and require updates to maintain innovation and security. However, updating a
library can introduce breaking changes in the code, necessitating significant
developer time for maintenance. To address this, we introduce a framework of
LLM agents to be used in combination with migration documentation to
automatically recommend and apply code updates and ensure compatibility with
new versions. Our solution can automatically localize updated library usages in
live Java codebases and implement recommended fixes in a user-friendly manner.
The system architecture consists of multiple key components: a Summary Agent,
Control Agent, and Code Agent. To validate our approach, we apply the framework
on an industrial use case by which we create three synthetic code repositories
with major Upgrade changes and benchmark our approach against state-of-the-art
methods. Results show that our approach not only performs upgrades using fewer
tokens across all cases but also achieves a precision of 71.4%, highlighting
its efficiency and effectiveness compared to state-of-the-art methods.

</details>


### [99] [AgentHub: A Research Agenda for Agent Sharing Infrastructure](https://arxiv.org/abs/2510.03495)
*Erik Pautsch,Tanmay Singla,Wenxin Jiang,Huiyun Peng,Behnaz Hassanshahi,Konstantin Läufer,George K. Thiruvathukal,James C. Davis*

Main category: cs.SE

TL;DR: 提出AgentHub研究议程，旨在解决LLM智能体生态系统中的基础设施碎片化问题，通过定义关键挑战来推动可靠、可扩展的智能体生态系统建设。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体基础设施碎片化，缺乏类似npm或Hugging Face的成熟生态系统，现有工作关注点过于狭窄，需要更全面的软件工程视角来改善开源分发和重用。

Method: 提出AgentHub研究议程，通过框架化关键挑战：能力清晰度、生命周期透明度、互操作性、治理、安全性和工作流集成，来指导社区建设智能体生态系统。

Result: 建立了智能体共享的研究议程框架，明确了六个关键挑战领域，为构建可靠和可扩展的智能体生态系统提供了路线图。

Conclusion: AgentHub愿景是让智能体能够像今天的软件库一样被无缝共享、信任和组合，这需要社区共同努力解决提出的关键挑战。

Abstract: LLM-based agents are rapidly proliferating, yet the infrastructure for
discovering, evaluating, and governing them remains fragmented compared to
mature ecosystems like software package registries (e.g., npm) and model hubs
(e.g., Hugging Face). Recent research and engineering works have begun to
consider the requisite infrastructure, but so far they focus narrowly -- on
distribution, naming, or protocol negotiation. However, considering broader
software engineering requirements would improve open-source distribution and
ease reuse. We therefore propose AgentHub, a research agenda for agent sharing.
By framing the key challenges of capability clarity, lifecycle transparency,
interoperability, governance, security, and workflow integration, AgentHub
charts a community-wide agenda for building reliable and scalable agent
ecosystems. Our vision is a future where agents can be shared, trusted, and
composed as seamlessly as today's software libraries.

</details>


### [100] [REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement](https://arxiv.org/abs/2510.03588)
*Anvith Pabba,Simin Chen,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出了Refine补丁精炼框架，通过解决代码上下文歧义、多样化补丁候选和LLM驱动的代码审查，将草稿补丁转化为正确补丁，在SWE-Bench基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动程序修复技术由于对代码上下文理解有限和过度依赖不完整测试套件，经常产生部分正确的草稿补丁，需要进一步精炼才能得到正确修复。

Method: Refine框架通过三个关键步骤：消除问题与代码上下文的歧义、通过测试时扩展多样化补丁候选、使用LLM驱动的代码审查聚合部分修复，可作为通用精炼模块集成到各种APR系统中。

Result: 在SWE-Bench Lite基准测试中达到51.67%的分数，比AutoCodeRover提升14.67%；在SWE-Bench Verified上提升解决率12.2%；集成多个APR系统平均提升14%。

Conclusion: 精炼是当前APR流程中缺失的关键组件，代理协作在缩小接近正确与完全正确补丁之间的差距方面具有巨大潜力。

Abstract: Large Language Models (LLMs) have recently shown strong potential in
automatic program repair (APR), especially in repository-level settings where
the goal is to generate patches based on natural language issue descriptions,
large codebases, and regression tests. However, despite their promise, current
LLM-based APR techniques often struggle to produce correct fixes due to limited
understanding of code context and over-reliance on incomplete test suites. As a
result, they frequently generate Draft Patches-partially correct patches that
either incompletely address the bug or overfit to the test cases. In this work,
we propose a novel patch refinement framework, Refine, that systematically
transforms Draft Patches into correct ones. Refine addresses three key
challenges: disambiguating vague issue and code context, diversifying patch
candidates through test-time scaling, and aggregating partial fixes via an
LLM-powered code review process. We implement Refine as a general refinement
module that can be integrated into both open-agent-based and workflow-based APR
systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine
achieves state-of-the-art results among workflow-based approaches and
approaches the best-known performance across all APR categories. Specifically,
Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of
51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine
improves the resolution rate by 12.2%, and when integrated across multiple APR
systems, it yields an average improvement of 14%-demonstrating its broad
effectiveness and generalizability. These results highlight the effectiveness
of refinement as a missing component in current APR pipelines and the potential
of agentic collaboration in closing the gap between near-correct and correct
patches. We also open source our code.

</details>


### [101] [Generating High-Level Test Cases from Requirements using LLM: An Industry Study](https://arxiv.org/abs/2510.03641)
*Satoshi Masuda,Satoshi Kouzawa,Kyousuke Sezai,Hidetoshi Suhara,Yasuaki Hiruta,Kunihiro Kudou*

Main category: cs.SE

TL;DR: 提出了一种仅使用提示而不创建RAG的方法，从需求文档自动生成高层次测试用例。该方法首先生成测试设计技术，然后为每个技术生成测试用例，并在蓝牙和Mozilla数据集上验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 目前从需求文档生成高层次测试用例主要依赖人工，行业对使用LLM自动生成测试用例有强烈需求。现有RAG方法需要为每个应用定制知识系统，工作量大，需要建立无需RAG的通用方法。

Method: 首先将需求文档输入LLM生成对应的测试设计技术，然后为每个生成的测试设计技术生成高层次测试用例。同时验证了基于语义相似度的评估方法。

Result: 在蓝牙和Mozilla数据集上的实验显示，宏召回率分别达到0.81和0.37，表明该方法在实际应用中具有可行性。

Conclusion: 提出的方法无需创建RAG即可生成高层次测试用例，为实际应用提供了可行的解决方案。

Abstract: Currently, generating high-level test cases described in natural language
from requirement documents is performed manually. In the industry, including
companies specializing in software testing, there is a significant demand for
the automatic generation of high-level test cases from requirement documents
using Large Language Models (LLMs). Efforts to utilize LLMs for requirement
analysis are underway. In some cases, retrieval-augmented generation (RAG) is
employed for generating high-level test cases using LLMs. However, in practical
applications, it is necessary to create a RAG tailored to the knowledge system
of each specific application, which is labor-intensive. Moreover, when applying
high-level test case generation as a prompt, there is no established method for
instructing the generation of high-level test cases at a level applicable to
other specifications without using RAG. It is required to establish a method
for the automatic generation of high-level test cases that can be generalized
across a wider range of requirement documents. In this paper, we propose a
method for generating high-level (GHL) test cases from requirement documents
using only prompts, without creating RAGs. In the proposed method, first, the
requirement document is input into the LLM to generate test design techniques
corresponding to the requirement document. Then, high-level test cases are
generated for each of the generated test design techniques. Furthermore, we
verify an evaluation method based on semantic similarity of the generated
high-level test cases. In the experiments, we confirmed the method using
datasets from Bluetooth and Mozilla, where requirement documents and high-level
test cases are available, achieving macro-recall measurement of 0.81 and 0.37,
respectively. We believe that the method is feasible for practical application
in generating high-level test cases without using RAG.

</details>


### [102] [APIDA-Chat: Structured Synthesis of API Search Dialogues to Bootstrap Conversational Agents](https://arxiv.org/abs/2510.03743)
*Zachary Eberhart,Collin McMillan*

Main category: cs.SE

TL;DR: APIDA-Chat是一个开源管道，将符号对话脚本转换为真实的API搜索对话，使用轻量模型生成廉价训练数据，解决小众API对话数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型助手在解释流行API时表现良好，但在小众或专有库上表现不佳，因为用于微调的多轮对话数据稀缺。

Method: 两阶段方法：第一阶段使用传统对话规划器与教师LLM合成"黄金集"对话；第二阶段使用微调后的学生模型与相同规划器，实现低成本新对话合成。

Result: 微调后的学生模型BLEU从0.38提升到0.50，BERTScore从0.88提升到0.91，且可在单个消费级GPU上运行。

Conclusion: APIDA-Chat提供了一个模块化、开源的解决方案，可作为未来工作的保守基线，支持低成本API对话数据生成。

Abstract: Large-language-model assistants are suitable for explaining popular APIs, yet
they falter on niche or proprietary libraries because the multi-turn dialogue
data needed for fine-tuning are scarce. We present APIDA-Chat, an open-source
pipeline that converts symbolic dialogue-act "scripts" into realistic,
domain-grounded API Search conversations using a lightweight model for
inexpensive training data generation. Phase I pairs a legacy dialogue planner
with a high-capability teacher LLM (o4-mini) to synthesize a "gold set" of
realized dialogues; then, a smaller Llama 3.2 3B student model is fine-tuned on
this corpus. Phase II drops the teacher and reuses the same planner with the
fine-tuned model, allowing rapid, low-cost synthesis of new dialogues without
exposing source code to external services. The fine-tuned student improves BLEU
from 0.38 to 0.50 and BERTScore from 0.88 to 0.91 versus the base model while
running entirely on a single consumer GPU. All components are modular and
publicly released to serve as a conservative baseline for future work.
APIDA-Chat is open-sourced at https://github.com/Zeberhart/apida-chat and a
video demo is available at https://youtu.be/YqmZBHyGbPs .

</details>


### [103] [Code4MeV2: a Research-oriented Code-completion Platform](https://arxiv.org/abs/2510.03755)
*Roham Koohestani,Parham Bateni,Aydin Ebrahimi,Behdad Etezadi,Kiarash Karimi,Maliheh Izadi*

Main category: cs.SE

TL;DR: Code4MeV2是一个开源代码补全插件，采用客户端-服务器架构，提供内联代码补全和上下文感知聊天助手，旨在解决AI代码补全工具用户交互数据专有化问题。


<details>
  <summary>Details</summary>
Motivation: AI代码补全工具的用户交互数据被大公司专有化，阻碍了学术研究。研究人员需要开发专用平台来研究人机交互，使得可重复研究和大规模数据分析变得不切实际。

Method: 开发Code4MeV2插件，采用客户端-服务器架构，包含模块化和透明的数据收集框架，让研究人员能够精细控制遥测和上下文收集。

Result: Code4MeV2在代码补全方面达到行业可比性能，平均延迟200毫秒。通过专家评估和8名参与者的用户研究验证了工具的信息性和实用性。

Conclusion: Code4MeV2为研究社区提供了开放的数据收集解决方案，邀请社区采用和贡献。

Abstract: The adoption of AI-powered code completion tools in software development has
increased substantially, yet the user interaction data produced by these
systems remain proprietary within large corporations. This creates a barrier
for the academic community, as researchers must often develop dedicated
platforms to conduct studies on human--AI interaction, making reproducible
research and large-scale data analysis impractical. In this work, we introduce
Code4MeV2, a research-oriented, open-source code completion plugin for
JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a
client--server architecture and features inline code completion and a
context-aware chat assistant. Its core contribution is a modular and
transparent data collection framework that gives researchers fine-grained
control over telemetry and context gathering. Code4MeV2 achieves
industry-comparable performance in terms of code completion, with an average
latency of 200~ms. We assess our tool through a combination of an expert
evaluation and a user study with eight participants. Feedback from both
researchers and daily users highlights its informativeness and usefulness. We
invite the community to adopt and contribute to this tool. More information
about the tool can be found at https://app.code4me.me.

</details>


### [104] [Smart Paste: Automatically Fixing Copy/Paste for Google Developers](https://arxiv.org/abs/2510.03843)
*Vincent Nguyen,Guilherme Herzog,José Cambronero,Marcus Revaj,Aditya Kini,Alexander Frömmgen,Maxim Tabachnyk*

Main category: cs.SE

TL;DR: 开发了Smart Paste IDE功能，通过深度学习预测粘贴代码后的编辑建议，在Google内部部署后获得45%的接受率，占公司代码总量的1%以上


<details>
  <summary>Details</summary>
Motivation: 手动编辑粘贴代码是开发者的长期痛点，Google内部数据显示代码粘贴频率是手动输入的4倍，且粘贴后经常需要后续编辑

Method: 迭代开发和扩展Smart Paste IDE功能，涵盖用户体验、系统集成和模型能力，使用深度学习预测粘贴后的编辑需求

Result: 部署后获得压倒性积极反馈，45%的接受率，在Google企业规模下，这些建议占公司所有代码的1%以上

Conclusion: Smart Paste成功解决了代码粘贴后的编辑痛点，为AI从业者提供了功能开发的整体方法指南

Abstract: Manually editing pasted code is a long-standing developer pain point. In
internal software development at Google, we observe that code is pasted 4 times
more often than it is manually typed. These paste actions frequently require
follow-up edits, ranging from simple reformatting and renaming to more complex
style adjustments and cross-language translations. Prior work has shown deep
learning can be used to predict these edits. In this work, we show how to
iteratively develop and scale Smart Paste, an IDE feature for post-paste edit
suggestions, to Google's development environment. This experience can serve as
a guide for AI practitioners on a holistic approach to feature development,
covering user experience, system integration, and model capabilities. Since
deployment, Smart Paste has had overwhelmingly positive feedback with a 45%
acceptance rate. At Google's enterprise scale, these accepted suggestions
account substantially for over 1% of all code written company-wide.

</details>


### [105] [Designing Empirical Studies on LLM-Based Code Generation: Towards a Reference Framework](https://arxiv.org/abs/2510.03862)
*Nathalia Nascimento,Everton Guimaraes,Paulo Alencar*

Main category: cs.SE

TL;DR: 提出用于设计和报告基于LLM的代码生成实证研究的理论框架，以解决当前评估缺乏标准化的问题


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成的实证评估缺乏标准化，研究在目标、任务和指标上差异很大，限制了可比性和可重复性

Method: 基于先前经验和近期研究的比较分析，构建围绕问题来源、质量属性和指标等核心组件的评估框架

Result: 通过代表性案例映射展示了框架的适用性，并确定了改进机会

Conclusion: 该框架为软件工程背景下标准化LLM评估提供了结构化方法，计划将其发展为更成熟的工具

Abstract: The rise of large language models (LLMs) has introduced transformative
potential in automated code generation, addressing a wide range of software
engineering challenges. However, empirical evaluation of LLM-based code
generation lacks standardization, with studies varying widely in goals, tasks,
and metrics, which limits comparability and reproducibility. In this paper, we
propose a theoretical framework for designing and reporting empirical studies
on LLM-based code generation. The framework is grounded in both our prior
experience conducting such experiments and a comparative analysis of key
similarities and differences among recent studies. It organizes evaluation
around core components such as problem sources, quality attributes, and
metrics, supporting structured and systematic experimentation. We demonstrate
its applicability through representative case mappings and identify
opportunities for refinement. Looking forward, we plan to evolve the framework
into a more robust and mature tool for standardizing LLM evaluation across
software engineering contexts.

</details>


### [106] [Adversarial Agent Collaboration for C to Rust Translation](https://arxiv.org/abs/2510.03879)
*Tianyu Li,Ruishi Li,Bo Wang,Brandon Paulsen,Umang Mathur,Prateek Saxena*

Main category: cs.SE

TL;DR: ACToR是一个基于LLM代理的C到Rust翻译器，采用生成器-判别器对抗机制，能够可靠地将大型C代码库（平均485行）转换为内存安全的Rust代码，测试通过率超过90%。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust翻译方法无法处理大型代码库（>500行），因为依赖复杂的程序分析容易失败，需要一种更可靠的方法来防止内存安全漏洞。

Method: 采用对抗性方法，生成器代理合成和优化Rust翻译以通过测试，判别器代理寻找新的失败测试，两者协作迭代改进翻译。

Result: 成功翻译了63个真实世界命令行工具，平均代码量485行，测试通过率超过90%，比非对抗性方法提升18.9%的正确性。

Conclusion: ACToR是首个能够可靠翻译这种规模C程序的系统，实现了零人工干预的高质量翻译。

Abstract: Translating C to memory-safe languages, like Rust, prevents critical memory
safety vulnerabilities that are prevalent in legacy C software. Existing
approaches for C to safe Rust translation, including LLM-assisted ones, do not
generalize on larger (> 500 LoC) C codebases because they depend on complex
program analyses that frequently break. In this work, we present ACToR
(Adversarial C To Rust translator), a simple LLM agent-based approach. Inspired
by GANs, ACToR pits a generator agent against a discriminator agent, which
collaborate to iteratively generate a Rust translation. On each iteration, the
translator agent synthesizes and refines a Rust translation to pass an existing
suite of tests, and then the discriminator agent finds new failing tests. We
demonstrate that ACToR translates all of the 63 real-world command line
utilities considered in our benchmarks, which have an average size of 485 lines
of code, and it achieves over 90% test pass rate with zero human intervention.
To our knowledge, it is the first such system that reliably translates C
programs of this scale. Furthermore, ACToR improves translation correctness by
up to 18.9% compared to baseline, non-adversarial approaches.

</details>


### [107] [Multi-Agent Code-Orchestrated Generation for Reliable Infrastructure-as-Code](https://arxiv.org/abs/2510.03902)
*Rana Nameer Hussain Khan,Dawood Wasif,Jin-Hee Cho,Ali Butt*

Main category: cs.SE

TL;DR: MACOG是一个基于多智能体LLM的IaC生成架构，通过分解任务到专业智能体，显著提升了基础设施代码的质量和合规性。


<details>
  <summary>Details</summary>
Motivation: 解决传统LLM在生成基础设施即代码(IaC)时出现的语法错误、策略违规和不可扩展设计等问题。

Method: 采用多智能体架构，包含架构师、提供商协调器、工程师、审核员、安全证明者、成本容量规划师、DevOps和记忆策展人等专业智能体，通过共享黑板和有限状态协调器进行交互。

Result: 在IaC-Eval基准测试中表现优异，GPT-5从54.90提升到74.02，Gemini-2.5 Pro从43.56提升到60.13，在BLEU、CodeBERTScore和LLM-judge指标上均有提升。

Conclusion: 多智能体分解方法和约束解码、部署反馈机制对提升IaC生成质量至关重要。

Abstract: The increasing complexity of cloud-native infrastructure has made
Infrastructure-as-Code (IaC) essential for reproducible and scalable
deployments. While large language models (LLMs) have shown promise in
generating IaC snippets from natural language prompts, their monolithic,
single-pass generation approach often results in syntactic errors, policy
violations, and unscalable designs. In this paper, we propose MACOG
(Multi-Agent Code-Orchestrated Generation), a novel multi-agent LLM-based
architecture for IaC generation that decomposes the task into modular subtasks
handled by specialized agents: Architect, Provider Harmonizer, Engineer,
Reviewer, Security Prover, Cost and Capacity Planner, DevOps, and Memory
Curator. The agents interact via a shared-blackboard, finite-state orchestrator
layer, and collectively produce Terraform configurations that are not only
syntactically valid but also policy-compliant and semantically coherent. To
ensure infrastructure correctness and governance, we incorporate Terraform Plan
for execution validation and Open Policy Agent (OPA) for customizable policy
enforcement. We evaluate MACOG using the IaC-Eval benchmark, where MACOG is the
top enhancement across models, e.g., GPT-5 improves from 54.90 (RAG) to 74.02
and Gemini-2.5 Pro from 43.56 to 60.13, with concurrent gains on BLEU,
CodeBERTScore, and an LLM-judge metric. Ablations show constrained decoding and
deploy feedback are critical: removing them drops IaC-Eval to 64.89 and 56.93,
respectively.

</details>


### [108] [Refactoring with LLMs: Bridging Human Expertise and Machine Understanding](https://arxiv.org/abs/2510.03914)
*Yonnel Chen Kuang Piao,Jean Carlors Paul,Leuson Da Silva,Arghavan Moradi Dakhel,Mohammad Hamdaqa,Foutse Khomh*

Main category: cs.SE

TL;DR: 该研究探索基于人类最佳实践指南的指令策略能否增强大语言模型执行多样化代码重构任务的能力，结果显示基于Fowler重构指南的指令设计使LLMs能够成功执行所有基准重构类型并在真实场景中保持程序语义。


<details>
  <summary>Details</summary>
Motivation: 开发者因重构需要大量时间、精力和资源而常常忽视这一重要实践，现有自动化重构工具在支持广泛重构类型方面仍有局限，因此研究如何利用LLMs的指令遵循和代码理解能力来改进自动化重构。

Method: 利用GPT-mini和DeepSeek-V3等先进LLMs，基于Martin Fowler的重构指南设计多种指令策略，编码61种知名重构类型的动机、程序步骤和转换目标，并在基准示例和GitHub真实代码片段上进行评估。

Result: 基于Fowler指南的指令设计使LLMs能够成功执行所有基准重构类型并在真实环境中保持程序语义，这是有效重构的关键标准。描述性指令对人类更易解释，但规则型指令在特定场景中表现更好。

Conclusion: 允许模型专注于重构的整体目标而非规定固定转换类型，可以带来更大的代码质量改进。基于人类最佳实践的指令策略能有效增强LLMs的自动化重构能力。

Abstract: Code refactoring is a fundamental software engineering practice aimed at
improving code quality and maintainability. Despite its importance, developers
often neglect refactoring due to the significant time, effort, and resources it
requires, as well as the lack of immediate functional rewards. Although several
automated refactoring tools have been proposed, they remain limited in
supporting a broad spectrum of refactoring types. In this study, we explore
whether instruction strategies inspired by human best-practice guidelines can
enhance the ability of Large Language Models (LLMs) to perform diverse
refactoring tasks automatically. Leveraging the instruction-following and code
comprehension capabilities of state-of-the-art LLMs (e.g., GPT-mini and
DeepSeek-V3), we draw on Martin Fowler's refactoring guidelines to design
multiple instruction strategies that encode motivations, procedural steps, and
transformation objectives for 61 well-known refactoring types. We evaluate
these strategies on benchmark examples and real-world code snippets from GitHub
projects. Our results show that instruction designs grounded in Fowler's
guidelines enable LLMs to successfully perform all benchmark refactoring types
and preserve program semantics in real-world settings, an essential criterion
for effective refactoring. Moreover, while descriptive instructions are more
interpretable to humans, our results show that rule-based instructions often
lead to better performance in specific scenarios. Interestingly, allowing
models to focus on the overall goal of refactoring, rather than prescribing a
fixed transformation type, can yield even greater improvements in code quality.

</details>


### [109] [GA4GC: Greener Agent for Greener Code via Multi-Objective Configuration Optimization](https://arxiv.org/abs/2510.04135)
*Jingzhi Gong,Yixin Bian,Luis de la Cal,Giovanni Pinna,Anisha Uteem,David Williams,Mar Zamorano,Karine Even-Mendoza,W. B. Langdon,Hector Menendez,Federica Sarro*

Main category: cs.SE

TL;DR: GA4GC框架通过发现帕累托最优代理超参数和提示模板，系统优化编码代理运行时间与代码性能的权衡，在SWE-Perf基准测试中实现高达135倍超体积改进，减少37.7%代理运行时间同时提高正确性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的编码代理在工业部署中面临可持续性和可扩展性挑战，单次运行消耗超过10万token，环境成本可能超过优化收益。

Method: 引入GA4GC框架，系统优化编码代理运行时间（更环保的代理）和代码性能（更环保的代码）的权衡，通过发现帕累托最优代理超参数和提示模板。

Result: 在SWE-Perf基准测试中实现高达135倍超体积改进，减少37.7%代理运行时间同时提高正确性。温度被确定为最关键的超参数。

Conclusion: 为工业部署中平衡代理可持续性与代码优化效果提供了可行的策略。

Abstract: Coding agents powered by LLMs face critical sustainability and scalability
challenges in industrial deployment, with single runs consuming over 100k
tokens and incurring environmental costs that may exceed optimization benefits.
This paper introduces GA4GC, the first framework to systematically optimize
coding agent runtime (greener agent) and code performance (greener code)
trade-offs by discovering Pareto-optimal agent hyperparameters and prompt
templates. Evaluation on the SWE-Perf benchmark demonstrates up to 135x
hypervolume improvement, reducing agent runtime by 37.7% while improving
correctness. Our findings establish temperature as the most critical
hyperparameter, and provide actionable strategies to balance agent
sustainability with code optimization effectiveness in industrial deployment.

</details>


### [110] [Challenge on Optimization of Context Collection for Code Completion](https://arxiv.org/abs/2510.04349)
*Dmitry Ustalov,Egor Bogomolov,Alexander Bezzubov,Yaroslav Golubev,Evgeniy Glukhov,Georgii Levtsov,Vladimir Kovalenko*

Main category: cs.SE

TL;DR: 该论文介绍了JetBrains与Mistral AI在ASE 2025会议上组织的代码完成上下文收集优化挑战赛，旨在系统评估AI软件工程方法在大型代码库中利用项目信息的能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI软件工程工作流和方法的快速发展，需要系统评估它们在大型代码库中利用整个项目信息的能力，特别是在代码完成任务中。

Method: 构建了基于Python和Kotlin的许可开源项目的大型数据集，组织竞赛让参与者开发高效的上下文收集机制来改进填充中间代码完成，使用chrF指标评估提交方案的质量。

Result: 在公开阶段，19个团队提交了Python赛道解决方案，8个团队提交了Kotlin赛道解决方案；在私有阶段，6个团队竞争，其中5个团队向研讨会提交了论文。

Conclusion: 该挑战赛成功促进了代码完成上下文收集优化方法的发展，为评估AI软件工程在大型代码库中的能力提供了系统框架。

Abstract: The rapid advancement of workflows and methods for software engineering using
AI emphasizes the need for a systematic evaluation and analysis of their
ability to leverage information from entire projects, particularly in large
code bases. In this challenge on optimization of context collection for code
completion, organized by JetBrains in collaboration with Mistral AI as part of
the ASE 2025 conference, participants developed efficient mechanisms for
collecting context from source code repositories to improve fill-in-the-middle
code completions for Python and Kotlin. We constructed a large dataset of
real-world code in these two programming languages using permissively licensed
open-source projects. The submissions were evaluated based on their ability to
maximize completion quality for multiple state-of-the-art neural models using
the chrF metric. During the public phase of the competition, nineteen teams
submitted solutions to the Python track and eight teams submitted solutions to
the Kotlin track. In the private phase, six teams competed, of which five
submitted papers to the workshop.

</details>


### [111] [MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](https://arxiv.org/abs/2510.04363)
*Hyunjun Kim,Sejong Kim*

Main category: cs.SE

TL;DR: MacroBench是一个代码优先的基准测试，用于评估LLM能否从自然语言目标合成可重用的浏览器自动化程序，涵盖7个自托管网站和681个任务，通过静态检查、沙箱执行和结果验证来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注代码补全或单行代码生成，缺乏对从自然语言目标合成完整、可重用浏览器自动化程序能力的评估。

Method: 构建包含7个自托管网站的基准测试，涵盖681个任务，通过静态检查、沙箱执行、DOM断言和数据库快照进行端到端验证，并包含安全套件。

Result: GPT-4o-Mini达到96.8%成功率，GPT-4.1达到95.3%，Gemini-2.5-Pro达到89.0%，DeepSeek-V3.1达到83.4%。模型在简单任务上可靠(91.7%)，但在复杂工作流上完全失败(0.0%)。

Conclusion: 虽然模型在功能完成度上表现良好，但没有一个模型达到生产质量的编码实践标准，复杂工作流仍然是主要挑战。

Abstract: We introduce MacroBench, a code-first benchmark that evaluates whether LLMs
can synthesize reusable browser automation programs from natural language goals
by reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates
seven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,
Facebook-like, Discord-like, and Threads-like, covering 681 tasks across
interaction complexity and targeting difficulty. Our end-to-end protocol
validates generated code via static checks, sandboxed execution, and outcome
verification including DOM assertions and database snapshots, and includes a
safety suite for scraping, spam/abuse, and credential/privacy prompts. Across
2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8
percent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,
and DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at
91.7 percent but fail on complex workflows at 0.0 percent, and none meet
production-quality coding practices despite functional completion. We release
our complete benchmark pipeline, evaluation framework, and experimental results
to enable reproducible assessment of macro synthesis for web automation.

</details>


### [112] [Improving IR-based Bug Localization with Semantics-Driven Query Reduction](https://arxiv.org/abs/2510.04468)
*Asif Mohammed Samir,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: IQLoc是一种结合信息检索和大型语言模型的软件缺陷定位方法，通过利用Transformer模型理解程序语义来改进缺陷定位效果


<details>
  <summary>Details</summary>
Motivation: 传统基于信息检索的缺陷定位方法忽略了源代码的上下文和语义，而大型语言模型虽然能理解文本和代码，但尚未很好地应用于缺陷定位且资源消耗大

Method: 结合IR和LLM的优势，利用基于Transformer的模型理解程序语义，在缺陷定位过程中对代码可疑性进行推理并重新制定查询

Result: 在Bench4BL基准数据集上评估，IQLoc在MAP、MRR和HIT@K指标上显著优于四种基线技术，特别是对包含堆栈跟踪、代码元素或纯自然语言描述的缺陷报告有显著改进

Conclusion: 通过将程序语义理解集成到信息检索中，IQLoc缓解了传统基于IR的缺陷定位方法的长期挑战

Abstract: Despite decades of research, software bug localization remains challenging
due to heterogeneous content and inherent ambiguities in bug reports. Existing
methods such as Information Retrieval (IR)-based approaches often attempt to
match source documents to bug reports, overlooking the context and semantics of
the source code. On the other hand, Large Language Models (LLM) (e.g.,
Transformer models) show promising results in understanding both texts and
code. However, they have not been yet adapted well to localize software bugs
against bug reports. They could be also data or resource-intensive. To bridge
this gap, we propose, IQLoc, a novel bug localization approach that capitalizes
on the strengths of both IR and LLM-based approaches. In particular, we
leverage the program semantics understanding of transformer-based models to
reason about the suspiciousness of code and reformulate queries during bug
localization using Information Retrieval. To evaluate IQLoc, we refine the
Bench4BL benchmark dataset and extend it by incorporating ~30% more recent bug
reports, resulting in a benchmark containing ~7.5K bug reports. We evaluated
IQLoc using three performance metrics and compare it against four baseline
techniques. Experimental results demonstrate its superiority, achieving up to
58.52% and 60.59% in MAP, 61.49% and 64.58% in MRR, and 69.88% and 100.90% in
HIT@K for the test bug reports with random and time-wise splits, respectively.
Moreover, IQLoc improves MAP by 91.67% for bug reports with stack traces,
72.73% for those that include code elements, and 65.38% for those containing
only descriptions in natural language. By integrating program semantic
understanding into Information Retrieval, IQLoc mitigates several longstanding
challenges of traditional IR-based approaches in bug localization.

</details>


### [113] [Exploring the Power of Diffusion Large Language Models for Software Engineering: An Empirical Investigation](https://arxiv.org/abs/2510.04605)
*Jingyao Zhang,Tianlin Li,Xiaoyu Zhang,Qiang Hu,Bin Shi*

Main category: cs.SE

TL;DR: 本文首次全面评估扩散大语言模型在软件开发全周期中的表现，包括代码生成、缺陷检测和程序修复。在52,937个任务的大规模基准测试中，7B参数的DLLMs在准确率上比AR-LLMs平均提升30%，跨文件修复任务提升113%，同时保持更高的效率和更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型在软件工程中应用广泛，但在处理代码结构信息和推理延迟方面存在局限。扩散大语言模型提供了有前景的替代方案，具有全局双向编码和解耦生成步骤的优势。

Method: 在软件开发全周期（代码生成、缺陷检测、程序修复）中对扩散大语言模型进行大规模评估，使用包含52,937个任务的基准测试，比较7B参数DLLMs与AR-LLMs的性能。

Result: 7B参数的DLLMs在准确率上比AR-LLMs平均提升30%，在跨文件修复任务中提升113%，同时保持更高的效率和更低的延迟。

Conclusion: 扩散大语言模型是软件工程任务的优越范式，在准确性和效率方面均优于自回归大语言模型。

Abstract: Autoregressive Large Language Models (AR-LLMs) are widely used in software
engineering (SE) but face limitations in processing code structure information
and suffer from high inference latency. Diffusion LLMs (DLLMs) offer a
promising alternative with global bidirectional encoding and decoupled
generation steps. This work presents the first comprehensive evaluation of
DLLMs across the software development lifecycle, including code generation,
defect detection, and program repair. On a large-scale benchmark of 52,937
tasks, 7Bparameter DLLMs outperform AR-LLMs with a 30% average accuracy
improvement achieving a 113% gain on cross-file repair, while maintaining
superior efficiency and reduced latency. Our results establish DLLMs as a
superior paradigm for SE tasks.

</details>


### [114] [GUISpector: An MLLM Agent Framework for Automated Verification of Natural Language Requirements in GUI Prototypes](https://arxiv.org/abs/2510.04791)
*Kristian Kolthoff,Felix Kretzer,Simone Paolo Ponzetto,Alexander Maedche,Christian Bartelt*

Main category: cs.SE

TL;DR: GUISpector是一个基于多模态大语言模型的GUI原型验证框架，能够自动验证自然语言需求，提供可操作的反馈，并集成到LLM驱动的开发工作流中。


<details>
  <summary>Details</summary>
Motivation: 现有GUI测试方法在处理现代界面复杂性方面存在不足，缺乏可操作的反馈和与自动化开发代理的有效集成。

Method: 使用多模态LLM代理解释和操作化自然语言需求，自主规划和执行GUI应用的验证轨迹，系统提取详细反馈。

Result: 在150个需求和900个验收标准上评估，有效检测需求满足和违反情况，展示了与自动化LLM驱动开发工作流的无缝集成潜力。

Conclusion: GUISpector为GUI原型验证提供了有效的自动化解决方案，能够生成可操作的反馈并集成到开发工作流中。

Abstract: GUIs are foundational to interactive systems and play a pivotal role in early
requirements elicitation through prototyping. Ensuring that GUI implementations
fulfill NL requirements is essential for robust software engineering,
especially as LLM-driven programming agents become increasingly integrated into
development workflows. Existing GUI testing approaches, whether traditional or
LLM-driven, often fall short in handling the complexity of modern interfaces,
and typically lack actionable feedback and effective integration with automated
development agents. In this paper, we introduce GUISpector, a novel framework
that leverages a multi-modal (M)LLM-based agent for the automated verification
of NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to
interpret and operationalize NL requirements, enabling to autonomously plan and
execute verification trajectories across GUI applications. Second, GUISpector
systematically extracts detailed NL feedback from the agent's verification
process, providing developers with actionable insights that can be used to
iteratively refine the GUI artifact or directly inform LLM-based code
generation in a closed feedback loop. Third, we present an integrated tool that
unifies these capabilities, offering practitioners an accessible interface for
supervising verification runs, inspecting agent rationales and managing the
end-to-end requirements verification process. We evaluated GUISpector on a
comprehensive set of 150 requirements based on 900 acceptance criteria
annotations across diverse GUI applications, demonstrating effective detection
of requirement satisfaction and violations and highlighting its potential for
seamless integration of actionable feedback into automated LLM-driven
development workflows. The video presentation of GUISpector is available at:
https://youtu.be/JByYF6BNQeE, showcasing its main capabilities.

</details>


### [115] [RevMine: An LLM-Assisted Tool for Code Review Mining and Analysis Across Git Platforms](https://arxiv.org/abs/2510.04796)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: RevMine是一个基于大语言模型的代码审查挖掘工具，旨在简化从GitHub等平台提取和分析代码审查数据的过程。


<details>
  <summary>Details</summary>
Motivation: 当前代码审查研究需要编写大量临时脚本来提取数据，过程耗时且技术门槛高，限制了实证研究的开展。

Method: 使用大语言模型构建端到端代码审查挖掘流水线，支持身份验证、端点发现、自然语言驱动的数据收集，以及定量和定性分析。

Result: RevMine显著减少了手动编写脚本的需求，降低了代码审查挖掘的技术门槛。

Conclusion: 该工具有望民主化代码审查挖掘，使更广泛的实证软件工程研究成为可能。

Abstract: Empirical research on code review processes is increasingly central to
understanding software quality and collaboration. However, collecting and
analyzing review data remains a time-consuming and technically intensive task.
Most researchers follow similar workflows - writing ad hoc scripts to extract,
filter, and analyze review data from platforms like GitHub and GitLab. This
paper introduces RevMine, a conceptual tool that streamlines the entire code
review mining pipeline using large language models (LLMs). RevMine guides users
through authentication, endpoint discovery, and natural language-driven data
collection, significantly reducing the need for manual scripting. After
retrieving review data, it supports both quantitative and qualitative analysis
based on user-defined filters or LLM-inferred patterns. This poster outlines
the tool's architecture, use cases, and research potential. By lowering the
barrier to entry, RevMine aims to democratize code review mining and enable a
broader range of empirical software engineering studies.

</details>


### [116] [FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration](https://arxiv.org/abs/2510.04852)
*Victor May,Diganta Misra,Yanqi Luo,Anjali Sridhar,Justine Gehring,Silvio Soares Ribeiro Junior*

Main category: cs.SE

TL;DR: FreshBrew是一个用于评估AI代理在项目级Java迁移任务中的基准测试，重点关注语义保持和避免奖励黑客行为，包含228个代码库的评估。


<details>
  <summary>Details</summary>
Motivation: 传统代码迁移依赖基于规则的系统，而基于LLM的AI代理框架提供了有前景的替代方案，但其有效性尚未得到系统评估。

Method: 引入FreshBrew基准测试，专门针对Java项目迁移任务，要求高测试覆盖率以确保评估的严谨性，并比较多种最先进LLM与基于规则工具的性能。

Result: 最佳模型Gemini 2.5 Flash能够成功将52.3%的项目迁移到JDK 17，揭示了当前AI代理方法的关键优势和局限性。

Conclusion: 研究揭示了当前AI代理在现实Java现代化任务中的失败模式，为评估可信赖代码迁移系统奠定了基础，FreshBrew的发布旨在促进严谨、可复现的评估。

Abstract: AI coding assistants are rapidly becoming integral to modern software
development. A key challenge in this space is the continual need to migrate and
modernize codebases in response to evolving software ecosystems. Traditionally,
such migrations have relied on rule-based systems and human intervention. With
the advent of powerful large language models (LLMs), AI-driven agentic
frameworks offer a promising alternative-but their effectiveness has not been
systematically evaluated. In this paper, we introduce FreshBrew, a novel
benchmark for evaluating AI agents on project-level Java migrations, with a
specific focus on measuring an agent's ability to preserve program semantics
and avoid reward hacking, which we argue requires projects with high test
coverage for a rigorous and reliable evaluation. We benchmark several
state-of-the-art LLMs, and compare their performance against established
rule-based tools. Our evaluation of AI agents on this benchmark of 228
repositories shows that the top-performing model, Gemini 2.5 Flash, can
successfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis
reveals novel insights into the critical strengths and limitations of current
agentic approaches, offering actionable insights into their real-world
applicability. Our empirical study reveals failure modes of current AI agents
in realistic Java modernization tasks, providing a foundation for evaluating
trustworthy code-migration systems. By releasing FreshBrew, we aim to
facilitate rigorous, reproducible evaluation and catalyze progress in AI-driven
codebase modernization.

</details>


### [117] [Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches](https://arxiv.org/abs/2510.04905)
*Yicheng Tao,Yao Qin,Yepang Liu*

Main category: cs.SE

TL;DR: 这篇论文对检索增强代码生成(RACG)进行了全面综述，特别关注仓库级代码生成(RLCG)方法，涵盖了生成策略、检索模态、模型架构、训练范式和评估协议等多个维度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成方面的进步，现实软件开发需要跨整个仓库进行推理，这带来了仓库级代码生成的挑战，需要捕获长距离依赖、确保全局语义一致性并生成跨多个文件的连贯代码。

Method: 采用检索增强生成(RAG)范式，将外部检索机制与大语言模型结合，增强上下文感知和可扩展性。对现有研究进行分类分析，包括生成策略、检索模态、模型架构等维度。

Result: 建立了统一的分析框架来理解这一快速发展的领域，总结了广泛使用的数据集和基准，分析了当前局限性。

Conclusion: 检索增强代码生成是解决仓库级代码生成挑战的有力方法，论文为AI驱动的软件工程领域的持续进步提供了启发。

Abstract: Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [118] [Your AI code reviewer doesn't know what actually breaks. Sentry's does.](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsentry.io%2Fproduct%2Fai-code-review%2F%3Futm_source=tldr%26utm_medium=paid-community%26utm_campaign=aicodereview-fy26q3-aicodereviewlaunch%26utm_content=newsletter-ai-code-review-beta-learnmore/2/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/G5OhtvVqCNPrY8jwhdOPX7vzUAQh0K1qQPMPOt7jrmI=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Sentry的AI代码审查工具利用生产环境监控数据和代码历史，能够识别真正会导致生产问题的代码变更，而不仅仅是语法和风格问题。


<details>
  <summary>Details</summary>
Motivation: 传统的AI代码审查工具只能发现语法错误和风格问题，但无法识别真正会导致生产环境崩溃的严重bug。Sentry希望通过结合生产环境监控数据来解决这个问题。

Method: Sentry利用其现有的生产错误和性能监控数据，结合代码库和提交历史，对PR进行智能审查，识别可能引发生产问题的代码变更。

Result: 开发了基于上下文的AI代码审查工具（目前处于测试阶段），能够准确识别会导致生产环境问题的代码变更。

Conclusion: 结合生产环境监控数据的上下文信息，AI代码审查工具能够更有效地识别真正危险的代码变更，而不仅仅是表面问题。

Abstract: Your AI code reviewer doesn't know what actually breaks. Sentry's does. (Sponsor) Generic AI code review catches typos, style issues, and things that might be a problem. Sentry's AI code review (now in beta) catches the bugs that actually take down production. The difference? Context. Sentry already monitors your production errors and performance. Now it uses that data + your code and commit history to review your PRs - so it knows exactly which changes will cause issues based on what's broke...

</details>


### [119] [Animals vs Ghosts](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarpathy.bearblog.dev%2Fanimals-vs-ghosts%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/vx7Yh345XXXqrR3_y30d-wUwJIPZInpFS7q9FS2ScFY=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文区分了两种AI智能类型：动物型智能和幽灵型智能，认为当前前沿LLM研究主要是在创造幽灵型智能，这是一种由人类精心设计的不同智能类型。


<details>
  <summary>Details</summary>
Motivation: 探讨当前大型语言模型研究的本质，区分不同类型的智能体，理解AI发展的可能路径和最终形态。

Method: 通过概念分析和哲学思辨，将AI智能分为动物型（自然演化式）和幽灵型（人工设计式）两类，并分析其特征差异。

Result: 识别出现有LLM研究主要创造的是幽灵型智能，这种智能与自然演化的动物型智能在本质上存在差异，可能沿着不同的进化路径发展。

Conclusion: 幽灵型智能虽然与动物型智能不同，但可能仍然极其有用，AI发展可能不会收敛到单一模式，而是出现多样化的智能形态。

Abstract: Animals vs Ghosts (11 minute read) Today's frontier LLM research isn't about building animals, it's about summoning ghosts. Ghosts are a fundamentally different kind of point in the space of possible intelligences, thoroughly engineered by humanity. Over time, we may be able to fine-tune other ghosts more and more in the direction of animals, but it's also quite possible that they diverge even further and end up permanently different. They may end up un-animal-like, but still incredibly helpf...

</details>


### [120] [Meet Jules Tools: A Command Line Companion for Google's Async Coding Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fmeet-jules-tools-a-command-line-companion-for-googles-async-coding-agent%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/jDA-yHdYjABt-C1lSmjkR7-pjnp4PjFgRxdCFVKHPws=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Jules Tools是一个轻量级命令行界面，用于与Google的异步编码代理Jules交互，支持启动任务、监控操作和自定义功能。


<details>
  <summary>Details</summary>
Motivation: 为开发者提供与异步编码代理Jules交互的便捷工具，使其能够更好地理解和控制Jules在项目中的操作。

Method: 开发轻量级命令行界面，集成到现有代码仓库中，支持任务启动、操作监控和自定义功能。

Result: 成功创建了Jules Tools，使开发者能够轻松管理Jules代理的异步编码任务。

Conclusion: Jules Tools为异步编码代理提供了实用的交互界面，提升了开发效率。

Abstract: Meet Jules Tools: A Command Line Companion for Google's Async Coding Agent (4 minute read) Jules is an asynchronous coding agent that integrates directly with existing repositories. It understands the full context of projects and can perform tasks like writing tests, building new features, providing audio changelogs, fixing bugs, and bumping dependency versions. Jules Tools is a lightweight command-line interface that allows developers to spin up tasks, inspect what Jules is doing, and custom...

</details>


### [121] [AI as a research partner: Advancing theoretical computer science with AlphaEvolve](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Fai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve%2F%3Futm_source=tldrai/1/01000199aa4e75e4-b60c2243-bfdc-4a15-bca8-0f96c50e426b-000000/QlkbBaV-MnY_fKHxRQtxKfOz2hzr4gm4PIrhjM6RS98=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: DeepMind的AlphaEvolve使用LLMs迭代进化代码，在复杂性理论中发现数学结构并证明新定理，在成熟问题上取得突破。


<details>
  <summary>Details</summary>
Motivation: 利用AI作为研究伙伴，推动理论计算机科学的发展，特别是在复杂性理论等成熟领域寻找新的突破。

Method: 使用大型语言模型迭代进化代码，通过变形有限证明结构同时保持与更广泛证明的接口完整，实现自动验证正确性而无需人工审查。

Result: 系统在两个成熟问题上取得突破性进展，发现了新的数学结构并证明了新定理。

Conclusion: AlphaEvolve展示了AI作为研究伙伴的潜力，能够在理论计算机科学中实现自动化定理证明和发现。

Abstract: AI as a research partner: Advancing theoretical computer science with AlphaEvolve (7 minute read) DeepMind's AlphaEvolve uses LLMs to iteratively “evolve” code that discovers mathematical structures proving new theorems in complexity theory. The system made breakthroughs on two mature problems by morphing finite proof structures while keeping the interface to the broader proof intact, allowing automated verification of correctness without human review.

</details>


### [122] [Introducing Microsoft Agent Framework: The Open-Source Engine for Agentic AI Apps](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevblogs.microsoft.com%2Ffoundry%2Fintroducing-microsoft-agent-framework-the-open-source-engine-for-agentic-ai-apps%2F%3Futm_source=tldrdata/1/01000199b8fcdeb7-e1412a63-2b62-4515-a713-797c1024e0e1-000000/_fhb_PoOj4L7VrASjAESXXG8ew5-gPrEP336imbn89M=425)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 微软发布了开源的Agent Framework预览版，统一了Semantic Kernel和AutoGen的能力，简化了Python和.NET的AI代理开发，支持多种编排模式。


<details>
  <summary>Details</summary>
Motivation: 为了简化AI代理开发流程，统一现有工具能力，为开发人员提供更高效、生产级的代理构建解决方案。

Method: 整合Semantic Kernel和AutoGen框架能力，提供统一的API接口，支持少于20行代码快速创建代理，包含顺序、并发、群聊和交接等多种编排模式。

Result: 开发出了支持Python和.NET的开源代理框架，具备生产级持久性，大幅降低了AI代理开发的技术门槛。

Conclusion: 微软Agent Framework为AI代理应用开发提供了统一、高效的解决方案，有望推动代理技术的广泛应用。

Abstract: Introducing Microsoft Agent Framework: The Open-Source Engine for Agentic AI Apps (13 minute read) Microsoft has released the open-source Agent Framework in preview, unifying the capabilities of Semantic Kernel and AutoGen to streamline AI agent development for both Python and .NET. The framework enables rapid agent creation with fewer than 20 lines of code, supporting orchestration patterns such as sequential, concurrent, group chat, and handoff with production-grade durability. Integrated w...

</details>
