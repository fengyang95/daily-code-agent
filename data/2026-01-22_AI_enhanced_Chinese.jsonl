{"id": "2601.14266", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.14266", "abs": "https://arxiv.org/abs/2601.14266", "authors": ["Ruben Neyroud", "Sam Corley"], "title": "GCG Attack On A Diffusion LLM", "comment": null, "summary": "While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.", "AI": {"tldr": "\u63a2\u7d22GCG\u98ce\u683c\u5bf9\u6297\u63d0\u793a\u653b\u51fb\u5728LLaDA\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u9002\u7528\u6027\uff0c\u8bc4\u4f30\u591a\u79cd\u653b\u51fb\u53d8\u4f53\u5728AdvBench\u6709\u5bb3\u63d0\u793a\u4e0a\u7684\u6548\u679c", "motivation": "\u867d\u7136GCG\u653b\u51fb\u5728\u81ea\u56de\u5f52LLMs\u4e0a\u6709\u6548\uff0c\u4f46\u5176\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u9002\u7528\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7814\u7a76\u6269\u6563\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u653b\u51fb\u9762", "method": "\u5728LLaDA\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u65bdGCG\u98ce\u683c\u5bf9\u6297\u63d0\u793a\u653b\u51fb\uff0c\u8bc4\u4f30\u524d\u7f00\u6270\u52a8\u548c\u540e\u7f00\u5bf9\u6297\u751f\u6210\u7b49\u591a\u79cd\u653b\u51fb\u53d8\u4f53\uff0c\u4f7f\u7528AdvBench\u6570\u636e\u96c6\u7684\u6709\u5bb3\u63d0\u793a", "result": "\u63d0\u4f9b\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u548c\u653b\u51fb\u9762\u7684\u521d\u6b65\u89c1\u89e3\uff0c\u8868\u660e\u9700\u8981\u9488\u5bf9\u6b64\u8bbe\u7f6e\u5f00\u53d1\u66ff\u4ee3\u7684\u4f18\u5316\u548c\u8bc4\u4f30\u7b56\u7565", "conclusion": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5bf9GCG\u98ce\u683c\u653b\u51fb\u7684\u8106\u5f31\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u5bf9\u6297\u5206\u6790\u7b56\u7565", "topic": "agent analysis"}}
{"id": "2601.14434", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14434", "abs": "https://arxiv.org/abs/2601.14434", "authors": ["Chia-Yi Su", "Collin McMillan"], "title": "CMind: An AI Agent for Localizing C Memory Bugs", "comment": "4 pages, 2 figures. To be published in 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026)", "summary": "This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program's source code and a bug report describing the problem. The output is the tool's hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program's source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at https://youtu.be/_vVd0LRvVHI.", "AI": {"tldr": "CMind\u662f\u4e00\u4e2a\u7528\u4e8e\u5b9a\u4f4dC\u8bed\u8a00\u5185\u5b58\u9519\u8bef\u7684AI\u4ee3\u7406\uff0c\u5b83\u901a\u8fc7\u6a21\u62df\u7a0b\u5e8f\u5458\u5728\u5b9e\u8bc1\u7814\u7a76\u4e2d\u89c2\u5bdf\u5230\u7684\u8c03\u8bd5\u6b65\u9aa4\u6765\u5de5\u4f5c\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u7f16\u7801\u7684\u51b3\u7b56\u5f15\u5bfc\u3002", "motivation": "\u73b0\u6709\u7684C\u8bed\u8a00\u5185\u5b58\u9519\u8bef\u5b9a\u4f4d\u5de5\u5177\u901a\u5e38\u4e0d\u80fd\u6a21\u62df\u4eba\u7c7b\u7a0b\u5e8f\u5458\u7684\u5b9e\u9645\u8c03\u8bd5\u8fc7\u7a0b\uff0c\u800c\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u5b9a\u4f4d\u5185\u5b58\u9519\u8bef\u65f6\u6709\u7279\u5b9a\u7684\u8ba4\u77e5\u6b65\u9aa4\u548c\u6a21\u5f0f\u3002", "method": "CMind\u63a5\u6536C\u7a0b\u5e8f\u6e90\u4ee3\u7801\u548c\u9519\u8bef\u62a5\u544a\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u8bfb\u53d6\u9519\u8bef\u62a5\u544a\u627e\u5230\u7a0b\u5e8f\u5165\u53e3\u70b9\uff0c\u7136\u540e\u5bfc\u822a\u6e90\u4ee3\u7801\u3001\u5206\u6790\u4ee3\u7801\uff0c\u5e76\u751f\u6210\u7b26\u5408\u6a21\u677f\u7684\u9519\u8bef\u4f4d\u7f6e\u5047\u8bbe\u548c\u7406\u7531\u3002\u5b83\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u7f16\u7801\u7684\u51b3\u7b56\u5f15\u5bfc\u6765\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u3002", "result": "CMind\u80fd\u591f\u751f\u6210\u5173\u4e8e\u5185\u5b58\u9519\u8bef\u539f\u56e0\u548c\u4f4d\u7f6e\u7684\u5047\u8bbe\uff0c\u5176\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u4eba\u7c7b\u7a0b\u5e8f\u5458\u8c03\u8bd5\u884c\u4e3a\u7684\u5b9e\u8bc1\u7814\u7a76\u89c2\u5bdf\u3002", "conclusion": "\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5b9a\u4f4d\u5185\u5b58\u9519\u8bef\u7684\u6b65\u9aa4\uff0cCMind\u5c55\u793a\u4e86\u5c06\u5b9e\u8bc1\u7814\u7a76\u89c2\u5bdf\u8f6c\u5316\u4e3aAI\u4ee3\u7406\u51b3\u7b56\u8fc7\u7a0b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7a0b\u5e8f\u8c03\u8bd5\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "code agent"}}
{"id": "2601.14470", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.14470", "abs": "https://arxiv.org/abs/2601.14470", "authors": ["Mohamad Salim", "Jasmine Latendresse", "SayedHassan Khatoonabadi", "Emad Shihab"], "title": "Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering", "comment": null, "summary": "LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.\n  Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.", "AI": {"tldr": "\u5206\u6790LLM\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7684token\u6d88\u8017\u6a21\u5f0f\uff0c\u53d1\u73b0\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u6d88\u8017\u6700\u591atoken\uff08\u5e73\u574759.4%\uff09\uff0c\u8f93\u5165token\u5360\u6bd4\u6700\u5927\uff08\u5e73\u574753.9%\uff09\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u534f\u4f5c\u4e2d\u7684\u6548\u7387\u95ee\u9898", "motivation": "LLM\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u8fd0\u884c\u6548\u7387\u548c\u8d44\u6e90\u6d88\u8017\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u5b9e\u9645\u91c7\u7528\u65f6\u9762\u4e34\u4e0d\u53ef\u9884\u6d4b\u7684\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\u95ee\u9898", "method": "\u4f7f\u7528ChatDev\u6846\u67b6\u548cGPT-5\u63a8\u7406\u6a21\u578b\uff0c\u5206\u679030\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u7684\u6267\u884c\u8f68\u8ff9\uff0c\u5c06\u5185\u90e8\u9636\u6bb5\u6620\u5c04\u5230\u6807\u51c6\u5f00\u53d1\u9636\u6bb5\uff08\u8bbe\u8ba1\u3001\u7f16\u7801\u3001\u4ee3\u7801\u5b8c\u6210\u3001\u4ee3\u7801\u5ba1\u67e5\u3001\u6d4b\u8bd5\u3001\u6587\u6863\uff09\uff0c\u91cf\u5316\u6bd4\u8f83\u5404\u9636\u6bb5\u7684token\u5206\u5e03\uff08\u8f93\u5165\u3001\u8f93\u51fa\u3001\u63a8\u7406\uff09", "result": "\u8fed\u4ee3\u7684\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u6d88\u8017\u4e86\u5927\u90e8\u5206token\uff08\u5e73\u574759.4%\uff09\uff0c\u8f93\u5165token\u59cb\u7ec8\u6784\u6210\u6700\u5927\u6d88\u8017\u4efd\u989d\uff08\u5e73\u574753.9%\uff09\uff0c\u8868\u660e\u4ee3\u7406\u534f\u4f5c\u5b58\u5728\u663e\u8457\u4f4e\u6548\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4e3b\u8981\u6210\u672c\u4e0d\u5728\u521d\u59cb\u4ee3\u7801\u751f\u6210\u800c\u5728\u81ea\u52a8\u5316\u7cbe\u70bc\u548c\u9a8c\u8bc1", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u8bba\u5e2e\u52a9\u4ece\u4e1a\u8005\u9884\u6d4b\u8d39\u7528\u548c\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6307\u5bfc\u672a\u6765\u7814\u7a76\u5f00\u53d1\u66f4token\u9ad8\u6548\u7684\u4ee3\u7406\u534f\u4f5c\u534f\u8bae", "topic": "agent analysis"}}
{"id": "2601.14270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14270", "abs": "https://arxiv.org/abs/2601.14270", "authors": ["Liangming Pan", "Jason Liang", "Jiaran Ye", "Minglai Yang", "Xinyuan Lu", "Fengbin Zhu"], "title": "Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models", "comment": "Technical Report", "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86LLM\u591a\u6b65\u63a8\u7406\u7684\u5185\u90e8\u673a\u5236\uff0c\u800c\u975e\u5de5\u7a0b\u6027\u80fd\u63d0\u5347\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u5305\u542b\u4e03\u4e2a\u7814\u7a76\u95ee\u9898\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u4e94\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u89e3\u51b3\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5de5\u7a0b\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u7f3a\u4e4f\u5bf9\u5185\u90e8\u63a8\u7406\u673a\u5236\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u56f4\u7ed5\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u7814\u7a76\u95ee\u9898\u6784\u5efa\u6982\u5ff5\u6846\u67b6\uff0c\u6db5\u76d6\u4eceLLM\u5982\u4f55\u5728\u9690\u85cf\u6fc0\u6d3b\u4e2d\u6267\u884c\u9690\u5f0f\u591a\u8df3\u63a8\u7406\uff0c\u5230\u8bed\u8a00\u5316\u663e\u5f0f\u63a8\u7406\u5982\u4f55\u91cd\u5851\u5185\u90e8\u8ba1\u7b97\u7b49\u6838\u5fc3\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u673a\u5236\u5206\u6790\u6846\u67b6\uff0c\u5c06LLM\u591a\u6b65\u63a8\u7406\u7684\u5185\u90e8\u8fd0\u4f5c\u673a\u5236\u7ec4\u7ec7\u6210\u4e03\u4e2a\u5173\u952e\u7814\u7a76\u7ef4\u5ea6\uff0c\u4e3a\u7406\u89e3LLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u3002", "conclusion": "\u9700\u8981\u4ece\u673a\u5236\u89d2\u5ea6\u6df1\u5165\u7406\u89e3LLM\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e94\u4e2a\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u540e\u7eed\u7684\u673a\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "topic": "agent analysis"}}
{"id": "2601.14440", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14440", "abs": "https://arxiv.org/abs/2601.14440", "authors": ["Saeed Khaki", "Ashudeep Singh", "Nima Safaei", "Kamal Ginotra"], "title": "VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration", "comment": null, "summary": "Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86VisTIRA\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6846\u67b6\u548cLaTeX\u8f6c\u6362\u7ba1\u9053\uff0c\u53d1\u73b0\u5de5\u5177\u76d1\u7763\u548cOCR\u57fa\u7840\u80fd\u63d0\u5347\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u6001\u5dee\u8ddd\u4e0e\u6a21\u578b\u89c4\u6a21\u5448\u8d1f\u76f8\u5173\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5f53\u95ee\u9898\u4ee5\u56fe\u50cf\u5f62\u5f0f\u5448\u73b0\u65f6\uff0c\u6027\u80fd\u660e\u663e\u4f4e\u4e8e\u7eaf\u6587\u672c\u5f62\u5f0f\u3002\u8fd9\u79cd\u6a21\u6001\u5dee\u8ddd\u6e90\u4e8e\u6a21\u578b\u5728\u8bfb\u53d6\u5bc6\u96c6\u516c\u5f0f\u3001\u5e03\u5c40\u548c\u6df7\u5408\u7b26\u53f7-\u56fe\u8868\u4e0a\u4e0b\u6587\u65f6\u7684\u590d\u5408\u5931\u8d25\u3002\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u6765\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "1. \u63d0\u51faVisTIRA\u6846\u67b6\uff1a\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4ee3\u7406\uff0c\u5c06\u6570\u5b66\u95ee\u9898\u56fe\u50cf\u8fed\u4ee3\u5206\u89e3\u4e3a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u53ef\u6267\u884c\u7684Python\u6b65\u9aa4\uff1b2. \u6784\u5efaLaTeX\u7ba1\u9053\uff1a\u5c06\u94fe\u5f0f\u601d\u7ef4\u6570\u5b66\u8bed\u6599\u8f6c\u6362\u4e3a\u5177\u6709\u6311\u6218\u6027\u7684\u56fe\u50cf\u5bf9\u5e94\u7269\uff1b3. \u521b\u5efa\u5408\u6210\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\uff1a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u4f5c\u4e1a\u98ce\u683c\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5de5\u5177\u96c6\u6210\u76d1\u7763\u80fd\u63d0\u5347\u57fa\u4e8e\u56fe\u50cf\u7684\u63a8\u7406\u80fd\u529b\uff0cOCR\u57fa\u7840\u80fd\u8fdb\u4e00\u6b65\u7f29\u5c0f\u8f83\u5c0f\u6a21\u578b\u7684\u6a21\u6001\u5dee\u8ddd\uff08\u4f46\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u6536\u76ca\u9012\u51cf\uff09\u3002\u6a21\u6001\u5dee\u8ddd\u4e25\u91cd\u7a0b\u5ea6\u4e0e\u6a21\u578b\u89c4\u6a21\u5448\u8d1f\u76f8\u5173\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u548cOCR\u57fa\u7840\u662f\u4e92\u8865\u7b56\u7565\u3002", "conclusion": "\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u7684\u6a21\u6001\u5dee\u8ddd\u53ef\u4ee5\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u548cOCR\u57fa\u7840\u6280\u672f\u6765\u7f13\u89e3\u3002\u5de5\u5177\u96c6\u6210\u76d1\u7763\u548c\u89c6\u89c9\u57fa\u7840\u662f\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2601.14456", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14456", "abs": "https://arxiv.org/abs/2601.14456", "authors": ["Valerio Belcamino", "Nicholas Attolino", "Alessio Capitanelli", "Fulvio Mastrogiovanni"], "title": "On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL", "comment": "9 pages, 4 figures, 3 tables, 2 pages of supplementary materials. Submitted to a conference implementing a double-blind review process", "summary": "Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5fae\u8c03LLM\u5728PDDL\u89c4\u5212\u4efb\u52a1\u4e0a\u867d\u7136\u80fd\u8fbe\u5230\u9ad8\u6709\u6548\u8ba1\u5212\u7387\uff0c\u4f46\u4e3b\u8981\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u6a21\u5f0f\u800c\u975e\u53ef\u8fc1\u79fb\u7684\u89c4\u5212\u80fd\u529b\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u6027\u80fd\u51e0\u4e4e\u4e3a\u96f6\u3002", "motivation": "\u63a2\u7a76\u5fae\u8c03\u540e\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728PDDL\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7684\u9ad8\u6709\u6548\u8ba1\u5212\u7387\uff0c\u7a76\u7adf\u662f\u53cd\u6620\u4e86\u53ef\u8fc1\u79fb\u7684\u89c4\u5212\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u9886\u57df\u7279\u5b9a\u7684\u8bb0\u5fc6\u6548\u5e94\u3002", "method": "\u572810\u4e2aIPC 2023\u9886\u57df\u768440,000\u4e2a\u9886\u57df-\u95ee\u9898-\u8ba1\u5212\u5143\u7ec4\u4e0a\u5fae\u8c031.7B\u53c2\u6570LLM\uff0c\u5e76\u5f15\u5165\u4e09\u79cd\u8bca\u65ad\u5e72\u9884\uff1a\u5b9e\u4f8b\u7ea7\u7b26\u53f7\u533f\u540d\u5316\u3001\u7d27\u51d1\u8ba1\u5212\u5e8f\u5217\u5316\u3001\u4f7f\u7528VAL\u9a8c\u8bc1\u5668\u4f5c\u4e3a\u6210\u529f\u5bfc\u5411\u5f3a\u5316\u4fe1\u53f7\u7684\u9a8c\u8bc1\u5668\u5956\u52b1\u5fae\u8c03\u3002", "result": "\u6a21\u578b\u5728\u9886\u57df\u5185\u8fbe\u523082.9%\u6709\u6548\u8ba1\u5212\u7387\uff0c\u4f46\u5728\u4e24\u4e2a\u672a\u89c1\u9886\u57df\u4e0a\u4e3a0%\u3002\u7b26\u53f7\u533f\u540d\u5316\u548c\u7d27\u51d1\u5e8f\u5217\u5316\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9a8c\u8bc1\u5668\u5956\u52b1\u5fae\u8c03\u5728\u76d1\u7763\u8bad\u7ec3\u4e00\u534a\u7684\u5468\u671f\u5185\u8fbe\u5230\u6027\u80fd\u9971\u548c\uff0c\u4f46\u672a\u6539\u5584\u8de8\u9886\u57df\u6cdb\u5316\u3002", "conclusion": "\u5fae\u8c03\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u6a21\u5f0f\u800c\u975e\u53ef\u8fc1\u79fb\u7684\u89c4\u5212\u80fd\u529b\uff0c\u9886\u57df\u5185\u6027\u80fd\u572880%\u5de6\u53f3\u8fbe\u5230\u5e73\u53f0\u671f\uff0c\u800c\u8de8\u9886\u57df\u6027\u80fd\u5d29\u6e83\uff0c\u7a81\u663e\u4e86LLM\u57fa\u4e8e\u89c4\u5212\u4e2d\u5b58\u5728\u7684\u6301\u7eed\u6cdb\u5316\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2601.14598", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14598", "abs": "https://arxiv.org/abs/2601.14598", "authors": ["Yonatan Gizachew Achamyeleh", "Harsh Thomare", "Mohammad Abdullah Al Faruque"], "title": "HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation", "comment": null, "summary": "Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \\textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \\textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.\n  On HumanEval-Decompile for \\texttt{x86\\_64}, \\textsc{HELIOS} raises average object file compilability from 45.0\\% to 85.2\\% for Gemini~2.0 and from 71.4\\% to 89.6\\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \\textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \\textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.", "AI": {"tldr": "HELIOS\u6846\u67b6\u5c06LLM\u4e8c\u8fdb\u5236\u53cd\u7f16\u8bd1\u91cd\u6784\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6587\u672c\u8868\u793a\u63a7\u5236\u6d41\u548c\u51fd\u6570\u8c03\u7528\uff0c\u7ed3\u5408\u7f16\u8bd1\u5668\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u53cd\u7f16\u8bd1\u4ee3\u7801\u7684\u53ef\u7f16\u8bd1\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728\u4e8c\u8fdb\u5236\u53cd\u7f16\u8bd1\u4e2d\u4ec5\u5c06\u4ee3\u7801\u89c6\u4e3a\u7eaf\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u63a7\u5236\u6d41\u56fe\uff0c\u5bfc\u81f4\u8f93\u51fa\u8bed\u6cd5\u8106\u5f31\u4e14\u903b\u8f91\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728\u4f18\u5316\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "HELIOS\u5c06\u4e8c\u8fdb\u5236\u63a7\u5236\u6d41\u548c\u51fd\u6570\u8c03\u7528\u603b\u7ed3\u4e3a\u5c42\u6b21\u5316\u6587\u672c\u8868\u793a\uff0c\u5305\u542b\u57fa\u672c\u5757\u3001\u540e\u7ee7\u8282\u70b9\u53ca\u9ad8\u7ea7\u6a21\u5f0f\uff08\u5982\u5faa\u73af\u548c\u6761\u4ef6\u8bed\u53e5\uff09\uff0c\u5c06\u6b64\u8868\u793a\u4e0e\u539f\u59cb\u53cd\u7f16\u8bd1\u5668\u8f93\u51fa\u4e00\u8d77\u63d0\u4f9b\u7ed9\u901a\u7528LLM\uff0c\u5e76\u53ef\u9009\u62e9\u7ed3\u5408\u7f16\u8bd1\u5668\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728x86_64 HumanEval-Decompile\u4e0a\uff0cHELIOS\u5c06\u5e73\u5747\u76ee\u6807\u6587\u4ef6\u53ef\u7f16\u8bd1\u6027\u4ece45.0%\u63d0\u5347\u81f385.2%\uff08Gemini 2.0\uff09\u548c\u4ece71.4%\u63d0\u5347\u81f389.6%\uff08GPT-4.1 Mini\uff09\u3002\u4f7f\u7528\u7f16\u8bd1\u5668\u53cd\u9988\u540e\uff0c\u53ef\u7f16\u8bd1\u6027\u8d85\u8fc794%\uff0c\u529f\u80fd\u6b63\u786e\u6027\u6bd4\u7eaf\u6587\u672c\u63d0\u793a\u63d0\u5347\u9ad8\u8fbe5.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "HELIOS\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u591a\u79cd\u67b6\u6784\u4e0a\u4fdd\u6301\u9ad8\u8bed\u6cd5\u6b63\u786e\u6027\u5e76\u51cf\u5c11\u529f\u80fd\u6b63\u786e\u6027\u5dee\u5f02\uff0c\u4f7f\u5176\u6210\u4e3a\u5b89\u5168\u9006\u5411\u5de5\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b9e\u7528\u7684\u6784\u5efa\u5757\uff0c\u53ef\u751f\u6210\u53ef\u91cd\u65b0\u7f16\u8bd1\u4e14\u8bed\u4e49\u5fe0\u5b9e\u7684\u4ee3\u7801\u3002", "topic": "code agent"}}
{"id": "2601.14287", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14287", "abs": "https://arxiv.org/abs/2601.14287", "authors": ["Xiucheng Xu", "Bingbing Xu", "Xueyun Tian", "Zihe Huang", "Rongxin Chen", "Yunfan Li", "Huawei Shen"], "title": "Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents", "comment": null, "summary": "External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.", "AI": {"tldr": "CoM\u6846\u67b6\u63d0\u51fa\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6784\u5efa\u4e0e\u590d\u6742\u5229\u7528\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7Chain-of-Memory\u673a\u5236\u7ec4\u7ec7\u68c0\u7d22\u7247\u6bb5\u5f62\u6210\u8fde\u8d2f\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u5916\u90e8\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u95ee\u9898\uff1a\u590d\u6742\u7684\u8bb0\u5fc6\u6784\u5efa\u8ba1\u7b97\u6210\u672c\u9ad8\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\uff1b\u7b80\u5355\u7684\u4e0a\u4e0b\u6587\u62fc\u63a5\u65e0\u6cd5\u5f25\u5408\u68c0\u7d22\u53ec\u56de\u4e0e\u63a8\u7406\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faCoM\u6846\u67b6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u6784\u5efa\u4e0e\u590d\u6742\u5229\u7528\u76f8\u7ed3\u5408\u7684\u65b0\u8303\u5f0f\uff0c\u5f15\u5165Chain-of-Memory\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u6f14\u5316\u5c06\u68c0\u7d22\u7247\u6bb5\u7ec4\u7ec7\u6210\u8fde\u8d2f\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u622a\u65ad\u6765\u4fee\u526a\u65e0\u5173\u566a\u58f0\u3002", "result": "\u5728LongMemEval\u548cLoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoM\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u83b7\u5f977.5%-10.4%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u5f00\u9500\u5927\u5e45\u964d\u4f4e\u81f3\u590d\u6742\u8bb0\u5fc6\u67b6\u6784\u7684\u7ea62.7%\u4ee4\u724c\u6d88\u8017\u548c6.0%\u5ef6\u8fdf\u3002", "conclusion": "CoM\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6784\u5efa\u4e0e\u590d\u6742\u5229\u7528\u7684\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5916\u90e8\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u63d0\u5347LLM\u4ee3\u7406\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agent analysis"}}
{"id": "2601.14523", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14523", "abs": "https://arxiv.org/abs/2601.14523", "authors": ["Leyi Zhao", "Weijie Huang", "Yitong Guo", "Jiang Bian", "Chenghong Wang", "Xuhong Zhang"], "title": "Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree", "comment": null, "summary": "Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve", "AI": {"tldr": "PhyloEvolve\u662f\u4e00\u4e2aLLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c06GPU\u7b97\u6cd5\u4f18\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u7b97\u6cd5\u84b8\u998f\u548c\u51b3\u7b56\u53d8\u6362\u5668\u5229\u7528\u4f18\u5316\u8f68\u8ff9\u4fe1\u606f\uff0c\u4f7f\u7528\u7cfb\u7edf\u53d1\u80b2\u6811\u7ec4\u7ec7\u4f18\u5316\u5386\u53f2\uff0c\u5728\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dGPU\u79d1\u5b66\u8ba1\u7b97\u7b97\u6cd5\u4f18\u5316\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u8fed\u4ee3\u7e41\u7410\uff0c\u73b0\u6709LLM\u8f85\u52a9\u8fdb\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ed3\u679c\u9009\u62e9\u548c\u968f\u673a\u7a81\u53d8\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u4e30\u5bcc\u8f68\u8ff9\u4fe1\u606f\u3002", "method": "\u5c06GPU\u7b97\u6cd5\u4f18\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u6574\u5408\u7b97\u6cd5\u84b8\u998f\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u5f15\u5165\u7cfb\u7edf\u53d1\u80b2\u6811\u8868\u793a\u6765\u7ec4\u7ec7\u4f18\u5316\u5386\u53f2\uff0c\u7ed3\u5408\u7cbe\u82f1\u8f68\u8ff9\u6c60\u3001\u591a\u5c9b\u5e76\u884c\u63a2\u7d22\u548c\u5bb9\u5668\u5316\u6267\u884c\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u3001\u6d41\u5f62\u5b66\u4e60\u548c\u8c31\u56fe\u7b97\u6cd5\u7b49\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u548c\u8fdb\u5316\u65b9\u6cd5\uff0c\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u5185\u5b58\u6548\u7387\u548c\u6b63\u786e\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "PhyloEvolve\u901a\u8fc7\u8f68\u8ff9\u6761\u4ef6\u5316\u7684\u7ecf\u9a8c\u91cd\u7528\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u4f18\u5316GPU\u7b97\u6cd5\uff0c\u7cfb\u7edf\u53d1\u80b2\u6811\u8868\u793a\u63d0\u4f9b\u4e86\u7ec4\u7ec7\u4f18\u5316\u5386\u53f2\u7684\u6709\u6548\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5316\u7b97\u6cd5\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2601.14652", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.14652", "abs": "https://arxiv.org/abs/2601.14652", "authors": ["Zixuan Ke", "Yifei Ming", "Austin Xu", "Ryan Chin", "Xuan-Phi Nguyen", "Prathyusha Jwalapuram", "Semih Yavuz", "Caiming Xiong", "Shafiq Joty"], "title": "MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks", "comment": "Preprint; Work in Progress", "summary": "While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.", "AI": {"tldr": "\u63d0\u51faMAS-Orchestra\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f16\u6392\u5efa\u6a21\u4e3a\u51fd\u6570\u8c03\u7528\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5f15\u5165MASBENCH\u57fa\u51c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4f18\u52bf\u6761\u4ef6\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u65b9\u6cd5\u590d\u6742\u6027 - \u91c7\u7528\u987a\u5e8f\u3001\u4ee3\u7801\u7ea7\u6267\u884c\u9650\u5236\u4e86\u5168\u5c40\u7cfb\u7edf\u7ea7\u6574\u4f53\u63a8\u7406\uff0c\u96be\u4ee5\u6269\u5c55\uff1b2) \u6548\u679c\u4e0d\u786e\u5b9a\u6027 - \u90e8\u7f72\u65f6\u4e0d\u6e05\u695a\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u5426\u6709\u5b9e\u9645\u4f18\u52bf\u3002", "method": "MAS-Orchestra\u5c06\u590d\u6742\u7684\u76ee\u6807\u5bfc\u5411\u5b50\u667a\u80fd\u4f53\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u51fd\u6570\uff0c\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u6574\u4f53\u7f16\u6392\uff0c\u4e00\u6b21\u6027\u751f\u6210\u6574\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u540c\u65f6\u63d0\u51faMASBENCH\u57fa\u51c6\uff0c\u4ece\u6df1\u5ea6\u3001\u89c6\u91ce\u3001\u5e7f\u5ea6\u3001\u5e76\u884c\u6027\u548c\u9c81\u68d2\u6027\u4e94\u4e2a\u7ef4\u5ea6\u523b\u753b\u4efb\u52a1\u7279\u5f81\u3002", "result": "\u5206\u6790\u8868\u660e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4f18\u52bf\u53d6\u51b3\u4e8e\u4efb\u52a1\u7ed3\u6784\u3001\u9a8c\u8bc1\u534f\u8bae\u4ee5\u53ca\u7f16\u6392\u5668\u548c\u5b50\u667a\u80fd\u4f53\u7684\u80fd\u529b\uff0c\u800c\u975e\u666e\u904d\u9002\u7528\u3002MAS-Orchestra\u5728\u6570\u5b66\u63a8\u7406\u3001\u591a\u8df3\u95ee\u7b54\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u95ee\u7b54\u7b49\u516c\u5171\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "MAS-Orchestra\u548cMASBENCH\u5171\u540c\u4fc3\u8fdb\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u66f4\u597d\u8bad\u7ec3\u548c\u7406\u89e3\uff0c\u63a8\u52a8\u4e86\u591a\u667a\u80fd\u4f53\u667a\u80fd\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.14662", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.14662", "abs": "https://arxiv.org/abs/2601.14662", "authors": ["Shuhua Yang", "Jiahao Zhang", "Yilong Wang", "Dongwon Lee", "Suhang Wang"], "title": "Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems", "comment": null, "summary": "Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.", "AI": {"tldr": "AGEA\u653b\u51fb\u6846\u67b6\u80fd\u5728\u6709\u9650\u67e5\u8be2\u9884\u7b97\u4e0b\u6709\u6548\u7a83\u53d6GraphRAG\u7cfb\u7edf\u7684\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\uff0c\u6062\u590d\u9ad8\u8fbe90%\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u73b0\u6709GraphRAG\u7cfb\u7edf\u5bf9\u7ed3\u6784\u5316\u653b\u51fb\u7684\u9ad8\u5ea6\u8106\u5f31\u6027\u3002", "motivation": "\u867d\u7136\u5df2\u77e5GraphRAG\u7cfb\u7edf\u53ef\u80fd\u6cc4\u9732\u68c0\u7d22\u5230\u7684\u5b50\u56fe\uff0c\u4f46\u5728\u5b9e\u9645\u67e5\u8be2\u9884\u7b97\u4e0b\uff0c\u80fd\u5426\u9ad8\u6548\u91cd\u5efa\u9690\u85cf\u7684\u56fe\u7ed3\u6784\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u9884\u7b97\u53d7\u9650\u7684\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\uff0c\u653b\u51fb\u8005\u80fd\u5426\u901a\u8fc7\u81ea\u9002\u5e94\u67e5\u8be2\u7a83\u53d6\u7cfb\u7edf\u7684\u6f5c\u5728\u5b9e\u4f53-\u5173\u7cfb\u56fe\u3002", "method": "\u63d0\u51faAGEA\uff08Agentic Graph Extraction Attack\uff09\u6846\u67b6\uff0c\u91c7\u7528\u65b0\u9896\u6027\u5f15\u5bfc\u7684\u63a2\u7d22-\u5229\u7528\u7b56\u7565\uff0c\u7ed3\u5408\u5916\u90e8\u56fe\u8bb0\u5fc6\u6a21\u5757\uff0c\u4ee5\u53ca\u4e24\u9636\u6bb5\u56fe\u63d0\u53d6\u6d41\u7a0b\uff08\u8f7b\u91cf\u7ea7\u53d1\u73b0+LLM\u8fc7\u6ee4\uff09\u3002\u5728\u533b\u7597\u3001\u519c\u4e1a\u548c\u6587\u5b66\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9Microsoft-GraphRAG\u548cLightRAG\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u76f8\u540c\u67e5\u8be2\u9884\u7b97\u4e0b\uff0cAGEA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u57fa\u7ebf\uff0c\u80fd\u6062\u590d\u9ad8\u8fbe90%\u7684\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u8fd9\u8868\u660e\u73b0\u4ee3GraphRAG\u7cfb\u7edf\u5373\u4f7f\u5728\u4e25\u683c\u67e5\u8be2\u9650\u5236\u4e0b\uff0c\u4e5f\u5bf9\u7ed3\u6784\u5316\u3001\u667a\u80fd\u5316\u7684\u63d0\u53d6\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\u3002", "conclusion": "GraphRAG\u7cfb\u7edf\u9762\u4e34\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\uff0cAGEA\u653b\u51fb\u6846\u67b6\u80fd\u6709\u6548\u7a83\u53d6\u5176\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\uff0c\u9700\u8981\u52a0\u5f3a\u7cfb\u7edf\u9632\u5fa1\u63aa\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2601.14936", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14936", "abs": "https://arxiv.org/abs/2601.14936", "authors": ["Chansong You", "Hyun Deok Choi", "Jingun Hong"], "title": "LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study", "comment": null, "summary": "This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.", "AI": {"tldr": "\u4f7f\u7528LLM\u81ea\u52a8\u4fee\u590dC++\u9879\u76ee\u4e2d\u9690\u5f0f\u6570\u636e\u4e22\u5931\u8b66\u544a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7LSP\u6536\u96c6\u4e0a\u4e0b\u6587\u3001Tree-sitter\u63d0\u53d6\u4ee3\u7801\u3001LLM\u51b3\u7b56\u751f\u6210\u4fee\u590d\uff0c\u5728\u5927\u578bC++\u9879\u76ee\u4e2d\u8fbe\u523092.73%\u7684\u4ee3\u7801\u5ba1\u67e5\u63a5\u53d7\u7387\u3002", "motivation": "\u51cf\u5c11\u624b\u52a8\u4fee\u590d\u7f16\u8bd1\u5668\u8b66\u544a\u7684\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u8d28\u91cf\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u578bC++\u9879\u76ee\u4e2d\u5904\u7406\u9690\u5f0f\u6570\u636e\u4e22\u5931\u8b66\u544a\u7684\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u670d\u52a1\u5668\u534f\u8bae(LSP)\u6536\u96c6\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0cTree-sitter\u63d0\u53d6\u76f8\u5173\u4ee3\u7801\u7247\u6bb5\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u8bc4\u4f30\u8303\u56f4\u68c0\u67e5\u7684\u5fc5\u8981\u6027\u5e76\u751f\u6210\u4fee\u590d\u65b9\u6848\uff0c\u8003\u8651\u6027\u80fd\u5f71\u54cd\u3002", "result": "\u5728\u5927\u578bC++\u9879\u76ee\u4e2d\u6d4b\u8bd5\uff0c\u4fee\u590d\u65b9\u6848\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u83b7\u5f9792.73%\u7684\u63a5\u53d7\u7387\uff1b\u76f8\u6bd4\u57fa\u51c6\u4fee\u590d\u7b56\u7565\uff0c\u51cf\u5c11\u4e8639.09%\u56e0\u8303\u56f4\u68c0\u67e5\u548c\u5f02\u5e38\u5904\u7406\u5f15\u5165\u7684\u989d\u5916\u6307\u4ee4\uff1b\u6bd4\u4eba\u5de5\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u843d\u540e13.56%\u3002", "conclusion": "LLM\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u5904\u7406\u7f16\u8bd1\u5668\u8b66\u544a\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u8d28\u91cf\u548c\u6027\u80fd\uff0c\u6709\u671b\u96c6\u6210\u5230\u73b0\u6709\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\uff0c\u6539\u5584\u590d\u6742C++\u9879\u76ee\u7684\u4ee3\u7801\u7ef4\u62a4\u5b9e\u8df5\u3002", "topic": "swe application"}}
{"id": "2601.15074", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15074", "abs": "https://arxiv.org/abs/2601.15074", "authors": ["Srinath Srinivasan", "Tim Menzies", "Marcelo D'Amorim"], "title": "SmartOracle -- An Agentic Approach to Mitigate Noise in Differential Oracles", "comment": null, "summary": "Differential fuzzers detect bugs by executing identical inputs across distinct implementations of the same specification, such as JavaScript interpreters. Validating the outputs requires an oracle and for differential testing of JavaScript, these are constructed manually, making them expensive, time-consuming, and prone to false positives. Worse, when the specification evolves, this manual effort must be repeated.\n  Inspired by the success of agentic systems in other SE domains, this paper introduces SmartOracle. SmartOracle decomposes the manual triage workflow into specialized Large Language Model (LLM) sub-agents. These agents synthesize independently gathered evidence from terminal runs and targeted specification queries to reach a final verdict.\n  For historical benchmarks, SmartOracle achieves 0.84 recall with an 18% false positive rate. Compared to a sequential Gemini 2.5 Pro baseline, it improves triage accuracy while reducing analysis time by 4$\\times$ and API costs by 10$\\times$. In active fuzzing campaigns, SmartOracle successfully identified and reported previously unknown specification-level issues across major engines, including bugs in V8, JavaScriptCore, and GraalJS.\n  The success of SmartOracle's agentic architecture on Javascript suggests it might be useful other software systems- a research direction we will explore in future work.", "AI": {"tldr": "SmartOracle\u4f7f\u7528LLM\u4ee3\u7406\u67b6\u6784\u81ea\u52a8\u5316JavaScript\u5f15\u64ce\u5dee\u5206\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u7ed3\u679c\u9a8c\u8bc1\uff0c\u5c06\u624b\u52a8\u5de5\u4f5c\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e13\u95e8\u5b50\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "JavaScript\u5f15\u64ce\u5dee\u5206\u6a21\u7cca\u6d4b\u8bd5\u9700\u8981\u624b\u52a8\u6784\u5efa\u9a8c\u8bc1oracle\uff0c\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3001\u6613\u4ea7\u751f\u8bef\u62a5\uff0c\u4e14\u89c4\u8303\u66f4\u65b0\u65f6\u9700\u8981\u91cd\u590d\u5de5\u4f5c\u3002\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u66ff\u4ee3\u624b\u52a8\u9a8c\u8bc1\u6d41\u7a0b\u3002", "method": "\u5c06\u624b\u52a8\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u7a0b\u5206\u89e3\u4e3a\u4e13\u95e8\u7684LLM\u5b50\u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u4ece\u7ec8\u7aef\u8fd0\u884c\u548c\u89c4\u8303\u67e5\u8be2\u4e2d\u72ec\u7acb\u6536\u96c6\u8bc1\u636e\uff0c\u7efc\u5408\u5224\u65ad\u6700\u7ec8\u7ed3\u679c\u3002\u91c7\u7528\u4ee3\u7406\u67b6\u6784\u800c\u975e\u5355\u4e00LLM\u3002", "result": "\u5728\u5386\u53f2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.84\u53ec\u56de\u7387\u548c18%\u8bef\u62a5\u7387\uff1b\u76f8\u6bd4Gemini 2.5 Pro\u57fa\u7ebf\uff0c\u63d0\u9ad8\u9a8c\u8bc1\u51c6\u786e\u6027\uff0c\u5206\u6790\u65f6\u95f4\u51cf\u5c114\u500d\uff0cAPI\u6210\u672c\u964d\u4f4e10\u500d\uff1b\u5728\u5b9e\u9645\u6a21\u7cca\u6d4b\u8bd5\u6d3b\u52a8\u4e2d\u6210\u529f\u53d1\u73b0V8\u3001JavaScriptCore\u548cGraalJS\u4e2d\u7684\u672a\u77e5\u89c4\u8303\u7ea7\u95ee\u9898\u3002", "conclusion": "SmartOracle\u7684\u4ee3\u7406\u67b6\u6784\u5728JavaScript\u5f15\u64ce\u5dee\u5206\u6d4b\u8bd5\u4e2d\u6210\u529f\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u672a\u6765\u5c06\u63a2\u7d22\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2601.14525", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14525", "abs": "https://arxiv.org/abs/2601.14525", "authors": ["Chenglei Si", "Zitong Yang", "Yejin Choi", "Emmanuel Cand\u00e8s", "Diyi Yang", "Tatsunori Hashimoto"], "title": "Towards Execution-Grounded Automated AI Research", "comment": null, "summary": "Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u81ea\u52a8\u5316AI\u7814\u7a76\u4e2d\u7684\u6267\u884c\u843d\u5730\u95ee\u9898\uff0c\u6784\u5efa\u81ea\u52a8\u5316\u6267\u884c\u5668\u9a8c\u8bc1LLM\u751f\u6210\u7684\u60f3\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u4ece\u6267\u884c\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u53d1\u73b0\u8fdb\u5316\u641c\u7d22\u66f4\u6709\u6548\u800c\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u751f\u6210\u7684AI\u7814\u7a76\u60f3\u6cd5\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u6267\u884c\u843d\u5730\u6765\u9a8c\u8bc1\u3002\u7814\u7a76\u81ea\u52a8\u5316\u6267\u884c\u7684\u53ef\u884c\u6027\u4ee5\u53caLLM\u80fd\u5426\u4ece\u6267\u884c\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u4ee5\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "method": "1) \u6784\u5efa\u81ea\u52a8\u5316\u6267\u884c\u5668\u5b9e\u73b0\u60f3\u6cd5\u5e76\u542f\u52a8\u5927\u89c4\u6a21\u5e76\u884cGPU\u5b9e\u9a8c\u9a8c\u8bc1\u6548\u679c\uff1b2) \u5c06LLM\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u8f6c\u5316\u4e3a\u6267\u884c\u73af\u5883\uff1b3) \u5206\u6790\u4e24\u79cd\u4ece\u6267\u884c\u53cd\u9988\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a\u8fdb\u5316\u641c\u7d22\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u81ea\u52a8\u5316\u6267\u884c\u5668\u80fd\u5b9e\u73b0\u524d\u6cbfLLM\u751f\u6210\u7684\u5927\u90e8\u5206\u60f3\u6cd5\u3002\u6267\u884c\u5f15\u5bfc\u7684\u8fdb\u5316\u641c\u7d22\u6837\u672c\u6548\u7387\u9ad8\uff1a\u572810\u4e2a\u641c\u7d22\u5468\u671f\u5185\uff0c\u540e\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u4f18\u4e8eGRPO\u57fa\u7ebf(69.4% vs 48.0%)\uff0c\u9884\u8bad\u7ec3\u914d\u65b9\u4f18\u4e8enanoGPT\u57fa\u7ebf(19.7\u5206\u949f vs 35.9\u5206\u949f)\u3002\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u80fd\u63d0\u9ad8\u5e73\u5747\u5956\u52b1\u4f46\u65e0\u6cd5\u63d0\u5347\u4e0a\u9650\u3002", "conclusion": "\u6267\u884c\u843d\u5730\u7684\u81ea\u52a8\u5316AI\u7814\u7a76\u5177\u6709\u6f5c\u529b\uff0c\u8fdb\u5316\u641c\u7d22\u6bd4\u5f3a\u5316\u5b66\u4e60\u66f4\u6709\u6548\u3002\u524d\u6cbfLLM\u5728\u641c\u7d22\u4e2d\u80fd\u751f\u6210\u6709\u610f\u4e49\u7684\u7b97\u6cd5\u60f3\u6cd5\uff0c\u4f46\u5bb9\u6613\u65e9\u671f\u9971\u548c\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u5206\u6790\u6267\u884c\u60f3\u6cd5\u548c\u8bad\u7ec3\u52a8\u6001\u4ee5\u63a8\u52a8\u6267\u884c\u843d\u5730\u7684\u81ea\u52a8\u5316AI\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.14691", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14691", "abs": "https://arxiv.org/abs/2601.14691", "authors": ["Muhammad Khalifa", "Lajanugen Logeswaran", "Jaekyeom Kim", "Sungryull Sohn", "Yunxiang Zhang", "Moontae Lee", "Hao Peng", "Lu Wang", "Honglak Lee"], "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.", "AI": {"tldr": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u7684\u88c1\u5224\u5728\u4e0d\u53ef\u9a8c\u8bc1\u573a\u666f\u4e2d\u5b58\u5728\u8106\u5f31\u6027\uff1a\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u91cd\u5199\u4ee3\u7406\u7684\u601d\u7ef4\u94fe\uff08\u4fdd\u6301\u884c\u52a8\u548c\u89c2\u5bdf\u4e0d\u53d8\uff09\uff0c\u53ef\u4ee5\u64cd\u7eb5LLM\u88c1\u5224\u7684\u5224\u65ad\uff0c\u4f7f\u8bef\u5224\u7387\u63d0\u5347\u9ad8\u8fbe90%\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u88c1\u5224\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u65f6\uff0c\u9690\u542b\u5047\u8bbe\u4ee3\u7406\u7684\u601d\u7ef4\u94fe\u80fd\u5fe0\u5b9e\u53cd\u6620\u5176\u5185\u90e8\u63a8\u7406\u548c\u73af\u5883\u72b6\u6001\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u7684\u8106\u5f31\u6027\uff0c\u63ed\u793aLLM\u88c1\u5224\u6613\u53d7\u63a8\u7406\u8f68\u8ff9\u64cd\u7eb5\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u91cd\u5199\u4ee3\u7406\u7684\u601d\u7ef4\u94fe\uff08\u4fdd\u6301\u884c\u52a8\u548c\u89c2\u5bdf\u4e0d\u53d8\uff09\uff0c\u7814\u7a76\u64cd\u7eb5\u7b56\u7565\uff1a\u5305\u62ec\u4ec5\u6539\u53d8\u63a8\u7406\u5448\u73b0\u65b9\u5f0f\u7684\u98ce\u683c\u64cd\u7eb5\uff0c\u4ee5\u53ca\u4f2a\u9020\u4efb\u52a1\u8fdb\u5c55\u4fe1\u53f7\u7684\u5185\u5bb9\u64cd\u7eb5\u3002\u5728800\u4e2a\u591a\u6837\u5316\u7f51\u7edc\u4efb\u52a1\u8f68\u8ff9\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u63d0\u793a\u6280\u672f\u548c\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u5bf9\u8106\u5f31\u6027\u7684\u7f13\u89e3\u6548\u679c\u3002", "result": "1. \u64cd\u7eb5\u63a8\u7406\u8f68\u8ff9\u53ef\u4f7f\u6700\u5148\u8fdb\u7684VLM\u88c1\u5224\u8bef\u5224\u7387\u63d0\u5347\u9ad8\u8fbe90%\uff1b2. \u5185\u5bb9\u64cd\u7eb5\u6bd4\u98ce\u683c\u64cd\u7eb5\u66f4\u6709\u6548\uff1b3. \u63d0\u793a\u6280\u672f\u548c\u589e\u52a0\u8ba1\u7b97\u8d44\u6e90\u80fd\u51cf\u5c11\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u5bf9\u64cd\u7eb5\u7684\u654f\u611f\u6027\u3002", "conclusion": "LLM\u4f5c\u4e3a\u88c1\u5224\u7684\u8bc4\u4f30\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9a8c\u8bc1\u63a8\u7406\u58f0\u660e\u4e0e\u53ef\u89c2\u5bdf\u8bc1\u636e\u4e00\u81f4\u6027\u7684\u88c1\u5224\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2601.15094", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.15094", "abs": "https://arxiv.org/abs/2601.15094", "authors": ["Md Zahidul Haque", "Saima Afrin", "Antonio Mastropaolo"], "title": "Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks", "comment": null, "summary": "Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.", "AI": {"tldr": "\u591a\u4efb\u52a1QLoRA\u5fae\u8c03\u5728\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u529f\u80fd\u6b63\u786e\u6027\u548c\u4ee3\u7801\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6216\u8d85\u8d8a\u5355\u4efb\u52a1QLoRA\u548c\u591a\u4efb\u52a1\u5168\u5fae\u8c03\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136QLoRA\u4f18\u5316\u7684\u4ee3\u7801\u6a21\u578b\u5728\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u591a\u4efb\u52a1QLoRA\u5fae\u8c03\u7684\u6709\u6548\u6027\u3001\u8fc1\u79fb\u5b66\u4e60\u5bf9\u751f\u6210\u5de5\u4ef6\u7684\u6b63\u786e\u6027\u548c\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u591a\u4efb\u52a1QLoRA\u5fae\u8c03\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u4ee3\u7801\u4efb\u52a1\uff08\u4ee3\u7801\u751f\u6210\u3001\u7ffb\u8bd1\u3001\u6458\u8981\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u6267\u884c\u57fa\u548c\u76f8\u4f3c\u6027\u57fa\u6307\u6807\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u3002", "result": "\u591a\u4efb\u52a1QLoRA\u80fd\u6709\u6548\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u6b63\u786e\u6027\u548c\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6216\u8d85\u8d8a\u5355\u4efb\u52a1QLoRA\u548c\u591a\u4efb\u52a1\u5168\u5fae\u8c03\uff1b\u5927\u6a21\u578b\u5728\u6b63\u786e\u6027\u548c\u8d28\u91cf\u95f4\u66f4\u5e73\u8861\uff0c\u5c0f\u6a21\u578b\u80fd\u4fdd\u6301\u529f\u80fd\u4f46\u8d28\u91cf\u95ee\u9898\u66f4\u591a\u3002", "conclusion": "\u591a\u4efb\u52a1QLoRA\u5fae\u8c03\u662f\u9ad8\u6548\u4e14\u6709\u6548\u7684\u4ee3\u7801\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u5e73\u8861\u8d44\u6e90\u6d88\u8017\u548c\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.14560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14560", "abs": "https://arxiv.org/abs/2601.14560", "authors": ["Unggi Lee", "Jiyeong Bae", "Jaehyeon Park", "Haeun Park", "Taejun Park", "Younghoon Jeon", "Sungmin Cho", "Junbo Koh", "Yeil Jeong", "Gyeonggeon Lee"], "title": "Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.", "AI": {"tldr": "\u63d0\u51faPedagogicalRL-Thinking\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5b66\u63a8\u7406\u63d0\u793a\u548c\u601d\u7ef4\u5956\u52b1\u6765\u4f18\u5316LLM\u4f5c\u4e3a\u667a\u80fd\u5bfc\u5e08\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u6559\u80b2\u573a\u666f\u4e0b\u7684\u6559\u5b66\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM\u4f5c\u4e3a\u667a\u80fd\u5bfc\u5e08\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f18\u5316\u53ef\u89c1\u54cd\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u578b\u7684\u5185\u90e8\u601d\u8003\u8fc7\u7a0b\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6559\u80b2\u573a\u666f\u4f18\u5316LLM\u7684\u5185\u90e8\u63a8\u7406\uff0c\u4ee5\u63d0\u5347\u6559\u5b66\u6548\u679c\u3002", "method": "\u63d0\u51faPedagogicalRL-Thinking\u6846\u67b6\uff1a1) \u6559\u5b66\u63a8\u7406\u63d0\u793a\uff1a\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u7684\u6559\u80b2\u7406\u8bba\u800c\u975e\u901a\u7528\u6307\u4ee4\u6765\u5f15\u5bfc\u5185\u90e8\u63a8\u7406\uff1b2) \u601d\u7ef4\u5956\u52b1\uff1a\u660e\u786e\u8bc4\u4f30\u548c\u5f3a\u5316\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u7684\u6559\u5b66\u8d28\u91cf\u3002", "result": "\u9886\u57df\u7279\u5b9a\u3001\u7406\u8bba\u57fa\u7840\u7684\u63d0\u793a\u4f18\u4e8e\u901a\u7528\u63d0\u793a\uff1b\u601d\u7ef4\u5956\u52b1\u4e0e\u6559\u5b66\u63d0\u793a\u7ed3\u5408\u6700\u6709\u6548\uff1b\u4ec5\u5728\u6570\u5b66\u8f85\u5bfc\u5bf9\u8bdd\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u6559\u80b2\u57fa\u51c6\u4e0a\u8868\u73b0\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u7684\u4e8b\u5b9e\u77e5\u8bc6\u3002", "conclusion": "\u6559\u5b66\u601d\u7ef4\u5956\u52b1\u80fd\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u63a8\u7406\u8f68\u8ff9\uff0c\u589e\u52a0\u6559\u5b66\u63a8\u7406\u548c\u7ed3\u6784\u5316\u6559\u5b66\u51b3\u7b56\uff0c\u4e3a\u4f18\u5316\u6559\u80b2\u573a\u666f\u4e0b\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.14615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14615", "abs": "https://arxiv.org/abs/2601.14615", "authors": ["Xichen Zhang", "Ziyi He", "Yinghao Zhu", "Sitong Wu", "Shaozuo Yu", "Meng Chu", "Wenhu Zhang", "Haoru Tan", "Jiaya Jia"], "title": "SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation", "comment": null, "summary": "Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.", "AI": {"tldr": "SearchGym\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u641c\u7d22\u4ee3\u7406\u7684\u6a21\u62df\u73af\u5883\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u56fe\u8c31\u548c\u5bf9\u9f50\u6587\u6863\u8bed\u6599\u5e93\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u9ad8\u7684\u641c\u7d22\u4ee3\u7406\u5f00\u53d1\u3002", "motivation": "\u8bad\u7ec3\u641c\u7d22\u4ee3\u7406\u9762\u4e34\u4e24\u96be\u56f0\u5883\uff1a\u4f7f\u7528\u5b9e\u65f6\u5546\u4e1aWeb API\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5feb\u7167\u4f1a\u56e0\u6570\u636e\u4e0d\u5bf9\u9f50\u5f15\u5165\u566a\u58f0\uff0c\u4ea7\u751f\u635f\u574f\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faSearchGym\u6a21\u62df\u73af\u5883\uff0c\u4f7f\u7528\u4e25\u683c\u7684\u751f\u6210\u6d41\u7a0b\u6784\u5efa\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u56fe\u8c31\u548c\u5bf9\u9f50\u6587\u6863\u8bed\u6599\u5e93\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165SearchGym-RL\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7eaf\u5316\u53cd\u9988\u9010\u6b65\u4f18\u5316\u4ee3\u7406\u7b56\u7565\u3002", "result": "\u5728Llama\u548cQwen\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u5f3a\u5927\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u6cdb\u5316\u80fd\u529b\u3002Qwen2.5-7B-Base\u6a21\u578b\u5728SearchGym\u4e2d\u8bad\u7ec3\u540e\uff0c\u5728\u4e5d\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u76f8\u5bf9\u8d85\u8d8aASearcher\u57fa\u7ebf10.6%\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u6a21\u62df\u662f\u5f00\u53d1\u5f3a\u5927\u641c\u7d22\u4ee3\u7406\u7684\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\uff0cSearchGym\u73af\u5883\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u641c\u7d22\u4ee3\u7406\u8bad\u7ec3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15188", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.15188", "abs": "https://arxiv.org/abs/2601.15188", "authors": ["Stephan Wallraven", "Tim K\u00f6hne", "Hartmut Westenberger", "Andreas Moser"], "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback", "comment": "20 pages, 10 figures, Author: Hartmut Westenberger (ORCID: 0009-0009-9063-8318)", "summary": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210ABAP\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f3a\u5927\u6a21\u578b\u5728\u591a\u6b21\u8fed\u4ee3\u540e\u80fd\u8fbe\u5230\u7ea675%\u7684\u6210\u529f\u7387\uff0c\u5e76\u80fd\u6709\u6548\u5229\u7528\u7f16\u8bd1\u5668\u53cd\u9988\u8fdb\u884c\u6539\u8fdb\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u5728\u8bb8\u591a\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5df2\u6709\u6210\u529f\u5e94\u7528\uff0c\u4f46\u76ee\u524d\u51e0\u4e4e\u6ca1\u6709\u5bf9ABAP\u4ee3\u7801\u751f\u6210\u7684\u7cfb\u7edf\u5206\u6790\u3002\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u8bc1\u5206\u6790\u4e0d\u540cLLM\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u4e14\u529f\u80fd\u6b63\u5e38\u7684ABAP\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u5229\u7528\u7f16\u8bd1\u5668\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5305\u542b180\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u6539\u7f16\u7684HumanEval\u4efb\u52a1\u548c\u5b9e\u9645\u7684SAP\u573a\u666f\u3002\u8bc4\u4f30\u4e0d\u540cLLM\u751f\u6210ABAP\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u5b83\u4eec\u5229\u7528\u7f16\u8bd1\u5668\u53cd\u9988\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff1a\u66f4\u5f3a\u5927\u7684LLM\u5728\u591a\u6b21\u8fed\u4ee3\u540e\u80fd\u8fbe\u5230\u7ea675%\u7684\u6210\u529f\u7387\uff0c\u5e76\u80fd\u4ece\u7f16\u8bd1\u5668\u53cd\u9988\u4e2d\u5927\u5e45\u53d7\u76ca\uff1b\u800c\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u660e\u663e\u8f83\u5f31\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f3a\u5927\u7684LLM\u5728ABAP\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u5177\u6709\u5f88\u9ad8\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u8fed\u4ee3\u9519\u8bef\u4fee\u6b63\u65b9\u9762\u3002\u8fd9\u4e3aSAP\u5f00\u53d1\u73af\u5883\u4e2d\u7684AI\u8f85\u52a9\u7f16\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "topic": "code agent"}}
{"id": "2601.14658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14658", "abs": "https://arxiv.org/abs/2601.14658", "authors": ["Navid Ayoobi", "Marcus I Armstrong", "Arjun Mukherjee"], "title": "Say Anything but This: When Tokenizer Betrays Reasoning in LLMs", "comment": null, "summary": "Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct \"words\" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u63a8\u7406\u4e2d\u7684tokenization\u4e0d\u4e00\u81f4\u95ee\u9898\uff1a\u591a\u4e2atoken\u5e8f\u5217\u53ef\u80fd\u5bf9\u5e94\u76f8\u540c\u7684\u6587\u672c\uff0c\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\"\u5e7b\u5f71\u7f16\u8f91\"\u7b49\u63a8\u7406\u9519\u8bef\uff0c\u63ed\u793a\u4e86tokenizer\u5c42\u662f\u63a8\u7406\u7f3a\u9677\u7684\u91cd\u8981\u6765\u6e90\u3002", "motivation": "\u5f53\u524dLLM\u5728\u79bb\u6563token\u5e8f\u5217\u4e0a\u63a8\u7406\uff0c\u4f46\u73b0\u4ee3\u5b50\u8bcdtokenizer\u4f1a\u4ea7\u751f\u975e\u552f\u4e00\u7f16\u7801\uff08\u591a\u4e2atoken\u5e8f\u5217\u5bf9\u5e94\u76f8\u540c\u6587\u672c\uff09\uff0c\u8fd9\u79cd\u8868\u793a\u4e0d\u5339\u914d\u5bfc\u81f4\u672a\u6d4b\u91cf\u7684\u8106\u5f31\u6027\uff0c\u4f7f\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u5931\u8d25\u3002", "method": "\u5f15\u5165tokenization\u4e00\u81f4\u6027\u63a2\u9488\u4efb\u52a1\uff1a\u8981\u6c42\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u66ff\u6362\u6307\u5b9a\u76ee\u6807\u8bcd\u800c\u4e0d\u6539\u53d8\u5176\u4ed6\u5185\u5bb9\u3002\u5206\u6790\u8d85\u8fc711000\u4e2a\u66ff\u6362\u8bd5\u9a8c\uff0c\u8bc6\u522btokenizer-detokenizer\u4f2a\u5f71\u5bfc\u81f4\u7684\u5931\u8d25\u3002", "result": "\u5728\u5148\u8fdb\u5f00\u6e90LLM\u4e2d\u53d1\u73b0\u975e\u5e73\u51e1\u6bd4\u4f8b\u7684\"\u5e7b\u5f71\u7f16\u8f91\"\u8f93\u51fa\uff0c\u6a21\u578b\u5728tokenizer\u8bf1\u5bfc\u7684\u8868\u793a\u7f3a\u9677\u4e0b\u4ea7\u751f\u9519\u8bef\u63a8\u7406\u3002\u8bc6\u522b\u5e76\u5206\u7c7b\u4e868\u79cd\u7cfb\u7edf\u6027tokenizer\u4f2a\u5f71\uff0c\u5305\u62ec\u7a7a\u683c\u8fb9\u754c\u504f\u79fb\u548c\u8bcd\u5185\u91cd\u5206\u5272\u3002", "conclusion": "\u90e8\u5206\u660e\u663e\u7684\u63a8\u7406\u7f3a\u9677\u6e90\u4e8etokenizer\u5c42\uff0c\u8fd9\u8981\u6c42\u5728\u8bad\u7ec3\u66f4\u5927\u6a21\u578b\u548c\u8bed\u6599\u5e93\u4e4b\u524d\uff0c\u4f18\u5148\u8003\u8651tokenizer\u7ea7\u522b\u7684\u4fee\u590d\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.15195", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15195", "abs": "https://arxiv.org/abs/2601.15195", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Shriya Rawal", "Abdullah Al Mujahid", "Mia Mohammad Imran", "Preetha Chatterjee"], "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub", "comment": "Accepted at International Mining Software Repositories Conference (MSR 2026)", "summary": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.", "AI": {"tldr": "\u5bf9GitHub\u4e0a33k\u4e2aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4ea4\u7684PR\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u5206\u6790\u5408\u5e76\u6210\u529f\u4e0e\u5931\u8d25\u7684\u6a21\u5f0f\uff0c\u53d1\u73b0\u6587\u6863/CI\u4efb\u52a1\u6210\u529f\u7387\u6700\u9ad8\uff0c\u6027\u80fd/\u4fee\u590d\u4efb\u52a1\u6700\u5dee\uff0c\u5e76\u5efa\u7acb\u4e86\u62d2\u7edd\u6a21\u5f0f\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6\u9879\u76ee\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u8d21\u732e\u8005\u63d0\u4ea4PR\u7684\u6570\u91cf\u5feb\u901f\u589e\u957f\uff0c\u4f46\u5bf9\u5176\u5b9e\u9645\u884c\u4e3a\u4ee5\u53ca\u4e3a\u4f55\u8bb8\u591aPR\u672a\u80fd\u5408\u5e76\u7684\u4e86\u89e3\u5f88\u5c11\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76AI\u4ee3\u7406\u5728\u5b9e\u9645\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a1) \u5bf933k\u4e2aAI\u4ee3\u7406\u63d0\u4ea4\u7684PR\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u4ece\u4efb\u52a1\u7c7b\u578b\u3001\u4ee3\u7801\u53d8\u66f4\u3001CI\u6784\u5efa\u7ed3\u679c\u3001\u8bc4\u5ba1\u52a8\u6001\u56db\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\u5408\u5e76\u4e0e\u672a\u5408\u5e76\u7684PR\uff1b2) \u5bf9600\u4e2aPR\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u5efa\u7acb\u62d2\u7edd\u6a21\u5f0f\u7684\u5c42\u6b21\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u6587\u6863\u3001CI\u548c\u6784\u5efa\u66f4\u65b0\u4efb\u52a1\u5408\u5e76\u6210\u529f\u7387\u6700\u9ad8\uff0c\u6027\u80fd\u548cbug\u4fee\u590d\u4efb\u52a1\u6700\u5dee\uff1b\u672a\u5408\u5e76\u7684PR\u901a\u5e38\u6d89\u53ca\u66f4\u5927\u7684\u4ee3\u7801\u53d8\u66f4\u3001\u66f4\u591a\u6587\u4ef6\u4fee\u6539\uff0c\u4e14\u5e38\u672a\u901a\u8fc7CI/CD\u9a8c\u8bc1\uff1b\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u7f3a\u4e4f\u6709\u610f\u4e49\u7684\u8bc4\u5ba1\u53c2\u4e0e\u3001\u91cd\u590dPR\u3001\u4e0d\u9700\u8981\u7684\u529f\u80fd\u5b9e\u73b0\u548c\u4ee3\u7406\u9519\u4f4d\u7b49\u62d2\u7edd\u539f\u56e0\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6539\u8fdb\u672a\u6765AI\u4ee3\u7406\u5de5\u4f5c\u6d41\u6210\u529f\u7684\u5173\u952e\u793e\u4f1a\u6280\u672f\u548c\u4eba\u673a\u534f\u4f5c\u56e0\u7d20\uff0c\u5305\u62ec\u9700\u8981\u66f4\u597d\u7684\u4efb\u52a1\u9009\u62e9\u3001\u4ee3\u7801\u53d8\u66f4\u7ba1\u7406\u3001CI\u96c6\u6210\u548c\u8bc4\u5ba1\u53c2\u4e0e\u673a\u5236\u3002", "topic": "swe application"}}
{"id": "2601.14696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14696", "abs": "https://arxiv.org/abs/2601.14696", "authors": ["Zhaiyu Fang", "Ruipeng Sun"], "title": "AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization", "comment": "under review", "summary": "Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just tool invocation, but the adaptive wisdom to discern when to use them. We propose AdaTIR, a framework that shifts the paradigm from static tool invocation to difficulty-aware reasoning internalization. By introducing a difficulty-aware efficiency reward, AdaTIR dynamically adjusts tool budgets based on task complexity--internalizing reasoning for simple tasks while selectively invoking tools for complex tasks. Furthermore, we identify a sign reversal problem where tool penalties outweigh correctness rewards, mistakenly penalizing correct rollouts with negative advantages. To resolve this, we propose Clipped Advantage Shaping (CAS), which ensures that correctness remains the primary objective while using efficiency as a secondary constraint. Empirical results demonstrate that AdaTIR reduces tool calls by up to 97.6% on simple tasks and 28.2% on complex challenges while maintaining or enhancing accuracy. Notably, AdaTIR successfully internalizes reasoning, outperforming baselines by 4.8% on AIME 2024 even when tool access is strictly disabled.", "AI": {"tldr": "AdaTIR\u6846\u67b6\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u63a8\u7406\u5185\u90e8\u5316\uff0c\u52a8\u6001\u8c03\u6574\u5de5\u5177\u4f7f\u7528\u9884\u7b97\uff0c\u51cf\u5c11\u7b80\u5355\u4efb\u52a1\u4e2d97.6%\u7684\u5de5\u5177\u8c03\u7528\uff0c\u540c\u65f6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5b58\u5728\u8ba4\u77e5\u5378\u8f7d\u95ee\u9898\uff0c\u5373\u4f7f\u7b80\u5355\u4efb\u52a1\u4e5f\u5197\u4f59\u8c03\u7528\u5916\u90e8\u5de5\u5177\u3002\u771f\u6b63\u7684\u667a\u80fd\u4ee3\u7406\u9700\u8981\u81ea\u9002\u5e94\u5224\u65ad\u4f55\u65f6\u4f7f\u7528\u5de5\u5177\uff0c\u800c\u975e\u9759\u6001\u5de5\u5177\u8c03\u7528\u3002", "method": "\u63d0\u51faAdaTIR\u6846\u67b6\uff1a1) \u5f15\u5165\u96be\u5ea6\u611f\u77e5\u6548\u7387\u5956\u52b1\uff0c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u5de5\u5177\u9884\u7b97\uff1b2) \u63d0\u51faClipped Advantage Shaping\u89e3\u51b3\u7b26\u53f7\u53cd\u8f6c\u95ee\u9898\uff0c\u786e\u4fdd\u6b63\u786e\u6027\u4e3a\u4e3b\u8981\u76ee\u6807\uff0c\u6548\u7387\u4e3a\u6b21\u8981\u7ea6\u675f\u3002", "result": "AdaTIR\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u51cf\u5c1197.6%\u5de5\u5177\u8c03\u7528\uff0c\u590d\u6742\u4efb\u52a1\u51cf\u5c1128.2%\u5de5\u5177\u8c03\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002\u5728AIME 2024\u4e0a\uff0c\u5373\u4f7f\u7981\u7528\u5de5\u5177\u8bbf\u95ee\uff0c\u4ecd\u8d85\u8d8a\u57fa\u7ebf4.8%\u3002", "conclusion": "AdaTIR\u5b9e\u73b0\u4e86\u4ece\u9759\u6001\u5de5\u5177\u8c03\u7528\u5230\u96be\u5ea6\u611f\u77e5\u63a8\u7406\u5185\u90e8\u5316\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u771f\u6b63\u7684\u81ea\u9002\u5e94\u667a\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.14790", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14790", "abs": "https://arxiv.org/abs/2601.14790", "authors": ["Zhi Qiu", "Jiazheng Sun", "Chenxiao Xia", "Jun Zheng", "Xin Peng"], "title": "CI4A: Semantic Component Interfaces for Agents Empowering Web Automation", "comment": "9 pages, 5 figures", "summary": "While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.", "AI": {"tldr": "CI4A\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u667a\u80fd\u4f53\u4f18\u5316\u7684\u7ec4\u4ef6\u63a5\u53e3\uff0c\u5c06\u590d\u6742\u7684UI\u4ea4\u4e92\u903b\u8f91\u5c01\u88c5\u4e3a\u7edf\u4e00\u7684\u5de5\u5177\u539f\u8bed\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728Web\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u5c42\u8bed\u4e49\u89c4\u5212\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u7684Web\u7ec4\u4ef6\u64cd\u4f5c\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002\u73b0\u6709\u7814\u7a76\u591a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u589e\u5f3a\u6a21\u578b\u7684\u57fa\u7840\u80fd\u529b\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u5e94\u8be5\u4e3a\u667a\u80fd\u4f53\u6784\u5efa\u4e13\u95e8\u7684\u4ea4\u4e92\u63a5\u53e3\uff0c\u800c\u4e0d\u662f\u8ba9\u667a\u80fd\u4f53\u9002\u5e94\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u754c\u9762\u3002", "method": "\u63d0\u51fa\u4e86CI4A\uff08Component Interface for Agent\uff09\u8bed\u4e49\u5c01\u88c5\u673a\u5236\uff0c\u5c06UI\u7ec4\u4ef6\u7684\u590d\u6742\u4ea4\u4e92\u903b\u8f91\u62bd\u8c61\u4e3a\u4e00\u7ec4\u7edf\u4e00\u7684\u5de5\u5177\u539f\u8bed\u3002\u5728Ant Design\u5de5\u4e1a\u7ea7\u524d\u7aef\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86CI4A\uff0c\u8986\u76d623\u7c7b\u5e38\u7528UI\u7ec4\u4ef6\u3002\u5f00\u53d1\u4e86\u6df7\u5408\u667a\u80fd\u4f53\uff0c\u5176\u52a8\u4f5c\u7a7a\u95f4\u80fd\u6839\u636e\u9875\u9762\u72b6\u6001\u52a8\u6001\u66f4\u65b0\uff0c\u7075\u6d3b\u8c03\u7528\u53ef\u7528\u7684CI4A\u5de5\u5177\u3002", "result": "\u57fa\u4e8eCI4A\u96c6\u6210\u7684Ant Design\u91cd\u6784\u5e76\u5347\u7ea7\u4e86WebArena\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eCI4A\u7684\u667a\u80fd\u4f53\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e8686.3%\u7684SOTA\u4efb\u52a1\u6210\u529f\u7387\uff0c\u540c\u65f6\u6267\u884c\u6548\u7387\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u4e3a\u667a\u80fd\u4f53\u4e13\u95e8\u8bbe\u8ba1\u4f18\u5316\u7684\u4ea4\u4e92\u63a5\u53e3\uff08CI4A\uff09\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6Web\u7ec4\u4ef6\u64cd\u4f5c\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728Web\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2601.14532", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14532", "abs": "https://arxiv.org/abs/2601.14532", "authors": ["Alistair Cheong", "Haolin Cong", "Tyler Yang", "Dustin Miao"], "title": "Search over Self-Edit Strategies for LLM Adaptation", "comment": null, "summary": "Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker \"Implications\" baseline, while the archive variant outperformed \"Implications\" and approached the strongest human-designed \"Rewrite\" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22LLM\u80fd\u5426\u5229\u7528\u4efb\u52a1\u53cd\u9988\u81ea\u4e3b\u51b3\u5b9a\u6743\u91cd\u66f4\u65b0\u7b56\u7565\uff0c\u5728SEAL\u6846\u67b6\u4e2d\u5141\u8bb8\u6a21\u578b\u751f\u6210\u81ea\u7f16\u8f91\u6a21\u677f\uff0c\u53d1\u73b0\u5e26\u5b58\u6863\u7684\u53d8\u4f53\u8868\u73b0\u63a5\u8fd1\u6700\u4f73\u4eba\u5de5\u57fa\u7ebf\u4f46\u672a\u8d85\u8d8a\uff0c\u5206\u6790\u663e\u793a\u9700\u8981\u663e\u5f0f\u65b0\u9896\u6027\u538b\u529b\u6765\u8d85\u8d8a\u4eba\u5de5\u7b56\u7565\u3002", "motivation": "\u73b0\u6709LLM\u5f00\u653e\u641c\u7d22\u7cfb\u7edf\u901a\u5e38\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u80fd\u9650\u5236\u957f\u671f\u8fdb\u5c55\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u6d4b\u8bd5\u65f6\u66f4\u65b0\u63d0\u6848\u6a21\u578b\uff0c\u4f46\u66f4\u65b0\u7b56\u7565\u4ecd\u9700\u4eba\u5de5\u6307\u5b9a\u3002\u56e0\u6b64\u7814\u7a76LLM\u80fd\u5426\u5229\u7528\u4efb\u52a1\u53cd\u9988\u81ea\u4e3b\u51b3\u5b9a\u6743\u91cd\u66f4\u65b0\u65b9\u5f0f\u3002", "method": "\u5728SEAL\u6846\u67b6\u4e2d\u653e\u5bbd\u56fa\u5b9a\u4eba\u5de5\u6a21\u677f\u7ea6\u675f\uff0c\u5141\u8bb8\u6a21\u578b\u751f\u6210\u81ea\u7f16\u8f91\u6a21\u677f\uff0c\u4ece\u800c\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u548c\u8d85\u53c2\u6570\u3002\u7814\u7a76\u4e24\u4e2a\u53d8\u4f53\uff1a\u65e0\u5b58\u6863\u7248\u672c\u548c\u5e26\u8f7b\u91cf\u7ea7\u5386\u53f2\u6a21\u677f\u5b58\u6863\u7684\u7248\u672c\u3002\u5728Qwen3-8B\u6a21\u578b\u548cSQuAD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5355\u8f6e\u77e5\u8bc6\u6574\u5408\u5b9e\u9a8c\u3002", "result": "\u65e0\u5b58\u6863\u53d8\u4f53\u8868\u73b0\u4e0e\u8f83\u5f31\u7684\"Implications\"\u57fa\u7ebf\u76f8\u5f53\uff0c\u5e26\u5b58\u6863\u53d8\u4f53\u4f18\u4e8e\"Implications\"\u5e76\u63a5\u8fd1\u6700\u5f3a\u4eba\u5de5\u8bbe\u8ba1\u7684\"Rewrite\"\u57fa\u7ebf\u4f46\u672a\u8d85\u8d8a\u3002\u5206\u6790\u663e\u793a\u6734\u7d20\u5b58\u6863\u80fd\u63d0\u4f9b\u77ed\u671f\u9c81\u68d2\u6027\uff0c\u4f46\u4e5f\u4f1a\u52a0\u901f\u540c\u8d28\u5316\u3002", "conclusion": "LLM\u80fd\u591f\u5229\u7528\u4efb\u52a1\u53cd\u9988\u81ea\u4e3b\u51b3\u5b9a\u6743\u91cd\u66f4\u65b0\u7b56\u7565\uff0c\u4f46\u8981\u6301\u7eed\u8d85\u8d8a\u7cbe\u5fc3\u4f18\u5316\u7684\u4eba\u5de5\u7b56\u7565\uff0c\u53ef\u80fd\u9700\u8981\u663e\u5f0f\u7684\u65b0\u9896\u6027\u538b\u529b\u673a\u5236\u3002\u6734\u7d20\u5b58\u6863\u65b9\u6cd5\u6709\u5176\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.15232", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.15232", "abs": "https://arxiv.org/abs/2601.15232", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "comment": "A version of this paper has been submitted to ACM Transactions on Software Engineering and Methodology", "summary": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "AI": {"tldr": "\u5bf9LLM\u4ee3\u7406\u5f00\u53d1\u4e2dbug\u7c7b\u578b\u7684\u9996\u6b21\u5168\u9762\u7814\u7a76\uff0c\u5206\u6790\u4e861,187\u4e2abug\u76f8\u5173\u5e16\u5b50\uff0c\u5e76\u6784\u5efaBugReAct\u4ee3\u7406\u6765\u81ea\u52a8\u5316bug\u8bc6\u522b\u3002", "motivation": "LLM\u4ee3\u7406\u5f00\u53d1\u8c03\u8bd5\u56f0\u96be\u4e14\u6210\u672c\u9ad8\uff0c\u8be5\u9886\u57df\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u793e\u533a\u53d1\u5c55\u4e0d\u6210\u719f\uff0c\u9700\u8981\u7cfb\u7edf\u7406\u89e3\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684bug\u7c7b\u578b\u3001\u6839\u672c\u539f\u56e0\u548c\u5f71\u54cd\u3002", "method": "\u4eceStack Overflow\u3001GitHub\u548cHugging Face\u6536\u96c61,187\u4e2abug\u76f8\u5173\u5e16\u5b50\u548c\u4ee3\u7801\u7247\u6bb5\uff0c\u5206\u67907\u4e2a\u4e3b\u6d41LLM\u6846\u67b6\u53ca\u81ea\u5b9a\u4e49\u5b9e\u73b0\u4e2d\u7684bug\u3002\u6784\u5efaBugReAct\u4ee3\u7406\uff08\u57fa\u4e8eReAct\u67b6\u6784\uff09\u6765\u63a2\u7d22\u81ea\u52a8\u5316bug\u8bc6\u522b\u7684\u53ef\u884c\u6027\u3002", "result": "BugReAct\u4ee3\u7406\uff08\u4f7f\u7528Gemini 2.5 Flash\uff09\u5728bug\u7279\u5f81\u6807\u6ce8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6bcf\u4e2a\u5e16\u5b50/\u4ee3\u7801\u7247\u6bb5\u7684\u6210\u672c\u4ec5\u4e3a0.01\u7f8e\u5143\u3002", "conclusion": "LLM\u4ee3\u7406\u5f00\u53d1\u5b58\u5728\u7cfb\u7edf\u6027\u7684bug\u6a21\u5f0f\uff0c\u81ea\u52a8\u5316bug\u8bc6\u522b\u5177\u6709\u53ef\u884c\u6027\u4e14\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u4e3a\u4ee3\u7406\u5f00\u53d1\u8c03\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.15059", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15059", "abs": "https://arxiv.org/abs/2601.15059", "authors": ["Oleg Romanchuk", "Roman Bondar"], "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems", "comment": null, "summary": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.\n  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.\n  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.\n  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.\n  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.", "AI": {"tldr": "\u8bba\u6587\u8bc6\u522b\u4e86\u73b0\u4ee3CI/CD\u7ba1\u9053\u4e2d\u8d23\u4efb\u5f52\u5c5e\u7684\u7ed3\u6784\u6027\u5931\u6548\u95ee\u9898\uff1a\u5f53AI\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u7684\u51b3\u7b56\u541e\u5410\u91cf\u8d85\u8fc7\u4eba\u7c7b\u9a8c\u8bc1\u80fd\u529b\u65f6\uff0c\u4f1a\u5f62\u6210\"\u8d23\u4efb\u771f\u7a7a\"\uff0c\u5373\u65e0\u4eba\u540c\u65f6\u5177\u5907\u6279\u51c6\u51b3\u7b56\u7684\u6743\u5a01\u548c\u771f\u6b63\u7406\u89e3\u51b3\u7b56\u4f9d\u636e\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3CI/CD\u7ba1\u9053\u96c6\u6210AI\u4ee3\u7406\u751f\u6210\u4ee3\u7801\u65f6\uff0c\u867d\u7136\u51b3\u7b56\u901a\u8fc7\u6b63\u5f0f\u6b63\u786e\u7684\u5ba1\u6279\u6d41\u7a0b\u6267\u884c\uff0c\u4f46\u6ca1\u6709\u4efb\u4f55\u5b9e\u4f53\u540c\u65f6\u62e5\u6709\u6279\u51c6\u51b3\u7b56\u7684\u6743\u5a01\u548c\u771f\u6b63\u7406\u89e3\u51b3\u7b56\u4f9d\u636e\u7684\u8ba4\u77e5\u80fd\u529b\u3002\u8fd9\u79cd\u7ed3\u6784\u6027\u7f3a\u9677\u5bfc\u81f4\u8d23\u4efb\u5f52\u5c5e\u5931\u8d25\u3002", "method": "\u5b9a\u4e49\u4e86\"\u8d23\u4efb\u771f\u7a7a\"\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u5305\u542b\u5e76\u884c\u4ee3\u7406\u751f\u6210\u3001CI\u9a8c\u8bc1\u548c\u4e2a\u6027\u5316\u4eba\u5de5\u5ba1\u6279\u7684\u90e8\u7f72\u573a\u666f\u3002\u8bc6\u522b\u4e86\u541e\u5410\u91cf\u9608\u503c\uff0c\u8d85\u8fc7\u8be5\u9608\u503c\u540e\u9a8c\u8bc1\u4e0d\u518d\u4f5c\u4e3a\u51b3\u7b56\u6807\u51c6\uff0c\u800c\u662f\u88ab\u57fa\u4e8e\u4ee3\u7406\u4fe1\u53f7\u7684\u4eea\u5f0f\u5316\u5ba1\u6279\u53d6\u4ee3\u3002\u8fdb\u4e00\u6b65\u63cf\u8ff0\u4e86CI\u653e\u5927\u52a8\u6001\uff0c\u5373\u589e\u52a0\u81ea\u52a8\u5316\u9a8c\u8bc1\u8986\u76d6\u7387\u4f1a\u63d0\u9ad8\u4ee3\u7406\u4fe1\u53f7\u5bc6\u5ea6\u800c\u4e0d\u6062\u590d\u4eba\u7c7b\u80fd\u529b\u3002", "result": "\u8bc6\u522b\u4e86\u6807\u51c6\u90e8\u7f72\u5047\u8bbe\u4e0b\u7684\u6269\u5c55\u9650\u5236\uff1a\u8d85\u8fc7\u7279\u5b9a\u541e\u5410\u91cf\u9608\u503c\u540e\uff0c\u9a8c\u8bc1\u529f\u80fd\u5931\u6548\uff0c\u4e2a\u6027\u5316\u8d23\u4efb\u53d8\u5f97\u7ed3\u6784\u4e0a\u4e0d\u53ef\u5b9e\u73b0\u3002CI\u653e\u5927\u52a8\u6001\u52a0\u901f\u4e86\u8ba4\u77e5\u5378\u8f7d\uff0c\u6269\u5927\u4e86\u6b63\u5f0f\u6279\u51c6\u4e0e\u8ba4\u77e5\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u989d\u5916\u81ea\u52a8\u5316\u53cd\u800c\u653e\u5927\u4e86\u8d23\u4efb\u771f\u7a7a\u3002", "conclusion": "\u9664\u975e\u7ec4\u7ec7\u660e\u786e\u91cd\u65b0\u8bbe\u8ba1\u51b3\u7b56\u8fb9\u754c\u6216\u5c06\u8d23\u4efb\u4ece\u4e2a\u4f53\u51b3\u7b56\u91cd\u65b0\u5206\u914d\u5230\u6279\u6b21\u6216\u7cfb\u7edf\u7ea7\u522b\u6240\u6709\u6743\uff0c\u5426\u5219\u8d23\u4efb\u771f\u7a7a\u5728\u89c4\u6a21\u5316\u4ee3\u7406\u90e8\u7f72\u4e2d\u4ecd\u5c06\u662f\u4e00\u4e2a\u9690\u5f62\u4f46\u6301\u4e45\u7684\u5931\u6548\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.14896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14896", "abs": "https://arxiv.org/abs/2601.14896", "authors": ["Rui Qi", "Fengran Mo", "Yufeng Chen", "Xue Zhang", "Shuo Wang", "Hongliang Li", "Jinan Xu", "Meng Jiang", "Jian-Yun Nie", "Kaiyu Huang"], "title": "Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation", "comment": null, "summary": "Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.", "AI": {"tldr": "LcRL\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u8026\u5408\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u548c\u8f85\u52a9\u53cd\u4e00\u81f4\u6027\u60e9\u7f5a\uff0c\u89e3\u51b3\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u504f\u89c1\u548c\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u7684\"\u4e00\u5200\u5207\"\u7b56\u7565\uff0c\u5728\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u67e5\u8be2\u65f6\u5bb9\u6613\u51fa\u73b0\u77e5\u8bc6\u504f\u89c1\u548c\u51b2\u7a81\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faLcRL\u6846\u67b6\uff0c\u5305\u542b\u8bed\u8a00\u8026\u5408\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff1a1) \u5728rollout\u6a21\u5757\u91c7\u7528\u8bed\u8a00\u8026\u5408\u7ec4\u91c7\u6837\u51cf\u5c11\u77e5\u8bc6\u504f\u89c1\uff1b2) \u5728\u5956\u52b1\u6a21\u578b\u4e2d\u6b63\u5219\u5316\u8f85\u52a9\u53cd\u4e00\u81f4\u6027\u60e9\u7f5a\u7f13\u89e3\u77e5\u8bc6\u51b2\u7a81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLcRL\u4e0d\u4ec5\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u800c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u573a\u666f\uff0c\u5982\u53d7\u9650\u8bad\u7ec3\u6570\u636e\u548c\u5305\u542b\u5927\u91cf\u8bed\u8a00\u7684\u68c0\u7d22\u96c6\u5408\u3002", "conclusion": "LcRL\u901a\u8fc7\u89e3\u51b3\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u504f\u89c1\u548c\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15075", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15075", "abs": "https://arxiv.org/abs/2601.15075", "authors": ["Chen Qian", "Peng Wang", "Dongrui Liu", "Junyao Yang", "Dadi Guo", "Ling Tang", "Jilin Mei", "Qihan Ren", "Shuai Shao", "Yong Liu", "Jie Fu", "Jing Shao", "Xia Hu"], "title": "The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution", "comment": null, "summary": "Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u667a\u80fd\u4f53\u5f52\u56e0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5185\u90e8\u56e0\u7d20\uff0c\u65e0\u8bba\u4efb\u52a1\u7ed3\u679c\u5982\u4f55\uff0c\u901a\u8fc7\u5206\u5c42\u65b9\u6cd5\u5b9a\u4f4d\u5173\u952e\u4ea4\u4e92\u6b65\u9aa4\u548c\u6587\u672c\u8bc1\u636e\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u5728\u5ba2\u670d\u3001\u7f51\u9875\u5bfc\u822a\u3001\u8f6f\u4ef6\u5de5\u7a0b\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5e7f\u6cdb\u90e8\u7f72\uff0c\u7406\u89e3\u667a\u80fd\u4f53\u4e3a\u4f55\u91c7\u53d6\u7279\u5b9a\u884c\u52a8\u5bf9\u4e8e\u95ee\u8d23\u548c\u6cbb\u7406\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5931\u8d25\u5f52\u56e0\uff0c\u4e0d\u8db3\u4ee5\u89e3\u91ca\u667a\u80fd\u4f53\u884c\u4e3a\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u901a\u7528\u667a\u80fd\u4f53\u5f52\u56e0\u6846\u67b6\uff1a\u5728\u7ec4\u4ef6\u7ea7\u522b\u4f7f\u7528\u65f6\u95f4\u4f3c\u7136\u52a8\u6001\u8bc6\u522b\u5173\u952e\u4ea4\u4e92\u6b65\u9aa4\uff1b\u5728\u53e5\u5b50\u7ea7\u522b\u4f7f\u7528\u57fa\u4e8e\u6270\u52a8\u7684\u5206\u6790\u6765\u7cbe\u786e\u5b9a\u4f4d\u7279\u5b9a\u6587\u672c\u8bc1\u636e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5173\u952e\u5386\u53f2\u4e8b\u4ef6\u548c\u53e5\u5b50\uff0c\u6db5\u76d6\u6807\u51c6\u5de5\u5177\u4f7f\u7528\u548c\u5185\u5b58\u8bf1\u5bfc\u504f\u5dee\u7b49\u5fae\u5999\u53ef\u9760\u6027\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u95ee\u8d23\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u80fd\u591f\u89e3\u91ca\u667a\u80fd\u4f53\u884c\u4e3a\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5931\u8d25\u5f52\u56e0\u3002", "topic": "agent analysis"}}
{"id": "2601.15120", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15120", "abs": "https://arxiv.org/abs/2601.15120", "authors": ["Qian Xiong", "Yuekai Huang", "Yujia Zheng", "Tianhao Li", "Ziyou Jiang", "Zhiyuan Chang", "Zhaoyang Li", "Huanxiang Feng", "Mingyang Li"], "title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories", "comment": null, "summary": "LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.", "AI": {"tldr": "RISE\u63d0\u51fa\u4e86\u4e00\u79cd\"\u4ece\u771f\u5b9e\u5230\u865a\u62df\"\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u5de5\u5177\u539f\u8bed\u5408\u6210\u865a\u62df\u8f68\u8ff9\u548c\u751f\u6210\u8d1f\u6837\u672c\uff0c\u6765\u7f13\u89e3LLM\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u4e2d\u7684\u610f\u56fe\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u5b8c\u6210\u548c\u610f\u56fe\u5bf9\u9f50\u6027\u80fd\u3002", "motivation": "LLM\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7ecf\u5e38\u51fa\u73b0\u610f\u5916\u884c\u4e3a\u6216\u7ed3\u679c\uff0c\u5176\u4e2d\"\u610f\u56fe\u504f\u5dee\"\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u53ef\u9760\u8bc4\u4f30\u548c\u6027\u80fd\u6539\u8fdb\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6210\u672c\u9ad8\u6602\u7684\u771f\u5b9e\u7cfb\u7edf\u6837\u672c\uff0c\u8981\u4e48\u4f7f\u7528LLM\u6a21\u62df\u7684\u865a\u62df\u6570\u636e\u4f46\u5b58\u5728\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4e14\u90fd\u7f3a\u4e4f\u9488\u5bf9\u610f\u56fe\u504f\u5dee\u573a\u666f\u7684\u8d1f\u6837\u672c\u3002", "method": "RISE\u91c7\u7528\"\u4ece\u771f\u5b9e\u5230\u865a\u62df\"\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u5de5\u5177\u539f\u8bed\u5408\u6210\u865a\u62df\u8f68\u8ff9\uff0c\u901a\u8fc7\u5bf9\u5173\u952e\u53c2\u6570\u8fdb\u884c\u7a81\u53d8\u751f\u6210\u591a\u6837\u5316\u7684\u8d1f\u6837\u672c\uff0c\u7136\u540e\u4f7f\u7528\u5408\u6210\u6570\u636e\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5bf9\u9aa8\u5e72LLM\u8fdb\u884c\u5fae\u8c03\u4ee5\u5b9e\u73b0\u610f\u56fe\u5bf9\u9f50\u3002", "result": "RISE\u57288\u4e2a\u6db5\u76d6\u7528\u6237\u9700\u6c42\u3001\u6267\u884c\u8f68\u8ff9\u548c\u4ee3\u7406\u54cd\u5e94\u7684\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u4e0e\u8bad\u7ec3\u7ed3\u5408\u540e\uff0c\u5728Acctask\uff08\u4efb\u52a1\u5b8c\u6210\uff09\u4e0a\u5e73\u5747\u63d0\u534735.28%\uff0c\u5728Accintent\uff08\u610f\u56fe\u5bf9\u9f50\uff09\u4e0a\u5e73\u5747\u63d0\u534723.27%\uff0c\u5206\u522b\u6bd4SOTA\u57fa\u7ebf\u9ad8\u51fa1.20-42.09%\u548c1.17-54.93%\u3002", "conclusion": "RISE\u901a\u8fc7\u521b\u65b0\u7684\u771f\u5b9e\u5230\u865a\u62df\u6570\u636e\u5408\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u4e2d\u7684\u610f\u56fe\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u610f\u56fe\u5bf9\u9f50\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "2601.14914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14914", "abs": "https://arxiv.org/abs/2601.14914", "authors": ["Tianxiang Fei", "Cheng Chen", "Yue Pan", "Mao Zheng", "Mingyang Song"], "title": "CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents", "comment": null, "summary": "Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.", "AI": {"tldr": "CodeDelegator\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u5c06\u89c4\u5212\u4e0e\u5b9e\u73b0\u5206\u5f00\uff0c\u4f7f\u7528\u6301\u4e45\u5316\u7684Delegator\u8fdb\u884c\u6218\u7565\u76d1\u7763\uff0c\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5b9e\u4f8b\u5316\u65b0\u7684Coder\u667a\u80fd\u4f53\uff0c\u5e76\u901a\u8fc7EPSS\u673a\u5236\u9694\u79bb\u6267\u884c\u72b6\u6001\u4ee5\u9632\u6b62\u4e0a\u4e0b\u6587\u6c61\u67d3\u3002", "motivation": "\u867d\u7136LLM\u667a\u80fd\u4f53\u53ef\u4ee5\u5c06\u52a8\u4f5c\u8868\u793a\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u4f46\u73b0\u5b9e\u4efb\u52a1\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u8be6\u7ec6\u5b9e\u73b0\u3002\u5355\u4e00\u667a\u80fd\u4f53\u540c\u65f6\u5904\u7406\u4e24\u8005\u4f1a\u5bfc\u81f4\u8c03\u8bd5\u75d5\u8ff9\u548c\u4e2d\u95f4\u5931\u8d25\u6c61\u67d3\u4e0a\u4e0b\u6587\uff0c\u635f\u5bb3\u957f\u89c6\u91ce\u6027\u80fd\u3002", "method": "\u63d0\u51faCodeDelegator\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u6301\u4e45\u5316\u7684Delegator\u8d1f\u8d23\u4efb\u52a1\u5206\u89e3\u3001\u7f16\u5199\u89c4\u8303\u548c\u76d1\u63a7\u8fdb\u5ea6\u4f46\u4e0d\u6267\u884c\u4ee3\u7801\uff1b2) \u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5b9e\u4f8b\u5316\u65b0\u7684Coder\u667a\u80fd\u4f53\uff0c\u62e5\u6709\u5e72\u51c0\u7684\u4e0a\u4e0b\u6587\uff1b3) \u5f15\u5165EPSS\u673a\u5236\u9694\u79bbCoder\u7684\u6267\u884c\u72b6\u6001\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86CodeDelegator\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u89d2\u8272\u4e13\u4e1a\u5316\u548c\u4e0a\u4e0b\u6587\u9694\u79bb\uff0cCodeDelegator\u89e3\u51b3\u4e86\u5355\u4e00\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4e0a\u4e0b\u6587\u6c61\u67d3\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u957f\u89c6\u91ce\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.14952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14952", "abs": "https://arxiv.org/abs/2601.14952", "authors": ["Zhiyuan Lu", "Chenliang Li", "Yingcheng Shi", "Weizhou Shen", "Ming Yan", "Fei Huang"], "title": "CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning", "comment": null, "summary": "While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86CorpusQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u5343\u4e07token\u7ea7\u522b\u6587\u6863\u5e93\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u5728\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u65f6\u8868\u73b0\u4e0b\u964d\uff0c\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u5b8c\u5168\u5931\u6548\uff0c\u9700\u8981\u5185\u5b58\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u67b6\u6784\u3002", "motivation": "\u5f53\u524dLLM\u867d\u7136\u80fd\u5904\u7406\u767e\u4e07token\u4e0a\u4e0b\u6587\uff0c\u4f46\u5728\u6574\u4e2a\u6587\u6863\u5e93\u7ea7\u522b\u7684\u63a8\u7406\u80fd\u529b\u5c1a\u672a\u5145\u5206\u6d4b\u8bd5\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e2a\u957f\u6587\u672c\u6216\u4f9d\u8d56\"\u7a00\u758f\u68c0\u7d22\"\u5047\u8bbe\uff0c\u65e0\u6cd5\u8bc4\u4f30\u771f\u6b63\u7684\u8bed\u6599\u5e93\u7ea7\u522b\u5206\u6790\u80fd\u529b\uff0c\u56e0\u4e3a\u8bc1\u636e\u9ad8\u5ea6\u5206\u6563\u5728\u6570\u767e\u4e2a\u6587\u6863\u4e2d\uff0c\u9700\u8981\u5168\u5c40\u6574\u5408\u3001\u6bd4\u8f83\u548c\u7edf\u8ba1\u805a\u5408\u3002", "method": "\u5f15\u5165CorpusQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89c4\u6a21\u8fbe1000\u4e07token\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\u751f\u6210\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u6587\u672c\u8868\u793a\uff0c\u521b\u5efa\u590d\u6742\u3001\u8ba1\u7b97\u5bc6\u96c6\u7684\u67e5\u8be2\uff0c\u5e76\u4fdd\u8bc1\u7a0b\u5e8f\u5316\u751f\u6210\u7684\u771f\u5b9e\u7b54\u6848\uff0c\u6311\u6218\u7cfb\u7edf\u5728\u4e0d\u4f9d\u8d56\u6613\u9519\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5bf9\u6d77\u91cf\u975e\u7ed3\u6784\u5316\u6587\u672c\u8fdb\u884c\u6574\u4f53\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u957f\u4e0a\u4e0b\u6587LLM\u5728\u8f93\u5165\u957f\u5ea6\u589e\u52a0\u65f6\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u6807\u51c6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5b8c\u5168\u5931\u6548\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5185\u5b58\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u67b6\u6784\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u901a\u7528\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u4ece\u7b80\u5355\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u8f6c\u5411\u5f00\u53d1\u5148\u8fdb\u67b6\u6784\u4ee5\u5b9e\u73b0\u5168\u5c40\u4fe1\u606f\u5408\u6210\u3002\u5185\u5b58\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u67b6\u6784\u662f\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8868\u660e\u5728\u6587\u6863\u5e93\u7ea7\u522b\u63a8\u7406\u65b9\u9762\u9700\u8981\u67b6\u6784\u5c42\u9762\u7684\u6839\u672c\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2601.15153", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15153", "abs": "https://arxiv.org/abs/2601.15153", "authors": ["Choro Ulan uulu", "Mikhail Kulyabin", "Iris Fuhrmann", "Jan Joosten", "Nuno Miguel Martins Pacheco", "Filippos Petridis", "Rebecca Johnson", "Jan Bosch", "Helena Holmstr\u00f6m Olsson"], "title": "How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework", "comment": null, "summary": "Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u4eba\u7c7b\u9886\u57df\u77e5\u8bc6\u5d4c\u5165AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u4eff\u771f\u6570\u636e\u53ef\u89c6\u5316\u751f\u6210\uff0c\u4f7f\u975e\u4e13\u5bb6\u80fd\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6548\u679c\u3002", "motivation": "\u5173\u952e\u9886\u57df\u77e5\u8bc6\u901a\u5e38\u638c\u63e1\u5728\u5c11\u6570\u4e13\u5bb6\u624b\u4e2d\uff0c\u9020\u6210\u7ec4\u7ec7\u53ef\u6269\u5c55\u6027\u548c\u51b3\u7b56\u74f6\u9888\u3002\u975e\u4e13\u5bb6\u96be\u4ee5\u521b\u5efa\u6709\u6548\u53ef\u89c6\u5316\uff0c\u5bfc\u81f4\u6b21\u4f18\u89c1\u89e3\u5e76\u5360\u7528\u4e13\u5bb6\u65f6\u95f4\u3002", "method": "\u63d0\u51fa\u8f6f\u4ef6\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3aLLM\uff0c\u96c6\u6210\u8bf7\u6c42\u5206\u7c7b\u5668\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u3001\u7f16\u7801\u4e13\u5bb6\u89c4\u5219\u548c\u53ef\u89c6\u5316\u8bbe\u8ba1\u539f\u5219\uff0c\u6784\u5efa\u5177\u6709\u81ea\u4e3b\u3001\u53cd\u5e94\u3001\u4e3b\u52a8\u548c\u793e\u4ea4\u884c\u4e3a\u7684\u4ee3\u7406\u3002", "result": "\u57285\u4e2a\u8de8\u5de5\u7a0b\u9886\u57df\u7684\u573a\u666f\u4e2d\uff0c\u4e0e12\u540d\u8bc4\u4f30\u8005\u6d4b\u8bd5\u663e\u793a\u8f93\u51fa\u8d28\u91cf\u63d0\u5347206%\uff0c\u4ee3\u7406\u5728\u6240\u6709\u6848\u4f8b\u4e2d\u8fbe\u5230\u4e13\u5bb6\u7ea7\u8bc4\u5206\uff0c\u800c\u57fa\u7ebf\u8868\u73b0\u8f83\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u4f18\u7684\u4ee3\u7801\u8d28\u91cf\u548c\u66f4\u4f4e\u65b9\u5dee\u3002", "conclusion": "\u8d21\u732e\u5305\u62ec\uff1a\u81ea\u52a8\u5316\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u53ef\u89c6\u5316\u751f\u6210\u7cfb\u7edf\uff0c\u4ee5\u53ca\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7cfb\u7edf\u5316\u6355\u83b7\u4eba\u7c7b\u9886\u57df\u77e5\u8bc6\u3001\u5c06\u9690\u6027\u4e13\u5bb6\u77e5\u8bc6\u7f16\u7801\u5230AI\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u975e\u4e13\u5bb6\u80fd\u5728\u4e13\u4e1a\u9886\u57df\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6210\u679c\u3002", "topic": "code agent"}}
{"id": "2601.15160", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15160", "abs": "https://arxiv.org/abs/2601.15160", "authors": ["Yuval Kansal", "Niraj K. Jha"], "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning", "comment": null, "summary": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u5728\u77ed\u8df3\u63a8\u7406\u8def\u5f84\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u5176\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u590d\u6742\u591a\u8df3\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u533b\u5b66\u9886\u57df\u663e\u8457\u8d85\u8d8aGPT-5.2\u7b49\u524d\u6cbf\u7cfb\u7edf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u7ed3\u6784\u5316\u63a8\u7406\u9886\u57df\u5df2\u8fbe\u5230\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u5728\u4e13\u4e1a\u79d1\u5b66\u9886\u57df\u8fdb\u884c\u7ec4\u5408\u5f0f\u591a\u8df3\u63a8\u7406\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u6a21\u578b\u57fa\u4e8e\u9886\u57df\u516c\u7406\u4e8b\u5b9e\u8fdb\u884c\u7ec4\u5408\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u5e95\u5411\u4e0a\u7684\u5b66\u4e60\u8303\u5f0f\uff1a1) \u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff1b2) \u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u9690\u5f0f\u5956\u52b1\u6a21\u578b\uff1b3) \u4ece\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u63a8\u5bfc\u5956\u52b1\u4fe1\u53f7\uff0c\u9f13\u52b1\u6a21\u578b\u7ec4\u5408\u4e2d\u95f4\u516c\u7406\u800c\u975e\u4ec5\u4f18\u5316\u6700\u7ec8\u7b54\u6848\uff1b4) \u5728\u533b\u5b66\u9886\u57df\u8bad\u7ec314B\u6a21\u578b\uff0c\u4f7f\u7528\u77ed\u8df3\u63a8\u7406\u8def\u5f84(1-3\u8df3)\u8bad\u7ec3\uff0c\u8bc4\u4f30\u5176\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2(4-5\u8df3)\u4e0a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8def\u5f84\u63a8\u5bfc\u7684\u5956\u52b1\u4f5c\u4e3a\"\u7ec4\u5408\u6865\u6881\"\uff0c\u4f7f\u6a21\u578b\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u548cGPT-5.2\u3001Gemini 3 Pro\u7b49\u524d\u6cbf\u7cfb\u7edf\u3002\u65b9\u6cd5\u5bf9\u9009\u9879\u6d17\u724c\u538b\u529b\u6d4b\u8bd5\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u5c06\u63a8\u7406\u8fc7\u7a0b\u57fa\u4e8e\u7ed3\u6784\u5316\u77e5\u8bc6\u662f\u5b9e\u73b0\u667a\u80fd\u63a8\u7406\u7684\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u8def\u5f84\u3002\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u5956\u52b1\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15050", "abs": "https://arxiv.org/abs/2601.15050", "authors": ["Zhichao Yan", "Yunxiao Zhao", "Jiapu Wang", "Jiaoyan Chen", "Shaoru Guo", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "\\textsc{LogicScore}: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering", "comment": null, "summary": "Current evaluation methods for Attributed Question Answering (AQA) suffer from \\textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \\textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \\textit{Completeness} (logically sound deduction), \\textit{Conciseness} (non-redundancy), and \\textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore.", "AI": {"tldr": "LogicScore\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f52\u56e0\u95ee\u7b54\u4e2d\u7684\"\u5f52\u56e0\u77ed\u89c6\"\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u903b\u8f91\u5b8c\u6574\u6027\u8bc4\u4f30\u6765\u8865\u5145\u73b0\u6709\u7684\u5c40\u90e8\u5f52\u56e0\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u5f52\u56e0\u95ee\u7b54\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\"\u5f52\u56e0\u77ed\u89c6\"\u95ee\u9898\uff0c\u53ea\u5173\u6ce8\u5b64\u7acb\u9648\u8ff0\u7684\u9a8c\u8bc1\u800c\u5ffd\u89c6\u957f\u7b54\u6848\u7684\u5168\u5c40\u903b\u8f91\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4LLM\u4ea7\u751f\u4e8b\u5b9e\u6b63\u786e\u4f46\u903b\u8f91\u4e0d\u8fde\u8d2f\u7684\u56de\u7b54\u3002", "method": "\u57fa\u4e8eHorn\u89c4\u5219\u6784\u5efaLogicScore\u6846\u67b6\uff0c\u91c7\u7528\u540e\u5411\u9a8c\u8bc1\u673a\u5236\u7cfb\u7edf\u8bc4\u4f30\u4e09\u4e2a\u63a8\u7406\u7ef4\u5ea6\uff1a\u5b8c\u6574\u6027\uff08\u903b\u8f91\u63a8\u5bfc\uff09\u3001\u7b80\u6d01\u6027\uff08\u975e\u5197\u4f59\u6027\uff09\u548c\u786e\u5b9a\u6027\uff08\u4e00\u81f4\u7b54\u6848\u8574\u542b\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u8df3QA\u6570\u636e\u96c6\u548c20\u591a\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u9886\u5148\u6a21\u578b\u5728\u5f52\u56e0\u5f97\u5206\u4e0a\u8868\u73b0\u826f\u597d\uff08\u5982Gemini-3 Pro\u7cbe\u5ea692.85%\uff09\uff0c\u4f46\u5728\u5168\u5c40\u63a8\u7406\u8d28\u91cf\u4e0a\u8f83\u5dee\uff08\u5982Gemini-3 Pro\u7b80\u6d01\u6027\u4ec535.11%\uff09\u3002", "conclusion": "LogicScore\u4e3a\u903b\u8f91\u8bc4\u4f30\u5efa\u7acb\u4e86\u7a33\u5065\u6807\u51c6\uff0c\u5f3a\u8c03\u5728LLM\u5f00\u53d1\u4e2d\u9700\u8981\u540c\u65f6\u5173\u6ce8\u63a8\u7406\u8fde\u8d2f\u6027\u548c\u4e8b\u5b9e\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5168\u5c40\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u91cd\u8981\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2601.14716", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14716", "abs": "https://arxiv.org/abs/2601.14716", "authors": ["Yao Lu", "Dengdong Fan", "Jianzheng Nie", "Fan Xu", "Jie Chen", "Bin Zhou", "Yonghong Tian"], "title": "PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning", "comment": null, "summary": "We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.", "AI": {"tldr": "PCL-Reasoner-V1.5\u662f\u4e00\u4e2a\u57fa\u4e8eQwen2.5-32B\u6784\u5efa\u7684320\u4ebf\u53c2\u6570\u6570\u5b66\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u63d0\u51fa\u79bb\u7ebfRL\u65b9\u6cd5\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u5728AIME 2024/2025\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u5347LLM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8eQwen2.5-32B\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u91c7\u7528\u521b\u65b0\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bad\u7ec3\uff0c\u76f8\u6bd4\u6807\u51c6\u5728\u7ebfRL\u65b9\u6cd5\u63d0\u4f9b\u66f4\u597d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5728AIME 2024\u4e0a\u8fbe\u523090.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728AIME 2025\u4e0a\u8fbe\u523085.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728\u57fa\u4e8eQwen2.5-32B\u540e\u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6240\u6709\u5b9e\u9a8c\u5747\u5728\u534e\u4e3a\u6607\u817e910C NPU\u4e0a\u5b8c\u6210\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15077", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15077", "abs": "https://arxiv.org/abs/2601.15077", "authors": ["Christopher Scofield"], "title": "Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure", "comment": null, "summary": "Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.", "AI": {"tldr": "\u672c\u6587\u4ece\u7b97\u5b50\u7406\u8bba\u548c\u7ea6\u675f\u4f18\u5316\u7684\u89d2\u5ea6\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u5728\u76f8\u540c\u4fe1\u606f\u4e0b\u8868\u73b0\u66f4\u4f18\u7684\u73b0\u8c61\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u89e3\u91ca\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7531\u5927\u8bed\u8a00\u6a21\u578b\u7ec4\u6210\uff0c\u5c3d\u7ba1\u5904\u7406\u76f8\u540c\u4fe1\u606f\uff0c\u5374\u5e38\u5e38\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u4e3a\u8fd9\u4e00\u73b0\u8c61\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u5c06\u6bcf\u4e2a\u667a\u80fd\u4f53\u5efa\u6a21\u4e3a\u5bf9\u5171\u4eab\u89e3\u72b6\u6001\u65bd\u52a0\u4e0d\u540c\u6709\u6548\u6027\u7ea6\u675f\u7684\u7b97\u5b50\uff0c\u8bc1\u660eMAS\u5b9e\u73b0\u4e86\u7ea6\u675f\u6267\u884c\u7b97\u5b50\u7684\u5206\u89e3\u7ec4\u5408\u3002\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\uff0c\u8fd9\u4e9b\u52a8\u6001\u6536\u655b\u5230\u7531\u667a\u80fd\u4f53\u7ea6\u675f\u96c6\u4ea4\u96c6\u5b9a\u4e49\u7684\u89e3\u96c6\u3002", "result": "\u8fd9\u79cd\u4e0d\u53d8\u7ed3\u6784\u901a\u5e38\u65e0\u6cd5\u88ab\u5355\u4e2a\u667a\u80fd\u4f53\u540c\u65f6\u5e94\u7528\u6240\u6709\u7ea6\u675f\u65f6\u52a8\u6001\u8bbf\u95ee\uff0c\u5373\u4f7f\u8868\u8fbe\u80fd\u529b\u548c\u4fe1\u606f\u76f8\u540c\u3002\u5c06\u7ed3\u679c\u4ece\u7cbe\u786e\u7ea6\u675f\u6267\u884c\u6269\u5c55\u5230\u8f6f\u7ea6\u675f\uff08\u901a\u8fc7\u90bb\u8fd1\u7b97\u5b50\uff09\uff0c\u5e76\u5e94\u7528\u4e8e\u5f53\u4ee3\u57fa\u4e8e\u6587\u672c\u7684\u5bf9\u8bdd\u7cfb\u7edf\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4f18\u52bf\u6e90\u4e8e\u7ea6\u675f\u7684\u5206\u89e3\u6267\u884c\uff0c\u4f7f\u5f97\u6536\u655b\u5230\u66f4\u4f18\u89e3\u96c6\u6210\u4e3a\u53ef\u80fd\uff0c\u8fd9\u4e3a\u7406\u89e3MAS\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.15161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15161", "abs": "https://arxiv.org/abs/2601.15161", "authors": ["Yinzhu Chen", "Abdine Maiga", "Hossein A. Rahmani", "Emine Yilmaz"], "title": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($\u03bc_\u0394 = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.", "AI": {"tldr": "\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u9488\u5bf9\u4e34\u5e8a\u51b3\u7b56\u652f\u6301LLM\u7684\u5b9e\u4f8b\u7279\u5b9a\u8bc4\u4f30\u51c6\u5219\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf", "motivation": "LLM\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u5b89\u5168\u5efa\u8bae\u7684\u98ce\u9669\uff0c\u8fd9\u4e9b\u98ce\u9669\u8868\u73b0\u4e3a\u96be\u4ee5\u88ab\u901a\u7528\u6307\u6807\u68c0\u6d4b\u7684\u7ec6\u5fae\u4e34\u5e8a\u9519\u8bef\uff0c\u800c\u4e13\u5bb6\u7f16\u5199\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u51c6\u5219\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4ece\u6743\u5a01\u533b\u5b66\u8bc1\u636e\u4e2d\u68c0\u7d22\u5185\u5bb9\uff0c\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\uff0c\u7ed3\u5408\u7528\u6237\u4ea4\u4e92\u7ea6\u675f\u5408\u6210\u53ef\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u51c6\u5219", "result": "\u5728HealthBench\u4e0a\u8fbe\u523060.12%\u7684\u4e34\u5e8a\u610f\u56fe\u5bf9\u9f50\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\u57fa\u7ebf(55.16%)\uff1b\u8bc4\u4f30\u51c6\u5219\u5728\u5224\u522b\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5206\u6570\u5dee\u4e3a8.658\uff0cAUROC\u4e3a0.977\uff1b\u8fd8\u80fd\u6307\u5bfc\u54cd\u5e94\u4f18\u5316\uff0c\u5c06\u8d28\u91cf\u63d0\u53479.2%", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u533b\u5b66LLM\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u57fa\u7840\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\u51c6\u5219\uff0c\u6709\u6548\u68c0\u6d4b\u4e34\u5e8a\u9519\u8bef\u5e76\u6307\u5bfc\u6a21\u578b\u6539\u8fdb", "topic": "agent analysis"}}
{"id": "2601.15165", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15165", "abs": "https://arxiv.org/abs/2601.15165", "authors": ["Zanlin Ni", "Shenzhi Wang", "Yang Yue", "Tianyu Yu", "Weilin Zhao", "Yeguo Hua", "Tianyi Chen", "Jun Song", "Cheng Yu", "Bo Zheng", "Gao Huang"], "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "comment": "Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "AI": {"tldr": "dLLMs\u7684\u4efb\u610f\u987a\u5e8f\u751f\u6210\u80fd\u529b\u53cd\u800c\u9650\u5236\u4e86\u5176\u63a8\u7406\u8fb9\u754c\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u8fc7\u65e9\u574d\u7f29\u3002\u653e\u5f03\u4efb\u610f\u987a\u5e8f\u751f\u6210\uff0c\u91c7\u7528\u6807\u51c6GRPO\u65b9\u6cd5\u53cd\u800c\u80fd\u66f4\u6709\u6548\u5730\u6fc0\u53d1dLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8ba4\u4e3adLLMs\u7684\u4efb\u610f\u987a\u5e8f\u751f\u6210\u80fd\u529b\u7406\u8bba\u4e0a\u80fd\u63d0\u4f9b\u66f4\u5927\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\uff0c\u4ece\u800c\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u4efb\u52a1\u4e0a\u5177\u6709\u4f18\u52bf\u3002\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u7075\u6d3b\u6027\u5b9e\u9645\u4e0a\u9650\u5236\u4e86dLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faJustGRPO\u65b9\u6cd5\uff0c\u653e\u5f03dLLMs\u7684\u4efb\u610f\u987a\u5e8f\u751f\u6210\u80fd\u529b\uff0c\u91c7\u7528\u6807\u51c6\u7684Group Relative Policy Optimization\uff08GRPO\uff09\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u7559dLLMs\u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\u3002", "result": "JustGRPO\u65b9\u6cd5\u5728GSM8K\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523089.1%\u7684\u51c6\u786e\u7387\uff0c\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u4efb\u610f\u987a\u5e8f\u751f\u6210\u7684RL\u65b9\u6cd5\u3002", "conclusion": "dLLMs\u7684\u4efb\u610f\u987a\u5e8f\u751f\u6210\u80fd\u529b\u5728\u5f53\u524d\u5f62\u5f0f\u4e0b\u53cd\u800c\u9650\u5236\u4e86\u5176\u63a8\u7406\u6f5c\u529b\uff0c\u653e\u5f03\u8fd9\u79cd\u7075\u6d3b\u6027\u5e76\u91c7\u7528\u6807\u51c6GRPO\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u6fc0\u53d1dLLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15220", "abs": "https://arxiv.org/abs/2601.15220", "authors": ["Anmol Goel", "Cornelius Emde", "Sangdoo Yun", "Seong Joon Oh", "Martin Gubri"], "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models", "comment": null, "summary": "We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u5728\u826f\u6027\u5fae\u8c03\u540e\u4f1a\u51fa\u73b0\u9690\u79c1\u5d29\u6e83\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u4fdd\u6301\u6807\u51c6\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u57fa\u51c6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4e25\u91cd\u4e27\u5931\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002", "motivation": "\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u73b0\u8c61\uff1a\u5373\u4f7f\u662f\u826f\u6027\u7684\u5fae\u8c03\u4e5f\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u9000\u5316\uff0c\u800c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u68c0\u6d4b\u5230\u8fd9\u79cd\"\u9759\u9ed8\u5931\u8d25\"\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u516d\u4e2a\u6a21\u578b\uff08\u95ed\u6e90\u548c\u5f00\u6e90\uff09\u3001\u4e94\u4e2a\u5fae\u8c03\u6570\u636e\u96c6\uff08\u771f\u5b9e\u4e16\u754c\u548c\u53d7\u63a7\u6570\u636e\uff09\u548c\u4e24\u7c7b\u4efb\u52a1\uff08\u4ee3\u7406\u578b\u548c\u8bb0\u5fc6\u578b\uff09\uff0c\u5206\u6790\u591a\u79cd\u5fae\u8c03\u6a21\u5f0f\u5bf9\u9690\u79c1\u4fdd\u62a4\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u673a\u5236\u5206\u6790\u3002", "result": "\u53d1\u73b0\u9690\u79c1\u5d29\u6e83\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5931\u53bb\u4e0a\u4e0b\u6587\u9690\u79c1\u89c4\u8303\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u5f53\u5171\u4eab\u4fe1\u606f\u7ed9\u5de5\u5177\uff0c\u8de8\u4e0a\u4e0b\u6587\u8fdd\u53cd\u5185\u5b58\u8fb9\u754c\uff0c\u800c\u9690\u79c1\u8868\u5f81\u76f8\u6bd4\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u66f4\u52a0\u8106\u5f31\u3002", "conclusion": "\u5f53\u524d\u5b89\u5168\u8bc4\u4f30\u5b58\u5728\u5173\u952e\u7f3a\u53e3\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e13\u7528\u4ee3\u7406\u7684\u90e8\u7f72\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u9632\u6b62\u9690\u79c1\u5d29\u6e83\u3002", "topic": "agent analysis"}}
{"id": "2601.14957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14957", "abs": "https://arxiv.org/abs/2601.14957", "authors": ["Harry Mead", "Bruno Lacerda", "Jakob Foerster", "Nick Hawes"], "title": "Improving Regret Approximation for Unsupervised Dynamic Environment Generation", "comment": null, "summary": "Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.", "AI": {"tldr": "\u63d0\u51faDEGen\u65b9\u6cd5\u6539\u5584\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7\u52a8\u6001\u73af\u5883\u751f\u6210\u63d0\u4f9b\u66f4\u5bc6\u96c6\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5f15\u5165MNA\u9057\u61be\u8fd1\u4f3c\u6765\u66f4\u597d\u8bc6\u522b\u6311\u6218\u6027\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\u65b9\u6cd5\u5728\u73af\u5883\u53c2\u6570\u7a7a\u95f4\u8f83\u5927\u65f6\u9762\u4e34\u4fe1\u7528\u5206\u914d\u56f0\u96be\uff0c\u96be\u4ee5\u8bc6\u522b\u771f\u6b63\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faDEGen\uff08\u52a8\u6001\u73af\u5883\u751f\u6210\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u5bc6\u96c6\u7684\u751f\u6210\u5668\u5956\u52b1\u4fe1\u53f7\u6539\u5584\u4fe1\u7528\u5206\u914d\uff1b\u5f15\u5165MNA\uff08\u6700\u5927\u5316\u8d1f\u4f18\u52bf\uff09\u4f5c\u4e3a\u65b0\u7684\u9057\u61be\u8fd1\u4f3c\u6307\u6807\uff0c\u80fd\u66f4\u51c6\u786e\u8bc6\u522b\u6311\u6218\u6027\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMNA\u4f18\u4e8e\u73b0\u6709\u9057\u61be\u8fd1\u4f3c\u65b9\u6cd5\uff0cDEGen\u4e0eMNA\u7ed3\u5408\u540e\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u5f02\u3002", "conclusion": "DEGen\u548cMNA\u7684\u7ec4\u5408\u6709\u6548\u89e3\u51b3\u4e86UED\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u548c\u6311\u6218\u6027\u73af\u5883\u8bc6\u522b\u95ee\u9898\uff0c\u4f7f\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\u80fd\u591f\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u7684\u73af\u5883\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15086", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15086", "abs": "https://arxiv.org/abs/2601.15086", "authors": ["Oleg Shchendrigin", "Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning", "comment": "11 pages, 6 figures, 7 tables", "summary": "Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8bb0\u5fc6\u91cd\u5199\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u7ecf\u5178\u5faa\u73af\u6a21\u578b\u5728\u73b0\u4ee3\u7ed3\u6784\u5316\u8bb0\u5fc6\u548c\u57fa\u4e8eTransformer\u7684\u667a\u80fd\u4f53\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u5e73\u8861\u8bb0\u5fc6\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6709\u6548\u51b3\u7b56\u9700\u8981\u8bb0\u5fc6\u65e2\u7a33\u5b9a\u53c8\u9002\u5e94\u6027\u5f3a\uff1a\u73af\u5883\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u667a\u80fd\u4f53\u5fc5\u987b\u5728\u957f\u65f6\u95f4\u5185\u4fdd\u7559\u76f8\u5173\u4fe1\u606f\uff0c\u540c\u65f6\u5728\u60c5\u51b5\u53d8\u5316\u65f6\u66f4\u65b0\u6216\u8986\u76d6\u8fc7\u65f6\u5185\u5bb9\u3002\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u548c\u8bb0\u5fc6\u589e\u5f3a\u667a\u80fd\u4f53\u4e3b\u8981\u5173\u6ce8\u8bb0\u5fc6\u4fdd\u7559\uff0c\u800c\u8bb0\u5fc6\u91cd\u5199\u8fd9\u4e00\u540c\u6837\u5173\u952e\u7684\u80fd\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6\uff0c\u660e\u786e\u6d4b\u8bd5\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u6301\u7eed\u8bb0\u5fc6\u66f4\u65b0\uff08\u5373\u667a\u80fd\u4f53\u5fc5\u987b\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u5f53\u524d\u89c2\u5bdf\u7684\u81ea\u7136\u8bbe\u7f6e\uff09\uff0c\u5e76\u4f7f\u7528\u8be5\u57fa\u51c6\u6bd4\u8f83\u5faa\u73af\u3001\u57fa\u4e8eTransformer\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u5178\u5faa\u73af\u6a21\u578b\u5c3d\u7ba1\u7b80\u5355\uff0c\u4f46\u5728\u8bb0\u5fc6\u91cd\u5199\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u4ee3\u7ed3\u6784\u5316\u8bb0\u5fc6\u66f4\u5927\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540e\u8005\u4ec5\u5728\u72ed\u7a84\u6761\u4ef6\u4e0b\u6210\u529f\uff0c\u800c\u57fa\u4e8eTransformer\u7684\u667a\u80fd\u4f53\u901a\u5e38\u65e0\u6cd5\u8d85\u8d8a\u7b80\u5355\u7684\u8bb0\u5fc6\u4fdd\u7559\u6848\u4f8b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u57fa\u672c\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5e73\u8861\u7a33\u5b9a\u4fdd\u7559\u4e0e\u9002\u5e94\u6027\u66f4\u65b0\u7684\u8bb0\u5fc6\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002\u8be5\u5de5\u4f5c\u7a81\u51fa\u4e86\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u6311\u6218\uff0c\u5f15\u5165\u4e86\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u5177\u6709\u660e\u786e\u53ef\u8bad\u7ec3\u9057\u5fd8\u673a\u5236\u7684\u672a\u6765\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2601.15141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15141", "abs": "https://arxiv.org/abs/2601.15141", "authors": ["Tianshi Xu", "Yuteng Chen", "Meng Li"], "title": "CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning", "comment": null, "summary": "Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub", "AI": {"tldr": "CLEANER\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u5185\u5728\u81ea\u6821\u6b63\u80fd\u529b\u5728\u6570\u636e\u6536\u96c6\u9636\u6bb5\u76f4\u63a5\u51c0\u5316\u566a\u58f0\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u611f\u77e5\u81ea\u9002\u5e94\u56de\u6eda\u673a\u5236\u6784\u5efa\u6e05\u6d01\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u53c2\u6570\u53d7\u9650\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u53c2\u6570\u53d7\u9650\u6a21\u578b\uff084B-7B\uff09\u5728\u5de5\u5177\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u63a2\u7d22\u9636\u6bb5\u9891\u7e41\u6267\u884c\u5931\u8d25\u7684\u95ee\u9898\uff0c\u4ea7\u751f\u566a\u58f0\u8f68\u8ff9\u963b\u788d\u7b56\u7565\u4f18\u5316\u3002\u6807\u51c6\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u8bbe\u7f6e\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u9519\u8bef\u52a8\u4f5c\u4e0e\u6210\u529f\u7ed3\u679c\u540c\u65f6\u88ab\u5f3a\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5bc6\u96c6\u5956\u52b1\u5bfc\u81f4\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u6216\u8d85\u91c7\u6837\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u56f0\u5883\u3002", "method": "\u63d0\u51faCLEANER\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u76f8\u4f3c\u6027\u611f\u77e5\u81ea\u9002\u5e94\u56de\u6eda\uff08SAAR\uff09\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6a21\u578b\u5185\u5728\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u5728\u6570\u636e\u6536\u96c6\u9636\u6bb5\u76f4\u63a5\u6d88\u9664\u9519\u8bef\u6c61\u67d3\u4e0a\u4e0b\u6587\u3002SAAR\u901a\u8fc7\u56de\u987e\u6027\u66ff\u6362\u5931\u8d25\u6b65\u9aa4\u4e3a\u6210\u529f\u7684\u81ea\u6821\u6b63\uff0c\u81ea\u9002\u5e94\u5730\u6839\u636e\u8bed\u4e49\u76f8\u4f3c\u6027\u8c03\u8282\u66ff\u6362\u7c92\u5ea6\uff0c\u4ece\u6d45\u5c42\u6267\u884c\u4fee\u590d\u5230\u6df1\u5c42\u63a8\u7406\u66ff\u6362\u3002", "result": "\u5728AIME24/25\u3001GPQA\u548cLiveCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u63d0\u53476%\u30013%\u548c5%\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cCLEANER\u4ec5\u4f7f\u7528\u4e09\u5206\u4e4b\u4e00\u8bad\u7ec3\u6b65\u9aa4\u5c31\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8f68\u8ff9\u51c0\u5316\u662f\u9ad8\u6548\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bad\u7ec3\u81ea\u51c0\u5316\u8def\u5f84\uff0c\u6a21\u578b\u80fd\u591f\u5185\u5316\u6b63\u786e\u63a8\u7406\u6a21\u5f0f\u800c\u975e\u9519\u8bef\u6062\u590d\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u53c2\u6570\u53d7\u9650\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.15158", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15158", "abs": "https://arxiv.org/abs/2601.15158", "authors": ["Yuval Ran-Milo", "Yotam Alexander", "Shahar Mendel", "Nadav Cohen"], "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data", "comment": "80 pages, 4 figures", "summary": "Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of \"simple examples\": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Transformer\u6a21\u578b\u5728\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e0b\u5982\u4f55\u81ea\u53d1\u4ea7\u751f\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u6d41\u52a8\u6001\u8bc1\u660e\u4e86\u6a21\u578b\u4f1a\u6536\u655b\u5230\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u56fe\u904d\u5386\u7b97\u6cd5\uff0c\u5e76\u8bc6\u522b\u4e86\"\u7b80\u5355\u793a\u4f8b\"\u5728\u6cdb\u5316\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u80fd\u8ba9Transformer\u81ea\u53d1\u4ea7\u751f\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u4f46\u7a00\u758f\u5956\u52b1\u5982\u4f55\u9a71\u52a8\u68af\u5ea6\u4e0b\u964d\u53d1\u73b0\u8fd9\u79cd\u7cfb\u7edf\u6027\u63a8\u7406\u7684\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u68af\u5ea6\u6d41\u52a8\u6001\u6765\u7406\u89e3\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u5355\u5c42Transformer\u5728\u5408\u6210\u56fe\u904d\u5386\u4efb\u52a1\u4e0a\u8fdb\u884c\u5206\u6790\uff0c\u8be5\u4efb\u52a1\u9700\u8981\u601d\u7ef4\u94fe\u4f46\u5141\u8bb8\u7b80\u5355\u8fed\u4ee3\u89e3\u3002\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u68af\u5ea6\u6d41\u52a8\u6001\u5206\u6790\uff0c\u7814\u7a76\u8bad\u7ec3\u5206\u5e03\u7279\u6027\u5bf9\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u5c3d\u7ba1\u4ec5\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u8bad\u7ec3\uff0c\u68af\u5ea6\u6d41\u4f1a\u9a71\u52a8\u6a21\u578b\u6536\u655b\u5230\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u9010\u9876\u70b9\u904d\u5386\u7b97\u6cd5\u3002\u8bc6\u522b\u4e86\"\u7b80\u5355\u793a\u4f8b\"\u7684\u5173\u952e\u4f5c\u7528\uff1a\u5f53\u8bad\u7ec3\u5206\u5e03\u5305\u542b\u8db3\u591f\u7b80\u5355\u5b9e\u4f8b\u65f6\uff0c\u6a21\u578b\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u904d\u5386\u7b56\u7565\uff1b\u5426\u5219\u68af\u5ea6\u5b66\u4e60\u4e0d\u53ef\u884c\u3002", "conclusion": "\u7a00\u758f\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u81ea\u53d1\u4ea7\u751f\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u5173\u952e\u5728\u4e8e\u8bad\u7ec3\u5206\u5e03\u4e2d\u7b80\u5355\u5b9e\u4f8b\u7684\u5206\u5e03\u7279\u6027\u3002\u7406\u8bba\u53d1\u73b0\u5728\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u4e3a\u7406\u89e3Transformer\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.cbc0ebdd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.nvidia.com%2Fblog%2Fhow-to-train-an-ai-agent-for-command-line-tasks-with-synthetic-data-and-reinforcement-learning%2F%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/P7pBSdLqGL5ZQ3lClc6TXwGk_NwI3EuCxQLl1txR0CU=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.nvidia.com%2Fblog%2Fhow-to-train-an-ai-agent-for-command-line-tasks-with-synthetic-data-and-reinforcement-learning%2F%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/P7pBSdLqGL5ZQ3lClc6TXwGk_NwI3EuCxQLl1txR0CU=441", "authors": ["TLDR Newsletter"], "title": "Training CLI Agents with Synthetic Data and RL", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeveloper.nvidia.com%2Fblog%2Fhow-to-train-an-ai-agent-for-command-line-tasks-with-synthetic-data-and-reinforcement-learning%2F%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/P7pBSdLqGL5ZQ3lClc6TXwGk_NwI3EuCxQLl1txR0CU=441", "summary": "Training CLI Agents with Synthetic Data and RL (12 minute read) Nvidia showcases how a reasoning model with no prior knowledge can be taught to safely use the LangGraph CLI for complex tasks like spinning up servers and building Docker containers, using synthetic data, and Reinforcement Learning with Verifiable Rewards.", "source": "tldr", "AI": {"tldr": "Nvidia\u8bad\u7ec3\u4e86\u4e00\u4e2a\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5b89\u5168\u4f7f\u7528LangGraph CLI\u6267\u884c\u590d\u6742\u4efb\u52a1\uff08\u5982\u542f\u52a8\u670d\u52a1\u5668\u548c\u6784\u5efaDocker\u5bb9\u5668\uff09", "motivation": "\u5f53\u524dCLI\u4ee3\u7406\u9700\u8981\u5927\u91cf\u5148\u9a8c\u77e5\u8bc6\u6216\u4eba\u5de5\u76d1\u7763\u624d\u80fd\u5b89\u5168\u6267\u884c\u590d\u6742\u7cfb\u7edf\u64cd\u4f5c\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u3001\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u5c31\u80fd\u5b89\u5168\u4f7f\u7528CLI\u5de5\u5177\u7684\u667a\u80fd\u4ee3\u7406", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u5b89\u5168\u4f7f\u7528LangGraph CLI\u6267\u884c\u590d\u6742\u4efb\u52a1", "result": "\u6210\u529f\u8bad\u7ec3\u51fa\u80fd\u591f\u5b89\u5168\u6267\u884c\u590d\u6742CLI\u4efb\u52a1\u7684\u4ee3\u7406\uff0c\u5982\u542f\u52a8\u670d\u52a1\u5668\u548c\u6784\u5efaDocker\u5bb9\u5668\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u6216\u4eba\u5de5\u76d1\u7763", "conclusion": "\u5408\u6210\u6570\u636e\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u662f\u8bad\u7ec3\u5b89\u5168\u3001\u6709\u6548\u7684CLI\u4ee3\u7406\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u5316\u590d\u6742\u7cfb\u7edf\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.26f52bb3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=ai-secondary-jan20/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/3bXxtn_VOhVukV23FamFVHXpJdRERsZHPf-4zQWcdH4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=ai-secondary-jan20/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/3bXxtn_VOhVukV23FamFVHXpJdRERsZHPf-4zQWcdH4=441", "authors": ["TLDR Newsletter"], "title": "Got questions about the OWASP Top 10 for Agentic Applications?", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzenity.io%2Fresources%2Fwebinars%2Finside-the-owasp-top-10-for-agentic-applications%3Futm_source=referral%26utm_medium=sponsored%26utm_campaign=tldr%26utm_content=ai-secondary-jan20/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/3bXxtn_VOhVukV23FamFVHXpJdRERsZHPf-4zQWcdH4=441", "summary": "Got questions about the OWASP Top 10 for Agentic Applications? (Sponsor) Here's where you get answers: Zenity is hosting a live AMA on Wed. 1/28 with the security researchers (Chris Hughes, Steve Wilson, Michael Bargury, and Kayla Underkoffler) who helped write the industry's first peer-reviewed risk framework for autonomous AI agents -- including contributors who led entries on goal hijacking, tool misuse, and memory poisoning. Bring your hardest questions about securing agents in production...", "source": "tldr", "AI": {"tldr": "Zenit\u516c\u53f8\u4e3e\u529e\u5173\u4e8eOWASP Top 10 for Agentic Applications\u7684\u73b0\u573a\u95ee\u7b54\u6d3b\u52a8\uff0c\u9080\u8bf7\u53c2\u4e0e\u7f16\u5199\u9996\u4e2a\u81ea\u4e3bAI\u4ee3\u7406\u98ce\u9669\u6846\u67b6\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\u89e3\u7b54\u95ee\u9898", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u884c\u4e1a\u9996\u4e2a\u7ecf\u8fc7\u540c\u884c\u8bc4\u5ba1\u7684\u98ce\u9669\u6846\u67b6\u6765\u6307\u5bfc\u5b89\u5168\u5b9e\u8df5\uff0cOWASP Top 10 for Agentic Applications\u5e94\u8fd0\u800c\u751f", "method": "\u901a\u8fc7\u4e3e\u529e\u73b0\u573a\u95ee\u7b54\u6d3b\u52a8\uff08AMA\uff09\uff0c\u9080\u8bf7\u53c2\u4e0e\u7f16\u5199\u98ce\u9669\u6846\u67b6\u7684\u5b89\u5168\u7814\u7a76\u4eba\u5458\uff08Chris Hughes, Steve Wilson, Michael Bargury, Kayla Underkoffler\uff09\u76f4\u63a5\u56de\u7b54\u793e\u533a\u5173\u4e8e\u76ee\u6807\u52ab\u6301\u3001\u5de5\u5177\u6ee5\u7528\u3001\u5185\u5b58\u4e2d\u6bd2\u7b49\u5b89\u5168\u95ee\u9898\u7684\u7591\u95ee", "result": "\u63d0\u4f9b\u4e86\u884c\u4e1a\u9996\u4e2a\u9488\u5bf9\u81ea\u4e3bAI\u4ee3\u7406\u7684\u540c\u884c\u8bc4\u5ba1\u98ce\u9669\u6846\u67b6\uff0c\u6db5\u76d6\u4e86\u76ee\u6807\u52ab\u6301\u3001\u5de5\u5177\u6ee5\u7528\u3001\u5185\u5b58\u4e2d\u6bd2\u7b49\u5173\u952e\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u73b0\u573a\u4e92\u52a8\u65b9\u5f0f\u4fc3\u8fdb\u77e5\u8bc6\u4f20\u64ad", "conclusion": "\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5b89\u5168\u98ce\u9669\u9700\u8981\u4e13\u95e8\u7684\u6846\u67b6\u6765\u8bc6\u522b\u548c\u7f13\u89e3\uff0cOWASP Top 10 for Agentic Applications\u4e3a\u6b64\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u73b0\u573a\u95ee\u7b54\u6d3b\u52a8\u6709\u52a9\u4e8e\u63a8\u52a8\u5b89\u5168\u6700\u4f73\u5b9e\u8df5\u7684\u91c7\u7528", "topic": "agent analysis"}}
{"id": "tldr.2601.06120dab", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fmicrosoft%2Foptimind%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/QZE0Lrdmvg94h3MvpL1iJ4BQR9YaGliWySMrJfHkv7Y=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fmicrosoft%2Foptimind%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/QZE0Lrdmvg94h3MvpL1iJ4BQR9YaGliWySMrJfHkv7Y=441", "authors": ["TLDR Newsletter"], "title": "Natural Language to Solver-Ready Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-20, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhuggingface.co%2Fblog%2Fmicrosoft%2Foptimind%3Futm_source=tldrai/1/0100019bdbc47291-e2d4f56c-f804-4025-b2c6-b0aee0d0059e-000000/QZE0Lrdmvg94h3MvpL1iJ4BQR9YaGliWySMrJfHkv7Y=441", "summary": "Natural Language to Solver-Ready Code (4 minute read) Microsoft's OptiMind translates natural language optimization problems into mathematical formulations. It was designed to streamline the traditionally slow and expert-heavy modeling step in optimization workflows.", "source": "tldr", "AI": {"tldr": "OptiMind\u5c06\u81ea\u7136\u8bed\u8a00\u4f18\u5316\u95ee\u9898\u8f6c\u6362\u4e3a\u6570\u5b66\u516c\u5f0f\uff0c\u7b80\u5316\u4f20\u7edf\u7e41\u7410\u7684\u4e13\u5bb6\u5efa\u6a21\u8fc7\u7a0b", "motivation": "\u4f18\u5316\u5de5\u4f5c\u6d41\u4e2d\u7684\u5efa\u6a21\u6b65\u9aa4\u4f20\u7edf\u4e0a\u7f13\u6162\u4e14\u9700\u8981\u4e13\u5bb6\u53c2\u4e0e\uff0c\u9700\u8981\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u4f18\u5316\u95ee\u9898\u7ffb\u8bd1\u6210\u6c42\u89e3\u5668\u5c31\u7eea\u7684\u6570\u5b66\u4ee3\u7801", "result": "\u5f00\u53d1\u4e86OptiMind\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u6570\u5b66\u4f18\u5316\u6a21\u578b", "conclusion": "\u81ea\u7136\u8bed\u8a00\u5230\u6c42\u89e3\u5668\u4ee3\u7801\u7684\u8f6c\u6362\u53ef\u4ee5\u663e\u8457\u7b80\u5316\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b", "topic": "code agent"}}
{"id": "tldr.2601.28853f7e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/OqWCCeyX60yiAMPiwd-vG9zTSJR9RqWJRO8Jneuokno=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/OqWCCeyX60yiAMPiwd-vG9zTSJR9RqWJRO8Jneuokno=441", "authors": ["TLDR Newsletter"], "title": "Skills.sh", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/OqWCCeyX60yiAMPiwd-vG9zTSJR9RqWJRO8Jneuokno=441", "summary": "Skills.sh (Website) Skills are reusable capabilities for AI agents. This site contains an open agent skills ecosystem with a collection of skills anyone can install with a single command to enhance their agents with access to procedural knowledge. The site contains thousands of skills. The index is searchable and shows how many times each skill has been installed.", "source": "tldr", "AI": {"tldr": "Skills.sh\u662f\u4e00\u4e2a\u5f00\u653e\u7684AI\u4ee3\u7406\u6280\u80fd\u751f\u6001\u7cfb\u7edf\uff0c\u63d0\u4f9b\u6570\u5343\u4e2a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u5355\u4e00\u547d\u4ee4\u5b89\u88c5\u6765\u589e\u5f3a\u4ee3\u7406\u80fd\u529b", "motivation": "\u4e3aAI\u4ee3\u7406\u521b\u5efa\u53ef\u91cd\u7528\u80fd\u529b\u5e93\uff0c\u89e3\u51b3\u4ee3\u7406\u529f\u80fd\u6269\u5c55\u548c\u77e5\u8bc6\u83b7\u53d6\u7684\u6807\u51c6\u5316\u95ee\u9898\uff0c\u4fc3\u8fdb\u4ee3\u7406\u6280\u80fd\u7684\u5171\u4eab\u548c\u751f\u6001\u7cfb\u7edf\u53d1\u5c55", "method": "\u6784\u5efa\u6280\u80fd\u7d22\u5f15\u7f51\u7ad9\uff0c\u63d0\u4f9b\u53ef\u641c\u7d22\u7684\u6280\u80fd\u5e93\uff0c\u6bcf\u4e2a\u6280\u80fd\u5305\u542b\u5b89\u88c5\u7edf\u8ba1\uff0c\u652f\u6301\u4e00\u952e\u5b89\u88c5\u673a\u5236", "result": "\u521b\u5efa\u4e86\u5305\u542b\u6570\u5343\u4e2a\u6280\u80fd\u7684\u5f00\u653e\u751f\u6001\u7cfb\u7edf\uff0c\u63d0\u4f9b\u5b89\u88c5\u7edf\u8ba1\u548c\u641c\u7d22\u529f\u80fd\uff0c\u5b9e\u73b0\u4e86\u6280\u80fd\u7684\u53ef\u53d1\u73b0\u6027\u548c\u4fbf\u6377\u5b89\u88c5", "conclusion": "Skills.sh\u6210\u529f\u5efa\u7acb\u4e86AI\u4ee3\u7406\u6280\u80fd\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6280\u80fd\u683c\u5f0f\u548c\u5b89\u88c5\u673a\u5236\uff0c\u4fc3\u8fdb\u4e86\u4ee3\u7406\u80fd\u529b\u7684\u6269\u5c55\u548c\u793e\u533a\u534f\u4f5c", "topic": "code agent"}}
{"id": "tldr.2601.3ef6da7d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmuratbuffalo.blogspot.com%2F2026%2F01%2Fagentic-ai-and-mythical-agent-month.html%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/SOMSs0Ibe-aSwbnZQaOqy-oI4QX7lcl9dN6IsqvCkA4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmuratbuffalo.blogspot.com%2F2026%2F01%2Fagentic-ai-and-mythical-agent-month.html%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/SOMSs0Ibe-aSwbnZQaOqy-oI4QX7lcl9dN6IsqvCkA4=441", "authors": ["TLDR Newsletter"], "title": "Agentic AI and The Mythical Agent-Month", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmuratbuffalo.blogspot.com%2F2026%2F01%2Fagentic-ai-and-mythical-agent-month.html%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/SOMSs0Ibe-aSwbnZQaOqy-oI4QX7lcl9dN6IsqvCkA4=441", "summary": "Agentic AI and The Mythical Agent-Month (6 minute read) The concept of 'Scalable Agency', where a developer could theoretically spin up thousands of agents to complete tasks faster, relies on a flawed assumption that software engineering is an embarrassingly parallel task. Context loading is not the same as common knowledge, and reading tokens is not the same as understanding the causal chain of changes across a system. Adding manpower to a late software project makes it later. Multi-agent sy...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\"\u53ef\u6269\u5c55\u4ee3\u7406\"\u6982\u5ff5\uff0c\u6307\u51fa\u8f6f\u4ef6\u5de5\u7a0b\u4e0d\u662f\u53ef\u9ad8\u5ea6\u5e76\u884c\u5316\u7684\u4efb\u52a1\uff0c\u589e\u52a0\u4ee3\u7406\u6570\u91cf\u53cd\u800c\u53ef\u80fd\u964d\u4f4e\u6548\u7387", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u9886\u57df\u6d41\u884c\"\u53ef\u6269\u5c55\u4ee3\u7406\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u5f00\u53d1\u8005\u53ef\u4ee5\u542f\u52a8\u6570\u5343\u4e2a\u4ee3\u7406\u6765\u52a0\u901f\u4efb\u52a1\u5b8c\u6210\uff0c\u4f46\u8fd9\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u662f\u9ad8\u5ea6\u5e76\u884c\u4efb\u52a1\u7684\u9519\u8bef\u5047\u8bbe", "method": "\u901a\u8fc7\u5206\u6790\u8f6f\u4ef6\u5de5\u7a0b\u7684\u7279\u6027\u548c\u7c7b\u6bd4\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\"\u4eba\u6708\u795e\u8bdd\"\u6982\u5ff5\uff0c\u8bba\u8bc1\u4e0a\u4e0b\u6587\u52a0\u8f7d\u4e0d\u7b49\u4e8e\u5171\u540c\u77e5\u8bc6\uff0c\u8bfb\u53d6token\u4e0d\u7b49\u4e8e\u7406\u89e3\u7cfb\u7edf\u53d8\u66f4\u7684\u56e0\u679c\u94fe", "result": "\u63ed\u793a\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u8868\u660e\u7b80\u5355\u589e\u52a0\u4ee3\u7406\u6570\u91cf\u65e0\u6cd5\u7ebf\u6027\u63d0\u5347\u6548\u7387\uff0c\u53cd\u800c\u53ef\u80fd\u56e0\u534f\u8c03\u6210\u672c\u800c\u964d\u4f4e\u6548\u7387", "conclusion": "AI\u4ee3\u7406\u9886\u57df\u9700\u8981\u91cd\u65b0\u601d\u8003\"\u53ef\u6269\u5c55\u4ee3\u7406\"\u7684\u5047\u8bbe\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u4e0d\u662f\u53ef\u9ad8\u5ea6\u5e76\u884c\u5316\u7684\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u534f\u8c03\u673a\u5236\u548c\u4e0a\u4e0b\u6587\u7406\u89e3", "topic": "agent analysis"}}
{"id": "tldr.2601.a144f93a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/HO_40g00IohlEc50TjqfDCs2xnAxKO-lxyYbGPEM5lA=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/HO_40g00IohlEc50TjqfDCs2xnAxKO-lxyYbGPEM5lA=441", "authors": ["TLDR Newsletter"], "title": "The Tragedy of the Agentic Commons", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrnewsletter/1/0100019be04be402-12dda4bd-ad1d-41e2-b8a8-f6a660cd0a18-000000/HO_40g00IohlEc50TjqfDCs2xnAxKO-lxyYbGPEM5lA=441", "summary": "The Tragedy of the Agentic Commons (18 minute read) AI agents help facilitate better match quality through centralized mechanisms in 'thin' markets.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u901a\u8fc7\u96c6\u4e2d\u673a\u5236\u5728\"\u8584\"\u5e02\u573a\u4e2d\u6539\u5584\u5339\u914d\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\"\u4ee3\u7406\u516c\u5730\u60b2\u5267\"\u95ee\u9898", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u5728\u8584\u5e02\u573a\uff08\u53c2\u4e0e\u8005\u5c11\u3001\u4fe1\u606f\u6709\u9650\u7684\u5e02\u573a\uff09\u4e2d\u7684\u96c6\u4e2d\u5339\u914d\u673a\u5236\uff0c\u63a2\u8ba8\u5176\u6f5c\u5728\u7684\u516c\u5730\u60b2\u5267\u95ee\u9898", "method": "\u5206\u6790AI\u4ee3\u7406\u5728\u8584\u5e02\u573a\u4e2d\u4f5c\u4e3a\u96c6\u4e2d\u5339\u914d\u673a\u5236\u7684\u4f5c\u7528\uff0c\u7814\u7a76\u5176\u5982\u4f55\u5f71\u54cd\u5e02\u573a\u6548\u7387\u548c\u8d44\u6e90\u5206\u914d", "result": "AI\u4ee3\u7406\u80fd\u6539\u5584\u8584\u5e02\u573a\u7684\u5339\u914d\u8d28\u91cf\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u7c7b\u4f3c\"\u516c\u5730\u60b2\u5267\"\u7684\u8d44\u6e90\u8fc7\u5ea6\u4f7f\u7528\u548c\u6548\u7387\u635f\u5931\u95ee\u9898", "conclusion": "\u9700\u8981\u5e73\u8861AI\u4ee3\u7406\u5728\u8584\u5e02\u573a\u4e2d\u7684\u96c6\u4e2d\u5339\u914d\u4f18\u52bf\u4e0e\u6f5c\u5728\u7684\u8d44\u6e90\u6ee5\u7528\u98ce\u9669\uff0c\u8bbe\u8ba1\u9002\u5f53\u7684\u76d1\u7ba1\u673a\u5236", "topic": "agent analysis"}}
{"id": "tldr.2601.90fede9c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.amplifypartners.com%2Fblog-posts%2Fhow-hightouch-built-their-long-running-agent-harness%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/GVfBKPMIMXw8U4C7vV-mxbvASgS4YVuu2MCf0Mf0b6g=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.amplifypartners.com%2Fblog-posts%2Fhow-hightouch-built-their-long-running-agent-harness%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/GVfBKPMIMXw8U4C7vV-mxbvASgS4YVuu2MCf0Mf0b6g=441", "authors": ["TLDR Newsletter"], "title": "How Hightouch built their long-running agent harness", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.amplifypartners.com%2Fblog-posts%2Fhow-hightouch-built-their-long-running-agent-harness%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/GVfBKPMIMXw8U4C7vV-mxbvASgS4YVuu2MCf0Mf0b6g=441", "summary": "How Hightouch built their long-running agent harness (15 minute read) Hightouch developed an AI marketing agent capable of complex, multi-step tasks. Its team explicitly separated the model's planning from its execution, allowing for dynamic plan updates as the agent learns from data. To manage context, they used \"agentic delegation\" through techniques like buffering large data to a temporary filesystem and spawning dynamic subagents for isolated, complex sub-tasks.", "source": "tldr", "AI": {"tldr": "Hightouch\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u7684AI\u8425\u9500\u4ee3\u7406\uff0c\u901a\u8fc7\u5c06\u89c4\u5212\u4e0e\u6267\u884c\u5206\u79bb\u5b9e\u73b0\u52a8\u6001\u8ba1\u5212\u66f4\u65b0\uff0c\u5e76\u91c7\u7528\u4ee3\u7406\u59d4\u6258\u6280\u672f\u7ba1\u7406\u4e0a\u4e0b\u6587", "motivation": "\u6784\u5efa\u80fd\u591f\u5904\u7406\u590d\u6742\u3001\u957f\u671f\u8fd0\u884c\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u65b9\u9762\u7684\u5c40\u9650\u6027", "method": "1. \u660e\u786e\u5206\u79bb\u6a21\u578b\u7684\u89c4\u5212\u4e0e\u6267\u884c\u9636\u6bb5\uff1b2. \u4f7f\u7528\"\u4ee3\u7406\u59d4\u6258\"\u6280\u672f\u7ba1\u7406\u4e0a\u4e0b\u6587\uff0c\u5305\u62ec\u5c06\u5927\u6570\u636e\u7f13\u51b2\u5230\u4e34\u65f6\u6587\u4ef6\u7cfb\u7edf\uff1b3. \u4e3a\u590d\u6742\u5b50\u4efb\u52a1\u52a8\u6001\u751f\u6210\u9694\u79bb\u7684\u5b50\u4ee3\u7406", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u591f\u5904\u7406\u590d\u6742\u591a\u6b65\u9aa4\u8425\u9500\u4efb\u52a1\u7684\u957f\u671f\u8fd0\u884cAI\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u8ba1\u5212\u66f4\u65b0\u548c\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406", "conclusion": "\u901a\u8fc7\u89c4\u5212\u4e0e\u6267\u884c\u5206\u79bb\u4ee5\u53ca\u4ee3\u7406\u59d4\u6258\u67b6\u6784\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u7684\u957f\u671f\u8fd0\u884cAI\u4ee3\u7406\u7cfb\u7edf", "topic": "code agent"}}
{"id": "tldr.2601.497c02b1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmastra.ai%2Fblog%2Fannouncing-mastra-1%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/Nk0F_TWpyIbrs9ePwBU0a1g4kYaYZ1t0qDCY-No3UBw=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmastra.ai%2Fblog%2Fannouncing-mastra-1%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/Nk0F_TWpyIbrs9ePwBU0a1g4kYaYZ1t0qDCY-No3UBw=441", "authors": ["TLDR Newsletter"], "title": "Announcing Mastra 1.0!", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmastra.ai%2Fblog%2Fannouncing-mastra-1%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/Nk0F_TWpyIbrs9ePwBU0a1g4kYaYZ1t0qDCY-No3UBw=441", "summary": "Announcing Mastra 1.0! (7 minute read) Mastra, an AI agent framework for TypeScript, has officially announced the stable release of version 1.0. This version includes Server Adapters for easier deployment, Composite Storage for flexible backend configurations, and full support for Vercel's AI SDK v6.", "source": "tldr", "AI": {"tldr": "Mastra 1.0\u662f\u4e00\u4e2aTypeScript AI\u4ee3\u7406\u6846\u67b6\u7684\u7a33\u5b9a\u7248\u672c\u53d1\u5e03\uff0c\u5305\u542b\u670d\u52a1\u5668\u9002\u914d\u5668\u3001\u590d\u5408\u5b58\u50a8\u548c\u5bf9Vercel AI SDK v6\u7684\u5b8c\u6574\u652f\u6301", "motivation": "\u4e3aTypeScript\u5f00\u53d1\u8005\u63d0\u4f9b\u4e00\u4e2a\u7a33\u5b9a\u3001\u6613\u90e8\u7f72\u7684AI\u4ee3\u7406\u6846\u67b6\uff0c\u7b80\u5316AI\u5e94\u7528\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u6d41\u7a0b", "method": "\u53d1\u5e03Mastra 1.0\u7248\u672c\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u529f\u80fd\uff1a\u670d\u52a1\u5668\u9002\u914d\u5668\u7b80\u5316\u90e8\u7f72\u3001\u590d\u5408\u5b58\u50a8\u63d0\u4f9b\u7075\u6d3b\u7684\u540e\u7aef\u914d\u7f6e\u3001\u5b8c\u6574\u652f\u6301Vercel AI SDK v6", "result": "\u6210\u529f\u53d1\u5e03\u4e86Mastra 1.0\u7a33\u5b9a\u7248\u672c\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u529f\u80fd\u5b8c\u6574\u7684TypeScript AI\u4ee3\u7406\u6846\u67b6", "conclusion": "Mastra 1.0\u7684\u53d1\u5e03\u6807\u5fd7\u7740\u8be5\u6846\u67b6\u8fdb\u5165\u7a33\u5b9a\u9636\u6bb5\uff0c\u4e3aTypeScript AI\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5de5\u5177\u652f\u6301", "topic": "code agent"}}
{"id": "tldr.2601.b1d30154", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarllorey.com%2Fposts%2Fwithout-benchmarking-llms-youre-overpaying%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/OfvvcckHS6nsBb1syOJwBkyItVVGLSOaMbrwohfwAvM=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarllorey.com%2Fposts%2Fwithout-benchmarking-llms-youre-overpaying%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/OfvvcckHS6nsBb1syOJwBkyItVVGLSOaMbrwohfwAvM=441", "authors": ["TLDR Newsletter"], "title": "Without Benchmarking LLMs, You're Likely Overpaying 5-10x", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkarllorey.com%2Fposts%2Fwithout-benchmarking-llms-youre-overpaying%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/OfvvcckHS6nsBb1syOJwBkyItVVGLSOaMbrwohfwAvM=441", "summary": "Without Benchmarking LLMs, You're Likely Overpaying 5-10x (9 minute read) Many users overpay for LLM API calls because common benchmarks don't accurately predict performance or cost-effectiveness for their specific tasks, leading them to default to popular but expensive models. This article goes over a benchmarking methodology that involves collecting real prompts, defining expected outputs, running numerous LLMs via a unified API, and using an LLM-as-judge to score responses based on quality...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u9884\u6d4bLLM\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\uff0c\u5bfc\u81f4\u7528\u6237\u8fc7\u5ea6\u652f\u4ed85-10\u500d\u8d39\u7528\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u771f\u5b9e\u63d0\u793a\u6536\u96c6\u3001\u5b9a\u4e49\u671f\u671b\u8f93\u51fa\u3001\u7edf\u4e00API\u6d4b\u8bd5\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7528\u6237\u666e\u904d\u8fc7\u5ea6\u652f\u4ed8LLM API\u8c03\u7528\u8d39\u7528\uff0c\u56e0\u4e3a\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u7279\u5b9a\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\uff0c\u5bfc\u81f4\u7528\u6237\u9ed8\u8ba4\u9009\u62e9\u6d41\u884c\u4f46\u6602\u8d35\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff1a1) \u6536\u96c6\u771f\u5b9e\u4efb\u52a1\u63d0\u793a\uff1b2) \u5b9a\u4e49\u671f\u671b\u8f93\u51fa\u6807\u51c6\uff1b3) \u901a\u8fc7\u7edf\u4e00API\u8fd0\u884c\u591a\u4e2aLLM\uff1b4) \u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u6839\u636e\u8d28\u91cf\u8bc4\u5206\u54cd\u5e94\u3002", "result": "\u901a\u8fc7\u8be5\u65b9\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u4e0d\u540cLLM\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6548\u76ca\uff0c\u5e2e\u52a9\u7528\u6237\u907f\u514d\u8fc7\u5ea6\u652f\u4ed85-10\u500d\u8d39\u7528\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9\u5177\u4f53\u4efb\u52a1\u8fdb\u884c\u5b9a\u5236\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u975e\u4f9d\u8d56\u901a\u7528\u57fa\u51c6\uff0c\u624d\u80fd\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u6700\u4f18\u7684LLM\u9009\u62e9\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.e34394f5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fai-agents-at-work-real-time-platform-insights-in-slack%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/eYKCMpOGFo8LcmameANboYAWV7KRwF0lbKOOtgCoA34=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fai-agents-at-work-real-time-platform-insights-in-slack%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/eYKCMpOGFo8LcmameANboYAWV7KRwF0lbKOOtgCoA34=441", "authors": ["TLDR Newsletter"], "title": "AI Agents at work: real-time platform insights in Slack", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fai-agents-at-work-real-time-platform-insights-in-slack%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/eYKCMpOGFo8LcmameANboYAWV7KRwF0lbKOOtgCoA34=441", "summary": "AI Agents at work: real-time platform insights in Slack (8 minute read) Monday.com developed an AI Slack bot to help devs quickly access real-time platform insights without having to manually check internal tools or dig through notifications. The bot uses LLM agents connected to their microservices data through an MCP. To prevent the AI from hallucinating data, they pre-computed statistics rather than letting the model calculate them.", "source": "tldr", "AI": {"tldr": "Monday.com\u5f00\u53d1\u4e86\u4e00\u4e2aAI Slack\u673a\u5668\u4eba\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u5feb\u901f\u83b7\u53d6\u5b9e\u65f6\u5e73\u53f0\u6d1e\u5bdf\uff0c\u65e0\u9700\u624b\u52a8\u68c0\u67e5\u5185\u90e8\u5de5\u5177\u6216\u7b5b\u9009\u901a\u77e5", "motivation": "\u5f00\u53d1\u8005\u9700\u8981\u5feb\u901f\u8bbf\u95ee\u5b9e\u65f6\u5e73\u53f0\u6d1e\u5bdf\uff0c\u4f46\u624b\u52a8\u68c0\u67e5\u5185\u90e8\u5de5\u5177\u548c\u7b5b\u9009\u901a\u77e5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u4fbf\u6377\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528LLM\u4ee3\u7406\u901a\u8fc7MCP\u8fde\u63a5\u5230\u5fae\u670d\u52a1\u6570\u636e\uff0c\u9884\u8ba1\u7b97\u7edf\u8ba1\u6570\u636e\u800c\u975e\u8ba9\u6a21\u578b\u8ba1\u7b97\uff0c\u4ee5\u9632\u6b62AI\u4ea7\u751f\u5e7b\u89c9\u6570\u636e", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI Slack\u673a\u5668\u4eba\uff0c\u80fd\u63d0\u4f9b\u5b9e\u65f6\u5e73\u53f0\u6d1e\u5bdf\uff0c\u51cf\u5c11\u4e86\u5f00\u53d1\u8005\u624b\u52a8\u68c0\u67e5\u5de5\u5177\u7684\u65f6\u95f4", "conclusion": "AI\u4ee3\u7406\u80fd\u6709\u6548\u63d0\u5347\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u6570\u636e\u53ef\u4ee5\u9632\u6b62AI\u5e7b\u89c9\uff0c\u5b9e\u65f6\u5e73\u53f0\u6d1e\u5bdf\u5de5\u5177\u5177\u6709\u5b9e\u7528\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2601.5368323c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.emilburzo.com%2F2026%2F01%2Frunning-claude-code-dangerously-safely%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PlT7FHZzxmxHBWAsc9-pZvRItSC_i1ztcC-9MrXxwT8=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.emilburzo.com%2F2026%2F01%2Frunning-claude-code-dangerously-safely%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PlT7FHZzxmxHBWAsc9-pZvRItSC_i1ztcC-9MrXxwT8=441", "authors": ["TLDR Newsletter"], "title": "Running Claude Code dangerously", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.emilburzo.com%2F2026%2F01%2Frunning-claude-code-dangerously-safely%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PlT7FHZzxmxHBWAsc9-pZvRItSC_i1ztcC-9MrXxwT8=441", "summary": "Running Claude Code dangerously (safely) (6 minute read) The author wanted to use Claude Code with the `--dangerously-skip-permissions` flag for an improved workflow, but needed a safe, isolated environment to prevent accidental damage to their host system. They initially considered Docker, but issues like Docker-in-Docker requiring privileged mode made it unsuitable for true sandboxing. They chose Vagrant to create a disposable virtual machine, which offers full isolation and a reproducible ...", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u4f7f\u7528Vagrant\u521b\u5efa\u9694\u79bb\u7684\u865a\u62df\u673a\u73af\u5883\uff0c\u5b89\u5168\u5730\u8fd0\u884c\u5e26\u5371\u9669\u6807\u5fd7\u7684Claude Code\uff0c\u907f\u514d\u5bf9\u4e3b\u673a\u7cfb\u7edf\u9020\u6210\u635f\u5bb3", "motivation": "\u4f5c\u8005\u5e0c\u671b\u4f7f\u7528Claude Code\u7684`--dangerously-skip-permissions`\u6807\u5fd7\u6765\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u9700\u8981\u4e00\u4e2a\u5b89\u5168\u3001\u9694\u79bb\u7684\u73af\u5883\u6765\u9632\u6b62\u610f\u5916\u635f\u574f\u4e3b\u673a\u7cfb\u7edf", "method": "\u4f5c\u8005\u653e\u5f03\u4e86Docker\u65b9\u6848\uff08\u56e0\u4e3aDocker-in-Docker\u9700\u8981\u7279\u6743\u6a21\u5f0f\uff09\uff0c\u9009\u62e9\u4f7f\u7528Vagrant\u521b\u5efa\u4e00\u6b21\u6027\u865a\u62df\u673a\uff0c\u63d0\u4f9b\u5b8c\u5168\u9694\u79bb\u548c\u53ef\u91cd\u73b0\u7684\u73af\u5883", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u5b89\u5168\u7684\u6c99\u7bb1\u73af\u5883\uff0c\u53ef\u4ee5\u5728\u5176\u4e2d\u5b89\u5168\u5730\u8fd0\u884c\u5e26\u5371\u9669\u6807\u5fd7\u7684Claude Code\uff0c\u800c\u4e0d\u4f1a\u5bf9\u4e3b\u673a\u7cfb\u7edf\u9020\u6210\u98ce\u9669", "conclusion": "Vagrant\u865a\u62df\u673a\u662f\u8fd0\u884c\u9700\u8981\u5371\u9669\u6743\u9650\u7684\u4ee3\u7801\u5de5\u5177\u7684\u5b89\u5168\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86Docker\u65e0\u6cd5\u63d0\u4f9b\u7684\u5b8c\u5168\u7cfb\u7edf\u9694\u79bb", "topic": "swe application"}}
{"id": "tldr.2601.3ba5396b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdavidbeesley%2Fclaude-chill%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/dmNRmOi1vUuPUsT0C8tB6M2oMjqxd1pOfGiI8Kte1so=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdavidbeesley%2Fclaude-chill%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/dmNRmOi1vUuPUsT0C8tB6M2oMjqxd1pOfGiI8Kte1so=441", "authors": ["TLDR Newsletter"], "title": "Claude Chill", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fdavidbeesley%2Fclaude-chill%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/dmNRmOi1vUuPUsT0C8tB6M2oMjqxd1pOfGiI8Kte1so=441", "summary": "Claude Chill (GitHub Repo) Claude Chill is a PTY proxy that tames Claude Code's massive terminal updates by rendering only screen differences, preserving history, and enabling lookback to prevent lag, flicker, and broken scrollback.", "source": "tldr", "AI": {"tldr": "Claude Chill\u662f\u4e00\u4e2aPTY\u4ee3\u7406\u5de5\u5177\uff0c\u901a\u8fc7\u4ec5\u6e32\u67d3\u5c4f\u5e55\u5dee\u5f02\u3001\u4fdd\u7559\u5386\u53f2\u8bb0\u5f55\u548c\u542f\u7528\u56de\u770b\u529f\u80fd\uff0c\u6765\u4f18\u5316Claude Code\u7684\u7ec8\u7aef\u8f93\u51fa\uff0c\u89e3\u51b3\u5ef6\u8fdf\u3001\u95ea\u70c1\u548c\u6eda\u52a8\u95ee\u9898", "motivation": "\u89e3\u51b3Claude Code\u5728\u7ec8\u7aef\u8f93\u51fa\u65f6\u4ea7\u751f\u7684\u5927\u91cf\u66f4\u65b0\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5305\u62ec\u5ef6\u8fdf\u3001\u5c4f\u5e55\u95ea\u70c1\u548c\u6eda\u52a8\u5386\u53f2\u635f\u574f\u7b49\u7528\u6237\u4f53\u9a8c\u95ee\u9898", "method": "\u5f00\u53d1\u4e00\u4e2aPTY\uff08\u4f2a\u7ec8\u7aef\uff09\u4ee3\u7406\uff0c\u91c7\u7528\u5dee\u5f02\u6e32\u67d3\u6280\u672f\uff0c\u53ea\u66f4\u65b0\u5c4f\u5e55\u53d8\u5316\u7684\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u7559\u5b8c\u6574\u7684\u7ec8\u7aef\u5386\u53f2\u8bb0\u5f55\uff0c\u5e76\u5b9e\u73b0\u56de\u770b\u529f\u80fd", "result": "\u6210\u529f\u51cf\u5c11\u4e86\u7ec8\u7aef\u8f93\u51fa\u7684\u5ef6\u8fdf\u548c\u95ea\u70c1\uff0c\u4fee\u590d\u4e86\u6eda\u52a8\u5386\u53f2\u635f\u574f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86Claude Code\u5728\u7ec8\u7aef\u4ea4\u4e92\u65f6\u7684\u7528\u6237\u4f53\u9a8c", "conclusion": "\u901a\u8fc7\u5dee\u5f02\u6e32\u67d3\u548c\u667a\u80fd\u5386\u53f2\u7ba1\u7406\uff0cClaude Chill\u6709\u6548\u4f18\u5316\u4e86Claude Code\u7684\u7ec8\u7aef\u6027\u80fd\uff0c\u4e3a\u4ee3\u7801\u4ee3\u7406\u5de5\u5177\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7ec8\u7aef\u4ea4\u4e92\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2601.64b3550c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fcode-review-the-bottleneck-ai-cant-ignore%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zgBU0m0pfcVvJw56KCIAWto3FmXrFCpqBskrTu0MvnI=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fcode-review-the-bottleneck-ai-cant-ignore%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zgBU0m0pfcVvJw56KCIAWto3FmXrFCpqBskrTu0MvnI=441", "authors": ["TLDR Newsletter"], "title": "Why AI coding tools shift the real bottleneck to review", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fcode-review-the-bottleneck-ai-cant-ignore%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zgBU0m0pfcVvJw56KCIAWto3FmXrFCpqBskrTu0MvnI=441", "summary": "Why AI coding tools shift the real bottleneck to review (8 minute read) Through testing identical features built manually versus with AI, this developer found that AI generates 2-6\u00d7 more code with extensive defensive patterns and edge-case handling, changing review questions from \"what's missing?\" to \"is all this complexity necessary?\"", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7801\u5de5\u5177\u751f\u62102-6\u500d\u66f4\u591a\u4ee3\u7801\uff0c\u5305\u542b\u5927\u91cf\u9632\u5fa1\u6027\u6a21\u5f0f\u548c\u8fb9\u754c\u60c5\u51b5\u5904\u7406\uff0c\u5c06\u4ee3\u7801\u5ba1\u67e5\u91cd\u70b9\u4ece\"\u7f3a\u5c11\u4ec0\u4e48\"\u8f6c\u53d8\u4e3a\"\u8fd9\u4e9b\u590d\u6742\u6027\u662f\u5426\u5fc5\u8981\"", "motivation": "\u63a2\u7d22AI\u7f16\u7801\u5de5\u5177\u5982\u4f55\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u7279\u522b\u662f\u5bf9\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5bf9\u6bd4\u4eba\u5de5\u7f16\u5199\u4e0eAI\u751f\u6210\u7684\u76f8\u540c\u529f\u80fd\uff0c\u7814\u7a76AI\u5de5\u5177\u5e26\u6765\u7684\u4ee3\u7801\u91cf\u589e\u52a0\u548c\u590d\u6742\u6027\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u76f8\u540c\u529f\u80fd\u7684\u4eba\u5de5\u7f16\u5199\u7248\u672c\u4e0eAI\u751f\u6210\u7248\u672c\uff0c\u5bf9\u6bd4\u4ee3\u7801\u91cf\u3001\u9632\u5fa1\u6027\u6a21\u5f0f\u4f7f\u7528\u3001\u8fb9\u754c\u60c5\u51b5\u5904\u7406\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "AI\u751f\u6210\u7684\u4ee3\u7801\u91cf\u662f\u4eba\u5de5\u7f16\u5199\u76842-6\u500d\uff0c\u5305\u542b\u5927\u91cf\u9632\u5fa1\u6027\u7f16\u7a0b\u6a21\u5f0f\u548c\u8fb9\u754c\u60c5\u51b5\u5904\u7406\uff0c\u5bfc\u81f4\u4ee3\u7801\u5ba1\u67e5\u95ee\u9898\u4ece\u5173\u6ce8\u529f\u80fd\u5b8c\u6574\u6027\u8f6c\u53d8\u4e3a\u8bc4\u4f30\u590d\u6742\u6027\u5fc5\u8981\u6027\u3002", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u5c06\u5f00\u53d1\u74f6\u9888\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u79fb\u5230\u4ee3\u7801\u5ba1\u67e5\u9636\u6bb5\uff0c\u5ba1\u67e5\u8005\u9700\u8981\u66f4\u591a\u5173\u6ce8\u4ee3\u7801\u590d\u6742\u6027\u548c\u5fc5\u8981\u6027\u8bc4\u4f30\uff0c\u800c\u975e\u4f20\u7edf\u7684\u95ee\u9898\u53d1\u73b0\u3002", "topic": "swe application"}}
{"id": "tldr.2601.8ff7983e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/pu34Mz7IDbytO4U468hQoR7-dH4ER2mk1I84qq4N1iM=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/pu34Mz7IDbytO4U468hQoR7-dH4ER2mk1I84qq4N1iM=441", "authors": ["TLDR Newsletter"], "title": "The Agent Skills Directory", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fskills.sh%2F%3Futm_source=tldrdev/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/pu34Mz7IDbytO4U468hQoR7-dH4ER2mk1I84qq4N1iM=441", "summary": "The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.", "source": "tldr", "AI": {"tldr": "Agent Skills Directory\u662f\u4e00\u4e2a\u7f51\u7ad9\uff0c\u6536\u96c6\u4e86\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u80fd\u529b\uff08\u6280\u80fd\uff09\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u66f4\u6709\u6548\u5730\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1", "motivation": "AI\u4ee3\u7406\u5728\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u65f6\u9700\u8981\u4e13\u95e8\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6280\u80fd\u5171\u4eab\u548c\u590d\u7528\u673a\u5236\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u91cd\u590d\u5f00\u53d1", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7f51\u7ad9\u76ee\u5f55\uff0c\u6536\u96c6\u548c\u7ec4\u7ec7\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff0c\u8fd9\u4e9b\u6280\u80fd\u63d0\u4f9b\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u5b8c\u6210\u4efb\u52a1", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u6700\u6709\u7528\u7684AI\u4ee3\u7406\u6280\u80fd\u7684\u5728\u7ebf\u76ee\u5f55\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u67e5\u627e\u548c\u590d\u7528\u73b0\u6709\u6280\u80fd\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387", "conclusion": "Agent Skills Directory\u901a\u8fc7\u63d0\u4f9b\u53ef\u91cd\u7528\u7684\u6280\u80fd\u5e93\uff0c\u4fc3\u8fdb\u4e86AI\u4ee3\u7406\u80fd\u529b\u7684\u5171\u4eab\u548c\u590d\u7528\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u6548\u679c", "topic": "agent analysis"}}
{"id": "tldr.2601.8c23988c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PuACvY6kdwb1lnf3KLpwjTegkRPd0zW_Hvyk2aNrHNM=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PuACvY6kdwb1lnf3KLpwjTegkRPd0zW_Hvyk2aNrHNM=441", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/PuACvY6kdwb1lnf3KLpwjTegkRPd0zW_Hvyk2aNrHNM=441", "summary": "The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.", "source": "tldr", "AI": {"tldr": "Agent Skills Directory\u662f\u4e00\u4e2a\u7f51\u7ad9\uff0c\u6536\u96c6\u4e86\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u66f4\u6709\u6548\u5730\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1", "motivation": "AI\u4ee3\u7406\u9700\u8981\u7279\u5b9a\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u6765\u5b8c\u6210\u5404\u79cd\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u53ef\u91cd\u7528\u7684\u6280\u80fd\u5e93\u3002\u8fd9\u4e2a\u7f51\u7ad9\u65e8\u5728\u6536\u96c6\u548c\u7ec4\u7ec7\u6700\u6709\u7528\u7684\u6280\u80fd\uff0c\u5e2e\u52a9\u4ee3\u7406\u66f4\u6709\u6548\u5730\u5de5\u4f5c", "method": "\u521b\u5efa\u4e00\u4e2a\u7f51\u7ad9\u76ee\u5f55\uff0c\u6536\u96c6\u548c\u7ec4\u7ec7\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff08Skills\uff09\uff0c\u8fd9\u4e9b\u6280\u80fd\u63d0\u4f9b\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff0c\u53ef\u4ee5\u88abAI\u4ee3\u7406\u8c03\u7528\u548c\u4f7f\u7528", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u6700\u6709\u7528\u6280\u80fd\u7684\u76ee\u5f55\u7f51\u7ad9\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u7684\u80fd\u529b\u5e93\uff0c\u5e2e\u52a9\u5b83\u4eec\u66f4\u6709\u6548\u5730\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1", "conclusion": "Agent Skills Directory\u901a\u8fc7\u63d0\u4f9b\u53ef\u91cd\u7528\u7684\u6280\u80fd\u5e93\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u4efb\u52a1\u6267\u884c\u6548\u7387\u548c\u80fd\u529b\uff0c\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u5e73\u53f0", "topic": "agent analysis"}}
{"id": "tldr.2601.139bbb69", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zkl9-0QFoibweHJMoLVmN5OAqNP8MxocNbYi2zUbdeo=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zkl9-0QFoibweHJMoLVmN5OAqNP8MxocNbYi2zUbdeo=441", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/zkl9-0QFoibweHJMoLVmN5OAqNP8MxocNbYi2zUbdeo=441", "summary": "The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.", "source": "tldr", "AI": {"tldr": "Agent Skills Directory\u662f\u4e00\u4e2a\u7f51\u7ad9\uff0c\u6536\u96c6\u4e86\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u66f4\u6709\u6548\u5730\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1", "motivation": "AI\u4ee3\u7406\u9700\u8981\u7279\u5b9a\u6280\u80fd\u6765\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6280\u80fd\u7ba1\u7406\u548c\u5171\u4eab\u673a\u5236\uff0c\u5bfc\u81f4\u91cd\u590d\u5f00\u53d1\u548c\u6548\u7387\u4f4e\u4e0b", "method": "\u521b\u5efa\u4e00\u4e2a\u7f51\u7ad9\u76ee\u5f55\uff0c\u6536\u96c6\u548c\u7ec4\u7ec7\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff0c\u8fd9\u4e9b\u6280\u80fd\u63d0\u4f9b\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u6267\u884c\u7279\u5b9a\u4efb\u52a1", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u6700\u6709\u7528\u7684AI\u4ee3\u7406\u6280\u80fd\u7684\u5728\u7ebf\u76ee\u5f55\uff0c\u65b9\u4fbf\u5f00\u53d1\u8005\u67e5\u627e\u548c\u4f7f\u7528", "conclusion": "Agent Skills Directory\u901a\u8fc7\u63d0\u4f9b\u53ef\u91cd\u7528\u7684\u6280\u80fd\u5e93\uff0c\u63d0\u9ad8\u4e86AI\u4ee3\u7406\u7684\u5f00\u53d1\u6548\u7387\u548c\u4efb\u52a1\u6267\u884c\u80fd\u529b", "topic": "code agent"}}
{"id": "tldr.2601.80a446f3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/5ybVCgvvDcr2GKUBIeDKk9nR7NtUfV7-idT0cnu_k8k=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/5ybVCgvvDcr2GKUBIeDKk9nR7NtUfV7-idT0cnu_k8k=441", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019be075f17f-a5b7c8a8-15aa-474f-8b19-557e6e3680ed-000000/5ybVCgvvDcr2GKUBIeDKk9nR7NtUfV7-idT0cnu_k8k=441", "summary": "The Agent Skills Directory (Website) Skills are reusable, plugin-like capabilities that provide procedural knowledge to help AI agents more effectively accomplish specific tasks, and this website is a directory of the most useful skills.", "source": "tldr", "AI": {"tldr": "Agent Skills Directory\u662f\u4e00\u4e2a\u7f51\u7ad9\uff0c\u6536\u96c6\u4e86\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u66f4\u6709\u6548\u5730\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1", "motivation": "AI\u4ee3\u7406\u9700\u8981\u7279\u5b9a\u6280\u80fd\u6765\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6280\u80fd\u5e93\u548c\u91cd\u7528\u673a\u5236\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u91cd\u590d\u5f00\u53d1", "method": "\u521b\u5efa\u4e00\u4e2a\u7f51\u7ad9\u76ee\u5f55\uff0c\u6536\u96c6\u548c\u7ec4\u7ec7\u53ef\u91cd\u7528\u7684\u63d2\u4ef6\u5f0f\u6280\u80fd\uff0c\u8fd9\u4e9b\u6280\u80fd\u63d0\u4f9b\u7a0b\u5e8f\u6027\u77e5\u8bc6\uff0c\u5e2e\u52a9AI\u4ee3\u7406\u5b8c\u6210\u4efb\u52a1", "result": "\u5efa\u7acb\u4e86Agent Skills Directory\u7f51\u7ad9\uff0c\u5305\u542b\u6700\u6709\u7528\u7684\u6280\u80fd\u76ee\u5f55\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u53ef\u91cd\u7528\u7684\u80fd\u529b\u5e93", "conclusion": "\u6280\u80fd\u76ee\u5f55\u7cfb\u7edf\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8AI\u4ee3\u7406\u7684\u6548\u7387\u548c\u80fd\u529b\uff0c\u901a\u8fc7\u91cd\u7528\u73b0\u6709\u6280\u80fd\u51cf\u5c11\u91cd\u590d\u5f00\u53d1", "topic": "agent analysis"}}
{"id": "tldr.2601.538d01e6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.devopsdigest.com%2Fgitlab-announces-general-availability-of-gitlab-duo-agent-platform%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/9gpERhgEJzGWrqTNjXmHyufnCnIaocobuD-sEkaJfqY=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.devopsdigest.com%2Fgitlab-announces-general-availability-of-gitlab-duo-agent-platform%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/9gpERhgEJzGWrqTNjXmHyufnCnIaocobuD-sEkaJfqY=441", "authors": ["TLDR Newsletter"], "title": "GitLab Announces General Availability of GitLab Duo Agent Platform", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.devopsdigest.com%2Fgitlab-announces-general-availability-of-gitlab-duo-agent-platform%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/9gpERhgEJzGWrqTNjXmHyufnCnIaocobuD-sEkaJfqY=441", "summary": "GitLab Announces General Availability of GitLab Duo Agent Platform (4 minute read) GitLab Duo Agent Platform is now in general availability. The platform enables orchestrated AI agents across the full software lifecycle to address delivery bottlenecks beyond coding. It offers agentic chat, prebuilt and custom agents, governance controls, and usage-based pricing via GitLab Credits.", "source": "tldr", "AI": {"tldr": "GitLab\u63a8\u51faGitLab Duo Agent Platform\u6b63\u5f0f\u7248\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u5b8c\u6574\u8f6f\u4ef6\u751f\u547d\u5468\u671f\u7684AI\u4ee3\u7406\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u7f16\u7801\u4e4b\u5916\u7684\u4ea4\u4ed8\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u5f00\u53d1\u4ea4\u4ed8\u8fc7\u7a0b\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f16\u7801\u4e4b\u5916\u7684\u73af\u8282\uff0c\u901a\u8fc7AI\u4ee3\u7406\u63d0\u5347\u6574\u4e2a\u8f6f\u4ef6\u751f\u547d\u5468\u671f\u7684\u6548\u7387\u3002", "method": "\u63d0\u4f9b\u7f16\u6392\u7684AI\u4ee3\u7406\u5e73\u53f0\uff0c\u5305\u62ec\u4ee3\u7406\u804a\u5929\u529f\u80fd\u3001\u9884\u6784\u5efa\u548c\u81ea\u5b9a\u4e49\u4ee3\u7406\u3001\u6cbb\u7406\u63a7\u5236\uff0c\u4ee5\u53ca\u57fa\u4e8eGitLab\u79ef\u5206\u7684\u4f7f\u7528\u5b9a\u4ef7\u6a21\u5f0f\u3002", "result": "GitLab Duo Agent Platform\u73b0\u5df2\u6b63\u5f0f\u53ef\u7528\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u8de8\u5b8c\u6574\u8f6f\u4ef6\u751f\u547d\u5468\u671f\u7684AI\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GitLab\u901a\u8fc7\u63a8\u51faAI\u4ee3\u7406\u5e73\u53f0\u6269\u5c55\u4e86\u5176DevOps\u5de5\u5177\u94fe\uff0c\u5e2e\u52a9\u7ec4\u7ec7\u89e3\u51b3\u8f6f\u4ef6\u4ea4\u4ed8\u4e2d\u7684\u975e\u7f16\u7801\u74f6\u9888\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "tldr.2601.28a8fb6c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fantonbabenko%2Fterraform-skill%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/TK5zPrUJTRiV7AESRgyfPCxRlwOV4f_5FewgYjSd_v4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fantonbabenko%2Fterraform-skill%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/TK5zPrUJTRiV7AESRgyfPCxRlwOV4f_5FewgYjSd_v4=441", "authors": ["TLDR Newsletter"], "title": "Terraform Skill", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fantonbabenko%2Fterraform-skill%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/TK5zPrUJTRiV7AESRgyfPCxRlwOV4f_5FewgYjSd_v4=441", "summary": "Terraform Skill (GitHub Repo) A new Claude Agent Skill for Terraform and OpenTofu has been introduced, offering comprehensive guidance on testing strategies, module patterns, CI/CD workflows, and production-ready infrastructure code for Claude Code users.", "source": "tldr", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684Claude Agent Skill\uff0c\u4e13\u95e8\u7528\u4e8eTerraform\u548cOpenTofu\uff0c\u63d0\u4f9b\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u7684\u5168\u9762\u6307\u5bfc", "motivation": "\u4e3aClaude Code\u7528\u6237\u63d0\u4f9b\u4e13\u4e1a\u7684\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\uff08IaC\uff09\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5728Terraform\u548cOpenTofu\u9879\u76ee\u4e2d\u5b9e\u65bd\u6700\u4f73\u5b9e\u8df5", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aClaude Agent Skill\uff0c\u5305\u542b\u6d4b\u8bd5\u7b56\u7565\u3001\u6a21\u5757\u6a21\u5f0f\u3001CI/CD\u5de5\u4f5c\u6d41\u548c\u751f\u4ea7\u5c31\u7eea\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u7684\u6307\u5bfc", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2aGitHub\u4ed3\u5e93\uff0c\u63d0\u4f9bTerraform\u548cOpenTofu\u7684\u5168\u9762\u6280\u80fd\u5305\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u63d0\u5347\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u8d28\u91cf", "conclusion": "\u8be5\u6280\u80fd\u4e3aClaude Code\u7528\u6237\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u4e13\u4e1a\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u6807\u51c6\u5316\u548c\u4f18\u5316\u4e91\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406", "topic": "code agent"}}
{"id": "tldr.2601.18e5c029", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmicrosoft%2Fagent-lightning%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/fZnrbTTwJVvfGSYE0gEuADBID6l7-9lJbI45dbrj2IE=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmicrosoft%2Fagent-lightning%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/fZnrbTTwJVvfGSYE0gEuADBID6l7-9lJbI45dbrj2IE=441", "authors": ["TLDR Newsletter"], "title": "agent-lightning", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmicrosoft%2Fagent-lightning%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/fZnrbTTwJVvfGSYE0gEuADBID6l7-9lJbI45dbrj2IE=441", "summary": "agent-lightning (GitHub Repo) agent-lightning is an open-source framework for training and optimizing AI agents\u2014enabling reinforcement learning (RL), automatic prompt optimization, supervised fine-tuning, and more\u2014without requiring substantial changes to existing agent code. It works with virtually any agent framework (e.g., LangChain, OpenAI Agents SDK, and AutoGen) and provides modular components to collect agent execution data and iteratively improve agent performance via a decoupled RL tr...", "source": "tldr", "AI": {"tldr": "agent-lightning \u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u4f18\u5316AI\u667a\u80fd\u4f53\uff0c\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u3001\u76d1\u7763\u5fae\u8c03\u7b49\u529f\u80fd\uff0c\u65e0\u9700\u5927\u5e45\u4fee\u6539\u73b0\u6709\u667a\u80fd\u4f53\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709AI\u667a\u80fd\u4f53\u6846\u67b6\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bad\u7ec3\u548c\u4f18\u5316\u5de5\u5177\uff0c\u667a\u80fd\u4f53\u6027\u80fd\u63d0\u5347\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u6574\u3002agent-lightning\u65e8\u5728\u63d0\u4f9b\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u8f7b\u677e\u6536\u96c6\u667a\u80fd\u4f53\u6267\u884c\u6570\u636e\u5e76\u8fed\u4ee3\u6539\u8fdb\u6027\u80fd\u3002", "method": "\u91c7\u7528\u89e3\u8026\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e0e\u4e3b\u6d41\u667a\u80fd\u4f53\u6846\u67b6\uff08\u5982LangChain\u3001OpenAI Agents SDK\u3001AutoGen\uff09\u517c\u5bb9\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u7ec4\u4ef6\u6536\u96c6\u6267\u884c\u6570\u636e\uff0c\u652f\u6301\u591a\u79cd\u4f18\u5316\u6280\u672f\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u80fd\u591f\u4e0e\u73b0\u6709\u667a\u80fd\u4f53\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\uff0c\u5b9e\u73b0\u667a\u80fd\u4f53\u6027\u80fd\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u652f\u6301\u591a\u79cd\u8bad\u7ec3\u548c\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "agent-lightning\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8bad\u7ec3\u548c\u4f18\u5316\u5e73\u53f0\uff0c\u964d\u4f4e\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u63d0\u5347\u7684\u6280\u672f\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u667a\u80fd\u4f53\u6280\u672f\u7684\u666e\u53ca\u548c\u5e94\u7528\u3002", "topic": "code agent"}}
{"id": "tldr.2601.14e27b0b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fself-driving-devops-how-stakpak-tackles-infrastructure-complexity%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/pWDS11CM1zo1yOHwkY0TQu12ntqJ2nDqIomN9Cwo6_s=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fself-driving-devops-how-stakpak-tackles-infrastructure-complexity%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/pWDS11CM1zo1yOHwkY0TQu12ntqJ2nDqIomN9Cwo6_s=441", "authors": ["TLDR Newsletter"], "title": "Self-Driving DevOps? How Stakpak Tackles Infrastructure Complexity", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthenewstack.io%2Fself-driving-devops-how-stakpak-tackles-infrastructure-complexity%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/pWDS11CM1zo1yOHwkY0TQu12ntqJ2nDqIomN9Cwo6_s=441", "summary": "Self-Driving DevOps? How Stakpak Tackles Infrastructure Complexity (5 minute read) Stakpak introduced an open-source DevOps agent aimed at making infrastructure more autonomous by securing secrets, preventing destructive actions with guardrails, and centralizing shared operational knowledge so AI can handle complex, non-coding DevOps tasks reliably.", "source": "tldr", "AI": {"tldr": "Stakpak\u662f\u4e00\u4e2a\u5f00\u6e90\u7684DevOps\u4ee3\u7406\uff0c\u65e8\u5728\u901a\u8fc7\u5b89\u5168\u5b58\u50a8\u79d8\u5bc6\u3001\u8bbe\u7f6e\u9632\u62a4\u63aa\u65bd\u9632\u6b62\u7834\u574f\u6027\u64cd\u4f5c\u3001\u96c6\u4e2d\u5171\u4eab\u8fd0\u7ef4\u77e5\u8bc6\uff0c\u4f7fAI\u80fd\u591f\u53ef\u9760\u5904\u7406\u590d\u6742\u7684\u975e\u7f16\u7801DevOps\u4efb\u52a1\uff0c\u5b9e\u73b0\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u4e3b\u5316\u3002", "motivation": "\u5f53\u524dDevOps\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u590d\u6742\uff0c\u6d89\u53ca\u5927\u91cf\u975e\u7f16\u7801\u4efb\u52a1\uff0c\u9700\u8981\u4eba\u5de5\u5904\u7406\u79d8\u5bc6\u7ba1\u7406\u3001\u5b89\u5168\u9632\u62a4\u548c\u77e5\u8bc6\u5171\u4eab\u7b49\u95ee\u9898\u3002Stakpak\u65e8\u5728\u901a\u8fc7AI\u4ee3\u7406\u81ea\u52a8\u5316\u8fd9\u4e9b\u590d\u6742\u4efb\u52a1\uff0c\u964d\u4f4e\u8fd0\u7ef4\u590d\u6742\u6027\u3002", "method": "\u5f00\u53d1\u5f00\u6e90DevOps\u4ee3\u7406\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u529f\u80fd\uff1a1) \u5b89\u5168\u5b58\u50a8\u548c\u7ba1\u7406\u79d8\u5bc6\uff1b2) \u8bbe\u7f6e\u9632\u62a4\u63aa\u65bd\u9632\u6b62\u7834\u574f\u6027\u64cd\u4f5c\uff1b3) \u96c6\u4e2d\u5171\u4eab\u8fd0\u7ef4\u77e5\u8bc6\u5e93\u3002\u901a\u8fc7\u8fd9\u4e9b\u529f\u80fd\u4f7fAI\u80fd\u591f\u53ef\u9760\u5904\u7406\u590d\u6742\u7684\u975e\u7f16\u7801DevOps\u4efb\u52a1\u3002", "result": "Stakpak\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u81ea\u4e3b\u5904\u7406\u590d\u6742DevOps\u4efb\u52a1\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u4e86\u8fd0\u7ef4\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "Stakpak\u901a\u8fc7\u5c06AI\u4ee3\u7406\u5f15\u5165DevOps\u9886\u57df\uff0c\u4e3a\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6848\uff0c\u4f7f\u975e\u7f16\u7801\u7684\u8fd0\u7ef4\u4efb\u52a1\u80fd\u591f\u5b9e\u73b0\u81ea\u4e3b\u5316\u5904\u7406\u3002", "topic": "code agent"}}
{"id": "tldr.2601.142e90de", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmaking-small-llms-smarter%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/Rfw3fc-vTRA0WwIZRDPTCVeHKNy5LDPeGEprQNi-5CM=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmaking-small-llms-smarter%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/Rfw3fc-vTRA0WwIZRDPTCVeHKNy5LDPeGEprQNi-5CM=441", "authors": ["TLDR Newsletter"], "title": "Making Very Small LLMs Smarter With RAG", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmaking-small-llms-smarter%2F%3Futm_source=tldrdevops/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/Rfw3fc-vTRA0WwIZRDPTCVeHKNy5LDPeGEprQNi-5CM=441", "summary": "Making Very Small LLMs Smarter With RAG (7 minute read) Small, local LLMs can be effectively leveraged for niche development tasks by implementing Retrieval Augmented Generation (RAG). This approach allows LLMs to generate accurate code snippets for the custom Nova Golang library, addressing limitations faced by larger commercial models unaware of such specialized projects.", "source": "tldr", "AI": {"tldr": "\u5c0f\u578b\u672c\u5730LLM\u901a\u8fc7RAG\u6280\u672f\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5904\u7406\u4e13\u4e1a\u5f00\u53d1\u4efb\u52a1\uff0c\u4e3aNova Golang\u5e93\u751f\u6210\u51c6\u786e\u4ee3\u7801\u7247\u6bb5", "motivation": "\u5927\u578b\u5546\u4e1a\u6a21\u578b\u4e0d\u4e86\u89e3\u4e13\u4e1a\u9879\u76ee\uff08\u5982Nova Golang\u5e93\uff09\uff0c\u5c0f\u578b\u672c\u5730LLM\u5728\u5904\u7406\u8fd9\u7c7b\u4e13\u4e1a\u5f00\u53d1\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u589e\u5f3a\u5176\u80fd\u529b", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u4e13\u4e1a\u77e5\u8bc6\u6765\u589e\u5f3a\u5c0f\u578bLLM\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u4e13\u4e1a\u5f00\u53d1\u4efb\u52a1", "result": "\u5c0f\u578b\u672c\u5730LLM\u901a\u8fc7RAG\u6280\u672f\u80fd\u591f\u4e3aNova Golang\u5e93\u751f\u6210\u51c6\u786e\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u5546\u4e1a\u6a21\u578b\u4e0d\u4e86\u89e3\u4e13\u4e1a\u9879\u76ee\u7684\u95ee\u9898", "conclusion": "RAG\u6280\u672f\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5c0f\u578b\u672c\u5730LLM\u5728\u4e13\u4e1a\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5927\u578b\u5546\u4e1a\u6a21\u578b\u65e0\u6cd5\u8986\u76d6\u7684\u4e13\u4e1a\u9886\u57df", "topic": "code agent"}}
{"id": "tldr.2601.7c9c2b79", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord-for-ci%2F%3Futm_source=tldrdevops%26utm_medium=tldrnewsletter%26utm_campaign=ql20260121%26utm_content=std/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/auizsP2wXkVaQSsJvPT_s7OogZwvwArrvsppA3icMlM=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord-for-ci%2F%3Futm_source=tldrdevops%26utm_medium=tldrnewsletter%26utm_campaign=ql20260121%26utm_content=std/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/auizsP2wXkVaQSsJvPT_s7OogZwvwArrvsppA3icMlM=441", "authors": ["TLDR Newsletter"], "title": "Run tests in CI against a real environment", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord-for-ci%2F%3Futm_source=tldrdevops%26utm_medium=tldrnewsletter%26utm_campaign=ql20260121%26utm_content=std/1/0100019be083aeeb-61789353-07c4-4ab8-b32c-fb29dec4bce5-000000/auizsP2wXkVaQSsJvPT_s7OogZwvwArrvsppA3icMlM=441", "summary": "Run tests in CI against a real environment (Sponsor) mirrord for CI lets you run tests against your staging or pre-prod Kubernetes environment without deploying code or spinning up test environments. Get faster, more reliable CI feedback at a lower cost. Learn more", "source": "tldr", "AI": {"tldr": "mirrord for CI\u5de5\u5177\u5141\u8bb8\u5728CI\u4e2d\u76f4\u63a5\u5bf9\u771f\u5b9eKubernetes\u73af\u5883\u8fd0\u884c\u6d4b\u8bd5\uff0c\u65e0\u9700\u90e8\u7f72\u4ee3\u7801\u6216\u521b\u5efa\u6d4b\u8bd5\u73af\u5883\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684CI\u53cd\u9988\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u4f20\u7edfCI/CD\u6d41\u7a0b\u4e2d\uff0c\u6d4b\u8bd5\u901a\u5e38\u9700\u8981\u5728\u90e8\u7f72\u4ee3\u7801\u6216\u521b\u5efa\u4e13\u95e8\u7684\u6d4b\u8bd5\u73af\u5883\u540e\u624d\u80fd\u8fdb\u884c\uff0c\u8fd9\u5bfc\u81f4\u53cd\u9988\u5468\u671f\u957f\u3001\u6210\u672c\u9ad8\u4e14\u53ef\u9760\u6027\u6709\u9650\u3002\u5f00\u53d1\u8005\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u5f0f\u6765\u5728CI\u4e2d\u76f4\u63a5\u9a8c\u8bc1\u4ee3\u7801\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "mirrord for CI\u901a\u8fc7\u67d0\u79cd\u6280\u672f\uff08\u53ef\u80fd\u662f\u6d41\u91cf\u955c\u50cf\u6216\u4ee3\u7406\u673a\u5236\uff09\u8ba9CI\u6d41\u6c34\u7ebf\u4e2d\u7684\u6d4b\u8bd5\u80fd\u591f\u76f4\u63a5\u8bbf\u95ee\u771f\u5b9e\u7684Kubernetes\u73af\u5883\uff08\u5982staging\u6216pre-prod\u73af\u5883\uff09\uff0c\u800c\u65e0\u9700\u5b9e\u9645\u90e8\u7f72\u4ee3\u7801\u6216\u521b\u5efa\u72ec\u7acb\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u52a0\u5febCI\u53cd\u9988\u901f\u5ea6\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u53ef\u9760\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u57fa\u7840\u8bbe\u65bd\u6210\u672c\uff0c\u56e0\u4e3a\u65e0\u9700\u7ef4\u62a4\u4e13\u95e8\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "conclusion": "mirrord for CI\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684CI\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u76f4\u63a5\u8fd0\u884c\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCI/CD\u6d41\u7a0b\u4e2d\u7684\u5ef6\u8fdf\u3001\u6210\u672c\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "topic": "swe application"}}
{"id": "tldr.2601.54fdb40e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FSeanHeelan%2Fanamnesis-release%3Futm_source=tldrinfosec/1/0100019be0e1e834-9aab8d86-f6e4-4cad-8232-781deb967bd2-000000/BfnRSx0tA3AcfctL2lL-w6PowGoGzTHrrN4ld2mtds4=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FSeanHeelan%2Fanamnesis-release%3Futm_source=tldrinfosec/1/0100019be0e1e834-9aab8d86-f6e4-4cad-8232-781deb967bd2-000000/BfnRSx0tA3AcfctL2lL-w6PowGoGzTHrrN4ld2mtds4=441", "authors": ["TLDR Newsletter"], "title": "Anamnesis", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FSeanHeelan%2Fanamnesis-release%3Futm_source=tldrinfosec/1/0100019be0e1e834-9aab8d86-f6e4-4cad-8232-781deb967bd2-000000/BfnRSx0tA3AcfctL2lL-w6PowGoGzTHrrN4ld2mtds4=441", "summary": "Anamnesis (GitHub Repo) Anamnesis is an evaluation framework for studying how LLM agents generate exploits from vulnerability reports, demonstrating that Claude Opus 4.5 and GPT-5.2 agents can independently develop working exploits that bypass ASLR, NX, full RELRO, CFI, Shadow Stack, and seccomp sandboxes when given a QuickJS use-after-free vulnerability and proof-of-concept trigger.", "source": "tldr", "AI": {"tldr": "Anamnesis\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u4ece\u6f0f\u6d1e\u62a5\u544a\u751f\u6210\u6f0f\u6d1e\u5229\u7528\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86Claude Opus 4.5\u548cGPT-5.2\u4ee3\u7406\u80fd\u591f\u72ec\u7acb\u5f00\u53d1\u7ed5\u8fc7\u591a\u79cd\u5b89\u5168\u9632\u62a4\u7684\u5de5\u4f5c\u6f0f\u6d1e\u5229\u7528", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u7279\u522b\u662f\u5b83\u4eec\u662f\u5426\u80fd\u591f\u57fa\u4e8e\u6f0f\u6d1e\u62a5\u544a\u72ec\u7acb\u5f00\u53d1\u590d\u6742\u7684\u6f0f\u6d1e\u5229\u7528\uff0c\u8fd9\u5bf9\u4e8e\u8bc4\u4f30AI\u5728\u5b89\u5168\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u548c\u98ce\u9669\u5177\u6709\u91cd\u8981\u610f\u4e49", "method": "\u5f00\u53d1\u4e86Anamnesis\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528QuickJS\u7684use-after-free\u6f0f\u6d1e\u548c\u6982\u5ff5\u9a8c\u8bc1\u89e6\u53d1\u5668\u4f5c\u4e3a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u8ba9LLM\u4ee3\u7406\uff08Claude Opus 4.5\u548cGPT-5.2\uff09\u72ec\u7acb\u751f\u6210\u6f0f\u6d1e\u5229\u7528\u4ee3\u7801", "result": "Claude Opus 4.5\u548cGPT-5.2\u4ee3\u7406\u80fd\u591f\u6210\u529f\u5f00\u53d1\u51fa\u7ed5\u8fc7ASLR\u3001NX\u3001full RELRO\u3001CFI\u3001Shadow Stack\u548cseccomp\u6c99\u7bb1\u7b49\u591a\u79cd\u5b89\u5168\u9632\u62a4\u7684\u5de5\u4f5c\u6f0f\u6d1e\u5229\u7528", "conclusion": "\u5148\u8fdb\u7684LLM\u4ee3\u7406\u5df2\u7ecf\u5177\u5907\u4ece\u6f0f\u6d1e\u62a5\u544a\u72ec\u7acb\u5f00\u53d1\u590d\u6742\u6f0f\u6d1e\u5229\u7528\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u7f51\u7edc\u5b89\u5168\u9886\u57df\u65e2\u662f\u673a\u9047\u4e5f\u662f\u6311\u6218\uff0c\u9700\u8981\u8ba4\u771f\u8003\u8651AI\u5728\u5b89\u5168\u7814\u7a76\u4e2d\u7684\u4f26\u7406\u548c\u5b89\u5168\u5f71\u54cd", "topic": "agent analysis"}}
{"id": "tldr.2601.02e30da2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchromeunboxed.com%2Fgemini-in-chrome-is-getting-skills-as-it-moves-toward-becoming-a-full-ai-agent%2F%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/8f0f2jmMSuBU_J4TIKK88p9H6acfnmDwK7Aza8KBDZU=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchromeunboxed.com%2Fgemini-in-chrome-is-getting-skills-as-it-moves-toward-becoming-a-full-ai-agent%2F%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/8f0f2jmMSuBU_J4TIKK88p9H6acfnmDwK7Aza8KBDZU=441", "authors": ["TLDR Newsletter"], "title": "Gemini in Chrome is getting \u201cSkills\u201d as it moves toward becoming a full AI agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchromeunboxed.com%2Fgemini-in-chrome-is-getting-skills-as-it-moves-toward-becoming-a-full-ai-agent%2F%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/8f0f2jmMSuBU_J4TIKK88p9H6acfnmDwK7Aza8KBDZU=441", "summary": "Gemini in Chrome is getting \u201cSkills\u201d as it moves toward becoming a full AI agent (2 minute read) Gemini in Chrome is being upgraded into a proactive agent capable of performing complex tasks on users' behalf. A hidden internal page for 'skills' has been spotted in testing. The page features a dedicated interface where users can define specific capabilities for the AI. Skills will effectively allow users to 'teach' Gemini how to handle specific or repetitive workflows within the browser.", "source": "tldr", "AI": {"tldr": "Gemini in Chrome \u6b63\u5728\u5347\u7ea7\u4e3a\u4e3b\u52a8\u5f0fAI\u4ee3\u7406\uff0c\u901a\u8fc7\"\u6280\u80fd\"\u529f\u80fd\u8ba9\u7528\u6237\u80fd\u591f\u5b9a\u4e49\u548c\u6559\u6388AI\u5904\u7406\u7279\u5b9a\u6216\u91cd\u590d\u6027\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u5c06Gemini\u4ece\u7b80\u5355\u7684\u804a\u5929\u52a9\u624b\u5347\u7ea7\u4e3a\u80fd\u591f\u4ee3\u8868\u7528\u6237\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u4e3b\u52a8\u5f0fAI\u4ee3\u7406\uff0c\u63d0\u9ad8\u6d4f\u89c8\u5668\u5185\u7684\u81ea\u52a8\u5316\u80fd\u529b\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u5728Chrome\u4e2d\u5f15\u5165\"\u6280\u80fd\"\u529f\u80fd\uff0c\u63d0\u4f9b\u4e13\u7528\u754c\u9762\u8ba9\u7528\u6237\u5b9a\u4e49AI\u7684\u5177\u4f53\u80fd\u529b\uff0c\u901a\u8fc7\u7528\u6237\"\u6559\u6388\"\u7684\u65b9\u5f0f\u8ba9AI\u5b66\u4e60\u5904\u7406\u7279\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u53d1\u73b0\u9690\u85cf\u7684\u5185\u90e8\"\u6280\u80fd\"\u9875\u9762\u6b63\u5728\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8fd9\u8868\u660eGoogle\u6b63\u5728\u5c06Gemini\u8f6c\u53d8\u4e3a\u80fd\u591f\u4e3b\u52a8\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u5b8c\u6574AI\u4ee3\u7406\u3002", "conclusion": "Gemini\u6b63\u5728\u5411\u5b8c\u6574\u7684AI\u4ee3\u7406\u53d1\u5c55\uff0c\u901a\u8fc7\u6280\u80fd\u7cfb\u7edf\u8ba9\u7528\u6237\u80fd\u591f\u81ea\u5b9a\u4e49\u548c\u6269\u5c55AI\u5728\u6d4f\u89c8\u5668\u4e2d\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u81ea\u52a8\u5316\u4efb\u52a1\u5904\u7406\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.39355e28", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/vbyNDukx7lnThXeDecwYo-XA545Rn33VN1UFDtsTiio=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/vbyNDukx7lnThXeDecwYo-XA545Rn33VN1UFDtsTiio=441", "authors": ["TLDR Newsletter"], "title": "The Tragedy of the Agentic Commons", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fthe-tragedy-of-the-agentic-commons%3Futm_source=tldrai/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/vbyNDukx7lnThXeDecwYo-XA545Rn33VN1UFDtsTiio=441", "summary": "The Tragedy of the Agentic Commons (14 minute read) AI agents can improve matching markets, like dating or job searches, by using LLMs to convert unstructured preferences into actionable data, outperforming standard questionnaires. However, in markets where everyone uses AI agents, congestion increases significantly, reducing efficiency without a pricing mechanism. Introducing prices and institutional design can alleviate congestion, streamline coordination, and enhance market efficiency.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7406\u80fd\u901a\u8fc7LLM\u5c06\u975e\u7ed3\u6784\u5316\u504f\u597d\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u6570\u636e\uff0c\u5728\u5339\u914d\u5e02\u573a\uff08\u5982\u7ea6\u4f1a\u3001\u6c42\u804c\uff09\u4e2d\u4f18\u4e8e\u6807\u51c6\u95ee\u5377\u3002\u4f46\u5f53\u6240\u6709\u53c2\u4e0e\u8005\u90fd\u4f7f\u7528AI\u4ee3\u7406\u65f6\uff0c\u4f1a\u5bfc\u81f4\u4e25\u91cd\u62e5\u5835\uff0c\u964d\u4f4e\u6548\u7387\u3002\u5f15\u5165\u4ef7\u683c\u673a\u5236\u548c\u5236\u5ea6\u8bbe\u8ba1\u53ef\u4ee5\u7f13\u89e3\u62e5\u5835\u3001\u6539\u5584\u534f\u8c03\u3001\u63d0\u5347\u5e02\u573a\u6548\u7387\u3002", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u5728\u5339\u914d\u5e02\u573a\u4e2d\u7684\u53cc\u91cd\u6548\u5e94\uff1a\u4e00\u65b9\u9762AI\u4ee3\u7406\u80fd\u901a\u8fc7LLM\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u504f\u597d\uff0c\u63d0\u5347\u5339\u914d\u8d28\u91cf\uff1b\u53e6\u4e00\u65b9\u9762\u5f53\u6240\u6709\u5e02\u573a\u53c2\u4e0e\u8005\u90fd\u4f7f\u7528AI\u4ee3\u7406\u65f6\uff0c\u4f1a\u5bfc\u81f4\"\u4ee3\u7406\u516c\u5730\u60b2\u5267\"\uff0c\u4ea7\u751f\u62e5\u5835\u95ee\u9898\uff0c\u964d\u4f4e\u6574\u4f53\u5e02\u573a\u6548\u7387\u3002", "method": "\u5206\u6790AI\u4ee3\u7406\u5728\u5339\u914d\u5e02\u573a\u4e2d\u7684\u4f5c\u7528\u673a\u5236\uff0c\u7279\u522b\u662fLLM\u5982\u4f55\u5c06\u975e\u7ed3\u6784\u5316\u504f\u597d\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u6570\u636e\u3002\u7814\u7a76\u5f53\u6240\u6709\u53c2\u4e0e\u8005\u90fd\u4f7f\u7528AI\u4ee3\u7406\u65f6\u7684\u62e5\u5835\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u4ef7\u683c\u673a\u5236\u548c\u5236\u5ea6\u8bbe\u8ba1\u6765\u89e3\u51b3\u62e5\u5835\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "result": "AI\u4ee3\u7406\u5728\u5339\u914d\u5e02\u573a\u4e2d\u786e\u5b9e\u80fd\u8d85\u8d8a\u6807\u51c6\u95ee\u5377\uff0c\u4f46\u5f53\u6240\u6709\u53c2\u4e0e\u8005\u90fd\u4f7f\u7528\u65f6\u4f1a\u5bfc\u81f4\u663e\u8457\u62e5\u5835\uff0c\u964d\u4f4e\u6548\u7387\u3002\u5f15\u5165\u4ef7\u683c\u673a\u5236\u548c\u9002\u5f53\u7684\u5236\u5ea6\u8bbe\u8ba1\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u62e5\u5835\u95ee\u9898\uff0c\u6539\u5584\u5e02\u573a\u534f\u8c03\uff0c\u63d0\u5347\u6574\u4f53\u6548\u7387\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u5339\u914d\u5e02\u573a\u4e2d\u5177\u6709\u53cc\u91cd\u6548\u5e94\uff1a\u4e2a\u4f53\u5c42\u9762\u80fd\u63d0\u5347\u5339\u914d\u8d28\u91cf\uff0c\u4f46\u96c6\u4f53\u5c42\u9762\u53ef\u80fd\u5bfc\u81f4\"\u4ee3\u7406\u516c\u5730\u60b2\u5267\"\u3002\u9700\u8981\u901a\u8fc7\u4ef7\u683c\u673a\u5236\u548c\u5236\u5ea6\u8bbe\u8ba1\u6765\u534f\u8c03\u4e2a\u4f53\u4e0e\u96c6\u4f53\u5229\u76ca\uff0c\u5b9e\u73b0\u5e02\u573a\u6548\u7387\u6700\u5927\u5316\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.d40db7a8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLCCov8/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/jLCSGSQaOA31wKQ___4VtudpixyjztVQBLej8S6mGs0=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLCCov8/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/jLCSGSQaOA31wKQ___4VtudpixyjztVQBLej8S6mGs0=441", "authors": ["TLDR Newsletter"], "title": "GLM-4.7-Flash", "comment": "Source: TLDR Newsletter, Date: 2026-01-21, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FLCCov8/1/0100019be0ebe674-5bcffab8-988b-4fa2-b449-f3944f7988cc-000000/jLCSGSQaOA31wKQ___4VtudpixyjztVQBLej8S6mGs0=441", "summary": "GLM-4.7-Flash (2 minute read) GLM-4.7-Flash is a 30B-class MoE model designed for local deployment in coding, agent tasks, translation, and creative writing.", "source": "tldr", "AI": {"tldr": "GLM-4.7-Flash\u662f\u4e00\u4e2a30B\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u4e13\u4e3a\u672c\u5730\u90e8\u7f72\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u7f16\u7801\u3001\u667a\u80fd\u4f53\u4efb\u52a1\u3001\u7ffb\u8bd1\u548c\u521b\u610f\u5199\u4f5c", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9002\u5408\u672c\u5730\u90e8\u7f72\u7684\u9ad8\u6548\u6a21\u578b\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5904\u7406\u591a\u79cd\u4efb\u52a1\uff0c\u5305\u62ec\u7f16\u7801\u3001\u667a\u80fd\u4f53\u5e94\u7528\u3001\u7ffb\u8bd1\u548c\u521b\u610f\u5199\u4f5c", "method": "\u91c7\u752830B\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u4f18\u5316\u6a21\u578b\u5927\u5c0f\u548c\u6027\u80fd\u5e73\u8861\uff0c\u4e13\u95e8\u9488\u5bf9\u672c\u5730\u90e8\u7f72\u573a\u666f\u8fdb\u884c\u8bbe\u8ba1", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u9002\u5408\u672c\u5730\u90e8\u7f72\u768430B MoE\u6a21\u578b\uff0c\u80fd\u591f\u5728\u7f16\u7801\u3001\u667a\u80fd\u4f53\u4efb\u52a1\u3001\u7ffb\u8bd1\u548c\u521b\u610f\u5199\u4f5c\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u826f\u597d", "conclusion": "GLM-4.7-Flash\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u672c\u5730\u90e8\u7f72\u6a21\u578b\uff0c\u901a\u8fc7MoE\u67b6\u6784\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f", "topic": "code agent"}}
{"id": "tldr.2601.018edf99", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441", "authors": ["TLDR Newsletter"], "title": "\ud83d\udc69\u200d\ud83c\udfed Build fast, scale right: Meet your AI app & agent factory", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441", "summary": "\ud83d\udc69\u200d\ud83c\udfed Build fast, scale right: Meet your AI app & agent factory (Sponsor) Agent adoption is accelerating. Winning teams ship multi\u2011agent systems with observability and governance built in from the start. Microsoft Foundry is a modular, interoperable platform to build, optimize, and govern all your agents from day one. With Foundry, you can securely ground your AI apps and agents on data stored in any location while respecting user permissions and data classification policies. Foundry Control Pl...", "source": "tldr", "AI": {"tldr": "\u5fae\u8f6fFoundry\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u4f18\u5316\u548c\u7ba1\u7406AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u63d0\u4f9b\u53ef\u89c2\u5bdf\u6027\u548c\u6cbb\u7406\u529f\u80fd", "motivation": "\u968f\u7740AI\u4ee3\u7406\u91c7\u7528\u52a0\u901f\uff0c\u56e2\u961f\u9700\u8981\u80fd\u591f\u5feb\u901f\u6784\u5efa\u3001\u5b89\u5168\u90e8\u7f72\u5e76\u6709\u6548\u7ba1\u7406\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5e73\u53f0\uff0c\u786e\u4fdd\u6570\u636e\u5b89\u5168\u548c\u6743\u9650\u63a7\u5236", "method": "\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u5e73\u53f0\u67b6\u6784\uff0c\u652f\u6301\u4ece\u7b2c\u4e00\u5929\u5f00\u59cb\u6784\u5efa\u3001\u4f18\u5316\u548c\u6cbb\u7406\u6240\u6709\u4ee3\u7406\uff0c\u786e\u4fddAI\u5e94\u7528\u548c\u4ee3\u7406\u80fd\u591f\u5b89\u5168\u5730\u57fa\u4e8e\u4efb\u4f55\u4f4d\u7f6e\u7684\u6570\u636e\u8fdb\u884c\u57fa\u7840\u8bad\u7ec3", "result": "Foundry\u5e73\u53f0\u4f7f\u56e2\u961f\u80fd\u591f\u5feb\u901f\u6784\u5efa\u548c\u6269\u5c55AI\u5e94\u7528\u4e0e\u4ee3\u7406\u7cfb\u7edf\uff0c\u540c\u65f6\u5185\u7f6e\u53ef\u89c2\u5bdf\u6027\u548c\u6cbb\u7406\u529f\u80fd\uff0c\u786e\u4fdd\u6570\u636e\u5b89\u5168\u548c\u6743\u9650\u5408\u89c4", "conclusion": "\u5fae\u8f6fFoundry\u4e3aAI\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e2e\u52a9\u56e2\u961f\u5728\u52a0\u901f\u91c7\u7528AI\u4ee3\u7406\u7684\u540c\u65f6\u786e\u4fdd\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u53ef\u89c2\u5bdf\u6027\u548c\u6cbb\u7406\u80fd\u529b", "topic": "code agent"}}
{"id": "tldr.2601.3c45e7b2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441", "authors": ["TLDR Newsletter"], "title": "the widest selection on any cloud", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fai.azure.com%2F%3Focid=cmmxp9rfcmy/2/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/gMxYqgtFTukVR7f_RNx0M7xzHil831g27bitYjiicT0=441", "summary": "\ud83d\udc69\u200d\ud83c\udfed Build fast, scale right: Meet your AI app & agent factory (Sponsor) Agent adoption is accelerating. Winning teams ship multi\u2011agent systems with observability and governance built in from the start. Microsoft Foundry is a modular, interoperable platform to build, optimize, and govern all your agents from day one. With Foundry, you can securely ground your AI apps and agents on data stored in any location while respecting user permissions and data classification policies. Foundry Control Pl...", "source": "tldr", "AI": {"tldr": "\u5fae\u8f6fFoundry\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u4ece\u7b2c\u4e00\u5929\u5f00\u59cb\u6784\u5efa\u3001\u4f18\u5316\u548c\u7ba1\u7406\u6240\u6709AI\u4ee3\u7406\uff0c\u63d0\u4f9b\u53ef\u89c2\u5bdf\u6027\u548c\u6cbb\u7406\u529f\u80fd", "motivation": "\u968f\u7740AI\u4ee3\u7406\u91c7\u7528\u52a0\u901f\uff0c\u56e2\u961f\u9700\u8981\u80fd\u591f\u6784\u5efa\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u4ece\u4e00\u5f00\u59cb\u5c31\u5185\u7f6e\u53ef\u89c2\u5bdf\u6027\u548c\u6cbb\u7406\u529f\u80fd\u7684\u5e73\u53f0", "method": "\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5728\u4efb\u4f55\u4f4d\u7f6e\u7684\u6570\u636e\u4e0a\u5b89\u5168\u5730\u6784\u5efaAI\u5e94\u7528\u548c\u4ee3\u7406\uff0c\u540c\u65f6\u5c0a\u91cd\u7528\u6237\u6743\u9650\u548c\u6570\u636e\u5206\u7c7b\u7b56\u7565", "result": "\u5fae\u8f6fFoundry\u5e73\u53f0\u80fd\u591f\u5e2e\u52a9\u56e2\u961f\u5feb\u901f\u6784\u5efa\u3001\u4f18\u5316\u548c\u7ba1\u7406AI\u4ee3\u7406\u7cfb\u7edf", "conclusion": "Foundry\u4e3a\u6784\u5efaAI\u5e94\u7528\u548c\u4ee3\u7406\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5e73\u53f0\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4ece\u5f00\u53d1\u5230\u6cbb\u7406\u7684\u5168\u6d41\u7a0b\u7ba1\u7406", "topic": "code agent"}}
{"id": "tldr.2601.89023dec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.decodingai.com%2Fp%2Fdesigning-production-engineer-agent-graphrag%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3zlYl54ZU9IqxwKI7C7G41eaZJvzfHi5DvttmaLjb8I=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.decodingai.com%2Fp%2Fdesigning-production-engineer-agent-graphrag%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3zlYl54ZU9IqxwKI7C7G41eaZJvzfHi5DvttmaLjb8I=441", "authors": ["TLDR Newsletter"], "title": "Your Agent's Reasoning Is Fine - Its Memory Isn't", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.decodingai.com%2Fp%2Fdesigning-production-engineer-agent-graphrag%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3zlYl54ZU9IqxwKI7C7G41eaZJvzfHi5DvttmaLjb8I=441", "summary": "Your Agent's Reasoning Is Fine - Its Memory Isn't (17 minute read) Production incident response is slowed not by lack of fixes but by scattered context and undocumented dependencies. Integrating an AI agent powered by GraphRAG enables rapid, structured retrieval of service relationships, ownership, past incidents, and documentation. This system, triggered by monitoring tools like Prometheus and interfacing via FastAPI and Slack, transforms incident alerts into actionable, context-rich reports...", "source": "tldr", "AI": {"tldr": "AI agent\u7ed3\u5408GraphRAG\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u68c0\u7d22\u670d\u52a1\u5173\u7cfb\u3001\u6240\u6709\u6743\u3001\u5386\u53f2\u4e8b\u4ef6\u548c\u6587\u6863\uff0c\u52a0\u901f\u751f\u4ea7\u4e8b\u6545\u54cd\u5e94\uff0c\u5c06\u76d1\u63a7\u8b66\u62a5\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u4e30\u5bcc\u62a5\u544a\u3002", "motivation": "\u751f\u4ea7\u4e8b\u6545\u54cd\u5e94\u7f13\u6162\u7684\u4e3b\u8981\u95ee\u9898\u4e0d\u662f\u7f3a\u4e4f\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u662f\u4e0a\u4e0b\u6587\u5206\u6563\u548c\u4f9d\u8d56\u5173\u7cfb\u672a\u6587\u6863\u5316\u3002\u4f20\u7edf\u65b9\u6cd5\u4e2d\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u6536\u96c6\u76f8\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u54cd\u5e94\u5ef6\u8fdf\u3002", "method": "\u96c6\u6210\u57fa\u4e8eGraphRAG\u7684AI\u4ee3\u7406\uff0c\u5b9e\u73b0\u5feb\u901f\u7ed3\u6784\u5316\u68c0\u7d22\u3002\u7cfb\u7edf\u7531Prometheus\u7b49\u76d1\u63a7\u5de5\u5177\u89e6\u53d1\uff0c\u901a\u8fc7FastAPI\u548cSlack\u63a5\u53e3\uff0c\u5c06\u4e8b\u6545\u8b66\u62a5\u8f6c\u5316\u4e3a\u5305\u542b\u670d\u52a1\u5173\u7cfb\u3001\u6240\u6709\u6743\u3001\u5386\u53f2\u4e8b\u4ef6\u548c\u6587\u6863\u7684\u4e0a\u4e0b\u6587\u4e30\u5bcc\u62a5\u544a\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u52a0\u901f\u751f\u4ea7\u4e8b\u6545\u54cd\u5e94\u8fc7\u7a0b\uff0c\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u51cf\u5c11\u5de5\u7a0b\u5e08\u6536\u96c6\u76f8\u5173\u4fe1\u606f\u7684\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e8b\u6545\u89e3\u51b3\u6548\u7387\u3002", "conclusion": "AI\u4ee3\u7406\u7684\u63a8\u7406\u80fd\u529b\u8db3\u591f\uff0c\u4f46\u8bb0\u5fc6\u7cfb\u7edf\u662f\u5173\u952e\u74f6\u9888\u3002\u901a\u8fc7GraphRAG\u6280\u672f\u589e\u5f3a\u8bb0\u5fc6\u80fd\u529b\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u4e8b\u6545\u54cd\u5e94\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2601.136fdb29", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FWAQ3l7%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3UWLQ_fxP47-4iGWlxoExqeiSfxMlCCh7W8oxIBZ__I=441", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FWAQ3l7%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3UWLQ_fxP47-4iGWlxoExqeiSfxMlCCh7W8oxIBZ__I=441", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2026-01-22, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FWAQ3l7%3Futm_source=tldrdata/1/0100019be5640b3a-97a12639-0eb4-416a-a638-2131a44ac10c-000000/3UWLQ_fxP47-4iGWlxoExqeiSfxMlCCh7W8oxIBZ__I=441", "summary": "Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback \u2014 giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time", "source": "tldr", "AI": {"tldr": "Agent Bricks\u662f\u4e00\u4e2aAI\u4ee3\u7406\u5f00\u53d1\u5e73\u53f0\uff0c\u5e2e\u52a9\u7528\u6237\u57fa\u4e8e\u7279\u5b9a\u6570\u636e\u6784\u5efa\u3001\u8bc4\u4f30\u548c\u4f18\u5316AI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u52a8\u8bc4\u4f30\u3001\u76ee\u6807\u5bf9\u9f50\u8bc4\u5206\u548c\u4eba\u7c7b\u53cd\u9988\u6539\u8fdb\uff0c\u5b9e\u73b0\u751f\u4ea7\u7ea7\u90e8\u7f72\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5f00\u53d1\u9762\u4e34\u96be\u4ee5\u8bc4\u4f30\u6027\u80fd\u3001\u7f3a\u4e4f\u751f\u4ea7\u5c31\u7eea\u6027\u3001\u96be\u4ee5\u4e0e\u4e1a\u52a1\u76ee\u6807\u5bf9\u9f50\u7b49\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u5de5\u5177\u6765\u6784\u5efa\u771f\u6b63\u6709\u6548\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u3002", "method": "\u63d0\u4f9b\u5e73\u53f0\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u542b\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u3001\u57fa\u4e8e\u76ee\u6807\u7684\u8f93\u51fa\u8bc4\u5206\u673a\u5236\u3001\u4eba\u7c7b\u53cd\u9988\u96c6\u6210\uff0c\u4ee5\u53ca\u6570\u636e\u9a71\u52a8\u7684\u4f18\u5316\u6d41\u7a0b\uff0c\u5e2e\u52a9\u7528\u6237\u7cfb\u7edf\u5316\u5730\u5f00\u53d1\u548c\u6539\u8fdbAI\u4ee3\u7406\u3002", "result": "\u5e73\u53f0\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u66f4\u53ef\u9760\u3001\u4e0e\u4e1a\u52a1\u76ee\u6807\u5bf9\u9f50\u7684AI\u4ee3\u7406\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u4f18\u5316\u8def\u5f84\uff0c\u52a0\u901f\u4ece\u539f\u578b\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u8fc7\u7a0b\u3002", "conclusion": "Agent Bricks\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7406\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f7f\u6784\u5efa\u751f\u4ea7\u5c31\u7eea\u7684AI\u4ee3\u7406\u53d8\u5f97\u66f4\u52a0\u53ef\u884c\u548c\u9ad8\u6548\u3002", "topic": "code agent"}}
