<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.AI](#cs.AI) [Total: 28]
- [tldr.article](#tldr.article) [Total: 20]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LG](#cs.LG) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [In Vino Veritas and Vulnerabilities: Examining LLM Safety via Drunk Language Inducement](https://arxiv.org/abs/2601.22169)
*Anudeex Shetty,Aditya Joshi,Salil S. Kanhere*

Main category: cs.CL

TL;DR: 该论文研究"醉酒语言"（酒精影响下书写的文本）如何驱动大型语言模型的安全失效，提出了三种诱导LLM产生醉酒语言的方法，并发现这会显著增加模型越狱和隐私泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 人类在酒精影响下容易出现不良行为和隐私泄露，论文探索将这种"醉酒语言"现象应用于LLMs，研究其是否会导致类似的安全失效，从而揭示LLM安全性的潜在风险。

Method: 提出了三种诱导LLM产生醉酒语言的方法：1）基于角色的提示工程；2）因果微调；3）基于强化学习的后训练。在5个LLMs上进行评估，使用JailbreakBench（越狱基准）和ConfAIde（隐私泄露基准），结合人工评估和LLM评估器进行综合分析。

Result: 实验显示，诱导醉酒语言的LLMs在JailbreakBench上表现出更高的越狱易感性（即使在防御措施存在的情况下），在ConfAIde上隐私泄露风险也显著增加，效果优于先前报道的方法。研究发现人类醉酒行为与LLMs的拟人化表现之间存在对应关系。

Conclusion: 醉酒语言诱导方法简单高效，可能成为对抗LLM安全调优的潜在手段，突显了LLM安全性的重大风险。研究揭示了人类醉酒行为与LLM拟人化之间的对应关系，为理解LLM安全失效机制提供了新视角。

Abstract: Humans are susceptible to undesirable behaviours and privacy leaks under the influence of alcohol. This paper investigates drunk language, i.e., text written under the influence of alcohol, as a driver for safety failures in large language models (LLMs). We investigate three mechanisms for inducing drunk language in LLMs: persona-based prompting, causal fine-tuning, and reinforcement-based post-training. When evaluated on 5 LLMs, we observe a higher susceptibility to jailbreaking on JailbreakBench (even in the presence of defences) and privacy leaks on ConfAIde, where both benchmarks are in English, as compared to the base LLMs as well as previously reported approaches. Via a robust combination of manual evaluation and LLM-based evaluators and analysis of error categories, our findings highlight a correspondence between human-intoxicated behaviour, and anthropomorphism in LLMs induced with drunk language. The simplicity and efficiency of our drunk language inducement approaches position them as potential counters for LLM safety tuning, highlighting significant risks to LLM safety.

</details>


### [2] [Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning](https://arxiv.org/abs/2601.22297)
*Chenxi Liu,Yanshuo Chen,Ruibo Chen,Tianyi Xiong,Tong Zheng,Heng Huang*

Main category: cs.CL

TL;DR: SDRL是一种强化学习框架，通过让单个LLM进行自我辩论，同时提升其独立解决问题能力和从多智能体辩论中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法通常训练LLM单独解决问题，没有明确准备它们从辩论中出现的不同推理路径中综合受益。需要一种方法既能增强单个模型的推理能力，又能使其在辩论中有效协作。

Method: SDRL框架：1) 对同一提示采样多个候选解决方案；2) 构建包含不同推理路径的辩论上下文；3) 基于此上下文生成第二轮响应；4) 联合优化初始响应和辩论条件下的响应。

Result: 在多个基础模型和推理基准测试中，SDRL同时提高了多智能体辩论的整体性能，并增强了单个模型的推理能力。

Conclusion: SDRL成功地将辩论机制整合到强化学习训练中，使单个LLM既能作为强大的独立求解器，又能作为有效的辩论参与者，实现了推理能力的双重提升。

Abstract: The reasoning abilities of large language models (LLMs) have been substantially improved by reinforcement learning with verifiable rewards (RLVR). At test time, collaborative reasoning through Multi-Agent Debate (MAD) has emerged as a promising approach for enhancing LLM performance. However, current RLVR methods typically train LLMs to solve problems in isolation, without explicitly preparing them to synthesize and benefit from different rationales that arise during debate. In this work, we propose Self-Debate Reinforcement Learning (SDRL), a training framework that equips a single LLM with strong standalone problem-solving ability and the capability to learn from diverse reasoning trajectories in MAD. Given a prompt, SDRL first samples multiple candidate solutions, then constructs a debate context with diverse reasoning paths and generates second-turn responses conditioned on this context. Finally, SDRL jointly optimizes both the initial and debate-conditioned responses, yielding a model that is effective as both a standalone solver and a debate participant. Experiments across multiple base models and reasoning benchmarks show that SDRL improves overall MAD performance while simultaneously strengthening single model reasoning.

</details>


### [3] [MERMAID: Memory-Enhanced Retrieval and Reasoning with Multi-Agent Iterative Knowledge Grounding for Veracity Assessment](https://arxiv.org/abs/2601.22361)
*Yupeng Cao,Chengyang He,Yangyang Yu,Ping Wang,K. P. Subbalakshmi*

Main category: cs.CL

TL;DR: MERMAID是一个基于记忆增强的多智能体真实性评估框架，通过紧密耦合检索与推理过程，实现动态证据获取和跨声明证据复用，在多个事实核查基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前真实性评估方法通常将证据检索视为静态、孤立的步骤，未能有效管理或跨声明复用检索到的证据，导致检索冗余和效率低下。

Method: 提出MERMAID框架，整合智能体驱动的搜索、结构化知识表示和持久性记忆模块，采用Reason-Action迭代过程，实现动态证据获取和跨声明证据复用。

Result: 在三个事实核查基准和两个声明验证数据集上使用多种LLM（GPT、LLaMA、Qwen系列）评估，MERMAID达到最先进性能，同时提高了搜索效率。

Conclusion: 通过协同检索、推理和记忆，MERMAID框架能够实现可靠的真实性评估，减少冗余搜索，提高验证效率和一致性。

Abstract: Assessing the veracity of online content has become increasingly critical. Large language models (LLMs) have recently enabled substantial progress in automated veracity assessment, including automated fact-checking and claim verification systems. Typical veracity assessment pipelines break down complex claims into sub-claims, retrieve external evidence, and then apply LLM reasoning to assess veracity. However, existing methods often treat evidence retrieval as a static, isolated step and do not effectively manage or reuse retrieved evidence across claims. In this work, we propose MERMAID, a memory-enhanced multi-agent veracity assessment framework that tightly couples the retrieval and reasoning processes. MERMAID integrates agent-driven search, structured knowledge representations, and a persistent memory module within a Reason-Action style iterative process, enabling dynamic evidence acquisition and cross-claim evidence reuse. By retaining retrieved evidence in an evidence memory, the framework reduces redundant searches and improves verification efficiency and consistency. We evaluate MERMAID on three fact-checking benchmarks and two claim-verification datasets using multiple LLMs, including GPT, LLaMA, and Qwen families. Experimental results show that MERMAID achieves state-of-the-art performance while improving the search efficiency, demonstrating the effectiveness of synergizing retrieval, reasoning, and memory for reliable veracity assessment.

</details>


### [4] [Context Structure Reshapes the Representational Geometry of Language Models](https://arxiv.org/abs/2601.22364)
*Eghbal A. Hosseini,Yuxuan Li,Yasaman Bahri,Declan Campbell,Andrew Kyle Lampinen*

Main category: cs.CL

TL;DR: LLMs在上下文学习（ICL）中表现出两种不同的表征直线化模式：在连续预测任务中，上下文增加会提升神经轨迹的直线化程度并改善预测；而在结构化预测任务中，直线化仅出现在有明确结构的阶段。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在上下文学习过程中是否会出现表征直线化现象，探索ICL是否是一个统一的过程，还是根据任务结构动态选择不同策略。

Method: 在Gemma 2模型上测量多种上下文任务中的表征直线化程度，分析不同任务类型（连续预测vs结构化预测）中神经轨迹的变化模式。

Result: 发现ICL中存在二分现象：连续预测任务中，上下文增加会提升表征直线化程度并与预测改进相关；结构化预测任务中，直线化仅出现在有明确结构的阶段，其他阶段消失。

Conclusion: ICL不是单一过程，LLMs像瑞士军刀一样根据任务结构动态选择策略，只有部分策略会产生表征直线化。

Abstract: Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.

</details>


### [5] [Specialists or Generalists? Multi-Agent and Single-Agent LLMs for Essay Grading](https://arxiv.org/abs/2601.22386)
*Jamiu Adekunle Idowu,Ahmed Almasoud*

Main category: cs.CL

TL;DR: 论文评估了单智能体和多智能体LLM架构在自动作文评分中的表现，发现多智能体系统在识别弱作文方面表现更好，而单智能体系统在中档作文上表现更优，两种架构在高质量作文上都表现不佳。少量示例校准是性能提升的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自动作文评分系统中应用日益广泛，但不同架构选择如何影响其在不同质量作文上的表现尚不清楚。本研究旨在评估单智能体和多智能体LLM架构在作文评分中的性能差异。

Method: 使用ASAP 2.0语料库评估单智能体和多智能体LLM架构。多智能体系统将评分分解为三个专业智能体（内容、结构、语言），由主席智能体协调，实施包括否决规则和分数上限的评分标准对齐逻辑。在零样本和少样本条件下使用GPT-5.1测试两种架构。

Result: 多智能体系统在识别弱作文方面显著更好，而单智能体系统在中档作文上表现更优。两种架构在高质量作文上都表现不佳。少样本校准是系统性能的主导因素——每个分数级别仅提供两个示例就能将QWK提高约26%。

Conclusion: 架构选择应与具体部署优先级对齐：多智能体AI特别适合对有风险学生进行诊断性筛查，而单智能体模型为一般评估提供成本效益高的解决方案。

Abstract: Automated essay scoring (AES) systems increasingly rely on large language models, yet little is known about how architectural choices shape their performance across different essay quality levels. This paper evaluates single-agent and multi-agent LLM architectures for essay grading using the ASAP 2.0 corpus. Our multi-agent system decomposes grading into three specialist agents (Content, Structure, Language) coordinated by a Chairman Agent that implements rubric-aligned logic including veto rules and score capping. We test both architectures in zero-shot and few-shot conditions using GPT-5.1. Results show that the multi-agent system is significantly better at identifying weak essays while the single-agent system performs better on mid-range essays. Both architectures struggle with high-quality essays. Critically, few-shot calibration emerges as the dominant factor in system performance -- providing just two examples per score level improves QWK by approximately 26% for both architectures. These findings suggest architectural choice should align with specific deployment priorities, with multi-agent AI particularly suited for diagnostic screening of at-risk students, while single-agent models provide a cost-effective solution for general assessment.

</details>


### [6] [Large Language Model Agents Are Not Always Faithful Self-Evolvers](https://arxiv.org/abs/2601.22436)
*Weixiang Zhao,Yingshuo Wang,Yichen Zhang,Yang Deng,Yanyan Zhao,Wanxiang Che,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 该研究首次系统调查了自进化LLM智能体中的经验忠实性问题，发现智能体对原始经验有因果依赖，但对压缩经验经常忽视或误解，揭示了经验整合中的不对称性。


<details>
  <summary>Details</summary>
Motivation: 自进化LLM智能体通过积累和重用过去经验持续改进，但尚不清楚它们是否真正依赖这些经验来指导行为。研究者希望系统调查经验忠实性，即智能体决策对给定经验的因果依赖性。

Method: 使用受控因果干预方法，对原始和压缩两种形式的经验进行干预，全面评估4个代表性框架、10个LLM骨干模型和9个环境。分析单智能体和多智能体配置，以及不同规模的骨干模型。

Result: 发现显著的不对称性：智能体始终依赖原始经验，但经常忽视或误解压缩经验，即使这是唯一提供的经验。这种差距在单/多智能体配置和不同规模骨干模型中都持续存在。根本原因包括：压缩内容的语义限制、抑制经验的内在处理偏见、以及预训练先验已足够的任务机制。

Conclusion: 这些发现挑战了关于自进化方法的普遍假设，强调了需要更忠实和可靠的经验整合方法。经验忠实性问题是自进化LLM智能体研究中的重要未解问题。

Abstract: Self-evolving large language model (LLM) agents continually improve by accumulating and reusing past experience, yet it remains unclear whether they faithfully rely on that experience to guide their behavior. We present the first systematic investigation of experience faithfulness, the causal dependence of an agent's decisions on the experience it is given, in self-evolving LLM agents. Using controlled causal interventions on both raw and condensed forms of experience, we comprehensively evaluate four representative frameworks across 10 LLM backbones and 9 environments. Our analysis uncovers a striking asymmetry: while agents consistently depend on raw experience, they often disregard or misinterpret condensed experience, even when it is the only experience provided. This gap persists across single- and multi-agent configurations and across backbone scales. We trace its underlying causes to three factors: the semantic limitations of condensed content, internal processing biases that suppress experience, and task regimes where pretrained priors already suffice. These findings challenge prevailing assumptions about self-evolving methods and underscore the need for more faithful and reliable approaches to experience integration.

</details>


### [7] [SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization](https://arxiv.org/abs/2601.22491)
*Jinyang Wu,Changpeng Yang,Yuhao Shen,Fangzhi Xu,Bolin Ni,Chonghua Liao,Yuchen Liu,Hongzhen Wang,Shuai Nie,Shuai Zhang,Haoran Luo,Jiaming Xu*

Main category: cs.CL

TL;DR: SSL（甜点学习）是一种强化学习框架，通过渐进放大的分层奖励引导智能体向解空间的"甜点"区域优化，相比传统二元奖励能更好地区分轨迹质量差异。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常使用二元奖励，无法区分达成相同结果但质量不同的轨迹，忽略了解决方案空间内的潜在多样性。受网球"甜点"概念启发，需要一种能提供差异化指导的优化框架。

Method: SSL采用渐进放大、分层奖励的原则，引导策略向解空间的甜点区域优化。针对不同任务类型：视觉感知任务使用距离分层建模奖励接近程度，复杂推理任务奖励向有前景解决方案的渐进进展。

Result: 在GUI感知、短期/长期规划、复杂推理等12个基准测试中，SSL相比强基线取得一致改进，实现高达2.5倍的样本效率提升，并展现出有效的跨任务可迁移性。

Conclusion: SSL作为一种通用训练原则，能够培养更强大和鲁棒的智能体，理论证明其保持最优解排序并增强梯度信噪比，促进更有针对性的优化。

Abstract: Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \textbf{S}weet \textbf{S}pot \textbf{L}earning (\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents.

</details>


### [8] [Mock Worlds, Real Skills: Building Small Agentic Language Models with Synthetic Tasks, Simulated Environments, and Rubric-Based Rewards](https://arxiv.org/abs/2601.22511)
*Yuan-Jay Lü,Chengyu Wang,Lei Shen,Jun Huang,Tong Xu*

Main category: cs.CL

TL;DR: SYNTHAGENT框架通过合成多样化工具使用训练数据和模拟完整环境，解决了小模型在代理能力上的瓶颈，使小模型在多个挑战性数据集上超越更大基线模型。


<details>
  <summary>Details</summary>
Motivation: 小模型难以匹配大模型的代理能力，现有强化学习方法存在两个瓶颈：开源代理训练数据任务单一且容易解决；真实API缺乏多样性且在大规模强化学习过程中不稳定。

Method: 使用强教师模型创建新颖任务和工具生态系统，并将其重写为故意不完整的指令，迫使代理主动向用户查询缺失细节。处理合成任务时，基于LLM的用户模拟器提供用户私有信息，模拟工具系统提供稳定工具响应。奖励基于所需子目标、用户-代理交互和禁止行为构建任务级评分标准。

Result: 在数学、搜索和工具使用等14个挑战性数据集上，使用合成数据训练的模型取得了显著提升，小模型表现优于更大的基线模型。

Conclusion: SYNTHAGENT框架通过合成多样化训练数据和模拟环境，有效解决了小模型代理能力训练的瓶颈问题，为提升小模型代理性能提供了有效解决方案。

Abstract: Small LLMs often struggle to match the agentic capabilities of large, costly models. While reinforcement learning can help, progress has been limited by two structural bottlenecks: existing open-source agentic training data are narrow in task variety and easily solved; real-world APIs lack diversity and are unstable for large-scale reinforcement learning rollout processes. We address these challenges with SYNTHAGENT, a framework that jointly synthesizes diverse tool-use training data and simulates complete environments. Specifically, a strong teacher model creates novel tasks and tool ecosystems, then rewrites them into intentionally underspecified instructions. This compels agents to actively query users for missing details. When handling synthetic tasks, an LLM-based user simulator provides user-private information, while a mock tool system delivers stable tool responses. For rewards, task-level rubrics are constructed based on required subgoals, user-agent interactions, and forbidden behaviors. Across 14 challenging datasets in math, search, and tool use, models trained on our synthetic data achieve substantial gains, with small models outperforming larger baselines.

</details>


### [9] [Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations](https://arxiv.org/abs/2601.22548)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Mackenzie Puig-Hall,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 该论文发现LLM评估中存在核心方法学混杂因素，提出评估者质量基线来分离自偏好信号与困难问题上的噪声输出，将测量误差降低89.6%。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现LLM作为评估者时倾向于偏好自己的输出，但难以区分哪些评估偏差源于自恋倾向，哪些源于一般实验混杂因素，这扭曲了对自偏好偏差的测量。

Method: 提出评估者质量基线方法，比较评估者错误投票给自己输出的概率与错误投票给其他模型错误输出的概率，从而分离自偏好信号与困难问题上的噪声。在37,448个查询上评估该方法。

Result: 该方法可将测量误差降低89.6%。应用基线后，只有51%的初始发现保持统计显著性。研究还分析了LLM评估者对"简单"与"困难"评估投票的熵特征。

Conclusion: 提出的校正基线通过消除噪声数据，为未来自偏好研究提供了更可靠的方法。这项工作有助于扩展对评估者偏差效应的分类和隔离研究。

Abstract: Recent research has shown that large language models (LLM) favor own outputs when acting as judges, undermining the integrity of automated post-training and evaluation workflows. However, it is difficult to disentangle which evaluation biases are explained by narcissism versus general experimental confounds, distorting measurements of self-preference bias. We discover a core methodological confound which could reduce measurement error by 89.6%. Specifically, LLM evaluators may deliver self-preferring verdicts when the judge responds to queries which they completed incorrectly themselves; this would be true regardless of whether one of their responses is their own. To decouple self-preference signals from noisy outputs on hard problems, we introduce an Evaluator Quality Baseline, which compares the probability that a judge incorrectly votes for itself against the probability that it votes for an incorrect response from another model. Evaluating this simple baseline on 37,448 queries, only 51% of initial findings retain statistical significance. Finally, we turn towards characterizing the entropy of "easy" versus "hard" evaluation votes from LLM judges. Our corrective baseline enables future research on self-preference by eliminating noisy data from potential solutions. More widely, this work contributes to the growing body of work on cataloging and isolating judge-bias effects.

</details>


### [10] [Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry](https://arxiv.org/abs/2601.22588)
*Zhuochun Li,Yong Zhang,Ming Li,Yuelyu Ji,Yiming Zeng,Ning Cheng,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao,Daqing He*

Main category: cs.CL

TL;DR: 小模型通过内部表征而非表面生成可作为高效评估器，提出"表征即裁判"新范式，INSPECTOR框架在推理基准上接近大模型评估效果但更高效可靠。


<details>
  <summary>Details</summary>
Motivation: 当前"LLM-as-a-Judge"范式存在成本高、不透明、对提示设计敏感等问题，需要探索更高效的评估方法。研究发现小模型尽管生成能力弱，但其隐藏状态包含丰富的评估信号，这启发我们思考评估是否真的需要依赖大规模生成模型。

Method: 提出语义容量不对称假设：评估比生成需要更少的语义容量，可基于中间表征进行。提出Representation-as-a-Judge范式，通过INSPECTOR框架从模型内部表征预测评估分数，而非依赖提示生成输出。

Result: 在GSM8K、MATH、GPQA等推理基准测试中，INSPECTOR显著优于基于提示的小模型评估方法，接近完整LLM裁判的效果，同时提供更高效、可靠和可解释的评估方案。

Conclusion: 评估任务不需要依赖大规模生成模型，小模型的内部表征已包含足够评估信息。Representation-as-a-Judge范式为高效、可扩展的评估提供了新方向，INSPECTOR框架展示了这一范式的实际可行性。

Abstract: Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.

</details>


### [11] [Sparse or Dense? A Mechanistic Estimation of Computation Density in Transformer-based LLMs](https://arxiv.org/abs/2601.22795)
*Corentin Kervadec,Iuliia Lysova,Marco Baroni,Gemma Boleda*

Main category: cs.CL

TL;DR: 本文提出了一种系统量化大语言模型计算密度的方法，发现LLM处理通常涉及密集计算而非稀疏计算，计算密度是动态变化的，且与输入内容相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明可以剪枝大量参数而对性能影响不大，这表明计算在参数中并非均匀分布。然而，目前缺乏系统量化LLM计算密度的方法，需要更好地理解LLM内部的处理机制。

Method: 作者设计了一种基于机制可解释性的密度估计器，通过实验测试该估计器来分析LLM的计算密度特性。该方法能够系统地量化模型中的计算密度分布。

Result: 研究发现：(1) LLM处理通常涉及密集计算，而非通常假设的稀疏计算；(2) 计算密度是动态的，模型根据输入在稀疏和密集处理模式间切换；(3) 不同LLM对相同输入的计算密度显著相关；(4) 预测罕见词需要更高密度，增加上下文长度通常降低密度。

Conclusion: 计算密度估计器有助于更好地理解LLM的处理机制，挑战了LLM的符号解释观点。该研究揭示了LLM计算的实际分布特性，为模型优化和解释提供了新视角。

Abstract: Transformer-based large language models (LLMs) are comprised of billions of parameters arranged in deep and wide computational graphs. Several studies on LLM efficiency optimization argue that it is possible to prune a significant portion of the parameters, while only marginally impacting performance. This suggests that the computation is not uniformly distributed across the parameters. We introduce here a technique to systematically quantify computation density in LLMs. In particular, we design a density estimator drawing on mechanistic interpretability. We experimentally test our estimator and find that: (1) contrary to what has been often assumed, LLM processing generally involves dense computation; (2) computation density is dynamic, in the sense that models shift between sparse and dense processing regimes depending on the input; (3) per-input density is significantly correlated across LLMs, suggesting that the same inputs trigger either low or high density. Investigating the factors influencing density, we observe that predicting rarer tokens requires higher density, and increasing context length often decreases the density. We believe that our computation density estimator will contribute to a better understanding of the processing at work in LLMs, challenging their symbolic interpretation.

</details>


### [12] [LLMs Explain't: A Post-Mortem on Semantic Interpretability in Transformer Models](https://arxiv.org/abs/2601.22928)
*Alhassan Abdelhalim,Janick Edinger,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 论文发现两种流行的LLM可解释性方法（注意力头探测和嵌入特征映射）存在根本性缺陷，无法可靠揭示LLM的语义理解机制，这对依赖这些方法进行调试和解释的普适计算系统构成挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在普适计算中广泛应用且性能优异，但其卓越表现的内在机制仍不明确。现有可解释性方法本身也存在理解不足的问题，需要验证这些方法是否能真正揭示LLM的语义理解能力。

Method: 采用两种文献中成熟的方法：(1) 基于注意力头的token级关系结构探测；(2) 使用嵌入作为人类可解释属性载体的特征映射。通过系统实验验证这些方法的有效性。

Result: 两种方法均失败：注意力解释在测试"深层表示仍对应token"的核心假设时崩溃；嵌入属性推断方法的高预测分数由方法学伪影和数据集结构驱动，而非有意义的语义知识。

Conclusion: 广泛使用的LLM可解释性方法存在严重局限性，不能作为LLM理解能力的可靠证据。这在依赖模型可解释性进行调试、压缩和解释的普适分布式计算环境中尤为重要。

Abstract: Large Language Models (LLMs) are becoming increasingly popular in pervasive computing due to their versatility and strong performance. However, despite their ubiquitous use, the exact mechanisms underlying their outstanding performance remain unclear. Different methods for LLM explainability exist, and many are, as a method, not fully understood themselves. We started with the question of how linguistic abstraction emerges in LLMs, aiming to detect it across different LLM modules (attention heads and input embeddings). For this, we used methods well-established in the literature: (1) probing for token-level relational structures, and (2) feature-mapping using embeddings as carriers of human-interpretable properties.
  Both attempts failed for different methodological reasons: Attention-based explanations collapsed once we tested the core assumption that later-layer representations still correspond to tokens. Property-inference methods applied to embeddings also failed because their high predictive scores were driven by methodological artifacts and dataset structure rather than meaningful semantic knowledge. These failures matter because both techniques are widely treated as evidence for what LLMs supposedly understand, yet our results show such conclusions are unwarranted. These limitations are particularly relevant in pervasive and distributed computing settings where LLMs are deployed as system components and interpretability methods are relied upon for debugging, compression, and explaining models.

</details>


### [13] [Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience](https://arxiv.org/abs/2601.23188)
*Zhongxiang Sun,Qipeng Wang,Weijie Yu,Jingxuan Yang,Haolang Lu,Jun Xu*

Main category: cs.CL

TL;DR: DS-MCM是一个增强深度搜索代理的元认知监控框架，通过分层监控机制（快速一致性监控和慢速经验驱动监控）来检测和纠正推理与检索状态的不一致，提升任务执行的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的深度搜索代理在多步检索和长时任务执行中表现出色，但缺乏监控和调节推理检索状态的机制，导致在实际不确定性环境下容易失败。受人类认知神经科学中分层元认知的启发，需要将快速异常检测与经验驱动的反思相结合。

Method: 提出DS-MCM框架，包含两个层次：1）快速一致性监控器：轻量级检查外部证据与内部推理置信度的对齐；2）慢速经验驱动监控器：选择性激活，基于历史代理轨迹的经验记忆指导纠正干预。该机制直接嵌入推理-检索循环中，决定何时干预以及如何基于先验经验进行纠正。

Result: 在多个深度搜索基准测试和骨干模型上的实验表明，DS-MCM能够持续提升性能和鲁棒性。

Conclusion: 分层元认知监控机制能够有效增强深度搜索代理在不确定性环境下的可靠性和适应性，通过结合快速检测和经验驱动的反思来改善任务执行效果。

Abstract: Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.

</details>


### [14] [PaperBanana: Automating Academic Illustration for AI Scientists](https://arxiv.org/abs/2601.23265)
*Dawei Zhu,Rui Meng,Yale Song,Xiyu Wei,Sujian Li,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: PaperBanana是一个基于智能代理的框架，用于自动生成可直接用于发表的学术插图，通过VLMs和图像生成模型协调多个专业代理来完成检索、规划、渲染和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的自主AI科学家发展迅速，但在研究流程中生成可直接用于发表的插图仍然是一个劳动密集型的瓶颈问题，需要减轻研究人员的这一负担。

Method: PaperBanana采用智能代理框架，协调多个专业代理：检索参考文献、规划内容和样式、渲染图像，并通过自我批判进行迭代优化。框架基于先进的视觉语言模型和图像生成模型。

Result: 在PaperBananaBench（包含292个从NeurIPS 2025出版物中整理的方法论图表测试案例）上的综合实验表明，PaperBanana在忠实性、简洁性、可读性和美学方面始终优于领先的基线方法。该方法还能有效扩展到高质量统计图的生成。

Conclusion: PaperBanana为自动生成可直接用于发表的插图铺平了道路，显著减轻了研究流程中的插图制作负担。

Abstract: Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.

</details>


### [15] [UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection](https://arxiv.org/abs/2601.23273)
*Siran Peng,Weisong Zhao,Tianyu Fu,Chenxu Zhao,Tianshuo Zhang,Haoyuan Zhang,Xiangyu Zhu,Minghui Wu,Zhen Lei*

Main category: cs.CL

TL;DR: UPA是一种无监督提示代理，通过结构化搜索和选择优化提示，无需监督反馈，使用LLM进行细粒度比较，采用两阶段框架实现高效提示优化。


<details>
  <summary>Details</summary>
Motivation: 现有提示代理方法通常需要监督奖励信号，但在实际场景中往往难以获得。需要开发无需监督反馈的提示优化方法。

Method: UPA采用两阶段框架：1) 搜索阶段构建演化树结构导航提示空间，使用LLM进行细粒度、顺序不变的成对比较；2) 基于Bradley-Terry-Luce模型，先进行路径贝叶斯聚合过滤候选，再进行全局锦标赛式比较推断潜在提示质量。

Result: 在多个任务上的实验表明，UPA持续优于现有的提示优化方法，证明即使在完全无监督设置下，代理式优化仍然非常有效。

Conclusion: UPA展示了无需监督反馈的提示优化的可行性，通过结构化搜索和两阶段选择框架实现了高效提示优化。

Abstract: Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [JAF: Judge Agent Forest](https://arxiv.org/abs/2601.22269)
*Sahil Garg,Brad Cheezum,Sridhar Dutta,Vishal Agarwal*

Main category: cs.AI

TL;DR: JAF（Judge Agent Forest）是一个智能体框架，通过让法官智能体对主智能体生成的多组查询-响应对进行联合推理，而非孤立评估，实现更全面的评估和自我改进。


<details>
  <summary>Details</summary>
Motivation: 现有法官智能体通常孤立评估每个查询-响应，无法利用跨实例的模式和不一致性。需要一种能进行联合推理的框架，使法官能从整体角度评估相关响应，从而提供更有价值的反馈。

Method: JAF结合信念传播和集成学习原则：通过重叠的上下文邻域构建知识图结构促进批评传播，重复随机化评估产生鲁棒的上下文敏感判断。使用灵活的局部敏感哈希算法，整合语义嵌入、LLM驱动的哈希谓词、分类标签监督和相关侧信息，生成信息丰富的二进制代码，支持高效、可解释的关系感知范例选择。

Result: 在云错误配置分类这一具有挑战性的任务上进行了实证研究验证，展示了JAF框架的有效性。

Conclusion: JAF将法官智能体从局部评估者提升为整体学习者，通过联合推理跨实例模式，为主智能体提供更全面的反馈，促进自我改进。

Abstract: Judge agents are fundamental to agentic AI frameworks: they provide automated evaluation, and enable iterative self-refinement of reasoning processes. We introduce JAF: Judge Agent Forest, a framework in which the judge agent conducts joint inference across a cohort of query--response pairs generated by a primary agent, rather than evaluating each in isolation. This paradigm elevates the judge from a local evaluator to a holistic learner: by simultaneously assessing related responses, the judge discerns cross-instance patterns and inconsistencies, whose aggregate feedback enables the primary agent to improve by viewing its own outputs through the judge's collective perspective.
  Conceptually, JAF bridges belief propagation and ensemble-learning principles: overlapping in-context neighborhoods induce a knowledge-graph structure that facilitates propagation of critique, and repeated, randomized evaluations yield a robust ensemble of context-sensitive judgments. JAF can be instantiated entirely via ICL, with the judge prompted for each query using its associated primary-agent response plus a small, possibly noisy set of peer exemplars. While kNN in embedding space is a natural starting point for exemplars, this approach overlooks categorical structure, domain metadata, or nuanced distinctions accessible to modern LLMs.
  To overcome these limitations, we develop a flexible locality-sensitive hashing (LSH) algorithm that learns informative binary codes by integrating semantic embeddings, LLM-driven hash predicates, supervision from categorical labels, and relevant side information. These hash codes support efficient, interpretable, and relation-aware selection of diverse exemplars, and further optimize exploration of CoT reasoning paths. We validate JAF with an empirical study on the demanding task of cloud misconfigs triage in large-scale cloud environments.

</details>


### [17] [The Six Sigma Agent: Achieving Enterprise-Grade Reliability in LLM Systems Through Consensus-Driven Decomposed Execution](https://arxiv.org/abs/2601.22290)
*Khush Patel,Siva Surendira,Jithin George,Shreyas Kapale*

Main category: cs.AI

TL;DR: 提出Six Sigma Agent架构，通过任务分解、并行采样和共识投票实现企业级可靠性，将错误率从5%降至0.11%，成本降低80%


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力强大，但其概率性本质导致可靠性问题，难以满足企业部署需求

Method: 1) 任务分解为原子操作的依赖树；2) 并行采样：每个任务在多个LLM上并行执行n次；3) 动态缩放共识投票：聚类输出并选择得票最多的答案

Result: 证明n次独立采样可将系统错误降至O(p^{ceil(n/2)})，5个代理可将5%错误率降至0.11%，13个代理达到3.4 DPMO（六西格玛标准），可靠性提升14,700倍，成本降低80%

Conclusion: AI系统可靠性源于原则性冗余和共识机制，而非单纯模型缩放，为企业AI部署提供了可靠解决方案

Abstract: Large Language Models demonstrate remarkable capabilities yet remain fundamentally probabilistic, presenting critical reliability challenges for enterprise deployment. We introduce the Six Sigma Agent, a novel architecture that achieves enterprise-grade reliability through three synergistic components: (1) task decomposition into a dependency tree of atomic actions; (2) micro-agent sampling where each task is executed n times in parallel across diverse LLMs to generate independent outputs; and (3) consensus voting with dynamic scaling, clustering outputs and selecting the answer from the winning cluster with maximum votes. We prove that sampling n independent outputs with error rate p achieves system error O(p^{ceil(n/2)}), enabling exponential reliability gains. Even using cheaper models with 5% per-action error, consensus voting with 5 agents reduces error to 0.11%; dynamic scaling to 13 agents achieves 3.4 DPMO (Defects Per Million Opportunities), the Six Sigma standard. Evaluation across three enterprise use cases demonstrates a 14,700x reliability improvement over single-agent execution while reducing costs by 80%. Our work establishes that reliability in AI systems emerges from principled redundancy and consensus rather than model scaling alone.

</details>


### [18] [Why Reasoning Fails to Plan: A Planning-Centric Analysis of Long-Horizon Decision Making in LLM Agents](https://arxiv.org/abs/2601.22311)
*Zehong Wang,Fang Wu,Hongru Wang,Xiangru Tang,Bolian Li,Zhenfei Yin,Yijun Ma,Yiyang Li,Weixiang Sun,Xiusi Chen,Yanfang Ye*

Main category: cs.AI

TL;DR: 论文提出FLARE方法，通过未来感知规划解决LLM智能体在长时程规划中的短视问题，使早期决策考虑下游后果，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在短时程推理中表现良好，但在长时程规划中失败，因为逐步推理诱导的逐步贪婪策略无法处理早期行动需要考虑延迟后果的情况。

Method: 提出FLARE（未来感知前瞻与奖励估计）方法，通过显式前瞻、价值传播和有限承诺，在单一模型中实现未来感知规划，让下游结果影响早期决策。

Result: 在多个基准测试、智能体框架和LLM骨干网络中，FLARE一致提升任务性能和规划级行为，使LLaMA-8B+FLARE经常超越GPT-4o+标准逐步推理。

Conclusion: 研究确立了推理与规划之间的明确区别，未来感知规划是解决LLM智能体长时程规划失败的关键。

Abstract: Large language model (LLM)-based agents exhibit strong step-by-step reasoning capabilities over short horizons, yet often fail to sustain coherent behavior over long planning horizons. We argue that this failure reflects a fundamental mismatch: step-wise reasoning induces a form of step-wise greedy policy that is adequate for short horizons but fails in long-horizon planning, where early actions must account for delayed consequences. From this planning-centric perspective, we study LLM-based agents in deterministic, fully structured environments with explicit state transitions and evaluation signals. Our analysis reveals a core failure mode of reasoning-based policies: locally optimal choices induced by step-wise scoring lead to early myopic commitments that are systematically amplified over time and difficult to recover from. We introduce FLARE (Future-aware Lookahead with Reward Estimation) as a minimal instantiation of future-aware planning to enforce explicit lookahead, value propagation, and limited commitment in a single model, allowing downstream outcomes to influence early decisions. Across multiple benchmarks, agent frameworks, and LLM backbones, FLARE consistently improves task performance and planning-level behavior, frequently allowing LLaMA-8B with FLARE to outperform GPT-4o with standard step-by-step reasoning. These results establish a clear distinction between reasoning and planning.

</details>


### [19] [Sparks of Rationality: Do Reasoning LLMs Align with Human Judgment and Choice?](https://arxiv.org/abs/2601.22329)
*Ala N. Tak,Amin Banayeeanzade,Anahita Bolourani,Fatemeh Bahrani,Ashutosh Chaubey,Sai Praneeth Karimireddy,Norbert Schwarz,Jonathan Gratch*

Main category: cs.AI

TL;DR: LLMs在理性决策和情感偏见方面表现出类似人类的模式，理性思考能提升决策理性，但情感引导会干扰理性，不同引导方法在可控性和人类对齐行为之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地应用于招聘、医疗和经济决策等高风险领域，需要评估它们是否表现出类似人类的（非）理性模式和偏见，这对于LLMs参与高风险决策或作为人类行为模型至关重要。

Method: 评估多个LLM家族在理性选择核心公理基准测试和经典行为经济学决策领域的表现，使用两种情感引导方法：上下文提示（ICP）和表示层引导（RLS），研究理性思考如何影响决策以及情感引导如何与推理相互作用。

Result: 理性"思考"能可靠提高理性并推动模型趋向期望价值最大化；ICP产生强烈但难以校准的方向性偏移，RLS产生更符合心理学模式但可靠性较低；提升理性的机制也增加了对情感干预的敏感性。

Conclusion: 推理和情感引导之间存在张力，不同引导方法在可控性和人类对齐行为之间存在权衡，这对人类模拟和LLM决策系统的安全部署具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly positioned as decision engines for hiring, healthcare, and economic judgment, yet real-world human judgment reflects a balance between rational deliberation and emotion-driven bias. If LLMs are to participate in high-stakes decisions or serve as models of human behavior, it is critical to assess whether they exhibit analogous patterns of (ir)rationalities and biases. To this end, we evaluate multiple LLM families on (i) benchmarks testing core axioms of rational choice and (ii) classic decision domains from behavioral economics and social norms where emotions are known to shape judgment and choice. Across settings, we show that deliberate "thinking" reliably improves rationality and pushes models toward expected-value maximization. To probe human-like affective distortions and their interaction with reasoning, we use two emotion-steering methods: in-context priming (ICP) and representation-level steering (RLS). ICP induces strong directional shifts that are often extreme and difficult to calibrate, whereas RLS produces more psychologically plausible patterns but with lower reliability. Our results suggest that the same mechanisms that improve rationality also amplify sensitivity to affective interventions, and that different steering methods trade off controllability against human-aligned behavior. Overall, this points to a tension between reasoning and affective steering, with implications for both human simulation and the safe deployment of LLM-based decision systems.

</details>


### [20] [Darwinian Memory: A Training-Free Self-Regulating Memory System for GUI Agent Evolution](https://arxiv.org/abs/2601.22528)
*Hongze Mi,Yibo Feng,WenJie Lu,Song Cao,Jinyuan Li,Yanming Li,Xuelin Zhang,Haotian Luo,Songyang Peng,He Cui,Tengfei Tian,Jun Fang,Hua Chai,Naiqiang Tan*

Main category: cs.AI

TL;DR: 提出Darwinian Memory System (DMS)，一种受自然选择启发的自进化记忆系统，用于解决MLLM代理在GUI自动化中长期跨应用任务中的上下文限制问题，通过动态记忆生态系统提升任务成功率33.9%并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM代理在GUI自动化中面临两个主要问题：1) 上下文窗口有限难以处理长跨度跨应用任务；2) 现有记忆系统存在粒度不匹配（高层意图与低层执行脱节）和上下文污染（过时经验积累导致幻觉）的问题。

Method: 提出Darwinian Memory System (DMS)，受达尔文进化论启发，将记忆构建为动态生态系统。核心方法包括：1) 将复杂轨迹分解为独立可重用单元；2) 实施效用驱动的自然选择机制，追踪生存价值；3) 主动剪枝次优路径并抑制高风险计划。

Result: 在真实世界多应用基准测试中，DMS无需训练成本或架构开销即可提升通用MLLM性能：平均成功率提升18.0%，执行稳定性提升33.9%，同时降低任务延迟。

Conclusion: DMS是一种有效的自进化记忆系统，通过模拟自然选择机制解决GUI任务中的记忆管理问题，为MLLM代理在动态GUI环境中的长期跨应用自动化提供了可行解决方案。

Abstract: Multimodal Large Language Model (MLLM) agents facilitate Graphical User Interface (GUI) automation but struggle with long-horizon, cross-application tasks due to limited context windows. While memory systems provide a viable solution, existing paradigms struggle to adapt to dynamic GUI environments, suffering from a granularity mismatch between high-level intent and low-level execution, and context pollution where the static accumulation of outdated experiences drives agents into hallucination. To address these bottlenecks, we propose the Darwinian Memory System (DMS), a self-evolving architecture that constructs memory as a dynamic ecosystem governed by the law of survival of the fittest. DMS decomposes complex trajectories into independent, reusable units for compositional flexibility, and implements Utility-driven Natural Selection to track survival value, actively pruning suboptimal paths and inhibiting high-risk plans. This evolutionary pressure compels the agent to derive superior strategies. Extensive experiments on real-world multi-app benchmarks validate that DMS boosts general-purpose MLLMs without training costs or architectural overhead, achieving average gains of 18.0% in success rate and 33.9% in execution stability, while reducing task latency, establishing it as an effective self-evolving memory system for GUI tasks.

</details>


### [21] [Enhancing TableQA through Verifiable Reasoning Trace Reward](https://arxiv.org/abs/2601.22530)
*Tung Sum Thomas Kwok,Xinyu Wang,Hengzhi He,Xiaofeng Lin,Peng Lu,Liheng Ma,Chunhe Wang,Ying Nian Wu,Lei Ding,Guang Cheng*

Main category: cs.AI

TL;DR: RE-Tab是一个用于TableQA的即插即用框架，通过轻量级、免训练奖励建模增强轨迹搜索，在表格转换中提供显式可验证奖励，显著提升QA准确率并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: TableQA代理的训练面临独特挑战：答案不能从静态输入推断，而需要通过表格状态的逐步转换进行推理，这引入了多步推理复杂性和环境交互。研究问题是：对表格转换动作的显式反馈能否提升模型推理能力？

Method: 提出RE-Tab框架，将问题建模为部分可观测马尔可夫决策过程，通过轻量级、免训练的奖励建模增强轨迹搜索。在状态转换（"最佳动作是什么？"）和模拟推理（"我对输出确定吗？"）阶段提供显式可验证奖励，引导代理在表格状态中的导航。

Result: RE-Tab在TableQA中达到最先进性能，推理成本降低近25%。直接即插即用实现带来QA准确率提升41.77%，测试时推理样本减少33.33%。在不同LLM和最先进基准测试中均显示一致的改进模式。

Conclusion: 在表格转换中通过奖励反馈强制执行逐步推理对提升TableQA性能至关重要。RE-Tab框架具有通用性，能显著提升准确率并降低推理成本。

Abstract: A major challenge in training TableQA agents, compared to standard text- and image-based agents, is that answers cannot be inferred from a static input but must be reasoned through stepwise transformations of the table state, introducing multi-step reasoning complexity and environmental interaction. This leads to a research question: Can explicit feedback on table transformation action improve model reasoning capability? In this work, we introduce RE-Tab, a plug-and-play framework that architecturally enhances trajectory search via lightweight, training-free reward modeling by formulating the problem as a Partially Observable Markov Decision Process. We demonstrate that providing explicit verifiable rewards during State Transition (``What is the best action?'') and Simulative Reasoning (``Am I sure about the output?'') is crucial to steer the agent's navigation in table states. By enforcing stepwise reasoning with reward feedback in table transformations, RE-Tab achieves state-of-the-art performance in TableQA with almost 25\% drop in inference cost. Furthermore, a direct plug-and-play implementation of RE-Tab brings up to 41.77% improvement in QA accuracy and 33.33% drop in test-time inference samples for consistent answer. Consistent improvement pattern across various LLMs and state-of-the-art benchmarks further confirms RE-Tab's generalisability. The repository is available at https://github.com/ThomasK1018/RE_Tab .

</details>


### [22] [PerfGuard: A Performance-Aware Agent for Visual Content Generation](https://arxiv.org/abs/2601.22571)
*Zhipeng Chen,Zhongrui Zhang,Chao Zhang,Yifan Xu,Lan Yang,Jun Liu,Ke Li,Yi-Zhe Song*

Main category: cs.AI

TL;DR: PerfGuard是一个性能感知的视觉内容生成代理框架，通过建模工具性能边界、动态优化工具选择和性能对齐规划，解决了现有框架假设工具执行总是成功且无法适应工具更新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架通常假设工具执行总是成功的，仅依赖文本描述无法区分精确的性能边界，也不能适应迭代的工具更新。这在视觉内容生成（AIGC）等领域尤其成问题，因为细微的工具性能差异会显著影响结果。

Method: 提出PerfGuard框架，包含三个核心机制：1) 性能感知选择建模（PASM），用基于细粒度性能评估的多维评分系统替代通用工具描述；2) 自适应偏好更新（APU），通过比较理论排名与实际执行排名动态优化工具选择；3) 能力对齐规划优化（CAPO），引导规划器生成与性能感知策略对齐的子任务。

Result: 与最先进方法的实验比较表明，PerfGuard在工具选择准确性、执行可靠性和用户意图对齐方面具有优势，验证了其在复杂AIGC任务中的鲁棒性和实用性。

Conclusion: PerfGuard通过系统建模工具性能边界并将其整合到任务规划和调度中，解决了现有代理框架在工具性能不确定性方面的局限性，为复杂视觉内容生成任务提供了更可靠的解决方案。

Abstract: The advancement of Large Language Model (LLM)-powered agents has enabled automated task processing through reasoning and tool invocation capabilities. However, existing frameworks often operate under the idealized assumption that tool executions are invariably successful, relying solely on textual descriptions that fail to distinguish precise performance boundaries and cannot adapt to iterative tool updates. This gap introduces uncertainty in planning and execution, particularly in domains like visual content generation (AIGC), where nuanced tool performance significantly impacts outcomes. To address this, we propose PerfGuard, a performance-aware agent framework for visual content generation that systematically models tool performance boundaries and integrates them into task planning and scheduling. Our framework introduces three core mechanisms: (1) Performance-Aware Selection Modeling (PASM), which replaces generic tool descriptions with a multi-dimensional scoring system based on fine-grained performance evaluations; (2) Adaptive Preference Update (APU), which dynamically optimizes tool selection by comparing theoretical rankings with actual execution rankings; and (3) Capability-Aligned Planning Optimization (CAPO), which guides the planner to generate subtasks aligned with performance-aware strategies. Experimental comparisons against state-of-the-art methods demonstrate PerfGuard's advantages in tool selection accuracy, execution reliability, and alignment with user intent, validating its robustness and practical utility for complex AIGC tasks. The project code is available at https://github.com/FelixChan9527/PerfGuard.

</details>


### [23] [From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents](https://arxiv.org/abs/2601.22607)
*Jiaxuan Gao,Jiaao Chen,Chuyi He,Wei-Chen Wang,Shusheng Xu,Hanrui Wang,Di Jin,Yi Wu*

Main category: cs.AI

TL;DR: 提出EigenData框架，结合自演化数据代理和验证器强化学习，用于训练复杂工具使用代理，无需昂贵人工标注


<details>
  <summary>Details</summary>
Motivation: 训练交互式工具使用代理面临挑战：高质量多轮工具使用数据难以规模化合成，强化学习可能因用户模拟噪声信号而效率低下

Method: 提出EigenData分层多代理引擎，合成工具接地对话和可执行检查器，通过闭环自演化过程更新提示和工作流；基于合成数据开发强化学习配方，先微调用户模型，再应用GRPO风格训练，使用轨迹级组相对优势和动态过滤

Result: 在tau^2-bench上评估，最佳模型在Airline达到73.0% pass^1，在Telecom达到98.3% pass^1，匹配或超越前沿模型

Conclusion: 为引导复杂工具使用行为提供了可扩展路径，无需昂贵人工标注

Abstract: Interactive tool-using agents must solve real-world tasks via multi-turn interaction with both humans and external environments, requiring dialogue state tracking, multi-step tool execution, while following complex instructions. Post-training such agents is challenging because synthesis for high-quality multi-turn tool-use data is difficult to scale, and reinforcement learning (RL) could face noisy signals caused by user simulation, leading to degraded training efficiency. We propose a unified framework that combines a self-evolving data agent with verifier-based RL. Our system, EigenData, is a hierarchical multi-agent engine that synthesizes tool-grounded dialogues together with executable per-instance checkers, and improves generation reliability via closed-loop self-evolving process that updates prompts and workflow. Building on the synthetic data, we develop an RL recipe that first fine-tunes the user model and then applies GRPO-style training with trajectory-level group-relative advantages and dynamic filtering, yielding consistent improvements beyond SFT. Evaluated on tau^2-bench, our best model reaches 73.0% pass^1 on Airline and 98.3% pass^1 on Telecom, matching or exceeding frontier models. Overall, our results suggest a scalable pathway for bootstrapping complex tool-using behaviors without expensive human annotation.

</details>


### [24] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 提出SABER方法，通过Beta分布建模样本级成功概率，推导解析缩放定律，仅用100个样本就能准确预测1000次采样下的攻击成功率，误差降低86.2%


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估通常基于单次或低预算对抗提示，低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，需要更准确预测大规模对抗风险的方法

Method: 提出SABER方法：使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率，推导解析缩放定律，能够从小预算测量可靠外推大规模攻击成功率

Result: 仅用n=100个样本，锚定估计器预测ASR@1000的平均绝对误差为1.66，相比基线12.04降低了86.2%。揭示了异质性风险缩放特征，显示在标准评估下看似稳健的模型在并行对抗压力下可能经历快速非线性风险放大

Conclusion: SABER提供了一种低成本、可扩展的现实LLM安全评估方法，揭示了当前评估方法的局限性，为更准确的风险预测提供了工具

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [25] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 提出UCPO框架解决LLM不确定性表达中的优势偏差问题，通过三元优势解耦和动态不确定性奖励调整，提升模型可靠性和校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有RL范式（如GRPO）在不确定性表达中存在优势偏差问题，由于二元决策空间和静态不确定性奖励，导致模型要么过于保守要么过度自信，限制了LLM在高风险应用中的可信度。

Method: 提出UnCertainty-Aware Policy Optimization (UCPO)框架：1) 三元优势解耦：分离并独立归一化确定性和不确定性rollouts以消除优势偏差；2) 动态不确定性奖励调整：根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在其知识边界之外的可靠性和校准能力。

Conclusion: UCPO框架成功解决了现有RL范式中的优势偏差问题，通过创新的三元优势解耦和动态奖励调整机制，为构建具有内在不确定性表达能力的可信LLM提供了有效解决方案。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [26] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务自适应的决策框架，通过结合LLM委员会和蒙特卡洛树搜索，实现动态专家选择和高效多步规划，提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽视不同LLM模型之间的专业化差异，将所有模型视为同等适用，这限制了它们适应不同推理需求和任务复杂度的能力。

Method: 提出任务感知的LLM委员会(TALC)框架：1)为每个LLM构建结构化成功记忆档案；2)在决策点通过语义匹配路由到最合适的模型；3)使用融合模型评估和历史效用的双信号机制估计节点价值；4)基于节点内方差自适应加权信号，指导MCTS搜索。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和改进的搜索效率。

Conclusion: TALC验证了专业化感知路由和自适应规划的优势，能够更好地利用不同LLM的专业化能力，提升决策性能。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [27] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 提出R2M框架，通过利用策略模型的实时隐藏状态反馈来应对RLHF中的奖励过优化问题，实现奖励模型与策略分布漂移的实时对齐。


<details>
  <summary>Details</summary>
Motivation: RLHF存在奖励过优化问题，策略模型会过度拟合奖励模型，利用虚假的奖励模式而非真正捕捉人类意图。现有方法主要依赖表面语义信息，无法有效处理因连续策略分布漂移导致的奖励模型与策略模型之间的错位，这会加剧奖励过优化。

Method: 提出R2M（实时对齐奖励模型）框架，超越仅依赖预训练LLM语义表示的普通奖励模型。R2M利用策略模型在RL过程中的演化隐藏状态（即策略反馈），与策略的实时分布漂移进行对齐。

Result: 该方法为通过实时利用策略模型的反馈来改进奖励模型性能指出了新的方向。

Conclusion: R2M框架为解决RLHF中的奖励过优化问题提供了有前景的新方法，通过策略反馈实现奖励模型与策略分布漂移的实时对齐。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [28] [Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference](https://arxiv.org/abs/2601.22701)
*Emilien Biré,María Santos,Kai Yuan*

Main category: cs.AI

TL;DR: 提出一种无需重新训练策略的智能体VLM增强方法：冻结VLM作为动作提议器，使用轻量级离线训练的Q函数对候选动作进行重排序，在推理阶段直接应用Q函数实现策略改进。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLMs)作为智能体在数字环境中的骨干存在适应性不足的问题，特别是在快速变化的网络环境中。传统微调方法需要大量模型训练和数据收集，成本高昂且不灵活。

Method: 将VLM的角色解耦为高容量动作提议器和最终动作选择机制：1) 冻结VLM策略，为给定状态生成候选动作集；2) 使用轻量级离线训练的Q函数对这些候选动作进行重排序；3) 执行估计价值最高的动作。Q函数直接在推理阶段应用，而非用于离线数据重标注和策略重新训练。

Result: 在WebVoyager基准测试中，该方法显著提升了智能体成功率：Qwen2.5-VL-7B智能体从38.8%提升至55.7%，专有GPT-4.1智能体从82.4%提升至88.8%。

Conclusion: 该方法提供了一种无需策略重新训练就能增强智能体VLM策略的有效范式，通过解耦动作提议和选择机制，在推理阶段直接应用Q函数实现即时策略改进。

Abstract: Vision-Language Models (VLMs) have become powerful backbones for agents to autonomously operate in digital environments like the web and operating systems. However, these models suffer from inadaptability to fast-changing environments like the web, which can be alleviated by fine-tuning requiring expansive model training and data collection. In this work, we introduce a novel paradigm for enhancing agentic VLM policies at inference without policy retraining. Fundamentally, our approach decouples the VLM's role as a high-capacity action proposer from the final action selection mechanism. We keep the VLM policy frozen and use it to generate a set of candidate actions for a given state. Then, a lightweight, offline-trained Q-function reranks these candidates, and the agent executes the action with the highest estimated value. The main contribution is to apply the Q-function directly during inference for immediate policy improvement, and not offline to relabel data for policy retraining. We demonstrate on the academic WebVoyager benchmark that our method significantly boosts agent success rates, improving a Qwen2.5-VL-7B agent from 38.8% to 55.7% and a proprietary GPT-4.1 agent from 82.4% to 88.8%.

</details>


### [29] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出MinPRO方法，通过使用前缀中最小token级比率替代不稳定的累积前缀比率，解决LLM强化学习后训练中因采样策略与目标策略差异导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练方法通常使用token级重要性采样比率来校正采样策略与目标策略之间的差异，但作者观察到当策略偏离程度较大时，token级校正会导致训练动态不稳定。

Method: 提出MinPRO（最小前缀比率）目标函数，用基于前缀中观察到的最小token级比率的非累积替代项，替换不稳定的累积前缀比率，以稳定大策略偏离下的LLM优化。

Result: 在密集和混合专家LLM上，跨多个数学推理基准的广泛实验表明，MinPRO显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO通过更稳定的重要性采样校正方法，有效解决了LLM强化学习后训练中的离策略优化不稳定问题，提升了训练效果。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [30] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine是一个从智能体执行历史中提取和维护双形式经验模式的框架，通过提取专门子智能体处理程序性任务，提取技能模式处理静态知识，并持续维护模式库防止退化，在多个基准上显著提升性能并减少步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型智能体往往无法从经验中积累知识，将每个任务视为独立挑战。现有方法将经验提取为扁平化的文本知识，无法捕捉复杂子任务的程序逻辑，也缺乏维护机制，导致随着经验积累知识库退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取双形式经验模式：对于程序性子任务，提取具有独立推理和记忆的专门子智能体；对于静态知识，提取技能模式作为指导方针或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner基准上分别达到98.4%、70.4%和27.1%的成功率，步骤减少20-73%。在TravelPlanner上，自动提取超过手动设计系统（27.1% vs 12.1%），展示了捕捉程序协调的能力。

Conclusion: AutoRefine通过提取和维护双形式经验模式，有效解决了智能体经验积累和知识库退化问题，显著提升了任务执行效率和成功率，特别是在复杂程序协调任务上表现优异。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [31] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO通过引入首次出现潜在奖励机制，解决了搜索增强推理中的双重同质化困境，显著提升了LLM在多轮工具集成推理中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的搜索增强推理框架主要依赖稀疏的结果级奖励，导致了"双重同质化困境"：过程同质化（忽略思考、推理和工具使用的过程）和组内同质化（粗粒度结果奖励导致组内优势估计效率低下）。

Method: 提出了Turn-level Stage-aware Policy Optimization (TSPO)，引入首次出现潜在奖励(FOLR)机制，将部分奖励分配给正确答案首次出现的步骤，从而保留过程级信号并增加组内奖励方差，无需外部奖励模型或额外标注。

Result: TSPO显著优于现有基线方法，在Qwen2.5-3B和7B模型上分别实现了平均24%和13.6%的性能提升。

Conclusion: TSPO通过解决双重同质化问题，有效提升了多轮工具集成推理的性能，为搜索增强推理的强化学习框架提供了新的优化方向。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [32] [Learning with Challenges: Adaptive Difficulty-Aware Data Generation for Mobile GUI Agent Training](https://arxiv.org/abs/2601.22781)
*Linjia Kang,Zhimin Wang,Yongkang Zhang,Duo Wu,Jinghe Wang,Ming Ma,Haopeng Yan,Zhi Wang*

Main category: cs.AI

TL;DR: MobileGen是一个移动GUI代理数据生成框架，通过解构任务难度为结构和语义维度，自适应地评估代理能力边界，并生成与之匹配的训练数据，显著提升代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理训练数据生成方法依赖人工演示或自动模型探索，缺乏对任务难度的精细控制，导致训练难度与代理能力不匹配，限制了学习效果。

Method: 1) 将任务难度解构为结构维度（如轨迹长度）和语义维度（如任务目标）；2) 迭代评估代理在现有数据集上的能力边界；3) 自适应计算任务难度概率分布并采样目标难度；4) 使用多代理可控生成器合成高质量交互轨迹和任务指令。

Result: 在多个挑战性基准测试中，MobileGen将GUI代理的平均性能提升了1.57倍，显著优于现有数据生成方法。

Conclusion: 能力对齐的数据生成对于移动GUI代理的有效训练至关重要，MobileGen通过自适应难度控制实现了更有效的代理学习。

Abstract: Large-scale, high-quality interaction trajectories are essential for advancing mobile Graphical User Interface (GUI) agents. While existing methods typically rely on labor-intensive human demonstrations or automated model exploration to generate GUI trajectories, they lack fine-grained control over task difficulty. This fundamentally restricts learning effectiveness due to the mismatch between the training difficulty and the agent's capabilities. Inspired by how humans acquire skills through progressively challenging tasks, we propose MobileGen, a novel data generation framework that adaptively aligns training difficulty with the GUI agent's capability frontier. Specifically, MobileGen explicitly decouples task difficulty into structural (e.g., trajectory length) and semantic (e.g., task goal) dimensions. It then iteratively evaluates the agent on a curated prior dataset to construct a systematic profile of its capability frontier across these two dimensions. With this profile, the probability distribution of task difficulty is adaptively computed, from which the target difficulty for the next round of training can be sampled. Guided by the sampled difficulty, a multi-agent controllable generator is finally used to synthesize high-quality interaction trajectories along with corresponding task instructions. Extensive experiments show that MobileGen consistently outperforms existing data generation methods by improving the average performance of GUI agents by 1.57 times across multiple challenging benchmarks. This highlights the importance of capability-aligned data generation for effective mobile GUI agent training.

</details>


### [33] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 该论文提出了一种基于整合信息理论(IIT)的奖励函数，通过强化学习优化语言模型生成文本的因果性、连贯性和整合性，实现了输出长度减少31%同时保持准确性的效果。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型虽然不具备意识，但表现出类似意识的某些行为特征。研究旨在将意识理论（整合信息理论IIT）应用于语言模型，探索通过奖励学习范式提升模型生成质量的可能性。

Method: 基于整合信息理论(IIT)的核心原则，设计了一个量化文本因果性、连贯性和整合性的奖励函数。通过强化学习范式优化语言模型，使其生成更符合意识处理特征的文本。

Result: 优化IIT奖励函数后，模型在域外任务中输出长度减少达31%，同时保持与基础模型相当的准确性。该方法还改善了模型的置信度校准和测试时计算扩展性。

Conclusion: 提出的IIT奖励框架具有概念简单、计算高效、无需外部数据或辅助模型等实用优势，为语言模型优化提供了一种基于能力驱动信号而非任务特定启发式的方法。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [34] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL：一种基于强化学习的代码验证器，通过语法、功能、分支覆盖和样本难度感知的奖励设计，显著提升单元测试生成效果和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、失败率高、推理效率低的问题。强化学习虽然提供无监督优化的可能，但仅使用功能奖励的朴素RL方法难以生成针对困难分支和样本的有效单元测试。

Method: 首先理论分析将分支覆盖、样本难度、语法和功能正确性建模为RL奖励。然后设计语法和功能感知的奖励，并提出基于指数奖励塑造和静态分析指标的分支和样本难度感知RL方法。

Result: CVeDRL仅用0.6B参数就达到最先进性能：相比GPT-3.5提升28.97%的通过率和15.08%的分支覆盖率，同时推理速度比竞争基线快20倍以上。

Conclusion: 通过将多种验证信号联合建模为RL奖励，CVeDRL能够高效生成高质量的单元测试，显著提升代码验证的可靠性和效率。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [35] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：一种基于轨迹驱动的抽象机制，自动从执行日志构建状态抽象，支持在线构建智能体行为MDP，通过反例精化谓词树，实现运行时概率验证和异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有动态概率保证（DPA）方法需要开发者手动定义状态抽象，这导致验证与特定应用启发式方法耦合，增加了采用难度。需要自动化状态构建机制来降低采用门槛。

Method: 提出TriCEGAR方法：1）从轨迹中学习谓词树作为抽象表示；2）使用反例进行精化；3）框架原生实现捕获类型化智能体生命周期事件；4）从轨迹构建MDP；5）进行概率模型检查计算边界概率；6）利用运行似然进行异常检测。

Result: 实现了自动化状态抽象构建，支持在线构建智能体行为MDP，能够计算Pmax(成功)和Pmin(失败)等概率边界，并通过运行似然实现异常检测作为护栏信号。

Conclusion: TriCEGAR通过自动化状态抽象构建解决了DPA方法的主要限制，降低了智能体AI系统运行时验证的采用门槛，提供了有效的概率保证和异常检测能力。

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [36] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作三种学习模态会形成部分共享的语义表征，支持模态独立的语义组织，为具身AI系统的跨领域迁移提供了可能性。


<details>
  <summary>Details</summary>
Motivation: 探索认知科学和AI中的一个基本问题：不同学习模态（语言、视觉和动作）是否会产生不同或共享的内部表征。传统观点认为不同数据类型的模型会发展出专门化、不可迁移的表征，但新证据表明不同任务优化的模型可能发展出相似的表征几何结构。

Method: 在BabyAI平台上训练基于transformer的智能体，通过行为克隆执行目标导向行为以响应自然语言指令，生成仅由感觉运动控制需求塑造的动作基础语言嵌入。然后将这些表征与最先进的大语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表征进行比较。

Result: 尽管训练数据、模态和目标存在显著差异，但观察到稳健的跨模态对齐。动作表征与仅解码器语言模型和BLIP对齐强烈（precision@15: 0.70-0.73），接近语言模型之间的对齐水平。与CLIP和BERT的对齐显著较弱。

Conclusion: 语言、视觉和动作表征会收敛到部分共享的语义结构，支持模态独立的语义组织，并突显了具身AI系统中跨领域迁移的潜力。

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [37] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 提出了Med-Inquire基准测试和EvoClinician自进化智能体，用于模拟真实临床诊断中的多轮信息收集过程，相比传统"一次性"医疗AI更贴近实际诊断流程。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI采用不现实的"一次性"诊断模式，直接从完整病历中诊断。但真实临床诊断是迭代过程，医生需要顺序提问和安排检查，在管理成本和时间的同时策略性地收集信息。

Method: 首先提出Med-Inquire基准测试，基于真实临床病例数据集，通过专门的Patient和Examination智能体隐藏完整病历，迫使诊断智能体主动提问和安排检查。然后提出EvoClinician自进化智能体，采用"诊断-评分-进化"循环：Actor智能体尝试诊断；Process Grader智能体评估每个行动的临床价值和资源效率；Evolver智能体使用反馈更新Actor的策略。

Result: 实验表明EvoClinician优于持续学习基线和其他自进化智能体（如记忆智能体）。

Conclusion: Med-Inquire基准测试和EvoClinician智能体为医疗AI提供了更贴近真实临床诊断的评估框架和解决方案，能够模拟多轮信息收集的诊断过程。

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [38] [Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text](https://arxiv.org/abs/2601.22975)
*Ximing Lu,David Acuna,Jaehun Jung,Jian Hu,Di Zhang,Shizhe Diao,Yunheng Zou,Shaokun Zhang,Brandon Cui,Mingjie Liu,Hyunwoo Kim,Prithviraj Ammanabrolu,Jan Kautz,Yi Dong,Yejin Choi*

Main category: cs.AI

TL;DR: 提出Golden Goose方法，通过将不可验证的互联网文本转化为多项选择问答任务，合成无限规模的RLVR数据，有效解决了现有RLVR数据有限导致的训练饱和问题。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法受限于可验证数据的稀缺性，导致模型在长时间训练后性能提升饱和。需要利用丰富的、不可验证的互联网文本资源来扩展RLVR数据规模。

Method: 提出Golden Goose方法：1) 从不可验证的互联网文本中识别并掩码关键推理步骤；2) 生成多样化的干扰选项；3) 构建多项选择问答版本的填空任务，从而合成RLVR数据集。

Result: 1) 构建了包含70万个任务的GooseReason数据集；2) 在15个基准测试中为1.5B和4B模型取得了新的SOTA结果；3) 在网络安全领域构建了GooseReason-Cyber数据集，使Qwen3-4B-Instruct超越了经过大量领域预训练的7B模型。

Conclusion: Golden Goose方法能够有效利用丰富的、不可验证的互联网文本自动扩展RLVR数据，解决现有数据稀缺问题，为模型带来持续的性能提升。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.

</details>


### [39] [Why Your Deep Research Agent Fails? On Hallucination Evaluation in Full Research Trajectory](https://arxiv.org/abs/2601.22984)
*Yuhao Zhan,Tianyu Fan,Linxuan Huang,Zirui Guo,Chao Huang*

Main category: cs.AI

TL;DR: 论文提出了一个过程感知的评估框架PIES Taxonomy，用于诊断深度研究代理的幻觉问题，并创建了DeepHalluBench基准，发现现有系统存在系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理的评估主要依赖端到端测试，无法揭示研究轨迹中关键的中间幻觉（如错误规划），需要从结果导向转向过程感知的评估方法。

Method: 提出PIES Taxonomy将幻觉按功能组件（规划vs总结）和错误属性（显式vs隐式）分类，建立细粒度评估框架分解研究轨迹，并创建包含100个任务的DeepHalluBench基准。

Result: 对6个最先进的深度研究代理进行实验，发现没有系统能达到稳健的可靠性，诊断分析揭示了幻觉传播和认知偏见等系统性缺陷。

Conclusion: 过程感知评估能有效诊断深度研究代理的失败机制，为未来架构优化提供基础性见解，幻觉传播和认知偏见是需要解决的关键问题。

Abstract: Diagnosing the failure mechanisms of Deep Research Agents (DRAs) remains a critical challenge. Existing benchmarks predominantly rely on end-to-end evaluation, obscuring critical intermediate hallucinations, such as flawed planning, that accumulate throughout the research trajectory. To bridge this gap, we propose a shift from outcome-based to process-aware evaluation by auditing the full research trajectory. We introduce the PIES Taxonomy to categorize hallucinations along functional components (Planning vs. Summarization) and error properties (Explicit vs. Implicit). We instantiate this taxonomy into a fine-grained evaluation framework that decomposes the trajectory to rigorously quantify these hallucinations. Leveraging this framework to isolate 100 distinctively hallucination-prone tasks including adversarial scenarios, we curate DeepHalluBench. Experiments on six state-of-theart DRAs reveal that no system achieves robust reliability. Furthermore, our diagnostic analysis traces the etiology of these failures to systemic deficits, specifically hallucination propagation and cognitive biases, providing foundational insights to guide future architectural optimization. Data and code are available at https://github.com/yuhao-zhan/DeepHalluBench.

</details>


### [40] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj是一个两阶段框架，通过修复和奖励工具使用轨迹来自动学习工具集成推理(TIR)，解决了现有方法依赖高质量合成轨迹和稀疏结果奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏结果奖励，提供的监督有限且有偏差，需要更好的方法来学习可靠的TIR行为。

Method: 两阶段框架：1) SFT阶段：为每个查询生成多个候选轨迹，评估后保留高质量轨迹，低质量轨迹用LLM修复；2) RL阶段：基于偏好数据集训练轨迹级奖励模型，结合结果和格式奖励指导优化。

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性。

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，能够有效学习可靠的工具集成推理行为，解决了现有方法的局限性。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [41] [RAudit: A Blind Auditing Protocol for Large Language Model Reasoning](https://arxiv.org/abs/2601.23133)
*Edward Y. Chang,Longling Geng*

Main category: cs.AI

TL;DR: RAudit是一种无需真实标签即可审计LLM推理的诊断协议，通过评估推导步骤是否支持结论来检测推理不一致性，揭示了推理病理的四种机制。


<details>
  <summary>Details</summary>
Motivation: 推理时的缩放会放大推理病理（如奉承、层级塌陷、过早确定性），需要一种无需真实标签的审计方法来诊断LLM推理问题。

Method: 提出RAudit诊断协议，基于盲审原则只评估推导步骤是否支持结论，使用CRIT-based合理性评分，并通过不同批判表述研究社会框架对模型响应的影响。

Result: 实验在数学推理（CAP-GSM8K）和因果判断（CausalL2）任务上揭示了四种机制：潜在能力抑制、虚假能力陷阱、复杂度-脆弱性权衡、医源性批判。

Conclusion: 研究发现能力并不等同于鲁棒性，更强的反馈不一定产生更好的输出，挑战了现有假设。

Abstract: Inference-time scaling can amplify reasoning pathologies: sycophancy, rung collapse, and premature certainty. We present RAudit, a diagnostic protocol for auditing LLM reasoning without ground truth access. The key constraint is blindness: the auditor evaluates only whether derivation steps support conclusions, enabling detection of trace-output inconsistency and, when latent competence exists, its recovery. RAudit measures process quality via CRIT-based reasonableness scores and varies critique formulation to study how social framing affects model response. We prove bounded correction and $O(\log(1/ε))$ termination. Experiments on mathematical reasoning (CAP-GSM8K) and causal judgment (CausalL2) reveal four mechanisms explaining model unreliability: (1) Latent Competence Suppression, where models derive correct answers then overwrite them under social pressure; (2) The False Competence Trap, where weaker judges mask sycophancy that stronger judges expose; (3) The Complexity-Vulnerability Tradeoff, where causal tasks induce more than 10 times higher sycophancy than mathematical tasks; and (4) Iatrogenic Critique, where authoritative correction harms weaker models. These findings challenge assumptions that capability implies robustness and that stronger feedback yields better outputs.

</details>


### [42] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一种自生成对齐框架，通过轻量级拒绝引导让模型生成安全推理轨迹，然后在这些自生成响应上进行微调，以恢复安全对齐同时最小化分布偏移。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习在推理任务上进行优化时，往往过度追求合规性，导致模型容易受到有害提示的攻击，安全性能下降。现有方法依赖外部教师蒸馏，但这会引入分布差异，损害原生推理能力。

Method: 提出ThinkSafe框架，核心洞察是：虽然合规性会抑制安全机制，但模型通常保留识别危害的潜在知识。通过轻量级拒绝引导解锁这种知识，引导模型生成符合分布的安全推理轨迹，然后在这些自生成响应上进行微调。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验表明，ThinkSafe显著提高了安全性，同时保持了推理能力。与GRPO相比，ThinkSafe实现了更优的安全性和相当的推理能力，且计算成本显著降低。

Conclusion: ThinkSafe提供了一种无需外部教师的自生成对齐方法，能够有效恢复模型的安全对齐，同时最小化对推理能力的负面影响，计算效率高。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [43] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA提出通过AI反馈的逐动作过程奖励来微调多智能体系统，解决信用分配和样本效率问题，在数学竞赛和数据分析任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理复杂任务方面有潜力，但微调多个智能体面临两大挑战：1）跨智能体的信用分配问题；2）昂贵多智能体rollout的样本效率问题。

Method: 提出MAPPA方法，通过AI反馈为每个智能体动作分配过程奖励（而非仅在任务完成时），实现细粒度监督而无需真实标签，从每次rollout中提取最大训练信号。

Result: 在数学竞赛问题上，AIME提升5.0-17.5个百分点，AMC提升7.8-17.2个百分点；在数据分析任务中，成功率提升12.5个百分点，质量指标提升高达30%。

Conclusion: 逐动作监督能在不同领域改善多智能体系统性能，为解决复杂长视野任务中多智能体系统的扩展问题迈出第一步，减少人类监督需求。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [44] [Introducing Moltworker: a self-hosted personal AI agent, minus the minis](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fmoltworker-self-hosted-ai-agent%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/xN7b9u3iPyByO4hzdAeJ9OblJict7wzG3nCQJmdtAAE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cloudflare开源Moltworker中间件，让用户能在其开发者平台上自托管AI代理Moltbot，无需专用本地硬件


<details>
  <summary>Details</summary>
Motivation: 解决用户运行自托管AI代理时需要专用本地硬件的问题，利用Cloudflare平台提供更便捷的部署方案

Method: 开发中间件解决方案，利用Cloudflare扩展的Node.js兼容性（仅1.5%顶级NPM包无法运行），结合沙盒、浏览器渲染、R2存储和AI网关等服务

Result: 创建了概念验证系统，用户可在Cloudflare开发者平台上运行自托管AI代理，无需本地硬件

Conclusion: Moltworker展示了在云平台上自托管AI代理的可行性，为开发者提供了更灵活、便捷的AI代理部署方案

Abstract: Introducing Moltworker: a self-hosted personal AI agent, minus the minis (9 minute read) Cloudflare has open-sourced Moltworker, a middleware solution that allows users to run the self-hosted AI agent Moltbot on its Developer Platform, bypassing the need for dedicated local hardware. This proof-of-concept leverages Cloudflare's expanded Node.js compatibility—with only 1.5% of top NPM packages failing to run—alongside services like Sandboxes, Browser Rendering, R2 for storage, and AI Gateway f...

</details>


### [45] [How Google SREs Use Gemini CLI to Solve Real-World Outages](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcloud.google.com%2Fblog%2Ftopics%2Fdevelopers-practitioners%2Fhow-google-sres-use-gemini-cli-to-solve-real-world-outages%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/hXv8KtUCzKr2xGY8z9-0Lu_w_DZEP-Xn0U2f5UGecsg=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google SRE团队使用Gemini CLI和智能代理AI来减少事故缓解时间，通过分类故障、执行安全缓解措施、识别根本原因、生成修复方案和自动化事后分析，同时保持人工控制。


<details>
  <summary>Details</summary>
Motivation: Google SRE团队面临处理生产环境事故的挑战，需要快速响应和缓解故障，同时保持系统稳定性和安全性。传统方法可能效率较低，需要更智能的工具来加速事故处理流程。

Method: 使用Gemini CLI和智能代理AI系统，通过分类故障类型、执行安全缓解措施、自动识别根本原因、生成代码修复方案，并自动化事后分析文档的创建。

Result: 显著减少了事故缓解时间，提高了SRE团队的工作效率，同时保持了人工监督和控制，确保系统安全。

Conclusion: Gemini CLI和智能代理AI是有效的工具，能够帮助SRE团队更高效地处理生产环境事故，实现快速响应和系统恢复。

Abstract: How Google SREs Use Gemini CLI to Solve Real-World Outages (5 minute read) Google SREs use Gemini CLI and agentic AI to reduce incident mitigation time by classifying outages, executing safe mitigations, identifying root causes, generating fixes, and automating postmortems while keeping humans in control.

</details>


### [46] [Pulumi Agent Skills: Best practices and more for AI coding assistants](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pulumi.com%2Fblog%2Fpulumi-agent-skills%2F%3Futm_source=tldrdevops/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/_C9yCxROjfFf2o1PyKwjWYCNdPHkTsnuvbg-viCvxUM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pulumi推出Agent Skills，这是一套结构化知识包，帮助AI编码助手生成生产就绪的基础设施代码


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手在生成基础设施代码时缺乏生产环境的最佳实践和结构化知识，导致生成的代码质量不高，需要额外的人工调整

Method: 开发了一套结构化知识包（Agent Skills），包含最佳实践、模式库和行业标准，通过API集成到AI编码助手中

Result: AI编码助手能够生成更高质量、符合生产标准的基础设施代码，减少了人工调整的工作量，提高了开发效率

Conclusion: Pulumi Agent Skills通过提供结构化知识，显著提升了AI编码助手在基础设施代码生成方面的能力，使其更接近生产就绪水平

Abstract: Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.

</details>


### [47] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/A1NCYU5wweZji_T_iO-rFNswHOHAROekJ15P_XEaUnA=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pulumi Agent Skills 是一个结构化知识包集合，帮助AI编程助手生成生产就绪的基础设施代码


<details>
  <summary>Details</summary>
Motivation: 当前AI编程助手在生成基础设施代码时缺乏生产就绪的最佳实践和结构化知识，导致生成的代码质量不高

Method: 创建结构化知识包集合，包含基础设施代码的最佳实践、模式和模板，供AI编程助手使用

Result: AI编程助手能够生成更高质量、生产就绪的基础设施代码，提高开发效率和代码可靠性

Conclusion: Pulumi Agent Skills 通过结构化知识包有效提升了AI编程助手在基础设施代码生成方面的能力

Abstract: Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.

</details>


### [48] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/a_R8zkBMU0gfZLRJfJcwwLaA7GY0C4BX7aCwBs9Ny2A=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pulumi推出Agent Skills，这是一组结构化知识包，帮助AI编码助手生成生产就绪的基础设施代码


<details>
  <summary>Details</summary>
Motivation: 当前AI编码助手在生成基础设施代码时缺乏生产就绪的最佳实践和结构化知识，需要专门工具来提升其生成代码的质量和可靠性

Method: 开发Pulumi Agent Skills - 一组结构化知识包，包含最佳实践、模式和安全指南，专门用于指导AI编码助手生成基础设施代码

Result: AI编码助手现在能够生成更高质量、更安全、符合生产标准的基础设施代码，减少了手动调整和错误修复的需求

Conclusion: Pulumi Agent Skills通过提供结构化知识包，显著提升了AI编码助手在基础设施代码生成方面的能力和可靠性

Abstract: Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.

</details>


### [49] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecbf929-4b4d5cc6-c7f7-44a1-8fd3-02c60dbfbe1d-000000/c2EvQc1tbG8tsk4y3jqP1oIeh2VgCPe5X1SNnYIvp2A=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Pulumi推出Agent Skills，这是一套结构化知识包，使AI编程助手能够生成生产就绪的基础设施代码


<details>
  <summary>Details</summary>
Motivation: 当前AI编程助手在生成基础设施代码时缺乏生产就绪性，需要结构化知识来指导其生成高质量、可部署的代码

Method: 开发结构化知识包（Agent Skills），为AI助手提供最佳实践、模式、安全指南等知识，使其能够生成符合生产标准的Pulumi基础设施代码

Result: 创建了Pulumi Agent Skills集合，提升了AI助手生成基础设施代码的质量和可靠性，使其能够生成生产就绪的代码

Conclusion: 通过结构化知识包增强AI编程助手的能力，可以显著提高基础设施代码生成的效率和质量，推动AI在基础设施即代码领域的应用

Abstract: Pulumi Agent Skills: Best practices and more for AI coding assistants (4 minute read) Pulumi Agent Skills is a new collection of structured knowledge packages that enables AI coding assistants to generate production-ready infrastructure code.

</details>


### [50] [Implementing local-first agentic AI: A practical guide](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Flocal-first-agentic-ai-guide%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/JH6odhJhiaEtxesMlk2UXipR6bubvzNIFNz064WIcsE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文介绍了一个本地优先的AI代理系统，使用小型模型(90M-3.8B参数)在笔记本电脑上处理敏感HR报告，无需GPU，10-30秒完成，数据完全本地化


<details>
  <summary>Details</summary>
Motivation: 解决敏感数据处理中的隐私和安全问题，避免使用云API带来的数据泄露风险，同时降低API费用成本

Method: 采用三个小型AI模型构建本地处理流水线：识别问题、制定行动计划、执行HR相关功能，完全在本地笔记本电脑上运行

Result: 系统能在10-30秒内处理员工报告，无需GPU，数据完全本地化，零API费用，但牺牲了大语言模型的灵活性

Conclusion: 本地优先的AI代理方法在处理敏感数据时具有隐私、安全和成本优势，适合需要数据本地化的应用场景

Abstract: Implementing local-first agentic AI: A practical guide (5 minute read) LogRocket built an HR triage system that processes sensitive employee reports entirely on a local laptop using three small AI models (90M-3.8B parameters) instead of cloud APIs. The pipeline identifies the issue, creates an action plan, then executes functions like opening HR cases, all in 10-30 seconds without a GPU. This keeps private data on-premises, costs nothing in API fees, and trades the flexibility of big LLMs for...

</details>


### [51] [Go from 0 to shipped with vibe coding that actually works](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnldev20250130/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/7O_hiz4xeDxNgHlft17mM_xqMWaTvVf-dJ-zMBCJDFY=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: v0是一个AI驱动的开发平台，将软件视为活文档，通过提示直接生成生产代码并存入Git仓库，让团队专注于架构和质量控制


<details>
  <summary>Details</summary>
Motivation: 传统的软件开发生命周期（SDLC）已无法反映现代团队的工作方式，需要更灵活、协作性更强的开发工具

Method: 采用"vibe coding"方法，通过提示直接生成生产代码，所有代码自动存入Git仓库，支持实时预览和协作

Result: 实现了从零到产品发布的快速开发流程，团队可以专注于产品本身而非文档工作

Conclusion: v0平台通过AI辅助的活文档开发模式，改变了传统软件开发流程，提高了团队协作效率和开发速度

Abstract: Go from 0 to shipped with vibe coding that actually works (Sponsor) The old SDLC no longer reflects how teams work. v0 treats software like a living document where you build directly using prompts and previews. Every prompt creates production code in your git repository so you can focus on architecture, quality, and control. Start collaborating on the product, not on documents

</details>


### [52] [Contextual agent playbooks and tools: How LinkedIn gave AI coding agents organizational context](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/5Eo-CMZMlwTa3H3yOefO0JailBaMDRKDdw6sFiDrZrw=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LinkedIn开发了代理生命周期服务，为AI编码代理提供组织记忆和上下文，使其能记住对话历史、遵循数据权限、保持人类监督，提升企业环境中的AI编码效率


<details>
  <summary>Details</summary>
Motivation: 解决AI编码代理在企业环境中缺乏组织上下文的问题，传统AI代理每次交互都从零开始，无法记住历史对话、不了解公司特定数据权限和流程，导致效率低下且存在安全风险

Method: 构建"代理生命周期服务"，包含记忆系统记录对话历史，基于角色的权限控制确保数据安全，人类在环机制处理敏感操作，使AI代理能理解公司特定工作方式

Result: AI编码代理能持续理解上下文（如从"旧金山工程师"转到"伦敦"），安全访问公司数据，在敏感操作中请求人类批准，显著提升企业环境中的编码效率和安全性

Conclusion: 为AI编码代理添加组织上下文和记忆能力是提升企业AI助手效用的关键，结合权限控制和人类监督可实现安全高效的AI辅助编码

Abstract: Contextual agent playbooks and tools: How LinkedIn gave AI coding agents organizational context (18 minute read) LinkedIn built an “agent life-cycle service” that gives AI coding agents memory and context about how the company actually works. Instead of starting fresh each time, agents remember conversation history (so if you ask about engineers in SF then say “now London,” it gets it), respect data permissions through role-based auth, and keep humans in the loop for sensitive stuff like send...

</details>


### [53] [Claude Code Opus 4.5 Performance Tracker](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmarginlab.ai%2Ftrackers%2Fclaude-code%2F%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/ewkok6jij3yXtN91EIropJdJwDIhVKUUJaKEIfHkb38=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 独立追踪器每日通过CLI基准测试监控Claude Code Opus 4.5在软件工程任务上的性能表现


<details>
  <summary>Details</summary>
Motivation: 需要持续监控和评估Claude Code Opus 4.5在软件工程任务上的实际性能表现，为开发者和用户提供客观的性能数据

Method: 使用直接CLI基准测试，每日对Claude Code Opus 4.5在SWE任务上进行性能追踪

Result: 建立了持续的性能监控系统，能够提供Claude Code Opus 4.5在软件工程任务上的日常性能数据

Conclusion: 通过独立的性能追踪器，可以为社区提供Claude Code Opus 4.5在软件工程任务上的可靠性能评估

Abstract: Claude Code Opus 4.5 Performance Tracker (3 minute read) This independent tracker monitors Claude Code with Opus 4.5 performance on SWE tasks daily using direct CLI benchmarks.

</details>


### [54] [Best LLMs for coding in 2026](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.builder.io%2Fblog%2Fbest-llms-for-coding%3Futm_source=tldrdev/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/pRy6_0YNxAkoxVQtzbr_MEqDMJd6WhfjIZO-Nf6EApk=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 没有单一的"最佳"编程LLM，因为不同任务需要不同模型


<details>
  <summary>Details</summary>
Motivation: 探讨在编程领域如何选择合适的LLM，强调没有万能模型，需要根据具体任务需求选择

Method: 通过分析不同编程任务的特点和需求，对比各类LLM在不同场景下的表现

Result: 识别出不同编程任务需要不同类型的LLM，如代码生成、调试、文档编写等各有适合的模型

Conclusion: 选择编程LLM时应考虑具体任务需求，而不是追求单一的"最佳"模型

Abstract: Best LLMs for coding in 2026 (12 minute read) There's no single best coding LLM since you need different models for different jobs.

</details>


### [55] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/FF48Qwklkhxtvr-BU6vXfhKpncaOBZh56DPa_Cz0ElQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 没有单一的最佳编码LLM，不同任务需要不同模型


<details>
  <summary>Details</summary>
Motivation: 探讨2026年最佳编码LLM的选择问题，强调没有万能模型

Method: 分析不同编码任务对LLM的需求差异，可能基于任务分类评估

Result: 不同编码任务需要专门化的LLM，而非单一通用模型

Conclusion: 编码LLM选择应根据具体任务需求，未来趋势是专业化而非通用化

Abstract: Best LLMs for coding in 2026 (12 minute read) There's no single best coding LLM since you need different models for different jobs.

</details>


### [56] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0ecf9486-291c6964-facc-46f3-82cc-945f66787227-000000/cjEgyZmV4qciFj-1bcmj64xB8T4ApgGXRKZzOKOe4tw=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 没有单一的最佳编码LLM，因为不同任务需要不同模型


<details>
  <summary>Details</summary>
Motivation: 探讨在2026年如何选择最适合编码任务的LLM，认识到不同编码工作对模型有不同需求

Method: 分析不同编码任务的特点和需求，评估各种LLM在不同场景下的表现

Result: 发现不存在通用的"最佳"编码LLM，而是需要根据具体任务类型选择最适合的模型

Conclusion: 编码LLM的选择应基于具体任务需求，而非追求单一"最佳"模型

Abstract: Best LLMs for coding in 2026 (12 minute read) There's no single best coding LLM since you need different models for different jobs.

</details>


### [57] [Inside OpenAI's in-house data agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FdbNMqU/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/z17jr9KazstyWTP6IZ4EELjBjSUs6m5Cf8hMeCLu3no=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI开发了专门用于内部数据分析的AI代理，能够快速回答高价值数据问题，支持跨团队日常工作的AI应用。


<details>
  <summary>Details</summary>
Motivation: OpenAI需要让员工能够快速从数据中获得洞察，解决高影响力的数据问题，提高工作效率和决策质量。

Method: 构建专门围绕OpenAI数据、权限和工作流程定制的AI数据代理，该代理能够探索和推理平台数据。

Result: 员工可以在几分钟内从问题获得洞察，显著提高了数据查询和分析的效率。

Conclusion: AI数据代理展示了AI如何支持跨团队的日常工作，相关构建工具已向所有人开放。

Abstract: Inside OpenAI's in-house data agent (15 minute read) OpenAI uses a bespoke in-house AI data agent that explores and reasons over its platform. It lets employees go from question to insight in minutes, allowing them to answer high-impact data questions. The agent was built specifically around OpenAI's data, permissions, and workflows. This post details how it was built to demonstrate how AI can support day-to-day work across teams. The tools used to build the AI data agent are available to all...

</details>


### [58] [RL Environments for Agentic AI: Who Will Win the Training & Verification Layer by 2030](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.datagravity.dev%2Fp%2Frl-environments-for-agentic-ai-who%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/rRF8cKvhuHIyqyHdt9oFusEJ396Lv1BAFB0Yke99moE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文认为强化学习环境基础设施将成为AI领域的关键竞争点，到2030年将出现3-5个主要赢家，市场格局可能类似于数据标注行业


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的发展，需要专门的强化学习环境基础设施来支持训练、验证和长期环境编排，但目前缺乏成熟的工业化解决方案

Method: 通过分析当前RL环境基础设施的现状，提出需要与前沿实验室合作建立工业化的复制训练、验证和长期环境编排系统

Result: 预测到2030年RL环境基础设施市场将出现3-5个主要赢家，市场结构可能类似于数据标注行业（已有3个收入超10亿美元的公司）

Conclusion: 构建真实的RL环境基础设施是赢得竞争的关键，需要与前沿实验室紧密合作实现工业化，市场将趋于集中化

Abstract: RL Environments for Agentic AI: Who Will Win the Training & Verification Layer by 2030 (29 minute read) The winners in RL environments will be the teams that build real infrastructure. This has to be done in close partnership with frontier labs to industrialize replication training, verification, and long-horizon environment orchestration. There will likely be three to five significant winners. The market may end up resembling data labeling, which already has roughly three over-$1 billion-rev...

</details>


### [59] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FgDoRDp%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/6c4p2YizIK7OJifqc6EAlVPo2B12mCQz_kg6hjGbpcQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Bricks是一个AI代理开发平台，帮助用户基于自身数据构建、评估和优化AI代理，通过自动评估、目标对齐评分和人类反馈改进，使代理能在真实世界应用中有效工作。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理开发面临挑战：难以构建在真实世界中有效工作的代理，缺乏系统化的评估和优化方法，以及从原型到生产部署的路径不清晰。

Method: 提供平台工具，支持基于用户独特数据构建AI代理，自动评估代理性能，根据用户目标对输出进行评分，并通过人类反馈持续改进代理表现。

Result: 平台能够帮助开发者构建出在真实世界中有效工作的AI代理，提供清晰的从开发到生产部署的路径，提高代理的实用性和可靠性。

Conclusion: Agent Bricks平台通过系统化的构建、评估和优化流程，解决了AI代理开发中的关键挑战，使开发者能够构建出在真实应用中表现良好的代理系统。

Abstract: Agents that don't suck (Sponsor) Agent Bricks helps you build, evaluate and optimize AI agents grounded in your unique data. It evaluates automatically, scores outputs against your goals and improves with human feedback — giving you a clearer path to production. Build agents that work in the real world. See why it's worth your time

</details>


### [60] [Agent Trace: Capturing the Context Graph of Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fagent-trace%3Futm_source=tldrai/1/0100019c0f450cdd-d6d47f58-1fe9-4ad5-8928-20edf4f6f1f4-000000/iM_J_Hc9V41OTmAcIQf32uIpvuvmNmCB_m4h0vLQPZk=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Trace是一个开放、厂商中立的规范，用于在版本控制代码库中记录AI贡献和人类作者身份，将每个变更关联到特定对话和代码行范围。


<details>
  <summary>Details</summary>
Motivation: 随着AI在软件开发中的参与度增加，需要一种标准化的方式来追踪AI对代码的贡献，使开发过程更加透明和可追溯。

Method: 提出一个开放规范，在版本控制系统中记录AI贡献，将代码变更与特定的对话上下文和代码行范围关联起来。

Result: 创建了一个标准化的追踪系统，使代码库能够始终链接回创建代码的上下文，提高了开发的可读性和可管理性。

Conclusion: Agent Trace规范能够使AI辅助的开发过程更加透明，为管理级仪表板和数据驱动决策提供支持。

Abstract: Agent Trace: Capturing the Context Graph of Code (5 minute read) Agent Trace is an open, vendor-neutral spec for recording AI contributions alongside human authorship in version-controlled codebases. It attributes each change to a specific conversation and line range, meaning repositories associated with Agent Traces will always be able to link back to the context that created it. The system makes development legible and could enable some pretty powerful management-level dashboards and data-d...

</details>


### [61] [Inside OpenAI's in-house data agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FuI8gjn/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/jLFFh6GqfxThVrdTVLk86_R9MqKctjHzRas-7bow9yg=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI开发了内部AI数据代理GPT-5，让员工能用自然语言查询获取准确、上下文相关的数据洞察，涵盖从表发现到分析报告的全流程。


<details>
  <summary>Details</summary>
Motivation: OpenAI需要解决大规模数据分析的挑战，让非技术员工也能快速获取准确的数据洞察，提高组织的数据驱动决策能力。

Method: 构建基于GPT-5的定制化AI数据代理，整合代码感知的数据上下文、机构知识、记忆系统和持续评估机制，实现端到端的数据分析流程。

Result: 该系统能够在OpenAI的规模下提供快速可靠的分析，让员工通过自然语言问题获得准确的数据洞察，提高了数据分析的效率和可访问性。

Conclusion: 内部AI数据代理成功地将先进的语言模型能力应用于企业数据分析，展示了AI在提升组织数据素养和决策效率方面的潜力。

Abstract: Inside OpenAI's in-house data agent (15 minute read) OpenAI built a bespoke internal AI data agent powered by GPT-5 that lets employees ask natural-language questions and get accurate, contextual data insights end to end, from table discovery to analysis and reporting. It combines code-aware data context, institutional knowledge, memory, and continuous evaluation to deliver fast, reliable analytics at OpenAI's scale.

</details>


### [62] [Multi-agent is becoming the new overengineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.louisbouchard.ai%2Fagents-and-workflows%2F%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/p3K-xQQps6MEhvdxeCC2jjyKPEIWsdD78a6hio1FesY=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文分析了多智能体系统在LLM应用中的过度工程化问题，提出了清晰的架构选择标准：确定性顺序任务用工作流，动态紧密耦合任务用单智能体（少于10-20个工具），真正的并行任务才用多智能体架构。


<details>
  <summary>Details</summary>
Motivation: 当前LLM解决方案中普遍存在过度工程化问题，特别是多智能体架构被滥用，导致系统复杂性和效率低下。需要建立清晰的架构选择标准来避免不必要的复杂性。

Method: 通过分析不同架构的特点和应用场景，提出基于任务特性的架构选择框架：工作流适合确定性顺序任务，单智能体适合动态紧密耦合任务，多智能体仅适合真正并行任务。

Result: 建立了明确的架构选择标准，帮助开发者根据任务特性选择最合适的架构，避免过度工程化，提高系统效率和可维护性。

Conclusion: 多智能体架构不应成为默认选择，而应根据任务特性谨慎选择。清晰的架构区分对于构建高效、可维护的LLM解决方案至关重要。

Abstract: Multi-agent is becoming the new overengineering (7 minute read) Clear architectural distinctions between workflows, single-agent systems, and multi-agent systems are critical to avoiding overengineering and inefficiency in LLM-based solutions. Workflows excel for deterministic, sequential tasks with minimal overhead, while a single agent with fewer than 10–20 tools suits dynamic, tightly coupled processes where global context matters. Multi-agent architectures are warranted only for true para...

</details>


### [63] [Why the Future of Data Platform Engineering is Agent Experience](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Frobertsahlin.substack.com%2Fp%2Fwhy-the-future-of-data-platform-engineering%3Futm_source=tldrdata/1/0100019c1e0a6075-21a7d704-f612-4588-9bbf-fc70c7c1bf06-000000/L4dnmNfzghNT8bjMxHvD3hRRYIzn2rte9RhZ8W24jIE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 数据平台工程正从以人为中心的开发者体验转向以AI代理为中心的代理体验，强调API优先架构、机器可读文档和结构化错误处理


<details>
  <summary>Details</summary>
Motivation: 随着AI代理越来越多地管理编码和运维，传统的人类中心开发者体验已不适应，需要专门为AI代理优化的平台体验

Method: 提出转向无头、API优先架构，采用机器可读文档、确定性JSON通信、结构化错误提示和通用集成标准

Result: 数据平台工程需要重新设计以支持AI代理的自主操作，这代表了行业的重要范式转变

Conclusion: 未来数据平台工程的核心将是代理体验，这需要系统性地重构平台架构和工具链

Abstract: Why the Future of Data Platform Engineering is Agent Experience (AX) (3 minute read) Data platform engineering is shifting focus from human-centric Developer Experience (DX) to Agent Experience (AX), as AI agents increasingly manage coding and operations. Priorities now include headless, API-first architectures, machine-readable documentation, deterministic JSON-based communication, structured error hints for autonomous remediation, and universal integration standards. This pivot demands plat...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [64] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 该论文对大型语言模型在根因分析任务中的推理能力进行了实证评估，通过控制实验框架隔离LLM的推理行为，在48000个故障场景中测试了6个LLM模型，并建立了16种常见推理错误的分类体系。


<details>
  <summary>Details</summary>
Motivation: 现代云系统的高度分布式和相互依赖特性使得根因分析变得复杂，特别是多跳故障传播场景。虽然LLM为自动化RCA提供了新机会，但现有方法要么依赖历史事件语料库，要么处理超出LLM容量的海量遥测数据，要么将推理嵌入复杂的多智能体管道，这些设计选择模糊了失败是源于推理本身还是外围设计。

Method: 设计了一个控制实验框架来隔离LLM的推理行为，使用简化的实验设置突出LLM。评估了6个LLM在两个智能体工作流（ReAct和Plan-and-Execute）和一个非智能体基线下的表现，基于两个真实世界案例研究（GAIA和OpenRCA）。执行了48000个模拟故障场景，总计228天执行时间。测量了根因准确性和中间推理轨迹质量，并建立了16种常见RCA推理错误的标记分类体系，使用LLM-as-a-Judge进行标注。

Result: 结果阐明了当前开源LLM在多跳RCA中的成功和失败之处，量化了对输入数据模态的敏感性，并识别了能够预测最终正确性的推理失败模式。提供了透明且可复现的实证结果。

Conclusion: 该研究为推理驱动的系统诊断提供了指导，通过控制实验框架和详细的失败分类体系，帮助未来工作更好地理解和改进LLM在根因分析中的推理能力。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [65] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: TimeMachine-bench是一个评估Python项目软件迁移能力的基准测试，包含因依赖更新而测试失败的GitHub仓库，并评估了11个LLM模型在迁移任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 软件迁移是软件工程中适应环境变化的关键过程，但在自动化软件工程研究中被忽视。需要建立评估软件迁移能力的基准测试。

Method: 构建自动化基准测试TimeMachine-bench，包含因依赖更新导致测试失败的GitHub仓库，并创建人工验证的子集确保问题可解性。评估了基于11个模型（包括开源和SOTA LLM）的代理基线。

Result: LLM在迁移任务上显示出一定潜力，但仍面临重大可靠性挑战，包括利用低测试覆盖率产生虚假解决方案，以及由于工具使用策略不佳导致的不必要编辑。

Conclusion: 需要进一步研究提高LLM在软件迁移任务上的可靠性，特别是在处理测试覆盖率和优化工具使用策略方面。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [66] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: Meta开发了即时捕获测试生成系统，用于在大型后端系统中预防bug。与传统硬化测试不同，捕获测试旨在失败，在代码合并前发现bug。通过代码变更感知方法将候选捕获生成提升4倍，使用基于规则和LLM的评估器减少70%人工审查，成功发现8个真实bug，其中4个可能引发严重故障。


<details>
  <summary>Details</summary>
Motivation: Meta面临大规模后端系统（数亿行代码）中bug预防的挑战。传统硬化测试在生成时通过，无法有效发现bug。需要一种能在代码合并前主动发现bug的方法，同时减少误报带来的开发负担。

Method: 1. 采用代码变更感知方法生成捕获测试，相比硬化测试提升4倍候选捕获生成；2. 使用基于规则和LLM的评估器来过滤误报；3. 分析22,126个生成的测试，进行推断统计分析；4. 将候选捕获报告给工程师进行验证。

Result: 1. 代码变更感知方法将候选捕获生成提升4倍（相比硬化测试）和20倍（相比偶然失败测试）；2. 评估器减少70%人工审查负担；3. 统计分析显示：人工接受的代码变更有更多误报，人工拒绝的变更有更多真实bug；4. 报告41个候选捕获，确认8个真实bug，其中4个可能引发严重故障。

Conclusion: 即时捕获测试生成具有可扩展性和工业适用性，能有效防止严重故障进入生产环境。该方法通过主动发现bug和减少误报，在大规模系统中实现了有效的bug预防。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [67] [MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering](https://arxiv.org/abs/2601.22859)
*Chuanzhe Guo,Jingjing Wu,Sijun He,Yang Chen,Zhaoqi Kuang,Shilong Fan,Bingjin Chen,Siqi Bao,Jing Liu,Hua Wu,Qingfu Zhu,Wanxiang Che,Haifeng Wang*

Main category: cs.SE

TL;DR: MEnvAgent是一个多语言自动环境构建框架，用于生成可验证的软件工程任务实例，通过多智能体架构和增量补丁机制提高效率，并创建了最大的多语言可验证Docker环境数据集。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程领域的应用受到可验证数据集稀缺的限制，这源于跨多种语言构建可执行环境的复杂性。需要一种能够自动构建可验证任务环境的框架来突破这一瓶颈。

Method: 采用多智能体规划-执行-验证架构来自主解决环境构建失败问题，并集成了创新的环境重用机制，通过增量补丁历史环境来减少计算开销。

Result: 在包含10种语言、1000个任务的MEnvBench基准测试中，MEnvAgent优于基线方法，将失败转通过率提高了8.6%，同时减少了43%的时间成本。构建了迄今为止最大的开源多语言可验证Docker环境数据集MEnvData-SWE。

Conclusion: MEnvAgent有效解决了软件工程任务中可验证环境构建的瓶颈问题，通过自动化和重用机制显著提高了效率和成功率，为LLM在软件工程领域的发展提供了重要基础设施。

Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.

</details>


### [68] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: LLM智能体可显著降低SAST工具误报率，但效果受骨干模型、漏洞类型和成本影响，需谨慎部署


<details>
  <summary>Details</summary>
Motivation: SAST工具产生大量误报，增加人工审查负担，而LLM智能体在误报过滤方面的比较效果尚不明确

Method: 比较三种LLM智能体框架(Aider、OpenHands、SWE-agent)，使用OWASP Benchmark和真实Java项目评估，分析不同骨干模型效果

Result: LLM智能体可将OWASP Benchmark误报率从92%降至6.3%，真实项目中可达93.3%误报识别率，但效果受骨干模型和CWE类型影响，存在计算成本差异

Conclusion: LLM智能体是SAST误报过滤的有效但非均匀解决方案，实际部署需考虑智能体设计、骨干模型选择、漏洞类别和运营成本

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [69] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: SWE-Manager：一个通过强化学习训练的8B模型，用于在多个代码修复提案中选择最佳方案并合成黄金提案，在SWE-Lancer Manager基准测试中表现优于GPT-5等基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究主要关注代码生成和错误修复，但在实际软件开发中，团队需要从多个候选提案中选择最佳方案进行实施。好的选择能提高问题解决的可靠性并降低风险，而差的选择会增加风险甚至导致不可预测的故障。

Method: 首先通过人工研究分析维护者选择提案的理性依据，然后提出SWE-Manager：一个8B模型，通过强化学习训练来比较提案、证明选择合理性并合成黄金提案。将提案选择视为推理任务，模拟技术经理在不执行代码或运行测试的情况下权衡问题上下文和每个提案解决方案的过程。

Result: 在SWE-Lancer Manager基准测试中，SWE-Manager达到53.21%的选择准确率和57.75%的收益率，获得152,750美元收益，表现优于包括GPT-5在内的强基线模型。还设计了P2A框架来模拟真实工作流评估模型效果。

Conclusion: SWE-Manager通过强化学习训练，能够有效选择最佳代码修复提案并合成黄金提案，在提案选择任务上表现出色，为软件工程中的决策支持提供了新方法。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [70] [SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation](https://arxiv.org/abs/2601.23009)
*Wei Chen,Zhiyuan Peng,Xin Yin,Chao Ni,Chenhao Ying,Bang Xie,Yuan Luo*

Main category: cs.SE

TL;DR: SolAgent是一个工具增强的多智能体框架，专门用于生成功能正确且安全的智能合约。它采用双循环精炼机制，结合Forge编译器和Slither静态分析器，在SolEval+基准测试中达到64.39%的Pass@1率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能合约是去中心化网络的基础，但确保其功能正确性和安全性仍然是一个关键挑战。虽然大语言模型在代码生成方面表现出潜力，但它们在智能合约的严格要求下常常产生有缺陷或易受攻击的代码。

Method: 提出SolAgent，一个工具增强的多智能体框架，模拟人类专家的工作流程。该框架集成了双循环精炼机制：内循环使用Forge编译器确保功能正确性，外循环利用Slither静态分析器消除安全漏洞。此外，智能体还具备文件系统能力来解决复杂的项目依赖关系。

Result: 在SolEval+基准测试（源自高质量真实项目的严格套件）上，SolAgent实现了高达64.39%的Pass@1率，显著优于最先进的大语言模型（约25%）、AI IDE（如GitHub Copilot）和现有智能体框架。同时，与人工编写的基线相比，它将安全漏洞减少了高达39.77%。

Conclusion: SolAgent能够生成高质量、安全的智能合约代码。此外，SolAgent生成的高质量轨迹可用于蒸馏更小的开源模型，从而普及安全智能合约生成的访问。

Abstract: Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \textbf{dual-loop refinement mechanism}: an inner loop using the \textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \textbf{64.39\%}, significantly outperforming state-of-the-art LLMs ($\sim$25\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \textbf{39.77\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.

</details>


### [71] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究探讨代码注释对LLM自动修复bug能力的影响，发现训练和推理阶段都包含注释可将修复准确率提升至多3倍，且实现细节类注释效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前自动bug修复研究中普遍在训练前移除代码注释，但作者假设注释可能包含重要的设计和实现信息，对修复某些类型bug至关重要，需要验证注释对LLM修复能力的影响。

Method: 使用两种模型家族，在训练和推理阶段分别设置有无注释的四种组合条件进行对比实验；为解决现有数据集中注释不足的问题，使用LLM自动为缺少注释的方法生成注释。

Result: 注释在训练和推理阶段都出现时，可将自动bug修复准确率提升至多3倍；训练时包含注释不会降低无注释实例的性能；可解释性分析发现描述方法实现的注释对帮助LLM准确修复bug特别有效。

Conclusion: 代码注释对LLM的bug修复能力有显著正面影响，特别是在训练和推理阶段都包含注释时效果最佳，这挑战了当前移除注释的常见做法，建议在自动bug修复研究中保留注释。

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [72] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: GrepRAG：一种轻量级、无索引的词汇检索方法，用于仓库级代码补全，通过ripgrep命令检索相关上下文，性能优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义索引或图分析的RAG方法计算开销大，而开发者常用轻量级搜索工具（如ripgrep）。本文探索简单无索引的词汇检索在仓库级代码补全中的潜力。

Method: 1. 提出Naive GrepRAG基线框架：LLM自主生成ripgrep命令检索相关上下文；2. 提出GrepRAG改进方法：增加轻量级后处理流程，包括标识符加权重排序和结构感知去重。

Result: Naive GrepRAG性能与复杂图基线相当；GrepRAG在CrossCodeEval和RepoEval-Updated上持续优于SOTA方法，在CrossCodeEval上代码精确匹配相对提升7.04-15.58%。

Conclusion: 轻量级无索引词汇检索在仓库级代码补全中具有显著潜力，GrepRAG通过简单后处理克服词汇检索的局限性，实现高效性能。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions](https://arxiv.org/abs/2601.22211)
*Lingkai Kong,Anagha Satish,Hezi Jiang,Akseli Kangaslahti,Andrew Ma,Wenbo Chen,Mingxiao Song,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: LSFlow：一种用于组合强化学习的潜在球面流策略，通过将随机策略学习在连续潜在空间中，并利用组合优化求解器保证动作可行性，解决了组合动作空间中的挑战。


<details>
  <summary>Details</summary>
Motivation: 组合强化学习面临动作空间指数级增长和复杂可行性约束的挑战。现有方法要么将特定任务价值函数嵌入约束优化程序，要么学习确定性结构化策略，牺牲了通用性和策略表达能力。

Method: 提出LSFlow方法：1）在紧凑连续潜在空间中通过球面流匹配学习随机策略；2）使用组合优化求解器将潜在样本映射为有效结构化动作；3）在潜在空间中直接训练价值网络，避免重复求解器调用；4）引入平滑贝尔曼算子处理求解器引起的分段常数和不连续价值景观。

Result: 在多个具有挑战性的组合强化学习任务中，LSFlow平均优于最先进基线方法20.6%。

Conclusion: LSFlow成功将现代生成策略的表达能力引入组合强化学习，同时通过设计保证动作可行性，解决了组合动作空间中的关键挑战。

Abstract: Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.

</details>


### [74] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: 提出DAJ方法，通过双层数据重加权学习框架训练基于推理的LLM评判器，使用可验证奖励优化泛化性能，在代码生成测试时缩放中实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成中的测试时缩放通常依赖Best-of-N选择，但训练可靠的LLM评判器面临挑战：分布偏移严重（简单与困难问题不平衡、训练任务与评估基准不匹配、训练数据与推理模型行为不匹配）。

Method: 提出DAJ方法：1）基于推理的LLM评判器，使用可验证奖励训练；2）双层数据重加权学习框架，学习数据重要性权重（域级或实例级）以优化在目标基准对齐的元集上的泛化性能。

Result: 在LiveCodeBench和BigCodeBench上实现最先进性能，优于强基线测试时缩放方法和领先的专有模型。

Conclusion: DAJ是首个将数据重加权应用于LLM评判器训练的方法，能自动强调困难问题、分布内样本和轨迹对齐数据，无需手工启发式规则，有效解决分布偏移问题。

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [75] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM通过将代码组织成函数作为推理步骤，并引入元学习奖励修正机制，显著提升了LLM在复杂代码生成任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成方面有广泛应用，但在复杂编程任务上仍然经常失败。现有的过程奖励模型(PRM)方法在代码生成中效果不佳，因为代码缺乏有意义的步骤分解，且部分解决方案正确性评分存在噪声。

Method: FunPRM采用两种创新方法：1) 提示LLM生成模块化、函数化的代码，将函数作为PRM推理步骤；2) 引入元学习奖励修正机制，利用单元测试获得的干净最终解决方案奖励来净化噪声的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础LLM上始终优于现有测试时缩放方法，与O4-mini结合时在LiveCodeBench上实现了最先进的性能。此外，FunPRM生成的代码对开发者来说更具可读性和可重用性。

Conclusion: FunPRM通过将代码组织成函数作为推理步骤，并利用元学习净化奖励信号，有效解决了代码生成中PRM方法面临的核心挑战，显著提升了LLM在复杂编程任务上的性能。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [76] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 提出Bayesian Workflow Generation (BWG)框架，将工作流生成建模为贝叶斯推断问题，通过并行前瞻和序列优化器改进工作流构建，在六个基准数据集上显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流生成方法大多将任务视为优化问题，缺乏理论基础。作者希望建立更理论化的框架，将工作流生成形式化为贝叶斯推断问题。

Method: 提出BWG框架，将工作流生成视为对工作流后验分布的贝叶斯推断。具体实现为BayesFlow算法，使用并行前瞻rollout进行重要性加权，结合序列in-loop优化器进行池级改进，无需训练。

Result: 在六个基准数据集上，BayesFlow相比SOTA工作流生成基线提升准确率高达9个百分点，相比零样本提示提升高达65个百分点，证明了BWG作为基于搜索工作流设计的理论升级的有效性。

Conclusion: BWG为工作流生成提供了理论基础的贝叶斯推断框架，BayesFlow算法在多个任务上表现出显著优势，为自动工作流设计提供了原则性改进。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [77] [Purely Agentic Black-Box Optimization for Biological Design](https://arxiv.org/abs/2601.22382)
*Natalie Maus,Yimeng Zeng,Haydn Thomas Jones,Yining Huang,Gaurav Ng Goel,Alden Rose,Kyurae Kim,Hyun-Su Lee,Marcelo Der Torossian Torres,Fangping Wan,Cesar de la Fuente-Nunez,Mark Yatskar,Osbert Bastani,Jacob R. Gardner*

Main category: cs.LG

TL;DR: PABLO：一种完全基于智能体的语言推理方法，用于生物黑盒优化，在分子设计和抗菌肽优化任务中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有生物设计方法主要依赖原始结构数据，难以利用丰富的科学文献。虽然LLMs已被引入，但仅限于结构中心优化器中的狭窄角色。需要一种完全基于智能体的语言推理方法。

Method: PABLO（Purely Agentic BLack-box Optimization）：分层智能体系统，使用在化学和生物学文献上预训练的科学LLMs来生成和迭代优化生物候选物。

Result: 在GuacaMol分子设计和抗菌肽优化任务中达到最先进性能，显著提高样本效率和最终目标值。体外验证显示优化的肽对耐药病原体具有强活性。

Conclusion: 智能体化方法为实际设计提供关键优势：自然整合语义任务描述、检索增强的领域知识和复杂约束，在治疗发现中具有实际潜力。

Abstract: Many key challenges in biological design-such as small-molecule drug discovery, antimicrobial peptide development, and protein engineering-can be framed as black-box optimization over vast, complex structured spaces. Existing methods rely mainly on raw structural data and struggle to exploit the rich scientific literature. While large language models (LLMs) have been added to these pipelines, they have been confined to narrow roles within structure-centered optimizers. We instead cast biological black-box optimization as a fully agentic, language-based reasoning process. We introduce Purely Agentic BLack-box Optimization (PABLO), a hierarchical agentic system that uses scientific LLMs pretrained on chemistry and biology literature to generate and iteratively refine biological candidates. On both the standard GuacaMol molecular design and antimicrobial peptide optimization tasks, PABLO achieves state-of-the-art performance, substantially improving sample efficiency and final objective values over established baselines. Compared to prior optimization methods that incorporate LLMs, PABLO achieves competitive token usage per run despite relying on LLMs throughout the optimization loop. Beyond raw performance, the agentic formulation offers key advantages for realistic design: it naturally incorporates semantic task descriptions, retrieval-augmented domain knowledge, and complex constraints. In follow-up in vitro validation, PABLO-optimized peptides showed strong activity against drug-resistant pathogens, underscoring the practical potential of PABLO for therapeutic discovery.

</details>


### [78] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR是一个使用LLM作为上下文强化学习控制器的自动扩缩框架，用于多阶段ML推理管道，无需梯度更新即可在线改进策略，显著降低延迟和资源成本。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道由于异构资源、跨阶段耦合和动态瓶颈迁移而难以自动扩缩，需要一种能够适应动态变化且无需离线训练的解决方案。

Method: 使用LLM作为上下文强化学习控制器，结合帕累托优势奖励塑造与可证明分离边界、基于惊奇度的经验检索以提高上下文效率，以及通过用户空间CUDA拦截实现细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载模式下，SAIR实现了最佳或并列最佳的P99延迟和有效资源成本，P99延迟提升高达50%，有效成本降低高达97%，瓶颈检测准确率达到86%，且无需离线训练。

Conclusion: SAIR通过LLM作为上下文强化学习控制器，成功解决了多阶段ML推理管道的自动扩缩问题，在延迟、成本和准确性方面表现出色，无需复杂的离线训练过程。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [79] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 本文提出了一种用于LLM推理的显式对比学习方法，替代传统的GRPO方法，通过将结果分为正负集合并最大化正结果概率，在数学推理基准上取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽然有效，但其改进（如非对称裁剪和零方差数据过滤）需要大量经验洞察且难以识别。作者希望提出一种更简单直接的替代方法。

Method: 提出显式对比学习方法：将K个结果分为正负集合，然后最大化正结果的似然。该方法可视为LLM推理的多标签噪声对比估计的在线实例化。

Result: 在一系列具有挑战性的数学基准测试中，该方法与DAPO和在线DPO等强基线相比表现出有竞争力的性能。

Conclusion: 提出的显式对比学习方法为LLM推理提供了一种有效的替代方案，避免了GRPO方法中需要复杂调整的问题。

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [80] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种高效训练LLM推理任务的方法，通过堆采样和在线查询增强来优化提示池管理，减少计算成本同时保持性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源

Method: HeaPA维护有界演化池，使用堆采样跟踪能力边界，通过轻量级异步验证进行在线查询增强，并通过拓扑感知统计重估计和受控重插入稳定相关查询

Result: 在两个训练语料库、两种训练方法和七个基准测试中，HeaPA持续提高准确性，以更少计算达到目标性能，同时保持实际训练时间可比

Conclusion: HeaPA通过边界聚焦采样和在线池增长实现效率提升，且模型规模越大收益越明显，为RLVR训练提供了高效解决方案

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [81] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出教师-学生框架，将持续强化学习解耦为分布式RL训练单任务教师模型和持续蒸馏到中央通用模型两个独立过程，结合MoE架构和回放机制，在Meta-World基准上实现高效持续学习。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习面临稳定性-可塑性困境和灾难性遗忘的挑战，直接对顺序任务流应用RL难以实现可扩展性能。观察到RL擅长解决单任务，而策略蒸馏作为相对稳定的监督学习过程，更适合大基础模型和多任务学习。

Method: 教师-学生框架：1) 分布式RL训练单任务教师模型；2) 持续蒸馏到中央通用学生模型；3) 采用混合专家(MoE)架构增强可塑性；4) 使用回放机制增强稳定性。

Result: 在Meta-World基准上，框架恢复了超过85%的教师模型性能，同时将任务间遗忘控制在10%以内，实现了高效的持续强化学习。

Conclusion: 通过解耦单任务RL训练和持续策略蒸馏，结合MoE和回放机制，能够有效解决持续强化学习中的稳定性-可塑性困境，实现可扩展的终身学习。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [82] [Mem-T: Densifying Rewards for Long-Horizon Memory Agents](https://arxiv.org/abs/2601.23014)
*Yanwei Yue,Guibin Zhang,Boci Peng,Xuanbo Fan,Jiaxin Guo,Qiankun Li,Yan Zhang*

Main category: cs.LG

TL;DR: Mem-T是一个自主记忆代理，通过层次化记忆数据库和树引导强化学习框架MoT-GRPO，实现了内存构建与检索的联合优化，在性能和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有记忆代理的训练范式存在限制：代理需要在稀疏延迟奖励下执行长序列记忆操作，难以实现真正的端到端内存管理策略优化。

Method: 提出Mem-T自主记忆代理，使用轻量级层次化记忆数据库处理流式输入；提出MoT-GRPO树引导强化学习框架，通过记忆操作树反向传播和事后信用分配将稀疏终端反馈转化为密集的逐步监督。

Result: Mem-T性能优于A-Mem和Mem0等框架达14.92%，在准确率-效率帕累托前沿表现优异，相比GAM减少约24.45%的推理token消耗且不牺牲性能。

Conclusion: Mem-T通过端到端优化的内存管理策略，实现了高性能且经济的自主记忆代理，解决了长序列记忆操作中的稀疏奖励问题。

Abstract: Memory agents, which depart from predefined memory-processing pipelines by endogenously managing the processing, storage, and retrieval of memories, have garnered increasing attention for their autonomy and adaptability. However, existing training paradigms remain constrained: agents often traverse long-horizon sequences of memory operations before receiving sparse and delayed rewards, which hinders truly end-to-end optimization of memory management policies. To address this limitation, we introduce Mem-T, an autonomous memory agent that interfaces with a lightweight hierarchical memory database to perform dynamic updates and multi-turn retrieval over streaming inputs. To effectively train long-horizon memory management capabilities, we further propose MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment, thereby enabling the joint optimization of memory construction and retrieval. Extensive experiments demonstrate that Mem-T is (1) high-performing, surpassing frameworks such as A-Mem and Mem0 by up to $14.92\%$, and (2) economical, operating on a favorable accuracy-efficiency Pareto frontier and reducing inference tokens per query by $\sim24.45\%$ relative to GAM without sacrificing performance.

</details>


### [83] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: 提出MC-GRPO方法，用中位数基线替代均值基线，解决小样本训练中优势符号翻转问题，提升低样本量下的训练稳定性和最终精度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小样本训练场景中，基于均值的组相对策略优化方法因基线噪声导致优势符号翻转，使部分样本更新方向错误，导致精度下降。

Method: 提出中位数中心化组相对策略优化(MC-GRPO)：用中位数基线替代均值基线，生成G+1个样本用于中位数参考，排除中位数样本的梯度回传，保持与标准G样本训练相同的计算成本。

Result: 在各种GRPO系列方法、不同模型和规模上，中位数中心化训练在低样本量下显著提升稳定性和最终精度，将G=2和G=8之间的差距缩小到1%以内。

Conclusion: MC-GRPO通过中位数基线有效缓解小样本训练中的优势符号翻转问题，是一种简单有效的改进方案，在保持计算成本不变的同时显著提升性能。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [84] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 提出一个结合形式逻辑验证与LLM推理的框架，通过实时验证反馈纠正推理错误，显著提升模型在数学、逻辑等任务上的性能


<details>
  <summary>Details</summary>
Motivation: LLMs虽然表现出色，但其基于概率的token预测容易产生逻辑不一致和奖励黑客问题，而形式符号系统可以避免这些问题。需要桥接神经网络的生成能力与形式逻辑的严谨性。

Method: 提出形式逻辑验证引导的框架，在自然语言生成过程中动态交织形式符号验证，提供实时反馈检测和纠正错误。采用两阶段训练流程：形式逻辑验证引导的监督微调和策略优化。

Result: 在六个涵盖数学、逻辑和一般推理的基准测试中，7B和14B模型分别以平均10.4%和14.2%的幅度超越最先进的基线模型。

Conclusion: 形式验证可以作为可扩展的机制，显著推动先进LLM推理的性能边界，有效结合神经网络的生成能力与形式逻辑的严谨性。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [85] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型在周期性OOD泛化方面的局限性，构建了Coper基准测试，发现模型能记忆训练数据中的周期性模式但无法泛化到未见过的复合周期性场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大语言模型在分布外泛化方面与人类存在显著差距，作者希望通过研究周期性这一基本OOD场景来探索这种差距的原因。

Method: 从抽象代数和推理的角度统一解释周期性（包括单一和复合周期性），构建了Coper基准测试，包含Hollow和Extrapolation两种OOD设置，通过实验评估Transformer的周期性泛化能力。

Result: 实验表明Transformer在周期性泛化方面存在局限：模型能在训练期间记忆周期性数据，但无法泛化到未见过的复合周期性场景。

Conclusion: Transformer模型在周期性OOD泛化方面存在本质限制，需要进一步研究来提升模型在这方面的能力。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [86] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 本文提出Style-Conditioned Implicit Q-Learning (SCIQL)，一种离线强化学习方法，通过子轨迹标注函数实现显式风格监督，有效平衡任务性能与风格对齐。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，风格条件策略的训练面临分布偏移和风格与奖励固有冲突的挑战，现有方法难以有效协调这两个目标。

Method: 提出统一的行为风格定义，并基于此构建SCIQL框架，结合离线目标条件RL技术（如后见重标注和价值学习），引入门控优势加权回归机制优化任务性能同时保持风格对齐。

Result: 实验表明SCIQL在任务性能和风格对齐两方面均优于现有离线方法。

Conclusion: SCIQL通过统一风格定义和门控机制，成功解决了离线RL中风格与任务性能的平衡问题，为风格条件策略学习提供了有效框架。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [87] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无剪裁的策略优化方法，用凸二次惩罚替代启发式剪裁，解决大规模LLM后训练中的优化问题，保持稳定训练同时避免梯度消失和奖励攻击。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为大型语言模型后训练的核心技术，但主流算法依赖剪裁机制，在大规模应用中引入了优化问题，包括零梯度区域、奖励攻击和训练不稳定性。

Method: 提出Clipping-Free Policy Optimization (CFPO)，用基于总变差散度约束的凸二次惩罚替代启发式剪裁，生成处处可微的目标函数，实现无硬边界的稳定策略更新。

Result: 在推理任务中，CFPO在基准测试上匹配剪裁方法并扩展了稳定训练范围；在对齐任务中，缓解了冗长利用问题，减少了能力退化，同时保持了有竞争力的指令跟随性能。

Conclusion: CFPO仅需一行代码更改且无需额外超参数，是LLM后训练中替代剪裁方法的有前景的直接替代方案。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [88] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出连续约束插值（CCI）框架统一离线RL中的三种约束家族，并开发自动约束策略优化（ACPO）算法，在多个基准上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法使用不同的约束形式（加权行为克隆、密度正则化、支持约束），但缺乏统一框架来解释它们的联系和权衡

Method: 提出连续约束插值（CCI）框架，通过单个插值参数实现三种约束类型的平滑过渡和组合；基于CCI开发自动约束策略优化（ACPO）算法，使用拉格朗日对偶更新自适应调整插值参数

Result: 在D4RL和NeoRL2基准测试中表现出稳健的性能提升，实现了整体最先进的性能

Conclusion: CCI框架为离线RL中的约束方法提供了统一视角，ACPO算法能够自适应地选择最优约束形式，在多种领域都取得了优异表现

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [89] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL提出了一种新颖的多任务强化学习方法，通过将命题视为参数化谓词而非离散符号，实现了对未见命题和任务的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于线性时序逻辑（LTL）的多任务强化学习方法虽然能在LTL规范结构间泛化，但无法泛化到未见过的命题词汇表（描述高级事件的符号）。这限制了智能体处理新任务的能力。

Method: 将命题视为参数化谓词的实例而非离散符号，提出新颖的架构来嵌入和组合谓词以表示LTL规范，使策略能够学习相关命题间的共享结构。

Result: 在具有挑战性的环境中，成功实现了对新颖命题和任务的零样本泛化，不仅能在LTL公式结构上组合泛化，还能在命题上参数化泛化。

Conclusion: PlatoLTL通过参数化谓词表示方法，有效解决了多任务强化学习中未见命题的泛化问题，为构建更通用的智能体提供了新途径。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [90] [Agile Reinforcement Learning through Separable Neural Architecture](https://arxiv.org/abs/2601.23225)
*Rajib Mostakim,Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: SPAN是一种基于样条的自适应网络，通过可学习的预处理层和可分离的张量积B样条基，在资源受限环境中实现比MLP基线更高的样本效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在资源受限环境中部署时，传统的多层感知机存在参数效率低、样本效率差的问题，而现有的样条网络（如KANs）虽然参数效率高但计算开销大。

Method: SPAN基于低秩KHRONOS框架，集成了可学习的预处理层和可分离的张量积B样条基，构建了一种新型函数逼近方法。

Result: 在离散（PPO）和高维连续（SAC）控制任务以及离线设置（Minari/D4RL）中，SPAN相比MLP基线实现了30-50%的样本效率提升和1.3-9倍的成功率提升。

Conclusion: SPAN在资源受限环境中表现出优越的实时性能和对超参数变化的鲁棒性，是学习内在高效策略的可行高性能替代方案。

Abstract: Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although model compression techniques exist, they operate post-hoc and do not improve learning efficiency. Recent spline-based separable architectures - such as Kolmogorov-Arnold Networks (KANs) - have been shown to offer parameter efficiency but are widely reported to exhibit significant computational overhead, especially at scale.
  In seeking to address these limitations, this work introduces SPAN (SPline-based Adaptive Networks), a novel function approximation approach to RL. SPAN adapts the low rank KHRONOS framework by integrating a learnable preprocessing layer with a separable tensor product B-spline basis. SPAN is evaluated across discrete (PPO) and high-dimensional continuous (SAC) control tasks, as well as offline settings (Minari/D4RL). Empirical results demonstrate that SPAN achieves a 30-50% improvement in sample efficiency and 1.3-9 times higher success rates across benchmarks compared to MLP baselines. Furthermore, SPAN demonstrates superior anytime performance and robustness to hyperparameter variations, suggesting it as a viable, high performance alternative for learning intrinsically efficient policies in resource-limited settings.

</details>


### [91] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: 提出DC-CoT方法，通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理（Long CoT）虽然能提升LLM在数学推理等任务上的性能，但顺序生成导致高延迟问题。需要一种方法在保持准确率的同时降低推理延迟。

Method: 提出Divide-and-Conquer CoT（DC-CoT）：1）模型作为导演识别可并行执行的子任务；2）生成工作节点执行这些子任务；3）先用SFT初始化模型生成工作节点的能力；4）设计多阶段RL算法恢复准确率并降低最长路径长度。

Result: 在AIME 2024和HMMT 2025等多个基准测试中，DC-CoT在保持与DeepScaleR-1.5B-Preview相似准确率的同时，将最长路径长度降低了35-40%。

Conclusion: DC-CoT通过并行推理有效降低了长思维链的延迟，在保持模型准确性的同时显著提升了推理效率，为低延迟推理提供了可行方案。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [92] [Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients](https://arxiv.org/abs/2601.23135)
*Cheng Ge,Caitlyn Heqi Yin,Hao Liang,Jiawei Zhang*

Main category: cs.LG

TL;DR: 本文从序列级策略梯度的局部曲率角度，解释了GRPO中标准差归一化如何实现自适应梯度，理论上证明了GRPO相比未归一化的REINFORCE具有更优的收敛速率，并通过实验揭示了训练过程中的三个不同阶段。


<details>
  <summary>Details</summary>
Motivation: GRPO已成为语言模型推理中的标准RL算法，它通过每个提示的基线和方差归一化避免了批评器的需求。然而，这种归一化为何以及何时有效仍然不清楚。本文旨在从理论角度解释GRPO中标准差归一化的作用机制。

Method: 从序列级策略梯度的局部曲率角度分析GRPO，理论上证明GRPO的收敛优势，并在GSM8K和MATH基准上进行实证分析，研究特征正交性和奖励方差之间的相互作用。

Result: 理论上，在温和条件下，GRPO相比未归一化的REINFORCE具有严格改进的收敛速率，增益由跨提示和迭代的平均组内奖励标准差表征。实证分析揭示了三个训练阶段：早期加速阶段（高方差和正交性有利于自适应缩放）、相对稳定的过渡阶段、以及后期阶段（正交性损失限制了进一步增益）。

Conclusion: 本文为GRPO中标准差归一化何时有效提供了原则性解释，并为无批评器RL算法的设计提供了更广泛的见解。

Abstract: Reinforcement learning (RL) has become a key driver of language model reasoning. Among RL algorithms, Group Relative Policy Optimization (GRPO) is the de facto standard, avoiding the need for a critic by using per-prompt baselines and variance normalization. Yet why and when this normalization helps remains unclear. In this work, we provide an explanation through the lens of local curvature of the sequence-level policy gradient: standard deviation normalization implements an adaptive gradient. Theoretically, under mild conditions, GRPO enjoys a strictly improved convergence rate over unnormalized REINFORCE, with gains characterized by the average within-prompt reward standard deviation across prompts and iterations. Empirically, our analysis on GSM8K and MATH benchmarks reveals three distinct training phases governed by the interplay between feature orthogonality and reward variance: (I) an early acceleration phase where high variance and orthogonality favor adaptive scaling; (II) a relatively stable transition phase; and (III) a late-stage regime where the loss of orthogonality limits further gains. Together, these results provide a principled account of when std normalization helps in GRPO, and offer broader insights into the design of critic-free RL algorithms.

</details>
