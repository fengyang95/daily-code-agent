<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [cs.AI](#cs.AI) [Total: 14]
- [tldr.article](#tldr.article) [Total: 6]
- [wechat.article](#wechat.article) [Total: 36]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh,Suhong Moon,Serina Chang*

Main category: cs.CL

TL;DR: GEMS是一种基于图神经网络的轻量级人类模拟方法，在离散选择模拟任务中比大型语言模型更高效且性能相当或更好。


<details>
  <summary>Details</summary>
Motivation: 探讨在人类模拟任务中，是否必须使用大型语言模型，或者可以使用更小、领域特定的模型来达到相同效果。

Method: 将离散选择模拟任务转化为图上的链接预测问题，使用图神经网络，仅在需要时融入语言表示。

Result: 在三个模拟数据集上的评估显示，GEMS在准确率上与LLM相当或更好，同时效率、可解释性和透明度显著更高。

Conclusion: 图神经网络为人类模拟提供了一种轻量级替代方案，在某些场景下可以替代大型语言模型。

Abstract: Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [2] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 该研究开发了一个自动化基础设施来评估医疗聊天机器人在涉及人口统计信息时的表现，发现LLM评估者之间一致性较低，建议使用多个LLM评估器以避免统计显著但不可泛化的结果。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人必须在涉及非医疗因素（如人口统计信息）时提供一致的建议，但目前LLM在医疗环境中存在幻觉、遗漏和偏见问题，需要了解其失效条件。

Method: 开发自动化基础设施：1）通过采样患者人口统计、病史、疾病和写作风格生成真实查询；2）使用多个LLM-as-a-judge设置和提示来评估答案，包括幻觉和遗漏检测以及治疗类别检测。

Result: LLM标注者表现出低一致性（平均Cohen's Kappa κ=0.118），只有特定的（回答、评估）LLM对在不同写作风格、性别和种族间产生统计显著差异。

Conclusion: 建议使用多个LLM作为评估器以避免统计显著但不可泛化的结果，特别是在缺乏真实数据时，并建议发布LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [3] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: LTD-Bench是一个突破性的基准测试，通过要求模型通过点阵或可执行代码生成绘图，将LLM评估从抽象分数转变为直接可观察的视觉输出，暴露了当前LLM在空间推理方面的根本局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估范式存在关键盲点，依赖不透明的数值指标掩盖了空间推理的基本限制，无法直观理解模型能力，导致报告性能与实际能力之间存在危险脱节。

Method: LTD-Bench采用全面方法，包含互补的生成任务（测试空间想象力）和识别任务（评估空间感知），跨越三个渐进难度级别，系统评估语言-空间映射的两个关键方向。

Result: 实验显示令人担忧的能力差距：即使在传统基准上表现优异的LLM，在建立语言与空间概念之间的双向映射方面也表现出严重缺陷，这削弱了它们作为真正世界模型的潜力。

Conclusion: LTD-Bench的视觉输出支持强大的诊断分析，为研究模型相似性提供了潜在方法，揭示了LLM在空间推理方面的根本局限性。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [4] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: TRACE框架通过程序化策略应用解决大语言模型与现实政策之间的对齐差距，实现精确的遗忘学习而不损害模型性能


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对齐方法产生静态、脆弱且维护成本高的模型，无法跟上不断发展的规范和政策，存在对齐-现实差距问题

Method: TRACE框架通过程序化筛选现有偏好数据、识别高影响冲突的alignment impact score，以及混合优化来精确反转、丢弃或保留偏好

Result: 在多个模型家族上实现稳健的重新对齐，在合成基准和PKU-SafeRLHF数据集上执行新原则而不降低通用能力

Conclusion: TRACE为维持LLM对齐提供了可扩展、动态且成本效益高的范式，为可持续和负责任的AI部署奠定基础

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [5] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 提出了CoRL框架，通过强化学习优化多LLM系统的性能与成本权衡，在集中式控制器下选择性调用专家模型，实现成本可控的高效协作。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多LLM系统为每个输入调用多个模型，导致推理成本高昂且不可控，需要设计成本高效且可控的集中式多代理系统。

Method: 使用强化学习框架，通过集中式控制器选择性协调专家模型池，在最大化任务性能的同时最小化总体推理成本，支持多预算条件下的自适应行为。

Result: 在四个不同基准测试中，CoRL系统在高预算设置下超越最佳专家LLM，在低预算模式下仍保持强劲性能。

Conclusion: 集中式协调为可扩展且成本高效的多代理LLM系统提供了有效解决方案。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [6] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan,Jie Lou,Zichao Li,Jiawei Chen,Yaojie Lu,Hongyu Lin,Le Sun,Debing Zhang,Xianpei Han*

Main category: cs.CL

TL;DR: MemSearcher是一个搜索代理工作流，通过迭代维护紧凑内存来平衡信息完整性和效率，在多轮交互中稳定上下文长度，显著提升性能同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统搜索代理要么保留完整交互历史导致长上下文和高成本，要么仅使用当前轮次丢弃关键信息，这种权衡限制了搜索代理的可扩展性。

Method: 提出MemSearcher工作流，每轮将用户问题与内存融合生成推理轨迹，执行搜索动作，并更新内存仅保留任务必需信息。使用多上下文GRPO强化学习框架联合优化推理、搜索策略和内存管理。

Result: 在七个公开基准测试中，相比强基线取得显著提升：Qwen2.5-3B-Instruct平均提升11%，Qwen2.5-7B-Instruct平均提升12%。3B版本的MemSearcher甚至优于7B基线。

Conclusion: 在信息完整性和效率之间取得平衡既能提高准确性又能降低计算开销，证明了MemSearcher设计的有效性。

Abstract: Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 提出人机共体现智能系统APEX，将人类执行能力与AI推理能力结合，通过混合现实实现物理实验和制造的智能化协作


<details>
  <summary>Details</summary>
Motivation: 解决传统AI模型局限于虚拟领域，而真实世界实验制造仍依赖人类监督的问题，弥合机器智能与物理执行之间的差距

Method: 开发APEX系统，通过可穿戴设备捕捉实验过程，AI进行情境推理、自适应规划和实时反馈，结合混合现实提供3D视觉指导

Result: 在洁净室柔性电子制造中实现情境感知推理，准确率超过通用多模态大语言模型，实时纠错并可将专业知识传授给初学者

Conclusion: 建立了一类新的代理-物理-人类智能系统，将代理推理从计算领域扩展到物理领域，使科研制造实现自主、可追溯、可解释和可扩展

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [8] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 使用基础模型自动搜索奖励函数，通过文本指令生成Gran Turismo 7赛车游戏的强化学习智能体，结合LLM奖励生成、VLM偏好评估和人类反馈，实现了与冠军级智能体GT Sophy竞争的表现。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，通过奖励函数定义期望行为是一个困难的过程，特别是在复杂环境如自动驾驶赛车中。传统方法难以将期望行为准确映射到奖励函数。

Method: 结合LLM生成奖励函数、VLM基于偏好的评估和人类反馈，构建自动化奖励设计系统，通过文本指令搜索最优奖励函数空间。

Result: 系统能够生成与冠军级RL赛车智能体GT Sophy竞争的赛车智能体，并能产生新颖的行为模式。

Conclusion: 该方法为实际应用中的自动化奖励设计铺平了道路，展示了基础模型在复杂RL任务中的有效性。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [9] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: 提出了STRMAC框架，通过状态感知路由实现多智能体系统的高效协作，分别编码交互历史和智能体知识来动态选择最适合的智能体，并引入自演化数据生成方法加速高质量执行路径的收集。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统存在智能体调度僵化和协调策略低效的问题，无法适应动态任务需求，限制了系统的潜力发挥。

Method: STRMAC框架分别编码交互历史和智能体知识，通过路由机制自适应选择每个步骤中最合适的单个智能体，并采用自演化数据生成方法加速高质量训练数据的收集。

Result: 在协作推理基准测试中达到最先进性能，比基线提升23.8%，相比穷举搜索减少90.1%的数据收集开销。

Conclusion: STRMAC框架通过状态感知路由和自演化数据生成，有效解决了多智能体系统中的协作效率问题，显著提升了系统性能并降低了训练成本。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [10] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 该论文分析了多智能体推理中出现的懒惰行为问题，提出了因果影响测量方法和可验证奖励机制来缓解这一问题，从而释放多智能体框架在复杂推理任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在多智能体推理框架中，经常出现一个智能体主导而另一个贡献很少的懒惰行为，这破坏了协作效果，使多智能体设置退化为无效的单智能体系统。

Method: 首先进行理论分析解释懒惰行为的产生原因，然后引入稳定高效的因果影响测量方法，最后提出可验证奖励机制，允许推理智能体丢弃噪声输出、整合指令并在必要时重新开始推理过程。

Result: 大量实验表明，该框架有效缓解了懒惰智能体行为，充分发挥了多智能体框架在复杂推理任务中的潜力。

Conclusion: 通过因果影响测量和可验证奖励机制，可以成功解决多智能体推理中的协作问题，提升整体性能。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [11] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi,Hyungmin Kim,Hyobin Ong,Minsu Jang,Dohyung Kim,Jaehong Kim,Youngwoo Yoon*

Main category: cs.AI

TL;DR: ReAcTree是一种分层任务规划方法，通过构建动态代理树将复杂目标分解为可管理的子目标，结合推理-行动-扩展机制和双记忆系统，显著提升了LLM在复杂长程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂长程任务时，依赖单一轨迹纠缠所有决策和观察，难以有效分解任务。需要解决分层规划和记忆管理的问题。

Method: 构建动态代理树，每个代理节点负责处理子目标，包含推理、行动和扩展能力；控制流节点协调执行策略；集成情景记忆和工作记忆双系统。

Result: 在WAH-NL和ALFRED数据集上，ReAcTree显著优于ReAct等基线方法。在WAH-NL上，使用Qwen 2.5 72B达到61%的目标成功率，几乎是ReAct（31%）的两倍。

Conclusion: 分层任务分解和双记忆系统能有效提升LLM在复杂任务中的规划能力，ReAcTree为具身智能体的任务规划提供了有效解决方案。

Abstract: Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [12] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为VMR的新训练策略，将开放式任务重新构建为可验证的多选格式，从而在缺乏标准答案的情况下有效训练LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖标准答案验证，无法应用于开放式任务。作者希望探索推理能力是否能提升开放式任务的性能。

Method: 提出了可验证多选重构(VMR)方法，将开放式数据重新构建为可验证的多选格式，使RLVR范式能够应用于开放式任务。

Result: 在8个开放式基准测试中，VMR训练相比基线平均提升了5.99个点，验证了方法的有效性。

Conclusion: 通过VMR方法成功将RLVR范式扩展到开放式领域，证明了强化推理能力确实能提升开放式任务的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [13] [Agentic AI for Mobile Network RAN Management and Optimization](https://arxiv.org/abs/2511.02532)
*Jorge Pellejero,Luis A. Hernández Gómez,Luis Mendo Tomás,Zoraida Frias Barroso*

Main category: cs.AI

TL;DR: 本文提出将Agentic AI应用于5G/6G网络RAN优化的框架，通过LAMs实现自主决策和动态优化，解决了复杂网络环境下手动优化的局限性。


<details>
  <summary>Details</summary>
Motivation: 5G/6G网络的复杂性使得手动优化变得无效，需要Agentic AI来自动化动态RAN环境中的决策过程，但目前缺乏统一的框架和定义。

Method: 提出Agentic AI核心概念和设计模式（反思、规划、工具使用、多智能体协作），并在5G RAN案例中展示时间序列分析和LAM驱动智能体的KPI自主决策协作。

Result: 建立了Agentic AI在移动网络中的理论基础，并通过实际案例验证了其在RAN管理和优化中的应用可行性。

Conclusion: Agentic AI为5G/6G网络自动化优化提供了有效方法，能够实现自主目标分解、上下文保持、持续学习和动态适应。

Abstract: Agentic AI represents a new paradigm for automating complex systems by using
Large AI Models (LAMs) to provide human-level cognitive abilities with
multimodal perception, planning, memory, and reasoning capabilities. This will
lead to a new generation of AI systems that autonomously decompose goals,
retain context over time, learn continuously, operate across tools and
environments, and adapt dynamically. The complexity of 5G and upcoming 6G
networks renders manual optimization ineffective, pointing to Agentic AI as a
method for automating decisions in dynamic RAN environments. However, despite
its rapid advances, there is no established framework outlining the
foundational components and operational principles of Agentic AI systems nor a
universally accepted definition.
  This paper contributes to ongoing research on Agentic AI in 5G and 6G
networks by outlining its core concepts and then proposing a practical use case
that applies Agentic principles to RAN optimization. We first introduce Agentic
AI, tracing its evolution from classical agents and discussing the progress
from workflows and simple AI agents to Agentic AI. Core design
patterns-reflection, planning, tool use, and multi-agent collaboration-are then
described to illustrate how intelligent behaviors are orchestrated. These
theorical concepts are grounded in the context of mobile networks, with a focus
on RAN management and optimization. A practical 5G RAN case study shows how
time-series analytics and LAM-driven agents collaborate for KPI-based
autonomous decision-making.

</details>


### [14] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 提出了首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规并使用归纳逻辑编程在线修复规范，确保屏蔽器优雅演化并保持最优奖励和逻辑合规性。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法假设固定的逻辑规范和手工抽象，在环境假设被违反时无法适应，导致安全性和性能问题。

Method: 基于GR(1)规范开发自适应屏蔽框架，使用归纳逻辑编程在线检测环境假设违规并自动修复规范，确保屏蔽器动态演化。

Result: 在Minepump和Atari Seaquest案例中，自适应屏蔽相比静态屏蔽能保持接近最优的奖励和完美的逻辑合规性。

Conclusion: 自适应屏蔽框架能够有效处理环境假设变化，在保证安全性的同时维持高性能，是强化学习中安全执行的重要进展。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [15] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 提出了一个协作迷宫求解基准来评估AI代理间的协作能力，发现存在'协作鸿沟'：单独表现良好的模型在协作时性能显著下降，并提出'接力推理'方法改善协作效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向多代理架构发展，异构代理间的有效协作变得至关重要，但目前缺乏大规模评估代理间协作能力的实证研究。

Method: 设计协作迷宫求解基准框架，评估32个领先的开源和闭源模型在单独、同质配对和异质配对三种情况下的表现。

Result: 发现明显的'协作鸿沟'：单独表现好的模型在协作时性能大幅下降，小型蒸馏模型在某些配对中几乎完全失败。采用'接力推理'方法（强代理先导后转交弱代理）能显著缩小协作鸿沟。

Conclusion: 需要协作感知的评估方法、增强协作能力的训练策略，以及能可靠激发代理潜在技能的交互设计，这些指导原则适用于AI-AI和人类-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [16] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: 提出了CostBench基准测试，评估LLM代理在成本优化规划和动态环境适应方面的能力，发现现有代理在成本感知规划和实时适应方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理评估主要关注任务完成度，忽视了资源效率和适应性，特别是代理在变化环境中制定和调整成本最优计划的能力。

Method: 开发了CostBench基准测试，包含旅行规划任务，支持多种原子和复合工具，以及四种动态阻塞事件（工具故障、成本变化等）来模拟现实世界的不确定性。

Result: 评估显示代理在成本感知规划方面存在显著差距：在静态设置中经常无法找到成本最优解，GPT-5在最难任务上的精确匹配率低于75%，动态条件下性能进一步下降约40%。

Conclusion: CostBench为开发既经济理性又鲁棒的未来代理奠定了基础，揭示了当前代理在成本优化和动态适应方面的弱点。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [17] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 提出了一种轻量级、模型无关的多模态诊断框架，通过将每个模态视为代理来识别模态破坏现象，即高置信度的单模态错误会覆盖其他证据并误导融合结果。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型发展迅速，但其推理过程仍然不透明，难以理解哪个模态驱动预测、如何解决冲突或何时某个模态主导决策。

Method: 将每个模态视为代理，生成候选标签和简要自评估，通过简单融合机制聚合输出，暴露贡献者（支持正确结果的模态）和破坏者（误导的模态）。

Result: 在情感识别基准测试中的应用揭示了系统性的可靠性特征，提供了关于失败是源于数据集伪影还是模型限制的见解。

Conclusion: 该框架为多模态推理提供了诊断支架，支持对融合动态的原则性审计，并为可能的干预措施提供信息。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [18] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 该论文提出了一种优化AI控制评估中攻击策略的方法，通过将攻击能力分解为五个技能组件，并在SHADE-Arena环境中进行优化，显著提升了攻击强度。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得复杂且高风险，需要准确评估其风险。AI控制框架可用于此目的，但在复杂智能体环境中，计算约束导致数据不足，难以获得强大的攻击策略。

Method: 将攻击能力分解为五个技能组件（怀疑建模、攻击选择、计划合成、执行和隐蔽性），开发攻击动态的概率模型，在模拟中优化攻击超参数，并将结果迁移到SHADE-Arena环境。

Result: 该方法显著提升了攻击强度，安全评分从基线0.87降低到0.41。

Conclusion: 通过技能分解和概率模型优化，可以在数据有限的情况下有效提升AI控制评估中的攻击策略强度。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


### [19] [Kosmos: An AI Scientist for Autonomous Discovery](https://arxiv.org/abs/2511.02824)
*Ludovico Mitchener,Angela Yiu,Benjamin Chang,Mathieu Bourdenx,Tyler Nadolski,Arvis Sulovari,Eric C. Landsness,Daniel L. Barabasi,Siddharth Narayanan,Nicky Evans,Shriya Reddy,Martha Foiani,Aizad Kamal,Leah P. Shriver,Fang Cao,Asmamaw T. Wassie,Jon M. Laurent,Edwin Melville-Green,Mayk Caldas,Albert Bou,Kaleigh F. Roberts,Sladjana Zagorac,Timothy C. Orr,Miranda E. Orr,Kevin J. Zwezdaryk,Ali E. Ghareeb,Laurie McCoy,Bruna Gomes,Euan A. Ashley,Karen E. Duff,Tonio Buonassisi,Tom Rainforth,Randall J. Bateman,Michael Skarlinski,Samuel G. Rodriques,Michaela M. Hinks,Andrew D. White*

Main category: cs.AI

TL;DR: Kosmos是一个AI科学家系统，能够自动化数据驱动的科学发现过程，通过结构化世界模型在数据分析和文献搜索代理之间共享信息，实现长达12小时、200次代理滚动的连贯研究，生成可追溯的科学报告。


<details>
  <summary>Details</summary>
Motivation: 当前AI科研代理在采取多次行动后会失去连贯性，限制了其发现深度。需要开发能够进行长期连贯科学研究的AI系统。

Method: 使用结构化世界模型连接数据分析和文献搜索代理，进行并行数据分析、文献搜索和假设生成的循环，每个运行平均执行42,000行代码和阅读1,500篇论文。

Result: 独立科学家评估79.4%的报告陈述准确，单次20循环运行相当于6个月人工研究时间，有价值发现数量与循环次数呈线性关系。在代谢组学、材料科学等领域做出了7项发现。

Conclusion: Kosmos证明了AI系统能够进行长期连贯的科学研究，产生可验证的新发现，显著加速科学发现过程。

Abstract: Data-driven scientific discovery requires iterative cycles of literature
search, hypothesis generation, and data analysis. Substantial progress has been
made towards AI agents that can automate scientific research, but all such
agents remain limited in the number of actions they can take before losing
coherence, thus limiting the depth of their findings. Here we present Kosmos,
an AI scientist that automates data-driven discovery. Given an open-ended
objective and a dataset, Kosmos runs for up to 12 hours performing cycles of
parallel data analysis, literature search, and hypothesis generation before
synthesizing discoveries into scientific reports. Unlike prior systems, Kosmos
uses a structured world model to share information between a data analysis
agent and a literature search agent. The world model enables Kosmos to
coherently pursue the specified objective over 200 agent rollouts, collectively
executing an average of 42,000 lines of code and reading 1,500 papers per run.
Kosmos cites all statements in its reports with code or primary literature,
ensuring its reasoning is traceable. Independent scientists found 79.4% of
statements in Kosmos reports to be accurate, and collaborators reported that a
single 20-cycle Kosmos run performed the equivalent of 6 months of their own
research time on average. Furthermore, collaborators reported that the number
of valuable scientific findings generated scales linearly with Kosmos cycles
(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that
span metabolomics, materials science, neuroscience, and statistical genetics.
Three discoveries independently reproduce findings from preprinted or
unpublished manuscripts that were not accessed by Kosmos at runtime, while four
make novel contributions to the scientific literature.

</details>


### [20] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: 提出了Agent-Omni框架，通过主代理系统协调现有基础模型，实现无需重新训练的灵活多模态推理


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型受限于固定模态对，需要大量对齐数据集进行微调，构建全能的文本、图像、音频、视频整合模型仍不实用且缺乏稳健推理支持

Method: 使用主代理系统，主代理解释用户意图，将子任务委托给特定模态代理，并整合它们的输出形成连贯响应

Result: 在文本、图像、音频、视频和全能基准测试中，Agent-Omni始终达到最先进性能，特别是在需要复杂跨模态推理的任务上

Conclusion: 基于代理的设计能够无缝集成专业化基础模型，确保对多样化输入的适应性，同时保持透明性和可解释性，框架模块化且易于扩展

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [21] [Agentic Commerce: Excitement, Caution, and Confidence](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.finextra.com%2Fblogposting%2F29696%2Fagentic-commerce-excitement-caution-and-confidence%3Futm_source=tldrfintech/1/0100019a4a0aaae9-081a40d6-9964-42e7-9fc5-e2b3deeba75e-000000/tOYWVh8MReFkr0NkknorGYAMjfk5ObCVZBwYiqaQuS8=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了代理商务（Agentic Commerce）作为数字支付新时代的可能性，其中AI软件能够代表用户进行搜索、决策和支付。


<details>
  <summary>Details</summary>
Motivation: 当前数字支付发展迅速，但主要关注即时支付、稳定币等基础设施。作者认为更根本的变革可能来自能够代表用户自主操作的AI软件，这代表了商业交互模式的根本转变。

Method: 通过分析现有AI应用（如搜索、分类、推荐商品）的发展趋势，提出代理商务的概念框架，探讨AI在商业决策和支付环节的自动化潜力。

Result: 识别出代理商务作为数字支付发展的潜在方向，强调其虽然低调但具有变革性的特点，能够实现搜索、决策和支付的端到端自动化。

Conclusion: 代理商务可能开启数字支付的新时代，其核心价值在于AI能够代表用户在设定限制内自主完成商业交易，这比单纯的技术基础设施升级更具颠覆性。

Abstract: Agentic Commerce: Excitement, Caution, and Confidence (8 minute read) The next era of digital payments might not begin with instant payments, stablecoins, or a brand-new rail. It might start with something far quieter but equally transformative: software that can make payments on our behalf. As of 2025, we have already seen applications that can search, catalog, and suggest items for us to shop. Agentic Commerce takes this to the next step, where AI can search, decide, and pay within limits w...

</details>


### [22] [Agentic AI Takes Over Patent Search](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pymnts.com%2Fartificial-intelligence-2%2F2025%2Fagentic-ai-takes-over-patent-search%2F%3Futm_source=tldrfintech/1/0100019a4a0aaae9-081a40d6-9964-42e7-9fc5-e2b3deeba75e-000000/5ZxNQGmxPiAp0-DECPDuH8pBzsyliug6tIx5xWmGoTA=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 代理AI正在改变专利搜索方式，从简单的关键词查找转向自主的、目标驱动的研究，模拟人类推理过程。


<details>
  <summary>Details</summary>
Motivation: 传统专利搜索方法存在局限性，需要更智能、更自主的AI系统来改进专利发现和起草过程。

Method: 采用代理AI方法，通过自主、目标驱动的研究来模拟人类推理过程进行专利搜索。

Result: 代理AI正在重新定义专利的发现和起草方式，实现更高效的专利搜索。

Conclusion: 代理AI代表了专利搜索领域的重大进步，能够更好地模拟人类推理过程。

Abstract: Agentic AI Takes Over Patent Search (6 minute read) Agentic AI is redefining how patents are discovered and drafted, shifting from simple keyword lookups to autonomous, goal-driven research that mimics human reasoning.

</details>


### [23] [Introducing Aardvark: OpenAI's agentic security researcher](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FQS8qrT/1/0100019a4a0c463e-c0c2ae0a-c2c9-4ae4-8d96-02d7f84cc8bf-000000/Oa9_Fl984EWX9UFlsjzMVmOyLyQjL1MAJUDqnZRDcg0=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI推出了名为Aardvark的自主安全研究代理，目前处于私有测试阶段，旨在帮助开发者和安全团队发现和修复漏洞。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动扫描代码库、评估风险并生成针对性补丁的AI安全研究代理，以提高软件安全性和漏洞修复效率。

Method: 使用AI驱动的推理技术，持续扫描代码仓库，对风险进行优先级排序，并提出针对性的修复补丁。

Result: Aardvark目前处于私有测试阶段，作为OpenAI的自主安全研究代理已开始运作。

Conclusion: Aardvark代表了AI在安全研究领域的应用进展，有望显著提升漏洞发现和修复的自动化水平。

Abstract: Introducing Aardvark: OpenAI's agentic security researcher (4 minute read) Aardvark is OpenAI's new autonomous security research agent. Now in private beta, it is designed to help developers and security teams find and fix vulnerabilities. It continuously scans code repositories, prioritizes risks, and proposes targeted patches using AI-powered reasoning.

</details>


### [24] [Tongyi DeepResearch: A New Era of Open-Source AI Researchers](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftongyi-agent.github.io%2Fblog%2Fintroducing-tongyi-deep-research%2F%3Futm_source=tldrai/1/0100019a4a311b84-715a6736-f2d9-41ae-a471-4602927bd195-000000/Q745ANrq2TjDx-k8MIkdJFwGPkwFcgeSCElHrdvh7B4=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Tongyi DeepResearch是一个完全开源的Web代理，在综合基准测试中性能与OpenAI的DeepResearch相当，专门用于长期深度信息搜索任务。


<details>
  <summary>Details</summary>
Motivation: 开发一个性能与OpenAI DeepResearch相当但完全开源的AI研究代理，为长期深度信息搜索任务提供高质量解决方案。

Method: 采用完整的经过实战验证的方法论来创建高级代理，具体技术细节未在摘要中详细说明。

Result: 在综合基准测试套件中实现了与OpenAI DeepResearch相当的性能表现。

Conclusion: Tongyi DeepResearch代表了开源AI研究代理的新时代，为创建类似高级代理提供了可行的方法论。

Abstract: Tongyi DeepResearch: A New Era of Open-Source AI Researchers (12 minute read) Tongyi DeepResearch is a full open-source Web Agent that achieves performance on par with OpenAI's DeepResearch across a comprehensive suite of benchmarks. It is specifically designed for long-horizon, deep information-seeking tasks. This post shares a complete and battle-tested methodology for creating advanced agents like Tongyi DeepResearch.

</details>


### [25] [The Browser Becomes an Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faspiringforintelligence.substack.com%2Fp%2Fthe-browser-becomes-an-agent%3Futm_source=tldrproduct/1/0100019a4e8cb125-e2da7e3a-ce6f-4b5b-a0b6-349687071d3d-000000/_n1vVxC6vrCNuYG_1JbmvMZXJInRoeQtp1oyi2ngXYo=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 浏览器正在从被动工具演变为能够理解、推理并为用户行动的智能代理平台，将浏览转变为主动协作，重新定义人类与AI在线的互动方式。


<details>
  <summary>Details</summary>
Motivation: 探索浏览器从被动工具向智能代理平台的转变，研究这种转变如何改变用户与网络的交互方式。

Method: 分析浏览器技术的演进趋势，研究智能代理功能在浏览器中的集成方式。

Result: 识别出浏览器正在成为能够主动协助用户的智能代理平台，这种转变将重新定义在线交互模式。

Conclusion: 浏览器向智能代理平台的演进代表了人机交互的重要范式转变，将浏览从被动活动转变为主动协作过程。

Abstract: The Browser Becomes an Agent (5 minute read) Browsers are evolving from passive tools into intelligent agent platforms that understand, reason, and act for users. This shift transforms browsing into proactive collaboration, redefining how humans and AI interact online.

</details>


### [26] [Web Codegen Scorer](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fangular%2Fweb-codegen-scorer%3Futm_source=tldrwebdev/1/0100019a4ec50d6d-8e9e4570-c7d9-4dba-8f04-303962b06824-000000/fxwNg3aXAeV9kn7-10AKwhWw0127qvHpt6XAdDOl7Ec=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Web Codegen Scorer是Google Angular团队开发的工具，用于评估LLM生成的web代码质量，支持跨模型、框架和工具的质量比较


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码生成领域的应用增多，需要标准化工具来评估和比较不同模型生成的web代码质量

Method: 开发专门的评分工具，利用既定的代码质量度量标准来评估LLM生成的web代码

Result: 创建了一个能够系统评估和比较不同LLM生成web代码质量的工具

Conclusion: Web Codegen Scorer为评估LLM生成的web代码提供了标准化方法，有助于模型比较和质量改进

Abstract: Web Codegen Scorer (GitHub Repo) The Web Codegen Scorer is a tool developed by the Angular team at Google to evaluate the quality of web code generated by LLMs. It allows users to compare code quality across different models, frameworks, and tools using established code quality measures.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [27] [【先进制造研究院】全球首个具身智能<em class="highlight">强化学习</em>技术正式应用 | 新闻资讯](http://mp.weixin.qq.com/s?__biz=MzkzNDY2NjI2MQ==&mid=2247495713&idx=7&sn=65174126d6ddd51d7ede14111a616e5f&chksm=c34b89f3951d79cc8761ed1f1e1c04051f2259622c19f0e5198067991e4966df32edd76cdefb#rd)
*先进制造研究院*

Main category: wechat.article

TL;DR: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。

</details>


### [28] [人大×百度NIPS 2025新成果：用多智能体<em class="highlight">强化学习</em>破解 RAG 模块 “各自为战”困局](http://mp.weixin.qq.com/s?__biz=Mzk0NzY0NzQzOA==&mid=2247489267&idx=1&sn=9da9707f6135e89d605e394fd0660d51&chksm=c2f629cc6f105d5f858aea97e95661b2df0c8342c4ffa06b5da85e6dc697507632d519508f10#rd)
*小白学AI算法*

Main category: wechat.article

TL;DR: 即便用强化学习（RL）优化，现有方法也只聚焦“检索器+生成器”两模块，无法应对复杂RAG的多模块协作需求。现有方案的“能力边界”目前RAG优化主要分为三类，但都存在局限：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 即便用强化学习（RL）优化，现有方法也只聚焦“检索器+生成器”两模块，无法应对复杂RAG的多模块协作需求。现有方案的“能力边界”目前RAG优化主要分为三类，但都存在局限：

</details>


### [29] [<em class="highlight">强化学习</em>又登Nature！结合知识图谱，训练效率提35%！](http://mp.weixin.qq.com/s?__biz=MzkyMTcxMjA0NA==&mid=2247491672&idx=1&sn=9bfe00baea2bde865e2109a8d7e1983f&chksm=c0f5dc4c68651d59003f3b6f2f6738e5b3342bc2f973718f31b5767bc7fec5f9a88822441a79#rd)
*沃的顶会*

Main category: wechat.article

TL;DR: 首次将强化学习应用于知识图谱构建过程本身，实现端到端的任务导向优化。提出两种任务感知的奖励函数，分别衡量图作为知识载体和知识索引的有效性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 首次将强化学习应用于知识图谱构建过程本身，实现端到端的任务导向优化。提出两种任务感知的奖励函数，分别衡量图作为知识载体和知识索引的有效性。

</details>


### [30] [全球首个具身智能<em class="highlight">强化学习</em>技术正式应用](http://mp.weixin.qq.com/s?__biz=Mzg3MjEwOTM1OQ==&mid=2247597963&idx=1&sn=4672c9d7eb549e45fe2e65d2dce4f49c&chksm=cf6b6d80dea2e49ded00761de2390675928bacca57c4f7e5330eace7a643259c92c1da341f80#rd)
*数智城乡研究*

Main category: wechat.article

TL;DR: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。

</details>


### [31] [版贸佳作 | <em class="highlight">强化学习</em>与随机优化：序贯决策的通用框架](http://mp.weixin.qq.com/s?__biz=Mzg4NTQ0NjU2OQ==&mid=2247488692&idx=1&sn=6bce162b4a67ce01da348d7877d3b946&chksm=ce153da3c7cae3a60841f29b2aa0aefaf13f9f711524c8472c832dec40111d8e60691939c9ca#rd)
*TUP Rights*

Main category: wechat.article

TL;DR: 强化学习与 随机优化： 序贯决策的通用框架。著译 郭 涛《强化学习与随机优化：序贯决策的通用框架》作者：[美] 沃伦·B.鲍威尔（Warren B. Powell）译者：郭涛


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习与 随机优化： 序贯决策的通用框架。著译 郭 涛《强化学习与随机优化：序贯决策的通用框架》作者：[美] 沃伦·B.鲍威尔（Warren B. Powell）译者：郭涛

</details>


### [32] [【本刊原创】基于深度<em class="highlight">强化学习</em>的智能物联边缘计算资源动态分配方法](http://mp.weixin.qq.com/s?__biz=MzU4MTY5MDQ4Nw==&mid=2247494722&idx=1&sn=d7388a66e3de41c4f7198fb8c61c7a5b&chksm=fc7bbbc56f8766c57fadec297b3d0220d5b587f9c3bdfe1afc979ea0d1ab2e48ab65b0a4a3bb#rd)
*智能物联技术*

Main category: wechat.article

TL;DR: 关键词：深度强化学习：智能物联：边缘计算资源：资源分配：动态分配。Abstract： When dynamically allocating edge computing resources， if the priority of edge nodes cannot be determined，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 关键词：深度强化学习：智能物联：边缘计算资源：资源分配：动态分配。Abstract： When dynamically allocating edge computing resources， if the priority of edge nodes cannot be determined，

</details>


### [33] [智元推出真机<em class="highlight">强化学习</em>，机器人训练周期从“数周”减至“数十分钟”；三星加速导入1c DRAM 设备](http://mp.weixin.qq.com/s?__biz=MzU3MTc1NTE1Mw==&mid=2247530238&idx=1&sn=b72cd5025df89a9abcb0103414fb181a&chksm=fd286cf4fd0197b34c8935a101906ed490dd5d936e48ce2ee1c13b81b8acc8c5b3c1baa6bc00#rd)
*核芯产业观察*

Main category: wechat.article

TL;DR: 智元推出真机强化学习，机器人训练周期从“数周”减至“数十分钟” 近日，智元机器人宣布其研发的真机强化学习技术，已在与龙旗科技合作的验证产线中成功落地。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 智元推出真机强化学习，机器人训练周期从“数周”减至“数十分钟” 近日，智元机器人宣布其研发的真机强化学习技术，已在与龙旗科技合作的验证产线中成功落地。

</details>


### [34] [<em class="highlight">强化学习</em>迎来爆发年！多项突破性研究登顶Nature](http://mp.weixin.qq.com/s?__biz=MzYyMTY5NTc0Mw==&mid=2247483801&idx=3&sn=db666d24dacdb461316c8fd25b95d148&chksm=febce25214c99300319fc111d4ec9d833f5a400cf3dcc0005b7acf09145d237836cc6dabe98e#rd)
*科研前沿社*

Main category: wechat.article

TL;DR: 强化学习的突破不仅发生在“大而全”的领域，也体现在“小而精”的系统中。斯坦福大学等机构的研究人员提出的AgentFlow框架，采用模块化架构和在线强化学习，使小规模模型在多项任务中超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习的突破不仅发生在“大而全”的领域，也体现在“小而精”的系统中。斯坦福大学等机构的研究人员提出的AgentFlow框架，采用模块化架构和在线强化学习，使小规模模型在多项任务中超越GPT-4o。

</details>


### [35] [<em class="highlight">强化学习</em>的 “指挥棒”：奖励为何是智能决策的核心？](http://mp.weixin.qq.com/s?__biz=MzIyMjc4ODQzMQ==&mid=2247484806&idx=1&sn=0457ba8e912037acaf49c70df9f56459&chksm=e963d20b96416ade01f546090474d69470f28ced0e6d13c1c20dbf524a54774577e79dd195f1#rd)
*强化学习*

Main category: wechat.article

TL;DR: 奖励：强化学习的 “目标语言”要理解奖励的核心作用，首先要明确它的本质 —— 奖励是环境对智能体行为的即时反馈，也是智能体定义 “目标” 的唯一语言。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 奖励：强化学习的 “目标语言”要理解奖励的核心作用，首先要明确它的本质 —— 奖励是环境对智能体行为的即时反馈，也是智能体定义 “目标” 的唯一语言。

</details>


### [36] [越来越感觉「大模型+<em class="highlight">强化学习</em>」是未来！ ICLR/ICML/AAAI 最新论文整理，这波组合太能打了！](http://mp.weixin.qq.com/s?__biz=MzE5MTYxNjkwMQ==&mid=2247485707&idx=2&sn=6692b49d0cc3aa6c39226c72275c3c59&chksm=97a4b194133e16742703506dbb5890ac7b4135266c9f8f988da1b9f931928b4be3096edc02f4#rd)
*LLM炼丹炉*

Main category: wechat.article

TL;DR: ICLR /ICML/AAAI 2025「大模型+强化学习」前沿论文76篇整理为帮助研究者快速把握ICLR 2025「大模型+强化学习」领域的核心进展，我梳理了76篇 ICLR /ICML/AAAI 2025｜「大模型+强化学习」相关的前沿论文，并提炼了每篇论文的核心关键词与


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ICLR /ICML/AAAI 2025「大模型+强化学习」前沿论文76篇整理为帮助研究者快速把握ICLR 2025「大模型+强化学习」领域的核心进展，我梳理了76篇 ICLR /ICML/AAAI 2025｜「大模型+强化学习」相关的前沿论文，并提炼了每篇论文的核心关键词与

</details>


### [37] [北航楚中毅&清华孙富春团队 | 基于虚实结合<em class="highlight">强化学习</em>的自生长机器人运动规划(MOOP)](http://mp.weixin.qq.com/s?__biz=MzA3MzQ5MzQyNA==&mid=2656854881&idx=3&sn=1640927c99ac593c45c6833cccfb1df0&chksm=85889c3b22b63146c6cbbbbc88e3a9ff0ced8b4e27cf9731c394ff1b9eededd0be8c6a4b230f#rd)
*中国科学杂志社*

Main category: wechat.article

TL;DR: 图3 （i） 双层DDPG强化学习网络构架 （ii） 预弯曲动作选择网络状态向量 （iii） 感知与误差纠正网络状态向量03 scis实验结果实验结果表明，该算法有效降低了自生长机器人的路径跟随误差，成功率可达95%以上（图4）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 图3 （i） 双层DDPG强化学习网络构架 （ii） 预弯曲动作选择网络状态向量 （iii） 感知与误差纠正网络状态向量03 scis实验结果实验结果表明，该算法有效降低了自生长机器人的路径跟随误差，成功率可达95%以上（图4）。

</details>


### [38] [[计算机与通信工程]基于语义选择的迁移<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzA4ODA3OTc3MA==&mid=2453923816&idx=1&sn=1808788e7badf624933427a97b2f013b&chksm=86b78a02de43a2b9c3c03e30f578a74e4e5c286c5cc621168077e34c0d74917eeb929525b579#rd)
*江苏大学学报自然科学版*

Main category: wechat.article

TL;DR: 强化学习是一种通过智能体与环境进行交互学习从而解决顺序决策任务的有效框架[1-3]，而深度学习与强化学习的结合则很好地解决了高维状态和动作空间的搜索问题.许多强化学习（reinforcement learning，RL）算法展示了超人的性能


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习是一种通过智能体与环境进行交互学习从而解决顺序决策任务的有效框架[1-3]，而深度学习与强化学习的结合则很好地解决了高维状态和动作空间的搜索问题.许多强化学习（reinforcement learning，RL）算法展示了超人的性能

</details>


### [39] [推荐阅读 | <em class="highlight">强化学习</em> + 约束规划求解组合优化问题](http://mp.weixin.qq.com/s?__biz=Mzg2MjA5ODI2Ng==&mid=2247496860&idx=1&sn=1d44bc577e4fc9f39ab0e4034b973ca2&chksm=cfa7d775c498f533a450ea42969d4bb7ffe42a0cdf39537b1e8028e1a3438796fe855e893bd7#rd)
*学术人人*

Main category: wechat.article

TL;DR: 强化学习的主要目的是研究并解决智能体的序贯决策问题，可以用四元组定义强化学习的行为。其中，为状态集合，为动作集合，为转移函数，为奖励函数。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习的主要目的是研究并解决智能体的序贯决策问题，可以用四元组定义强化学习的行为。其中，为状态集合，为动作集合，为转移函数，为奖励函数。

</details>


### [40] [全球首个具身智能<em class="highlight">强化学习</em>技术正式应用](http://mp.weixin.qq.com/s?__biz=Mzg5NjcxODA4OA==&mid=2247503651&idx=1&sn=e768cfcc8e26c86bbdefda977917aedc&chksm=c121fdf6fe8180a37af073670e5c18e45d66e6b1e91eac14dcc70772b25b1d494ec9b4124a96#rd)
*国家智能制造专家委员会*

Main category: wechat.article

TL;DR: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 全球首个具身智能强化学习技术正式应用 在上海的一家智能设备产线里，这批新来的机器人员工就搭载了全球首个真机强化学习技术。有了这项技术，机器人的训练周期得到了大幅缩减。

</details>


### [41] [RL精选中文书籍-《轻松RL<em class="highlight">强化学习</em>中文教程》免费分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247572001&idx=2&sn=99ae3f87983729c107771f6282e179e1&chksm=96f1ef947c97d31f6099f038811ab414cea406442c285ef06ebe6bdd69270cd4b2774aaaa7cb#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 目录 第1章强化学习概述。1.1 强化学习。...... ....... ....... ....... .....。1.1.1 强化学习与监督学习。...... ....... ....... ....... ....... .。1。1.1.2 强化学习的由来与分类...... ....... ....... ....... .......。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 目录 第1章强化学习概述。1.1 强化学习。...... ....... ....... ....... .....。1.1.1 强化学习与监督学习。...... ....... ....... ....... ....... .。1。1.1.2 强化学习的由来与分类...... ....... ....... ....... .......。

</details>


### [42] [当 π0 / π0.5 学会<em class="highlight">强化学习</em>：πRL 让 VLA 从模仿人类走向自我进化](http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247490248&idx=1&sn=0198f4a0af75846d6cfc1b21c0a98fca&chksm=c1642e41eb6b4b0df1c28c0e726e770bf105da999ddb5b0cbf1e231a21ecb7225745d979d051#rd)
*具身智能研究室*

Main category: wechat.article

TL;DR: 让模型学会基本的任务执行方式。Ⅲ. 在线强化学习阶段模型与环境交互，通过奖励信号进行自我优化；动作空间来自 flow 模型的连续去噪轨迹；PPO 或 GRPO 用于更新策略。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 让模型学会基本的任务执行方式。Ⅲ. 在线强化学习阶段模型与环境交互，通过奖励信号进行自我优化；动作空间来自 flow 模型的连续去噪轨迹；PPO 或 GRPO 用于更新策略。

</details>


### [43] [论文研读与思考|从新手到专家：通过逐步<em class="highlight">强化学习</em>优化LLM代理策略](http://mp.weixin.qq.com/s?__biz=MzU4NjcxMTY3Mg==&mid=2247488025&idx=2&sn=f7133be9197ff93e3dac9c779981da3f&chksm=fccacc171beb9bfb1bbf9edd9d50690b2a4fee947d5a25d58d1011909d0875d81f7fdf856f93#rd)
*玄枢战队-Arcane Hub*

Main category: wechat.article

TL;DR: 逆强化学习：针对现有数据集中缺乏每一步推理的奖励信号的问题，文章引入逆强化学习（IRL）方法[1，14，23，31]。该方法首先基于专家和智能体的行为推断逐步奖励函数，随后利用该奖励函数对智能体策略进行细粒度优化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 逆强化学习：针对现有数据集中缺乏每一步推理的奖励信号的问题，文章引入逆强化学习（IRL）方法[1，14，23，31]。该方法首先基于专家和智能体的行为推断逐步奖励函数，随后利用该奖励函数对智能体策略进行细粒度优化。

</details>


### [44] [吴宝康大讲堂·行业坐标 | 侯宇清：<em class="highlight">强化学习</em>在大模型时代的典型应用](http://mp.weixin.qq.com/s?__biz=MzIwNzE0OTEwMg==&mid=2651962988&idx=3&sn=52f54c654ad49408d320266d71280185&chksm=8d6ef56553beb4fabe9632f82c2efc2dec48d626126cdf7f7c5cee1576c9edfebdeb479498a7#rd)
*RUC信息资源管理学院*

Main category: wechat.article

TL;DR: 强化学习在大 领域的 学习在大模型领域的 典型 典型应用。侯宇清 图1 周文杰教授主持讲座 侯宇清老师从“预训练+微调”的方法演进切入，系统梳理了指令学习（prompt learning）、少样本学习（few-shot/zero-shot）、监督微调（SFT）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习在大 领域的 学习在大模型领域的 典型 典型应用。侯宇清 图1 周文杰教授主持讲座 侯宇清老师从“预训练+微调”的方法演进切入，系统梳理了指令学习（prompt learning）、少样本学习（few-shot/zero-shot）、监督微调（SFT）

</details>


### [45] [过难样本如何学习？谷歌提出SRL：监督<em class="highlight">强化学习</em>，小模型也能解决难任务](http://mp.weixin.qq.com/s?__biz=MzkyMjY3MjY1MQ==&mid=2247484621&idx=1&sn=85fd39dc5f838869342014f042e5ec6c&chksm=c0b27625c5134b11a6af7b32f75d3160ad725a663ebf2df4c79f4644438485d9aafd73239e5a#rd)
*oneLLM*

Main category: wechat.article

TL;DR: 提出的监督强化学习（SRL） 通过“步骤化动作建模+密集相似度奖励”，解决了的学习问题，使小模型（如7B规模）能学会SFT/RLVR无法处理的任务。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 提出的监督强化学习（SRL） 通过“步骤化动作建模+密集相似度奖励”，解决了的学习问题，使小模型（如7B规模）能学会SFT/RLVR无法处理的任务。

</details>


### [46] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（11/5）part6](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483823&idx=1&sn=bd80b05dfc5d24be0fea682611c75c1c&chksm=e94c021a9520b5dafaed4a705af3998b9bcf170b7817f389fae91b1a3dd41156c4268fb0a04a#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 市场定位：公司强调 AgentLink 是低代码、最快速的接入方式，可让 GitHub、Notion 等 SaaS 应用像普通插件一样被代理访问，同时保留现有质量标准【295413570873432L95-L102】【295413570873432L111-L118】。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 市场定位：公司强调 AgentLink 是低代码、最快速的接入方式，可让 GitHub、Notion 等 SaaS 应用像普通插件一样被代理访问，同时保留现有质量标准【295413570873432L95-L102】【295413570873432L111-L118】。

</details>


### [47] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（11/5）part5](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483818&idx=1&sn=52e92f8665c34af2a4cd697c5cde942f&chksm=e92351a90692834e58e0d2eb470e122ed82463005ae5dec8eb2f5fc48a53852a994cf4ac1f90#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 实战指南：并行代理写代码开源LLM 11 月榜：Qwen 领跑1｜【Startup】— Incredibuild 收购 Kypso 加速AI原生流水线（ET：2025-11-03 00：00；窗口：Yesterday）来源：Incredibuild 新闻室（ET 20251103）｜链接：https：//www.incredibuild.com/news/incredibuild-ac


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 实战指南：并行代理写代码开源LLM 11 月榜：Qwen 领跑1｜【Startup】— Incredibuild 收购 Kypso 加速AI原生流水线（ET：2025-11-03 00：00；窗口：Yesterday）来源：Incredibuild 新闻室（ET 20251103）｜链接：https：//www.incredibuild.com/news/incredibuild-ac

</details>


### [48] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（11/5）part4](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483813&idx=1&sn=d718818124c3e4c282fd759f24b1d39f&chksm=e9b0259c229a025e7d5e1d81df4f1d48f812903d820c7e06aa36af4f04d194dcf541d566c1e6#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 1｜【类型：Code Agent 研究】— Team Atlanta 使用 RL 学习检索，自动修复安全漏洞（ET：20251102 12：00；窗口：Yesterday）来源：Team Atlanta 博客（20251102）｜链接：https：//team-atlanta.github.io/blog/post-custom-model/｜源头优先


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1｜【类型：Code Agent 研究】— Team Atlanta 使用 RL 学习检索，自动修复安全漏洞（ET：20251102 12：00；窗口：Yesterday）来源：Team Atlanta 博客（20251102）｜链接：https：//team-atlanta.github.io/blog/post-custom-model/｜源头优先

</details>


### [49] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（11/5）part2](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483803&idx=1&sn=72581995cee38ba734484775ee66dccf&chksm=e9f5e7c433a5c005a9d825a668fa0330bb9e7de3329943e72e8a321e443981f79faae0081db8#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 1｜【类型：Code Agent 研究】— DeepAgent 统一推理框架：动态检索 + 内存折叠 + RL（ET：20251101 未标时；窗口：昨日）来源：MarkTechPost（Asif Razzaq，1101）｜链接：https：//www.marktechpost.com/2025/11/01/deepagent-a-deep-reasoning-ai-agent-that-perf...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1｜【类型：Code Agent 研究】— DeepAgent 统一推理框架：动态检索 + 内存折叠 + RL（ET：20251101 未标时；窗口：昨日）来源：MarkTechPost（Asif Razzaq，1101）｜链接：https：//www.marktechpost.com/2025/11/01/deepagent-a-deep-reasoning-ai-agent-that-performs-a

</details>


### [50] [从“听话”到“思考”：<em class="highlight">Agentic</em> AI与普通Agent的本质区别，你必须搞懂](http://mp.weixin.qq.com/s?__biz=MzA3OTkzNzgwMQ==&mid=2247483660&idx=1&sn=5dfbab3700f1475d4c2e9c25db1a7bf0&chksm=9e44848de313ecc496a440ffd50a2e0d8ffd4a383e4f7c1234409f28dfd95eb0a905aa7ae612#rd)
*事儿君的人生实验室*

Main category: wechat.article

TL;DR: “Agentic”这个词，强调的是 “具备自主代理能力”。如果说Agent AI是一个听从指令的员工，那么Agentic AI就是一个能自己做规划、协调资源、复盘纠错、并坚持完成复杂项目的“项目经理”或“创业者”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: “Agentic”这个词，强调的是 “具备自主代理能力”。如果说Agent AI是一个听从指令的员工，那么Agentic AI就是一个能自己做规划、协调资源、复盘纠错、并坚持完成复杂项目的“项目经理”或“创业者”。

</details>


### [51] [利用强大且安全的数据实践提升<em class="highlight">Agentic</em> AI | SCIRP论文推荐](http://mp.weixin.qq.com/s?__biz=Mzk0NTI0MzA5NQ==&mid=2247531167&idx=1&sn=8a3fdeb60ca38265942ae95952617858&chksm=c2e9cb2090f9a4fc909576cc067e692c76d9668049c91c197d785ea8dd7e98e2152e0fcd813f#rd)
*SCIRP学术交流*

Main category: wechat.article

TL;DR: 具体说来，研究人员提出了运用自主数据完整性层（ADIL）以提升Agentic AI的独立性，这一灵活的架构融合了安全工程和数据科学的最佳实践，可以确保Agentic AI系统使用干净、经过验证且与上下文相关的数据运行，有助于提高Agenti


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 具体说来，研究人员提出了运用自主数据完整性层（ADIL）以提升Agentic AI的独立性，这一灵活的架构融合了安全工程和数据科学的最佳实践，可以确保Agentic AI系统使用干净、经过验证且与上下文相关的数据运行，有助于提高Agenti

</details>


### [52] [从0到1构建<em class="highlight">智能体</em>应用：核心方法论与实践路径](http://mp.weixin.qq.com/s?__biz=MzI4NTE5NzUxNQ==&mid=2247484032&idx=1&sn=24b8243168aa9e7057acbf8f2c43ba43&chksm=ea9bb0c1b6d03f9e7364780e1b63f97c4367ee762fa6ae060bb8f61836a07cb9b174032f0b88#rd)
*OpenModels*

Main category: wechat.article

TL;DR: 7. ai evals al interface analyze measure improve common agentic frameworksNo-code Framework LLM MCP Tools （Functions everywhere） Agents OrchestrationOpenAl Agents API OpenAl Remote Predefined （web，fil...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 7. ai evals al interface analyze measure improve common agentic frameworksNo-code Framework LLM MCP Tools （Functions everywhere） Agents OrchestrationOpenAl Agents API OpenAl Remote Predefined （web，file/code） Threads

</details>


### [53] [从被动到主动：吴恩达揭开 <em class="highlight">Agentic</em> AI 的真正革命](http://mp.weixin.qq.com/s?__biz=Mzg4MTE3MDAxOQ==&mid=2247500138&idx=3&sn=a7eeef8263244b29ee1fbfc98e169d13&chksm=ceed7d0707b24dfd9feb85235a49965b794473e4988ac7ba5691bd43f3a8fa9cf61d6f73a822#rd)
*灵度智能*

Main category: wechat.article

TL;DR: 设计一个 Agentic 系统时，最关键的一步是任务拆解。吴恩达把这一过程讲得很工程化，却又很哲学。一个任务往往可以拆成两类部分：模型与工具。模型负责语言理解、生成、信息提取等；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 设计一个 Agentic 系统时，最关键的一步是任务拆解。吴恩达把这一过程讲得很工程化，却又很哲学。一个任务往往可以拆成两类部分：模型与工具。模型负责语言理解、生成、信息提取等；

</details>


### [54] [香港中文大学李济舟团队Photon Science | <em class="highlight">代理</em>式人工智能(<em class="highlight">Agentic</em> AI)加速同步辐射材料表征](http://mp.weixin.qq.com/s?__biz=Mzk3NTk0NjM1NA==&mid=2247483718&idx=1&sn=d52b125ee951a5c1b5f297d2e509f20b&chksm=c52a43f478c89a689ad1b7ad55cefb5faf4026b97191b5166c7af40e19f1c8dabeb394ef95ed#rd)
*Photon Science 光子科学*

Main category: wechat.article

TL;DR: 在此基础上，论文进一步提出了一个面向真实科研场景的端到端科研智能体链（Agentic Research Chain），以探索锂电池正极材料在循环退化过程中的成分演化和结构变化（见图3）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在此基础上，论文进一步提出了一个面向真实科研场景的端到端科研智能体链（Agentic Research Chain），以探索锂电池正极材料在循环退化过程中的成分演化和结构变化（见图3）。

</details>


### [55] [万字长文，一篇讲透<em class="highlight">Agentic</em> RAG：基于顶会论文的系统化梳理与解析](http://mp.weixin.qq.com/s?__biz=MzI3NjUwNDg4Mg==&mid=2247484986&idx=1&sn=9f0381dd2996b9ee1330ba361346ba38&chksm=ea0255b52e35ddc49860247f71526bd368d1583e06ce480c102558f7336533e248a65ab29392#rd)
*Tommy学习录*

Main category: wechat.article

TL;DR: 论文名称：AGENTIC RETRIEVAL-AUGMENTED GENERATION： A SURVEY ON AGENTIC RAG论文地址：https：//arxiv.org/pdf/2501.09136Github地址：https：//github.com/asinghcsu/AgenticRAG-Surveyagentic retrieval-augmented generation： ...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 论文名称：AGENTIC RETRIEVAL-AUGMENTED GENERATION： A SURVEY ON AGENTIC RAG论文地址：https：//arxiv.org/pdf/2501.09136Github地址：https：//github.com/asinghcsu/AgenticRAG-Surveyagentic retrieval-augmented generation： a survey on agentic rag aditi singh abul ehtesham saket kumar depa

</details>


### [56] [美团“全能猫”横空出世！LongCat-Flash-Omni多模态<em class="highlight">大模型</em>开源即登顶，交互速度震撼业内](http://mp.weixin.qq.com/s?__biz=MzAxMzAyNzc3OA==&mid=2448047077&idx=3&sn=a90a254fa03171f9a0913b34884c7912&chksm=8eaec851987518f32fb8c03a2b9b844f2694e48cbaa1b8155e676d3d8e4ee3cd62162668e371#rd)
*想进化的猿*

Main category: wechat.article

TL;DR: 这款多模态大模型的开源发布，立即引起了业内的广泛关注。凭借其在多个基准测试中的卓越表现，LongCat-Flash-Omni不仅打破了多个闭源竞品的优势，且凭借“开源即SOTA”（State-of-the-Art）成功登顶，标志着美团在多模态AI领域的


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这款多模态大模型的开源发布，立即引起了业内的广泛关注。凭借其在多个基准测试中的卓越表现，LongCat-Flash-Omni不仅打破了多个闭源竞品的优势，且凭借“开源即SOTA”（State-of-the-Art）成功登顶，标志着美团在多模态AI领域的

</details>


### [57] [全球首个AI投资大赛收官：阿里千问夺冠，美国四<em class="highlight">大模型</em>均亏损](http://mp.weixin.qq.com/s?__biz=MzA3ODMxNTIxMA==&mid=2650672798&idx=4&sn=c181b40ef792b915979f0688bb6af423&chksm=860a695bdcd1929ef3838411d63469df006250b9cf3c7fa13ffbf5a09260b1268df69774a850#rd)
*太空与网络*

Main category: wechat.article

TL;DR: AI大模型在各种性能基准榜单中屡创新高，如何评估大模型在真实、动态、竞争激烈的环境中的决策水平，是当下AI竞技场最受人关注的领域。美国人工智能研究实验室nof1.ai发布的“Alpha Arena”（阿尔法竞技场），向六大顶尖模


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: AI大模型在各种性能基准榜单中屡创新高，如何评估大模型在真实、动态、竞争激烈的环境中的决策水平，是当下AI竞技场最受人关注的领域。美国人工智能研究实验室nof1.ai发布的“Alpha Arena”（阿尔法竞技场），向六大顶尖模

</details>


### [58] [2025年TOP 9 <em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&mid=2247543557&idx=1&sn=bba59d4337a84e568e795eea159cdf92&chksm=ea227003c5ea721c12715d25cf83de68af713b665aa693ea8213152412ebbdf6f7794f96b042#rd)
*算法进阶*

Main category: wechat.article

TL;DR: 引言 本文汇总了全球的主流大模型，并分析最新趋势和重要创新。以下，我们重点介绍了我们认为目前在行业内引起关注的 9 个 LLM，每个模型都具备独特的功能和专业优势，在自然语言处理、代码合成、小样本学习或可扩展性


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 引言 本文汇总了全球的主流大模型，并分析最新趋势和重要创新。以下，我们重点介绍了我们认为目前在行业内引起关注的 9 个 LLM，每个模型都具备独特的功能和专业优势，在自然语言处理、代码合成、小样本学习或可扩展性

</details>


### [59] [<em class="highlight">大模型</em>应用开发技术路线（下）：智能代理与多模态应用开发指南](http://mp.weixin.qq.com/s?__biz=MzI1MDc3ODM1MA==&mid=2247483794&idx=1&sn=455a45e522fd6945773dbb25c1f523f7&chksm=e8582e7850840fcbc0c91b5f7195a803c4e761289344fbb9bfd3d4375f7ee1795b89fcbfbd31#rd)
*六边形架构*

Main category: wechat.article

TL;DR: 在前两篇文章中，我们探讨了《大模型应用开发技术路线（上）：从概念到RAG实战，这套方法论让我从0到1落地企业级AI应用》和《大模型应用开发技术路线（中）：大模型微调与定制从概念到落地》。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在前两篇文章中，我们探讨了《大模型应用开发技术路线（上）：从概念到RAG实战，这套方法论让我从0到1落地企业级AI应用》和《大模型应用开发技术路线（中）：大模型微调与定制从概念到落地》。

</details>


### [60] [<em class="highlight">大模型</em>安全测评基准综述研究](http://mp.weixin.qq.com/s?__biz=MzUzMjk3MTU1MA==&mid=2247484551&idx=1&sn=ff0ea60c6aafcdc69db7abd265e92c28&chksm=fb31802cf039504e108759f9ee9b08796d55f5b0d55711bc50f893777d6c5311535cc0d8490b#rd)
*人工智能演进录*

Main category: wechat.article

TL;DR: 2020年，OpenAI发布ChatGPT模型，模型参数量达到1 750亿，正式开启大语言模型（Large Language Model，LLM）时代，LLM又称为大模型。在后续的技术发展和商业落地过程中，多模态模型、行业应用模型、终端模型百花齐放，大模型的含义已


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2020年，OpenAI发布ChatGPT模型，模型参数量达到1 750亿，正式开启大语言模型（Large Language Model，LLM）时代，LLM又称为大模型。在后续的技术发展和商业落地过程中，多模态模型、行业应用模型、终端模型百花齐放，大模型的含义已

</details>


### [61] [2025年中国<em class="highlight">大模型</em>行业发展研究报告：CBDG四维生态成为新范式，体系化竞争成为关键-36Kr-40页](http://mp.weixin.qq.com/s?__biz=MzUyNjY5MzMwMw==&mid=2247699388&idx=2&sn=f34077ff6ddf75fc54d85fc4c58377c6&chksm=fb240741585bbb77726ea66b4106a2db26c2edff42196ce368511a2d40de96f735166f2c88e9#rd)
*月説数智领地*

Main category: wechat.article

TL;DR: 大模型作为ai发展核心引擎，按应用 广度分通用与行业模型，按部署形态分云端与端，侧模型，按技术路径分闭源与开源模型，2024年中国大模型市场规模约294.16亿元，预计2026年突破700亿元，多模态 融合与智能体演进成竞争焦点


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型作为ai发展核心引擎，按应用 广度分通用与行业模型，按部署形态分云端与端，侧模型，按技术路径分闭源与开源模型，2024年中国大模型市场规模约294.16亿元，预计2026年突破700亿元，多模态 融合与智能体演进成竞争焦点

</details>


### [62] [硅谷<em class="highlight">大模型</em>的中国风 越刮越猛！](http://mp.weixin.qq.com/s?__biz=MzI2MzA4MzYzMQ==&mid=2650775437&idx=1&sn=3f36f70fd49fa2b85df266311795233a&chksm=f3a16601257c673c30b2a1a7d624137d139879412e63f44e0929d569f11821c07e1075800a44#rd)
*行者吴江*

Main category: wechat.article

TL;DR: 中国大模型的崛起颠覆了传统技术路径。Meta的Llama系列虽开源，但衍生模型需支付数千万美元授权费；而Qwen不仅开放源代码，更提供“模型即服务”的完整生态。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 中国大模型的崛起颠覆了传统技术路径。Meta的Llama系列虽开源，但衍生模型需支付数千万美元授权费；而Qwen不仅开放源代码，更提供“模型即服务”的完整生态。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [63] [Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs](https://arxiv.org/abs/2511.02197)
*Shufan Wang,Xing Hu,Junkai Chen,Zhiyuan Pan,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一个针对代码推理任务的LLM置信度分析与增强框架，通过实证研究评估主流LLM的置信度可靠性，并验证提示策略优化和数学校准技术的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代码智能领域的广泛应用，需要评估其输出在代码推理任务中的可靠性和可控性，置信度估计是评估这些方面的有效方法。

Method: 采用置信度分析与增强框架，包括对主流LLM进行全面的实证研究，评估提示策略优化和数学校准（如Platt Scaling）在提高置信度可靠性方面的有效性。

Result: DeepSeek-Reasoner在各种任务中表现最佳，在ECE、Brier Score和Performance Score指标上分别优于其他模型0.680、0.636和13.652。结合重新评估提示策略和Platt Scaling的混合策略在上述三个指标上比原始性能提升了0.541、0.628和15.084。

Conclusion: 具有推理能力的模型表现出更优的置信度可靠性，混合策略在增强各种模型的置信度可靠性方面最有效。当前LLM在复杂推理任务中的置信度仍有较大改进空间。

Abstract: With the widespread application of large language models (LLMs) in the field
of code intelligence, increasing attention has been paid to the reliability and
controllability of their outputs in code reasoning tasks. Confidence estimation
serves as an effective and convenient approach for evaluating these aspects.
This paper proposes a confidence analysis and enhancement framework for LLMs
tailored to code reasoning tasks. We conduct a comprehensive empirical study on
the confidence reliability of mainstream LLMs across different tasks, and
further evaluate the effectiveness of techniques such as prompt strategy
optimisation and mathematical calibration (e.g., Platt Scaling) in improving
confidence reliability. Our results show that DeepSeek-Reasoner achieves the
best performance across various tasks, outperforming other models by up to
$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance
Score, respectively. The hybrid strategy combining the reassess prompt strategy
and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$
over the original performance in the aforementioned three metrics. These
results indicate that models with reasoning capabilities demonstrate superior
confidence reliability, and that the hybrid strategy is the most effective in
enhancing the confidence reliability of various models. Meanwhile, we elucidate
the impact of different task complexities, model scales, and strategies on
confidence performance, and highlight that the confidence of current LLMs in
complex reasoning tasks still has considerable room for improvement. This study
not only provides a research foundation and technical reference for the
application of confidence in LLM-assisted software engineering, but also points
the way for future optimisation and engineering deployment of confidence
mechanisms.

</details>


### [64] [EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents](https://arxiv.org/abs/2511.02399)
*Junwei Liu,Chen Xu,Chong Wang,Tong Bai,Weitong Chen,Kaseng Wong,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 提出了EvoDev框架，这是一种受特性驱动开发启发的迭代软件开发方法，通过构建特性映射图来显式建模特性间依赖关系，在Android开发任务中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理方法主要采用线性瀑布式流水线，过度简化了真实世界开发的迭代性质，难以处理复杂的大规模项目。

Method: 将用户需求分解为一组用户价值特性，构建特性映射图（有向无环图）显式建模特性间依赖关系，每个节点维护业务逻辑、设计和代码等多层次信息，沿依赖关系传播上下文。

Result: 在Android开发任务评估中，EvoDev比最佳基线Claude Code性能提升56.8%，在不同基础LLM上单代理性能提升16.0%-76.6%。

Conclusion: 依赖建模、上下文传播和工作流感知的代理设计对复杂软件项目至关重要，总结了设计迭代式LLM驱动开发框架的实用见解。

Abstract: Recent advances in large language model agents offer the promise of
automating end-to-end software development from natural language requirements.
However, existing approaches largely adopt linear, waterfall-style pipelines,
which oversimplify the iterative nature of real-world development and struggle
with complex, large-scale projects. To address these limitations, we propose
EvoDev, an iterative software development framework inspired by feature-driven
development. EvoDev decomposes user requirements into a set of user-valued
features and constructs a Feature Map, a directed acyclic graph that explicitly
models dependencies between features. Each node in the feature map maintains
multi-level information, including business logic, design, and code, which is
propagated along dependencies to provide context for subsequent development
iterations. We evaluate EvoDev on challenging Android development tasks and
show that it outperforms the best-performing baseline, Claude Code, by a
substantial margin of 56.8%, while improving single-agent performance by
16.0%-76.6% across different base LLMs, highlighting the importance of
dependency modeling, context propagation, and workflow-aware agent design for
complex software projects. Our work summarizes practical insights for designing
iterative, LLM-driven development frameworks and informs future training of
base LLMs to better support iterative software development.

</details>


### [65] [Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering](https://arxiv.org/abs/2511.02475)
*Jürgen Cito,Dominik Bork*

Main category: cs.SE

TL;DR: 生成式AI使快速"氛围编码"成为可能，但导致原型与工程软件边界模糊，产生脆弱系统。需要重新构想软件模型，将其作为人类意图、AI生成和长期系统演化之间的中介。


<details>
  <summary>Details</summary>
Motivation: 生成式AI降低了软件开发门槛，但导致系统缺乏健壮性、安全性和可维护性，原型与工程软件边界模糊，需要新的软件模型来应对这些挑战。

Method: 提出将软件模型从前期蓝图转变为事后从AI生成代码中恢复，以恢复理解、暴露风险并指导改进。

Result: 模型作为人类意图、AI生成和长期系统演化之间的中介，为可持续的AI驱动软件工程提供路径。

Conclusion: 软件模型应重新定位为AI驱动软件开发中的中介角色，促进可持续的软件工程实践。

Abstract: Generative AI enables rapid ``vibe coding," where natural language prompts
yield working software systems. While this lowers barriers to software
creation, it also collapses the boundary between prototypes and engineered
software, leading to fragile systems that lack robustness, security, and
maintainability. We argue that this shift motivates a reimagining of software
models. Rather than serving only as upfront blueprints, models can be recovered
post-hoc from AI-generated code to restore comprehension, expose risks, and
guide refinement. In this role, models serve as mediators between human intent,
AI generation, and long-term system evolution, providing a path toward
sustainable AI-driven software engineering.

</details>


### [66] [ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation](https://arxiv.org/abs/2511.02713)
*Qianru Meng,Zhaochun Ren,Joost Visser*

Main category: cs.SE

TL;DR: ReleaseEval是一个用于自动化发布说明生成的基准测试，包含94,987个发布说明，支持三种任务设置：从提交消息生成、结合提交树结构生成、利用代码差异生成。


<details>
  <summary>Details</summary>
Motivation: 解决自动化发布说明生成中数据集限制（缺乏明确许可、可复现性差）和任务设计不完整（主要依赖提交消息，忽略细粒度上下文）的问题。

Method: 构建ReleaseEval基准测试，包含来自3,369个仓库的94,987个发布说明，支持三种不同输入粒度的任务设置：commit2sum、tree2sum和diff2sum。

Result: 大型语言模型在所有任务中都优于传统基线方法，在tree2sum任务中表现尤为突出，但在diff2sum任务中仍面临挑战。

Conclusion: LLMs擅长利用结构化信息，但在从长代码差异中抽象信息方面仍有困难。

Abstract: Automated release note generation addresses the challenge of documenting
frequent software updates, where manual efforts are time-consuming and prone to
human error. Although recent advances in language models further enhance this
process, progress remains hindered by dataset limitations, including the lack
of explicit licensing and limited reproducibility, and incomplete task design
that relies mainly on commit messages for summarization while overlooking
fine-grained contexts such as commit hierarchies and code changes. To fill this
gap, we introduce ReleaseEval, a reproducible and openly licensed benchmark
designed to systematically evaluate language models for automated release note
generation. ReleaseEval comprises 94,987 release notes from 3,369 repositories
across 6 programming languages, and supports three task settings with three
levels of input granularity: (1) commit2sum, which generates release notes from
commit messages; (2) tree2sum, which incorporates commit tree structures; and
(3) diff2sum, which leverages fine-grained code diffs. Both automated and human
evaluations show that large language models consistently outperform traditional
baselines across all tasks, achieving substantial gains on tree2sum, while
still struggling on diff2sum. These findings highlight LLMs' proficiency in
leveraging structured information while revealing challenges in abstracting
from long code diffs.

</details>


### [67] [From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu](https://arxiv.org/abs/2511.02827)
*Mohamed Almukhtar,Anwar Ghammam,Marouane Kessentini,Hua Ming*

Main category: cs.SE

TL;DR: 该论文提出了PyQu工具，通过分析3400多个开源Python机器学习项目的370万次提交，识别出61种能直接提升软件质量的代码变更，并将其分为13个类别，其中41%是首次发现的质量提升变更。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI代码生成和Python机器学习系统的普及，软件质量成为重要关切。当前缺乏工具来理解代码变更如何影响ML系统质量，现有研究只关注变更内容而忽略了变更与质量的关系。

Method: 对3340个开源Python ML项目进行大规模实证研究，涵盖370万次提交和2.7万亿行代码。开发PyQu工具利用低级软件指标识别质量提升提交，并进行主题分析。

Result: PyQu工具在识别质量提升提交方面达到平均准确率0.84、精确率0.85、召回率0.84和F1分数0.85。识别出61种直接影响软件质量的代码变更，分为13个类别，其中41%是现有Python变更检测工具未发现的新类型。

Conclusion: 该研究为研究人员、从业者、教育工作者和工具开发者提供了重要基础，推动了Python机器学习软件自动化质量评估和最佳实践的探索。

Abstract: In an era shaped by Generative Artificial Intelligence for code generation
and the rising adoption of Python-based Machine Learning systems (MLS),
software quality has emerged as a major concern. As these systems grow in
complexity and importance, a key obstacle lies in understanding exactly how
specific code changes affect overall quality-a shortfall aggravated by the lack
of quality assessment tools and a clear mapping between ML systems code changes
and their quality effects. Although prior work has explored code changes in
MLS, it mostly stops at what the changes are, leaving a gap in our knowledge of
the relationship between code changes and the MLS quality. To address this gap,
we conducted a large-scale empirical study of 3,340 open-source Python ML
projects, encompassing more than 3.7 million commits and 2.7 trillion lines of
code. We introduce PyQu, a novel tool that leverages low level software metrics
to identify quality-enhancing commits with an average accuracy, precision, and
recall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic
analysis, we identified 61 code changes, each demonstrating a direct impact on
enhancing software quality, and we classified them into 13 categories based on
contextual characteristics. 41% of the changes are newly discovered by our
study and have not been identified by state-of-the-art Python changes detection
tools. Our work offers a vital foundation for researchers, practitioners,
educators, and tool developers, advancing the quest for automated quality
assessment and best practices in Python-based ML software.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)
*Zijian Zhang,Rong Wang,Shiyang Li,Yuebo Luo,Mingyi Hong,Caiwen Ding*

Main category: cs.LG

TL;DR: CudaForge是一个无需训练的多智能体工作流，用于自动生成和优化CUDA内核，通过Coder和Judge两个LLM智能体的迭代协作，结合硬件反馈，实现了97.6%的正确率和平均1.68倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 手动设计CUDA内核成本高、耗时，现有自动方法生成的内核效率低、计算开销大且泛化能力差。

Method: 采用无需训练的多智能体工作流，包含Coder和Judge两个LLM智能体，通过迭代生成、测试、分析硬件反馈和优化CUDA内核。

Result: 在KernelBench上显著超越OpenAI-o3和Kevin等SOTA模型，正确率达97.6%，平均速度提升1.68倍，在多种GPU和基础模型上表现出强泛化能力。

Conclusion: 多智能体、无需训练的工作流能够实现成本效益高、可泛化且高性能的CUDA内核优化。

Abstract: Developing efficient CUDA kernels is increasingly critical for AI
applications such as large-scale LLM training. However, manual kernel design is
both costly and time-consuming, motivating automatic approaches that leverage
LLMs for code generation. Existing methods for automatic kernel generation,
however, often produce low-efficiency kernels, incur high computational
overhead, and fail to generalize across settings. In this work, we propose
CudaForge, a training-free multi-agent workflow for CUDA kernel generation and
optimization. Our workflow is inspired by the iterative workflow of human
experts, which contains steps such as developing initial kernels, testing
correctness, analyzing hardware feedback, and iterative improvement. More
specifically, CudaForge employs two LLM agents: a Coder and a Judge, that
iteratively generate, correct, and optimize CUDA kernels, while integrating
hardware feedback such as Nsight Compute (NCU) metrics. In extensive
evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3,
achieves 97.6\% correctness of generated kernels and an average 1.68$\times$
speedup over PyTorch baselines, substantially surpassing state-of-the-art
models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed,
CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090,
3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4,
QwQ-32B), while maintaining high efficiency. In particular, generating an
optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$
0.3 API cost, which is significantly cheaper than existing agentic work that
costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that
multi-agent, training-free workflows can enable cost-effective, generalizable,
and high-performance CUDA kernel optimization. Code available at
https://github.com/OptimAI-Lab/CudaForge

</details>


### [69] [Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch](https://arxiv.org/abs/2511.01934)
*Yirong Zeng,Xiao Ding,Yutai Hou,Yuxian Wang,Li Du,Juyi Dai,Qiuyang Ding,Duyu Tang,Dandan Tu,Weiwen Liu,Bing Qin,Ting Liu*

Main category: cs.LG

TL;DR: 提出基于强化学习的Tool-Zero模型系列，通过动态泛化引导的奖励设计，使LLM能够自主使用通用工具，相比SFT和RL-with-SFT模型性能提升超过7%。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调的方法在复杂工具使用场景中泛化能力不足，需要探索纯强化学习是否能有效激发模型内在推理能力并增强工具无关的泛化性。

Method: 采用动态泛化引导的奖励设计，从探索性到利用性工具使用模式逐步调整奖励，直接从基础模型进行强化学习训练。

Result: 实验结果显示，在相同设置下相比SFT和RL-with-SFT模型性能提升超过7%，在跨数据集和数据集内评估中均表现稳定。

Conclusion: 纯强化学习能够有效激发LLM的内在推理能力，实现工具无关的泛化，提出的方法具有有效性和鲁棒性。

Abstract: Training tool-augmented LLMs has emerged as a promising approach to enhancing
language models' capabilities for complex tasks. The current supervised
fine-tuning paradigm relies on constructing extensive domain-specific datasets
to train models. However, this approach often struggles to generalize
effectively to unfamiliar or intricate tool-use scenarios. Recently,
reinforcement learning (RL) paradigm can endow LLMs with superior reasoning and
generalization abilities. In this work, we address a key question: Can the pure
RL be used to effectively elicit a model's intrinsic reasoning capabilities and
enhance the tool-agnostic generalization? We propose a dynamic
generalization-guided reward design for rule-based RL, which progressively
shifts rewards from exploratory to exploitative tool-use patterns. Based on
this design, we introduce the Tool-Zero series models. These models are trained
to enable LLMs to autonomously utilize general tools by directly scaling up RL
from Zero models (i.e., base models without post-training). Experimental
results demonstrate that our models achieve over 7% performance improvement
compared to both SFT and RL-with-SFT models under the same experimental
settings. These gains are consistently replicated across cross-dataset and
intra-dataset evaluations, validating the effectiveness and robustness of our
methods.

</details>


### [70] [Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR](https://arxiv.org/abs/2511.01937)
*Abdelaziz Bounhar,Hadi Abdine,Evan Dufraisse,Ahmad Chamma,Amr Mohamed,Dani Bouch,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.LG

TL;DR: 通过保留并适度加权中等难度问题作为隐式长度正则化器，防止LLM在推理任务中过度冗长，实现无显式长度惩罚的简洁输出。


<details>
  <summary>Details</summary>
Motivation: 标准RLVR流程过滤掉简单问题，导致模型主要在需要长推理链的难题上训练，使模型混淆"思考更久"与"思考更好"，产生过度冗长的输出。

Method: 保留并适度加权中等难度问题作为隐式长度正则化器，让模型接触可解决的短链任务来约束输出分布。

Result: 在Qwen3-4B-Thinking-2507上的RLVR实验实现了基线pass@1 AIME25准确率，同时平均解决方案长度缩短近一半。

Conclusion: 该方法实现了"免费涌现的简洁性"：模型学会解决更难问题而不增加输出长度，无需任何显式长度惩罚。

Abstract: Large language models (LLMs) trained for step-by-step reasoning often become
excessively verbose, raising inference cost. Standard Reinforcement Learning
with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for
training efficiency, leaving the model to train primarily on harder problems
that require longer reasoning chains. This skews the output length distribution
upward, resulting in a \textbf{model that conflates ``thinking longer'' with
``thinking better''}. In this work, we show that retaining and modestly
up-weighting moderately easy problems acts as an implicit length regularizer.
Exposing the model to solvable short-chain tasks constrains its output
distribution and prevents runaway verbosity. The result is
\textbf{\emph{emergent brevity for free}}: the model learns to solve harder
problems without inflating the output length, \textbf{ despite the absence of
any explicit length penalization}. RLVR experiments using this approach on
\textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline
pass@1 AIME25 accuracy while generating solutions that are, on average, nearly
twice as short. The code is available at
\href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and
models on
\href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging
Face}.

</details>


### [71] [Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning](https://arxiv.org/abs/2511.02044)
*Vivswan Shah,Randy Cogill,Hanwei Yue,Gopinath Chennupati,Rinat Khaziev*

Main category: cs.LG

TL;DR: 在LLM分类微调中，为标签附加解释（即使是随机token构成的伪解释）能提升模型性能，主要原因是额外的token预算促进了更丰富的中间计算，起到了正则化作用。


<details>
  <summary>Details</summary>
Motivation: 研究在LLM分类微调中，为标签附加解释是否能提升模型性能，并探索这种提升背后的机制。

Method: 使用多LLM集成生成的数据，对7B参数模型进行微调，在六个对话数据集上评估。比较标签加解释训练与仅标签基线的效果，并测试用随机token构成的伪解释替代真实解释的效果。

Result: 标签加解释训练在18个数据集和任务设置中均优于仅标签基线。令人意外的是，即使使用缺乏语义的伪解释（如乱序或词袋变体），也能提升准确率并缩小与真实解释的差距。

Conclusion: 解释增强微调（无论是真实解释还是精心构造的随机token序列）能提高LLM分类的准确性和可靠性，主要收益来自结构而非语义，额外的token预算促进了更丰富的中间计算并起到正则化作用。

Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels.
We ask whether attaching brief explanations to each label during fine-tuning
yields better models. We evaluate conversational response quality along three
axes: naturalness, comprehensiveness, and on-topic adherence, each rated on
5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune
a 7B-parameter model and test across six diverse conversational datasets.
Across 18 dataset, task settings, label-plus-explanation training outperforms
label-only baselines.
  A central and unexpected result concerns random tokens. We replace
human-written explanations with text that is syntactically incoherent yet
vocabulary-aligned with the originals (e.g., shuffled or bag-of-words
variants). Despite lacking semantics, these pseudo-explanations still improve
accuracy over label-only training and often narrow much of the gap to true
explanations. The effect persists across datasets and training seeds,
indicating that gains arise less from meaning than from structure: the extra
token budget encourages richer intermediate computation and acts as a
regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit
higher activation entropy in intermediate layers alongside sharper predictive
mass at the output layer, consistent with increased deliberation before
decision. Overall, explanation-augmented fine-tuning, whether with genuine
rationales or carefully constructed random token sequences, improves accuracy
and reliability for LLM classification while clarifying how token-level
scaffolding shapes computation during inference.

</details>


### [72] [Learning Interactive World Model for Object-Centric Reinforcement Learning](https://arxiv.org/abs/2511.02225)
*Fan Feng,Phillip Lippe,Sara Magliacane*

Main category: cs.LG

TL;DR: FIOC-WM是一个对象中心强化学习框架，通过学习对象及其交互的结构化表示，提高策略学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大多数对象中心RL方法只关注单个对象而忽略交互关系，这限制了策略的鲁棒性和迁移性。

Method: 从像素学习对象中心潜在表示和交互结构，使用预训练视觉编码器，构建层次化策略：高层选择交互类型和顺序，低层执行交互。

Result: 在模拟机器人和具身AI基准测试中，FIOC-WM相比基线方法提高了策略学习的样本效率和泛化能力。

Conclusion: 显式的模块化交互学习对于鲁棒控制至关重要。

Abstract: Agents that understand objects and their interactions can learn policies that
are more robust and transferable. However, most object-centric RL methods
factor state by individual objects while leaving interactions implicit. We
introduce the Factored Interactive Object-Centric World Model (FIOC-WM), a
unified framework that learns structured representations of both objects and
their interactions within a world model. FIOC-WM captures environment dynamics
with disentangled and modular representations of object interactions, improving
sample efficiency and generalization for policy learning. Concretely, FIOC-WM
first learns object-centric latents and an interaction structure directly from
pixels, leveraging pre-trained vision encoders. The learned world model then
decomposes tasks into composable interaction primitives, and a hierarchical
policy is trained on top: a high level selects the type and order of
interactions, while a low level executes them. On simulated robotic and
embodied-AI benchmarks, FIOC-WM improves policy-learning sample efficiency and
generalization over world-model baselines, indicating that explicit, modular
interaction learning is crucial for robust control.

</details>


### [73] [The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute](https://arxiv.org/abs/2511.02309)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 本文研究发现，在同等计算资源下，顺序推理链比并行推理链在语言模型推理任务中表现更好，并提出了一种基于逆熵加权投票的新方法来进一步提升顺序推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 重新审视语言模型推理中的测试时扩展策略，探讨在相同token预算和计算资源下，并行推理链与顺序推理链哪种策略更有效。

Method: 在5个开源模型和3个推理基准上全面评估，比较并行自一致性解码与顺序推理链的性能，并引入基于逆熵加权投票的训练无关方法。

Result: 顺序推理在95.6%的配置中优于并行推理，准确率提升高达46.7%。逆熵加权投票进一步提升了顺序推理的成功率。

Conclusion: 顺序推理应成为现代LLM推理的默认策略，这挑战了自Wang等人以来主导的并行推理范式。

Abstract: We revisit test-time scaling for language model reasoning and ask a
fundamental question: at equal token budget and compute, is it better to run
multiple independent chains in parallel, or to run fewer chains that
iteratively refine through sequential steps? Through comprehensive evaluation
across 5 state-of-the-art open source models and 3 challenging reasoning
benchmarks, we find that sequential scaling where chains explicitly build upon
previous attempts consistently outperforms the dominant parallel
self-consistency paradigm in 95.6% of configurations with gains in accuracy
upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel
training-free method to further boost the accuracy of sequential scaling. By
weighing answers in proportion to the inverse entropy of their reasoning
chains, we increase our success rate over parallel majority and establish it as
the optimal test-time scaling strategy. Our findings fundamentally challenge
the parallel reasoning orthodoxy that has dominated test-time scaling since
Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning
sequential refinement as the robust default for modern LLM reasoning and
necessitating a paradigm shift in how we approach inference-time optimization.

</details>


### [74] [Large-scale automatic carbon ion treatment planning for head and neck cancers via parallel multi-agent reinforcement learning](https://arxiv.org/abs/2511.02314)
*Jueye Zhang,Chao Yang,Youfang Lai,Kai-Wen Li,Wenting Yan,Yunzhou Xia,Haimei Zhang,Jingjing Zhou,Gen Yang,Chen Lin,Tian Li,Yibao Zhang*

Main category: cs.LG

TL;DR: 提出了一种可扩展的多智能体强化学习框架，用于并行调优头颈癌碳离子治疗中的45个治疗计划参数，生成与专家手动计划相当或更好的治疗计划。


<details>
  <summary>Details</summary>
Motivation: 头颈癌治疗计划制定困难，碳离子治疗虽具有剂量适形性好等优势，但因相对生物有效性建模复杂导致参数调优耗时且依赖经验。现有深度学习方法存在数据偏差和计划可行性问题，强化学习方法在指数级大的参数空间中探索效率低。

Method: 使用集中训练分散执行(CTDE)的QMIX框架，结合Double DQN、Dueling DQN和循环编码(DRQN)，采用紧凑的历史DVH向量作为状态输入，线性动作-值变换映射离散动作到参数调整，设计基于临床知识的片段化奖励函数，通过多进程系统与治疗计划系统并行交互。

Result: 在头颈癌数据集上同时调优45个参数，生成的治疗计划与专家手动计划相当或更好（相对计划评分：RL 85.93±7.85% vs Manual 85.02±6.92%），对五个危及器官有显著改善。

Conclusion: 该框架能有效探索高维参数空间，通过与治疗计划系统直接交互生成具有临床竞争力的碳离子治疗计划，显著改善了危及器官保护。

Abstract: Head-and-neck cancer (HNC) planning is difficult because multiple critical
organs-at-risk (OARs) are close to complex targets. Intensity-modulated
carbon-ion therapy (IMCT) offers superior dose conformity and OAR sparing but
remains slow due to relative biological effectiveness (RBE) modeling, leading
to laborious, experience-based, and often suboptimal tuning of many
treatment-planning parameters (TPPs). Recent deep learning (DL) methods are
limited by data bias and plan feasibility, while reinforcement learning (RL)
struggles to efficiently explore the exponentially large TPP search space. We
propose a scalable multi-agent RL (MARL) framework for parallel tuning of 45
TPPs in IMCT. It uses a centralized-training decentralized-execution (CTDE)
QMIX backbone with Double DQN, Dueling DQN, and recurrent encoding (DRQN) for
stable learning in a high-dimensional, non-stationary environment. To enhance
efficiency, we (1) use compact historical DVH vectors as state inputs, (2)
apply a linear action-to-value transform mapping small discrete actions to
uniform parameter adjustments, and (3) design an absolute, clinically informed
piecewise reward aligned with plan scores. A synchronous multi-process worker
system interfaces with the PHOENIX TPS for parallel optimization and
accelerated data collection. On a head-and-neck dataset (10 training, 10
testing), the method tuned 45 parameters simultaneously and produced plans
comparable to or better than expert manual ones (relative plan score: RL
$85.93\pm7.85%$ vs Manual $85.02\pm6.92%$), with significant (p-value $<$ 0.05)
improvements for five OARs. The framework efficiently explores high-dimensional
TPP spaces and generates clinically competitive IMCT plans through direct TPS
interaction, notably improving OAR sparing.

</details>


### [75] [Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2511.02567)
*Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了一种新的邻域约束方法来解决离线强化学习中的外推误差问题，通过将Bellman目标中的动作选择限制在数据集动作的邻域并合中，避免了传统约束方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习存在由分布外动作引起的外推误差问题，传统约束方法（密度约束、支撑约束和样本约束）各有局限性：密度和样本约束过于保守，支撑约束难以准确建模行为策略。

Method: 提出邻域约束方法，将动作选择限制在数据集动作的邻域并合中；使用数据质量作为适应标准设计自适应邻域约束；基于高效的双层优化框架开发ANQ算法。

Result: ANQ在标准离线RL基准测试中达到最先进性能，在噪声或有限数据场景下表现出强大的鲁棒性。

Conclusion: 邻域约束方法有效解决了离线RL中的外推误差问题，在理论上能约束外推误差和分布偏移，在实践上实现了高性能和强鲁棒性。

Abstract: Offline reinforcement learning (RL) suffers from extrapolation errors induced
by out-of-distribution (OOD) actions. To address this, offline RL algorithms
typically impose constraints on action selection, which can be systematically
categorized into density, support, and sample constraints. However, we show
that each category has inherent limitations: density and sample constraints
tend to be overly conservative in many scenarios, while the support constraint,
though least restrictive, faces challenges in accurately modeling the behavior
policy. To overcome these limitations, we propose a new neighborhood constraint
that restricts action selection in the Bellman target to the union of
neighborhoods of dataset actions. Theoretically, the constraint not only bounds
extrapolation errors and distribution shift under certain conditions, but also
approximates the support constraint without requiring behavior policy modeling.
Moreover, it retains substantial flexibility and enables pointwise conservatism
by adapting the neighborhood radius for each data point. In practice, we employ
data quality as the adaptation criterion and design an adaptive neighborhood
constraint. Building on an efficient bilevel optimization framework, we develop
a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning
(ANQ), to perform Q learning with target actions satisfying this constraint.
Empirically, ANQ achieves state-of-the-art performance on standard offline RL
benchmarks and exhibits strong robustness in scenarios with noisy or limited
data.

</details>


### [76] [Natural-gas storage modelling by deep reinforcement learning](https://arxiv.org/abs/2511.02646)
*Tiziano Balaconi,Aldo Glielmo,Marco Taboga*

Main category: cs.LG

TL;DR: GasRL是一个将天然气市场模拟与基于深度强化学习的存储运营商策略相结合的模拟器，用于分析最优库存管理如何影响均衡价格及供需动态。研究发现SAC算法表现最佳，能实现盈利性、市场清算稳健性和价格稳定等多重目标，且产生的价格动态特征与真实市场高度吻合。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析最优库存管理策略对天然气市场均衡价格和供需动态的影响，并评估欧盟强制最低存储阈值政策对市场韧性的作用。

Method: 开发GasRL模拟器，结合校准的天然气市场表示和基于深度强化学习的存储运营商策略模型，测试多种RL算法（包括SAC），并模拟欧盟存储阈值政策的影响。

Result: SAC算法在GasRL环境中表现最优，成功实现存储运营商的多重目标，产生的均衡价格动态特征（波动性和季节性）与真实价格高度匹配，且无需对价格数据进行显式校准。存储阈值政策能提高市场对意外供应冲击的韧性。

Conclusion: GasRL模拟器能有效分析存储管理策略对市场的影响，SAC算法是解决此类问题的有效工具，欧盟存储阈值政策有助于增强市场韧性。

Abstract: We introduce GasRL, a simulator that couples a calibrated representation of
the natural gas market with a model of storage-operator policies trained with
deep reinforcement learning (RL). We use it to analyse how optimal stockpile
management affects equilibrium prices and the dynamics of demand and supply. We
test various RL algorithms and find that Soft Actor Critic (SAC) exhibits
superior performance in the GasRL environment: multiple objectives of storage
operators - including profitability, robust market clearing and price
stabilisation - are successfully achieved. Moreover, the equilibrium price
dynamics induced by SAC-derived optimal policies have characteristics, such as
volatility and seasonality, that closely match those of real-world prices.
Remarkably, this adherence to the historical distribution of prices is obtained
without explicitly calibrating the model to price data. We show how the
simulator can be used to assess the effects of EU-mandated minimum storage
thresholds. We find that such thresholds have a positive effect on market
resilience against unanticipated shifts in the distribution of supply shocks.
For example, with unusually large shocks, market disruptions are averted more
often if a threshold is in place.

</details>


### [77] [Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs](https://arxiv.org/abs/2511.02690)
*Georgios Tzannetos,Parameswaran Kamalaruban,Adish Singla*

Main category: cs.LG

TL;DR: 提出一种课程学习策略，通过在训练过程中逐步收紧约束条件，帮助智能体逐步掌握部署要求，加速训练过程并提升在复杂约束环境下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 在严格约束条件下（如资源限制或安全要求）训练智能体面临重大挑战，特别是当这些约束使任务变得复杂时。需要一种方法能够平滑过渡到具有挑战性的部署环境。

Method: 采用课程学习策略，从简化的约束条件开始训练，逐步引入完整的部署条件。该方法受到无约束强化学习中自定进度学习技术的启发，在强化学习和大型语言模型智能体上进行了验证。

Result: 理论分析和实证验证表明，该课程策略相比从一开始就施加完整约束的基线方法能够加速训练。在LLM应用中，该方法能够压缩输出思维链token，在消费级硬件上实现显著的推理加速。

Conclusion: 课程设计在提升在复杂轨迹约束下运行的智能体的效率和性能方面具有潜力，特别适用于资源受限的部署场景。

Abstract: Training agents to operate under strict constraints during deployment, such
as limited resource budgets or stringent safety requirements, presents
significant challenges, especially when these constraints render the task
complex. In this work, we propose a curriculum learning strategy that gradually
tightens constraints during training, enabling the agent to incrementally
master the deployment requirements. Inspired by self-paced learning techniques
in unconstrained reinforcement learning (RL), our approach facilitates a
smoother transition to challenging environments by initially training on
simplified versions of the constraints and progressively introducing the full
deployment conditions. We provide a theoretical analysis using an RL agent in a
binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum
strategy can accelerate training relative to a baseline approach that imposes
the trajectory constraints from the outset. Moreover, we empirically validate
the effectiveness and generality of our method across both RL and large
language model (LLM) agents in diverse settings, including a binary-tree MDP, a
multi-task navigation domain, and a math reasoning task with two benchmarks.
These results highlight the potential of curriculum design in enhancing the
efficiency and performance of agents operating under complex trajectory
constraints during deployment. Moreover, when applied to LLMs, our strategy
enables compression of output chain-of-thought tokens, achieving a substantial
inference speedup on consumer hardware, demonstrating its effectiveness for
resource-constrained deployment.

</details>


### [78] [From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos](https://arxiv.org/abs/2511.02762)
*Xun Wang,Zhuoran Li,Yanshan Lin,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: SoCo框架通过将单智能体知识迁移到协作学习中，显著提升了多智能体强化学习的训练效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习方法依赖成本高昂的多智能体数据，而单智能体经验在许多重要场景中更容易获取，但现有方法未能充分利用这些单智能体知识

Method: SoCo框架首先从单智能体演示中预训练共享的单智能体策略，然后通过策略融合机制（包含MoE类门控选择器和动作编辑器）在多智能体训练期间将其适应于协作任务

Result: 在多种协作任务上的实验表明，SoCo显著提升了骨干算法的训练效率和性能

Conclusion: 单智能体演示为多智能体数据提供了可扩展且有效的补充，使协作学习更加实用和广泛适用

Abstract: Training a team of agents from scratch in multi-agent reinforcement learning
(MARL) is highly inefficient, much like asking beginners to play a symphony
together without first practicing solo. Existing methods, such as offline or
transferable MARL, can ease this burden, but they still rely on costly
multi-agent data, which often becomes the bottleneck. In contrast, solo
experiences are far easier to obtain in many important scenarios, e.g.,
collaborative coding, household cooperation, and search-and-rescue. To unlock
their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that
transfers solo knowledge into cooperative learning. SoCo first pretrains a
shared solo policy from solo demonstrations, then adapts it for cooperation
during multi-agent training through a policy fusion mechanism that combines an
MoE-like gating selector and an action editor. Experiments across diverse
cooperative tasks show that SoCo significantly boosts the training efficiency
and performance of backbone algorithms. These results demonstrate that solo
demonstrations provide a scalable and effective complement to multi-agent data,
making cooperative learning more practical and broadly applicable.

</details>
