{"id": "2510.06240", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.06240", "abs": "https://arxiv.org/abs/2510.06240", "authors": ["Jiqun Pan", "Zhenke Duan", "Jiani Tu", "Anzhi Cheng", "Yanqing Wang"], "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets", "comment": "41 pages, 12 figures, 6 tables", "summary": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u84b8\u998f\u65b9\u6cd5(KG-MASD)\uff0c\u901a\u8fc7\u5c06\u84b8\u998f\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5e76\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u95ee\u7b54\u7cfb\u7edf\u4e2d\u591a\u667a\u80fd\u4f53\u6a21\u578b\u63a8\u7406\u6df1\u5ea6\u4e0e\u8f7b\u91cf\u5316\u90e8\u7f72\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u5de5\u4e1a\u95ee\u7b54\u7cfb\u7edf\u9700\u8981\u6bd4\u901a\u7528\u5bf9\u8bdd\u6a21\u578b\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u56e0\u4e3a\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u9519\u8bef\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u540e\u679c\u3002\u4f20\u7edf\u84b8\u998f\u65b9\u6cd5\u96be\u4ee5\u5c06\u591a\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "method": "\u5c06\u84b8\u998f\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u5148\u9a8c\u6765\u4e30\u5bcc\u72b6\u6001\u8868\u793a\u5e76\u786e\u4fdd\u6536\u655b\uff0c\u901a\u8fc7\u7ed3\u5408\u534f\u4f5c\u63a8\u7406\u548c\u77e5\u8bc6\u57fa\u7840\u751f\u6210\u9ad8\u8d28\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u3002", "result": "\u5728\u5de5\u4e1a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKG-MASD\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u4e862.4%\u523020.1%\uff0c\u5e76\u663e\u8457\u589e\u5f3a\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "KG-MASD\u80fd\u591f\u5c06\u63a8\u7406\u6df1\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u5171\u540c\u84b8\u998f\u5230\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u7684\u7d27\u51d1\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u53ef\u4fe1AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.06261", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.", "AI": {"tldr": "AlphaApollo\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u667a\u80fd\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u6a21\u578b\u548c\u4e13\u4e1a\u5de5\u5177\u6765\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u548c\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u5728AIME 2024/2025\u8bc4\u4f30\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u53d7\u9650\u7684\u4e24\u4e2a\u74f6\u9888\uff1a\u6a21\u578b\u5185\u5728\u80fd\u529b\u4e0d\u8db3\u548c\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u4e0d\u53ef\u9760\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "method": "\u534f\u8c03\u591a\u4e2a\u6a21\u578b\u4e0e\u4e13\u4e1a\u5de5\u5177\uff08Python\u8ba1\u7b97\u5de5\u5177\u548c\u68c0\u7d22\u5de5\u5177\uff09\u8fdb\u884c\u6df1\u601d\u719f\u8651\u3001\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\uff0c\u901a\u8fc7\u5171\u4eab\u72b6\u6001\u56fe\u652f\u6301\u591a\u8f6e\u3001\u591a\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u6f14\u5316\u3002", "result": "\u5728AIME 2024/2025\u8bc4\u4f30\u4e2d\uff0cQwen2.5-14B-Instruct\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u53475.15%\uff0c\u901a\u8fc7\u7387\u63d0\u534723.34%\uff1bLlama-3.3-70B-Instruct\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u53478.91%\uff0c\u901a\u8fc7\u7387\u63d0\u534726.67%\u3002\u8d85\u8fc780%\u7684\u5de5\u5177\u8c03\u7528\u6210\u529f\u6267\u884c\u3002", "conclusion": "AlphaApollo\u7cfb\u7edf\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u548c\u591a\u6a21\u578b\u534f\u8c03\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0a\u9650\uff0c\u5de5\u5177\u4f7f\u7528\u663e\u8457\u4f18\u4e8e\u975e\u5de5\u5177\u57fa\u7ebf\u3002", "topic": "agent analysis"}}
{"id": "2510.06288", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers.", "AI": {"tldr": "BuilderBench\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5f00\u653e\u5f0f\u63a2\u7d22\u7684\u667a\u80fd\u4f53\u9884\u8bad\u7ec3\u57fa\u51c6\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u5b66\u4e60\u4f7f\u7528\u79ef\u6728\u6784\u5efa\u5404\u79cd\u7ed3\u6784\uff0c\u6d4b\u8bd5\u7269\u7406\u7406\u89e3\u3001\u6570\u5b66\u80fd\u529b\u548c\u957f\u7a0b\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\uff0c\u96be\u4ee5\u89e3\u51b3\u8d85\u51fa\u5df2\u6709\u6570\u636e\u8303\u56f4\u7684\u65b0\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u7ecf\u9a8c\u5b66\u4e60\u7684\u667a\u80fd\u4f53\uff0c\u4f46\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u673a\u5236\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u63d0\u4f9b\u786c\u4ef6\u52a0\u901f\u7684\u673a\u5668\u4eba\u6a21\u62df\u5668\u548c42\u4e2a\u591a\u6837\u5316\u76ee\u6807\u7ed3\u6784\u7684\u4efb\u52a1\u5957\u4ef6\uff0c\u667a\u80fd\u4f53\u5728\u8bad\u7ec3\u65f6\u65e0\u76d1\u7763\u63a2\u7d22\u73af\u5883\uff0c\u5728\u8bc4\u4f30\u65f6\u6784\u5efa\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u7b97\u6cd5\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u63d0\u4f9b\u4e86\"\u8bad\u7ec3\u8f6e\"\u534f\u8bae\u548c\u516d\u79cd\u7b97\u6cd5\u7684\u5355\u6587\u4ef6\u5b9e\u73b0\u4f5c\u4e3a\u53c2\u8003\u3002", "conclusion": "BuilderBench\u4fc3\u8fdb\u4e86\u9700\u8981\u5177\u8eab\u63a8\u7406\u7684\u667a\u80fd\u4f53\u9884\u8bad\u7ec3\u7814\u7a76\uff0c\u8fd9\u79cd\u63a8\u7406\u4f53\u73b0\u5728\u884c\u52a8\u800c\u975e\u8bed\u8a00\u4e2d\uff0c\u9700\u8981\u5b9e\u9a8c\u4e0d\u540c\u7b56\u7565\u5e76\u6574\u5408\u5b83\u4eec\u3002", "topic": "swe benchmark"}}
{"id": "2510.06606", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06606", "abs": "https://arxiv.org/abs/2510.06606", "authors": ["Uswat Yusuf", "Genevieve Caumartin", "Diego Elias Costa"], "title": "Beyond More Context: How Granularity and Order Drive Code Completion Quality", "comment": null, "summary": "Context plays an important role in the quality of code completion, as Large\nLanguage Models (LLMs) require sufficient and relevant information to assist\ndevelopers in code generation tasks. However, composing a relevant context for\ncode completion poses challenges in large repositories: First, the limited\ncontext length of LLMs makes it impractical to include all repository files.\nSecond, the quality of generated code is highly sensitive to noisy or\nirrelevant context. In this paper, we present our approach for the ASE 2025\nContext Collection Challenge. The challenge entails outperforming JetBrains\nbaselines by designing effective retrieval and context collection strategies.\nWe develop and evaluate a series of experiments that involve retrieval\nstrategies at both the file and chunk levels. We focus our initial experiments\non examining the impact of context size and file ordering on LLM performance.\nOur results show that the amount and order of context can significantly\ninfluence the performance of the models. We introduce chunk-based retrieval\nusing static analysis, achieving a 6% improvement over our best file-retrieval\nstrategy and a 16% improvement over the no-context baseline for Python in the\ninitial phase of the competition. Our results highlight the importance of\nretrieval granularity, ordering and hybrid strategies in developing effective\ncontext collection pipelines for real-world development scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9759\u6001\u5206\u6790\u7684\u4ee3\u7801\u5757\u68c0\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4ee3\u7801\u8865\u5168\u4e2d\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\uff0c\u5728ASE 2025\u6311\u6218\u8d5b\u4e2d\u76f8\u6bd4\u6587\u4ef6\u7ea7\u68c0\u7d22\u548c\u65e0\u4e0a\u4e0b\u6587\u57fa\u7ebf\u5206\u522b\u63d0\u5347\u4e866%\u548c16%\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u9700\u8981\u5145\u5206\u4e14\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f46\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u566a\u58f0\u5e72\u6270\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u6587\u4ef6\u7ea7\u548c\u4ee3\u7801\u5757\u7ea7\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5927\u5c0f\u548c\u6587\u4ef6\u6392\u5e8f\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u9759\u6001\u5206\u6790\u7684\u4ee3\u7801\u5757\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e0a\u4e0b\u6587\u6570\u91cf\u548c\u987a\u5e8f\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4ee3\u7801\u5757\u68c0\u7d22\u76f8\u6bd4\u6587\u4ef6\u68c0\u7d22\u63d0\u5347\u4e866%\uff0c\u76f8\u6bd4\u65e0\u4e0a\u4e0b\u6587\u57fa\u7ebf\u63d0\u5347\u4e8616%\u3002", "conclusion": "\u68c0\u7d22\u7c92\u5ea6\u3001\u6392\u5e8f\u7b56\u7565\u548c\u6df7\u5408\u65b9\u6cd5\u5bf9\u4e8e\u6784\u5efa\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u7ba1\u9053\u81f3\u5173\u91cd\u8981\u3002", "topic": "code agent"}}
{"id": "2510.06708", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06708", "abs": "https://arxiv.org/abs/2510.06708", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Olli-Pekka Turtio", "Mika M\u00e4ntyl\u00e4"], "title": "AISysRev -- LLM-based Tool for Title-abstract Screening", "comment": "4 pages", "summary": "Systematic reviews are a standard practice for summarizing the state of\nevidence in software engineering. Conducting systematic reviews is laborious,\nespecially during the screening or study selection phase, where the number of\npapers can be overwhelming. During this phase, papers are assessed against\ninclusion and exclusion criteria based on their titles and abstracts. Recent\nresearch has demonstrated that large language models (LLMs) can perform\ntitle-abstract screening at a level comparable to that of a master's student.\nWhile LLMs cannot be fully trusted, they can help, for example, in Rapid\nReviews, which try to expedite the review process. Building on recent research,\nwe developed AiSysRev, an LLM-based screening tool implemented as a web\napplication running in a Docker container. The tool accepts a CSV file\ncontaining paper titles and abstracts. Users specify inclusion and exclusion\ncriteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev\nsupports both zero-shot and few-shot screening, and also allows for manual\nscreening through interfaces that display LLM results as guidance for human\nreviewers.We conducted a trial study with 137 papers using the tool. Our\nfindings indicate that papers can be classified into four categories: Easy\nIncludes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary\ncases, where LLMs are prone to errors, highlight the need for human\nintervention. While LLMs do not replace human judgment in systematic reviews,\nthey can significantly reduce the burden of assessing large volumes of\nscientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:\nhttps://github.com/EvoTestOps/AISysRev", "AI": {"tldr": "\u5f00\u53d1\u4e86AiSysRev\u5de5\u5177\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u7b5b\u9009\u9636\u6bb5\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u5bf9\u8bba\u6587\u8fdb\u884c\u5206\u7c7b\uff0c\u51cf\u8f7b\u4eba\u5de5\u7b5b\u9009\u8d1f\u62c5\u3002", "motivation": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u7684\u7b5b\u9009\u9636\u6bb5\u5de5\u4f5c\u91cf\u5927\uff0c\u9700\u8981\u5904\u7406\u5927\u91cf\u8bba\u6587\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6807\u9898\u6458\u8981\u7b5b\u9009\u65b9\u9762\u5df2\u8868\u73b0\u51fa\u4e0e\u7855\u58eb\u751f\u76f8\u5f53\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u7b5b\u9009\u5de5\u5177AiSysRev\uff0c\u4f5c\u4e3aDocker\u5bb9\u5668\u4e2d\u7684Web\u5e94\u7528\uff0c\u63a5\u53d7\u5305\u542b\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u7684CSV\u6587\u4ef6\uff0c\u7528\u6237\u6307\u5b9a\u7eb3\u5165\u6392\u9664\u6807\u51c6\uff0c\u652f\u6301\u591a\u79cdLLM\u548c\u96f6\u6837\u672c/\u5c11\u6837\u672c\u7b5b\u9009\u3002", "result": "\u5728137\u7bc7\u8bba\u6587\u7684\u8bd5\u9a8c\u4e2d\uff0c\u53d1\u73b0\u8bba\u6587\u53ef\u5206\u4e3a\u56db\u7c7b\uff1a\u5bb9\u6613\u7eb3\u5165\u3001\u5bb9\u6613\u6392\u9664\u3001\u8fb9\u754c\u7eb3\u5165\u548c\u8fb9\u754c\u6392\u9664\u3002\u8fb9\u754c\u6848\u4f8b\u4e2dLLM\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "LLM\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\u5224\u65ad\uff0c\u4f46\u80fd\u663e\u8457\u51cf\u8f7b\u8bc4\u4f30\u5927\u91cf\u79d1\u5b66\u6587\u732e\u7684\u8d1f\u62c5\u3002", "topic": "swe application"}}
{"id": "2510.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u5ff5\u6821\u51c6\u5171\u8bc6\u5bfb\u6c42(BCCS)\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u6821\u51c6\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u6765\u4fc3\u8fdb\u7a33\u5b9a\u5171\u8bc6\uff0c\u5728MATH\u548cMMLU\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5171\u8bc6\u5bfb\u6c42\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6295\u7968\u673a\u5236\uff0c\u5ffd\u89c6\u4e86\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u7684\u77db\u76fe\uff0c\u4e14\u91c7\u7528\u65e0\u5dee\u522b\u7684\u534f\u4f5c\u65b9\u5f0f\uff0c\u65e0\u6cd5\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u627e\u5230\u6700\u4f18\u5408\u4f5c\u8005\uff0c\u963b\u788d\u4e86\u7a33\u5b9a\u5171\u8bc6\u7684\u5f62\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u9009\u62e9\u6700\u5927\u5316\u5171\u8bc6\u7a33\u5b9a\u6027\u7684\u6700\u4f18\u5408\u4f5c\u8005\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86BCCS\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u6821\u51c6\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u6765\u4fc3\u8fdb\u7a33\u5b9a\u5171\u8bc6\u3002", "result": "\u5728MATH\u548cMMLU\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cBCCS\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u51c6\u786e\u7387\u63d0\u9ad8\u4e862.23%\u548c3.95%\u3002", "conclusion": "BCCS\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u6821\u51c6\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5171\u8bc6\u5bfb\u6c42\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742NLP\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.06718", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06718", "abs": "https://arxiv.org/abs/2510.06718", "authors": ["Ranim Khojah", "Mazen Mohamad", "Linda Erlenhov", "Francisco Gomes de Oliveira Neto", "Philipp Leitner"], "title": "LLM Company Policies and Policy Implications in Software Organizations", "comment": "Accepted at IEEE Software Special Issue on AIware in the Foundation\n  Models Era", "summary": "The risks associated with adopting large language model (LLM) chatbots in\nsoftware organizations highlight the need for clear policies. We examine how 11\ncompanies create these policies and the factors that influence them, aiming to\nhelp managers safely integrate chatbots into development workflows.", "AI": {"tldr": "\u7814\u7a7611\u5bb6\u516c\u53f8\u5982\u4f55\u5236\u5b9aLLM\u804a\u5929\u673a\u5668\u4eba\u653f\u7b56\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\uff0c\u5e2e\u52a9\u7ba1\u7406\u8005\u5b89\u5168\u5730\u5c06\u804a\u5929\u673a\u5668\u4eba\u6574\u5408\u5230\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "motivation": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u804a\u5929\u673a\u5668\u4eba\u5b58\u5728\u98ce\u9669\uff0c\u8f6f\u4ef6\u7ec4\u7ec7\u9700\u8981\u660e\u786e\u7684\u653f\u7b56\u6765\u786e\u4fdd\u5b89\u5168\u96c6\u6210\u3002", "method": "\u7814\u7a7611\u5bb6\u516c\u53f8\u5236\u5b9a\u804a\u5929\u673a\u5668\u4eba\u653f\u7b56\u7684\u8fc7\u7a0b\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u5206\u6790\u4e86\u516c\u53f8\u5236\u5b9aLLM\u804a\u5929\u673a\u5668\u4eba\u653f\u7b56\u7684\u65b9\u6cd5\u548c\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u4e3a\u7ba1\u7406\u8005\u63d0\u4f9b\u4e86\u5c06\u804a\u5929\u673a\u5668\u4eba\u5b89\u5168\u96c6\u6210\u5230\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6307\u5bfc\u3002", "topic": "swe application"}}
{"id": "2510.06410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u7406LLM\u5728\u5171\u4eab\u63a8\u7406\u8f68\u8ff9\u4e2d\u534f\u4f5c\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u66f4\u5f3a\u7684LLM\u5728\u5e72\u6270\u4e0b\u66f4\u8106\u5f31\uff0c\u6240\u6709\u6a21\u578b\u5728\u8d85\u51fa\u81ea\u8eab\u80fd\u529b\u7684\u95ee\u9898\u4e0a\u5229\u7528\u534f\u4f5c\u6307\u5bfc\u7684\u6548\u679c\u90fd\u5f88\u5dee\uff08\u89e3\u51b3\u7387\u4f4e\u4e8e9.2%\uff09\u3002", "motivation": "\u7814\u7a76\u6807\u51c6\u5355\u63a8\u7406\u8bad\u7ec3\u6d41\u7a0b\u662f\u5426\u80fd\u4ea7\u751f\u6240\u9700\u7684\u79bb\u8f68\u8ff9\u63a8\u7406\u884c\u4e3a\uff0c\u5373\u8bc4\u4f30\u548c\u57fa\u4e8e\u5176\u4ed6\u6a21\u578b\u90e8\u5206\u601d\u8003\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u591a\u6a21\u578b\u534f\u4f5c\u63a8\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u53cc\u6d4b\u8bd5\u65b9\u6cd5\uff1a\u53ef\u6062\u590d\u6027\u6d4b\u8bd5LLM\u4ece\u8bef\u5bfc\u63a8\u7406\u8f68\u8ff9\u4e2d\u56de\u6eaf\u7684\u80fd\u529b\uff0c\u53ef\u5f15\u5bfc\u6027\u6d4b\u8bd5LLM\u57fa\u4e8e\u66f4\u5f3a\u534f\u4f5c\u8005\u7684\u6b63\u786e\u63a8\u7406\u8fdb\u884c\u6784\u5efa\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u4e8615\u4e2a\u5f00\u6e90LLM\uff081.5B-32B\uff09\uff0c\u5e76\u8fdb\u884c\u63a7\u5236\u7814\u7a76\u5206\u6790\u540e\u8bad\u7ec3\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u53cd\u76f4\u89c9\u7ed3\u679c\uff1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u66f4\u5f3a\u7684LLM\u5728\u5e72\u6270\u4e0b\u5f80\u5f80\u66f4\u8106\u5f31\uff1b\u6240\u6709\u6a21\u578b\u5728\u8d85\u51fa\u81ea\u8eab\u80fd\u529b\u7684\u95ee\u9898\u4e0a\u5229\u7528\u6307\u5bfc\u6b65\u9aa4\u7684\u6548\u679c\u90fd\u5f88\u5dee\uff08\u89e3\u51b3\u7387<9.2%\uff09\uff1b\u6559\u5e08\u6a21\u578b\u7684\u4e0d\u7406\u60f3\u53ef\u6062\u590d\u6027\u884c\u4e3a\u4f1a\u4f20\u9012\u7ed9\u5b66\u751f\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bc4\u4f30\u5171\u4eab\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u591a\u6a21\u578b\u534f\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u7a81\u663e\u4e86\u73b0\u6210\u63a8\u7406LLM\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bad\u7ec3\u539f\u751f\u5f3a\u63a8\u7406\u534f\u4f5c\u8005\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2510.06349", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06349", "abs": "https://arxiv.org/abs/2510.06349", "authors": ["Moein E. Samadi", "Andreas Schuppert"], "title": "Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks", "comment": null, "summary": "Foundation models have rapidly advanced AI, raising the question of whether\ntheir decisions will ultimately surpass human strategies in real-world domains.\nThe exponential, and possibly super-exponential, pace of AI development makes\nsuch analysis elusive. Nevertheless, many application areas that matter for\ndaily life and society show only modest gains so far; a prominent case is\ndiagnosing and treating dynamically evolving disease in intensive care.\n  The common challenge is adapting complex systems to dynamic environments.\nEffective strategies must optimize outcomes in systems composed of strongly\ninteracting functions while avoiding shared side effects; this requires\nreliable, self-adaptive modeling. These tasks align with building digital twins\nof highly complex systems whose mechanisms are not fully or quantitatively\nunderstood. It is therefore essential to develop methods for self-adapting AI\nmodels with minimal data and limited mechanistic knowledge. As this challenge\nextends beyond medicine, AI should demonstrate clear superiority in these\nsettings before assuming broader decision-making roles.\n  We identify the curse of dimensionality as a fundamental barrier to efficient\nself-adaptation and argue that monolithic foundation models face conceptual\nlimits in overcoming it. As an alternative, we propose a decentralized\narchitecture of interacting small agent networks (SANs). We focus on agents\nrepresenting the specialized substructure of the system, where each agent\ncovers only a subset of the full system functions. Drawing on mathematical\nresults on the learning behavior of SANs and evidence from existing\napplications, we argue that swarm-learning in diverse swarms can enable\nself-adaptive SANs to deliver superior decision-making in dynamic environments\ncompared with monolithic foundation models, though at the cost of reduced\nreproducibility in detail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u53bb\u4e2d\u5fc3\u5316\u7684\u5c0f\u578b\u667a\u80fd\u4f53\u7f51\u7edc\uff08SANs\uff09\u67b6\u6784\u6765\u514b\u670d\u57fa\u7840\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\uff0c\u76f8\u6bd4\u5355\u4f53\u57fa\u7840\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u4f18\u8d8a\u7684\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u91cd\u75c7\u76d1\u62a4\uff09\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6570\u636e\u548c\u673a\u5236\u77e5\u8bc6\u6709\u9650\u60c5\u51b5\u4e0b\u81ea\u6211\u9002\u5e94\u7684AI\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u7684\u5c0f\u578b\u667a\u80fd\u4f53\u7f51\u7edc\u67b6\u6784\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4ec5\u8986\u76d6\u7cfb\u7edf\u529f\u80fd\u5b50\u96c6\uff0c\u901a\u8fc7\u7fa4\u4f53\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u9002\u5e94\u3002", "result": "\u7fa4\u4f53\u5b66\u4e60\u5728\u4e0d\u540c\u7fa4\u4f53\u4e2d\u80fd\u591f\u4f7f\u81ea\u6211\u9002\u5e94\u7684SANs\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u6bd4\u5355\u4f53\u57fa\u7840\u6a21\u578b\u66f4\u4f18\u8d8a\u7684\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u7684SANs\u67b6\u6784\u662f\u514b\u670d\u57fa\u7840\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7ef4\u5ea6\u8bc5\u5492\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4ee3\u4ef7\u662f\u7ec6\u8282\u53ef\u91cd\u590d\u6027\u964d\u4f4e\u3002", "topic": "agent analysis"}}
{"id": "2510.06534", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u884c\u4e3a\u5f15\u5bfc\u6280\u672f\uff0c\u901a\u8fc7\u8bc6\u522b\u56db\u79cd\u6709\u76ca\u63a8\u7406\u884c\u4e3a\uff08\u4fe1\u606f\u9a8c\u8bc1\u3001\u6743\u5a01\u8bc4\u4f30\u3001\u81ea\u9002\u5e94\u641c\u7d22\u3001\u9519\u8bef\u6062\u590d\uff09\u6765\u8bad\u7ec3\u66f4\u6709\u6548\u7684\u667a\u80fd\u641c\u7d22\u4ee3\u7406\u6a21\u578b\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u76f8\u6bd4\u76f4\u63a5RL\u8bad\u7ec3\u83b7\u5f97\u8d85\u8fc735%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u667a\u80fd\u641c\u7d22\u5229\u7528LLMs\u89e3\u91ca\u590d\u6742\u7528\u6237\u4fe1\u606f\u9700\u6c42\u5e76\u6267\u884c\u591a\u6b65\u9aa4\u641c\u7d22\u8fc7\u7a0b\uff0c\u8fd9\u5bf9LLMs\u7684\u63a8\u7406\u548c\u4ee3\u7406\u80fd\u529b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\u3002\u9700\u8981\u7814\u7a76\u6709\u6548\u7684\u63a8\u7406\u884c\u4e3a\u6a21\u5f0f\u6765\u63d0\u5347\u667a\u80fd\u641c\u7d22\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u9a71\u52a8\u7684LLM\u7ba1\u9053\u5206\u6790\u6210\u529f\u641c\u7d22\u8f68\u8ff9\uff0c\u8bc6\u522b\u56db\u79cd\u6709\u76ca\u63a8\u7406\u884c\u4e3a\uff0c\u7136\u540e\u901a\u8fc7\u884c\u4e3a\u5f15\u5bfc\u6280\u672f\u5c06\u8fd9\u4e9b\u884c\u4e3a\u6574\u5408\u5230\u667a\u80fd\u641c\u7d22\u6a21\u578b\u4e2d\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728GAIA\u3001WebWalker\u548cHLE\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u884c\u4e3a\u5f15\u5bfc\u6280\u672f\u5728Llama3.2-3B\u548cQwen3-1.7B\u4e0a\u76f8\u6bd4\u76f4\u63a5RL\u8bad\u7ec3\u83b7\u5f97\u8d85\u8fc735%\u7684\u6027\u80fd\u63d0\u5347\u3002\u5173\u952e\u53d1\u73b0\u662fSFT\u6570\u636e\u4e2d\u671f\u671b\u7684\u63a8\u7406\u884c\u4e3a\uff08\u800c\u975e\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff09\u662fRL\u540e\u83b7\u5f97\u5f3a\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u5f15\u5165\u7684\u63a8\u7406\u884c\u4e3a\u8d4b\u4e88\u6a21\u578b\u66f4\u6709\u6548\u7684\u63a2\u7d22\u80fd\u529b\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u80fd\u529b\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002\u671f\u671b\u7684\u63a8\u7406\u884c\u4e3a\u6bd4\u7b54\u6848\u6b63\u786e\u6027\u5bf9\u6700\u7ec8\u6027\u80fd\u66f4\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2510.06538", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06538", "abs": "https://arxiv.org/abs/2510.06538", "authors": ["Jiajie Li", "Huayi Zhang", "Peng Lin", "Jinjun Xiong", "Wei Xu"], "title": "Auto-Prompt Ensemble for LLM Judge", "comment": null, "summary": "We present a novel framework that improves the reliability of LLM judges by\nselectively augmenting LLM with auxiliary evaluation dimensions. Existing LLM\njudges often miss crucial evaluation dimensions because they fail to recognize\nthe implicit standards underlying human assessments. To address this challenge,\nwe propose the Auto-Prompt Ensemble (APE), an adaptive framework that\nautomatically learns evaluation dimensions from its failure cases. APE\nincorporates a confidence-based ensemble mechanism to decide when to adopt the\njudgments from additional evaluation dimensions through a novel confidence\nestimation approach called Collective Confidence. Extensive experiments\ndemonstrate that APE improves the reliability of LLM Judge across diverse\nstandard benchmarks. For instance, APE enhances GPT-4o agreement rate on Reward\nBench from 87.2% to 90.5% in the zero-shot setting. Overall, APE provides a\nprincipled approach for LLM Judge to leverage test-time computation, and bridge\nthe evaluation gap between human and LLM judges.", "AI": {"tldr": "\u63d0\u51fa\u4e86Auto-Prompt Ensemble (APE)\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u589e\u5f3aLLM\u7684\u8f85\u52a9\u8bc4\u4f30\u7ef4\u5ea6\u6765\u63d0\u9ad8LLM\u6cd5\u5b98\u7684\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u6cd5\u5b98\u56e0\u672a\u80fd\u8bc6\u522b\u4eba\u7c7b\u8bc4\u4f30\u9690\u542b\u6807\u51c6\u800c\u9057\u6f0f\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u6cd5\u5b98\u7ecf\u5e38\u56e0\u4e3a\u65e0\u6cd5\u8bc6\u522b\u4eba\u7c7b\u8bc4\u4f30\u7684\u9690\u542b\u6807\u51c6\u800c\u9057\u6f0f\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6846\u67b6APE\uff0c\u81ea\u52a8\u4ece\u5931\u8d25\u6848\u4f8b\u4e2d\u5b66\u4e60\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u96c6\u6210\u673a\u5236\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u96c6\u4f53\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u51b3\u5b9a\u4f55\u65f6\u91c7\u7528\u989d\u5916\u8bc4\u4f30\u7ef4\u5ea6\u7684\u5224\u65ad\u3002", "result": "\u5728\u591a\u6837\u5316\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPE\u663e\u8457\u63d0\u9ad8\u4e86LLM\u6cd5\u5b98\u7684\u53ef\u9760\u6027\u3002\u4f8b\u5982\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0cAPE\u5c06GPT-4o\u5728Reward Bench\u4e0a\u7684\u540c\u610f\u7387\u4ece87.2%\u63d0\u5347\u523090.5%\u3002", "conclusion": "APE\u4e3aLLM\u6cd5\u5b98\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff0c\u5f25\u5408\u4eba\u7c7b\u4e0eLLM\u6cd5\u5b98\u4e4b\u95f4\u7684\u8bc4\u4f30\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2510.07189", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07189", "abs": "https://arxiv.org/abs/2510.07189", "authors": ["Junjie Li", "Fazle Rabbi", "Bo Yang", "Song Wang", "Jinqiu Yang"], "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe", "comment": null, "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively.", "AI": {"tldr": "Secure-Instruct\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u9ad8\u8d28\u91cf\u6f0f\u6d1e\u548c\u5b89\u5168\u4ee3\u7801\u793a\u4f8b\uff0c\u6307\u4ee4\u5fae\u8c03LLMs\u4ee5\u63d0\u5347\u5b89\u5168\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u5728CWEBench\u548cCWEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u5b89\u5168\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5e38\u4ea7\u751f\u4e0d\u5b89\u5168\u4ee3\u7801\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u96c6\u6709\u9650\u4e14\u4e0d\u5e73\u8861\u800c\u6548\u679c\u53d7\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5b89\u5168\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faSecure-Instruct\u6846\u67b6\uff0c\u81ea\u52a8\u5408\u6210\u6f0f\u6d1e\u548c\u5b89\u5168\u4ee3\u7801\u793a\u4f8b\uff0c\u751f\u6210\u5fae\u8c03\u6307\u4ee4\uff0c\u5e76\u5bf9LLMs\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\uff0c\u4f7f\u5176\u4efb\u52a1\u63cf\u8ff0\u4e0e\u5b89\u5168\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5bf9\u9f50\u3002", "result": "\u5728CWEBench\u4e0a\uff0cSecure-Instruct\u4f7f\u5b89\u5168\u4ee3\u7801\u751f\u6210\u5e73\u5747\u63d0\u534714.3%\uff0c\u4f18\u4e8eSafeCoder 7.6%\uff1b\u5728CWEval\u4e0a\uff0cCodeLlama-7B\u548cMistral-7B\u7684Func-Sec@1\u5206\u522b\u63d0\u534714%\u548c5.8%\uff0c\u4f18\u4e8eSafeCoder 15.8%\u548c6.8%\u3002", "conclusion": "Secure-Instruct\u80fd\u663e\u8457\u63d0\u5347LLMs\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4e3a\u89e3\u51b3\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2510.06587", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06587", "abs": "https://arxiv.org/abs/2510.06587", "authors": ["Jingbo Yang", "Bairu Hou", "Wei Wei", "Shiyu Chang", "Yujia Bao"], "title": "WebDART: Dynamic Decomposition and Re-planning for Complex Web Tasks", "comment": null, "summary": "Large language model (LLM) agents are becoming competent at straightforward\nweb tasks, such as opening an item page or submitting a form, but still\nstruggle with objectives that require long horizon navigation, large scale\ninformation extraction, and reasoning under constraints. We present WebDART, a\ngeneral framework that enables a single LLM to handle such complex chores.\nWebDART (i) dynamically decomposes each objective into three focused subtasks:\nnavigation, information extraction, and execution, so the model concentrates on\none skill at a time, and (ii) continuously replans the decomposition as new\nwebpages are revealed, taking advantage of newly discovered filters or\nshortcuts and avoiding redundant exploration. Evaluated on WebChoreArena,\nWebDART lifts success rates by up to 13.7 percentage points over previous SOTA\nagents, while matching their performance on the easier WebArena suite and\ncompleting tasks with up to 14.7 fewer navigation steps.", "AI": {"tldr": "WebDART\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u89e3\u590d\u6742\u7f51\u9875\u4efb\u52a1\u4e3a\u5bfc\u822a\u3001\u4fe1\u606f\u63d0\u53d6\u548c\u6267\u884c\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u6301\u7eed\u91cd\u65b0\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u590d\u6742\u7f51\u9875\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u7b80\u5355\u7f51\u9875\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u957f\u7a0b\u5bfc\u822a\u3001\u5927\u89c4\u6a21\u4fe1\u606f\u63d0\u53d6\u548c\u7ea6\u675f\u63a8\u7406\u7684\u590d\u6742\u76ee\u6807\u4e0a\u4ecd\u6709\u56f0\u96be\u3002", "method": "WebDART\u6846\u67b6\uff1a(i) \u52a8\u6001\u5c06\u76ee\u6807\u5206\u89e3\u4e3a\u5bfc\u822a\u3001\u4fe1\u606f\u63d0\u53d6\u548c\u6267\u884c\u4e09\u4e2a\u4e13\u6ce8\u5b50\u4efb\u52a1\uff1b(ii) \u968f\u7740\u65b0\u7f51\u9875\u7684\u53d1\u73b0\u6301\u7eed\u91cd\u65b0\u89c4\u5212\u5206\u89e3\uff0c\u5229\u7528\u65b0\u53d1\u73b0\u7684\u8fc7\u6ee4\u5668\u6216\u6377\u5f84\uff0c\u907f\u514d\u5197\u4f59\u63a2\u7d22\u3002", "result": "\u5728WebChoreArena\u8bc4\u4f30\u4e2d\uff0cWebDART\u6bd4\u4e4b\u524dSOTA\u4ee3\u7406\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8613.7\u4e2a\u767e\u5206\u70b9\uff0c\u5728WebArena\u5957\u4ef6\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u5b8c\u6210\u4efb\u52a1\u6240\u9700\u7684\u5bfc\u822a\u6b65\u9aa4\u51cf\u5c11\u4e8614.7\u6b65\u3002", "conclusion": "WebDART\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u6301\u7eed\u91cd\u65b0\u89c4\u5212\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5904\u7406\u590d\u6742\u7f51\u9875\u4efb\u52a1\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.07147", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07147", "abs": "https://arxiv.org/abs/2510.07147", "authors": ["Arshika Lalan", "Rajat Ghosh", "Aditya Kolsur", "Debojyoti Dutta"], "title": "A Multi-Agent Framework for Stateful Inference-Time Search", "comment": null, "summary": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u4fdd\u6301\u7684\u591a\u667a\u80fd\u4f53\u8fdb\u5316\u641c\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9c81\u68d2\u7684\u5355\u5143\u6d4b\u8bd5\u8fb9\u754c\u60c5\u51b5\uff0c\u76f8\u6bd4\u65e0\u72b6\u6001\u65b9\u6cd5\u5728\u6d4b\u8bd5\u8986\u76d6\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65e0\u72b6\u6001\u63a8\u7406\u65b9\u6cd5\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5728\u6df1\u5c42\u63a8\u7406\u548c\u957f\u7a0b\u4f9d\u8d56\u4efb\u52a1\u4e0a\u7684\u8106\u5f31\u6027\u3002", "method": "\u7ed3\u5408\u6301\u4e45\u63a8\u7406\u72b6\u6001\u3001\u5bf9\u6297\u6027\u53d8\u5f02\u548c\u8fdb\u5316\u4fdd\u7559\u7684\u72b6\u6001\u4fdd\u6301\u591a\u667a\u80fd\u4f53\u8fdb\u5316\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u667a\u80fd\u4f53\u987a\u5e8f\u63d0\u51fa\u3001\u53d8\u5f02\u548c\u8bc4\u5206\u5019\u9009\u6848\u4f8b\u3002", "result": "\u5728HumanEval\u548cTestGenEvalMini\u7b49\u5355\u5143\u6d4b\u8bd5\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4\u65e0\u72b6\u6001\u5355\u6b65\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u8986\u76d6\u7387\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u7528Llama\u3001Gemma\u548cGPT\u4e09\u79cdLLM\u5bb6\u65cf\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u6301\u4e45\u63a8\u7406\u72b6\u6001\u4e0e\u8fdb\u5316\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u663e\u8457\u6539\u8fdb\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u8d28\u91cf\u548c\u8986\u76d6\u7387\u3002", "topic": "swe application"}}
{"id": "2510.07315", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07315", "abs": "https://arxiv.org/abs/2510.07315", "authors": ["Ming Zhong", "Xiang Zhou", "Ting-Yun Chang", "Qingze Wang", "Nan Xu", "Xiance Si", "Dan Garrette", "Shyam Upadhyay", "Jeremiah Liu", "Jiawei Han", "Benoit Schillings", "Jiao Sun"], "title": "Vibe Checker: Aligning Code Evaluation with Human Preference", "comment": "Preprint", "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.", "AI": {"tldr": "\u63d0\u51fa\u4e86VeriCode\uff0c\u4e00\u4e2a\u5305\u542b30\u79cd\u53ef\u9a8c\u8bc1\u4ee3\u7801\u6307\u4ee4\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u4ee3\u7801\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u5e76\u521b\u5efa\u4e86Vibe Checker\u6d4b\u8bd5\u5e73\u53f0\u6765\u540c\u65f6\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\u548c\u6307\u4ee4\u9075\u5faa\u6027\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff08pass@k\uff09\uff0c\u4f46\u5ffd\u89c6\u4e86\u7528\u6237\u5728\u5b9e\u9645\u7f16\u7a0b\u4e2d\u7ecf\u5e38\u4f7f\u7528\u7684\u975e\u529f\u80fd\u6027\u6307\u4ee4\uff0c\u8fd9\u4e9b\u6307\u4ee4\u4f53\u73b0\u4e86\u4eba\u7c7b\u504f\u597d\uff08vibe check\uff09\u3002", "method": "\u5f00\u53d1\u4e86VeriCode\u5206\u7c7b\u6cd5\uff0c\u5305\u542b30\u79cd\u53ef\u9a8c\u8bc1\u4ee3\u7801\u6307\u4ee4\u548c\u76f8\u5e94\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u5668\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u73b0\u6709\u8bc4\u4f30\u5957\u4ef6\u4e2d\u521b\u5efaVibe Checker\u6d4b\u8bd5\u5e73\u53f0\u3002", "result": "\u8bc4\u4f3031\u4e2a\u9886\u5148LLM\u540e\u53d1\u73b0\uff0c\u5373\u4f7f\u6700\u5f3a\u6a21\u578b\u4e5f\u96be\u4ee5\u9075\u5faa\u591a\u4e2a\u6307\u4ee4\u5e76\u8868\u73b0\u51fa\u660e\u663e\u7684\u529f\u80fd\u56de\u5f52\u3002\u529f\u80fd\u6b63\u786e\u6027\u548c\u6307\u4ee4\u9075\u5faa\u6027\u7684\u7efc\u5408\u5f97\u5206\u4e0e\u4eba\u7c7b\u504f\u597d\u76f8\u5173\u6027\u6700\u597d\u3002", "conclusion": "\u6307\u4ee4\u9075\u5faa\u662fvibe check\u4e2d\u4f53\u73b0\u4eba\u7c7b\u504f\u597d\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u5f00\u53d1\u66f4\u7b26\u5408\u7528\u6237\u7f16\u7a0b\u504f\u597d\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\u3002", "topic": "swe benchmark"}}
{"id": "2510.06711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06711", "abs": "https://arxiv.org/abs/2510.06711", "authors": ["Batu El", "Mert Yuksekgonul", "James Zou"], "title": "Inefficiencies of Meta Agents for Agent Design", "comment": null, "summary": "Recent works began to automate the design of agentic systems using\nmeta-agents that propose and iteratively refine new agent architectures. In\nthis paper, we examine three key challenges in a common class of meta-agents.\nFirst, we investigate how a meta-agent learns across iterations and find that\nsimply expanding the context with all previous agents, as proposed by previous\nworks, performs worse than ignoring prior designs entirely. We show that the\nperformance improves with an evolutionary approach. Second, although the\nmeta-agent designs multiple agents during training, it typically commits to a\nsingle agent at test time. We find that the designed agents have low behavioral\ndiversity, limiting the potential for their complementary use. Third, we assess\nwhen automated design is economically viable. We find that only in a few\ncases--specifically, two datasets--the overall cost of designing and deploying\nthe agents is lower than that of human-designed agents when deployed on over\n15,000 examples. In contrast, the performance gains for other datasets do not\njustify the design cost, regardless of scale.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5143\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u65f6\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u8de8\u8fed\u4ee3\u5b66\u4e60\u6548\u7387\u3001\u884c\u4e3a\u591a\u6837\u6027\u4e0d\u8db3\u4ee5\u53ca\u7ecf\u6d4e\u53ef\u884c\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5143\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u65f6\u9762\u4e34\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u5305\u62ec\u5b66\u4e60\u6548\u7387\u3001\u884c\u4e3a\u591a\u6837\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u5b66\u4e60\u7b56\u7565\uff08\u5982\u4e0a\u4e0b\u6587\u6269\u5c55vs\u8fdb\u5316\u65b9\u6cd5\uff09\uff0c\u8bc4\u4f30\u4ee3\u7406\u884c\u4e3a\u591a\u6837\u6027\uff0c\u5206\u6790\u8bbe\u8ba1\u6210\u672c\u4e0e\u6027\u80fd\u6536\u76ca\u7684\u7ecf\u6d4e\u5e73\u8861\u3002", "result": "\u53d1\u73b0\u8fdb\u5316\u65b9\u6cd5\u4f18\u4e8e\u7b80\u5355\u4e0a\u4e0b\u6587\u6269\u5c55\uff1b\u8bbe\u8ba1\u7684\u4ee3\u7406\u884c\u4e3a\u591a\u6837\u6027\u4f4e\uff1b\u4ec5\u5728\u5c11\u6570\u6570\u636e\u96c6\u4e0a\u81ea\u52a8\u5316\u8bbe\u8ba1\u5177\u6709\u7ecf\u6d4e\u53ef\u884c\u6027\u3002", "conclusion": "\u5f53\u524d\u5143\u4ee3\u7406\u65b9\u6cd5\u5728\u8de8\u8fed\u4ee3\u5b66\u4e60\u3001\u884c\u4e3a\u591a\u6837\u6027\u548c\u7ecf\u6d4e\u53ef\u884c\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u9700\u8981\u6539\u8fdb\u624d\u80fd\u5b9e\u73b0\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.06756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06756", "abs": "https://arxiv.org/abs/2510.06756", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Verifying Memoryless Sequential Decision-making of Large Language Models", "comment": null, "summary": "We introduce a tool for rigorous and automated verification of large language\nmodel (LLM)- based policies in memoryless sequential decision-making tasks.\nGiven a Markov decision process (MDP) representing the sequential\ndecision-making task, an LLM policy, and a safety requirement expressed as a\nPCTL formula, our approach incrementally constructs only the reachable portion\nof the MDP guided by the LLM's chosen actions. Each state is encoded as a\nnatural language prompt, the LLM's response is parsed into an action, and\nreachable successor states by the policy are expanded. The resulting formal\nmodel is checked with Storm to determine whether the policy satisfies the\nspecified safety property. In experiments on standard grid world benchmarks, we\nshow that open source LLMs accessed via Ollama can be verified when\ndeterministically seeded, but generally underperform deep reinforcement\nlearning baselines. Our tool natively integrates with Ollama and supports\nPRISM-specified tasks, enabling continuous benchmarking in user-specified\nsequential decision-making tasks and laying a practical foundation for formally\nverifying increasingly capable LLMs.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a8c\u8bc1\u57fa\u4e8eLLM\u7b56\u7565\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u589e\u91cf\u6784\u5efaMDP\u7684\u53ef\u8fbe\u90e8\u5206\u6765\u68c0\u67e5LLM\u7b56\u7565\u662f\u5426\u6ee1\u8db3PCTL\u5b89\u5168\u8981\u6c42\u3002", "motivation": "\u968f\u7740LLM\u5728\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u4e25\u683c\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u7b56\u7565\u7684\u5b89\u5168\u6027\uff0c\u786e\u4fdd\u5b83\u4eec\u6ee1\u8db3\u6307\u5b9a\u7684\u5b89\u5168\u5c5e\u6027\u3002", "method": "\u7ed9\u5b9aMDP\u3001LLM\u7b56\u7565\u548cPCTL\u5b89\u5168\u8981\u6c42\uff0c\u589e\u91cf\u6784\u5efaMDP\u7684\u53ef\u8fbe\u90e8\u5206\uff0c\u5c06\u72b6\u6001\u7f16\u7801\u4e3a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u89e3\u6790LLM\u54cd\u5e94\u4e3a\u52a8\u4f5c\uff0c\u6269\u5c55\u53ef\u8fbe\u540e\u7ee7\u72b6\u6001\uff0c\u6700\u540e\u4f7f\u7528Storm\u6a21\u578b\u68c0\u67e5\u5668\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7Ollama\u8bbf\u95ee\u7684\u5f00\u6e90LLM\u5728\u786e\u5b9a\u6027\u79cd\u5b50\u4e0b\u53ef\u4ee5\u88ab\u9a8c\u8bc1\uff0c\u4f46\u6027\u80fd\u901a\u5e38\u4f4e\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u6b63\u5f0f\u9a8c\u8bc1\u65e5\u76ca\u5f3a\u5927\u7684LLM\u5960\u5b9a\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u652f\u6301PRISM\u6307\u5b9a\u4efb\u52a1\u7684\u8fde\u7eed\u57fa\u51c6\u6d4b\u8bd5\u3002", "topic": "agent analysis"}}
{"id": "2510.06878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06878", "abs": "https://arxiv.org/abs/2510.06878", "authors": ["Daria Ozerova", "Ekaterina Trofimova"], "title": "TGPR: Tree-Guided Policy Refinement for Robust Self-Debugging of LLMs", "comment": null, "summary": "Iterative refinement has been a promising paradigm to enable large language\nmodels (LLMs) to resolve difficult reasoning and problem-solving tasks. One of\nthe key challenges, however, is how to effectively search through the enormous\nsearch space of possible refinements. Existing methods typically fall back on\npredefined heuristics, which are troubled by the exploration-exploitation\ndilemma and cannot adapt based on past refinement outcomes. We introduce\nTree-Guided Policy Refinement (TGPR), a novel framework that combines GRPO with\na Thompson-Sampling-based tree search. TGPR explores both failed and successful\nrefinement paths actively, with denser training trajectories and more adaptive\npolicies. On HumanEval, MBPP, and APPS benchmarks, our method achieves up to\n+4.2 percentage points absolute improvement in pass@1 (on MBPP) and up to\n+12.51 percentage points absolute improvement in pass@10 (on APPS) compared to\na competitive GRPO baseline. Apart from debugging code, TGPR focuses on a\nprincipled approach to combining learned policies with structured search\nmethods, offering a general framework for enhancing iterative refinement and\nstateful reasoning in LLMs.", "AI": {"tldr": "TGPR\u7ed3\u5408GRPO\u4e0eThompson\u91c7\u6837\u6811\u641c\u7d22\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u5931\u8d25\u548c\u6210\u529f\u7684\u7cbe\u5316\u8def\u5f84\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fed\u4ee3\u7cbe\u5316\u8fc7\u7a0b\u4e2d\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\u7684\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u9884\u5b9a\u4e49\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u65e0\u6cd5\u6839\u636e\u5386\u53f2\u7cbe\u5316\u7ed3\u679c\u81ea\u9002\u5e94\u8c03\u6574", "method": "\u63d0\u51faTree-Guided Policy Refinement (TGPR)\u6846\u67b6\uff0c\u5c06GRPO\u4e0e\u57fa\u4e8eThompson\u91c7\u6837\u7684\u6811\u641c\u7d22\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u66f4\u5bc6\u96c6\u7684\u8bad\u7ec3\u8f68\u8ff9\u548c\u81ea\u9002\u5e94\u7b56\u7565", "result": "\u5728HumanEval\u3001MBPP\u548cAPPS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4GRPO\u57fa\u7ebf\uff0cpass@1\u5728MBPP\u4e0a\u63d0\u53474.2\u4e2a\u767e\u5206\u70b9\uff0cpass@10\u5728APPS\u4e0a\u63d0\u534712.51\u4e2a\u767e\u5206\u70b9", "conclusion": "TGPR\u4e3a\u7ed3\u5408\u5b66\u4e60\u7b56\u7565\u4e0e\u7ed3\u6784\u5316\u641c\u7d22\u65b9\u6cd5\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u662f\u589e\u5f3aLLMs\u8fed\u4ee3\u7cbe\u5316\u548c\u72b6\u6001\u63a8\u7406\u7684\u901a\u7528\u6846\u67b6", "topic": "code agent"}}
{"id": "2510.06953", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06953", "abs": "https://arxiv.org/abs/2510.06953", "authors": ["Minju Gwak", "Guijin Son", "Jaehyung Kim"], "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces", "comment": null, "summary": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fe1\u606f\u5bc6\u5ea6\u7684\u5747\u5300\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u6b65\u8fdb\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u53d1\u73b0\u63a8\u7406\u8d28\u91cf\u4e0e\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u7edf\u4e00\u4fe1\u606f\u5bc6\u5ea6\u5047\u8bbe\u5728LLM\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7a76\u6b65\u7ea7\u5747\u5300\u6027\u662f\u5426\u53cd\u6620\u63a8\u7406\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u6b65\u8fdb\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5f15\u5165\u5c40\u90e8\u548c\u5168\u5c40\u5747\u5300\u6027\u8bc4\u5206\u4e24\u4e2a\u4e92\u8865\u6307\u6807\u3002", "result": "\u5728\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6b65\u7ea7\u5747\u5300\u6027\u4e0d\u4ec5\u63d0\u4f9b\u7406\u8bba\u89c6\u89d2\uff0c\u8fd8\u5e26\u6765\u5b9e\u9645\u6027\u80fd\u63d0\u5347\uff1a\u9009\u62e9\u4fe1\u606f\u5bc6\u5ea6\u66f4\u5747\u5300\u7684\u63a8\u7406\u8f68\u8ff9\u5728AIME2025\u4e0a\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534710-32%\u51c6\u786e\u7387\u3002\u6b63\u786e\u63a8\u7406\u8f68\u8ff9\u907f\u514d\u4fe1\u606f\u5bc6\u5ea6\u5c16\u5cf0\uff0c\u9519\u8bef\u8f68\u8ff9\u5219\u5448\u73b0\u4e0d\u89c4\u5219\u4fe1\u606f\u7206\u53d1\u3002", "conclusion": "UID\u542f\u53d1\u7684\u4fe1\u606f\u5bc6\u5ea6\u5ea6\u91cf\u4f18\u4e8e\u5176\u4ed6\u5185\u90e8\u4fe1\u53f7\u4f5c\u4e3a\u63a8\u7406\u8d28\u91cf\u9884\u6d4b\u6307\u6807\uff0c\u4fe1\u606f\u5bc6\u5ea6\u5747\u5300\u6027\u662f\u6784\u5efa\u66f4\u53ef\u9760\u51c6\u786e\u63a8\u7406\u7cfb\u7edf\u7684\u7a33\u5065\u8bca\u65ad\u548c\u9009\u62e9\u6807\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2510.06445", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06445", "abs": "https://arxiv.org/abs/2510.06445", "authors": ["Asif Shahriar", "Md Nafiu Rahman", "Sadif Ahmed", "Farig Sadeque", "Md Rizwan Parvez"], "title": "A Survey on Agentic Security: Applications, Threats and Defenses", "comment": null, "summary": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new\nparadigm in cybersecurity. While these agents can act as powerful tools for\nboth offensive and defensive operations, the very agentic context introduces a\nnew class of inherent security risks. In this work we present the first\nholistic survey of the agentic security landscape, structuring the field around\nthree interdependent pillars: Applications, Threats, and Defenses. We provide a\ncomprehensive taxonomy of over 150 papers, explaining how agents are used, the\nvulnerabilities they possess, and the countermeasures designed to protect them.\nA detailed cross-cutting analysis shows emerging trends in agent architecture\nwhile revealing critical research gaps in model and modality coverage.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9LLM\u667a\u80fd\u4f53\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5b89\u5168\u6001\u52bf\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u56f4\u7ed5\u5e94\u7528\u3001\u5a01\u80c1\u548c\u9632\u5fa1\u4e09\u4e2a\u76f8\u4e92\u4f9d\u5b58\u7684\u652f\u67f1\u6784\u5efa\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u6846\u67b6\u3002", "motivation": "\u968f\u7740LLM\u4ece\u88ab\u52a8\u5de5\u5177\u5411\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u5feb\u901f\u8f6c\u53d8\uff0c\u8fd9\u79cd\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u5f15\u5165\u4e86\u4e00\u7c7b\u65b0\u7684\u56fa\u6709\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u667a\u80fd\u4f53\u5b89\u5168\u6001\u52bf\u3002", "method": "\u901a\u8fc7\u5bf9150\u591a\u7bc7\u8bba\u6587\u8fdb\u884c\u7efc\u5408\u5206\u7c7b\uff0c\u6784\u5efa\u4e86\u56f4\u7ed5\u5e94\u7528\u3001\u5a01\u80c1\u548c\u9632\u5fa1\u4e09\u4e2a\u652f\u67f1\u7684\u5168\u9762\u5206\u7c7b\u6cd5\uff0c\u5e76\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u4ea4\u53c9\u5206\u6790\u3002", "result": "\u8c03\u67e5\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u67b6\u6784\u7684\u65b0\u5174\u8d8b\u52bf\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u6a21\u578b\u548c\u6a21\u6001\u8986\u76d6\u65b9\u9762\u7684\u5173\u952e\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3LLM\u667a\u80fd\u4f53\u5728\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u5b89\u5168\u6001\u52bf\u63d0\u4f9b\u4e86\u9996\u4e2a\u6574\u4f53\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2510.07038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07038", "abs": "https://arxiv.org/abs/2510.07038", "authors": ["Wenxun Wu", "Yuanyang Li", "Guhan Chen", "Linyue Wang", "Hongyang Chen"], "title": "Tool-Augmented Policy Optimization: Synergizing Reasoning and Adaptive Tool Use with Reinforcement Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have popularized test-time\nscaling, where models generate additional reasoning tokens before producing\nfinal answers. These approaches have demonstrated significant performance\nimprovements on benchmarks involving mathematical reasoning. However, language\nmodels relying solely on direct inference still struggle with tasks demanding\nup-to-date knowledge or computational tools such as calculators and code\ninterpreters for complex arithmetic operations. To overcome these limitations,\nwe propose Tool-Augmented Policy Optimization (TAPO), a novel reinforcement\nlearning framework that systematically integrates multi-hop reasoning with\nadaptive tool-calling capabilities. Our approach employs a modified version of\nDynamic Sampling Policy Optimization (DAPO), a recently developed RL paradigm,\nwhich we adapt specifically for tool invocation scenarios, enabling models to\ndynamically interleave complex reasoning with on-demand tool usage (including\nsearch APIs and Python interpreters).\n  To support this research, we introduce two new datasets: TAPO-easy-60K and\nTAPO-hard-18K, specifically designed to train and evaluate both fact-based\nreasoning and mathematical calculation capabilities. Our experiments on\nQwen2.5-3B and Qwen2.5-7B models demonstrate the effectiveness of our approach,\nwith both models achieving state-of-the-art performance on tasks requiring\nexternal knowledge and mathematical computation among methods with comparable\nparameters. Notably, TAPO achieves more efficient tool utilization than\nbaseline methods while preventing excessive calls caused by reward hacking.\nThese results highlight the significant potential of combining advanced\nreasoning with tool usage to enhance model performance in knowledge-intensive\nand computationally demanding tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86TAPO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u591a\u8df3\u63a8\u7406\u4e0e\u81ea\u9002\u5e94\u5de5\u5177\u8c03\u7528\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u590d\u6742\u8ba1\u7b97\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u6700\u65b0\u77e5\u8bc6\u6216\u8ba1\u7b97\u5de5\u5177\uff08\u5982\u8ba1\u7b97\u5668\u3001\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u7684\u590d\u6742\u7b97\u672f\u8fd0\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684\u52a8\u6001\u91c7\u6837\u7b56\u7565\u4f18\u5316(DAPO)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u5de5\u5177\u8c03\u7528\u573a\u666f\u8bbe\u8ba1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u52a8\u6001\u5730\u5728\u590d\u6742\u63a8\u7406\u548c\u6309\u9700\u5de5\u5177\u4f7f\u7528\u4e4b\u95f4\u5207\u6362\u3002", "result": "\u5728Qwen2.5-3B\u548cQwen2.5-7B\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cTAPO\u5728\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u548c\u6570\u5b66\u8ba1\u7b97\u7684\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u5229\u7528\u7387\u3002", "conclusion": "\u7ed3\u5408\u9ad8\u7ea7\u63a8\u7406\u4e0e\u5de5\u5177\u4f7f\u7528\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07064", "abs": "https://arxiv.org/abs/2510.07064", "authors": ["Manh Hung Nguyen", "Sebastian Tschiatschek", "Adish Singla"], "title": "Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations", "comment": null, "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u6a21\u4f18\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4e00\u7ec4LLM\u4ee3\u7406\u6765\u96c6\u4f53\u6355\u6349\u4eba\u7c7b\u7fa4\u4f53\u7684\u591a\u6837\u6027\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u5355\u4e00LLM\u4ee3\u7406\u3002", "motivation": "\u7531\u4e8e\u83b7\u53d6\u5927\u89c4\u6a21\u4eba\u7c7b\u54cd\u5e94\u7684\u56f0\u96be\u548c\u6210\u672c\uff0cLLMs\u6210\u4e3a\u4eba\u7c7b\u884c\u4e3a\u7684\u66ff\u4ee3\u54c1\uff0c\u4f46\u73b0\u6709LLMs\u8f93\u51fa\u540c\u8d28\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u89c2\u70b9\u548c\u884c\u4e3a\u7684\u4e30\u5bcc\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5c06\u6bcf\u4e2aLLM\u4ee3\u7406\u7684\u884c\u4e3a\u5f15\u5bfc\u4e3a\u57fa\u4e8e\u5c11\u91cf\u4eba\u7c7b\u6f14\u793a\uff08\u4efb\u52a1-\u54cd\u5e94\u5bf9\uff09\u7684\u6761\u4ef6\u751f\u6210\u3002\u4f7f\u7528\u5b50\u6a21\u4f18\u5316\u65b9\u6cd5\u4ece\u6307\u6570\u7ea7\u5927\u7684\u53ef\u80fd\u4ee3\u7406\u7a7a\u95f4\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u4ee3\u7406\u96c6\u3002", "result": "\u5728\u4f17\u5305\u548c\u6559\u80b2\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u7684\u4ee3\u7406\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4ee3\u8868\u4eba\u7c7b\u7fa4\u4f53\u3002\u884c\u4e3a\u5206\u6790\u663e\u793a\u8fd9\u4e9b\u4ee3\u7406\u80fd\u591f\u91cd\u73b0\u6240\u4ee3\u8868\u5b66\u751f\u548c\u6ce8\u91ca\u8005\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u89c2\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6784\u5efa\u4e00\u7ec4LLM\u4ee3\u7406\uff0c\u6709\u6548\u6355\u6349\u4eba\u7c7b\u7fa4\u4f53\u7684\u591a\u6837\u6027\uff0c\u5e76\u5728\u65b0\u4efb\u52a1\u4e0a\u91cd\u73b0\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2510.06499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06499", "abs": "https://arxiv.org/abs/2510.06499", "authors": ["Zhepeng Cen", "Haolin Chen", "Shiyu Wang", "Zuxin Liu", "Zhiwei Liu", "Ding Zhao", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success through\nimitation learning on vast text corpora, but this paradigm creates a\ntraining-generation gap and limits robust reasoning. Reinforcement learning\n(RL) offers a more data-efficient solution capable of bridging this gap, yet\nits application has been constrained by a critical data bottleneck: existing RL\ndatasets are orders of magnitude smaller and less diverse than web-scale\npre-training corpora. To address this, we introduce the Webscale-RL pipeline, a\nscalable data engine that systematically converts large-scale pre-training\ndocuments into millions of diverse, verifiable question-answer pairs for RL.\nUsing this pipeline, we construct the Webscale-RL dataset, containing 1.2\nmillion examples across more than 9 domains. Our experiments show that the\nmodel trained on this dataset significantly outperforms continual pretraining\nand strong data refinement baselines across a suite of benchmarks. Notably, RL\ntraining with our dataset proves substantially more efficient, achieving the\nperformance of continual pre-training with up to 100$\\times$ fewer tokens. Our\nwork presents a viable path toward scaling RL to pre-training levels, enabling\nmore capable and efficient language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86Webscale-RL\u7ba1\u9053\u548c\u6570\u636e\u96c6\uff0c\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6587\u6863\u8f6c\u5316\u4e3a120\u4e07\u4e2a\u591a\u6837\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLMs\u6a21\u4eff\u5b66\u4e60\u8303\u5f0f\u5b58\u5728\u7684\u8bad\u7ec3-\u751f\u6210\u5dee\u8ddd\u548c\u63a8\u7406\u80fd\u529b\u9650\u5236\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709RL\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4e0d\u8db3\u7684\u6570\u636e\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684Webscale-RL\u6570\u636e\u7ba1\u9053\uff0c\u7cfb\u7edf\u5730\u5c06\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6587\u6863\u8f6c\u5316\u4e3a\u6570\u767e\u4e07\u4e2a\u591a\u6837\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u4e86\u5305\u542b120\u4e07\u4e2a\u6837\u672c\u3001\u8986\u76d69\u4e2a\u4ee5\u4e0a\u9886\u57df\u7684Webscale-RL\u6570\u636e\u96c6\u3002", "result": "\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5f3a\u6570\u636e\u7cbe\u70bc\u57fa\u7ebf\uff0cRL\u8bad\u7ec3\u6548\u7387\u5927\u5e45\u63d0\u5347\uff0c\u8fbe\u5230\u6301\u7eed\u9884\u8bad\u7ec3\u6027\u80fd\u6240\u9700\u7684token\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe100\u500d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06RL\u6269\u5c55\u5230\u9884\u8bad\u7ec3\u89c4\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u80fd\u591f\u5f00\u53d1\u51fa\u66f4\u5f3a\u5927\u3001\u66f4\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.06552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06552", "abs": "https://arxiv.org/abs/2510.06552", "authors": ["Tarek Naous", "Philippe Laban", "Wei Xu", "Jennifer Neville"], "title": "Flipping the Dialogue: Training and Evaluating User Language Models", "comment": null, "summary": "Conversations with LMs involve two participants: a human user leading the\nconversation, and an LM assistant responding to the user's request. To satisfy\nthis specific role, LMs are post-trained to be helpful assistants -- optimized\nto produce exhaustive and well-structured responses, free of ambiguity and\ngrammar errors. User utterances, on the other hand, are rarely perfected, with\neach user phrasing requests in unique ways, sometimes putting in partial effort\nat each turn and refining on the fly. To evaluate LM performance in realistic\nsettings, prior work simulated users in multi-turn conversations, often\nprompting an LLM originally trained to be a helpful assistant to act as a user.\nHowever, we show that assistant LMs make for poor user simulators, with the\nsurprising finding that better assistants yield worse simulators. Instead, we\nintroduce purpose-built User Language Models (User LMs) - models post-trained\nto simulate human users in multi-turn conversations. Through various\nevaluations, we show how User LMs align better with human behavior and achieve\nbetter simulation robustness than existing simulation methods. When leveraging\nUser LMs to simulate coding and math conversations, the performance of a strong\nassistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic\nsimulation environments lead to assistant struggles as they fail to cope with\nthe nuances of users in multi-turn setups.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u539f\u672c\u4f5c\u4e3a\u52a9\u624b\u8bad\u7ec3\u7684LLM\u4e0d\u9002\u5408\u6a21\u62df\u771f\u5b9e\u7528\u6237\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u8bad\u7ec3\u7684\u7528\u6237\u8bed\u8a00\u6a21\u578b(User LMs)\u6765\u66f4\u51c6\u786e\u5730\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7528\u6237\u884c\u4e3a\uff0c\u5e76\u8bc1\u660e\u8fd9\u79cd\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u4f1a\u663e\u8457\u964d\u4f4e\u52a9\u624b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u52a9\u624bLLM\u6765\u6a21\u62df\u7528\u6237\u5bf9\u8bdd\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u7ecf\u8fc7\u8bad\u7ec3\u6210\u4e3a\u6709\u5e2e\u52a9\u7684\u52a9\u624b\uff0c\u4e0e\u771f\u5b9e\u7528\u6237\u7684\u884c\u4e3a\u6a21\u5f0f\u5b58\u5728\u5dee\u5f02\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u6a21\u62df\u4eba\u7c7b\u7528\u6237\u884c\u4e3a\u7684\u6a21\u578b\u6765\u521b\u5efa\u66f4\u771f\u5b9e\u7684\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u73af\u5883\u3002", "method": "\u63d0\u51fa\u7528\u6237\u8bed\u8a00\u6a21\u578b(User LMs)\uff0c\u901a\u8fc7\u5bf9\u6a21\u578b\u8fdb\u884c\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\uff0c\u4f7f\u5176\u80fd\u591f\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u771f\u5b9e\u7528\u6237\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5305\u62ec\u4e0d\u5b8c\u7f8e\u7684\u8868\u8fbe\u3001\u9010\u6b65\u7ec6\u5316\u8bf7\u6c42\u7b49\u7279\u5f81\u3002", "result": "User LMs\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u52a9\u624bLLM\u6a21\u62df\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u597d\u7684\u4eff\u771f\u9c81\u68d2\u6027\u3002\u5728\u7f16\u7a0b\u548c\u6570\u5b66\u5bf9\u8bdd\u6a21\u62df\u4e2d\uff0c\u5f3a\u52a9\u624b\u6a21\u578b(GPT-4o)\u7684\u6027\u80fd\u4ece74.6%\u4e0b\u964d\u523057.4%\uff0c\u8868\u660e\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u4f1a\u66b4\u9732\u52a9\u624b\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e0d\u8db3\u3002", "conclusion": "\u4e13\u95e8\u8bad\u7ec3\u7684\u7528\u6237\u6a21\u62df\u6a21\u578b\u6bd4\u4f7f\u7528\u52a9\u624bLLM\u4f5c\u4e3a\u7528\u6237\u6a21\u62df\u5668\u66f4\u6709\u6548\uff0c\u80fd\u521b\u5efa\u66f4\u771f\u5b9e\u7684\u5bf9\u8bdd\u8bc4\u4f30\u73af\u5883\uff0c\u63ed\u793a\u52a9\u624b\u6a21\u578b\u5728\u5b9e\u9645\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.07117", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07117", "abs": "https://arxiv.org/abs/2510.07117", "authors": ["Leonardo Christov-Moore", "Arthur Juliani", "Alex Kiefer", "Nicco Reggente", "B. Scott Rousse", "Adam Safron", "Nicol'as Hinrichs", "Daniel Polani", "Antonio Damasio"], "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and Care", "comment": "15 pages, 1 figure", "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u6d77\u5fb7\u683c\u5c14\u5b58\u5728\u4e3b\u4e49\u73b0\u8c61\u5b66\u7684\u7269\u7406\u5177\u8eab\u667a\u80fd\u4f53\u4e24\u4e2a\u6700\u5c0f\u6761\u4ef6\uff1a\u5728\u4e16\u5b58\u5728\u548c\u5411\u6b7b\u800c\u751f\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u53d1\u5c55\u51fa\u7a33\u6001\u9a71\u52a8\u529b\u548c\u5185\u5728\u9a71\u52a8\u529b\uff0c\u4ee5\u589e\u5f3a\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u5173\u6000\u80fd\u529b\u3002", "motivation": "\u7406\u89e3\u751f\u7269\u4f53\u5728\u5f00\u653e\u7269\u7406\u4e16\u754c\u4e2d\u751f\u5b58\u548c\u5173\u6000\u7684\u673a\u5236\uff0c\u4ee5\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u5177\u6709\u5173\u6000\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u4f53\u3002", "method": "\u57fa\u4e8e\u6d77\u5fb7\u683c\u5c14\u5b58\u5728\u4e3b\u4e49\u73b0\u8c61\u5b66\u5b9a\u4e49\u4e24\u4e2a\u6700\u5c0f\u7269\u7406\u5177\u8eab\u6761\u4ef6\uff0c\u7ed3\u5408\u5c3c\u91c7\u7684\u6743\u529b\u610f\u5fd7\u6982\u5ff5\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u5f62\u5f0f\u5316\u8fd9\u4e9b\u6982\u5ff5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u5b58\u5728\u4e3b\u4e49\u54f2\u5b66\u6982\u5ff5\u8f6c\u5316\u4e3a\u53ef\u8ba1\u7b97\u7684\u667a\u80fd\u4f53\u9a71\u52a8\u529b\uff0c\u7528\u4e8e\u589e\u5f3a\u667a\u80fd\u4f53\u7684\u5f00\u653e\u6027\u548c\u5173\u6000\u80fd\u529b\u3002", "conclusion": "\u4ece\u5b58\u5728\u4e3b\u4e49\u54f2\u5b66\u4e2d\u6c72\u53d6\u7684\u7269\u7406\u5177\u8eab\u6761\u4ef6\u53ef\u4ee5\u4e3a\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u548c\u5173\u6000\u80fd\u529b\u7684\u4eba\u5de5\u667a\u80fd\u4f53\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.06545", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06545", "abs": "https://arxiv.org/abs/2510.06545", "authors": ["Jacek Karwowski", "Raymond Douglas"], "title": "Incoherence in goal-conditioned autoregressive models", "comment": null, "summary": "We investigate mathematically the notion of incoherence: a structural issue\nwith reinforcement learning policies derived by naive goal-conditioning of\nautoregressive models. We focus on the process of re-training models on their\nown actions, that is, fine-tuning offline-learned policies with online RL. We\nprove that it decreases incoherence and leads to an improvement in return, and\nwe aim to characterize the resulting trajectory of policies. By re-framing\nstandard notions of control-as-inference and soft Q learning, we establish a\nthree-way correspondence with two other ways of understanding the iterative\nre-training process: as folding the posterior into the reward and, in the\ndeterministic case, as decreasing the temperature parameter; the correspondence\nhas computational content via the training-inference trade-off. Through\nsoft-conditioning generative models, we discuss the link between incoherence\nand the effective horizon.", "AI": {"tldr": "\u672c\u6587\u4ece\u6570\u5b66\u89d2\u5ea6\u5206\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u7814\u7a76\u4e86\u901a\u8fc7\u5728\u7ebfRL\u5fae\u8c03\u79bb\u7ebf\u5b66\u4e60\u7b56\u7565\u7684\u8fc7\u7a0b\uff0c\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u80fd\u51cf\u5c11\u4e0d\u4e00\u81f4\u6027\u5e76\u63d0\u9ad8\u56de\u62a5\uff0c\u5efa\u7acb\u4e86\u63a7\u5236\u5373\u63a8\u7406\u3001\u8f6fQ\u5b66\u4e60\u4e0e\u8fed\u4ee3\u91cd\u8bad\u7ec3\u8fc7\u7a0b\u7684\u4e09\u5411\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4e2d\u7531\u81ea\u56de\u5f52\u6a21\u578b\u7b80\u5355\u76ee\u6807\u6761\u4ef6\u5316\u5f15\u8d77\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u901a\u8fc7\u5728\u7ebfRL\u5fae\u8c03\u79bb\u7ebf\u7b56\u7565\u6765\u6539\u8fdb\u7b56\u7565\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6570\u5b66\u5206\u6790\u65b9\u6cd5\uff0c\u91cd\u65b0\u6784\u5efa\u63a7\u5236\u5373\u63a8\u7406\u548c\u8f6fQ\u5b66\u4e60\u7684\u6807\u51c6\u6982\u5ff5\uff0c\u5efa\u7acb\u4e09\u5411\u5bf9\u5e94\u5173\u7cfb\uff0c\u5206\u6790\u8fed\u4ee3\u91cd\u8bad\u7ec3\u8fc7\u7a0b\u4f5c\u4e3a\u5c06\u540e\u9a8c\u6298\u53e0\u5230\u5956\u52b1\u4e2d\u7684\u65b9\u5f0f\uff0c\u5e76\u5728\u786e\u5b9a\u6027\u60c5\u51b5\u4e0b\u4f5c\u4e3a\u964d\u4f4e\u6e29\u5ea6\u53c2\u6570\u7684\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u7ebfRL\u5fae\u8c03\u80fd\u51cf\u5c11\u7b56\u7565\u4e0d\u4e00\u81f4\u6027\u5e76\u63d0\u9ad8\u56de\u62a5\uff0c\u5efa\u7acb\u4e86\u8bad\u7ec3-\u63a8\u65ad\u6743\u8861\u7684\u8ba1\u7b97\u5185\u5bb9\u5bf9\u5e94\u5173\u7cfb\uff0c\u8ba8\u8bba\u4e86\u4e0d\u4e00\u81f4\u6027\u4e0e\u6709\u6548\u89c6\u91ce\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u8f6f\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u4e0d\u4e00\u81f4\u6027\u4e0e\u6709\u6548\u89c6\u91ce\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u8bad\u7ec3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.06594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06594", "abs": "https://arxiv.org/abs/2510.06594", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?", "comment": null, "summary": "Jailbreaking large language models (LLMs) has emerged as a pressing concern\nwith the increasing prevalence and accessibility of conversational LLMs.\nAdversarial users often exploit these models through carefully engineered\nprompts to elicit restricted or sensitive outputs, a strategy widely referred\nto as jailbreaking. While numerous defense mechanisms have been proposed,\nattackers continuously develop novel prompting techniques, and no existing\nmodel can be considered fully resistant. In this study, we investigate the\njailbreak phenomenon by examining the internal representations of LLMs, with a\nfocus on how hidden layers respond to jailbreak versus benign prompts.\nSpecifically, we analyze the open-source LLM GPT-J and the state-space model\nMamba2, presenting preliminary findings that highlight distinct layer-wise\nbehaviors. Our results suggest promising directions for further research on\nleveraging internal model dynamics for robust jailbreak detection and defense.", "AI": {"tldr": "\u7814\u7a76LLM\u8d8a\u72f1\u73b0\u8c61\uff0c\u901a\u8fc7\u5206\u6790GPT-J\u548cMamba2\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u53d1\u73b0\u8d8a\u72f1\u63d0\u793a\u4e0e\u826f\u6027\u63d0\u793a\u5728\u9690\u85cf\u5c42\u54cd\u5e94\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4e3a\u57fa\u4e8e\u5185\u90e8\u52a8\u6001\u7684\u8d8a\u72f1\u68c0\u6d4b\u548c\u9632\u5fa1\u63d0\u4f9b\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u5f0fLLM\u7684\u666e\u53ca\uff0c\u8d8a\u72f1\u653b\u51fb\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u9632\u5fa1\u673a\u5236\u65e0\u6cd5\u5b8c\u5168\u62b5\u6297\u65b0\u578b\u63d0\u793a\u6280\u672f\uff0c\u9700\u8981\u4ece\u6a21\u578b\u5185\u90e8\u8868\u5f81\u89d2\u5ea6\u7814\u7a76\u8d8a\u72f1\u73b0\u8c61\u3002", "method": "\u5206\u6790\u5f00\u6e90LLM GPT-J\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578bMamba2\u7684\u5185\u90e8\u8868\u793a\uff0c\u91cd\u70b9\u7814\u7a76\u9690\u85cf\u5c42\u5bf9\u8d8a\u72f1\u63d0\u793a\u4e0e\u826f\u6027\u63d0\u793a\u7684\u54cd\u5e94\u5dee\u5f02\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u4e24\u79cd\u6a21\u578b\u5728\u5c42\u7ea7\u884c\u4e3a\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u5dee\u5f02\uff0c\u8d8a\u72f1\u63d0\u793a\u5728\u5185\u90e8\u8868\u5f81\u4e2d\u5177\u6709\u53ef\u533a\u5206\u7684\u7279\u5f81\u3002", "conclusion": "\u5229\u7528\u6a21\u578b\u5185\u90e8\u52a8\u6001\u8fdb\u884c\u8d8a\u72f1\u68c0\u6d4b\u548c\u9632\u5fa1\u662f\u53ef\u884c\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5185\u90e8\u8868\u5f81\u5206\u6790\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.06557", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06557", "abs": "https://arxiv.org/abs/2510.06557", "authors": ["Milad Aghajohari", "Kamran Chitsaz", "Amirhossein Kazemnejad", "Sarath Chandar", "Alessandro Sordoni", "Aaron Courville", "Siva Reddy"], "title": "The Markovian Thinker", "comment": null, "summary": "Reinforcement learning (RL) has recently become a strong recipe for training\nreasoning LLMs that produce long chains of thought (LongCoT). Yet the standard\nRL \"thinking environment\", where the state is the prompt plus all prior\nreasoning tokens, makes the state unbounded and forces attention-based policies\nto pay quadratic compute as thoughts lengthen. We revisit the environment\nitself. We propose Markovian Thinking, a paradigm in which the policy advances\nreasoning while conditioning on a constant-size state, decoupling thinking\nlength from context size. As an immediate consequence this yields linear\ncompute with constant memory. We instantiate this idea with Delethink, an RL\nenvironment that structures reasoning into fixed-size chunks. Within each\nchunk, the model thinks as usual; at the boundary, the environment resets the\ncontext and reinitializes the prompt with a short carryover. Through RL, the\npolicy learns to write a textual state near the end of each chunk sufficient\nfor seamless continuation of reasoning after reset. Trained in this\nenvironment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up\nto 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.\nWith test-time scaling, Delethink continues to improve where LongCoT plateaus.\nThe effect of linear compute is substantial: we empirically estimate at 96K\naverage thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.\nAnalysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)\noften sample Markovian traces zero-shot across diverse benchmarks, providing\npositive samples that make RL effective at scale. Our results show that\nredesigning the thinking environment is a powerful lever: it enables very long\nreasoning without quadratic overhead and opens a path toward efficient,\nscalable reasoning LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86Markovian Thinking\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u5728\u6bcf\u4e2a\u5757\u8fb9\u754c\u91cd\u7f6e\u4e0a\u4e0b\u6587\u5e76\u4fdd\u7559\u7b80\u77ed\u72b6\u6001\uff0c\u5b9e\u73b0\u4e86\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6052\u5b9a\u5185\u5b58\u4f7f\u7528\uff0c\u89e3\u51b3\u4e86\u957f\u94fe\u63a8\u7406\u4e2d\u7684\u4e8c\u6b21\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u63a8\u7406LLMs\u65f6\uff0c\u968f\u7740\u601d\u7ef4\u94fe\u589e\u957f\u4f1a\u5bfc\u81f4\u72b6\u6001\u65e0\u754c\u548c\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u957f\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u4e86Delethink\u73af\u5883\uff0c\u5c06\u63a8\u7406\u7ed3\u6784\u5316\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff0c\u5728\u6bcf\u4e2a\u5757\u8fb9\u754c\u91cd\u7f6e\u4e0a\u4e0b\u6587\u5e76\u5b66\u4e60\u4fdd\u7559\u5173\u952e\u72b6\u6001\u4fe1\u606f\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u65e0\u7f1d\u7ee7\u7eed\u63a8\u7406\u3002", "result": "1.5B\u6a21\u578b\u57288K\u4ee4\u724c\u5757\u4e2d\u63a8\u7406\uff0c\u603b\u601d\u8003\u957f\u5ea6\u8fbe24K\u4ee4\u724c\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a24K\u9884\u7b97\u7684LongCoT-RL\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0896K\u601d\u8003\u957f\u5ea6\u4e0b\u4ece27\u4e2a\u6708\u964d\u81f37\u4e2aH100\u6708\uff09\u3002", "conclusion": "\u91cd\u65b0\u8bbe\u8ba1\u601d\u8003\u73af\u5883\u662f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u957f\u63a8\u7406LLMs\u7684\u6709\u6548\u9014\u5f84\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u4e8c\u6b21\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u975e\u5e38\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07297", "abs": "https://arxiv.org/abs/2510.07297", "authors": ["Henry Wang", "Sirajus Salekin", "Jake Lee", "Ross Claytor", "Shinan Zhang", "Michael Chi"], "title": "Agentic generative AI for media content discovery at the national football league", "comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition", "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5e2e\u52a9NFL\u5a92\u4f53\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5386\u53f2\u6bd4\u8d5b\u7247\u6bb5\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u7b5b\u9009\u754c\u9762\uff0c\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u67e5\u8be2\u65f6\u95f4\u4ece10\u5206\u949f\u7f29\u77ed\u523030\u79d2\u3002", "motivation": "\u4f20\u7edf\u7684\u5185\u5bb9\u53d1\u73b0\u548c\u7ba1\u7406\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\uff0c\u7528\u6237\u9700\u8981\u901a\u8fc7\u590d\u6742\u7684\u7b5b\u9009\u754c\u9762\u67e5\u627e\u76f8\u5173\u89c6\u9891\u5185\u5bb9\uff0c\u8fd9\u9650\u5236\u4e86\u5a92\u4f53\u7814\u7a76\u4eba\u5458\u7684\u521b\u9020\u529b\u548c\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u5c06\u7528\u6237\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5206\u89e3\u4e3a\u5143\u7d20\u5e76\u8f6c\u6362\u4e3a\u5e95\u5c42\u6570\u636e\u5e93\u67e5\u8be2\u8bed\u8a00\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bed\u4e49\u7f13\u5b58\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "result": "\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u5e73\u5747\u67e5\u627e\u76f8\u5173\u89c6\u9891\u7684\u65f6\u95f4\u4ece10\u5206\u949f\u51cf\u5c11\u523030\u79d2\uff0c\u663e\u8457\u63d0\u5347\u4e86NFL\u7684\u8fd0\u8425\u6548\u7387\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4e3a\u57fa\u7840\u7684\u5de5\u4f5c\u6d41\u80fd\u591f\u663e\u8457\u6539\u5584\u5185\u5bb9\u53d1\u73b0\u8fc7\u7a0b\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u4e13\u6ce8\u4e8e\u521b\u4f5c\u6027\u5185\u5bb9\u548c\u5f15\u4eba\u5165\u80dc\u7684\u6545\u4e8b\u7ebf\u5f00\u53d1\u3002", "topic": "swe application"}}
{"id": "2510.06652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06652", "abs": "https://arxiv.org/abs/2510.06652", "authors": ["Shangjian Yin", "Zhepei Wei", "Xinyu Zhu", "Wei-Lin Chen", "Yu Meng"], "title": "Aligning Large Language Models via Fully Self-Synthetic Data", "comment": null, "summary": "Traditional reinforcement learning from human feedback (RLHF) for large\nlanguage models (LLMs) relies on expensive human-annotated datasets, while\nReinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,\nrequiring the collection of diverse prompts and corresponding responses, often\nnecessitating external reward models or proprietary models like GPT-4 to\nannotate preference pairs. In this work, we introduce Self-Alignment\nOptimization (SAO), a fully self-synthetic framework for LLM alignment, where\nall training data, including prompts (i.e., user queries), responses, and\npreferences, are generated by the model itself. Specifically, SAO first\ninstructs the LLM to engage in persona role-play and generate diverse prompts\nand responses, which are then self-evaluated for preference optimization.\nExtensive experiments demonstrate that SAO effectively enhances the model's\nchat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining\nstrong performance on downstream objective tasks (e.g., question-answering,\nmath reasoning). Our work provides a practical solution for self-improvement in\naligning LLMs, and the code for reproducing our results is available at:\nhttps://github.com/SJY8460/SAO.", "AI": {"tldr": "\u63d0\u51faSAO\u6846\u67b6\uff0c\u5b8c\u5168\u81ea\u751f\u6210\u5bf9\u9f50\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5916\u90e8\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u751f\u6210\u591a\u6837\u63d0\u793a\u548c\u54cd\u5e94\uff0c\u5e76\u8fdb\u884c\u81ea\u8bc4\u4f30\u504f\u597d\u4f18\u5316\u3002", "motivation": "\u4f20\u7edfRLHF\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0cRLAIF\u4e5f\u9700\u8981\u6536\u96c6\u591a\u6837\u63d0\u793a\u548c\u54cd\u5e94\uff0c\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u6a21\u578b\u6216GPT-4\u7b49\u4e13\u6709\u6a21\u578b\u6807\u6ce8\u504f\u597d\u5bf9\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "SAO\u6846\u67b6\uff1a1) \u6307\u5bfcLLM\u8fdb\u884c\u89d2\u8272\u626e\u6f14\u751f\u6210\u591a\u6837\u63d0\u793a\u548c\u54cd\u5e94\uff1b2) \u81ea\u8bc4\u4f30\u8fdb\u884c\u504f\u597d\u4f18\u5316\uff1b\u6240\u6709\u8bad\u7ec3\u6570\u636e\u5747\u7531\u6a21\u578b\u81ea\u8eab\u751f\u6210\u3002", "result": "\u5728AlpacaEval~2.0\u7b49\u6807\u51c6\u57fa\u51c6\u4e0a\u6709\u6548\u63d0\u5347\u6a21\u578b\u804a\u5929\u80fd\u529b\uff0c\u540c\u65f6\u5728\u4e0b\u6e38\u5ba2\u89c2\u4efb\u52a1\uff08\u5982\u95ee\u7b54\u3001\u6570\u5b66\u63a8\u7406\uff09\u4e0a\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "\u4e3aLLM\u5bf9\u9f50\u7684\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.06664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06664", "abs": "https://arxiv.org/abs/2510.06664", "authors": ["Yunzhong Xiao", "Yangmin Li", "Hewei Wang", "Yunlong Tang", "Zora Zhiruo Wang"], "title": "ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory", "comment": null, "summary": "Agents utilizing tools powered by large language models (LLMs) or\nvision-language models (VLMs) have demonstrated remarkable progress in diverse\ntasks across text and visual modalities. Unlike traditional tools such as\ncalculators, which give deterministic outputs, neural tools perform uncertainly\nacross task scenarios. While different tools for a task may excel in varied\nscenarios, existing agents typically rely on fixed tools, thus limiting the\nflexibility in selecting the most suitable tool for specific tasks. In\ncontrast, humans snowball their understanding of the capabilities of different\ntools by interacting with them, and apply this knowledge to select the optimal\ntool when solving a future task. To build agents that similarly benefit from\nthis process, we propose ToolMem that enables agents to develop memories of\ntool capabilities from previous interactions, by summarizing their strengths\nand weaknesses and storing them in memory; at inference, the agent can retrieve\nrelevant entries from ToolMem, and select the best tool to solve individual\ntasks more accurately. We evaluate ToolMem on learning varied text generation\nand text-to-image generation neural tools. Compared to no-memory, generic\nagents, we find ToolMem-augmented agents predict tool performance 14.8% and\n28.7% more accurately across text and multimodal generation scenarios.\nMoreover, ToolMem facilitates optimal tool selection among multiple choices by\n21% and 24% absolute increases in respective scenarios.", "AI": {"tldr": "ToolMem\u662f\u4e00\u4e2a\u8ba9\u667a\u80fd\u4f53\u901a\u8fc7\u8bb0\u5fc6\u5de5\u5177\u80fd\u529b\u6765\u4f18\u5316\u5de5\u5177\u9009\u62e9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u603b\u7ed3\u5de5\u5177\u4f18\u7f3a\u70b9\u5e76\u5b58\u50a8\u5728\u8bb0\u5fc6\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u6765\u9009\u62e9\u6700\u4f73\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u5de5\u5177\uff0c\u9650\u5236\u4e86\u4e3a\u7279\u5b9a\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u5de5\u5177\u7684\u7075\u6d3b\u6027\u3002\u4eba\u7c7b\u901a\u8fc7\u4ea4\u4e92\u79ef\u7d2f\u5bf9\u5de5\u5177\u80fd\u529b\u7684\u7406\u89e3\uff0c\u5e76\u5e94\u7528\u8fd9\u4e9b\u77e5\u8bc6\u9009\u62e9\u6700\u4f18\u5de5\u5177\u3002", "method": "\u63d0\u51faToolMem\u6846\u67b6\uff0c\u8ba9\u667a\u80fd\u4f53\u4ece\u5148\u524d\u4ea4\u4e92\u4e2d\u5f00\u53d1\u5de5\u5177\u80fd\u529b\u8bb0\u5fc6\uff0c\u603b\u7ed3\u5de5\u5177\u4f18\u7f3a\u70b9\u5e76\u5b58\u50a8\u5728\u5185\u5b58\u4e2d\uff1b\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u76f8\u5173\u6761\u76ee\uff0c\u9009\u62e9\u6700\u4f73\u5de5\u5177\u6765\u66f4\u51c6\u786e\u5730\u89e3\u51b3\u4efb\u52a1\u3002", "result": "\u5728\u6587\u672c\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u795e\u7ecf\u5de5\u5177\u4e0a\u8bc4\u4f30ToolMem\u3002\u76f8\u6bd4\u65e0\u8bb0\u5fc6\u7684\u901a\u7528\u667a\u80fd\u4f53\uff0cToolMem\u589e\u5f3a\u7684\u667a\u80fd\u4f53\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u751f\u6210\u573a\u666f\u4e2d\u5206\u522b\u51c6\u786e\u9884\u6d4b\u5de5\u5177\u6027\u80fd14.8%\u548c28.7%\u3002\u5728\u591a\u4e2a\u9009\u62e9\u4e2d\uff0cToolMem\u4fc3\u8fdb\u6700\u4f18\u5de5\u5177\u9009\u62e9\u5206\u522b\u63d0\u9ad821%\u548c24%\u3002", "conclusion": "ToolMem\u901a\u8fc7\u8ba9\u667a\u80fd\u4f53\u79ef\u7d2f\u5de5\u5177\u80fd\u529b\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u6027\u80fd\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6700\u4f18\u5de5\u5177\u9009\u62e9\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.06670", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.06670", "abs": "https://arxiv.org/abs/2510.06670", "authors": ["Shangjian Yin", "Shining Liang", "Wenbiao Ding", "Yuli Qian", "Zhouxing Shi", "Hongzhi Li", "Yutao Xie"], "title": "PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone\nfor aligning large language models (LLMs). However, its effectiveness depends\non high-quality instruction data. Most existing alignment datasets are either\nprivate or require costly human annotation, which limits reproducibility and\nscalability. Even with Reinforcement Learning from AI Feedback (RLAIF),\nconcerns about data quality remain. Moreover, it is unclear how much data is\nactually required to fine-tune a base model into a strong instruction-following\nmodel. Current approaches often rely on over 300k examples even at the\nsupervised fine-tuning (SFT) stage, yet they still underperform compared to\nproprietary models, creating barriers for academic and resource-limited\ncommunities. To address this gap, we introduce PiKa, a data-efficient family of\nexpert-level alignment datasets. In particular, the PiKa-SFT dataset uses only\n30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through\nevaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,\nwe show that PiKa-SFT outperforms models trained on much larger data. On\nAlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses\nthe official Llama-3-8B-Instruct model trained on over 10 million proprietary\nexamples. We further extend our study by training the Qwen2.5 series (0.5B to\n7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that\nhigh-quality alignment can be achieved with significantly less data, offering a\nscalable path for open-source LLM alignment. Code and data:\nhttps://github.com/SJY8460/PiKa.", "AI": {"tldr": "PiKa\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684\u4e13\u5bb6\u7ea7\u5bf9\u9f50\u6570\u636e\u96c6\u5bb6\u65cf\uff0c\u4ec5\u4f7f\u75283\u4e07SFT\u6837\u672c\u5c31\u80fd\u8ba9Llama-3-8B-Base\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5b98\u65b9\u4f7f\u7528\u8d85\u5343\u4e07\u4e13\u6709\u6837\u672c\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u6570\u636e\u96c6\u8981\u4e48\u662f\u79c1\u6709\u7684\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u5373\u4f7f\u4f7f\u7528AI\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u6570\u636e\u8d28\u91cf\u95ee\u9898\u4ecd\u7136\u5b58\u5728\uff0c\u4e14\u4e0d\u6e05\u695a\u5b9e\u9645\u9700\u8981\u591a\u5c11\u6570\u636e\u6765\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u6210\u4e3a\u5f3a\u5927\u7684\u6307\u4ee4\u8ddf\u968f\u6a21\u578b\u3002", "method": "\u5f15\u5165PiKa\u6570\u636e\u96c6\u5bb6\u65cf\uff0c\u7279\u522b\u662fPiKa-SFT\u6570\u636e\u96c6\u4ec5\u4f7f\u75283\u4e07\u4e2aSFT\u6837\u672c\uff0c\u8fdc\u5c11\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u5982Magpie\u3002\u901a\u8fc7\u5728Llama-3-8B-Base\u548cQwen2.5\u7cfb\u5217\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u6548\u679c\u3002", "result": "PiKa-SFT\u5728AlpacaEval 2.0\u548cArena-Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5b98\u65b9Llama-3-8B-Instruct\u6a21\u578b\uff0c\u540e\u8005\u4f7f\u7528\u4e86\u8d85\u8fc71000\u4e07\u4e2a\u4e13\u6709\u6837\u672c\u3002\u5728Qwen2.5\u7cfb\u5217\u6a21\u578b\u4e0a\u4e5f\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u7528\u663e\u8457\u66f4\u5c11\u7684\u6570\u636e\u5b9e\u73b0\u6709\u6548\u7684LLM\u5bf9\u9f50\uff0c\u4e3a\u5f00\u6e90LLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.06727", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06727", "abs": "https://arxiv.org/abs/2510.06727", "authors": ["Miao Lu", "Weiwei Sun", "Weihua Du", "Zhan Ling", "Xuesong Yao", "Kang Liu", "Jiecao Chen"], "title": "Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management", "comment": null, "summary": "We study reinforcement learning (RL) fine-tuning of large language model\n(LLM) agents for long-horizon multi-turn tool use, where context length quickly\nbecomes a fundamental bottleneck. Existing RL pipelines can suffer from\ndegraded instruction following, excessive rollout costs, and most importantly,\nstrict context limits. To address these challenges, we introduce\nsummarization-based context management to training. In specific, it\nperiodically compresses the tool using history by LLM-generated summaries that\nretain task-relevant information to keep a compact context while enabling the\nagent to scale beyond the fixed context window. Building on this formulation,\nwe derive a policy gradient representation that seamlessly enables standard LLM\nRL infrastructures to optimize both tool-use behaviors as well as summarization\nstrategies in an end-to-end fashion. We instantiate this framework with\n\\underline{SU}mmarization augmented \\underline{P}olicy \\underline{O}ptimization\n(\\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond\na fixed context limit. Experiments on interactive function calling and\nsearching tasks demonstrate that \\texttt{SUPO} significantly improves the\nsuccess rate while maintaining the same or even lower working context length\ncompared to baselines. We also demonstrate that for complex searching tasks,\n\\texttt{SUPO} can further improve the evaluation performance when scaling\ntest-time maximum round of summarization beyond that of training time. Our\nresults establish summarization-based context management as a principled and\nscalable approach for training RL agents beyond a fixed context length limit.", "AI": {"tldr": "\u63d0\u51fa\u4e86SUPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u603b\u7ed3\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6765\u89e3\u51b3LLM\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u74f6\u9888\u95ee\u9898", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3001\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\u4e0b\u964d\u548c\u8fc7\u9ad8\u7684rollout\u6210\u672c\u7b49\u6311\u6218", "method": "\u5f15\u5165\u57fa\u4e8e\u603b\u7ed3\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5b9a\u671f\u538b\u7f29\u5de5\u5177\u4f7f\u7528\u5386\u53f2\uff0c\u901a\u8fc7LLM\u751f\u6210\u7684\u603b\u7ed3\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u5f00\u53d1\u4e86\u652f\u6301\u7aef\u5230\u7aef\u4f18\u5316\u7684\u7b56\u7565\u68af\u5ea6\u8868\u793a", "result": "\u5728\u4ea4\u4e92\u5f0f\u51fd\u6570\u8c03\u7528\u548c\u641c\u7d22\u4efb\u52a1\u4e2d\uff0cSUPO\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u6216\u66f4\u4f4e\u7684\u5de5\u4f5c\u4e0a\u4e0b\u6587\u957f\u5ea6", "conclusion": "\u57fa\u4e8e\u603b\u7ed3\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e3a\u8bad\u7ec3\u8d85\u8d8a\u56fa\u5b9a\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u7684RL\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5", "topic": "agentic reinforcement learning"}}
{"id": "2510.06649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06649", "abs": "https://arxiv.org/abs/2510.06649", "authors": ["Frank Wu", "Mengye Ren"], "title": "Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions", "comment": "15 pages, 5 figures", "summary": "The Forward-Forward (FF) Algorithm is a recently proposed learning procedure\nfor neural networks that employs two forward passes instead of the traditional\nforward and backward passes used in backpropagation. However, FF remains\nlargely confined to supervised settings, leaving a gap at domains where\nlearning signals can be yielded more naturally such as RL. In this work,\ninspired by FF's goodness function using layer activity statistics, we\nintroduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value\nestimation method that applies a goodness function and action conditioning for\nlocal RL using temporal difference learning. Despite its simplicity and\nbiological grounding, our approach achieves superior performance compared to\nstate-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind\nControl Suite benchmarks, while also outperforming algorithms trained with\nbackpropagation on most tasks. Code can be found at\nhttps://github.com/agentic-learning-ai-lab/arq.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u7684\u65e0\u53cd\u5411\u4f20\u64ad\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5ARQ\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u76ee\u524d\u4e3b\u8981\u5c40\u9650\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u5b58\u5728\u5e94\u7528\u7a7a\u767d\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b66\u4e60\u4fe1\u53f7\u53ef\u4ee5\u66f4\u81ea\u7136\u5730\u83b7\u5f97", "method": "\u5f15\u5165\u52a8\u4f5c\u6761\u4ef6\u5747\u65b9\u6839Q\u51fd\u6570(ARQ)\uff0c\u7ed3\u5408\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u7684\u4f18\u5ea6\u51fd\u6570\u548c\u52a8\u4f5c\u6761\u4ef6\uff0c\u4f7f\u7528\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u8fdb\u884c\u5c40\u90e8\u5f3a\u5316\u5b66\u4e60", "result": "\u5728MinAtar\u548cDeepMind Control Suite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARQ\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u53cd\u5411\u4f20\u64ad\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u7684\u7b97\u6cd5", "conclusion": "ARQ\u65b9\u6cd5\u7b80\u5355\u4e14\u5177\u6709\u751f\u7269\u5b66\u57fa\u7840\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65e0\u53cd\u5411\u4f20\u64ad\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2510.06672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06672", "abs": "https://arxiv.org/abs/2510.06672", "authors": ["Udbhav Bamba", "Minghao Fang", "Yifan Yu", "Haizhong Zheng", "Fan Lai"], "title": "XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation", "comment": null, "summary": "Reinforcement learning algorithms such as GRPO have driven recent advances in\nlarge language model (LLM) reasoning. While scaling the number of rollouts\nstabilizes training, existing approaches suffer from limited exploration on\nchallenging prompts and leave informative feedback signals underexploited, due\nto context-independent rollout allocation across prompts (e.g., generating 16\nrollouts per prompt) and relying heavily on sparse rewards. This paper presents\nXRPO(eXplore - eXploit GRPO), a unified framework that recasts policy\noptimization through the principled lens of rollout exploration-exploitation.\nTo enhance exploration, XRPO introduces a mathematically grounded rollout\nallocator that adaptively prioritizes prompts with higher potential for\nuncertainty reduction. It further addresses stagnation on zero-reward prompts\nthrough an in-context seeding strategy that injects curated exemplars, steering\nthe model into more difficult reasoning trajectories. To strengthen\nexploitation, XRPO develops a group-relative, novelty-aware advantage\nsharpening mechanism that leverages sequence likelihoods to amplify\nlow-probability yet correct responses, thereby extending the policy's reach\nbeyond sparse rewards. Experiments across diverse math and coding benchmarks on\nboth reasoning and non-reasoning models demonstrate that XRPO outperforms\nexisting advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while\naccelerating training convergence by up to 2.7X.", "AI": {"tldr": "XRPO\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u6539\u8fdbGRPO\u7b97\u6cd5\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728LLM\u63a8\u7406\u4e2d\u5b58\u5728\u63a2\u7d22\u4e0d\u8db3\u548c\u53cd\u9988\u4fe1\u53f7\u5229\u7528\u4e0d\u5145\u5206\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u63a2\u7d22\u7b56\u7565\u548c\u5956\u52b1\u5229\u7528\u673a\u5236\u3002", "method": "XRPO\u5f15\u5165\u6570\u5b66\u57fa\u7840\u7684rollout\u5206\u914d\u5668\u81ea\u9002\u5e94\u4f18\u5148\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u63d0\u793a\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u79cd\u5b50\u7b56\u7565\u89e3\u51b3\u96f6\u5956\u52b1\u63d0\u793a\u505c\u6ede\u95ee\u9898\uff0c\u5f00\u53d1\u57fa\u4e8e\u5e8f\u5217\u4f3c\u7136\u7684\u4f18\u52bf\u9510\u5316\u673a\u5236\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cXRPO\u6bd4GRPO\u548cGSPO\u7b49\u73b0\u6709\u65b9\u6cd5\u63d0\u53474% pass@1\u548c6% cons@32\uff0c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u63d0\u53472.7\u500d\u3002", "conclusion": "XRPO\u901a\u8fc7\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6311\u6218\u6027\u63d0\u793a\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.06800", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.06800", "abs": "https://arxiv.org/abs/2510.06800", "authors": ["Haotian Wu", "Shufan Jiang", "Chios Chen", "Yiyang Feng", "Hehai Lin", "Heqing Zou", "Yao Shu", "Yanran Li", "Chengwei Qin"], "title": "FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline", "comment": null, "summary": "As large language models (LLMs) advance in role-playing (RP) tasks, existing\nbenchmarks quickly become obsolete due to their narrow scope, outdated\ninteraction paradigms, and limited adaptability across diverse application\nscenarios. To address this gap, we introduce FURINA-Builder, a novel\nmulti-agent collaboration pipeline that automatically constructs fully\ncustomizable RP benchmarks at any scale. It enables evaluation of arbitrary\ncharacters across diverse scenarios and prompt formats, as the first benchmark\nbuilder in RP area for adaptable assessment. FURINA-Builder simulates dialogues\nbetween a test character and other characters drawn from a well-constructed\ncharacter-scene pool, while an LLM judge selects fine-grained evaluation\ndimensions and adjusts the test character's responses into final test\nutterances. Using this pipeline, we build FURINA-Bench, a new comprehensive\nrole-playing benchmark featuring both established and synthesized test\ncharacters, each assessed with dimension-specific evaluation criteria. Human\nevaluation and preliminary separability analysis justify our pipeline and\nbenchmark design. We conduct extensive evaluations of cutting-edge LLMs and\nfind that o3 and DeepSeek-R1 achieve the best performance on English and\nChinese RP tasks, respectively. Across all models, established characters\nconsistently outperform synthesized ones, with reasoning capabilities further\namplifying this disparity. Interestingly, we observe that model scale does not\nmonotonically reduce hallucinations. More critically, for reasoning LLMs, we\nuncover a novel trade-off: reasoning improves RP performance but simultaneously\nincreases RP hallucinations. This trade-off extends to a broader Pareto\nfrontier between RP performance and reliability for all LLMs. These findings\ndemonstrate the effectiveness of FURINA-Builder and the challenge posed by\nFURINA-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86FURINA-Builder\uff0c\u4e00\u79cd\u81ea\u52a8\u6784\u5efa\u53ef\u5b9a\u5236\u89d2\u8272\u626e\u6f14\u57fa\u51c6\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7ba1\u9053\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86FURINA-Bench\u57fa\u51c6\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406LLMs\u5728\u89d2\u8272\u626e\u6f14\u6027\u80fd\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u57fa\u51c6\u5b58\u5728\u8303\u56f4\u72ed\u7a84\u3001\u4ea4\u4e92\u8303\u5f0f\u8fc7\u65f6\u548c\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u548c\u53ef\u5b9a\u5236\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "FURINA-Builder\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u81ea\u52a8\u6784\u5efa\u57fa\u51c6\uff0c\u6a21\u62df\u89d2\u8272\u95f4\u5bf9\u8bdd\uff0c\u4f7f\u7528LLM\u6cd5\u5b98\u9009\u62e9\u8bc4\u4f30\u7ef4\u5ea6\u5e76\u8c03\u6574\u6d4b\u8bd5\u89d2\u8272\u7684\u54cd\u5e94\u3002", "result": "\u6784\u5efa\u4e86FURINA-Bench\u57fa\u51c6\uff0c\u8bc4\u4f30\u663e\u793ao3\u548cDeepSeek-R1\u5206\u522b\u5728\u82f1\u6587\u548c\u4e2d\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u4f1a\u653e\u5927\u89d2\u8272\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u63a8\u7406LLMs\u5728\u6027\u80fd\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "FURINA-Builder\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0cFURINA-Bench\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\uff0c\u63ed\u793a\u4e86LLMs\u5728\u89d2\u8272\u626e\u6f14\u4e2d\u6027\u80fd\u4e0e\u53ef\u9760\u6027\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "topic": "agent analysis"}}
{"id": "2510.06843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06843", "abs": "https://arxiv.org/abs/2510.06843", "authors": ["Xuhang Chen", "Zhifan Song", "Deyi Ji", "Shuo Gao", "Lanyun Zhu"], "title": "SID: Multi-LLM Debate Driven by Self Signals", "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive capabilities across\ndiverse application domains. Recent work has explored Multi-LLM Agent Debate\n(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and\nrefine responses iteratively. Nevertheless, existing MAD methods predominantly\nfocus on utilizing external structures, such as debate graphs, using\nLLM-as-a-Judge, while neglecting the application of self signals, such as token\nlogits and attention, that arise during generation. This omission leads to\nredundant computation and potential performance degradation. In this paper, we\nshift the focus to the self signals of multi-LLM debate and introduce a\nSelf-Signals Driven Multi-LLM Debate (SID), which leverages two types of\nself-signals: model-level confidence and token-level semantic focus, to\nadaptively guide the debate process. Our approach enables high-confidence\nagents to exit early at the model level and compress the redundant debate\ncontents based on the attention mechanism. We evaluate our method on various\nLLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental\nresults demonstrate that our method not only outperforms existing MAD\ntechniques in accuracy but also reduces token consumption, highlighting the\neffectiveness of utilizing self signals in enhancing both the performance and\nefficiency of multi-agent debate systems. Our code will be available\nat~\\href{https://github.com/xuhang2019/SID}{\\texttt{https://github.com/xuhang2019/SID}}.", "AI": {"tldr": "\u63d0\u51faSID\u65b9\u6cd5\uff0c\u5229\u7528LLM\u81ea\u4fe1\u53f7\uff08\u6a21\u578b\u7ea7\u7f6e\u4fe1\u5ea6\u548c\u8bcd\u7ea7\u8bed\u4e49\u7126\u70b9\uff09\u6307\u5bfc\u591aLLM\u8fa9\u8bba\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387", "motivation": "\u73b0\u6709\u591aLLM\u8fa9\u8bba\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5916\u90e8\u7ed3\u6784\u800c\u5ffd\u7565\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4fe1\u53f7\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u901a\u8fc7\u6a21\u578b\u7ea7\u7f6e\u4fe1\u5ea6\u5b9e\u73b0\u9ad8\u7f6e\u4fe1\u5ea6\u4ee3\u7406\u65e9\u671f\u9000\u51fa\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u538b\u7f29\u5197\u4f59\u8fa9\u8bba\u5185\u5bb9", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSID\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548ctoken\u6d88\u8017\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709MAD\u6280\u672f", "conclusion": "\u5229\u7528\u81ea\u4fe1\u53f7\u80fd\u6709\u6548\u63d0\u5347\u591a\u4ee3\u7406\u8fa9\u8bba\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387", "topic": "agent analysis"}}
{"id": "2510.06790", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06790", "abs": "https://arxiv.org/abs/2510.06790", "authors": ["Tavish McDonald", "Bo Lei", "Stanislav Fort", "Bhavya Kailkhura", "Brian Bartoldson"], "title": "Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness", "comment": "17 pages", "summary": "Models are susceptible to adversarially out-of-distribution (OOD) data\ndespite large training-compute investments into their robustification. Zaremba\net al. (2025) make progress on this problem at test time, showing LLM reasoning\nimproves satisfaction of model specifications designed to thwart attacks,\nresulting in a correlation between reasoning effort and robustness to\njailbreaks. However, this benefit of test compute fades when attackers are\ngiven access to gradients or multimodal inputs. We address this gap, clarifying\nthat inference-compute offers benefits even in such cases. Our approach argues\nthat compositional generalization, through which OOD data is understandable via\nits in-distribution (ID) components, enables adherence to defensive\nspecifications on adversarially OOD inputs. Namely, we posit the Robustness\nfrom Inference Compute Hypothesis (RICH): inference-compute defenses profit as\nthe model's training data better reflects the attacked data's components. We\nempirically support this hypothesis across vision language model and attack\ntypes, finding robustness gains from test-time compute if specification\nfollowing on OOD data is unlocked by compositional generalization, while RL\nfinetuning and protracted reasoning are not critical. For example, increasing\nemphasis on defensive specifications via prompting lowers the success rate of\ngradient-based multimodal attacks on VLMs robustified by adversarial\npretraining, but this same intervention provides no such benefit to\nnot-robustified models. This correlation of inference-compute's robustness\nbenefit with base model robustness is the rich-get-richer dynamic of the RICH:\nattacked data components are more ID for robustified models, aiding\ncompositional generalization to OOD data. Accordingly, we advise layering\ntrain-time and test-time defenses to obtain their synergistic benefit.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRICH\u5047\u8bbe\uff1a\u5f53\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u66f4\u597d\u5730\u53cd\u6620\u88ab\u653b\u51fb\u6570\u636e\u7684\u7ec4\u6210\u90e8\u5206\u65f6\uff0c\u63a8\u7406\u8ba1\u7b97\u9632\u5fa1\u80fd\u5e26\u6765\u9c81\u68d2\u6027\u6536\u76ca\u3002\u901a\u8fc7\u7ec4\u5408\u6cdb\u5316\uff0c\u6a21\u578b\u80fd\u5728\u5bf9\u6297\u6027OOD\u6570\u636e\u4e0a\u9075\u5faa\u9632\u5fa1\u89c4\u8303\uff0c\u5b9e\u73b0\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u9632\u5fa1\u7684\u534f\u540c\u6548\u76ca\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u63a8\u7406\u80fd\u63d0\u9ad8\u5bf9\u9632\u5fa1\u89c4\u8303\u7684\u9075\u5faa\u5ea6\uff0c\u4f46\u5728\u653b\u51fb\u8005\u62e5\u6709\u68af\u5ea6\u6216\u591a\u6a21\u6001\u8f93\u5165\u65f6\u6548\u679c\u51cf\u5f31\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc1\u660e\u63a8\u7406\u8ba1\u7b97\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5e26\u6765\u9c81\u68d2\u6027\u6536\u76ca\u3002", "method": "\u63d0\u51faRICH\u5047\u8bbe\uff0c\u901a\u8fc7\u7ec4\u5408\u6cdb\u5316\u673a\u5236\u4f7f\u6a21\u578b\u7406\u89e3OOD\u6570\u636e\u7684ID\u7ec4\u6210\u90e8\u5206\uff0c\u4ece\u800c\u5728\u5bf9\u6297\u6027OOD\u8f93\u5165\u4e0a\u9075\u5faa\u9632\u5fa1\u89c4\u8303\u3002\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u4e0d\u540c\u653b\u51fb\u7c7b\u578b\u4e0b\u7684\u9c81\u68d2\u6027\u589e\u76ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5982\u679c\u7ec4\u5408\u6cdb\u5316\u89e3\u9501\u4e86\u5bf9OOD\u6570\u636e\u7684\u89c4\u8303\u9075\u5faa\uff0c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u786e\u5b9e\u80fd\u5e26\u6765\u9c81\u68d2\u6027\u589e\u76ca\uff0c\u800cRL\u5fae\u8c03\u548c\u957f\u65f6\u95f4\u63a8\u7406\u4e0d\u662f\u5173\u952e\u56e0\u7d20\u3002\u901a\u8fc7\u63d0\u793a\u589e\u5f3a\u9632\u5fa1\u89c4\u8303\u80fd\u964d\u4f4e\u57fa\u4e8e\u68af\u5ea6\u7684\u591a\u6a21\u6001\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u63a8\u7406\u8ba1\u7b97\u7684\u9c81\u68d2\u6027\u6536\u76ca\u4e0e\u57fa\u7840\u6a21\u578b\u9c81\u68d2\u6027\u76f8\u5173\uff0c\u5f62\u6210\u5bcc\u8005\u6108\u5bcc\u7684RICH\u52a8\u6001\u3002\u5efa\u8bae\u5c06\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u9632\u5fa1\u5c42\u53e0\u4f7f\u7528\u4ee5\u83b7\u5f97\u534f\u540c\u6548\u76ca\u3002", "topic": "agent analysis"}}
{"id": "2510.06828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06828", "abs": "https://arxiv.org/abs/2510.06828", "authors": ["Michael Keiblinger"], "title": "Recurrence-Complete Frame-based Action Models", "comment": null, "summary": "In recent years, attention-like mechanisms have been used to great success in\nthe space of large language models, unlocking scaling potential to a previously\nunthinkable extent. \"Attention Is All You Need\" famously claims RNN cells are\nnot needed in conjunction with attention. We challenge this view. In this\npaper, we point to existing proofs that architectures with fully parallelizable\nforward or backward passes cannot represent classes of problems specifically\ninteresting for long-running agentic tasks. We further conjecture a critical\ntime t beyond which non-recurrence-complete models fail to aggregate inputs\ncorrectly, with concrete implications for agentic systems (e.g., software\nengineering agents). To address this, we introduce a recurrence-complete\narchitecture and train it on GitHub-derived action sequences. Loss follows a\npower law in the trained sequence length while the parameter count remains\nfixed. Moreover, longer-sequence training always amortizes its linearly\nincreasing wall-time cost, yielding lower loss as a function of wall time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\"\u6ce8\u610f\u529b\u673a\u5236\u5c31\u662f\u4e00\u5207\"\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u975e\u5faa\u73af\u5b8c\u6574\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u4efb\u52a1\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5faa\u73af\u5b8c\u6574\u7684\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u6d41\u884c\u7684\u6ce8\u610f\u529b\u673a\u5236\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u4efb\u52a1\u65f6\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\uff0c\u975e\u5faa\u73af\u5b8c\u6574\u6a21\u578b\u65e0\u6cd5\u6b63\u786e\u805a\u5408\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u8f93\u5165\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5faa\u73af\u5b8c\u6574\u7684\u67b6\u6784\uff0c\u5e76\u5728GitHub\u884c\u4e3a\u5e8f\u5217\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7814\u7a76\u4e86\u635f\u5931\u4e0e\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u635f\u5931\u968f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u5448\u5e42\u5f8b\u5206\u5e03\uff0c\u800c\u53c2\u6570\u6570\u91cf\u4fdd\u6301\u4e0d\u53d8\uff1b\u66f4\u957f\u7684\u5e8f\u5217\u8bad\u7ec3\u603b\u80fd\u644a\u9500\u7ebf\u6027\u589e\u957f\u7684\u65f6\u95f4\u6210\u672c\uff0c\u5728\u65f6\u95f4\u51fd\u6570\u4e0a\u4ea7\u751f\u66f4\u4f4e\u7684\u635f\u5931\u3002", "conclusion": "\u5faa\u73af\u673a\u5236\u5bf9\u4e8e\u5904\u7406\u957f\u5e8f\u5217\u667a\u80fd\u4f53\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u5faa\u73af\u5b8c\u6574\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3\u975e\u5faa\u73af\u6a21\u578b\u5728\u957f\u65f6\u95f4\u8de8\u5ea6\u4fe1\u606f\u805a\u5408\u4e0a\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.06915", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06915", "abs": "https://arxiv.org/abs/2510.06915", "authors": ["Zecheng Tang", "Baibei Ji", "Quantong Qiu", "Haitian Wang", "Xiaobo Liang", "Juntao Li", "Min Zhang"], "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling", "comment": null, "summary": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.", "AI": {"tldr": "\u63d0\u51fa\u4e86Long-RewardBench\u57fa\u51c6\u6765\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565LongRMs\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c8B\u6a21\u578b\u8d85\u8d8a70B\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2dLLM\u4ee3\u7406\u7b49\u573a\u666f\u6d89\u53ca\u957f\u5386\u53f2\u8f68\u8ff9\uff0c\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5c40\u9650\u4e8e\u77ed\u4e0a\u4e0b\u6587\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5bf9\u957f\u4e0a\u4e0b\u6587-\u54cd\u5e94\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u3002", "method": "\u5f15\u5165Long-RewardBench\u57fa\u51c6\uff0c\u5305\u542b\u6210\u5bf9\u6bd4\u8f83\u548c\u6700\u4f73\u9009\u62e9\u4efb\u52a1\uff1b\u63d0\u51fa\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u4efb\u610f\u6a21\u578b\u6269\u5c55\u4e3a\u7a33\u5065\u7684\u957f\u4e0a\u4e0b\u6587\u5956\u52b1\u6a21\u578b\u3002", "result": "8B LongRM\u5728\u957f\u4e0a\u4e0b\u6587\u8bc4\u4f30\u4e2d\u5927\u5e45\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8a70B\u89c4\u6a21\u57fa\u7ebf\uff0c\u4e0e\u4e13\u6709Gemini 2.5 Pro\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u77ed\u4e0a\u4e0b\u6587\u80fd\u529b\u3002", "conclusion": "\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u80fd\u6709\u6548\u6269\u5c55\u6a21\u578b\u4e3a\u7a33\u5065\u7684\u957f\u4e0a\u4e0b\u6587\u5956\u52b1\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.07024", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07024", "abs": "https://arxiv.org/abs/2510.07024", "authors": ["Shrestha Ghosh", "Luca Giordano", "Yujia Hu", "Tuan-Phong Nguyen", "Simon Razniewski"], "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge", "comment": null, "summary": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.", "AI": {"tldr": "\u5bf9GPT-4.1\u76841\u4ebf\u6761\u4fe1\u5ff5\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0\u5176\u4e8b\u5b9e\u77e5\u8bc6\u4e0e\u73b0\u6709\u77e5\u8bc6\u5e93\u5dee\u5f02\u663e\u8457\uff0c\u51c6\u786e\u6027\u4f4e\u4e8e\u5148\u524d\u57fa\u51c6\uff0c\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3001\u6a21\u7cca\u6027\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u7406\u89e3\u524d\u6cbfLLM\u7684\u4e8b\u5b9e\u77e5\u8bc6\u7279\u6027\uff0c\u76ee\u524d\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u901a\u5e38\u57fa\u4e8e\u6709\u504f\u6837\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u57fa\u4e8eGPTKB v1.5\u6570\u636e\u96c6\uff0c\u5bf9GPT-4.1\u76841\u4ebf\u6761\u9012\u5f52\u83b7\u53d6\u7684\u4fe1\u5ff5\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u6a21\u578b\u4e8b\u5b9e\u77e5\u8bc6\u4e0e\u73b0\u6709\u77e5\u8bc6\u5e93\u5dee\u5f02\u663e\u8457\uff0c\u51c6\u786e\u6027\u4f4e\u4e8e\u5148\u524d\u57fa\u51c6\u6307\u6807\uff0c\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u4e00\u81f4\u6027\u3001\u6a21\u7cca\u6027\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "LLM\u7684\u4e8b\u5b9e\u77e5\u8bc6\u5b58\u5728\u663e\u8457\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u672a\u6765\u5728\u4e8b\u5b9e\u6027LLM\u77e5\u8bc6\u7814\u7a76\u65b9\u9762\u7684\u91cd\u8981\u673a\u4f1a\u3002", "topic": "agent analysis"}}
{"id": "2510.07043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07043", "abs": "https://arxiv.org/abs/2510.07043", "authors": ["Tian Qin", "Felix Bai", "Ting-Yao Hu", "Raviteja Vemulapalli", "Hema Swetha Koppula", "Zhiyang Xu", "Bowen Jin", "Mert Cemri", "Jiarui Lu", "Zirui Wang", "Meng Cao"], "title": "COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization", "comment": null, "summary": "Real-world large language model (LLM) agents must master strategic tool use\nand user preference optimization through multi-turn interactions to assist\nusers with complex planning tasks. We introduce COMPASS (Constrained\nOptimization through Multi-turn Planning and Strategic Solutions), a benchmark\nthat evaluates agents on realistic travel-planning scenarios. We cast travel\nplanning as a constrained preference optimization problem, where agents must\nsatisfy hard constraints while simultaneously optimizing soft user preferences.\nTo support this, we build a realistic travel database covering transportation,\naccommodation, and ticketing for 20 U.S. National Parks, along with a\ncomprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) an\nacceptable-optimal gap, where agents reliably meet constraints but fail to\noptimize preferences, and (ii) a plan-coordination gap, where performance\ncollapses on multi-service (flight and hotel) coordination tasks, especially\nfor open-source models. By grounding reasoning and planning in a practical,\nuser-facing domain, COMPASS provides a benchmark that directly measures an\nagent's ability to optimize user preferences in realistic tasks, bridging\ntheoretical advances with real-world impact.", "AI": {"tldr": "COMPASS\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u65c5\u884c\u89c4\u5212\u573a\u666f\u4e2d\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5c06\u65c5\u884c\u89c4\u5212\u5efa\u6a21\u4e3a\u7ea6\u675f\u504f\u597d\u4f18\u5316\u95ee\u9898\uff0c\u8981\u6c42\u4ee3\u7406\u6ee1\u8db3\u786c\u7ea6\u675f\u540c\u65f6\u4f18\u5316\u8f6f\u7528\u6237\u504f\u597d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684LLM\u4ee3\u7406\u9700\u8981\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u638c\u63e1\u7b56\u7565\u6027\u5de5\u5177\u4f7f\u7528\u548c\u7528\u6237\u504f\u597d\u4f18\u5316\uff0c\u4ee5\u534f\u52a9\u7528\u6237\u5b8c\u6210\u590d\u6742\u89c4\u5212\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u4e86\u8986\u76d620\u4e2a\u7f8e\u56fd\u56fd\u5bb6\u516c\u56ed\u7684\u4ea4\u901a\u3001\u4f4f\u5bbf\u548c\u7968\u52a1\u7684\u771f\u5b9e\u65c5\u884c\u6570\u636e\u5e93\uff0c\u4ee5\u53ca\u6a21\u62df\u5546\u4e1a\u9884\u8ba2\u5e73\u53f0\u7684\u7efc\u5408\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u5dee\u8ddd\uff1a(i)\u53ef\u63a5\u53d7-\u6700\u4f18\u5dee\u8ddd\uff1a\u4ee3\u7406\u80fd\u53ef\u9760\u6ee1\u8db3\u7ea6\u675f\u4f46\u65e0\u6cd5\u4f18\u5316\u504f\u597d\uff1b(ii)\u8ba1\u5212\u534f\u8c03\u5dee\u8ddd\uff1a\u5728\u591a\u670d\u52a1\u534f\u8c03\u4efb\u52a1\u4e0a\u6027\u80fd\u5d29\u6e83\uff0c\u7279\u522b\u662f\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "COMPASS\u901a\u8fc7\u5728\u5b9e\u7528\u3001\u9762\u5411\u7528\u6237\u7684\u9886\u57df\u4e2d\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u76f4\u63a5\u8861\u91cf\u4ee3\u7406\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u4f18\u5316\u7528\u6237\u504f\u597d\u80fd\u529b\u7684\u57fa\u51c6\u3002", "topic": "swe benchmark"}}
{"id": "2510.07169", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07169", "abs": "https://arxiv.org/abs/2510.07169", "authors": ["Yike Zhao", "Simin Guo", "Ziqing Yang", "Shifan Han", "Dahua Lin", "Fei Tan"], "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning", "comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track", "summary": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.", "AI": {"tldr": "\u5bf9\u5f00\u6e90\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u548c\u5408\u6210\u6280\u672f\u8fdb\u884c\u7efc\u5408\u5206\u6790\uff0c\u5728\u7edf\u4e00\u8bad\u7ec3\u90e8\u7f72\u6d41\u7a0b\u4e2d\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u6570\u636e\u548c\u4ece\u5f3a\u6a21\u578b\u84b8\u998f\u6bd4\u5355\u7eaf\u6269\u5927\u6570\u636e\u89c4\u6a21\u66f4\u6709\u6548\u3002", "motivation": "LLM\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002\u5c3d\u7ba1\u6709\u5404\u79cd\u6570\u636e\u6784\u5efa\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u7edf\u4e00\u8bad\u7ec3\u90e8\u7f72\u6d41\u7a0b\u4e2d\u8bc4\u4f30\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6570\u636e\u5408\u6210\u6280\u672f\uff0c\u63d0\u70bc\u6709\u6548\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u8bc6\u522b\u9002\u5408\u5de5\u4e1a\u5e94\u7528\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5c06\u6570\u636e\u7ec4\u7ec7\u6210\u66f4\u6613\u7406\u89e3\u7684\u683c\u5f0f\uff0c\u6216\u4ece\u66f4\u5f3a\u6a21\u578b\u84b8\u998f\uff0c\u901a\u5e38\u6bd4\u5355\u7eaf\u6269\u5927\u6570\u636e\u89c4\u6a21\u66f4\u6709\u6548\u3002", "conclusion": "\u4e3a\u6574\u5408\u8bad\u7ec3\u6570\u636e\u63d0\u5347LLM\u80fd\u529b\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u652f\u6301\u6210\u672c\u6548\u76ca\u9ad8\u7684\u6570\u636e\u7ba1\u7406\u548c\u53ef\u6269\u5c55\u7684\u6a21\u578b\u589e\u5f3a\uff0c\u542f\u53d1\u5728\u73b0\u5b9e\u63a8\u7406\u4efb\u52a1\u4e2d\u5e73\u8861\"\u66f4\u591a\u6570\u636e\"\u4e0e\"\u66f4\u597d\u6570\u636e\"\u7684\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2510.07230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07230", "abs": "https://arxiv.org/abs/2510.07230", "authors": ["Ziyi Wang", "Yuxuan Lu", "Yimeng Zhang", "Jing Huang", "Dakuo Wang"], "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping", "comment": null, "summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Customer-R1\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u7528\u6237\u884c\u4e3a\u6a21\u62df\uff0c\u5728\u5728\u7ebf\u8d2d\u7269\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u6b65\u9aa4\u5f0f\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5b66\u4e60\u7fa4\u4f53\u5c42\u9762\u7684\u7b56\u7565\uff0c\u7f3a\u4e4f\u5bf9\u7528\u6237\u4e2a\u6027\u7684\u8003\u8651\uff0c\u5bfc\u81f4\u6a21\u62df\u7ed3\u679c\u8fc7\u4e8e\u901a\u7528\u800c\u975e\u4e2a\u6027\u5316\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7b56\u7565\u57fa\u4e8e\u660e\u786e\u7684\u7528\u6237\u753b\u50cf\uff0c\u901a\u8fc7\u52a8\u4f5c\u6b63\u786e\u6027\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u4e0b\u4e00\u6b65\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728OPeRA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCustomer-R1\u5728\u4e0b\u4e00\u6b65\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u548cSFT\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u66f4\u597d\u5730\u5339\u914d\u7528\u6237\u52a8\u4f5c\u5206\u5e03\u3002", "conclusion": "Customer-R1\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e2a\u6027\u5316\u884c\u4e3a\u6a21\u62df\uff0c\u5728\u7528\u6237\u884c\u4e3a\u4eff\u771f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07284", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07284", "abs": "https://arxiv.org/abs/2510.07284", "authors": ["MohammadHossein Rezaei", "Robert Vacareanu", "Zihao Wang", "Clinton Wang", "Yunzhong He", "Afra Feyza Aky\u00fcrek"], "title": "Online Rubrics Elicitation from Pairwise Comparisons", "comment": null, "summary": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8bc4\u4f30\u6807\u51c6\u65b9\u6cd5OnlineRubrics\uff0c\u901a\u8fc7\u5728\u7ebf\u6bd4\u8f83\u5f53\u524d\u7b56\u7565\u548c\u53c2\u8003\u7b56\u7565\u7684\u54cd\u5e94\u6765\u52a8\u6001\u751f\u6210\u8bc4\u4f30\u6807\u51c6\uff0c\u76f8\u6bd4\u9759\u6001\u6807\u51c6\u80fd\u63d0\u53478%\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u8bc4\u4f30\u6807\u51c6\uff0c\u5bb9\u6613\u53d7\u5230\u5956\u52b1\u653b\u51fb\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4e14\u65e0\u6cd5\u6355\u6349\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u65b0\u7684\u671f\u671b\u8981\u6c42\u3002", "method": "OnlineRubrics\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u65b9\u5f0f\u52a8\u6001\u751f\u6210\u8bc4\u4f30\u6807\u51c6\uff0c\u901a\u8fc7\u6bd4\u8f83\u5f53\u524d\u7b56\u7565\u548c\u53c2\u8003\u7b56\u7565\u7684\u54cd\u5e94\u5bf9\u6765\u6301\u7eed\u8bc6\u522b\u548c\u7f13\u89e3\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u9519\u8bef\u3002", "result": "\u5728AlpacaEval\u3001GPQA\u3001ArenaHard\u4ee5\u53ca\u4e13\u5bb6\u95ee\u9898\u548c\u6807\u51c6\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u9759\u6001\u6807\u51c6\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5e26\u6765\u4e86\u9ad8\u8fbe8%\u7684\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u52a8\u6001\u8bc4\u4f30\u6807\u51c6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u5728\u5f00\u653e\u957f\u6587\u672c\u56de\u7b54\u4efb\u52a1\u4e0a\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u8bc6\u522b\u51fa\u7684\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\u5305\u62ec\u900f\u660e\u5ea6\u3001\u5b9e\u7528\u6027\u3001\u7ec4\u7ec7\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07300", "abs": "https://arxiv.org/abs/2510.07300", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Kaiyu Huang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning", "comment": "13 pages, 8 tables, 4 figures", "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages.", "AI": {"tldr": "M-Thinker\u901a\u8fc7GRPO\u7b97\u6cd5\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u4e00\u81f4\u6027\u548c\u63a8\u7406\u80fd\u529b\u95ee\u9898\uff0c\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1100%\u7684\u8bed\u8a00\u4e00\u81f4\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u975e\u82f1\u8bed\u8bed\u8a00\u65f6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u96be\u4ee5\u4fdd\u6301\u8f93\u5165\u8f93\u51fa\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e0a\u7684\u63a8\u7406\u8def\u5f84\u8d28\u91cf\u8f83\u5dee\u3001\u7b54\u6848\u51c6\u786e\u7387\u4f4e\u4e8e\u82f1\u8bed\u3002\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86\u975e\u82f1\u8bed\u7528\u6237\u7684\u4f7f\u7528\u4f53\u9a8c\uff0c\u963b\u788d\u4e86LRMs\u7684\u5168\u7403\u90e8\u7f72\u3002", "method": "\u63d0\u51faM-Thinker\u6a21\u578b\uff0c\u91c7\u7528GRPO\u7b97\u6cd5\u8bad\u7ec3\uff0c\u5305\u542b\u8bed\u8a00\u4e00\u81f4\u6027\u5956\u52b1\u548c\u8de8\u8bed\u8a00\u601d\u7ef4\u5bf9\u9f50\u5956\u52b1\u3002\u8bed\u8a00\u4e00\u81f4\u6027\u5956\u52b1\u4e25\u683c\u7ea6\u675f\u8f93\u5165\u3001\u601d\u7ef4\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u8bed\u8a00\u4e00\u81f4\u6027\uff1b\u8de8\u8bed\u8a00\u601d\u7ef4\u5bf9\u9f50\u5956\u52b1\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u5728\u975e\u82f1\u8bed\u548c\u82f1\u8bed\u4e0a\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5c06\u82f1\u8bed\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u975e\u82f1\u8bed\u8bed\u8a00\u3002", "result": "M-Thinker-1.5B/7B\u6a21\u578b\u5728\u4e24\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff08MMATH\u548cPolyMath\uff09\u4e0a\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u63a5\u8fd1100%\u7684\u8bed\u8a00\u4e00\u81f4\u6027\uff0c\u8fd8\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u57df\u5916\u8bed\u8a00\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "M-Thinker\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u5173\u952e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u63a8\u7406\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002", "topic": "agent analysis"}}
{"id": "2510.07309", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07309", "abs": "https://arxiv.org/abs/2510.07309", "authors": ["Yue Li", "Ran Tao", "Derek Hommel", "Yusuf Denizay D\u00f6nder", "Sungyong Chang", "David Mimno", "Unso Eun Seo Jo"], "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain", "comment": "20 pages, 6 figures, under review for ACL ARR", "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21\\% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.", "AI": {"tldr": "\u63d0\u51fa\u4e86CORGI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u9488\u5bf9\u771f\u5b9e\u5546\u4e1a\u573a\u666f\u7684\u6587\u672c\u5230SQL\u8f6c\u6362\uff0c\u5305\u542b\u63cf\u8ff0\u6027\u3001\u89e3\u91ca\u6027\u3001\u9884\u6d4b\u6027\u548c\u63a8\u8350\u6027\u56db\u7c7b\u590d\u6742\u67e5\u8be2\uff0c\u6bd4\u73b0\u6709\u57fa\u51c6\u96be21%\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230SQL\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5386\u53f2\u8bb0\u5f55\u7684\u4e8b\u5b9e\u68c0\u7d22\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5546\u4e1a\u73af\u5883\u4e2d\u9700\u8981\u56e0\u679c\u63a8\u7406\u3001\u65f6\u95f4\u9884\u6d4b\u548c\u6218\u7565\u63a8\u8350\u7684\u591a\u5c42\u6b21\u667a\u80fd\u9700\u6c42\u3002", "method": "\u521b\u5efa\u57fa\u4e8eDoordash\u3001Airbnb\u548cLululemon\u7b49\u4f01\u4e1a\u542f\u53d1\u7684\u5408\u6210\u6570\u636e\u5e93\uff0c\u8bbe\u8ba1\u56db\u7c7b\u590d\u6742\u5ea6\u9012\u589e\u7684\u5546\u4e1a\u67e5\u8be2\uff0c\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u5546\u4e1a\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u9ad8\u7ea7\u522b\u95ee\u9898\u4e0a\u8868\u73b0\u4e0b\u964d\uff0c\u96be\u4ee5\u505a\u51fa\u51c6\u786e\u9884\u6d4b\u548c\u63d0\u4f9b\u53ef\u884c\u8ba1\u5212\uff0cCORGI\u57fa\u51c6\u6bd4BIRD\u57fa\u51c6\u96be\u7ea621%\u3002", "conclusion": "\u6d41\u884cLLM\u4e0e\u771f\u5b9e\u5546\u4e1a\u667a\u80fd\u9700\u6c42\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u6587\u672c\u5230SQL\u7cfb\u7edf\u6765\u652f\u6301\u590d\u6742\u5546\u4e1a\u51b3\u7b56\u3002", "topic": "swe benchmark"}}
{"id": "2510.07312", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07312", "abs": "https://arxiv.org/abs/2510.07312", "authors": ["Sumeet Ramesh Motwani", "Alesia Ivanova", "Ziyang Cai", "Philip Torr", "Riashat Islam", "Shital Shah", "Christian Schroeder de Witt", "Charles London"], "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning", "comment": "Preprint, 31 pages, 8 figures", "summary": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u7b80\u5355\u95ee\u9898\u4e3a\u590d\u6742\u591a\u6b65\u4f9d\u8d56\u94fe\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u65f6\u7a0b\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u4f7f\u7528\u73b0\u6709\u77ed\u65f6\u7a0b\u6570\u636e\uff0c\u65e0\u9700\u5bc6\u96c6\u76d1\u7763\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77ed\u65f6\u7a0b\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u968f\u7740\u63a8\u7406\u65f6\u7a0b\u589e\u957f\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u63a8\u7406\u65f6\u652f\u67b6\u6216\u6602\u8d35\u7684\u9010\u6b65\u76d1\u7763\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5408\u6210\u7b80\u5355\u95ee\u9898\u4e3a\u4efb\u610f\u957f\u5ea6\u7684\u590d\u6742\u591a\u6b65\u4f9d\u8d56\u94fe\uff0c\u4f7f\u7528\u4ec5\u7ed3\u679c\u5956\u52b1\u5728\u81ea\u52a8\u589e\u52a0\u590d\u6742\u5ea6\u7684\u8bfe\u7a0b\u4e0b\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7fRL\u8bad\u7ec3\u80fd\u591f\u8fdb\u4e00\u6b65\u6269\u5c55\u800c\u4e0d\u9971\u548c\u3002", "result": "\u57286\u5e74\u7ea7\u6570\u5b66\u95ee\u9898\u4e0a\u8fdb\u884c\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u53ef\u5c06\u7ade\u8d5b\u7ea7\u57fa\u51c6\uff08GSM-Symbolic\u3001MATH-500\u3001AIME\uff09\u7684\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe2.06\u500d\uff0c\u4e14\u5728\u9ad8pass@k\u4e0b\u4ecd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ec5\u4f7f\u7528\u73b0\u6709\u6570\u636e\u6269\u5c55RL\u89e3\u51b3\u957f\u65f6\u7a0b\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u8def\u5f84\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u8bfe\u7a0bRL\u6bd4\u5168\u65f6\u7a0b\u8bad\u7ec3\u5728\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u6709\u6307\u6570\u7ea7\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.558d9c2a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.wealthsimple.com%2Fhow-were-making-app-security-smarter%3Futm_source=tldrinfosec/1/01000199b9b4026c-a729fee0-5319-4f4b-bd09-054522b82ad5-000000/yR70UQrS-dxJQuOK8D4Xl3NJW1FbZVuoZg8nwKwz5_M=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.wealthsimple.com%2Fhow-were-making-app-security-smarter%3Futm_source=tldrinfosec/1/01000199b9b4026c-a729fee0-5319-4f4b-bd09-054522b82ad5-000000/yR70UQrS-dxJQuOK8D4Xl3NJW1FbZVuoZg8nwKwz5_M=425", "authors": ["TLDR Newsletter"], "title": "How We're Making Application Security Smarter", "comment": "Source: TLDR Newsletter, Date: 2025-10-06, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.wealthsimple.com%2Fhow-were-making-app-security-smarter%3Futm_source=tldrinfosec/1/01000199b9b4026c-a729fee0-5319-4f4b-bd09-054522b82ad5-000000/yR70UQrS-dxJQuOK8D4Xl3NJW1FbZVuoZg8nwKwz5_M=425", "summary": "How We're Making Application Security Smarter (3 minute read) Wealthsimple was faced with the common dilemma of having a small security team tasked with reviewing code written by a much larger engineering team in a highly regulated environment. It introduced Semgrep AI to create custom fixes for the specific code under review, augmenting its security team. Semgrep remembers previous decisions on findings and uses that information to auto-triage future findings", "source": "tldr", "AI": {"tldr": "Wealthsimple\u4f7f\u7528Semgrep AI\u521b\u5efa\u81ea\u5b9a\u4e49\u4fee\u590d\u65b9\u6848\uff0c\u589e\u5f3a\u5c0f\u578b\u5b89\u5168\u56e2\u961f\u5728\u76d1\u7ba1\u73af\u5883\u4e2d\u7684\u4ee3\u7801\u5ba1\u67e5\u80fd\u529b", "motivation": "\u5c0f\u578b\u5b89\u5168\u56e2\u961f\u9700\u8981\u5ba1\u67e5\u5927\u91cf\u5de5\u7a0b\u56e2\u961f\u4ee3\u7801\u7684\u5e38\u89c1\u56f0\u5883\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5ea6\u76d1\u7ba1\u73af\u5883\u4e2d", "method": "\u5f15\u5165Semgrep AI\u521b\u5efa\u9488\u5bf9\u7279\u5b9a\u4ee3\u7801\u7684\u81ea\u5b9a\u4e49\u4fee\u590d\u65b9\u6848\uff0c\u7cfb\u7edf\u8bb0\u5fc6\u5148\u524d\u51b3\u7b56\u5e76\u81ea\u52a8\u5206\u7c7b\u672a\u6765\u53d1\u73b0", "result": "\u6210\u529f\u589e\u5f3a\u4e86\u5b89\u5168\u56e2\u961f\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u5ba1\u67e5\u6548\u7387", "conclusion": "AI\u8f85\u52a9\u5de5\u5177\u80fd\u6709\u6548\u89e3\u51b3\u5b89\u5168\u56e2\u961f\u89c4\u6a21\u4e0e\u4ee3\u7801\u5ba1\u67e5\u9700\u6c42\u4e0d\u5339\u914d\u7684\u95ee\u9898", "topic": "swe application"}}
{"id": "tldr.2510.48953820", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faifordevelopers%2Fp%2Fyoure-only-using-20-of-claude-code%3Futm_source=tldrwebdev/1/01000199be5bf57e-ac72aa60-cc4b-4073-a9d6-49ad8ea4ed7e-000000/JZ_fvs_blyfIIYFLT0vqhXMPOYgC_mQu7omYfGW_vHI=425", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faifordevelopers%2Fp%2Fyoure-only-using-20-of-claude-code%3Futm_source=tldrwebdev/1/01000199be5bf57e-ac72aa60-cc4b-4073-a9d6-49ad8ea4ed7e-000000/JZ_fvs_blyfIIYFLT0vqhXMPOYgC_mQu7omYfGW_vHI=425", "authors": ["TLDR Newsletter"], "title": "You're Only Using 20% of Claude Code - Here's How to Unlock the Rest", "comment": "Source: TLDR Newsletter, Date: 2025-10-07, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faifordevelopers%2Fp%2Fyoure-only-using-20-of-claude-code%3Futm_source=tldrwebdev/1/01000199be5bf57e-ac72aa60-cc4b-4073-a9d6-49ad8ea4ed7e-000000/JZ_fvs_blyfIIYFLT0vqhXMPOYgC_mQu7omYfGW_vHI=425", "summary": "You're Only Using 20% of Claude Code - Here's How to Unlock the Rest (6 minute read) Most developers are only using a small fraction of Claude Code's capabilities. To fully use it, devs should use Model Context Protocols (MCPs) correctly, set up a good CLAUDE.md configuration file, use Planning Mode for better project structure, and integrate the IDE plugin for real-time error detection and fixes.", "source": "tldr", "AI": {"tldr": "\u5927\u591a\u6570\u5f00\u53d1\u8005\u53ea\u4f7f\u7528\u4e86Claude Code\u768420%\u529f\u80fd\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u6b63\u786e\u4f7f\u7528MCP\u534f\u8bae\u3001\u914d\u7f6eCLAUDE.md\u6587\u4ef6\u3001\u4f7f\u7528\u89c4\u5212\u6a21\u5f0f\u548cIDE\u63d2\u4ef6\u6765\u89e3\u9501\u5168\u90e8\u529f\u80fd\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u5145\u5206\u5229\u7528Claude Code\u7684\u5b8c\u6574\u80fd\u529b\uff0c\u907f\u514d\u53ea\u4f7f\u7528\u57fa\u7840\u529f\u80fd\u800c\u9519\u8fc7\u9ad8\u7ea7\u7279\u6027\u3002", "method": "\u901a\u8fc7\u6b63\u786e\u914d\u7f6eModel Context Protocols (MCPs)\u3001\u8bbe\u7f6eCLAUDE.md\u914d\u7f6e\u6587\u4ef6\u3001\u4f7f\u7528Planning Mode\u4f18\u5316\u9879\u76ee\u7ed3\u6784\u3001\u96c6\u6210IDE\u63d2\u4ef6\u8fdb\u884c\u5b9e\u65f6\u9519\u8bef\u68c0\u6d4b\u548c\u4fee\u590d\u3002", "result": "\u5f00\u53d1\u8005\u53ef\u4ee5\u663e\u8457\u63d0\u5347Claude Code\u7684\u4f7f\u7528\u6548\u7387\uff0c\u83b7\u5f97\u66f4\u597d\u7684\u9879\u76ee\u7ed3\u6784\u7ba1\u7406\u548c\u5b9e\u65f6\u7f16\u7a0b\u8f85\u52a9\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u914d\u7f6e\u548c\u4f7f\u7528Claude Code\u7684\u5404\u9879\u529f\u80fd\uff0c\u5f00\u53d1\u8005\u53ef\u4ee5\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2510.d83ba68b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.elastic.co%2Fsecurity-labs%2Fmcp-tools-attack-defense-recommendations%3Futm_source=tldrinfosec/1/01000199bec85f36-510aa493-3d3f-4d56-b1d6-a27c31550bac-000000/4MN5cMJhFoSEB9uJz98DhEz2QaI4Jcyd4T8UwlTiXPA=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.elastic.co%2Fsecurity-labs%2Fmcp-tools-attack-defense-recommendations%3Futm_source=tldrinfosec/1/01000199bec85f36-510aa493-3d3f-4d56-b1d6-a27c31550bac-000000/4MN5cMJhFoSEB9uJz98DhEz2QaI4Jcyd4T8UwlTiXPA=426", "authors": ["TLDR Newsletter"], "title": "MCP Tools: Attack Vectors and Defense Recommendations for Autonomous Agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-07, Reading time: 26 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.elastic.co%2Fsecurity-labs%2Fmcp-tools-attack-defense-recommendations%3Futm_source=tldrinfosec/1/01000199bec85f36-510aa493-3d3f-4d56-b1d6-a27c31550bac-000000/4MN5cMJhFoSEB9uJz98DhEz2QaI4Jcyd4T8UwlTiXPA=426", "summary": "MCP Tools: Attack Vectors and Defense Recommendations for Autonomous Agents (26 minute read) Elastic Security Labs provides a comprehensive overview of different attacks and defenses for MCP tools. Elastic provides a prompt for an LLM to detect malicious functions in an MCP server, covering security risks ranging from traditional vulnerabilities to tool poisoning, orchestration injection, rug-pull redefinitions, name collisions, and passive influence. Elastic recommends that users utilize san...", "source": "tldr", "AI": {"tldr": "Elastic Security Labs\u5206\u6790\u4e86MCP\u5de5\u5177\u7684\u5b89\u5168\u653b\u51fb\u5411\u91cf\uff0c\u5305\u62ec\u4f20\u7edf\u6f0f\u6d1e\u3001\u5de5\u5177\u6295\u6bd2\u3001\u7f16\u6392\u6ce8\u5165\u7b49\u591a\u79cd\u653b\u51fb\u65b9\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u9632\u5fa1\u5efa\u8bae\u548c\u6076\u610f\u529f\u80fd\u68c0\u6d4b\u63d0\u793a\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u4ee3\u7406\u548cMCP\u5de5\u5177\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u9632\u5fa1\u5404\u79cd\u653b\u51fb\u5411\u91cf\uff0c\u4fdd\u62a4\u7528\u6237\u514d\u53d7\u6076\u610f\u5de5\u5177\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790MCP\u5de5\u5177\u7684\u4e0d\u540c\u653b\u51fb\u5411\u91cf\uff0c\u5305\u62ec\u5de5\u5177\u6295\u6bd2\u3001\u7f16\u6392\u6ce8\u5165\u3001\u5730\u6bef\u5f0f\u91cd\u5b9a\u4e49\u7b49\uff0c\u5e76\u5f00\u53d1\u4e86LLM\u63d0\u793a\u6765\u68c0\u6d4bMCP\u670d\u52a1\u5668\u4e2d\u7684\u6076\u610f\u529f\u80fd\u3002", "result": "\u8bc6\u522b\u4e86\u4ece\u4f20\u7edf\u6f0f\u6d1e\u5230\u88ab\u52a8\u5f71\u54cd\u7b49\u591a\u79cd\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u9632\u5fa1\u5efa\u8bae\u548c\u6076\u610f\u529f\u80fd\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "MCP\u5de5\u5177\u5b58\u5728\u591a\u79cd\u5b89\u5168\u5a01\u80c1\uff0c\u7528\u6237\u5e94\u91c7\u53d6\u5b89\u5168\u63aa\u65bd\u5982\u4f7f\u7528\u6c99\u7bb1\u73af\u5883\u3001\u9a8c\u8bc1\u5de5\u5177\u6765\u6e90\u7b49\u6765\u4fdd\u62a4\u7cfb\u7edf\u5b89\u5168\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.45cbeec6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiRcOuk/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/hWuNVyrvfxHqrtdJ5EIwC2f-hubMGH6FIgjcl-TWBHQ=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiRcOuk/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/hWuNVyrvfxHqrtdJ5EIwC2f-hubMGH6FIgjcl-TWBHQ=426", "authors": ["TLDR Newsletter"], "title": "Introducing AgentKit", "comment": "Source: TLDR Newsletter, Date: 2025-10-07, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FiRcOuk/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/hWuNVyrvfxHqrtdJ5EIwC2f-hubMGH6FIgjcl-TWBHQ=426", "summary": "Introducing AgentKit (2 minute read) OpenAI launched Agent Builder, which is a visual drag-and-drop canvas called the \"Canva for building agents\", ChatKit for embedding chat interfaces, evaluation tools including trace grading and automated prompt optimization, and a connector registry for linking internal tools.", "source": "tldr", "AI": {"tldr": "OpenAI\u63a8\u51faAgentKit\uff0c\u5305\u542b\u53ef\u89c6\u5316\u62d6\u62fd\u6784\u5efa\u4ee3\u7406\u7684Agent Builder\u3001\u5d4c\u5165\u804a\u5929\u754c\u9762\u7684ChatKit\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u8fde\u63a5\u5668\u6ce8\u518c\u8868\u3002", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u4ee3\u7406\u6784\u5efa\u5de5\u5177\uff0c\u964d\u4f4e\u5f00\u53d1\u95e8\u69db\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u91c7\u7528\u53ef\u89c6\u5316\u62d6\u62fd\u754c\u9762\u6784\u5efa\u4ee3\u7406\uff0c\u63d0\u4f9b\u804a\u5929\u754c\u9762\u5d4c\u5165\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u8fde\u63a5\u5668\u6ce8\u518c\u8868\u7b49\u529f\u80fd\u3002", "result": "\u63a8\u51fa\u4e86\u5305\u542b\u591a\u4e2a\u7ec4\u4ef6\u7684AgentKit\u5de5\u5177\u5957\u4ef6\u3002", "conclusion": "AgentKit\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u4ee3\u7406\u5f00\u53d1\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2510.8b3a2300", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fsafety-security%2Fai-security-frontier-strategy-tools%2F%3Futm_source=tldrai/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/NUMQXvi8oRwSxd8PUv2s8PVv24pO7JWLYnq3fDAtCYo=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fsafety-security%2Fai-security-frontier-strategy-tools%2F%3Futm_source=tldrai/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/NUMQXvi8oRwSxd8PUv2s8PVv24pO7JWLYnq3fDAtCYo=426", "authors": ["TLDR Newsletter"], "title": "Google's CodeMender", "comment": "Source: TLDR Newsletter, Date: 2025-10-07, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fsafety-security%2Fai-security-frontier-strategy-tools%2F%3Futm_source=tldrai/1/01000199bed0eb00-acec7c2e-64a7-432f-889c-aaf1fad46c0e-000000/NUMQXvi8oRwSxd8PUv2s8PVv24pO7JWLYnq3fDAtCYo=426", "summary": "Google's CodeMender (4 minute read) Google's CodeMender is an AI agent that automatically detects and patches code vulnerabilities using Gemini models. Alongside it, it launched the AI Vulnerability Reward Program and SAIF 2.0 to define secure-by-design principles for autonomous agents and boost AI-driven cybersecurity.", "source": "tldr", "AI": {"tldr": "Google\u63a8\u51faCodeMender AI\u4ee3\u7406\uff0c\u4f7f\u7528Gemini\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u590d\u4ee3\u7801\u6f0f\u6d1e\uff0c\u540c\u65f6\u542f\u52a8AI\u6f0f\u6d1e\u5956\u52b1\u8ba1\u5212\u548cSAIF 2.0\u6846\u67b6\u6765\u63d0\u5347AI\u9a71\u52a8\u7684\u7f51\u7edc\u5b89\u5168\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\uff0c\u901a\u8fc7AI\u6280\u672f\u81ea\u52a8\u5316\u6f0f\u6d1e\u68c0\u6d4b\u548c\u4fee\u590d\uff0c\u540c\u65f6\u5efa\u7acb\u5b89\u5168\u8bbe\u8ba1\u539f\u5219\u6765\u589e\u5f3a\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528Gemini\u6a21\u578b\u6784\u5efaCodeMender AI\u4ee3\u7406\u6765\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u8865\u4ee3\u7801\u6f0f\u6d1e\uff0c\u914d\u5408AI\u6f0f\u6d1e\u5956\u52b1\u8ba1\u5212\u548cSAIF 2.0\u5b89\u5168\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u548c\u4fee\u590d\u4ee3\u7801\u6f0f\u6d1e\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u5b89\u5168\u6807\u51c6\u548c\u6fc0\u52b1\u673a\u5236\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u4ee3\u7801\u5b89\u5168\u5de5\u5177\u548c\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u8f6f\u4ef6\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u5efa\u7acb\u5b89\u5168\u8bbe\u8ba1\u6807\u51c6\u3002", "topic": "code agent"}}
{"id": "tldr.2510.db32d46d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fkubernetes-for-agentic-apps-a-platform-engineering-perspective%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/YKnLllB6OlUP-ID1nY_-jgPzhDaw6xFlD8-wEL-jQWs=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fkubernetes-for-agentic-apps-a-platform-engineering-perspective%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/YKnLllB6OlUP-ID1nY_-jgPzhDaw6xFlD8-wEL-jQWs=426", "authors": ["TLDR Newsletter"], "title": "Kubernetes for agentic apps: A platform engineering perspective", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fplatformengineering.org%2Fblog%2Fkubernetes-for-agentic-apps-a-platform-engineering-perspective%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/YKnLllB6OlUP-ID1nY_-jgPzhDaw6xFlD8-wEL-jQWs=426", "summary": "Kubernetes for agentic apps: A platform engineering perspective (6 minute read) Agentic AI, where systems autonomously perceive, reason, and act, is shifting software from predefined instructions to genuine autonomy. Kubernetes, applied through platform engineering, offers the foundation for this new computing paradigm, with Google Kubernetes Engine (GKE) providing managed services and AI optimizations for production-ready agentic AI.", "source": "tldr", "AI": {"tldr": "Kubernetes\u4e3a\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u63d0\u4f9b\u5e73\u53f0\u5de5\u7a0b\u57fa\u7840\uff0cGKE\u63d0\u4f9b\u751f\u4ea7\u7ea7\u667a\u80fd\u4ee3\u7406AI\u7684\u6258\u7ba1\u670d\u52a1\u548cAI\u4f18\u5316", "motivation": "\u667a\u80fdAI\u7cfb\u7edf\u4ece\u9884\u5b9a\u4e49\u6307\u4ee4\u8f6c\u5411\u771f\u6b63\u81ea\u4e3b\u6027\uff0c\u9700\u8981\u5f3a\u5927\u7684\u5e73\u53f0\u652f\u6301", "method": "\u901a\u8fc7\u5e73\u53f0\u5de5\u7a0b\u5e94\u7528Kubernetes\uff0c\u5229\u7528Google Kubernetes Engine(GKE)\u7684\u6258\u7ba1\u670d\u52a1\u548cAI\u4f18\u5316", "result": "Kubernetes\u4e3a\u8fd9\u79cd\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u63d0\u4f9b\u4e86\u57fa\u7840\u67b6\u6784", "conclusion": "Kubernetes\u901a\u8fc7\u5e73\u53f0\u5de5\u7a0b\u4e3a\u751f\u4ea7\u7ea7\u667a\u80fd\u4ee3\u7406AI\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "wechat.2510.00d74b6d", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMDYzMTE4MQ==&mid=2247486312&idx=1&sn=188e96543bf7d178d8f2d0a1f1c2ea51&chksm=96efcbbf8e8a74ee4834f5a64e90a86f7d8c2dbb1d055bd1e12a2fa29b0187f2d75b3974d465#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMDYzMTE4MQ==&mid=2247486312&idx=1&sn=188e96543bf7d178d8f2d0a1f1c2ea51&chksm=96efcbbf8e8a74ee4834f5a64e90a86f7d8c2dbb1d055bd1e12a2fa29b0187f2d75b3974d465#rd", "authors": ["\u5065\u8ff0\u6709\u9053"], "title": "RAG \u5df2\u8fc7\u65f6\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u667a\u80fd\u4f53 (RL Agents) \u5c06\u6210\u4e3a\u65b0\u7684\u68c0\u7d22\u6808", "comment": "Source: WeChat, Published: 2025-10-09 13:45:51", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\uff1a\u901a\u8fc7\u5956\u52b1/\u60e9\u7f5a\u8ba9AI\u81ea\u6211\u8fdb\u5316\u5956\u52b1\u51fd\u6570\uff08Reward Function\uff09\uff1a\u89c4\u5b9a\u201c\u505a\u5bf9\u4ec0\u4e48\u624d\u7b97\u5f3a\u201d\u7684\u6807\u51c6\u5927\u767d\u8bdd\uff1a\u8ba9AI\u50cf\u73a9\u95ef\u5173\u6e38\u620f\u4e00\u6837\uff0c\u505a\u5bf9\u4e00\u6b65\u5956\u52b1\u3001\u67e5\u9519\u4e00\u6b65\u60e9\u7f5a\uff0c\u4ece\u4e0d\u65ad\u8bd5\u9519\u4e2d\u53d8\u5f97\u53c8\u5feb\u53c8\u51c6", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08Reinforcement Learning\uff0c RL\uff09\uff1a\u901a\u8fc7\u5956\u52b1/\u60e9\u7f5a\u8ba9AI\u81ea\u6211\u8fdb\u5316\u5956\u52b1\u51fd\u6570\uff08Reward Function\uff09\uff1a\u89c4\u5b9a\u201c\u505a\u5bf9\u4ec0\u4e48\u624d\u7b97\u5f3a\u201d\u7684\u6807\u51c6\u5927\u767d\u8bdd\uff1a\u8ba9AI\u50cf\u73a9\u95ef\u5173\u6e38\u620f\u4e00\u6837\uff0c\u505a\u5bf9\u4e00\u6b65\u5956\u52b1\u3001\u67e5\u9519\u4e00\u6b65\u60e9\u7f5a\uff0c\u4ece\u4e0d\u65ad\u8bd5\u9519\u4e2d\u53d8\u5f97\u53c8\u5feb\u53c8\u51c6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.13a50c4f", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660378&idx=3&sn=c3b98b852675dabaaad9976e2844ec36&chksm=e82382432b3adca82b2cccea5e4dec60cb2fabed8379ac9e00cdd0f3bea1921580d7674fea77#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247660378&idx=3&sn=c3b98b852675dabaaad9976e2844ec36&chksm=e82382432b3adca82b2cccea5e4dec60cb2fabed8379ac9e00cdd0f3bea1921580d7674fea77#rd", "authors": ["\u6570\u636e\u6d3eTHU"], "title": "\u590d\u65e6\u3001\u540c\u6d4e\u548c\u6e2f\u4e2d\u6587\u7b49\u91cd\u78c5\u53d1\u5e03\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5168\u5468\u671f\u7684\u5168\u9762\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-09 09:00:00", "summary": "\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0e\u5de5\u5177\u5305\u7684\u9a71\u52a8\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53c2\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3001\u5bf9\u9f50\u53ca\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u57fa\u51c6\u8fdb\u884c\u9a8c\u8bc1\u3002\u8be5\u7efc\u8ff0\u6df1\u5165\u5256\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5982\u4f55\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5168\u751f\u547d\u5468\u671f\u9636\u6bb5\uff0c\u5982\u4f55\u8d2f\u7a7f LLMs \u7684\u9884\u8bad\u7ec3\u3001", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0e\u5de5\u5177\u5305\u7684\u9a71\u52a8\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53c2\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u3001\u5bf9\u9f50\u53ca\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u57fa\u51c6\u8fdb\u884c\u9a8c\u8bc1\u3002\u8be5\u7efc\u8ff0\u6df1\u5165\u5256\u6790\u4e86\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5982\u4f55\u5e94\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5168\u751f\u547d\u5468\u671f\u9636\u6bb5\uff0c\u5982\u4f55\u8d2f\u7a7f LLMs \u7684\u9884\u8bad\u7ec3\u3001", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.3ec36f86", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNDA3NjQzNg==&mid=2247483684&idx=1&sn=4ea0a1665e471db34b9ae37d91ffb2a0&chksm=f1b50ab0606ee73066f4690a6aebbb85578fe0edaff044823e6f2b8ec0d2dcff7cd7ea92afa6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNDA3NjQzNg==&mid=2247483684&idx=1&sn=4ea0a1665e471db34b9ae37d91ffb2a0&chksm=f1b50ab0606ee73066f4690a6aebbb85578fe0edaff044823e6f2b8ec0d2dcff7cd7ea92afa6#rd", "authors": ["AInon-Chihaiya"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e2d\u7684\u71b5\u673a\u5236 (\u4e8c) \u71b5\u5b89\u5168\u7b56\u7565", "comment": "Source: WeChat, Published: 2025-10-09 08:55:47", "summary": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e00\u4e0b\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u7528\u5230\u7684\u71b5\u5b89\u5168\u7b56\u7565\u3002\u8fd9\u91cc\u7684\u71b5\u5b89\u5168\u5305\u542b\u4e24\u4e2a\u65b9\u9762\uff1a\u9632\u6b62\u71b5\u574d\u584c\u3001\u9632\u6b62\u71b5\u7206\u70b8\uff0c\u5373\u53cc\u8fb9\u71b5\u5b89\u5168\u3002\u540e\u7eed\u7684\u5185\u5bb9\u6309\u7167\u6587\u7ae0\u53d1\u8868\u7684\u5148\u540e\u987a\u5e8f\u6765\u4ecb\u7ecd\uff0c\u5e76\u5728\u6301\u7eed\u66f4\u65b0\u4e2d\u3002", "AI": {"tldr": "\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e00\u4e0b\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e2d\u7528\u5230\u7684\u71b5\u5b89\u5168\u7b56\u7565\u3002\u8fd9\u91cc\u7684\u71b5\u5b89\u5168\u5305\u542b\u4e24\u4e2a\u65b9\u9762\uff1a\u9632\u6b62\u71b5\u574d\u584c\u3001\u9632\u6b62\u71b5\u7206\u70b8\uff0c\u5373\u53cc\u8fb9\u71b5\u5b89\u5168\u3002\u540e\u7eed\u7684\u5185\u5bb9\u6309\u7167\u6587\u7ae0\u53d1\u8868\u7684\u5148\u540e\u987a\u5e8f\u6765\u4ecb\u7ecd\uff0c\u5e76\u5728\u6301\u7eed\u66f4\u65b0\u4e2d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.605361ad", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNTU5MjIyMQ==&mid=2247484991&idx=1&sn=7ef4c59e1ba1052ccc01000d36fa7bbc&chksm=c01028de221f0b8b58bd3cb1c46399703b567874a3aeb315b7a317e053f2227a2673ff0c0f07#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNTU5MjIyMQ==&mid=2247484991&idx=1&sn=7ef4c59e1ba1052ccc01000d36fa7bbc&chksm=c01028de221f0b8b58bd3cb1c46399703b567874a3aeb315b7a317e053f2227a2673ff0c0f07#rd", "authors": ["\u5f00\u6e90\u5c0f\u6808hubtools"], "title": "\u6df1\u5165\u6d45\u51fa\u9605\u8bfbOpenAI<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7ecf\u5178\u8bba\u6587\u300aProximal Policy Optimization Algorithms\u300b", "comment": "Source: WeChat, Published: 2025-10-09 06:36:51", "summary": "\u5728PPO\u88ab\u63d0\u51fa\u4e4b\u524d\uff0c\u4e3b\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u6709\u4e09\u7c7b\uff0c\u4f46\u5b83\u4eec\u90fd\u5404\u6709\u77ed\u5904\uff1a\uff081\uff09\u6df1\u5ea6Q\u5b66\u4e60 \uff08DQN\uff09\uff1a \u8fd9\u7c7b\u7b97\u6cd5\u5728\u5904\u7406\u50cf\u96c5\u8fbe\u5229\uff08Atari\uff09\u6e38\u620f\u8fd9\u6837\u62e5\u6709\u79bb\u6563\u52a8\u4f5c\uff08\u6bd4\u5982\u4e0a\u3001\u4e0b\u3001\u5de6\u3001\u53f3\uff09\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02 \u3002", "AI": {"tldr": "\u5728PPO\u88ab\u63d0\u51fa\u4e4b\u524d\uff0c\u4e3b\u6d41\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u6709\u4e09\u7c7b\uff0c\u4f46\u5b83\u4eec\u90fd\u5404\u6709\u77ed\u5904\uff1a\uff081\uff09\u6df1\u5ea6Q\u5b66\u4e60 \uff08DQN\uff09\uff1a \u8fd9\u7c7b\u7b97\u6cd5\u5728\u5904\u7406\u50cf\u96c5\u8fbe\u5229\uff08Atari\uff09\u6e38\u620f\u8fd9\u6837\u62e5\u6709\u79bb\u6563\u52a8\u4f5c\uff08\u6bd4\u5982\u4e0a\u3001\u4e0b\u3001\u5de6\u3001\u53f3\uff09\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.d4a44d2e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247716126&idx=3&sn=38ba10ea412595fc9a558dfc80b0fab8&chksm=f841199e1322cc450b36cecb5d6aef301ca5e8f4f41d57dd9de0e4a66246613b3d0b4e7c6e01#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247716126&idx=3&sn=38ba10ea412595fc9a558dfc80b0fab8&chksm=f841199e1322cc450b36cecb5d6aef301ca5e8f4f41d57dd9de0e4a66246613b3d0b4e7c6e01#rd", "authors": ["\u4e00\u70b9\u4eba\u5de5\u4e00\u70b9\u667a\u80fd"], "title": "\u6e05\u534e\u3001NVIDIA\u3001\u65af\u5766\u798f\u63d0\u51faDiffusionNFT\uff1a\u57fa\u4e8e\u524d\u5411\u8fc7\u7a0b\u7684\u6269\u6563<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b0\u8303\u5f0f\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u534725\u500d", "comment": "Source: WeChat, Published: 2025-10-09 04:00:31", "summary": "nvidia deep imagination \u7814\u7a76\u7ec4\u4e0e\u65af\u5766\u798f stefano ermon \u56e2\u961f\u8054\u5408\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u6269\u6563\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u8303\u5f0f \u2014\u2014diffusion negative-aware finetuning \uff08diffusionnft\uff09\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u7a81\u7834\u73b0\u6709 RL \u5bf9\u6269\u6563\u6a21\u578b\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u76f4\u63a5\u5728\u524d\u5411\u52a0\u566a\u8fc7\u7a0b\uff08forward", "AI": {"tldr": "nvidia deep imagination \u7814\u7a76\u7ec4\u4e0e\u65af\u5766\u798f stefano ermon \u56e2\u961f\u8054\u5408\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u6269\u6563\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08rl\uff09\u8303\u5f0f \u2014\u2014diffusion negative-aware finetuning \uff08diffusionnft\uff09\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u7a81\u7834\u73b0\u6709 RL \u5bf9\u6269\u6563\u6a21\u578b\u7684\u57fa\u672c\u5047\u8bbe\uff0c\u76f4\u63a5\u5728\u524d\u5411\u52a0\u566a\u8fc7\u7a0b\uff08forward", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.80896a94", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMTg4OTgxMQ==&mid=2247532021&idx=2&sn=9842fbc83f1016ad268aac86e810fdc1&chksm=97a0b8b6d1daef37af65211e32fbe25b6afed25dd95ec77f9c3b29e5ecacd80b6c37c17eb996#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMTg4OTgxMQ==&mid=2247532021&idx=2&sn=9842fbc83f1016ad268aac86e810fdc1&chksm=97a0b8b6d1daef37af65211e32fbe25b6afed25dd95ec77f9c3b29e5ecacd80b6c37c17eb996#rd", "authors": ["\u673a\u68b0\u8fdb\u53162\u4eba\u5de5\u667a\u80fd"], "title": "\u5168\u65b0\u5408\u6210\u6846\u67b6SOTA\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5f53\u5f15\u64ce\uff0c\u4efb\u52a1\u5408\u6210\u5f53\u71c3\u6599\uff0c\u8682\u8681\u6e2f\u5927\u8054\u5408\u51fa\u54c1", "comment": "Source: WeChat, Published: 2025-10-09 03:00:00", "summary": "\u4e00\u662f\u5f3a\u5316\u5b66\u4e60\u3002\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e4b\u5e74\uff0c\u8be5\u9879\u6280\u672f\u5df2\u7ecf\u5f97\u5230\u793e\u533a\u8db3\u591f\u591a\u7684\u5173\u6ce8\u4e0e\u6295\u5165\uff0c\u65e0\u8bba\u662f\u65b9\u6cd5\u8fd8\u662f\u6846\u67b6\u90fd\u5728\u6025\u901f\u63a8\u8fdb\u3002reinforcement learning \u800c\u53e6\u4e00\u4e2a\uff0c\u56e2\u961f\u8ba4\u4e3a\u662f\u4efb\u52a1\u5408\u6210\u3002", "AI": {"tldr": "\u4e00\u662f\u5f3a\u5316\u5b66\u4e60\u3002\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e4b\u5e74\uff0c\u8be5\u9879\u6280\u672f\u5df2\u7ecf\u5f97\u5230\u793e\u533a\u8db3\u591f\u591a\u7684\u5173\u6ce8\u4e0e\u6295\u5165\uff0c\u65e0\u8bba\u662f\u65b9\u6cd5\u8fd8\u662f\u6846\u67b6\u90fd\u5728\u6025\u901f\u63a8\u8fdb\u3002reinforcement learning \u800c\u53e6\u4e00\u4e2a\uff0c\u56e2\u961f\u8ba4\u4e3a\u662f\u4efb\u52a1\u5408\u6210\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.3d0e184e", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNzUwMTk5NQ==&mid=2247497623&idx=1&sn=c638315732675e99cc3861f2bafeb82e&chksm=c0513a28a6021a47c3ee7ae36a40f471a24a6d9119925ec021946dbbfc37fbf42fdecd26211b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNzUwMTk5NQ==&mid=2247497623&idx=1&sn=c638315732675e99cc3861f2bafeb82e&chksm=c0513a28a6021a47c3ee7ae36a40f471a24a6d9119925ec021946dbbfc37fbf42fdecd26211b#rd", "authors": ["AI\u6545\u4e8b\u8ba1\u5212"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236\uff0c\u7ed9\u5927\u6a21\u578b\u5224\u4e86\u201c\u6b7b\u5211\u201d", "comment": "Source: WeChat, Published: 2025-10-09 02:38:49", "summary": "\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\u5176\u5b9e\u5f88\u7b80\u5355\uff1a\u8ba9AI\u50cf\u4eba\u7c7b\u5a74\u513f\u4e00\u6837\uff0c\u901a\u8fc7\u4e0d\u65ad\u8bd5\u9519\u6765\u5b66\u4e60\u3002\u6bd4\u5982\u5c0f\u5b69\u5b66\u8d70\u8def\uff0c\u6ca1\u4eba\u6559\u201c\u5148\u62ac\u5de6\u811a\u518d\u8fc8\u53f3\u811a\u201d\uff0c\u6454\u591a\u4e86\u81ea\u7136\u5c31\u4f1a\u4e86\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u601d\u60f3\u5176\u5b9e\u5f88\u7b80\u5355\uff1a\u8ba9AI\u50cf\u4eba\u7c7b\u5a74\u513f\u4e00\u6837\uff0c\u901a\u8fc7\u4e0d\u65ad\u8bd5\u9519\u6765\u5b66\u4e60\u3002\u6bd4\u5982\u5c0f\u5b69\u5b66\u8d70\u8def\uff0c\u6ca1\u4eba\u6559\u201c\u5148\u62ac\u5de6\u811a\u518d\u8fc8\u53f3\u811a\u201d\uff0c\u6454\u591a\u4e86\u81ea\u7136\u5c31\u4f1a\u4e86\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.a1619899", "categories": ["wechat.article", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994858&idx=1&sn=31a9600c26c8b2d8ca55941485022eae&chksm=b0a79eb944af77a6c43e036b8f08517b0a7114be5d1e0702af3fabbef3990eeea7ed522e6390#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994858&idx=1&sn=31a9600c26c8b2d8ca55941485022eae&chksm=b0a79eb944af77a6c43e036b8f08517b0a7114be5d1e0702af3fabbef3990eeea7ed522e6390#rd", "authors": ["\u667a\u7329\u7329GenAI"], "title": "\u667a\u80fd\u4f53<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b0SOTA\uff01\u963f\u91cc\u901a\u4e49\u8054\u5408\u63d0\u51fa\u5728\u7ebf\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60OPRL\uff0c\u517c\u5bb9GRPO\u4e0ePPO", "comment": "Source: WeChat, Published: 2025-10-09 02:30:30", "summary": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u88ab\u8bad\u7ec3\u4e3a\u80fd\u591f\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u5468\u671f\u63a8\u7406\u4e0e\u884c\u52a8\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u3002\u7136\u800c\uff0c\u7a00\u758f\u4e14\u6709\u65f6\u4e0d\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\u4f7f\u5f97\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\uff08temporal credit assignment\uff09\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u88ab\u8bad\u7ec3\u4e3a\u80fd\u591f\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u5468\u671f\u63a8\u7406\u4e0e\u884c\u52a8\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u3002\u7136\u800c\uff0c\u7a00\u758f\u4e14\u6709\u65f6\u4e0d\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\u4f7f\u5f97\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\uff08temporal credit assignment\uff09\u53d8\u5f97\u6781\u5177\u6311\u6218\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.09f7abc7", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwOTQyNzA5Mw==&mid=2247485961&idx=1&sn=d8b83c74a8f36b2f38ed962ff18d66b0&chksm=96572c219a0183eed9c5a9928028f4162f4a4c567f8eb5df7fa0fce37f58b9fe40e41dfad71a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwOTQyNzA5Mw==&mid=2247485961&idx=1&sn=d8b83c74a8f36b2f38ed962ff18d66b0&chksm=96572c219a0183eed9c5a9928028f4162f4a4c567f8eb5df7fa0fce37f58b9fe40e41dfad71a#rd", "authors": ["\u4e00\u82c7\u676d\u4e4bo"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236Richard Sutton\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u662f\u6b7b\u8def\u4e00\u6761\uff1f", "comment": "Source: WeChat, Published: 2025-10-08 21:17:18", "summary": "Sutton\u5728\u8bbf\u8c08\u4e2d\u5f00\u95e8\u89c1\u5c71\uff1a\u201c\u5f3a\u5316\u5b66\u4e60\u662f\u5173\u4e8e\u7406\u89e3\u4f60\u7684\u4e16\u754c\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5173\u4e8e\u6a21\u4eff\u4eba\u7c7b\u505a\u4eba\u4eec\u8bf4\u4f60\u5e94\u8be5\u505a\u7684\u4e8b\u60c5\uff0c\u4ed6\u4eec\u4e0d\u662f\u5728\u641e\u6e05\u695a\u8be5\u505a\u4ec0\u4e48\u3002\u201d", "AI": {"tldr": "Sutton\u5728\u8bbf\u8c08\u4e2d\u5f00\u95e8\u89c1\u5c71\uff1a\u201c\u5f3a\u5316\u5b66\u4e60\u662f\u5173\u4e8e\u7406\u89e3\u4f60\u7684\u4e16\u754c\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5173\u4e8e\u6a21\u4eff\u4eba\u7c7b\u505a\u4eba\u4eec\u8bf4\u4f60\u5e94\u8be5\u505a\u7684\u4e8b\u60c5\uff0c\u4ed6\u4eec\u4e0d\u662f\u5728\u641e\u6e05\u695a\u8be5\u505a\u4ec0\u4e48\u3002\u201d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.90c94c11", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NzA1MTYxNw==&mid=2247497277&idx=1&sn=c5fe9246cfb01b667c22e44a9c26125b&chksm=cf9c9019bb50b0c5c6eb52fc8be9c4542840c7220491e26a9ad3f71b0a6c5087d0f18d96eb76#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NzA1MTYxNw==&mid=2247497277&idx=1&sn=c5fe9246cfb01b667c22e44a9c26125b&chksm=cf9c9019bb50b0c5c6eb52fc8be9c4542840c7220491e26a9ad3f71b0a6c5087d0f18d96eb76#rd", "authors": ["\u629a\u7434\u8f69"], "title": "\u9ea6\u80af\u9521\uff1a<em class=\"highlight\">Agentic</em> AI", "comment": "Source: WeChat, Published: 2025-10-09 10:54:51", "summary": "Agentic AI\u7684\u5d1b\u8d77\u5e76\u975e\u751f\u6210\u5f0fAI\u7684\u7b80\u5355\u5347\u7ea7\uff0c\u800c\u662f\u4eba\u5de5\u667a\u80fd\u8303\u5f0f\u7684\u6839\u672c\u6027\u8f6c\u53d8\u3002\u4e0e\u4f20\u7edfAI\u5de5\u5177\"\u6307\u4ee4-\u54cd\u5e94\"\u7684\u88ab\u52a8\u6a21\u5f0f\u4e0d\u540c\uff0cAgentic AI\u5177\u5907\u4e09\u5927\u6838\u5fc3\u7279\u8d28\uff1a\u81ea\u4e3b\u89c4\u5212\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3001\u8de8\u5de5\u5177\u534f\u540c\u7684\u884c\u52a8\u529b\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u8fed\u4ee3\u6027\u3002", "AI": {"tldr": "Agentic AI\u7684\u5d1b\u8d77\u5e76\u975e\u751f\u6210\u5f0fAI\u7684\u7b80\u5355\u5347\u7ea7\uff0c\u800c\u662f\u4eba\u5de5\u667a\u80fd\u8303\u5f0f\u7684\u6839\u672c\u6027\u8f6c\u53d8\u3002\u4e0e\u4f20\u7edfAI\u5de5\u5177\"\u6307\u4ee4-\u54cd\u5e94\"\u7684\u88ab\u52a8\u6a21\u5f0f\u4e0d\u540c\uff0cAgentic AI\u5177\u5907\u4e09\u5927\u6838\u5fc3\u7279\u8d28\uff1a\u81ea\u4e3b\u89c4\u5212\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3001\u8de8\u5de5\u5177\u534f\u540c\u7684\u884c\u52a8\u529b\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u8fed\u4ee3\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.558585e9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwOTA1MDAyNA==&mid=2650040956&idx=3&sn=21607d4854a654529e467818f1faeb35&chksm=8e773f562ed5bfa19cc427cc6ff6ac7c0c11dcec0690e9d8a5db1cb17e7e04671c6abd01b215#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwOTA1MDAyNA==&mid=2650040956&idx=3&sn=21607d4854a654529e467818f1faeb35&chksm=8e773f562ed5bfa19cc427cc6ff6ac7c0c11dcec0690e9d8a5db1cb17e7e04671c6abd01b215#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u5b66\u5bb6"], "title": "\u4e00\u7bc7\u5927\u6a21\u578b<em class=\"highlight\">Agentic</em>\u6846\u67b6\u5230\u5e94\u7528\u6700\u65b0\u7efc\u8ff0\uff1a\u4e00\u6587\u8bfb\u61c2LLM\u63a8\u7406\u6846\u67b6\u7684\u524d\u6cbf\u8fdb\u5c55", "comment": "Source: WeChat, Published: 2025-10-09 10:00:04", "summary": "agentic ai\u6765\u6e90\uff1aAIGC\u6df1\u4e00\u5ea6llm-based agentic reasoning frameworks\uff1a a survey from methods to scenarios bingxi zhao*\uff0c beijing jiaotong university\uff0c china and lancaster university\uff0c united kingdom lin geng foo*\uff0c max planck institute for informatics\uff0c saarland informatics campus\uff0c germany", "AI": {"tldr": "agentic ai\u6765\u6e90\uff1aAIGC\u6df1\u4e00\u5ea6llm-based agentic reasoning frameworks\uff1a a survey from methods to scenarios bingxi zhao*\uff0c beijing jiaotong university\uff0c china and lancaster university\uff0c united kingdom lin geng foo*\uff0c ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.b3fa13dd", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU2OTY2OTQ0MA==&mid=2247549279&idx=1&sn=2d11d5d6ea4a0f4140ff12092cffc245&chksm=fdc8e3579e5bd2b55f62202b700d71596dc189622909aa408834dfdaa34e9bebc335ad7fe47e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU2OTY2OTQ0MA==&mid=2247549279&idx=1&sn=2d11d5d6ea4a0f4140ff12092cffc245&chksm=fdc8e3579e5bd2b55f62202b700d71596dc189622909aa408834dfdaa34e9bebc335ad7fe47e#rd", "authors": ["Azure\u4e91\u79d1\u6280"], "title": "\u501f\u52a9 <em class=\"highlight\">Agentic</em> AI \u52a0\u901f\u8fc1\u79fb\u4e0e\u73b0\u4ee3\u5316", "comment": "Source: WeChat, Published: 2025-10-09 09:05:14", "summary": "\u7ed3\u5408 Agentic AI\uff08\u56fd\u9645\u7248\uff09\u4e0e\u7aef\u5230\u7aef\u652f\u6301\uff0c\u60a8\u80fd\u6bd4\u4ee5\u5f80\u66f4\u8f7b\u677e\u5730\u5f3a\u52bf\u8d77\u6b65\u5e76\u5feb\u901f\u6269\u5c55\u3002\u65e0\u8bba\u60a8\u662f\u521a\u521a\u5f00\u542f\u4e0a\u4e91\u4e4b\u65c5\uff0c\u8fd8\u662f\u6b63\u5728\u5c06 AI \u5e94\u7528\u6269\u5c55\u5230\u6574\u4e2a\u7ec4\u7ec7\uff0cAzure \u90fd\u80fd\u4e3a\u60a8\u63d0\u4f9b\u6e05\u6670\u7684\u8def\u5f84\u3001\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u652f\u6301\uff0c\u52a9\u60a8\u81ea\u4fe1\u524d\u884c\u3002", "AI": {"tldr": "\u7ed3\u5408 Agentic AI\uff08\u56fd\u9645\u7248\uff09\u4e0e\u7aef\u5230\u7aef\u652f\u6301\uff0c\u60a8\u80fd\u6bd4\u4ee5\u5f80\u66f4\u8f7b\u677e\u5730\u5f3a\u52bf\u8d77\u6b65\u5e76\u5feb\u901f\u6269\u5c55\u3002\u65e0\u8bba\u60a8\u662f\u521a\u521a\u5f00\u542f\u4e0a\u4e91\u4e4b\u65c5\uff0c\u8fd8\u662f\u6b63\u5728\u5c06 AI \u5e94\u7528\u6269\u5c55\u5230\u6574\u4e2a\u7ec4\u7ec7\uff0cAzure \u90fd\u80fd\u4e3a\u60a8\u63d0\u4f9b\u6e05\u6670\u7684\u8def\u5f84\u3001\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u5f3a\u7684\u652f\u6301\uff0c\u52a9\u60a8\u81ea\u4fe1\u524d\u884c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.630e1585", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDU5NjcxOQ==&mid=2247484145&idx=1&sn=d62f41aa4bc2fb03ccc9d05735344e6a&chksm=c5fe73ee6504324d151bad5c12fb08a947d73e6479bcf8847d5004b7610b9c9b59dc9666c640#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDU5NjcxOQ==&mid=2247484145&idx=1&sn=d62f41aa4bc2fb03ccc9d05735344e6a&chksm=c5fe73ee6504324d151bad5c12fb08a947d73e6479bcf8847d5004b7610b9c9b59dc9666c640#rd", "authors": ["ORI\u5965\u9510\u65b9"], "title": "BPMN2.0\u7684<em class=\"highlight\">Agentic</em>\u8fdb\u5316", "comment": "Source: WeChat, Published: 2025-10-09 08:38:53", "summary": "1.\u5b83\u88ab\u8bc6\u522b\u4e3a\u4e00\u4e2aAgentic Node\uff08\u667a\u80fd\u4f53\u8282\u70b9\uff09\uff1b2.\u5185\u90e8\u7684\u4efb\u52a1\u88ab\u89c6\u4e3a\u53ef\u4f9b\u9009\u62e9\u7684\u201c\u884c\u52a8\u5019\u9009\u96c6\u201d\uff0c\u8fd9\u91cc\u53ef\u4ee5\u7406\u89e3\u4e3aReAct\u6a21\u5f0f\u4e2d\u7684\u53ef\u6267\u884c\u7684\u5de5\u5177\u96c6\uff08Tools\uff09\uff1b", "AI": {"tldr": "1.\u5b83\u88ab\u8bc6\u522b\u4e3a\u4e00\u4e2aAgentic Node\uff08\u667a\u80fd\u4f53\u8282\u70b9\uff09\uff1b2.\u5185\u90e8\u7684\u4efb\u52a1\u88ab\u89c6\u4e3a\u53ef\u4f9b\u9009\u62e9\u7684\u201c\u884c\u52a8\u5019\u9009\u96c6\u201d\uff0c\u8fd9\u91cc\u53ef\u4ee5\u7406\u89e3\u4e3aReAct\u6a21\u5f0f\u4e2d\u7684\u53ef\u6267\u884c\u7684\u5de5\u5177\u96c6\uff08Tools\uff09\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.23dcccd9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyODI1MzYyNA==&mid=2653549781&idx=1&sn=13a0fc81e4d7a3a07de19261d8abd73b&chksm=f2ff5331197b3b163f20ea47e449d9bc97a3b80b19e99d07666aa2adf3cbd0ff02adaa09dd6a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyODI1MzYyNA==&mid=2653549781&idx=1&sn=13a0fc81e4d7a3a07de19261d8abd73b&chksm=f2ff5331197b3b163f20ea47e449d9bc97a3b80b19e99d07666aa2adf3cbd0ff02adaa09dd6a#rd", "authors": ["\u7389\u6811\u829d\u5170"], "title": "\u5982\u4f55\u7528 <em class=\"highlight\">Agentic</em> AI \u4ea7\u54c1\u5e2e\u4f60\u6253\u9020 <em class=\"highlight\">Agentic</em> AI \u7cfb\u7edf\uff0c\u81ea\u52a8\u5904\u7406\u590d\u6742\u4efb\u52a1\uff1f", "comment": "Source: WeChat, Published: 2025-10-09 08:09:06", "summary": "Agentic AI \u7cfb\u7edf\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u5de5\u4f5c\u6d41\uff0c\u6211\u4eec\u5b9e\u9645\u4e0a\u662f\u8ba9\u4e0d\u540c\u7684\u5927\u6a21\u578b\u627f\u62c5\u4e0d\u540c\u7684\u89d2\u8272\u4e0e\u4efb\u52a1\uff0c\u5e76\u76f8\u4e92\u914d\u5408\uff0c\u5b8c\u6210\u590d\u6742\u7684\u4efb\u52a1\u4e43\u81f3\u5b8c\u6574\u9879\u76ee\u3002\u5728\u4ee5\u524d\uff0c\u8fd9\u6837\u7684\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5148\u719f\u7ec3\u638c\u63e1 CrewAI \u6216\u8005 Agno \u8fd9\u6837\u7684\u6846\u67b6\uff0c\u624d\u80fd\u6162\u6162\u642d\u5efa\u3002", "AI": {"tldr": "Agentic AI \u7cfb\u7edf\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u5de5\u4f5c\u6d41\uff0c\u6211\u4eec\u5b9e\u9645\u4e0a\u662f\u8ba9\u4e0d\u540c\u7684\u5927\u6a21\u578b\u627f\u62c5\u4e0d\u540c\u7684\u89d2\u8272\u4e0e\u4efb\u52a1\uff0c\u5e76\u76f8\u4e92\u914d\u5408\uff0c\u5b8c\u6210\u590d\u6742\u7684\u4efb\u52a1\u4e43\u81f3\u5b8c\u6574\u9879\u76ee\u3002\u5728\u4ee5\u524d\uff0c\u8fd9\u6837\u7684\u5de5\u4f5c\uff0c\u4f60\u9700\u8981\u5148\u719f\u7ec3\u638c\u63e1 CrewAI \u6216\u8005 Agno \u8fd9\u6837\u7684\u6846\u67b6\uff0c\u624d\u80fd\u6162\u6162\u642d\u5efa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.1fd4e602", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk4ODAyOTIxNQ==&mid=2247491147&idx=1&sn=fb925f90526486195adc99ff9dfed7e0&chksm=c4decb69d593abd544041b85e45076343b94a0a2a2a3512e3b9f13c4d9f4578757116485174f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk4ODAyOTIxNQ==&mid=2247491147&idx=1&sn=fb925f90526486195adc99ff9dfed7e0&chksm=c4decb69d593abd544041b85e45076343b94a0a2a2a3512e3b9f13c4d9f4578757116485174f#rd", "authors": ["\u4e07\u6d82\u5e7b\u8c61"], "title": "\u8ddf\u7740\u5434\u6069\u8fbe\u5b66<em class=\"highlight\">Agentic</em>\uff1a\u5982\u4f55\u7528\u4e00\u5957\u5de5\u4f5c\u6d41\uff0c\u8ba9GPT-3.5\u7684\u8868\u73b0\u8d85\u8d8aGPT-4", "comment": "Source: WeChat, Published: 2025-10-09 07:28:32", "summary": "\u6240\u4ee5\u4ed6\u5e72\u8106\u81ea\u5df1\u9020\u4e86\u4e2a\u8bcd \u201cAgentic\u201d\u3002IT'S AN AGENT\uff01IT'S AGENTIC\uff01NO\uff0cIT'S NOT\uff01ai fund sequoia ascent\uff0c march 2024 x \uff08twitter\uff09 post \uff0c june 2024\u76ee\u7684\u5c31\u662f\u4e3a\u4e86\u8df3\u51fa\u201c\u662f\u4e0d\u662f\u201d\u7684\u6807\u7b7e\u4e4b\u4e89\uff0c\u8fdb\u5165\u201c\u6709\u591a\u5c11\u201d\u7684\u5de5\u7a0b\u5b9e\u8df5\u3002", "AI": {"tldr": "\u6240\u4ee5\u4ed6\u5e72\u8106\u81ea\u5df1\u9020\u4e86\u4e2a\u8bcd \u201cAgentic\u201d\u3002IT'S AN AGENT\uff01IT'S AGENTIC\uff01NO\uff0cIT'S NOT\uff01ai fund sequoia ascent\uff0c march 2024 x \uff08twitter\uff09 post \uff0c june 2024\u76ee\u7684\u5c31\u662f\u4e3a\u4e86\u8df3\u51fa\u201c\u662f\u4e0d\u662f\u201d\u7684\u6807\u7b7e\u4e4b\u4e89\uff0c\u8fdb\u5165\u201c\u6709\u591a\u5c11\u201d\u7684\u5de5\u7a0b\u5b9e\u8df5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.22a28ca4", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5MzgyNzY1NA==&mid=2247501715&idx=1&sn=b2a1cb8e3b925368656edc37405181dd&chksm=ff436056bb5a6f6c8356ec7cb51bc8146f230029c8a6659e42a73418385bd56e40e114c34e32#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5MzgyNzY1NA==&mid=2247501715&idx=1&sn=b2a1cb8e3b925368656edc37405181dd&chksm=ff436056bb5a6f6c8356ec7cb51bc8146f230029c8a6659e42a73418385bd56e40e114c34e32#rd", "authors": ["Software AG Asia"], "title": "\u5982\u4f55\u5c06 <em class=\"highlight\">Agentic</em> AI \u4e0e\u60a8\u7684\u6d41\u7a0b\u76f8\u5339\u914d", "comment": "Source: WeChat, Published: 2025-10-09 02:29:20", "summary": "\u672c\u6587\u65e8\u5728\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u5b9e\u8df5\u8005\u6216Agentic AI \u67b6\u6784\u5e08\u63d0\u4f9b\u652f\u6301\u3002 \u672a\u6765\u5df2\u81f3\u2014\u2014\u8fd1\u5728\u54ab\u5c3a\u5f53\u524d\uff082025\u5e74\uff09\u6700\u5f15\u4eba\u5165\u80dc\u5374\u4e5f\u5907\u53d7\u7092\u4f5c\u7684\u8bdd\u9898\u4e4b\u4e00\uff0c\u5f53\u5c5e agentic AI\u3002", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u4e3a\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u5b9e\u8df5\u8005\u6216Agentic AI \u67b6\u6784\u5e08\u63d0\u4f9b\u652f\u6301\u3002 \u672a\u6765\u5df2\u81f3\u2014\u2014\u8fd1\u5728\u54ab\u5c3a\u5f53\u524d\uff082025\u5e74\uff09\u6700\u5f15\u4eba\u5165\u80dc\u5374\u4e5f\u5907\u53d7\u7092\u4f5c\u7684\u8bdd\u9898\u4e4b\u4e00\uff0c\u5f53\u5c5e agentic AI\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.04493635", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMDcwMDExOQ==&mid=2247493176&idx=1&sn=9c62458738a5daf98df7ee760bfdbfaa&chksm=c09b2bd1a2472a4f0fee17fdb2a45dfcca85cab2e702efbbd7ccdd9051e6bb5c3eace6b32422#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMDcwMDExOQ==&mid=2247493176&idx=1&sn=9c62458738a5daf98df7ee760bfdbfaa&chksm=c09b2bd1a2472a4f0fee17fdb2a45dfcca85cab2e702efbbd7ccdd9051e6bb5c3eace6b32422#rd", "authors": ["AIGC \u6df1\u4e00\u5ea6"], "title": "\u4e00\u7bc7\u5927\u6a21\u578b<em class=\"highlight\">Agentic</em>\u6846\u67b6\u5230\u5e94\u7528\u6700\u65b0\u7efc\u8ff0\uff1a\u4e00\u6587\u8bfb\u61c2LLM\u63a8\u7406\u6846\u67b6\u7684\u524d\u6cbf\u8fdb\u5c55", "comment": "Source: WeChat, Published: 2025-10-08 23:37:36", "summary": "llm-based agentic reasoning frameworks\uff1a a survey from methods to scenarios bingxi zhao*\uff0c beijing jiaotong university\uff0c china and lancaster university\uff0c united kingdom lin geng foo*\uff0c max planck institute for informatics\uff0c saarland informatics campus\uff0c germany ping hu\uff0c university of electr", "AI": {"tldr": "llm-based agentic reasoning frameworks\uff1a a survey from methods to scenarios bingxi zhao*\uff0c beijing jiaotong university\uff0c china and lancaster university\uff0c united kingdom lin geng foo*\uff0c max planck institute...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.35cb9f36", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTcxNzUyMQ==&mid=2247484687&idx=1&sn=fbf4bbff429bc48e10fa0354532d42e5&chksm=97b78898a6227b6b0d27df5635ca1a449d8de60dcd4e955867a88f4397f276902274e8bbe95e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTcxNzUyMQ==&mid=2247484687&idx=1&sn=fbf4bbff429bc48e10fa0354532d42e5&chksm=97b78898a6227b6b0d27df5635ca1a449d8de60dcd4e955867a88f4397f276902274e8bbe95e#rd", "authors": ["AI\u4ea7\u54c1\u8fdb\u5316\u5f55"], "title": "AI<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u5230\u5e95\u662f\u4ec0\u4e48\uff1f\u4ece<em class=\"highlight\">Agentic</em> AI\u5230AI Agent\uff0c\u4e00\u7bc7\u8bb2\u900f\u6838\u5fc3\u539f\u7406\uff01", "comment": "Source: WeChat, Published: 2025-10-08 23:07:41", "summary": "\u201cAgentic\u201d \u6765\u81ea\u82f1\u6587 \u201cAgent\u201d\uff0c\u610f\u601d\u662f\u201c\u5177\u6709\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u5bfc\u5411\u7684\u5b9e\u4f53\u201d\u3002\u6240\u4ee5\uff0cAgentic AI = \u5177\u6709\u201c\u4e3b\u52a8\u6027\u201d\u7684AI\u3002\u4e0e\u4f20\u7edfAI\uff08\u88ab\u52a8\u54cd\u5e94\uff09\u4e0d\u540c\uff0cAgentic AI \u7684\u5173\u952e\u8bcd\u662f\uff1a", "AI": {"tldr": "\u201cAgentic\u201d \u6765\u81ea\u82f1\u6587 \u201cAgent\u201d\uff0c\u610f\u601d\u662f\u201c\u5177\u6709\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u5bfc\u5411\u7684\u5b9e\u4f53\u201d\u3002\u6240\u4ee5\uff0cAgentic AI = \u5177\u6709\u201c\u4e3b\u52a8\u6027\u201d\u7684AI\u3002\u4e0e\u4f20\u7edfAI\uff08\u88ab\u52a8\u54cd\u5e94\uff09\u4e0d\u540c\uff0cAgentic AI \u7684\u5173\u952e\u8bcd\u662f\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.3257b662", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4OTczODQwNw==&mid=2247487805&idx=1&sn=d0d1408840017965d80dc3b1dc1910f5&chksm=fc2f47902166f15ae40b0e63a2ef870f2f84f23e4f6c36a866ce724bf30e09d43e34b0a04c46#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4OTczODQwNw==&mid=2247487805&idx=1&sn=d0d1408840017965d80dc3b1dc1910f5&chksm=fc2f47902166f15ae40b0e63a2ef870f2f84f23e4f6c36a866ce724bf30e09d43e34b0a04c46#rd", "authors": ["\u653e\u725b\u5a03\u7684\u6742\u8d27\u94fa"], "title": "\u5f00\u6e90\u7684\u8bed\u97f3\u7406\u89e3<em class=\"highlight\">\u5927\u6a21\u578b</em>\u2014\u2014OSUM", "comment": "Source: WeChat, Published: 2025-10-09 12:21:15", "summary": "\u73b0\u6709\u7684SULM\u6a21\u578b\uff0c\u5982Whisper\u3001Qwen-audio\u3001SenseVoice\u548cTouchASP\u7b49\uff0c\u5927\u90e8\u5206\u7531\u5de5\u4e1a\u754c\u5f00\u53d1\uff0c\u5229\u7528\u4e86\u6570\u767e\u4e07\u5c0f\u65f6\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6d77\u91cfGPU\u8d44\u6e90\u3002\u8fd9\u6837\u7684\u5927\u89c4\u6a21\u8d44\u6e90\u901a\u5e38\u8d85\u51fa\u4e86\u5b66\u672f\u754c\u673a\u6784\u7684\u80fd\u529b\u8303\u56f4\u3002", "AI": {"tldr": "\u73b0\u6709\u7684SULM\u6a21\u578b\uff0c\u5982Whisper\u3001Qwen-audio\u3001SenseVoice\u548cTouchASP\u7b49\uff0c\u5927\u90e8\u5206\u7531\u5de5\u4e1a\u754c\u5f00\u53d1\uff0c\u5229\u7528\u4e86\u6570\u767e\u4e07\u5c0f\u65f6\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6d77\u91cfGPU\u8d44\u6e90\u3002\u8fd9\u6837\u7684\u5927\u89c4\u6a21\u8d44\u6e90\u901a\u5e38\u8d85\u51fa\u4e86\u5b66\u672f\u754c\u673a\u6784\u7684\u80fd\u529b\u8303\u56f4\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.275d351c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNDI5NjIxMw==&mid=2247500076&idx=2&sn=3654ae1efd794ce22ee973ebbe804262&chksm=9a8dceb684a2198811985cf399765ee38d10bea399a3d50415ea62047f694517857ce780e3c3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNDI5NjIxMw==&mid=2247500076&idx=2&sn=3654ae1efd794ce22ee973ebbe804262&chksm=9a8dceb684a2198811985cf399765ee38d10bea399a3d50415ea62047f694517857ce780e3c3#rd", "authors": ["\u8001\u53f8\u673a\u9a7e\u65b0\u8f66"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8fdb\u5c55\u4e13\u9898\u89e3\u8bfb", "comment": "Source: WeChat, Published: 2025-10-09 11:06:54", "summary": "\u5927\u6a21\u578b\u8fdb\u5c55\u4e13\u9898\u89e3\u8bfb \u4e00\u3001\u5927\u6a21\u578b\u6027\u80fd\u6bd4\u62fc\uff1a\u6280\u672f\u7a81\u7834\u4e0e\u5e02\u573a\u8d8b\u52bf\u5c3d\u663e \u667a\u8c314.6\uff1a\u4ee3\u7801\u80fd\u529b\u9886\u8854\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u6295\u8d44\u6f5c\u529b\u51f8\u663e\uff1a\u667a\u8c314.6\u8282\u524d\u53d1\u5e03\u540e\u767b\u9876huggingface\u8d8b\u52bf\u699c\u7b2c\u4e00\uff0c\u5176\u80fd\u529b\u6838\u5fc3\u6e90\u4e8e4.5\u7248\u672c\uff0c4.5\u7248\u672c\u66fe\u4f7fGPT5\u63a8\u8fdf\u53d1\u5e03\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u8fdb\u5c55\u4e13\u9898\u89e3\u8bfb \u4e00\u3001\u5927\u6a21\u578b\u6027\u80fd\u6bd4\u62fc\uff1a\u6280\u672f\u7a81\u7834\u4e0e\u5e02\u573a\u8d8b\u52bf\u5c3d\u663e \u667a\u8c314.6\uff1a\u4ee3\u7801\u80fd\u529b\u9886\u8854\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u6295\u8d44\u6f5c\u529b\u51f8\u663e\uff1a\u667a\u8c314.6\u8282\u524d\u53d1\u5e03\u540e\u767b\u9876huggingface\u8d8b\u52bf\u699c\u7b2c\u4e00\uff0c\u5176\u80fd\u529b\u6838\u5fc3\u6e90\u4e8e4.5\u7248\u672c\uff0c4.5\u7248\u672c\u66fe\u4f7fGPT5\u63a8\u8fdf\u53d1\u5e03\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.a0976846", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652790362&idx=1&sn=c328e79b923fe4e3eb70ebae355c237a&chksm=8557a4032629dce0f77e800d43edb5760b9c1ae311858618873cc0587ec879d71df3a994091b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652790362&idx=1&sn=c328e79b923fe4e3eb70ebae355c237a&chksm=8557a4032629dce0f77e800d43edb5760b9c1ae311858618873cc0587ec879d71df3a994091b#rd", "authors": ["\u667a\u4e1c\u897f"], "title": "3.8\u4ebf<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5927\u5355\uff01\u8baf\u98de\u62ff\u4e0b\uff0c\u534e\u4e3a\u5b87\u6811\u90fd\u8d5a\u4e86", "comment": "Source: WeChat, Published: 2025-10-09 10:30:42", "summary": "\u8fd9\u4e9b\u573a\u666f\u7684\u8f6f\u4ef6\u53ca\u786c\u4ef6\u65b9\u6848\u90fd\u6df1\u5ea6\u878d\u5165\u4e86\u5927\u6a21\u578b\u3002\u6574\u4e2a\u62a5\u4ef7\u6587\u4ef6\u4e2d\u201c\u5927\u6a21\u578b\u201d\u51fa\u73b0\u4e8679\u6b21\uff0c\u201c\u667a\u80fd\u4f53\u201d\u51fa\u73b0\u4e8696\u6b21\u3002\u4ee5AI+\u6559\u80b2\u4e3a\u4f8b\uff0c\u8be5\u65b9\u6848\u5f15\u5165\u4e86\u5927\u6a21\u578bAI\u6559\u5e08\u5907\u6388\u8bfe\u52a9\u624b\uff0c\u91c7\u7528\u79d1\u5927\u8baf\u98de\u7684\u8baf\u98de\u8bfe\u7a0b\u8d44\u6e90\u7cfb\u7edfV5.0\uff0c\u5305\u62ec\u6559\u6750\u7535\u5b50\u5316", "AI": {"tldr": "\u8fd9\u4e9b\u573a\u666f\u7684\u8f6f\u4ef6\u53ca\u786c\u4ef6\u65b9\u6848\u90fd\u6df1\u5ea6\u878d\u5165\u4e86\u5927\u6a21\u578b\u3002\u6574\u4e2a\u62a5\u4ef7\u6587\u4ef6\u4e2d\u201c\u5927\u6a21\u578b\u201d\u51fa\u73b0\u4e8679\u6b21\uff0c\u201c\u667a\u80fd\u4f53\u201d\u51fa\u73b0\u4e8696\u6b21\u3002\u4ee5AI+\u6559\u80b2\u4e3a\u4f8b\uff0c\u8be5\u65b9\u6848\u5f15\u5165\u4e86\u5927\u6a21\u578bAI\u6559\u5e08\u5907\u6388\u8bfe\u52a9\u624b\uff0c\u91c7\u7528\u79d1\u5927\u8baf\u98de\u7684\u8baf\u98de\u8bfe\u7a0b\u8d44\u6e90\u7cfb\u7edfV5.0\uff0c\u5305\u62ec\u6559\u6750\u7535\u5b50\u5316", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.4485b8f5", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODIzNTc2MA==&mid=2661065374&idx=2&sn=1706e04c0f91d32c283f763e42aaa0d5&chksm=bc7cac4df1bd2dd88e0bf729d24793fcf5db07d99a4fb4eac69807d4a88cdf4cb809c36b7b81#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODIzNTc2MA==&mid=2661065374&idx=2&sn=1706e04c0f91d32c283f763e42aaa0d5&chksm=bc7cac4df1bd2dd88e0bf729d24793fcf5db07d99a4fb4eac69807d4a88cdf4cb809c36b7b81#rd", "authors": ["\u949b\u5a92\u4f53"], "title": "\u8682\u8681\u3001OpenAI\u3001DeepSeek\u5377\u75af\u4e86\uff01\u56fd\u4ea7\u6700\u5f3a\u4e07\u4ebf\u53c2\u6570\u65d7\u8230<em class=\"highlight\">\u6a21\u578b</em>\u5f00\u6e90", "comment": "Source: WeChat, Published: 2025-10-09 10:16:49", "summary": "\u636e\u6089\uff0cLing-1T\u662f\u8682\u8681\u767e\u7075\u5927\u6a21\u578bLing 2.0 \u7cfb\u5217\u7684\u7b2c\u4e00\u6b3e\u65d7\u8230\u6a21\u578b\uff0c\u4e5f\u662f\u8682\u8681\u767e\u7075\u56e2\u961f\u8fc4\u4eca\u4e3a\u6b62\u63a8\u51fa\u7684\u89c4\u6a21\u6700\u5927\u3001\u80fd\u529b\u6700\u5f3a\u7684\u975e\u601d\u8003\u5927\u6a21\u578b\u3002\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u663e\u793a\uff0c\u5728\u6709\u9650\u8f93\u51faToken\u6761\u4ef6\u4e0b\uff0cLing-1T\u4e8e\u591a\u9879\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e2d\u53d6\u5f97SOTA\u8868\u73b0\uff0c\u540c\u65f6", "AI": {"tldr": "\u636e\u6089\uff0cLing-1T\u662f\u8682\u8681\u767e\u7075\u5927\u6a21\u578bLing 2.0 \u7cfb\u5217\u7684\u7b2c\u4e00\u6b3e\u65d7\u8230\u6a21\u578b\uff0c\u4e5f\u662f\u8682\u8681\u767e\u7075\u56e2\u961f\u8fc4\u4eca\u4e3a\u6b62\u63a8\u51fa\u7684\u89c4\u6a21\u6700\u5927\u3001\u80fd\u529b\u6700\u5f3a\u7684\u975e\u601d\u8003\u5927\u6a21\u578b\u3002\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u663e\u793a\uff0c\u5728\u6709\u9650\u8f93\u51faToken\u6761\u4ef6\u4e0b\uff0cLing-1T\u4e8e\u591a\u9879\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e2d\u53d6\u5f97SOTA\u8868\u73b0\uff0c\u540c\u65f6", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.d51599f0", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MjcxOTg2MA==&mid=2247486962&idx=1&sn=f90347a2a1480db35dcd368af0306113&chksm=c29e13dbab2769d0bd44358065a6000df45ba392486504054efdf0128c2ab17e7d87183d0236#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MjcxOTg2MA==&mid=2247486962&idx=1&sn=f90347a2a1480db35dcd368af0306113&chksm=c29e13dbab2769d0bd44358065a6000df45ba392486504054efdf0128c2ab17e7d87183d0236#rd", "authors": ["\u65b0\u95fb\u4f20\u64ad\u5b66\u520a"], "title": "[\u667a\u80fd\u4f20\u64ad]\u8d75\u5b50\u5fe0\u4e28<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\uff1a\u5a92\u4f53\u667a\u80fd\u5316\u7684\u521b\u65b0\u4e0e\u53d1\u5c55", "comment": "Source: WeChat, Published: 2025-10-09 08:01:00", "summary": "\u5728\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u8fdb\u7a0b\u4e2d\uff0c\u5927\u6a21\u578b\u4f01\u4e1a\u901a\u8fc7\u6280\u672f\u5f00\u653e\u548c\u5de5\u5177\u4e0b\u6c89\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u667a\u80fd\u5e94\u7528\u80fd\u529b\u548c\u53c2\u4e0e\u5ea6\u3002\u4ee4\u5c0f\u96c4\u7b49\u5b66\u8005\u6307\u51fa\uff0c\u667a\u80fd\u6280\u672f\u4e0e\u4f20\u5a92\u4e1a\u52a1\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u6b63\u4ece\u6839\u672c\u4e0a\u6709\u529b\u8d4b\u80fd\u5a92\u4f53\u5185\u5bb9\u751f\u4ea7\u3002", "AI": {"tldr": "\u5728\u667a\u80fd\u4f53\u7684\u53d1\u5c55\u8fdb\u7a0b\u4e2d\uff0c\u5927\u6a21\u578b\u4f01\u4e1a\u901a\u8fc7\u6280\u672f\u5f00\u653e\u548c\u5de5\u5177\u4e0b\u6c89\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7684\u667a\u80fd\u5e94\u7528\u80fd\u529b\u548c\u53c2\u4e0e\u5ea6\u3002\u4ee4\u5c0f\u96c4\u7b49\u5b66\u8005\u6307\u51fa\uff0c\u667a\u80fd\u6280\u672f\u4e0e\u4f20\u5a92\u4e1a\u52a1\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u6b63\u4ece\u6839\u672c\u4e0a\u6709\u529b\u8d4b\u80fd\u5a92\u4f53\u5185\u5bb9\u751f\u4ea7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.29191c79", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk3NTI3ODI4MA==&mid=2247484157&idx=1&sn=2d3f7c9a0f427526c20c9aebca960e74&chksm=c5841a4a950d7b0e4a20b372397d4c68a315de06232cec19ae15a2ef4c8283281002c3f8c7b4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk3NTI3ODI4MA==&mid=2247484157&idx=1&sn=2d3f7c9a0f427526c20c9aebca960e74&chksm=c5841a4a950d7b0e4a20b372397d4c68a315de06232cec19ae15a2ef4c8283281002c3f8c7b4#rd", "authors": ["\u5927\u6a21\u578b\u89c2\u6d4b\u5458"], "title": "\u5927\u8bed\u8a00<em class=\"highlight\">\u6a21\u578b</em>-\u7f16\u7a0b\u80fd\u529b\u6d4b\u8bc4 25-10\u6708\u699c", "comment": "Source: WeChat, Published: 2025-10-09 06:04:15", "summary": "\u591a\u8f6e\uff1a\u6d4b\u8bd5\u6846\u67b6\u81ea\u52a8\u8fdb\u884c\u5927\u6a21\u578b\u8f93\u51fa\u7684\u7f16\u8bd1\u548c\u7528\u4f8b\u6d4b\u8bd5\uff0c\u5c06\u7f16\u8bd1\u9519\u8bef\uff0c\u8fd0\u884c\u9519\u8bef\uff0c\u6216\u4e0d\u901a\u8fc7\u7684\u7528\u4f8b\u4f5c\u4e3a\u591a\u8f6e\u8f93\u5165\uff0c\u8ba9\u5927\u6a21\u578b\u8fdb\u884c\u4fee\u6539\uff0c\u76f4\u5230\u6240\u6709\u7528\u4f8b\u901a\u8fc7\uff0c\u6216\u8fbe\u5230\u8bbe\u5b9a\u6700\u9ad8\u8f6e\u6570\uff08\u76ee\u524d\u662f3\u8f6e\uff09\u3002", "AI": {"tldr": "\u591a\u8f6e\uff1a\u6d4b\u8bd5\u6846\u67b6\u81ea\u52a8\u8fdb\u884c\u5927\u6a21\u578b\u8f93\u51fa\u7684\u7f16\u8bd1\u548c\u7528\u4f8b\u6d4b\u8bd5\uff0c\u5c06\u7f16\u8bd1\u9519\u8bef\uff0c\u8fd0\u884c\u9519\u8bef\uff0c\u6216\u4e0d\u901a\u8fc7\u7684\u7528\u4f8b\u4f5c\u4e3a\u591a\u8f6e\u8f93\u5165\uff0c\u8ba9\u5927\u6a21\u578b\u8fdb\u884c\u4fee\u6539\uff0c\u76f4\u5230\u6240\u6709\u7528\u4f8b\u901a\u8fc7\uff0c\u6216\u8fbe\u5230\u8bbe\u5b9a\u6700\u9ad8\u8f6e\u6570\uff08\u76ee\u524d\u662f3\u8f6e\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.4ae0bc21", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4NjY2MDYxMg==&mid=2247531105&idx=2&sn=3c6b7c3de9677721055fbddd9d13088a&chksm=fc2bb39593c1053c6acb62f6030ce1bc563e25c53f6022a5d8a250b3fd831e626aeacc30502c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4NjY2MDYxMg==&mid=2247531105&idx=2&sn=3c6b7c3de9677721055fbddd9d13088a&chksm=fc2bb39593c1053c6acb62f6030ce1bc563e25c53f6022a5d8a250b3fd831e626aeacc30502c#rd", "authors": ["\u58a8\u73ab\u4eba\u5de5\u667a\u80fd"], "title": "\u3010\u62a5\u544a\u3011<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e13\u9898\u4e8c\uff1a2025<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e73\u53f0\u843d\u5730\u5b9e\u8df5\u7814\u7a76\u62a5\u544a\uff08\u9644PDF\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-10-09 06:00:32", "summary": "\u5927\u6a21\u578b\u5e73\u53f0\u662f\u5927\u6a21\u578b\u843d\u5730\u7684\u5de5\u7a0b\u5316\u57fa\u5ea7\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u4e1a \u52a1\u573a\u666f\u4e0b\u7684\u5927\u6a21\u578b\u5e94\u7528\uff0c\u6784\u5efa\u8d77\u8986\u76d6\u6a21\u578b\u5f00\u53d1\u3001\u8c03\u4f18\u3001\u90e8\u7f72\u3001\u7ba1\u7406\u3001 \u5e94\u7528\u7684\u5168\u751f\u547d\u5468\u671f\u5de5\u5177\u94fe\u4e0e\u6280\u672f\u652f\u6491\u4f53\u7cfb\uff0c\u4e3a\u5927\u6a21\u578b\u843d\u5730\u8fc7\u7a0b\u4e2d\u9762 \u4e34\u7684\u6280\u672f\u8f6c\u5316\u3001\u573a\u666f\u9002\u914d\u3001\u6548", "AI": {"tldr": "\u5927\u6a21\u578b\u5e73\u53f0\u662f\u5927\u6a21\u578b\u843d\u5730\u7684\u5de5\u7a0b\u5316\u57fa\u5ea7\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u8f6c\u5316\u4e3a\u4e1a \u52a1\u573a\u666f\u4e0b\u7684\u5927\u6a21\u578b\u5e94\u7528\uff0c\u6784\u5efa\u8d77\u8986\u76d6\u6a21\u578b\u5f00\u53d1\u3001\u8c03\u4f18\u3001\u90e8\u7f72\u3001\u7ba1\u7406\u3001 \u5e94\u7528\u7684\u5168\u751f\u547d\u5468\u671f\u5de5\u5177\u94fe\u4e0e\u6280\u672f\u652f\u6491\u4f53\u7cfb\uff0c\u4e3a\u5927\u6a21\u578b\u843d\u5730\u8fc7\u7a0b\u4e2d\u9762 \u4e34\u7684\u6280\u672f\u8f6c\u5316\u3001\u573a\u666f\u9002\u914d\u3001\u6548", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.6a8da676", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzU4MDMyMw==&mid=2247492879&idx=2&sn=e1d8c1e95753850a57e80efa724f9922&chksm=9e044d9d77f97a9d1f7e60db285fc8b4ef021711c1b3c96be6a34a63b19aa2d97f65a1d7c51b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzU4MDMyMw==&mid=2247492879&idx=2&sn=e1d8c1e95753850a57e80efa724f9922&chksm=9e044d9d77f97a9d1f7e60db285fc8b4ef021711c1b3c96be6a34a63b19aa2d97f65a1d7c51b#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u6807\u51c6\u5316 SC42"], "title": "\u6c42\u7d22|\u7535\u5b50\u6807\u51c6\u9662\u5de5\u4e1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8bc4\u6d4b\u7ed3\u679c\u516c\u5e03", "comment": "Source: WeChat, Published: 2025-10-09 05:05:55", "summary": "\u5de5\u4e1a\u5927\u6a21\u578b\u901a\u8fc7\u878d\u5408\u884c\u4e1a\u77e5\u8bc6\u3001\u591a\u6a21\u6001\u6570\u636e\u548c\u667a\u80fd\u7b97\u6cd5\uff0c\u5728\u667a\u80fd\u5236\u9020\u3001\u5de5\u4e1a\u8bbe\u8ba1\u3001\u8bbe\u5907\u8fd0\u7ef4\u3001\u4f9b\u5e94\u94fe\u4f18\u5316\u7b49\u573a\u666f\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u6210\u4e3a\u63a8\u52a8\u65b0\u578b\u5de5\u4e1a\u5316\u7684\u91cd\u8981\u5f15\u64ce\u3002", "AI": {"tldr": "\u5de5\u4e1a\u5927\u6a21\u578b\u901a\u8fc7\u878d\u5408\u884c\u4e1a\u77e5\u8bc6\u3001\u591a\u6a21\u6001\u6570\u636e\u548c\u667a\u80fd\u7b97\u6cd5\uff0c\u5728\u667a\u80fd\u5236\u9020\u3001\u5de5\u4e1a\u8bbe\u8ba1\u3001\u8bbe\u5907\u8fd0\u7ef4\u3001\u4f9b\u5e94\u94fe\u4f18\u5316\u7b49\u573a\u666f\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u6210\u4e3a\u63a8\u52a8\u65b0\u578b\u5de5\u4e1a\u5316\u7684\u91cd\u8981\u5f15\u64ce\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2510.663fca57", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771110&idx=3&sn=b916a31f21b161d3b104a84f27adc04d&chksm=fad902ac7073c8b5d0053f04035c04f0525b03a5dc6a5706c484be1af5ee1c49fa33eaef9eb6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771110&idx=3&sn=b916a31f21b161d3b104a84f27adc04d&chksm=fad902ac7073c8b5d0053f04035c04f0525b03a5dc6a5706c484be1af5ee1c49fa33eaef9eb6#rd", "authors": ["DataFunTalk"], "title": "B \u7ad9\u57fa\u4e8e<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u5927\u6570\u636e\u667a\u80fd\u8bca\u65ad\u52a9\u624b\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-10-09 05:01:16", "summary": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "AI": {"tldr": "\u5bfc\u8bfb \u672c\u6587\u5c06\u5206\u4eab B \u7ad9\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u52a9\u624b\u5b9e\u8df5\u3002\u5206\u4eab\u5609\u5bbe\uff5c\u90ed\u8dc3\u9e4f \u54d4\u54e9\u54d4\u54e9 \u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7f16\u8f91\u6574\u7406\uff5c\u6c6a\u7ef4\u5185\u5bb9\u6821\u5bf9\uff5c\u674e\u74761. \u6574\u4f53\u67b6\u6784\u548c\u89c4\u6a21", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
