<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [tldr.article](#tldr.article) [Total: 5]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.LG](#cs.LG) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Learning to Evict from Key-Value Cache](https://arxiv.org/abs/2602.10238)
*Luca Moschella,Laura Manduchi,Ozan Sener*

Main category: cs.CL

TL;DR: 提出KVP框架，通过强化学习训练轻量级代理来预测token的未来效用，实现KV缓存的自适应管理，显著优于现有启发式方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时KV缓存内存需求巨大，现有基于启发式（如最近性、注意力分数）的缓存淘汰方法只是间接代理token未来效用，且引入计算开销。

Method: 将KV缓存淘汰重构为强化学习问题，引入KVP框架：训练轻量级每头RL代理，使用预计算的生成轨迹和仅键值向量，学习基于未来效用的专门淘汰策略，无需修改底层LLM或增加推理开销。

Result: 在两个不同模型系列上，在长上下文基准RULER和多轮对话基准OASST2-4k上显著优于基线。在标准下游任务（LongBench、BOOLQ、ARC）的零样本测试中，KVP展现出良好的泛化能力，能适应更长上下文。

Conclusion: 学习预测token未来效用是自适应KV缓存管理的强大且可扩展范式，KVP框架通过RL代理有效解决了KV缓存内存效率问题。

Abstract: The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.

</details>


### [2] [Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation](https://arxiv.org/abs/2602.10356)
*Tianci Xue,Zeyi Liao,Tianneng Shi,Zilu Wang,Kai Zhang,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: ACuRL是一个自主课程强化学习框架，无需人工标注数据，通过环境探索和课程任务生成实现计算机使用代理的持续学习，在特定环境中获得4-22%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实数字环境高度多样且动态变化，导致代理经常遇到未见场景和分布偏移，需要持续学习。但获取高质量、环境相关的代理数据成本高昂，需要避免依赖人工标注。

Method: 提出ACuRL框架：1)代理先探索目标环境获取初始经验；2)课程任务生成器利用这些经验和前一轮反馈合成适合代理当前能力的新任务；3)引入CUAJudge自动评估器提供可靠奖励信号，与人类判断一致性达93%。

Result: 方法有效支持环境内和跨环境持续学习，在现有环境中获得4-22%性能提升且无灾难性遗忘。分析显示高度稀疏的参数更新（如20%），解释了有效且鲁棒的适应能力。

Conclusion: ACuRL框架通过自主课程强化学习，无需人工数据即可实现计算机使用代理在特定环境中的持续适应，提供了一种高效且可扩展的解决方案。

Abstract: Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.

</details>


### [3] [Neuro-Symbolic Synergy for Interactive World Modeling](https://arxiv.org/abs/2602.10480)
*Hongyu Zhao,Siyu Zhou,Haolin Yang,Zengyi Qin,Tianyi Zhou*

Main category: cs.CL

TL;DR: NeSyS框架结合LLM的语义先验与符号世界模型的逻辑一致性，通过交替训练实现表达性与鲁棒性，在多个交互环境中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: LLM作为世界模型时容易产生幻觉，特别是在需要严格遵守确定性转换规则的边角情况下，而符号世界模型虽然逻辑一致但缺乏语义表达能力，需要结合两者优势。

Method: 提出神经符号协同框架，将LLM的概率语义先验与可执行的符号规则结合，通过交替训练两种模型，符号世界模型直接约束LLM的输出概率分布，神经世界模型仅在符号规则未覆盖的轨迹上进行微调。

Result: 在ScienceWorld、Webshop和Plancraft三个交互环境中，NeSyS在预测准确性和数据效率方面均优于基线方法，训练数据减少50%而不损失准确性。

Conclusion: NeSyS框架成功结合了神经和符号方法的优势，实现了表达性与鲁棒性的平衡，为构建更可靠的世界模型提供了有效途径。

Abstract: Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.

</details>


### [4] [LHAW: Controllable Underspecification for Long-Horizon Tasks](https://arxiv.org/abs/2602.10525)
*George Pu,Michael S. Lee,Udari Madhushani Sehwag,David J. Lee,Bryan Zhu,Yash Maurya,Mohit Raghavendra,Yuan Xue,Samuel Marc Denton*

Main category: cs.CL

TL;DR: LHAW是一个模块化、数据集无关的合成管道，可将任何明确定义的任务转化为可控的未指定变体，用于系统评估长时程工作流代理在模糊情境下的澄清行为。


<details>
  <summary>Details</summary>
Motivation: 长时程工作流代理在真正自主系统中至关重要，但其可靠执行依赖于在模糊情境下进行推理和寻求澄清的能力。目前缺乏可扩展的、任务无关的框架来系统性地收集和测量模糊性对自定义工作流的影响。

Method: LHAW通过系统性地在四个维度（目标、约束、输入、上下文）以可配置的严重程度移除信息，将明确定义的任务转化为可控的未指定变体。不同于依赖LLM预测模糊性的方法，LHAW通过实证代理试验验证变体，并根据观察到的终端状态差异将变体分类为结果关键型、发散型或良性型。

Result: 发布了来自TheAgentCompany、SWE-Bench Pro和MCP-Atlas的285个任务变体，并进行了形式化分析，测量了当前代理在模糊设置下如何检测、推理和解决未指定问题。

Conclusion: LHAW为长时程设置中代理澄清行为的成本敏感评估提供了首个系统框架，有助于开发可靠的自主系统。

Abstract: Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.

</details>


### [5] [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560)
*Leheng Sheng,Yongtao Zhang,Wenchang Ma,Yaorui Shi,Ting Huang,Xiang Wang,An Zhang,Ke Shen,Tat-Seng Chua*

Main category: cs.CL

TL;DR: GRU-Mem通过引入文本控制门（更新门和退出门）改进MemAgent的长上下文推理，使用强化学习训练门控机制，实现更稳定高效的内存更新和推理循环退出。


<details>
  <summary>Details</summary>
Motivation: 现有MemAgent在处理长上下文时存在两个关键问题：1）内存可能因无差别更新而爆炸式增长；2）推理循环缺乏退出机制，导致收集足够证据后仍进行不必要计算。

Method: 提出GRU-Mem，引入两个文本控制门（更新门和退出门），使用强化学习训练门控行为，奖励正确的更新和退出决策，实现选择性内存更新和及时循环退出。

Result: 在各种长上下文推理任务上，GRU-Mem通常优于原始MemAgent，推理速度最高提升400%。

Conclusion: GRU-Mem通过门控机制有效解决了长上下文推理中的内存爆炸和计算效率问题，实现了更稳定高效的推理性能。

Abstract: While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\text{update}}$ and $r^{\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\% times inference speed acceleration.

</details>


### [6] [Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters](https://arxiv.org/abs/2602.10604)
*Ailin Huang,Ang Li,Aobo Kong,Bin Wang,Binxing Jiao,Bo Dong,Bojun Wang,Boyu Chen,Brian Li,Buyun Ma,Chang Su,Changxin Miao,Changyi Wan,Chao Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengting Feng,Chengyuan Yao,Chunrui Han,Dan Ma,Dapeng Shi,Daxin Jiang,Dehua Ma,Deshan Sun,Di Qi,Enle Liu,Fajie Zhang,Fanqi Wan,Guanzhe Huang,Gulin Yan,Guoliang Cao,Guopeng Li,Han Cheng,Hangyu Guo,Hanshan Zhang,Hao Nie,Haonan Jia,Haoran Lv,Hebin Zhou,Hekun Lv,Heng Wang,Heung-Yeung Shum,Hongbo Huang,Hongbo Peng,Hongyu Zhou,Hongyuan Wang,Houyong Chen,Huangxi Zhu,Huimin Wu,Huiyong Guo,Jia Wang,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiashu Lv,Jiashuo Liu,Jiayi Fu,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yang,Jie Zhou,Jieyi Hou,Jing Bai,Jingcheng Hu,Jingjing Xie,Jingwei Wu,Jingyang Zhang,Jishi Zhou,Junfeng Liu,Junzhe Lin,Ka Man Lo,Kai Liang,Kaibo Liu,Kaijun Tan,Kaiwen Yan,Kaixiang Li,Kang An,Kangheng Lin,Lei Yang,Liang Lv,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lina Chen,Luck Ma,Mengqiang Ren,Michael Li,Ming Li,Mingliang Li,Mingming Zhang,Mingrui Chen,Mitt Huang,Na Wang,Peng Liu,Qi Han,Qian Zhao,Qinglin He,Qinxin Du,Qiuping Wu,Quan Sun,Rongqiu Yang,Ruihang Miao,Ruixin Han,Ruosi Wan,Ruyan Guo,Shan Wang,Shaoliang Pang,Shaowen Yang,Shengjie Fan,Shijie Shang,Shiliang Yang,Shiwei Li,Shuangshuang Tian,Siqi Liu,Siye Wu,Siyu Chen,Song Yuan,Tiancheng Cao,Tianchi Yue,Tianhao Cheng,Tianning Li,Tingdan Luo,Wang You,Wei Ji,Wei Yuan,Wei Zhang,Weibo Wu,Weihao Xie,Wen Sun,Wenjin Deng,Wenzhen Zheng,Wuxun Xie,Xiangfeng Wang,Xiangwen Kong,Xiangyu Liu,Xiangyu Zhang,Xiaobo Yang,Xiaojia Liu,Xiaolan Yuan,Xiaoran Jiao,Xiaoxiao Ren,Xiaoyun Zhang,Xin Li,Xin Liu,Xin Wu,Xing Chen,Xingping Yang,Xinran Wang,Xu Zhao,Xuan He,Xuanti Feng,Xuedan Cai,Xuqiang Zhou,Yanbo Yu,Yang Li,Yang Xu,Yanlin Lai,Yanming Xu,Yaoyu Wang,Yeqing Shen,Yibo Zhu,Yichen Lv,Yicheng Cao,Yifeng Gong,Yijing Yang,Yikun Yang,Yin Zhao,Yingxiu Zhao,Yinmin Zhang,Yitong Zhang,Yixuan Zhang,Yiyang Chen,Yongchi Zhao,Yongshen Long,Yongyao Wang,Yousong Guan,Yu Zhou,Yuang Peng,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yudi Zhao,Yue Peng,Yueqiang Lin,Yufan Lu,Yuling Zhao,Yunzhou Ju,Yurong Zhang,Yusheng Li,Yuxiang Yang,Yuyang Chen,Yuzhu Cai,Zejia Weng,Zetao Hong,Zexi Li,Zhe Xie,Zheng Ge,Zheng Gong,Zheng Zeng,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhiheng Hu,Zidong Yang,Zili Wang,Ziqi Ren,Zixin Zhang,Zixuan Wang*

Main category: cs.CL

TL;DR: Step 3.5 Flash是一个稀疏专家混合模型，通过196B参数基础与11B激活参数实现高效推理，结合滑动窗口/全注意力机制和多令牌预测，在数学、代码和工具使用任务上达到前沿模型性能。


<details>
  <summary>Details</summary>
Motivation: 构建智能体时最关键的要素是敏锐的推理能力和快速可靠的执行效率，需要在保持前沿智能水平的同时实现计算效率的平衡。

Method: 采用稀疏专家混合架构，结合3:1滑动窗口/全注意力交替机制和多令牌预测减少延迟；设计可扩展的强化学习框架，融合可验证信号与偏好反馈，支持大规模离策略训练的稳定自我改进。

Result: 在多个基准测试中表现优异：IMO-AnswerBench 85.4%，LiveCodeBench-v6 86.4%，tau2-Bench 88.2%，BrowseComp 69.0%，Terminal-Bench 2.0 51.0%，性能与GPT-5.2 xHigh和Gemini 3.0 Pro等前沿模型相当。

Conclusion: Step 3.5 Flash通过重新定义效率前沿，为在真实工业环境中部署复杂智能体提供了高密度基础，平衡了智能水平与计算效率。

Abstract: We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.

</details>


### [7] [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652)
*Yongshi Ye,Hui Jiang,Feihu Jiang,Tian Lan,Yichao Du,Biao Fu,Xiaodong Shi,Qianghuai Jia,Longyue Wang,Weihua Luo*

Main category: cs.CL

TL;DR: UMEM框架联合优化LLM智能体的记忆提取与管理，通过语义邻域建模和GRPO奖励提升记忆泛化能力，在多个基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体方法主要优化记忆管理，但将记忆提取视为静态过程，导致记忆泛化能力差，智能体积累的是实例特定噪声而非鲁棒记忆

Method: 提出统一记忆提取与管理(UMEM)框架，联合优化LLM同时进行记忆提取和管理；引入语义邻域建模，通过GRPO优化模型获得邻域级边际效用奖励，确保记忆在语义相关查询簇中的泛化能力

Result: 在五个基准测试中显著优于强基线方法，在多轮交互任务中提升达10.67%；在持续演化过程中保持单调增长曲线

Conclusion: UMEM通过联合优化记忆提取与管理，结合语义邻域建模，有效解决了智能体记忆泛化问题，为自演化智能体提供了更鲁棒的记忆机制

Abstract: Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.

</details>


### [8] [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715)
*Yifei Li,Weidong Guo,Lingling Zhang,Rongman Xu,Muye Huang,Hui Liu,Lijiao Xu,Yu Xu,Jun Liu*

Main category: cs.CL

TL;DR: LoCoMo-Plus是一个评估LLM对话系统认知记忆能力的基准，专注于"线索-触发语义断开"场景，要求模型在长对话中保留和应用潜在约束，而非简单的表面事实回忆。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆基准主要关注表面事实回忆，但在真实对话中，合适的回应往往依赖于未明确查询的隐含约束（如用户状态、目标或价值观）。需要评估模型在这种"线索-触发语义断开"场景下的认知记忆能力。

Method: 1) 提出LoCoMo-Plus基准，评估认知记忆在"线索-触发语义断开"场景下的表现；2) 指出传统字符串匹配指标和显式任务类型提示在此类场景中的不适用性；3) 提出基于约束一致性的统一评估框架。

Result: 实验表明，认知记忆仍然具有挑战性，现有基准未能捕捉到模型在此类场景下的失败。在不同骨干模型、基于检索的方法和记忆系统中都观察到了这些问题。

Conclusion: LoCoMo-Plus揭示了现有对话记忆评估的局限性，强调了评估认知记忆能力的重要性，特别是在"线索-触发语义断开"场景下。提出的约束一致性框架为更全面的对话记忆评估提供了新方向。

Abstract: Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.

</details>


### [9] [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740)
*Yuming Yan,Shuo Yang,Kai Tang,Sihong Chen,Yang Zhang,Ke Xu,Dan Hu,Qun Yu,Pengfei Hu,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: 提出RCPA方法，通过课程感知渐进调制机制，分阶段适应新领域知识同时保持VLM通用能力


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在专业领域表现不佳，监督微调会导致灾难性遗忘，持续预训练计算成本过高，需要高效的领域适应方法

Method: RCPA方法：早期阶段应用部分输出约束安全引入新领域概念，随着模型熟悉度增加，逐步过渡到完整生成优化，平衡领域知识获取与通用能力保持

Result: 在多个专业领域和通用基准测试中验证了RCPA的有效性，为构建高性能领域自适应VLM提供了实用路径

Conclusion: RCPA通过课程感知渐进调制机制成功解决了VLM领域适应中的优化崩溃问题，平衡了专业能力提升与通用能力保持

Abstract: Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.

</details>


### [10] [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801)
*Haotian Sheng,Heyong Wang,Ming Hong,Hongman He,Junqiu Liu*

Main category: cs.CL

TL;DR: 提出LSCL方法，通过深度学习模型帮助黑盒LLM表达知识边界，解决幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有知识边界表达研究主要针对白盒LLM，而黑盒LLM（仅提供API访问）的方法尚未充分探索。LLM缺乏对内部知识的意识导致幻觉问题，需要让LLM能像人类一样表达超出知识边界的问题

Method: 基于知识蒸馏框架设计深度学习模型LSCL，以黑盒LLM的输入问题、输出答案和token概率作为输入，构建输入与模型内部知识状态的映射，量化表达黑盒LLM的知识边界

Result: 在多个公开数据集和主流黑盒LLM上的实验表明，LSCL能有效帮助黑盒LLM准确表达知识边界，在准确率和召回率等指标上显著优于现有基线模型

Conclusion: LSCL为解决黑盒LLM的知识边界表达问题提供了有效方案，对于不支持token概率访问的黑盒LLM也提出了性能接近的替代方法

Abstract: Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.

</details>


### [11] [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816)
*Deyuan Liu,Zecheng Wang,Zhanyue Qin,Zhiying Tu,Dianhui Chu,Dianbo Sui*

Main category: cs.CL

TL;DR: 提出Token Constraint Bound (δ_TCB)新指标，量化LLM内部状态能承受的最大扰动而不改变主导的下一个token预测，用于评估LLM预测的上下文稳定性。


<details>
  <summary>Details</summary>
Motivation: LLM对输入上下文微小变化敏感，影响可靠性。传统指标如准确率和困惑度无法评估局部预测鲁棒性，因为归一化的输出概率会掩盖LLM内部状态对扰动的潜在弹性。

Method: 引入Token Constraint Bound (δ_TCB)指标，量化LLM内部状态能承受的最大扰动而不显著改变其主导的下一个token预测。该指标与输出嵌入空间几何内在相关，提供模型内部预测承诺稳定性的洞察。

Result: 实验显示δ_TCB与有效提示工程相关，并揭示了在上下文学习和文本生成过程中被困惑度忽略的关键预测不稳定性。δ_TCB为分析和潜在改进LLM预测的上下文稳定性提供了原则性补充方法。

Conclusion: δ_TCB是一个新颖的指标，能够量化LLM内部状态的鲁棒性，为评估和改进LLM预测的上下文稳定性提供了有价值的工具，弥补了传统指标的不足。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($δ_{\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $δ_{\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $δ_{\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $δ_{\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.

</details>


### [12] [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874)
*Binwei Yan,Yifei Fu,Mingjian Zhu,Hanting Chen,Mingxuan Yuan,Yunhe Wang,Hailin Hu*

Main category: cs.CL

TL;DR: C-MOP是一个基于聚类的动量优化提示框架，通过边界感知对比采样和动量引导语义聚类来稳定提示优化，解决现有方法中的噪声和冲突信号问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法常受到噪声和冲突更新信号的困扰，需要更稳定的优化框架来提升大语言模型的性能。

Method: 提出C-MOP框架：1) BACS利用批次信息挖掘硬负样本、锚点和边界对来表征正负提示样本的典型表示和决策边界；2) MGSC引入带时间衰减的文本动量机制，从迭代中波动的梯度中提取持久共识。

Result: C-MOP在实验中一致优于PromptWizard和ProTeGi等SOTA基线，平均提升1.58%和3.35%。仅用3B激活参数的通用LLM就能超越70B领域专用密集LLM。

Conclusion: C-MOP通过边界感知对比采样和动量引导语义聚类有效解决了提示优化中的噪声和冲突问题，实现了精确的提示演化。

Abstract: Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.

</details>


### [13] [Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models](https://arxiv.org/abs/2602.10953)
*Mingyu Cao,Alvaro Correia,Christos Louizos,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: SOAR是一种无需训练的DLM解码算法，根据模型置信度自适应调整搜索策略：低置信度时扩大搜索避免过早决策，高置信度时并行解码减少迭代次数，在数学推理和代码生成任务上提升质量同时保持推理速度。


<details>
  <summary>Details</summary>
Motivation: 标准DLM解码采用贪心策略（每次选择置信度最高的位置解掩码），这种局部决策可能导致次优的解码顺序，特别是在需要推理的任务中。需要一种能自适应模型不确定性的解码方法，在质量和效率间取得平衡。

Method: 提出SOAR解码算法：1）低置信度时扩大搜索空间，考虑替代的解码决策；2）高置信度时并行解码多个位置，减少去噪迭代次数；3）无需额外训练，直接应用于现有DLM模型。

Result: 在GSM8K、MBPP、HumanEval等数学推理和代码生成基准测试中，使用Dream-7B和LLaDA-8B模型，SOAR提升了生成质量，同时保持了有竞争力的推理速度。

Conclusion: SOAR为DLM解码提供了实用的质量-效率平衡方案，通过自适应不确定性调整搜索策略，改善了推理密集型任务的生成效果。

Abstract: Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.

</details>


### [14] [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096)
*Soumya Suvra Ghosal,Souradip Chakraborty,Vaibhav Singh,Furong Huang,Dinesh Manocha,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SafeThink：一种轻量级推理时防御方法，通过监控推理轨迹并在安全阈值被违反时注入优化前缀来恢复多模态大模型的安全性，同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的链式思维后训练虽然能提升多模态大模型的推理能力，但会同时降低安全对齐性并增加越狱成功率。需要一种方法能在保持推理性能的同时恢复安全性。

Method: 提出SafeThink方法，将安全恢复视为满足性约束而非最大化目标。使用安全奖励模型监控推理轨迹，当安全阈值被违反时，条件性地注入优化的短前缀（"Wait, think safely"）来纠正推理方向。

Result: 在6个开源多模态大模型和4个越狱基准测试中，SafeThink将攻击成功率降低30-60%（如LlamaV-o1从63.33%降至5.74%），同时保持推理性能（MathVista准确率从65.20%到65.00%）。实验发现安全恢复通常只需1-3个推理步骤的干预。

Conclusion: SafeThink是一种有效的推理时防御方法，能在不损害推理能力的情况下显著提升多模态大模型的安全性，且干预成本低，只需在关键推理步骤进行少量纠正。

Abstract: Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix ("Wait, think safely") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [15] [Why "just prompt better" doesn't work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bicameral-ai.com%2Fblog%2Ftech-debt-meeting%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ghJLKyBZokmC9AfW2U5UoWIKgeyDqPJf9NPGBJthItw=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI编程助手常因无法促进约束发现而增加开发时间，而非解决核心问题。主要问题是沟通摩擦，技术约束发现较晚或与利益相关者沟通不畅。AI工具通过不加批判地生成代码使情况恶化，绕过了开发者自然发现隐含约束的人类实现阶段。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨为什么"只是更好地提示"无法解决AI编程助手的根本问题。当前AI编码工具虽然能生成代码，但经常增加开发时间，因为它们无法促进约束发现过程，而这是软件开发中的关键环节。

Method: 通过分析AI编程助手在实际开发中的使用情况，识别出沟通摩擦和技术约束发现延迟的问题。研究AI工具如何绕过人类实现阶段，导致隐含约束未被发现。

Result: 研究发现AI编码助手实际上可能增加开发时间，因为它们无法促进约束发现过程。AI工具不加批判地生成代码，绕过了开发者自然发现隐含约束的关键阶段，导致技术约束发现较晚或沟通不畅。

Conclusion: 仅仅改进提示策略无法解决AI编程助手的根本问题。需要重新思考AI工具的设计，使其能够更好地支持约束发现过程，而不仅仅是代码生成。

Abstract: Why "just prompt better" doesn't work (9 minute read) AI coding assistants often increase development time due to their inability to facilitate constraint discovery, rather than solving core problems. A primary issue is communication friction, where technical constraints are discovered late or poorly communicated to non-technical stakeholders. AI tools make this worse by uncritically generating code, bypassing the human implementation phase where developers naturally uncover implicit constrai...

</details>


### [16] [Introducing Showboat and Rodney, so agents can demo what they've built](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F10%2Fshowboat-and-rodney%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ipwsGgq-5WM-lNTpLbZZ3OUKQvArVK5cpmT-pgqY378=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Showboat和Rodney是两个帮助AI编码代理测试和展示所构建软件的新工具，超越自动化测试，创建可验证的工件


<details>
  <summary>Details</summary>
Motivation: 当前AI编码代理缺乏有效展示和验证其构建软件功能的方式，需要超越传统自动化测试的工具来创建可验证的演示工件

Method: Showboat是CLI工具，用于构建包含命令执行、输出和图像的Markdown文档；Rodney提供CLI浏览器自动化，支持视觉验证

Result: 开发了两个互补工具：Showboat创建可验证的演示文档，Rodney支持浏览器自动化测试，共同帮助AI代理展示软件功能

Conclusion: Showboat和Rodney为AI编码代理提供了新的测试和演示能力，通过创建可验证的视觉工件，增强了软件功能的证明和展示

Abstract: Introducing Showboat and Rodney, so agents can demo what they've built (11 minute read) Showboat and Rodney are two new tools designed to help AI coding agents test and demonstrate the software they build, moving beyond automated tests to verifiable artifacts. Showboat is a CLI tool that enables agents to construct Markdown documents, embedding command executions, their outputs, and images to visually prove code functionality. Complementing this, Rodney provides CLI browser automation, allowi...

</details>


### [17] [Tambo](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ftambo-ai%2Ftambo%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/fCEr5MNE0bscdYpr6FxTOvsNCA5-9kilJjwnVgVpg_U=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Tambo AI是一个开源的React工具包，用于构建能够生成和交互动态用户界面的AI代理


<details>
  <summary>Details</summary>
Motivation: 让开发者能够轻松创建能够动态生成和交互UI的AI代理，通过注册React组件和Zod模式，使LLM代理能够根据用户输入选择和渲染UI元素

Method: 开发者使用Zod模式注册React组件，LLM代理根据用户输入选择和流式传输props来渲染UI元素，支持动态界面生成和交互

Result: 创建了一个开源React工具包，使AI代理能够生成和交互动态UI，如显示图表或更新任务板

Conclusion: Tambo AI为构建能够动态生成和交互用户界面的AI代理提供了一个实用的开源解决方案

Abstract: Tambo (GitHub Repo) Tambo AI is an open-source React toolkit for building AI agents capable of generating and interacting with dynamic user interfaces. Developers register their React components with Zod schemas, allowing an LLM agent to select and stream props to render UI elements based on user input, like displaying charts or updating task boards.

</details>


### [18] [Hello Entire World](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fentire.io%2Fblog%2Fhello-entire-world%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/00RfVg3U-AMQpj0zAWrEzzcl41l6KEZmgXT9XTmms9U=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Entire公司推出基于git兼容数据库、通用语义推理层和AI原生SDLC的下一代开发者平台，其首个开源产品Entire CLI引入"检查点"功能自动捕获和版本化代理上下文，提升AI代理可追溯性


<details>
  <summary>Details</summary>
Motivation: 构建下一代开发者平台，解决当前AI代理在软件开发中缺乏可追溯性和上下文管理的问题，实现人类与AI代理的更好协作

Method: 基于git兼容数据库构建开放可扩展平台，采用通用语义推理层，开发AI原生软件开发生命周期(SDLC)，通过Entire CLI的"检查点"功能自动捕获和版本化代理上下文

Result: 推出了首个开源产品Entire CLI，引入检查点机制改善AI代理的可追溯性，为构建完整的人类-代理协作平台奠定基础

Conclusion: Entire平台通过创新的架构设计解决了AI代理在软件开发中的可追溯性问题，为下一代开发者平台的发展提供了新方向

Abstract: Hello Entire World (6 minute read) Entire is a new company created by GitHub's ex-CEO Thomas Dohmke with the mission to build the world's next developer platform. Its vision is to create an open and scalable platform based on a git-compatible database, a universal semantic reasoning layer, and an AI-native SDLC for human-agent collaboration. Its first open-source product, the Entire CLI, introduces "Checkpoints" to automatically capture and version agent context, improving AI agent traceabili...

</details>


### [19] [Eight more months of agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/81bDQPPjP-OHnUlRo3SZzsT-ivoPv1WV2jO-cUdmbz8=444)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LLM智能体显著提升编程效率，使传统IDE过时


<details>
  <summary>Details</summary>
Motivation: 传统IDE在LLM智能体时代已显过时，需要探索智能体如何改变编程工作流程

Method: 分析8个月来LLM智能体在编程领域的发展和应用

Result: LLM智能体大幅提升编程效率，改变了传统编程工具的使用方式

Conclusion: LLM智能体正在重塑编程实践，传统IDE需要适应这一变革

Abstract: Eight more months of agents (8 minute read) LLM agents have dramatically improved programming efficiency recently and made traditional IDEs obsolete.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Discovering Differences in Strategic Behavior Between Humans and LLMs](https://arxiv.org/abs/2602.10324)
*Caroline Wang,Daniel Kasenberg,Kim Stachenfeld,Pablo Samuel Castro*

Main category: cs.AI

TL;DR: 使用AlphaEvolve程序发现工具，从数据中直接发现人类和LLM行为的可解释模型，揭示在迭代石头剪刀布游戏中，前沿LLM展现出比人类更深层次的策略行为。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地部署在社交和策略场景中，理解其行为与人类行为的差异变得至关重要。现有行为博弈论模型无法完全捕捉人类或LLM等黑盒非人类智能体的特殊行为。

Method: 采用AlphaEvolve这一先进的程序发现工具，直接从数据中发现人类和LLM行为的可解释模型，实现对人类和LLM行为驱动因素的无限制发现。

Result: 在迭代石头剪刀布游戏的分析中，发现前沿LLM能够展现出比人类更深层次的策略行为。

Conclusion: 这些结果为理解策略互动中人类和LLM行为差异的结构性驱动因素奠定了基础。

Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.

</details>


### [21] [Found-RL: foundation model-enhanced reinforcement learning for autonomous driving](https://arxiv.org/abs/2602.10458)
*Yansong Qu,Zihao Sheng,Zilin Huang,Jiancong Chen,Yuhao Luo,Tianyi Wang,Yiheng Feng,Samuel Labi,Sikai Chen*

Main category: cs.AI

TL;DR: Found-RL是一个专为自动驾驶设计的平台，通过异步批处理推理框架将视觉语言模型（VLM）的丰富语义知识高效集成到强化学习中，解决了VLM推理延迟问题，使轻量级RL模型能达到接近VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶中存在样本效率低和语义可解释性不足的问题，而基础模型（特别是视觉语言模型）虽然能提供丰富的上下文知识，但其高推理延迟阻碍了在高频RL训练循环中的部署。

Method: 1. 异步批处理推理框架：将繁重的VLM推理与仿真循环解耦；2. 监督机制：值边界正则化（VMR）和优势加权动作指导（AWAG）来蒸馏VLM动作建议；3. 采用高吞吐量CLIP进行密集奖励塑造，并通过条件对比动作对齐解决CLIP的动态盲区问题。

Result: 轻量级RL模型能达到接近VLM的性能（与数十亿参数的VLM相比），同时保持实时推理（约500 FPS）。

Conclusion: Found-RL提供了一个端到端管道，有效解决了VLM与RL集成中的延迟瓶颈，使轻量级模型能够利用VLM的丰富语义知识，在自动驾驶中实现高效、实时的强化学习。

Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.

</details>


### [22] [MERIT Feedback Elicits Better Bargaining in LLM Negotiators](https://arxiv.org/abs/2602.10467)
*Jihwan Oh,Murad Aghazada,Yooju Shin,Se-Young Yun,Taehyeon Kim*

Main category: cs.AI

TL;DR: 提出了AgoraBench基准测试框架，通过效用反馈机制评估LLM在复杂谈判场景中的表现，包含9种挑战性设置、基于效用理论的人类对齐指标，以及通过提示和微调增强LLM谈判能力的学习流程。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在谈判任务中表现不佳，缺乏战略深度和适应复杂人类因素的能力，而当前基准测试未能充分捕捉这些局限性。需要建立更全面的评估框架来提升LLM的谈判能力。

Method: 1) 创建AgoraBench基准测试，涵盖欺骗、垄断等9种挑战性谈判场景；2) 基于效用理论设计人类对齐的经济学指标（代理效用、谈判力、获取比率）；3) 构建人类偏好数据集和学习流程，通过提示和微调增强LLM谈判能力。

Result: 基线LLM策略通常偏离人类偏好，而提出的机制显著改善了谈判性能，产生了更深层次的战略行为和更强的对手意识。

Conclusion: 通过效用反馈为中心的框架，能够有效评估和提升LLM在复杂谈判场景中的表现，使其更符合人类偏好和经济理性。

Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.

</details>


### [23] [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598)
*Shuai Han,Mehdi Dastani,Shihan Wang*

Main category: cs.AI

TL;DR: NSAM是一个神经符号动作屏蔽框架，能在深度强化学习中自动学习与领域约束一致的符号模型，通过动作屏蔽排除不可行动作，提高样本效率并减少约束违反。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要手动指定符号接地函数和动作屏蔽技术，限制了深度强化学习在约束环境中的适用性。需要一种能自动学习符号模型并集成到DRL中的方法。

Method: 提出神经符号动作屏蔽(NSAM)框架，在最小监督下自动学习与高维状态约束一致的符号模型，基于学习到的符号模型生成动作屏蔽，排除不可行动作，实现符号推理与深度策略优化的端到端集成。

Result: 在多个约束领域评估NSAM，实验结果表明NSAM显著提高了DRL智能体的样本效率，同时大幅减少了约束违反。

Conclusion: NSAM成功实现了符号推理与深度强化学习的端到端集成，符号接地和策略学习的改进相互促进，为约束环境下的DRL提供了有效解决方案。

Abstract: Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.

</details>


### [24] [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625)
*Nanxu Gong,Haotian Li,Sixun Dong,Jianxun Lian,Yanjie Fu,Xing Xie*

Main category: cs.AI

TL;DR: 研究发现推理模型在心理理论任务上并不优于非推理模型，有时表现更差，揭示了推理模型在社交认知任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在数学和编程等领域的逐步推理能力有所提升，但尚不清楚这种优势是否能转移到社交认知技能（如心理理论）上。研究旨在系统评估推理模型在心理理论任务上的表现。

Method: 对9个先进的大型语言模型进行系统研究，比较推理模型与非推理模型在三个代表性心理理论基准测试上的表现。进行细粒度分析，并设计两种干预方法：慢到快自适应推理和思考到匹配捷径预防。

Result: 推理模型在心理理论任务上并不一致优于非推理模型，有时表现更差。分析发现：1）慢思考崩溃：响应越长准确率显著下降；2）适度自适应推理有益：限制推理长度可缓解失败；3）选项匹配捷径：移除选择题选项后推理模型表现显著改善。干预方法验证了这些问题。

Conclusion: 大型推理模型在形式推理（如数学、代码）方面的进步不能完全转移到心理理论这种典型的社交推理任务上。实现稳健的心理理论需要开发超越现有推理方法的独特能力。

Abstract: Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.

</details>


### [25] [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635)
*Keane Ong,Sabri Boughorbel,Luwei Xiao,Chanakya Ekbote,Wei Dai,Ao Qu,Jingyao Wu,Rui Mao,Ehsan Hoque,Erik Cambria,Gianmarco Mengaldo,Paul Pu Liang*

Main category: cs.AI

TL;DR: HARPO是一种强化学习方法，通过调节优势函数来平衡异构任务和样本的学习，开发了Omnisapiens-7B 2.0社交行为处理基础模型，在多项行为任务上表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立地建模人类行为维度（情感、认知或社交属性），任务特定建模增加了训练成本且限制了跨行为设置的泛化能力。虽然最近的推理RL方法可以在多个行为任务上训练统一模型，但未明确解决跨异构行为数据的学习问题。

Method: 提出HARPO（异构感知相对策略优化）RL方法，通过调节优势函数来确保在策略优化过程中，没有任何单个任务或样本产生不成比例的影响，从而平衡跨异构任务和样本的学习。

Result: 使用HARPO开发了Omnisapiens-7B 2.0社交行为处理基础模型。相对于现有行为基础模型，在多任务和保留设置上分别获得高达+16.85%和+9.37%的性能提升，同时产生更明确和鲁棒的推理轨迹。HARPO在与其他RL方法比较中也表现最稳定。

Conclusion: HARPO方法有效解决了跨异构行为数据的学习问题，开发的Omnisapiens-7B 2.0模型在社交行为处理任务上表现出色，为开发社交智能AI提供了有效工具。

Abstract: To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.

</details>


### [26] [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699)
*Jie Jiang,Yangru Huang,Zeyu Wang,Changping Wang,Yuling Xiong,Jun Zhang,Huan Yu*

Main category: cs.AI

TL;DR: V-STAR框架通过价值引导采样和树状优势强化，解决生成式推荐中RL训练的概率-奖励不匹配问题，提升探索效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于似然的解码方法在生成式推荐的RL微调中存在概率-奖励不匹配问题，导致探索不足和优势压缩，限制了模型性能。

Method: 提出V-STAR框架，包含价值引导高效解码(VED)识别关键节点并选择性深化高潜力前缀，以及Sibling-GRPO利用树状拓扑计算兄弟相对优势，集中学习信号于关键分支决策。

Result: 在离线和在线数据集上的实验表明，V-STAR在严格延迟约束下优于现有基线方法，提供更高的准确性和候选集多样性。

Conclusion: V-STAR通过解决概率-奖励不匹配问题，有效提升了生成式推荐系统的探索效率和性能，为RL在推荐系统中的应用提供了新思路。

Abstract: Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.

</details>


### [27] [See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch](https://arxiv.org/abs/2602.10814)
*Xingyi Zhang,Yulei Ye,Kaifeng Huang,Wenhao Li,Xiangfeng Wang*

Main category: cs.AI

TL;DR: ScratchWorld是一个评估多模态GUI代理在Scratch环境中编程能力的基准测试，包含83个任务，采用两种交互模式来诊断代理失败原因，实验显示推理与执行之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 像Scratch这样的积木式编程环境在低代码教育中扮演核心角色，但评估AI代理通过图形用户界面构建程序的能力仍未被充分探索。需要建立一个基准来评估多模态GUI代理在Scratch中的程序构建任务。

Method: 基于Use-Modify-Create教学框架，设计了包含83个任务的ScratchWorld基准，涵盖创建、调试、扩展和计算四个问题类别。采用两种交互模式：原始模式需要细粒度拖放操作来评估视觉运动控制，复合模式使用高级语义API来分离程序推理和GUI执行。提出基于执行的评估协议，通过浏览器环境中的运行时测试验证Scratch程序的功能正确性。

Result: 对最先进的多模态语言模型和GUI代理进行广泛实验，揭示了显著的推理-执行差距，尽管代理具有较强的规划能力，但在细粒度GUI操作方面仍面临持续挑战。

Conclusion: ScratchWorld基准能够有效评估多模态GUI代理在Scratch环境中的程序构建能力，揭示了当前代理在推理与执行之间的差距，为未来GUI代理研究提供了重要基准。

Abstract: Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.

</details>


### [28] [GameDevBench: Evaluating Agentic Capabilities Through Game Development](https://arxiv.org/abs/2602.11103)
*Wayne Chi,Yixiong Fang,Arnav Yayavaram,Siddharth Yayavaram,Seth Karten,Qiuhong Anna Wei,Runkun Chen,Alexander Wang,Valerie Chen,Ameet Talwalkar,Chris Donahue*

Main category: cs.AI

TL;DR: GameDevBench：首个游戏开发智能体评测基准，包含132个需要多模态理解的任务，平均任务复杂度是现有软件开发基准的3倍以上，最佳智能体仅能解决54.5%的任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态编码智能体发展滞后，缺乏结合软件工程复杂性和深度多模态理解的评测基准。游戏开发提供了理想的测试场景，需要智能体在大型代码库中操作多模态资产（着色器、精灵、动画等）。

Method: 从网络和视频教程中收集132个游戏开发任务，构建GameDevBench基准。任务需要显著的多模态理解能力，复杂度高。引入两种简单的图像和视频反馈机制来提升智能体的多模态能力。

Result: 智能体在游戏开发任务上表现不佳，最佳智能体仅解决54.5%的任务。任务难度与多模态复杂度强相关：游戏玩法任务成功率46.9%，2D图形任务降至31.6%。引入的反馈机制显著提升性能，Claude Sonnet 4.5从33.3%提升至47.7%。

Conclusion: 游戏开发是多模态智能体研究的理想测试平台，现有智能体在多模态理解方面仍有不足。简单的多模态反馈机制能有效提升性能，GameDevBench的发布将推动智能体游戏开发研究。

Abstract: Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.

</details>


### [29] [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885)
*Leheng Sheng,Wenchang Ma,Ruixin Hong,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RLCER：通过自提出、自演化的评分标准来奖励思维链，无需人工标注，在强化学习中超越结果中心的RLVR方法


<details>
  <summary>Details</summary>
Motivation: 传统思维链奖励方法面临人工标注成本高、静态奖励模型难以适应思维链分布变化和奖励攻击等挑战，需要一种无需人工标注且能自主演化的思维链奖励方法

Method: 提出RLCER方法，通过自提出和自演化的评分标准来奖励思维链，增强结果中心的RLVR方法，这些评分标准可作为提示中的提示进一步改进推理性能

Result: RLCER在无需结果奖励的情况下提供可靠的思维链监督信号，性能超越结果中心的RLVR方法，且自提出的评分标准作为提示提示能进一步提升推理时性能

Conclusion: 自提出和自演化的评分标准为思维链奖励提供了有效的自主监督机制，无需人工标注，并能适应思维链分布的变化

Abstract: Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \textbf{RLCER} (\textbf{R}einforcement \textbf{L}earning with \textbf{C}oT Supervision via Self-\textbf{E}volving \textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.

</details>


### [30] [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999)
*Yusong Lin,Haiyang Wang,Shuzhe Wu,Lue Fan,Feiyang Pan,Sanyuan Zhao,Dandan Tu*

Main category: cs.AI

TL;DR: 提出了CLI-Gym方法，通过模拟环境历史来大规模生成环境密集型任务，并训练出LiberCoder模型在Terminal-Bench上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 代理编码需要与运行时环境（如命令行界面）有效交互，但目前缺乏大规模获取环境密集型任务的方法来增强代理能力

Method: 基于Dockerfile与代理任务的类比，使用代理模拟和探索环境历史，通过执行反馈指导。通过追踪健康环境的历史，将其状态反转到早期有运行时故障的状态，从而生成包含错误状态和相应错误消息的任务

Result: 生成了1,655个环境密集型任务（同类中最大的集合），通过精心策划的成功轨迹训练的LiberCoder模型在Terminal-Bench上实现了+21.1%的绝对提升（达到46.1%），优于各种强基线

Conclusion: 这是第一个用于大规模推导环境密集型任务的公开流程，CLI-Gym方法有效解决了环境密集型任务获取的规模化问题

Abstract: Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.

</details>


### [31] [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136)
*Jiayi Zhou,Yang Sheng,Hantao Lou,Yaodong Yang,Jie Fu*

Main category: cs.AI

TL;DR: 论文提出FoT框架，通过双向形式思维架构将自然语言需求转化为形式化规范，使用Dafny和Z3求解器提供数学保证而非概率评分，显著提升LLM代理的行为安全性验证效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在关键领域应用增加，确保其行为安全性变得至关重要。当前主流的LLM-as-a-Judge监督范式存在根本困境：概率系统如何可靠监督其他概率系统而不继承其失败模式？形式化验证提供了原则性解决方案，但自然语言需求到形式化规范的转化成为瓶颈。

Method: 提出FoT神经符号框架，采用双向形式思维架构：LLM作为规范编译器，自上而下将高层人类意图分解为原子可验证约束，然后自下而上使用Dafny规范和Z3可满足性模理论求解证明合规性，提供数学保证而非概率评分。

Result: 在三个基准测试（行为安全、多领域约束遵守、代理向上欺骗检测）上验证，7个代理模型实验显示：FoT相比LLM-as-a-Judge基线平均提升16.6%，实现弱到强泛化（7B法官检测72B代理欺骗准确率超90%），通过迭代细化提供近线性安全性改进。

Conclusion: FoT框架成功解决了自然语言需求到形式化规范的转化瓶颈，为LLM代理行为安全监督提供了数学保证的方法，显著优于传统概率监督范式，支持弱到强泛化和迭代改进。

Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [32] [AgentTrace: A Structured Logging Framework for Agent System Observability](https://arxiv.org/abs/2602.10133)
*Adam AlSayyad,Kelvin Yuxiang Huang,Richik Pal*

Main category: cs.SE

TL;DR: AgentTrace是一个动态可观测性和遥测框架，通过运行时检测LLM代理，捕获结构化日志，解决高风险领域部署的安全和可追溯性问题。


<details>
  <summary>Details</summary>
Motivation: LLM代理的非确定性行为使得传统静态审计方法失效，现有安全方法无法提供足够的透明度或可追溯性，限制了LLM代理在高风险领域的采用。

Method: AgentTrace在运行时以最小开销检测代理，捕获三个层面的结构化日志：操作层面、认知层面和上下文层面，强调连续、可内省的跟踪捕获。

Result: AgentTrace能够实现更可靠的代理部署、细粒度风险分析和知情信任校准，为代理安全、问责和实时监控提供基础层。

Conclusion: AgentTrace通过动态可观测性和遥测框架，解决了LLM代理在敏感环境中部署的关键安全问题，为高风险领域采用LLM代理铺平道路。

Abstract: Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned software assurance. Existing security methods, such as proxy-level input filtering and model glassboxing, fail to provide sufficient transparency or traceability into agent reasoning, state changes, or environmental interactions. In this work, we introduce AgentTrace, a dynamic observability and telemetry framework designed to fill this gap. AgentTrace instruments agents at runtime with minimal overhead, capturing a rich stream of structured logs across three surfaces: operational, cognitive, and contextual. Unlike traditional logging systems, AgentTrace emphasizes continuous, introspectable trace capture, designed not just for debugging or benchmarking, but as a foundational layer for agent security, accountability, and real-time monitoring. Our research highlights how AgentTrace can enable more reliable agent deployment, fine-grained risk analysis, and informed trust calibration, thereby addressing critical concerns that have so far limited the use of LLM agents in sensitive environments.

</details>


### [33] [Can Large Language Models Implement Agent-Based Models? An ODD-based Replication Study](https://arxiv.org/abs/2602.10140)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 评估17个当代LLM在ODD到代码翻译任务中的表现，使用PPHPC捕食者-猎物模型作为参考，发现GPT-4.1能生成统计有效且高效的实现，但可执行性不足以保证科学用途。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能够从文本描述合成可执行代码，需要评估它们能否从标准化规范可靠地实现基于代理的模型，以支持复制、验证和确认。

Method: 使用PPHPC捕食者-猎物模型作为完全指定的参考，评估17个当代LLM在受控的ODD到代码翻译任务中的表现。生成的Python实现通过分阶段可执行性检查、与已验证NetLogo基线的模型无关统计比较，以及运行时效率和可维护性的定量测量进行评估。

Result: 行为上忠实的实现是可实现的但并非保证，可执行性本身不足以满足科学用途。GPT-4.1始终产生统计有效且高效的实现，Claude 3.7 Sonnet表现良好但可靠性较低。

Conclusion: LLM作为模型工程工具既有前景也有当前局限性，对可重复的基于代理和环境建模具有启示意义。

Abstract: Large language models (LLMs) can now synthesize non-trivial executable code from textual descriptions, raising an important question: can LLMs reliably implement agent-based models from standardized specifications in a way that supports replication, verification, and validation? We address this question by evaluating 17 contemporary LLMs on a controlled ODD-to-code translation task, using the PPHPC predator-prey model as a fully specified reference. Generated Python implementations are assessed through staged executability checks, model-independent statistical comparison against a validated NetLogo baseline, and quantitative measures of runtime efficiency and maintainability. Results show that behaviorally faithful implementations are achievable but not guaranteed, and that executability alone is insufficient for scientific use. GPT-4.1 consistently produces statistically valid and efficient implementations, with Claude 3.7 Sonnet performing well but less reliably. Overall, the findings clarify both the promise and current limitations of LLMs as model engineering tools, with implications for reproducible agent-based and environmental modelling.

</details>


### [34] [EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems](https://arxiv.org/abs/2602.10171)
*Wentao Zhang,Jianfeng Wang,Liheng Liang,Yilei Zhao,HaiBin Wen,Zhe Zhao*

Main category: cs.SE

TL;DR: EvoCodeBench是一个评估自进化LLM驱动编码系统的基准，它追踪性能动态、比较人类表现，并支持多语言分析，超越了传统仅关注静态正确性的基准。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准主要强调静态正确性，假设推理期间模型能力固定，无法捕捉推理时的自我进化（如迭代改进解决方案时准确性和效率的提升），且资源成本核算有限，很少将模型性能与人类程序员校准。此外，许多基准被高资源语言主导，跨语言鲁棒性和长尾语言稳定性未充分探索。

Method: 提出EvoCodeBench基准，用于评估跨编程语言的自进化LLM驱动编码系统，并与人类性能直接比较。该基准追踪性能动态，测量解决方案正确性以及效率指标（如解决时间、内存消耗和重复问题解决尝试中的改进算法设计）。通过直接比较模型与人类程序员在相同任务上的表现，实现以人为中心的相对性能评估。支持多种编程语言，在统一协议下进行系统性的跨语言和长尾稳定性分析。

Result: 自进化系统在效率方面随时间显示出可测量的增益，人类相对和多语言分析提供了仅通过准确性无法获得的见解。EvoCodeBench为评估进化中的LLM驱动系统的编码智能奠定了基础。

Conclusion: EvoCodeBench建立了一个评估进化LLM驱动系统中编码智能的基础，通过追踪性能动态、比较人类表现和多语言分析，提供了超越传统准确性评估的全面视角。

Abstract: As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.

</details>


### [35] [TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation](https://arxiv.org/abs/2602.10471)
*Steven Liu,Jane Luo,Xin Zhang,Aofan Liu,Hao Liu,Jie Wu,Ziyang Huang,Yangyu Huang,Yu Kang,Scarlett Li*

Main category: cs.SE

TL;DR: TestExplora是一个评估LLMs作为主动测试者的基准，包含2389个任务，要求模型通过对比实现与文档意图来发现bug，当前最先进模型的最大F2P率仅为16.06%


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件开发的评估主要关注回归预防和bug复现，忽视了主动发现缺陷的目标，存在"合规陷阱"问题，需要评估LLMs在真实仓库环境中作为主动测试者的能力

Method: 提出TestExplora基准，包含2389个任务来自482个仓库，隐藏所有缺陷相关信号，要求模型通过对比实现与文档推导的意图来发现bug；采用持续、时间感知的数据收集方法防止数据泄露

Result: 最先进模型的最大Fail-to-Pass率仅为16.06%；SWEAgent配合GPT-5-mini达到17.27%的F2P和29.7%的F2P@5，表明智能体探索在主动bug发现任务中有效

Conclusion: LLMs在主动软件质量保证方面存在显著能力差距，处理复杂跨模块交互和利用智能体探索是提升能力的关键方向

Abstract: Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.

</details>


### [36] [From Prompt-Response to Goal-Directed Systems: The Evolution of Agentic AI Software Architecture](https://arxiv.org/abs/2602.10479)
*Mamdouh Alenezi*

Main category: cs.SE

TL;DR: 该论文探讨了从静态生成模型向自主智能代理系统的架构转型，提出了生产级LLM代理的参考架构、多代理拓扑分类和企业级加固清单，分析了行业平台发展趋势。


<details>
  <summary>Details</summary>
Motivation: 研究智能代理从传统理论（反应式、审慎式、BDI模型）向现代LLM中心化方法的演进，解决当前代理系统在可扩展性、可靠性和企业级部署方面的挑战。

Method: 通过连接传统智能代理理论与当代LLM方法，分析行业平台（Kore.ai、Salesforce Agentforce等），提出参考架构、拓扑分类和加固清单。

Result: 识别出行业向标准化代理循环、注册表和可审计控制机制的趋同趋势，提出了支持可扩展组合自主性的共享协议、类型化契约和分层治理结构的发展方向。

Conclusion: 代理式AI发展将类似Web服务的成熟过程，需要共享协议和分层治理结构，但可验证性、互操作性和安全自主性仍是未来研究和实际部署的关键挑战。

Abstract: Agentic AI denotes an architectural transition from stateless, prompt-driven generative models toward goal-directed systems capable of autonomous perception, planning, action, and adaptation through iterative control loops. This paper examines this transition by connecting foundational intelligent agent theories, including reactive, deliberative, and Belief-Desire-Intention models, with contemporary LLM-centric approaches such as tool invocation, memory-augmented reasoning, and multi-agent coordination. The paper presents three primary contributions: (i) a reference architecture for production-grade LLM agents that separates cognitive reasoning from execution using typed tool interfaces; (ii) a taxonomy of multi-agent topologies, together with their associated failure modes and mitigation approaches; and (iii) an enterprise hardening checklist that incorporates governance, observability, and reproducibility considerations. Through an analysis of emerging industry platforms, including Kore.ai, Salesforce Agentforce, TrueFoundry, ZenML, and LangChain, the study identifies a convergence toward standardized agent loops, registries, and auditable control mechanisms. It is argued that the subsequent phase of agentic AI development will parallel the maturation of web services, relying on shared protocols, typed contracts, and layered governance structures to support scalable and composable autonomy. The persistent challenges related to verifiability, interoperability, and safe autonomy remain key areas for future research and practical deployment.

</details>


### [37] [Consistency Meets Verification: Enhancing Test Generation Quality in Large Language Models Without Ground-Truth Solutions](https://arxiv.org/abs/2602.10522)
*Hamed Taherkhani,Alireza DaghighFarsoodeh,Mohammad Chowdhury,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: ConVerTest是一个两阶段测试生成管道，无需真实代码即可合成可靠测试，通过自一致性、验证链和双重执行协议提高测试有效性、行覆盖率和变异分数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试生成方法依赖真实代码进行验证，可能导致错误传播，且在测试驱动开发中适用性有限。需要一种无需先验代码实现即可生成可靠测试的方法。

Method: 提出ConVerTest两阶段管道：1) 使用自一致性通过多数投票生成收敛测试用例；2) 使用验证链进行迭代推理引导的代码优化；3) 双重执行协议通过共识交叉验证代码和测试。

Result: 在BIGCODEBENCH和LBPP基准测试中，ConVerTest相比基线方法，测试有效性提高39%，行覆盖率提高28%，变异分数提高18%。

Conclusion: ConVerTest是缓解幻觉、增强自主软件测试代理可靠性的稳健解决方案，为无需真实代码的测试生成提供了有效方法。

Abstract: Large Language Models (LLMs) have significantly advanced automated test generation, yet existing methods often rely on ground-truth code for verification, risking bug propagation and limiting applicability in test-driven development. We present ConVerTest, a novel two-stage pipeline for synthesizing reliable tests without requiring prior code implementations. ConVerTest integrates three core strategies: (i) Self-Consistency(SC) to generate convergent test cases via majority voting; (ii) Chain-of-Verification (CoVe) for iterative, reasoning-guided code refinement; and (iii) a Dual Execution Agreement to crossvalidate code and tests through consensus. Experiments on BIGCODEBENCH and LESS BASIC PYTHON PROBLEMS (LBPP) benchmarks demonstrate that ConVerTest improves test validity, line coverage, and mutation scores by up to 39%, 28%, and 18% respectively over baselines. Our findings highlight ConVerTest as a robust solution for mitigating hallucinations and enhancing the reliability of autonomous software testing agents.

</details>


### [38] [ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents](https://arxiv.org/abs/2602.10620)
*YoungHoon Jeon,Suwan Kim,Haein Son,Sookbun Lee,Yeil Jeong,Unggi Lee*

Main category: cs.SE

TL;DR: 提出了ISD-Agent-Bench基准，包含25,795个场景，用于评估LLM代理在教学设计自动化中的表现，发现结合经典ISD理论和现代ReAct推理的代理性能最佳。


<details>
  <summary>Details</summary>
Motivation: LLM代理在自动化教学设计方面显示出潜力，但缺乏标准化基准且存在LLM作为评判者的偏见风险，需要可靠的评估方法。

Method: 使用Context Matrix框架生成25,795个场景，结合51个上下文变量和33个ADDIE子步骤；采用多评判协议使用不同提供商的LLM确保可靠性；比较基于经典ISD理论（ADDIE、Dick & Carey、Rapid Prototyping）的代理与现有代理。

Result: 在1,017个测试场景中，结合经典ISD框架和现代ReAct推理的代理性能最高，优于纯理论代理和技术导向方法；理论质量与基准表现强相关，理论代理在问题中心设计和目标评估对齐方面优势显著。

Conclusion: 经典ISD理论与现代推理技术的结合在教学设计自动化中表现最佳，为系统化的LLM-based ISD研究奠定了基础。

Abstract: Large Language Model (LLM) agents have shown promising potential in automating Instructional Systems Design (ISD), a systematic approach to developing educational programs. However, evaluating these agents remains challenging due to the lack of standardized benchmarks and the risk of LLM-as-judge bias. We present ISD-Agent-Bench, a comprehensive benchmark comprising 25,795 scenarios generated via a Context Matrix framework that combines 51 contextual variables across 5 categories with 33 ISD sub-steps derived from the ADDIE model. To ensure evaluation reliability, we employ a multi-judge protocol using diverse LLMs from different providers, achieving high inter-judge reliability. We compare existing ISD agents with novel agents grounded in classical ISD theories such as ADDIE, Dick \& Carey, and Rapid Prototyping ISD. Experiments on 1,017 test scenarios demonstrate that integrating classical ISD frameworks with modern ReAct-style reasoning achieves the highest performance, outperforming both pure theory-based agents and technique-only approaches. Further analysis reveals that theoretical quality strongly correlates with benchmark performance, with theory-based agents showing significant advantages in problem-centered design and objective-assessment alignment. Our work provides a foundation for systematic LLM-based ISD research.

</details>


### [39] [Hidden Licensing Risks in the LLMware Ecosystem](https://arxiv.org/abs/2602.10758)
*Bo Wang,Yueyang Chen,Jieke Shi,Minghui Li,Yunbo Lyu,Yinan Wu,Youfang Lin,Zhou Yang*

Main category: cs.SE

TL;DR: 该论文研究了LLMware（集成LLM的软件系统）中的许可证兼容性问题，构建了大规模数据集，分析了许可证分布和冲突，并提出了LiAgent框架来检测许可证不兼容问题。


<details>
  <summary>Details</summary>
Motivation: LLMware系统将LLM与传统软件组件结合，形成了跨越开源软件、模型和数据集复杂供应链，但其中许可证兼容性问题尚未得到充分研究，存在潜在的法律风险。

Method: 1) 从GitHub和Hugging Face收集LLMware供应链数据（12,180个仓库、3,988个LLM、708个数据集）；2) 分析许可证分布和讨论；3) 评估现有检测方法；4) 提出LiAgent框架（基于LLM的代理）进行生态系统级许可证兼容性分析。

Result: 1) LLMware许可证分布与传统OSS生态系统显著不同；2) 许可证选择和维护占讨论的84%；3) 现有检测方法F1分数仅为58%和76%；4) LiAgent达到87%的F1分数，提升14个百分点；5) 检测到60个不兼容问题，其中11个被开发者确认，两个冲突模型下载量分别超过1.07亿和500万。

Conclusion: LLMware生态系统面临严重的许可证兼容性挑战，需要专门的分析工具。LiAgent框架在检测许可证冲突方面表现优异，为LLMware的可持续发展提供了重要支持。

Abstract: Large Language Models (LLMs) are increasingly integrated into software systems, giving rise to a new class of systems referred to as LLMware. Beyond traditional source-code components, LLMware embeds or interacts with LLMs that depend on other models and datasets, forming complex supply chains across open-source software (OSS), models, and datasets. However, licensing issues emerging from these intertwined dependencies remain largely unexplored. Leveraging GitHub and Hugging Face, we curate a large-scale dataset capturing LLMware supply chains, including 12,180 OSS repositories, 3,988 LLMs, and 708 datasets. Our analysis reveals that license distributions in LLMware differ substantially from traditional OSS ecosystems. We further examine license-related discussions and find that license selection and maintenance are the dominant concerns, accounting for 84% of cases. To understand incompatibility risks, we analyze license conflicts along supply chains and evaluate state-of-the-art detection approaches, which achieve only 58% and 76% F1 scores in this setting. Motivated by these limitations, we propose LiAgent, an LLM-based agent framework for ecosystem-level license compatibility analysis. LiAgent achieves an F1 score of 87%, improving performance by 14 percentage points over prior methods. We reported 60 incompatibility issues detected by LiAgent, 11 of which have been confirmed by developers. Notably, two conflicted LLMs have over 107 million and 5 million downloads on Hugging Face, respectively, indicating potentially widespread downstream impact. We conclude with implications and recommendations to support the sustainable growth of the LLMware ecosystem.

</details>


### [40] [VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection](https://arxiv.org/abs/2602.10787)
*Samal Mukhtar,Yinghua Yao,Zhu Sun,Mustafa Mustafa,Yew Soon Ong,Youcheng Sun*

Main category: cs.SE

TL;DR: VulReaD：一种基于知识图谱引导的漏洞推理与检测方法，通过安全知识图谱和教师LLM生成CWE一致的对比推理监督，使用ORPO微调学生模型，显著提升漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前软件漏洞检测主要关注二元评估，且LLM生成的解释往往缺乏与CWE类别的语义一致性。需要一种能够超越二元分类、实现CWE级别推理的漏洞检测方法。

Method: 提出VulReaD方法：1) 使用安全知识图谱作为语义骨干；2) 利用强教师LLM生成CWE一致的对比推理监督；3) 使用ORPO（Odds Ratio Preference Optimization）微调学生模型，鼓励符合分类学的推理同时抑制无支持的说明。

Result: 在三个真实世界数据集上，VulReaD相比最先进的基线方法：二元F1提升8-10%，多类分类的Macro-F1提升30%，Micro-F1提升18%。LLM在二元检测中优于深度学习基线，KG引导的推理增强了CWE覆盖率和可解释性。

Conclusion: VulReaD通过知识图谱引导的推理方法，有效提升了漏洞检测的准确性和可解释性，实现了从二元分类到CWE级别推理的转变，为软件漏洞检测提供了新的解决方案。

Abstract: Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.

</details>


### [41] [PELLI: Framework to effectively integrate LLMs for quality software generation](https://arxiv.org/abs/2602.10808)
*Rasmus Krebs,Somnath Mazumdar*

Main category: cs.SE

TL;DR: 本文提出PELLI框架，通过迭代分析评估LLM生成的代码质量，涵盖可维护性、性能和可靠性三个非功能性需求，比较了五种主流LLM在三个应用领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究在比较LLM生成的代码时存在两个主要问题：1) 主要只考虑可靠性作为评估指标；2) 只选择少数LLM（如Codex和ChatGPT）进行比较。需要更全面的代码质量评估框架。

Method: 提出PELLI（Programmatic Excellence via LLM Iteration）框架，这是一个基于迭代分析的过程，用于维护高质量的代码变更。框架包含：1) 全面的评估方法，生成三个主要非功能性需求（可维护性、性能、可靠性）的量化指标；2) 选择五种流行的LLM进行比较；3) 在三个应用领域中应用，遵循Python编码标准。

Result: 基于三个非功能性需求的评估发现：1) GPT-4T和Gemini表现略好；2) 提示设计会影响整体代码质量；3) 每个应用领域在不同指标上表现出高低不一的分数，即使在同一指标上，不同提示也会产生差异。

Conclusion: PELLI框架可以作为开发人员的实用指南，在遵循公认质量标准的同时利用LLM。该研究结果对推进LLM技术在实际应用中的发展至关重要，为利益相关者提供了清晰的见解，了解这些LLM的优势和需要改进的地方。

Abstract: Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric and selected a few LLMs (such as Codex and ChatGPT) for comparision. This paper proposes a comprehensive code quality assessment framework called Programmatic Excellence via LLM Iteration (PELLI). PELLI is an iterative analysis-based process that upholds high-quality code changes. We extended the state-of-the-art by performing a comprehensive evaluation that generates quantitative metrics for analyzing three primary nonfunctional requirements (such as maintainability, performance, and reliability) while selecting five popular LLMs. For PELLI's applicability, we selected three application domains while following Python coding standards. Following this framework, practitioners can ensure harmonious integration between LLMs and human developers, ensuring that their potential is fully realized. PELLI can serve as a practical guide for developers aiming to leverage LLMs while adhering to recognized quality standards. This study's outcomes are crucial for advancing LLM technologies in real-world applications, providing stakeholders with a clear understanding of where these LLMs excel and where they require further refinement. Overall, based on three nonfunctional requirements, we have found that GPT-4T and Gemini performed slightly better. We also found that prompt design can influence the overall code quality. In addition, each application domain demonstrated high and low scores across various metrics, and even within the same metrics across different prompts.

</details>


### [42] [FeatureBench: Benchmarking Agentic Coding for Complex Feature Development](https://arxiv.org/abs/2602.10975)
*Qixing Zhou,Jiacheng Zhang,Haiyang Wang,Rui Hao,Jiahe Wang,Minghao Han,Yuxue Yang,Shuzhe Wu,Feiyang Pan,Lue Fan,Dandan Tu,Zhaoxiang Zhang*

Main category: cs.SE

TL;DR: FeatureBench是一个评估LLM代理在端到端、面向功能软件开发中编码能力的基准测试，通过执行评估和自动化任务收集，发现当前最先进模型在复杂功能开发任务上表现较差（仅11%成功率）。


<details>
  <summary>Details</summary>
Motivation: 现有代理编码基准测试存在任务范围有限（如仅修复单个PR中的bug）、依赖非可执行评估、缺乏自动化更新机制等问题，需要更全面的评估框架来了解LLM代理在真实软件开发中的能力边界。

Method: 提出FeatureBench基准测试，采用执行评估协议和可扩展的测试驱动方法，通过依赖图追踪单元测试，自动从代码仓库中提取跨多个提交和PR的功能级编码任务，确保功能分离后其他功能正常运行。

Result: 从24个开源仓库中收集了200个挑战性评估任务和3825个可执行环境，实验显示最先进的Claude 4.5 Opus模型在SWE-bench上达到74.4%解决率，但在FeatureBench上仅成功11.0%的任务。

Conclusion: FeatureBench揭示了当前LLM代理在复杂功能开发任务上的局限性，其自动化任务收集工具使基准测试易于扩展和更新，避免数据泄露，构建的可验证环境对代理训练也有潜在价值。

Abstract: Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Towards Autonomous Mathematics Research](https://arxiv.org/abs/2602.10177)
*Tony Feng,Trieu H. Trinh,Garrett Bingham,Dawsen Hwang,Yuri Chervonyi,Junehyuk Jung,Joonkyung Lee,Carlo Pagano,Sang-hyun Kim,Federico Pasqualotto,Sergei Gukov,Jonathan N. Lee,Junsu Kim,Kaiying Hou,Golnaz Ghiasi,Yi Tay,YaGuang Li,Chenkai Kuang,Yuan Liu,Hanzhao,Lin,Evan Zheran Liu,Nigamaa Nayakanti,Xiaomeng Yang,Heng-tze Cheng,Demis Hassabis,Koray Kavukcuoglu,Quoc V. Le,Thang Luong*

Main category: cs.LG

TL;DR: Aletheia是一个数学研究智能体，能够从奥数题到博士级问题，甚至协助生成研究论文和解决开放问题，展示了AI在数学研究中的自主性和协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理系统虽然能在国际数学奥林匹克竞赛中达到金牌水平，但从竞赛级问题解决过渡到专业研究需要处理大量文献和构建长程证明。因此需要开发能够应对数学研究复杂性的智能体。

Method: Aletheia采用迭代生成、验证和修订解决方案的端到端自然语言处理流程，结合Gemini Deep Think进行挑战性推理，使用新颖的推理时间扩展定律，并密集使用工具来应对数学研究的复杂性。

Result: 展示了三个里程碑：1) AI完全自主生成的研究论文(Feng26)；2) 人机协作证明独立集边界的研究论文(LeeSeo26)；3) 对700个开放问题的半自主评估，包括自主解决四个开放问题。

Conclusion: 提出需要制定标准来量化AI辅助结果的自主性和新颖性，并对人机协作在数学研究中的前景进行了反思。

Abstract: Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.

</details>


### [44] [How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge](https://arxiv.org/abs/2602.10210)
*Junhong Lin,Bing Zhang,Song Wang,Ziyan Liu,Dan Gutfreund,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 提出了HybridRAG-Bench框架，用于评估LLM在混合知识（非结构化文本+结构化知识图谱）上的检索密集型多跳推理能力，通过自动从arXiv最新科学文献构建基准，避免预训练数据污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试与LLM预训练数据重叠严重，难以区分真正的检索推理与参数记忆。需要构建能评估混合知识增强系统真实检索推理能力的基准。

Method: 从arXiv最新科学文献自动构建混合知识表示（非结构化文本+结构化知识图谱），生成基于显式推理路径的知识密集型问答对，支持灵活领域和时间范围选择。

Result: 在人工智能、治理与政策、生物信息学三个领域的实验表明，HybridRAG-Bench能有效评估真实的检索推理能力而非参数记忆，为混合知识增强推理系统提供原则性测试平台。

Conclusion: HybridRAG-Bench框架解决了现有基准的数据污染问题，为评估LLM在混合知识上的检索密集型多跳推理提供了可靠、可定制的评估方法。

Abstract: Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.

</details>


### [45] [Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2602.10224)
*Shiting Huang,Zecheng Li,Yu Zeng,Qingnan Ren,Zhen Fang,Qisheng Su,Kou Shi,Lin Chen,Zehui Chen,Feng Zhao*

Main category: cs.LG

TL;DR: 提出MEL框架，通过自我蒸馏元经验来增强LLM的推理能力，在RLVR基础上增加对比分析和错误归因机制，实现知识复用


<details>
  <summary>Details</summary>
Motivation: RLVR虽然有效，但缺乏错误归因和经验内化机制，限制了细粒度信用分配和可复用知识形成，需要引入类似人类学习周期的元经验学习

Method: 在标准RLVR基础上，利用LLM自我验证能力对正确和错误轨迹进行对比分析，识别推理错误的分叉点，总结为可泛化的元经验，通过最小化负对数似然将其内化到参数记忆中

Result: MEL在基准测试中取得一致改进，在不同模型规模上获得3.92%-4.73%的Pass@1增益

Conclusion: MEL框架通过元经验学习有效解决了RLVR的元学习瓶颈，实现了更好的错误归因和知识复用，显著提升了LLM的推理性能

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.

</details>


### [46] [Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents](https://arxiv.org/abs/2602.10226)
*Haochen Wang,Yi Wu,Daryl Chang,Li Wei,Lukasz Heldt*

Main category: cs.LG

TL;DR: 提出一个基于LLM的自进化系统，用于自动化生成、训练和部署YouTube推荐模型的改进，通过内外循环代理实现端到端工作流，超越传统人工迭代方法。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习系统（如视频平台推荐模型）的优化面临巨大超参数搜索空间和复杂优化器/架构设计挑战，传统依赖人工迭代效率低下，需要自动化解决方案。

Method: 构建基于Google Gemini LLM的自进化系统，包含离线代理（内循环）使用代理指标进行高吞吐量假设生成，在线代理（外循环）在生产环境中用延迟业务指标验证候选方案，代理扮演专业机器学习工程师角色。

Result: 在YouTube成功部署多个生产版本，证明LLM驱动的自主进化在开发速度和模型性能上都超越了传统工程工作流。

Conclusion: 基于LLM的自进化系统能够自主发现优化算法、模型架构和奖励函数的创新改进，为大规模机器学习系统优化提供了高效自动化解决方案。

Abstract: Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.

</details>


### [47] [Confounding Robust Continuous Control via Automatic Reward Shaping](https://arxiv.org/abs/2602.10305)
*Mateo Juliani,Mingxuan Li,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出一种从离线数据中自动学习奖励塑形函数的方法，用于连续控制问题，即使在存在未观测混杂变量的情况下也能保证性能。


<details>
  <summary>Details</summary>
Motivation: 奖励塑形被广泛用于加速强化学习训练，但如何设计有效的奖励塑形函数，特别是针对复杂连续控制问题，仍然缺乏系统方法。现有方法难以处理离线数据中可能存在的未观测混杂变量。

Method: 基于因果贝尔曼方程学习最优状态值的紧上界，将其作为基于势的奖励塑形框架中的势函数。使用Soft-Actor-Critic在多个连续控制基准上进行测试。

Result: 在多个常用连续控制基准测试中表现出色，在未观测混杂变量存在的情况下具有强大的性能保证。

Conclusion: 这是从因果视角实现混杂鲁棒连续控制的重要第一步，为自动学习奖励塑形函数提供了有效方法。

Abstract: Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.

</details>


### [48] [Simple LLM Baselines are Competitive for Model Diffing](https://arxiv.org/abs/2602.10371)
*Elias Kempf,Simon Schrodi,Bartosz Cywiński,Thomas Brox,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文提出模型差异分析方法的评估指标，比较LLM基线和稀疏自编码器方法，发现改进的LLM基线在发现抽象行为差异方面表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 标准LLM评估只能测试设计好的能力，会遗漏模型修订间的意外行为差异或新出现的未对齐倾向。现有模型差异分析方法缺乏系统比较和评估标准。

Method: 提出评估指标（泛化性、有趣性、抽象层次）来比较现有方法，包括LLM基线和稀疏自编码器方法，并改进LLM基线方法。

Result: 改进的LLM基线方法与SAE方法表现相当，且通常能发现更抽象的行为差异。

Conclusion: 模型差异分析需要系统评估框架，改进的LLM方法在发现抽象行为差异方面具有优势，为模型行为分析提供了实用工具。

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

</details>


### [49] [Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation](https://arxiv.org/abs/2602.10430)
*Jie Jiang,Yusen Huo,Xiangxin Zhan,Changping Wang,Jun Zhang*

Main category: cs.LG

TL;DR: DRPO通过分布鲁棒优化解决离线强化学习中低质量数据导致的模型崩溃问题，使用硬过滤机制恢复高质量行为分布。


<details>
  <summary>Details</summary>
Motivation: 基于策略的强化学习在生成式推荐中面临离线历史日志中低质量数据主导的问题，导致严重的模型崩溃。现有方法无法在方差减少和噪声模仿之间取得平衡。

Method: 提出分布鲁棒策略优化(DRPO)，将目标重新表述为乐观分布鲁棒优化问题，证明硬过滤是该DRO目标的精确解，能够最优地恢复高质量行为同时严格丢弃导致发散的噪声。

Result: 在混合质量推荐基准测试中，DRPO实现了最先进的性能表现。

Conclusion: 通过识别噪声行为策略中潜在的高质量分布，并采用分布鲁棒优化框架，DRPO成功解决了离线强化学习中的模型崩溃问题。

Abstract: Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.

</details>


### [50] [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576)
*Boxiao Wang,Kai Li,Tianyi Liu,Chen Li,Junzhe Wang,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 提出PiT-PO框架，通过强化学习将LLM转变为自适应生成器，在符号回归中实现物理一致且结构简洁的方程发现


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLM作为静态生成器，仅通过提示引导，无法基于搜索反馈更新模型内部表示，导致物理不一致或数学冗余表达式

Method: PiT-PO框架：通过强化学习将LLM演化为自适应生成器，采用双重约束机制——强制层次化物理有效性，同时应用细粒度token级惩罚抑制冗余结构

Result: 在标准基准测试中达到最先进性能，成功为挑战性流体动力学问题发现新的湍流模型，使小规模模型超越闭源大型模型

Conclusion: PiT-PO能够生成科学一致且结构简洁的方程，民主化高性能科学发现，使小规模模型也能取得优异表现

Abstract: Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

</details>


### [51] [Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving](https://arxiv.org/abs/2602.10512)
*Sho Sonoda,Shunta Akiyama,Yuya Uezato*

Main category: cs.LG

TL;DR: 论文为LLM指导的形式化定理证明提供理论分析，将策略提议建模为有限时域确定性MDP中的随机策略，通过引入参考策略生成的问题分布和潜在变量模型解释经验成功与最坏情况难度的差距，主要分离结果表明分层（cut-aware）学习器相比扁平（cut-free）学习器具有指数级数据效率优势。


<details>
  <summary>Details</summary>
Motivation: 解释LLM指导的形式化定理证明（如Lean中）中经验成功与最坏情况理论难度之间的差距，为最近基于子目标分解的智能定理证明器提供理论依据。

Method: 将策略提议建模为有限时域确定性MDP中的随机策略，状态和动作空间为一般紧度量空间，假设Lipschitz策略。引入由参考策略q生成的问题分布，包括具有可重用cut/lemma/sketch结构的潜在变量模型（用证明DAG表示）。在top-k搜索协议和Tsybakov型边界条件下，推导有限时域成功概率的下界。

Result: 主要分离结果表明：当cut消除将深度为D的DAG扩展为规模Ω(Λ^D)的无cut树，而cut-aware分层过程规模为O(λ^D)且λ≪Λ时，扁平（cut-free）学习器相比分层（cut-aware）学习器需要指数级更多数据。这为智能定理证明器中的子目标分解提供了原理性证明。

Conclusion: 论文为LLM指导的形式化定理证明提供了理论框架，解释了经验成功与理论难度的差距，并证明了分层学习在数据效率上的指数级优势，为智能定理证明器中的子目标分解策略提供了理论支持。

Abstract: We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $Ω(Λ^D)$ while the cut-aware hierarchical process has size $O(λ^D)$ with $λ\llΛ$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.

</details>


### [52] [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693)
*Guobin Shen,Chenxiao Zhao,Xiang Cheng,Lei Huang,Xing Yu*

Main category: cs.LG

TL;DR: VESPO是一种用于大型语言模型强化学习的变分序列级软策略优化方法，通过变分公式推导出闭式重塑核，直接处理序列级重要性权重，解决了策略陈旧和异步训练导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强化学习中训练稳定性是一个核心挑战。策略陈旧、异步训练以及训练与推理引擎不匹配会导致行为策略与当前策略偏离，引发训练崩溃。重要性采样虽然能纠正这种分布偏移，但存在高方差问题，现有解决方案缺乏统一的理论基础。

Method: 提出VESPO方法，将方差减少纳入变分公式中，推导出闭式重塑核，直接操作序列级重要性权重，无需长度归一化。该方法通过变分优化提案分布来稳定训练。

Result: 在数学推理基准测试中，VESPO能在高达64倍的陈旧比率和完全异步执行下保持训练稳定，在密集模型和专家混合模型上均取得一致性能提升。

Conclusion: VESPO为序列级重要性权重提供了一种理论基础的解决方案，能有效应对策略陈旧和异步训练带来的挑战，提高RL训练稳定性。

Abstract: Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO

</details>


### [53] [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539)
*Guozheng Ma,Lu Li,Haoyu Wang,Zixuan Liu,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文提出了DAWN方法，通过数据锚定预热和归一化解决残差强化学习中价值学习的两个关键瓶颈，显著提升了学习效率。


<details>
  <summary>Details</summary>
Motivation: 残差强化学习虽然能稳定在线优化预训练策略，但其价值学习存在独特挑战，特别是冷启动问题和结构尺度不匹配问题，这些问题尚未得到充分理解。

Method: 提出DAWN方法，包含两个核心组件：1) 使用基础策略转换作为价值锚点进行隐式预热；2) 通过评论家归一化恢复表示敏感性以辨别价值差异。

Result: DAWN在多样化基准测试、策略架构和观察模态中展示了显著的效率提升，证明了该方法在残差强化学习中的有效性。

Conclusion: 通过系统分析残差强化学习中价值学习的关键瓶颈，提出了简单而原则性的解决方案DAWN，该方法能有效提升学习效率，为残差强化学习的价值学习问题提供了新见解。

Abstract: Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.

</details>


### [54] [ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents](https://arxiv.org/abs/2602.10863)
*Cong Pang,Xuyu Feng,Yujie Yi,Zixuan Chen,Jiawei Hong,Tiankuo Yao,Nang Yuan,Jiapeng Luo,Lewei Lu,Xin Lou*

Main category: cs.LG

TL;DR: 提出视觉原生搜索框架，将网页表示为视觉快照，结合信息感知信用分配方法，在开放网络环境中解决强化学习代理的信用分配瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的信息搜索代理在开放网络环境中面临低信噪比反馈的挑战：文本解析器丢弃布局语义并引入非结构化噪声，而长时程训练通常依赖稀疏结果奖励，难以识别哪些检索动作真正重要。

Method: 1) 视觉原生搜索框架：将网页表示为视觉快照，利用布局线索快速定位关键证据并抑制干扰；2) 信息感知信用分配：通过后验分析估计每个检索快照对最终结果的贡献，将密集学习信号传播到关键搜索步骤；3) 集成GRPO训练流程。

Result: 在多样化信息搜索基准测试中，该方法持续优于基于文本的基线方法，证明视觉快照基础与信息级信用分配能够缓解开放网络环境中的信用分配瓶颈。

Conclusion: 视觉原生表示结合信息级信用分配是解决开放网络环境中强化学习代理信用分配问题的有效方法，代码和数据集已开源。

Abstract: Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.

</details>


### [55] [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018)
*Returaj Burnwal,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出OSIL算法，从离线非偏好演示中学习安全策略，无需显式安全成本或奖励标注


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，在线学习可能风险高，准确指定安全成本困难，但收集反映不安全行为的轨迹相对容易

Method: 将安全策略学习建模为约束马尔可夫决策过程，从非偏好演示中推导奖励最大化目标的下界并学习成本模型

Result: OSIL能够学习满足成本约束的安全策略，且不降低奖励性能，优于多个基线方法

Conclusion: 提出的离线安全模仿学习方法能够从非偏好演示中有效推断安全性，实现安全且奖励最大化的行为学习

Abstract: This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.

</details>


### [56] [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087)
*Jianxun Wang,Grant C. Forbes,Leonardo Villalobos-Arias,David L. Roberts*

Main category: cs.LG

TL;DR: 该论文提出了一种基于f-散度的自适应约束离线强化学习方法，通过线性规划形式和凸共轭理论连接f-散度与贝尔曼残差约束，在多样但探索有限的数据集上实现RL目标与行为策略约束的平衡。


<details>
  <summary>Details</summary>
Motivation: 离线RL数据集通常存在多样性不足、环境探索有限、且来自不同专业水平的行为策略等问题。有限的探索会影响Q/V值估计，而向多样行为策略的约束可能过于保守，需要在RL目标与行为策略约束之间找到平衡。

Method: 首先通过线性规划形式和凸共轭理论建立f-散度与贝尔曼残差优化约束的连接，然后引入灵活的f-散度函数公式，基于离线训练数据集对算法学习目标实施自适应约束。

Result: 在MuJoCo、Fetch和AdroitHand环境上的实验结果表明，所提出的线性规划形式正确，且灵活的f-散度在应用于兼容的约束优化算法时，能够从具有挑战性的数据集中提升学习性能。

Conclusion: 通过f-散度的自适应约束机制，能够在离线RL中更好地平衡优化目标与数据支持约束，特别是在处理多样但探索有限的数据集时表现出优势。

Abstract: Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \textit{Q} or \textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.

</details>


### [57] [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793)
*Li-Min Chu,Kai-Siang Ma,Ming-Hong Chen,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: 提出首个半监督跨域模仿学习（SS-CDIL）算法，仅使用少量目标专家演示和未标记的不完美轨迹，通过新颖的跨域损失函数和自适应权重平衡源域与目标域知识，在MuJoCo和Robosuite上实现稳定且数据高效的政策学习。


<details>
  <summary>Details</summary>
Motivation: 跨域模仿学习（CDIL）通过跨域迁移专家知识加速政策学习，在专家数据收集成本高的应用中很有价值。现有方法要么是监督式（依赖代理任务和显式对齐），要么是无监督式（无需配对数据对齐分布），但往往不稳定。

Method: 提出SS-CDIL设置及首个算法，仅使用离线数据（少量目标专家演示和未标记的不完美轨迹）。为处理域差异，提出新颖的跨域损失函数学习域间状态-动作映射，并设计自适应权重函数平衡源域和目标域知识。

Result: 在MuJoCo和Robosuite上的实验显示，相比基线方法取得一致性能提升，证明该方法能以最小监督实现稳定且数据高效的政策学习。

Conclusion: 提出的SS-CDIL算法是首个半监督跨域模仿学习方法，通过理论证明和实验验证，在仅需少量目标专家演示和未标记轨迹的情况下，能有效处理域差异并实现稳定高效的政策学习。

Abstract: Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.

</details>


### [58] [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819)
*Linxuan Xia,Xiaolong Yang,Yongyuan Chen,Enyue Zhao,Deng Cai,Yasheng Wang,Boxi Wu*

Main category: cs.LG

TL;DR: RePO通过让策略模型重新表述离策略知识为符合自身分布的高质量轨迹，解决了LLM领域对齐中离策略RL训练不稳定和硬样本利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在领域特定数据上的对齐面临挑战：SFT会损害模型通用性，on-policy RL难以有效吸收超出当前推理水平的硬样本，而off-policy RL又因分布偏移导致训练不稳定。

Method: 提出Rephrasing Policy Optimization (RePO)：让策略模型先理解离策略知识，然后将其重新表述为符合自身风格和参数分布的轨迹，用这些高质量轨迹动态替换低奖励的rollouts。

Result: 在多个基准测试中，RePO提高了硬样本利用率，超越了现有基线方法，实现了最先进的性能。

Conclusion: RePO成功调和了有效吸收离策略知识与保持on-policy RL训练稳定性之间的矛盾，为LLM领域对齐提供了新解决方案。

Abstract: Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.

</details>


### [59] [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986)
*Abhishek Vijaya Kumar,Bhaskar Kataria,Byungsoo Oh,Emaad Manzoor,Rachee Singh*

Main category: cs.LG

TL;DR: TVCACHE是一个用于LLM智能体后训练的状态感知工具值缓存系统，通过最长前缀匹配确保缓存一致性，在多种任务中实现高达70%的缓存命中率，将工具调用执行时间中位数降低6.9倍。


<details>
  <summary>Details</summary>
Motivation: 在LLM智能体后训练中，外部工具调用通常需要数秒甚至数分钟，导致GPU空闲时间增加，显著提升了后训练时间和成本。虽然许多工具调用在并行rollout中重复出现，但简单缓存输出是不正确的，因为工具输出依赖于先前智能体交互诱导的环境状态。

Method: TVCACHE维护观察到的工具调用序列树，并执行最长前缀匹配进行缓存查找：只有当智能体的完整工具历史与先前执行的序列完全匹配时才会发生缓存命中，这保证了相同的环境状态。系统在三种不同工作负载上进行评估：终端任务、SQL生成和视频理解。

Result: TVCACHE实现了高达70%的缓存命中率，将工具调用执行时间中位数降低了6.9倍，且在后训练奖励积累方面没有性能下降。

Conclusion: TVCACHE通过状态感知的缓存机制有效解决了LLM智能体后训练中工具调用延迟问题，显著减少了训练时间和成本，同时保证了训练质量。

Abstract: In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.

</details>


### [60] [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128)
*Reinhard Heckel,Mahdi Soltanolkotabi,Christos Thramboulidis*

Main category: cs.LG

TL;DR: 该论文提出非对称提示加权方法，在强化学习中为低成功概率的提示分配更高权重，加速从零开始的RL训练收敛。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法（如GRPO、DAPO、RLOO）主要关注中等成功概率的模糊提示，而对非常容易或非常困难的提示降权。然而在从零开始的RL训练中，低成功概率区域需要更有效的学习策略。

Method: 提出非对称提示加权方法，为低（甚至零）经验成功概率的提示分配更高权重。通过理论分析确定在固定更新预算下最小化从初始成功率到目标准确率所需时间的最优权重。

Result: 非对称加权特别有利于从零开始的RL训练（如R1-Zero），能显著加速收敛；而在SFT后RL中效果较小，因为模型已具备较高准确率。理论证明在低成功区域，最优权重确实是非对称的。

Conclusion: 非对称提示加权是强化学习中的有效策略，特别适用于从零开始的训练场景，通过提升低成功概率提示的权重来加速有效时间收敛。

Abstract: Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.

</details>
