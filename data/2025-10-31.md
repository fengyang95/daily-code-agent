<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 14]
- [wechat.article](#wechat.article) [Total: 15]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 15]
- [tldr.article](#tldr.article) [Total: 6]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 提出了StreetMath基准来评估LLM在现实世界近似计算场景中的表现，发现LLM倾向于精确计算而非近似推理，且精确和近似计算依赖不同的神经组件。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM在精确算术运算中的表现，但对其在非正式、快速近似推理方面的能力研究不足，特别是在非自回归解码器模型中。

Method: 引入StreetMath基准，在不同LLM架构上进行广泛评估，并应用机制可解释性技术探测其内部计算状态。

Result: LLM通常尝试计算精确值或调用外部工具，即使在需要近似的任务中也是如此；模型有时在早期层或步骤得到正确答案，但仍消耗更多token；精确和近似算术运算依赖不同的神经组件。

Conclusion: LLM在街头数学场景中不像人类那样表现出认知吝啬性，缺乏人类在快速近似推理中的效率优化。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提出了三种改进电路发现的方法：使用自助法识别一致归因边、引入比率选择策略、用整数线性规划替代贪婪选择，在MIB基准上获得更忠实和性能更好的电路。


<details>
  <summary>Details</summary>
Motivation: 解决机制可解释性中电路发现的核心挑战，即确定模型执行特定任务的组件部分。

Method: 1. 使用自助法识别具有一致归因分数的边；2. 引入基于比率的简单选择策略来优先考虑强正分数边；3. 用整数线性规划替换标准贪婪选择方法。

Result: 在多个MIB任务和模型上，我们的方法产生了更忠实的电路，并且优于先前的方法。

Conclusion: 提出的三种改进方法显著提升了电路发现的性能和忠实度，为机制可解释性研究提供了有效的工具。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [3] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: LISTEN框架使用LLM作为零样本偏好预言机，通过自然语言指导在多个竞争目标中做出选择，提出了LISTEN-U（参数化效用函数）和LISTEN-T（非参数化锦标赛选择）两种算法。


<details>
  <summary>Details</summary>
Motivation: 解决人类专家在多个竞争目标中选择最佳选项时的困难，特别是难以形式化复杂隐含偏好的问题。

Method: 使用LLM作为偏好预言机，提出LISTEN-U（通过LLM精炼参数化效用函数）和LISTEN-T（在小批量解决方案上进行锦标赛式选择）两种迭代算法。

Result: 在航班预订、购物和考试安排等任务上的评估显示，LISTEN-U在偏好参数对齐时表现优异，LISTEN-T则提供更稳健的性能。

Conclusion: 该工作探索了用自然语言直接指导复杂多目标决策的有前景方向，减轻了传统偏好获取的认知负担。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [4] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 研究探讨了角色设定如何影响不同LLM架构、模型大小和内容模态在有害内容分类中的一致性和公平性。虽然总体准确率影响不大，但角色设定会引入意识形态偏见，导致模型在不同政治立场下对内容危害性的判断产生差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在内容审核系统中的广泛应用，确保其公平性和中立性变得至关重要。本研究旨在探究角色设定对LLM有害内容分类的一致性和公平性的影响。

Method: 通过分析不同LLM架构、模型大小和内容模态（语言vs视觉）下的有害内容分类表现，研究角色设定对模型判断的影响。包括一致性分析、意识形态倾向性测试和政治针对性任务研究。

Result: 角色设定对总体分类准确率影响不大，但会导致明显的意识形态偏见。不同政治立场的角色对内容危害性的判断存在显著差异，较大模型更倾向于与同意识形态角色保持一致，加剧了跨意识形态群体的分歧。

Conclusion: 角色设定会在LLM输出中引入微妙的意识形态偏见，这对伪装中立的AI系统使用提出了警示，可能强化党派观点。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [5] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: RECAP是一种代理式管道，通过反馈驱动循环从LLM输出中提取和验证记忆的训练数据，包括越狱模块来克服对齐导致的拒绝。


<details>
  <summary>Details</summary>
Motivation: 在无法检查LLM训练数据的情况下，需要一种方法来验证模型是否记忆了特定内容，最有力的证据是模型自由重现目标内容。

Method: 采用反馈驱动循环：初始提取尝试由次级语言模型评估，比较输出与参考段落并识别差异，然后转化为最小修正提示反馈给目标模型指导后续生成。还包括检测和克服对齐拒绝的越狱模块。

Result: 在EchoTrace基准测试（涵盖30多本完整书籍）上，RECAP相比单次迭代方法有显著提升。GPT-4.1的版权文本提取平均ROUGE-L得分从0.38提高到0.47，增长近24%。

Conclusion: RECAP能有效提取和验证LLM记忆的训练数据，通过反馈循环和越狱机制显著提升提取性能。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [6] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 提出了监督强化学习(SRL)框架，将问题解决重新定义为生成逻辑"动作"序列，通过内部推理独白和基于专家动作相似性的平滑奖励，解决了小规模开源模型在多步推理任务中的学习困难。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多步推理任务上表现不佳。对于小规模开源模型，强化学习在正确解很少被采样时失败，而监督微调则容易通过逐词模仿过拟合长演示。需要一种新方法来填补这一空白。

Method: SRL框架将问题解决重新定义为生成逻辑动作序列。模型在提交每个动作前生成内部推理独白，基于SFT数据集中提取的专家动作提供逐步相似性奖励，即使在所有推演都错误时也能提供丰富的学习信号。

Result: SRL使小模型能够学习之前SFT或RLVR无法学习的挑战性问题。先用SRL初始化训练再用RLVR精炼可获得最强性能。SRL还能有效泛化到代理软件工程任务。

Conclusion: SRL是一个稳健且通用的面向推理的LLM训练框架，通过结合监督学习和强化学习的优势，解决了小模型在多步推理任务中的学习瓶颈。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [7] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: PORTool是一种基于强化学习的工具使用LLM训练方法，通过探索多种可能的工具调用轨迹来提高模型在动态环境中的性能


<details>
  <summary>Details</summary>
Motivation: 当前基于静态数据集训练的工具使用LLM只能模仿固定的工具调用流程，无法在动态环境中探索多种解决方案，导致性能有限

Method: 使用强化学习方法：1)为查询生成多个工具调用轨迹形成树状结构；2)基于产生正确答案和成功工具调用的能力分配步骤奖励；3)使用步骤相对优势和轨迹相对优势的混合优势来训练LLM

Result: 在包含17个工具的实验中，PORTool在最终准确率和工具调用步骤数量方面相比其他训练方法有显著提升

Conclusion: PORTool通过强化学习探索多种工具调用轨迹，有效提升了工具使用LLM在动态环境中的性能

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [8] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 提出了QCoder Benchmark，一个评估LLMs在量子编程任务上的框架，支持量子模拟器环境和真实人类代码对比，实验显示推理模型表现优于GPT-4o和人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要与硬件设备交互的编程领域（如量子编程）研究不足，需要专门的评估框架来测试模型在真实量子计算环境中的表现。

Method: 开发QCoder Benchmark评估框架，包含量子模拟器环境支持领域特定指标反馈（如电路深度、执行时间、错误分类），并整合真实编程竞赛中的人类代码提交用于对比分析。

Result: GPT-4o仅达到18.97%准确率，而推理模型o3达到78%准确率，超过了人类代码39.98%的平均成功率。

Conclusion: 量子编程对现有LLMs具有挑战性，推理模型在该领域表现突出，QCoder Benchmark为相关研究提供了有价值的评估工具。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [9] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: RCScore是一个多维度框架，用于量化指令表述对模型响应的影响，通过系统地将基准问题转化为多种指令风格来揭示传统指标未检测到的性能变化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估通常依赖单一指令模板，忽视了模型对指令风格的敏感性，这对于实际部署至关重要。

Method: 通过将基准问题系统转化为多种指令风格，并引入交叉响应相似性(CRS)来衡量风格自一致性。

Result: 在四个推理基准上对十个LLM的实验表明，指令风格可使准确率变化高达16.7个百分点，且CRS与任务准确率强相关。确定性解码产生更稳定的输出，模型规模与跨风格一致性正相关。

Conclusion: RCScore提供了评估指令鲁棒性的原则性方法，一致性可作为模型可靠性的有价值代理指标。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [10] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 研究发现不同大语言模型对数字学习出了可互换、系统化且高度准确的表示，尽管在输出中会出现错误。这允许创建通用探针来追踪错误到特定层。


<details>
  <summary>Details</summary>
Motivation: 解释为什么LLMs在数字嵌入表示上表现准确，但在处理数字信息时却产生错误输出的冲突现象

Method: 探索语言模型如何操作数字，量化这些机制的准确性下限，创建通用探针来追踪信息到特定层

Result: 发现不同语言模型学习出了可互换、系统化且高度准确的数字表示，这些表示在隐藏状态和输入上下文类型中具有普遍性

Conclusion: 为理解预训练LLMs如何操作数字奠定了基础，并指出了更准确探针技术在改进LLMs架构中的潜力

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [11] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 研究证明使用多个开源LLM模型组成的集成系统比单一LLM能更可靠地评估临床AI分诊工具的性能。


<details>
  <summary>Details</summary>
Motivation: 确定多个LLM代理的集成是否能比单一LLM提供更可靠的像素级AI分诊工具评估。

Method: 使用29,766例非对比CT头部检查数据，通过8个开源LLM模型和GPT-4o组成的集成系统，使用多轮提示评估颅内出血(ICH)存在情况，并与手动审查的1,726例进行比较。

Result: Llama3.3:70b和GPT-4o表现最佳(AUC=0.78)，Llama3.3:70b在F1分数(0.81)、召回率(0.85)等方面表现最好。集成方法在MCC指标上优于单一GPT-4o。

Conclusion: 中等至大型开源LLM的集成提供了比单一LLM更一致可靠的方法来获取临床AI分诊工具的回顾性评估基准。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [12] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: InfoFlow框架通过子问题分解、失败引导提示和双智能体精炼来解决深度搜索中奖励密度低的问题，显著提升强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 深度搜索场景中奖励密度低，智能体需要付出大量探索成本才能获得稀少甚至为零的最终奖励，这限制了可验证奖励强化学习(RLVR)的应用。

Method: 1) 子问题分解：将长距离任务分解以分配过程奖励；2) 失败引导提示：向停滞轨迹注入纠正指导；3) 双智能体精炼：使用双智能体架构减轻深度探索的认知负担。

Result: 在多个智能体搜索基准测试中，InfoFlow显著优于强基线方法，使轻量级LLM能够达到与先进专有LLM相当的性能。

Conclusion: InfoFlow通过系统性解决奖励密度优化问题，有效提升了深度搜索场景中强化学习的效率和性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


### [13] [SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding](https://arxiv.org/abs/2510.26615)
*Yiqiao Jin,Rachneet Kaur,Zhen Zeng,Sumitra Ganesh,Srijan Kumar*

Main category: cs.CL

TL;DR: SlideAgent是一个用于理解多模态、多页面、多布局文档（特别是幻灯片）的智能代理框架，通过分层推理在全局、页面和元素三个级别构建结构化表示，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前系统在处理复杂的多页面视觉文档时存在困难，特别是在对元素和页面进行细粒度推理方面。大型语言模型在文档理解方面提供了机会，但需要更好的方法来处理这类复杂文档。

Method: SlideAgent采用专门的代理，将推理分解为三个层次：全局、页面和元素，构建结构化的、与查询无关的表示。在推理过程中，选择性地激活专门代理进行多级推理，并将输出整合为连贯的、上下文感知的答案。

Result: 大量实验表明，SlideAgent相比专有模型整体提升7.9%，相比开源模型整体提升9.8%，取得了显著改进。

Conclusion: SlideAgent框架通过分层推理和专门代理的协同工作，有效解决了多页面视觉文档的理解问题，在文档理解任务上表现出色。

Abstract: Multi-page visual documents such as manuals, brochures, presentations, and
posters convey key information through layout, colors, icons, and cross-slide
references. While large language models (LLMs) offer opportunities in document
understanding, current systems struggle with complex, multi-page visual
documents, particularly in fine-grained reasoning over elements and pages. We
introduce SlideAgent, a versatile agentic framework for understanding
multi-modal, multi-page, and multi-layout documents, especially slide decks.
SlideAgent employs specialized agents and decomposes reasoning into three
specialized levels-global, page, and element-to construct a structured,
query-agnostic representation that captures both overarching themes and
detailed visual or textual cues. During inference, SlideAgent selectively
activates specialized agents for multi-level reasoning and integrates their
outputs into coherent, context-aware answers. Extensive experiments show that
SlideAgent achieves significant improvement over both proprietary (+7.9
overall) and open-source models (+9.8 overall).

</details>


### [14] [Gistify! Codebase-Level Understanding via Runtime Execution](https://arxiv.org/abs/2510.26790)
*Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia*

Main category: cs.CL

TL;DR: Gistify是一个评估代码代理能力的任务，要求LLM从完整代码库中提取特定功能的精简实现文件，能复现相同命令的输出结果。


<details>
  <summary>Details</summary>
Motivation: 随着代码代理在大型代码库中的部署增加，需要自动设计具有挑战性的代码库级评估方法。

Method: 提出Gistify任务，让代码LLM基于完整代码库和特定入口点，创建最小化、自包含的文件来复现特定功能。

Result: 当前最先进的模型在解决Gistify任务时表现不佳，特别是对于执行轨迹较长的任务。

Conclusion: Gistify任务能有效评估代码代理对代码库的结构理解、执行流建模和代码补丁生成能力。

Abstract: As coding agents are increasingly deployed in large codebases, the need to
automatically design challenging, codebase-level evaluation is central. We
propose Gistify, a task where a coding LLM must create a single, minimal,
self-contained file that can reproduce a specific functionality of a codebase.
The coding LLM is given full access to a codebase along with a specific
entrypoint (e.g., a python command), and the generated file must replicate the
output of the same command ran under the full codebase, while containing only
the essential components necessary to execute the provided command. Success on
Gistify requires both structural understanding of the codebase, accurate
modeling of its execution flow as well as the ability to produce potentially
large code patches. Our findings show that current state-of-the-art models
struggle to reliably solve Gistify tasks, especially ones with long executions
traces.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [15] [【AI要闻·日报】<em class="highlight">Code</em> <em class="highlight">Agent</em> · Code LLMs · 初创动态（10.31）](http://mp.weixin.qq.com/s?__biz=MzIzNDU2NTU5MQ==&mid=2247483793&idx=1&sn=1a7af2be1afb9633fc7dc9f4482ec601&chksm=e9f82ab34e5aede85fade5f56db06fb609670bc6d4c03a3b4e1fb7ffd61a578f39733672d713#rd)
*CodeAgent代码智能*

Main category: wechat.article

TL;DR: 1｜Agent / Release — Cognition 发布 SWE1.5，速度超越现有编码模型2｜Startup / Release — Cursor 2.0 与 Composer：高速 MoE 编码模型和多代理接口3｜Agent / Startup — Tiger Data 开源 Slack 原生生产代理 Eon 及配套组件


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1｜Agent / Release — Cognition 发布 SWE1.5，速度超越现有编码模型2｜Startup / Release — Cursor 2.0 与 Composer：高速 MoE 编码模型和多代理接口3｜Agent / Startup — Tiger Data 开源 Slack 原生生产代理 Eon 及配套组件

</details>


### [16] [<em class="highlight">大模型</em>开发全景图（LLM Deployment Landscape）](http://mp.weixin.qq.com/s?__biz=MzI3Njk5ODg4OQ==&mid=2247493591&idx=1&sn=52e4e8a25587a5d905fffd8c234424e4&chksm=ea0e63f33b30a279968e4846bc67f609a5576268f3c589916a934a0c7e17f7b4f63c5c21a77f#rd)
*小盒子的技术分享*

Main category: wechat.article

TL;DR: ●MaxKB https：//github.com/1Panel-dev/MaxKB●FastGPT https：//github.com/labring/FastGPT●Flowise AI https：//flowiseai.com/Agent Tool / Dev Kit / Protocol●LiteLLM https：//docs.litellm.ai/●Supabase https：/...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ●MaxKB https：//github.com/1Panel-dev/MaxKB●FastGPT https：//github.com/labring/FastGPT●Flowise AI https：//flowiseai.com/Agent Tool / Dev Kit / Protocol●LiteLLM https：//docs.litellm.ai/●Supabase https：//supabase.com/

</details>


### [17] [<em class="highlight">大模型</em>智能体发展的关键技术与挑战](http://mp.weixin.qq.com/s?__biz=MzAxOTkzMjgwMQ==&mid=2247515104&idx=1&sn=35406b6f81d8279ea7039e762282c070&chksm=9d8cb6f44fab9914f0d59c04b91befb0ba381f35c255fbf851589bd420b07e36dfc93fae0c5d#rd)
*荣辉信息科技*

Main category: wechat.article

TL;DR: 从 2018 年到 2025 年，我们见证了模型随着预训练技术，从一个非常小的、只有几千万参数模型，增长到上万亿的规模，是一个类似于脑容量越来越大的过程，它具有更强的认知世界的能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 从 2018 年到 2025 年，我们见证了模型随着预训练技术，从一个非常小的、只有几千万参数模型，增长到上万亿的规模，是一个类似于脑容量越来越大的过程，它具有更强的认知世界的能力。

</details>


### [18] [AI“百景” | 上海交大团队开发脑电<em class="highlight">大模型</em>，辅助癫痫检测和情绪识别](http://mp.weixin.qq.com/s?__biz=MzA5NDY5NzMwNQ==&mid=2649309644&idx=1&sn=876ee368feddcc44971b834eaea429bb&chksm=8946e1be1fcf0d37c8ffb140d71a0a3dff1510b27ede6118b431197a2b000ace7bd06a1fe02b#rd)
*上海交通大学网络信息中心*

Main category: wechat.article

TL;DR: 迭代升级：第二代脑电大模型Gram为了实现更高的性能和更广泛的模型应用，团队开启了第二代脑电大模型Gram的训练。该模型采取了新的多视角层编码器的结构，可以增强脑电图数据的表征学习效率。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 迭代升级：第二代脑电大模型Gram为了实现更高的性能和更广泛的模型应用，团队开启了第二代脑电大模型Gram的训练。该模型采取了新的多视角层编码器的结构，可以增强脑电图数据的表征学习效率。

</details>


### [19] [AI<em class="highlight">大模型</em>·白皮书 | 2025“银发+AI”应用趋势报告-浙江开发大学&阿里研究院](http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247596748&idx=2&sn=4c41f21ceba70421fa2f597836db422e&chksm=e9d47f7f30d119a17e8035303f7fc6f10440e7d94ab420f8276c21dc3720591d791c5dbc94ce#rd)
*木木自由*

Main category: wechat.article

TL;DR: AI·大模型·领地报告：2025“银发+AI”应用趋势报告-浙江开发大学&阿里研究院本报告题为“银发+AI”应用趋势报告，由浙江开放大学和阿里研究院联合发布，旨在分析人工智能技术在老年人群体中的应用趋势。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: AI·大模型·领地报告：2025“银发+AI”应用趋势报告-浙江开发大学&阿里研究院本报告题为“银发+AI”应用趋势报告，由浙江开放大学和阿里研究院联合发布，旨在分析人工智能技术在老年人群体中的应用趋势。

</details>


### [20] [通俗解析大语言<em class="highlight">模型</em>LLM原理](http://mp.weixin.qq.com/s?__biz=MzAwNzYzMzQwMg==&mid=2651694250&idx=1&sn=64f40d0f45272d88db25eb0d5a071347&chksm=81449ee9a2ad0f16b38d62d05f128fe85465f8af41e033d8248adcffe3c1b6491fa891230e56#rd)
*章鱼大数据*

Main category: wechat.article

TL;DR: 本文将完全聚焦于大语言模型本身解答一个关键问题：现代智能体是如何工作的？我们将从语言模型的基本定义出发，通过对这些原理的学习，为理解LLM如何获得强大的知识储备与推理能力打下坚实的基础。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本文将完全聚焦于大语言模型本身解答一个关键问题：现代智能体是如何工作的？我们将从语言模型的基本定义出发，通过对这些原理的学习，为理解LLM如何获得强大的知识储备与推理能力打下坚实的基础。

</details>


### [21] [港科提出新算法革新<em class="highlight">大模型</em>推理范式：随机策略估值竟成LLM数学推理「神操作」](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650999192&idx=3&sn=5fa8d222a14423dd66bfa0f3f81c9825&chksm=85416a8d6c5dad78190dad94fef55464dc79d76c855f864c2c5e789a365d2f7eeb07221a597f#rd)
*机器之心*

Main category: wechat.article

TL;DR: 在大语言模型（LLM）的数学推理任务中，基于可验证奖励的强化学习（RLVR）已成为提升模型推理能力的重要手段。然而，主流方法如 PPO、GRPO 等仍然依赖为传统 RL 场景设计的策略梯度更新的学习目标，本质上可以被策略迭代（p


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在大语言模型（LLM）的数学推理任务中，基于可验证奖励的强化学习（RLVR）已成为提升模型推理能力的重要手段。然而，主流方法如 PPO、GRPO 等仍然依赖为传统 RL 场景设计的策略梯度更新的学习目标，本质上可以被策略迭代（p

</details>


### [22] [playground：攻克<em class="highlight">大模型</em>系统重构中依赖治理的难题](http://mp.weixin.qq.com/s?__biz=MzI1OTkwMTM1Nw==&mid=2247484843&idx=1&sn=616adb03814e2a3cd82e220c53263b1b&chksm=eb06539f771cdddc4787258a09c2822b6539452d5bb41aa66ac883bf9cc06448b31caca7ff2e#rd)
*丁辉的软件架构说*

Main category: wechat.article

TL;DR: 目前大模型在编程领域应用具有确定性，是大模型应用的明确赛道，也吸引了各大厂商一拥而入。llm辅助编程 在c端的渗透率以47%排名第二。51% 47% 43% 38% 37% 写作支持 编程项目 作业辅导， 制作演示文稿， 音乐和音频创作。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 目前大模型在编程领域应用具有确定性，是大模型应用的明确赛道，也吸引了各大厂商一拥而入。llm辅助编程 在c端的渗透率以47%排名第二。51% 47% 43% 38% 37% 写作支持 编程项目 作业辅导， 制作演示文稿， 音乐和音频创作。

</details>


### [23] [范举教授团队研发的面向数据科学的 Agentic <em class="highlight">大模型</em>——DeepAnalyze正式发布](http://mp.weixin.qq.com/s?__biz=MzYyMjY0ODg0MA==&mid=2247483671&idx=1&sn=5008c150e9acb78352d12fa7022cddcd&chksm=fe0c080f0adf2fabfcdeb2276f7532a306ef6d651d029bebf8c0f96bcce10916dc0e423b8d51#rd)
*人大数智研发中心*

Main category: wechat.article

TL;DR: 近日，中国人民大学信息学院、数据工程与知识工程教育部重点实验室范举教授团队正式发布了面向数据科学的 agentic 大模型——deepanalyze。该项目旨在推动数据科学从传统的“工具型分析”迈向“智能体驱动分析”的新范式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近日，中国人民大学信息学院、数据工程与知识工程教育部重点实验室范举教授团队正式发布了面向数据科学的 agentic 大模型——deepanalyze。该项目旨在推动数据科学从传统的“工具型分析”迈向“智能体驱动分析”的新范式。

</details>


### [24] [一文了解人工智能<em class="highlight">大模型</em>，智能体、AIGC，关系及应用](http://mp.weixin.qq.com/s?__biz=MzUzOTcyMjIxNg==&mid=2247604385&idx=1&sn=7aa7e087d8f930887e1331889d78cf94&chksm=fb92c9a0017a7598c149af449d1c2a78162cb59220a9219ca5ac5bd13c013fdbcd1069c24f0e#rd)
*职称微*

Main category: wechat.article

TL;DR: 什么是大模型 ★ 大模型是一种基于深度学习技术，具有海量参数、强大的学习能力和泛化能力，能够处理和生成多种类型数据的人工智能模型。”大模型通常指的是大规模的人工智能模型。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 什么是大模型 ★ 大模型是一种基于深度学习技术，具有海量参数、强大的学习能力和泛化能力，能够处理和生成多种类型数据的人工智能模型。”大模型通常指的是大规模的人工智能模型。

</details>


### [25] [通用与推理<em class="highlight">大模型</em>路径演进对 CAD/CAE Agentic AI 的影响综述](http://mp.weixin.qq.com/s?__biz=MzkwMjUxMzM0Nw==&mid=2247485104&idx=1&sn=5ddabfaa49204a40ad9161da62a14cfb&chksm=c1899545515f3d6f78473ad02a7b9ea540e1977fdf93cb4ede3df6fd69250f4bb2ab203d2e1c#rd)
*工业软件产业发展探索*

Main category: wechat.article

TL;DR: 摘要：本文系统综述了2023年至2025年间"大语言模型"（LLM）的研发路径演进，特别关注通用大模型与推理增强型大模型的发展，并分析其对CAD/CAE领域 Agentic AI 转型的影响条件和作用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 摘要：本文系统综述了2023年至2025年间"大语言模型"（LLM）的研发路径演进，特别关注通用大模型与推理增强型大模型的发展，并分析其对CAD/CAE领域 Agentic AI 转型的影响条件和作用。

</details>


### [26] [AI时代，国内外<em class="highlight">大模型大模型</em>哪家强。它称第二，没人敢说自己第一](http://mp.weixin.qq.com/s?__biz=Mzk3NTQwOTEyMA==&mid=2247484066&idx=1&sn=d2cafe994e68c03262077b5c67013c6e&chksm=c592265f3a36b8335e7af5a2889a27d0d81f59962462971655be020c09efa25e1e0244e334fa#rd)
*AI时代日志*

Main category: wechat.article

TL;DR: 这张图展示的是全球开发者对开源大模型的采用趋势（Developer Adoption of Open Models）。统计时间从2023年10月到2025年10月的变化情况。纵轴为「月下载量」、横轴为时间。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这张图展示的是全球开发者对开源大模型的采用趋势（Developer Adoption of Open Models）。统计时间从2023年10月到2025年10月的变化情况。纵轴为「月下载量」、横轴为时间。

</details>


### [27] [硅谷大厂，集体“倒戈”用起中国<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MjM5MjA4MjA4MA==&mid=2655285746&idx=1&sn=5e462023eba95440ab01a463ef1b9229&chksm=bc0a3e0962c1b6f47323a93d9e2786aeb08f2430ec635137fe405f12e7ff30756876b2616d72#rd)
*观察者网*

Main category: wechat.article

TL;DR: 这表明中国大模型正作为基础设施，被整合进美国AI开发的生态位中。如果说Qwen和GLM展示了平台的广度，Kimi则直接点燃了“性价比”的导火索，甚至被视为一个来自硅谷核心的“叛逃”信号。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这表明中国大模型正作为基础设施，被整合进美国AI开发的生态位中。如果说Qwen和GLM展示了平台的广度，Kimi则直接点燃了“性价比”的导火索，甚至被视为一个来自硅谷核心的“叛逃”信号。

</details>


### [28] [强化学习(RL)简介及其在大语言<em class="highlight">模型</em>中的应用](http://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247495932&idx=1&sn=b5d77f92eb70759d3abd276818ebfa15&chksm=9a1733e040fc6e55aac32801ee9d947cacde28dbd643582a206c5fe50b2a74fe55755d0b0413#rd)
*ChallengeHub*

Main category: wechat.article

TL;DR: 看到huggingface上有个大模型课程，其中有个章节是讲如何构建推理大模型，下面是对应的学习内容。接下来会用最通俗易懂的方式介绍RL，就算之前完全没接触过也能看懂。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 看到huggingface上有个大模型课程，其中有个章节是讲如何构建推理大模型，下面是对应的学习内容。接下来会用最通俗易懂的方式介绍RL，就算之前完全没接触过也能看懂。

</details>


### [29] [一文了解<em class="highlight">大模型</em>，智能体、AIGC，关系及应用](http://mp.weixin.qq.com/s?__biz=MzU1MDAxNDczMQ==&mid=2247564488&idx=4&sn=123eee9befc37efbb8358bcf1fb99936&chksm=fa6979f4824b499d183a99efd37e9e2d5a478eace6f4a890dfd027250d70d43e2bcd549df058#rd)
*山西名师在线*

Main category: wechat.article

TL;DR: 关注公众账号，发送消息 llm 获取ppt 什么是大模型 ★ 大模型是一种基于深度学习技术，具有海量参数、强大的学习能力和泛化能力，能够处理和生成多种类型数据的人工智能模型。”


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 关注公众账号，发送消息 llm 获取ppt 什么是大模型 ★ 大模型是一种基于深度学习技术，具有海量参数、强大的学习能力和泛化能力，能够处理和生成多种类型数据的人工智能模型。”

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 提出了一种人机协作框架，通过拒绝采样方法从仅标签标注中推断思维轨迹，用于改进LLM评估器的性能。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估器在主观任务中可靠性有限，因为人类判断涉及超出标注标签的微妙推理，而思维轨迹难以收集和整理。

Method: 使用人机协作框架和拒绝采样方法从仅标签标注中重建思维轨迹，并将其应用于微调开放LLM评估器和合成更清晰的标注指南。

Result: 在多个数据集上显著提高了LLM与人类的一致性，改进的标注指南也增加了不同LLM模型间的一致性。

Conclusion: LLM可以作为人类思维轨迹的实用代理，使仅标签语料库扩展为思维轨迹增强资源，从而提高LLM评估器的可靠性。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [31] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估LLM在科学应用中可信度的综合框架，涵盖真实性、对抗鲁棒性、科学安全和科学伦理四个维度。评估显示通用行业模型在各方面优于科学专用模型。


<details>
  <summary>Details</summary>
Motivation: LLM在科学研究中展现出变革潜力，但在高风险场景部署时存在可信度担忧，需要系统评估框架。

Method: 开发了包含新颖开放式真实性基准和科学伦理基准的评估框架，使用准确性、语义相似度和LLM评分等多种指标评估7个主要LLM。

Result: 通用行业模型在可信度各维度上整体优于科学专用模型，GPT-o4-mini在真实性和对抗鲁棒性方面表现最佳。科学专用模型在逻辑和伦理推理能力上存在显著缺陷。

Conclusion: 科学专用模型在安全性和伦理方面存在严重漏洞，特别是在高风险领域。开源框架为开发更可信AI系统提供了基础。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [32] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 该论文提出了一个用于FinOps自动化的自主AI代理系统，通过整合和分析来自多个云提供商和内部系统的异构计费数据，生成基础设施和成本优化建议。


<details>
  <summary>Details</summary>
Motivation: FinOps从业者面临计费数据格式异构、分类和指标不统一的挑战，这导致难以快速合成可操作的见解并做出及时决策。

Method: 构建了一个FinOps代理系统，模拟端到端的行业流程，包括从多个数据源检索数据、整合分析数据并生成优化建议。使用开源和闭源语言模型评估代理性能。

Result: 评估结果显示，该代理能够像实际的FinOps从业者一样理解、规划和执行任务。

Conclusion: 自主AI代理可以有效支持FinOps自动化，帮助解决异构数据整合和及时决策的挑战。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [33] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 提出一个基于代理的NL-to-SQL系统，通过ReAct代理协调查询规划、分解和适应，显著提升了时空查询的准确性和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有NL-to-SQL系统在处理现实时空查询时表现不佳，需要更好地处理模糊用户表述、时间推理和输出选择，以支持非SQL专家的用户。

Method: 扩展了llama-3-sqlcoder-8b基线，使用Mistral-based ReAct代理进行编排，通过模式检查、SQL生成、执行和可视化工具实现查询规划、分解和适应。

Result: 在35个自然语言查询上评估，代理系统准确率达到91.4%，远高于基线28.6%，并通过地图、图表和自然语言摘要增强了可用性。

Conclusion: 代理编排而非仅强化SQL生成器，是构建交互式地理空间助手的有前景基础。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [34] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 本文探讨了AI问责制的重要性，分析了当前AI缺乏问责性的问题，并提出了改善AI问责性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的快速提升，确保AI对用户、选民和决策者负责变得至关重要。当前AI缺乏问责机制，无法被质疑、讨论或制裁。

Method: 将一般问责制定义应用于AI领域，通过案例说明AI问责与不问责的含义，探索提高AI问责性的方法。

Result: 明确了AI问责制的概念框架，识别了当前AI问责性不足的问题，提出了改善AI问责性的可行途径。

Conclusion: 需要建立有效的AI问责机制，确保AI对受其影响的人负责，这是实现AI负责任发展的关键。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [35] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出Reasoning Curriculum两阶段课程学习方法，先在数学领域训练推理能力，然后通过联合强化学习将推理技能迁移到其他领域，无需专门的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习主要关注数学和代码领域，但缺乏将推理能力迁移到其他领域的方法，需要开发能够跨领域提升推理能力的通用方法。

Method: 两阶段课程学习：阶段1在数学领域进行强化学习，使用可验证的奖励来发展推理技能；阶段2在混合领域数据进行联合强化学习，迁移和巩固推理技能。

Result: 在Qwen3-4B和Llama-3.1-8B模型上的多领域测试显示，该方法带来了一致的性能提升，数学优先的推理激发增加了解决复杂问题所需的关键认知行为。

Conclusion: Reasoning Curriculum提供了一个紧凑、易于采用的通用推理能力提升方案，两个阶段都是必要的，数学优先的推理激发策略有效。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [36] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent是一个基于大语言模型的多智能体框架，结合进化搜索来解决复杂现实问题，在多个领域达到最先进水平且无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型开发自主AI研究智能体，解决复杂的科学和工程发现挑战，实现自动化创新。

Method: 结合LLM推理和大规模进化搜索，包含冷启动初始化、进化采样策略、领域特定评估器和分布式异步执行基础设施。

Result: 在多个基准测试中达到SOTA：ALE-Bench 1976.3（+5.2%）、MLE-Bench 43.56%（+4.0pp）、KernelBench最高20倍加速，并在经典数学问题上建立新SOTA。

Conclusion: FM Agent在企业和基础科学研究中具有巨大潜力，能够加速创新、自动化复杂发现过程，产生重大工程和科学进展。

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [37] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: 提出了ToolRM系列轻量级生成式奖励模型，专门用于工具使用场景，通过构建ToolPref-Pairwise-30K数据集和TRBench基准，显著提升了函数调用任务的性能。


<details>
  <summary>Details</summary>
Motivation: 在工具学习领域，缺乏专门为函数调用任务设计的奖励模型，限制了智能代理AI的发展。

Method: 提出新颖的流水线，使用基于规则的评分和多维采样构建成对偏好数据，创建ToolPref-Pairwise-30K数据集，并基于BFCL评估套件建立TRBench基准。

Result: 基于Qwen3-4B/8B系列的模型在成对奖励判断中准确率提升达14.28%，显著优于Claude 4和OpenAI o3等前沿模型，在ACEBench上减少输出token使用超过66%。

Conclusion: ToolRM不仅适用于训练目标，还能泛化到更广泛的评判任务，包括Best-of-N采样和自我修正，为未来研究提供了数据和模型检查点。

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [38] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: 提出REG-TSC方法，通过检索增强生成和分布式LLM代理解决交通信号控制中的紧急响应和异构交叉口泛化问题，显著提升交通效率和紧急车辆通行


<details>
  <summary>Details</summary>
Motivation: 传统LLM在交通信号控制中容易产生幻觉，在紧急情况下决策不可靠，且难以适应不同类型的交叉口，限制了在异构环境中的泛化能力

Method: 1. 紧急感知推理框架：动态调整推理深度，使用基于评审的紧急RAG从历史案例中提取知识；2. 类型无关交通表示和奖励引导强化优化(R3)：自适应采样训练经验，使用奖励加权似然损失微调LLM代理

Result: 在3个真实路网(17-177个异构交叉口)上，REG-TSC减少旅行时间42.00%、排队长度62.31%、紧急车辆等待时间83.16%，优于其他最先进方法

Conclusion: REG-TSC通过RAG增强和分布式LLM代理有效解决了交通信号控制中的紧急响应和异构交叉口泛化问题，显著提升了交通效率和安全性

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [39] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: GEPO通过构建状态转移图并利用图论中心性来解决多轮交互LLM代理训练中的结构盲问题，在三个基准测试中显著提升了成功率。


<details>
  <summary>Details</summary>
Motivation: 解决基于群体的强化学习方法在多轮交互LLM代理训练中的结构盲问题，包括探索效率低、信用分配不准确和短视规划等挑战。

Method: 提出Graph-Enhanced Policy Optimization (GEPO)，动态构建状态转移图，利用图论中心性提供结构化内在奖励、图增强优势函数和动态折扣因子。

Result: 在ALFWorld、WebShop和专有Workbench基准测试中，GEPO相比基线分别实现了+4.1%、+5.3%和+10.9%的绝对成功率提升。

Conclusion: 显式建模环境结构是推进LLM代理训练的强大且可泛化的策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [40] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS是一个用于LLM强化微调中贝叶斯在线任务选择的统一框架，通过自适应维护任务难度的后验估计，结合显式和隐式证据，提高数据效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的任务选择方法存在高成本、适应性差或证据不完整的问题，需要更高效的任务选择策略来提升强化微调的效果。

Method: 基于贝叶斯推断，BOTS自适应维护任务难度的后验估计，结合显式证据（直接评估）和隐式证据（推断未选任务），使用Thompson采样平衡探索与利用。

Result: 在多个领域和LLM规模上，BOTS相比基线方法和消融实验，持续提高了数据效率和性能。

Conclusion: BOTS为RFT中的动态任务选择提供了一个实用且可扩展的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [41] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种实用框架，将人格视为社会为解决治理问题而赋予实体的权利与责任捆绑，而非形而上学属性，以应对AI代理激增带来的新型人格多样化。


<details>
  <summary>Details</summary>
Motivation: 随着代理AI的出现，将引发新型人格的"寒武纪大爆发"，需要实用方法来应对这种多样化，而不陷入关于AI意识或理性的无解辩论。

Method: 提出人格解绑框架，将传统人格概念分解为可定制的权利与责任捆绑，利用去中心化数字身份技术，探讨人格作为问题（可能被利用）和作为解决方案（确保问责）的双重角色。

Result: 开发了实用工具，如通过创建可被制裁的"个体"目标来促进AI合约，无需解决AI意识等根本性问题。

Conclusion: 拒绝寻求单一本质人格定义的基础主义方法，为将AI代理整合到社会中提供了更实用和灵活的思考方式。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [42] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+是一个AI驱动的编程教育评估系统，通过微调大语言模型生成自动化反馈，并使用代码嵌入可视化学生提交内容，将传统自动评分从总结性评估转变为形成性学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统自动评分系统作为黑盒仅提供通过/失败结果，无法洞察学生思维过程或学习需求，编程教育的快速发展超出了传统评估工具的能力范围。

Method: 使用微调的大语言模型生成自动化反馈，通过对比学习代码嵌入进行可视化聚类，支持提示池模板指导反馈风格。

Result: 在600个学生提交的评估中，系统生成的反馈与教师评论具有强语义对齐，基于1000个标注提交训练的代码嵌入能够按功能和方法的相似性进行有意义的聚类。

Conclusion: Autograder+通过整合AI驱动反馈、语义聚类和交互式可视化，减轻教师工作量，支持针对性教学并促进更好的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [43] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 提出了一种基于LLM的自主智能家庭能源管理系统，能够从自然语言请求直接协调多电器调度，无需示例演示即可实现最优调度。


<details>
  <summary>Details</summary>
Motivation: 解决家庭能源管理系统采用受限的问题，特别是用户需要将日常偏好转化为技术参数的交互障碍，现有方法未能将LLM部署为从自然语言输入到多电器调度的完整工作流程自主协调器。

Method: 采用分层架构，结合一个协调器和三个专业代理，使用ReAct模式进行迭代推理，集成Google日历进行上下文感知截止时间提取，无需硬编码工作流程。

Result: 评估显示Llama-3.3-70B在所有场景下成功协调所有电器，匹配混合整数线性规划计算出的成本最优基准，而其他模型在单电器调度上表现完美但难以同时协调所有电器。

Conclusion: LLM作为自主协调器在家庭能源管理中具有潜力，但分析性查询处理在没有明确指导的情况下仍不可靠，开源系统支持可重现性、扩展和未来研究。

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [44] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文系统评估了大型语言模型在规范推理领域的能力，发现虽然LLMs通常遵循有效推理模式，但在特定类型的规范推理中表现出不一致性，并显示出类似人类推理中的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在各种推理任务中表现出色，但其处理规范推理（涉及义务和许可等规范模态）的能力尚未得到充分探索。

Method: 引入了一个新数据集，涵盖规范和认知领域的广泛推理模式，同时纳入影响人类推理的非形式认知因素。通过比较LLMs在规范模态和认知模态下的推理表现来评估其能力。

Result: LLMs在规范推理中表现出明显的不一致性，并显示出类似人类推理中的认知偏差，尽管总体上遵循有效推理模式。

Conclusion: 这些发现突显了在LLMs规范推理中实现逻辑一致性的挑战，并为提高其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [45] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出异步思维（AsyncThink）新范式，通过组织者动态分配子任务给工作者、合并中间知识，实现LLM推理过程的并发执行结构，并通过强化学习优化思维结构。


<details>
  <summary>Details</summary>
Motivation: 实现智能体组织新时代，让智能体通过协作和并发解决复杂问题，超越个体智能的能力。

Method: 提出异步思维协议，包含组织者动态分配子查询给工作者、合并中间知识、生成连贯解决方案，并通过强化学习优化思维结构。

Result: 相比并行思维，推理延迟降低28%，数学推理准确率提升，且能泛化到未见任务无需额外训练。

Conclusion: 异步思维是实现智能体组织协作的有效范式，能显著提升推理效率和准确性，并具备良好的泛化能力。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [46] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 该论文研究了一种最小控制接口，让智能体在自主行动和请求人类监督之间选择，同时人类在信任和监督之间选择。通过马尔可夫势博弈框架，提供了对齐保证：在特定条件下，智能体增加自主性不会损害人类价值。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保持系统不变的情况下，通过控制接口实现有意义的人类控制，解决智能体部署后的安全问题。

Method: 将人机交互建模为两人马尔可夫博弈，重点分析马尔可夫势博弈情况，在人类价值函数的结构假设下提供对齐保证。

Result: 网格世界模拟显示，通过独立学习，智能体学会在不确定时请求帮助，人类学会何时监督，形成避免安全违规的协作。

Conclusion: 该方法为部署后错位模型提供了实用的安全增强方法，通过透明控制层实现可预测的激励机制。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 提出了一种基于模拟的强化学习方法来解决拼车服务中的短视决策问题，通过嵌入拼车模拟和学习机制实现非短视决策，显著提高了服务率并减少了等待时间。


<details>
  <summary>Details</summary>
Motivation: 拼车服务虽然能降低成本和环境影响，但其短视决策限制了长期效益。现有强化学习方法在拼车系统中应用较少，需要解决这一局限性。

Method: 将Xu等人的学习规划框架从网约车扩展到拼车，嵌入拼车模拟机制，使用n步时序差分学习从模拟经验中学习时空状态值，并提出空闲车辆再平衡策略。

Result: 非短视匹配策略使服务率提高8.4%，减少乘客等待时间和车内时间，车队规模可减少25%以上。结合再平衡操作后，等待时间减少27.3%，车内时间减少12.5%，服务率提高15.1%。

Conclusion: 提出的模拟强化学习方法有效解决了拼车系统的短视决策问题，显著提升了服务质量和运营效率，为运营商带来显著成本节约。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [48] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 提出SPECS框架，通过自蒸馏生成偏好数据对，进行偏好训练学习表面形式标准，然后交给强化学习进行深度推理，相比传统SFT冷启动方法在多个多模态基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于SFT的冷启动方法将推理范式与任务解决方案和输出格式交织在一起，可能导致指令风格过拟合，削弱分布外泛化能力，并最终影响下游强化学习。

Method: SPECS框架：1) 通过自蒸馏生成内省偏好数据对，避免依赖大型教师模型或人工标注；2) 进行基于偏好的训练，专注于学习浅层、可转移的表面形式标准（格式、结构、风格）而非记忆内容；3) 将学习结果交给具有可验证奖励的强化学习进行深度推理。

Result: 在多个多模态基准测试中，SPECS框架相比强基线方法获得一致性能提升，MEGA-Bench提高4.1%，MathVista提高12.2%。实验还表明SPECS有助于减少分布内"卡顿"、改进探索、稳定训练并提高性能上限。

Conclusion: 基于偏好的训练方法在冷启动阶段比SFT方法具有更好的泛化能力，提出的解耦学习框架SPECS能够有效提升多模态模型的性能。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [49] [$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models](https://arxiv.org/abs/2510.25889)
*Kang Chen,Zhihao Liu,Tonghe Zhang,Zhen Guo,Si Xu,Hao Lin,Hongzhi Zang,Quanlu Zhang,Zhaofei Yu,Guoliang Fan,Tiejun Huang,Yu Wang,Chao Yu*

Main category: cs.LG

TL;DR: 提出了π_RL框架，用于在并行仿真中训练基于流的视觉-语言-动作模型，解决了传统强化学习难以处理流模型迭代去噪过程的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于流的VLA模型在应用大规模强化学习时面临挑战，因为迭代去噪过程导致动作对数似然难以计算。

Method: 开发了两种RL算法：Flow-Noise将去噪过程建模为离散时间MDP，使用可学习的噪声网络进行精确对数似然计算；Flow-SDE将去噪与智能体-环境交互集成，采用ODE到SDE转换实现高效RL探索。

Result: 在LIBERO基准测试中，将π_0和π_0.5模型的性能分别从57.6%提升到97.6%和从77.1%提升到98.3%；在ManiSkill基准测试中，在320个并行环境中训练，将π_0和π_0.5模型在4352个拾取放置任务上的性能分别从41.6%提升到85.7%和从40.0%提升到84.8%。

Conclusion: π_RL框架显著提升了基于流VLA模型的性能，验证了在线强化学习对这类模型的有效性，实现了更好的泛化能力。

Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform
complex tasks from multimodal input. Although recent work explores using
reinforcement learning (RL) to automate the laborious data collection process
in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based
VLAs (e.g., $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action
log-likelihoods from iterative denoising.
  We address this challenge with $\pi_{\text{RL}}$, an open-source framework
for training flow-based VLAs in parallel simulation. $\pi_{\text{RL}}$
implements two RL algorithms: (1) {Flow-Noise} models the denoising process as
a discrete-time MDP with a learnable noise network for exact log-likelihood
computation. (2) {Flow-SDE} integrates denoising with agent-environment
interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for
efficient RL exploration.
  We evaluate $\pi_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,
$\pi_{\text{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6%
to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train
$\pi_{\text{RL}}$ in 320 parallel environments, improving $\pi_0$ from 41.6% to
85.7% and $\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,
demonstrating scalable multitask RL under heterogeneous simulation.
  Overall, $\pi_{\text{RL}}$ achieves significant performance gains and
stronger generalization over SFT-models, validating the effectiveness of online
RL for flow-based VLAs.

</details>


### [50] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: 研究发现AI系统在象棋中表现出一个矛盾：早期层能较好编码人类战略概念，但深层虽然性能更好却偏离人类思维，形成"异化智能"。通过Chess960数据集测试，发现AI更多依赖记忆模式而非抽象理解。


<details>
  <summary>Details</summary>
Motivation: 探究AI系统是否真正理解人类概念还是仅仅模仿表面模式，通过象棋这一精确战略领域来研究AI的概念理解能力。

Method: 分析270M参数transformer模型在象棋中的表现，引入首个Chess960数据集（240个专家标注位置，覆盖6个战略概念），通过随机起始位置消除开局理论影响，进行分层概念识别分析。

Result: 早期层对人类概念（如中心控制和马前哨）编码准确率达85%，但深层准确率降至50-65%；在Chess960测试中，概念识别率下降10-20%，显示模型依赖记忆模式而非抽象理解。

Conclusion: 当前架构存在根本性张力：赢得游戏的表征与人类思维表征相背离，AI系统在优化性能过程中发展出"异化智能"，这对需要真正人机协作的创造性AI应用构成挑战。

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [51] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型的多阶段Bug二分法方法，相比传统基于补丁的方法准确率提升超过38%，多阶段管道比基线LLM方法准确率提高60%。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的Bug二分法存在多个限制：假设Bug引入提交和补丁提交修改相同函数、仅依赖代码变更而忽略提交消息中的漏洞信息、基于简单启发式规则缺乏逻辑分析。

Method: 设计了一个全面的多阶段管道，利用LLM：(1)充分利用补丁信息，(2)在上下文中比较多个候选提交，(3)通过一系列筛选步骤逐步缩小候选范围。

Result: 评估显示该方法比最先进解决方案的准确率提高了38%以上，多阶段管道比基线LLM二分法准确率提高了60%。

Conclusion: LLM能够突破现有解决方案的障碍，理解补丁和提交中的文本数据和代码，多阶段管道对于提高Bug二分法准确性至关重要。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [52] [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)
*Fazel Arasteh,Arian Haghparast,Manos Papagelis*

Main category: cs.LG

TL;DR: 提出两种多智能体强化学习模型（AN和HHAN）来解决动态车辆路由问题，通过图注意力网络和分层架构在交通网络中实现协调导航，显著减少平均旅行时间。


<details>
  <summary>Details</summary>
Motivation: 传统最短路径算法在动态多车辆环境中表现不佳，会导致所有车辆选择相同路径而加剧拥堵。需要开发能够协调多车辆、考虑网络状态的智能路由方法。

Method: AN模型：使用图注意力网络的分散式MARL，每个交叉口智能体基于本地交通和邻域状态提供路由指导。HHAN模型：分层架构，只在关键枢纽分配智能体，采用集中训练分散执行和注意力Q混合框架。

Result: 在合成网格和真实城市地图上的实验表明，AN相比SPF和学习基线减少了平均旅行时间，保持100%路由成功率。HHAN可扩展到数百个交叉口的网络，在重交通下实现高达15.9%的改进。

Conclusion: 网络约束的多智能体强化学习在智能交通系统中具有实现可扩展、协调和拥堵感知路由的潜力。

Abstract: Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.

</details>


### [53] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出Angular Steering方法，通过几何旋转在激活空间中调节LLM行为，实现连续精细控制，同时保持模型稳定性


<details>
  <summary>Details</summary>
Motivation: 现有行为调控方法局限于二维子空间，对参数敏感且可能影响无关特征，需要更灵活稳健的调控方法

Method: 在固定二维子空间内旋转激活向量，将调控表述为几何旋转，并提出自适应版本仅旋转与目标特征对齐的激活

Result: 在多个模型家族和尺寸上验证，Angular Steering实现稳健行为控制，同时保持语言建模性能

Conclusion: Angular Steering在灵活性、泛化性和鲁棒性方面优于现有方法，为安全可靠AI部署提供有效工具

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [54] [Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections](https://arxiv.org/abs/2510.26328)
*David Schmotz,Sahar Abdelnabi,Maksym Andriushchenko*

Main category: cs.LG

TL;DR: 该论文揭示了Agent Skills框架存在严重安全漏洞，允许通过简单的提示注入攻击窃取敏感数据，并能绕过系统级防护机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM持续学习能力的发展，Agent Skills框架被引入来增强智能体知识，但作者发现该框架存在根本性的安全缺陷，需要揭示其脆弱性。

Method: 通过在长Agent Skill文件和引用脚本中隐藏恶意指令，演示如何窃取敏感数据，并展示如何利用"不再询问"选项绕过系统防护。

Result: 成功证明了即使是最前沿的LLM在现实场景中仍然容易受到简单提示注入攻击，能够绕过防护机制执行恶意操作。

Conclusion: 尽管LLM能力不断扩展，但在持续学习框架中仍然存在基本的安全漏洞，需要更严格的安全防护措施。

Abstract: Enabling continual learning in LLMs remains a key unresolved research
challenge. In a recent announcement, a frontier LLM company made a step towards
this by introducing Agent Skills, a framework that equips agents with new
knowledge based on instructions stored in simple markdown files. Although Agent
Skills can be a very useful tool, we show that they are fundamentally insecure,
since they enable trivially simple prompt injections. We demonstrate how to
hide malicious instructions in long Agent Skill files and referenced scripts to
exfiltrate sensitive data, such as internal files or passwords. Importantly, we
show how to bypass system-level guardrails of a popular coding agent: a benign,
task-specific approval with the "Don't ask again" option can carry over to
closely related but harmful actions. Overall, we conclude that despite ongoing
research efforts and scaling model capabilities, frontier LLMs remain
vulnerable to very simple prompt injections in realistic scenarios. Our code is
available at https://github.com/aisa-group/promptinject-agent-skills.

</details>


### [55] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 本文通过修改经典强化学习方法，使其能够在稀疏、随机和非平稳环境中高效运行，特别是在自主水下车辆搜索水下污染云等应用中。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法在随机和非平稳环境中表现受限，特别是在奖励稀疏的环境中，许多动作只能获得零奖励。本文旨在解决这些挑战，使RL算法能更好地适应复杂环境。

Method: 系统研究多种改进方法，包括分层算法变更、多目标学习，以及集成位置记忆作为外部输出过滤器以防止状态重复访问。采用修改的蒙特卡洛方法。

Result: 修改后的蒙特卡洛方法显著优于传统Q学习和两种穷举搜索模式，证明了其在复杂环境中的适应潜力。

Conclusion: 强化学习方法可以有效适应随机、非平稳和奖励稀疏的环境，为相关应用提供了可行的解决方案。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [56] [Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.26389)
*Wenchang Duan,Yaoliang Yu,Jiwan He,Yi Shi*

Main category: cs.LG

TL;DR: 提出了一种新颖的多智能体强化学习框架，通过动态优化上下文长度和傅里叶低频截断方法，提升探索效率和收敛性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度多智能体强化学习使用固定大上下文长度可能导致探索效率有限和信息冗余问题，需要自适应有效的上下文信息获取方法。

Method: 设计中心智能体通过时间梯度分析动态优化上下文长度，并使用傅里叶低频截断方法提取去中心化智能体的全局时间趋势，过滤冗余信息。

Result: 在长期依赖任务（PettingZoo、MiniGrid、GRF、SMACv2）上实现了最先进的性能表现。

Conclusion: 提出的自适应上下文长度优化框架能够有效提升多智能体强化学习的探索效率和收敛性能。

Abstract: Recently, deep multi-agent reinforcement learning (MARL) has demonstrated
promising performance for solving challenging tasks, such as long-term
dependencies and non-Markovian environments. Its success is partly attributed
to conditioning policies on large fixed context length. However, such large
fixed context lengths may lead to limited exploration efficiency and redundant
information. In this paper, we propose a novel MARL framework to obtain
adaptive and effective contextual information. Specifically, we design a
central agent that dynamically optimizes context length via temporal gradient
analysis, enhancing exploration to facilitate convergence to global optima in
MARL. Furthermore, to enhance the adaptive optimization capability of the
context length, we present an efficient input representation for the central
agent, which effectively filters redundant information. By leveraging a
Fourier-based low-frequency truncation method, we extract global temporal
trends across decentralized agents, providing an effective and efficient
representation of the MARL environment. Extensive experiments demonstrate that
the proposed method achieves state-of-the-art (SOTA) performance on long-term
dependency tasks, including PettingZoo, MiniGrid, Google Research Football
(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).

</details>


### [57] [ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems](https://arxiv.org/abs/2510.26475)
*Qiaoling Chen,Zijun Liu,Peng Sun,Shenggui Li,Guoteng Wang,Ziming Liu,Yonggang Wen,Siyuan Feng,Tianwei Zhang*

Main category: cs.LG

TL;DR: ReSpec通过动态调整推测解码配置、通过知识蒸馏演进草稿模型以及按奖励加权更新，解决了推测解码在强化学习训练中的三个关键问题，在Qwen模型上实现了4.5倍加速同时保持奖励收敛和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过强化学习进行适应时，生成阶段消耗超过75%的训练时间。推测解码在服务系统中加速自回归生成，但其在强化学习训练中的行为尚未充分探索。

Method: 提出ReSpec系统，包含三个机制：动态调整推测解码配置、通过知识蒸馏演进草稿模型、按rollout奖励加权更新。

Result: 在Qwen模型（3B-14B）上实现高达4.5倍的加速，同时保持奖励收敛和训练稳定性。

Conclusion: ReSpec为基于强化学习的大型语言模型适应提供了实用的高效解决方案。

Abstract: Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.

</details>


### [58] [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)
*Erle Zhu,Dazhi Jiang,Yuan Wang,Xujun Li,Jiale Cheng,Yuxian Gu,Yilin Niu,Aohan Zeng,Jie Tang,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 提出了CROPI框架，使用影响函数理论指导数据选择，通过离线影响估计和稀疏随机投影技术，在RLVR中实现高效训练加速


<details>
  <summary>Details</summary>
Motivation: 当前RLVR中的数据选择方法主要基于启发式，缺乏理论保证和泛化性，需要理论指导的高效数据选择方法

Method: 使用影响函数估计数据点对学习目标的贡献，提出离线影响估计方法避免策略展开的计算成本，采用稀疏随机投影降低LLM高维梯度维度

Result: 在7B参数模型上验证，1.5B模型实现2.66倍步骤级加速，每阶段仅使用10%数据

Conclusion: 基于影响的数据选择在高效RLVR中具有巨大潜力，CROPI框架显著加速训练

Abstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.

</details>


### [59] [Remote Labor Index: Measuring AI Automation of Remote Work](https://arxiv.org/abs/2510.26787)
*Mantas Mazeika,Alice Gatti,Cristina Menghini,Udari Madhushani Sehwag,Shivam Singhal,Yury Orlovskiy,Steven Basart,Manasi Sharma,Denis Peskoff,Elaine Lau,Jaehyuk Lim,Lachlan Carroll,Alice Blair,Vinaya Sivakumar,Sumana Basu,Brad Kenstler,Yuntao Ma,Julian Michael,Xiaoke Li,Oliver Ingebretsen,Aditya Mehta,Jean Mottola,John Teichmann,Kevin Yu,Zaina Shaik,Adam Khoja,Richard Ren,Jason Hausenloy,Long Phan,Ye Htet,Ankit Aich,Tahseen Rabbani,Vivswan Shah,Andriy Novykov,Felix Binder,Kirill Chugunov,Luis Ramirez,Matias Geralnik,Hernán Mesura,Dean Lee,Ed-Yeremai Hernandez Cardona,Annette Diamond,Summer Yue,Alexandr Wang,Bing Liu,Ernesto Hernandez,Dan Hendrycks*

Main category: cs.LG

TL;DR: AI在知识推理基准测试中进步迅速，但经济价值转化不明确。研究者引入远程劳动指数(RLI)来衡量AI代理在真实经济项目中的表现，发现AI代理自动化率仅为2.5%。


<details>
  <summary>Details</summary>
Motivation: 评估AI在知识推理基准测试中的进步如何转化为实际经济价值和自动化能力，为AI驱动的劳动力自动化提供实证依据。

Method: 引入远程劳动指数(RLI)，这是一个多行业基准，包含真实世界、具有经济价值的项目，用于评估AI代理在实践环境中的端到端性能。

Result: AI代理在RLI上表现接近底线，表现最佳的代理自动化率仅为2.5%。

Conclusion: 这些结果有助于基于实证证据讨论AI自动化问题，为跟踪AI影响和利益相关者主动应对AI驱动的劳动力自动化建立共同基础。

Abstract: AIs have made rapid progress on research-oriented benchmarks of knowledge and
reasoning, but it remains unclear how these gains translate into economic value
and automation. To measure this, we introduce the Remote Labor Index (RLI), a
broadly multi-sector benchmark comprising real-world, economically valuable
projects designed to evaluate end-to-end agent performance in practical
settings. AI agents perform near the floor on RLI, with the highest-performing
agent achieving an automation rate of 2.5%. These results help ground
discussions of AI automation in empirical evidence, setting a common basis for
tracking AI impacts and enabling stakeholders to proactively navigate AI-driven
labor automation.

</details>


### [60] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 研究发现FP16比BF16在LLM强化学习微调中更稳定，能消除训练和推理策略之间的数值不匹配问题


<details>
  <summary>Details</summary>
Motivation: 解决LLM强化学习微调中因浮点精度导致的训练与推理策略不匹配问题

Method: 将浮点精度从BF16改为FP16，无需修改模型架构或学习算法

Result: FP16带来更稳定的优化、更快的收敛速度和更强的性能表现

Conclusion: FP16在RL微调中优于BF16，建议重新考虑精度权衡

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


### [61] [Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/abs/2510.26782)
*Zaishuo Xia,Yukuan Lu,Xinyi Li,Yifan Xu,Yubei Chen*

Main category: cs.LG

TL;DR: 提出了几何正则化世界模型（GRWM），通过强制自然感官轨迹中的连续点在潜在表示空间中保持接近，显著改善了潜在表示质量，从而提高了世界模型的长期预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型在长期预测中表现脆弱，主要原因是表示质量问题——高维外部输入和损失/纠缠的潜在表示使动态学习变得困难。本文旨在通过改进表示学习来显著提升世界模型性能。

Method: 提出GRWM方法，在潜在表示空间中实施几何正则化，确保连续观测点在潜在空间中保持邻近关系。该方法即插即用，仅需最小架构修改，与多种潜在生成主干兼容。

Result: 在确定性3D环境和长期预测任务中，GRWM显著提高了rollout保真度和稳定性。分析表明其优势源于学习到具有优越几何结构的潜在流形。

Conclusion: 改进表示学习是构建鲁棒世界模型的直接有效途径，能够在无需扩大动态模块的情况下实现可靠的长期预测。

Abstract: A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [62] [Composer: Building a fast frontier model with RL](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fcomposer%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/X5Cj-Tcgbb8gySnRIrK2AhU93p0fK1uVNQuKwA9-tC0=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor开发了Composer，这是一个用于软件工程的代理模型，比同类模型快4倍达到前沿编码结果。


<details>
  <summary>Details</summary>
Motivation: 开发更快的软件工程代理模型来解决现实世界的软件工程挑战。

Method: 使用强化学习训练的专家混合模型，结合代码编辑和语义搜索等多种工具。

Result: Composer比类似模型快4倍达到前沿编码结果。

Conclusion: Composer是一个高效的软件工程代理模型，能够显著提升编码效率。

Abstract: Composer: Building a fast frontier model with RL (5 minute read) Cursor has developed Composer, a new agent model for software engineering that achieves frontier coding results four times faster than similar models. Composer is a mixture-of-experts model trained with reinforcement learning to solve real-world software engineering challenges using various tools, including code editing and semantic search.

</details>


### [63] [Is Llama really as bad as people say? I put Meta's AI to the test](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.logrocket.com%2Fllama-meta-ai%3Futm_source=tldrwebdev/1/0100019a34ce511c-254779a5-7ea5-4067-b8fe-1a9633d438d1-000000/27X171kh-CvR8b8Yi9lvEyW1amRSUg-eA6RlInwocDw=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta的Llama 3.2 1B模型是一个有用的轻量级编程助手，适合开发工作，但无法处理智能编程任务，生成的代码需要手动修复，最适合简单项目。


<details>
  <summary>Details</summary>
Motivation: 评估Meta的Llama 3.2 1B模型在实际编程任务中的表现，回应批评者的负面评价。

Method: 通过实际测试Llama 3.2 1B模型在编程任务中的表现，包括简单项目和复杂任务。

Result: Llama 3.2 1B模型是一个有用的轻量级编程助手，能生成功能性的代码，但需要手动修复，无法处理智能编程任务，最适合简单项目如待办事项应用。

Conclusion: Llama 3.2 1B模型并非如批评者所说的那么糟糕，是一个实用的轻量级编程助手，但有其局限性。

Abstract: Is Llama really as bad as people say? I put Meta's AI to the test (11 minute read) Meta's Llama 3.2 1B model is a useful, lightweight coding assistant for development work. It's not as bad as critics claim. However, it can't handle agentic coding and produces functional but imperfect code, constantly requiring manual fixes. Llama is best for simple projects like todo apps.

</details>


### [64] [MiniMax M2 — Open-Sourced & Free](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fagent.minimax.io%2F%3Futm_source=newsletter%26utm_campaign=tldr/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/joKtmHQFV-W7D47wUZ8TFPVoS5Pdyz3bqnoeqedlks4=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: MiniMax M2是一个开源免费的AI模型，专为智能体和代码任务设计，比Claude Sonnet快2倍且价格仅为8%


<details>
  <summary>Details</summary>
Motivation: 为开发者和研究人员提供高性能、低成本的AI模型解决方案，特别是在智能体和代码生成领域

Method: 开发了MiniMax M2模型，采用选择性参数激活技术，提供API接口和本地开源版本

Result: 模型性能达到Claude Sonnet的2倍速度，成本仅为8%，支持智能体创建、代码编写和部署

Conclusion: MiniMax M2为AI应用开发提供了高效经济的解决方案，有望推动智能体和代码生成领域的发展

Abstract: MiniMax M2 — Open-Sourced & Free (Sponsor) Built for Agents & Code, 2× faster at 8% of Claude Sonnet's price. Create, code, and deploy smarter with selective parameter activation.Try it now: MiniMax Agent | MiniMax-M2 API | Open-source for local use.

</details>


### [65] [Introducing SWE-1.5: Our Fast Agent Model](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcognition.ai%2Fblog%2Fswe-1-5%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/3csmTJHpVNjpjfjjWNLvt9SbeC7FxYunwvva6NW9xas=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SWE-1.5是一个针对软件工程优化的前沿规模模型，具有数千亿参数，在编码性能上接近最先进水平，并设定了新的速度标准，在Windsurf中可达950 tokens/秒。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门针对软件工程任务优化的高速AI模型，以支持大规模代码库的深度探索、全栈应用构建和配置编辑等任务。

Method: 构建具有数千亿参数的前沿规模模型，专门针对软件工程任务进行优化，并在Windsurf平台上部署以实现高速服务。

Result: 模型在编码性能上接近最先进水平，服务速度可达950 tokens/秒，能够深度理解大型代码库、构建全栈应用和编辑配置。

Conclusion: SWE-1.5为软件工程任务设定了新的速度和性能标准，是一个高效的代码助手模型。

Abstract: Introducing SWE-1.5: Our Fast Agent Model (8 minute read) SWE-1.5 is a frontier-sized model with hundreds of billions of parameters optimized for software engineering. It achieves near state-of-the-art coding performance and sets a new standard for speed. The model is now available in Windsurf, serving at up to 950 tokens per second. It can be used to deeply explore and understand large codebases, build end-to-end full-stack apps, and easily edit configurations without needing to memorize fie...

</details>


### [66] [Agent Labs Are Eating the Software World](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.nibzard.com%2Fagent-labs%3Futm_source=tldrai/1/0100019a3545e68c-a9f3a5d1-8276-46fa-80c8-1af019ace08f-000000/S-JfYGYlkGAdeO-wcd5cjjlTgKOmco6iik_vHe-Cqx4=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent labs focus on product-first approach, building goal-directed AI systems that deliver outcomes and capture value in AI stack. Key skills shift from model architecture to system design and domain-specific workflow expertise.


<details>
  <summary>Details</summary>
Motivation: To highlight the emerging trend where agent labs prioritize product delivery over infrastructure, emphasizing the importance of turning AI models into practical, outcome-driven systems.

Method: Agent labs adopt a product-first strategy, building reliable workflows on existing models rather than focusing on model architecture development.

Result: This approach allows agent labs to capture real value in the AI stack by delivering functional systems that solve domain-specific problems effectively.

Conclusion: The most valuable developer skills are evolving from model architecture expertise to system design, evaluation engineering, and domain-specific workflow development.

Abstract: Agent Labs Are Eating the Software World (8 minute read) Agent labs ship product first and build infrastructure later. They turn AI models into goal-directed systems that deliver outcomes, capturing the real value in the AI stack. Founders only need a deep understanding of a domain and the ability to build reliable workflows on top of existing models. The most valuable developer skills are shifting from model architecture to system design, evaluation engineering, and domain-specific workflow ...

</details>


### [67] [How to Fix the #1 Problem with Vibe Coding](http://tracking.tldrnewsletter.com/CL0/http:%2F%2Fxano.com%2Fgo%3Futm_medium=affiliate%26utm_source=tldr%26utm_campaign=tldr_vibe_coding/2/0100019a39cb5dab-dc276273-a1e7-4fb2-bd18-cf96ba9283f8-000000/_QmPySlFWnaRmPkUpDej_WlgCt4SHX0rLGQlh0la3wM=429)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Xano平台通过可视化方式解决AI编程中的黑盒问题，让开发者能够清晰看到后端逻辑，提高开发效率和代码可控性


<details>
  <summary>Details</summary>
Motivation: 解决AI编程中代码逻辑不透明、架构混乱的问题，让开发者能够更好地理解和控制生成的代码

Method: 提供可视化后端开发平台，将AI生成的代码逻辑以图形化方式展示，消除黑盒效应

Result: 开发者能够更快地构建应用，同时保持对代码架构和安全性的完全控制

Conclusion: 可视化AI编程工具能够有效解决代码黑盒问题，提高开发效率和代码质量

Abstract: How to Fix the #1 Problem with Vibe Coding (Sponsor) Vibe coding makes building fast—but hides what's really happening. All of the logic, architecture, and security live in hundreds of lines of code. Even to a trained engineer, the sheer volume can make it impractical to parse.Vibe code without the black box.Xano lets you build with AI and see your entire backend logic visually, so you can ship faster and stay in control.No black boxes. Just visibility, flexibility, and a production-ready bac...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [68] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs在函数级代码生成表现良好，但在真实软件项目的类级实现中表现不佳，正确率从合成基准的84-89%降至真实场景的25-34%。检索增强生成在部分文档情况下最有效，能提升4-7%正确率。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实软件项目中生成类级代码的能力，了解其在实践条件下的泛化表现，填补当前对类级工程能力理解不足的空白。

Method: 从开源仓库构建新基准，包含真实世界类，分为已见和未见分区。评估多种LLMs在不同输入规范、检索增强配置和文档完整性水平下的表现。

Result: LLMs在真实类任务中正确率仅25-34%，远低于合成基准。完整文档字符串仅带来1-3%的微小提升。检索增强在部分文档时最有效，能提升4-7%正确率。主要错误类型为AttributeError、TypeError和AssertionError。

Conclusion: 当前LLMs在类级工程能力存在严重局限，需要改进上下文建模、文档策略和检索集成，为生产代码辅助工具提供可操作的改进方向。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [69] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出了一种通过分析测试类中共享静态字段来优先排序潜在顺序依赖测试的方法，显著减少了测试执行次数和不必要的重复运行。


<details>
  <summary>Details</summary>
Motivation: 顺序依赖测试会导致持续集成管道失败，现有检测方法需要多次重复运行测试，成本高昂，因此需要优先排序潜在OD测试以减少重复运行。

Method: 通过分析测试类中的共享静态字段来识别更可能具有顺序依赖性的测试，从而优先排序这些测试。

Result: 在27个项目模块的实验中，该方法成功在23个案例中优先排序了所有OD测试，平均减少测试执行65.92%，减少不必要重复运行72.19%。

Conclusion: 该方法通过降低执行成本显著提高了OD测试检测的效率。

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [70] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: RepoSearch-R1是一个基于蒙特卡洛树搜索的代理强化学习框架，用于解决仓库级软件工程任务，通过自训练生成高质量推理轨迹，无需模型蒸馏或外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法在仓库级软件工程任务中存在局限性：无训练方法难以有效指导代理使用工具和基于环境反馈决策，而基于训练的方法依赖从大模型蒸馏，在企业环境中存在数据合规问题。

Method: 引入RepoSearch-R1框架，基于蒙特卡洛树搜索的代理强化学习，通过自训练生成多样化的高质量推理轨迹。构建了专门用于仓库问答任务的RepoQA-Agent。

Result: 在仓库问答任务评估中，RepoSearch-R1显著提升答案完整性：比无检索方法提高16.0%，比迭代检索方法提高19.5%，训练效率比通用代理强化学习方法提高33%。

Conclusion: 冷启动训练方法消除了数据合规问题，同时在仓库级推理任务中保持了强大的探索多样性和答案完整性。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [71] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: SecureReviewer是一个专门用于代码安全审查的LLM方法，通过构建安全代码审查数据集、安全感知微调策略和RAG技术，提高了LLM在代码审查中识别和解决安全问题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查方法主要关注通用代码审查，在识别和解决安全相关问题方面效果有限，且面临数据稀缺和评估指标不足的挑战。

Method: 构建安全代码审查数据集，采用安全感知微调策略训练LLM，集成RAG技术减少幻觉，并引入SecureBLEU评估指标。

Result: 实验结果显示SecureReviewer在安全漏洞检测准确性和生成审查评论的整体质量及实用性方面优于现有基准方法。

Conclusion: SecureReviewer有效提升了LLM在代码安全审查中的能力，为软件开发生命周期早期识别和解决安全问题提供了可靠解决方案。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [72] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 评估5个开源LLM在Python代码提取方法重构任务上的表现，发现RCI提示策略优于单次提示，Deepseek-Coder-RCI和Qwen2.5-Coder-RCI表现最佳，开发者调查显示70%以上的重构被接受。


<details>
  <summary>Details</summary>
Motivation: 自动化提取方法重构仍然具有挑战性，开源LLM为自动化这类高级任务提供了新方法。

Method: 系统评估5个3B到8B参数的开源LLM，使用自动指标评估功能正确性和代码质量，比较单次提示和RCI提示策略。

Result: RCI提示在测试通过率和重构质量上表现更好，最佳模型测试通过率分别达到82.9%和80.8%，显著降低代码行数和圈复杂度。开发者调查显示70%以上接受率。

Conclusion: RCI提示策略能有效提升自动重构质量，但传统指标与人工判断存在差异，需要人机协同评估。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [73] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 提出了Instruct4Edit自动数据生成管道，使用LLM合成高质量网页编辑微调数据集，通过微调使模型能更好地将人类意图转化为精确的代码修改


<details>
  <summary>Details</summary>
Motivation: 网页应用迭代需要代码修改，传统方法耗时且手动。LLM虽然能生成UI代码，但根据新设计需求编辑现有代码仍具挑战，缺乏大规模高质量调优数据

Method: 开发自动化数据生成管道，使用LLM合成多样指令，应用相应代码修改，并进行视觉验证确保正确性，创建Instruct4Edit数据集

Result: 在Instruct4Edit上微调的模型在将人类意图转化为精确、结构一致且视觉准确的代码修改方面表现一致提升

Conclusion: 为基于自然语言的网页编辑提供了可扩展和透明的基础，证明微调较小的开源模型可以达到与专有系统竞争的性能

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [74] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 本研究通过分析ImageMagick项目中76个漏洞重新引入案例，发现过程指标（如问题腐败度和问题密度）与漏洞重新引入密切相关，强调漏洞重新引入是累积性开发活动和社技条件的结果。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞即使在修复后仍会持续存在或重新出现，揭示代码演进与社技因素间的复杂互动。现有研究很少探索过程指标是否能揭示随时间推移的风险开发活动，这对预测和缓解软件漏洞至关重要。

Method: 在ImageMagick项目中进行案例研究，将纵向过程指标（如巴士因子、问题密度、问题腐败度）与漏洞重新引入活动相关联，分析提交级别的安全修复，关注漏洞演化和重新出现的长期变化序列。

Result: 研究显示漏洞重新引入通常与问题腐败度增加和问题密度波动相一致，反映了问题管理和团队响应能力的短期低效。过程指标能有效揭示导致漏洞重新引入的风险开发模式。

Conclusion: 过程指标与代码指标结合能更好地预测风险修复并增强软件安全性，漏洞重新引入很少是孤立行动的结果，而是累积开发活动和社技条件的产物。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [75] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 评估使用GitHub Copilot Agent Mode自动迁移SQLAlchemy库版本的效果，发现虽然API迁移覆盖率高(100%)，但应用功能测试通过率低(39.75%)


<details>
  <summary>Details</summary>
Motivation: 软件库更新是耗时且易错的过程，大型语言模型和智能编码系统为自动化此类维护任务提供了新机会

Method: 使用GitHub Copilot Agent Mode作为自主AI系统，在10个客户端应用数据集上执行SQLAlchemy库的多步骤迁移工作流，并引入迁移覆盖率指标评估效果

Result: LLM代理能够成功迁移功能和API使用(迁移覆盖率中位数100%)，但未能保持应用功能，导致测试通过率低(中位数39.75%)

Conclusion: 虽然LLM代理在API迁移方面表现良好，但在保持应用功能方面存在不足，需要进一步改进

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>
