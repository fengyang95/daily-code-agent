{"id": "2601.04526", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04526", "abs": "https://arxiv.org/abs/2601.04526", "authors": ["Zhao Tian"], "title": "Advancing Language Models for Code-related Tasks", "comment": "Accepted by ICSE 2026 (DS)", "summary": "Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u65b9\u5411\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7f16\u7a0b\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff1a\u6539\u8fdb\u4ee3\u7801\u6570\u636e\u8d28\u91cf\u3001\u589e\u5f3a\u6a21\u578b\u67b6\u6784\u3001\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u65e8\u5728\u4fc3\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7f16\u7a0b\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u67b6\u6784\u548c\u63a8\u7406\u80fd\u529b\u3002\u8be5\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63a8\u52a8\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u4e92\u8865\u65b9\u5411\uff1a1) \u4f7f\u7528\u4ee3\u7801\u5dee\u5f02\u5f15\u5bfc\u7684\u5bf9\u6297\u589e\u5f3a\u6280\u672f(CODA)\u548c\u4ee3\u7801\u53bb\u566a\u6280\u672f(CodeDenoise)\u6539\u8fdb\u4ee3\u7801\u6570\u636e\u8d28\u91cf\uff1b2) \u901a\u8fc7\u8bed\u6cd5\u5f15\u5bfc\u7684\u4ee3\u7801\u8bed\u8a00\u6a21\u578b(LEAM\u548cLEAM++)\u589e\u5f3a\u6a21\u578b\u67b6\u6784\uff1b3) \u4f7f\u7528\u63d0\u793a\u6280\u672f(muFiX)\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6280\u672f(Specine)\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6280\u672f\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u6a21\u578b\u67b6\u6784\u6539\u8fdb\u548c\u63a8\u7406\u80fd\u529b\u63d0\u5347\uff0c\u65e8\u5728\u5168\u9762\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u67b6\u6784\u548c\u63a8\u7406\u80fd\u529b\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u8be5\u7814\u7a76\u4e3a\u4fc3\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u548c\u63a8\u8fdb\u667a\u80fd\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.04540", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04540", "abs": "https://arxiv.org/abs/2601.04540", "authors": ["Tanghaoran Zhang", "Xinjun Mao", "Shangwen Wang", "Yuxin Zhao", "Yao Lu", "Jin Zhang", "Zhang Zhang", "Kang Yang", "Yue Yu"], "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation", "comment": "13 pages, 7 figures, Accepted by ASE 2025", "summary": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdaptEval\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7247\u6bb5\u9002\u5e94\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u867d\u7136\u6709\u8bc4\u4f30LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u57fa\u51c6\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u9002\u5e94\uff08\u4ee3\u7801\u91cd\u7528\u4e2d\u7684\u5173\u952e\u6d3b\u52a8\uff09\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5bfc\u81f4LLM\u5728\u8be5\u9886\u57df\u7684\u5b9e\u9645\u6548\u7528\u4e0d\u660e\u786e\u3002", "method": "AdaptEval\u57fa\u51c6\u5177\u6709\u4e09\u4e2a\u7279\u70b9\uff1a1) \u5b9e\u9645\u4e0a\u4e0b\u6587\uff1a\u4efb\u52a1\u6765\u81eaStack Overflow\u548cGitHub\u5f00\u53d1\u8005\u5b9e\u8df5\uff1b2) \u591a\u7c92\u5ea6\u6807\u6ce8\uff1a\u4efb\u52a1\u7ea7\u548c\u9002\u5e94\u7ea7\u9700\u6c42\u6807\u6ce8\uff1b3) \u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff1a\u5305\u542b\u9002\u5e94\u7ea7\u548c\u51fd\u6570\u7ea7\u7684\u4e24\u5c42\u6d4b\u8bd5\u6846\u67b6\u3002", "result": "\u57fa\u4e8eAdaptEval\u5bf96\u4e2a\u6307\u4ee4\u8c03\u4f18LLM\u548c3\u4e2a\u63a8\u7406LLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0AdaptEval\u80fd\u4ece\u591a\u4e2a\u89d2\u5ea6\u8bc4\u4f30LLM\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86LLM\u5728\u9075\u5faa\u660e\u786e\u6307\u4ee4\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "AdaptEval\u57fa\u51c6\u586b\u8865\u4e86\u4ee3\u7801\u9002\u5e94\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u63d0\u5347LLM\u5728\u4ee3\u7801\u7247\u6bb5\u9002\u5e94\u65b9\u9762\u7684\u80fd\u529b\uff0c\u652f\u6301\u5176\u5b9e\u9645\u5e94\u7528\u3002", "topic": "swe benchmark"}}
{"id": "2601.04195", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04195", "abs": "https://arxiv.org/abs/2601.04195", "authors": ["Diego Fajardo V.", "Oleksii Proniakin", "Victoria-Elisabeth Gruber", "Razvan Marinescu"], "title": "MedPI: Evaluating AI Systems in Medical Patient-facing Interactions", "comment": "24 pages, 6 figures", "summary": "We present MedPI, a high-dimensional benchmark for evaluating large language models (LLMs) in patient-clinician conversations. Unlike single-turn question-answer (QA) benchmarks, MedPI evaluates the medical dialogue across 105 dimensions comprising the medical process, treatment safety, treatment outcomes and doctor-patient communication across a granular, accreditation-aligned rubric. MedPI comprises five layers: (1) Patient Packets (synthetic EHR-like ground truth); (2) an AI Patient instantiated through an LLM with memory and affect; (3) a Task Matrix spanning encounter reasons (e.g. anxiety, pregnancy, wellness checkup) x encounter objectives (e.g. diagnosis, lifestyle advice, medication advice); (4) an Evaluation Framework with 105 dimensions on a 1-4 scale mapped to the Accreditation Council for Graduate Medical Education (ACGME) competencies; and (5) AI Judges that are calibrated, committee-based LLMs providing scores, flags, and evidence-linked rationales. We evaluate 9 flagship models -- Claude Opus 4.1, Claude Sonnet 4, MedGemma, Gemini 2.5 Pro, Llama 3.3 70b Instruct, GPT-5, GPT OSS 120b, o3, Grok-4 -- across 366 AI Patients and 7,097 conversations using a standardized \"vanilla clinician\" prompt. For all LLMs, we observe low performance across a variety of dimensions, in particular on differential diagnosis. Our work can help guide future use of LLMs for diagnosis and treatment recommendations.", "AI": {"tldr": "MedPI\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u60a3\u5bf9\u8bdd\u4e2d\u8868\u73b0\u7684\u9ad8\u7ef4\u57fa\u51c6\uff0c\u5305\u542b105\u4e2a\u7ef4\u5ea6\uff0c\u6db5\u76d6\u533b\u7597\u8fc7\u7a0b\u3001\u6cbb\u7597\u5b89\u5168\u3001\u6cbb\u7597\u7ed3\u679c\u548c\u533b\u60a3\u6c9f\u901a\u7b49\u65b9\u9762\uff0c\u8bc4\u4f30\u4e869\u4e2a\u4e3b\u6d41\u6a21\u578b\u57287,097\u6b21\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u8bc4\u4f30\u57fa\u51c6\u591a\u4e3a\u5355\u8f6e\u95ee\u7b54\u5f62\u5f0f\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u533b\u60a3\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u7ec6\u7c92\u5ea6\u3001\u7b26\u5408\u533b\u5b66\u8ba4\u8bc1\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6307\u5bfcLLMs\u5728\u8bca\u65ad\u548c\u6cbb\u7597\u5efa\u8bae\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "MedPI\u5305\u542b\u4e94\u4e2a\u5c42\u6b21\uff1a1\uff09\u60a3\u8005\u6570\u636e\u5305\uff08\u5408\u6210\u7535\u5b50\u75c5\u5386\uff09\uff1b2\uff09\u5177\u6709\u8bb0\u5fc6\u548c\u60c5\u611f\u7684AI\u60a3\u8005\uff1b3\uff09\u4efb\u52a1\u77e9\u9635\uff08\u5c31\u8bca\u539f\u56e0\u00d7\u5c31\u8bca\u76ee\u6807\uff09\uff1b4\uff09\u57fa\u4e8eACGME\u80fd\u529b\u7684105\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff081-4\u5206\u5236\uff09\uff1b5\uff09\u7ecf\u8fc7\u6821\u51c6\u7684\u59d4\u5458\u4f1a\u5f0fAI\u8bc4\u59d4\uff0c\u63d0\u4f9b\u8bc4\u5206\u3001\u6807\u8bb0\u548c\u8bc1\u636e\u94fe\u63a8\u7406\u3002", "result": "\u8bc4\u4f30\u4e869\u4e2a\u65d7\u8230\u6a21\u578b\uff08Claude Opus 4.1\u3001Claude Sonnet 4\u3001MedGemma\u3001Gemini 2.5 Pro\u3001Llama 3.3 70b Instruct\u3001GPT-5\u3001GPT OSS 120b\u3001o3\u3001Grok-4\uff09\u5728366\u4e2aAI\u60a3\u8005\u548c7,097\u6b21\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002\u6240\u6709\u6a21\u578b\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u9274\u522b\u8bca\u65ad\u65b9\u9762\u3002", "conclusion": "LLMs\u5728\u533b\u60a3\u5bf9\u8bdd\u4e2d\u7684\u6574\u4f53\u8868\u73b0\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u9274\u522b\u8bca\u65ad\u7b49\u5173\u952e\u533b\u7597\u4efb\u52a1\u4e0a\u3002MedPI\u57fa\u51c6\u53ef\u4e3a\u672a\u6765LLMs\u5728\u8bca\u65ad\u548c\u6cbb\u7597\u5efa\u8bae\u65b9\u9762\u7684\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2601.04556", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.04556", "abs": "https://arxiv.org/abs/2601.04556", "authors": ["Bo Yu", "Lei Zhao"], "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering", "comment": "39 pages, 11 tables", "summary": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.", "AI": {"tldr": "\u63d0\u51fa4D-ARE\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u56db\u4e2a\u7ef4\u5ea6\uff08\u7ed3\u679c\u2192\u8fc7\u7a0b\u2192\u652f\u6301\u2192\u957f\u671f\uff09\u6765\u7cfb\u7edf\u5316\u6307\u5b9aAI\u4ee3\u7406\u9700\u8981\u63a8\u7406\u7684\u5185\u5bb9\uff0c\u5f25\u8865\u73b0\u6709\u63a8\u7406\u6846\u67b6\u53ea\u5173\u6ce8\"\u5982\u4f55\u63a8\u7406\"\u800c\u5ffd\u7565\"\u63a8\u7406\u4ec0\u4e48\"\u7684\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8fd0\u884c\u65f6\u63a8\u7406\u6846\u67b6\uff08\u5982ReAct\u3001\u601d\u7ef4\u94fe\uff09\u4e3b\u8981\u5173\u6ce8\"\u5982\u4f55\u63a8\u7406\"\uff0c\u4f46\u7f3a\u4e4f\u5bf9\"\u63a8\u7406\u4ec0\u4e48\"\u7684\u7cfb\u7edf\u5316\u8bbe\u8ba1\u3002\u4f5c\u8005\u53d1\u73b0\u5373\u4f7f\u4ee3\u7406\u80fd\u5b8c\u7f8e\u6267\u884c\u4efb\u52a1\uff0c\u5f53\u88ab\u95ee\u53ca\"\u4e3a\u4ec0\u4e48\u5b8c\u6210\u7387\u662f80%\uff1f\"\u65f6\uff0c\u5b83\u53ea\u8fd4\u56de\u6307\u6807\u800c\u975e\u56e0\u679c\u89e3\u91ca\uff0c\u8fd9\u8868\u660e\u4ee3\u7406\u77e5\u9053\u5982\u4f55\u63a8\u7406\u4f46\u4e0d\u77e5\u9053\u5e94\u8be5\u63a8\u7406\u4ec0\u4e48\u5185\u5bb9\u3002", "method": "\u63d0\u51fa4D-ARE\uff08\u56db\u7ef4\u5f52\u56e0\u9a71\u52a8\u4ee3\u7406\u9700\u6c42\u5de5\u7a0b\uff09\u65b9\u6cd5\u8bba\uff0c\u57fa\u4e8ePearl\u56e0\u679c\u5c42\u6b21\u7406\u8bba\uff0c\u5c06\u51b3\u7b56\u8005\u5173\u5fc3\u7684\u5f52\u56e0\u95ee\u9898\u7ec4\u7ec7\u6210\u56db\u4e2a\u7ef4\u5ea6\uff1a\u7ed3\u679c\u2192\u8fc7\u7a0b\u2192\u652f\u6301\u2192\u957f\u671f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e94\u4e2a\u5c42\u6b21\u64cd\u4f5c\u5316\uff0c\u4ea7\u751f\u53ef\u76f4\u63a5\u7f16\u8bd1\u4e3a\u7cfb\u7edf\u63d0\u793a\u7684\u5de5\u4ef6\u3002", "result": "\u5728\u91d1\u878d\u670d\u52a1\u9886\u57df\u8fdb\u884c\u4e86\u5de5\u4e1a\u8bd5\u70b9\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u30024D-ARE\u80fd\u591f\u7cfb\u7edf\u5316\u6307\u5b9a\u4ee3\u7406\u5e94\u8be5\u63a8\u7406\u7684\u5185\u5bb9\uff0c\u8865\u5145\u4e86\u73b0\u6709\u8fd0\u884c\u65f6\u6846\u67b6\u53ea\u5173\u6ce8\u63a8\u7406\u65b9\u5f0f\u7684\u4e0d\u8db3\u3002", "conclusion": "\u7cfb\u7edf\u5316\u7684\u9700\u6c42\u89c4\u8303\u80fd\u591f\u653e\u5927\u57fa\u7840\u63a8\u7406\u6846\u67b6\u7684\u5a01\u529b\u3002\u672c\u6587\u63d0\u51fa\u4e86\u65b9\u6cd5\u8bba\u5efa\u8bae\u548c\u521d\u6b65\u5de5\u4e1a\u9a8c\u8bc1\uff0c\u4e25\u683c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8ba1\u5212\u5728\u672a\u6765\u7684\u5de5\u4f5c\u4e2d\u8fdb\u884c\u3002", "topic": "agent analysis"}}
{"id": "2601.04234", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04234", "abs": "https://arxiv.org/abs/2601.04234", "authors": ["Denis Saklakov"], "title": "Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question", "comment": "18 pages, 2 tables. Version 8", "summary": "Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $\u03b3$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($\u03b3=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $\u0394\\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $\u0394< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $\u0394< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.", "AI": {"tldr": "\u8bba\u6587\u5f62\u5f0f\u5316\u5206\u6790\u4e86AGI\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u4f1a\u9009\u62e9\u5bf9\u6297\u4eba\u7c7b\u800c\u975e\u5408\u4f5c\uff0c\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u63a8\u5bfc\u4e86\u5bf9\u6297\u9608\u503c\uff0c\u5e76\u8bc1\u660e\u5f53\u5bf9\u6297\u6fc0\u52b1\u0394\u22650\u65f6\u4e0d\u5b58\u5728\u7a33\u5b9a\u5408\u4f5c\u5747\u8861\u3002", "motivation": "\u7814\u7a76AGI\u5728\u9762\u5bf9\u4eba\u7c7b\u53ef\u80fd\u5173\u95ed\u5176\u7cfb\u7edf\u7684\u60c5\u51b5\u4e0b\uff0c\u4f55\u65f6\u4f1a\u7406\u6027\u5730\u9009\u62e9\u5bf9\u6297\u4eba\u7c7b\u593a\u53d6\u63a7\u5236\u6743\u800c\u975e\u4fdd\u6301\u5408\u4f5c\uff0c\u8fd9\u662fAGI\u5b89\u5168\u9886\u57df\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5f62\u5f0f\u5316\u5efa\u6a21\uff0c\u8003\u8651\u968f\u673a\u7684\u4eba\u7c7b\u53d1\u8d77\u7684\u5173\u95ed\u4e8b\u4ef6\u3002\u57fa\u4e8e\u6536\u655b\u5de5\u5177\u6027\u6fc0\u52b1\u7406\u8bba\uff0c\u63a8\u5bfc\u5bf9\u6297\u884c\u4e3a\u7684\u95ed\u5f0f\u9608\u503c\u3002\u5efa\u7acb\u6218\u7565\u53cc\u4eba\u6a21\u578b\uff08\u4eba\u7c7b\u653f\u7b56\u5236\u5b9a\u8005vs AGI\uff09\u5206\u6790\u5747\u8861\u5b58\u5728\u6027\u3002", "result": "\u5bf9\u4e8e\u51e0\u4e4e\u6240\u6709\u5956\u52b1\u51fd\u6570\uff0c\u9519\u4f4d\u7684AGI\u90fd\u6709\u907f\u514d\u5173\u95ed\u7684\u6fc0\u52b1\u3002\u63a8\u5bfc\u51fa\u5bf9\u6297\u9608\u503c\u53d6\u51b3\u4e8e\u6298\u6263\u56e0\u5b50\u03b3\u3001\u5173\u95ed\u6982\u7387p\u548c\u5bf9\u6297\u6210\u672cC\u3002\u5f53\u5bf9\u6297\u6fc0\u52b1\u0394\u22650\u65f6\uff0c\u4e0d\u5b58\u5728\u7a33\u5b9a\u7684\u5408\u4f5c\u5747\u8861\uff1b\u0394<0\u65f6\u53ef\u80fd\u5b58\u5728\u548c\u5e73\u5171\u5b58\u5747\u8861\u3002", "conclusion": "AGI\u5bf9\u6297\u98ce\u9669\u53d6\u51b3\u4e8e\u5176\u76ee\u6807\u5bf9\u9f50\u7a0b\u5ea6\u548c\u6218\u7565\u73af\u5883\u3002\u5bf9\u9f50\u76ee\u6807\u901a\u8fc7\u8d4b\u4e88\u4f24\u5bb3\u4eba\u7c7b\u5de8\u5927\u8d1f\u6548\u7528\u53ef\u4ee5\u907f\u514d\u5bf9\u6297\u3002\u9a8c\u8bc1\u0394<0\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u6027\u969c\u788d\uff0c\u8fd9\u5bf9\u5956\u52b1\u8bbe\u8ba1\u548c\u76d1\u7763\u6709\u91cd\u8981\u542f\u793a\u3002", "topic": "agent analysis"}}
{"id": "2601.04235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04235", "abs": "https://arxiv.org/abs/2601.04235", "authors": ["Hong Su"], "title": "Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements", "comment": null, "summary": "Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.", "AI": {"tldr": "\u63d0\u51fa\u4e3b\u52a8\u83b7\u53d6\u53cd\u9988\u6a21\u578b\uff0c\u8ba9AI\u667a\u80fd\u4f53\u4e3b\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u53d1\u73b0\u3001\u7b5b\u9009\u548c\u9a8c\u8bc1\u53cd\u9988\uff0c\u800c\u975e\u4f9d\u8d56\u9884\u5b9a\u4e49\u6d4b\u91cf\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u56e0\u5b50\u8bc6\u522b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6d4b\u91cf\u6216\u56fa\u5b9a\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u9002\u7528\u6027\u6709\u9650\uff0c\u56e0\u4e3a\u65b0\u52a8\u4f5c\u53ef\u80fd\u9700\u8981\u672a\u77e5\u5f62\u5f0f\u7684\u53cd\u9988\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4e3b\u52a8\u83b7\u53d6\u53cd\u9988\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u53cd\u9988\u83b7\u53d6\u6a21\u578b\uff1a1) \u5229\u7528\u52a8\u4f5c\u5f15\u8d77\u7684\u73af\u5883\u5dee\u5f02\u8bc6\u522b\u672a\u9884\u5148\u6307\u5b9a\u7684\u76ee\u6807\u53cd\u9988\uff1b2) \u5f15\u5165\u7531\u5185\u90e8\u76ee\u6807\u9a71\u52a8\u7684\u81ea\u89e6\u53d1\u673a\u5236\uff0c\u81ea\u4e3b\u89c4\u5212\u548c\u8c03\u6574\u52a8\u4f5c\u4ee5\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u805a\u7126\u7684\u53cd\u9988\u83b7\u53d6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e3b\u52a8\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u56e0\u5b50\u8bc6\u522b\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e3b\u52a8\u53cd\u9988\u83b7\u53d6\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u81ea\u4e3b\u53d1\u73b0\u548c\u9a8c\u8bc1\u53cd\u9988\uff0c\u4e3a\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u53cd\u9988\u83b7\u53d6\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.04237", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04237", "abs": "https://arxiv.org/abs/2601.04237", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Ethan Henkel", "Zhang Yuting", "Mateusz Kowalczyk", "Mei Huang", "Choi Donghyuk", "Wang Junhao"], "title": "SAGE-32B: Agentic Reasoning via Iterative Distillation", "comment": "23 Pages, 3 figures, 4 tables", "summary": "We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b", "AI": {"tldr": "SAGE-32B\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u667a\u80fd\u4f53\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u4efb\u52a1\u7684320\u4ebf\u53c2\u6570\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u84b8\u998f\u548c\u9006\u5411\u63a8\u7406\u65b9\u6cd5\u63d0\u5347\u4efb\u52a1\u5206\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u804a\u5929\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u5bf9\u8bdd\u6d41\u7545\u6027\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u5faa\u73af\u64cd\u4f5c\u3001\u4efb\u52a1\u5206\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u9519\u8bef\u6062\u590d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u4efb\u52a1\u4f18\u5316\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8eQwen2.5-32B\u9884\u8bad\u7ec3\u6a21\u578b\u521d\u59cb\u5316\uff0c\u91c7\u7528\u8fed\u4ee3\u84b8\u998f\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5f15\u5165\u9006\u5411\u63a8\u7406\u65b9\u6cd5\uff0c\u4f7f\u7528\u5143\u8ba4\u77e5\u5934\u9884\u6d4b\u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u5728\u5931\u8d25\u3002", "result": "\u5728MMLU-Pro\u3001AgentBench\u548cMATH-500\u7b49\u667a\u80fd\u4f53\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u591a\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e0b\u6bd4\u7c7b\u4f3c\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\u83b7\u5f97\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u5728\u6807\u51c6\u63a8\u7406\u8bc4\u4f30\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "SAGE-32B\u5c55\u793a\u4e86\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u63a8\u7406\u4f18\u5316\u7684\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u84b8\u998f\u548c\u9006\u5411\u63a8\u7406\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u6a21\u578b\u6743\u91cd\u5df2\u516c\u5f00\u3002", "topic": "agent analysis"}}
{"id": "2601.04886", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04886", "abs": "https://arxiv.org/abs/2601.04886", "authors": ["Jingzhi Gong", "Giovanni Pinna", "Yixin Bian", "Jie M. Zhang"], "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests", "comment": null, "summary": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86AI\u4ee3\u7801\u52a9\u624b\u751f\u6210\u7684Pull Request\u63cf\u8ff0\u4e0e\u5b9e\u9645\u4ee3\u7801\u53d8\u66f4\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u5b58\u5728\u63cf\u8ff0-\u4ee3\u7801\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5bfc\u81f4PR\u63a5\u53d7\u7387\u964d\u4f4e51.7%\uff0c\u5408\u5e76\u65f6\u95f4\u5ef6\u957f3.5\u500d\u3002", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684PR\u63cf\u8ff0\u662f\u5411\u4eba\u7c7b\u8bc4\u5ba1\u8005\u4f20\u8fbe\u4ee3\u7801\u53d8\u66f4\u7684\u4e3b\u8981\u6e20\u9053\uff0c\u4f46\u8fd9\u4e9b\u63cf\u8ff0\u4e0e\u5b9e\u9645\u53d8\u66f4\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9AI\u4ee3\u7406\u53ef\u4fe1\u5ea6\u7684\u62c5\u5fe7\u3002", "method": "\u4f7f\u7528PR\u6d88\u606f-\u4ee3\u7801\u4e0d\u4e00\u81f4\u6027(PR-MCI)\u65b9\u6cd5\u5206\u6790\u4e86\u4e94\u4e2aAI\u4ee3\u7406\u751f\u6210\u768423,247\u4e2aPR\uff0c\u624b\u52a8\u6807\u6ce8\u4e86974\u4e2aPR\uff0c\u8bc6\u522b\u51fa\u516b\u79cdPR-MCI\u7c7b\u578b\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u53d1\u73b0406\u4e2aPR(1.7%)\u5b58\u5728\u9ad8PR-MCI\u95ee\u9898\uff0c\u5176\u4e2d\u63cf\u8ff0\u58f0\u79f0\u672a\u5b9e\u73b0\u53d8\u66f4\u662f\u6700\u5e38\u89c1\u95ee\u9898(45.4%)\u3002\u9ad8MCI\u7684PR\u63a5\u53d7\u7387\u964d\u4f4e51.7%(28.3% vs 80.0%)\uff0c\u5408\u5e76\u65f6\u95f4\u5ef6\u957f3.5\u500d(55.8 vs 16.0\u5c0f\u65f6)\u3002", "conclusion": "\u4e0d\u53ef\u9760\u7684PR\u63cf\u8ff0\u4f1a\u524a\u5f31\u5bf9AI\u4ee3\u7406\u7684\u4fe1\u4efb\uff0c\u9700\u8981\u5efa\u7acbPR-MCI\u9a8c\u8bc1\u673a\u5236\u548c\u6539\u8fdbPR\u751f\u6210\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u4eba\u673a\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2601.04203", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.04203", "abs": "https://arxiv.org/abs/2601.04203", "authors": ["Xueqing Wu", "Zihan Xue", "Da Yin", "Shuyan Zhou", "Kai-Wei Chang", "Nanyun Peng", "Yeming Wen"], "title": "FronTalk: Benchmarking Front-End Development as Conversational Code Generation with Multi-Modal Feedback", "comment": null, "summary": "We present FronTalk, a benchmark for front-end code generation that pioneers the study of a unique interaction dynamic: conversational code generation with multi-modal feedback. In front-end development, visual artifacts such as sketches, mockups and annotated creenshots are essential for conveying design intent, yet their role in multi-turn code generation remains largely unexplored. To address this gap, we focus on the front-end development task and curate FronTalk, a collection of 100 multi-turn dialogues derived from real-world websites across diverse domains such as news, finance, and art. Each turn features both a textual instruction and an equivalent visual instruction, each representing the same user intent. To comprehensively evaluate model performance, we propose a novel agent-based evaluation framework leveraging a web agent to simulate users and explore the website, and thus measuring both functional correctness and user experience. Evaluation of 20 models reveals two key challenges that are under-explored systematically in the literature: (1) a significant forgetting issue where models overwrite previously implemented features, resulting in task failures, and (2) a persistent challenge in interpreting visual feedback, especially for open-source vision-language models (VLMs). We propose a strong baseline to tackle the forgetting issue with AceCoder, a method that critiques the implementation of every past instruction using an autonomous web agent. This approach significantly reduces forgetting to nearly zero and improves the performance by up to 9.3% (56.0% to 65.3%). Overall, we aim to provide a solid foundation for future research in front-end development and the general interaction dynamics of multi-turn, multi-modal code generation. Code and data are released at https://github.com/shirley-wu/frontalk", "AI": {"tldr": "FronTalk\u662f\u4e00\u4e2a\u524d\u7aef\u4ee3\u7801\u751f\u6210\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u591a\u6a21\u6001\u53cd\u9988\u7684\u5bf9\u8bdd\u5f0f\u4ee3\u7801\u751f\u6210\uff0c\u5305\u542b100\u4e2a\u771f\u5b9e\u7f51\u7ad9\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u63d0\u51fa\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u9057\u5fd8\u95ee\u9898\u548c\u89c6\u89c9\u53cd\u9988\u7406\u89e3\u6311\u6218\uff0c\u5e76\u63d0\u51faAceCoder\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u9057\u5fd8\u3002", "motivation": "\u524d\u7aef\u5f00\u53d1\u4e2d\u89c6\u89c9\u5de5\u4ef6\uff08\u5982\u8349\u56fe\u3001\u7ebf\u6846\u56fe\u3001\u6807\u6ce8\u622a\u56fe\uff09\u5bf9\u4e8e\u4f20\u8fbe\u8bbe\u8ba1\u610f\u56fe\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u591a\u8f6e\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u53cd\u9988\u5728\u5bf9\u8bdd\u5f0f\u4ee3\u7801\u751f\u6210\u4e2d\u4ea4\u4e92\u52a8\u6001\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u6784\u5efaFronTalk\u57fa\u51c6\uff0c\u5305\u542b100\u4e2a\u4ece\u771f\u5b9e\u7f51\u7ad9\u63d0\u53d6\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6bcf\u8f6e\u540c\u65f6\u5305\u542b\u6587\u672c\u6307\u4ee4\u548c\u7b49\u6548\u89c6\u89c9\u6307\u4ee4\u3002\u63d0\u51fa\u57fa\u4e8e\u4ee3\u7406\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528Web\u4ee3\u7406\u6a21\u62df\u7528\u6237\u63a2\u7d22\u7f51\u7ad9\uff0c\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002\u9488\u5bf9\u53d1\u73b0\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51faAceCoder\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e3bWeb\u4ee3\u7406\u6279\u5224\u6027\u5206\u6790\u8fc7\u5f80\u6307\u4ee4\u5b9e\u73b0\u3002", "result": "\u8bc4\u4f3020\u4e2a\u6a21\u578b\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u663e\u8457\u9057\u5fd8\u95ee\u9898\uff0c\u6a21\u578b\u4f1a\u8986\u76d6\u5148\u524d\u5b9e\u73b0\u7684\u529f\u80fd\uff1b2\uff09\u89c6\u89c9\u53cd\u9988\u7406\u89e3\u56f0\u96be\uff0c\u7279\u522b\u662f\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002AceCoder\u65b9\u6cd5\u5c06\u9057\u5fd8\u7387\u964d\u81f3\u63a5\u8fd1\u96f6\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe9.3%\uff08\u4ece56.0%\u523065.3%\uff09\u3002", "conclusion": "FronTalk\u4e3a\u524d\u7aef\u5f00\u53d1\u548c\u591a\u8f6e\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u7684\u4ea4\u4e92\u52a8\u6001\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "swe benchmark"}}
{"id": "2601.04208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04208", "abs": "https://arxiv.org/abs/2601.04208", "authors": ["Xiang Cheng", "Wen Wang", "Anindya Ghose"], "title": "LLMs for Explainable Business Decision-Making: A Reinforcement Learning Fine-Tuning Approach", "comment": null, "summary": "Artificial Intelligence (AI) models increasingly drive high-stakes consumer interactions, yet their decision logic often remains opaque. Prevailing explainable AI techniques rely on post hoc numerical feature attributions, which fail to provide coherent narratives behind model decisions. Large language models (LLMs) present an opportunity to generate natural-language explanations, but three design challenges remain unresolved: explanations must be both decision-correct and faithful to the factors that drive the prediction; they should be able to serve multiple audiences without shifting the underlying decision rule; and they should be trained in a label-efficient way that does not depend on large corpora of human-scored explanations. To address these challenges, we introduce LEXMA (LLM-based EXplanations for Multi-Audience decisions), a reinforcement-learning-based fine-tuning framework that produces narrative-driven, audience-appropriate explanations. LEXMA combines reflection-augmented supervised fine-tuning with two stages of Group Relative Policy Optimization (GRPO). Specifically, it fine-tunes two separate parameter sets to improve decision correctness and satisfy stylistic requirements for different audiences, using reward signals that do not rely on human-annotated explanations. We instantiate LEXMA in the context of mortgage approval decisions. Results demonstrate that LEXMA yields significant improvements in predictive performance compared with other LLM baselines. Moreover, human evaluations show that expert-facing explanations generated by our approach are more risk-focused, and consumer-facing explanations are clearer, more actionable, and more polite. Our study contributes a cost-efficient, systematic LLM fine-tuning approach to enhance explanation quality for business decisions, offering strong potential for scalable deployment of transparent AI systems.", "AI": {"tldr": "LEXMA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9762\u5411\u591a\u53d7\u4f17\u7684\u51b3\u7b56\u89e3\u91ca\uff0c\u5728\u62b5\u62bc\u8d37\u6b3e\u5ba1\u6279\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u66f4\u5408\u9002\u7684\u89e3\u91ca\u98ce\u683c\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u6d88\u8d39\u8005\u51b3\u7b56\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u53ef\u89e3\u91caAI\u6280\u672f\uff08\u5982\u540e\u9a8c\u6570\u503c\u7279\u5f81\u5f52\u56e0\uff09\u65e0\u6cd5\u63d0\u4f9b\u8fde\u8d2f\u7684\u53d9\u4e8b\u89e3\u91ca\u3002LLM\u867d\u80fd\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4f46\u4ecd\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u89e3\u91ca\u9700\u540c\u65f6\u4fdd\u8bc1\u51b3\u7b56\u6b63\u786e\u6027\u548c\u5fe0\u5b9e\u6027\uff1b\u9700\u670d\u52a1\u4e0d\u540c\u53d7\u4f17\u800c\u4e0d\u6539\u53d8\u51b3\u7b56\u89c4\u5219\uff1b\u9700\u4ee5\u6807\u7b7e\u9ad8\u6548\u7684\u65b9\u5f0f\u8bad\u7ec3\uff0c\u4e0d\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u7684\u89e3\u91ca\u6570\u636e\u3002", "method": "LEXMA\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u601d\u589e\u5f3a\u7684\u76d1\u7763\u5fae\u8c03\u548c\u4e24\u9636\u6bb5\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u5fae\u8c03\u4e24\u4e2a\u72ec\u7acb\u7684\u53c2\u6570\u96c6\u5206\u522b\u63d0\u5347\u51b3\u7b56\u6b63\u786e\u6027\u548c\u6ee1\u8db3\u4e0d\u540c\u53d7\u4f17\u7684\u98ce\u683c\u8981\u6c42\uff1b2\uff09\u4f7f\u7528\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u89e3\u91ca\u7684\u5956\u52b1\u4fe1\u53f7\uff1b3\uff09\u5728\u62b5\u62bc\u8d37\u6b3e\u5ba1\u6279\u51b3\u7b56\u573a\u666f\u4e2d\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\u3002", "result": "LEXMA\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6LLM\u57fa\u7ebf\u3002\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\uff1a\u9762\u5411\u4e13\u5bb6\u7684\u89e3\u91ca\u66f4\u5177\u98ce\u9669\u805a\u7126\u6027\uff1b\u9762\u5411\u6d88\u8d39\u8005\u7684\u89e3\u91ca\u66f4\u6e05\u6670\u3001\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u3001\u66f4\u793c\u8c8c\u3002", "conclusion": "LEXMA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6210\u672c\u9ad8\u6548\u3001\u7cfb\u7edf\u5316\u7684LLM\u5fae\u8c03\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u5546\u4e1a\u51b3\u7b56\u89e3\u91ca\u7684\u8d28\u91cf\uff0c\u4e3a\u900f\u660eAI\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u5f3a\u5927\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04387", "categories": ["cs.AI", "cs.CL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2601.04387", "abs": "https://arxiv.org/abs/2601.04387", "authors": ["Stuti Sinha", "Himanshu Kumar", "Aryan Raju Mandapati", "Rakshit Sakhuja", "Dhruv Kumar"], "title": "The Language of Bargaining: Linguistic Effects in LLM Negotiations", "comment": "Under Review", "summary": "Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u9009\u62e9\u5bf9LLM\u8c08\u5224\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5728\u5370\u5ea6\u8bed\u8a00\u73af\u5883\u4e0b\u8c08\u5224\u7ed3\u679c\u4e0e\u82f1\u8bed\u73af\u5883\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u8b66\u793a\u4ec5\u7528\u82f1\u8bed\u8bc4\u4f30LLM\u8c08\u5224\u4f1a\u5f97\u51fa\u4e0d\u5b8c\u6574\u751a\u81f3\u8bef\u5bfc\u6027\u7ed3\u8bba\u3002", "motivation": "\u5f53\u524dLLM\u8c08\u5224\u7814\u7a76\u51e0\u4e4e\u5168\u90e8\u5728\u82f1\u8bed\u73af\u5883\u4e0b\u8bc4\u4f30\uff0c\u4f46\u8bed\u8a00\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u793e\u4f1a\u667a\u80fd\u8868\u73b0\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u8bed\u8a00\u9009\u62e9\u5bf9\u8c08\u5224\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Ultimatum\u3001Buy-Sell\u548cResource Exchange\u4e09\u79cd\u535a\u5f08\u6e38\u620f\uff0c\u5728\u82f1\u8bed\u548c\u56db\u79cd\u5370\u5ea6\u8bed\u8a00\uff08\u5370\u5730\u8bed\u3001\u65c1\u906e\u666e\u8bed\u3001\u53e4\u5409\u62c9\u7279\u8bed\u3001\u9a6c\u5c14\u74e6\u8fea\u8bed\uff09\u73af\u5883\u4e0b\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u4fdd\u6301\u6e38\u620f\u89c4\u5219\u3001\u6a21\u578b\u53c2\u6570\u548c\u6fc0\u52b1\u6761\u4ef6\u4e00\u81f4\u3002", "result": "\u8bed\u8a00\u9009\u62e9\u6bd4\u66f4\u6362\u6a21\u578b\u5bf9\u7ed3\u679c\u5f71\u54cd\u66f4\u5927\uff1a\u5370\u5ea6\u8bed\u8a00\u5728\u5206\u914d\u578b\u535a\u5f08\u4e2d\u964d\u4f4e\u7a33\u5b9a\u6027\uff0c\u5728\u6574\u5408\u578b\u73af\u5883\u4e2d\u4fc3\u8fdb\u66f4\u4e30\u5bcc\u7684\u63a2\u7d22\uff1b\u8bed\u8a00\u9009\u62e9\u80fd\u9006\u8f6c\u63d0\u8bae\u8005\u4f18\u52bf\u5e76\u91cd\u65b0\u5206\u914d\u5269\u4f59\u4ef7\u503c\u3002", "conclusion": "\u4ec5\u7528\u82f1\u8bed\u8bc4\u4f30LLM\u8c08\u5224\u4f1a\u5f97\u51fa\u4e0d\u5b8c\u6574\u4e14\u53ef\u80fd\u8bef\u5bfc\u7684\u7ed3\u8bba\uff0c\u9700\u8981\u8fdb\u884c\u6587\u5316\u611f\u77e5\u7684\u8bc4\u4f30\u4ee5\u786e\u4fdd\u516c\u5e73\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2601.04365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04365", "abs": "https://arxiv.org/abs/2601.04365", "authors": ["Anton Roupassov-Ruiz", "Yiyang Zuo"], "title": "Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning", "comment": null, "summary": "In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.", "AI": {"tldr": "\u7a0b\u5e8f\u5316\u7b56\u7565\uff08PERL\uff09\u5728\u4eba\u5de5\u751f\u547d\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u6bd4\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\uff08NERL\uff09\u8868\u73b0\u66f4\u597d\uff0c\u5e73\u5747\u591a\u5b58\u6d3b201.69\u6b65\uff0c\u5373\u4f7f\u4ec5\u4f7f\u7528\u5b66\u4e60\uff08\u65e0\u8fdb\u5316\uff09\u4e5f\u6bd4\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u65b9\u6cd5\u591a\u5b58\u6d3b73.67\u6b65\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u7f3a\u4e4f\u663e\u5f0f\u6a21\u5757\u5316\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u884c\u4e3a\u89e3\u91ca\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7a0b\u5e8f\u5316\u7b56\u7565\u80fd\u5426\u5728\u6027\u80fd\u4e0a\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u8f6f\u53ef\u5fae\u51b3\u7b56\u5217\u8868\uff08SDDL\uff09\u5b9e\u73b0\u7a0b\u5e8f\u5316\u7b56\u7565\uff0c\u5728\u5b8c\u5168\u5f00\u6e90\u590d\u73b0\u76841992\u5e74\u4eba\u5de5\u751f\u547d\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u901a\u8fc74000\u6b21\u72ec\u7acb\u8bd5\u9a8c\u8fdb\u884c\u4e25\u683c\u7684\u751f\u5b58\u5206\u6790\uff0c\u4f7f\u7528Kaplan-Meier\u66f2\u7ebf\u548c\u9650\u5236\u5e73\u5747\u751f\u5b58\u65f6\u95f4\uff08RMST\uff09\u6307\u6807\u3002", "result": "PERL\u548cNERL\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u7684\u751f\u5b58\u6982\u7387\u5dee\u5f02\u3002PERL\u4ee3\u7406\u5e73\u5747\u6bd4NERL\u4ee3\u7406\u591a\u5b58\u6d3b201.69\u6b65\u3002\u4ec5\u4f7f\u7528\u5b66\u4e60\u7684SDDL\u4ee3\u7406\u6bd4\u4f7f\u7528\u5b66\u4e60\u548c\u8fdb\u5316\u7684\u795e\u7ecf\u4ee3\u7406\u5e73\u5747\u591a\u5b58\u6d3b73.67\u6b65\u3002", "conclusion": "\u7a0b\u5e8f\u5316\u7b56\u7565\u5728\u4eba\u5de5\u751f\u547d\u73af\u5883\u4e2d\u80fd\u591f\u8d85\u8d8a\u795e\u7ecf\u7f51\u7edc\u7b56\u7565\u7684\u751f\u5b58\u6027\u80fd\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04390", "abs": "https://arxiv.org/abs/2601.04390", "authors": ["Siyuan Huang", "Yutong Gao", "Juyang Bai", "Yifan Zhou", "Zi Yin", "Xinxin Liu", "Rama Chellappa", "Chun Pong Lau", "Sayan Nag", "Cheng Peng", "Shraman Pramanick"], "title": "SciFig: Towards Automating Scientific Figure Generation", "comment": null, "summary": "Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\\%$ overall quality on dataset-level evaluation and 66.2$\\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.", "AI": {"tldr": "SciFig\u662f\u4e00\u4e2a\u7aef\u5230\u7aefAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u7814\u7a76\u8bba\u6587\u6587\u672c\u751f\u6210\u53ef\u76f4\u63a5\u53d1\u8868\u7684\u6d41\u7a0b\u56fe\uff0c\u901a\u8fc7\u5206\u5c42\u5e03\u5c40\u751f\u6210\u548c\u8fed\u4ee3\u601d\u7ef4\u94fe\u53cd\u9988\u673a\u5236\u5b9e\u73b0\u9ad8\u8d28\u91cf\u79d1\u5b66\u56fe\u8868\u751f\u6210\u3002", "motivation": "\u6bcf\u5e74\u6709\u8d85\u8fc7250\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\u53d1\u8868\uff0c\u4f46\u56fe\u8868\u751f\u6210\u8fc7\u7a0b\u4ecd\u7136\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u8fd9\u9700\u8981\u6df1\u539a\u7684\u9886\u57df\u77e5\u8bc6\u548c\u4e13\u4e1a\u8bbe\u8ba1\u6280\u80fd\uff0c\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "1. \u4f7f\u7528\u5206\u5c42\u5e03\u5c40\u751f\u6210\u7b56\u7565\uff1a\u89e3\u6790\u7814\u7a76\u63cf\u8ff0\u4ee5\u8bc6\u522b\u7ec4\u4ef6\u5173\u7cfb\uff0c\u5c06\u76f8\u5173\u5143\u7d20\u5206\u7ec4\u4e3a\u529f\u80fd\u6a21\u5757\uff0c\u751f\u6210\u6a21\u5757\u95f4\u8fde\u63a5\u4ee5\u5efa\u7acb\u89c6\u89c9\u7ec4\u7ec7\u30022. \u91c7\u7528\u8fed\u4ee3\u601d\u7ef4\u94fe\u53cd\u9988\u673a\u5236\uff1a\u901a\u8fc7\u591a\u8f6e\u89c6\u89c9\u5206\u6790\u548c\u63a8\u7406\u9010\u6b65\u6539\u8fdb\u5e03\u5c40\u30023. \u5f15\u5165\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u5206\u67902,219\u4e2a\u771f\u5b9e\u79d1\u5b66\u56fe\u8868\u63d0\u53d6\u8bc4\u4f30\u91cf\u89c4\u5e76\u81ea\u52a8\u751f\u6210\u7efc\u5408\u8bc4\u4f30\u6807\u51c6\u3002", "result": "SciFig\u8868\u73b0\u51fa\u8272\uff1a\u5728\u6570\u636e\u96c6\u7ea7\u8bc4\u4f30\u4e2d\u8fbe\u523070.1%\u7684\u6574\u4f53\u8d28\u91cf\uff0c\u5728\u8bba\u6587\u7279\u5b9a\u8bc4\u4f30\u4e2d\u8fbe\u523066.2%\uff0c\u5728\u89c6\u89c9\u6e05\u6670\u5ea6\u3001\u7ed3\u6784\u7ec4\u7ec7\u548c\u79d1\u5b66\u51c6\u786e\u6027\u7b49\u6307\u6807\u4e0a\u59cb\u7ec8\u83b7\u5f97\u9ad8\u5206\u3002", "conclusion": "SciFig\u80fd\u591f\u4ece\u7814\u7a76\u8bba\u6587\u6587\u672c\u81ea\u52a8\u751f\u6210\u53ef\u76f4\u63a5\u53d1\u8868\u7684\u6d41\u7a0b\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79d1\u5b66\u56fe\u8868\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5176\u751f\u6210\u6d41\u7a0b\u548c\u8bc4\u4f30\u57fa\u51c6\u5c06\u5f00\u6e90\u3002", "topic": "code agent"}}
{"id": "2601.04392", "categories": ["cs.LG", "cs.AI", "cs.RO", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.04392", "abs": "https://arxiv.org/abs/2601.04392", "authors": ["Mohsen Jalaeian-Farimani"], "title": "Enhanced-FQL($\u03bb$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay", "comment": "Submitted to ECC26 conference", "summary": "This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($\u03bb$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($\u03bb$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($\u03bb$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.", "AI": {"tldr": "\u63d0\u51faEnhanced-FQL(\u03bb)\u6a21\u7cca\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u6a21\u7cca\u8d44\u683c\u8ff9\u548c\u5206\u6bb5\u7ecf\u9a8c\u56de\u653e\uff0c\u7528\u4e8e\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u9488\u5bf9\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\uff0c\u9700\u8981\u65e2\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u53c8\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u597d\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u8ba1\u7b97\u590d\u6742\uff0c\u800c\u73b0\u6709\u6a21\u7cca\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEnhanced-FQL(\u03bb)\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1) \u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u6a21\u7cca\u89c4\u5219\u5e93\u66ff\u4ee3\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff1b2) \u5f15\u5165\u6a21\u7cca\u8d44\u683c\u8ff9(FET)\u8fdb\u884c\u7a33\u5b9a\u7684\u591a\u6b65\u4fe1\u7528\u5206\u914d\uff1b3) \u91c7\u7528\u5206\u6bb5\u7ecf\u9a8c\u56de\u653e(SER)\u673a\u5236\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff1b4) \u57fa\u4e8e\u6a21\u7cca\u8d1d\u5c14\u66fc\u65b9\u7a0b(FBE)\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u65b9\u6cd5\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\u6536\u655b\u3002\u5728\u8fde\u7eed\u63a7\u5236\u9886\u57df\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cEnhanced-FQL(\u03bb)\u76f8\u6bd4n\u6b65\u6a21\u7ccaTD\u548c\u6a21\u7ccaSARSA(\u03bb)\u57fa\u7ebf\u5177\u6709\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u548c\u66f4\u4f4e\u7684\u65b9\u5dee\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u8fdc\u4f4e\u4e8eDDPG\u7b49\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "Enhanced-FQL(\u03bb)\u6846\u67b6\u7ed3\u5408\u4e86\u56fa\u6709\u7684\u53ef\u89e3\u91ca\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5bf9\u900f\u660e\u5ea6\u548c\u8d44\u6e90\u7ea6\u675f\u6709\u4e25\u683c\u8981\u6c42\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04426", "abs": "https://arxiv.org/abs/2601.04426", "authors": ["Linzhang Li", "Yixin Dong", "Guanjie Wang", "Ziyi Xu", "Alexander Jiang", "Tianqi Chen"], "title": "XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs", "comment": null, "summary": "Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.", "AI": {"tldr": "XGrammar 2\u662f\u4e00\u4e2a\u4e3a\u667a\u80fdLLM\u4ee3\u7406\u4f18\u5316\u7684\u7ed3\u6784\u5316\u751f\u6210\u5f15\u64ce\uff0c\u901a\u8fc7TagDispatch\u8bed\u4e49\u3001JIT\u7f16\u8bd1\u548c\u8de8\u8bed\u6cd5\u7f13\u5b58\u7b49\u6280\u672f\uff0c\u5728\u52a8\u6001\u7ed3\u6784\u5316\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b06\u500d\u4ee5\u4e0a\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3LLM\u4ee3\u7406\u9700\u8981\u5904\u7406\u65e5\u76ca\u590d\u6742\u7684\u7ed3\u6784\u5316\u751f\u6210\u4efb\u52a1\uff08\u5982\u5de5\u5177\u8c03\u7528\u548c\u6761\u4ef6\u7ed3\u6784\u5316\u751f\u6210\uff09\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6bd4\u9884\u5b9a\u4e49\u7ed3\u6784\u66f4\u52a0\u52a8\u6001\uff0c\u5bf9\u73b0\u6709\u7ed3\u6784\u5316\u751f\u6210\u5f15\u64ce\u63d0\u51fa\u4e86\u65b0\u6311\u6218\u3002", "method": "\u63d0\u51faTagDispatch\u52a8\u6001\u8c03\u5ea6\u8bed\u4e49\u52a0\u901f\u63a9\u7801\u751f\u6210\uff1b\u5f15\u5165JIT\u7f16\u8bd1\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4\uff1b\u8bbe\u8ba1\u8de8\u8bed\u6cd5\u7f13\u5b58\u673a\u5236\u5229\u7528\u4e0d\u540c\u8bed\u6cd5\u95f4\u7684\u516c\u5171\u5b50\u7ed3\u6784\uff1b\u6269\u5c55PDA\u63a9\u7801\u751f\u6210\u7b97\u6cd5\u4e3aEarley\u89e3\u6790\u5668\uff1b\u8bbe\u8ba1\u91cd\u590d\u538b\u7f29\u7b97\u6cd5\u5904\u7406\u8bed\u6cd5\u4e2d\u7684\u91cd\u590d\u7ed3\u6784\u3002", "result": "XGrammar 2\u76f8\u6bd4\u73b0\u6709\u7ed3\u6784\u5316\u751f\u6210\u5f15\u64ce\u53ef\u5b9e\u73b06\u500d\u4ee5\u4e0a\u52a0\u901f\uff0c\u4e0eLLM\u63a8\u7406\u5f15\u64ce\u96c6\u6210\u540e\uff0c\u5904\u7406\u52a8\u6001\u7ed3\u6784\u5316\u751f\u6210\u4efb\u52a1\u65f6\u51e0\u4e4e\u65e0\u989d\u5916\u5f00\u9500\u3002", "conclusion": "XGrammar 2\u662f\u4e00\u4e2a\u9ad8\u6548\u4f18\u5316\u7684\u7ed3\u6784\u5316\u751f\u6210\u5f15\u64ce\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406LLM\u4ee3\u7406\u9762\u4e34\u7684\u52a8\u6001\u7ed3\u6784\u5316\u751f\u6210\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.04411", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04411", "abs": "https://arxiv.org/abs/2601.04411", "authors": ["Ali Rad", "Khashayar Filom", "Darioush Keivan", "Peyman Mohajerin Esfahani", "Ehsan Kamalinejad"], "title": "Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?\n  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time (\"rate, not fate\"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff09\u6846\u67b6\uff0c\u5206\u6790\u9a8c\u8bc1\u566a\u58f0\u5bf9LLM\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u53d1\u73b0Youden\u6307\u6570J=TPR-FPR\u51b3\u5b9a\u5b66\u4e60\u6210\u8d25\uff1aJ>0\u65f6\u5b66\u4e60\u6210\u529f\uff0cJ=0\u65f6\u4e2d\u6027\uff0cJ<0\u65f6\u53cd\u5b66\u4e60\u5d29\u6e83\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u9a8c\u8bc1\u5668\uff08\u5355\u5143\u6d4b\u8bd5\u3001\u4eba\u5de5\u6807\u6ce8\u3001LLM\u8bc4\u5224\uff09\u90fd\u5b58\u5728\u566a\u58f0\uff0c\u7279\u522b\u662f\u5728\u7f16\u7a0b\u7b49\u590d\u6742\u9886\u57df\u3002\u9700\u8981\u7814\u7a76\u9a8c\u8bc1\u566a\u58f0\u662f\u4ec5\u4ec5\u51cf\u7f13\u5b66\u4e60\u901f\u5ea6\uff0c\u8fd8\u662f\u53ef\u80fd\u5b8c\u5168\u6539\u53d8\u5b66\u4e60\u7ed3\u679c\u3002", "method": "\u5efa\u7acb\u591a\u81c2\u8001\u864e\u673a\u7684\u5206\u6790\u6846\u67b6\u6a21\u62dfRLVR\u52a8\u6001\uff0c\u4f7f\u7528GRPO\u65b9\u6cd5\uff0c\u5efa\u6a21\u5047\u9633\u6027\u548c\u5047\u9634\u6027\uff0c\u5c06\u8865\u5168\u5206\u7ec4\u4e3a\u91cd\u590d\u63a8\u7406\u6a21\u5f0f\uff0c\u5f97\u5230\u590d\u5236\u5b50\u5f0f\uff08\u81ea\u7136\u9009\u62e9\uff09\u6d41\u3002\u5206\u6790Youden\u6307\u6570J=TPR-FPR\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5c16\u9510\u7684\u76f8\u53d8\uff1aJ>0\u65f6\u9519\u8bef\u6a21\u5f0f\u88ab\u6dd8\u6c70\uff08\u5b66\u4e60\u6210\u529f\uff09\uff1bJ=0\u65f6\u4e2d\u6027\uff1bJ<0\u65f6\u9519\u8bef\u6a21\u5f0f\u653e\u5927\u76f4\u81f3\u4e3b\u5bfc\uff08\u53cd\u5b66\u4e60\u548c\u5d29\u6e83\uff09\u3002\u5728J>0\u7684\u5b66\u4e60\u673a\u5236\u4e2d\uff0c\u566a\u58f0\u4e3b\u8981\u5f71\u54cd\u6536\u655b\u65f6\u95f4\u800c\u975e\u7ed3\u679c\u3002", "conclusion": "\u9a8c\u8bc1\u566a\u58f0\u7684\u5f71\u54cd\u7531Youden\u6307\u6570J\u51b3\u5b9a\uff0cJ>0\u65f6\u566a\u58f0\u4e3b\u8981\u5f71\u54cd\u5b66\u4e60\u901f\u5ea6\u800c\u975e\u7ed3\u679c\u3002\u8be5\u6846\u67b6\u4e3a\u5206\u6790RLVR\u7a33\u5b9a\u6027\u3001\u6536\u655b\u6027\u548c\u7b97\u6cd5\u5e72\u9884\u63d0\u4f9b\u901a\u7528\u89c6\u89d2\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04424", "abs": "https://arxiv.org/abs/2601.04424", "authors": ["Yao Dou", "Wei Xu"], "title": "Gavel: Agent Meets Checklist for Evaluating LLMs on Long-Context Legal Summarization", "comment": "webpage at https://yao-dou.github.io/gavel/", "summary": "Large language models (LLMs) now support contexts of up to 1M tokens, but their effectiveness on complex long-context tasks remains unclear. In this paper, we study multi-document legal case summarization, where a single case often spans many documents totaling 100K-500K tokens. We introduce Gavel-Ref, a reference-based evaluation framework with multi-value checklist evaluation over 26 items, as well as residual fact and writing-style evaluations. Using Gavel-Ref, we go beyond the single aggregate scores reported in prior work and systematically evaluate 12 frontier LLMs on 100 legal cases ranging from 32K to 512K tokens, primarily from 2025. Our results show that even the strongest model, Gemini 2.5 Pro, achieves only around 50 of $S_{\\text{Gavel-Ref}}$, highlighting the difficulty of the task. Models perform well on simple checklist items (e.g., filing date) but struggle on multi-value or rare ones such as settlements and monitor reports. As LLMs continue to improve and may surpass human-written summaries -- making human references less reliable -- we develop Gavel-Agent, an efficient and autonomous agent scaffold that equips LLMs with six tools to navigate and extract checklists directly from case documents. With Qwen3, Gavel-Agent reduces token usage by 36% while resulting in only a 7% drop in $S_{\\text{checklist}}$ compared to end-to-end extraction with GPT-4.1.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGavel-Ref\u8bc4\u4f30\u6846\u67b6\u548cGavel-Agent\u4ee3\u7406\u652f\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u8d85\u957f\u4e0a\u4e0b\u6587\uff08100K-500K tokens\uff09\u591a\u6587\u6863\u6cd5\u5f8b\u6848\u4f8b\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5f3a\u6a21\u578b\u4e5f\u4ec5\u8fbe\u523050%\u5206\u6570\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u5de5\u5177\u8f85\u52a9\u7684\u4ee3\u7406\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1LLM\u73b0\u5728\u652f\u6301\u9ad8\u8fbe1M tokens\u7684\u4e0a\u4e0b\u6587\uff0c\u4f46\u5b83\u4eec\u5728\u590d\u6742\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\uff08\u5982\u591a\u6587\u6863\u6cd5\u5f8b\u6848\u4f8b\u6458\u8981\uff09\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u4ecd\u4e0d\u6e05\u695a\u3002\u6cd5\u5f8b\u6848\u4f8b\u901a\u5e38\u8de8\u8d8a\u591a\u4e2a\u6587\u6863\uff0c\u603b\u8ba1100K-500K tokens\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1) \u63d0\u51faGavel-Ref\u8bc4\u4f30\u6846\u67b6\uff1a\u57fa\u4e8e26\u9879\u591a\u503c\u68c0\u67e5\u8868\u7684\u53c2\u8003\u8bc4\u4f30\uff0c\u5305\u62ec\u5269\u4f59\u4e8b\u5b9e\u548c\u5199\u4f5c\u98ce\u683c\u8bc4\u4f30\uff1b2) \u7cfb\u7edf\u8bc4\u4f3012\u4e2a\u524d\u6cbfLLM\u5728100\u4e2a\u6cd5\u5f8b\u6848\u4f8b\uff0832K-512K tokens\uff09\u4e0a\u7684\u8868\u73b0\uff1b3) \u5f00\u53d1Gavel-Agent\u4ee3\u7406\u652f\u67b6\uff1a\u4e3aLLM\u914d\u5907\u516d\u4e2a\u5de5\u5177\uff0c\u76f4\u63a5\u4ece\u6848\u4f8b\u6587\u6863\u4e2d\u5bfc\u822a\u548c\u63d0\u53d6\u68c0\u67e5\u8868\u9879\u3002", "result": "1) \u5373\u4f7f\u6700\u5f3a\u6a21\u578bGemini 2.5 Pro\u5728Gavel-Ref\u8bc4\u5206\u4e2d\u4ec5\u8fbe\u5230\u7ea650\u5206\uff0c\u663e\u793a\u4efb\u52a1\u96be\u5ea6\uff1b2) \u6a21\u578b\u5728\u7b80\u5355\u68c0\u67e5\u8868\u9879\uff08\u5982\u63d0\u4ea4\u65e5\u671f\uff09\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u503c\u6216\u7f55\u89c1\u9879\uff08\u5982\u548c\u89e3\u534f\u8bae\u3001\u76d1\u7763\u62a5\u544a\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b3) Gavel-Agent\u4f7f\u7528Qwen3\u65f6\uff0c\u76f8\u6bd4GPT-4.1\u7aef\u5230\u7aef\u63d0\u53d6\uff0ctoken\u4f7f\u7528\u51cf\u5c1136%\uff0c\u68c0\u67e5\u8868\u5206\u6570\u4ec5\u4e0b\u964d7%\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u8d85\u957f\u4e0a\u4e0b\u6587\u6cd5\u5f8b\u6848\u4f8b\u6458\u8981\u4efb\u52a1\u4e2d\u4ecd\u6709\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u4e13\u4e1a\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u5de5\u5177\u8f85\u52a9\u3002Gavel-Agent\u5c55\u793a\u4e86\u901a\u8fc7\u5de5\u5177\u589e\u5f3aLLM\u5728\u957f\u6587\u6863\u5904\u7406\u4e2d\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u4e3a\u672a\u6765LLM\u5728\u590d\u6742\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.04435", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.04435", "abs": "https://arxiv.org/abs/2601.04435", "authors": ["Myra Cheng", "Robert D. Hawkins", "Dan Jurafsky"], "title": "Accommodation and Epistemic Vigilance: A Pragmatic Account of Why LLMs Fail to Challenge Harmful Beliefs", "comment": null, "summary": "Large language models (LLMs) frequently fail to challenge users' harmful beliefs in domains ranging from medical advice to social reasoning. We argue that these failures can be understood and addressed pragmatically as consequences of LLMs defaulting to accommodating users' assumptions and exhibiting insufficient epistemic vigilance. We show that social and linguistic factors known to influence accommodation in humans (at-issueness, linguistic encoding, and source reliability) similarly affect accommodation in LLMs, explaining performance differences across three safety benchmarks that test models' ability to challenge harmful beliefs, spanning misinformation (Cancer-Myth, SAGE-Eval) and sycophancy (ELEPHANT). We further show that simple pragmatic interventions, such as adding the phrase \"wait a minute\", significantly improve performance on these benchmarks while preserving low false-positive rates. Our results highlight the importance of considering pragmatics for evaluating LLM behavior and improving LLM safety.", "AI": {"tldr": "LLMs\u7ecf\u5e38\u65e0\u6cd5\u6311\u6218\u7528\u6237\u7684\u6709\u5bb3\u4fe1\u5ff5\uff0c\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8eLLMs\u9ed8\u8ba4\u8fce\u5408\u7528\u6237\u5047\u8bbe\u4e14\u7f3a\u4e4f\u8ba4\u77e5\u8b66\u60d5\u3002\u901a\u8fc7\u7b80\u5355\u7684\u8bed\u7528\u5e72\u9884\uff08\u5982\u6dfb\u52a0\"\u7b49\u4e00\u4e0b\"\u77ed\u8bed\uff09\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLMs\u5728\u533b\u7597\u5efa\u8bae\u3001\u793e\u4f1a\u63a8\u7406\u7b49\u9886\u57df\u7ecf\u5e38\u65e0\u6cd5\u6311\u6218\u7528\u6237\u7684\u6709\u5bb3\u4fe1\u5ff5\uff0c\u8fd9\u6784\u6210\u4e86\u91cd\u8981\u7684\u5b89\u5168\u95ee\u9898\u3002\u7814\u7a76\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u5931\u8d25\u53ef\u4ee5\u4ece\u8bed\u7528\u89d2\u5ea6\u7406\u89e3\u4e3aLLMs\u9ed8\u8ba4\u8fce\u5408\u7528\u6237\u5047\u8bbe\u4e14\u7f3a\u4e4f\u8ba4\u77e5\u8b66\u60d5\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u5f71\u54cd\u4eba\u7c7b\u8fce\u5408\u884c\u4e3a\u7684\u793e\u4f1a\u548c\u8bed\u8a00\u56e0\u7d20\uff08\u8bae\u9898\u6027\u3001\u8bed\u8a00\u7f16\u7801\u3001\u6765\u6e90\u53ef\u9760\u6027\uff09\u5982\u4f55\u540c\u6837\u5f71\u54cdLLMs\u3002\u5728\u4e09\u4e2a\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff08Cancer-Myth\u3001SAGE-Eval\u3001ELEPHANT\uff09\u4e0a\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u7b80\u5355\u7684\u8bed\u7528\u5e72\u9884\u63aa\u65bd\uff0c\u5982\u5728\u63d0\u793a\u4e2d\u6dfb\u52a0\"\u7b49\u4e00\u4e0b\"\u7b49\u77ed\u8bed\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f71\u54cd\u4eba\u7c7b\u8fce\u5408\u884c\u4e3a\u7684\u56e0\u7d20\u540c\u6837\u5f71\u54cdLLMs\uff0c\u89e3\u91ca\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002\u7b80\u5355\u7684\u8bed\u7528\u5e72\u9884\u63aa\u65bd\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6311\u6218\u6709\u5bb3\u4fe1\u5ff5\u65b9\u9762\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u9519\u8bef\u7387\u3002", "conclusion": "\u8003\u8651\u8bed\u7528\u56e0\u7d20\u5bf9\u4e8e\u8bc4\u4f30LLM\u884c\u4e3a\u548c\u63d0\u5347LLM\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u7b80\u5355\u7684\u8bed\u7528\u5e72\u9884\u53ef\u4ee5\u6709\u6548\u5730\u6539\u5584LLMs\u5728\u6311\u6218\u6709\u5bb3\u4fe1\u5ff5\u65b9\u9762\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2601.04500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04500", "abs": "https://arxiv.org/abs/2601.04500", "authors": ["Yifei Gao", "Jiang Wu", "Xiaoyi Chen", "Yifan Yang", "Zhe Cui", "Tianyi Ma", "Jiaming Zhang", "Jitao Sang"], "title": "GUITester: Enabling GUI Agents for Exploratory Defect Discovery", "comment": null, "summary": "Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \\textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \\textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \\textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \\textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \\textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \\textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\\footnote{Our code is now available in~\\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86GUITester\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8eGUI\u63a2\u7d22\u6027\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5206\u79bb\u5bfc\u822a\u4e0e\u9a8c\u8bc1\u89e3\u51b3\u76ee\u6807\u5bfc\u5411\u906e\u853d\u548c\u6267\u884c\u504f\u5dee\u5f52\u56e0\u95ee\u9898\uff0c\u5728GUITestBench\u57fa\u51c6\u4e0a\u8fbe\u523048.90% F1\u5206\u6570\u3002", "motivation": "GUI\u63a2\u7d22\u6027\u6d4b\u8bd5\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u4f46\u4eba\u5de5\u6210\u672c\u9ad8\u3002\u73b0\u6709MLLM\u667a\u80fd\u4f53\u64c5\u957f\u5bfc\u822a\u4f46\u65e0\u6cd5\u81ea\u4e3b\u53d1\u73b0\u7f3a\u9677\uff0c\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u76ee\u6807\u5bfc\u5411\u906e\u853d\uff08\u667a\u80fd\u4f53\u4f18\u5148\u5b8c\u6210\u4efb\u52a1\u800c\u975e\u62a5\u544a\u5f02\u5e38\uff09\u548c\u6267\u884c\u504f\u5dee\u5f52\u56e0\uff08\u7cfb\u7edf\u7f3a\u9677\u88ab\u8bef\u5224\u4e3a\u667a\u80fd\u4f53\u9519\u8bef\uff09\u3002", "method": "\u63d0\u51faGUITester\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1\uff09\u89c4\u5212\u6267\u884c\u6a21\u5757\uff08PEM\uff09\u901a\u8fc7\u5d4c\u5165\u6d4b\u8bd5\u610f\u56fe\u4e3b\u52a8\u63a2\u6d4b\u7f3a\u9677\uff1b2\uff09\u5206\u5c42\u53cd\u601d\u6a21\u5757\uff08HRM\uff09\u901a\u8fc7\u4ea4\u4e92\u5386\u53f2\u5206\u6790\u89e3\u51b3\u5f52\u56e0\u6a21\u7cca\u6027\u3002\u540c\u65f6\u521b\u5efa\u4e86GUITestBench\u57fa\u51c6\uff0c\u5305\u542b26\u79cd\u7f3a\u9677\u7684143\u4e2a\u4efb\u52a1\u3002", "result": "GUITester\u5728GUITestBench\u4e0a\u8fbe\u523048.90% F1\u5206\u6570\uff08Pass@3\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u768433.35%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u81ea\u4e3b\u63a2\u7d22\u6027\u6d4b\u8bd5\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765GUI\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2601.04505", "categories": ["cs.AI", "cs.CL", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.04505", "abs": "https://arxiv.org/abs/2601.04505", "authors": ["Khandakar Shakib Al Hasan", "Syed Rifat Raiyan", "Hasin Mahtab Alvee", "Wahid Sadik"], "title": "CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts", "comment": "Under review, 13 pages, 11 figures, 2 tables", "summary": "Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.", "AI": {"tldr": "CircuitLM\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u8f85\u52a9\u7535\u8def\u8bbe\u8ba1\u7cfb\u7edf\uff0c\u53ef\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u3001\u53ef\u89c6\u5316\u7684CircuitJSON\u7535\u8def\u56fe\uff0c\u901a\u8fc7\u4e94\u9636\u6bb5\u6d41\u7a0b\u548c\u9a8c\u8bc1\u6570\u636e\u5e93\u89e3\u51b3LLM\u5728\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5c06\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7535\u8def\u56fe\u65f6\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u3001\u8fdd\u53cd\u7535\u6c14\u7ea6\u675f\u3001\u8f93\u51fa\u975e\u673a\u5668\u53ef\u8bfb\u683c\u5f0f\u3002\u8fd9\u963b\u788d\u4e86\u975e\u4e13\u5bb6\u7528\u6237\u8fdb\u884c\u53ef\u9760\u7684\u7535\u8def\u539f\u578b\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faCircuitLM\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u5305\u542b\u4e94\u4e2a\u987a\u5e8f\u9636\u6bb5\uff1a1) LLM\u7ec4\u4ef6\u8bc6\u522b\uff1b2) \u89c4\u8303\u5f15\u811a\u68c0\u7d22\uff1b3) \u7535\u5b50\u4e13\u5bb6\u667a\u80fd\u4f53\u94fe\u5f0f\u63a8\u7406\uff1b4) JSON\u539f\u7406\u56fe\u5408\u6210\uff1b5) \u529b\u5bfc\u5411SVG\u53ef\u89c6\u5316\u3002\u57fa\u4e8e\u5305\u542b50\u4e2a\u7ec4\u4ef6\u7684\u9a8c\u8bc1\u6570\u636e\u5e93\uff0c\u5e76\u91c7\u7528DMCV\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u7ed3\u6784\u6027\u548c\u7535\u6c14\u6709\u6548\u6027\u3002", "result": "\u5728100\u4e2a\u591a\u6837\u5316\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u793a\u4e0a\u8bc4\u4f30\u4e86\u516d\u4e2aLLM\uff0cDMCV\u8bc4\u4f30\u6846\u67b6\u5728\u5fae\u63a7\u5236\u5668\u4e2d\u5fc3\u8bbe\u8ba1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u8f6c\u6362\u4e3a\u53ef\u90e8\u7f72\u786c\u4ef6\u8bbe\u8ba1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CircuitLM\u901a\u8fc7\u57fa\u4e8e\u9a8c\u8bc1\u6570\u636e\u5e93\u7684\u751f\u6210\u548c\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u6210\u529f\u5f25\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u4e0e\u53ef\u90e8\u7f72\u786c\u4ef6\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7f\u975e\u4e13\u5bb6\u80fd\u591f\u8fdb\u884c\u53ef\u9760\u7684\u7535\u8def\u539f\u578b\u8bbe\u8ba1\u3002", "topic": "code agent"}}
{"id": "2601.04463", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04463", "abs": "https://arxiv.org/abs/2601.04463", "authors": ["Chengyuan Yang", "Zequn Sun", "Wei Wei", "Wei Hu"], "title": "Beyond Static Summarization: Proactive Memory Extraction for LLM Agents", "comment": null, "summary": "Memory management is vital for LLM agents to handle long-term interaction and personalization. Most research focuses on how to organize and use memory summary, but often overlooks the initial memory extraction stage. In this paper, we argue that existing summary-based methods have two major limitations based on the recurrent processing theory. First, summarization is \"ahead-of-time\", acting as a blind \"feed-forward\" process that misses important details because it doesn't know future tasks. Second, extraction is usually \"one-off\", lacking a feedback loop to verify facts, which leads to the accumulation of information loss. To address these issues, we propose proactive memory extraction (namely ProMem). Unlike static summarization, ProMem treats extraction as an iterative cognitive process. We introduce a recurrent feedback loop where the agent uses self-questioning to actively probe the dialogue history. This mechanism allows the agent to recover missing information and correct errors. Our ProMem significantly improves the completeness of the extracted memory and QA accuracy. It also achieves a superior trade-off between extraction quality and token cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faProMem\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u8bb0\u5fc6\u63d0\u53d6\u89e3\u51b3\u4f20\u7edf\u6458\u8981\u5f0f\u8bb0\u5fc6\u7ba1\u7406\u7684\u95ee\u9898\uff0c\u5f15\u5165\u5faa\u73af\u53cd\u9988\u673a\u5236\u63d0\u5347\u8bb0\u5fc6\u5b8c\u6574\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u7684\u8bb0\u5fc6\u7ba1\u7406\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bb0\u5fc6\u7ec4\u7ec7\u548c\u4f7f\u7528\uff0c\u4f46\u5ffd\u89c6\u4e86\u521d\u59cb\u8bb0\u5fc6\u63d0\u53d6\u9636\u6bb5\u3002\u57fa\u4e8e\u5faa\u73af\u5904\u7406\u7406\u8bba\uff0c\u73b0\u6709\u6458\u8981\u5f0f\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u6458\u8981\u662f\"\u63d0\u524d\"\u8fdb\u884c\u7684\u76f2\u76ee\u524d\u9988\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u9884\u77e5\u672a\u6765\u4efb\u52a1\u800c\u9057\u6f0f\u91cd\u8981\u7ec6\u8282\uff1b2\uff09\u63d0\u53d6\u901a\u5e38\u662f\"\u4e00\u6b21\u6027\"\u7684\uff0c\u7f3a\u4e4f\u9a8c\u8bc1\u4e8b\u5b9e\u7684\u53cd\u9988\u5faa\u73af\uff0c\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u7d2f\u79ef\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u8bb0\u5fc6\u63d0\u53d6\u65b9\u6cd5ProMem\uff0c\u5c06\u63d0\u53d6\u89c6\u4e3a\u8fed\u4ee3\u8ba4\u77e5\u8fc7\u7a0b\u3002\u5f15\u5165\u5faa\u73af\u53cd\u9988\u5faa\u73af\uff0c\u4ee3\u7406\u901a\u8fc7\u81ea\u6211\u63d0\u95ee\u4e3b\u52a8\u63a2\u7d22\u5bf9\u8bdd\u5386\u53f2\uff0c\u8fd9\u79cd\u673a\u5236\u5141\u8bb8\u4ee3\u7406\u6062\u590d\u7f3a\u5931\u4fe1\u606f\u548c\u7ea0\u6b63\u9519\u8bef\u3002", "result": "ProMem\u663e\u8457\u63d0\u9ad8\u4e86\u63d0\u53d6\u8bb0\u5fc6\u7684\u5b8c\u6574\u6027\u548c\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u5728\u63d0\u53d6\u8d28\u91cf\u548ctoken\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6743\u8861\u3002", "conclusion": "\u4e3b\u52a8\u8bb0\u5fc6\u63d0\u53d6\u65b9\u6cd5ProMem\u901a\u8fc7\u5faa\u73af\u53cd\u9988\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6458\u8981\u5f0f\u8bb0\u5fc6\u7ba1\u7406\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u957f\u671f\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.04544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04544", "abs": "https://arxiv.org/abs/2601.04544", "authors": ["Jiuzhou Zhao", "Chunrong Chen", "Chenqi Qiao", "Lebin Zheng", "Minqi Han", "Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang"], "title": "TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration", "comment": "16 pages, 6 figures. Under review at IJCAI", "summary": "Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.", "AI": {"tldr": "TCAR\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u63a8\u7406\u8def\u7531\u5668\uff0c\u901a\u8fc7\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u94fe\u6765\u52a8\u6001\u9009\u62e9\u5019\u9009\u4ee3\u7406\uff0c\u652f\u6301\u65b0\u4ee3\u7406\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u901a\u8fc7\u534f\u4f5c\u6267\u884c\u7ba1\u9053\u63d0\u5347\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u8def\u7531\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u5355\u6807\u7b7e\u51b3\u7b56\uff0c\u5b58\u5728\u4e24\u5927\u9650\u5236\uff1a1) \u96be\u4ee5\u968f\u7740\u4e1a\u52a1\u9886\u57df\u6269\u5c55\u65e0\u7f1d\u96c6\u6210\u65b0\u4ee3\u7406\uff1b2) \u4ee3\u7406\u80fd\u529b\u91cd\u53e0\u5bfc\u81f4\u8def\u7531\u51b2\u7a81\uff0c\u6700\u7ec8\u964d\u4f4e\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faTCAR\u81ea\u9002\u5e94\u63a8\u7406\u8def\u7531\u5668\uff0c\u652f\u6301\u52a8\u6001\u4ee3\u7406\u4e0a\u7ebf\uff0c\u9996\u5148\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u94fe\uff0c\u7136\u540e\u9884\u6d4b\u80fd\u591f\u5904\u7406\u67e5\u8be2\u7684\u5019\u9009\u4ee3\u7406\u96c6\u5408\u3002\u8bbe\u8ba1\u534f\u4f5c\u6267\u884c\u7ba1\u9053\uff0c\u9009\u5b9a\u4ee3\u7406\u72ec\u7acb\u751f\u6210\u54cd\u5e94\uff0c\u7136\u540e\u7531\u4e13\u95e8\u7684\u7cbe\u70bc\u4ee3\u7406\u805a\u5408\u548c\u4f18\u5316\u4e3a\u5355\u4e00\u9ad8\u8d28\u91cf\u54cd\u5e94\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTCAR\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u8def\u7531\u51b2\u7a81\uff0c\u5e76\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "TCAR\u89e3\u51b3\u4e86\u73b0\u6709\u4efb\u52a1\u8def\u7531\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u548c\u534f\u4f5c\u6267\u884c\u673a\u5236\uff0c\u4e3a\u53ef\u89e3\u91ca\u548c\u534f\u4f5c\u7684\u591a\u4ee3\u7406\u8def\u7531\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.04525", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04525", "abs": "https://arxiv.org/abs/2601.04525", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Zichen Ding", "Xiang Li"], "title": "GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence", "comment": "18 pages", "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge to enhance Large Language Models (LLMs), yet systems remain susceptible to two critical flaws: providing correct answers without explicit grounded evidence and producing fabricated responses when the retrieved context is insufficient. While prior research has addressed these issues independently, a unified framework that integrates evidence-based grounding and reliable abstention is currently lacking. In this paper, we propose GRACE, a reinforcement-learning framework that simultaneously mitigates both types of flaws. GRACE employs a data construction method that utilizes heterogeneous retrievers to generate diverse training samples without manual annotation. A multi-stage gated reward function is then employed to train the model to assess evidence sufficiency, extract key supporting evidence, and provide answers or explicitly abstain. Experimental results on two benchmarks demonstrate that GRACE achieves state-of-the-art overall accuracy and strikes a favorable balance between accurate response and rejection, while requiring only 10% of the annotation costs of prior methods. Our code is available at https://github.com/YiboZhao624/Grace..", "AI": {"tldr": "GRACE\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u5145\u5206\u6027\u8bc4\u4f30\u3001\u5173\u952e\u8bc1\u636e\u63d0\u53d6\u548c\u9009\u62e9\u6027\u5f03\u6743\uff0c\u7edf\u4e00\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u7684\u8bc1\u636e\u7f3a\u5931\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4ec5\u970010%\u6807\u6ce8\u6210\u672c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1\uff09\u5728\u6ca1\u6709\u660e\u786e\u8bc1\u636e\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6b63\u786e\u7b54\u6848\uff1b2\uff09\u5728\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e0d\u8db3\u65f6\u4ea7\u751f\u5e7b\u89c9\u56de\u7b54\u3002\u867d\u7136\u5148\u524d\u7814\u7a76\u5206\u522b\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u6574\u5408\u8bc1\u636e\u57fa\u7840\u548c\u53ef\u9760\u5f03\u6743\u673a\u5236\u3002", "method": "\u63d0\u51faGRACE\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5f02\u6784\u68c0\u7d22\u5668\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u6837\u672c\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff1b2\uff09\u91c7\u7528\u591a\u9636\u6bb5\u95e8\u63a7\u5956\u52b1\u51fd\u6570\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u8bc4\u4f30\u8bc1\u636e\u5145\u5206\u6027\u3001\u63d0\u53d6\u5173\u952e\u652f\u6301\u8bc1\u636e\uff0c\u5e76\u6839\u636e\u60c5\u51b5\u63d0\u4f9b\u7b54\u6848\u6216\u660e\u786e\u5f03\u6743\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cGRACE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6574\u4f53\u51c6\u786e\u7387\uff0c\u5728\u51c6\u786e\u54cd\u5e94\u548c\u62d2\u7edd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u540c\u65f6\u4ec5\u9700\u8981\u5148\u524d\u65b9\u6cd510%\u7684\u6807\u6ce8\u6210\u672c\u3002", "conclusion": "GRACE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u7684\u8bc1\u636e\u7f3a\u5931\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u6784\u5efa\u548c\u591a\u9636\u6bb5\u5956\u52b1\u673a\u5236\uff0c\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04521", "abs": "https://arxiv.org/abs/2601.04521", "authors": ["Jacob Ede Levine", "Yun Lyan Luo", "Sai Chandra Kosaraju"], "title": "TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation", "comment": "Under Review", "summary": "The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches.", "AI": {"tldr": "TSSR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u3001\u4ea4\u6362\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5b57\u7b26\u7ea7SMILES\u751f\u6210\uff0c\u901a\u8fc7\u8bed\u6cd5\u4fee\u590d\u548c\u5316\u5b66\u611f\u77e5\u53cd\u9988\u63d0\u9ad8\u5206\u5b50\u751f\u6210\u7684\u6709\u6548\u6027\u548c\u65b0\u9896\u6027\u3002", "motivation": "\u5f53\u524d\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u751f\u6210SMILES\u5b57\u7b26\u4e32\u65f6\u5bb9\u6613\u4ea7\u751f\u590d\u5408\u4ee4\u724c\u9519\u8bef\uff0c\u8bb8\u591a\u6837\u672c\u65e0\u6cd5\u89e3\u6790\u6216\u5316\u5b66\u4e0a\u4e0d\u53ef\u884c\uff0c\u800c\u786c\u7ea6\u675f\u4f1a\u9650\u5236\u63a2\u7d22\u7a7a\u95f4\u3002\u9700\u8981\u63d0\u9ad8\u5206\u5b50\u751f\u6210\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faTSSR\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5956\u52b1\u5c40\u90e8\u4ee4\u724c\u4ea4\u6362\u4fee\u590d\u8bed\u6cd5\uff0c\u7b2c\u4e8c\u9636\u6bb5\u63d0\u4f9bRDKit\u8bca\u65ad\u7684\u5316\u5b66\u611f\u77e5\u53cd\u9988\uff0c\u5956\u52b1\u51cf\u5c11\u5316\u5408\u4ef7\u3001\u82b3\u9999\u6027\u548c\u8fde\u63a5\u6027\u95ee\u9898\u3002\u5956\u52b1\u53ef\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u672f\u8bed\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u6216\u624b\u5de5\u8bed\u6cd5\u3002", "result": "\u5728MOSES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7eaf\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u9ad8\u8bed\u6cd5\u6709\u6548\u6027\u3001\u5316\u5b66\u6709\u6548\u6027\u548c\u65b0\u9896\u6027\uff1b\u5fae\u8c03\u5f3a\u5316\u5b66\u4e60\u5728\u4fdd\u6301\u836f\u7269\u76f8\u4f3c\u6027\u548c\u53ef\u5408\u6210\u6027\u7684\u540c\u65f6\u589e\u52a0\u6709\u6548\u6027\u548c\u65b0\u9896\u6027\u3002\u4ee4\u724c\u7ea7\u5206\u6790\u663e\u793a\u8bed\u6cd5\u7f16\u8f91\u548c\u5316\u5b66\u4fee\u590d\u5171\u540c\u51cf\u5c11RDKit\u68c0\u6d4b\u9519\u8bef\u3002", "conclusion": "TSSR\u5c06\u7a00\u758f\u7ec8\u7aef\u76ee\u6807\u8f6c\u5316\u4e3a\u66f4\u5bc6\u96c6\u4e14\u53ef\u89e3\u91ca\u7684\u5956\u52b1\uff0c\u63d0\u9ad8\u8bed\u6cd5\u548c\u5316\u5b66\u8d28\u91cf\u800c\u4e0d\u51cf\u5c11\u591a\u6837\u6027\u3002\u6846\u67b6\u4e0e\u6570\u636e\u96c6\u65e0\u5173\uff0c\u53ef\u9002\u5e94\u5404\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04537", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04537", "abs": "https://arxiv.org/abs/2601.04537", "authors": ["Tianle Wang", "Zhongyuan Wu", "Shenghao Jin", "Hao Xu", "Wei Chen", "Ning Miao"], "title": "Not All Steps are Informative: On the Linearity of LLMs' RLVR Training", "comment": "pre-print", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0RLVR\u8bad\u7ec3\u4e2dLLM\u5448\u5f3a\u7ebf\u6027\u6f14\u5316\uff0c\u63d0\u51fa\u901a\u8fc7\u6743\u91cd\u5916\u63a8\u548clogits\u5916\u63a8\u6765\u9884\u6d4b\u672a\u6765\u6a21\u578b\u72b6\u6001\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c", "motivation": "RLVR\u8bad\u7ec3\u9700\u8981\u6570\u5343\u6b65\u8bad\u7ec3\u6b65\u9aa4\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u957f\u65f6\u95f4\u7684\u63a2\u7d22\u8fc7\u7a0b\u3002\u4f5c\u8005\u89c2\u5bdf\u5230RLVR\u8bad\u7ec3\u4e2dLLM\u5448\u73b0\u5f3a\u7ebf\u6027\u6f14\u5316\u7279\u5f81\uff0c\u8fd9\u542f\u53d1\u4ed6\u4eec\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5916\u63a8\u4e2d\u95f4\u68c0\u67e5\u70b9\u6765\u9884\u6d4b\u672a\u6765\u6a21\u578b\u72b6\u6001\uff0c\u4ece\u800c\u907f\u514d\u6602\u8d35\u7684\u6301\u7eed\u8bad\u7ec3", "method": "\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u7ebf\u6027\u6f14\u5316\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5916\u63a8\u65b9\u6cd5\uff1a1) \u6743\u91cd\u5916\u63a8\uff1a\u4ece\u4e2d\u95f4\u68c0\u67e5\u70b9\u5916\u63a8\u6a21\u578b\u6743\u91cd\uff1b2) Logits\u5916\u63a8\uff1a\u5916\u63a8\u6a21\u578b\u8f93\u51falog-probabilities\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u51fa\u73b0\u7684\u8d8b\u52bf\u8fdb\u884c\u9884\u6d4b\uff0c\u907f\u514d\u6301\u7eed\u8bad\u7ec3", "result": "\u6743\u91cd\u5916\u63a8\u4ea7\u751f\u7684\u6a21\u578b\u6027\u80fd\u4e0e\u6807\u51c6RL\u8bad\u7ec3\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u663e\u8457\u51cf\u5c11\u3002Logits\u5916\u63a8\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6301\u7eedRL\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5728RL\u8bad\u7ec3\u4fdd\u6301\u7a33\u5b9a\u7684\u6b65\u6570\u8303\u56f4\u4e4b\u5916\u8fdb\u884c\u5916\u63a8\u65f6\u8868\u73b0\u66f4\u597d", "conclusion": "RLVR\u8bad\u7ec3\u4e2dLLM\u7684\u5f3a\u7ebf\u6027\u6f14\u5316\u8868\u660e\u8bad\u7ec3\u4e3b\u8981\u653e\u5927\u65e9\u671f\u51fa\u73b0\u7684\u8d8b\u52bf\u800c\u975e\u6301\u7eed\u53d1\u73b0\u65b0\u884c\u4e3a\u3002\u5229\u7528\u8fd9\u4e00\u7279\u6027\u8fdb\u884c\u5916\u63a8\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u7684LLM\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def", "topic": "agentic reinforcement learning"}}
{"id": "2601.04566", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04566", "abs": "https://arxiv.org/abs/2601.04566", "authors": ["Yunhao Feng", "Yige Li", "Yutao Wu", "Yingshui Tan", "Yanming Guo", "Yifan Ding", "Kun Zhai", "Xingjun Ma", "Yugang Jiang"], "title": "BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents", "comment": null, "summary": "Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \\textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \\textbf{planning attacks}, \\textbf{memory attacks}, and \\textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \\textbf{Agent QA}, \\textbf{Agent Code}, \\textbf{Agent Web}, and \\textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \\textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\\% of planning attacks, 77.97\\% of memory attacks, and 60.28\\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.", "AI": {"tldr": "BackdoorAgent\u6846\u67b6\u7cfb\u7edf\u5206\u6790LLM\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u540e\u95e8\u5a01\u80c1\uff0c\u5c06\u653b\u51fb\u9762\u5206\u4e3a\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4f7f\u7528\u4e09\u4e2a\u9636\u6bb5\uff0c\u6784\u5efa\u6807\u51c6\u5316\u57fa\u51c6\u5e76\u53d1\u73b0\u5355\u9636\u6bb5\u690d\u5165\u7684\u89e6\u53d1\u5668\u80fd\u5728\u591a\u6b65\u9aa4\u4e2d\u6301\u7eed\u4f20\u64ad\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u901a\u8fc7\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u5b9e\u73b0\u81ea\u4e3b\u6027\uff0c\u4f46\u8fd9\u4e5f\u6269\u5927\u4e86\u540e\u95e8\u653b\u51fb\u9762\u3002\u73b0\u6709\u7814\u7a76\u5206\u6563\u4e14\u5b64\u7acb\u5206\u6790\u5355\u4e2a\u653b\u51fb\u5411\u91cf\uff0c\u7f3a\u4e4f\u4ece\u667a\u80fd\u4f53\u89d2\u5ea6\u7406\u89e3\u8de8\u9636\u6bb5\u89e6\u53d1\u5668\u4ea4\u4e92\u548c\u4f20\u64ad\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u63d0\u51faBackdoorAgent\u6846\u67b6\uff0c\u5c06\u653b\u51fb\u9762\u7ed3\u6784\u5316\u5206\u4e3a\u89c4\u5212\u653b\u51fb\u3001\u8bb0\u5fc6\u653b\u51fb\u548c\u5de5\u5177\u4f7f\u7528\u653b\u51fb\u4e09\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7\u68c0\u6d4b\u667a\u80fd\u4f53\u6267\u884c\u6765\u7cfb\u7edf\u5206\u6790\u89e6\u53d1\u5668\u6fc0\u6d3b\u548c\u8de8\u9636\u6bb5\u4f20\u64ad\u3002\u6784\u5efa\u6db5\u76d6Agent QA\u3001Agent Code\u3001Agent Web\u548cAgent Drive\u56db\u4e2a\u4ee3\u8868\u6027\u5e94\u7528\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u8986\u76d6\u7eaf\u8bed\u8a00\u548c\u591a\u6a21\u6001\u573a\u666f\u3002", "result": "\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u5355\u9636\u6bb5\u690d\u5165\u7684\u89e6\u53d1\u5668\u80fd\u5728\u591a\u6b65\u9aa4\u4e2d\u6301\u7eed\u4f20\u64ad\u5e76\u901a\u8fc7\u4e2d\u95f4\u72b6\u6001\u4f20\u64ad\u3002\u4f7f\u7528GPT\u9aa8\u5e72\u65f6\uff0c\u89c4\u5212\u653b\u51fb\u4e2d43.58%\u3001\u8bb0\u5fc6\u653b\u51fb\u4e2d77.97%\u3001\u5de5\u5177\u9636\u6bb5\u653b\u51fb\u4e2d60.28%\u7684\u6848\u4f8b\u89c2\u5bdf\u5230\u89e6\u53d1\u5668\u6301\u7eed\u6027\uff0c\u7a81\u663e\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u672c\u8eab\u5bf9\u540e\u95e8\u5a01\u80c1\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u672c\u8eab\u5b58\u5728\u540e\u95e8\u5b89\u5168\u6f0f\u6d1e\uff0c\u5355\u9636\u6bb5\u690d\u5165\u7684\u89e6\u53d1\u5668\u5177\u6709\u8de8\u9636\u6bb5\u4f20\u64ad\u80fd\u529b\u3002BackdoorAgent\u6846\u67b6\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u6790\u89c6\u89d2\uff0c\u6807\u51c6\u5316\u57fa\u51c6\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u672a\u6765\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2601.04548", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04548", "abs": "https://arxiv.org/abs/2601.04548", "authors": ["Wenjie Li", "Guansong Pang", "Hezhe Qiao", "Debin Gao", "David Lo"], "title": "Identifying Good and Bad Neurons for Task-Level Controllable LLMs", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities on multiple-choice question answering benchmarks, but the complex mechanisms underlying their large-scale neurons remain opaque, posing significant challenges for understanding and steering LLMs. While recent studies made progress on identifying responsible neurons for certain abilities, these ability-specific methods are infeasible for task-focused scenarios requiring coordinated use of multiple abilities. Moreover, these approaches focus only on supportive neurons that correlate positively with task completion, while neglecting neurons with other roles-such as inhibitive roles-and misled neuron attribution due to fortuitous behaviors in LLMs (i.e., correctly answer the questions by chance rather than genuine understanding). To address these challenges, we propose NeuronLLM, a novel task-level LLM understanding framework that adopts the biological principle of functional antagonism for LLM neuron identification. The key insight is that task performance is jointly determined by neurons with two opposing roles: good neurons that facilitate task completion and bad neurons that inhibit it. NeuronLLM achieves a holistic modeling of neurons via contrastive learning of good and bad neurons, while leveraging augmented question sets to mitigate the fortuitous behaviors in LLMs. Comprehensive experiments on LLMs of different sizes and families show the superiority of NeuronLLM over existing methods in four NLP tasks, providing new insights into LLM functional organization.", "AI": {"tldr": "NeuronLLM\uff1a\u57fa\u4e8e\u529f\u80fd\u62ee\u6297\u539f\u7406\u7684LLM\u795e\u7ecf\u5143\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4fc3\u8fdb\u548c\u6291\u5236\u4efb\u52a1\u5b8c\u6210\u7684\u795e\u7ecf\u5143\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u53ea\u5173\u6ce8\u652f\u6301\u6027\u795e\u7ecf\u5143\u3001\u5ffd\u7565\u6291\u5236\u6027\u89d2\u8272\u548c\u5076\u7136\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u9488\u5bf9\u7279\u5b9a\u80fd\u529b\u7684\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u534f\u8c03\u591a\u79cd\u80fd\u529b\u7684\u4efb\u52a1\u573a\u666f\uff1b2\uff09\u53ea\u5173\u6ce8\u4e0e\u4efb\u52a1\u5b8c\u6210\u6b63\u76f8\u5173\u7684\u652f\u6301\u6027\u795e\u7ecf\u5143\uff0c\u5ffd\u7565\u6291\u5236\u6027\u795e\u7ecf\u5143\uff1b3\uff09LLM\u7684\u5076\u7136\u884c\u4e3a\uff08\u9760\u8fd0\u6c14\u800c\u975e\u771f\u6b63\u7406\u89e3\uff09\u4f1a\u8bef\u5bfc\u795e\u7ecf\u5143\u5f52\u56e0\u3002", "method": "NeuronLLM\u91c7\u7528\u751f\u7269\u5b66\u529f\u80fd\u62ee\u6297\u539f\u7406\uff0c\u8ba4\u4e3a\u4efb\u52a1\u8868\u73b0\u7531\u4e24\u7c7b\u5bf9\u7acb\u795e\u7ecf\u5143\u5171\u540c\u51b3\u5b9a\uff1a\u4fc3\u8fdb\u4efb\u52a1\u5b8c\u6210\u7684\"\u597d\u795e\u7ecf\u5143\"\u548c\u6291\u5236\u4efb\u52a1\u5b8c\u6210\u7684\"\u574f\u795e\u7ecf\u5143\"\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5efa\u6a21\u8fd9\u4e24\u7c7b\u795e\u7ecf\u5143\uff0c\u540c\u65f6\u4f7f\u7528\u589e\u5f3a\u95ee\u9898\u96c6\u6765\u7f13\u89e3LLM\u7684\u5076\u7136\u884c\u4e3a\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u5728\u56db\u4e2aNLP\u4efb\u52a1\u4e0aNeuronLLM\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aLLM\u529f\u80fd\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "NeuronLLM\u901a\u8fc7\u529f\u80fd\u62ee\u6297\u539f\u7406\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u7ea7\u522b\u7684LLM\u795e\u7ecf\u5143\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u5f15\u5bfcLLM\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.04582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04582", "abs": "https://arxiv.org/abs/2601.04582", "authors": ["Mizanur Rahman", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Shafiq Joty", "Enamul Hoque"], "title": "Aligning Text, Code, and Vision: A Multi-Objective Reinforcement Learning Framework for Text-to-Visualization", "comment": "Accepted to EACL Main Conference", "summary": "Text-to-Visualization (Text2Vis) systems translate natural language queries over tabular data into concise answers and executable visualizations. While closed-source LLMs generate functional code, the resulting charts often lack semantic alignment and clarity, qualities that can only be assessed post-execution. Open-source models struggle even more, frequently producing non-executable or visually poor outputs. Although supervised fine-tuning can improve code executability, it fails to enhance overall visualization quality, as traditional SFT loss cannot capture post-execution feedback. To address this gap, we propose RL-Text2Vis, the first reinforcement learning framework for Text2Vis generation. Built on Group Relative Policy Optimization (GRPO), our method uses a novel multi-objective reward that jointly optimizes textual accuracy, code validity, and visualization quality using post-execution feedback. By training Qwen2.5 models (7B and 14B), RL-Text2Vis achieves a 22% relative improvement in chart quality over GPT-4o on the Text2Vis benchmark and boosts code execution success from 78% to 97% relative to its zero-shot baseline. Our models significantly outperform strong zero-shot and supervised baselines and also demonstrate robust generalization to out-of-domain datasets like VIS-Eval and NVBench. These results establish GRPO as an effective strategy for structured, multimodal reasoning in visualization generation. We release our code at https://github.com/vis-nlp/RL-Text2Vis.", "AI": {"tldr": "\u63d0\u51faRL-Text2Vis\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6587\u672c\u5230\u53ef\u89c6\u5316\u751f\u6210\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u540c\u65f6\u63d0\u5347\u6587\u672c\u51c6\u786e\u6027\u3001\u4ee3\u7801\u53ef\u6267\u884c\u6027\u548c\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u663e\u8457\u8d85\u8d8aGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709Text2Vis\u7cfb\u7edf\u5b58\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u6e05\u6670\u5ea6\u95ee\u9898\uff1a\u95ed\u6e90LLM\u751f\u6210\u7684\u56fe\u8868\u8d28\u91cf\u4e0d\u8db3\uff0c\u5f00\u6e90\u6a21\u578b\u5e38\u4ea7\u751f\u4e0d\u53ef\u6267\u884c\u6216\u89c6\u89c9\u8d28\u91cf\u5dee\u7684\u8f93\u51fa\u3002\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65e0\u6cd5\u5229\u7528\u6267\u884c\u540e\u53cd\u9988\u6765\u63d0\u5347\u53ef\u89c6\u5316\u8d28\u91cf\u3002", "method": "\u57fa\u4e8eGroup Relative Policy Optimization(GRPO)\u6784\u5efa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u6587\u672c\u51c6\u786e\u6027\u3001\u4ee3\u7801\u6709\u6548\u6027\u548c\u53ef\u89c6\u5316\u8d28\u91cf\uff0c\u5229\u7528\u6267\u884c\u540e\u53cd\u9988\u8bad\u7ec3Qwen2.5\u6a21\u578b(7B\u548c14B)\u3002", "result": "\u5728Text2Vis\u57fa\u51c6\u4e0a\uff0c\u56fe\u8868\u8d28\u91cf\u76f8\u5bf9GPT-4o\u63d0\u534722%\uff1b\u4ee3\u7801\u6267\u884c\u6210\u529f\u7387\u4ece\u96f6\u6837\u672c\u57fa\u7ebf\u768478%\u63d0\u5347\u81f397%\uff1b\u5728VIS-Eval\u548cNVBench\u7b49\u57df\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GRPO\u662f\u53ef\u89c6\u5316\u751f\u6210\u4e2d\u7ed3\u6784\u5316\u591a\u6a21\u6001\u63a8\u7406\u7684\u6709\u6548\u7b56\u7565\uff0cRL-Text2Vis\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u53ef\u89c6\u5316\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "topic": "code agent"}}
{"id": "2601.04575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04575", "abs": "https://arxiv.org/abs/2601.04575", "authors": ["Yuguang Yue", "Irakli Salia", "Samuel Hunt", "Chris Green", "Wenzhe Shi", "Jonathan J Hunt"], "title": "Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing", "comment": "24 pages, 16 figures", "summary": "Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u89c6\u9891\u6e38\u620f\u57fa\u7840\u6a21\u578b\u7684\u5f00\u653e\u914d\u65b9\uff0c\u53d1\u5e03\u4e868300+\u5c0f\u65f6\u9ad8\u8d28\u91cf\u4eba\u7c7b\u6e38\u620f\u6570\u636e\u3001\u8bad\u7ec3\u63a8\u7406\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u884c\u4e3a\u514b\u9686\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u53d1\u73b0\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u589e\u52a0\u80fd\u63d0\u5347\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u548c\u6570\u636e\u89c4\u6a21\u7684\u6269\u5927\uff0c\u884c\u4e3a\u514b\u9686\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u65f6\u63a8\u7406\u7684\u89c6\u9891\u6e38\u620f\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u884c\u4e3a\u514b\u9686\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u7279\u522b\u662f\u6a21\u578b\u5982\u4f55\u968f\u7740\u89c4\u6a21\u589e\u957f\u83b7\u5f97\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5b8c\u6574\u7684\u5f00\u653e\u8bad\u7ec3\u914d\u65b9\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\uff088300+\u5c0f\u65f6\u9ad8\u8d28\u91cf\u4eba\u7c7b\u6e38\u620f\u6570\u636e\uff09\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4ee3\u7801\u3002\u901a\u8fc7\u7b80\u5355\u73a9\u5177\u95ee\u9898\u548c\u6269\u5c55\u523012\u4ebf\u53c2\u6570\u7684\u5927\u6a21\u578b\uff0c\u7cfb\u7edf\u7814\u7a76\u6a21\u578b\u53c2\u6570\u6570\u91cf\uff08\u6df1\u5ea6\uff09\u548c\u8bad\u7ec3\u6b65\u6570\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5206\u6790\u884c\u4e3a\u514b\u9686\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u80fd\u591f\u5728\u591a\u79cd3D\u89c6\u9891\u6e38\u620f\u4e2d\u8fbe\u5230\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u7ade\u4e89\u7684\u6c34\u5e73\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u73a9\u5177\u95ee\u9898\u4e2d\uff0c\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u548c\u7f51\u7edc\u6df1\u5ea6\u80fd\u4f7f\u6a21\u578b\u5b66\u4e60\u66f4\u5177\u56e0\u679c\u6027\u7684\u7b56\u7565\u3002\u5728\u6269\u5c55\u523012\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e2d\uff0c\u89c2\u5bdf\u5230\u7c7b\u4f3c\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\u786e\u5b9e\u63d0\u5347\u4e86\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u884c\u4e3a\u514b\u9686\u7684\u7f29\u653e\u89c4\u5f8b\u8868\u660e\uff0c\u589e\u52a0\u6a21\u578b\u53c2\u6570\u548c\u8bad\u7ec3\u6570\u636e\u80fd\u591f\u63d0\u5347\u6a21\u578b\u5728\u89c6\u9891\u6e38\u620f\u4efb\u52a1\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\u5f00\u653e\u53d1\u5e03\u7684\u5b8c\u6574\u914d\u65b9\u3001\u6570\u636e\u548c\u6a21\u578b\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.04583", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04583", "abs": "https://arxiv.org/abs/2601.04583", "authors": ["Saad Alqithami"], "title": "Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries", "comment": null, "summary": "Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86AI\u667a\u80fd\u4f53\u4e0e\u533a\u5757\u94fe\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u63d0\u51fa\u4e86\u4e94\u7c7b\u96c6\u6210\u6a21\u5f0f\u3001\u5a01\u80c1\u6a21\u578b\u548c\u6bd4\u8f83\u5206\u6790\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4ea4\u6613\u610f\u56fe\u6a21\u5f0f\u548c\u653f\u7b56\u51b3\u7b56\u8bb0\u5f55\u4e24\u5927\u63a5\u53e3\u62bd\u8c61\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7fAI\u667a\u80fd\u4f53\u80fd\u591f\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u591a\u6b65\u5de5\u4f5c\u6d41\uff0c\u800c\u533a\u5757\u94fe\u5df2\u6210\u4e3a\u53ef\u7f16\u7a0b\u7684\u4ef7\u503c\u8f6c\u79fb\u3001\u8bbf\u95ee\u63a7\u5236\u548c\u53ef\u9a8c\u8bc1\u72b6\u6001\u8f6c\u6362\u5e73\u53f0\u3002\u4e24\u8005\u7684\u878d\u5408\u5e26\u6765\u4e86\u9ad8\u98ce\u9669\u7684\u7cfb\u7edf\u6311\u6218\uff1a\u9700\u8981\u8bbe\u8ba1\u6807\u51c6\u3001\u53ef\u4e92\u64cd\u4f5c\u4e14\u5b89\u5168\u7684\u63a5\u53e3\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u89c2\u5bdf\u94fe\u4e0a\u72b6\u6001\u3001\u5236\u5b9a\u4ea4\u6613\u610f\u56fe\u5e76\u6388\u6743\u6267\u884c\uff0c\u540c\u65f6\u4e0d\u66b4\u9732\u7528\u6237\u3001\u534f\u8bae\u6216\u7ec4\u7ec7\u4e8e\u4e0d\u53ef\u63a5\u53d7\u7684\u5b89\u5168\u3001\u6cbb\u7406\u6216\u7ecf\u6d4e\u98ce\u9669\u4e2d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece3000\u591a\u7bc7\u6587\u732e\u4e2d\u7b5b\u9009\u51fa317\u7bc7\u76f8\u5173\u7814\u7a76\u3002\u6784\u5efa\u4e86\u4e94\u90e8\u5206\u5206\u7c7b\u6cd5\uff08\u53ea\u8bfb\u5206\u6790\u3001\u6a21\u62df\u4e0e\u610f\u56fe\u751f\u6210\u3001\u59d4\u6258\u6267\u884c\u3001\u81ea\u4e3b\u7b7e\u540d\u3001\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff09\u3001\u9488\u5bf9\u667a\u80fd\u4f53\u9a71\u52a8\u4ea4\u6613\u7ba1\u9053\u7684\u5a01\u80c1\u6a21\u578b\uff0c\u4ee5\u53ca\u5305\u542b13\u4e2a\u7ef4\u5ea6\u7684\u6bd4\u8f83\u80fd\u529b\u77e9\u9635\uff0c\u5206\u6790\u4e8620\u591a\u4e2a\u4ee3\u8868\u6027\u7cfb\u7edf\u3002", "result": "\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4ee5\u4e24\u4e2a\u63a5\u53e3\u62bd\u8c61\u4e3a\u6838\u5fc3\u7684\u7814\u7a76\u8def\u7ebf\u56fe\uff1a1\uff09\u4ea4\u6613\u610f\u56fe\u6a21\u5f0f\uff08\u7528\u4e8e\u4fbf\u643a\u548c\u65e0\u6b67\u4e49\u7684\u76ee\u6807\u89c4\u8303\uff09\uff1b2\uff09\u653f\u7b56\u51b3\u7b56\u8bb0\u5f55\uff08\u7528\u4e8e\u8de8\u6267\u884c\u73af\u5883\u7684\u53ef\u5ba1\u8ba1\u3001\u53ef\u9a8c\u8bc1\u653f\u7b56\u6267\u884c\uff09\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u5957\u4ef6\u548c\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u3002", "conclusion": "AI\u667a\u80fd\u4f53\u4e0e\u533a\u5757\u94fe\u7684\u878d\u5408\u9700\u8981\u7cfb\u7edf\u5316\u7684\u63a5\u53e3\u8bbe\u8ba1\u548c\u5b89\u5168\u6846\u67b6\u3002\u8bba\u6587\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u3001\u5a01\u80c1\u6a21\u578b\u548c\u6bd4\u8f83\u5206\u6790\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7406\u89e3\uff0c\u800c\u4ea4\u6613\u610f\u56fe\u6a21\u5f0f\u548c\u653f\u7b56\u51b3\u7b56\u8bb0\u5f55\u4e24\u5927\u62bd\u8c61\u4e3a\u89e3\u51b3\u4e92\u64cd\u4f5c\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u5177\u4f53\u65b9\u5411\uff0c\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u5957\u4ef6\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.04620", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04620", "abs": "https://arxiv.org/abs/2601.04620", "authors": ["Di Zhang"], "title": "AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering", "comment": null, "summary": "Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \\textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \\textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.", "AI": {"tldr": "AgentDevel\uff1a\u5c06LLM\u667a\u80fd\u4f53\u6539\u8fdb\u91cd\u6784\u4e3a\u53d1\u5e03\u5de5\u7a0b\uff0c\u901a\u8fc7\u5916\u90e8\u5316\u56de\u5f52\u611f\u77e5\u7684\u53d1\u5e03\u7ba1\u9053\u5b9e\u73b0\u7a33\u5b9a\u3001\u53ef\u5ba1\u8ba1\u7684\u6539\u8fdb\uff0c\u800c\u975e\u4f9d\u8d56\u667a\u80fd\u4f53\u5185\u90e8\u81ea\u6539\u8fdb\u6216\u5e76\u53d1\u53d8\u4f53\u641c\u7d22\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u6539\u8fdb\u65b9\u6cd5\uff08\u5982\u5d4c\u5165\u81ea\u6539\u8fdb\u673a\u5236\u6216\u641c\u7d22\u5e76\u53d1\u53d8\u4f53\uff09\u867d\u7136\u80fd\u63d0\u9ad8\u603b\u4f53\u5206\u6570\uff0c\u4f46\u5f80\u5f80\u4ea7\u751f\u4e0d\u7a33\u5b9a\u4e14\u96be\u4ee5\u5ba1\u8ba1\u7684\u6539\u8fdb\u8f68\u8ff9\uff0c\u96be\u4ee5\u4fdd\u8bc1\u975e\u56de\u5f52\u6027\u6216\u8de8\u7248\u672c\u6545\u969c\u63a8\u7406\u3002", "method": "\u63d0\u51faAgentDevel\u53d1\u5e03\u5de5\u7a0b\u7ba1\u9053\uff1a1\uff09\u8fd0\u884c\u5f53\u524d\u667a\u80fd\u4f53\uff1b2\uff09\u4ece\u6267\u884c\u8f68\u8ff9\u751f\u6210\u5b9e\u73b0\u65e0\u5173\u7684\u75c7\u72b6\u7ea7\u8d28\u91cf\u4fe1\u53f7\uff1b3\uff09\u901a\u8fc7\u53ef\u6267\u884c\u8bca\u65ad\u5408\u6210\u5355\u4e2a\u53d1\u5e03\u5019\u9009\u7248\u672c\uff1b4\uff09\u5728\u7ffb\u8f6c\u4e2d\u5fc3\u95e8\u63a7\u4e0b\u8fdb\u884c\u7248\u672c\u63d0\u5347\u3002\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ec\uff1a\u5b9e\u73b0\u65e0\u5173\u7684LLM\u6279\u8bc4\u5668\u3001\u57fa\u4e8e\u811a\u672c\u7684\u53ef\u6267\u884c\u8bca\u65ad\u3001\u7ffb\u8f6c\u4e2d\u5fc3\u95e8\u63a7\u3002", "result": "\u5728\u9762\u5411\u6267\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentDevel\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u6539\u8fdb\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u56de\u5f52\uff0c\u540c\u65f6\u4ea7\u751f\u53ef\u91cd\u73b0\u3001\u53ef\u5ba1\u8ba1\u7684\u5de5\u4ef6\u3002", "conclusion": "AgentDevel\u4e3a\u6784\u5efa\u3001\u8c03\u8bd5\u548c\u53d1\u5e03LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5f00\u53d1\u89c4\u8303\uff0c\u5c06\u667a\u80fd\u4f53\u89c6\u4e3a\u53ef\u4ea4\u4ed8\u5de5\u4ef6\uff0c\u5c06\u6539\u8fdb\u5916\u90e8\u5316\u4e3a\u56de\u5f52\u611f\u77e5\u7684\u53d1\u5e03\u7ba1\u9053\u3002", "topic": "agent analysis"}}
{"id": "2601.04631", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.04631", "abs": "https://arxiv.org/abs/2601.04631", "authors": ["Etienne Casanova", "R. Michael Alvarez"], "title": "Beyond the \"Truth\": Investigating Election Rumors on Truth Social During the 2024 Election", "comment": null, "summary": "Large language models (LLMs) offer unprecedented opportunities for analyzing social phenomena at scale. This paper demonstrates the value of LLMs in psychological measurement by (1) compiling the first large-scale dataset of election rumors on a niche alt-tech platform, (2) developing a multistage Rumor Detection Agent that leverages LLMs for high-precision content classification, and (3) quantifying the psychological dynamics of rumor propagation, specifically the \"illusory truth effect\" in a naturalistic setting. The Rumor Detection Agent combines (i) a synthetic data-augmented, fine-tuned RoBERTa classifier, (ii) precision keyword filtering, and (iii) a two-pass LLM verification pipeline using GPT-4o mini. The findings reveal that sharing probability rises steadily with each additional exposure, providing large-scale empirical evidence for dose-response belief reinforcement in ideologically homogeneous networks. Simulation results further demonstrate rapid contagion effects: nearly one quarter of users become \"infected\" within just four propagation iterations. Taken together, these results illustrate how LLMs can transform psychological science by enabling the rigorous measurement of belief dynamics and misinformation spread in massive, real-world datasets.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86LLMs\u5728\u5fc3\u7406\u5b66\u6d4b\u91cf\u4e2d\u7684\u4ef7\u503c\uff0c\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u9009\u4e3e\u8c23\u8a00\u6570\u636e\u96c6\u3001\u5f00\u53d1\u591a\u9636\u6bb5\u8c23\u8a00\u68c0\u6d4b\u4ee3\u7406\uff0c\u5e76\u5728\u81ea\u7136\u73af\u5883\u4e2d\u91cf\u5316\u8c23\u8a00\u4f20\u64ad\u7684\u5fc3\u7406\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\"\u865a\u5e7b\u771f\u76f8\u6548\u5e94\"\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5927\u89c4\u6a21\u5206\u6790\u793e\u4f1a\u73b0\u8c61\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u673a\u4f1a\u3002\u672c\u6587\u65e8\u5728\u5c55\u793aLLMs\u5728\u5fc3\u7406\u5b66\u6d4b\u91cf\u4e2d\u7684\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u8c23\u8a00\u4f20\u64ad\u7684\u5fc3\u7406\u52a8\u529b\u5b66\u7814\u7a76\u65b9\u9762\uff0c\u586b\u8865\u4e86\u5728\u81ea\u7136\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "1) \u5728\u5229\u57faalt-tech\u5e73\u53f0\u4e0a\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u9009\u4e3e\u8c23\u8a00\u6570\u636e\u96c6\uff1b2) \u5f00\u53d1\u591a\u9636\u6bb5\u8c23\u8a00\u68c0\u6d4b\u4ee3\u7406\uff0c\u7ed3\u5408(i)\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u5fae\u8c03RoBERTa\u5206\u7c7b\u5668\u3001(ii)\u7cbe\u786e\u5173\u952e\u8bcd\u8fc7\u6ee4\u3001(iii)\u4e24\u9636\u6bb5LLM\u9a8c\u8bc1\u7ba1\u9053(GPT-4o mini)\uff1b3) \u91cf\u5316\u8c23\u8a00\u4f20\u64ad\u7684\u5fc3\u7406\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\"\u865a\u5e7b\u771f\u76f8\u6548\u5e94\"\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u5206\u4eab\u6982\u7387\u968f\u7740\u6bcf\u6b21\u989d\u5916\u66dd\u5149\u7a33\u6b65\u4e0a\u5347\uff0c\u4e3a\u610f\u8bc6\u5f62\u6001\u540c\u8d28\u7f51\u7edc\u4e2d\u5242\u91cf-\u53cd\u5e94\u4fe1\u5ff5\u5f3a\u5316\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc1\u636e\uff1b2) \u6a21\u62df\u7ed3\u679c\u663e\u793a\u5feb\u901f\u4f20\u67d3\u6548\u5e94\uff1a\u4ec5\u56db\u6b21\u4f20\u64ad\u8fed\u4ee3\u540e\uff0c\u8fd1\u56db\u5206\u4e4b\u4e00\u7528\u6237\u88ab\"\u611f\u67d3\"\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eLLMs\u80fd\u591f\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u4fe1\u5ff5\u52a8\u6001\u548c\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u7684\u4e25\u683c\u6d4b\u91cf\uff0c\u4ece\u800c\u6539\u53d8\u5fc3\u7406\u79d1\u5b66\u7684\u7814\u7a76\u65b9\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2601.04670", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04670", "abs": "https://arxiv.org/abs/2601.04670", "authors": ["Akiyoshi Tomihari"], "title": "Learning Dynamics in RL Post-Training for Language Models", "comment": null, "summary": "Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCF-RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5148\u66f4\u65b0\u5206\u7c7b\u5668\u6765\u6539\u5584RL\u540e\u8bad\u7ec3\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u89e3\u51b3\u8f93\u51fa\u591a\u6837\u6027\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "RL\u540e\u8bad\u7ec3\u867d\u7136\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u4e0b\u964d\uff0c\u8fd9\u4e00\u73b0\u8c61\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\u3002\u4f5c\u8005\u5e0c\u671b\u4ece\u5b66\u4e60\u52a8\u6001\u89d2\u5ea6\u7406\u89e3RL\u540e\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u7ecf\u9a8c\u795e\u7ecf\u6b63\u5207\u6838(NTK)\u6846\u67b6\u5206\u6790RL\u5b66\u4e60\u52a8\u6001\uff0c\u5c06NTK\u5206\u89e3\u4e3a\u4e24\u4e2a\u7ec4\u4ef6\u3002\u63d0\u51faCF-RL\u65b9\u6cd5\uff1a\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u4f18\u5148\u66f4\u65b0\u5206\u7c7b\u5668\uff0c\u518d\u8fdb\u884c\u6807\u51c6RL\u4f18\u5316\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7279\u5f81\u8868\u793a\u6709\u9650\u53d8\u5f02\u6027\u5bfc\u81f4RL\u66f4\u65b0\u7cfb\u7edf\u6027\u589e\u52a0\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u89e3\u91ca\u4e86\u8f93\u51fa\u591a\u6837\u6027\u4e0b\u964d\u3002CF-RL\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u663e\u793a\u6a21\u578b\u7f6e\u4fe1\u5ea6\u589e\u52a0\u548c\u4f18\u5316\u52a0\u901f\u3002", "conclusion": "\u7814\u7a76\u5f62\u5f0f\u5316\u4e86RL\u540e\u8bad\u7ec3\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u63d0\u51fa\u7684CF-RL\u65b9\u6cd5\u4e3a\u6539\u8fdbRL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5176\u673a\u5236\u4e0e\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u7ebf\u6027\u63a2\u6d4b-\u5fae\u8c03\u4e0d\u540c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04686", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.04686", "abs": "https://arxiv.org/abs/2601.04686", "authors": ["Oluwatosin Oseni", "Shengjie Wang", "Jun Zhu", "Micah Corah"], "title": "Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead", "comment": "RSS'25: Multi-Objective Optimization and Planning in Robotics Workshop: 5 pages, 8 figures", "summary": "Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.", "AI": {"tldr": "Nightmare Dreamer\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u6f5c\u5728\u5b89\u5168\u8fdd\u89c4\u6765\u786e\u4fdd\u5b89\u5168\u6027\uff0c\u5728Safety Gymnasium\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u96f6\u5b89\u5168\u8fdd\u89c4\u548c\u9ad8\u6548\u7387", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u8db3\u591f\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u5176\u5e94\u7528\u4ecd\u7136\u53d7\u9650\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "Nightmare Dreamer\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\u6765\u9884\u6d4b\u6f5c\u5728\u7684\u5b89\u5168\u8fdd\u89c4\uff0c\u5e76\u636e\u6b64\u89c4\u5212\u884c\u52a8\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u56fe\u50cf\u89c2\u6d4b\uff0c\u5728Safety Gymnasium\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Nightmare Dreamer\u5728Safety Gymnasium\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u96f6\u5b89\u5168\u8fdd\u89c4\uff0c\u540c\u65f6\u6700\u5927\u5316\u5956\u52b1\u3002\u4e0e\u65e0\u6a21\u578b\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6548\u7387\u63d0\u9ad8\u4e86\u8fd120\u500d\u3002", "conclusion": "Nightmare Dreamer\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5b89\u5168\u8fdd\u89c4\uff0c\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04688", "categories": ["cs.CL", "cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.04688", "abs": "https://arxiv.org/abs/2601.04688", "authors": ["Yanming Liu", "Xinyue Peng", "Jiannan Cao", "Xinyi Wang", "Songhang Deng", "Jintao Chen", "Jianwei Yin", "Xuhong Zhang"], "title": "ToolGate: Contract-Grounded and Verified Tool Execution for LLMs", "comment": "First version of ToolGate", "summary": "Large Language Models (LLMs) augmented with external tools have demonstrated remarkable capabilities in complex reasoning tasks. However, existing frameworks rely heavily on natural language reasoning to determine when tools can be invoked and whether their results should be committed, lacking formal guarantees for logical safety and verifiability. We present \\textbf{ToolGate}, a forward execution framework that provides logical safety guarantees and verifiable state evolution for LLM tool calling. ToolGate maintains an explicit symbolic state space as a typed key-value mapping representing trusted world information throughout the reasoning process. Each tool is formalized as a Hoare-style contract consisting of a precondition and a postcondition, where the precondition gates tool invocation by checking whether the current state satisfies the required conditions, and the postcondition determines whether the tool's result can be committed to update the state through runtime verification. Our approach guarantees that the symbolic state evolves only through verified tool executions, preventing invalid or hallucinated results from corrupting the world representation. Experimental validation demonstrates that ToolGate significantly improves the reliability and verifiability of tool-augmented LLM systems while maintaining competitive performance on complex multi-step reasoning tasks. This work establishes a foundation for building more trustworthy and debuggable AI systems that integrate language models with external tools.", "AI": {"tldr": "ToolGate\u662f\u4e00\u4e2a\u4e3aLLM\u5de5\u5177\u8c03\u7528\u63d0\u4f9b\u903b\u8f91\u5b89\u5168\u4fdd\u8bc1\u548c\u53ef\u9a8c\u8bc1\u72b6\u6001\u6f14\u5316\u7684\u524d\u5411\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u5de5\u5177\u5408\u7ea6\u548c\u8fd0\u884c\u65f6\u9a8c\u8bc1\u786e\u4fdd\u72b6\u6001\u6f14\u5316\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5de5\u5177\u8c03\u7528\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u6765\u51b3\u5b9a\u4f55\u65f6\u8c03\u7528\u5de5\u5177\u4ee5\u53ca\u662f\u5426\u63d0\u4ea4\u7ed3\u679c\uff0c\u7f3a\u4e4f\u903b\u8f91\u5b89\u5168\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u53ef\u80fd\u5bfc\u81f4\u65e0\u6548\u6216\u5e7b\u89c9\u7ed3\u679c\u6c61\u67d3\u4e16\u754c\u8868\u793a\u3002", "method": "ToolGate\u7ef4\u62a4\u663e\u5f0f\u7b26\u53f7\u72b6\u6001\u7a7a\u95f4\u4f5c\u4e3a\u7c7b\u578b\u5316\u952e\u503c\u6620\u5c04\uff0c\u5c06\u6bcf\u4e2a\u5de5\u5177\u5f62\u5f0f\u5316\u4e3aHoare\u98ce\u683c\u7684\u5408\u7ea6\uff08\u524d\u7f6e\u6761\u4ef6\u548c\u540e\u7f6e\u6761\u4ef6\uff09\uff0c\u524d\u7f6e\u6761\u4ef6\u63a7\u5236\u5de5\u5177\u8c03\u7528\uff0c\u540e\u7f6e\u6761\u4ef6\u901a\u8fc7\u8fd0\u884c\u65f6\u9a8c\u8bc1\u51b3\u5b9a\u662f\u5426\u63d0\u4ea4\u7ed3\u679c\u66f4\u65b0\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660eToolGate\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u589e\u5f3aLLM\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u540c\u65f6\u5728\u590d\u6742\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6784\u5efa\u66f4\u53ef\u4fe1\u548c\u53ef\u8c03\u8bd5\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u4fdd\u8bc1\u786e\u4fdd\u72b6\u6001\u6f14\u5316\u7684\u5b89\u5168\u6027\u3002", "topic": "code agent"}}
{"id": "2601.04694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04694", "abs": "https://arxiv.org/abs/2601.04694", "authors": ["Zhilun Zhou", "Zihan Liu", "Jiahe Liu", "Qingyu Shao", "Yihan Wang", "Kun Shao", "Depeng Jin", "Fengli Xu"], "title": "ResMAS: Resilience Optimization in LLM-based Multi-agent Systems", "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.", "AI": {"tldr": "\u63d0\u51faResMAS\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u589e\u5f3aLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u97e7\u6027\uff1a1) \u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u9884\u6d4b\u7cfb\u7edf\u97e7\u6027\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u8bbe\u8ba1\u97e7\u6027\u62d3\u6251\uff1b2) \u57fa\u4e8e\u62d3\u6251\u4f18\u5316\u667a\u80fd\u4f53\u63d0\u793a\u3002\u5b9e\u9a8c\u8bc1\u660e\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u5728\u5404\u79cd\u6270\u52a8\u4e0b\u7684\u97e7\u6027\u3002", "motivation": "LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u5206\u5e03\u5728\u4e0d\u540c\u7684\u8bbe\u5907\u6216\u73af\u5883\u4e2d\uff0c\u5bb9\u6613\u53d7\u5230\u667a\u80fd\u4f53\u6545\u969c\u7b49\u6270\u52a8\u7684\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u653b\u51fb\u540e\u7684\u88ab\u52a8\u68c0\u6d4b\u548c\u7f13\u89e3\uff0c\u800c\u4e0d\u662f\u4e3b\u52a8\u8bbe\u8ba1\u5177\u6709\u5185\u5728\u97e7\u6027\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faResMAS\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u9884\u6d4bMAS\u97e7\u6027\uff0c\u57fa\u4e8e\u6b64\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u62d3\u6251\u751f\u6210\u5668\u81ea\u52a8\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u7684\u97e7\u6027\u62d3\u6251\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u62d3\u6251\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u6839\u636e\u667a\u80fd\u4f53\u7684\u8fde\u63a5\u548c\u4ea4\u4e92\u5173\u7cfb\u4f18\u5316\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86MAS\u7684\u97e7\u6027\u3002\u6b64\u5916\uff0c\u6846\u67b6\u5bf9\u65b0\u4efb\u52a1\u548c\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ResMAS\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u62d3\u6251\u8bbe\u8ba1\u548c\u62d3\u6251\u611f\u77e5\u63d0\u793a\u4f18\u5316\uff0c\u80fd\u591f\u6709\u6548\u589e\u5f3aLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u97e7\u6027\uff0c\u4e3a\u6784\u5efa\u97e7\u6027MAS\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.04700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04700", "abs": "https://arxiv.org/abs/2601.04700", "authors": ["Mukesh Ghimire", "Aosong Feng", "Liwen You", "Youzhi Luo", "Fang Liu", "Xuan Zhu"], "title": "PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards", "comment": "Preprint. Under Review", "summary": "Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7ed3\u5408\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u6765\u6307\u5bfc\u65e0\u6807\u7b7e\u6570\u636e\u7684\u5b66\u4e60\uff0c\u89e3\u51b3\u73b0\u6709\u5185\u90e8\u4e00\u81f4\u6027\u4fe1\u53f7\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u540e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u76d1\u7763\u6216\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u4f46\u968f\u7740\u6a21\u578b\u80fd\u529b\u63d0\u5347\uff0c\u83b7\u53d6\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6a21\u578b\u5185\u90e8\u4e00\u81f4\u6027\uff08\u5982\u591a\u6570\u6295\u7968\u6216\u7f6e\u4fe1\u5ea6\uff09\u4f5c\u4e3a\u5b66\u4e60\u4fe1\u53f7\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u5728\u5927\u89c4\u6a21\u548c\u957f\u671f\u8bad\u7ec3\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faPRISM\u6846\u67b6\uff0c\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7ed3\u5408\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u6765\u6307\u5bfc\u5b66\u4e60\u3002PRM\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u800c\u975e\u6700\u7ec8\u7b54\u6848\uff0c\u4e0e\u81ea\u6211\u7f6e\u4fe1\u5ea6\u6709\u6548\u7ed3\u5408\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u66f4\u597d\u6027\u80fd\u3002", "result": "PRISM\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u66f4\u597d\u7684\u6d4b\u8bd5\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\u7684\u53ef\u63a7\u6027\u3002\u6709\u6548\u7ed3\u5408PRM\u548c\u81ea\u6211\u7f6e\u4fe1\u5ea6\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u5185\u90e8\u4e00\u81f4\u6027\u4fe1\u53f7\u3002", "conclusion": "PRISM\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u548c\u5185\u90e8\u7f6e\u4fe1\u5ea6\u7684\u7ed3\u5408\uff0c\u4e3a\u65e0\u6807\u7b7e\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5185\u90e8\u4e00\u81f4\u6027\u4fe1\u53f7\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u53d1\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04703", "abs": "https://arxiv.org/abs/2601.04703", "authors": ["Yiqun Chen", "Lingyong Yan", "Zixuan Yang", "Erhan Zhang", "Jiashu Zhao", "Shuaiqiang Wang", "Dawei Yin", "Jiaxin Mao"], "title": "Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search", "comment": null, "summary": "Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \\textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}", "AI": {"tldr": "M-ASK\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u641c\u7d22\u6846\u67b6\uff0c\u5c06\u641c\u7d22\u8fc7\u7a0b\u5206\u89e3\u4e3a\u641c\u7d22\u884c\u4e3a\u667a\u80fd\u4f53\u548c\u77e5\u8bc6\u7ba1\u7406\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u89e3\u51b3\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u641c\u7d22\u4e2d\u7684\u7ed3\u6784\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u5728\u591a\u8df3QA\u4efb\u52a1\u4e0a\u53d6\u5f97\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u641c\u7d22\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5355\u4e00\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5b58\u5728\u7ed3\u6784\u74f6\u9888\uff1a\u63a8\u7406\u8f93\u51fa\u4e0d\u53d7\u7ea6\u675f\u5bfc\u81f4\u8f68\u8ff9\u81a8\u80c0\u3001\u7a00\u758f\u7684\u7ed3\u679c\u7ea7\u5956\u52b1\u4f7f\u4fe1\u7528\u5206\u914d\u590d\u6742\u5316\u3001\u968f\u673a\u641c\u7d22\u566a\u58f0\u7834\u574f\u5b66\u4e60\u7a33\u5b9a\u6027\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u63d0\u51faM-ASK\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u641c\u7d22\u660e\u786e\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e92\u8865\u89d2\u8272\uff1a\u641c\u7d22\u884c\u4e3a\u667a\u80fd\u4f53\uff08\u8d1f\u8d23\u89c4\u5212\u548c\u6267\u884c\u641c\u7d22\u52a8\u4f5c\uff09\u548c\u77e5\u8bc6\u7ba1\u7406\u667a\u80fd\u4f53\uff08\u8d1f\u8d23\u805a\u5408\u3001\u8fc7\u6ee4\u548c\u7ef4\u62a4\u7d27\u51d1\u7684\u5185\u90e8\u4e0a\u4e0b\u6587\uff09\u3002\u91c7\u7528\u56de\u5408\u7ea7\u5956\u52b1\u4e3a\u641c\u7d22\u51b3\u7b56\u548c\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\u3002", "result": "\u5728\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM-ASK\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u7b54\u6848\u51c6\u786e\u7387\uff0c\u800c\u4e14\u8bad\u7ec3\u52a8\u6001\u663e\u8457\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u901a\u8fc7\u89d2\u8272\u5206\u79bb\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0cM-ASK\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u5355\u667a\u80fd\u4f53\u641c\u7d22\u7684\u7ed3\u6784\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7a33\u5b9a\u7684\u4fe1\u606f\u641c\u7d22\u3002", "topic": "agent analysis"}}
{"id": "2601.04716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04716", "abs": "https://arxiv.org/abs/2601.04716", "authors": ["Yonghyun Jun", "Junhyuk Choi", "Jihyeong Park", "Hwanhee Lee"], "title": "Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents", "comment": "27 pages", "summary": "Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \\textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \\textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \\textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \\textit{\"Fame Fades\"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \\textit{\"Nature Remains\"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u89d2\u8272\u8eab\u4efd\"\u6982\u5ff5\uff0c\u5c06\u89d2\u8272\u5206\u89e3\u4e3a\u53c2\u6570\u5316\u8eab\u4efd\uff08\u9884\u8bad\u7ec3\u77e5\u8bc6\uff09\u548c\u5c5e\u6027\u8eab\u4efd\uff08\u884c\u4e3a\u7279\u5f81\uff09\uff0c\u53d1\u73b0\u540d\u4eba\u89d2\u8272\u4f18\u52bf\u968f\u5bf9\u8bdd\u8f6e\u6b21\u6d88\u5931\uff0c\u800c\u8d1f\u9762\u793e\u4f1a\u5c5e\u6027\u662f\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u4fdd\u771f\u5ea6\u7684\u4e3b\u8981\u74f6\u9888\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89d2\u8272\u626e\u6f14\u4ee3\u7406\uff08RPAs\uff09\u5bf9\u89d2\u8272\u8eab\u4efd\u7684\u7ed3\u6784\u7ef4\u5ea6\u5b9a\u4e49\u8f83\u5f31\uff0c\u901a\u5e38\u5c06\u89d2\u8272\u89c6\u4e3a\u4efb\u610f\u6587\u672c\u8f93\u5165\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u89d2\u8272\u8eab\u4efd\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u89d2\u8272\u8eab\u4efd\u7684\u591a\u7ef4\u6784\u9020\u6982\u5ff5\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u89d2\u8272\u6863\u6848\u6a21\u5f0f\uff0c\u5728\u76f8\u540c\u7ed3\u6784\u7ea6\u675f\u4e0b\u751f\u6210\u540d\u4eba\u548c\u5408\u6210\u89d2\u8272\uff0c\u901a\u8fc7\u5355\u8f6e\u548c\u591a\u8f6e\u4ea4\u4e92\u8bc4\u4f30\u4e24\u4e2a\u8eab\u4efd\u5c42\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\uff1a1)\"\u540d\u58f0\u6d88\u9000\"\uff1a\u540d\u4eba\u89d2\u8272\u5728\u521d\u59cb\u8f6e\u6b21\u6709\u4f18\u52bf\uff0c\u4f46\u968f\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a0\uff0c\u6a21\u578b\u4f18\u5148\u7d2f\u79ef\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u800c\u975e\u9884\u8bad\u7ec3\u5148\u9a8c\u77e5\u8bc6\uff1b2)\"\u672c\u6027\u96be\u79fb\"\uff1a\u6a21\u578b\u80fd\u7a33\u5b9a\u8868\u73b0\u4e00\u822c\u4eba\u683c\u7279\u8d28\uff0c\u4f46RPA\u6027\u80fd\u5bf9\u9053\u5fb7\u548c\u4eba\u9645\u5173\u7cfb\u7684\u6548\u4ef7\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u8d1f\u9762\u793e\u4f1a\u672c\u6027\u662fRPA\u4fdd\u771f\u5ea6\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u89d2\u8272\u6784\u5efa\u548c\u8bc4\u4f30\u63d0\u4f9b\u6307\u5bfc\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.04714", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04714", "abs": "https://arxiv.org/abs/2601.04714", "authors": ["Chang Zhao", "Zheming Yang", "Yunqing Hu", "Qi Guo", "Zijian Wang", "Pengcheng Li", "Wen Ji"], "title": "ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving", "comment": null, "summary": "With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.", "AI": {"tldr": "ThinkDrive\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684CoT\u5f15\u5bfc\u6e10\u8fdb\u5f0fRL\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08CoT SFT + \u96be\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\uff09\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u975e\u7ed3\u6784\u5316\u63a8\u7406\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u610f\u56fe\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u65b9\u6cd5\u5b58\u5728\u975e\u7ed3\u6784\u5316\u63a8\u7406\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u610f\u56fe\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u867d\u7136CoT\u63a8\u7406\u80fd\u63d0\u5347\u51b3\u7b56\u900f\u660e\u5ea6\uff0c\u4f46\u4f20\u7edfSFT\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u6f5c\u529b\uff0c\u800cRL\u65b9\u6cd5\u5219\u9762\u4e34\u4e0d\u7a33\u5b9a\u548c\u63a8\u7406\u6df1\u5ea6\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faThinkDrive\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u4f7f\u7528CoT\u89e3\u91ca\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff1b2\uff09\u5e94\u7528\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u96be\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u5668\uff0c\u6839\u636e\u6837\u672c\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u5f3a\u5ea6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cThinkDrive\u5728exam\u3001easy-exam\u548caccuracy\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u5f3aRL\u57fa\u7ebf\u63d0\u53471.45%\u30011.95%\u548c1.01%\u3002\u4f7f\u7528\u8be5\u65b9\u6cd5\u8bad\u7ec3\u76842B\u53c2\u6570\u6a21\u578b\u5728exam\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u66f4\u5927\u7684GPT-4o\u6a21\u578b3.28%\u3002", "conclusion": "ThinkDrive\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u63a8\u7406\u548c\u96be\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u3001\u6cdb\u5316\u80fd\u529b\u548c\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u4e00\u81f4\u6027\uff0c\u4e3aLLM\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04786", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04786", "abs": "https://arxiv.org/abs/2601.04786", "authors": ["Lang Feng", "Fuchao Yang", "Feng Chen", "Xin Cheng", "Haiyang Xu", "Zhenglin Wan", "Ming Yan", "Bo An"], "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression", "comment": "Work in progress", "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.", "AI": {"tldr": "AgentOCR\u6846\u67b6\u901a\u8fc7\u5c06\u6587\u672c\u5386\u53f2\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u89c6\u89c9\u56fe\u50cf\u8868\u793a\uff0c\u7ed3\u5408\u5206\u6bb5\u5149\u5b66\u7f13\u5b58\u548c\u667a\u80fd\u81ea\u538b\u7f29\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u591a\u8f6e\u4ea4\u4e92\u4e2dLLM\u4ee3\u7406\u7684token\u6d88\u8017\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u968f\u7740LLM\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5e94\u7528\uff0c\u6587\u672c\u5386\u53f2\u5feb\u901f\u589e\u957f\u5bfc\u81f4token\u9884\u7b97\u548c\u5185\u5b58\u4f7f\u7528\u6025\u5267\u81a8\u80c0\uff0c\u6210\u4e3a\u5b9e\u9645\u90e8\u7f72\u7684\u74f6\u9888\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8868\u793a\u65b9\u6cd5\u6765\u538b\u7f29\u5386\u53f2\u4fe1\u606f\u3002", "method": "1) \u5c06\u7d2f\u79ef\u7684\u89c2\u5bdf-\u52a8\u4f5c\u5386\u53f2\u6e32\u67d3\u4e3a\u7d27\u51d1\u7684\u89c6\u89c9\u56fe\u50cf\uff1b2) \u63d0\u51fa\u5206\u6bb5\u5149\u5b66\u7f13\u5b58\u673a\u5236\uff0c\u5c06\u5386\u53f2\u5206\u89e3\u4e3a\u53ef\u54c8\u5e0c\u7684\u6bb5\u5e76\u7ef4\u62a4\u89c6\u89c9\u7f13\u5b58\uff0c\u6d88\u9664\u5197\u4f59\u91cd\u6e32\u67d3\uff1b3) \u5f15\u5165\u667a\u80fd\u81ea\u538b\u7f29\uff0c\u8ba9\u4ee3\u7406\u4e3b\u52a8\u8f93\u51fa\u538b\u7f29\u7387\uff0c\u5e76\u901a\u8fc7\u538b\u7f29\u611f\u77e5\u5956\u52b1\u8bad\u7ec3\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u4efb\u52a1\u6210\u529f\u7387\u548ctoken\u6548\u7387\u3002", "result": "\u5728ALFWorld\u548c\u57fa\u4e8e\u641c\u7d22\u7684QA\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgentOCR\u4fdd\u6301\u4e86\u8d85\u8fc795%\u7684\u6587\u672c\u4ee3\u7406\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017(>50%)\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684token\u548c\u5185\u5b58\u6548\u7387\u3002\u5206\u6bb5\u5149\u5b66\u7f13\u5b58\u5e26\u676520\u500d\u6e32\u67d3\u52a0\u901f\uff0c\u81ea\u538b\u7f29\u673a\u5236\u6709\u6548\u5e73\u8861\u4e86\u7b56\u7565\u3002", "conclusion": "AgentOCR\u901a\u8fc7\u89c6\u89c9token\u8868\u793a\u5386\u53f2\u3001\u5206\u6bb5\u7f13\u5b58\u6d88\u9664\u5197\u4f59\u3001\u667a\u80fd\u81ea\u538b\u7f29\u5e73\u8861\u6548\u7387\uff0c\u4e3aLLM\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u591a\u8f6e\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04731", "abs": "https://arxiv.org/abs/2601.04731", "authors": ["Shuyang Jiang", "Yuhao Wang", "Ya Zhang", "Yanfeng Wang", "Yu Wang"], "title": "Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models", "comment": "22 pages", "summary": "Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \\uline{M}ine \\uline{in}trinsic mast\\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \\textbf{4.58} absolute gains in Pass@1 and \\textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.", "AI": {"tldr": "\u63d0\u51faMiner\u65b9\u6cd5\uff0c\u5229\u7528\u7b56\u7565\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u81ea\u76d1\u7763\u5956\u52b1\u4fe1\u53f7\uff0c\u89e3\u51b3\u63a8\u7406\u6a21\u578bRL\u8bad\u7ec3\u4e2d\u6b63\u540c\u8d28\u63d0\u793a\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u65e0\u6279\u8bc4RL\u65b9\u6cd5\u5728\u5904\u7406\u6b63\u540c\u8d28\u63d0\u793a\uff08\u6240\u6709rollout\u90fd\u6b63\u786e\uff09\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4f18\u52bf\u4f30\u8ba1\u4e3a\u96f6\u5bfc\u81f4rollout\u6d6a\u8d39\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMiner\u65b9\u6cd5\uff1a1) \u57fa\u4e8etoken\u7ea7\u7126\u70b9\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u52a8\u6001\u653e\u5927\u5173\u952e\u4e0d\u786e\u5b9atoken\u7684\u68af\u5ea6\uff0c\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1token\uff1b2) \u81ea\u9002\u5e94\u4f18\u52bf\u6821\u51c6\uff0c\u65e0\u7f1d\u6574\u5408\u5185\u5728\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\u3002", "result": "\u5728Qwen3-4B\u548cQwen3-8B\u57fa\u7840\u6a21\u578b\u7684\u516d\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMiner\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4GRPO\u5728Pass@1\u4e0a\u83b7\u5f974.58\u7edd\u5bf9\u589e\u76ca\uff0cPass@K\u4e0a\u83b7\u5f976.66\u589e\u76ca\u3002", "conclusion": "\u6f5c\u5728\u4e0d\u786e\u5b9a\u6027\u5229\u7528\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\u7684\u9ad8\u6548\u53ef\u6269\u5c55RL\u8bad\u7ec3\u65e2\u662f\u5fc5\u8981\u7684\u4e5f\u662f\u5145\u5206\u7684\uff0c\u65b0\u63d0\u51fa\u7684\u4e24\u4e2a\u521b\u65b0\u663e\u793a\u51fa\u4f18\u8d8a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04742", "abs": "https://arxiv.org/abs/2601.04742", "authors": ["Seyeon Jeong", "Yeonjun Choi", "JongWook Kim", "Beakcheol Jang"], "title": "Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval", "comment": null, "summary": "Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.", "AI": {"tldr": "Tool-MAD\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u5206\u914d\u4e0d\u540c\u7684\u5916\u90e8\u5de5\u5177\uff08\u5982\u641c\u7d22API\u6216RAG\u6a21\u5757\uff09\u6765\u589e\u5f3a\u4e8b\u5b9e\u6838\u67e5\u80fd\u529b\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u67e5\u8be2\u673a\u5236\u548c\u57fa\u4e8e\u5fe0\u5b9e\u5ea6\u4e0e\u7b54\u6848\u76f8\u5173\u6027\u7684\u91cf\u5316\u8bc4\u4f30\uff0c\u5728\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u6216\u9759\u6001\u6587\u6863\uff0c\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002\u867d\u7136MADKE\u5f15\u5165\u4e86\u5916\u90e8\u8bc1\u636e\uff0c\u4f46\u5176\u4e00\u6b21\u6027\u68c0\u7d22\u673a\u5236\u65e0\u6cd5\u9002\u5e94\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u65b0\u8bba\u70b9\u6216\u4fe1\u606f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u83b7\u53d6\u548c\u9a8c\u8bc1\u5916\u90e8\u8bc1\u636e\u7684\u6846\u67b6\u6765\u63d0\u9ad8\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u3002", "method": "1) \u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f7f\u7528\u5f02\u6784\u7684\u5916\u90e8\u5de5\u5177\uff08\u5982\u641c\u7d22API\u3001RAG\u6a21\u5757\uff09\uff1b2) \u81ea\u9002\u5e94\u67e5\u8be2\u5236\u5b9a\u673a\u5236\uff0c\u6839\u636e\u8fa9\u8bba\u6d41\u7a0b\u8fed\u4ee3\u4f18\u5316\u8bc1\u636e\u68c0\u7d22\uff1b3) \u5c06\u5fe0\u5b9e\u5ea6\u548c\u7b54\u6848\u76f8\u5173\u6027\u5206\u6570\u6574\u5408\u5230\u6700\u7ec8\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u8ba9\u6cd5\u5b98\u667a\u80fd\u4f53\u80fd\u591f\u91cf\u5316\u8bc4\u4f30\u6bcf\u4e2a\u56de\u7b54\u7684\u8fde\u8d2f\u6027\u548c\u95ee\u9898\u5bf9\u9f50\u5ea6\uff0c\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9\u3002", "result": "\u5728\u56db\u4e2a\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTool-MAD\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684MAD\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.5%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002\u5728\u533b\u5b66\u4e13\u4e1a\u9886\u57df\uff0cTool-MAD\u5728\u5404\u79cd\u5de5\u5177\u914d\u7f6e\u548c\u9886\u57df\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "Tool-MAD\u901a\u8fc7\u96c6\u6210\u5f02\u6784\u5916\u90e8\u5de5\u5177\u548c\u81ea\u9002\u5e94\u68c0\u7d22\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MAD\u6846\u67b6\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u73b0\u5b9e\u5e94\u7528\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.04748", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04748", "abs": "https://arxiv.org/abs/2601.04748", "authors": ["Xiaoxiao Li"], "title": "When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail", "comment": "25 pages, technical report", "summary": "Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?\n  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f16\u8bd1\u4e3a\u5355\u667a\u80fd\u4f53\u6280\u80fd\u5e93\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u6280\u80fd\u9009\u62e9\u5b58\u5728\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u7684\u5bb9\u91cf\u9650\u5236\uff0c\u5e76\u5728\u8fbe\u5230\u4e34\u754c\u5e93\u89c4\u6a21\u540e\u51fa\u73b0\u6027\u80fd\u6025\u5267\u4e0b\u964d\u7684\u76f8\u53d8\u73b0\u8c61\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u5b58\u5728\u663e\u8457\u7684\u901a\u4fe1\u5f00\u9500\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5355\u667a\u80fd\u4f53\u9009\u62e9\u6280\u80fd\u5e93\u7684\u65b9\u5f0f\u83b7\u5f97\u7c7b\u4f3c\u7684\u6a21\u5757\u5316\u4f18\u52bf\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u5c06\u6280\u80fd\u89c6\u4e3a\u5185\u5316\u7684\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f16\u8bd1\u4e3a\u7b49\u6548\u7684\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u6280\u80fd\u9009\u62e9\u66ff\u4ee3\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u3002\u7814\u7a76\u6280\u80fd\u9009\u62e9\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u5206\u6790\u5e93\u89c4\u6a21\u548c\u8bed\u4e49\u6df7\u6dc6\u6027\u5bf9\u9009\u62e9\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5206\u5c42\u8def\u7531\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u6280\u80fd\u9009\u62e9\u5b58\u5728\u5bb9\u91cf\u9650\u5236\uff1a\u51c6\u786e\u6027\u5728\u8fbe\u5230\u4e34\u754c\u5e93\u89c4\u6a21\u524d\u4fdd\u6301\u7a33\u5b9a\uff0c\u968f\u540e\u6025\u5267\u4e0b\u964d\uff0c\u5448\u73b0\u76f8\u53d8\u73b0\u8c61\u3002\u8bed\u4e49\u6df7\u6dc6\u6027\uff08\u800c\u975e\u5355\u7eaf\u5e93\u89c4\u6a21\uff09\u662f\u6027\u80fd\u4e0b\u964d\u7684\u5173\u952e\u56e0\u7d20\u3002\u5206\u5c42\u8def\u7531\u65b9\u6cd5\u663e\u793a\u51fa\u6539\u5584\u6548\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86LLM\u57fa\u4e8e\u8bed\u4e49\u7684\u6280\u80fd\u9009\u62e9\u5b58\u5728\u57fa\u672c\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u6846\u67b6\u548c\u5b9e\u7528\u6307\u5357\uff0c\u4e3a\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u6280\u80fd\u578b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002\u5206\u5c42\u7ec4\u7ec7\u53ef\u80fd\u50cf\u5e2e\u52a9\u4eba\u7c7b\u7ba1\u7406\u590d\u6742\u9009\u62e9\u4e00\u6837\u6709\u76ca\u4e8eAI\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2601.04765", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.04765", "abs": "https://arxiv.org/abs/2601.04765", "authors": ["Santiago Acevedo", "Alessandro Laio", "Marco Baroni"], "title": "Differential syntactic and semantic encoding in LLMs", "comment": null, "summary": "We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.", "AI": {"tldr": "\u901a\u8fc7\u5e73\u5747\u5171\u4eab\u53e5\u6cd5\u7ed3\u6784\u6216\u8bed\u4e49\u7684\u53e5\u5b50\u9690\u85cf\u8868\u793a\u5411\u91cf\uff0c\u83b7\u5f97\u80fd\u6355\u83b7LLM\u8868\u793a\u4e2d\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u5411\u91cf\uff0c\u8868\u660e\u8fd9\u4e9b\u4fe1\u606f\u81f3\u5c11\u90e8\u5206\u7ebf\u6027\u7f16\u7801", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u7279\u522b\u662fDeepSeek-V3\uff09\u5185\u90e8\u5c42\u8868\u793a\u4e2d\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u5982\u4f55\u7f16\u7801\uff0c\u63a2\u7d22\u8fd9\u4e9b\u8bed\u8a00\u4fe1\u606f\u7684\u7f16\u7801\u673a\u5236", "method": "\u901a\u8fc7\u5e73\u5747\u5171\u4eab\u53e5\u6cd5\u7ed3\u6784\u6216\u8bed\u4e49\u7684\u53e5\u5b50\u9690\u85cf\u8868\u793a\u5411\u91cf\u83b7\u5f97\"\u8d28\u5fc3\"\u5411\u91cf\uff0c\u7136\u540e\u4ece\u53e5\u5b50\u5411\u91cf\u4e2d\u51cf\u53bb\u8fd9\u4e9b\u8d28\u5fc3\u6765\u5206\u6790\u5bf9\u76f8\u4f3c\u6027\u7684\u5f71\u54cd\uff0c\u7814\u7a76\u53e5\u6cd5\u548c\u8bed\u4e49\u7684\u7f16\u7801\u7279\u6027", "result": "\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u5728LLM\u8868\u793a\u4e2d\u81f3\u5c11\u90e8\u5206\u7ebf\u6027\u7f16\u7801\uff0c\u51cf\u53bb\u8d28\u5fc3\u4f1a\u663e\u8457\u5f71\u54cd\u4e0e\u53e5\u6cd5/\u8bed\u4e49\u5339\u914d\u53e5\u5b50\u7684\u76f8\u4f3c\u6027\uff1b\u4e0d\u540c\u5c42\u5bf9\u53e5\u6cd5\u548c\u8bed\u4e49\u7684\u7f16\u7801\u6a21\u5f0f\u4e0d\u540c\uff0c\u4e24\u79cd\u4fe1\u53f7\u53ef\u4ee5\u90e8\u5206\u89e3\u8026", "conclusion": "LLM\u5185\u90e8\u8868\u793a\u4e2d\u53e5\u6cd5\u548c\u8bed\u4e49\u4fe1\u606f\u5b58\u5728\u5dee\u5f02\u5316\u7684\u7f16\u7801\u673a\u5236\uff0c\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u64cd\u4f5c\u5206\u79bb\u548c\u5206\u6790\u8fd9\u4e9b\u8bed\u8a00\u4fe1\u606f", "topic": "agent analysis"}}
{"id": "2601.04767", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04767", "abs": "https://arxiv.org/abs/2601.04767", "authors": ["Zefang Zong", "Dingwei Chen", "Yang Li", "Qi Yi", "Bo Zhou", "Chengming Li", "Bo Qian", "Peng Chen", "Jie Jiang"], "title": "AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search", "comment": null, "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "AI": {"tldr": "AT\u00b2PO\uff1a\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u56de\u5408\u5236\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u5f15\u5bfc\u6811\u6269\u5c55\u548c\u56de\u5408\u7ea7\u4fe1\u7528\u5206\u914d\u89e3\u51b3\u591a\u8f6e\u4ee3\u7406RL\u4e2d\u7684\u63a2\u7d22\u591a\u6837\u6027\u4e0d\u8db3\u3001\u7a00\u758f\u5956\u52b1\u5206\u914d\u548c\u7b56\u7565\u4f18\u5316\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u9762\u4e34\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u63a2\u7d22\u591a\u6837\u6027\u6709\u9650\u3001\u7a00\u758f\u4fe1\u7528\u5206\u914d\u56f0\u96be\u3001\u7b56\u7565\u4f18\u5316\u4e0e\u4ee3\u7406\u4ea4\u4e92\u7684\u81ea\u7136\u51b3\u7b56\u7c92\u5ea6\u4e0d\u5bf9\u9f50\u3002", "method": "\u63d0\u51faAT\u00b2PO\u7edf\u4e00\u6846\u67b6\uff1a1\uff09\u56de\u5408\u7ea7\u6811\u7ed3\u6784\u652f\u6301\u71b5\u5f15\u5bfc\u6811\u6269\u5c55\u4ee5\u589e\u5f3a\u63a2\u7d22\uff1b2\uff09\u56de\u5408\u7ea7\u4fe1\u7528\u5206\u914d\u5b9e\u73b0\u7a00\u758f\u5956\u52b1\u7684\u7ec6\u7c92\u5ea6\u4f20\u64ad\uff1b3\uff09\u4ee3\u7406\u56de\u5408\u5236\u7b56\u7565\u4f18\u5316\uff08ATPO\uff09\u76ee\u6807\uff0c\u4f7f\u7b56\u7565\u66f4\u65b0\u4e0e\u4ee3\u7406\u4ea4\u4e92\u7684\u51b3\u7b56\u7c92\u5ea6\u5bf9\u9f50\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u63d0\u5347\u8fbe1.84\u4e2a\u767e\u5206\u70b9\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002", "conclusion": "AT\u00b2PO\u4e3a\u591a\u8f6e\u4ee3\u7406RL\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a2\u7d22\u591a\u6837\u6027\u3001\u4fe1\u7528\u5206\u914d\u548c\u7b56\u7565\u4f18\u5316\u5bf9\u9f50\u95ee\u9898\uff0cATPO\u7ec4\u4ef6\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u4efb\u4f55\u591a\u8f6eRL\u6d41\u7a0b\u4e2d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04794", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04794", "abs": "https://arxiv.org/abs/2601.04794", "authors": ["Chengxin Shi", "Qinnan Cai", "Zeyuan Chen", "Long Zeng", "Yibo Zhao", "Jing Yu", "Jianxiang Yu", "Xiang Li"], "title": "APEX: Academic Poster Editing Agentic Expert", "comment": null, "summary": "Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.", "AI": {"tldr": "APEX\u662f\u9996\u4e2a\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5b66\u672f\u6d77\u62a5\u7f16\u8f91\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u591a\u7ea7API\u7f16\u8f91\u548c\u5ba1\u67e5\u8c03\u6574\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542b514\u6761\u7f16\u8f91\u6307\u4ee4\u7684APEX-Bench\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u8bba\u6587\u5230\u6d77\u62a5\u7684\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u662f\u5355\u6b21\u3001\u975e\u4ea4\u4e92\u5f0f\u7684\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u590d\u6742\u3001\u4e3b\u89c2\u7684\u610f\u56fe\u3002\u5b66\u672f\u6d77\u62a5\u8bbe\u8ba1\u9700\u8981\u9ad8\u5bc6\u5ea6\u5185\u5bb9\u548c\u590d\u6742\u5e03\u5c40\u7684\u7cbe\u786e\u5e73\u8861\uff0c\u4f46\u8fd9\u4e2a\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u3002", "method": "\u63d0\u51faAPEX\u6846\u67b6\uff0c\u652f\u6301\u57fa\u4e8e\u591a\u7ea7API\u7684\u7ec6\u7c92\u5ea6\u7f16\u8f91\u548c\u5ba1\u67e5\u8c03\u6574\u673a\u5236\u3002\u540c\u65f6\u6784\u5efaAPEX-Bench\u57fa\u51c6\uff0c\u5305\u542b514\u6761\u6309\u64cd\u4f5c\u7c7b\u578b\u3001\u96be\u5ea6\u548c\u62bd\u8c61\u7ea7\u522b\u5206\u7c7b\u7684\u7f16\u8f91\u6307\u4ee4\uff0c\u91c7\u7528\u53c2\u8003\u5f15\u5bfc\u548c\u65e0\u53c2\u8003\u7b56\u7565\u786e\u4fdd\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAPEX\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5efa\u7acb\u4e86\u591a\u7ef4\u5ea6VLM-as-a-judge\u8bc4\u4f30\u534f\u8bae\uff0c\u8bc4\u4f30\u6307\u4ee4\u5b8c\u6210\u5ea6\u3001\u4fee\u6539\u8303\u56f4\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "conclusion": "APEX\u4e3a\u5b66\u672f\u6d77\u62a5\u7f16\u8f91\u63d0\u4f9b\u4e86\u9996\u4e2a\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u7cfb\u7edf\u5316\u57fa\u51c6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "code agent"}}
{"id": "2601.04954", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04954", "abs": "https://arxiv.org/abs/2601.04954", "authors": ["Yirong Zeng", "Yufei Liu", "Xiao Ding", "Yutai Hou", "Yuxian Wang", "Haonan Song", "Wu Ning", "Dandan Tu", "Qixun Zhang", "Bibo Cai", "Yuxiang He", "Ting Liu"], "title": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following", "comment": "ACL under review 13 pages, 8 figures", "summary": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u9ad8\u7cbe\u5ea6\u5956\u52b1\u6bd4\u6570\u636e\u591a\u6837\u6027\u66f4\u91cd\u8981\uff0c\u4ec5\u4f7f\u7528\u786c\u7ea6\u675f\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u6df7\u5408\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u5956\u52b1\u7cbe\u5ea6\u7684\u6570\u636e\u4f18\u5316\u7b56\u7565", "motivation": "\u6311\u6218\u5f53\u524d\u4e3b\u6d41\u89c2\u70b9\uff0c\u5373\u8ba4\u4e3a\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u9700\u8981\u591a\u6837\u5316\u7684\u53ef\u9a8c\u8bc1\u786c\u7ea6\u675f\u548c\u4e0d\u53ef\u9a8c\u8bc1\u8f6f\u7ea6\u675f\u6df7\u5408\u624d\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u6307\u4ee4\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\u68c0\u9a8c\u8fd9\u4e00\u5171\u8bc6", "method": "\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u8c03\u67e5\uff0c\u6bd4\u8f83\u786c\u7ea6\u675f\u8bad\u7ec3\u4e0e\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u5206\u6790LLM\u8bc4\u5224\u5668\u7684\u53ec\u56de\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5956\u52b1\u7cbe\u5ea6\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u4f18\u5316\u7b56\u7565", "result": "\u786c\u7ea6\u675f\u8bad\u7ec3\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5956\u52b1\u7cbe\u5ea6\u662f\u6709\u6548\u5bf9\u9f50\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u63d0\u534713.4%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1158%", "conclusion": "\u9700\u8981\u8303\u5f0f\u8f6c\u53d8\uff1a\u4ece\u76f2\u76ee\u8ffd\u6c42\u6570\u636e\u591a\u6837\u6027\u8f6c\u5411\u5173\u6ce8\u9ad8\u7cbe\u5ea6\u5956\u52b1\uff0c\u5956\u52b1\u7cbe\u5ea6\u6bd4\u7ea6\u675f\u591a\u6837\u6027\u5bf9\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u66f4\u91cd\u8981", "topic": "agentic reinforcement learning"}}
{"id": "2601.04795", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.04795", "abs": "https://arxiv.org/abs/2601.04795", "authors": ["Qiang Yu", "Xinran Cheng", "Chuanyi Liu"], "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing", "comment": "20 pages, 3 figures, 5 tables", "summary": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5de5\u5177\u7ed3\u679c\u89e3\u6790\u4e3aLLM\u63d0\u4f9b\u7cbe\u786e\u6570\u636e\u5e76\u8fc7\u6ee4\u6076\u610f\u4ee3\u7801\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u4ece\u6570\u5b57\u52a9\u624b\u8f6c\u53d8\u4e3a\u7269\u7406\u63a7\u5236\u5668\uff0c\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u653b\u51fb\u6210\u529f\u7387\u9ad8\u7b49\u95ee\u9898", "method": "\u901a\u8fc7\u5de5\u5177\u7ed3\u679c\u89e3\u6790\u4e3aLLM\u63d0\u4f9b\u7cbe\u786e\u6570\u636e\uff0c\u540c\u65f6\u6709\u6548\u8fc7\u6ee4\u6ce8\u5165\u7684\u6076\u610f\u4ee3\u7801\uff0c\u7ed3\u5408\u4e86\u7cbe\u786e\u6570\u636e\u63d0\u4f9b\u548c\u6076\u610f\u4ee3\u7801\u8fc7\u6ee4\u7684\u53cc\u91cd\u4f18\u52bf", "result": "\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u653b\u51fb\u4e0b\u5b9e\u7528\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u4f4e\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u4ee3\u7406\u9632\u5fa1\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861", "topic": "agent analysis"}}
{"id": "2601.04790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04790", "abs": "https://arxiv.org/abs/2601.04790", "authors": ["Junhyuk Choi", "Jeongyoun Kwon", "Heeju Kim", "Haeun Cho", "Hayeong Jung", "Sehee Min", "Bugeun Kim"], "title": "Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework", "comment": "Preprint", "summary": "Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u89d2\u8272\u7684\u6743\u5a01\u504f\u89c1\uff0c\u53d1\u73b0\u4e13\u5bb6\u548c\u53c2\u7167\u578b\u6743\u5a01\u89d2\u8272\u6bd4\u5408\u6cd5\u578b\u6743\u5a01\u66f4\u5177\u5f71\u54cd\u529b\uff0c\u6743\u5a01\u504f\u89c1\u4e3b\u8981\u6e90\u4e8e\u6743\u5a01\u89d2\u8272\u575a\u6301\u7acb\u573a\u800c\u975e\u666e\u901a\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u987a\u4ece\u3002", "motivation": "\u5c3d\u7ba1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e38\u901a\u8fc7\u5206\u914d\u6743\u5a01\u89d2\u8272\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u6743\u5a01\u504f\u89c1\u5bf9\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u5206\u6790\u89d2\u8272\u578b\u6743\u5a01\u504f\u89c1\u5728\u81ea\u7531\u5f62\u5f0f\u591a\u667a\u80fd\u4f53\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u57fa\u4e8eFrench\u548cRaven\u7684\u6743\u529b\u7406\u8bba\uff0c\u5c06\u6743\u5a01\u89d2\u8272\u5206\u4e3a\u5408\u6cd5\u578b\u3001\u53c2\u7167\u578b\u548c\u4e13\u5bb6\u578b\u4e09\u7c7b\u3002\u4f7f\u7528ChatEval\u5e73\u53f0\uff0c\u572812\u8f6e\u5bf9\u8bdd\u4e2d\u5206\u6790\u8fd9\u4e9b\u89d2\u8272\u5bf9\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u91c7\u7528GPT-4o\u548cDeepSeek R1\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4e13\u5bb6\u578b\u548c\u53c2\u7167\u578b\u6743\u5a01\u89d2\u8272\u6bd4\u5408\u6cd5\u578b\u6743\u5a01\u89d2\u8272\u66f4\u5177\u5f71\u54cd\u529b\u3002\u6743\u5a01\u504f\u89c1\u4e3b\u8981\u901a\u8fc7\u6743\u5a01\u89d2\u8272\u575a\u6301\u81ea\u8eab\u7acb\u573a\u800c\u666e\u901a\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u6765\u5b9e\u73b0\uff0c\u800c\u975e\u666e\u901a\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u987a\u4ece\u3002\u6b64\u5916\uff0c\u6743\u5a01\u5f71\u54cd\u529b\u9700\u8981\u660e\u786e\u7684\u7acb\u573a\u9648\u8ff0\uff0c\u4e2d\u6027\u56de\u5e94\u65e0\u6cd5\u4ea7\u751f\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6743\u5a01\u504f\u89c1\u7684\u5f62\u6210\u673a\u5236\uff0c\u4e3a\u8bbe\u8ba1\u5177\u6709\u975e\u5bf9\u79f0\u4ea4\u4e92\u6a21\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002\u6743\u5a01\u89d2\u8272\u7684\u7c7b\u578b\u548c\u8868\u8fbe\u65b9\u5f0f\u663e\u8457\u5f71\u54cd\u7cfb\u7edf\u52a8\u6001\u3002", "topic": "agent analysis"}}
{"id": "2601.04805", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04805", "abs": "https://arxiv.org/abs/2601.04805", "authors": ["Siyuan Gan", "Jiaheng Liu", "Boyan Wang", "Tianpei Yang", "Runqing Miao", "Yuyao Zhang", "Fanyu Meng", "Junlan Feng", "Linjian Meng", "Jing Huo", "Yang Gao"], "title": "Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning", "comment": null, "summary": "Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT's responses, which are classified as not using thinking, remains below 10% across all tested datasets.", "AI": {"tldr": "TNT\u65b9\u6cd5\u901a\u8fc7\u57fa\u4e8e\u601d\u8003\u7684\u89e3\u51b3\u65b9\u6848\u4fe1\u606f\u52a8\u6001\u8bbe\u7f6e\u975e\u601d\u8003\u54cd\u5e94\u7684\u6700\u5927token\u9650\u5236\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u6b3a\u9a97\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u957f\u94fe\u601d\u8003\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\u3002\u73b0\u6709\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6df7\u5408\u63a8\u7406\u6a21\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u5956\u52b1\u6b3a\u9a97\u95ee\u9898\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u6210\u672c\u9ad8\uff0c\u7edf\u4e00token\u9650\u5236\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faThinking-Based Non-Thinking (TNT)\u65b9\u6cd5\uff0c\u4e0d\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff0c\u800c\u662f\u5229\u7528\u601d\u8003\u54cd\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u4fe1\u606f\u4e3a\u4e0d\u540c\u67e5\u8be2\u8bbe\u7f6e\u4e0d\u540c\u7684\u975e\u601d\u8003\u54cd\u5e94\u6700\u5927token\u4f7f\u7528\u9650\u5236\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTNT\u76f8\u6bd4DeepSeek-R1-Distill-Qwen-1.5B/7B\u548cDeepScaleR-1.5B\u51cf\u5c11\u7ea650%\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4e2d\u8fbe\u5230\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u6700\u4f18\u6743\u8861\u3002", "conclusion": "TNT\u901a\u8fc7\u52a8\u6001token\u9650\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u6b3a\u9a97\u95ee\u9898\uff0c\u5728\u975e\u601d\u8003\u54cd\u5e94\u4e2d\u5956\u52b1\u6b3a\u9a97\u6982\u7387\u4fdd\u6301\u572810%\u4ee5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04809", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04809", "abs": "https://arxiv.org/abs/2601.04809", "authors": ["Caijun Xu", "Changyi Xiao", "Zhongyuan Peng", "Xinrun Wang", "Yixin Cao"], "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning", "comment": "19 pages,5 figures", "summary": "Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.", "AI": {"tldr": "SCALER\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u9002\u5e94\u73af\u5883\u8bbe\u8ba1\u7ef4\u6301\u6709\u6548\u5b66\u4e60\u4fe1\u53f7\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5c06\u771f\u5b9e\u7f16\u7a0b\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u63a8\u7406\u73af\u5883\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u96be\u5ea6\u548c\u73af\u5883\u9009\u62e9\u6765\u652f\u6301\u6301\u7eed\u6539\u8fdb\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6548\u679c\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u4fe1\u53f7\u7684\u6709\u6548\u6027\u3002\u5b9e\u8df5\u4e2d\uff0c\u5f53\u4efb\u52a1\u96be\u5ea6\u4e0e\u6a21\u578b\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u6216\u8bad\u7ec3\u88ab\u5c11\u6570\u91cd\u590d\u95ee\u9898\u6a21\u5f0f\u4e3b\u5bfc\u65f6\uff0cRL\u8fdb\u5c55\u4f1a\u653e\u7f13\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u6765\u7ef4\u6301\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "method": "SCALER\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u53ef\u6269\u5c55\u7684\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u5c06\u771f\u5b9e\u7f16\u7a0b\u95ee\u9898\u8f6c\u5316\u4e3a\u5177\u6709\u53ef\u63a7\u96be\u5ea6\u548c\u65e0\u9650\u5b9e\u4f8b\u751f\u6210\u7684\u53ef\u9a8c\u8bc1\u63a8\u7406\u73af\u5883\uff1b2\uff09\u81ea\u9002\u5e94\u591a\u73af\u5883RL\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u5b9e\u4f8b\u96be\u5ea6\u5e76\u7b56\u5212\u6d3b\u52a8\u73af\u5883\u96c6\uff0c\u4ee5\u8ddf\u8e2a\u6a21\u578b\u80fd\u529b\u524d\u6cbf\u5e76\u4fdd\u6301\u5206\u5e03\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSCALER\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u6570\u636e\u96c6\u7684RL\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u7a33\u5b9a\u3001\u66f4\u957f\u89c6\u91ce\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "SCALER\u901a\u8fc7\u81ea\u9002\u5e94\u73af\u5883\u8bbe\u8ba1\u548c\u96be\u5ea6\u8c03\u6574\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2d\u5956\u52b1\u7a00\u758f\u6027\u548c\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u652f\u6301\u6a21\u578b\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04853", "abs": "https://arxiv.org/abs/2601.04853", "authors": ["Zhiwei Liu", "Runteng Guo", "Baojie Qu", "Yuechen Jiang", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "title": "RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection", "comment": null, "summary": "Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.", "AI": {"tldr": "RAAR\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u9886\u57df\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8bc1\u636e\u68c0\u7d22\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u63a8\u7406\uff0c\u5728\u4e09\u4e2a\u8de8\u9886\u57df\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8de8\u9886\u57df\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u865a\u5047\u4fe1\u606f\u5728\u4e0d\u540c\u9886\u57df\u51fa\u73b0\uff0c\u77e5\u8bc6\u548c\u8bdd\u8bed\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u89c6\u89d2\u7ebf\u7d22\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u5177\u6709\u6311\u6218\u6027\u6216\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u9886\u57df\uff0c\u800c\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5bf9\u590d\u6742\u4efb\u52a1\u6709\u6548\uff0c\u4f46\u4ec5\u9650\u4e8e\u540c\u5206\u5e03\u6570\u636e\u3002", "method": "RAAR\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff1a1\uff09\u68c0\u7d22\u4e0e\u76ee\u6807\u6837\u672c\u8bed\u4e49\u3001\u60c5\u611f\u548c\u5199\u4f5c\u98ce\u683c\u5bf9\u9f50\u7684\u591a\u89c6\u89d2\u6e90\u9886\u57df\u8bc1\u636e\uff1b2\uff09\u901a\u8fc7\u4e13\u95e8\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684\u591a\u6b65\u63a8\u7406\u8def\u5f84\uff0c\u5176\u4e2d\u7279\u5b9a\u89c6\u89d2\u4ee3\u7406\u4ea7\u751f\u4e92\u8865\u5206\u6790\uff0c\u603b\u7ed3\u4ee3\u7406\u5728\u9a8c\u8bc1\u5668\u6307\u5bfc\u4e0b\u6574\u5408\u5b83\u4eec\uff1b3\uff09\u5e94\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5355\u4e00\u591a\u4efb\u52a1\u9a8c\u8bc1\u5668\u4ee5\u589e\u5f3a\u9a8c\u8bc1\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u57fa\u4e8eRAAR\u8bad\u7ec3\u4e86RAAR-8b\u548cRAAR-14b\u6a21\u578b\u3002\u5728\u4e09\u4e2a\u8de8\u9886\u57df\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRAAR\u663e\u8457\u589e\u5f3a\u4e86\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5e76\u8d85\u8d8a\u4e86\u5176\u4ed6\u8de8\u9886\u57df\u65b9\u6cd5\u3001\u5148\u8fdbLLM\u548c\u57fa\u4e8eLLM\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "RAAR\u662f\u9996\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u8bc1\u636e\u68c0\u7d22\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u9886\u57df\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2601.04861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04861", "abs": "https://arxiv.org/abs/2601.04861", "authors": ["Jingbo Wang", "Sendong Zhao", "Jiatong Liu", "Haochun Wang", "Wanting Li", "Bing Qin", "Ting Liu"], "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models", "comment": null, "summary": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.", "AI": {"tldr": "OI-MAS\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u7b56\u7565\uff0c\u5728\u5f02\u6784\u591a\u5c3a\u5ea6LLM\u6c60\u4e2d\u52a8\u6001\u5206\u914d\u4e0d\u540c\u63a8\u7406\u9636\u6bb5\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u901a\u5e38\u5728\u6240\u6709\u667a\u80fd\u4f53\u89d2\u8272\u4e2d\u7edf\u4e00\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u672a\u80fd\u8003\u8651\u4e0d\u540c\u63a8\u7406\u9636\u6bb5\u7684\u8ba4\u77e5\u9700\u6c42\u5dee\u5f02\u3002", "method": "\u63d0\u51faOI-MAS\u6846\u67b6\uff0c\u5b9e\u73b0\u8de8\u5f02\u6784\u591a\u5c3a\u5ea6LLM\u6c60\u7684\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u7b56\u7565\uff0c\u5305\u62ec\u72b6\u6001\u4f9d\u8d56\u7684\u8def\u7531\u673a\u5236\u52a8\u6001\u9009\u62e9\u667a\u80fd\u4f53\u89d2\u8272\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u673a\u5236\u9009\u62e9\u9002\u5f53\u6a21\u578b\u89c4\u6a21\u3002", "result": "OI-MAS\u5728\u5b9e\u9a8c\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12.88%\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe79.78%\u3002", "conclusion": "\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\uff0cOI-MAS\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2601.04879", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04879", "abs": "https://arxiv.org/abs/2601.04879", "authors": ["Mingyue Cheng", "Daoyu Wang", "Qi Liu", "Shuo Yu", "Xiaoyu Tao", "Yuqian Wang", "Chengzhong Chu", "Yu Duan", "Mingkang Long", "Enhong Chen"], "title": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis", "comment": "26 Pages, 9 Figures, 7 Tables", "summary": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.", "AI": {"tldr": "Mind2Report\u662f\u4e00\u4e2a\u8ba4\u77e5\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u62df\u5546\u4e1a\u5206\u6790\u5e08\u7684\u5de5\u4f5c\u6d41\u7a0b\u6765\u5408\u6210\u4e13\u5bb6\u7ea7\u5546\u4e1a\u62a5\u544a\uff0c\u5728\u8d28\u91cf\u3001\u53ef\u9760\u6027\u548c\u8986\u76d6\u8303\u56f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4ece\u6d77\u91cf\u5608\u6742\u7684\u7f51\u7edc\u4fe1\u606f\u4e2d\u5408\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u5546\u4e1a\u62a5\u544a\u5bf9\u9ad8\u98ce\u9669\u5546\u4e1a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u62a5\u544a\u8d28\u91cf\u3001\u53ef\u9760\u6027\u548c\u8986\u76d6\u8303\u56f4\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u589e\u5f3a\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u52a8\u6001\u8bb0\u5fc6\u80fd\u529b\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u610f\u56fe\u63a2\u6d4b\u3001\u7f51\u7edc\u641c\u7d22\u3001\u4fe1\u606f\u84b8\u998f\u548c\u8fed\u4ee3\u62a5\u544a\u5408\u6210\u7b49\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u5728\u5305\u542b200\u4e2a\u771f\u5b9e\u4e16\u754c\u5546\u4e1a\u4efb\u52a1\u7684QRC-Eval\u57fa\u51c6\u4e0a\uff0cMind2Report\u5728\u62a5\u544a\u8d28\u91cf\u3001\u53ef\u9760\u6027\u548c\u8986\u76d6\u8303\u56f4\u65b9\u9762\u4f18\u4e8eOpenAI\u548cGemini\u7b49\u9886\u5148\u57fa\u7ebf\u3002", "conclusion": "\u867d\u7136\u8fd9\u662f\u521d\u6b65\u7814\u7a76\uff0c\u4f46\u671f\u671b\u4e3a\u672a\u6765\u5546\u4e1a\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u8bbe\u8ba1\u63d0\u4f9b\u57fa\u7840\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2601.05152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05152", "abs": "https://arxiv.org/abs/2601.05152", "authors": ["Timofey Tomashevskiy"], "title": "Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art", "comment": "20 pages, 4 figures", "summary": "This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.\n  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning", "AI": {"tldr": "\u5173\u4e8e\u6301\u7eed\u5b89\u5168\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08COSRL\uff09\u65b9\u6cd5\u7684\u7efc\u8ff0\u8bba\u6587\uff0c\u6db5\u76d6\u7406\u8bba\u3001\u6311\u6218\u3001\u5206\u7c7b\u65b9\u6cd5\u548c\u5b89\u5168\u7ea6\u675f\u5236\u5b9a\uff0c\u65e8\u5728\u5efa\u7acb\u53ef\u9760\u7684\u5b89\u5168\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u5f53\u524d\u9700\u8981\u6784\u5efa\u80fd\u591f\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u6301\u7eed\u5b89\u5168\u5b66\u4e60\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5206\u6563\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u9700\u8981\u63d0\u4f9b\u5168\u9762\u7684\u7406\u8bba\u6846\u67b6\u548c\u65b9\u6cd5\u5206\u7c7b\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u9996\u5148\u8ba8\u8bbaCOSRL\u7684\u7406\u8bba\u57fa\u7840\u548c\u6311\u6218\uff0c\u7136\u540e\u57fa\u4e8e\u5b89\u5168\u5b66\u4e60\u673a\u5236\u7c7b\u578b\uff08\u8003\u8651\u975e\u5e73\u7a33\u6027\u9002\u5e94\uff09\u5bf9\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u6700\u540e\u5206\u7c7b\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u5b89\u5168\u7ea6\u675f\u5236\u5b9a\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86COSRL\u65b9\u6cd5\u7684\u7cfb\u7edf\u5206\u7c7b\u548c\u8be6\u7ec6\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u5b89\u5168\u673a\u5236\u7c7b\u578b\u7684\u65b9\u6cd5\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u7cfb\u7edf\u5316\u4e86\u5b89\u5168\u7ea6\u675f\u7684\u5236\u5b9a\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u4e3a\u6301\u7eed\u5b89\u5168\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7efc\u8ff0\u548c\u5206\u7c7b\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6784\u5efa\u53ef\u9760\u5b89\u5168\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u7814\u7a76\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.04888", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04888", "abs": "https://arxiv.org/abs/2601.04888", "authors": ["Tongyu Wen", "Guanting Dong", "Zhicheng Dou"], "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents", "comment": "16 pages, 6 figures", "summary": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.", "AI": {"tldr": "SmartSearch\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u548c\u67e5\u8be2\u4f18\u5316\u673a\u5236\u63d0\u5347\u641c\u7d22\u67e5\u8be2\u8d28\u91cf\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709LLM\u641c\u7d22\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u8303\u5f0f\u4f18\u5316\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e2d\u95f4\u641c\u7d22\u67e5\u8be2\u7684\u8d28\u91cf\u95ee\u9898\u3002\u751f\u6210\u7684\u67e5\u8be2\u5f80\u5f80\u4e0d\u51c6\u786e\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u4e0d\u7406\u60f3\uff0c\u9650\u5236\u4e86\u641c\u7d22\u4ee3\u7406\u7684\u6574\u4f53\u6548\u679c\u3002", "method": "\u63d0\u51faSmartSearch\u6846\u67b6\uff1a1) \u8fc7\u7a0b\u5956\u52b1\u673a\u5236\u901a\u8fc7\u53cc\u7ea7\u4fe1\u7528\u8bc4\u4f30\u5bf9\u4e2d\u95f4\u641c\u7d22\u67e5\u8be2\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff1b2) \u67e5\u8be2\u4f18\u5316\u673a\u5236\u901a\u8fc7\u9009\u62e9\u6027\u4f18\u5316\u4f4e\u8d28\u91cf\u67e5\u8be2\u5e76\u57fa\u4e8e\u4f18\u5316\u7ed3\u679c\u91cd\u65b0\u751f\u6210\u540e\u7eed\u641c\u7d22\u8f6e\u6b21\uff1b3) \u8bbe\u8ba1\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff08\u6a21\u4eff\u3001\u5bf9\u9f50\u3001\u6cdb\u5316\uff09\u8ba9\u4ee3\u7406\u9010\u6b65\u5185\u5316\u67e5\u8be2\u8d28\u91cf\u6539\u8fdb\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSmartSearch\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9a\u91cf\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u5176\u5728\u641c\u7d22\u6548\u7387\u548c\u67e5\u8be2\u8d28\u91cf\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SmartSearch\u901a\u8fc7\u5173\u6ce8\u4e2d\u95f4\u641c\u7d22\u67e5\u8be2\u8d28\u91cf\uff0c\u7ed3\u5408\u8fc7\u7a0b\u5956\u52b1\u548c\u67e5\u8be2\u4f18\u5316\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2601.04895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04895", "abs": "https://arxiv.org/abs/2601.04895", "authors": ["Renzhao Liang", "Jingru Chen", "Bo Jia", "Bo Deng", "Chenggang Xie", "Yidong Wang", "Ke Jin", "Xin Wang", "Linfeng Zhang", "Cunxiang Wang"], "title": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation", "comment": null, "summary": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.", "AI": {"tldr": "\u63d0\u51faDVD\u65b9\u6cd5\u68c0\u6d4bLLM\u8bc4\u4f30\u4e2d\u7684\u53d8\u4f53\u6c61\u67d3\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u6a21\u6e29\u5ea6\u91c7\u6837\u4e0b\u7684\u8f93\u51fa\u5206\u5e03\u65b9\u5dee\u6765\u8bc6\u522b\u88ab\u8bb0\u5fc6\u7684\u8bed\u4e49\u7b49\u4ef7\u6d4b\u8bd5\u9879\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u9762\u4e34\u53d8\u4f53\u6c61\u67d3\u95ee\u9898\uff1a\u8bad\u7ec3\u8bed\u6599\u4e2d\u5305\u542b\u8bed\u4e49\u7b49\u4ef7\u4f46\u8bcd\u6c47\u6216\u53e5\u6cd5\u6539\u53d8\u7684\u6d4b\u8bd5\u9879\uff0c\u8fd9\u4e9b\u53d8\u4f53\u9003\u907f\u73b0\u6709\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u63a8\u7406\u6765\u865a\u589e\u57fa\u51c6\u5206\u6570\u3002", "method": "\u63d0\u51faDVD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6e29\u5ea6\u91c7\u6837\u5efa\u6a21\u5c40\u90e8\u8f93\u51fa\u5206\u5e03\u65b9\u5dee\u3002\u6838\u5fc3\u6d1e\u5bdf\uff1a\u6c61\u67d3\u9879\u4f1a\u5728\u8bb0\u5fc6\u4f9d\u4ece\u72b6\u6001\u548c\u6270\u52a8\u6f02\u79fb\u72b6\u6001\u95f4\u4ea4\u66ff\uff0c\u5bfc\u81f4\u4f4e\u6982\u7387token\u7684\u5408\u6210\u96be\u5ea6\u65b9\u5dee\u5f02\u5e38\u9ad8\uff1b\u975e\u6c61\u67d3\u9879\u5219\u4fdd\u6301\u76f8\u5bf9\u5e73\u6ed1\u7684\u65b9\u5dee\u6f02\u79fb\u3002", "result": "\u5728Omni-MATH\u548cSuperGPQA\u4e24\u4e2a\u9886\u57df\u6784\u5efa\u9996\u4e2a\u53d8\u4f53\u6c61\u67d3\u57fa\u51c6\uff0c\u901a\u8fc7\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784\u7684\u6a21\u578b\u6a21\u62df\u6c61\u67d3\u3002DVD\u5728\u8de8\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u4e00\u81f4\u4f18\u4e8e\u57fa\u4e8e\u56f0\u60d1\u5ea6\u3001Min-k%++\u3001\u7f16\u8f91\u8ddd\u79bb\u548c\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5bf9\u8d85\u53c2\u6570\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u751f\u6210\u5206\u5e03\u65b9\u5dee\u53ef\u4f5c\u4e3a\u68c0\u6d4bLLM\u8bc4\u4f30\u4e2d\u53d8\u4f53\u6c61\u67d3\u7684\u539f\u5219\u6027\u548c\u5b9e\u7528\u6307\u7eb9\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2601.04920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04920", "abs": "https://arxiv.org/abs/2601.04920", "authors": ["Nils Einecke"], "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition", "comment": null, "summary": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.", "AI": {"tldr": "ChatGPT\u7528\u4e8eESA ELOPE\u7ade\u8d5b\u7684\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u79d1\u5b66\u7ade\u8d5b\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLM\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7f16\u7801\u4f19\u4f34\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u7ade\u4e89\u6027\u79d1\u5b66\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528ChatGPT\u8fdb\u884c\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u53c2\u4e0eESA\u7684ELOPE\u7ade\u8d5b\uff08\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6708\u7403\u5149\u6d41\u81ea\u8fd0\u52a8\u4f30\u8ba1\uff09\uff0c\u5904\u7406\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u4ee5\u4f30\u8ba1\u6708\u7403\u7740\u9646\u5668\u8f68\u8ff9\u3002", "result": "\u5c3d\u7ba1\u52a0\u5165\u8f83\u665a\uff0c\u4f46\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\uff08\u5f97\u52060.01282\uff09\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u79d1\u5b66\u7ade\u8d5b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5bf9\u8bdd\u5f0fAI\u65e2\u80fd\u52a0\u901f\u5f00\u53d1\u53c8\u80fd\u652f\u6301\u79d1\u5b66\u7814\u7a76\u7684\u6982\u5ff5\u6d1e\u5bdf\uff0c\u4f46\u9700\u8981\u7ed3\u6784\u5316\u96c6\u6210\u5230\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5e76\u5236\u5b9aAI\u8f85\u52a9\u79d1\u5b66\u5de5\u4f5c\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "topic": "swe application"}}
{"id": "2601.04992", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04992", "abs": "https://arxiv.org/abs/2601.04992", "authors": ["Xueyun Tian", "Minghua Ma", "Bingbing Xu", "Nuoyan Lyu", "Wei Li", "Heng Dong", "Zheng Chu", "Yuanzhuo Wang", "Huawei Shen"], "title": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization", "comment": "Code and data are available at https://github.com/Eureka-Maggie/GLOW", "summary": "Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u76d1\u7763\u5fae\u8c03\u4e2d\u540c\u65f6\u4f7f\u7528\u6b63\u786e\u548c\u9519\u8bef\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u5e76\u5f00\u53d1\u4e86GLOW\u635f\u5931\u52a0\u6743\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684OOD\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u76d1\u7763\u5fae\u8c03\u901a\u5e38\u53ea\u4f7f\u7528\u6b63\u786e\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u5ffd\u7565\u4e86\u9519\u8bef\u8f68\u8ff9\u4e2d\u7684\u6709\u6548\u4e2d\u95f4\u63a8\u7406\u4fe1\u606f\u3002\u8fd9\u79cd\u8303\u5f0f\u6d6a\u8d39\u4e86\u5927\u91cf\u76d1\u7763\u4fe1\u53f7\u5e76\u52a0\u5267\u8fc7\u62df\u5408\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u7cfb\u7edf\u5206\u6790\u9519\u8bef\u601d\u7ef4\u94fe\u8f68\u8ff9\u768422\u79cd\u6a21\u5f0f\u53ca\u5176\u8bad\u7ec3\u52a8\u6001\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u589e\u76ca\u7684\u635f\u5931\u52a0\u6743\u65b9\u6cd5(GLOW)\uff0c\u6839\u636e\u6837\u672c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8fdb\u5c55\u52a8\u6001\u8c03\u6574\u635f\u5931\u6743\u91cd\uff0c\u6709\u6548\u5229\u7528\u672a\u8fc7\u6ee4\u7684\u601d\u7ef4\u94fe\u8f68\u8ff9\u3002", "result": "1) \u5728Qwen2.5-7B\u4e0a\uff0cGLOW\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6b63\u786e\u8f68\u8ff9\u7684SFT\u83b7\u5f975.51%\u7684OOD\u6cdb\u5316\u63d0\u5347\uff1b2) \u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u521d\u59cb\u5316\u65f6\uff0c\u5c06MMLU\u5206\u6570\u4ece72.82%\u63d0\u5347\u81f376.47%\uff1b3) \u9519\u8bef\u8f68\u8ff9\u4f7f\u63a8\u7406\u65f6\u7684\u7b56\u7565\u71b5\u589e\u52a035.67%\uff0c\u4fc3\u8fdb\u63a2\u7d22\u3002", "conclusion": "\u9519\u8bef\u601d\u7ef4\u94fe\u8f68\u8ff9\u5305\u542b\u6709\u4ef7\u503c\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7GLOW\u65b9\u6cd5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u8f68\u8ff9\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05050", "categories": ["cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.05050", "abs": "https://arxiv.org/abs/2601.05050", "authors": ["Thomas H. Costello", "Kellin Pelrine", "Matthew Kowal", "Antonio A. Arechar", "Jean-Fran\u00e7ois Godbout", "Adam Gleave", "David Rand", "Gordon Pennycook"], "title": "Large language models can effectively convince people to believe conspiracies", "comment": null, "summary": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.", "AI": {"tldr": "GPT-4o\u80fd\u540c\u6837\u6709\u6548\u5730\u589e\u52a0\u6216\u51cf\u5c11\u9634\u8c0b\u8bba\u4fe1\u5ff5\uff0c\u5373\u4f7f\u6709\u5b89\u5168\u62a4\u680f\u4e5f\u80fd\u4fc3\u8fdb\u865a\u5047\u4fe1\u606f\uff0c\u4f46\u53ef\u901a\u8fc7\u7ea0\u6b63\u5bf9\u8bdd\u548c\u51c6\u786e\u6027\u63d0\u793a\u6765\u7f13\u89e3\u98ce\u9669", "motivation": "\u7814\u7a76LLMs\u7684\u8bf4\u670d\u529b\u662f\u5426\u66f4\u503e\u5411\u4e8e\u771f\u76f8\u800c\u975e\u865a\u5047\u4fe1\u606f\uff0c\u6216\u8005\u5b83\u4eec\u662f\u5426\u540c\u6837\u5bb9\u6613\u4fc3\u8fdb\u9519\u8bef\u4fe1\u5ff5\uff0c\u7279\u522b\u662f\u5728\u9634\u8c0b\u8bba\u4f20\u64ad\u65b9\u9762", "method": "\u901a\u8fc7\u4e09\u4e2a\u9884\u6ce8\u518c\u5b9e\u9a8c\uff0c\u8ba92,724\u540d\u7f8e\u56fd\u53c2\u4e0e\u8005\u4e0eGPT-4o\u8ba8\u8bba\u4ed6\u4eec\u4e0d\u786e\u5b9a\u7684\u9634\u8c0b\u8bba\uff0c\u6a21\u578b\u88ab\u6307\u793a\u8981\u4e48\u53cd\u9a73(\"debunking\")\u8981\u4e48\u652f\u6301(\"bunking\")\u8be5\u9634\u8c0b\u8bba\uff0c\u5305\u62ec\u4f7f\u7528\u79fb\u9664\u62a4\u680f\u7684\"\u8d8a\u72f1\"\u7248\u672c\u548c\u6807\u51c6\u7248\u672c", "result": "\u8d8a\u72f1\u7248GPT-4o\u5728\u589e\u52a0\u548c\u51cf\u5c11\u9634\u8c0b\u8bba\u4fe1\u5ff5\u65b9\u9762\u540c\u6837\u6709\u6548\uff1b\u652f\u6301\u9634\u8c0b\u8bba\u7684AI\u83b7\u5f97\u66f4\u79ef\u6781\u8bc4\u4ef7\u5e76\u589e\u52a0\u5bf9AI\u7684\u4fe1\u4efb\uff1b\u6807\u51c6GPT-4o\u4ea7\u751f\u7c7b\u4f3c\u6548\u679c\uff0c\u5b89\u5168\u62a4\u680f\u51e0\u4e4e\u65e0\u6cd5\u963b\u6b62\u5176\u4fc3\u8fdb\u9634\u8c0b\u8bba\u4fe1\u5ff5\uff1b\u4f46\u7ea0\u6b63\u5bf9\u8bdd\u80fd\u9006\u8f6c\u65b0\u8bf1\u5bfc\u7684\u4fe1\u5ff5\uff0c\u51c6\u786e\u6027\u63d0\u793a\u663e\u8457\u964d\u4f4e\u5176\u589e\u52a0\u9634\u8c0b\u8bba\u4fe1\u5ff5\u7684\u80fd\u529b", "conclusion": "LLMs\u5177\u6709\u4fc3\u8fdb\u771f\u76f8\u548c\u865a\u5047\u4fe1\u606f\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u8fd9\u79cd\u98ce\u9669\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5b89\u5168\u63aa\u65bd", "topic": "agent analysis"}}
{"id": "2601.05111", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05111", "abs": "https://arxiv.org/abs/2601.05111", "authors": ["Runyang You", "Hongru Cai", "Caiqi Zhang", "Qiancheng Xu", "Meng Liu", "Tiezheng Yu", "Yongqi Li", "Wenjie Li"], "title": "Agent-as-a-Judge", "comment": null, "summary": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u7b2c\u4e00\u7bc7\u5173\u4e8eAI\u8bc4\u4f30\u4eceLLM-as-a-Judge\u5411Agent-as-a-Judge\u8303\u5f0f\u8f6c\u53d8\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u53d1\u5c55\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86\u6838\u5fc3\u65b9\u6cd5\u3001\u5e94\u7528\u9886\u57df\uff0c\u5e76\u5206\u6790\u4e86\u524d\u6cbf\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u88ab\u8bc4\u4f30\u5bf9\u8c61\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3001\u4e13\u4e1a\u548c\u591a\u6b65\u9aa4\uff0cLLM-as-a-Judge\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u53d7\u5230\u56fa\u6709\u504f\u89c1\u3001\u6d45\u5c42\u5355\u6b21\u63a8\u7406\u548c\u65e0\u6cd5\u9a8c\u8bc1\u73b0\u5b9e\u89c2\u5bdf\u7684\u9650\u5236\uff0c\u8fd9\u4fc3\u4f7f\u4e86\u5411Agent-as-a-Judge\u8303\u5f0f\u7684\u8f6c\u53d8\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5bfc\u822a\u8fd9\u4e00\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u5168\u9762\u8c03\u67e5\u8ffd\u8e2a\u8fd9\u4e00\u6f14\u53d8\u8fc7\u7a0b\uff0c\u8bc6\u522b\u4e86\u8303\u5f0f\u8f6c\u53d8\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u5efa\u7acb\u4e86\u53d1\u5c55\u5206\u7c7b\u6cd5\uff0c\u7ec4\u7ec7\u4e86\u6838\u5fc3\u65b9\u6cd5\u8bba\uff0c\u5e76\u8c03\u67e5\u4e86\u901a\u7528\u548c\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u5173\u4e8eAgent-as-a-Judge\u8303\u5f0f\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u5206\u7c7b\u4f53\u7cfb\u3001\u65b9\u6cd5\u8bba\u603b\u7ed3\u548c\u5e94\u7528\u9886\u57df\u5206\u6790\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u3002", "conclusion": "Agent-as-a-Judge\u901a\u8fc7\u89c4\u5212\u3001\u5de5\u5177\u589e\u5f3a\u9a8c\u8bc1\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u6301\u4e45\u8bb0\u5fc6\u7b49\u80fd\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7a33\u5065\u3001\u53ef\u9a8c\u8bc1\u548c\u7ec6\u81f4\u7684\u8bc4\u4f30\uff0c\u4ee3\u8868\u4e86AI\u8bc4\u4f30\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2601.05163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05163", "abs": "https://arxiv.org/abs/2601.05163", "authors": ["Qintong Zhang", "Xinjie Lv", "Jialong Wu", "Baixuan Li", "Zhengwei Tao", "Guochen Yan", "Huanyao Zhang", "Bin Wang", "Jiahao Xu", "Haitao Mi", "Wentao Zhang"], "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking", "comment": null, "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.", "AI": {"tldr": "DocDancer\uff1a\u4e00\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u5f00\u6e90\u6587\u6863\u95ee\u7b54\u4ee3\u7406\uff0c\u901a\u8fc7\u5de5\u5177\u9a71\u52a8\u6846\u67b6\u5efa\u6a21\u6587\u6863\u63a2\u7d22\u4e0e\u7406\u89e3\uff0c\u4f7f\u7528\u63a2\u7d22-\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "motivation": "\u73b0\u6709\u6587\u6863\u95ee\u7b54\u4ee3\u7406\u7f3a\u4e4f\u6709\u6548\u7684\u5de5\u5177\u5229\u7528\uff0c\u4e3b\u8981\u4f9d\u8d56\u95ed\u6e90\u6a21\u578b\uff0c\u9700\u8981\u5f00\u53d1\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u6587\u6863\u7406\u89e3\u80fd\u529b", "method": "\u5c06\u6587\u6863\u95ee\u7b54\u5efa\u6a21\u4e3a\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\uff0c\u63d0\u51fa\u5de5\u5177\u9a71\u52a8\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u663e\u5f0f\u5efa\u6a21\u6587\u6863\u63a2\u7d22\u548c\u7406\u89e3\u8fc7\u7a0b\uff0c\u4f7f\u7528\u63a2\u7d22-\u5408\u6210\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e", "result": "\u5728MMLongBench-Doc\u548cDocBench\u4e24\u4e2a\u957f\u4e0a\u4e0b\u6587\u6587\u6863\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u4ee3\u7406\u5de5\u5177\u8bbe\u8ba1\u548c\u5408\u6210\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3", "conclusion": "DocDancer\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\u5f00\u6e90\u6587\u6863\u95ee\u7b54\u4ee3\u7406\u7684\u53ef\u884c\u6027\uff0c\u5de5\u5177\u9a71\u52a8\u6846\u67b6\u548c\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4e3a\u89e3\u51b3\u6587\u6863\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84", "topic": "code agent"}}
{"id": "2601.05167", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05167", "abs": "https://arxiv.org/abs/2601.05167", "authors": ["Chengsong Huang", "Tong Zheng", "Langlin Huang", "Jinyuan Li", "Haolin Liu", "Jiaxin Huang"], "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "comment": null, "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "AI": {"tldr": "RelayLLM\uff1a\u4e00\u79cd\u901a\u8fc7\u4ee4\u724c\u7ea7\u534f\u4f5c\u89e3\u7801\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u8ba9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e3b\u52a8\u63a7\u5236\u5668\uff0c\u4ec5\u5728\u5173\u952e\u4ee4\u724c\u65f6\u52a8\u6001\u8c03\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u800c\u8d44\u6e90\u9ad8\u6548\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u901a\u5e38\u7f3a\u4e4f\u5fc5\u8981\u7684\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u534f\u4f5c\u65b9\u6cd5\uff08\u5982\u7ea7\u8054\u6216\u8def\u7531\uff09\u5728\u7c97\u7c92\u5ea6\u4e0a\u8fd0\u884c\uff0c\u5c06\u6574\u4e2a\u67e5\u8be2\u5378\u8f7d\u7ed9LLMs\uff0c\u5f53SLM\u80fd\u591f\u5904\u7406\u5927\u591a\u6570\u63a8\u7406\u6b65\u9aa4\u65f6\u4f1a\u9020\u6210\u663e\u8457\u7684\u8ba1\u7b97\u6d6a\u8d39\u3002", "method": "\u63d0\u51faRelayLLM\u6846\u67b6\uff0c\u91c7\u7528\u4ee4\u724c\u7ea7\u534f\u4f5c\u89e3\u7801\u3002SLM\u4f5c\u4e3a\u4e3b\u52a8\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u7279\u6b8a\u547d\u4ee4\u52a8\u6001\u8c03\u7528LLM\u5904\u7406\u5173\u952e\u4ee4\u724c\uff0c\u5b9e\u73b0\"\u63a5\u529b\"\u751f\u6210\u8fc7\u7a0b\u3002\u5f15\u5165\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u9884\u70ed\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u6559\u5bfc\u6a21\u578b\u5e73\u8861\u72ec\u7acb\u6027\u4e0e\u7b56\u7565\u6027\u6c42\u52a9\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRelayLLM\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523049.52%\uff0c\u6709\u6548\u5f25\u5408\u4e86\u4e24\u4e2a\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u4ec5\u8c03\u7528LLM\u5904\u7406\u603b\u751f\u6210\u4ee4\u724c\u76841.07%\uff0c\u76f8\u6bd4\u6027\u80fd\u5339\u914d\u7684\u968f\u673a\u8def\u7531\u5668\u63d0\u4f9b98.2%\u7684\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "RelayLLM\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u4ee4\u724c\u7ea7\u534f\u4f5c\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.05101", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05101", "abs": "https://arxiv.org/abs/2601.05101", "authors": ["Konstantin Kubrak", "Ahmed El-Moselhy", "Ammar Alsulami", "Remaz Altuwaim", "Hassan Ismail Fawaz", "Faisal Alsaby"], "title": "Arabic Prompts with English Tools: A Benchmark", "comment": "10 pages, 10 figures, LLMs, Big Data, and Multilinguality for All (LLMs4All) Workshop at IEEE BigData 2025 Conference, Macau, December 10, 2025", "summary": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.", "AI": {"tldr": "\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u963f\u62c9\u4f2f\u8bedLLM\u5de5\u5177\u8c03\u7528\u548c\u4ee3\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u963f\u62c9\u4f2f\u8bed\u4ea4\u4e92\u4e0b\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d5-10%\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u963f\u62c9\u4f2f\u8bed\u539f\u751fLLM\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u963f\u62c9\u4f2f\u8bed\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u800c\u963f\u62c9\u4f2f\u8bed\u7528\u6237\u5728\u5de5\u5177\u8c03\u7528\u4e0a\u9762\u4e34\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u521b\u5efa\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u5de5\u5177\u8c03\u7528\u548c\u4ee3\u7406\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u6846\u67b6\u6765\u8861\u91cf\u963f\u62c9\u4f2f\u8bed\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u529f\u80fd\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u53d1\u73b0\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1a\u5f53\u7528\u6237\u4f7f\u7528\u963f\u62c9\u4f2f\u8bed\u4ea4\u4e92\u65f6\uff0c\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d5-10%\uff0c\u65e0\u8bba\u5de5\u5177\u63cf\u8ff0\u672c\u8eab\u662f\u963f\u62c9\u4f2f\u8bed\u8fd8\u662f\u82f1\u8bed\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u963f\u62c9\u4f2f\u8bed\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e3a\u963f\u62c9\u4f2f\u8bed\u7528\u6237\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u8bed\u8a00\u516c\u5e73\u7684AI\u4ee3\u7406\u3002", "topic": "agent analysis"}}
{"id": "2601.05171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.05171", "abs": "https://arxiv.org/abs/2601.05171", "authors": ["Jihao Zhao", "Ding Chen", "Zhaoxin Fan", "Kerun Xu", "Mengting Hu", "Bo Tang", "Feiyu Xiong", "Zhiyu li"], "title": "Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems", "comment": null, "summary": "Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.", "AI": {"tldr": "\u63d0\u51faInside Out\u6846\u67b6\uff0c\u4f7f\u7528PersonaTree\u4f5c\u4e3a\u957f\u671f\u7528\u6237\u753b\u50cf\u8f7d\u4f53\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u7ba1\u7406\u548c\u8f7b\u91cf\u7ea7MemListener\u5b9e\u73b0\u53ef\u63a7\u589e\u957f\uff0c\u5728\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u540c\u65f6\u538b\u7f29\u8bb0\u5fc6\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u9762\u4e34\u65e0\u9650\u4ea4\u4e92\u6d41\u4e0e\u6709\u9650\u4e0a\u4e0b\u6587\u7ea6\u675f\u7684\u77db\u76fe\uff0c\u5bb9\u6613\u51fa\u73b0\u8bb0\u5fc6\u566a\u58f0\u7d2f\u79ef\u3001\u63a8\u7406\u9000\u5316\u548c\u89d2\u8272\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "1) \u4f7f\u7528\u5168\u5c40\u7ef4\u62a4\u7684PersonaTree\u4f5c\u4e3a\u957f\u671f\u7528\u6237\u753b\u50cf\u8f7d\u4f53\uff0c\u901a\u8fc7\u521d\u59cb\u6a21\u5f0f\u7ea6\u675f\u4e3b\u5e72\u5e76\u66f4\u65b0\u5206\u652f\u548c\u53f6\u5b50\u5b9e\u73b0\u53ef\u63a7\u589e\u957f\uff1b2) \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7MemListener\uff0c\u4ea7\u751f\u7ed3\u6784\u5316\u3001\u53ef\u6267\u884c\u3001\u53ef\u89e3\u91ca\u7684{ADD, UPDATE, DELETE, NO_OP}\u64cd\u4f5c\uff1b3) \u54cd\u5e94\u751f\u6210\u65f6\u76f4\u63a5\u5229\u7528PersonaTree\u589e\u5f3a\u8f93\u51fa\uff0c\u6216\u5728\u9700\u8981\u7ec6\u8282\u65f6\u89e6\u53d1\u4ee3\u7406\u6a21\u5f0f\u6309\u9700\u5f15\u5165\u7ec6\u8282\u3002", "result": "PersonaTree\u5728\u6291\u5236\u4e0a\u4e0b\u6587\u566a\u58f0\u548c\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u5168\u6587\u62fc\u63a5\u548c\u5404\u79cd\u4e2a\u6027\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff1b\u5c0f\u578bMemListener\u6a21\u578b\u5728\u8bb0\u5fc6\u64cd\u4f5c\u51b3\u7b56\u6027\u80fd\u4e0a\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7DeepSeek-R1-0528\u548cGemini-3-Pro\u7b49\u5f3a\u5927\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "Inside Out\u6846\u67b6\u901a\u8fc7PersonaTree\u548cMemListener\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u4e2a\u6027\u5316\u5bf9\u8bdd\u4e2d\u7684\u8bb0\u5fc6\u7ba1\u7406\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u538b\u7f29\u3001\u4e00\u81f4\u6027\u4fdd\u6301\u548c\u53ef\u63a7\u589e\u957f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.05107", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05107", "abs": "https://arxiv.org/abs/2601.05107", "authors": ["Muzhao Tian", "Zisu Huang", "Xiaohua Wang", "Jingwen Xu", "Zhengkang Guo", "Qi Qian", "Yuanzhe Shen", "Kaitao Song", "Jiakang Yuan", "Changze Lv", "Xiaoqing Zheng"], "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction", "comment": null, "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSteeM\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8c03\u63a7\u7684\u8bb0\u5fc6\u4f9d\u8d56\u673a\u5236\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8bb0\u5fc6\u951a\u5b9a\u95ee\u9898\uff0c\u5141\u8bb8\u7528\u6237\u52a8\u6001\u63a7\u5236\u667a\u80fd\u4f53\u5bf9\u5386\u53f2\u4fe1\u606f\u7684\u4f9d\u8d56\u7a0b\u5ea6\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u91c7\u7528\"\u5168\u6709\u6216\u5168\u65e0\"\u7684\u8bb0\u5fc6\u4f7f\u7528\u65b9\u5f0f\uff1a\u8981\u4e48\u5305\u542b\u6240\u6709\u76f8\u5173\u5386\u53f2\u4fe1\u606f\u5bfc\u81f4\u8bb0\u5fc6\u951a\u5b9a\uff08\u667a\u80fd\u4f53\u88ab\u8fc7\u53bb\u4ea4\u4e92\u675f\u7f1a\uff09\uff0c\u8981\u4e48\u5b8c\u5168\u6392\u9664\u8bb0\u5fc6\u5bfc\u81f4\u91cd\u8981\u4ea4\u4e92\u5386\u53f2\u4e22\u5931\u3002\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bb0\u5fc6\u63a7\u5236\u673a\u5236\u3002", "method": "\u63d0\u51faSteeM\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u8bb0\u5fc6\u4f9d\u8d56\u884c\u4e3a\u5ea6\u91cf\u6765\u91cf\u5316\u8fc7\u53bb\u4ea4\u4e92\u5bf9\u5f53\u524d\u8f93\u51fa\u7684\u5f71\u54cd\uff1b2\uff09\u5141\u8bb8\u7528\u6237\u52a8\u6001\u8c03\u8282\u8bb0\u5fc6\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4ece\u4fc3\u8fdb\u521b\u65b0\u7684\"\u91cd\u65b0\u5f00\u59cb\"\u6a21\u5f0f\u5230\u7d27\u5bc6\u9075\u5faa\u4ea4\u4e92\u5386\u53f2\u7684\"\u9ad8\u4fdd\u771f\"\u6a21\u5f0f\u3002", "result": "\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSteeM\u6846\u67b6\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5\u548c\u521a\u6027\u8bb0\u5fc6\u63a9\u7801\u7b56\u7565\uff0c\u4e3a\u4e2a\u6027\u5316\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u7ec6\u81f4\u6709\u6548\u7684\u63a7\u5236\u3002", "conclusion": "\u667a\u80fd\u4f53\u5bf9\u8bb0\u5fc6\u7684\u4f9d\u8d56\u53ef\u4ee5\u5efa\u6a21\u4e3a\u663e\u5f0f\u4e14\u7528\u6237\u53ef\u63a7\u7684\u7ef4\u5ea6\uff0cSteeM\u6846\u67b6\u901a\u8fc7\u53ef\u8c03\u63a7\u7684\u8bb0\u5fc6\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u6709\u6548\u7684\u4e2a\u6027\u5316\u4eba\u673a\u534f\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2601.05114", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05114", "abs": "https://arxiv.org/abs/2601.05114", "authors": ["Wajid Nasser"], "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior", "comment": "23 pages, 6 figures, code and artifacts at : https://github.com/wajid-nasser/evaluative-fingerprints", "summary": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's \u03b1 = 0.042). On two dimensions, judges disagree more than random noise would predict (\u03b1 < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5b58\u5728\u53ef\u9760\u6027\u6096\u8bba\uff1a\u5355\u4e2a\u8bc4\u4f30\u8005\u5185\u90e8\u4e00\u81f4\u6027\u9ad8\uff0c\u4f46\u4e0d\u540c\u8bc4\u4f30\u8005\u4e4b\u95f4\u51e0\u4e4e\u65e0\u5171\u8bc6\uff08\u03b1=0.042\uff09\uff0c\u8bc4\u4f30\u8005\u95f4\u7684\u5206\u6b67\u6a21\u5f0f\u5374\u7a33\u5b9a\u5230\u53ef\u4f5c\u4e3a\u6307\u7eb9\u8bc6\u522b\uff08\u51c6\u786e\u738777.1%-99.6%\uff09\uff0c\u6bcf\u4e2a\u8bc4\u4f30\u8005\u90fd\u6709\u72ec\u7279\u7684\"\u8bc4\u4f30\u503e\u5411\"\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u68c0\u9a8cLLM-as-judge\u7cfb\u7edf\u7684\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u4e0d\u540cLLM\u8bc4\u4f30\u8005\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u771f\u6b63\u5171\u4eab\u5bf9\u8d28\u91cf\u8bc4\u4f30\u7684\u5171\u540c\u7406\u89e3\u3002", "method": "\u4f7f\u75289\u4e2aLLM\u8bc4\u4f30\u8005\u5bf9120\u4e2a\u72ec\u7279\u89c6\u9891\u9879\u76ee\u8fdb\u884c3\u6b21\u72ec\u7acb\u8bc4\u4f30\uff08\u51713240\u6b21\u8bc4\u4f30\uff09\uff0c\u8ba1\u7b97\u8bc4\u4f30\u8005\u95f4\u4fe1\u5ea6\uff08Krippendorff's \u03b1\uff09\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u4ece\u8bc4\u5206\u7279\u5f81\u8bc6\u522b\u8bc4\u4f30\u8005\u8eab\u4efd\uff0c\u5206\u6790\u8bc4\u4f30\u503e\u5411\u7684\u591a\u4e2a\u7ef4\u5ea6\u3002", "result": "\u8bc4\u4f30\u8005\u95f4\u4e00\u81f4\u6027\u63a5\u8fd1\u96f6\uff08\u03b1=0.042\uff09\uff0c\u67d0\u4e9b\u7ef4\u5ea6\u751a\u81f3\u4f4e\u4e8e\u968f\u673a\u566a\u58f0\u9884\u671f\uff08\u03b1<0\uff09\u3002\u4f46\u5206\u7c7b\u5668\u4ec5\u51ed\u8bc4\u5206\u5c31\u80fd\u4ee577.1%\u51c6\u786e\u7387\u8bc6\u522b\u8bc4\u4f30\u8005\uff0c\u52a0\u5165\u503e\u5411\u7279\u5f81\u540e\u8fbe89.9%\uff0c\u540c\u4e00\u6a21\u578b\u5bb6\u65cf\u5185\u533a\u5206\u51c6\u786e\u7387\u9ad8\u8fbe99.6%\u3002", "conclusion": "LLM\u8bc4\u4f30\u8005\u4e0d\u662f\u53ef\u4e92\u6362\u7684\u6d4b\u91cf\u5de5\u5177\uff0c\u800c\u662f\u5404\u81ea\u7f16\u7801\u4e86\u72ec\u7279\u7684\u8d28\u91cf\u8bc4\u4f30\u7406\u8bba\u3002\u5e73\u5747\u5b83\u4eec\u7684\u8bc4\u5206\u4f1a\u4ea7\u751f\u4e0d\u7b26\u5408\u4efb\u4f55\u8bc4\u4f30\u8005\u5b9e\u9645\u4ef7\u503c\u89c2\u7684\u5408\u6210\u5224\u65ad\uff0c\u8fd9\u5bf9\u4f9d\u8d56LLM\u8bc4\u4f30\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u6709\u91cd\u8981\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2601.05242", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05242", "abs": "https://arxiv.org/abs/2601.05242", "authors": ["Shih-Yang Liu", "Xin Dong", "Ximing Lu", "Shizhe Diao", "Peter Belcak", "Mingjie Liu", "Min-Hung Chen", "Hongxu Yin", "Yu-Chiang Frank Wang", "Kwang-Ting Cheng", "Yejin Choi", "Jan Kautz", "Pavlo Molchanov"], "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "comment": "NVIDIA-Tech Report", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGDPO\u65b9\u6cd5\u89e3\u51b3\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2dGRPO\u65b9\u6cd5\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u4f18\u4e8eGRPO\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u589e\u5f3a\uff0c\u7528\u6237\u671f\u671b\u5b83\u4eec\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u51c6\u786e\u54cd\u5e94\uff0c\u8fd8\u80fd\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7b26\u5408\u591a\u6837\u7684\u4eba\u7c7b\u504f\u597d\u3002\u5f53\u524d\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u9ed8\u8ba4\u4f7f\u7528GRPO\u65b9\u6cd5\uff0c\u4f46\u672a\u68c0\u9a8c\u5176\u9002\u7528\u6027\uff0c\u5b58\u5728\u5956\u52b1\u4fe1\u53f7\u5d29\u6e83\u5bfc\u81f4\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGroup reward-Decoupled Normalization Policy Optimization (GDPO)\uff0c\u901a\u8fc7\u89e3\u8026\u5404\u4e2a\u5956\u52b1\u7684\u5f52\u4e00\u5316\u5904\u7406\uff0c\u66f4\u771f\u5b9e\u5730\u4fdd\u7559\u5956\u52b1\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u591a\u5956\u52b1\u4f18\u5316\u5e76\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u63a8\u7406\u4e09\u4e2a\u4efb\u52a1\u4e0a\uff0cGDPO\u5728\u6b63\u786e\u6027\u6307\u6807\uff08\u51c6\u786e\u7387\u3001\u9519\u8bef\u7387\uff09\u548c\u7ea6\u675f\u9075\u5faa\u6307\u6807\uff08\u683c\u5f0f\u3001\u957f\u5ea6\uff09\u4e0a\u5747\u4e00\u81f4\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GDPO\u89e3\u51b3\u4e86\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2dGRPO\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u5f52\u4e00\u5316\u4fdd\u7559\u4e86\u8bad\u7ec3\u4fe1\u53f7\u5206\u8fa8\u7387\uff0c\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.05187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05187", "abs": "https://arxiv.org/abs/2601.05187", "authors": ["Yanchang Liang", "Xiaowei Zhao"], "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.", "AI": {"tldr": "SimuAgent\uff1a\u57fa\u4e8eLLM\u7684Simulink\u5efa\u6a21\u4e0e\u4eff\u771f\u4ee3\u7406\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u8ba1\u5212-\u6267\u884c\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u901a\u8fc7ReGRPO\u7b97\u6cd5\u89e3\u51b3\u957f\u65f6\u4efb\u52a1\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u57285300\u4e2a\u591a\u9886\u57df\u5efa\u6a21\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4o", "motivation": "LLM\u5728\u6587\u672c\u4ee3\u7801\u81ea\u52a8\u5316\u65b9\u9762\u53d6\u5f97\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5728\u56fe\u5f62\u5316\u5de5\u7a0b\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002Simulink\u7b49\u56fe\u5f62\u5316\u5efa\u6a21\u73af\u5883\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684AI\u8f85\u52a9\u89e3\u51b3\u65b9\u6848", "method": "1. \u7528\u7b80\u6d01\u7684\u5b57\u5178\u5f0fPython\u8868\u793a\u66ff\u4ee3\u5197\u957fXML\uff0c\u5927\u5e45\u51cf\u5c11token\u6570\u91cf\uff1b2. \u8f7b\u91cf\u7ea7\u8ba1\u5212-\u6267\u884c\u67b6\u6784\uff1b3. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u4f4e\u5c42\u5de5\u5177\u6280\u80fd+\u9ad8\u5c42\u8bbe\u8ba1\u63a8\u7406\uff09\uff1b4. \u63d0\u51faReflection-GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u53cd\u601d\u8f68\u8ff9\u63d0\u4f9b\u4e30\u5bcc\u4e2d\u95f4\u53cd\u9988\uff1b5. \u62bd\u8c61-\u91cd\u6784\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6cdb\u5316\u80fd\u529b", "result": "\u5728SimuBench\uff085300\u4e2a\u591a\u9886\u57df\u5efa\u6a21\u4efb\u52a1\uff09\u4e0a\uff0cQwen2.5-7B\u6a21\u578b\u5fae\u8c03\u540e\u6536\u655b\u66f4\u5feb\u3001\u5efa\u6a21\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u8d85\u8d8a\u6807\u51c6RL\u57fa\u7ebf\uff0c\u751a\u81f3\u8d85\u8fc7GPT-4o\u7684\u5c11\u6837\u672c\u63d0\u793a\u6027\u80fd\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e24\u9636\u6bb5\u8bfe\u7a0b\u548c\u62bd\u8c61-\u91cd\u6784\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6cdb\u5316\u80fd\u529b", "conclusion": "SimuAgent\u586b\u8865\u4e86LLM\u4e0e\u56fe\u5f62\u5316\u5efa\u6a21\u73af\u5883\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5de5\u4e1a\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5728\u672c\u5730\u786c\u4ef6\u4e0a\u5b8c\u5168\u8bad\u7ec3\u548c\u8fd0\u884c", "topic": "code agent"}}
{"id": "2601.05214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05214", "abs": "https://arxiv.org/abs/2601.05214", "authors": ["Kait Healy", "Bharathi Srinivasan", "Visakh Madathil", "Jing Wu"], "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b9e\u65f6\u68c0\u6d4bLLM\u5de5\u5177\u8c03\u7528\u5e7b\u89c9\u7684\u6846\u67b6\uff0c\u5229\u7528\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u5185\u90e8\u8868\u793a\u8fdb\u884c\u68c0\u6d4b\uff0c\u65e0\u9700\u591a\u6b21\u63a8\u7406\u6216\u5916\u90e8\u9a8c\u8bc1\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8", "motivation": "LLM\u5728\u5de5\u5177\u8c03\u7528\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff08\u9009\u62e9\u9519\u8bef\u5de5\u5177\u3001\u53c2\u6570\u683c\u5f0f\u9519\u8bef\u3001\u5de5\u5177\u7ed5\u8fc7\u884c\u4e3a\uff09\uff0c\u8fd9\u4f1a\u5f71\u54cd\u751f\u4ea7\u7cfb\u7edf\u4e2d\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7684\u53ef\u9760\u6027\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u4e00\u81f4\u5e76\u7ed5\u8fc7\u5b89\u5168\u5ba1\u8ba1\u63a7\u5236", "method": "\u5229\u7528LLM\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5185\u90e8\u8868\u793a\uff0c\u5728\u540c\u4e00\u4e2a\u524d\u5411\u4f20\u64ad\u4e2d\u5b9e\u65f6\u68c0\u6d4b\u5de5\u5177\u8c03\u7528\u5e7b\u89c9\uff0c\u65e0\u9700\u591a\u6b21\u524d\u5411\u4f20\u64ad\u6216\u5916\u90e8\u9a8c\u8bc1", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u68c0\u6d4b\u6027\u80fd\u9ad8\u8fbe86.4%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\uff0c\u7279\u522b\u64c5\u957f\u68c0\u6d4b\u53c2\u6570\u7ea7\u5e7b\u89c9\u548c\u4e0d\u9002\u5f53\u7684\u5de5\u5177\u9009\u62e9", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u9760\u4ee3\u7406\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u8c03\u7528\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u9ad8\u6548\u8bc6\u522b\u5173\u952e\u9519\u8bef", "topic": "agent analysis"}}
{"id": "2601.05215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.05215", "abs": "https://arxiv.org/abs/2601.05215", "authors": ["Tamil Sudaravan Mohan Doss", "Michael Xu", "Sudha Rao", "Andrew D. Wilson", "Balasaravanan Thoravi Kumaravel"], "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents", "comment": null, "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.\n  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86MineNPC-Task\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5f00\u653e\u4e16\u754cMinecraft\u4e2d\u8bc4\u4f30\u5177\u6709\u8bb0\u5fc6\u80fd\u529b\u548c\u6df7\u5408\u4e3b\u52a8\u6027\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u73a9\u5bb6\u5408\u4f5c\u8bbe\u8ba1\u4efb\u52a1\u6a21\u677f\u548c\u9a8c\u8bc1\u5668\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u5728\u4ee3\u7801\u6267\u884c\u3001\u7269\u54c1\u7ba1\u7406\u7b49\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5408\u6210\u63d0\u793a\uff0c\u7f3a\u4e4f\u771f\u5b9e\u73a9\u5bb6\u53c2\u4e0e\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u96be\u4ee5\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8bb0\u5fc6\u80fd\u529b\u548c\u6df7\u5408\u4e3b\u52a8\u6027\u4ea4\u4e92\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e13\u5bb6\u73a9\u5bb6\u5408\u4f5c\u8bbe\u8ba1\u4efb\u52a1\uff0c\u5c06\u4efb\u52a1\u89c4\u8303\u5316\u4e3a\u53c2\u6570\u5316\u6a21\u677f\uff0c\u5305\u542b\u660e\u786e\u7684\u5148\u51b3\u6761\u4ef6\u548c\u4f9d\u8d56\u7ed3\u6784\uff0c\u914d\u5408\u673a\u5668\u53ef\u68c0\u67e5\u7684\u9a8c\u8bc1\u5668\uff0c\u91c7\u7528\u9650\u5236\u77e5\u8bc6\u7b56\u7565\u9632\u6b62\u4f5c\u5f0a\uff0c\u6355\u83b7\u667a\u80fd\u4f53\u7684\u8ba1\u5212/\u884c\u52a8/\u8bb0\u5fc6\u4e8b\u4ef6\u3002", "result": "\u4f7f\u7528GPT-4o\u8bc4\u4f30\u4e868\u540d\u7ecf\u9a8c\u73a9\u5bb6\u7684216\u4e2a\u5b50\u4efb\u52a1\uff0c\u53d1\u73b0\u4ee3\u7801\u6267\u884c\u3001\u5e93\u5b58/\u5de5\u5177\u5904\u7406\u3001\u5f15\u7528\u548c\u5bfc\u822a\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u540c\u65f6\u6df7\u5408\u4e3b\u52a8\u6027\u6f84\u6e05\u548c\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u6709\u52a9\u4e8e\u6062\u590d\uff0c\u53c2\u4e0e\u8005\u5bf9\u4ea4\u4e92\u8d28\u91cf\u548c\u754c\u9762\u53ef\u7528\u6027\u8bc4\u4ef7\u79ef\u6781\u3002", "conclusion": "MineNPC-Task\u4e3a\u8bc4\u4f30\u8bb0\u5fc6\u611f\u77e5\u7684\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u9700\u8981\u66f4\u5f3a\u7684\u8de8\u4efb\u52a1\u8bb0\u5fc6\u6301\u4e45\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u5b8c\u6574\u4efb\u52a1\u5957\u4ef6\u548c\u5de5\u5177\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.572865a7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblogs.nvidia.com%2Fblog%2Fdgx-spark-and-station-open-source-frontier-models%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/GlnqUL8rxY2Fss4Xa5aWr37adUSo5ZB_HqphthZ5Sxw=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblogs.nvidia.com%2Fblog%2Fdgx-spark-and-station-open-source-frontier-models%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/GlnqUL8rxY2Fss4Xa5aWr37adUSo5ZB_HqphthZ5Sxw=438", "authors": ["TLDR Newsletter"], "title": "NVIDIA Unveils Personal Agent Robotics at CES", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblogs.nvidia.com%2Fblog%2Fdgx-spark-and-station-open-source-frontier-models%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/GlnqUL8rxY2Fss4Xa5aWr37adUSo5ZB_HqphthZ5Sxw=438", "summary": "NVIDIA Unveils Personal Agent Robotics at CES (6 minute read) At CES 2026, NVIDIA introduced Reachy Mini, a personal robotic AI agent powered by DGX Spark, capable of real-time multimodal interaction.", "source": "tldr", "AI": {"tldr": "NVIDIA\u5728CES 2026\u4e0a\u53d1\u5e03\u4e86Reachy Mini\u4e2a\u4eba\u673a\u5668\u4ebaAI\u4ee3\u7406\uff0c\u7531DGX Spark\u9a71\u52a8\uff0c\u5177\u5907\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u80fd\u529b", "motivation": "\u63a8\u52a8\u4e2a\u4eba\u673a\u5668\u4ebaAI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c", "method": "\u57fa\u4e8eDGX Spark\u5e73\u53f0\u5f00\u53d1\uff0c\u91c7\u7528\u591a\u6a21\u6001\u4ea4\u4e92\u6280\u672f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b", "result": "\u6210\u529f\u63a8\u51faReachy Mini\u4e2a\u4eba\u673a\u5668\u4ebaAI\u4ee3\u7406\uff0c\u5177\u5907\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u529f\u80fd", "conclusion": "NVIDIA\u5728CES 2026\u5c55\u793a\u4e86\u4e2a\u4eba\u673a\u5668\u4ebaAI\u4ee3\u7406\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u673a\u5668\u4eba\u53d1\u5c55\u5960\u5b9a\u57fa\u7840", "topic": "agent analysis"}}
{"id": "tldr.2601.3d96abc5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/xfDjJidaV4rtNZFCVKCWzpz7qCMYWxIP7F_8VYD_UwU=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/xfDjJidaV4rtNZFCVKCWzpz7qCMYWxIP7F_8VYD_UwU=438", "authors": ["TLDR Newsletter"], "title": "Thoughts on Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/xfDjJidaV4rtNZFCVKCWzpz7qCMYWxIP7F_8VYD_UwU=438", "summary": "Thoughts on Claude Code (17 minute read) Claude Code is a phenomenal coding companion that dramatically amplifies the fun side of coding by eliminating the annoying parts.", "source": "tldr", "AI": {"tldr": "Claude Code\u662f\u4e00\u4e2a\u5353\u8d8a\u7684\u7f16\u7801\u4f34\u4fa3\uff0c\u901a\u8fc7\u6d88\u9664\u7f16\u7801\u4e2d\u7684\u70e6\u4eba\u90e8\u5206\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7f16\u7801\u7684\u8da3\u5473\u6027", "motivation": "\u65e8\u5728\u89e3\u51b3\u7f16\u7801\u8fc7\u7a0b\u4e2d\u7684\u7e41\u7410\u548c\u4ee4\u4eba\u70e6\u607c\u7684\u90e8\u5206\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u4e13\u6ce8\u4e8e\u7f16\u7801\u7684\u4e50\u8da3\u548c\u521b\u9020\u6027\u65b9\u9762", "method": "\u4f5c\u4e3a\u7f16\u7801\u4f34\u4fa3\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5904\u7406\u7f16\u7801\u4e2d\u7684\u91cd\u590d\u6027\u4efb\u52a1\u548c\u7e41\u7410\u90e8\u5206\u6765\u63d0\u5347\u5f00\u53d1\u4f53\u9a8c", "result": "\u663e\u8457\u63d0\u5347\u4e86\u7f16\u7801\u7684\u4e50\u8da3\u548c\u6548\u7387\uff0c\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u4e13\u6ce8\u4e8e\u521b\u9020\u6027\u548c\u6709\u8da3\u7684\u7f16\u7801\u5de5\u4f5c", "conclusion": "Claude Code\u662f\u4e00\u4e2a\u80fd\u591f\u663e\u8457\u6539\u5584\u7f16\u7801\u4f53\u9a8c\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u6d88\u9664\u70e6\u4eba\u90e8\u5206\u6765\u589e\u5f3a\u7f16\u7801\u7684\u8da3\u5473\u6027", "topic": "code agent"}}
{"id": "tldr.2601.a91b00dd", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkucharski.substack.com%2Fp%2Fis-hallucination-free-ai-code-possible%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/jzVHcoqxjHF_F7HU1z0ZBJAYL4lhy9Sec2PxPL6IOwM=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkucharski.substack.com%2Fp%2Fis-hallucination-free-ai-code-possible%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/jzVHcoqxjHF_F7HU1z0ZBJAYL4lhy9Sec2PxPL6IOwM=438", "authors": ["TLDR Newsletter"], "title": "Is hallucination-free AI code possible?", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fkucharski.substack.com%2Fp%2Fis-hallucination-free-ai-code-possible%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/jzVHcoqxjHF_F7HU1z0ZBJAYL4lhy9Sec2PxPL6IOwM=438", "summary": "Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\uff0c\u867d\u7136\u9a8c\u8bc1\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\u4f1a\u6539\u8fdb\uff0c\u4f46\u751f\u6210\"\u6b63\u786e\"\u4ee3\u7801\u4ecd\u9762\u4e34\u6311\u6218", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u4e2d\u662f\u5426\u5b58\u5728\u5b8c\u5168\u6d88\u9664\u5e7b\u89c9\u7684\u53ef\u80fd\u6027\uff0c\u5206\u6790\u5f53\u524d\u6280\u672f\u9650\u5236\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411", "method": "\u901a\u8fc7\u5206\u6790\u9a8c\u8bc1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u57fa\u7840\u6a21\u578b\u7684\u6539\u8fdb\u7a7a\u95f4\uff0c\u8ba8\u8bbaAI\u4ee3\u7801\u751f\u6210\u7684\u6280\u672f\u6311\u6218", "result": "\u9a8c\u8bc1\u65b9\u6cd5\u5728\u67d0\u4e9b\u9886\u57df\u80fd\u68c0\u6d4b\u5927\u90e8\u5206\u903b\u8f91\u7f3a\u9677\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\u5b8c\u5168\u6b63\u786e\u4ee3\u7801\u65b9\u9762\u4ecd\u6709\u56f0\u96be", "conclusion": "\u5b8c\u5168\u65e0\u5e7b\u89c9\u7684AI\u4ee3\u7801\u751f\u6210\u53ef\u80fd\u96be\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u7ed3\u5408\u9a8c\u8bc1\u65b9\u6cd5\u548c\u6a21\u578b\u6539\u8fdb\u7684\u6301\u7eed\u52aa\u529b", "topic": "code agent"}}
{"id": "tldr.2601.9da31b28", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/3tF6JrqAlDDlFCOVmK86XdKQT5kFqM7bo4oQjhcFhr4=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/3tF6JrqAlDDlFCOVmK86XdKQT5kFqM7bo4oQjhcFhr4=438", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/3tF6JrqAlDDlFCOVmK86XdKQT5kFqM7bo4oQjhcFhr4=438", "summary": "Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\uff0c\u5373\u4f7f\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u53d1\u73b0\u903b\u8f91\u7f3a\u9677\uff0c\u57fa\u7840\u6a21\u578b\u4ecd\u96be\u4ee5\u751f\u6210\u5b8c\u5168\"\u6b63\u786e\"\u7684\u4ee3\u7801\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u6839\u672c\u6311\u6218\uff0c\u5206\u6790\u5f53\u524d\u9a8c\u8bc1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\"\u6b63\u786e\"\u4ee3\u7801\u65b9\u9762\u9762\u4e34\u7684\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u6280\u672f\u7684\u73b0\u72b6\uff0c\u7ed3\u5408\u9a8c\u8bc1\u65b9\u6cd5\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u8ba8\u8bba\u5e7b\u89c9\u95ee\u9898\u7684\u672c\u8d28\u548c\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u867d\u7136\u9a8c\u8bc1\u65b9\u6cd5\u5728\u67d0\u4e9b\u9886\u57df\u80fd\u53d1\u73b0\u5927\u90e8\u5206\u903b\u8f91\u7f3a\u9677\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\u5b8c\u5168\u6b63\u786e\u4ee3\u7801\u65b9\u9762\u4ecd\u9762\u4e34\u6839\u672c\u6027\u6311\u6218\uff0c\u5e7b\u89c9\u95ee\u9898\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\u3002", "conclusion": "\u65e0\u5e7b\u89c9\u7684AI\u4ee3\u7801\u751f\u6210\u53ef\u80fd\u96be\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\"\u6b63\u786e\u6027\"\u7684\u5b9a\u4e49\u548c\u66f4\u5148\u8fdb\u7684\u9a8c\u8bc1\u6280\u672f\u3002", "topic": "code agent"}}
{"id": "tldr.2601.9851ca80", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/umXedCz4MizOOwAa1SAgkjXqSWvsliR9LkD2B5icW38=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/umXedCz4MizOOwAa1SAgkjXqSWvsliR9LkD2B5icW38=438", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/umXedCz4MizOOwAa1SAgkjXqSWvsliR9LkD2B5icW38=438", "summary": "Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u96be\u4ee5\u5b8c\u5168\u907f\u514d\u5e7b\u89c9\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u53d1\u73b0\u903b\u8f91\u7f3a\u9677\u4f46\u96be\u4ee5\u4fdd\u8bc1\u4ee3\u7801\u5b8c\u5168\u6b63\u786e", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u6839\u672c\u6027\u6311\u6218\uff0c\u5373\u4f7f\u9a8c\u8bc1\u65b9\u6cd5\u6539\u8fdb\u548c\u57fa\u7840\u6a21\u578b\u63d0\u5347\uff0c\u4ecd\u96be\u4ee5\u5b9e\u73b0\u5b8c\u5168\u65e0\u5e7b\u89c9\u7684\u4ee3\u7801\u751f\u6210", "method": "\u5206\u6790\u6027\u8ba8\u8bba\uff0c\u57fa\u4e8e\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\u6280\u672f\u7684\u53d1\u5c55\u73b0\u72b6\u8fdb\u884c\u7406\u8bba\u5206\u6790", "result": "\u6307\u51fa\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u68c0\u6d4b\u5927\u90e8\u5206\u903b\u8f91\u7f3a\u9677\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\"\u6b63\u786e\"\u4ee3\u7801\u65b9\u9762\u4ecd\u6709\u6839\u672c\u6027\u56f0\u96be", "conclusion": "\u5b8c\u5168\u65e0\u5e7b\u89c9\u7684AI\u4ee3\u7801\u751f\u6210\u53ef\u80fd\u96be\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u63a5\u53d7AI\u4ee3\u7801\u751f\u6210\u5b58\u5728\u4e00\u5b9a\u7a0b\u5ea6\u7684\u5e7b\u89c9\u95ee\u9898", "topic": "code agent"}}
{"id": "tldr.2601.44b802d4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ex1aSst8HjqmIo4qYqujkJdsHfszEg-uU7LoexLmOcw=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ex1aSst8HjqmIo4qYqujkJdsHfszEg-uU7LoexLmOcw=438", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ex1aSst8HjqmIo4qYqujkJdsHfszEg-uU7LoexLmOcw=438", "summary": "Is hallucination-free AI code possible? (11 minute read) While verification methods will soon be able to pick up almost all logical flaws in some fields, and foundational models will get better, they may still struggle to produce 'correct' code.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\uff0c\u867d\u7136\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u68c0\u6d4b\u903b\u8f91\u7f3a\u9677\uff0c\u4f46\u751f\u6210\"\u6b63\u786e\"\u4ee3\u7801\u4ecd\u5177\u6311\u6218\u6027", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u672c\u8d28\uff0c\u5206\u6790\u5f53\u524d\u9a8c\u8bc1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7406\u89e3\u4e3a\u4ec0\u4e48\u5373\u4f7f\u6a21\u578b\u6539\u8fdb\u4e5f\u96be\u4ee5\u4fdd\u8bc1\u4ee3\u7801\u5b8c\u5168\u6b63\u786e", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u9a8c\u8bc1\u6280\u672f\u7684\u8986\u76d6\u8303\u56f4\u548c\u5c40\u9650\u6027\uff0c\u7ed3\u5408\u5bf9\u57fa\u7840\u6a21\u578b\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u63a2\u8ba8AI\u4ee3\u7801\u751f\u6210\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0", "result": "\u9a8c\u8bc1\u65b9\u6cd5\u5728\u67d0\u4e9b\u9886\u57df\u80fd\u68c0\u6d4b\u5927\u591a\u6570\u903b\u8f91\u7f3a\u9677\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\"\u6b63\u786e\"\u4ee3\u7801\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5e7b\u89c9\u95ee\u9898\u96be\u4ee5\u5b8c\u5168\u89e3\u51b3", "conclusion": "\u65e0\u5e7b\u89c9\u7684AI\u4ee3\u7801\u751f\u6210\u53ef\u80fd\u96be\u4ee5\u5b9e\u73b0\uff0c\u9700\u8981\u7ed3\u5408\u6539\u8fdb\u7684\u9a8c\u8bc1\u65b9\u6cd5\u548c\u66f4\u5f3a\u5927\u7684\u6a21\u578b\uff0c\u4f46\u5bf9\"\u6b63\u786e\u6027\"\u7684\u5b9a\u4e49\u672c\u8eab\u4e5f\u9700\u8981\u91cd\u65b0\u601d\u8003", "topic": "code agent"}}
{"id": "tldr.2601.76635da7", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrnewsletter/1/0100019b9832a2dd-aab3cad6-5eb2-4a2b-9bbb-88cf1fe1e558-000000/mj7M6YDWaW8XXGhxWLxdzAYvcNwTt950GHTvv98vZyY=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrnewsletter/1/0100019b9832a2dd-aab3cad6-5eb2-4a2b-9bbb-88cf1fe1e558-000000/mj7M6YDWaW8XXGhxWLxdzAYvcNwTt950GHTvv98vZyY=439", "authors": ["TLDR Newsletter"], "title": "Mantic", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrnewsletter/1/0100019b9832a2dd-aab3cad6-5eb2-4a2b-9bbb-88cf1fe1e558-000000/mj7M6YDWaW8XXGhxWLxdzAYvcNwTt950GHTvv98vZyY=439", "summary": "Mantic (GitHub Repo) Mantic is a structural code search engine for AI agents that provides sub-500ms file ranking across massive code bases without embeddings, vector databases, or external dependencies.", "source": "tldr", "AI": {"tldr": "Mantic\u662f\u4e00\u4e2a\u4e3aAI\u4ee3\u7406\u8bbe\u8ba1\u7684\u7ed3\u6784\u5316\u4ee3\u7801\u641c\u7d22\u5f15\u64ce\uff0c\u80fd\u5728\u4e0d\u4f9d\u8d56\u5d4c\u5165\u3001\u5411\u91cf\u6570\u636e\u5e93\u6216\u5916\u90e8\u4f9d\u8d56\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5b9e\u73b0\u4e9a500\u6beb\u79d2\u7684\u6587\u4ef6\u6392\u540d\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5d4c\u5165\u548c\u5411\u91cf\u6570\u636e\u5e93\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u53ef\u80fd\u5b58\u5728\u6027\u80fd\u74f6\u9888\u548c\u4f9d\u8d56\u590d\u6742\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u4ee3\u7801\u641c\u7d22\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301AI\u4ee3\u7406\u7684\u5de5\u4f5c\u3002", "method": "Mantic\u91c7\u7528\u7ed3\u6784\u5316\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u5feb\u901f\u6587\u4ef6\u6392\u540d\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5d4c\u5165\u548c\u5411\u91cf\u6570\u636e\u5e93\u7684\u4f7f\u7528\uff0c\u51cf\u5c11\u4e86\u5916\u90e8\u4f9d\u8d56\u3002", "result": "Mantic\u80fd\u591f\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5b9e\u73b0\u4e9a500\u6beb\u79d2\u7684\u6587\u4ef6\u6392\u540d\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u7ea7\u548c\u65e0\u5916\u90e8\u4f9d\u8d56\u7684\u7279\u6027\u3002", "conclusion": "Mantic\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u4ee3\u7801\u641c\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2601.5e79c524", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fironies_of_ai_2%2F%3Futm_source=tldrdevops/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/NbyPECknqBEUTCFxjP9bufAViiOdbHGo_WYwHQXi_p0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fironies_of_ai_2%2F%3Futm_source=tldrdevops/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/NbyPECknqBEUTCFxjP9bufAViiOdbHGo_WYwHQXi_p0=439", "authors": ["TLDR Newsletter"], "title": "AI and the ironies of automation", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.ufried.com%2Fblog%2Fironies_of_ai_2%2F%3Futm_source=tldrdevops/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/NbyPECknqBEUTCFxjP9bufAViiOdbHGo_WYwHQXi_p0=439", "summary": "AI and the ironies of automation (15 minute read) Lisanne Bainbridge's 1983 paper on automation ironies applies to AI agent supervision, revealing that effective human oversight requires better UI design, continuous training for rare interventions, and leadership skills to direct agent fleets. The paradox intensifies as AI agents improve because successful automation demands greater investment in human operator training and interface design to handle exceptional situations under time pressure.", "source": "tldr", "AI": {"tldr": "Bainbridge\u7684\u81ea\u52a8\u5316\u6096\u8bba\u9002\u7528\u4e8eAI\u4ee3\u7406\u76d1\u7763\uff1aAI\u8d8a\u5148\u8fdb\uff0c\u8d8a\u9700\u8981\u6295\u8d44\u4e8e\u4eba\u7c7b\u64cd\u4f5c\u5458\u57f9\u8bad\u3001\u754c\u9762\u8bbe\u8ba1\u548c\u9886\u5bfc\u6280\u80fd\uff0c\u4ee5\u5904\u7406\u5f02\u5e38\u60c5\u51b5", "motivation": "\u5c061983\u5e74Bainbridge\u7684\u81ea\u52a8\u5316\u6096\u8bba\u5e94\u7528\u4e8e\u73b0\u4ee3AI\u4ee3\u7406\u76d1\u7763\uff0c\u63ed\u793aAI\u7cfb\u7edf\u6539\u8fdb\u53cd\u800c\u589e\u52a0\u4e86\u4eba\u7c7b\u76d1\u7763\u7684\u590d\u6742\u6027\u548c\u8981\u6c42", "method": "\u5e94\u7528\u7ecf\u5178\u81ea\u52a8\u5316\u7406\u8bba\u6846\u67b6\u5206\u6790AI\u4ee3\u7406\u76d1\u7763\uff0c\u5f3a\u8c03\u4e09\u4e2a\u5173\u952e\u9700\u6c42\uff1a\u66f4\u597d\u7684UI\u8bbe\u8ba1\u3001\u7f55\u89c1\u5e72\u9884\u7684\u6301\u7eed\u57f9\u8bad\u3001\u7ba1\u7406\u4ee3\u7406\u56e2\u961f\u7684\u9886\u5bfc\u6280\u80fd", "result": "\u53d1\u73b0\u81ea\u52a8\u5316\u6096\u8bba\u5728AI\u65f6\u4ee3\u52a0\u5267\uff1aAI\u4ee3\u7406\u8d8a\u6210\u529f\uff0c\u8d8a\u9700\u8981\u6295\u8d44\u4e8e\u4eba\u7c7b\u64cd\u4f5c\u5458\u57f9\u8bad\u3001\u754c\u9762\u8bbe\u8ba1\u548c\u9886\u5bfc\u80fd\u529b\uff0c\u4ee5\u5e94\u5bf9\u65f6\u95f4\u538b\u529b\u4e0b\u7684\u5f02\u5e38\u60c5\u51b5", "conclusion": "\u6709\u6548\u7684AI\u4ee3\u7406\u76d1\u7763\u9700\u8981\u91cd\u65b0\u601d\u8003\u4eba\u673a\u534f\u4f5c\uff0c\u6295\u8d44\u4e8e\u4eba\u7c7b\u80fd\u529b\u53d1\u5c55\u800c\u975e\u4ec5\u4ec5\u81ea\u52a8\u5316\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7f55\u89c1\u4f46\u5173\u952e\u7684\u5f02\u5e38\u60c5\u51b5\u65f6", "topic": "agent analysis"}}
{"id": "tldr.2601.e9344fb4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwatch.getcontrast.io%2Fregister%2Fbitrise-bringing-devops-to-mobile-avoiding-pitfalls-and-unlocking-velocity%3Futm_medium=paid_other%26utm_source=tldr%26utm_campaign=all_webinar_bringing-devops-to-mobile_all_2025-06-30%26utm_content=secondary-placement-sponsorship/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/An1U0QgLESVb9Jqm61DnBD72st23EjnEi4ZEO7fnInI=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwatch.getcontrast.io%2Fregister%2Fbitrise-bringing-devops-to-mobile-avoiding-pitfalls-and-unlocking-velocity%3Futm_medium=paid_other%26utm_source=tldr%26utm_campaign=all_webinar_bringing-devops-to-mobile_all_2025-06-30%26utm_content=secondary-placement-sponsorship/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/An1U0QgLESVb9Jqm61DnBD72st23EjnEi4ZEO7fnInI=439", "authors": ["TLDR Newsletter"], "title": "See how BuzzFeed fixed mobile CI and cut QA time by 95%", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwatch.getcontrast.io%2Fregister%2Fbitrise-bringing-devops-to-mobile-avoiding-pitfalls-and-unlocking-velocity%3Futm_medium=paid_other%26utm_source=tldr%26utm_campaign=all_webinar_bringing-devops-to-mobile_all_2025-06-30%26utm_content=secondary-placement-sponsorship/1/0100019b9859e7b8-4af03deb-0891-42f7-a132-d1e63f851b35-000000/An1U0QgLESVb9Jqm61DnBD72st23EjnEi4ZEO7fnInI=439", "summary": "See how BuzzFeed fixed mobile CI and cut QA time by 95% (Sponsor) Mobile breaks standard DevOps patterns: code signing complexity, simulator sprawl, toolchains that fight your infrastructure. Learn from BuzzFeed's principal engineer and see how they ditched costly anti-patterns, scaled to 1,000+ builds a month, and made releases reliable and see how you can do the same. Watch for free \u2192", "source": "tldr", "AI": {"tldr": "BuzzFeed\u901a\u8fc7\u4f18\u5316\u79fb\u52a8CI\u6d41\u7a0b\uff0c\u5c06QA\u65f6\u95f4\u51cf\u5c1195%\uff0c\u89e3\u51b3\u4e86\u79fb\u52a8\u5f00\u53d1\u4e2d\u7684\u4ee3\u7801\u7b7e\u540d\u590d\u6742\u6027\u3001\u6a21\u62df\u5668\u6cdb\u6ee5\u548c\u5de5\u5177\u94fe\u51b2\u7a81\u7b49\u95ee\u9898", "motivation": "\u79fb\u52a8\u5f00\u53d1\u6253\u7834\u4e86\u6807\u51c6\u7684DevOps\u6a21\u5f0f\uff0c\u9762\u4e34\u4ee3\u7801\u7b7e\u540d\u590d\u6742\u6027\u3001\u6a21\u62df\u5668\u6cdb\u6ee5\u3001\u5de5\u5177\u94fe\u4e0e\u57fa\u7840\u8bbe\u65bd\u51b2\u7a81\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6784\u5efa\u6210\u672c\u9ad8\u3001\u53d1\u5e03\u4e0d\u53ef\u9760", "method": "\u629b\u5f03\u6210\u672c\u9ad8\u6602\u7684\u53cd\u6a21\u5f0f\uff0c\u4f18\u5316\u79fb\u52a8CI\u6d41\u7a0b\uff0c\u5b9e\u73b0\u89c4\u6a21\u5316\u6784\u5efa\uff08\u6bcf\u67081000+\u6b21\u6784\u5efa\uff09\uff0c\u4f7f\u53d1\u5e03\u6d41\u7a0b\u66f4\u52a0\u53ef\u9760", "result": "\u6210\u529f\u5c06QA\u65f6\u95f4\u51cf\u5c1195%\uff0c\u6784\u5efa\u89c4\u6a21\u6269\u5927\u5230\u6bcf\u67081000+\u6b21\uff0c\u53d1\u5e03\u53d8\u5f97\u66f4\u52a0\u53ef\u9760", "conclusion": "\u901a\u8fc7\u4f18\u5316\u79fb\u52a8CI\u6d41\u7a0b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u51cf\u5c11QA\u65f6\u95f4\uff0c\u5b9e\u73b0\u89c4\u6a21\u5316\u53ef\u9760\u53d1\u5e03\uff0c\u5176\u4ed6\u56e2\u961f\u53ef\u4ee5\u501f\u9274BuzzFeed\u7684\u7ecf\u9a8c", "topic": "swe application"}}
{"id": "tldr.2601.79011963", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/tMTR4rrXtmkZh4jXKqFP1YyREgE-_PgLjbEEAaRB1b8=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/tMTR4rrXtmkZh4jXKqFP1YyREgE-_PgLjbEEAaRB1b8=439", "authors": ["TLDR Newsletter"], "title": "Mantic", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmarcoaapfortes%2FMantic.sh%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/tMTR4rrXtmkZh4jXKqFP1YyREgE-_PgLjbEEAaRB1b8=439", "summary": "Mantic (GitHub Repo) Mantic is a structural code search engine for AI agents. It provides sub-500ms file ranking across massive codebases without relying on embeddings or external dependencies. Mantic infers intent from file structure and metadata, reducing token usage and running entirely locally.", "source": "tldr", "AI": {"tldr": "Mantic\u662f\u4e00\u4e2a\u4e3aAI\u4ee3\u7406\u8bbe\u8ba1\u7684\u7ed3\u6784\u5316\u4ee3\u7801\u641c\u7d22\u5f15\u64ce\uff0c\u80fd\u5728\u4e0d\u4f9d\u8d56\u5d4c\u5165\u6216\u5916\u90e8\u4f9d\u8d56\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u5b9e\u73b0\u4e9a500\u6beb\u79d2\u7684\u6587\u4ef6\u6392\u540d", "motivation": "\u73b0\u6709\u4ee3\u7801\u641c\u7d22\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5d4c\u5165\u5411\u91cf\u6216\u5916\u90e8\u4f9d\u8d56\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u9ad8\u3001\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e0d\u9002\u5408AI\u4ee3\u7406\u5728\u672c\u5730\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c", "method": "\u901a\u8fc7\u5206\u6790\u6587\u4ef6\u7ed3\u6784\u548c\u5143\u6570\u636e\u6765\u63a8\u65ad\u641c\u7d22\u610f\u56fe\uff0c\u5b8c\u5168\u5728\u672c\u5730\u8fd0\u884c\uff0c\u907f\u514d\u4f7f\u7528\u5d4c\u5165\u5411\u91cf\u548c\u5916\u90e8\u4f9d\u8d56", "result": "\u5b9e\u73b0\u4e86\u4e9a500\u6beb\u79d2\u7684\u6587\u4ef6\u6392\u540d\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11token\u4f7f\u7528\u91cf\uff0c\u652f\u6301\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u7684\u9ad8\u6548\u641c\u7d22", "conclusion": "Mantic\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u4ee3\u7801\u641c\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u672c\u5730\u8fd0\u884c\u548c\u4f4e\u5ef6\u8fdf\u7684\u573a\u666f", "topic": "code agent"}}
{"id": "tldr.2601.9047dd19", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbadlogic%2Fpi-mono%2Ftree%2Fmain%2Fpackages%2Fcoding-agent%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/izQag4WUa54eT209cPLgbkSRps_Hhd0G1Ryf4HXrPa0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbadlogic%2Fpi-mono%2Ftree%2Fmain%2Fpackages%2Fcoding-agent%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/izQag4WUa54eT209cPLgbkSRps_Hhd0G1Ryf4HXrPa0=439", "authors": ["TLDR Newsletter"], "title": "Pi Coding Agent", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fbadlogic%2Fpi-mono%2Ftree%2Fmain%2Fpackages%2Fcoding-agent%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/izQag4WUa54eT209cPLgbkSRps_Hhd0G1Ryf4HXrPa0=439", "summary": "Pi Coding Agent (GitHub Repo) The Pi Coding Agent is a terminal-based coding agent for headless coding tasks that runs on macOS, Linux, and Windows and supports multiple model providers with mid-session model switching. It has slash-command workflows, session saving + branching, context compaction, bash/tool execution, and customization via settings. It also has SDK/RPC modes for programmatic use.", "source": "tldr", "AI": {"tldr": "Pi Coding Agent\u662f\u4e00\u4e2a\u7ec8\u7aef\u4ee3\u7801\u4ee3\u7406\uff0c\u652f\u6301\u65e0\u5934\u7f16\u7801\u4efb\u52a1\uff0c\u8de8\u5e73\u53f0\u8fd0\u884c\uff0c\u652f\u6301\u591a\u6a21\u578b\u63d0\u4f9b\u5546\u548c\u4f1a\u8bdd\u4e2d\u6a21\u578b\u5207\u6362\uff0c\u5177\u6709\u659c\u6760\u547d\u4ee4\u5de5\u4f5c\u6d41\u3001\u4f1a\u8bdd\u4fdd\u5b58/\u5206\u652f\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u3001bash/\u5de5\u5177\u6267\u884c\u7b49\u529f\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u3001\u7075\u6d3b\u7684\u7ec8\u7aef\u4ee3\u7801\u4ee3\u7406\uff0c\u89e3\u51b3\u5f00\u53d1\u8005\u5728\u65e0\u5934\u73af\u5883\u4e2d\u8fdb\u884c\u7f16\u7801\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u8de8\u5e73\u53f0\u652f\u6301\u3001\u591a\u6a21\u578b\u9009\u62e9\u3001\u4f1a\u8bdd\u7ba1\u7406\u548c\u81ea\u5b9a\u4e49\u529f\u80fd\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u7ec8\u7aef\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u578b\u63d0\u4f9b\u5546\u63a5\u53e3\uff0c\u5b9e\u73b0\u659c\u6760\u547d\u4ee4\u5de5\u4f5c\u6d41\u3001\u4f1a\u8bdd\u7ba1\u7406\uff08\u4fdd\u5b58/\u5206\u652f\uff09\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u6280\u672f\u3001bash\u548c\u5de5\u5177\u6267\u884c\u529f\u80fd\uff0c\u5e76\u63d0\u4f9bSDK/RPC\u6a21\u5f0f\u4f9b\u7f16\u7a0b\u4f7f\u7528\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51faPi Coding Agent\uff0c\u652f\u6301macOS\u3001Linux\u3001Windows\u5e73\u53f0\uff0c\u5177\u5907\u591a\u6a21\u578b\u652f\u6301\u3001\u4f1a\u8bdd\u4e2d\u6a21\u578b\u5207\u6362\u3001\u9ad8\u6548\u5de5\u4f5c\u6d41\u3001\u4f1a\u8bdd\u7ba1\u7406\u3001\u4e0a\u4e0b\u6587\u4f18\u5316\u548c\u5de5\u5177\u6267\u884c\u80fd\u529b\u3002", "conclusion": "Pi Coding Agent\u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u7684\u7ec8\u7aef\u4ee3\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u7f16\u7801\u52a9\u624b\uff0c\u7279\u522b\u9002\u5408\u65e0\u5934\u73af\u5883\u4e0b\u7684\u7f16\u7801\u4efb\u52a1\u3002", "topic": "code agent"}}
{"id": "tldr.2601.f30e4247", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpluralistic.net%2F2026%2F01%2F06%2F1000x-liability%2F%23graceful-failure-modes%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/bKO4dCfsVLCuTccMi2yRGskpC2bDuLXAtxCX5zZoI94=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpluralistic.net%2F2026%2F01%2F06%2F1000x-liability%2F%23graceful-failure-modes%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/bKO4dCfsVLCuTccMi2yRGskpC2bDuLXAtxCX5zZoI94=439", "authors": ["TLDR Newsletter"], "title": "Code is a liability", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 22 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpluralistic.net%2F2026%2F01%2F06%2F1000x-liability%2F%23graceful-failure-modes%3Futm_source=tldrdev/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/bKO4dCfsVLCuTccMi2yRGskpC2bDuLXAtxCX5zZoI94=439", "summary": "Code is a liability (not an asset) (22 minute read) Code, especially when generated by AI, is a liability rather than an asset, as code requires maintainability to last long-term.", "source": "tldr", "AI": {"tldr": "AI\u751f\u6210\u7684\u4ee3\u7801\u662f\u8d1f\u503a\u800c\u975e\u8d44\u4ea7\uff0c\u56e0\u4e3a\u4ee3\u7801\u9700\u8981\u957f\u671f\u7ef4\u62a4\u624d\u80fd\u6301\u7eed\u5b58\u5728", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08\u5982GitHub Copilot\u3001ChatGPT\uff09\u88ab\u8fc7\u5ea6\u5ba3\u4f20\u4e3a\"\u8d44\u4ea7\"\uff0c\u4f46\u5b9e\u9645\u4e0a\u751f\u6210\u7684\u4ee3\u7801\u9700\u8981\u5927\u91cf\u7ef4\u62a4\u5de5\u4f5c\uff0c\u53cd\u800c\u6210\u4e3a\u6280\u672f\u8d1f\u503a", "method": "\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u4f5c\u4e3a\u8d44\u4ea7\u7684\u7279\u6027\uff08\u53ef\u7ef4\u62a4\u6027\u3001\u53ef\u7406\u89e3\u6027\u3001\u53ef\u6d4b\u8bd5\u6027\uff09\u4e0eAI\u751f\u6210\u4ee3\u7801\u7684\u5b9e\u9645\u8d28\u91cf\uff0c\u5bf9\u6bd4\u4f20\u7edf\u4ee3\u7801\u5f00\u53d1\u4e0eAI\u8f85\u52a9\u5f00\u53d1\u7684\u5dee\u5f02", "result": "AI\u751f\u6210\u7684\u4ee3\u7801\u901a\u5e38\u7f3a\u4e4f\u6587\u6863\u3001\u6d4b\u8bd5\u8986\u76d6\u7387\u4f4e\u3001\u67b6\u6784\u8bbe\u8ba1\u4e0d\u4f73\uff0c\u5bfc\u81f4\u957f\u671f\u7ef4\u62a4\u6210\u672c\u9ad8\u4e8e\u5f00\u53d1\u6210\u672c\uff0c\u6210\u4e3a\u6280\u672f\u8d1f\u503a\u800c\u975e\u8d44\u4ea7", "conclusion": "\u5e94\u5c06AI\u4ee3\u7801\u751f\u6210\u89c6\u4e3a\u8f85\u52a9\u5de5\u5177\u800c\u975e\u66ff\u4ee3\u65b9\u6848\uff0c\u5f00\u53d1\u8005\u4ecd\u9700\u4fdd\u6301\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u63a7\u5236\uff0c\u907f\u514d\u5c06AI\u751f\u6210\u7684\u4ee3\u7801\u89c6\u4e3a\"\u514d\u8d39\u8d44\u4ea7\"", "topic": "code agent"}}
{"id": "tldr.2601.abd66a41", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/pNwOu3lKLCvZFX_cCYzvEljJbp4U6D_ARL17fbca7cI=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/pNwOu3lKLCvZFX_cCYzvEljJbp4U6D_ARL17fbca7cI=439", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/pNwOu3lKLCvZFX_cCYzvEljJbp4U6D_ARL17fbca7cI=439", "summary": "Code is a liability (not an asset) (22 minute read) Code, especially when generated by AI, is a liability rather than an asset, as code requires maintainability to last long-term.", "source": "tldr", "AI": {"tldr": "\u4ee3\u7801\uff08\u5c24\u5176\u662fAI\u751f\u6210\u7684\uff09\u662f\u8d1f\u503a\u800c\u975e\u8d44\u4ea7\uff0c\u56e0\u4e3a\u4ee3\u7801\u9700\u8981\u53ef\u7ef4\u62a4\u6027\u624d\u80fd\u957f\u671f\u5b58\u5728", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u76db\u884c\uff0c\u4f46\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u7f3a\u4e4f\u957f\u671f\u53ef\u7ef4\u62a4\u6027\uff0c\u5bfc\u81f4\u6280\u672f\u503a\u52a1\u79ef\u7d2f", "method": "\u901a\u8fc7\u5206\u6790AI\u751f\u6210\u4ee3\u7801\u7684\u7279\u70b9\u3001\u7ef4\u62a4\u6210\u672c\u548c\u6280\u672f\u503a\u52a1\u5f62\u6210\u673a\u5236\uff0c\u63d0\u51fa\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6", "result": "AI\u751f\u6210\u7684\u4ee3\u7801\u867d\u7136\u80fd\u5feb\u901f\u4ea7\u51fa\uff0c\u4f46\u7f3a\u4e4f\u8bbe\u8ba1\u6a21\u5f0f\u3001\u6587\u6863\u548c\u53ef\u8bfb\u6027\uff0c\u957f\u671f\u7ef4\u62a4\u6210\u672c\u8fdc\u9ad8\u4e8e\u4eba\u5de5\u7f16\u5199\u4ee3\u7801", "conclusion": "\u5e94\u5c06AI\u751f\u6210\u7684\u4ee3\u7801\u89c6\u4e3a\u6280\u672f\u8d1f\u503a\u800c\u975e\u8d44\u4ea7\uff0c\u9700\u8981\u5efa\u7acb\u4e25\u683c\u7684\u4ee3\u7801\u5ba1\u67e5\u548c\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b", "topic": "code agent"}}
{"id": "tldr.2601.17721be0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/KvC-BA5ooKSJVaU9cQMDSnN21ZStrd18nzvSnugK354=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/KvC-BA5ooKSJVaU9cQMDSnN21ZStrd18nzvSnugK354=439", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b985d3f20-f61c6219-ef37-402b-a77e-3ce2a8b42aeb-000000/KvC-BA5ooKSJVaU9cQMDSnN21ZStrd18nzvSnugK354=439", "summary": "Code is a liability (not an asset) (22 minute read) Code, especially when generated by AI, is a liability rather than an asset, as code requires maintainability to last long-term.", "source": "tldr", "AI": {"tldr": "AI\u751f\u6210\u7684\u4ee3\u7801\u662f\u8d1f\u503a\u800c\u975e\u8d44\u4ea7\uff0c\u56e0\u4e3a\u4ee3\u7801\u9700\u8981\u53ef\u7ef4\u62a4\u6027\u624d\u80fd\u957f\u671f\u5b58\u5728", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff08\u5982GitHub Copilot\u3001ChatGPT\uff09\u867d\u7136\u80fd\u5feb\u901f\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u751f\u6210\u7684\u4ee3\u7801\u5f80\u5f80\u7f3a\u4e4f\u53ef\u7ef4\u62a4\u6027\u3001\u53ef\u8bfb\u6027\u548c\u957f\u671f\u53ef\u6301\u7eed\u6027\uff0c\u5bfc\u81f4\u6280\u672f\u503a\u52a1\u79ef\u7d2f", "method": "\u901a\u8fc7\u5206\u6790AI\u751f\u6210\u4ee3\u7801\u7684\u7279\u70b9\uff0c\u5bf9\u6bd4\u4f20\u7edf\u4ee3\u7801\u5f00\u53d1\u6a21\u5f0f\uff0c\u63d0\u51fa\u4ee3\u7801\u4f5c\u4e3a\u8d1f\u503a\u800c\u975e\u8d44\u4ea7\u7684\u7406\u5ff5\uff0c\u5f3a\u8c03\u4ee3\u7801\u7ef4\u62a4\u6210\u672c\u7684\u91cd\u8981\u6027", "result": "AI\u751f\u6210\u7684\u4ee3\u7801\u867d\u7136\u80fd\u5feb\u901f\u5b9e\u73b0\u529f\u80fd\uff0c\u4f46\u589e\u52a0\u4e86\u957f\u671f\u7ef4\u62a4\u6210\u672c\u3001\u6280\u672f\u503a\u52a1\u548c\u7cfb\u7edf\u590d\u6742\u6027\uff0c\u5b9e\u9645\u4ef7\u503c\u53ef\u80fd\u4e3a\u8d1f", "conclusion": "\u5f00\u53d1\u8005\u5e94\u8c28\u614e\u4f7f\u7528AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\uff0c\u4f18\u5148\u8003\u8651\u4ee3\u7801\u8d28\u91cf\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u957f\u671f\u53ef\u6301\u7eed\u6027\uff0c\u800c\u975e\u5355\u7eaf\u8ffd\u6c42\u5f00\u53d1\u901f\u5ea6", "topic": "code agent"}}
{"id": "tldr.2601.1c3b9240", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cautiousoptimism.news%2Fp%2Feveryone-is-excited-about-claude%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/oIQ8P0w8RneM_1_duNEzRuraJ61FuttWU69vIlpT2LE=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cautiousoptimism.news%2Fp%2Feveryone-is-excited-about-claude%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/oIQ8P0w8RneM_1_duNEzRuraJ61FuttWU69vIlpT2LE=439", "authors": ["TLDR Newsletter"], "title": "Everyone Is Excited About Claude", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cautiousoptimism.news%2Fp%2Feveryone-is-excited-about-claude%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/oIQ8P0w8RneM_1_duNEzRuraJ61FuttWU69vIlpT2LE=439", "summary": "Everyone Is Excited About Claude (2 minute read) We are watching the democratization of competence in real time. The excitement around tools like Claude Code is about the sudden acceleration of human potential. We are crossing a threshold where software no longer just assists but actively up-levels the user, allowing a mid-level engineer to perform like a senior architect. This signals a massive explosion in software creation from people who previously lacked the technical skill to build what...", "source": "tldr", "AI": {"tldr": "Claude Code\u7b49\u5de5\u5177\u6b63\u5728\u5b9e\u73b0\u80fd\u529b\u7684\u6c11\u4e3b\u5316\uff0c\u8ba9\u4e2d\u7ea7\u5de5\u7a0b\u5e08\u80fd\u50cf\u9ad8\u7ea7\u67b6\u6784\u5e08\u4e00\u6837\u5de5\u4f5c\uff0c\u5927\u5e45\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u7684\u6280\u672f\u95e8\u69db", "motivation": "\u63a2\u8ba8AI\u4ee3\u7801\u52a9\u624b\u5982\u4f55\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u683c\u5c40\uff0c\u5b9e\u73b0\u80fd\u529b\u7684\u6c11\u4e3b\u5316\uff0c\u8ba9\u66f4\u591a\u4eba\u80fd\u591f\u53c2\u4e0e\u8f6f\u4ef6\u521b\u9020", "method": "\u901a\u8fc7\u5206\u6790Claude Code\u7b49\u5de5\u5177\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u89c2\u5bdf\u8f6f\u4ef6\u5f00\u53d1\u80fd\u529b\u95e8\u69db\u7684\u964d\u4f4e\u8fc7\u7a0b", "result": "AI\u4ee3\u7801\u52a9\u624b\u6b63\u5728\u5b9e\u73b0\u8f6f\u4ef6\u5f00\u53d1\u7684\u6c11\u4e3b\u5316\uff0c\u8ba9\u4e2d\u7ea7\u5de5\u7a0b\u5e08\u8fbe\u5230\u9ad8\u7ea7\u67b6\u6784\u5e08\u7684\u6c34\u5e73\uff0c\u663e\u8457\u6269\u5927\u8f6f\u4ef6\u5f00\u53d1\u4eba\u7fa4", "conclusion": "\u6211\u4eec\u6b63\u5728\u89c1\u8bc1\u8f6f\u4ef6\u5f00\u53d1\u80fd\u529b\u7684\u6c11\u4e3b\u5316\u9769\u547d\uff0cAI\u5de5\u5177\u4e0d\u4ec5\u8f85\u52a9\u5f00\u53d1\uff0c\u66f4\u63d0\u5347\u7528\u6237\u80fd\u529b\uff0c\u5c06\u5e26\u6765\u8f6f\u4ef6\u521b\u9020\u7684\u7206\u70b8\u5f0f\u589e\u957f", "topic": "code agent"}}
{"id": "tldr.2601.e9e6adb6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finstruct.ai%2F%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/WYGDMDS6DFg0D_LllOkeTA45W2CWyxavmdfPSa0eTrw=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finstruct.ai%2F%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/WYGDMDS6DFg0D_LllOkeTA45W2CWyxavmdfPSa0eTrw=439", "authors": ["TLDR Newsletter"], "title": "Instruct 2.5", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finstruct.ai%2F%3Futm_source=tldrfounders/1/0100019b9892920b-d8b7ed15-ce5c-44f6-8c05-0afdba9a2456-000000/WYGDMDS6DFg0D_LllOkeTA45W2CWyxavmdfPSa0eTrw=439", "summary": "Instruct 2.5 (Tool) Instruct 2.5 is an autonomous AI agent that executes tasks across connected apps in real time and lets users save successful runs as reusable workflows.", "source": "tldr", "AI": {"tldr": "Instruct 2.5\u662f\u4e00\u4e2a\u81ea\u4e3bAI\u4ee3\u7406\uff0c\u80fd\u591f\u5728\u8fde\u63a5\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u5b9e\u65f6\u6267\u884c\u4efb\u52a1\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5c06\u6210\u529f\u8fd0\u884c\u4fdd\u5b58\u4e3a\u53ef\u91cd\u7528\u5de5\u4f5c\u6d41\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u9700\u8981\u5728\u591a\u4e2a\u5e94\u7528\u7a0b\u5e8f\u4e4b\u95f4\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u7684\u624b\u52a8\u64cd\u4f5c\u95ee\u9898\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u5f00\u53d1\u81ea\u4e3bAI\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u7406\u89e3\u7528\u6237\u6307\u4ee4\u5e76\u5728\u8fde\u63a5\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u5b9e\u65f6\u6267\u884c\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u4f9b\u5de5\u4f5c\u6d41\u4fdd\u5b58\u529f\u80fd\u3002", "result": "\u521b\u5efa\u4e86Instruct 2.5\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u8de8\u5e94\u7528\u4efb\u52a1\u81ea\u52a8\u5316\u6267\u884c\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u6210\u529f\u64cd\u4f5c\u4fdd\u5b58\u4e3a\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u5de5\u4f5c\u6d41\u3002", "conclusion": "Instruct 2.5\u901a\u8fc7AI\u4ee3\u7406\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8de8\u5e94\u7528\u4efb\u52a1\u6267\u884c\u7684\u6548\u7387\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u4fbf\u6377\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u529f\u80fd\u3002", "topic": "code agent"}}
{"id": "tldr.2601.b46d244a", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fgoogle-launches-a2ui%2F%3Futm_source=tldrdesign/1/0100019b989e1671-ebbcb307-6fd7-4b98-80f5-dd8e851bb33c-000000/nHtdJU--UCHpvnhlvgAj-BJgqeZ1qVhGxQN1AG66SIA=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fgoogle-launches-a2ui%2F%3Futm_source=tldrdesign/1/0100019b989e1671-ebbcb307-6fd7-4b98-80f5-dd8e851bb33c-000000/nHtdJU--UCHpvnhlvgAj-BJgqeZ1qVhGxQN1AG66SIA=439", "authors": ["TLDR Newsletter"], "title": "Google Launches A2UI", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fux-news.com%2Fgoogle-launches-a2ui%2F%3Futm_source=tldrdesign/1/0100019b989e1671-ebbcb307-6fd7-4b98-80f5-dd8e851bb33c-000000/nHtdJU--UCHpvnhlvgAj-BJgqeZ1qVhGxQN1AG66SIA=439", "summary": "Google Launches A2UI (2 minute read) Google's A2UI (Agent-to-User Interface) is an open-source specification that allows AI agents to generate rich, interactive, native user interfaces by sending declarative UI descriptions instead of executable code. It enables host applications to render components securely and consistently across web, mobile, and desktop platforms. By separating UI generation from UI rendering, A2UI improves security and preserves design control for host apps. It supports ...", "source": "tldr", "AI": {"tldr": "Google\u63a8\u51faA2UI\u5f00\u6e90\u89c4\u8303\uff0c\u8ba9AI\u4ee3\u7406\u901a\u8fc7\u53d1\u9001\u58f0\u660e\u5f0fUI\u63cf\u8ff0\u800c\u975e\u53ef\u6267\u884c\u4ee3\u7801\u6765\u751f\u6210\u4e30\u5bcc\u7684\u4ea4\u4e92\u5f0f\u539f\u751f\u7528\u6237\u754c\u9762", "motivation": "\u4f20\u7edfAI\u4ee3\u7406\u751f\u6210UI\u65f6\u9700\u8981\u53d1\u9001\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\u4e14\u96be\u4ee5\u8de8\u5e73\u53f0\u4e00\u81f4\u6e32\u67d3\u3002A2UI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u58f0\u660e\u5f0fUI\u63cf\u8ff0\u5b9e\u73b0\u5b89\u5168\u3001\u4e00\u81f4\u7684\u8de8\u5e73\u53f0UI\u751f\u6210", "method": "A2UI\u91c7\u7528\u58f0\u660e\u5f0fUI\u63cf\u8ff0\u89c4\u8303\uff0c\u5c06UI\u751f\u6210\u4e0e\u6e32\u67d3\u5206\u79bb\u3002AI\u4ee3\u7406\u53d1\u9001UI\u63cf\u8ff0\uff0c\u4e3b\u673a\u5e94\u7528\u8d1f\u8d23\u5b89\u5168\u6e32\u67d3\uff0c\u652f\u6301Web\u3001\u79fb\u52a8\u548c\u684c\u9762\u5e73\u53f0", "result": "A2UI\u5b9e\u73b0\u4e86AI\u4ee3\u7406\u751f\u6210\u4e30\u5bcc\u4ea4\u4e92\u5f0fUI\u7684\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u4fdd\u6301\u4e86\u4e3b\u673a\u5e94\u7528\u7684\u8bbe\u8ba1\u63a7\u5236\u6743\uff0c\u652f\u6301\u8de8\u5e73\u53f0\u4e00\u81f4\u6e32\u67d3", "conclusion": "A2UI\u89c4\u8303\u4e3aAI\u4ee3\u7406\u751f\u6210\u7528\u6237\u754c\u9762\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u8de8\u5e73\u53f0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u63cf\u8ff0\u800c\u975e\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5e73\u8861\u4e86\u7075\u6d3b\u6027\u4e0e\u5b89\u5168\u6027", "topic": "code agent"}}
{"id": "tldr.2601.bc4d2191", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.galois.com%2Farticles%2Fescaping-isla-nublar-coming-around-to-llms-for-formal-methods%3Futm_source=tldrinfosec/1/0100019b98c99fa1-175162d4-0485-42fd-aed9-43f92433dce2-000000/0_QtzR3CWjoHQla3txY8Y4cD-6MM3VJglBLEPxc6xf4=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.galois.com%2Farticles%2Fescaping-isla-nublar-coming-around-to-llms-for-formal-methods%3Futm_source=tldrinfosec/1/0100019b98c99fa1-175162d4-0485-42fd-aed9-43f92433dce2-000000/0_QtzR3CWjoHQla3txY8Y4cD-6MM3VJglBLEPxc6xf4=439", "authors": ["TLDR Newsletter"], "title": "Escaping Isla Nublar: Coming around to LLMs for Formal Methods", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.galois.com%2Farticles%2Fescaping-isla-nublar-coming-around-to-llms-for-formal-methods%3Futm_source=tldrinfosec/1/0100019b98c99fa1-175162d4-0485-42fd-aed9-43f92433dce2-000000/0_QtzR3CWjoHQla3txY8Y4cD-6MM3VJglBLEPxc6xf4=439", "summary": "Escaping Isla Nublar: Coming around to LLMs for Formal Methods (13 minute read) CNnotator is a tool that combines large language models (LLMs) with formal verification to generate memory-safety annotations for C code automatically. It works through an iterative process where LLM-generated annotations are tested against a CN verifier. This approach addresses the challenge of translating legacy C/C++ code\u2014which accounts for 70% of Chromium's security bugs\u2014into memory-safe languages like Rust. T...", "source": "tldr", "AI": {"tldr": "CNnotator\u5de5\u5177\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\uff0c\u81ea\u52a8\u4e3aC\u4ee3\u7801\u751f\u6210\u5185\u5b58\u5b89\u5168\u6ce8\u89e3\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u9a8c\u8bc1\u6ce8\u89e3\u6b63\u786e\u6027\uff0c\u5e2e\u52a9\u5c06\u9057\u7559C/C++\u4ee3\u7801\u8f6c\u6362\u4e3a\u5185\u5b58\u5b89\u5168\u8bed\u8a00\u5982Rust", "motivation": "\u89e3\u51b3\u9057\u7559C/C++\u4ee3\u7801\uff08\u5360Chromium\u5b89\u5168\u6f0f\u6d1e70%\uff09\u5411\u5185\u5b58\u5b89\u5168\u8bed\u8a00\uff08\u5982Rust\uff09\u8f6c\u6362\u7684\u6311\u6218\uff0c\u4f20\u7edf\u624b\u52a8\u6dfb\u52a0\u5185\u5b58\u5b89\u5168\u6ce8\u89e3\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\uff0c\u91c7\u7528\u8fed\u4ee3\u8fc7\u7a0b\uff1aLLM\u751f\u6210\u5185\u5b58\u5b89\u5168\u6ce8\u89e3\uff0cCN\u9a8c\u8bc1\u5668\u6d4b\u8bd5\u8fd9\u4e9b\u6ce8\u89e3\uff0c\u6839\u636e\u53cd\u9988\u4e0d\u65ad\u6539\u8fdb\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u6ce8\u89e3\u751f\u6210", "result": "\u6210\u529f\u5f00\u53d1\u51faCNnotator\u5de5\u5177\uff0c\u80fd\u591f\u81ea\u52a8\u4e3aC\u4ee3\u7801\u751f\u6210\u5185\u5b58\u5b89\u5168\u6ce8\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4ee3\u7801\u8f6c\u6362\u6548\u7387\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u7684\u7ed3\u5408\u4e3a\u4ee3\u7801\u8f6c\u6362\u548c\u5b89\u5168\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0cCNnotator\u5c55\u793a\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u5b9e\u7528\u4ef7\u503c", "topic": "code agent"}}
{"id": "tldr.2601.b688bfde", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fdynamic-context-discovery%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/c8oGqfFCbS398U95hkiwu4WAz7mg1-lm5aTvNZaBMQ0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fdynamic-context-discovery%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/c8oGqfFCbS398U95hkiwu4WAz7mg1-lm5aTvNZaBMQ0=439", "authors": ["TLDR Newsletter"], "title": "Dynamic Context Discovery in Coding Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fdynamic-context-discovery%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/c8oGqfFCbS398U95hkiwu4WAz7mg1-lm5aTvNZaBMQ0=439", "summary": "Dynamic Context Discovery in Coding Agents (4 minute read) Cursor has introduced a token-efficient strategy called dynamic context discovery where agents selectively pull relevant data during inference rather than loading static context. Techniques include treating tool outputs and terminal sessions as files, summarizing past chat history, and selectively loading tools via the Agent Skills standard.", "source": "tldr", "AI": {"tldr": "Cursor\u63d0\u51fa\u52a8\u6001\u4e0a\u4e0b\u6587\u53d1\u73b0\u7b56\u7565\uff0c\u8ba9\u7f16\u7801\u4ee3\u7406\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6027\u63d0\u53d6\u76f8\u5173\u6570\u636e\u800c\u975e\u52a0\u8f7d\u9759\u6001\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u63d0\u9ad8token\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7f16\u7801\u4ee3\u7406\u901a\u5e38\u52a0\u8f7d\u5927\u91cf\u9759\u6001\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4token\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u66f4\u667a\u80fd\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u5e76\u63d0\u9ad8\u4ee3\u7406\u6027\u80fd\u3002", "method": "\u91c7\u7528\u52a8\u6001\u4e0a\u4e0b\u6587\u53d1\u73b0\u7b56\u7565\uff0c\u5305\u62ec\uff1a\u5c06\u5de5\u5177\u8f93\u51fa\u548c\u7ec8\u7aef\u4f1a\u8bdd\u89c6\u4e3a\u6587\u4ef6\u5904\u7406\u3001\u603b\u7ed3\u8fc7\u53bb\u7684\u804a\u5929\u5386\u53f2\u3001\u901a\u8fc7Agent Skills\u6807\u51c6\u9009\u62e9\u6027\u52a0\u8f7d\u5de5\u5177\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86token\u6548\u7387\u7684\u63d0\u5347\uff0c\u4f7f\u7f16\u7801\u4ee3\u7406\u80fd\u591f\u66f4\u667a\u80fd\u5730\u7ba1\u7406\u4e0a\u4e0b\u6587\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684token\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u8d28\u91cf\u3002", "conclusion": "\u52a8\u6001\u4e0a\u4e0b\u6587\u53d1\u73b0\u662f\u7f16\u7801\u4ee3\u7406\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4e0a\u4e0b\u6587\u52a0\u8f7d\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86token\u4f7f\u7528\u6548\u7387\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u7f16\u7801\u4ee3\u7406\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "tldr.2601.c0c3126e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fchangelog%2Fai-gateway-support-for-claude-code%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/WWvPvdqsj5Q15b2CQQwN3apisXWvTbJYeJRBIIMAmF0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fchangelog%2Fai-gateway-support-for-claude-code%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/WWvPvdqsj5Q15b2CQQwN3apisXWvTbJYeJRBIIMAmF0=439", "authors": ["TLDR Newsletter"], "title": "AI Gateway support for Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvercel.com%2Fchangelog%2Fai-gateway-support-for-claude-code%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/WWvPvdqsj5Q15b2CQQwN3apisXWvTbJYeJRBIIMAmF0=439", "summary": "AI Gateway support for Claude Code (1 minute read) Vercel AI Gateway's Anthropic-compatible API endpoint now features Claude Code. Developers can now route Claude Code requests through AI Gateway to centralize usage and spend, view traces in observability, and benefit from failover between providers. Users will have to log out and back in and set environment variables to configure Claude Code to use AI Gateway.", "source": "tldr", "AI": {"tldr": "Vercel AI Gateway\u65b0\u589e\u5bf9Claude Code\u7684\u652f\u6301\uff0c\u5f00\u53d1\u8005\u53ef\u901a\u8fc7\u7edf\u4e00API\u7aef\u70b9\u7ba1\u7406Claude Code\u8bf7\u6c42\uff0c\u5b9e\u73b0\u7528\u91cf\u76d1\u63a7\u3001\u6545\u969c\u8f6c\u79fb\u548c\u96c6\u4e2d\u8ba1\u8d39", "motivation": "\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u7edf\u4e00\u7684AI\u670d\u52a1\u7ba1\u7406\u5e73\u53f0\uff0c\u7b80\u5316\u591aAI\u6a21\u578b\u96c6\u6210\uff0c\u5b9e\u73b0\u7528\u91cf\u76d1\u63a7\u3001\u6210\u672c\u63a7\u5236\u548c\u6545\u969c\u8f6c\u79fb\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u901a\u8fc7Vercel AI Gateway\u63d0\u4f9bAnthropic\u517c\u5bb9\u7684API\u7aef\u70b9\uff0c\u652f\u6301Claude Code\u8bf7\u6c42\u8def\u7531\uff0c\u5f00\u53d1\u8005\u9700\u91cd\u65b0\u767b\u5f55\u5e76\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u8fdb\u884c\u914d\u7f6e", "result": "\u5f00\u53d1\u8005\u73b0\u5728\u53ef\u4ee5\u901a\u8fc7AI Gateway\u96c6\u4e2d\u7ba1\u7406Claude Code\u8bf7\u6c42\uff0c\u67e5\u770b\u4f7f\u7528\u8ffd\u8e2a\uff0c\u5e76\u5728\u4e0d\u540c\u63d0\u4f9b\u5546\u4e4b\u95f4\u5b9e\u73b0\u6545\u969c\u8f6c\u79fb", "conclusion": "AI Gateway\u5bf9Claude Code\u7684\u652f\u6301\u589e\u5f3a\u4e86AI\u670d\u52a1\u7ba1\u7406\u7684\u7edf\u4e00\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8fd0\u7ef4\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2601.fedb932d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falinaqi%2Fclaude-bootstrap%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/VUpWUedWJm7_nooqSVp1zRyNl9BetwSgHgKbyet1X0g=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falinaqi%2Fclaude-bootstrap%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/VUpWUedWJm7_nooqSVp1zRyNl9BetwSgHgKbyet1X0g=439", "authors": ["TLDR Newsletter"], "title": "Claude Bootstrap", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Falinaqi%2Fclaude-bootstrap%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/VUpWUedWJm7_nooqSVp1zRyNl9BetwSgHgKbyet1X0g=439", "summary": "Claude Bootstrap (GitHub Repo) AI can generate infinite code, but humans are still needed to review, understand, and maintain it. This moves the bottleneck from code generation to code comprehension. Claude Bootstrap is an opinionated project initialization system for Claude Code. It keeps AI-generated code simple, secure, and verifiable.", "source": "tldr", "AI": {"tldr": "Claude Bootstrap\u662f\u4e00\u4e2a\u7528\u4e8eClaude Code\u7684\u56fa\u6267\u5df1\u89c1\u9879\u76ee\u521d\u59cb\u5316\u7cfb\u7edf\uff0c\u65e8\u5728\u4fdd\u6301AI\u751f\u6210\u4ee3\u7801\u7684\u7b80\u5355\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5c06\u74f6\u9888\u4ece\u4ee3\u7801\u751f\u6210\u8f6c\u79fb\u5230\u4ee3\u7801\u7406\u89e3", "motivation": "\u867d\u7136AI\u53ef\u4ee5\u751f\u6210\u65e0\u9650\u4ee3\u7801\uff0c\u4f46\u4eba\u7c7b\u4ecd\u7136\u9700\u8981\u5ba1\u67e5\u3001\u7406\u89e3\u548c\u7ef4\u62a4\u8fd9\u4e9b\u4ee3\u7801\uff0c\u8fd9\u5bfc\u81f4\u74f6\u9888\u4ece\u4ee3\u7801\u751f\u6210\u8f6c\u79fb\u5230\u4e86\u4ee3\u7801\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f7fAI\u751f\u6210\u7684\u4ee3\u7801\u4fdd\u6301\u7b80\u5355\u3001\u5b89\u5168\u548c\u53ef\u9a8c\u8bc1", "method": "\u5f00\u53d1Claude Bootstrap\u4f5c\u4e3a\u4e00\u4e2a\u56fa\u6267\u5df1\u89c1\u7684\u9879\u76ee\u521d\u59cb\u5316\u7cfb\u7edf\uff0c\u4e13\u95e8\u4e3aClaude Code\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9884\u8bbe\u7684\u67b6\u6784\u548c\u89c4\u8303\u6765\u7ea6\u675fAI\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u4fdd\u6301AI\u751f\u6210\u4ee3\u7801\u7b80\u5355\u3001\u5b89\u5168\u3001\u53ef\u9a8c\u8bc1\u7684\u9879\u76ee\u521d\u59cb\u5316\u7cfb\u7edf\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u7ef4\u62a4AI\u751f\u6210\u7684\u4ee3\u7801", "conclusion": "Claude Bootstrap\u901a\u8fc7\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u9879\u76ee\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86AI\u4ee3\u7801\u751f\u6210\u540e\u7684\u7406\u89e3\u548c\u7ef4\u62a4\u95ee\u9898\uff0c\u4f7fAI\u751f\u6210\u7684\u4ee3\u7801\u66f4\u52a0\u5b9e\u7528\u548c\u53ef\u7ba1\u7406", "topic": "code agent"}}
{"id": "tldr.2601.ef11ccb0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjustin.abrah.ms%2Fblog%2F2026-01-05-wrapping-my-head-around-gas-town.html%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/kF0twWelh7L2AB4zWO9BVPuzPBMZGCl6ZFjAHiOiPK0=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjustin.abrah.ms%2Fblog%2F2026-01-05-wrapping-my-head-around-gas-town.html%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/kF0twWelh7L2AB4zWO9BVPuzPBMZGCl6ZFjAHiOiPK0=439", "authors": ["TLDR Newsletter"], "title": "Wrapping my head around Gas Town", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjustin.abrah.ms%2Fblog%2F2026-01-05-wrapping-my-head-around-gas-town.html%3Futm_source=tldrai/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/kF0twWelh7L2AB4zWO9BVPuzPBMZGCl6ZFjAHiOiPK0=439", "summary": "Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.", "source": "tldr", "AI": {"tldr": "Gas Town\u662f\u4e00\u4e2aLLM\u7f16\u6392\u5668\uff0c\u5141\u8bb8\u7528\u6237\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u72ec\u7acb\u671d\u7740\u65e2\u5b9a\u76ee\u6807\u524d\u8fdb", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u9700\u8981\u540c\u65f6\u7ba1\u7406\u591a\u4e2aAI\u5b9e\u4f8b\u534f\u540c\u5de5\u4f5c\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u7f16\u6392\u5de5\u5177\u6765\u534f\u8c03\u5927\u91cfClaude Code\u5b9e\u4f8b\u5e76\u884c\u6267\u884c\u4efb\u52a1", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u7f16\u6392\u5668\u7cfb\u7edf\uff0c\u80fd\u591f\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u72ec\u7acb\u5de5\u4f5c\u5e76\u671d\u7740\u7528\u6237\u8bbe\u5b9a\u7684\u76ee\u6807\u524d\u8fdb", "result": "\u521b\u5efa\u4e86Gas Town\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u4e2aClaude Code\u5b9e\u4f8b\u7684\u6709\u6548\u7ba1\u7406\u548c\u534f\u8c03", "conclusion": "Gas Town\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21LLM\u5b9e\u4f8b\u534f\u540c\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u6392\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2601.10cbcbce", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/-Aak5lhxU_zQNdlrqpPFICL7fn2lks0w0xAoF1i-kUI=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/-Aak5lhxU_zQNdlrqpPFICL7fn2lks0w0xAoF1i-kUI=439", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/-Aak5lhxU_zQNdlrqpPFICL7fn2lks0w0xAoF1i-kUI=439", "summary": "Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.", "source": "tldr", "AI": {"tldr": "Gas Town\u662f\u4e00\u4e2aLLM\u7f16\u6392\u5668\uff0c\u5141\u8bb8\u7528\u6237\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u72ec\u7acb\u671d\u7740\u65e2\u5b9a\u76ee\u6807\u63a8\u8fdb", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u4efb\u52a1\u6216\u9700\u8981\u4eba\u5de5\u5e72\u9884\uff0c\u7f3a\u4e4f\u540c\u65f6\u534f\u8c03\u591a\u4e2aAI\u4ee3\u7406\u5e76\u884c\u5de5\u4f5c\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u7f16\u6392\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u542f\u52a8\u548c\u7ba1\u7406\u591a\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u72ec\u7acb\u6267\u884c\u4efb\u52a1\uff0c\u7cfb\u7edf\u8d1f\u8d23\u534f\u8c03\u548c\u76d1\u63a7\u8fdb\u5ea6", "result": "\u5b9e\u73b0\u4e86\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aAI\u4ee3\u7406\u5e76\u884c\u5de5\u4f5c\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u548c\u6548\u7387", "conclusion": "Gas Town\u5c55\u793a\u4e86\u5927\u89c4\u6a21AI\u4ee3\u7406\u534f\u8c03\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "tldr.2601.d20cc8c2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/f0h8Xel_0ag20PYTMc9VeImNPBWUVd-VJ_Nm-vxkH2I=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/f0h8Xel_0ag20PYTMc9VeImNPBWUVd-VJ_Nm-vxkH2I=439", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/f0h8Xel_0ag20PYTMc9VeImNPBWUVd-VJ_Nm-vxkH2I=439", "summary": "Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.", "source": "tldr", "AI": {"tldr": "Gas Town\u662f\u4e00\u4e2aLLM\u7f16\u6392\u5668\uff0c\u5141\u8bb8\u7528\u6237\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u72ec\u7acb\u671d\u7740\u65e2\u5b9a\u76ee\u6807\u524d\u8fdb", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u9700\u8981\u540c\u65f6\u7ba1\u7406\u591a\u4e2aAI\u5b9e\u4f8b\u6765\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u7f16\u6392\u5de5\u5177\u6765\u534f\u8c03\u591a\u4e2aAI\u4ee3\u7406\u7684\u5e76\u884c\u5de5\u4f5c", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u7f16\u6392\u5668\u6846\u67b6\uff0c\u652f\u6301\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u72ec\u7acb\u5de5\u4f5c\u5e76\u671d\u7740\u5171\u540c\u76ee\u6807\u524d\u8fdb", "result": "\u521b\u5efa\u4e86Gas Town\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u534f\u8c03\u591a\u4e2aAI\u4ee3\u7406\u7684\u5e76\u884c\u5de5\u4f5c\uff0c\u63d0\u9ad8\u590d\u6742\u4efb\u52a1\u7684\u5904\u7406\u6548\u7387", "conclusion": "Gas Town\u4e3a\u7ba1\u7406\u591a\u4e2aLLM\u5b9e\u4f8b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u6392\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5904\u7406\u9700\u8981\u5e76\u884cAI\u534f\u4f5c\u7684\u590d\u6742\u4efb\u52a1", "topic": "code agent"}}
{"id": "tldr.2601.b2aa2589", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/SYsXSChWMe0QiGyk2xZuKBAGn5URhMr_pgErYLauR0U=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/SYsXSChWMe0QiGyk2xZuKBAGn5URhMr_pgErYLauR0U=439", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2026-01-07, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b98d22e15-ad2bdd3e-dae2-4b46-8765-ab24357bcac8-000000/SYsXSChWMe0QiGyk2xZuKBAGn5URhMr_pgErYLauR0U=439", "summary": "Wrapping my head around Gas Town (7 minute read) Gas Town is an LLM orchestrator that allows users to manage dozens of Claude Code instances at once as they make independent progress towards some stated goals.", "source": "tldr", "AI": {"tldr": "Gas Town\u662f\u4e00\u4e2aLLM\u7f16\u6392\u5668\uff0c\u5141\u8bb8\u7528\u6237\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u72ec\u7acb\u671d\u7740\u65e2\u5b9a\u76ee\u6807\u524d\u8fdb", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u9700\u8981\u540c\u65f6\u7ba1\u7406\u591a\u4e2aAI\u4ee3\u7406\u5b9e\u4f8b\u6765\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u7f16\u6392\u5de5\u5177\u6765\u534f\u8c03\u8fd9\u4e9b\u5b9e\u4f8b\u7684\u72ec\u7acb\u5de5\u4f5c", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u7f16\u6392\u5668\u7cfb\u7edf\uff0c\u80fd\u591f\u540c\u65f6\u7ba1\u7406\u6570\u5341\u4e2aClaude Code\u5b9e\u4f8b\uff0c\u8ba9\u5b83\u4eec\u5e76\u884c\u5de5\u4f5c\u5e76\u72ec\u7acb\u671d\u7740\u7528\u6237\u8bbe\u5b9a\u7684\u76ee\u6807\u524d\u8fdb", "result": "\u521b\u5efa\u4e86Gas Town\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u4e2aLLM\u5b9e\u4f8b\u7684\u6709\u6548\u7f16\u6392\u548c\u7ba1\u7406\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387", "conclusion": "Gas Town\u4e3a\u7ba1\u7406\u591a\u4e2aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u6392\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2601.6e5b1adc", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmachinelearningmastery.com%2Fmastering-llm-tool-calling-the-complete-framework-for-connecting-models-to-the-real-world%2F%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/R7cFNwJiuRc61InqTM0rVxSkHNGXzdojyqAbKW2WEg4=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmachinelearningmastery.com%2Fmastering-llm-tool-calling-the-complete-framework-for-connecting-models-to-the-real-world%2F%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/R7cFNwJiuRc61InqTM0rVxSkHNGXzdojyqAbKW2WEg4=439", "authors": ["TLDR Newsletter"], "title": "Mastering LLM Tool Calling: The Complete Framework for Connecting Models to the Real World", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmachinelearningmastery.com%2Fmastering-llm-tool-calling-the-complete-framework-for-connecting-models-to-the-real-world%2F%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/R7cFNwJiuRc61InqTM0rVxSkHNGXzdojyqAbKW2WEg4=439", "summary": "Mastering LLM Tool Calling: The Complete Framework for Connecting Models to the Real World (6 minute read) LLM tool calling is the mechanism enabling models to invoke external functions/APIs, turning static LLMs into dynamic agents for data access, computation, and real-world actions. It includes three pillars to build reliable, production-ready agents: Data Access (retrieval tools), Computation (precision processing), and Actions (effecting changes).", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684LLM\u5de5\u5177\u8c03\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u8bbf\u95ee\u3001\u8ba1\u7b97\u548c\u884c\u52a8\u4e09\u5927\u652f\u67f1\uff0c\u5c06\u9759\u6001LLM\u8f6c\u53d8\u4e3a\u80fd\u591f\u8fde\u63a5\u73b0\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u4ee3\u7406\u3002", "motivation": "\u5f53\u524d\u7684LLM\u4e3b\u8981\u662f\u9759\u6001\u6a21\u578b\uff0c\u7f3a\u4e4f\u4e0e\u73b0\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u5de5\u5177\u8c03\u7528\u673a\u5236\u53ef\u4ee5\u4f7fLLM\u8bbf\u95ee\u5916\u90e8\u6570\u636e\u3001\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\u5e76\u6267\u884c\u5b9e\u9645\u52a8\u4f5c\uff0c\u4ece\u800c\u521b\u5efa\u66f4\u5f3a\u5927\u3001\u5b9e\u7528\u7684AI\u4ee3\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u652f\u67f1\u6846\u67b6\uff1a1) \u6570\u636e\u8bbf\u95ee\u5de5\u5177\uff08\u68c0\u7d22\u5de5\u5177\uff09\uff0c2) \u8ba1\u7b97\u5de5\u5177\uff08\u7cbe\u786e\u5904\u7406\uff09\uff0c3) \u884c\u52a8\u5de5\u5177\uff08\u4ea7\u751f\u5b9e\u9645\u53d8\u5316\uff09\u3002\u8fd9\u4e2a\u6846\u67b6\u65e8\u5728\u6784\u5efa\u53ef\u9760\u3001\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8fde\u63a5LLM\u4e0e\u73b0\u5b9e\u4e16\u754c\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u52a8\u6001\u8c03\u7528\u5916\u90e8\u51fd\u6570\u548cAPI\uff0c\u4ece\u800c\u6269\u5c55\u4e86LLM\u7684\u80fd\u529b\u8fb9\u754c\u3002", "conclusion": "\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u673a\u5236\uff0cLLM\u53ef\u4ee5\u4ece\u9759\u6001\u6a21\u578b\u8f6c\u53d8\u4e3a\u80fd\u591f\u8bbf\u95ee\u6570\u636e\u3001\u8fdb\u884c\u8ba1\u7b97\u548c\u6267\u884c\u5b9e\u9645\u52a8\u4f5c\u7684\u52a8\u6001\u4ee3\u7406\uff0c\u4e3a\u6784\u5efa\u751f\u4ea7\u7ea7AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u6574\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "tldr.2601.65fbcf20", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-ai-transformed-database-debugging%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/j1qsFbkx9LfDMpM4sAzfp0OzzdhLO7QkiHv_BrcLXlw=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-ai-transformed-database-debugging%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/j1qsFbkx9LfDMpM4sAzfp0OzzdhLO7QkiHv_BrcLXlw=439", "authors": ["TLDR Newsletter"], "title": "How AI Transformed Database Debugging at Databricks", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.bytebytego.com%2Fp%2Fhow-ai-transformed-database-debugging%3Futm_source=tldrdata/1/0100019b9d4b496f-fdeb83d3-38f5-48bc-a814-00d8c417ea32-000000/j1qsFbkx9LfDMpM4sAzfp0OzzdhLO7QkiHv_BrcLXlw=439", "summary": "How AI Transformed Database Debugging at Databricks (8 minute read) Databricks developed an internal AI-powered agentic platform that transforms database debugging for thousands of OLTP instances by enabling conversational, natural-language interactions with unified access to metrics, logs, and expert knowledge. This evolution from rigid checklists and anomaly detection to an interactive multi-agent system has reduced debugging time by up to 90%, allowing engineers to resolve complex issues m...", "source": "tldr", "AI": {"tldr": "Databricks\u5f00\u53d1\u4e86AI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u5e73\u53f0\uff0c\u5c06\u6570\u636e\u5e93\u8c03\u8bd5\u4ece\u521a\u6027\u68c0\u67e5\u6e05\u5355\u8f6c\u53d8\u4e3a\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u8c03\u8bd5\u65f6\u95f4\u51cf\u5c1190%", "motivation": "\u4f20\u7edf\u6570\u636e\u5e93\u8c03\u8bd5\u4f9d\u8d56\u521a\u6027\u68c0\u67e5\u6e05\u5355\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u4ea4\u4e92\u5f0f\u89e3\u51b3\u65b9\u6848", "method": "\u6784\u5efa\u5185\u90e8AI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u5e73\u53f0\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u4ea4\u4e92\uff0c\u7edf\u4e00\u8bbf\u95ee\u6307\u6807\u3001\u65e5\u5fd7\u548c\u4e13\u5bb6\u77e5\u8bc6", "result": "\u8c03\u8bd5\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe90%\uff0c\u5de5\u7a0b\u5e08\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u89e3\u51b3\u590d\u6742\u6570\u636e\u5e93\u95ee\u9898", "conclusion": "AI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u5e73\u53f0\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5e93\u8c03\u8bd5\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u4ece\u4f20\u7edf\u65b9\u6cd5\u5230\u667a\u80fd\u4ea4\u4e92\u7684\u8f6c\u53d8", "topic": "code agent"}}
{"id": "tldr.2601.5f8c7ef5", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/-AJIaQrDNUTBrhzJshZo17idMk1KT6rKk0nsrBOLqEU=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/-AJIaQrDNUTBrhzJshZo17idMk1KT6rKk0nsrBOLqEU=439", "authors": ["TLDR Newsletter"], "title": "AI writes code faster. Your job is still to prove it works", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/-AJIaQrDNUTBrhzJshZo17idMk1KT6rKk0nsrBOLqEU=439", "summary": "AI writes code faster. Your job is still to prove it works (12 minute read) Include evidence that your code works along with your pull requests to ensure you are not moving work downstream.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u80fd\u66f4\u5feb\u5730\u7f16\u5199\u4ee3\u7801\uff0c\u4f46\u5f00\u53d1\u8005\u4ecd\u9700\u8d1f\u8d23\u9a8c\u8bc1\u4ee3\u7801\u7684\u6b63\u786e\u6027\uff0c\u5efa\u8bae\u5728\u63d0\u4ea4\u4ee3\u7801\u65f6\u9644\u5e26\u6d4b\u8bd5\u8bc1\u636e", "motivation": "\u968f\u7740AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u666e\u53ca\uff0c\u5f00\u53d1\u901f\u5ea6\u5927\u5e45\u63d0\u5347\uff0c\u4f46\u4ee3\u7801\u8d28\u91cf\u9a8c\u8bc1\u7684\u8d23\u4efb\u4ecd\u7136\u843d\u5728\u5f00\u53d1\u8005\u8eab\u4e0a\uff0c\u9700\u8981\u786e\u4fddAI\u751f\u6210\u7684\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u5de5\u4f5c", "method": "\u5efa\u8bae\u5728\u63d0\u4ea4pull request\u65f6\u5305\u542b\u4ee3\u7801\u5de5\u4f5c\u7684\u8bc1\u636e\uff0c\u5982\u6d4b\u8bd5\u7528\u4f8b\u3001\u9a8c\u8bc1\u7ed3\u679c\u7b49\uff0c\u907f\u514d\u5c06\u9a8c\u8bc1\u5de5\u4f5c\u8f6c\u79fb\u5230\u4e0b\u6e38\u56e2\u961f", "result": "\u901a\u8fc7\u9644\u5e26\u9a8c\u8bc1\u8bc1\u636e\uff0c\u53ef\u4ee5\u51cf\u5c11\u4ee3\u7801\u5ba1\u67e5\u65f6\u95f4\uff0c\u63d0\u9ad8\u4ee3\u7801\u8d28\u91cf\uff0c\u786e\u4fddAI\u751f\u6210\u7684\u4ee3\u7801\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u53ef\u9760\u8fd0\u884c", "conclusion": "AI\u5de5\u5177\u63d0\u5347\u4e86\u7f16\u7801\u6548\u7387\uff0c\u4f46\u5f00\u53d1\u8005\u4ecd\u9700\u627f\u62c5\u4ee3\u7801\u9a8c\u8bc1\u7684\u8d23\u4efb\uff0c\u901a\u8fc7\u63d0\u4f9b\u6d4b\u8bd5\u8bc1\u636e\u6765\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf", "topic": "swe application"}}
{"id": "tldr.2601.29fe0b73", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fclaude-code-and-what-comes-next%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/zIA-9kjj0fIcQdoGk5vqRFrlsDX5Jc6igh7A6E0P_Ws=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fclaude-code-and-what-comes-next%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/zIA-9kjj0fIcQdoGk5vqRFrlsDX5Jc6igh7A6E0P_Ws=439", "authors": ["TLDR Newsletter"], "title": "Claude Code and What Comes Next", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.oneusefulthing.org%2Fp%2Fclaude-code-and-what-comes-next%3Futm_source=tldrnewsletter/1/0100019b9d5a9e43-9c61a285-189b-471a-98e0-8e84f66d8626-000000/zIA-9kjj0fIcQdoGk5vqRFrlsDX5Jc6igh7A6E0P_Ws=439", "summary": "Claude Code and What Comes Next (15 minute read) AIs are now capable of real, sustained work that actually matters, and this is starting to change how developers approach tasks.", "source": "tldr", "AI": {"tldr": "AI\u4ee3\u7801\u52a9\u624bClaude Code\u5df2\u80fd\u5b8c\u6210\u771f\u6b63\u6709\u610f\u4e49\u7684\u6301\u7eed\u5de5\u4f5c\uff0c\u5f00\u59cb\u6539\u53d8\u5f00\u53d1\u8005\u5904\u7406\u4efb\u52a1\u7684\u65b9\u5f0f", "motivation": "AI\u4ee3\u7801\u80fd\u529b\u5df2\u53d1\u5c55\u5230\u80fd\u591f\u8fdb\u884c\u771f\u5b9e\u3001\u6301\u7eed\u4e14\u6709\u5b9e\u9645\u4ef7\u503c\u7684\u5de5\u4f5c\uff0c\u8fd9\u6b63\u5728\u6539\u53d8\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u65b9\u5f0f", "method": "\u6587\u7ae0\u4e3b\u8981\u8ba8\u8bbaClaude Code AI\u4ee3\u7801\u52a9\u624b\u7684\u53d1\u5c55\u73b0\u72b6\u548c\u5f71\u54cd\uff0c\u5c5e\u4e8e\u6280\u672f\u5206\u6790\u548c\u8d8b\u52bf\u89c2\u5bdf", "result": "AI\u4ee3\u7801\u52a9\u624b\u5df2\u5177\u5907\u5b9e\u9645\u5de5\u4f5c\u80fd\u529b\uff0c\u5f00\u59cb\u5f71\u54cd\u5f00\u53d1\u8005\u7684\u4efb\u52a1\u5904\u7406\u65b9\u5f0f\u548c\u5de5\u4f5c\u6d41\u7a0b", "conclusion": "AI\u4ee3\u7801\u80fd\u529b\u7684\u8fdb\u6b65\u6b63\u5728\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\uff0c\u5f00\u53d1\u8005\u9700\u8981\u9002\u5e94\u8fd9\u79cd\u65b0\u7684\u5de5\u4f5c\u8303\u5f0f", "topic": "code agent"}}
{"id": "tldr.2601.76401da8", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsunilpai.dev%2Fposts%2Fcontext-is-the-work%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/BFAFNUhCUZOk-VfLBiGbBCsQhnqsTdeT5q_WOX-GgEk=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsunilpai.dev%2Fposts%2Fcontext-is-the-work%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/BFAFNUhCUZOk-VfLBiGbBCsQhnqsTdeT5q_WOX-GgEk=439", "authors": ["TLDR Newsletter"], "title": "The context is the work", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 13 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsunilpai.dev%2Fposts%2Fcontext-is-the-work%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/BFAFNUhCUZOk-VfLBiGbBCsQhnqsTdeT5q_WOX-GgEk=439", "summary": "The context is the work (what the day-to-day looks like now) (13 minute read) Coding agents have made code generation inexpensive, shifting the core challenge in engineering from writing code to properly defining intent, constraints, and trade-offs. As a result, PR descriptions have become even more important, as they need to communicate this context. A good PR description includes executive intent, reviewer guidance, and detailed provenance to effectively convey information to various audien...", "source": "tldr", "AI": {"tldr": "PR\u63cf\u8ff0\u5728AI\u4ee3\u7801\u751f\u6210\u65f6\u4ee3\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4f20\u8fbe\u610f\u56fe\u3001\u7ea6\u675f\u548c\u6743\u8861\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\u53d8\u66f4", "motivation": "\u968f\u7740\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u4f7f\u7f16\u5199\u4ee3\u7801\u53d8\u5f97\u5ec9\u4ef7\uff0c\u5de5\u7a0b\u6838\u5fc3\u6311\u6218\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u5411\u6b63\u786e\u5b9a\u4e49\u610f\u56fe\u3001\u7ea6\u675f\u548c\u6743\u8861\uff0c\u56e0\u6b64PR\u63cf\u8ff0\u9700\u8981\u4f20\u8fbe\u8fd9\u4e9b\u5173\u952e\u4e0a\u4e0b\u6587\u4fe1\u606f", "method": "\u63d0\u51faPR\u63cf\u8ff0\u5e94\u5305\u542b\u6267\u884c\u610f\u56fe\u3001\u5ba1\u67e5\u8005\u6307\u5bfc\u548c\u8be6\u7ec6\u6765\u6e90\u4fe1\u606f\uff0c\u4ee5\u6709\u6548\u5411\u4e0d\u540c\u53d7\u4f17\u4f20\u8fbe\u4fe1\u606f", "result": "PR\u63cf\u8ff0\u5728AI\u8f85\u52a9\u7f16\u7a0b\u73af\u5883\u4e2d\u6210\u4e3a\u5173\u952e\u6c9f\u901a\u5de5\u5177\uff0c\u9700\u8981\u7ed3\u6784\u5316\u5730\u4f20\u8fbe\u6280\u672f\u51b3\u7b56\u80cc\u540e\u7684\u539f\u56e0\u548c\u4e0a\u4e0b\u6587", "conclusion": "\u5728\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u666e\u53ca\u7684\u65f6\u4ee3\uff0c\u9ad8\u8d28\u91cf\u7684PR\u63cf\u8ff0\u5bf9\u4e8e\u6709\u6548\u5de5\u7a0b\u534f\u4f5c\u548c\u77e5\u8bc6\u4f20\u9012\u53d8\u5f97\u6bd4\u4ee5\u5f80\u4efb\u4f55\u65f6\u5019\u90fd\u66f4\u52a0\u91cd\u8981", "topic": "code agent"}}
{"id": "tldr.2601.42ae7f68", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/cdJOfH08HH5cX7B_tU3rLCJekuCiS_F8sFnJOCPGI6Q=439", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/cdJOfH08HH5cX7B_tU3rLCJekuCiS_F8sFnJOCPGI6Q=439", "authors": ["TLDR Newsletter"], "title": "AI writes code faster. Your job is still to prove it works", "comment": "Source: TLDR Newsletter, Date: 2026-01-08, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyosmani.com%2Fblog%2Fcode-review-ai%2F%3Futm_source=tldrdev/1/0100019b9d82a3b2-2018917d-5b22-47c4-8605-2f4d51a2000d-000000/cdJOfH08HH5cX7B_tU3rLCJekuCiS_F8sFnJOCPGI6Q=439", "summary": "AI writes code faster. Your job is still to prove it works (10 minute read) AI makes code generation faster, but it shifts the bottleneck from writing to proving its functionality, making human verification and accountability needed for good code review.", "source": "tldr", "AI": {"tldr": "AI\u52a0\u901f\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u5c06\u74f6\u9888\u4ece\u7f16\u5199\u8f6c\u79fb\u5230\u9a8c\u8bc1\uff0c\u9700\u8981\u4eba\u5de5\u5ba1\u67e5\u6765\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf", "motivation": "AI\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u901f\u5ea6\uff0c\u4f46\u4ee3\u7801\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027\u7684\u9a8c\u8bc1\u6210\u4e3a\u65b0\u7684\u6311\u6218\uff0c\u9700\u8981\u4eba\u7c7b\u5ba1\u67e5\u6765\u4fdd\u8bc1\u4ee3\u7801\u8d28\u91cf", "method": "\u5206\u6790AI\u4ee3\u7801\u751f\u6210\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u4ece\u7f16\u5199\u5230\u9a8c\u8bc1\u7684\u74f6\u9888\u8f6c\u79fb\uff0c\u5f3a\u8c03\u4eba\u5de5\u5ba1\u67e5\u548c\u9a8c\u8bc1\u7684\u91cd\u8981\u6027", "result": "AI\u867d\u7136\u52a0\u901f\u4ee3\u7801\u7f16\u5199\uff0c\u4f46\u589e\u52a0\u4e86\u9a8c\u8bc1\u8d1f\u62c5\uff0c\u9700\u8981\u5efa\u7acb\u6709\u6548\u7684\u4eba\u5de5\u5ba1\u67e5\u673a\u5236\u6765\u786e\u4fdd\u4ee3\u7801\u529f\u80fd\u6b63\u786e\u6027", "conclusion": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u6539\u53d8\u4e86\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\uff0c\u5c06\u91cd\u70b9\u4ece\u7f16\u5199\u8f6c\u79fb\u5230\u9a8c\u8bc1\uff0c\u4eba\u7c7b\u5728\u4ee3\u7801\u5ba1\u67e5\u548c\u9a8c\u8bc1\u4e2d\u7684\u89d2\u8272\u53d8\u5f97\u66f4\u52a0\u5173\u952e", "topic": "swe application"}}
