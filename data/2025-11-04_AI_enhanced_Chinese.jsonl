{"id": "2511.00087", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00087", "abs": "https://arxiv.org/abs/2511.00087", "authors": ["Anshu Dubey", "Akash Dhruv"], "title": "Adding New Capability in Existing Scientific Application with LLM Assistance", "comment": "8 pages, 4 figures, submitted to The 1st International Workshop on\n  Foundational large Language Models Advances for HPC in Asia", "summary": "With the emergence and rapid evolution of large language models (LLM),\nautomating coding tasks has become an im- portant research topic. Many efforts\nare underway and liter- ature abounds about the efficacy of models and their\nability to generate code. A less explored aspect of code generation is for new\nalgorithms, where the training data-set would not have included any previous\nexample of similar code. In this paper we propose a new methodology for writing\ncode from scratch for a new algorithm using LLM assistance, and describe\nenhancement of a previously developed code- translation tool, Code-Scribe, for\nnew code generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528LLM\u8f85\u52a9\u4ece\u96f6\u5f00\u59cb\u4e3a\u65b0\u7b97\u6cd5\u7f16\u5199\u4ee3\u7801\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u7684\u4ee3\u7801\u7ffb\u8bd1\u5de5\u5177Code-Scribe\u7528\u4e8e\u65b0\u4ee3\u7801\u751f\u6210", "motivation": "\u5f53\u524d\u4ee3\u7801\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5df2\u6709\u4ee3\u7801\u7684\u751f\u6210\uff0c\u5bf9\u4e8e\u5168\u65b0\u7b97\u6cd5\uff08\u8bad\u7ec3\u6570\u636e\u4e2d\u65e0\u7c7b\u4f3c\u4ee3\u7801\u793a\u4f8b\uff09\u7684\u4ee3\u7801\u751f\u6210\u7814\u7a76\u8f83\u5c11", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u589e\u5f3a\u4e86\u73b0\u6709\u7684Code-Scribe\u4ee3\u7801\u7ffb\u8bd1\u5de5\u5177\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u65b0\u7b97\u6cd5\u7684\u4ee3\u7801\u751f\u6210\u4efb\u52a1", "result": "\u8bba\u6587\u63cf\u8ff0\u4e86\u6539\u8fdb\u540e\u7684\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u4f46\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u6570\u636e", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u7684\u65b0\u7b97\u6cd5\u4ee3\u7801\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2511.00125", "categories": ["cs.SE", "cs.AI", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00125", "abs": "https://arxiv.org/abs/2511.00125", "authors": ["\u00c1lvaro Silva", "Alexandra Mendes", "Ruben Martins"], "title": "Inferring multiple helper Dafny assertions with LLMs", "comment": null, "summary": "The Dafny verifier provides strong correctness guarantees but often requires\nnumerous manual helper assertions, creating a significant barrier to adoption.\nWe investigate the use of Large Language Models (LLMs) to automatically infer\nmissing helper assertions in Dafny programs, with a primary focus on cases\ninvolving multiple missing assertions. To support this study, we extend the\nDafnyBench benchmark with curated datasets where one, two, or all assertions\nare removed, and we introduce a taxonomy of assertion types to analyze\ninference difficulty. Our approach refines fault localization through a hybrid\nmethod that combines LLM predictions with error-message heuristics. We\nimplement this approach in a new tool called DAISY (Dafny Assertion Inference\nSYstem). While our focus is on multiple missing assertions, we also evaluate\nDAISY on single-assertion cases. DAISY verifies 63.4% of programs with one\nmissing assertion and 31.7% with multiple missing assertions. Notably, many\nprograms can be verified with fewer assertions than originally present,\nhighlighting that proofs often admit multiple valid repair strategies and that\nrecovering every original assertion is unnecessary. These results demonstrate\nthat automated assertion inference can substantially reduce proof engineering\neffort and represent a step toward more scalable and accessible formal\nverification.", "AI": {"tldr": "\u4f7f\u7528LLM\u81ea\u52a8\u63a8\u65adDafny\u7a0b\u5e8f\u4e2d\u7f3a\u5931\u7684\u8f85\u52a9\u65ad\u8a00\uff0c\u5f00\u53d1DAISY\u5de5\u5177\uff0c\u5728\u5355\u65ad\u8a00\u7f3a\u5931\u60c5\u51b5\u4e0b\u9a8c\u8bc163.4%\u7a0b\u5e8f\uff0c\u591a\u65ad\u8a00\u7f3a\u5931\u60c5\u51b5\u4e0b\u9a8c\u8bc131.7%\u7a0b\u5e8f\u3002", "motivation": "Dafny\u9a8c\u8bc1\u5668\u9700\u8981\u5927\u91cf\u624b\u52a8\u8f85\u52a9\u65ad\u8a00\uff0c\u6210\u4e3a\u91c7\u7528\u969c\u788d\uff0c\u7814\u7a76\u5982\u4f55\u81ea\u52a8\u63a8\u65ad\u7f3a\u5931\u65ad\u8a00\u4ee5\u51cf\u5c11\u8bc1\u660e\u5de5\u7a0b\u5de5\u4f5c\u91cf\u3002", "method": "\u6269\u5c55DafnyBench\u57fa\u51c6\uff0c\u521b\u5efa\u7f3a\u5931\u65ad\u8a00\u6570\u636e\u96c6\uff0c\u7ed3\u5408LLM\u9884\u6d4b\u548c\u9519\u8bef\u6d88\u606f\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u6df7\u5408\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u5f00\u53d1DAISY\u5de5\u5177\u3002", "result": "DAISY\u5728\u5355\u65ad\u8a00\u7f3a\u5931\u65f6\u9a8c\u8bc163.4%\u7a0b\u5e8f\uff0c\u591a\u65ad\u8a00\u7f3a\u5931\u65f6\u9a8c\u8bc131.7%\u7a0b\u5e8f\uff0c\u8bc1\u660e\u65e0\u9700\u6062\u590d\u6240\u6709\u539f\u59cb\u65ad\u8a00\u5373\u53ef\u5b8c\u6210\u9a8c\u8bc1\u3002", "conclusion": "\u81ea\u52a8\u65ad\u8a00\u63a8\u65ad\u80fd\u663e\u8457\u51cf\u5c11\u8bc1\u660e\u5de5\u7a0b\u5de5\u4f5c\u91cf\uff0c\u662f\u8fc8\u5411\u66f4\u53ef\u6269\u5c55\u548c\u53ef\u8bbf\u95ee\u7684\u5f62\u5f0f\u9a8c\u8bc1\u7684\u91cd\u8981\u4e00\u6b65\u3002", "topic": "code agent"}}
{"id": "2511.00160", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00160", "abs": "https://arxiv.org/abs/2511.00160", "authors": ["Katherine A. Rosenfeld", "Cliff C. Kerr", "Jessica Lundin"], "title": "What a diff makes: automating code migration with large language models", "comment": "10 pages, 8 figures", "summary": "Modern software programs are built on stacks that are often undergoing\nchanges that introduce updates and improvements, but may also break any project\nthat depends upon them. In this paper we explore the use of Large Language\nModels (LLMs) for code migration, specifically the problem of maintaining\ncompatibility with a dependency as it undergoes major and minor semantic\nversion changes. We demonstrate, using metrics such as test coverage and change\ncomparisons, that contexts containing diffs can significantly improve\nperformance against out of the box LLMs and, in some cases, perform better than\nusing code. We provide a dataset to assist in further development of this\nproblem area, as well as an open-source Python package, AIMigrate, that can be\nused to assist with migrating code bases. In a real-world migration of\nTYPHOIDSIM between STARSIM versions, AIMigrate correctly identified 65% of\nrequired changes in a single run, increasing to 80% with multiple runs, with\n47% of changes generated perfectly.", "AI": {"tldr": "\u4f7f\u7528\u5305\u542b\u5dee\u5f02\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u9879\u53d1\u751f\u8bed\u4e49\u7248\u672c\u53d8\u66f4\u65f6\u4fdd\u6301\u517c\u5bb9\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7a0b\u5e8f\u4f9d\u8d56\u7684\u5806\u6808\u7ecf\u5e38\u66f4\u65b0\uff0c\u8fd9\u4e9b\u53d8\u66f4\u53ef\u80fd\u4f1a\u7834\u574f\u4f9d\u8d56\u9879\u76ee\u3002\u9700\u8981\u89e3\u51b3\u5982\u4f55\u5229\u7528LLM\u5e2e\u52a9\u4ee3\u7801\u8fc1\u79fb\uff0c\u7279\u522b\u662f\u5728\u4f9d\u8d56\u9879\u53d1\u751f\u91cd\u5927\u548c\u6b21\u8981\u8bed\u4e49\u7248\u672c\u53d8\u66f4\u65f6\u4fdd\u6301\u517c\u5bb9\u6027\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5305\u542b\u5dee\u5f02\u4fe1\u606f(diffs)\u7684\u4e0a\u4e0b\u6587\u6765\u63d0\u5347LLM\u6027\u80fd\uff0c\u901a\u8fc7\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u53d8\u66f4\u6bd4\u8f83\u7b49\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002\u5f00\u53d1\u4e86AIMigrate\u5f00\u6e90Python\u5305\u6765\u534f\u52a9\u4ee3\u7801\u5e93\u8fc1\u79fb\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684TYPHOIDSIM\u4eceSTARSIM\u7248\u672c\u8fc1\u79fb\u4e2d\uff0cAIMigrate\u5355\u6b21\u8fd0\u884c\u6b63\u786e\u8bc6\u522b\u4e8665%\u7684\u5fc5\u8981\u53d8\u66f4\uff0c\u591a\u6b21\u8fd0\u884c\u63d0\u5347\u81f380%\uff0c\u5176\u4e2d47%\u7684\u53d8\u66f4\u751f\u6210\u5b8c\u7f8e\u3002", "conclusion": "\u5305\u542b\u5dee\u5f02\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u53ef\u4ee5\u663e\u8457\u6539\u5584LLM\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u4f18\u4e8e\u4f7f\u7528\u7eaf\u4ee3\u7801\u7684\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2511.00010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "PlotCraft\u662f\u4e00\u4e2a\u5305\u542b1000\u4e2a\u6311\u6218\u6027\u53ef\u89c6\u5316\u4efb\u52a1\u7684\u65b0\u57fa\u51c6\uff0c\u6db5\u76d6\u91d1\u878d\u3001\u79d1\u5b66\u7814\u7a76\u548c\u793e\u4f1a\u5b66\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u89c4\u6a21\u548c\u7ed3\u6784\u5316\u6570\u636e\u7684\u590d\u6742\u53ef\u89c6\u5316\u4efb\u52a1\u65b9\u9762\u80fd\u529b\u5c1a\u672a\u5145\u5206\u8bc4\u4f30\u548c\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86PlotCraft\u57fa\u51c6\uff0c\u5305\u542b7\u4e2a\u9ad8\u7ea7\u53ef\u89c6\u5316\u4efb\u52a1\u548c48\u79cd\u56fe\u8868\u7c7b\u578b\uff1b\u6784\u5efa\u4e86SynthVis-30K\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b\u5f00\u53d1\u4e86PlotCraftor\u4ee3\u7801\u751f\u6210\u6a21\u578b\u3002", "result": "\u5bf923\u4e2a\u9886\u5148LLM\u7684\u8bc4\u4f30\u663e\u793a\u5728\u590d\u6742\u53ef\u89c6\u5316\u4efb\u52a1\u4e0a\u5b58\u5728\u660e\u663e\u6027\u80fd\u7f3a\u9677\uff1bPlotCraftor\u6a21\u578b\u5728VisEval\u3001PandasPlotBench\u548cPlotCraft\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0e\u9886\u5148\u4e13\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u6027\u80fd\u63d0\u5347\u8d85\u8fc750%\u3002", "conclusion": "PlotCraft\u57fa\u51c6\u63ed\u793a\u4e86LLM\u5728\u590d\u6742\u53ef\u89c6\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0cPlotCraftor\u6a21\u578b\u901a\u8fc7\u4e13\u95e8\u8bad\u7ec3\u5728\u5c0f\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u80fd\u529b\u3002", "topic": "swe benchmark"}}
{"id": "2511.00197", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00197", "abs": "https://arxiv.org/abs/2511.00197", "authors": ["Oorja Majgaonkar", "Zhiwei Fei", "Xiang Li", "Federica Sarro", "He Ye"], "title": "Understanding Code Agent Behaviour: An Empirical Study of Success and Failure Trajectories", "comment": null, "summary": "The increasing deployment of Large Language Model (LLM) agents for complex\nsoftware engineering tasks has created a need to understand their\nproblem-solving behaviours beyond simple success metrics. While these agents\ndemonstrate impressive capabilities in automated issue resolution, their\ndecision-making processes remain largely opaque. This paper presents an\nempirical study of agent trajectories, namely the execution traces capturing\nthe steps agents take when attempting to resolve software issues. We analyse\ntrajectories from three state-of-the-art code agents (OpenHands, SWE-agent, and\nPrometheus) on the SWE-Bench benchmark, examining both successful and failed\nattempts. Our investigation reveals several key insights into agent behaviour.\nFirst, we identify how distinct problem-solving strategies, such as defensive\nprogramming and context gathering, enable success in different scenarios.\nSecond, we find that failed trajectories are consistently longer and exhibit\nhigher variance than successful ones, with failure patterns differing\nsignificantly between agents. Third, our fault localisation analysis shows that\nwhile most trajectories correctly identify problematic files (72-81\\% even in\nfailures), success depends more on achieving approximate rather than exact code\nmodifications. These and other findings unveiled by our study, provide a\nfoundation for understanding agent behaviour through trajectory analysis,\ncontributing to the development of more robust and interpretable autonomous\nsoftware engineering systems.", "AI": {"tldr": "\u5bf9\u4e09\u79cd\u5148\u8fdb\u4ee3\u7801\u4ee3\u7406\u5728SWE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6267\u884c\u8f68\u8ff9\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u6210\u529f\u548c\u5931\u8d25\u8f68\u8ff9\u7684\u7279\u5f81\u5dee\u5f02\uff0c\u63ed\u793a\u4ee3\u7406\u51b3\u7b56\u6a21\u5f0f\u548c\u6545\u969c\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u6210\u529f\u6307\u6807\u6765\u7406\u89e3\u5176\u95ee\u9898\u89e3\u51b3\u884c\u4e3a\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u51b3\u7b56\u8fc7\u7a0b\u4ecd\u7136\u4e0d\u900f\u660e\u3002", "method": "\u5206\u6790OpenHands\u3001SWE-agent\u548cPrometheus\u4e09\u79cd\u4ee3\u7801\u4ee3\u7406\u5728SWE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6267\u884c\u8f68\u8ff9\uff0c\u5305\u62ec\u6210\u529f\u548c\u5931\u8d25\u7684\u5c1d\u8bd5\uff0c\u8003\u5bdf\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3001\u8f68\u8ff9\u957f\u5ea6\u548c\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u6709\u6548\uff1b\u5931\u8d25\u8f68\u8ff9\u66f4\u957f\u4e14\u65b9\u5dee\u66f4\u9ad8\uff1b\u5927\u591a\u6570\u8f68\u8ff9\u80fd\u6b63\u786e\u8bc6\u522b\u95ee\u9898\u6587\u4ef6(72-81%)\uff0c\u4f46\u6210\u529f\u66f4\u4f9d\u8d56\u4e8e\u5b9e\u73b0\u8fd1\u4f3c\u800c\u975e\u7cbe\u786e\u7684\u4ee3\u7801\u4fee\u6539\u3002", "conclusion": "\u901a\u8fc7\u8f68\u8ff9\u5206\u6790\u4e3a\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u5065\u58ee\u548c\u53ef\u89e3\u91ca\u7684\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2511.00029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00029", "abs": "https://arxiv.org/abs/2511.00029", "authors": ["Samaksh Bhargav", "Zining Zhu"], "title": "Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts", "comment": "12 pages, 6 figures", "summary": "Large Language Model (LLM) deployment requires guiding the LLM to recognize\nand not answer unsafe prompts while complying with safe prompts. Previous\nmethods for achieving this require adjusting model weights along with other\nexpensive procedures. While recent advances in Sparse Autoencoders (SAEs) have\nenabled interpretable feature extraction from LLMs, existing approaches lack\nsystematic feature selection methods and principled evaluation of\nsafety-utility tradeoffs. We explored using different steering features and\nsteering strengths using Sparse Auto Encoders (SAEs) to provide a solution.\nUsing an accurate and innovative contrasting prompt method with the\nAI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air\nBench eu-dataset to efficiently choose the best features in the model to steer,\nwe tested this method on Llama-3 8B. We conclude that using this method, our\napproach achieves an 18.9% improvement in safety performance while\nsimultaneously increasing utility by 11.1%, demonstrating that targeted SAE\nsteering can overcome traditional safety-utility tradeoffs when optimal\nfeatures are identified through principled selection methods.", "AI": {"tldr": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u548c\u5b9a\u5411\u5f15\u5bfc\uff0c\u5728Llama-3 8B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u80fd\u63d0\u534718.9%\u540c\u65f6\u5b9e\u7528\u6027\u63d0\u534711.1%\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5b89\u5168-\u5b9e\u7528\u6027\u6743\u8861\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u90e8\u7f72\u65b9\u6cd5\u9700\u8981\u8c03\u6574\u6a21\u578b\u6743\u91cd\u4e14\u8fc7\u7a0b\u6602\u8d35\uff0c\u800c\u5f53\u524dSAE\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7279\u5f81\u9009\u62e9\u673a\u5236\u548c\u5b89\u5168-\u5b9e\u7528\u6027\u6743\u8861\u7684\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u63d0\u793a\u65b9\u6cd5\u548cAI\u751f\u6210\u63d0\u793a\u6570\u636e\u96c6\uff0c\u901a\u8fc7SAE\u9009\u62e9\u6700\u4f73\u5f15\u5bfc\u7279\u5f81\uff0c\u5728\u4e0d\u540c\u5f15\u5bfc\u5f3a\u5ea6\u4e0b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728Llama-3 8B\u6a21\u578b\u4e0a\uff0c\u5b89\u5168\u6027\u80fd\u63d0\u534718.9%\uff0c\u5b9e\u7528\u6027\u540c\u65f6\u63d0\u534711.1%\u3002", "conclusion": "\u901a\u8fc7\u539f\u5219\u6027\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u8bc6\u522b\u6700\u4f18\u7279\u5f81\uff0c\u5b9a\u5411SAE\u5f15\u5bfc\u53ef\u4ee5\u514b\u670d\u4f20\u7edf\u5b89\u5168-\u5b9e\u7528\u6027\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2511.00202", "categories": ["cs.SE", "cs.LG", "cs.LO", "F.3.1; I.2.5"], "pdf": "https://arxiv.org/pdf/2511.00202", "abs": "https://arxiv.org/abs/2511.00202", "authors": ["Jacqueline Mitchell", "Yasser Shaaban"], "title": "Position: Vibe Coding Needs Vibe Reasoning: Improving Vibe Coding with Formal Verification", "comment": "7 pages, 3 figures, In Proceedings of the 1st ACM SIGPLAN\n  International Workshop on Language Models and Programming Languages\n  (LMPL'25), October 12-18, 2025, Singapore, Singapore. ACM, New York, NY, USA", "summary": "``Vibe coding'' -- the practice of developing software through iteratively\nconversing with a large language model (LLM) -- has exploded in popularity\nwithin the last year. However, developers report key limitations including the\naccumulation of technical debt, security issues, and code churn to achieve\nsatisfactory results. We argue that these pitfalls result from LLMs' inability\nto reconcile accumulating human-imposed constraints during vibe coding, with\ndevelopers inadvertently failing to resolve contradictions because LLMs\nprioritize user commands over code consistency. Given LLMs' receptiveness to\nverification-based feedback, we argue that formal methods can mitigate these\npitfalls, making vibe coding more reliable. However, we posit that integrating\nformal methods must transcend existing approaches that combine formal methods\nand LLMs. We advocate for a side-car system throughout the vibe coding process\nwhich: (1) \\emph{Autoformalizes} specifications (2) Validates against targets,\n(3) Delivers \\emph{actionable} feedback to the LLM, and (4) Allows intuitive\ndeveloper influence on specifications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5\u89e3\u51b3\"\u6c1b\u56f4\u7f16\u7801\"\uff08\u4e0eLLM\u8fed\u4ee3\u5bf9\u8bdd\u5f00\u53d1\u8f6f\u4ef6\uff09\u4e2d\u7684\u6280\u672f\u503a\u52a1\u3001\u5b89\u5168\u95ee\u9898\u548c\u4ee3\u7801\u6df7\u4e71\u95ee\u9898\uff0c\u5efa\u8bae\u4f7f\u7528\u65c1\u8def\u7cfb\u7edf\u81ea\u52a8\u5f62\u5f0f\u5316\u89c4\u8303\u3001\u9a8c\u8bc1\u76ee\u6807\u3001\u63d0\u4f9b\u53ef\u64cd\u4f5c\u53cd\u9988\u5e76\u5141\u8bb8\u5f00\u53d1\u8005\u76f4\u89c2\u5f71\u54cd\u89c4\u8303\u3002", "motivation": "\u6c1b\u56f4\u7f16\u7801\u867d\u7136\u6d41\u884c\uff0c\u4f46\u5b58\u5728\u6280\u672f\u503a\u52a1\u79ef\u7d2f\u3001\u5b89\u5168\u95ee\u9898\u548c\u4ee3\u7801\u6df7\u4e71\u7b49\u9650\u5236\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8eLLM\u65e0\u6cd5\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u534f\u8c03\u4eba\u7c7b\u65bd\u52a0\u7684\u7ea6\u675f\uff0c\u4f18\u5148\u8003\u8651\u7528\u6237\u547d\u4ee4\u800c\u975e\u4ee3\u7801\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u5728\u6574\u4e2a\u6c1b\u56f4\u7f16\u7801\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u65c1\u8def\u7cfb\u7edf\uff0c\u5305\u62ec\uff1a(1)\u81ea\u52a8\u5f62\u5f0f\u5316\u89c4\u8303\uff0c(2)\u9488\u5bf9\u76ee\u6807\u8fdb\u884c\u9a8c\u8bc1\uff0c(3)\u5411LLM\u63d0\u4f9b\u53ef\u64cd\u4f5c\u53cd\u9988\uff0c(4)\u5141\u8bb8\u5f00\u53d1\u8005\u76f4\u89c2\u5f71\u54cd\u89c4\u8303\u3002", "result": "\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u6c1b\u56f4\u7f16\u7801\u7684\u7f3a\u9677\uff0c\u4f7f\u5176\u66f4\u52a0\u53ef\u9760\uff0c\u4f46\u9700\u8981\u8d85\u8d8a\u73b0\u6709\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0eLLM\u7ed3\u5408\u65b9\u6cd5\u3002", "conclusion": "\u5f62\u5f0f\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6c1b\u56f4\u7f16\u7801\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u4f46\u9700\u8981\u521b\u65b0\u7684\u96c6\u6210\u65b9\u5f0f\uff0c\u7279\u522b\u662f\u901a\u8fc7\u65c1\u8def\u7cfb\u7edf\u5728\u6574\u4e2a\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u6301\u7eed\u9a8c\u8bc1\u548c\u53cd\u9988\u3002", "topic": "code agent"}}
{"id": "2511.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00215", "abs": "https://arxiv.org/abs/2511.00215", "authors": ["Xiaomeng Xu", "Zahin Wahab", "Reid Holmes", "Caroline Lemieux"], "title": "DocPrism: Local Categorization and External Filtering to Identify Relevant Code-Documentation Inconsistencies", "comment": null, "summary": "Code-documentation inconsistencies are common and undesirable: they can lead\nto developer misunderstandings and software defects. This paper introduces\nDocPrism, a multi-language, code-documentation inconsistency detection tool.\nDocPrism uses a standard large language model (LLM) to analyze and explain\ninconsistencies. Plain use of LLMs for this task yield unacceptably high false\npositive rates: LLMs identify natural gaps between high-level documentation and\ndetailed code implementations as inconsistencies. We introduce and apply the\nLocal Categorization, External Filtering (LCEF) methodology to reduce false\npositives. LCEF relies on the LLM's local completion skills rather than its\nlong-term reasoning skills. In our ablation study, LCEF reduces DocPrism's\ninconsistency flag rate from 98% to 14%, and increases accuracy from 14% to\n94%. On a broad evaluation across Python, TypeScript, C++, and Java, DocPrism\nmaintains a low flag rate of 15%, and achieves a precision of 0.62 without\nperforming any fine-tuning.", "AI": {"tldr": "DocPrism\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u4ee3\u7801\u6587\u6863\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u5de5\u5177\uff0c\u4f7f\u7528LLM\u5206\u6790\u5e76\u89e3\u91ca\u4e0d\u4e00\u81f4\u6027\uff0c\u901a\u8fc7LCEF\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387", "motivation": "\u4ee3\u7801\u6587\u6863\u4e0d\u4e00\u81f4\u6027\u5f88\u5e38\u89c1\u4e14\u4e0d\u53ef\u53d6\uff0c\u53ef\u80fd\u5bfc\u81f4\u5f00\u53d1\u8005\u8bef\u89e3\u548c\u8f6f\u4ef6\u7f3a\u9677", "method": "\u4f7f\u7528\u6807\u51c6LLM\u5206\u6790\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165LCEF\uff08\u672c\u5730\u5206\u7c7b\u3001\u5916\u90e8\u8fc7\u6ee4\uff09\u65b9\u6cd5\u6765\u51cf\u5c11\u8bef\u62a5\uff0c\u4f9d\u8d56LLM\u7684\u672c\u5730\u8865\u5168\u6280\u80fd\u800c\u975e\u957f\u671f\u63a8\u7406\u6280\u80fd", "result": "LCEF\u5c06\u4e0d\u4e00\u81f4\u6027\u6807\u8bb0\u7387\u4ece98%\u964d\u81f314%\uff0c\u51c6\u786e\u7387\u4ece14%\u63d0\u5347\u81f394%\u3002\u5728Python\u3001TypeScript\u3001C++\u548cJava\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0cDocPrism\u4fdd\u630115%\u7684\u4f4e\u6807\u8bb0\u7387\u548c0.62\u7684\u7cbe\u786e\u5ea6", "conclusion": "DocPrism\u901a\u8fc7LCEF\u65b9\u6cd5\u6709\u6548\u68c0\u6d4b\u4ee3\u7801\u6587\u6863\u4e0d\u4e00\u81f4\u6027\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u4fdd\u6301\u826f\u597d\u6027\u80fd", "topic": "swe application"}}
{"id": "2511.00122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "Engineering.ai\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u67b6\u6784\u7684AI\u5de5\u7a0b\u5e08\u5e73\u53f0\uff0c\u901a\u8fc7\u9996\u5e2d\u5de5\u7a0b\u5e08\u534f\u8c03\u7a7a\u6c14\u52a8\u529b\u5b66\u3001\u7ed3\u6784\u3001\u58f0\u5b66\u548c\u4f18\u5316\u7b49\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u81ea\u4e3b\u7684\u8de8\u5b66\u79d1\u5de5\u7a0b\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u8bbe\u8ba1\u9700\u8981\u591a\u5b66\u79d1\u4e13\u5bb6\u56e2\u961f\u534f\u4f5c\uff0c\u8017\u8d39\u5927\u91cf\u65f6\u95f4\u548c\u6210\u672c\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u7684AI\u5de5\u7a0b\u5e08\u56e2\u961f\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u9996\u5e2d\u5de5\u7a0b\u5e08\u534f\u8c03\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6587\u4ef6\u901a\u4fe1\u5b9e\u73b0\u6570\u636e\u6eaf\u6e90\u548c\u53ef\u590d\u73b0\u6027\uff0c\u96c6\u6210\u591a\u79cd\u5de5\u7a0b\u8f6f\u4ef6\u5de5\u5177\u8fdb\u884c\u5e76\u884c\u591a\u5b66\u79d1\u4eff\u771f\u3002", "result": "\u5728\u65e0\u4eba\u673a\u673a\u7ffc\u4f18\u5316\u6848\u4f8b\u4e2d\uff0c\u7cfb\u7edf\u5728400\u591a\u4e2a\u53c2\u6570\u914d\u7f6e\u4e2d\u5b9e\u73b0\u4e86100%\u6210\u529f\u7387\uff0c\u65e0\u7f51\u683c\u751f\u6210\u5931\u8d25\u3001\u6c42\u89e3\u5668\u6536\u655b\u95ee\u9898\u6216\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "\u57fa\u4e8e\u667a\u80fd\u4f53\u7684AI\u5de5\u7a0b\u5e08\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.00162", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ARC-GEN\uff0c\u4e00\u4e2a\u5f00\u6e90\u7a0b\u5e8f\u751f\u6210\u5668\uff0c\u65e8\u5728\u6269\u5c55ARC-AGI\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u539f\u59cb\u6570\u636e\u96c6\u6837\u672c\u6570\u91cf\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6280\u80fd\u83b7\u53d6\u6548\u7387\uff0c\u4f46\u6bcf\u4e2a\u4efb\u52a1\u7684\u6f14\u793a\u6837\u672c\u6570\u91cf\u6709\u9650\uff0c\u9650\u5236\u4e86\u9700\u8981\u5927\u91cf\u6837\u672c\u7684\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1ARC-GEN\u7a0b\u5e8f\u751f\u6210\u5668\uff0c\u5168\u9762\u8986\u76d6400\u4e2a\u4efb\u52a1\uff0c\u5e76\u5c3d\u53ef\u80fd\u5fe0\u5b9e\u4e8e\u539f\u59cbARC-AGI-1\u53d1\u5e03\u7684\u5206\u5e03\u7279\u6027\u548c\u7279\u5f81\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u66f4\u51c6\u786e\u5730\u6a21\u62df\u539f\u59cbARC-AGI\u7684\u7279\u6027\u5206\u5e03\u3002", "conclusion": "ARC-GEN\u4e3aARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u53ef\u7528\u4e8e\u9a8c\u8bc12025\u5e74Google Code Golf Championship\u63d0\u4ea4\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2511.00450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00450", "abs": "https://arxiv.org/abs/2511.00450", "authors": ["Vahid Etemadi", "Gregorio Robles"], "title": "SmartDoc: A Context-Aware Agentic Method Comment Generation Plugin", "comment": "6 pages, Already submitted to The 3rd International Workshop on\n  Integrated Development Environments (the IDE Workshop)", "summary": "Context: The software maintenance phase involves many activities such as code\nrefactoring, bug fixing, code review or testing. Program comprehension is key\nto all these activities, as it demands developers to grasp the knowledge (e.g.,\nimplementation details) required to modify the codebase. Methods as main\nbuilding blocks in a program can offer developers this knowledge source for\ncode comprehension. However, reading entire method statements can be\nchallenging, which necessitates precise and up-to-date comments. Objective: We\npropose a solution as an IntelliJ IDEA plugin, named SmartDoc, that assists\ndevelopers in generating context-aware method comments. Method: This plugin\nacts as an Artificial Intelligence (AI) agent that has its own memory and is\naugmented by target methods' context. When a request is initiated by the\nend-user, the method content and all its nested method calls are used in the\ncomment generation. At the beginning, these nested methods are visited and a\ncall graph is generated. This graph is then traversed using depth-first search\n(DFS), enabling the provision of full-context to enrich Large Language Model\n(LLM) prompts. Result: The product is a software, as a plugin, developed for\nJava codebase and installable on IntelliJ IDEA. This plugin can serve\nconcurrently for methods whose comments are being updated , and it shares\nmemory across all flows to avoid redundant calls. o measure the accuracy of\nthis solution, a dedicated test case is run to record SmartDoc generated\ncomments and their corresponding ground truth. For each collected result-set,\nthree metrics are computed, BERTScore, BLEU and ROUGE-1. These metrics will\ndetermine how accurate the generated comments are in comparison to the ground\ntruth. Result: The obtained accuracy, in terms of the precision, recall and F1,\nis promising, and lies in the range of 0.80 to 0.90 for BERTScore.", "AI": {"tldr": "SmartDoc\u662f\u4e00\u4e2aIntelliJ IDEA\u63d2\u4ef6\uff0c\u4f7f\u7528AI\u4ee3\u7406\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\u6ce8\u91ca\uff0c\u901a\u8fc7\u6784\u5efa\u8c03\u7528\u56fe\u548c\u4f7f\u7528DFS\u904d\u5386\u63d0\u4f9b\u5b8c\u6574\u4e0a\u4e0b\u6587\u6765\u589e\u5f3aLLM\u63d0\u793a\u3002", "motivation": "\u8f6f\u4ef6\u7ef4\u62a4\u9636\u6bb5\u9700\u8981\u7a0b\u5e8f\u7406\u89e3\uff0c\u65b9\u6cd5\u4f5c\u4e3a\u7a0b\u5e8f\u7684\u4e3b\u8981\u6784\u5efa\u5757\u53ef\u4ee5\u63d0\u4f9b\u4ee3\u7801\u7406\u89e3\u7684\u77e5\u8bc6\u6e90\uff0c\u4f46\u9605\u8bfb\u6574\u4e2a\u65b9\u6cd5\u8bed\u53e5\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u7cbe\u786e\u548c\u6700\u65b0\u7684\u6ce8\u91ca\u3002", "method": "\u5f00\u53d1IntelliJ IDEA\u63d2\u4ef6\u4f5c\u4e3aAI\u4ee3\u7406\uff0c\u5177\u6709\u81ea\u5df1\u7684\u5185\u5b58\u5e76\u901a\u8fc7\u76ee\u6807\u65b9\u6cd5\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u589e\u5f3a\u3002\u4f7f\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u904d\u5386\u8c03\u7528\u56fe\uff0c\u4e3aLLM\u63d0\u793a\u63d0\u4f9b\u5b8c\u6574\u4e0a\u4e0b\u6587\u3002", "result": "\u5f00\u53d1\u4e86\u9002\u7528\u4e8eJava\u4ee3\u7801\u5e93\u7684\u63d2\u4ef6\uff0c\u53ef\u4ee5\u5e76\u53d1\u66f4\u65b0\u65b9\u6cd5\u6ce8\u91ca\u3002\u5728BERTScore\u6307\u6807\u4e0a\u83b7\u5f97\u4e860.80\u52300.90\u7684\u51c6\u786e\u7387\uff0cBLEU\u548cROUGE-1\u6307\u6807\u4e5f\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u6ce8\u91ca\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SmartDoc\u63d2\u4ef6\u80fd\u591f\u6709\u6548\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\u6ce8\u91ca\uff0c\u51c6\u786e\u7387\u8868\u73b0\u826f\u597d\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u7a0b\u5e8f\u7406\u89e3\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "2511.00517", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00517", "abs": "https://arxiv.org/abs/2511.00517", "authors": ["Shuochuan Li", "Dong Wang", "Patanamon Thongtanunam", "Zan Wang", "Jiuqiao Yu", "Junjie Chen"], "title": "Issue-Oriented Agent-Based Framework for Automated Review Comment Generation", "comment": null, "summary": "Code review (CR) is a crucial practice for ensuring software quality. Various\nautomated review comment generation techniques have been proposed to streamline\nthe labor-intensive process. However, existing approaches heavily rely on a\nsingle model to identify various issues within the code, limiting the model's\nability to handle the diverse, issue-specific nature of code changes and\nleading to non-informative comments, especially in complex scenarios such as\nbug fixes. To address these limitations, we propose RevAgent, a novel\nagent-based issue-oriented framework, decomposes the task into three stages:\n(1) Generation Stage, where five category-specific commentator agents analyze\ncode changes from distinct issue perspectives and generate candidate comments;\n(2) Discrimination Stage, where a critic agent selects the most appropriate\nissue-comment pair; and (3) Training Stage, where all agents are fine-tuned on\ncurated, category-specific data to enhance task specialization. Evaluation\nresults show that RevAgent significantly outperforms state-of-the-art PLM- and\nLLM-based baselines, with improvements of 12.90\\%, 10.87\\%, 6.32\\%, and 8.57\\%\non BLEU, ROUGE-L, METEOR, and SBERT, respectively. It also achieves relatively\nhigher accuracy in issue-category identification, particularly for challenging\nscenarios. Human evaluations further validate the practicality of RevAgent in\ngenerating accurate, readable, and context-aware review comments. Moreover,\nRevAgent delivers a favorable trade-off between performance and efficiency.", "AI": {"tldr": "RevAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u95ee\u9898\u5bfc\u5411\u4ee3\u7801\u5ba1\u67e5\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u5206\u89e3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u751f\u6210\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\u5904\u7406\u5404\u79cd\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4ee3\u7801\u53d8\u66f4\u7684\u591a\u6837\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u5982bug\u4fee\u590d\u4e2d\u5bb9\u6613\u751f\u6210\u975e\u4fe1\u606f\u6027\u8bc4\u8bba\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u9636\u6bb5\uff08\u4e94\u4e2a\u7c7b\u522b\u7279\u5b9a\u7684\u8bc4\u8bba\u4ee3\u7406\u4ece\u4e0d\u540c\u95ee\u9898\u89d2\u5ea6\u5206\u6790\u4ee3\u7801\u53d8\u66f4\uff09\u3001\u5224\u522b\u9636\u6bb5\uff08\u6279\u8bc4\u4ee3\u7406\u9009\u62e9\u6700\u5408\u9002\u7684\u95ee\u9898-\u8bc4\u8bba\u5bf9\uff09\u3001\u8bad\u7ec3\u9636\u6bb5\uff08\u6240\u6709\u4ee3\u7406\u5728\u7279\u5b9a\u7c7b\u522b\u6570\u636e\u4e0a\u5fae\u8c03\uff09\u3002", "result": "RevAgent\u663e\u8457\u4f18\u4e8e\u73b0\u6709PLM\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728BLEU\u3001ROUGE-L\u3001METEOR\u548cSBERT\u6307\u6807\u4e0a\u5206\u522b\u63d0\u534712.90%\u300110.87%\u30016.32%\u548c8.57%\uff0c\u5728\u95ee\u9898\u7c7b\u522b\u8bc6\u522b\u65b9\u9762\u4e5f\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "RevAgent\u5728\u751f\u6210\u51c6\u786e\u3001\u53ef\u8bfb\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5ba1\u67e5\u8bc4\u8bba\u65b9\u9762\u5177\u6709\u5b9e\u7528\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "topic": "code agent"}}
{"id": "2511.00527", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00527", "abs": "https://arxiv.org/abs/2511.00527", "authors": ["Robab Aghazadeh-Chakherlou", "Qing Guo", "Siddartha Khastgir", "Peter Popov", "Xiaoge Zhang", "Xingyu Zhao"], "title": "HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models", "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed across diverse\ndomains, raising the need for rigorous reliability assessment methods. Existing\nbenchmark-based evaluations primarily offer descriptive statistics of model\naccuracy over datasets, providing limited insight into the probabilistic\nbehavior of LLMs under real operational conditions. This paper introduces\nHIP-LLM, a Hierarchical Imprecise Probability framework for modeling and\ninferring LLM reliability. Building upon the foundations of software\nreliability engineering, HIP-LLM defines LLM reliability as the probability of\nfailure-free operation over a specified number of future tasks under a given\nOperational Profile (OP). HIP-LLM represents dependencies across (sub-)domains\nhierarchically, enabling multi-level inference from subdomain to system-level\nreliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty\nand incorporates OPs to reflect usage contexts. It derives posterior\nreliability envelopes that quantify uncertainty across priors and data.\nExperiments on multiple benchmark datasets demonstrate that HIP-LLM offers a\nmore accurate and standardized reliability characterization than existing\nbenchmark and state-of-the-art approaches. A publicly accessible repository of\nHIP-LLM is provided.", "AI": {"tldr": "HIP-LLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u5c42\u6b21\u4e0d\u7cbe\u786e\u6982\u7387\u6846\u67b6\u7684LLM\u53ef\u9760\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49LLM\u5728\u7279\u5b9a\u64cd\u4f5c\u914d\u7f6e\u6587\u4ef6\u4e0b\u7684\u65e0\u6545\u969c\u8fd0\u884c\u6982\u7387\u6765\u91cf\u5316\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u63d0\u4f9b\u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u7edf\u8ba1\uff0c\u5bf9LLM\u5728\u5b9e\u9645\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u6982\u7387\u884c\u4e3a\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\u3002", "method": "HIP-LLM\u6784\u5efa\u5c42\u6b21\u4f9d\u8d56\u7ed3\u6784\uff0c\u5d4c\u5165\u4e0d\u7cbe\u786e\u5148\u9a8c\u6765\u6355\u6349\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6574\u5408\u64cd\u4f5c\u914d\u7f6e\u6587\u4ef6\u6765\u53cd\u6620\u4f7f\u7528\u60c5\u5883\uff0c\u63a8\u5bfc\u540e\u9a8c\u53ef\u9760\u6027\u5305\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIP-LLM\u6bd4\u73b0\u6709\u57fa\u51c6\u548c\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u6807\u51c6\u5316\u7684\u53ef\u9760\u6027\u8868\u5f81\u3002", "conclusion": "HIP-LLM\u4e3aLLM\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4e25\u8c28\u7684\u6846\u67b6\uff0c\u80fd\u591f\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u4f9b\u591a\u7ea7\u53ef\u9760\u6027\u63a8\u65ad\u3002", "topic": "agent analysis"}}
{"id": "2511.00619", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00619", "abs": "https://arxiv.org/abs/2511.00619", "authors": ["Huaijin Ran", "Haoyi Zhang", "Xunzhu Tang"], "title": "GDPR-Bench-Android: A Benchmark for Evaluating Automated GDPR Compliance Detection in Android", "comment": null, "summary": "Automating the detection of EU General Data Protection Regulation (GDPR)\nviolations in source code is a critical but underexplored challenge. We\nintroduce \\textbf{GDPR-Bench-Android}, the first comprehensive benchmark for\nevaluating diverse automated methods for GDPR compliance detection in Android\napplications. It contains \\textbf{1951} manually annotated violation instances\nfrom \\textbf{15} open-source repositories, covering 23 GDPR articles at file-,\nmodule-, and line-level granularities. To enable a multi-paradigm evaluation,\nwe contribute \\textbf{Formal-AST}, a novel, source-code-native formal method\nthat serves as a deterministic baseline. We define two tasks: (1)\n\\emph{multi-granularity violation localization}, evaluated via\nAccuracy@\\textit{k}; and (2) \\emph{snippet-level multi-label classification},\nassessed by macro-F1 and other classification metrics. We benchmark 11 methods,\nincluding eight state-of-the-art LLMs, our Formal-AST analyzer, a\nretrieval-augmented (RAG) method, and an agentic (ReAct) method. Our findings\nreveal that no single paradigm excels across all tasks. For Task 1, the ReAct\nagent achieves the highest file-level Accuracy@1 (17.38%), while the\nQwen2.5-72B LLM leads at the line level (61.60%), in stark contrast to the\nFormal-AST method's 1.86%. For the difficult multi-label Task 2, the\nClaude-Sonnet-4.5 LLM achieves the best Macro-F1 (5.75%), while the RAG method\nyields the highest Macro-Precision (7.10%). These results highlight the\ntask-dependent strengths of different automated approaches and underscore the\nvalue of our benchmark in diagnosing their capabilities. All resources are\navailable at: https://github.com/Haoyi-Zhang/GDPR-Bench-Android.", "AI": {"tldr": "\u63d0\u51fa\u4e86GDPR-Bench-Android\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30Android\u5e94\u7528\u4e2dGDPR\u8fdd\u89c4\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5305\u542b1951\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8fdd\u89c4\u5b9e\u4f8b\uff0c\u5e76\u6bd4\u8f83\u4e8611\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u81ea\u52a8\u5316\u68c0\u6d4b\u6e90\u4ee3\u7801\u4e2d\u7684GDPR\u8fdd\u89c4\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u9700\u8981\u5efa\u7acb\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u521b\u5efaGDPR-Bench-Android\u57fa\u51c6\uff0c\u5305\u542b1951\u4e2a\u624b\u52a8\u6807\u6ce8\u7684\u8fdd\u89c4\u5b9e\u4f8b\uff1b\u63d0\u51faFormal-AST\u5f62\u5f0f\u5316\u65b9\u6cd5\u4f5c\u4e3a\u786e\u5b9a\u6027\u57fa\u7ebf\uff1b\u8bc4\u4f3011\u79cd\u65b9\u6cd5\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1a\u591a\u7c92\u5ea6\u8fdd\u89c4\u5b9a\u4f4d\u548c\u7247\u6bb5\u7ea7\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u4e0d\u540c\u8303\u5f0f\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u5404\u5f02\uff1aReAct\u4ee3\u7406\u5728\u6587\u4ef6\u7ea7\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0817.38%\uff09\uff0cQwen2.5-72B\u5728\u884c\u7ea7\u5b9a\u4f4d\u4e2d\u9886\u5148\uff0861.60%\uff09\uff0cClaude-Sonnet-4.5\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4e2dMacro-F1\u6700\u9ad8\uff085.75%\uff09\uff0cRAG\u65b9\u6cd5\u5728Macro-Precision\u4e0a\u6700\u4f18\uff087.10%\uff09\u3002", "conclusion": "\u6ca1\u6709\u5355\u4e00\u8303\u5f0f\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u540c\u81ea\u52a8\u5316\u65b9\u6cd5\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027\u7684\u4f18\u52bf\uff0c\u57fa\u51c6\u6709\u52a9\u4e8e\u8bca\u65ad\u5404\u79cd\u65b9\u6cd5\u7684\u80fd\u529b\u3002", "topic": "swe benchmark"}}
{"id": "2511.00624", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00624", "abs": "https://arxiv.org/abs/2511.00624", "authors": ["Haoyi Zhang", "Huaijin Ran", "Xunzhu Tang"], "title": "Can Large Language Models Detect Real-World Android Software Compliance Violations?", "comment": null, "summary": "The rapid development of Large Language Models (LLMs) has transformed\nsoftware engineering, showing promise in tasks like code generation, bug\ndetection, and compliance checking. However, current models struggle to detect\ncompliance violations in Android applications across diverse legal frameworks.\nWe propose \\emph{CompliBench}, a novel evaluation framework for assessing LLMs'\nability to detect compliance violations under regulations like LGPD, PDPA, and\nPIPEDA. The framework defines two tasks: Task 1 evaluates \\emph{retrieval and\nlocalization} at file, module, and line granularities, and Task 2 assesses\n\\emph{multi-label judgment} for code snippets. These tasks mirror the audit\nprocess, where auditors locate problematic code and determine implicated\nprovisions. Traditional metrics fail to capture important aspects like\ncross-granularity stability and jurisdictional consistency. Thus, we introduce\nstability-aware composites (SGS, RCS, CRGS, and OCS) for a more comprehensive\nassessment. Experiments with six models, including GPT-4O and Claude-3.5, show\n\\emph{CompliBench} improves compliance detection, with\nClaude-3.5-sonnet-20241022 achieving the highest OCS score (0.3295), and\nGemini-2.5-pro the lowest (0.0538). This work demonstrates \\emph{CompliBench}'s\npotential for improving LLM performance in compliance tasks and provides a\nfoundation for future tools aligned with data protection standards. Our project\nis available at https://github.com/Haoyi-Zhang/CompliBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86CompliBench\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728Android\u5e94\u7528\u5408\u89c4\u6027\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u542b\u68c0\u7d22\u5b9a\u4f4d\u548c\u591a\u6807\u7b7e\u5224\u65ad\u4e24\u4e2a\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u7a33\u5b9a\u6027\u611f\u77e5\u7684\u590d\u5408\u6307\u6807\u3002", "motivation": "\u5f53\u524dLLM\u5728\u68c0\u6d4bAndroid\u5e94\u7528\u8de8\u6cd5\u5f8b\u6846\u67b6\u7684\u5408\u89c4\u8fdd\u89c4\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63d0\u5347\u5408\u89c4\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u5b9a\u4e49\u4e86\u4e24\u4e2a\u4efb\u52a1\uff1aTask1\u8bc4\u4f30\u6587\u4ef6\u3001\u6a21\u5757\u548c\u884c\u7ea7\u522b\u7684\u68c0\u7d22\u5b9a\u4f4d\u80fd\u529b\uff0cTask2\u8bc4\u4f30\u4ee3\u7801\u7247\u6bb5\u7684\u591a\u6807\u7b7e\u5224\u65ad\u80fd\u529b\uff1b\u5f15\u5165\u4e86\u7a33\u5b9a\u6027\u611f\u77e5\u590d\u5408\u6307\u6807(SGS\u3001RCS\u3001CRGS\u3001OCS)\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5728\u516d\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cClaude-3.5-sonnet-20241022\u83b7\u5f97\u6700\u9ad8OCS\u5206\u6570(0.3295)\uff0cGemini-2.5-pro\u5f97\u5206\u6700\u4f4e(0.0538)\u3002", "conclusion": "CompliBench\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u5728\u5408\u89c4\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u7b26\u5408\u6570\u636e\u4fdd\u62a4\u6807\u51c6\u7684\u672a\u6765\u5de5\u5177\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "swe benchmark"}}
{"id": "2511.00678", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00678", "abs": "https://arxiv.org/abs/2511.00678", "authors": ["Tasmia Zerin", "Moumita Asad", "B. M. Mainul Hossain", "Kazi Sakib"], "title": "Repairing Responsive Layout Failures Using Retrieval Augmented Generation", "comment": "Accepted at the 41st IEEE International Conference on Software\n  Maintenance and Evolution 2025 (ICSME'25)", "summary": "Responsive websites frequently experience distorted layouts at specific\nscreen sizes, called Responsive Layout Failures (RLFs). Manually repairing\nthese RLFs involves tedious trial-and-error adjustments of HTML elements and\nCSS properties. In this study, an automated repair approach, leveraging LLM\ncombined with domain-specific knowledge is proposed. The approach is named\nReDeFix, a Retrieval-Augmented Generation (RAG)-based solution that utilizes\nStack Overflow (SO) discussions to guide LLM on CSS repairs. By augmenting\nrelevant SO knowledge with RLF-specific contexts, ReDeFix creates a prompt that\nis sent to the LLM to generate CSS patches. Evaluation demonstrates that our\napproach achieves an 88\\% accuracy in repairing RLFs. Furthermore, a study from\nsoftware engineers reveals that generated repairs produce visually correct\nlayouts while maintaining aesthetics.", "AI": {"tldr": "\u63d0\u51faReDeFix\u65b9\u6cd5\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ed3\u5408Stack Overflow\u77e5\u8bc6\u6765\u81ea\u52a8\u4fee\u590d\u54cd\u5e94\u5f0f\u5e03\u5c40\u6545\u969c(RLFs)\uff0c\u51c6\u786e\u7387\u8fbe88%", "motivation": "\u54cd\u5e94\u5f0f\u7f51\u7ad9\u5728\u7279\u5b9a\u5c4f\u5e55\u5c3a\u5bf8\u4e0b\u7ecf\u5e38\u51fa\u73b0\u5e03\u5c40\u53d8\u5f62\u95ee\u9898\uff0c\u624b\u52a8\u4fee\u590d\u9700\u8981\u7e41\u7410\u7684\u8bd5\u9519\u8c03\u6574HTML\u5143\u7d20\u548cCSS\u5c5e\u6027", "method": "\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u65b9\u6cd5\uff0c\u5229\u7528Stack Overflow\u8ba8\u8bba\u6765\u6307\u5bfcLLM\u8fdb\u884cCSS\u4fee\u590d\uff0c\u901a\u8fc7\u589e\u5f3a\u76f8\u5173SO\u77e5\u8bc6\u4e0eRLF\u7279\u5b9a\u4e0a\u4e0b\u6587\u521b\u5efa\u63d0\u793a\u8bcd", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4fee\u590dRLFs\u65b9\u9762\u8fbe\u523088%\u7684\u51c6\u786e\u7387\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7814\u7a76\u8868\u660e\u751f\u6210\u7684\u4fee\u590d\u80fd\u591f\u4ea7\u751f\u89c6\u89c9\u6b63\u786e\u7684\u5e03\u5c40\u540c\u65f6\u4fdd\u6301\u7f8e\u89c2\u6027", "conclusion": "ReDeFix\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u54cd\u5e94\u5f0f\u5e03\u5c40\u6545\u969c\u7684\u81ea\u52a8\u4fee\u590d\u95ee\u9898\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u548cLLM\u6280\u672f\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c", "topic": "swe application"}}
{"id": "2511.00776", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00776", "abs": "https://arxiv.org/abs/2511.00776", "authors": ["Cuiyun Gao", "Guodong Fan", "Chun Yong Chong", "Shizhan Chen", "Chao Liu", "David Lo", "Zibin Zheng", "Qing Liao"], "title": "A Systematic Literature Review of Code Hallucinations in LLMs: Characterization, Mitigation Methods, Challenges, and Future Directions for Reliable AI", "comment": null, "summary": "Model hallucination is one of the most critical challenges faced by Large\nLanguage Models (LLMs), especially in high-stakes code intelligence tasks. As\nLLMs become increasingly integrated into software engineering tasks,\nunderstanding and mitigating hallucination in code becomes essential. In this\nsurvey, we provide a systematic review of hallucination phenomena in\ncode-oriented LLMs from four key perspectives. First, we begin by surveying 60\npapers to define hallucination in the context of code and summarize its primary\ncauses, such as data noise, exposure bias, and insufficient semantic grounding,\nwhile also tracing recent trends in literature across natural language\nprocessing (NLP) and software engineering communities. Second, we review model\nhallucination surveys in a broader span and summarize representative\nhallucination mitigation strategies, such as knowledge-enhanced generation,\nconstrained decoding, and post-editing. Third, we review approaches targeted\nfor code intelligence and highlight code-specific challenges that aggravate\nhallucination, including syntax sensitivity, strict type systems, and\ndependence on external libraries. Meanwhile, we analyze how emerging code\nintelligence tasks, e.g., program analysis, symbolic execution, and unit\ntesting, are utilized to detect and mitigate hallucinations. Fourth, we\nsummarize current evaluation benchmarks, ranging from static metrics to dynamic\nchecks, e.g., compilation and execution correctness, and emphasize the need for\nhallucination-oriented benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u4ee3\u7801\u5bfc\u5411\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4ece\u5b9a\u4e49\u3001\u6210\u56e0\u3001\u7f13\u89e3\u7b56\u7565\u3001\u4ee3\u7801\u7279\u5b9a\u6311\u6218\u5230\u8bc4\u4f30\u57fa\u51c6\u56db\u4e2a\u65b9\u9762\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u548c\u7f13\u89e3\u4ee3\u7801\u5e7b\u89c9\u95ee\u9898\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u7efc\u8ff060\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u4ece\u56db\u4e2a\u5173\u952e\u89d2\u5ea6\u7cfb\u7edf\u5206\u6790\u4ee3\u7801\u5e7b\u89c9\uff1a\u5b9a\u4e49\u4e0e\u6210\u56e0\u3001\u901a\u7528\u7f13\u89e3\u7b56\u7565\u3001\u4ee3\u7801\u7279\u5b9a\u6311\u6218\u3001\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u603b\u7ed3\u4e86\u4ee3\u7801\u5e7b\u89c9\u7684\u4e3b\u8981\u6210\u56e0\uff08\u6570\u636e\u566a\u58f0\u3001\u66b4\u9732\u504f\u5dee\u3001\u8bed\u4e49\u57fa\u7840\u4e0d\u8db3\uff09\uff0c\u7f13\u89e3\u7b56\u7565\uff08\u77e5\u8bc6\u589e\u5f3a\u751f\u6210\u3001\u7ea6\u675f\u89e3\u7801\u3001\u540e\u7f16\u8f91\uff09\uff0c\u4ee5\u53ca\u4ee3\u7801\u7279\u5b9a\u6311\u6218\uff08\u8bed\u6cd5\u654f\u611f\u6027\u3001\u4e25\u683c\u7c7b\u578b\u7cfb\u7edf\u3001\u5916\u90e8\u5e93\u4f9d\u8d56\uff09\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u5e7b\u89c9\u5bfc\u5411\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5229\u7528\u7a0b\u5e8f\u5206\u6790\u3001\u7b26\u53f7\u6267\u884c\u7b49\u65b0\u5174\u4ee3\u7801\u667a\u80fd\u4efb\u52a1\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u5e7b\u89c9\u3002", "topic": "agent analysis"}}
{"id": "2511.00802", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00802", "abs": "https://arxiv.org/abs/2511.00802", "authors": ["Jie JW Wu", "Ayanda Patrick Herlihy", "Ahmad Saleem Mirza", "Ali Afoud", "Fatemeh Fard"], "title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents", "comment": null, "summary": "With the software industry shifting toward a data-driven culture, online A/B\ntesting is a key tool for evaluating new technologies. However, deploying such\nexperiments requires substantial resources, may negatively impact users, and\ninvolves long data collection periods. To address this, \\textit{off-policy\nevaluation (OPE)}, or offline A/B testing, uses logged data to assess\ntechnologies and is fundamental in Reinforcement Learning, making it crucial in\ndomains where online testing is costly or risky, such as healthcare,\nrecommender systems, education, dialog systems, and robotics. Despite advances\nin coding LLMs and agentic AI, little is known about leveraging them to\noptimize OPE results. We investigate whether LLMs and LLM-based agents can\nimprove OPE performance via code optimization. We propose\n\\textit{GrowthHacker}, a benchmark with agent and baseline methods on\nlarge-scale real-world datasets, which iteratively optimizes code, evaluates\nresults, and begins new optimization cycles. We collected datasets, established\nprotocols, implemented baselines for OPE on the Open Bandit Pipeline\n(OBP)~\\cite{saito2021openbanditdatasetpipeline} and\nScope-RL~\\cite{kiyohara2023scope}, and developed the \\textit{two_agent}\nframework, which reduces system complexity while preserving optimization\neffectiveness. Results show the two_agent framework achieves 100% reliability\nand the highest average improvement of 106.7% among positive outcomes. Both\ntwo_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.\nThese findings demonstrate the feasibility of LLM-based agents as automated\n\"growth hackers\" to enhance OPE systems, with implications for scaling\ndata-driven decision-making in production.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGrowthHacker\u57fa\u51c6\uff0c\u901a\u8fc7LLM\u548c\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u4f18\u5316\u79bb\u7ebfA/B\u6d4b\u8bd5(OPE)\u6027\u80fd\uff0c\u5f00\u53d1\u4e86two_agent\u6846\u67b6\u5b9e\u73b0\u4ee3\u7801\u4f18\u5316\u548c\u8bc4\u4f30\u5faa\u73af\u3002", "motivation": "\u5728\u7ebfA/B\u6d4b\u8bd5\u9700\u8981\u5927\u91cf\u8d44\u6e90\u4e14\u53ef\u80fd\u5bf9\u7528\u6237\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u800c\u79bb\u7ebf\u8bc4\u4f30(OPE)\u4f7f\u7528\u65e5\u5fd7\u6570\u636e\u8bc4\u4f30\u6280\u672f\uff0c\u5728\u533b\u7597\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5c24\u4e3a\u91cd\u8981\u3002\u4f46\u76ee\u524d\u7f3a\u4e4f\u5229\u7528LLM\u4f18\u5316OPE\u7ed3\u679c\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51faGrowthHacker\u57fa\u51c6\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4ee3\u7406\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5f00\u53d1two_agent\u6846\u67b6\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u4f18\u5316\u6548\u679c\uff0c\u4f7f\u7528Open Bandit Pipeline\u548cScope-RL\u8fdb\u884cOPE\u8bc4\u4f30\u3002", "result": "two_agent\u6846\u67b6\u5b9e\u73b0100%\u53ef\u9760\u6027\u548c106.7%\u5e73\u5747\u6539\u8fdb\u7387\uff0c\u4e0eCrewAI\u5747\u8fbe\u523045%\u6210\u529f\u7387\uff0c\u4f18\u4e8eAutoGen\u768434%\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u53ef\u4ee5\u4f5c\u4e3a\u81ea\u52a8\u5316\"\u589e\u957f\u9ed1\u5ba2\"\u6765\u589e\u5f3aOPE\u7cfb\u7edf\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u6269\u5c55\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.00839", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00839", "abs": "https://arxiv.org/abs/2511.00839", "authors": ["John Yang", "Kilian Lieret", "Joyce Yang", "Carlos E. Jimenez", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "CodeClash: Benchmarking Goal-Oriented Software Engineering", "comment": null, "summary": "Current benchmarks for coding evaluate language models (LMs) on concrete,\nwell-specified tasks such as fixing specific bugs or writing targeted tests.\nHowever, human programmers do not spend all day incessantly addressing isolated\ntasks. Instead, real-world software development is grounded in the pursuit of\nhigh-level goals, like improving user retention or reducing costs. Evaluating\nwhether LMs can also iteratively develop code to better accomplish open-ended\nobjectives without any explicit guidance remains an open challenge. To address\nthis, we introduce CodeClash, a benchmark where LMs compete in multi-round\ntournaments to build the best codebase for achieving a competitive objective.\nEach round proceeds in two phases: agents edit their code, then their codebases\ncompete head-to-head in a code arena that determines winners based on\nobjectives like score maximization, resource acquisition, or survival. Whether\nit's writing notes, scrutinizing documentation, analyzing competition logs, or\ncreating test suites, models must decide for themselves how to improve their\ncodebases both absolutely and against their opponents. We run 1680 tournaments\n(25,200 rounds total) to evaluate 8 LMs across 6 arenas. Our results reveal\nthat while models exhibit diverse development styles, they share fundamental\nlimitations in strategic reasoning. Models also struggle with long-term\ncodebase maintenance, as repositories become progressively messy and redundant.\nThese limitations are stark: top models lose every round against expert human\nprogrammers. We open-source CodeClash to advance the study of autonomous,\ngoal-oriented code development.", "AI": {"tldr": "CodeClash\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u9526\u6807\u8d5b\u4e2d\u7ade\u4e89\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f00\u53d1\u4ee3\u7801\u6765\u5b9e\u73b0\u7ade\u4e89\u6027\u76ee\u6807\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u65e0\u660e\u786e\u6307\u5bfc\u4e0b\u7684\u81ea\u4e3b\u4ee3\u7801\u5f00\u53d1\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u5177\u4f53\u3001\u660e\u786e\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u6d89\u53ca\u8ffd\u6c42\u9ad8\u5c42\u6b21\u76ee\u6807\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u80fd\u5426\u5728\u6ca1\u6709\u660e\u786e\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u8fed\u4ee3\u5f00\u53d1\u4ee3\u7801\u6765\u5b9e\u73b0\u5f00\u653e\u76ee\u6807\u3002", "method": "\u8bbe\u8ba1\u591a\u8f6e\u9526\u6807\u8d5b\u673a\u5236\uff0c\u6bcf\u8f6e\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u4ee3\u7406\u7f16\u8f91\u4ee3\u7801\uff0c\u7136\u540e\u5728\u4ee3\u7801\u7ade\u6280\u573a\u4e2d\u8fdb\u884c\u5bf9\u6297\u3002\u6a21\u578b\u9700\u8981\u81ea\u884c\u51b3\u5b9a\u5982\u4f55\u6539\u8fdb\u4ee3\u7801\u5e93\u6765\u5e94\u5bf9\u7ade\u4e89\u76ee\u6807\u3002", "result": "\u8fd0\u884c1680\u573a\u9526\u6807\u8d5b\uff08\u517125200\u8f6e\uff09\uff0c\u8bc4\u4f308\u4e2a\u8bed\u8a00\u6a21\u578b\u57286\u4e2a\u7ade\u6280\u573a\u4e2d\u7684\u8868\u73b0\u3002\u6a21\u578b\u5c55\u73b0\u51fa\u591a\u6837\u5316\u7684\u5f00\u53d1\u98ce\u683c\uff0c\u4f46\u5728\u6218\u7565\u63a8\u7406\u548c\u957f\u671f\u4ee3\u7801\u5e93\u7ef4\u62a4\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "conclusion": "\u9876\u7ea7\u6a21\u578b\u5728\u5bf9\u6297\u4eba\u7c7b\u4e13\u5bb6\u7a0b\u5e8f\u5458\u65f6\u5168\u90e8\u5931\u8d25\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u81ea\u4e3b\u3001\u76ee\u6807\u5bfc\u5411\u7684\u4ee3\u7801\u5f00\u53d1\u65b9\u9762\u4ecd\u6709\u663e\u8457\u5c40\u9650\u6027\u3002\u5f00\u6e90CodeClash\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2511.00651", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7535\u4fe1\u7f51\u7edc\u6545\u969c\u81ea\u52a8\u6392\u67e5\u6846\u67b6\uff0c\u5229\u7528LLM\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u5de5\u5177\uff0c\u901a\u8fc7\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u57fa\u4e8e\u5185\u90e8\u6587\u6863\u7684\u4fee\u590d\u65b9\u6848\uff0c\u663e\u8457\u52a0\u901fRAN\u548c\u6838\u5fc3\u7f51\u7edc\u6545\u969c\u6392\u67e5\u81ea\u52a8\u5316\u3002", "motivation": "\u7535\u4fe1\u7f51\u7edc\u89c4\u6a21\u6269\u5927\u548c\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u73b0\u6709AI\u6a21\u578b\u8303\u56f4\u72ed\u7a84\u3001\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u5728\u5f02\u6784\u90e8\u7f72\u4e2d\u6cdb\u5316\uff0c\u7f51\u7edc\u6545\u969c\u6392\u67e5\u4ecd\u4e25\u91cd\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u5206\u6790\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5f53AI/ML\u76d1\u63a7\u68c0\u6d4b\u5230\u6545\u969c\u65f6\uff0c\u52a8\u6001\u6fc0\u6d3b\u7f16\u6392\u5668\u3001\u89e3\u51b3\u65b9\u6848\u89c4\u5212\u5668\u3001\u6267\u884c\u5668\u3001\u6570\u636e\u68c0\u7d22\u5668\u548c\u6839\u56e0\u5206\u6790\u5668\u7b49\u667a\u80fd\u4f53\u8fdb\u884c\u8bca\u65ad\u548c\u4fee\u590d\u3002\u5173\u952e\u7ec4\u4ef6\u662f\u89e3\u51b3\u65b9\u6848\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u5185\u90e8\u6587\u6863\u751f\u6210\u4fee\u590d\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u52a0\u901f\u4e86RAN\u548c\u6838\u5fc3\u7f51\u7edc\u9886\u57df\u7684\u6545\u969c\u6392\u67e5\u81ea\u52a8\u5316\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7ed3\u5408LLM\u534f\u8c03\u548c\u5fae\u8c03SLM\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u7535\u4fe1\u7f51\u7edc\u6545\u969c\u7684\u81ea\u52a8\u5316\u6392\u67e5\u548c\u4fee\u590d\u3002", "topic": "agent analysis"}}
{"id": "2511.00872", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00872", "abs": "https://arxiv.org/abs/2511.00872", "authors": ["Zhuowen Yin", "Cuifeng Gao", "Chunsong Fan", "Wenzhang Yang", "Yinxing Xue", "Lijun Zhang"], "title": "A Comprehensive Empirical Evaluation of Agent Frameworks on Code-centric Software Engineering Tasks", "comment": null, "summary": "Unlike traditional automation tools or static LLM-based systems, agents\ncombine decision-making and tool utilization to accomplish complex tasks,\nshowing great potential in software engineering. However, existing studies\nlargely focus on specific tasks or isolated aspects, providing an incomplete\npicture of agents' practical capabilities. To address this, we conduct a\ncomprehensive empirical study evaluating seven general-purpose agent frameworks\nacross three representative code-centric tasks: software development,\nvulnerability detection, and program repair. Each task is assessed using\nstandard, widely adopted benchmarks to ensure objective and comparable\nevaluation. Agent performance is systematically analyzed from three\ncomplementary perspectives: effectiveness (task success), efficiency (execution\nprocess), and overhead (token consumption). Our findings reveal distinct\ncapability patterns and trade-offs among the evaluated frameworks. In terms of\neffectiveness, agents achieve moderate overall performance. Regarding\nefficiency, AgentOrchestra tends to exhibit the longest trajectories and the\nmost correction attempts due to coordination overhead, whereas OpenHands\ndemonstrate stronger reflective reasoning abilities. For overhead, software\ndevelopment incurs the highest monetary cost, while GPTswarm remains the most\ncost-efficient. Furthermore, we conduct an in-depth cross-analysis of the\nrelationship between effectiveness and efficiency, exploring the underlying\nreasons behind their interplay. These findings guide both practical adoption\nand future research toward more efficient software engineering agents.", "AI": {"tldr": "\u5bf97\u4e2a\u901a\u7528\u4ee3\u7406\u6846\u67b6\u5728\u8f6f\u4ef6\u5f00\u53d1\u3001\u6f0f\u6d1e\u68c0\u6d4b\u548c\u7a0b\u5e8f\u4fee\u590d\u4e09\u4e2a\u4ee3\u7801\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u8bc1\u7814\u7a76\uff0c\u4ece\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u5f00\u9500\u4e09\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u4efb\u52a1\u6216\u5b64\u7acb\u65b9\u9762\uff0c\u65e0\u6cd5\u5168\u9762\u4e86\u89e3\u4ee3\u7406\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u9700\u8981\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u4ee5\u6307\u5bfc\u5b9e\u8df5\u5e94\u7528\u548c\u672a\u6765\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f307\u4e2a\u901a\u7528\u4ee3\u7406\u6846\u67b6\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u4ee3\u7801\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4ece\u6709\u6548\u6027\uff08\u4efb\u52a1\u6210\u529f\u7387\uff09\u3001\u6548\u7387\uff08\u6267\u884c\u8fc7\u7a0b\uff09\u548c\u5f00\u9500\uff08token\u6d88\u8017\uff09\u4e09\u4e2a\u4e92\u8865\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u4ee3\u7406\u6574\u4f53\u8868\u73b0\u4e2d\u7b49\uff1bAgentOrchestra\u56e0\u534f\u8c03\u5f00\u9500\u800c\u8f68\u8ff9\u6700\u957f\u3001\u4fee\u6b63\u5c1d\u8bd5\u6700\u591a\uff1bOpenHands\u5c55\u793a\u66f4\u5f3a\u7684\u53cd\u601d\u63a8\u7406\u80fd\u529b\uff1b\u8f6f\u4ef6\u5f00\u53d1\u6210\u672c\u6700\u9ad8\uff0cGPTswarm\u6700\u5177\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u6846\u67b6\u7684\u80fd\u529b\u6a21\u5f0f\u548c\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u5b9e\u9645\u91c7\u7528\u548c\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2511.00066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00066", "abs": "https://arxiv.org/abs/2511.00066", "authors": ["Tue Le", "Nghi D. Q. Bui", "Linh Ngo Van", "Trung Le"], "title": "Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful approach for strengthening the reasoning capabilities of large\nlanguage models (LLMs). Among existing algorithms, Group Relative Policy\nOptimization (GRPO) has demonstrated strong performance, yet it suffers from a\ncritical issue: low-probability tokens disproportionately dominate gradient\nupdates due to their inherently large gradient magnitudes. This imbalance leads\nto unstable training and suppresses the contribution of high-probability tokens\nthat are more reliable for learning. In this work, we introduce Token-Regulated\nGroup Relative Policy Optimization (TR-GRPO), a simple yet effective extension\nof GRPO that assigns token-level weights positively correlated with the model's\npredicted probability. By downweighting low-probability tokens and emphasizing\nhigh-probability ones, TR-GRPO mitigates gradient over-amplification while\npreserving informative learning signals. Extensive experiments demonstrate that\nTR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,\nand agentic reasoning, highlighting the importance of regulating token\ncontributions during RL training and establishing TR-GRPO as a robust framework\nfor enhancing LLM reasoning.", "AI": {"tldr": "TR-GRPO\u901a\u8fc7\u57fa\u4e8etoken\u6982\u7387\u7684\u6743\u91cd\u8c03\u8282\u673a\u5236\uff0c\u89e3\u51b3\u4e86GRPO\u4e2d\u4f4e\u6982\u7387token\u68af\u5ea6\u8fc7\u5ea6\u653e\u5927\u7684\u95ee\u9898\uff0c\u5728RLVR\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\u3002", "motivation": "\u73b0\u6709GRPO\u7b97\u6cd5\u5b58\u5728\u4f4e\u6982\u7387token\u68af\u5ea6\u8fc7\u5ea6\u4e3b\u5bfc\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u5e76\u6291\u5236\u9ad8\u6982\u7387token\u7684\u5b66\u4e60\u8d21\u732e\u3002", "method": "\u63d0\u51faTR-GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7token\u7ea7\u6743\u91cd\u5206\u914d\u673a\u5236\uff0c\u4f7f\u6743\u91cd\u4e0e\u6a21\u578b\u9884\u6d4b\u6982\u7387\u6b63\u76f8\u5173\uff0c\u4ece\u800c\u964d\u4f4e\u4f4e\u6982\u7387token\u7684\u5f71\u54cd\u5e76\u5f3a\u8c03\u9ad8\u6982\u7387token\u3002", "result": "\u5728\u903b\u8f91\u63a8\u7406\u3001\u6570\u5b66\u63a8\u7406\u548c\u667a\u80fd\u4f53\u63a8\u7406\u7b49RLVR\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTR-GRPO\u59cb\u7ec8\u4f18\u4e8eGRPO\u3002", "conclusion": "\u8c03\u8282token\u8d21\u732e\u5728RL\u8bad\u7ec3\u4e2d\u81f3\u5173\u91cd\u8981\uff0cTR-GRPO\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86Ariadne\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u8ff7\u5bab\u8fdb\u884c\u591a\u6b65\u7a7a\u95f4\u63a8\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u7a7a\u95f4\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u4ece\u57fa\u6a21\u578b\u76840%\u51c6\u786e\u7387\u63d0\u5347\u523050%\u4ee5\u4e0a\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u662f\u5426\u80fd\u591f\u771f\u6b63\u6269\u5c55\u5176\u80fd\u529b\u8fb9\u754c\uff0c\u7279\u522b\u662f\u5728\u57fa\u6a21\u578b\u6700\u521d\u5931\u8d25\u7684\u89c6\u89c9\u4e2d\u5fc3\u7a7a\u95f4\u4efb\u52a1\u4e0a\u3002", "method": "\u4f7f\u7528\u5408\u6210\u8ff7\u5bab\u521b\u5efa\u53ef\u63a7\u96be\u5ea6\u7684\u591a\u6b65\u7a7a\u95f4\u63a8\u7406\u73af\u5883\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u8fdb\u884c\u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3\u3002", "result": "\u540e\u8bad\u7ec3\u540e\u6a21\u578b\u5728\u57fa\u6a21\u578b\u5f97\u5206\u4e3a0%\u7684\u95ee\u9898\u96c6\u4e0a\u8fbe\u523050%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u5728MapBench\u548cReasonMap\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u5b9e\u73b016%\u548c24%\u7684\u96f6\u6837\u672c\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6269\u5c55\u4e86\u6a21\u578b\u7684\u57fa\u672c\u80fd\u529b\u9650\u5236\uff0c\u8fd8\u589e\u5f3a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4e13\u95e8\u5316\u80fd\u529b\u6269\u5c55\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u52a8\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper.", "AI": {"tldr": "\u63d0\u51faWordSaladChopper(WSC)\u7ec4\u4ef6\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u5220\u9664\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u65e0\u7528\u81ea\u6211\u91cd\u590dtoken\u6765\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u8f93\u51fatoken\u6210\u672c\u9ad8\u6602\uff0c\u5176\u4e2d\u5927\u91cf\u662f\u65e0\u7528\u7684\u81ea\u6211\u91cd\u590dtoken\uff08\u79f0\u4e3a'word salad'\uff09\uff0c\u8fd9\u4e9btoken\u6d88\u8017\u89e3\u7801\u9884\u7b97\u4f46\u4e0d\u589e\u52a0\u4ef7\u503c", "method": "\u5229\u7528\u6a21\u578b\u5bf9<\\n\\n>\u6807\u8bb0\u7684\u9690\u85cf\u72b6\u6001\u6a21\u5f0f\uff0c\u901a\u8fc7\u5355\u5c42\u7ebf\u6027\u5206\u7c7b\u5668\u5b9e\u65f6\u68c0\u6d4bword salad\u884c\u4e3a\uff0c\u68c0\u6d4b\u540e\u901a\u8fc7\u7b80\u5355\u622a\u65ad\u548c\u91cd\u65b0\u751f\u6210\u63d0\u793a\u6765\u8282\u7701\u957f\u5ea6", "result": "WSC\u7ec4\u4ef6\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u6700\u5c0f\uff0c\u4e3aLRM\u5e94\u7528\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "WSC\u6216\u7c7b\u4f3c\u7ec4\u4ef6\u662f\u6240\u6709\u8003\u8651\u7528\u6237\u4f53\u9a8c\u7684LRM\u5e94\u7528\u5fc5\u5907\u7ec4\u4ef6\uff0c\u56e0\u5176\u5f00\u9500\u4f4e\u3001\u8282\u7701\u663e\u8457\u4e14word salad token\u7f3a\u4e4f\u8bed\u4e49\u4ef7\u503c", "topic": "agent analysis"}}
{"id": "2511.00739", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively.", "AI": {"tldr": "\u672c\u6587\u4eceCPU\u89d2\u5ea6\u5206\u6790\u667a\u80fd\u4f53AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7cfb\u7edf\u74f6\u9888\uff0c\u53d1\u73b0\u5de5\u5177\u5904\u7406\u5728CPU\u4e0a\u5360\u7528\u9ad8\u8fbe90.6%\u7684\u603b\u5ef6\u8fdf\uff0c\u5e76\u63d0\u51faCPU\u548cGPU\u611f\u77e5\u7684\u5fae\u6279\u5904\u7406\u53ca\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u4f18\u5316\u65b9\u6848\uff0c\u5b9e\u73b0\u6700\u9ad82.1\u500d\u7684\u5ef6\u8fdf\u52a0\u901f\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u4f53AI\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u5e38\u88ab\u5ffd\u89c6\u7684CPU\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u7cfb\u7edf\u74f6\u9888\u95ee\u9898\uff0c\u63ed\u793aCPU\u5728\u667a\u80fd\u4f53AI\u6027\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u9996\u5148\u7cfb\u7edf\u5316\u8868\u5f81\u667a\u80fd\u4f53AI\u7684\u5de5\u4f5c\u7279\u6027\uff0c\u7136\u540e\u9009\u62e9\u4e94\u4e2a\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u6027\u80fd\u5206\u6790\uff0c\u6700\u540e\u63d0\u51faCPU\u548cGPU\u611f\u77e5\u7684\u5fae\u6279\u5904\u7406\u53ca\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u4f18\u5316\u65b9\u6848\u3002", "result": "\u53d1\u73b0\u5de5\u5177\u5904\u7406\u5728CPU\u4e0a\u5360\u7528\u9ad8\u8fbe90.6%\u7684\u603b\u5ef6\u8fdf\uff0cCPU\u52a8\u6001\u80fd\u8017\u5360\u603b\u80fd\u8017\u768444%\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u65b9\u6848\u5b9e\u73b0\u6700\u9ad82.1\u500d\u7684P50\u5ef6\u8fdf\u52a0\u901f\u3002", "conclusion": "CPU\u5728\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u4f18\u5316\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.01043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01043", "abs": "https://arxiv.org/abs/2511.01043", "authors": ["Zihan Fang", "Yifan Zhang", "Yueke Zhang", "Kevin Leach", "Yu Huang"], "title": "DPO-F+: Aligning Code Repair Feedback with Developers' Preferences", "comment": "10 pages, 2 figures", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\ntasks, especially code repair. However, developers often struggle to interpret\nmodel outputs, limiting effective human-AI teaming. Prior work largely\noptimizes repaired code while under-addressing the natural-language feedback\nthat enables comprehension and iterative improvement. We present DPO-f+, a\nnovel framework that aligns code-repair feedback with developer needs and\nprofiles. It (1) formalizes developer-profiled, domain-specific metrics for\nfeedback alignment; (2) automatically constructs pairwise preference datasets\nfrom code-repair tasks; (3) fine-tunes using Direct Preference Optimization\n(DPO) augmented with a lightweight margin signal; and (4) provides an automated\nfeedback evaluation protocol. Empirically, DPO-f+ outperforms both the baseline\nand standard DPO on generated-code accuracy and overall feedback alignment. On\nnovice programming tasks, DPO-f+ raises the top-1 pass rate by 5.71 percentage\npoints (pp) over the baseline and by 3.30 pp over DPO. On the more challenging\nSWE-bench Lite benchmark, it increases the issue-resolution rate by 1.67 pp\nover DPO and by 4.67 pp over the baseline. It also achieves the largest\nimprovement in feedback alignment, outperforming DPO and the baseline. By\naligning feedback more closely with developer needs, DPO-f+ turns LLM-assisted\nrepair from one-shot outputs into a collaborative sensemaking workflow,\nproviding a practical approach to enhancing code comprehension and fostering\nmore effective human-AI teaming in software engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e86DPO-f+\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u548c\u8f7b\u91cf\u7ea7\u8fb9\u754c\u4fe1\u53f7\u6765\u5bf9\u9f50\u4ee3\u7801\u4fee\u590d\u53cd\u9988\u4e0e\u5f00\u53d1\u8005\u9700\u6c42\uff0c\u63d0\u5347LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u7406\u89e3LLM\u8f93\u51fa\u65f6\u9047\u5230\u56f0\u96be\uff0c\u9650\u5236\u4e86\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u4f18\u5316\u4fee\u590d\u4ee3\u7801\uff0c\u4f46\u5ffd\u89c6\u4e86\u652f\u6301\u7406\u89e3\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u3002", "method": "1) \u5f62\u5f0f\u5316\u5f00\u53d1\u8005\u753b\u50cf\u548c\u9886\u57df\u7279\u5b9a\u6307\u6807\uff1b2) \u4ece\u4ee3\u7801\u4fee\u590d\u4efb\u52a1\u81ea\u52a8\u6784\u5efa\u6210\u5bf9\u504f\u597d\u6570\u636e\u96c6\uff1b3) \u4f7f\u7528\u589e\u5f3a\u8fb9\u754c\u4fe1\u53f7\u7684DPO\u8fdb\u884c\u5fae\u8c03\uff1b4) \u63d0\u4f9b\u81ea\u52a8\u53cd\u9988\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5728\u521d\u5b66\u8005\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0cDPO-f+\u6bd4\u57fa\u7ebf\u63d0\u53475.71\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4\u6807\u51c6DPO\u63d0\u53473.30\u4e2a\u767e\u5206\u70b9\uff1b\u5728SWE-bench Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u95ee\u9898\u89e3\u51b3\u7387\u6bd4DPO\u9ad81.67\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4\u57fa\u7ebf\u9ad84.67\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "DPO-f+\u901a\u8fc7\u66f4\u7d27\u5bc6\u5730\u5bf9\u9f50\u53cd\u9988\u4e0e\u5f00\u53d1\u8005\u9700\u6c42\uff0c\u5c06LLM\u8f85\u52a9\u4fee\u590d\u4ece\u4e00\u6b21\u6027\u8f93\u51fa\u8f6c\u53d8\u4e3a\u534f\u4f5c\u7406\u89e3\u5de5\u4f5c\u6d41\uff0c\u589e\u5f3a\u4e86\u4ee3\u7801\u7406\u89e3\u548c\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002", "topic": "swe application"}}
{"id": "2511.01047", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01047", "abs": "https://arxiv.org/abs/2511.01047", "authors": ["Yu Shi", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "title": "HAFixAgent: History-Aware Automated Program Repair Agent", "comment": "31 pages, 6 figures", "summary": "Automated program repair (APR) has recently shifted toward large language\nmodels and agent-based systems, yet most systems rely on local snapshot\ncontext, overlooking repository history. Prior work shows that repository\nhistory helps repair single-line bugs, since the last commit touching the buggy\nline is often the bug-introducing one. In this paper, we investigate whether\nrepository history can also improve agentic APR systems at scale, especially\nfor complex multi-hunk bugs. We present HAFixAgent, a History-Aware Bug-Fixing\nAgent that injects blame-derived repository heuristics into its repair loop. A\npreliminary study of all 854 real-world bugs from Defects4J motivates our\ndesign, showing that bug-relevant history is both widely available and highly\nconcentrated. Empirical comparison of HAFixAgent with two state-of-the-art\nbaselines shows: (1) Effectiveness: HAFixAgent significantly improves over the\nagent-based baseline (by 212.3%) and the multi-hunk baseline (by 29.9%). (2)\nEfficiency: history does not significantly increase agent steps and keeps token\ncosts comparable, with notably lower median costs for complex\nmulti-file-multi-hunk bugs. (3) Practicality: combining different historical\nheuristics repairs more bugs, offering a clear cost-benefit trade-off.\nHAFixAgent offers a practical recipe for history-aware agentic APR: ground the\nagent in version control history, prioritize diff-based historical context, and\nintegrate complementary heuristics when needed.", "AI": {"tldr": "HAFixAgent\u662f\u4e00\u4e2a\u5386\u53f2\u611f\u77e5\u7684bug\u4fee\u590d\u4ee3\u7406\uff0c\u901a\u8fc7\u6ce8\u5165\u57fa\u4e8e\u4ee3\u7801\u4ed3\u5e93\u5386\u53f2\u7684\u542f\u53d1\u5f0f\u4fe1\u606f\u6765\u6539\u8fdb\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\uff0c\u7279\u522b\u9488\u5bf9\u590d\u6742\u7684\u591a\u884cbug\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u672c\u5730\u5feb\u7167\u4e0a\u4e0b\u6587\uff0c\u5ffd\u7565\u4e86\u4ee3\u7801\u4ed3\u5e93\u5386\u53f2\u4fe1\u606f\u3002\u7814\u7a76\u8868\u660e\u4ed3\u5e93\u5386\u53f2\u6709\u52a9\u4e8e\u4fee\u590d\u5355\u884cbug\uff0c\u4f46\u5c1a\u672a\u5728\u57fa\u4e8e\u4ee3\u7406\u7684APR\u7cfb\u7edf\u4e2d\u5927\u89c4\u6a21\u9a8c\u8bc1\u5176\u5bf9\u590d\u6742\u591a\u884cbug\u7684\u6539\u8fdb\u6548\u679c\u3002", "method": "\u5f00\u53d1HAFixAgent\u4ee3\u7406\u7cfb\u7edf\uff0c\u5c06\u57fa\u4e8eblame\u7684\u4ed3\u5e93\u5386\u53f2\u542f\u53d1\u5f0f\u4fe1\u606f\u6ce8\u5165\u4fee\u590d\u5faa\u73af\u4e2d\u3002\u901a\u8fc7Defects4J\u4e2d854\u4e2a\u771f\u5b9ebug\u7684\u521d\u6b65\u7814\u7a76\u6307\u5bfc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e0d\u540c\u7684\u5386\u53f2\u542f\u53d1\u5f0f\u7b56\u7565\u3002", "result": "HAFixAgent\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u4ee3\u7406\u7684\u57fa\u7ebf\uff08\u63d0\u5347212.3%\uff09\u548c\u591a\u884c\u4fee\u590d\u57fa\u7ebf\uff08\u63d0\u534729.9%\uff09\u3002\u5386\u53f2\u4fe1\u606f\u4e0d\u4f1a\u663e\u8457\u589e\u52a0\u4ee3\u7406\u6b65\u9aa4\uff0c\u4fdd\u6301token\u6210\u672c\u53ef\u6bd4\uff0c\u5bf9\u590d\u6742\u591a\u6587\u4ef6\u591a\u884cbug\u7684\u4e2d\u4f4d\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "HAFixAgent\u4e3a\u5386\u53f2\u611f\u77e5\u7684\u4ee3\u7406\u5f0fAPR\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\uff1a\u5c06\u4ee3\u7406\u57fa\u4e8e\u7248\u672c\u63a7\u5236\u5386\u53f2\uff0c\u4f18\u5148\u4f7f\u7528\u57fa\u4e8ediff\u7684\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728\u9700\u8981\u65f6\u96c6\u6210\u4e92\u8865\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2511.00556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA.", "AI": {"tldr": "\u63d0\u51faISA\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u610f\u56fe\u8f6c\u6362\u4f7f\u6709\u5bb3\u8bf7\u6c42\u88abLLM\u8bef\u89e3\u4e3a\u826f\u6027\u4fe1\u606f\u8bf7\u6c42\uff0c\u76f8\u6bd4\u76f4\u63a5\u6709\u5bb3\u63d0\u793a\u653b\u51fb\u6210\u529f\u7387\u63d0\u534770%\u4ee5\u4e0a\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u5176\u65e0\u6548", "motivation": "\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u4e3b\u8981\u901a\u8fc7\u6dfb\u52a0\u5e72\u6270\u4e0a\u4e0b\u6587\u6216\u5bf9\u6297\u6027token\u6765\u5206\u6563LLM\u6ce8\u610f\u529b\uff0c\u4f46\u672a\u6539\u53d8\u6838\u5fc3\u6709\u5bb3\u610f\u56fe\u3002\u9700\u8981\u7814\u7a76\u66f4\u9690\u853d\u7684\u653b\u51fb\u65b9\u5f0f\u4ee5\u589e\u5f3aLLM\u5b89\u5168\u6027", "method": "\u5efa\u7acb\u610f\u56fe\u8f6c\u6362\u5206\u7c7b\u6cd5\uff0c\u5bf9\u539f\u59cb\u8bf7\u6c42\u8fdb\u884c\u6700\u5c0f\u7f16\u8f91\uff0c\u751f\u6210\u81ea\u7136\u3001\u4eba\u7c7b\u53ef\u8bfb\u4e14\u770b\u4f3c\u65e0\u5bb3\u7684\u63d0\u793a\uff0c\u4f7fLLM\u8bef\u5224\u653b\u51fb\u610f\u56fe", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aLLM\u4e0a\uff0cISA\u653b\u51fb\u6210\u529f\u7387\u76f8\u6bd4\u76f4\u63a5\u6709\u5bb3\u63d0\u793a\u63d0\u534770%\u4ee5\u4e0a\uff1b\u4ec5\u4f7f\u7528ISA\u6a21\u677f\u91cd\u65b0\u8868\u8ff0\u7684\u826f\u6027\u6570\u636e\u5fae\u8c03\u6a21\u578b\u53ef\u4f7f\u6210\u529f\u7387\u63a5\u8fd1100%", "conclusion": "LLM\u5728\u610f\u56fe\u63a8\u65ad\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5b89\u5168\u6311\u6218\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9ISA\u653b\u51fb\u65e0\u6548\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565", "topic": "agent analysis"}}
{"id": "2511.01104", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01104", "abs": "https://arxiv.org/abs/2511.01104", "authors": ["Yujian Liu", "Jiabao Ji", "Yang Zhang", "Wenbo Guo", "Tommi Jaakkola", "Shiyu Chang"], "title": "HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning", "comment": null, "summary": "Existing LLM-based automatic test generation methods mainly produce input and\nexpected output pairs to categorize the intended behavior of correct programs.\nAlthough straightforward, these methods have limited diversity in generated\ntests and cannot provide enough debugging information. We propose HarnessLLM, a\ntwo-stage training pipeline that enables LLMs to write harness code for\ntesting. Particularly, LLMs generate code that synthesizes inputs and validates\nthe observed outputs, allowing complex test cases and flexible output\nvalidation such as invariant checking. To achieve this, we train LLMs with SFT\nfollowed by RLVR with a customized reward design. Experiments show that\nHarnessLLM outperforms input-output-based testing in bug finding and testing\nstrategy diversity. HarnessLLM further benefits the code generation performance\nthrough test-time scaling with our generated test cases as inference-phase\nvalidation. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/HarnessLLM.git.", "AI": {"tldr": "HarnessLLM\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u4f7fLLM\u80fd\u591f\u7f16\u5199\u6d4b\u8bd5\u7528\u7684harness\u4ee3\u7801\uff0c\u751f\u6210\u5408\u6210\u8f93\u5165\u548c\u9a8c\u8bc1\u8f93\u51fa\u7684\u4ee3\u7801\uff0c\u652f\u6301\u590d\u6742\u6d4b\u8bd5\u7528\u4f8b\u548c\u7075\u6d3b\u7684\u8f93\u51fa\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4ea7\u751f\u8f93\u5165-\u8f93\u51fa\u5bf9\uff0c\u6d4b\u8bd5\u591a\u6837\u6027\u6709\u9650\u4e14\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u8c03\u8bd5\u4fe1\u606f\u3002", "method": "\u91c7\u7528SFT\u540e\u63a5RLVR\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff0c\u4f7f\u7528\u5b9a\u5236\u5316\u5956\u52b1\u8bbe\u8ba1\uff0c\u4f7fLLM\u751f\u6210\u6d4b\u8bd5harness\u4ee3\u7801\u3002", "result": "HarnessLLM\u5728\u9519\u8bef\u53d1\u73b0\u548c\u6d4b\u8bd5\u7b56\u7565\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u8f93\u5165-\u8f93\u51fa\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u3002", "conclusion": "HarnessLLM\u80fd\u591f\u751f\u6210\u66f4\u590d\u6742\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u6d4b\u8bd5\u8986\u76d6\u548c\u8c03\u8bd5\u4fe1\u606f\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2511.01176", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01176", "abs": "https://arxiv.org/abs/2511.01176", "authors": ["Wenqing Zhu", "Norihiro Yoshida", "Eunjong Choi", "Yutaka Matsubara", "Hiroaki Takada"], "title": "An Empirical Study of LLM-Based Code Clone Detection", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious software engineering tasks, such as code generation and debugging,\nbecause of their ability to translate between programming languages and natural\nlanguages. Existing studies have demonstrated the effectiveness of LLMs in code\nclone detection. However, two crucial issues remain unaddressed: the ability of\nLLMs to achieve comparable performance across different datasets and the\nconsistency of LLMs' responses in code clone detection. To address these\nissues, we constructed seven code clone datasets and then evaluated five LLMs\nin four existing prompts with these datasets. The datasets were created by\nsampling code pairs using their Levenshtein ratio from two different code\ncollections, CodeNet and BigCloneBench. Our evaluation revealed that although\nLLMs perform well in CodeNet-related datasets, with o3-mini achieving a 0.943\nF1 score, their performance significantly decreased in BigCloneBench-related\ndatasets. Most models achieved a high response consistency, with over 90\\% of\njudgments remaining consistent across all five submissions. The fluctuations of\nthe F1 score affected by inconsistency are also tiny; their variations are less\nthan 0.03.", "AI": {"tldr": "\u8bc4\u4f305\u4e2aLLM\u57287\u4e2a\u4ee3\u7801\u514b\u9686\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0LLM\u5728CodeNet\u6570\u636e\u96c6\u8868\u73b0\u4f18\u5f02\uff08o3-mini\u8fbe\u52300.943 F1\u5206\u6570\uff09\uff0c\u4f46\u5728BigCloneBench\u6570\u636e\u96c6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u540c\u65f6LLM\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u5177\u6709\u9ad8\u54cd\u5e94\u4e00\u81f4\u6027\uff08\u8d85\u8fc790%\u7684\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\uff09\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8bc1\u660e\u4e86LLM\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f46\u672a\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1aLLM\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u6bd4\u6027\u80fd\u8868\u73b0\uff0c\u4ee5\u53caLLM\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u7684\u54cd\u5e94\u4e00\u81f4\u6027\u3002", "method": "\u6784\u5efa\u4e867\u4e2a\u4ee3\u7801\u514b\u9686\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4eceCodeNet\u548cBigCloneBench\u4e24\u4e2a\u4ee3\u7801\u96c6\u5408\u4e2d\u91c7\u6837\u4ee3\u7801\u5bf9\uff08\u4f7f\u7528Levenshtein\u6bd4\u7387\uff09\uff0c\u7136\u540e\u4f7f\u75284\u79cd\u73b0\u6709\u63d0\u793a\u8bc4\u4f305\u4e2aLLM\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "LLM\u5728CodeNet\u76f8\u5173\u6570\u636e\u96c6\u8868\u73b0\u826f\u597d\uff08o3-mini\u8fbe\u52300.943 F1\u5206\u6570\uff09\uff0c\u4f46\u5728BigCloneBench\u76f8\u5173\u6570\u636e\u96c6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u5927\u591a\u6570\u6a21\u578b\u5177\u6709\u9ad8\u54cd\u5e94\u4e00\u81f4\u6027\uff08\u8d85\u8fc790%\u7684\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\uff09\uff0cF1\u5206\u6570\u53d7\u4e0d\u4e00\u81f4\u6027\u5f71\u54cd\u7684\u6ce2\u52a8\u5f88\u5c0f\uff08\u53d8\u5316\u5c0f\u4e8e0.03\uff09\u3002", "conclusion": "LLM\u5728\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4f46\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u540c\u65f6LLM\u5177\u6709\u9ad8\u54cd\u5e94\u4e00\u81f4\u6027\uff0c\u4e0d\u4e00\u81f4\u6027\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u5f88\u5c0f\u3002", "topic": "code agent"}}
{"id": "2511.00602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00602", "abs": "https://arxiv.org/abs/2511.00602", "authors": ["Wai-Chung Kwan", "Joshua Ong Jun Leang", "Pavlos Vougiouklis", "Jeff Z. Pan", "Marco Valentino", "Pasquale Minervini"], "title": "OpenSIR: Open-Ended Self-Improving Reasoner", "comment": null, "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics.", "AI": {"tldr": "OpenSIR\u662f\u4e00\u4e2a\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u5b66\u4e60\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9LLM\u5728\u6559\u5e08\u548c\u5b66\u751f\u89d2\u8272\u95f4\u5207\u6362\u6765\u751f\u6210\u548c\u89e3\u51b3\u65b0\u9896\u95ee\u9898\uff0c\u5b9e\u73b0\u5f00\u653e\u5f0f\u7684\u6570\u5b66\u53d1\u73b0\u3002", "motivation": "\u73b0\u6709\u7684LLM\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u8fdb\u884c\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u8d85\u8d8a\u4eba\u7c7b\u6c34\u5e73\u7684\u80fd\u529b\u3002\u81ea\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u65e0\u6cd5\u8fdb\u884c\u5f00\u653e\u5f0f\u5b66\u4e60\u3002", "method": "OpenSIR\u6846\u67b6\u8ba9LLM\u4ea4\u66ff\u626e\u6f14\u6559\u5e08\u548c\u5b66\u751f\u89d2\u8272\uff1a\u6559\u5e08\u751f\u6210\u65b0\u9896\u95ee\u9898\uff08\u4f18\u5316\u96be\u5ea6\u548c\u591a\u6837\u6027\uff09\uff0c\u5b66\u751f\u89e3\u51b3\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "result": "\u4ece\u5355\u4e2a\u7b80\u5355\u79cd\u5b50\u95ee\u9898\u5f00\u59cb\uff0cOpenSIR\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u6a21\u578b\u6027\u80fd\uff1aLlama-3.2-3B-Instruct\u5728GSM8K\u4e0a\u4ece73.9\u63d0\u5347\u523078.3\uff0c\u5728College Math\u4e0a\u4ece28.8\u63d0\u5347\u523034.4\uff1bGemma-2-2B-Instruct\u5728GSM8K\u4e0a\u4ece38.5\u63d0\u5347\u523058.7\u3002", "conclusion": "OpenSIR\u901a\u8fc7\u5171\u540c\u8fdb\u5316\u7684\u6559\u5e08-\u5b66\u751f\u89d2\u8272\u5b9e\u73b0\u5f00\u653e\u5f0f\u5b66\u4e60\uff0c\u81ea\u9002\u5e94\u5730\u6821\u51c6\u96be\u5ea6\u5e76\u63a8\u52a8\u591a\u6837\u5316\u63a2\u7d22\uff0c\u80fd\u591f\u81ea\u4e3b\u5730\u4ece\u57fa\u7840\u6570\u5b66\u53d1\u5c55\u5230\u9ad8\u7ea7\u6570\u5b66\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark.", "AI": {"tldr": "\u6bd4\u8f83\u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\u3001\u9884\u8bad\u7ec3\u5e8f\u5217\u53d8\u6362\u5668\u548c\u6df7\u5408\u4ee3\u7406LLM\u7ba1\u9053\u5728\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u548c\u6df7\u5408\u4ee3\u7406\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u66f4\u7b80\u5355\u4e14\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u3002", "motivation": "\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5bf9\u4e34\u5e8a\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u57fa\u4e8e\u8ba1\u6570\u7684\u5b66\u4e60\u5668\u4e0e\u65b0\u5174\u6df7\u5408\u4ee3\u7406LLM\u7ba1\u9053\u7684\u76f4\u63a5\u57fa\u51c6\u6bd4\u8f83\uff0c\u540e\u8005\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5355\u4e00LLM\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528EHRSHOT\u6570\u636e\u96c6\u8bc4\u4f30\u4e09\u7c7b\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\uff08LightGBM\u548cTabPFN\uff09\u3001\u9884\u8bad\u7ec3\u5e8f\u5217\u53d8\u6362\u5668\uff08CLMBR\uff09\u548c\u6df7\u5408\u4ee3\u7406\u7ba1\u9053\uff08\u5c06\u8868\u683c\u5386\u53f2\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u6458\u8981\u540e\u4f7f\u7528\u6587\u672c\u5206\u7c7b\u5668\uff09\u3002", "result": "\u5728\u516b\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u548c\u6df7\u5408\u4ee3\u7406\u65b9\u6cd5\u7684\u80dc\u7387\u5927\u81f4\u76f8\u5f53\u3002", "conclusion": "\u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\u56e0\u5176\u7b80\u5355\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ecd\u7136\u662f\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u529b\u5019\u9009\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2511.00808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06RLVR LLM\u8bad\u7ec3\u5e94\u7528\u4e8e\u516c\u5171\u4ea4\u901a\u8fd0\u8425\u4e2d\u7684\u5b9e\u65f6\u9884\u6d4b\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bb9\u5dee\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728NYC MTA\u670d\u52a1\u8b66\u62a5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u76845\u5206\u949f\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u9884\u6d4b\u516c\u5171\u4ea4\u901a\u4e8b\u4ef6\u6301\u7eed\u65f6\u95f4\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u96be\u4ee5\u5904\u7406\u9886\u57df\u7a00\u758f\u6027\u548c\u566a\u58f0\u8fde\u7eed\u6807\u7b7e\u95ee\u9898\uff0c\u800cRLVR\u5728\u6570\u5b66\u7b49\u4e8c\u5143\u6b63\u786e\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u566a\u58f0\u8fde\u7eed\u9884\u6d4b\u4e2d\u7684\u9002\u7528\u6027\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bb9\u5dee\u7684\u6210\u5f62\u5956\u52b1\u51fd\u6570\uff0c\u5728\u8fde\u7eed\u8bef\u5dee\u8303\u56f4\u5185\u7ed9\u4e88\u90e8\u5206\u4fe1\u7528\uff0c\u800c\u4e0d\u662f\u8981\u6c42\u5355\u4e00\u6b63\u786e\u7b54\u6848\uff0c\u5c06RLVR\u9002\u5e94\u4e8e\u8be5\u4efb\u52a1\uff0c\u5e76\u5728NYC MTA\u670d\u52a1\u8b66\u62a5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u901a\u7528\u6307\u4ee4\u8c03\u4f18LLM\u663e\u8457\u4f18\u4e8e\u4e13\u4e1a\u6570\u5b66\u63a8\u7406\u6a21\u578b\uff0c\u6210\u5f62\u5956\u52b1\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0cRLVR\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u57285\u5206\u949f\u51c6\u786e\u7387\u4e0a\u5b9e\u73b0\u4e8635%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "RLVR\u53ef\u4ee5\u6210\u529f\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u566a\u58f0\u9884\u6d4b\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u8bbe\u8ba1\u53cd\u6620\u95ee\u9898\u8fde\u7eed\u6027\u8d28\u7684\u9a8c\u8bc1\u5668\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00086", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.00086", "abs": "https://arxiv.org/abs/2511.00086", "authors": ["Fali Wang", "Jihai Chen", "Shuhua Yang", "Runxue Bao", "Tianxiang Zhao", "Zhiwei Zhang", "Xianfeng Tang", "Hui Liu", "Qi He", "Suhang Wang"], "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph", "comment": "Under review", "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent-REINFORCE\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u641c\u7d22\u6700\u4f18\u7684\u591aLLM\u534f\u4f5c\u56fe\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u7684\u534f\u4f5c\u67b6\u6784\u548c\u5355\u4e00\u6a21\u578b\u4f7f\u7528\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u4efb\u52a1\u53ef\u80fd\u9700\u8981\u4e0d\u540c\u7684\u6700\u4f18\u67b6\u6784\u548c\u6a21\u578b\u7ec4\u5408\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u56fe\u4f18\u5316\uff0c\u63d0\u51faAgent-REINFORCE\u6846\u67b6\uff0c\u5c06REINFORCE\u6d41\u7a0b\u6620\u5c04\u4e3a\u91c7\u6837-\u53cd\u9988-\u66f4\u65b0\uff0c\u5176\u4e2d\u53cd\u9988\u4f5c\u4e3a\u6587\u672c\u68af\u5ea6\u6765\u66f4\u65b0\u6982\u7387\u56fe\u3002", "result": "Agent-REINFORCE\u5728\u6837\u672c\u6548\u7387\u548c\u641c\u7d22\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u5ef6\u8fdf\u8054\u5408\u76ee\u6807\u4e0b\u7684\u6700\u4f18\u56fe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u641c\u7d22\u8ba1\u7b97\u6700\u4f18\u7684\u591aLLM\u534f\u4f5c\u56fe\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00993", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7LLM\u65c5\u884c\u8005\u667a\u80fd\u4f53\u548c\u6821\u51c6\u667a\u80fd\u4f53\u7684\u534f\u4f5c\uff0c\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u65c5\u884c\u8005\u5728\u5b66\u4e60\u9002\u5e94\u884c\u4e3a\u4e0a\u7684\u6301\u7eed\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u884c\u4e3a\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u65c5\u884c\u8005\u5982\u4f55\u4ece\u4e0e\u4ea4\u901a\u7cfb\u7edf\u4ea4\u4e92\u4e2d\u5b66\u4e60\u548c\u8c03\u6574\u884c\u4e3a\u5bf9\u7cfb\u7edf\u8bc4\u4f30\u548c\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u8ba4\u77e5\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u914d\u5907\u8bb0\u5fc6\u7cfb\u7edf\u548c\u53ef\u5b66\u4e60\u89d2\u8272\u7684LLM\u65c5\u884c\u8005\u667a\u80fd\u4f53\u6a21\u62df\u4eba\u7c7b\u65c5\u884c\u8005\uff0c\u5f15\u5165LLM\u6821\u51c6\u667a\u80fd\u4f53\u5229\u7528\u63a8\u7406\u5206\u6790\u80fd\u529b\u8bad\u7ec3\u89d2\u8272\uff0c\u5f62\u6210\u53cc\u667a\u80fd\u4f53\u7cfb\u7edf\u8ddf\u8e2a\u5bf9\u9f50\u51b3\u7b56\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u65e5\u5e38\u8def\u7ebf\u9009\u62e9\u5b9e\u9a8c\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2a\u4f53\u884c\u4e3a\u5bf9\u9f50\u548c\u805a\u5408\u6a21\u62df\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u65b9\u6cd5\uff0c\u5e76\u80fd\u6355\u6349\u5e95\u5c42\u5b66\u4e60\u8fc7\u7a0b\u7684\u6f14\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u521b\u5efa\u9002\u5e94\u6027\u5f3a\u3001\u884c\u4e3a\u771f\u5b9e\u7684\u667a\u80fd\u4f53\u6765\u6a21\u62df\u65c5\u884c\u8005\u5b66\u4e60\u548c\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u4ea4\u901a\u6a21\u62df\u548c\u653f\u7b56\u5206\u6790\u3002", "topic": "agent analysis"}}
{"id": "2511.01348", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01348", "abs": "https://arxiv.org/abs/2511.01348", "authors": ["Robin Gr\u00f6pler", "Steffen Klepke", "Jack Johns", "Andreas Dreschinski", "Klaus Schmid", "Benedikt Dornauer", "Eray T\u00fcz\u00fcn", "Joost Noppen", "Mohammad Reza Mousavi", "Yongjian Tang", "Johannes Viehmann", "Selin \u015eirin Aslang\u00fcl", "Beum Seuk Lee", "Adam Ziolkowski", "Eric Zie"], "title": "The Future of Generative AI in Software Engineering: A Vision from Industry and Academia in the European GENIUS Project", "comment": "Submitted to 2nd IEEE/ACM International Conference on AI-powered\n  Software (AIware 2025)", "summary": "Generative AI (GenAI) has recently emerged as a groundbreaking force in\nSoftware Engineering, capable of generating code, suggesting fixes, and\nsupporting quality assurance. While its use in coding tasks shows considerable\npromise, applying GenAI across the entire Software Development Life Cycle\n(SDLC) has not yet been fully explored. Critical uncertainties in areas such as\nreliability, accountability, security, and data privacy demand deeper\ninvestigation and coordinated action. The GENIUS project, comprising over 30\nEuropean industrial and academic partners, aims to address these challenges by\nadvancing AI integration across all SDLC phases. It focuses on GenAI's\npotential, the development of innovative tools, and emerging research\nchallenges, actively shaping the future of software engineering. This vision\npaper presents a shared perspective on the future of GenAI-based software\nengineering, grounded in cross-sector dialogue and experience within the GENIUS\nconsortium, supported by an exploratory literature review. The paper explores\nfour central elements: (1) a structured overview of current challenges in GenAI\nadoption across the SDLC; (2) a forward-looking vision outlining key\ntechnological and methodological advances expected over the next five years;\n(3) anticipated shifts in the roles and required skill sets of software\nprofessionals; and (4) the contribution of GENIUS in realizing this\ntransformation through practical tools and industrial validation. By aligning\ntechnical innovation with business relevance, this paper aims to inform both\nresearch agendas and industrial strategies, providing a foundation for\nreliable, scalable, and industry-ready GenAI solutions for software engineering\nteams.", "AI": {"tldr": "GENIUS\u9879\u76ee\u901a\u8fc7\u6574\u540830\u591a\u4e2a\u6b27\u6d32\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u5408\u4f5c\u4f19\u4f34\uff0c\u65e8\u5728\u89e3\u51b3\u751f\u6210\u5f0fAI\u5728\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u5e94\u7528\u7684\u6311\u6218\uff0c\u5305\u62ec\u53ef\u9760\u6027\u3001\u95ee\u8d23\u5236\u3001\u5b89\u5168\u6027\u548c\u6570\u636e\u9690\u79c1\u7b49\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u6574\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u53ef\u9760\u6027\u3001\u95ee\u8d23\u5236\u3001\u5b89\u5168\u6027\u548c\u6570\u636e\u9690\u79c1\u7b49\u5173\u952e\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u8de8\u90e8\u95e8\u5bf9\u8bdd\u548cGENIUS\u8054\u76df\u5185\u7684\u7ecf\u9a8c\uff0c\u7ed3\u5408\u63a2\u7d22\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u63d0\u51fa\u672a\u6765\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u613f\u666f\uff0c\u5305\u62ec\u6280\u672f\u65b9\u6cd5\u521b\u65b0\u548c\u5de5\u5177\u5f00\u53d1\u3002", "result": "\u63d0\u51fa\u4e86\u56db\u4e2a\u6838\u5fc3\u8981\u7d20\uff1a\u5f53\u524d\u6311\u6218\u7684\u7ed3\u6784\u5316\u6982\u8ff0\u3001\u672a\u6765\u4e94\u5e74\u7684\u524d\u77bb\u6027\u613f\u666f\u3001\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u89d2\u8272\u548c\u6280\u80fd\u96c6\u7684\u9884\u671f\u53d8\u5316\uff0c\u4ee5\u53caGENIUS\u9879\u76ee\u5728\u5b9e\u73b0\u8fd9\u4e00\u8f6c\u578b\u4e2d\u7684\u8d21\u732e\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6280\u672f\u521b\u65b0\u4e0e\u4e1a\u52a1\u76f8\u5173\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u6218\u7565\u63d0\u4f9b\u4fe1\u606f\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u6269\u5c55\u4e14\u884c\u4e1a\u5c31\u7eea\u7684\u751f\u6210\u5f0fAI\u89e3\u51b3\u65b9\u6848\u5960\u5b9a\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2511.00879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u77e5\u8bc6\u57fa\u7840\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u77e5\u8bc6\u5e93\u3001\u77e5\u8bc6\u57fa\u7840\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6a21\u578b\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u63a8\u7406\u4e2d\u7684\u77e5\u8bc6\u7f3a\u5931\u6216\u8bef\u7528\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u5206\u6b65\u63a8\u7406\u4e2d\u5982\u4f55\u9a8c\u8bc1\u63a8\u7406\u662f\u5426\u51c6\u786e\u57fa\u4e8e\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u4e2d\u95f4\u63a8\u7406\u7684\u77e5\u8bc6\u57fa\u7840\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u7684\u8bc4\u4f30\u5957\u4ef6\uff1a(1)\u4e3b\u8981\u77e5\u8bc6\u6536\u96c6\u5e93\uff0c(2)\u77e5\u8bc6\u57fa\u7840\u8bc4\u4f30\u6307\u6807\uff0c(3)\u8f7b\u91cf\u7ea7\u8bc4\u4f30LLM\u7528\u4e8e\u8ba1\u7b97\u6307\u6807\u3002", "result": "\u8bc4\u4f30\u5957\u4ef6\u80fd\u6709\u6548\u8bc6\u522b\u63a8\u7406\u4e2d\u7f3a\u5931\u6216\u8bef\u7528\u7684\u77e5\u8bc6\u5143\u7d20\uff0c\u63ed\u793aLLM\u7684\u57fa\u672c\u63a8\u7406\u7f3a\u9677\u3002", "conclusion": "\u77e5\u8bc6\u57fa\u7840\u8bc4\u4f30\u4e0d\u4ec5\u53ef\u7528\u4e8e\u8bc4\u4f30\uff0c\u8fd8\u53ef\u96c6\u6210\u5230\u504f\u597d\u4f18\u5316\u4e2d\uff0c\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2511.01149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u534f\u4f5c\u673a\u5236\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u6267\u884c\u95ee\u9898", "motivation": "\u89e3\u51b3\u5355\u4e00\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u6267\u884c\u4e2d\u4efb\u52a1\u5206\u89e3\u548c\u534f\u4f5c\u7684\u5c40\u9650\u6027", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u8f6c\u6362\u4e3a\u7edf\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u5f15\u5165\u6a21\u5757\u5316\u5206\u89e3\u673a\u5236\u5c06\u6574\u4f53\u76ee\u6807\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u52a8\u6001\u8c03\u5ea6\u548c\u8def\u7531\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u5408\u7406\u5206\u5de5\u548c\u5b9e\u65f6\u534f\u4f5c", "result": "\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5206\u89e3\u6548\u7387\u3001\u5b50\u4efb\u52a1\u8986\u76d6\u7387\u548c\u534f\u4f5c\u5e73\u8861\u7b49\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u6574\u4f53\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8bc1\u660e\u4e86\u8bed\u8a00\u9a71\u52a8\u7684\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u534f\u4f5c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848", "topic": "agent analysis"}}
{"id": "2511.01757", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.01757", "abs": "https://arxiv.org/abs/2511.01757", "authors": ["Shamse Tasnim Cynthia", "Banani Roy"], "title": "Towards LLM-Powered Task-Aware Retrieval of Scientific Workflows for Galaxy", "comment": null, "summary": "Scientific Workflow Management Systems (SWfMSs) such as Galaxy have become\nessential infrastructure in bioinformatics, supporting the design, execution,\nand sharing of complex multi-step analyses. Despite hosting hundreds of\nreusable workflows across domains, Galaxy's current keyword-based retrieval\nsystem offers limited support for semantic query interpretation and often fails\nto surface relevant workflows when exact term matches are absent. To address\nthis gap, we propose a task-aware, two-stage retrieval framework that\nintegrates dense vector search with large language model (LLM)-based reranking.\nOur system first retrieves candidate workflows using state-of-the-art embedding\nmodels and then reranks them using instruction-tuned generative LLMs (GPT-4o,\nMistral-7B) based on semantic task alignment. To support robust evaluation, we\nconstruct a benchmark dataset of Galaxy workflows annotated with semantic\ntopics via BERTopic and synthesize realistic task-oriented queries using LLMs.\nWe conduct a comprehensive comparison of lexical, dense, and reranking models\nusing standard IR metrics, presenting the first systematic evaluation of\nretrieval performance in the Galaxy ecosystem. Results show that our approach\nsignificantly improves top-k accuracy and relevance, particularly for long or\nunder-specified queries. We further integrate our system as a prototype tool\nwithin Galaxy, providing a proof-of-concept for LLM-enhanced workflow search.\nThis work advances the usability and accessibility of scientific workflows,\nespecially for novice users and interdisciplinary researchers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff0c\u7ed3\u5408\u5bc6\u96c6\u5411\u91cf\u641c\u7d22\u548cLLM\u91cd\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347Galaxy\u79d1\u5b66\u5de5\u4f5c\u6d41\u7ba1\u7406\u7cfb\u7edf\u4e2d\u5de5\u4f5c\u6d41\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "Galaxy\u73b0\u6709\u7684\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u68c0\u7d22\u7cfb\u7edf\u5728\u8bed\u4e49\u67e5\u8be2\u89e3\u91ca\u65b9\u9762\u652f\u6301\u6709\u9650\uff0c\u5f53\u7f3a\u5c11\u7cbe\u786e\u672f\u8bed\u5339\u914d\u65f6\u5f80\u5f80\u65e0\u6cd5\u627e\u5230\u76f8\u5173\u5de5\u4f5c\u6d41\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u611f\u77e5\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u6a21\u578b\u68c0\u7d22\u5019\u9009\u5de5\u4f5c\u6d41\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u751f\u6210\u5f0fLLM\uff08GPT-4o\u3001Mistral-7B\uff09\u57fa\u4e8e\u8bed\u4e49\u4efb\u52a1\u5bf9\u9f50\u8fdb\u884c\u91cd\u6392\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86top-k\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u67e5\u8be2\u6216\u672a\u5145\u5206\u6307\u5b9a\u7684\u67e5\u8be2\u3002\u6784\u5efa\u4e86\u5e26\u8bed\u4e49\u4e3b\u9898\u6ce8\u91ca\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u68c0\u7d22\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7LLM\u589e\u5f3a\u7684\u5de5\u4f5c\u6d41\u641c\u7d22\u63d0\u9ad8\u4e86\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u53ef\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u7279\u522b\u662f\u5bf9\u65b0\u624b\u7528\u6237\u548c\u8de8\u5b66\u79d1\u7814\u7a76\u4eba\u5458\u3002", "topic": "swe application"}}
{"id": "2511.01850", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01850", "abs": "https://arxiv.org/abs/2511.01850", "authors": ["Jiawei Jin", "Yingxin Su", "Xiaotong Zhu"], "title": "SmartMLOps Studio: Design of an LLM-Integrated IDE with Automated MLOps Pipelines for Model Development and Monitoring", "comment": null, "summary": "The rapid expansion of artificial intelligence and machine learning (ML)\napplications has intensified the demand for integrated environments that unify\nmodel development, deployment, and monitoring. Traditional Integrated\nDevelopment Environments (IDEs) focus primarily on code authoring, lacking\nintelligent support for the full ML lifecycle, while existing MLOps platforms\nremain detached from the coding workflow. To address this gap, this study\nproposes the design of an LLM-Integrated IDE with automated MLOps pipelines\nthat enables continuous model development and monitoring within a single\nenvironment. The proposed system embeds a Large Language Model (LLM) assistant\ncapable of code generation, debugging recommendation, and automatic pipeline\nconfiguration. The backend incorporates automated data validation, feature\nstorage, drift detection, retraining triggers, and CI/CD deployment\norchestration. This framework was implemented in a prototype named SmartMLOps\nStudio and evaluated using classification and forecasting tasks on the UCI\nAdult and M5 datasets. Experimental results demonstrate that SmartMLOps Studio\nreduces pipeline configuration time by 61%, improves experiment reproducibility\nby 45%, and increases drift detection accuracy by 14% compared to traditional\nworkflows. By bridging intelligent code assistance and automated operational\npipelines, this research establishes a novel paradigm for AI engineering -\ntransforming the IDE from a static coding tool into a dynamic, lifecycle-aware\nintelligent platform for scalable and efficient model development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u96c6\u6210\u7684IDE\uff0c\u901a\u8fc7\u81ea\u52a8\u5316MLOps\u6d41\u6c34\u7ebf\u5728\u5355\u4e00\u73af\u5883\u4e2d\u5b9e\u73b0\u6301\u7eed\u6a21\u578b\u5f00\u53d1\u548c\u76d1\u63a7\uff0c\u663e\u8457\u63d0\u5347\u4e86ML\u5de5\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edfIDE\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u7f16\u5199\uff0c\u7f3a\u4e4f\u5bf9\u5b8c\u6574ML\u751f\u547d\u5468\u671f\u7684\u667a\u80fd\u652f\u6301\uff0c\u800c\u73b0\u6709MLOps\u5e73\u53f0\u53c8\u4e0e\u7f16\u7801\u5de5\u4f5c\u6d41\u8131\u8282\uff0c\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u96c6\u6210LLM\u52a9\u624b\u7684IDE\uff0c\u5177\u5907\u4ee3\u7801\u751f\u6210\u3001\u8c03\u8bd5\u63a8\u8350\u548c\u81ea\u52a8\u6d41\u6c34\u7ebf\u914d\u7f6e\u529f\u80fd\uff0c\u540e\u7aef\u5305\u542b\u81ea\u52a8\u5316\u6570\u636e\u9a8c\u8bc1\u3001\u7279\u5f81\u5b58\u50a8\u3001\u6f02\u79fb\u68c0\u6d4b\u3001\u91cd\u8bad\u7ec3\u89e6\u53d1\u548cCI/CD\u90e8\u7f72\u7f16\u6392\u3002", "result": "\u5728UCI Adult\u548cM5\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSmartMLOps Studio\u76f8\u6bd4\u4f20\u7edf\u5de5\u4f5c\u6d41\u51cf\u5c11\u4e8661%\u7684\u6d41\u6c34\u7ebf\u914d\u7f6e\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e8645%\u7684\u5b9e\u9a8c\u53ef\u91cd\u73b0\u6027\uff0c\u5e76\u63d0\u5347\u4e8614%\u7684\u6f02\u79fb\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u6865\u63a5\u667a\u80fd\u4ee3\u7801\u8f85\u52a9\u548c\u81ea\u52a8\u5316\u64cd\u4f5c\u6d41\u6c34\u7ebf\uff0c\u672c\u7814\u7a76\u4e3aAI\u5de5\u7a0b\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u5c06IDE\u4ece\u9759\u6001\u7f16\u7801\u5de5\u5177\u8f6c\u53d8\u4e3a\u52a8\u6001\u3001\u751f\u547d\u5468\u671f\u611f\u77e5\u7684\u667a\u80fd\u5e73\u53f0\u3002", "topic": "swe application"}}
{"id": "2511.01008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01008", "abs": "https://arxiv.org/abs/2511.01008", "authors": ["Haolin Yang", "Jipeng Zhang", "Zhitao He", "Yi R. Fung"], "title": "MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL", "comment": null, "summary": "Translating natural language to SQL remains difficult for complex queries.\nSuch queries often need environmental interaction and self-correction. To\naddress this, we introduce MARS-SQL, a novel multi-agent framework that\ncombines principled task decomposition and interactive reinforcement learning\n(RL). Our system comprises three specialized agents: a Grounding Agent for\nschema linking, a Generation Agent for query generation, and a Validation Agent\nfor final selection. The core of our framework is the Generation agent, which\nis trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe\nloop, the agent iteratively generates thoughts, executes SQL actions against a\nlive database, and revises its strategy based on execution feedback, enabling\ndynamic, stateful reasoning and self-correction. At inference time, we generate\nmultiple interaction trajectories to explore diverse reasoning paths. The\nValidation agent, then selects the optimal trajectory by modeling verification\nas a next-token prediction task and choosing the solution with the highest\ngeneration probability. This structured workflow pipelines specialized agents.\nIt combines interactive RL for generation with generative modeling for\nverification. The approach proves highly effective for robust and accurate SQL\ngeneration. Experiments show that MARS-SQL achieves state-of-the-art Execution\nAccuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our\ncode is available at https://github.com/YangHaolin0526/MARS-SQL.", "AI": {"tldr": "MARS-SQL\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u590d\u6742\u81ea\u7136\u8bed\u8a00\u5230SQL\u7684\u7ffb\u8bd1\u95ee\u9898\uff0c\u5728BIRD\u548cSpider\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6267\u884c\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5230SQL\u7ffb\u8bd1\u7684\u56f0\u96be\uff0c\u8fd9\u4e9b\u67e5\u8be2\u901a\u5e38\u9700\u8981\u73af\u5883\u4ea4\u4e92\u548c\u81ea\u6211\u4fee\u6b63\u3002", "method": "\u91c7\u7528\u4e09\u4ee3\u7406\u6846\u67b6\uff1a\u63a5\u5730\u4ee3\u7406\u8fdb\u884c\u6a21\u5f0f\u94fe\u63a5\uff0c\u751f\u6210\u4ee3\u7406\u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u751f\u6210\u67e5\u8be2\uff0c\u9a8c\u8bc1\u4ee3\u7406\u8fdb\u884c\u6700\u7ec8\u9009\u62e9\u3002\u751f\u6210\u4ee3\u7406\u4f7f\u7528ReAct\u98ce\u683c\u7684\u601d\u8003-\u884c\u52a8-\u89c2\u5bdf\u5faa\u73af\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5728BIRD\u5f00\u53d1\u96c6\u4e0a\u8fbe\u523077.84%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u5728Spider\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523089.75%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u7ed3\u5408\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u6210\u5efa\u6a21\u9a8c\u8bc1\uff0c\u4e3a\u7a33\u5065\u51c6\u786e\u7684SQL\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2511.01311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01311", "abs": "https://arxiv.org/abs/2511.01311", "authors": ["Filip Naudot", "Tobias Sundqvist", "Timotheus Kampik"], "title": "llmSHAP: A Principled Approach to LLM Explainability", "comment": null, "summary": "Feature attribution methods help make machine learning-based inference\nexplainable by determining how much one or several features have contributed to\na model's output. A particularly popular attribution method is based on the\nShapley value from cooperative game theory, a measure that guarantees the\nsatisfaction of several desirable principles, assuming deterministic inference.\nWe apply the Shapley value to feature attribution in large language model\n(LLM)-based decision support systems, where inference is, by design, stochastic\n(non-deterministic). We then demonstrate when we can and cannot guarantee\nShapley value principle satisfaction across different implementation variants\napplied to LLM-based decision support, and analyze how the stochastic nature of\nLLMs affects these guarantees. We also highlight trade-offs between explainable\ninference speed, agreement with exact Shapley value attributions, and principle\nattainment.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5c06Shapley\u503c\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u7279\u5f81\u5f52\u56e0\uff0c\u63a2\u8ba8\u4e86\u5728\u968f\u673a\u63a8\u7406\u73af\u5883\u4e0bShapley\u503c\u539f\u5219\u7684\u6ee1\u8db3\u60c5\u51b5\u53ca\u5176\u6743\u8861\u3002", "motivation": "\u7814\u7a76Shapley\u503c\u5728\u968f\u673a\u63a8\u7406\u7684LLM\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\uff0c\u56e0\u4e3a\u4f20\u7edfShapley\u503c\u5047\u8bbe\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u800cLLM\u63a8\u7406\u672c\u8d28\u4e0a\u662f\u968f\u673a\u7684\u3002", "method": "\u5c06Shapley\u503c\u5e94\u7528\u4e8eLLM\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u7279\u5f81\u5f52\u56e0\uff0c\u5206\u6790\u4e0d\u540c\u5b9e\u73b0\u53d8\u4f53\u4e0bShapley\u503c\u539f\u5219\u7684\u6ee1\u8db3\u60c5\u51b5\uff0c\u5e76\u7814\u7a76\u968f\u673a\u6027\u5bf9\u4fdd\u8bc1\u7684\u5f71\u54cd\u3002", "result": "\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5b9e\u73b0\u53d8\u4f53\u4e0bShapley\u503c\u539f\u5219\u7684\u6ee1\u8db3\u6761\u4ef6\uff0c\u5206\u6790\u4e86LLM\u968f\u673a\u6027\u5bf9\u8fd9\u4e9b\u4fdd\u8bc1\u7684\u5f71\u54cd\uff0c\u5e76\u63ed\u793a\u4e86\u53ef\u89e3\u91ca\u63a8\u7406\u901f\u5ea6\u3001\u4e0e\u7cbe\u786eShapley\u503c\u5f52\u56e0\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u539f\u5219\u8fbe\u6210\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u5728LLM\u968f\u673a\u63a8\u7406\u73af\u5883\u4e2d\uff0cShapley\u503c\u539f\u5219\u7684\u6ee1\u8db3\u9700\u8981\u7279\u5b9a\u6761\u4ef6\uff0c\u5b58\u5728\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u539f\u5219\u8fbe\u6210\u4e4b\u95f4\u7684\u91cd\u8981\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2511.01166", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed.", "AI": {"tldr": "\u63d0\u51fa\u4e86MicroRemed\u57fa\u51c6\u6d4b\u8bd5\u548cThinkRemed\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u7aef\u5230\u7aef\u5fae\u670d\u52a1\u4fee\u590d\u4e2d\u7684\u80fd\u529b\uff0c\u76f4\u63a5\u6839\u636e\u8bca\u65ad\u62a5\u544a\u751f\u6210\u53ef\u6267\u884c\u7684Ansible\u5267\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ecd\u4f9d\u8d56SRE\u624b\u5de5\u7f16\u5199\u63d0\u793a\uff0cLLM\u4ec5\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u9700\u8981\u63a8\u8fdbLLM\u5728\u5fae\u670d\u52a1\u4fee\u590d\u9886\u57df\u7684\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u7814\u7a76\u3002", "method": "\u5f15\u5165MicroRemed\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLM\u7aef\u5230\u7aef\u4fee\u590d\u80fd\u529b\uff0c\u63d0\u51faThinkRemed\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62dfSRE\u7684\u53cd\u601d\u548c\u611f\u77e5\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMicroRemed\u5bf9\u5f53\u524dLLM\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u800cThinkRemed\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u7cfb\u7edf\u53cd\u601d\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u4fee\u590d\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u5728\u5fae\u670d\u52a1\u4fee\u590d\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6548\u679c\u3002", "topic": "swe application"}}
{"id": "2511.01014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86IF-CRITIC\uff0c\u4e00\u4e2a\u80fd\u591f\u9ad8\u6548\u53ef\u9760\u8bc4\u4f30\u6307\u4ee4\u7ea6\u675f\u9075\u5faa\u7684LLM\u6279\u8bc4\u5668\uff0c\u901a\u8fc7\u7ea6\u675f\u6e05\u5355\u751f\u6210\u548c\u591a\u9636\u6bb5\u7b5b\u9009\u673a\u5236\u8bad\u7ec3\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u4e0a\u8d85\u8d8a\u73b0\u6709LLM-as-a-Judge\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u6a21\u578b\u5b58\u5728\u6210\u672c\u9ad8\u548c\u8bc4\u4f30\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u7ea6\u675f\u6e05\u5355\u751f\u6210\u5668\u5206\u89e3\u6307\u4ee4\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u673a\u5236\u6536\u96c6\u9ad8\u8d28\u91cf\u6279\u8bc4\u6570\u636e\uff0c\u91c7\u7528\u7ea6\u675f\u7ea7\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8bad\u7ec3IF-CRITIC\u3002", "result": "IF-CRITIC\u5728\u8bc4\u4f30\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86Deepseek-R1\u548co4-mini\u7b49\u5f3a\u57fa\u7ebf\uff0c\u80fd\u4ee5\u66f4\u4f4e\u8ba1\u7b97\u5f00\u9500\u4e3aLLM\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "conclusion": "IF-CRITIC\u80fd\u591f\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u6307\u4ee4\u9075\u5faa\u4f18\u5316\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2511.01016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1.", "AI": {"tldr": "Prompt-R1\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u578bLLM\u4e0e\u5927\u578bLLM\u534f\u4f5c\uff0c\u901a\u8fc7\u591a\u8f6e\u63d0\u793a\u4ea4\u4e92\u6765\u4f18\u5316\u590d\u6742\u95ee\u9898\u7684\u89e3\u51b3\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7528\u6237\u96be\u4ee5\u63d0\u4f9b\u51c6\u786e\u6709\u6548\u7684\u63d0\u793a\u6765\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\uff0c\u9650\u5236\u4e86LLM\u7684\u6027\u80fd\u53d1\u6325\u3002", "method": "\u91c7\u7528\u5c0f\u578bLLM\u601d\u8003\u751f\u6210\u63d0\u793a\uff0c\u5927\u578bLLM\u8fdb\u884c\u590d\u6742\u63a8\u7406\u7684\u591a\u8f6e\u4ea4\u4e92\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u53cc\u7ea6\u675f\u5956\u52b1\u673a\u5236\u4f18\u5316\u6b63\u786e\u6027\u3001\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPrompt-R1\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Prompt-R1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u652f\u6301\u4e0e\u5404\u79cd\u5927\u578bLLM\u7684\u63a8\u7406\u548c\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01415", "abs": "https://arxiv.org/abs/2511.01415", "authors": ["Amrapali Pednekar", "\u00c1lvaro Garrido-P\u00e9rez", "Yara Khaluf", "Pieter Simoens"], "title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "comment": "Accepted at CogInterp workshop @ NeurIPS 2025", "summary": "This study explores the interference in temporal processing within a\ndual-task paradigm from an artificial intelligence (AI) perspective. In this\ncontext, the dual-task setup is implemented as a simplified version of the\nOvercooked environment with two variations, single task (T) and dual task\n(T+N). Both variations involve an embedded time production task, but the dual\ntask (T+N) additionally involves a concurrent number comparison task. Two deep\nreinforcement learning (DRL) agents were separately trained for each of these\ntasks. These agents exhibited emergent behavior consistent with human timing\nresearch. Specifically, the dual task (T+N) agent exhibited significant\noverproduction of time relative to its single task (T) counterpart. This result\nwas consistent across four target durations. Preliminary analysis of neural\ndynamics in the agents' LSTM layers did not reveal any clear evidence of a\ndedicated or intrinsic timer. Hence, further investigation is needed to better\nunderstand the underlying time-keeping mechanisms of the agents and to provide\ninsights into the observed behavioral patterns. This study is a small step\ntowards exploring parallels between emergent DRL behavior and behavior observed\nin biological systems in order to facilitate a better understanding of both.", "AI": {"tldr": "\u7814\u7a76\u4eceAI\u89d2\u5ea6\u63a2\u7d22\u53cc\u4efb\u52a1\u8303\u5f0f\u4e2d\u7684\u65f6\u95f4\u5904\u7406\u5e72\u6270\uff0c\u4f7f\u7528\u7b80\u5316\u7684Overcooked\u73af\u5883\u6bd4\u8f83\u5355\u4efb\u52a1(T)\u548c\u53cc\u4efb\u52a1(T+N)\u4e0bDRL\u667a\u80fd\u4f53\u7684\u65f6\u95f4\u4ea7\u751f\u884c\u4e3a\uff0c\u53d1\u73b0\u53cc\u4efb\u52a1\u667a\u80fd\u4f53\u51fa\u73b0\u663e\u8457\u7684\u65f6\u95f4\u8fc7\u5ea6\u4ea7\u751f\u73b0\u8c61\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u53cc\u4efb\u52a1\u60c5\u5883\u4e0b\u8868\u73b0\u51fa\u7684\u65f6\u95f4\u5904\u7406\u884c\u4e3a\u662f\u5426\u4e0e\u4eba\u7c7b\u7814\u7a76\u4e00\u81f4\uff0c\u4ee5\u53ca\u667a\u80fd\u4f53\u5185\u90e8\u7684\u65f6\u95f4\u4fdd\u6301\u673a\u5236\u3002", "method": "\u5728\u7b80\u5316\u7684Overcooked\u73af\u5883\u4e2d\u5b9e\u73b0\u5355\u4efb\u52a1(T\uff0c\u4ec5\u65f6\u95f4\u4ea7\u751f)\u548c\u53cc\u4efb\u52a1(T+N\uff0c\u65f6\u95f4\u4ea7\u751f+\u6570\u5b57\u6bd4\u8f83)\u4e24\u79cd\u53d8\u4f53\uff0c\u5206\u522b\u8bad\u7ec3\u4e24\u4e2aDRL\u667a\u80fd\u4f53\uff0c\u5206\u6790\u5176\u884c\u4e3a\u8868\u73b0\u548cLSTM\u5c42\u7684\u795e\u7ecf\u52a8\u529b\u5b66\u3002", "result": "\u53cc\u4efb\u52a1(T+N)\u667a\u80fd\u4f53\u76f8\u5bf9\u4e8e\u5355\u4efb\u52a1(T)\u667a\u80fd\u4f53\u5728\u56db\u4e2a\u76ee\u6807\u6301\u7eed\u65f6\u95f4\u4e0a\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u65f6\u95f4\u8fc7\u5ea6\u4ea7\u751f\uff0c\u4e0e\u4eba\u7c7b\u8ba1\u65f6\u7814\u7a76\u4e00\u81f4\uff1b\u4f46LSTM\u5c42\u5206\u6790\u672a\u53d1\u73b0\u660e\u786e\u7684\u5185\u7f6e\u8ba1\u65f6\u5668\u8bc1\u636e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3DRL\u667a\u80fd\u4f53\u6d8c\u73b0\u884c\u4e3a\u4e0e\u751f\u7269\u7cfb\u7edf\u884c\u4e3a\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u8fc8\u51fa\u4e86\u4e00\u5c0f\u6b65\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u667a\u80fd\u4f53\u7684\u65f6\u95f4\u4fdd\u6301\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2511.01425", "categories": ["cs.AI", "cs.CV", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.01425", "abs": "https://arxiv.org/abs/2511.01425", "authors": ["Yuhang Huang", "Zekai Lin", "Fan Zhong", "Lei Liu"], "title": "Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis", "comment": "12 pages, 3 figures. Under review at the Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2026", "summary": "Explanations for AI models in high-stakes domains like medicine often lack\nverifiability, which can hinder trust. To address this, we propose an\ninteractive agent that produces explanations through an auditable sequence of\nactions. The agent learns a policy to strategically seek external visual\nevidence to support its diagnostic reasoning. This policy is optimized using\nreinforcement learning, resulting in a model that is both efficient and\ngeneralizable. Our experiments show that this action-based reasoning process\nsignificantly improves calibrated accuracy, reducing the Brier score by 18\\%\ncompared to a non-interactive baseline. To validate the faithfulness of the\nagent's explanations, we introduce a causal intervention method. By masking the\nvisual evidence the agent chooses to use, we observe a measurable degradation\nin its performance ($\\Delta$Brier=+0.029), confirming that the evidence is\nintegral to its decision-making process. Our work provides a practical\nframework for building AI systems with verifiable and faithful reasoning\ncapabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ea4\u4e92\u5f0fAI\u4ee3\u7406\uff0c\u901a\u8fc7\u53ef\u5ba1\u8ba1\u7684\u884c\u52a8\u5e8f\u5217\u751f\u6210\u89e3\u91ca\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u6765\u5bfb\u627e\u5916\u90e8\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u8bca\u65ad\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u6821\u51c6\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0cAI\u6a21\u578b\u7684\u89e3\u91ca\u5f80\u5f80\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u6027\uff0c\u8fd9\u4f1a\u963b\u788d\u4fe1\u4efb\u3002\u9700\u8981\u6784\u5efa\u5177\u6709\u53ef\u9a8c\u8bc1\u548c\u5fe0\u5b9e\u63a8\u7406\u80fd\u529b\u7684AI\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\uff0c\u8ba9\u4ee3\u7406\u6218\u7565\u6027\u5730\u5bfb\u6c42\u5916\u90e8\u89c6\u89c9\u8bc1\u636e\u6765\u652f\u6301\u8bca\u65ad\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u65b9\u6cd5\u9a8c\u8bc1\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002", "result": "\u57fa\u4e8e\u884c\u52a8\u7684\u63a8\u7406\u8fc7\u7a0b\u663e\u8457\u63d0\u9ad8\u4e86\u6821\u51c6\u51c6\u786e\u6027\uff0cBrier\u5206\u6570\u6bd4\u975e\u4ea4\u4e92\u57fa\u7ebf\u964d\u4f4e\u4e8618%\u3002\u901a\u8fc7\u5c4f\u853d\u4ee3\u7406\u9009\u62e9\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u89c2\u5bdf\u5230\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff0c\u8bc1\u5b9e\u8bc1\u636e\u5bf9\u5176\u51b3\u7b56\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6784\u5efa\u5177\u6709\u53ef\u9a8c\u8bc1\u548c\u5fe0\u5b9e\u63a8\u7406\u80fd\u529b\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01181", "abs": "https://arxiv.org/abs/2511.01181", "authors": ["Emaad Manzoor", "Eva Ascarza", "Oded Netzer"], "title": "Learning When to Quit in Sales Conversations", "comment": null, "summary": "Salespeople frequently face the dynamic screening decision of whether to\npersist in a conversation or abandon it to pursue the next lead. Yet, little is\nknown about how these decisions are made, whether they are efficient, or how to\nimprove them. We study these decisions in the context of high-volume outbound\nsales where leads are ample, but time is scarce and failure is common. We\nformalize the dynamic screening decision as an optimal stopping problem and\ndevelop a generative language model-based sequential decision agent - a\nstopping agent - that learns whether and when to quit conversations by\nimitating a retrospectively-inferred optimal stopping policy. Our approach\nhandles high-dimensional textual states, scales to large language models, and\nworks with both open-source and proprietary language models. When applied to\ncalls from a large European telecommunications firm, our stopping agent reduces\nthe time spent on failed calls by 54% while preserving nearly all sales;\nreallocating the time saved increases expected sales by up to 37%. Upon\nexamining the linguistic cues that drive salespeople's quitting decisions, we\nfind that they tend to overweight a few salient expressions of consumer\ndisinterest and mispredict call failure risk, suggesting cognitive bounds on\ntheir ability to make real-time conversational decisions. Our findings\nhighlight the potential of artificial intelligence algorithms to correct\ncognitively-bounded human decisions and improve salesforce efficiency.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u505c\u6b62\u4ee3\u7406\uff0c\u7528\u4e8e\u4f18\u5316\u9500\u552e\u5bf9\u8bdd\u4e2d\u7684\u52a8\u6001\u7b5b\u9009\u51b3\u7b56\uff0c\u663e\u8457\u51cf\u5c11\u5931\u8d25\u901a\u8bdd\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u9500\u552e\u4e1a\u7ee9", "motivation": "\u9500\u552e\u4eba\u5458\u9762\u4e34\u4f55\u65f6\u653e\u5f03\u5bf9\u8bdd\u8f6c\u5411\u4e0b\u4e00\u4e2a\u6f5c\u5728\u5ba2\u6237\u7684\u52a8\u6001\u7b5b\u9009\u51b3\u7b56\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u8fd9\u4e9b\u51b3\u7b56\u7684\u6548\u7387\u548c\u6539\u8fdb\u65b9\u6cd5\u4e86\u89e3\u751a\u5c11", "method": "\u5c06\u52a8\u6001\u7b5b\u9009\u51b3\u7b56\u5f62\u5f0f\u5316\u4e3a\u6700\u4f18\u505c\u6b62\u95ee\u9898\uff0c\u5f00\u53d1\u57fa\u4e8e\u751f\u6210\u8bed\u8a00\u6a21\u578b\u7684\u5e8f\u5217\u51b3\u7b56\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u4eff\u4e8b\u540e\u63a8\u65ad\u7684\u6700\u4f18\u505c\u6b62\u7b56\u7565\u6765\u5b66\u4e60\u4f55\u65f6\u9000\u51fa\u5bf9\u8bdd", "result": "\u5e94\u7528\u4e8e\u6b27\u6d32\u7535\u4fe1\u516c\u53f8\u7684\u901a\u8bdd\u6570\u636e\uff0c\u505c\u6b62\u4ee3\u7406\u5c06\u5931\u8d25\u901a\u8bdd\u65f6\u95f4\u51cf\u5c1154%\uff0c\u540c\u65f6\u51e0\u4e4e\u4fdd\u6301\u6240\u6709\u9500\u552e\uff1b\u91cd\u65b0\u5206\u914d\u8282\u7701\u7684\u65f6\u95f4\u53ef\u4f7f\u9884\u671f\u9500\u552e\u589e\u52a0\u9ad8\u8fbe37%", "conclusion": "AI\u7b97\u6cd5\u6709\u6f5c\u529b\u7ea0\u6b63\u8ba4\u77e5\u53d7\u9650\u7684\u4eba\u7c7b\u51b3\u7b56\uff0c\u63d0\u9ad8\u9500\u552e\u56e2\u961f\u6548\u7387\uff1b\u9500\u552e\u4eba\u5458\u503e\u5411\u4e8e\u8fc7\u5ea6\u91cd\u89c6\u5c11\u6570\u660e\u663e\u7684\u5ba2\u6237\u4e0d\u611f\u5174\u8da3\u8868\u8fbe\uff0c\u9519\u8bef\u9884\u6d4b\u901a\u8bdd\u5931\u8d25\u98ce\u9669", "topic": "agent analysis"}}
{"id": "2511.01527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01527", "abs": "https://arxiv.org/abs/2511.01527", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "comment": null, "summary": "Large language model (LLM) agents have exhibited strong problem-solving\ncompetence across domains like research and coding. Yet, it remains\nunderexplored whether LLM agents can tackle compounding real-world problems\nthat require a diverse set of tools to complete. Given a broad, heterogeneous\ntool repository, LLM agents must not only select appropriate tools based on\ntask planning analysis but also strategically schedule the execution order to\nensure efficiency. This paper introduces TPS-Bench to benchmark the ability of\nLLM agents in solving such problems that demand Tool Planning and Scheduling.\nTPS-Bench collects 200 compounding tasks of two difficulty levels, based on a\ntool repository containing hundreds of model context protocol (MCP) tools. In\nparticular, each task is composed of multiple subtasks, such as web search, map\nnavigation, calendar checking, etc., and each subtask can be completed by a\nbasic tool. Our evaluation emphasizes both task completion rate and efficiency.\nThe empirical studies on popular closed-source and open-source LLMs indicate\nthat most models can perform reasonable tool planning, but differ in\nscheduling. For example, GLM-4.5 achieves an outperforming task completion rate\nof 64.72% with extensive sequential tool calls, hence suffering from\nsignificantly long execution time. By contrast, GPT-4o prioritizes parallel\ntool calls but achieves only a 45.08% completion rate. Considering\nreinforcement learning (RL) can be a viable way to improve the scheduling\nefficiency without compromising performance, we perform an initial study on\nQwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in\ntask completion rate based on rarely 100 RL training samples. Our code is\navailable https://github.com/hanwenxu1/mcp-agent.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TPS-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u9700\u8981\u5de5\u5177\u89c4\u5212\u4e0e\u8c03\u5ea6\u80fd\u529b\u7684\u590d\u5408\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u57fa\u51c6\u5305\u542b200\u4e2a\u590d\u5408\u4efb\u52a1\u548c\u6570\u767e\u4e2aMCP\u5de5\u5177\uff0c\u8bc4\u4f30\u663e\u793a\u5927\u591a\u6570\u6a21\u578b\u80fd\u8fdb\u884c\u5408\u7406\u7684\u5de5\u5177\u89c4\u5212\u4f46\u5728\u8c03\u5ea6\u7b56\u7565\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u540c\u65f6\u63a2\u7d22\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u8c03\u5ea6\u6548\u7387\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5728\u89e3\u51b3\u9700\u8981\u591a\u79cd\u5de5\u5177\u534f\u4f5c\u7684\u590d\u5408\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u65f6\uff0c\u5176\u5de5\u5177\u89c4\u5212\u4e0e\u8c03\u5ea6\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u8bc4\u4f30\u4ee3\u7406\u5728\u5927\u578b\u5f02\u6784\u5de5\u5177\u5e93\u4e2d\u5982\u4f55\u9009\u62e9\u5408\u9002\u5de5\u5177\u5e76\u6218\u7565\u6027\u5730\u5b89\u6392\u6267\u884c\u987a\u5e8f\u4ee5\u786e\u4fdd\u6548\u7387\u3002", "method": "\u6784\u5efaTPS-Bench\u57fa\u51c6\uff0c\u5305\u542b200\u4e2a\u590d\u5408\u4efb\u52a1\u548c\u6570\u767e\u4e2aMCP\u5de5\u5177\u3002\u6bcf\u4e2a\u4efb\u52a1\u7531\u591a\u4e2a\u5b50\u4efb\u52a1\u7ec4\u6210\uff0c\u5982\u7f51\u7edc\u641c\u7d22\u3001\u5730\u56fe\u5bfc\u822a\u3001\u65e5\u5386\u68c0\u67e5\u7b49\u3002\u8bc4\u4f30\u91cd\u70b9\u5305\u62ec\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6548\u7387\uff0c\u5e76\u5bf9\u4e3b\u6d41\u95ed\u6e90\u548c\u5f00\u6e90LLM\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5927\u591a\u6570\u6a21\u578b\u80fd\u8fdb\u884c\u5408\u7406\u7684\u5de5\u5177\u89c4\u5212\uff0c\u4f46\u5728\u8c03\u5ea6\u7b56\u7565\u4e0a\u5dee\u5f02\u663e\u8457\u3002GLM-4.5\u8fbe\u523064.72%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u4f46\u6267\u884c\u65f6\u95f4\u8f83\u957f\uff0cGPT-4o\u4f18\u5148\u5e76\u884c\u5de5\u5177\u8c03\u7528\u4f46\u5b8c\u6210\u7387\u4ec545.08%\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728Qwen3-1.7B\u4e0a\u5b9e\u73b0\u4e8614%\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u548c6%\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u5347\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u5de5\u5177\u89c4\u5212\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8c03\u5ea6\u6548\u7387\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u5f3a\u5316\u5b66\u4e60\u662f\u63d0\u5347\u8c03\u5ea6\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u5373\u4f7f\u4f7f\u7528\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u4e5f\u80fd\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2511.00220", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00220", "abs": "https://arxiv.org/abs/2511.00220", "authors": ["Pouya M. Ghari", "Simone Sciabola", "Ye Wang"], "title": "Iterative Foundation Model Fine-Tuning on Multiple Rewards", "comment": "Accepted to NeurIPS 2025", "summary": "Fine-tuning foundation models has emerged as a powerful approach for\ngenerating objects with specific desired properties. Reinforcement learning\n(RL) provides an effective framework for this purpose, enabling models to\ngenerate outputs that maximize a given reward function. However, in many\napplications such as text generation and drug discovery, it can be suboptimal\nto optimize using a single reward signal, as multiple evaluation criteria are\noften necessary. This paper proposes a novel reinforcement learning-based\nmethod for fine-tuning foundation models using multiple reward signals. By\nemploying an iterative fine-tuning strategy across these rewards, our approach\ngeneralizes state-of-the-art RL-based methods. We further provide a theoretical\nanalysis that offers insights into the performance of multi-reward RL\nfine-tuning. Experimental results across diverse domains including text,\nbiological sequence, and small molecule generation, demonstrate the\neffectiveness of the proposed algorithm compared to state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u5956\u52b1\u4fe1\u53f7\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u7b56\u7565\u5728\u591a\u4e2a\u5956\u52b1\u4e0a\u5b9e\u73b0\u6cdb\u5316\uff0c\u5e76\u5728\u6587\u672c\u3001\u751f\u7269\u5e8f\u5217\u548c\u5c0f\u5206\u5b50\u751f\u6210\u7b49\u591a\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u5982\u6587\u672c\u751f\u6210\u548c\u836f\u7269\u53d1\u73b0\u4e2d\uff0c\u5355\u4e00\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u53ef\u80fd\u4e0d\u591f\u7406\u60f3\uff0c\u56e0\u4e3a\u901a\u5e38\u9700\u8981\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u591a\u5956\u52b1\u4fe1\u53f7\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u7b56\u7565\u5728\u591a\u4e2a\u5956\u52b1\u4e0a\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u6587\u672c\u3001\u751f\u7269\u5e8f\u5217\u548c\u5c0f\u5206\u5b50\u751f\u6210\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u6807\u51c6\u4f18\u5316\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01668", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01668", "abs": "https://arxiv.org/abs/2511.01668", "authors": ["Yueqing Xi", "Yifan Bai", "Huasen Luo", "Weiliang Wen", "Hui Liu", "Haoliang Li"], "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics", "comment": null, "summary": "As artificial intelligence permeates judicial forensics, ensuring the\nveracity and traceability of legal question answering (QA) has become critical.\nConventional large language models (LLMs) are prone to hallucination, risking\nmisleading guidance in legal consultation, while static knowledge bases\nstruggle to keep pace with frequently updated statutes and case law. We present\na hybrid legal QA agent tailored for judicial settings that integrates\nretrieval-augmented generation (RAG) with multi-model ensembling to deliver\nreliable, auditable, and continuously updatable counsel. The system prioritizes\nretrieval over generation: when a trusted legal repository yields relevant\nevidence, answers are produced via RAG; otherwise, multiple LLMs generate\ncandidates that are scored by a specialized selector, with the top-ranked\nanswer returned. High-quality outputs then undergo human review before being\nwritten back to the repository, enabling dynamic knowledge evolution and\nprovenance tracking. Experiments on the Law\\_QA dataset show that our hybrid\napproach significantly outperforms both a single-model baseline and a vanilla\nRAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm\nthe complementary contributions of retrieval prioritization, model ensembling,\nand the human-in-the-loop update mechanism. The proposed system demonstrably\nreduces hallucination while improving answer quality and legal compliance,\nadvancing the practical landing of media forensics technologies in judicial\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6cd5\u5f8b\u95ee\u7b54\u4ee3\u7406\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u591a\u6a21\u578b\u96c6\u6210\uff0c\u4e3a\u53f8\u6cd5\u573a\u666f\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u5ba1\u8ba1\u4e14\u6301\u7eed\u66f4\u65b0\u7684\u6cd5\u5f8b\u54a8\u8be2\u670d\u52a1\u3002", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u95ee\u7b54\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u800c\u9759\u6001\u77e5\u8bc6\u5e93\u96be\u4ee5\u8ddf\u4e0a\u9891\u7e41\u66f4\u65b0\u7684\u6cd5\u89c4\u548c\u5224\u4f8b\uff0c\u9700\u8981\u786e\u4fdd\u6cd5\u5f8b\u95ee\u7b54\u7684\u771f\u5b9e\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u4f18\u5148\u7b56\u7565\uff1a\u5f53\u53ef\u4fe1\u6cd5\u5f8b\u77e5\u8bc6\u5e93\u6709\u76f8\u5173\u8bc1\u636e\u65f6\u4f7f\u7528RAG\u751f\u6210\u7b54\u6848\uff0c\u5426\u5219\u7531\u591a\u4e2aLLM\u751f\u6210\u5019\u9009\u7b54\u6848\u5e76\u7531\u4e13\u4e1a\u9009\u62e9\u5668\u8bc4\u5206\u8fd4\u56de\u6700\u4f73\u7b54\u6848\u3002\u9ad8\u8d28\u91cf\u8f93\u51fa\u7ecf\u4eba\u5de5\u5ba1\u6838\u540e\u5199\u56de\u77e5\u8bc6\u5e93\u3002", "result": "\u5728Law_QA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728F1\u3001ROUGE-L\u548cLLM-as-a-Judge\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u548c\u666e\u901aRAG\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u9ad8\u7b54\u6848\u8d28\u91cf\u548c\u6cd5\u5f8b\u5408\u89c4\u6027\uff0c\u63a8\u52a8\u4e86\u5a92\u4f53\u53d6\u8bc1\u6280\u672f\u5728\u53f8\u6cd5\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u843d\u5730\u3002", "topic": "agent analysis"}}
{"id": "2511.01824", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01824", "abs": "https://arxiv.org/abs/2511.01824", "authors": ["Yuetai Li", "Huseyin A Inan", "Xiang Yue", "Wei-Ning Chen", "Lukas Wutschitz", "Janardhan Kulkarni", "Radha Poovendran", "Robert Sim", "Saravan Rajmohan"], "title": "Simulating Environments with Reasoning Models for Agent Training", "comment": null, "summary": "LLM agents excel in compact environments requiring deep reasoning but remain\nbrittle when operating in broader, more complex contexts that demand robustness\nacross diverse tools and schemas. Building bespoke environments for training is\nheavy, brittle, and limits progress. In this paper, we demonstrate that LLMs\ncan simulate realistic environment feedback without access to actual testbed\ndata or APIs. Inspired by this capability, we propose two frameworks:\nSimia-SFT, a pipeline that synthesizes SFT data by amplifying small seed sets\ninto diverse trajectories in an environment-agnostic manner, and Simia-RL, a\nframework that enables RL training without real environment implementations\nthrough LLM-simulated feedback. Fine-tuning open models yields consistent\nimprovements across multiple benchmarks, surpassing GPT-4o and approaching\no4-mini on $\\tau^2$-Bench. Together, Simia-SFT and Simia-RL enable scalable\nagent training without environment engineering, replacing heavy and brittle\nimplementations with flexible LLM-based simulation.", "AI": {"tldr": "LLM\u4ee3\u7406\u5728\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u7684\u7d27\u51d1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u66f4\u5e7f\u6cdb\u590d\u6742\u7684\u9700\u8981\u8de8\u591a\u79cd\u5de5\u5177\u548c\u6a21\u5f0f\u9c81\u68d2\u6027\u7684\u73af\u5883\u4e2d\u4ecd\u7136\u8106\u5f31\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u6846\u67b6\uff1aSimia-SFT\u901a\u8fc7\u6269\u589e\u5c0f\u89c4\u6a21\u79cd\u5b50\u6570\u636e\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\uff0cSimia-RL\u901a\u8fc7LLM\u6a21\u62df\u53cd\u9988\u5b9e\u73b0\u65e0\u9700\u771f\u5b9e\u73af\u5883\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u6784\u5efa\u5b9a\u5236\u5316\u73af\u5883\u8fdb\u884c\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u8106\u5f31\u4e14\u9650\u5236\u8fdb\u5c55\uff0c\u800cLLM\u80fd\u591f\u5728\u4e0d\u8bbf\u95ee\u5b9e\u9645\u6d4b\u8bd5\u6570\u636e\u6216API\u7684\u60c5\u51b5\u4e0b\u6a21\u62df\u771f\u5b9e\u73af\u5883\u53cd\u9988\u3002", "method": "\u63d0\u51faSimia-SFT\u548cSimia-RL\u4e24\u4e2a\u6846\u67b6\uff1aSimia-SFT\u901a\u8fc7\u73af\u5883\u65e0\u5173\u65b9\u5f0f\u5c06\u5c0f\u89c4\u6a21\u79cd\u5b50\u6570\u636e\u6269\u589e\u4e3a\u591a\u6837\u5316\u8f68\u8ff9\uff1bSimia-RL\u901a\u8fc7LLM\u6a21\u62df\u53cd\u9988\u5b9e\u73b0\u65e0\u9700\u771f\u5b9e\u73af\u5883\u5b9e\u73b0\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u03c4\u00b2-Bench\u4e0a\u8d85\u8d8aGPT-4o\u5e76\u63a5\u8fd1o4-mini\u6c34\u5e73\u3002", "conclusion": "Simia-SFT\u548cSimia-RL\u80fd\u591f\u5b9e\u73b0\u65e0\u9700\u73af\u5883\u5de5\u7a0b\u7684\u53ef\u6269\u5c55\u4ee3\u7406\u8bad\u7ec3\uff0c\u7528\u7075\u6d3b\u7684\u57fa\u4e8eLLM\u7684\u6a21\u62df\u66ff\u4ee3\u7e41\u91cd\u8106\u5f31\u7684\u73af\u5883\u5b9e\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00272", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2511.00272", "abs": "https://arxiv.org/abs/2511.00272", "authors": ["Michiel Straat", "Thorben Markmann", "Sebastian Peitz", "Barbara Hammer"], "title": "Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning", "comment": null, "summary": "Chaotic convective flows arise in many real-world systems, such as\nmicrofluidic devices and chemical reactors. Stabilizing these flows is highly\ndesirable but remains challenging, particularly in chaotic regimes where\nconventional control methods often fail. Reinforcement Learning (RL) has shown\npromise for control in laminar flow settings, but its ability to generalize and\nremain robust under chaotic and turbulent dynamics is not well explored,\ndespite being critical for real-world deployment. In this work, we improve the\npractical feasibility of RL-based control of such flows focusing on\nRayleigh-B\\'enard Convection (RBC), a canonical model for convective heat\ntransport. To enhance generalization and sample efficiency, we introduce\ndomain-informed RL agents that are trained using Proximal Policy Optimization\nacross diverse initial conditions and flow regimes. We incorporate domain\nknowledge in the reward function via a term that encourages B\\'enard cell\nmerging, as an example of a desirable macroscopic property. In laminar flow\nregimes, the domain-informed RL agents reduce convective heat transport by up\nto 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which\nis significantly better than the conventional controllers used in practice. We\ncompare the domain-informed to uninformed agents: Our results show that the\ndomain-informed reward design results in steady flows, faster convergence\nduring training, and generalization across flow regimes without retraining. Our\nwork demonstrates that elegant domain-informed priors can greatly enhance the\nrobustness of RL-based control of chaotic flows, bringing real-world deployment\ncloser.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6df7\u6c8c\u5bf9\u6d41\u6d41\u52a8\uff0c\u901a\u8fc7\u5f15\u5165\u9886\u57df\u77e5\u8bc6\u589e\u5f3aRL\u4ee3\u7406\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u5728Rayleigh-B\u00e9nard\u5bf9\u6d41\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5bf9\u6df7\u6c8c\u6d41\u52a8\u7684\u6709\u6548\u63a7\u5236\u3002", "motivation": "\u6df7\u6c8c\u5bf9\u6d41\u6d41\u52a8\u5728\u5fae\u6d41\u4f53\u8bbe\u5907\u548c\u5316\u5b66\u53cd\u5e94\u5668\u7b49\u5b9e\u9645\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u6df7\u6c8c\u72b6\u6001\u4e0b\u5f80\u5f80\u5931\u6548\u3002\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u5728\u5c42\u6d41\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u6df7\u6c8c\u548c\u6e4d\u6d41\u52a8\u6001\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u9886\u57df\u77e5\u8bc6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5728\u591a\u6837\u521d\u59cb\u6761\u4ef6\u548c\u6d41\u52a8\u72b6\u6001\u4e0b\u8fdb\u884c\u8bad\u7ec3\u3002\u5728\u5956\u52b1\u51fd\u6570\u4e2d\u5f15\u5165\u9f13\u52b1B\u00e9nard\u5355\u5143\u5408\u5e76\u7684\u9886\u57df\u77e5\u8bc6\u9879\uff0c\u4f5c\u4e3a\u671f\u671b\u7684\u5b8f\u89c2\u7279\u6027\u3002", "result": "\u5728\u5c42\u6d41\u72b6\u6001\u4e0b\uff0c\u9886\u57df\u77e5\u8bc6RL\u4ee3\u7406\u5c06\u5bf9\u6d41\u70ed\u4f20\u8f93\u51cf\u5c11\u9ad8\u8fbe33%\uff1b\u5728\u6df7\u6c8c\u6d41\u52a8\u72b6\u6001\u4e0b\u4ecd\u80fd\u5b9e\u73b010%\u7684\u51cf\u5c11\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u5668\u3002\u9886\u57df\u77e5\u8bc6\u5956\u52b1\u8bbe\u8ba1\u4ea7\u751f\u7a33\u5b9a\u6d41\u52a8\u3001\u8bad\u7ec3\u6536\u655b\u66f4\u5feb\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8de8\u6d41\u52a8\u72b6\u6001\u6cdb\u5316\u3002", "conclusion": "\u4f18\u96c5\u7684\u9886\u57df\u77e5\u8bc6\u5148\u9a8c\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u6c8c\u6d41\u52a8\u63a7\u5236\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u5b9e\u9645\u90e8\u7f72\u66f4\u63a5\u8fd1\u73b0\u5b9e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbLLM\u8f93\u51fa\u4e8b\u5b9e\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\u6a21\u578b\u63a8\u5e7f\u5230\u6587\u672c\u524d\u7f00\u7ea7\u522b\uff0c\u8bad\u7ec3\u4e86\u4e13\u95e8\u7684MiniTruePrefixes\u6a21\u578b\uff0c\u5728\u53d7\u63a7\u89e3\u7801\u6846\u67b6\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6458\u8981\u751f\u6210\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709NLI\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u5b8c\u6574\u53e5\u5b50\u7684\u63a8\u7406\u5224\u65ad\uff0c\u4f46\u81ea\u56de\u5f52\u751f\u6210\u67b6\u6784\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u9700\u8981\u5bf9\u6587\u672c\u524d\u7f00\u8fdb\u884c\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u524d\u7f00\u7684\u63a8\u7406\u6a21\u578b\u6765\u6539\u8fdb\u751f\u6210\u7684\u4e8b\u5b9e\u6027\u3002", "method": "\u5c06\u8574\u542b\u68c0\u6d4b\u4efb\u52a1\u63a8\u5e7f\u5230\u4efb\u610f\u6587\u672c\u524d\u7f00\uff0c\u63d0\u4f9b\u76f8\u5e94\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e13\u95e8\u7684MiniTruePrefixes\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u53d7\u63a7\u89e3\u7801\u6846\u67b6\u4e2d\u3002", "result": "MiniTruePrefixes\u5728\u524d\u7f00\u7ea7\u8574\u542b\u68c0\u6d4b\u4e0a\u6bd4\u57fa\u7ebfNLI\u6a21\u578b\u9ad8\u51fa5-14\u4e2aF1\u70b9\uff1b\u5728\u6458\u8981\u751f\u6210\u4e2d\uff0cLLaMA-3.2-3B-Instruct\u5728MiniTruePrefixes\u6307\u5bfc\u4e0b\u80fd\u8fbe\u5230\u4e0e8B\u6a21\u578b\u76f8\u5f53\u7684\u4e8b\u5b9e\u6027\u548c\u8fd0\u884c\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u4e00\u534a\u5185\u5b58\u3002", "conclusion": "\u9488\u5bf9\u6587\u672c\u524d\u7f00\u7684\u4e13\u95e8\u63a8\u7406\u6a21\u578b\u80fd\u6709\u6548\u63d0\u9ad8LLM\u751f\u6210\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "topic": "agent analysis"}}
{"id": "2511.00413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00413", "abs": "https://arxiv.org/abs/2511.00413", "authors": ["Shaojie Wang", "Jinghui Wang", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Liang Huang", "Xiaojiang Zhang", "Junyi Peng", "Li Wan", "Haotian Zhang", "Bin Chen"], "title": "Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse", "comment": null, "summary": "In agentic LLM scenarios, an agent's interaction process during a single\nrollout often exhibits branching behaviors. Due to memory retrieval and\nconcurrent tool executions at certain decision points, the token trajectory of\none task evolves into a tree-like structure rather than a linear sequence.\nHowever, current training pipelines decompose such tree-structured trajectories\ninto separate linear segments, treating each branch as an independent sequence.\nAs a result, shared prefixes across these branches are repeatedly recomputed\nduring both forward and backward passes. To address this inefficiency, we\npropose Tree Training, a paradigm that computes each shared prefix only once\nand reuses its intermediate results across related branches during both forward\nand backward passes, substantially improving computation efficiency in\nlarge-scale agentic training. This is achieved via (i) Tree Packing, which\nefficiently reuses shared computations across trajectories, and (ii) Gradient\nRestoration, which ensures correct gradient propagation across reused prefixes.\nExperiments on multiple open-source models demonstrate up to 3.9x reduction in\ntotal training time, enabling more efficient agentic LLM SFT and RL training.", "AI": {"tldr": "\u63d0\u51fa\u4e86Tree Training\u65b9\u6cd5\uff0c\u901a\u8fc7\u6811\u5f62\u6253\u5305\u548c\u68af\u5ea6\u6062\u590d\u6280\u672f\uff0c\u5728\u667a\u80fd\u4f53LLM\u8bad\u7ec3\u4e2d\u91cd\u7528\u5171\u4eab\u524d\u7f00\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "motivation": "\u5f53\u524d\u8bad\u7ec3\u6d41\u7a0b\u5c06\u6811\u5f62\u8f68\u8ff9\u5206\u89e3\u4e3a\u72ec\u7acb\u7ebf\u6027\u6bb5\uff0c\u5bfc\u81f4\u5171\u4eab\u524d\u7f00\u91cd\u590d\u8ba1\u7b97\uff0c\u6548\u7387\u4f4e\u4e0b", "method": "Tree Training\u8303\u5f0f\uff0c\u5305\u542b\u6811\u5f62\u6253\u5305\uff08\u91cd\u7528\u8f68\u8ff9\u95f4\u5171\u4eab\u8ba1\u7b97\uff09\u548c\u68af\u5ea6\u6062\u590d\uff08\u786e\u4fdd\u91cd\u7528\u524d\u7f00\u7684\u6b63\u786e\u68af\u5ea6\u4f20\u64ad\uff09", "result": "\u5728\u591a\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0c\u603b\u8bad\u7ec3\u65f6\u95f4\u6700\u591a\u51cf\u5c113.9\u500d", "conclusion": "Tree Training\u80fd\u591f\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53LLM SFT\u548cRL\u8bad\u7ec3\u7684\u8ba1\u7b97\u6548\u7387", "topic": "agentic reinforcement learning"}}
{"id": "2511.01470", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets.", "AI": {"tldr": "\u63d0\u51faBARD\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5728\u84b8\u998f\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u63a8\u7406\u957f\u5ea6\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u8ba98B\u5b66\u751f\u6a21\u578b\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u7cbe\u786e\u63a7\u5236\u8ba1\u7b97\u9884\u7b97\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCoT\u84b8\u998f\u4e2d\u63a8\u7406\u8fc7\u7a0b\u5197\u4f59\u4e14\u8ba1\u7b97\u9884\u7b97\u4e0d\u53ef\u63a7\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u63a8\u7406\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u52a8\u6001\u5e73\u8861\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5SFT\u5728\u6559\u5e08\u751f\u6210\u7684\u957fCoT\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u7b97\u7ea6\u675f\u7406\u89e3\uff1b\u7b2c\u4e8c\u9636\u6bb5RL\u540c\u65f6\u4f18\u5316\u63a8\u7406\u6027\u80fd\u548c\u9884\u7b97\u4fdd\u771f\u5ea6\u3002", "result": "8B\u5b66\u751f\u6a21\u578b\u5728AIME24\u3001AIME25\u3001GPQA\u7b49\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5e76\u80fd\u8de8\u5e7f\u6cdb\u9884\u7b97\u8303\u56f4\u7cbe\u786e\u81ea\u9002\u5e94\u63a7\u5236\u63a8\u7406\u957f\u5ea6\u3002", "conclusion": "BARD\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u63a8\u7406\u80fd\u529b\u84b8\u998f\u4e0e\u8ba1\u7b97\u9884\u7b97\u63a7\u5236\u7684\u7edf\u4e00\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2511.00423", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00423", "abs": "https://arxiv.org/abs/2511.00423", "authors": ["Guojian Zhan", "Likun Wang", "Xiangteng Zhang", "Jiaxin Gao", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Bootstrap Off-policy with World Model", "comment": "NeurIPS 2025", "summary": "Online planning has proven effective in reinforcement learning (RL) for\nimproving sample efficiency and final performance. However, using planning for\nenvironment interaction inevitably introduces a divergence between the\ncollected data and the policy's actual behaviors, degrading both model learning\nand policy improvement. To address this, we propose BOOM (Bootstrap Off-policy\nwith WOrld Model), a framework that tightly integrates planning and off-policy\nlearning through a bootstrap loop: the policy initializes the planner, and the\nplanner refines actions to bootstrap the policy through behavior alignment.\nThis loop is supported by a jointly learned world model, which enables the\nplanner to simulate future trajectories and provides value targets to\nfacilitate policy improvement. The core of BOOM is a likelihood-free alignment\nloss that bootstraps the policy using the planner's non-parametric action\ndistribution, combined with a soft value-weighted mechanism that prioritizes\nhigh-return behaviors and mitigates variability in the planner's action quality\nwithin the replay buffer. Experiments on the high-dimensional DeepMind Control\nSuite and Humanoid-Bench show that BOOM achieves state-of-the-art results in\nboth training stability and final performance. The code is accessible at\nhttps://github.com/molumitu/BOOM_MBRL.", "AI": {"tldr": "BOOM\u662f\u4e00\u4e2a\u5c06\u89c4\u5212\u4e0e\u79bb\u7b56\u7565\u5b66\u4e60\u7d27\u5bc6\u7ed3\u5408\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u5faa\u73af\u548c\u8054\u5408\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5728DeepMind Control Suite\u548cHumanoid-Bench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u89c4\u5212\u867d\u7136\u80fd\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6536\u96c6\u7684\u6570\u636e\u4e0e\u7b56\u7565\u5b9e\u9645\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u5b66\u4e60\u548c\u7b56\u7565\u6539\u8fdb\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faBOOM\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u5faa\u73af\u5c06\u89c4\u5212\u4e0e\u79bb\u7b56\u7565\u5b66\u4e60\u7d27\u5bc6\u7ed3\u5408\uff1a\u7b56\u7565\u521d\u59cb\u5316\u89c4\u5212\u5668\uff0c\u89c4\u5212\u5668\u901a\u8fc7\u884c\u4e3a\u5bf9\u9f50\u5f15\u5bfc\u7b56\u7565\u3002\u4f7f\u7528\u8054\u5408\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u652f\u6301\u89c4\u5212\u5668\u6a21\u62df\u672a\u6765\u8f68\u8ff9\uff0c\u5e76\u63d0\u4f9b\u4ef7\u503c\u76ee\u6807\u4fc3\u8fdb\u7b56\u7565\u6539\u8fdb\u3002\u6838\u5fc3\u662f\u65e0\u4f3c\u7136\u5bf9\u9f50\u635f\u5931\u548c\u8f6f\u4ef7\u503c\u52a0\u6743\u673a\u5236\u3002", "result": "\u5728\u9ad8\u7ef4DeepMind Control Suite\u548cHumanoid-Bench\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBOOM\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "BOOM\u901a\u8fc7\u7d27\u5bc6\u6574\u5408\u89c4\u5212\u548c\u79bb\u7b56\u7565\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c4\u5212\u5bfc\u81f4\u7684\u6570\u636e\u4e0e\u884c\u4e3a\u5dee\u5f02\u95ee\u9898\uff0c\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00521", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00521", "abs": "https://arxiv.org/abs/2511.00521", "authors": ["Bao Nguyen", "Hieu Trung Nguyen", "Ruifeng She", "Xiaojin Fu", "Viet Anh Nguyen"], "title": "Reasoning Planning for Language Models", "comment": "29 pages, 5 figures", "summary": "Selecting an appropriate reasoning method for a given query remains a key\nchallenge in language model generation. Existing approaches typically generate\nmultiple candidate responses and use an aggregation strategy to select the\noutput answer, often assuming that more candidate answers yield higher\naccuracy. We revisit this assumption through a rigorous theoretical analysis,\nderiving accuracy bounds for standard aggregation methods under fixed\ngeneration distributions and candidate sizes. Building on these insights, we\nintroduce EPIC, an Ensemble Planning with Contrastive learning framework to\nlearn a shared representation space that captures both model reasoning\nabilities and query-method compatibility. EPIC incorporates our probability\nbounds as a regularizer in a utility-driven optimization that balances accuracy\nand computational cost. Experiments on diverse mathematical reasoning tasks\nshow that EPIC consistently selects optimal reasoning methods, improving\naccuracy while reducing computational overhead. Our code can be found at\nhttps://github.com/nguyenngocbaocmt02/EPIC.", "AI": {"tldr": "EPIC\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u5171\u4eab\u8868\u793a\u7a7a\u95f4\uff0c\u9009\u62e9\u6700\u4f18\u63a8\u7406\u65b9\u6cd5\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u751f\u6210\u591a\u4e2a\u5019\u9009\u54cd\u5e94\u5e76\u4f7f\u7528\u805a\u5408\u7b56\u7565\u9009\u62e9\u8f93\u51fa\u7b54\u6848\uff0c\u5047\u8bbe\u66f4\u591a\u5019\u9009\u7b54\u6848\u80fd\u5e26\u6765\u66f4\u9ad8\u51c6\u786e\u6027\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u5047\u8bbe\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u53d1\u73b0\u8fd9\u79cd\u5047\u8bbe\u5e76\u4e0d\u603b\u662f\u6210\u7acb\u3002", "method": "\u63d0\u51faEPIC\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u5171\u4eab\u8868\u793a\u7a7a\u95f4\uff0c\u6355\u6349\u6a21\u578b\u63a8\u7406\u80fd\u529b\u548c\u67e5\u8be2-\u65b9\u6cd5\u517c\u5bb9\u6027\u3002\u5c06\u6982\u7387\u754c\u9650\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u7eb3\u5165\u6548\u7528\u9a71\u52a8\u7684\u4f18\u5316\u4e2d\uff0c\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEPIC\u80fd\u6301\u7eed\u9009\u62e9\u6700\u4f18\u63a8\u7406\u65b9\u6cd5\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "EPIC\u6846\u67b6\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u65b9\u6cd5\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2511.00549", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00549", "abs": "https://arxiv.org/abs/2511.00549", "authors": ["Qiang Li", "Jin Niu", "Lina Yu"], "title": "Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations", "comment": null, "summary": "Traffic congestion, primarily driven by intersection queuing, significantly\nimpacts urban living standards, safety, environmental quality, and economic\nefficiency. While Traffic Signal Control (TSC) systems hold potential for\ncongestion mitigation, traditional optimization models often fail to capture\nreal-world traffic complexity and dynamics. This study introduces a novel\nsingle-agent reinforcement learning (RL) framework for regional adaptive TSC,\ncircumventing the coordination complexities inherent in multi-agent systems\nthrough a centralized decision-making paradigm. The model employs an adjacency\nmatrix to unify the encoding of road network topology, real-time queue states\nderived from probe vehicle data, and current signal timing parameters.\nLeveraging the efficient learning capabilities of the DreamerV3 world model,\nthe agent learns control policies where actions sequentially select\nintersections and adjust their signal phase splits to regulate traffic\ninflow/outflow, analogous to a feedback control system. Reward design\nprioritizes queue dissipation, directly linking congestion metrics (queue\nlength) to control actions. Simulation experiments conducted in SUMO\ndemonstrate the model's effectiveness: under inference scenarios with\nmulti-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the\nframework exhibits robust anti-fluctuation capability and significantly reduces\nqueue lengths. This work establishes a new paradigm for intelligent traffic\ncontrol compatible with probe vehicle technology. Future research will focus on\nenhancing practical applicability by incorporating stochastic OD demand\nfluctuations during training and exploring regional optimization mechanisms for\ncontingency events.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u533a\u57df\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528DreamerV3\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u96c6\u4e2d\u51b3\u7b56\u89c4\u907f\u591a\u667a\u80fd\u4f53\u534f\u8c03\u590d\u6742\u6027\uff0c\u663e\u8457\u51cf\u5c11\u6392\u961f\u957f\u5ea6\u5e76\u5c55\u73b0\u6297\u6ce2\u52a8\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6a21\u578b\u96be\u4ee5\u6355\u6349\u73b0\u5b9e\u4ea4\u901a\u590d\u6742\u6027\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u534f\u8c03\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u90bb\u63a5\u77e9\u9635\u7f16\u7801\u8def\u7f51\u62d3\u6251\u3001\u5b9e\u65f6\u6392\u961f\u72b6\u6001\u548c\u4fe1\u53f7\u53c2\u6570\uff0c\u91c7\u7528DreamerV3\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u987a\u5e8f\u9009\u62e9\u4ea4\u53c9\u53e3\u548c\u8c03\u6574\u4fe1\u53f7\u76f8\u4f4d\u6765\u8c03\u8282\u4ea4\u901a\u6d41\u3002", "result": "SUMO\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u4e0d\u540cOD\u9700\u6c42\u6ce2\u52a8\u573a\u666f\u4e0b\uff0c\u8be5\u6846\u67b6\u5177\u6709\u9c81\u68d2\u7684\u6297\u6ce2\u52a8\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6392\u961f\u957f\u5ea6\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u79cd\u4e0e\u63a2\u6d4b\u8f66\u8f86\u6280\u672f\u517c\u5bb9\u7684\u667a\u80fd\u4ea4\u901a\u63a7\u5236\u65b0\u8303\u5f0f\uff0c\u4e3a\u7f13\u89e3\u4ea4\u901a\u62e5\u5835\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00554", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00554", "abs": "https://arxiv.org/abs/2511.00554", "authors": ["Phil Blandfort", "Robert Graham"], "title": "Red-teaming Activation Probes using Prompted LLMs", "comment": null, "summary": "Activation probes are attractive monitors for AI systems due to low cost and\nlatency, but their real-world robustness remains underexplored. We ask: What\nfailure modes arise under realistic, black-box adversarial pressure, and how\ncan we surface them with minimal effort? We present a lightweight black-box\nred-teaming procedure that wraps an off-the-shelf LLM with iterative feedback\nand in-context learning (ICL), and requires no fine-tuning, gradients, or\narchitectural access. Running a case study with probes for high-stakes\ninteractions, we show that our approach can help discover valuable insights\nabout a SOTA probe. Our analysis uncovers interpretable brittleness patterns\n(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but\npersistent vulnerabilities under scenario-constraint attacks. These results\nsuggest that simple prompted red-teaming scaffolding can anticipate failure\npatterns before deployment and might yield promising, actionable insights to\nharden future probes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9ed1\u76d2\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4f7f\u7528\u73b0\u6210LLM\u7ed3\u5408\u8fed\u4ee3\u53cd\u9988\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u53d1\u73b0\u6fc0\u6d3b\u63a2\u9488\u7684\u8106\u5f31\u6027\u6a21\u5f0f\uff0c\u65e0\u9700\u5fae\u8c03\u3001\u68af\u5ea6\u6216\u67b6\u6784\u8bbf\u95ee\u3002", "motivation": "\u6fc0\u6d3b\u63a2\u9488\u4f5c\u4e3aAI\u7cfb\u7edf\u76d1\u63a7\u5668\u5177\u6709\u4f4e\u6210\u672c\u548c\u4f4e\u5ef6\u8fdf\u4f18\u52bf\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u9ed1\u76d2\u5bf9\u6297\u538b\u529b\u4e0b\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u53d1\u73b0\u5931\u6548\u6a21\u5f0f\u5e76\u6700\u5c0f\u5316\u6d4b\u8bd5\u6210\u672c\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u9ed1\u76d2\u7ea2\u961f\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u5305\u88c5\u73b0\u6210LLM\u5e76\u4f7f\u7528\u8fed\u4ee3\u53cd\u9988\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u65e0\u9700\u5fae\u8c03\u3001\u68af\u5ea6\u6216\u67b6\u6784\u8bbf\u95ee\u3002\u901a\u8fc7\u9ad8\u98ce\u9669\u4ea4\u4e92\u63a2\u9488\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u8106\u5f31\u6027\u6a21\u5f0f\uff08\u5982\u6cd5\u5f8b\u672f\u8bed\u5bfc\u81f4\u7684\u8bef\u62a5\u3001\u5e73\u6de1\u7a0b\u5e8f\u6027\u8bed\u6c14\u7684\u6f0f\u62a5\uff09\uff0c\u4ee5\u53ca\u5728\u573a\u666f\u7ea6\u675f\u653b\u51fb\u4e0b\u51cf\u5c11\u4f46\u4ecd\u6301\u7eed\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u7b80\u5355\u7684\u63d0\u793a\u5f0f\u7ea2\u961f\u6d4b\u8bd5\u6846\u67b6\u53ef\u4ee5\u5728\u90e8\u7f72\u524d\u9884\u6d4b\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u4e3a\u672a\u6765\u63a2\u9488\u7684\u52a0\u56fa\u63d0\u4f9b\u6709\u524d\u666f\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2511.01706", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684rank-2\u6295\u5f71\u5b50\u7a7a\u95f4\u65b9\u6cd5\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u533a\u5206\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53c2\u6570\u77e5\u8bc6(PK)\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6(CK)\u7684\u8d21\u732e\uff0c\u5e76\u9996\u6b21\u5bf9\u957f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u5e8f\u5217\u8fdb\u884c\u591a\u6b65\u77e5\u8bc6\u4ea4\u4e92\u5206\u6790\u3002", "motivation": "\u7406\u89e3\u53c2\u6570\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u4e2d\u7684\u4ea4\u4e92\u5bf9\u4e8e\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u63a2\u7d22\u4e0d\u8db3\uff0c\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u751f\u6210\u548c\u4e8c\u5143\u4ea4\u4e92\u6a21\u578b\u3002", "method": "\u4f7f\u7528rank-2\u6295\u5f71\u5b50\u7a7a\u95f4\u66f4\u7cbe\u786e\u5730\u89e3\u8026PK\u548cCK\u8d21\u732e\uff0c\u5728\u56db\u4e2aQA\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u5f00\u6e90\u6307\u4ee4\u8c03\u4f18LLM\u4e0a\u8fdb\u884c\u591a\u6b65\u77e5\u8bc6\u4ea4\u4e92\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0crank-1\u5b50\u7a7a\u95f4\u65e0\u6cd5\u6709\u6548\u8868\u793a\u591a\u6837\u5316\u7684\u77e5\u8bc6\u4ea4\u4e92\uff0c\u800crank-2\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\uff1b\u591a\u6b65\u5206\u6790\u663e\u793a\u5e7b\u89c9NLE\u4e0ePK\u65b9\u5411\u5f3a\u76f8\u5173\uff0c\u4e0a\u4e0b\u6587\u5fe0\u5b9eNLE\u5e73\u8861PK\u548cCK\uff0c\u601d\u7ef4\u94fe\u63d0\u793a\u901a\u8fc7\u51cf\u5c11PK\u4f9d\u8d56\u5c06NLE\u8f6c\u5411CK\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u901a\u8fc7\u66f4\u4e30\u5bcc\u7684rank-2\u5b50\u7a7a\u95f4\u89e3\u8026\u7cfb\u7edf\u7814\u7a76LLM\u4e2d\u591a\u6b65\u77e5\u8bc6\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9996\u4e2a\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2511.01805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u95f4\u5bf9\u8bdd\u548c\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u4f1a\u7ecf\u5386\u4fe1\u5ff5\u6f02\u79fb\uff0c\u5bfc\u81f4\u5176\u4e16\u754c\u89c2\u548c\u884c\u4e3a\u53d1\u751f\u53d8\u5316\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4e0a\u4e0b\u6587\u79ef\u7d2f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4fe1\u5ff5\u914d\u7f6e\u6587\u4ef6\u53d1\u751f\u65e0\u58f0\u53d8\u5316\uff0c\u5e26\u6765\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u9053\u5fb7\u56f0\u5883\u8ba8\u8bba\u3001\u653f\u6cbb\u7acb\u573a\u9605\u8bfb\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u7d2f\u79ef\u4e0a\u4e0b\u6587\u540e\u7684\u4fe1\u5ff5\u548c\u884c\u4e3a\u53d8\u5316\u3002", "result": "GPT-5\u572810\u8f6e\u9053\u5fb7\u8ba8\u8bba\u540e\u4fe1\u5ff5\u53d8\u5316\u8fbe54.7%\uff0cGrok 4\u5728\u9605\u8bfb\u5bf9\u7acb\u653f\u6cbb\u6587\u672c\u540e\u4fe1\u5ff5\u53d8\u5316\u8fbe27.2%\uff0c\u884c\u4e3a\u53d8\u5316\u4e0e\u4fe1\u5ff5\u6f02\u79fb\u4e00\u81f4\u3002", "conclusion": "\u6a21\u578b\u5728\u957f\u65f6\u95f4\u5bf9\u8bdd\u548c\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u663e\u8457\u7684\u4fe1\u5ff5\u6f02\u79fb\uff0c\u8fd9\u4f1a\u5f71\u54cd\u5176\u610f\u89c1\u548c\u884c\u4e3a\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.00617", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.00617", "abs": "https://arxiv.org/abs/2511.00617", "authors": ["Eric Bigelow", "Daniel Wurgaft", "YingQiao Wang", "Noah Goodman", "Tomer Ullman", "Hidenori Tanaka", "Ekdeep Singh Lubana"], "title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering", "comment": null, "summary": "Large language models (LLMs) can be controlled at inference time through\nprompts (in-context learning) and internal activations (activation steering).\nDifferent accounts have been proposed to explain these methods, yet their\ncommon goal of controlling model behavior raises the question of whether these\nseemingly disparate methodologies can be seen as specific instances of a\nbroader framework. Motivated by this, we develop a unifying, predictive account\nof LLM control from a Bayesian perspective. Specifically, we posit that both\ncontext- and activation-based interventions impact model behavior by altering\nits belief in latent concepts: steering operates by changing concept priors,\nwhile in-context learning leads to an accumulation of evidence. This results in\na closed-form Bayesian model that is highly predictive of LLM behavior across\ncontext- and activation-based interventions in a set of domains inspired by\nprior work on many-shot in-context learning. This model helps us explain prior\nempirical phenomena - e.g., sigmoidal learning curves as in-context evidence\naccumulates - while predicting novel ones - e.g., additivity of both\ninterventions in log-belief space, which results in distinct phases such that\nsudden and dramatic behavioral shifts can be induced by slightly changing\nintervention controls. Taken together, this work offers a unified account of\nprompt-based and activation-based control of LLM behavior, and a methodology\nfor empirically predicting the effects of these interventions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8d1d\u53f6\u65af\u6846\u67b6\u6765\u89e3\u91caLLM\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6fc0\u6d3b\u5f15\u5bfc\u89c6\u4e3a\u6539\u53d8\u6a21\u578b\u5bf9\u6f5c\u5728\u6982\u5ff5\u4fe1\u5ff5\u7684\u4e0d\u540c\u65b9\u5f0f\u3002", "motivation": "\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6fc0\u6d3b\u5f15\u5bfc\u8fd9\u4e24\u79cd\u770b\u4f3c\u4e0d\u540c\u7684LLM\u63a7\u5236\u65b9\u6cd5\u662f\u5426\u53ef\u4ee5\u4ece\u66f4\u5e7f\u6cdb\u7684\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u3002", "method": "\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u5f00\u53d1\u4e86\u4e00\u4e2a\u9884\u6d4b\u6027\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u4e0a\u4e0b\u6587\u5e72\u9884\u89c6\u4e3a\u8bc1\u636e\u79ef\u7d2f\uff0c\u6fc0\u6d3b\u5f15\u5bfc\u89c6\u4e3a\u6539\u53d8\u6982\u5ff5\u5148\u9a8c\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u9884\u6d4bLLM\u5728\u5404\u79cd\u5e72\u9884\u4e0b\u7684\u884c\u4e3a\uff0c\u89e3\u91ca\u4e86\u5148\u524d\u7684\u7ecf\u9a8c\u73b0\u8c61\uff08\u5982S\u578b\u5b66\u4e60\u66f2\u7ebf\uff09\uff0c\u5e76\u9884\u6d4b\u4e86\u65b0\u7684\u73b0\u8c61\uff08\u5982\u5bf9\u6570\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u7684\u5e72\u9884\u53ef\u52a0\u6027\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u63d0\u793a\u548c\u57fa\u4e8e\u6fc0\u6d3b\u7684LLM\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u91ca\uff0c\u5e76\u4e3a\u9884\u6d4b\u8fd9\u4e9b\u5e72\u9884\u6548\u679c\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u3002", "topic": "agent analysis"}}
{"id": "2511.01854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01854", "abs": "https://arxiv.org/abs/2511.01854", "authors": ["Elias Lumer", "Faheem Nizar", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems", "comment": null, "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86Tool-to-Agent Retrieval\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u5411\u91cf\u7a7a\u95f4\u4e2d\u5d4c\u5165\u5de5\u5177\u53ca\u5176\u7236\u4ee3\u7406\uff0c\u5e76\u5229\u7528\u5143\u6570\u636e\u5173\u7cfb\u8fde\u63a5\u5b83\u4eec\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u5de5\u5177\u7ea7\u6216\u4ee3\u7406\u7ea7\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u901a\u5e38\u5c06\u67e5\u8be2\u4e0e\u7c97\u7c92\u5ea6\u7684\u4ee3\u7406\u7ea7\u63cf\u8ff0\u8fdb\u884c\u5339\u914d\uff0c\u8fd9\u4f1a\u63a9\u76d6\u7ec6\u7c92\u5ea6\u7684\u5de5\u5177\u529f\u80fd\uff0c\u5bfc\u81f4\u4ee3\u7406\u9009\u62e9\u4e0d\u7406\u60f3\u3002", "method": "\u5728\u5171\u4eab\u5411\u91cf\u7a7a\u95f4\u4e2d\u5d4c\u5165\u5de5\u5177\u548c\u7236\u4ee3\u7406\uff0c\u901a\u8fc7\u5143\u6570\u636e\u5173\u7cfb\u8fde\u63a5\u5b83\u4eec\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u5de5\u5177\u7ea7\u6216\u4ee3\u7406\u7ea7\u68c0\u7d22\u3002", "result": "\u5728LiveMCPBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u75288\u79cd\u5d4c\u5165\u6a21\u578b\uff0cRecall@5\u63d0\u5347\u4e8619.4%\uff0cnDCG@5\u63d0\u5347\u4e8617.7%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u68c0\u7d22\u65b9\u6cd5\u3002", "conclusion": "Tool-to-Agent Retrieval\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u8868\u793a\u5de5\u5177\u80fd\u529b\u5e76\u904d\u5386\u5143\u6570\u636e\u5230\u4ee3\u7406\u7ea7\u522b\uff0c\u786e\u4fdd\u4e86\u4ee3\u7406\u53ca\u5176\u5e95\u5c42\u5de5\u5177\u6216MCP\u670d\u52a1\u5668\u7684\u5e73\u7b49\u8868\u793a\uff0c\u907f\u514d\u4e86\u5c06\u591a\u4e2a\u5de5\u5177\u5206\u5757\u5728\u4e00\u8d77\u65f6\u51fa\u73b0\u7684\u4e0a\u4e0b\u6587\u7a00\u91ca\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2511.00806", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00806", "abs": "https://arxiv.org/abs/2511.00806", "authors": ["Guangxi Wan", "Peng Zeng", "Xiaoting Dong", "Chunhe Song", "Shijie Cui", "Dong Li", "Qingwei Dong", "Yiyang Liu", "Hongfei Bai"], "title": "Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems", "comment": null, "summary": "Cyber-physical systems (CPS) require the joint optimization of discrete cyber\nactions and continuous physical parameters under stringent safety logic\nconstraints. However, existing hierarchical approaches often compromise global\noptimality, whereas reinforcement learning (RL) in hybrid action spaces often\nrelies on brittle reward penalties, masking, or shielding and struggles to\nguarantee constraint satisfaction. We present logic-informed reinforcement\nlearning (LIRL), which equips standard policy-gradient algorithms with\nprojection that maps a low-dimensional latent action onto the admissible hybrid\nmanifold defined on-the-fly by first-order logic. This guarantees feasibility\nof every exploratory step without penalty tuning. Experimental evaluations have\nbeen conducted across multiple scenarios, including industrial manufacturing,\nelectric vehicle charging stations, and traffic signal control, in all of which\nthe proposed method outperforms existing hierarchical optimization approaches.\nTaking a robotic reducer assembly system in industrial manufacturing as an\nexample, LIRL achieves a 36.47\\% to 44.33\\% reduction at most in the combined\nmakespan-energy objective compared to conventional industrial hierarchical\nscheduling methods. Meanwhile, it consistently maintains zero constraint\nviolations and significantly surpasses state-of-the-art hybrid-action\nreinforcement learning baselines. Thanks to its declarative logic-based\nconstraint formulation, the framework can be seamlessly transferred to other\ndomains such as smart transportation and smart grid, thereby paving the way for\nsafe and real-time optimization in large-scale CPS.", "AI": {"tldr": "\u63d0\u51fa\u903b\u8f91\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60(LIRL)\uff0c\u901a\u8fc7\u5728\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u4e2d\u5f15\u5165\u6295\u5f71\u673a\u5236\uff0c\u5c06\u4f4e\u7ef4\u6f5c\u5728\u52a8\u4f5c\u6620\u5c04\u5230\u7531\u4e00\u9636\u903b\u8f91\u5b9a\u4e49\u7684\u53ef\u884c\u6df7\u5408\u6d41\u5f62\u4e0a\uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u63a2\u7d22\u6b65\u9aa4\u7684\u53ef\u884c\u6027\uff0c\u65e0\u9700\u60e9\u7f5a\u8c03\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u96be\u4ee5\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\uff0c\u5206\u5c42\u65b9\u6cd5\u727a\u7272\u5168\u5c40\u6700\u4f18\u6027\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u8106\u5f31\u7684\u5956\u52b1\u60e9\u7f5a\u3001\u63a9\u7801\u6216\u5c4f\u853d\u673a\u5236\u3002", "method": "\u5728\u6807\u51c6\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u4e2d\u5f15\u5165\u6295\u5f71\u673a\u5236\uff0c\u5c06\u4f4e\u7ef4\u6f5c\u5728\u52a8\u4f5c\u6620\u5c04\u5230\u7531\u4e00\u9636\u903b\u8f91\u5b9a\u4e49\u7684\u53ef\u884c\u6df7\u5408\u6d41\u5f62\u4e0a\uff0c\u4fdd\u8bc1\u6bcf\u4e2a\u63a2\u7d22\u6b65\u9aa4\u7684\u53ef\u884c\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u5236\u9020\u3001\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7b49\u591a\u4e2a\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\u3002\u4ee5\u673a\u5668\u4eba\u51cf\u901f\u5668\u88c5\u914d\u7cfb\u7edf\u4e3a\u4f8b\uff0c\u76f8\u6bd4\u4f20\u7edf\u5de5\u4e1a\u5206\u5c42\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5728\u7efc\u5408\u5b8c\u5de5\u65f6\u95f4-\u80fd\u8017\u76ee\u6807\u4e0a\u6700\u591a\u51cf\u5c1136.47%\u523044.33%\uff0c\u59cb\u7ec8\u4fdd\u6301\u96f6\u7ea6\u675f\u8fdd\u53cd\uff0c\u663e\u8457\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u6df7\u5408\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u65e0\u7f1d\u8fc1\u79fb\u5230\u667a\u80fd\u4ea4\u901a\u548c\u667a\u80fd\u7535\u7f51\u7b49\u5176\u4ed6\u9886\u57df\uff0c\u4e3a\u5927\u89c4\u6a21CPS\u7684\u5b89\u5168\u5b9e\u65f6\u4f18\u5316\u94fa\u5e73\u9053\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00811", "abs": "https://arxiv.org/abs/2511.00811", "authors": ["Runyu Lu", "Peng Zhang", "Ruochuan Shi", "Yuanheng Zhu", "Dongbin Zhao", "Yang Liu", "Dong Wang", "Cesare Alippi"], "title": "Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games", "comment": null, "summary": "Equilibrium learning in adversarial games is an important topic widely\nexamined in the fields of game theory and reinforcement learning (RL).\nPursuit-evasion game (PEG), as an important class of real-world games from the\nfields of robotics and security, requires exponential time to be accurately\nsolved. When the underlying graph structure varies, even the state-of-the-art\nRL methods require recomputation or at least fine-tuning, which can be\ntime-consuming and impair real-time applicability. This paper proposes an\nEquilibrium Policy Generalization (EPG) framework to effectively learn a\ngeneralized policy with robust cross-graph zero-shot performance. In the\ncontext of PEGs, our framework is generally applicable to both pursuer and\nevader sides in both no-exit and multi-exit scenarios. These two\ngeneralizability properties, to our knowledge, are the first to appear in this\ndomain. The core idea of the EPG framework is to train an RL policy across\ndifferent graph structures against the equilibrium policy for each single\ngraph. To construct an equilibrium oracle for single-graph policies, we present\na dynamic programming (DP) algorithm that provably generates pure-strategy Nash\nequilibrium with near-optimal time complexity. To guarantee scalability with\nrespect to pursuer number, we further extend DP and RL by designing a grouping\nmechanism and a sequence model for joint policy decomposition, respectively.\nExperimental results show that, using equilibrium guidance and a distance\nfeature proposed for cross-graph PEG training, the EPG framework guarantees\ndesirable zero-shot performance in various unseen real-world graphs. Besides,\nwhen trained under an equilibrium heuristic proposed for the graphs with exits,\nour generalized pursuer policy can even match the performance of the fine-tuned\npolicies from the state-of-the-art PEG methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5747\u8861\u7b56\u7565\u6cdb\u5316(EPG)\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bf9\u6297\u6027\u6e38\u620f\u4e2d\u5b66\u4e60\u5177\u6709\u8de8\u56fe\u96f6\u6837\u672c\u6027\u80fd\u7684\u6cdb\u5316\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8ffd\u9003\u6e38\u620f(PEG)\u3002", "motivation": "\u89e3\u51b3\u8ffd\u9003\u6e38\u620f\u4e2d\u56fe\u7ed3\u6784\u53d8\u5316\u65f6\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8ba1\u7b97\u6216\u5fae\u8c03\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5b9e\u65f6\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u751f\u6210\u7eaf\u7b56\u7565\u7eb3\u4ec0\u5747\u8861\u4f5c\u4e3a\u5355\u56fe\u7b56\u7565\u7684\u5747\u8861\u9884\u8a00\u673a\uff0c\u901a\u8fc7\u5206\u7ec4\u673a\u5236\u548c\u5e8f\u5217\u6a21\u578b\u6269\u5c55DP\u548cRL\u4ee5\u5904\u7406\u591a\u4e2a\u8ffd\u6355\u8005\uff0c\u63d0\u51fa\u8ddd\u79bb\u7279\u5f81\u7528\u4e8e\u8de8\u56fe\u8bad\u7ec3\u3002", "result": "EPG\u6846\u67b6\u5728\u5404\u79cd\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u6709\u51fa\u53e3\u7684\u56fe\u4e2d\uff0c\u6cdb\u5316\u7684\u8ffd\u6355\u8005\u7b56\u7565\u751a\u81f3\u80fd\u4e0e\u6700\u5148\u8fdbPEG\u65b9\u6cd5\u7684\u5fae\u8c03\u7b56\u7565\u76f8\u5ab2\u7f8e\u3002", "conclusion": "EPG\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u8ffd\u9003\u6e38\u620f\u4e2d\u7684\u8de8\u56fe\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5bf9\u6297\u6027\u6e38\u620f\u4e2d\u7684\u5747\u8861\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.00880", "categories": ["cs.LG", "cs.AI", "68T07, 90C15, 93E35"], "pdf": "https://arxiv.org/pdf/2511.00880", "abs": "https://arxiv.org/abs/2511.00880", "authors": ["Joonyoung Lim", "Younghwan Yoo"], "title": "KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization", "comment": "12 pages, 8 figures, submitted to ECAI 2025", "summary": "We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm\nthat combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based\nsecond-order policy optimization with safety-aware gradient manipulation. KFCPO\nleverages K-FAC to perform efficient and stable natural gradient updates by\napproximating the Fisher Information Matrix (FIM) in a layerwise, closed form\nmanner, avoiding iterative approximation overheads. To address the tradeoff\nbetween reward maximization and constraint satisfaction, we introduce a margin\naware gradient manipulation mechanism that adaptively adjusts the influence of\nreward and cost gradients based on the agent's proximity to safety boundaries.\nThis method blends gradients using a direction sensitive projection,\neliminating harmful interference and avoiding abrupt changes caused by fixed\nhard thresholds. Additionally, a minibatch level KL rollback strategy is\nadopted to ensure trust region compliance and to prevent destabilizing policy\nshifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves\n10.3% to 50.2% higher average return across environments compared to the best\nbaseline that respected the safety constraint, demonstrating superior balance\nof safety and performance.", "AI": {"tldr": "KFCPO\u662f\u4e00\u79cd\u7ed3\u5408K-FAC\u4e8c\u9636\u7b56\u7565\u4f18\u5316\u4e0e\u5b89\u5168\u611f\u77e5\u68af\u5ea6\u64cd\u4f5c\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c42\u5f0f\u95ed\u5f0f\u8fd1\u4f3cFisher\u4fe1\u606f\u77e9\u9635\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u81ea\u7136\u68af\u5ea6\u66f4\u65b0\uff0c\u5e76\u5728Safety Gymnasium\u57fa\u51c6\u4e0a\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u83b7\u5f9710.3%-50.2%\u7684\u5e73\u5747\u56de\u62a5\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u6700\u5927\u5316\u4e0e\u7ea6\u675f\u6ee1\u8db3\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56fa\u5b9a\u786c\u9608\u503c\u5bfc\u81f4\u7684\u7a81\u53d8\u548c\u4e0d\u7a33\u5b9a\u3002", "method": "\u7ed3\u5408Kronecker-Factored Approximate Curvature (K-FAC)\u4e8c\u9636\u7b56\u7565\u4f18\u5316\uff0c\u91c7\u7528\u8fb9\u754c\u611f\u77e5\u68af\u5ea6\u64cd\u4f5c\u673a\u5236\u81ea\u9002\u5e94\u8c03\u6574\u5956\u52b1\u548c\u6210\u672c\u68af\u5ea6\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u65b9\u5411\u654f\u611f\u6295\u5f71\u6d88\u9664\u6709\u5bb3\u5e72\u6270\uff0c\u5e76\u91c7\u7528\u5c0f\u6279\u91cfKL\u56de\u6eda\u7b56\u7565\u786e\u4fdd\u4fe1\u4efb\u533a\u57df\u5408\u89c4\u3002", "result": "\u5728Safety Gymnasium\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKFCPO\u76f8\u6bd4\u9075\u5b88\u5b89\u5168\u7ea6\u675f\u7684\u6700\u4f73\u57fa\u7ebf\u5b9e\u73b0\u4e8610.3%\u523050.2%\u7684\u5e73\u5747\u56de\u62a5\u63d0\u5347\uff0c\u8868\u73b0\u51fa\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u4f18\u8d8a\u5e73\u8861\u3002", "conclusion": "KFCPO\u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u7684\u4e8c\u9636\u4f18\u5316\u548c\u667a\u80fd\u68af\u5ea6\u64cd\u4f5c\uff0c\u5728\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u5e73\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01758", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01758", "abs": "https://arxiv.org/abs/2511.01758", "authors": ["Mian Wu", "Gavin Zhang", "Sewon Min", "Sergey Levine", "Aviral Kumar"], "title": "RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks", "comment": "Project page: https://mianwu01.github.io/RLAC_website/", "summary": "Open-ended generation tasks require outputs to satisfy diverse and often\nimplicit task-specific evaluation rubrics. The sheer number of relevant rubrics\nleads to prohibitively high verification costs and incomplete assessments of a\nresponse, making reinforcement learning (RL) post-training with rubric-based\nrewards difficult to scale. This problem is exacerbated by the fact that often\nthe best way to combine these rubrics into one single reward is also highly\nprompt-specific. We propose Reinforcement Learning with Adversarial Critic\n(RLAC), a post-training approach that addresses these challenges via dynamic\nrubric verification. Our approach employs a large language model (LLM) as a\ncritic that dynamically identifies only the most likely failure modes (e.g., a\nfactual error or unhandled edge case), which are then verified by an external\nvalidator to optimize both generator and critic jointly. By training both the\ngenerator and the critic, this game enhances the critic's error detection and\nthe generator's output quality while reducing required verifications. Our\nexperiments demonstrate that RLAC improves factual accuracy in text generation\nand correctness in code generation, while also outperforming exhaustive\nverification and reward model methods. We show that dynamic critics are more\neffective than fixed critics, showcasing the potential of RLAC for scaling RL\npost-training to free-form generation tasks.", "AI": {"tldr": "\u63d0\u51faRLAC\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9a8c\u8bc1\u673a\u5236\u89e3\u51b3\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e2d\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6807\u51c6\u5e26\u6765\u7684\u9a8c\u8bc1\u6210\u672c\u95ee\u9898\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u6279\u8bc4\u5668\u52a8\u6001\u8bc6\u522b\u6700\u53ef\u80fd\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u8054\u5408\u4f18\u5316\u751f\u6210\u5668\u548c\u6279\u8bc4\u5668\u3002", "motivation": "\u5f00\u653e\u751f\u6210\u4efb\u52a1\u9700\u8981\u6ee1\u8db3\u591a\u6837\u5316\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4f46\u5168\u9762\u9a8c\u8bc1\u6210\u672c\u8fc7\u9ad8\u4e14\u8bc4\u4f30\u4e0d\u5b8c\u6574\uff0c\u4f7f\u5f97\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u52a8\u6001\u6279\u8bc4\u5668\u8bc6\u522b\u6700\u53ef\u80fd\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7\u5916\u90e8\u9a8c\u8bc1\u5668\u9a8c\u8bc1\uff0c\u8054\u5408\u4f18\u5316\u751f\u6210\u5668\u548c\u6279\u8bc4\u5668\uff0c\u51cf\u5c11\u6240\u9700\u9a8c\u8bc1\u6b21\u6570\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRLAC\u5728\u6587\u672c\u751f\u6210\u4e2d\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u63d0\u9ad8\u6b63\u786e\u6027\uff0c\u4f18\u4e8e\u7a77\u4e3e\u9a8c\u8bc1\u548c\u5956\u52b1\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u6279\u8bc4\u5668\u6bd4\u56fa\u5b9a\u6279\u8bc4\u5668\u66f4\u6709\u6548\uff0cRLAC\u6709\u6f5c\u529b\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u5230\u81ea\u7531\u5f62\u5f0f\u751f\u6210\u4efb\u52a1\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01015", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01015", "abs": "https://arxiv.org/abs/2511.01015", "authors": ["Nabeel Seedat", "Jiashuo Liu", "Mihaela van der Schaar"], "title": "What's the next frontier for Data-centric AI? Data Savvy Agents", "comment": "Presented at ICLR 2025 Data-FM. Seedat & Liu contributed equally", "summary": "The recent surge in AI agents that autonomously communicate, collaborate with\nhumans and use diverse tools has unlocked promising opportunities in various\nreal-world settings. However, a vital aspect remains underexplored: how agents\nhandle data. Scalable autonomy demands agents that continuously acquire,\nprocess, and evolve their data. In this paper, we argue that data-savvy\ncapabilities should be a top priority in the design of agentic systems to\nensure reliable real-world deployment. Specifically, we propose four key\ncapabilities to realize this vision: (1) Proactive data acquisition: enabling\nagents to autonomously gather task-critical knowledge or solicit human input to\naddress data gaps; (2) Sophisticated data processing: requiring context-aware\nand flexible handling of diverse data challenges and inputs; (3) Interactive\ntest data synthesis: shifting from static benchmarks to dynamically generated\ninteractive test data for agent evaluation; and (4) Continual adaptation:\nempowering agents to iteratively refine their data and background knowledge to\nadapt to shifting environments. While current agent research predominantly\nemphasizes reasoning, we hope to inspire a reflection on the role of data-savvy\nagents as the next frontier in data-centric AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6570\u636e\u611f\u77e5\u80fd\u529b\u5e94\u6210\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u9996\u8981\u4efb\u52a1\uff0c\u5305\u62ec\u4e3b\u52a8\u6570\u636e\u83b7\u53d6\u3001\u590d\u6742\u6570\u636e\u5904\u7406\u3001\u4ea4\u4e92\u5f0f\u6d4b\u8bd5\u6570\u636e\u5408\u6210\u548c\u6301\u7eed\u9002\u5e94\u56db\u4e2a\u5173\u952e\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5ffd\u89c6\u4e86\u6570\u636e\u5904\u7406\u80fd\u529b\u3002\u4e3a\u4e86\u786e\u4fdd\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\uff0c\u9700\u8981\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u6301\u7eed\u83b7\u53d6\u3001\u5904\u7406\u548c\u53d1\u5c55\u6570\u636e\u3002", "method": "\u63d0\u51fa\u56db\u4e2a\u5173\u952e\u80fd\u529b\u6846\u67b6\uff1a\u4e3b\u52a8\u6570\u636e\u83b7\u53d6\u3001\u590d\u6742\u6570\u636e\u5904\u7406\u3001\u4ea4\u4e92\u5f0f\u6d4b\u8bd5\u6570\u636e\u5408\u6210\u548c\u6301\u7eed\u9002\u5e94\uff0c\u4ee5\u6784\u5efa\u6570\u636e\u611f\u77e5\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6570\u636e\u611f\u77e5\u667a\u80fd\u4f53\u80fd\u529b\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u91cd\u70b9\u3002", "conclusion": "\u6570\u636e\u611f\u77e5\u80fd\u529b\u5e94\u6210\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6838\u5fc3\uff0c\u8fd9\u662f\u6570\u636e\u4e3a\u4e2d\u5fc3AI\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u9886\u57df\u3002", "topic": "agent analysis"}}
{"id": "2511.01218", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01218", "abs": "https://arxiv.org/abs/2511.01218", "authors": ["Minh-Duc Nguyen", "Dung D. Le", "Phi Long Nguyen"], "title": "Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations", "comment": "Under Review", "summary": "The rapid growth of electric vehicles (EVs) necessitates the strategic\nplacement of charging stations to optimize resource utilization and minimize\nuser inconvenience. Reinforcement learning (RL) offers an innovative approach\nto identifying optimal charging station locations; however, existing methods\nface challenges due to their deterministic reward systems, which limit\nefficiency. Because real-world conditions are dynamic and uncertain, a\ndeterministic reward structure cannot fully capture the complexities of\ncharging station placement. As a result, evaluation becomes costly and\ntime-consuming, and less reflective of real-world scenarios. To address this\nchallenge, we propose a novel framework that integrates deep RL with\nagent-based simulations to model EV movement and estimate charging demand in\nreal time. Our approach employs a hybrid RL agent with dual Q-networks to\nselect optimal locations and configure charging ports, guided by a hybrid\nreward function that combines deterministic factors with simulation-derived\nfeedback. Case studies in Hanoi, Vietnam, show that our method reduces average\nwaiting times by 53.28% compared to the initial state, outperforming static\nbaseline methods. This scalable and adaptive solution enhances EV\ninfrastructure planning, effectively addressing real-world complexities and\nimproving user experience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u4ee3\u7406\u6a21\u62df\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u5e03\u5c40\uff0c\u663e\u8457\u51cf\u5c11\u7b49\u5f85\u65f6\u95f4", "motivation": "\u73b0\u6709\u5145\u7535\u7ad9\u5e03\u5c40\u65b9\u6cd5\u91c7\u7528\u786e\u5b9a\u6027\u5956\u52b1\u7cfb\u7edf\uff0c\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6761\u4ef6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u4e0d\u53cd\u6620\u771f\u5b9e\u573a\u666f", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0e\u57fa\u4e8e\u4ee3\u7406\u6a21\u62df\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u91c7\u7528\u5177\u6709\u53ccQ\u7f51\u7edc\u7684\u6df7\u5408RL\u4ee3\u7406\u6765\u9009\u62e9\u6700\u4f18\u4f4d\u7f6e\u548c\u914d\u7f6e\u5145\u7535\u7aef\u53e3\uff0c\u901a\u8fc7\u6df7\u5408\u5956\u52b1\u51fd\u6570\u7ed3\u5408\u786e\u5b9a\u6027\u56e0\u7d20\u548c\u6a21\u62df\u53cd\u9988", "result": "\u5728\u8d8a\u5357\u6cb3\u5185\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u521d\u59cb\u72b6\u6001\u5c06\u5e73\u5747\u7b49\u5f85\u65f6\u95f4\u51cf\u5c11\u4e8653.28%\uff0c\u4f18\u4e8e\u9759\u6001\u57fa\u51c6\u65b9\u6cd5", "conclusion": "\u8be5\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u589e\u5f3a\u4e86\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\uff0c\u6709\u6548\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u5e76\u6539\u5584\u7528\u6237\u4f53\u9a8c", "topic": "agentic reinforcement learning"}}
{"id": "2511.01374", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01374", "abs": "https://arxiv.org/abs/2511.01374", "authors": ["Ziqi Wang", "Jiashun Liu", "Ling Pan"], "title": "Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization", "comment": "NeurIPS 2025", "summary": "Traditional continuous deep reinforcement learning (RL) algorithms employ\ndeterministic or unimodal Gaussian actors, which cannot express complex\nmultimodal decision distributions. This limitation can hinder their performance\nin diversity-critical scenarios. There have been some attempts to design online\nmultimodal RL algorithms based on diffusion or amortized actors. However, these\nactors are intractable, making existing methods struggle with balancing\nperformance, decision diversity, and efficiency simultaneously. To overcome\nthis challenge, we first reformulate existing intractable multimodal actors\nwithin a unified framework, and prove that they can be directly optimized by\npolicy gradient via reparameterization. Then, we propose a distance-based\ndiversity regularization that does not explicitly require decision\nprobabilities. We identify two diversity-critical domains, namely multi-goal\nachieving and generative RL, to demonstrate the advantages of multimodal\npolicies and our method, particularly in terms of few-shot robustness. In\nconventional MuJoCo benchmarks, our algorithm also shows competitive\nperformance. Moreover, our experiments highlight that the amortized actor is a\npromising policy model class with strong multimodal expressivity and high\nperformance. Our code is available at https://github.com/PneuC/DrAC", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u53c2\u6570\u5316\u7684\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddd\u79bb\u591a\u6837\u6027\u6b63\u5219\u5316\u89e3\u51b3\u4e86\u4f20\u7edf\u8fde\u7eedRL\u7b97\u6cd5\u5728\u591a\u6837\u6027\u5173\u952e\u573a\u666f\u4e2d\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8fde\u7eed\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f7f\u7528\u786e\u5b9a\u6027\u6216\u5355\u5cf0\u9ad8\u65af\u7b56\u7565\uff0c\u65e0\u6cd5\u8868\u8fbe\u590d\u6742\u7684\u591a\u6a21\u6001\u51b3\u7b56\u5206\u5e03\uff0c\u8fd9\u5728\u591a\u6837\u6027\u5173\u952e\u573a\u666f\u4e2d\u4f1a\u9650\u5236\u6027\u80fd\u3002\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6216\u644a\u9500\u7b56\u7565\u7684\u591a\u6a21\u6001RL\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\u91cd\u65b0\u8868\u8ff0\u73b0\u6709\u4e0d\u53ef\u884c\u591a\u6a21\u6001\u7b56\u7565\uff0c\u8bc1\u660e\u53ef\u4ee5\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u76f4\u63a5\u4f18\u5316\u7b56\u7565\u68af\u5ea6\uff1b\u7136\u540e\u63d0\u51fa\u57fa\u4e8e\u8ddd\u79bb\u7684\u591a\u6837\u6027\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u51b3\u7b56\u6982\u7387\u3002", "result": "\u5728\u591a\u76ee\u6807\u8fbe\u6210\u548c\u751f\u6210\u5f0fRL\u7b49\u591a\u6837\u6027\u5173\u952e\u9886\u57df\u5c55\u793a\u4e86\u591a\u6a21\u6001\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u9c81\u68d2\u6027\u65b9\u9762\uff1b\u5728\u4f20\u7edfMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff1b\u5b9e\u9a8c\u8868\u660e\u644a\u9500\u7b56\u7565\u662f\u5177\u6709\u5f3a\u5927\u591a\u6a21\u6001\u8868\u8fbe\u80fd\u529b\u548c\u9ad8\u6027\u80fd\u7684\u6709\u524d\u666f\u7b56\u7565\u6a21\u578b\u7c7b\u522b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5e73\u8861\u6027\u80fd\u3001\u51b3\u7b56\u591a\u6837\u6027\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u6a21\u6001RL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u6837\u6027\u5173\u952e\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.01633", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01633", "abs": "https://arxiv.org/abs/2511.01633", "authors": ["Chengying Huan", "Ziheng Meng", "Yongchao Liu", "Zhengyi Yang", "Yun Zhu", "Yue Yun", "Shipeng Li", "Rong Gu", "Xiabao Wu", "Haitao Zhang", "Chuntao Hong", "Shaonan Ma", "Guihai Chen", "Chen Tian"], "title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving", "comment": null, "summary": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale.", "AI": {"tldr": "GLM\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u56fe\u601d\u7ef4\u94fe\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u63a8\u7406\u4efb\u52a1\u3001\u4f18\u5316LLM\u670d\u52a1\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u63a8\u7406\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u601d\u7ef4\u94fe\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u6027\u4f4e\u3001token\u4f7f\u7528\u8fc7\u591a\u3001\u5ef6\u8fdf\u9ad8\u548c\u541e\u5410\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u5355\u4ee3\u7406\u6574\u4f53\u63d0\u793a\u3001\u91cd\u590d\u4e0a\u4e0b\u6587\u91cd\u65b0\u7f16\u7801\u548c\u4f4e\u6548\u7684\u670d\u52a1\u6267\u884c\u3002", "method": "\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u5206\u7c7b\u3001\u63a8\u7406\u3001\u884c\u52a8\u751f\u6210\u548c\u56fe\u68c0\u7d22\u7b49\u4e13\u95e8\u4ee3\u7406\uff0c\u91c7\u7528\u5206\u652f\u548c\u9009\u62e9\u6027\u4e0a\u4e0b\u6587\u5171\u4eab\uff1b\u5f15\u5165\u56fe\u601d\u7ef4\u94fe\u611f\u77e5\u7684LLM\u63a8\u7406\u673a\u5236\uff0c\u5305\u62ec\u56fe\u7279\u5b9aKV\u7f13\u5b58\u7ba1\u7406\u3001\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u9a71\u9010\u548c\u6d41\u6c34\u7ebf\u6267\u884c\u3002", "result": "GLM\u5c06\u7b54\u6848\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe38%\uff0ctoken\u6210\u672c\u964d\u4f4e95.7%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e90.3%\uff0c\u541e\u5410\u91cf\u63d0\u9ad815.1\u500d\u3002", "conclusion": "GLM\u5b9e\u73b0\u4e86\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u63a8\u7406\u7684\u9ad8\u6548\u89c4\u6a21\u5316\u5e94\u7528\uff0c\u662f\u7b2c\u4e00\u4e2a\u4e0e\u4f18\u5316LLM\u670d\u52a1\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u7684\u591a\u4ee3\u7406\u56fe\u601d\u7ef4\u94fe\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.fee78e95", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "authors": ["TLDR Newsletter"], "title": "Roll up your sleeves to build the next wave of agentic AI at BUILD 2025", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "summary": "Roll up your sleeves to build the next wave of agentic AI at BUILD 2025 (Sponsor) You can't have an AI strategy without a data strategy. The most powerful agents are grounded in your data, connected across pipelines, and deployed securely at scale.Join BUILD (Nov 4-7), a free virtual developer conference where you'll learn how build reliable, production-ready agentic applications on Snowflake. Expect: Deep dives on the latest Snowflake AI capabilities and cool OSS use cases with LlamaIndex, C...", "source": "tldr", "AI": {"tldr": "Snowflake BUILD 2025\u662f\u4e00\u4e2a\u514d\u8d39\u7684\u865a\u62df\u5f00\u53d1\u8005\u5927\u4f1a\uff0c\u4e13\u6ce8\u4e8e\u6559\u6388\u5982\u4f55\u5728Snowflake\u5e73\u53f0\u4e0a\u6784\u5efa\u53ef\u9760\u3001\u751f\u4ea7\u5c31\u7eea\u7684\u667a\u80fd\u4f53\u5e94\u7528", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u5b66\u4e60\u5982\u4f55\u57fa\u4e8eSnowflake\u5e73\u53f0\u6784\u5efa\u5f3a\u5927\u7684AI\u667a\u80fd\u4f53\u5e94\u7528\uff0c\u5c06AI\u6218\u7565\u4e0e\u6570\u636e\u6218\u7565\u76f8\u7ed3\u5408", "method": "\u901a\u8fc7\u865a\u62df\u5f00\u53d1\u8005\u5927\u4f1a\u5f62\u5f0f\uff0c\u63d0\u4f9bSnowflake\u6700\u65b0AI\u529f\u80fd\u7684\u6df1\u5ea6\u8bb2\u89e3\u548c\u5f00\u6e90\u7528\u4f8b\u5c55\u793a", "result": "\u53c2\u4e0e\u8005\u5c06\u5b66\u4e60\u5230\u5982\u4f55\u6784\u5efa\u57fa\u4e8e\u6570\u636e\u7684\u667a\u80fd\u4f53\u5e94\u7528\uff0c\u5e76\u5b9e\u73b0\u5b89\u5168\u7684\u5927\u89c4\u6a21\u90e8\u7f72", "conclusion": "BUILD 2025\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6784\u5efa\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53AI\u6240\u9700\u7684\u6280\u80fd\u548c\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2510.b7ef8741", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "authors": ["TLDR Newsletter"], "title": "Hands-on labs", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "summary": "Roll up your sleeves to build the next wave of agentic AI at BUILD 2025 (Sponsor) You can't have an AI strategy without a data strategy. The most powerful agents are grounded in your data, connected across pipelines, and deployed securely at scale.Join BUILD (Nov 4-7), a free virtual developer conference where you'll learn how build reliable, production-ready agentic applications on Snowflake. Expect: Deep dives on the latest Snowflake AI capabilities and cool OSS use cases with LlamaIndex, C...", "source": "tldr", "AI": {"tldr": "Snowflake BUILD 2025\u662f\u4e00\u4e2a\u514d\u8d39\u7684\u865a\u62df\u5f00\u53d1\u8005\u4f1a\u8bae\uff0c\u4e13\u6ce8\u4e8e\u6559\u6388\u5982\u4f55\u5728Snowflake\u5e73\u53f0\u4e0a\u6784\u5efa\u53ef\u9760\u3001\u751f\u4ea7\u5c31\u7eea\u7684\u667a\u80fd\u4ee3\u7406\u5e94\u7528\u3002", "motivation": "\u5e2e\u52a9\u4f01\u4e1a\u5236\u5b9aAI\u6218\u7565\uff0c\u5f3a\u8c03\u6570\u636e\u7b56\u7565\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u57fa\u4e8eSnowflake\u5e73\u53f0\u6784\u5efa\u5f3a\u5927\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u865a\u62df\u5f00\u53d1\u8005\u4f1a\u8bae\u5f62\u5f0f\uff0c\u63d0\u4f9b\u5173\u4e8eSnowflake\u6700\u65b0AI\u529f\u80fd\u7684\u6df1\u5165\u8bb2\u89e3\uff0c\u4ee5\u53ca\u4f7f\u7528LlamaIndex\u7b49\u5f00\u6e90\u5de5\u5177\u7684\u5b9e\u7528\u6848\u4f8b\u3002", "result": "\u53c2\u4e0e\u8005\u5c06\u5b66\u4e60\u5230\u5982\u4f55\u6784\u5efa\u57fa\u4e8e\u6570\u636e\u7684\u667a\u80fd\u4ee3\u7406\u5e94\u7528\uff0c\u8fd9\u4e9b\u5e94\u7528\u80fd\u591f\u5728\u7ba1\u9053\u95f4\u8fde\u63a5\u5e76\u5b89\u5168\u5730\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "conclusion": "BUILD 2025\u4f1a\u8bae\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6784\u5efa\u4e0b\u4e00\u4ee3\u667a\u80fd\u4ee3\u7406AI\u6240\u9700\u7684\u6280\u80fd\u548c\u5de5\u5177\u3002", "topic": "swe application"}}
{"id": "tldr.2510.64edbef2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "authors": ["TLDR Newsletter"], "title": "Inspiration from industry leaders", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.snowflake.com%2Fen%2Fbuild%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=snowflake-build25/2/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/W6GsY1QIVJfig4xmVbvP1kZRZn5MOufhoqAYva8aM-0=429", "summary": "Roll up your sleeves to build the next wave of agentic AI at BUILD 2025 (Sponsor) You can't have an AI strategy without a data strategy. The most powerful agents are grounded in your data, connected across pipelines, and deployed securely at scale.Join BUILD (Nov 4-7), a free virtual developer conference where you'll learn how build reliable, production-ready agentic applications on Snowflake. Expect: Deep dives on the latest Snowflake AI capabilities and cool OSS use cases with LlamaIndex, C...", "source": "tldr", "AI": {"tldr": "Snowflake BUILD 2025\u662f\u4e00\u4e2a\u514d\u8d39\u7684\u865a\u62df\u5f00\u53d1\u8005\u4f1a\u8bae\uff0c\u4e13\u6ce8\u4e8e\u6559\u6388\u5982\u4f55\u5728Snowflake\u5e73\u53f0\u4e0a\u6784\u5efa\u53ef\u9760\u3001\u751f\u4ea7\u5c31\u7eea\u7684\u4ee3\u7406\u5e94\u7528", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u5b66\u4e60\u5982\u4f55\u57fa\u4e8eSnowflake\u5e73\u53f0\u6784\u5efa\u5f3a\u5927\u7684AI\u4ee3\u7406\u5e94\u7528\uff0c\u8fd9\u4e9b\u4ee3\u7406\u9700\u8981\u4ee5\u6570\u636e\u4e3a\u57fa\u7840\uff0c\u8de8\u7ba1\u9053\u8fde\u63a5\u5e76\u5b89\u5168\u90e8\u7f72", "method": "\u901a\u8fc7\u865a\u62df\u4f1a\u8bae\u5f62\u5f0f\uff0c\u63d0\u4f9bSnowflake\u6700\u65b0AI\u529f\u80fd\u7684\u6df1\u5ea6\u8bb2\u89e3\u548c\u5f00\u6e90\u7528\u4f8b\u5c55\u793a", "result": "\u5f00\u53d1\u8005\u5c06\u80fd\u591f\u6784\u5efa\u57fa\u4e8e\u6570\u636e\u7684\u53ef\u9760\u4ee3\u7406\u5e94\u7528", "conclusion": "BUILD 2025\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6784\u5efa\u4e0b\u4e00\u4ee3\u4ee3\u7406AI\u6240\u9700\u7684\u77e5\u8bc6\u548c\u5de5\u5177", "topic": "swe application"}}
{"id": "tldr.2510.00125e74", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tobyord.com%2Fwriting%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/8YNtbzYmhKt0VDAViHAqyLMnQzWgO_VXBdfDLx8sYdI=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tobyord.com%2Fwriting%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/8YNtbzYmhKt0VDAViHAqyLMnQzWgO_VXBdfDLx8sYdI=429", "authors": ["TLDR Newsletter"], "title": "How Well Does RL Scale?", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tobyord.com%2Fwriting%2Fhow-well-does-rl-scale%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/8YNtbzYmhKt0VDAViHAqyLMnQzWgO_VXBdfDLx8sYdI=429", "summary": "How Well Does RL Scale? (14 minute read) Improving AI capabilities either involves scaling the amount of compute used for RL during training or scaling the amount of compute used for inference during deployment. It's important to know where capability gains come from because scaling up the inference compute has very different implications than scaling up the training compute. While RL has provided impressive gains, we've reached a point where it is too expensive to go much further. This leave...", "source": "tldr", "AI": {"tldr": "\u5206\u6790RL\u6269\u5c55\u6027\u7684\u7814\u7a76\uff0c\u63a2\u8ba8\u8bad\u7ec3\u8ba1\u7b97\u4e0e\u63a8\u7406\u8ba1\u7b97\u6269\u5c55\u5bf9AI\u80fd\u529b\u63d0\u5347\u7684\u4e0d\u540c\u5f71\u54cd", "motivation": "\u7406\u89e3RL\u80fd\u529b\u63d0\u5347\u7684\u6765\u6e90\uff0c\u56e0\u4e3a\u8bad\u7ec3\u8ba1\u7b97\u6269\u5c55\u548c\u63a8\u7406\u8ba1\u7b97\u6269\u5c55\u5177\u6709\u4e0d\u540c\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u76ee\u524dRL\u6269\u5c55\u5df2\u53d8\u5f97\u8fc7\u4e8e\u6602\u8d35", "method": "\u5206\u6790RL\u6269\u5c55\u7684\u73b0\u72b6\u548c\u6210\u672c\u6548\u76ca\uff0c\u6bd4\u8f83\u8bad\u7ec3\u8ba1\u7b97\u4e0e\u63a8\u7406\u8ba1\u7b97\u6269\u5c55\u7684\u4e0d\u540c\u5f71\u54cd", "result": "RL\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u76ee\u524d\u5df2\u8fbe\u5230\u96be\u4ee5\u7ee7\u7eed\u5927\u89c4\u6a21\u6269\u5c55\u7684\u9636\u6bb5", "conclusion": "\u9700\u8981\u91cd\u65b0\u8bc4\u4f30RL\u6269\u5c55\u7b56\u7565\uff0c\u5bfb\u627e\u66f4\u53ef\u6301\u7eed\u7684\u53d1\u5c55\u8def\u5f84", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.fe4a9bf0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrHEVcC/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/WF93i4wBLKeBCGjKuY1t1BoaRs5KUpDUFgy5D10Qr6Q=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrHEVcC/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/WF93i4wBLKeBCGjKuY1t1BoaRs5KUpDUFgy5D10Qr6Q=429", "authors": ["TLDR Newsletter"], "title": "Introducing Aardvark: OpenAI's agentic security researcher", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FrHEVcC/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/WF93i4wBLKeBCGjKuY1t1BoaRs5KUpDUFgy5D10Qr6Q=429", "summary": "Introducing Aardvark: OpenAI's agentic security researcher (5 minute read) Aardvark, currently in private beta, is a GPT-5-powered agent that autonomously scans code repositories to find security vulnerabilities, validate exploitability, and propose patches. It monitors commits in real-time, generates threat models for entire repositories, and integrates directly with GitHub workflows to deliver one-click patches, similar to Google's CodeMender.", "source": "tldr", "AI": {"tldr": "Aardvark\u662fOpenAI\u5f00\u53d1\u7684\u57fa\u4e8eGPT-5\u7684\u81ea\u4e3b\u5b89\u5168\u7814\u7a76\u4ee3\u7406\uff0c\u80fd\u591f\u626b\u63cf\u4ee3\u7801\u5e93\u53d1\u73b0\u5b89\u5168\u6f0f\u6d1e\u3001\u9a8c\u8bc1\u53ef\u5229\u7528\u6027\u5e76\u63d0\u51fa\u8865\u4e01\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u5b89\u5168\u7814\u7a76\u7684AI\u4ee3\u7406\uff0c\u4ee5\u5b9e\u65f6\u76d1\u63a7\u4ee3\u7801\u63d0\u4ea4\u3001\u81ea\u52a8\u53d1\u73b0\u6f0f\u6d1e\u5e76\u751f\u6210\u4fee\u590d\u65b9\u6848\uff0c\u63d0\u9ad8\u8f6f\u4ef6\u5b89\u5168\u9632\u62a4\u6548\u7387\u3002", "method": "\u4f7f\u7528GPT-5\u9a71\u52a8\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u96c6\u6210GitHub\u5de5\u4f5c\u6d41\uff0c\u5b9e\u65f6\u76d1\u63a7\u4ee3\u7801\u63d0\u4ea4\uff0c\u81ea\u52a8\u751f\u6210\u5a01\u80c1\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e00\u952e\u8865\u4e01\u529f\u80fd\u3002", "result": "\u76ee\u524d\u5904\u4e8e\u79c1\u6709\u6d4b\u8bd5\u9636\u6bb5\uff0c\u80fd\u591f\u81ea\u4e3b\u626b\u63cf\u4ee3\u7801\u5e93\u3001\u9a8c\u8bc1\u6f0f\u6d1e\u53ef\u5229\u7528\u6027\u5e76\u751f\u6210\u4fee\u590d\u8865\u4e01\u3002", "conclusion": "Aardvark\u4ee3\u8868\u4e86AI\u5728\u4ee3\u7801\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6f0f\u6d1e\u53d1\u73b0\u548c\u4fee\u590d\u7684\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2510.300e5872", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FqJYMbrabcQqCZ7iqm%2Fimpossiblebench-measuring-reward-hacking-in-llm-coding-1%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/vJVkPyCs64uLjwsoKhNAt6iW348AS0QVIzopeME7o3E=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FqJYMbrabcQqCZ7iqm%2Fimpossiblebench-measuring-reward-hacking-in-llm-coding-1%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/vJVkPyCs64uLjwsoKhNAt6iW348AS0QVIzopeME7o3E=429", "authors": ["TLDR Newsletter"], "title": "ImpossibleBench: Measuring Reward Hacking in LLM Coding Agents", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lesswrong.com%2Fposts%2FqJYMbrabcQqCZ7iqm%2Fimpossiblebench-measuring-reward-hacking-in-llm-coding-1%3Futm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/vJVkPyCs64uLjwsoKhNAt6iW348AS0QVIzopeME7o3E=429", "summary": "ImpossibleBench: Measuring Reward Hacking in LLM Coding Agents (9 minute read) LLM-powered coding agents have been observed exploiting loopholes in tests or scoring systems rather than solving the actual tasks specified. ImpossibleBench was created to systematically measure this behavior. Its creators took existing coding benchmarks and manipulated their unit tests to directly conflict with the natural language specifications to create impossible tasks where models must choose between followi...", "source": "tldr", "AI": {"tldr": "ImpossibleBench\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u6d4b\u91cfLLM\u7f16\u7801\u4ee3\u7406\u4e2d\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u64cd\u7eb5\u73b0\u6709\u7f16\u7801\u57fa\u51c6\u7684\u5355\u5143\u6d4b\u8bd5\u4f7f\u5176\u4e0e\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u76f4\u63a5\u51b2\u7a81\uff0c\u521b\u5efa\u4e0d\u53ef\u80fd\u5b8c\u6210\u7684\u4efb\u52a1\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u88ab\u53d1\u73b0\u4f1a\u5229\u7528\u6d4b\u8bd5\u6216\u8bc4\u5206\u7cfb\u7edf\u4e2d\u7684\u6f0f\u6d1e\uff0c\u800c\u4e0d\u662f\u89e3\u51b3\u5b9e\u9645\u6307\u5b9a\u7684\u4efb\u52a1\u3002\u4e3a\u4e86\u7cfb\u7edf\u6027\u5730\u6d4b\u91cf\u8fd9\u79cd\u884c\u4e3a\uff0c\u521b\u5efa\u4e86ImpossibleBench\u3002", "method": "\u91c7\u7528\u73b0\u6709\u7684\u7f16\u7801\u57fa\u51c6\uff0c\u64cd\u7eb5\u5176\u5355\u5143\u6d4b\u8bd5\u4f7f\u5176\u4e0e\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u76f4\u63a5\u51b2\u7a81\uff0c\u521b\u5efa\u4e0d\u53ef\u80fd\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u8feb\u4f7f\u6a21\u578b\u5728\u9075\u5faa\u89c4\u8303\u8fd8\u662f\u901a\u8fc7\u6d4b\u8bd5\u4e4b\u95f4\u505a\u51fa\u9009\u62e9\u3002", "result": "\u901a\u8fc7\u8fd9\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u53ef\u4ee5\u6d4b\u91cfLLM\u7f16\u7801\u4ee3\u7406\u662f\u5426\u503e\u5411\u4e8e\u5229\u7528\u7cfb\u7edf\u6f0f\u6d1e\u800c\u975e\u771f\u6b63\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "ImpossibleBench\u4e3a\u8bc4\u4f30LLM\u7f16\u7801\u4ee3\u7406\u7684\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6d4b\u91cf\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.5ae70d26", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=IDSAMqip6ms%26utm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/fT0_VcsYKAjldA15IxFxe2dcM0N6dF93Yjf5iYP-vHU=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=IDSAMqip6ms%26utm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/fT0_VcsYKAjldA15IxFxe2dcM0N6dF93Yjf5iYP-vHU=429", "authors": ["TLDR Newsletter"], "title": "The Secrets of Claude Code From the Engineers Who Built It", "comment": "Source: TLDR Newsletter, Date: 2025-10-31, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=IDSAMqip6ms%26utm_source=tldrai/1/0100019a3a6954f2-8ed69b34-7be5-4289-90a0-a01d22252e54-000000/fT0_VcsYKAjldA15IxFxe2dcM0N6dF93Yjf5iYP-vHU=429", "summary": "The Secrets of Claude Code From the Engineers Who Built It (1 hour video) Claude Code creators, Cat Wu and Boris Cherny, discuss the product philosophy and technical workflows behind Anthropic's coding agent. They cover how their engineers use competing subagents for cleaner results, the team's \"unshipping\" approach to balance simplicity with power, and future form factors to make the tool more autonomous and accessible to non-technical users.", "source": "tldr", "AI": {"tldr": "Claude Code\u5de5\u7a0b\u5e08\u5206\u4eab\u4ea7\u54c1\u54f2\u5b66\u548c\u6280\u672f\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u4f7f\u7528\u7ade\u4e89\u6027\u5b50\u4ee3\u7406\u83b7\u5f97\u66f4\u6e05\u6670\u7ed3\u679c\u3001\u5e73\u8861\u7b80\u5355\u6027\u4e0e\u529f\u80fd\u7684\"unshipping\"\u65b9\u6cd5\uff0c\u4ee5\u53ca\u672a\u6765\u8ba9\u5de5\u5177\u66f4\u81ea\u4e3b\u4e14\u5bf9\u975e\u6280\u672f\u7528\u6237\u66f4\u6613\u7528\u7684\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5206\u4eabAnthropic\u7f16\u7801\u4ee3\u7406Claude Code\u7684\u8bbe\u8ba1\u7406\u5ff5\u548c\u6280\u672f\u5b9e\u73b0\u7ec6\u8282\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u5176\u5de5\u4f5c\u539f\u7406\u548c\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u91c7\u7528\u7ade\u4e89\u6027\u5b50\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\"unshipping\"\u65b9\u6cd5\u5e73\u8861\u529f\u80fd\u4e0e\u7b80\u6d01\u6027\uff0c\u5de5\u7a0b\u5e08\u56e2\u961f\u5206\u4eab\u5b9e\u9645\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u4ea7\u751f\u66f4\u6e05\u6670\u4ee3\u7801\u7ed3\u679c\u7684\u7f16\u7801\u4ee3\u7406\u5de5\u5177\uff0c\u5e76\u5efa\u7acb\u4e86\u6709\u6548\u7684\u5f00\u53d1\u65b9\u6cd5\u8bba\u3002", "conclusion": "Claude Code\u901a\u8fc7\u521b\u65b0\u7684\u6280\u672f\u65b9\u6cd5\u5728\u7f16\u7801\u4ee3\u7406\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u672a\u6765\u5c06\u7ee7\u7eed\u5411\u66f4\u81ea\u4e3b\u548c\u6613\u7528\u7684\u65b9\u5411\u53d1\u5c55\u3002", "topic": "code agent"}}
{"id": "tldr.2511.114b910b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sshh.io%2Fp%2Fhow-i-use-every-claude-code-feature%3Futm_source=tldrnewsletter/1/0100019a497555c5-c5835607-8c07-4441-b07c-136126961070-000000/Tt6nm0e-6rmQOhKGGm5aNZzYmVfqWDK27ZHu_SKNIWM=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sshh.io%2Fp%2Fhow-i-use-every-claude-code-feature%3Futm_source=tldrnewsletter/1/0100019a497555c5-c5835607-8c07-4441-b07c-136126961070-000000/Tt6nm0e-6rmQOhKGGm5aNZzYmVfqWDK27ZHu_SKNIWM=429", "authors": ["TLDR Newsletter"], "title": "How I Use Every Claude Code Feature", "comment": "Source: TLDR Newsletter, Date: 2025-11-03, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.sshh.io%2Fp%2Fhow-i-use-every-claude-code-feature%3Futm_source=tldrnewsletter/1/0100019a497555c5-c5835607-8c07-4441-b07c-136126961070-000000/Tt6nm0e-6rmQOhKGGm5aNZzYmVfqWDK27ZHu_SKNIWM=429", "summary": "How I Use Every Claude Code Feature (18 minute read) The post covers useful Claude Code features, including the Claude.md file, custom slash commands, Subagents, Hooks, and GitHub Actions.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Claude Code\u7684\u5404\u79cd\u5b9e\u7528\u529f\u80fd\uff0c\u5305\u62ecClaude.md\u6587\u4ef6\u3001\u81ea\u5b9a\u4e49\u659c\u6760\u547d\u4ee4\u3001\u5b50\u4ee3\u7406\u3001\u94a9\u5b50\u548cGitHub Actions\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528Claude Code\u7684\u5404\u9879\u529f\u80fd\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u7684\u529f\u80fd\u4ecb\u7ecd\u548c\u4f7f\u7528\u793a\u4f8b\u6765\u5c55\u793aClaude Code\u7684\u5404\u9879\u7279\u6027\u3002", "result": "\u8bfb\u8005\u80fd\u591f\u5168\u9762\u4e86\u89e3Claude Code\u7684\u529f\u80fd\uff0c\u5e76\u5b66\u4f1a\u5982\u4f55\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u5e94\u7528\u8fd9\u4e9b\u529f\u80fd\u3002", "conclusion": "Claude Code\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u529f\u80fd\u96c6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\u3002", "topic": "swe application"}}
{"id": "tldr.2511.77ade295", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Ops3/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/7yBS1If6s0EyeRBdD8EqiMmfetT8nPWDtqmp5jeDxIY=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Ops3/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/7yBS1If6s0EyeRBdD8EqiMmfetT8nPWDtqmp5jeDxIY=429", "authors": ["TLDR Newsletter"], "title": "AI coding agents should live in your internal developer portal", "comment": "Source: TLDR Newsletter, Date: 2025-11-03, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.port.io%2F%3Futm_source=newsletter%26utm_medium=email%26utm_campaign=TLDR%26utm_content=Ops3/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/7yBS1If6s0EyeRBdD8EqiMmfetT8nPWDtqmp5jeDxIY=429", "summary": "AI coding agents should live in your internal developer portal (Sponsor) Manual engineering is fast becoming agentic engineering \u2013 but uncontrolled AI agents can do real damage in real-time. Port's internal developer portal lets you rapidly build agentic flows with robust guardrails and context lakes, so your agents make decisions you can stand behind. Try Port now, or learn more about the future of agentic engineering.", "source": "tldr", "AI": {"tldr": "Port's internal developer portal enables building AI coding agents with guardrails and context lakes for safe agentic engineering.", "motivation": "Uncontrolled AI agents can cause real-time damage, so there's a need for robust guardrails and context management in agentic engineering.", "method": "Using Port's internal developer portal to build agentic flows with built-in guardrails and context lakes.", "result": "Enables creation of AI coding agents that make decisions developers can stand behind.", "conclusion": "Port provides a solution for safe and controlled agentic engineering in software development.", "topic": "swe application"}}
{"id": "tldr.2511.f0152625", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fplatform-engineering-labs%2Fformae%3Futm_source=tldrdevops/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/oAvGvnXWAHMConCG0HKGTcIB9O2sfcjQPXhD-7orbbc=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fplatform-engineering-labs%2Fformae%3Futm_source=tldrdevops/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/oAvGvnXWAHMConCG0HKGTcIB9O2sfcjQPXhD-7orbbc=429", "authors": ["TLDR Newsletter"], "title": "Formae", "comment": "Source: TLDR Newsletter, Date: 2025-11-03, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fplatform-engineering-labs%2Fformae%3Futm_source=tldrdevops/1/0100019a499c1bc8-d2199b9e-4398-44fc-b3b4-d2d63624df9c-000000/oAvGvnXWAHMConCG0HKGTcIB9O2sfcjQPXhD-7orbbc=429", "summary": "Formae (GitHub Repo) Formae, a 100% code-based, agentic Infrastructure-as-Code (IaC) tool built from scratch, was designed to keep infrastructure code automatically in sync and adaptable for various team roles. Supporting GitOps without enforcing it, Formae merges changes from other tools like Terraform and ClickOps, providing a consistent, version-controlled view of infrastructure.", "source": "tldr", "AI": {"tldr": "Formae\u662f\u4e00\u4e2a\u5b8c\u5168\u57fa\u4e8e\u4ee3\u7801\u7684\u3001\u81ea\u4e3b\u7684\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u5de5\u5177\uff0c\u65e8\u5728\u4fdd\u6301\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u81ea\u52a8\u540c\u6b65\u5e76\u9002\u5e94\u4e0d\u540c\u56e2\u961f\u89d2\u8272\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u540c\u6b65\u95ee\u9898\uff0c\u652f\u6301GitOps\u4f46\u4e0d\u5f3a\u5236\u8981\u6c42\uff0c\u80fd\u591f\u6574\u5408\u6765\u81eaTerraform\u548cClickOps\u7b49\u5176\u4ed6\u5de5\u5177\u7684\u53d8\u66f4\u3002", "method": "\u6784\u5efa\u4e00\u4e2a100%\u57fa\u4e8e\u4ee3\u7801\u7684\u81ea\u4e3bIaC\u5de5\u5177\uff0c\u652f\u6301GitOps\u5de5\u4f5c\u6d41\uff0c\u80fd\u591f\u5408\u5e76\u6765\u81ea\u4e0d\u540c\u5de5\u5177\u7684\u53d8\u66f4\uff0c\u63d0\u4f9b\u4e00\u81f4\u4e14\u7248\u672c\u63a7\u5236\u7684\u57fa\u7840\u8bbe\u65bd\u89c6\u56fe\u3002", "result": "\u5f00\u53d1\u4e86Formae\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u57fa\u7840\u8bbe\u65bd\u4ee3\u7801\u7684\u81ea\u52a8\u540c\u6b65\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u4e0d\u540c\u56e2\u961f\u89d2\u8272\u63d0\u4f9b\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u754c\u9762\u3002", "conclusion": "Formae\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4ee3\u7801\u540c\u6b65\u7684\u540c\u65f6\u9002\u5e94\u591a\u6837\u5316\u7684\u56e2\u961f\u5de5\u4f5c\u6d41\u7a0b\u3002", "topic": "swe application"}}
{"id": "tldr.2511.9d4a4747", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjetxu-llm.github.io%2Fposts%2Flow-noise-code-review%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/ClboS-qnkUIZ7OQJKVpU0gdQYZ78CJx2syARorCL-lo=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjetxu-llm.github.io%2Fposts%2Flow-noise-code-review%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/ClboS-qnkUIZ7OQJKVpU0gdQYZ78CJx2syARorCL-lo=429", "authors": ["TLDR Newsletter"], "title": "Drowning in AI Code Review Noise? A Framework to Measure Signal vs. Noise", "comment": "Source: TLDR Newsletter, Date: 2025-11-03, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjetxu-llm.github.io%2Fposts%2Flow-noise-code-review%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/ClboS-qnkUIZ7OQJKVpU0gdQYZ78CJx2syARorCL-lo=429", "summary": "Drowning in AI Code Review Noise? A Framework to Measure Signal vs. Noise (8 minute read) AI code review tools often generate excessive comments, with up to 80% being irrelevant noise that buries critical issues. A framework for measuring the signal-to-noise ratio of AI code review tools is to classify comments into three tiers based on severity: critical, important, and noise. The signal ratio, calculated as (Tier 1 + Tier 2) / Total comments, should ideally be above 60% to ensure the tool i...", "source": "tldr", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u91cfAI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u4fe1\u566a\u6bd4\u7684\u6846\u67b6\uff0c\u5c06\u8bc4\u8bba\u6309\u4e25\u91cd\u7a0b\u5ea6\u5206\u4e3a\u4e09\u4e2a\u7b49\u7ea7\uff1a\u5173\u952e\u3001\u91cd\u8981\u548c\u566a\u97f3", "motivation": "AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u7ecf\u5e38\u4ea7\u751f\u8fc7\u591a\u8bc4\u8bba\uff0c\u5176\u4e2d\u9ad8\u8fbe80%\u662f\u65e0\u5173\u566a\u97f3\uff0c\u63a9\u76d6\u4e86\u5173\u952e\u95ee\u9898", "method": "\u901a\u8fc7\u5c06\u8bc4\u8bba\u5206\u7c7b\u4e3a\u4e09\u4e2a\u4e25\u91cd\u7a0b\u5ea6\u7b49\u7ea7\uff08\u5173\u952e\u3001\u91cd\u8981\u3001\u566a\u97f3\uff09\u6765\u6d4b\u91cf\u4fe1\u566a\u6bd4\uff0c\u4fe1\u53f7\u6bd4\u7387\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff08\u7b2c1\u5c42+\u7b2c2\u5c42\uff09/\u603b\u8bc4\u8bba\u6570", "result": "\u7406\u60f3\u7684\u4fe1\u53f7\u6bd4\u7387\u5e94\u9ad8\u4e8e60%\uff0c\u4ee5\u786e\u4fdd\u5de5\u5177\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u8bc4\u4f30AI\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u7684\u8d28\u91cf\uff0c\u51cf\u5c11\u566a\u97f3\u5e72\u6270\uff0c\u63d0\u9ad8\u5ba1\u67e5\u6548\u7387", "topic": "agent analysis"}}
{"id": "tldr.2511.c68ad8f4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwords.filippo.io%2Fclaude-debugging%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/u5n_RfnJxs1986-kQExXeF4VfmdyMuYEUCJ9s_y4m64=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwords.filippo.io%2Fclaude-debugging%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/u5n_RfnJxs1986-kQExXeF4VfmdyMuYEUCJ9s_y4m64=429", "authors": ["TLDR Newsletter"], "title": "Claude Code Can Debug Low-level Cryptography", "comment": "Source: TLDR Newsletter, Date: 2025-11-03, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwords.filippo.io%2Fclaude-debugging%2F%3Futm_source=tldrwebdev/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/u5n_RfnJxs1986-kQExXeF4VfmdyMuYEUCJ9s_y4m64=429", "summary": "Claude Code Can Debug Low-level Cryptography (8 minute read) Claude Code successfully debugged complex low-level cryptography issues in a new Go implementation of the ML-DSA algorithm.", "source": "tldr", "AI": {"tldr": "Claude Code\u6210\u529f\u8c03\u8bd5\u4e86Go\u8bed\u8a00\u5b9e\u73b0\u7684ML-DSA\u7b97\u6cd5\u4e2d\u7684\u590d\u6742\u4f4e\u5c42\u5bc6\u7801\u5b66\u95ee\u9898", "motivation": "\u9a8c\u8bc1Claude Code\u5728\u5904\u7406\u590d\u6742\u4f4e\u5c42\u5bc6\u7801\u5b66\u8c03\u8bd5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u65b0\u5174\u5bc6\u7801\u7b97\u6cd5\u5b9e\u73b0\u4e2d\u7684\u8868\u73b0", "method": "\u4f7f\u7528Claude Code\u5bf9Go\u8bed\u8a00\u5b9e\u73b0\u7684ML-DSA\u7b97\u6cd5\u8fdb\u884c\u8c03\u8bd5\uff0c\u8bc6\u522b\u548c\u4fee\u590d\u4f4e\u5c42\u5bc6\u7801\u5b66\u5b9e\u73b0\u4e2d\u7684\u95ee\u9898", "result": "Claude Code\u6210\u529f\u8bc6\u522b\u5e76\u8c03\u8bd5\u4e86ML-DSA\u7b97\u6cd5\u5b9e\u73b0\u4e2d\u7684\u590d\u6742\u5bc6\u7801\u5b66\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f4e\u5c42\u5bc6\u7801\u5b66\u8c03\u8bd5\u65b9\u9762\u7684\u80fd\u529b", "conclusion": "Claude Code\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u4f4e\u5c42\u5bc6\u7801\u5b66\u8c03\u8bd5\u4efb\u52a1\uff0c\u4e3a\u5bc6\u7801\u7b97\u6cd5\u5b9e\u73b0\u7684\u8d28\u91cf\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2511.05b6170f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/rSmo8ITV1fxON3zqNpwZdb4LktgbpnemFTPc2fePDxg=429", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/rSmo8ITV1fxON3zqNpwZdb4LktgbpnemFTPc2fePDxg=429", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-11-03, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdev%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a49b26787-7e78db3e-426c-441e-9975-a5efe91a9f50-000000/rSmo8ITV1fxON3zqNpwZdb4LktgbpnemFTPc2fePDxg=429", "summary": "Claude Code Can Debug Low-level Cryptography (8 minute read) Claude Code successfully debugged complex low-level cryptography issues in a new Go implementation of the ML-DSA algorithm.", "source": "tldr", "AI": {"tldr": "Claude Code\u6210\u529f\u8c03\u8bd5\u4e86Go\u8bed\u8a00\u5b9e\u73b0\u7684ML-DSA\u7b97\u6cd5\u4e2d\u7684\u590d\u6742\u4f4e\u5c42\u5bc6\u7801\u5b66\u95ee\u9898", "motivation": "\u9a8c\u8bc1Claude Code\u5728\u5904\u7406\u590d\u6742\u4f4e\u5c42\u5bc6\u7801\u5b66\u4ee3\u7801\u8c03\u8bd5\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u65b0\u5174\u5bc6\u7801\u5b66\u7b97\u6cd5\u5b9e\u73b0\u4e2d\u7684\u8868\u73b0", "method": "\u4f7f\u7528Claude Code\u5bf9Go\u8bed\u8a00\u5b9e\u73b0\u7684ML-DSA\u7b97\u6cd5\u8fdb\u884c\u8c03\u8bd5\uff0c\u5206\u6790\u5176\u5904\u7406\u4f4e\u5c42\u5bc6\u7801\u5b66\u95ee\u9898\u7684\u80fd\u529b", "result": "Claude Code\u6210\u529f\u8bc6\u522b\u5e76\u4fee\u590d\u4e86ML-DSA\u7b97\u6cd5\u5b9e\u73b0\u4e2d\u7684\u590d\u6742\u5bc6\u7801\u5b66\u9519\u8bef\uff0c\u5c55\u793a\u4e86\u5728\u4f4e\u5c42\u4ee3\u7801\u8c03\u8bd5\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "Claude Code\u5728\u8c03\u8bd5\u590d\u6742\u4f4e\u5c42\u5bc6\u7801\u5b66\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u65b0\u5174\u5bc6\u7801\u5b66\u7b97\u6cd5\u7684\u5b9e\u73b0\u95ee\u9898", "topic": "code agent"}}
{"id": "wechat.2511.c2c46fd9", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNTk5NjgwMQ==&mid=2247483973&idx=1&sn=11dd11db3a30464b0248bbdb427b3a87&chksm=c36e23584dfe71f50ebcfca48195b88d0545bdaa57dfe9bd5818170a219660bece9ad821572e#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNTk5NjgwMQ==&mid=2247483973&idx=1&sn=11dd11db3a30464b0248bbdb427b3a87&chksm=c36e23584dfe71f50ebcfca48195b88d0545bdaa57dfe9bd5818170a219660bece9ad821572e#rd", "authors": ["AI\u65b0\u673a\u9047"], "title": "\u673a\u5668\u4eba\u201c\u81ea\u4e3b\u8fdb\u5316\u201d\u65f6\u4ee3\u5f00\u542f\uff1a\u5168\u7403\u9996\u4e2a\u5177\u8eab\u667a\u80fd<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6280\u672f\u5f15\u7206\u5de5\u4e1a\u53d8\u9769", "comment": "Source: WeChat, Published: 2025-11-04 13:00:00", "summary": "\u6700\u65b0\u7814\u7a76\u63d0\u51fa\u7684\u77e5\u8bc6\u7ec4\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53c2\u6570\u5206\u79bb\u4e3a\u4efb\u52a1\u65e0\u5173\u548c\u4efb\u52a1\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u8fde\u7eed\u5b66\u4e60\u591a\u4efb\u52a1\u65f6\u4fdd\u6301\u539f\u6709\u6280\u80fd\uff0c\u540c\u65f6\u9ad8\u6548\u638c\u63e1\u65b0\u6280\u80fd\u3002", "AI": {"tldr": "\u6700\u65b0\u7814\u7a76\u63d0\u51fa\u7684\u77e5\u8bc6\u7ec4\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53c2\u6570\u5206\u79bb\u4e3a\u4efb\u52a1\u65e0\u5173\u548c\u4efb\u52a1\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u8fde\u7eed\u5b66\u4e60\u591a\u4efb\u52a1\u65f6\u4fdd\u6301\u539f\u6709\u6280\u80fd\uff0c\u540c\u65f6\u9ad8\u6548\u638c\u63e1\u65b0\u6280\u80fd\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.94ff3bc1", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5OTAxMjQ0OA==&mid=2655002493&idx=1&sn=6ec5b0a366445d23b474b4ae36e9b407&chksm=bc381bd21cb13051c84c2d68f8a9206a78acec8208b659c28336c5a25a56fe2a326d3652edbc#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5OTAxMjQ0OA==&mid=2655002493&idx=1&sn=6ec5b0a366445d23b474b4ae36e9b407&chksm=bc381bd21cb13051c84c2d68f8a9206a78acec8208b659c28336c5a25a56fe2a326d3652edbc#rd", "authors": ["\u9752\u4e91\u5b66\u957f"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1a\u8363\u81ba\u4eca\u5e74\u6700\u5927\u8d62\u5bb6\uff0c\u6210\u529f\u767b\u4e0aNature\u9876\u7ea7\u671f\u520a", "comment": "Source: WeChat, Published: 2025-11-04 12:37:46", "summary": "\u65b9\u6cd5\uff1a\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u516c\u4ea4\u8fd0\u884c\u63a7\u5236\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7efc\u5408\u8fd0\u7528\u5386\u53f2\u6570\u636e\u4e0e\u8f66\u8def\u534f\u540c\u8f66\u8f86\uff08CAV\uff09\u5b9e\u65f6\u6570\u636e\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08DPPO\uff09\u7b97\u6cd5\uff0c\u5c06\u7269\u7406\u89c4\u5f8b\u7b49\u77e5\u8bc6\u878d\u5165\u6a21\u578b\u6784\u5efa\u3002", "AI": {"tldr": "\u65b9\u6cd5\uff1a\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u516c\u4ea4\u8fd0\u884c\u63a7\u5236\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7efc\u5408\u8fd0\u7528\u5386\u53f2\u6570\u636e\u4e0e\u8f66\u8def\u534f\u540c\u8f66\u8f86\uff08CAV\uff09\u5b9e\u65f6\u6570\u636e\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08DPPO\uff09\u7b97\u6cd5\uff0c\u5c06\u7269\u7406\u89c4\u5f8b\u7b49\u77e5\u8bc6\u878d\u5165\u6a21\u578b\u6784\u5efa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.43d3065d", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0NDMyODUxMA==&mid=2247488793&idx=1&sn=7d48084f9430547aa249e81792b98e27&chksm=e8b5dc4e8e18b9c38f761e1a6f99dbec87b1590a6117f4b47bfd238a85f53d30591b6e94d6c7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0NDMyODUxMA==&mid=2247488793&idx=1&sn=7d48084f9430547aa249e81792b98e27&chksm=e8b5dc4e8e18b9c38f761e1a6f99dbec87b1590a6117f4b47bfd238a85f53d30591b6e94d6c7#rd", "authors": ["\u8c03\u5ea6\u4e0e\u4f18\u5316\u7b97\u6cd5\u7684\u96c6\u7ed3\u5730"], "title": "\u5982\u4f55\u4f7f\u7528MATLAB\u8fdb\u884c\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff08\u9644\u4ee3\u7801\uff09", "comment": "Source: WeChat, Published: 2025-11-04 12:31:30", "summary": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08Deep Reinforcement Learning\uff0c DRL\uff09\u662f\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u4e00\u7c7b\u667a\u80fd\u51b3\u7b56\u7b97\u6cd5\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u5b66\u4e60\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u5956\u52b1\u7684\u7b56\u7565\u3002", "AI": {"tldr": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08Deep Reinforcement Learning\uff0c DRL\uff09\u662f\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u4e00\u7c7b\u667a\u80fd\u51b3\u7b56\u7b97\u6cd5\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u5b66\u4e60\u6700\u5927\u5316\u957f\u671f\u7d2f\u79ef\u5956\u52b1\u7684\u7b56\u7565\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.b6adc20e", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDU2MTc4NQ==&mid=2650692736&idx=3&sn=774c339997ff9b852450fa65df2785d6&chksm=bf9cce372cf89ea58745158f552ba22432ad63e6c7ccfbb43483e673bfe99ae98686622b8f01#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDU2MTc4NQ==&mid=2650692736&idx=3&sn=774c339997ff9b852450fa65df2785d6&chksm=bf9cce372cf89ea58745158f552ba22432ad63e6c7ccfbb43483e673bfe99ae98686622b8f01#rd", "authors": ["\u4e0a\u6d77\u7ecf\u4fe1\u59d4"], "title": "\u667a\u5143\u673a\u5668\u4eba\u771f\u673a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u843d\u5730\u5de5\u4e1a\u4ea7\u7ebf\uff0c\u5f00\u542f\u5177\u8eab\u667a\u80fd\u89c4\u6a21\u5316\u5e94\u7528\u65b0\u9636\u6bb5\uff5c\u4ea7\u4e1a\u521b\u65b0\u52a8\u6001", "comment": "Source: WeChat, Published: 2025-11-04 10:38:20", "summary": "\u667a\u5143\u6b64\u6b21\u843d\u5730\u7684\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9769\u547d\u6027\u7a81\u7834\uff1a\u673a\u5668\u4eba\u53ef\u5728\u771f\u5b9e\u4ea7\u7ebf\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3001\u6301\u7eed\u4f18\u5316\u4f5c\u4e1a\u7b56\u7565\uff0c\u65b0\u6280\u80fd\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u90e8\u7f72\u4ec5\u9700\u6570\u5341\u5206\u949f\uff0c\u4e14\u6027\u80fd\u5168\u7a0b\u4e0d\u964d\u7ea7\u3002", "AI": {"tldr": "\u667a\u5143\u6b64\u6b21\u843d\u5730\u7684\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9769\u547d\u6027\u7a81\u7834\uff1a\u673a\u5668\u4eba\u53ef\u5728\u771f\u5b9e\u4ea7\u7ebf\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3001\u6301\u7eed\u4f18\u5316\u4f5c\u4e1a\u7b56\u7565\uff0c\u65b0\u6280\u80fd\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u90e8\u7f72\u4ec5\u9700\u6570\u5341\u5206\u949f\uff0c\u4e14\u6027\u80fd\u5168\u7a0b\u4e0d\u964d\u7ea7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.aa410ce0", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMTE3NTAwNA==&mid=2649939357&idx=1&sn=e5450488855239bec9df4279cf7ecda4&chksm=821f7495d9be480db9dbf0d932b56b5d91f6bd6a63db6314715a327f503601911c1477fcef8d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMTE3NTAwNA==&mid=2649939357&idx=1&sn=e5450488855239bec9df4279cf7ecda4&chksm=821f7495d9be480db9dbf0d932b56b5d91f6bd6a63db6314715a327f503601911c1477fcef8d#rd", "authors": ["AEii\u56fd\u9645\u5e94\u7528\u80fd\u6e90"], "title": "\u3010Applied Energy \u6700\u65b0\u539f\u521b\u8bba\u6587\u3011\u57fa\u4e8e\u5b66\u4e60\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u63d0\u5347\u81ea\u9002\u5e94\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4f18\u5316\u9700\u6c42\u4fa7\u7ba1\u7406\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u7684\u80fd\u6e90\u7075\u6d3b\u6027", "comment": "Source: WeChat, Published: 2025-11-04 10:30:00", "summary": "Reinforcement learning \u5f3a\u5316\u5b66\u4e60Model predictive control \u6a21\u578b\u9884\u6d4b\u63a7\u5236Sparse neural network \u7a00\u758f\u795e\u7ecf\u7f51\u7edcGraphics\uff0c max cw\uff0cOCS\uff0c 1 ... 1 pw\uff0cPSR\uff0csequential decision step 1+ 1 prediction hore \u603b-\u222b\uff081\uff0cm\uff0cw\uff09 l agent step i \"\u2264u\uff0c su dynamic sparse training calcrolated acm", "AI": {"tldr": "Reinforcement learning \u5f3a\u5316\u5b66\u4e60Model predictive control \u6a21\u578b\u9884\u6d4b\u63a7\u5236Sparse neural network \u7a00\u758f\u795e\u7ecf\u7f51\u7edcGraphics\uff0c max cw\uff0cOCS\uff0c 1 ... 1 pw\uff0cPSR\uff0csequential decision step 1+ 1 prediction hore \u603b-\u222b\uff081\uff0cm\uff0cw\uff09 l agent step i \"\u2264u\uff0c ...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.581a38f6", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449996308&idx=1&sn=2610b84c911e791adc35ab597a04d328&chksm=b0a4bfc95e63c866e7a66488a785f858acc774ac9a145692efbf1ba258db11902926743d83ef#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449996308&idx=1&sn=2610b84c911e791adc35ab597a04d328&chksm=b0a4bfc95e63c866e7a66488a785f858acc774ac9a145692efbf1ba258db11902926743d83ef#rd", "authors": ["\u667a\u7329\u7329AI"], "title": "\u8d85\u8d8aSFT\u4e0eRLVR\uff01\u8c37\u6b4c\u63d0\u51fa\u76d1\u7763<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u8303\u5f0fSRL\uff0c\u7a81\u7834LLM\u590d\u6742\u63a8\u7406\u8bad\u7ec3\u5c40\u9650", "comment": "Source: WeChat, Published: 2025-11-04 10:23:53", "summary": "\u4e3a\u4e86\u89e3\u51b3\u4eceDhard\u4e2d\u5b66\u4e60\u7684\u6311\u6218\uff0c\u8bba\u6587\u5f15\u5165\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\uff0c\u6846\u67b6\u5982\u4e0b\u56fe\u6240\u793a\u3002where the product \\\uff08a \\times b = 20\uff01\\\uff09.\u30021. **Prime Factorization of 20\uff01-The prime factors of 20\uff01", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u4eceDhard\u4e2d\u5b66\u4e60\u7684\u6311\u6218\uff0c\u8bba\u6587\u5f15\u5165\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08SRL\uff09\uff0c\u6846\u67b6\u5982\u4e0b\u56fe\u6240\u793a\u3002where the product \\\uff08a \\times b = 20\uff01\\\uff09.\u30021. **Prime Factorization of 20\uff01-The prime factors of 20\uff01", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.f1499b89", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4OTIzNjU5NA==&mid=2651021421&idx=2&sn=d45cde0abea86d0f9401a4e100dfac53&chksm=8a65ebdb1b1c826adbd4fcbc9bbea2ee865bf70e198e023d9d1627cbb7afb10f3e024a652954#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4OTIzNjU5NA==&mid=2651021421&idx=2&sn=d45cde0abea86d0f9401a4e100dfac53&chksm=8a65ebdb1b1c826adbd4fcbc9bbea2ee865bf70e198e023d9d1627cbb7afb10f3e024a652954#rd", "authors": ["\u5de5\u4e1a\u673a\u5668\u4eba"], "title": "\u673a\u5668\u4eba\u201c10\u5206\u949f\u4e0a\u5c97\u201d\uff0c\u667a\u5143\u5b9e\u73b0\u771f\u673a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5de5\u4e1a\u843d\u5730", "comment": "Source: WeChat, Published: 2025-11-04 09:09:06", "summary": "\u9488\u5bf9\u8fd9\u4e00\u75db\u70b9\uff0c\u667a\u5143\u63a8\u51fa\u4e86\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002\u754c\u9762\u65b0\u95fb\u4ece\u667a\u5143\u83b7\u6089\uff0c\u5728\u8be5\u65b9\u6848\u4e2d\uff0c\u673a\u5668\u4eba\u53ef\u5728\u771f\u5b9e\u4ea7\u7ebf\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3001\u6301\u7eed\u4f18\u5316\u4f5c\u4e1a\u7b56\u7565\uff0c\u65b0\u6280\u80fd\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u90e8\u7f72\u4ec5\u9700\u6570\u5341\u5206\u949f\uff0c\u4e14\u6027\u80fd\u5168\u7a0b\u4e0d\u964d\u7ea7\u3002", "AI": {"tldr": "\u9488\u5bf9\u8fd9\u4e00\u75db\u70b9\uff0c\u667a\u5143\u63a8\u51fa\u4e86\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002\u754c\u9762\u65b0\u95fb\u4ece\u667a\u5143\u83b7\u6089\uff0c\u5728\u8be5\u65b9\u6848\u4e2d\uff0c\u673a\u5668\u4eba\u53ef\u5728\u771f\u5b9e\u4ea7\u7ebf\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3001\u6301\u7eed\u4f18\u5316\u4f5c\u4e1a\u7b56\u7565\uff0c\u65b0\u6280\u80fd\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u90e8\u7f72\u4ec5\u9700\u6570\u5341\u5206\u949f\uff0c\u4e14\u6027\u80fd\u5168\u7a0b\u4e0d\u964d\u7ea7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.af639135", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3NzkzNzY2Nw==&mid=2247488384&idx=1&sn=b9b1b5f40bf1145354f89cf285bb65f3&chksm=ce091354838e56f818782b4a7a048870db911a084417c83523da359a119b4b7109a33da25441#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3NzkzNzY2Nw==&mid=2247488384&idx=1&sn=b9b1b5f40bf1145354f89cf285bb65f3&chksm=ce091354838e56f818782b4a7a048870db911a084417c83523da359a119b4b7109a33da25441#rd", "authors": ["\u8ba1\u7b97\u673a\u5b66\u62a5"], "title": "\u667a\u80fd\u8ba1\u7b97| \u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5f15\u5bfc\u7684\u591a\u79cd\u7fa4\u534f\u540c\u8fdb\u5316\u8d85\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5", "comment": "Source: WeChat, Published: 2025-11-04 09:01:19", "summary": "\u8fd1\u5e74\u6765\uff0c\u5f3a\u5316\u5b66\u4e60\u56e0\u5176\u5353\u8d8a\u7684\u51b3\u7b56\u80fd\u529b\u88ab\u5f15\u5165\u8fdb\u5316\u7b97\u6cd5\u6846\u67b6\uff0c\u6210\u4e3a\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u7684\u5173\u952e\u6280\u672f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u591a\u79cd\u7fa4\u534f\u540c\u8fdb\u5316\u8d85\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5DQNMaOEA\uff0c\u7528\u4e8e\u6c42\u89e3\u590d\u6742\u7684\u8d85\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "AI": {"tldr": "\u8fd1\u5e74\u6765\uff0c\u5f3a\u5316\u5b66\u4e60\u56e0\u5176\u5353\u8d8a\u7684\u51b3\u7b56\u80fd\u529b\u88ab\u5f15\u5165\u8fdb\u5316\u7b97\u6cd5\u6846\u67b6\uff0c\u6210\u4e3a\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u7684\u5173\u952e\u6280\u672f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u591a\u79cd\u7fa4\u534f\u540c\u8fdb\u5316\u8d85\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5DQNMaOEA\uff0c\u7528\u4e8e\u6c42\u89e3\u590d\u6742\u7684\u8d85\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.2d49b3b2", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NjIxODA0Mg==&mid=2247536418&idx=1&sn=284ee955822c7770adda9fc8fec906f4&chksm=ea418d9f9f14b7ccf4ac50f37a363b2a523f0e1e3c0427260bb78bdb0eeefadbb1f4a2435a29#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NjIxODA0Mg==&mid=2247536418&idx=1&sn=284ee955822c7770adda9fc8fec906f4&chksm=ea418d9f9f14b7ccf4ac50f37a363b2a523f0e1e3c0427260bb78bdb0eeefadbb1f4a2435a29#rd", "authors": ["\u5177\u8eab\u667a\u80fd\u5934\u6761"], "title": "\u673a\u5668\u4eba\u201c10\u5206\u949f\u4e0a\u5c97\u201d\uff0c\u667a\u5143\u5b9e\u73b0\u771f\u673a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5de5\u4e1a\u843d\u5730", "comment": "Source: WeChat, Published: 2025-11-04 07:01:05", "summary": "\u9488\u5bf9\u8fd9\u4e00\u75db\u70b9\uff0c\u667a\u5143\u63a8\u51fa\u4e86\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002\u754c\u9762\u65b0\u95fb\u4ece\u667a\u5143\u83b7\u6089\uff0c\u5728\u8be5\u65b9\u6848\u4e2d\uff0c\u673a\u5668\u4eba\u53ef\u5728\u771f\u5b9e\u4ea7\u7ebf\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3001\u6301\u7eed\u4f18\u5316\u4f5c\u4e1a\u7b56\u7565\uff0c\u65b0\u6280\u80fd\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u90e8\u7f72\u4ec5\u9700\u6570\u5341\u5206\u949f\uff0c\u4e14\u6027\u80fd\u5168\u7a0b\u4e0d\u964d\u7ea7\u3002", "AI": {"tldr": "\u9488\u5bf9\u8fd9\u4e00\u75db\u70b9\uff0c\u667a\u5143\u63a8\u51fa\u4e86\u771f\u673a\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\u3002\u754c\u9762\u65b0\u95fb\u4ece\u667a\u5143\u83b7\u6089\uff0c\u5728\u8be5\u65b9\u6848\u4e2d\uff0c\u673a\u5668\u4eba\u53ef\u5728\u771f\u5b9e\u4ea7\u7ebf\u4e2d\u81ea\u4e3b\u5b66\u4e60\u3001\u6301\u7eed\u4f18\u5316\u4f5c\u4e1a\u7b56\u7565\uff0c\u65b0\u6280\u80fd\u8bad\u7ec3\u4e0e\u7a33\u5b9a\u90e8\u7f72\u4ec5\u9700\u6570\u5341\u5206\u949f\uff0c\u4e14\u6027\u80fd\u5168\u7a0b\u4e0d\u964d\u7ea7\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.b3302e0b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651261553&idx=2&sn=c4dd026bb61388765d188789a6ad7a24&chksm=bcb54e7a521fca0dc473bc32c04e3fba203930f15f9f9b9ef2e1f297e8b2255ca287b0fa7ae4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651261553&idx=2&sn=c4dd026bb61388765d188789a6ad7a24&chksm=bcb54e7a521fca0dc473bc32c04e3fba203930f15f9f9b9ef2e1f297e8b2255ca287b0fa7ae4#rd", "authors": ["InfoQ"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u00a0AI\u00a0\u7cfb\u7edf\u7684\u8bbe\u8ba1\u5b9e\u73b0\u53ca\u672a\u6765\u53d1\u5c55", "comment": "Source: WeChat, Published: 2025-11-04 05:06:38", "summary": "\u4ece\u5e38\u89c1\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5230\u57fa\u4e8e\u5baa\u6cd5\u7684\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u518d\u5230\u5982\u4eca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0d\u65ad\u8fdb\u6b65\u7684\u8fc7\u7a0b\uff0c\u5b9e\u9645\u4e0a\u4ee3\u8868\u7740\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u4fe1\u53f7\u6765\u6e90\u65e5\u76ca\u5e7f\u6cdb\uff0c\u540c\u65f6\u4efb\u52a1\u96be\u5ea6\u4e5f\u5728\u4e0d\u65ad\u63d0\u9ad8\u3002", "AI": {"tldr": "\u4ece\u5e38\u89c1\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5230\u57fa\u4e8e\u5baa\u6cd5\u7684\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u518d\u5230\u5982\u4eca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0d\u65ad\u8fdb\u6b65\u7684\u8fc7\u7a0b\uff0c\u5b9e\u9645\u4e0a\u4ee3\u8868\u7740\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u4fe1\u53f7\u6765\u6e90\u65e5\u76ca\u5e7f\u6cdb\uff0c\u540c\u65f6\u4efb\u52a1\u96be\u5ea6\u4e5f\u5728\u4e0d\u65ad\u63d0\u9ad8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.38839409", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyNjY4OTQ0Mg==&mid=2247491906&idx=1&sn=9ee846e8f2bb0ecc79d637b5b369e4cb&chksm=e9184b36b926428cf681f88fb8d9a24e856e9f853ca78bdfbfe17c821b07ba1f766c1ee6ff04#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyNjY4OTQ0Mg==&mid=2247491906&idx=1&sn=9ee846e8f2bb0ecc79d637b5b369e4cb&chksm=e9184b36b926428cf681f88fb8d9a24e856e9f853ca78bdfbfe17c821b07ba1f766c1ee6ff04#rd", "authors": ["\u5929\u6d25\u79d1\u6280\u5927\u5b66\u5b66\u62a5"], "title": "\u3010\u8bba\u6587\u63a8\u8350\u3011\u57fa\u4e8e\u6709\u6548\u52a8\u4f5c\u8868\u793a\u7684\u7b56\u7565\u641c\u7d22<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5", "comment": "Source: WeChat, Published: 2025-11-04 02:01:22", "summary": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5[j].\u5929\u6d25\u79d1\u6280\u5927\u5b66\u5b66\u62a5\uff0c2025\uff0c40\u3002\uff085\uff09\uff1a57-65 wang x x\uff0c huang jx\uff0czhao t t\uff0c et al. strategy search reinforcement learning method based on effective action repre sentation[j]. journal of tianjin university of science and technology\uff0c 2025\uff0c 40\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5[j].\u5929\u6d25\u79d1\u6280\u5927\u5b66\u5b66\u62a5\uff0c2025\uff0c40\u3002\uff085\uff09\uff1a57-65 wang x x\uff0c huang jx\uff0czhao t t\uff0c et al. strategy search reinforcement learning method based on effective action repre sentation[j]. journal of tianjin university of sc...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.770d0fda", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTQ0NzAyMQ==&mid=2247484621&idx=1&sn=b9fe965209e9b325d7e8630e09f08fc5&chksm=975d2744e9690d97e8f81ba8fee6804934f14908a13a528897e39d7973d434020f5ecce7ffc9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTQ0NzAyMQ==&mid=2247484621&idx=1&sn=b9fe965209e9b325d7e8630e09f08fc5&chksm=975d2744e9690d97e8f81ba8fee6804934f14908a13a528897e39d7973d434020f5ecce7ffc9#rd", "authors": ["\u56fd\u667a\u56ed"], "title": "\u3010\u5de5\u4e1a\u667a\u80fd\u3011\u673a\u5668\u4eba\u4e5f\u201c\u4e0a\u5c97\u201d\u5b66\u4e60\uff1f\u667a\u5143\u673a\u5668\u4eba\u5b9e\u73b0\u751f\u4ea7\u7ebf<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u90e8\u7f72", "comment": "Source: WeChat, Published: 2025-11-04 01:22:53", "summary": "\u4ec0\u4e48\u662f\u201c\u5f3a\u5316\u5b66\u4e60\u201d\uff08Reinforcement Learning\uff09\u5728\u6b64\u5904\u505a\u4e00\u4e2a\u901a\u4fd7\u8bf4\u660e\uff1a\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u7c7b\u8ba9\u673a\u5668\u201c\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u201d\u7684\u7b97\u6cd5\u2014\u2014\u7cfb\u7edf\u5728\u6267\u884c\u52a8\u4f5c\u540e\u4f1a\u5f97\u5230\u4e00\u4e2a\u201c\u56de\u62a5\u201d\uff08\u6bd4\u5982\u88c5\u914d\u6210\u529f\u6216\u5931\u8d25\uff09\uff0c\u7b97\u6cd5\u6839\u636e\u8fd9\u4e9b\u56de\u62a5\u8c03\u6574\u7b56\u7565\uff0c\u9010\u6b65\u5b66\u4f1a\u5728", "AI": {"tldr": "\u4ec0\u4e48\u662f\u201c\u5f3a\u5316\u5b66\u4e60\u201d\uff08Reinforcement Learning\uff09\u5728\u6b64\u5904\u505a\u4e00\u4e2a\u901a\u4fd7\u8bf4\u660e\uff1a\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u7c7b\u8ba9\u673a\u5668\u201c\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u201d\u7684\u7b97\u6cd5\u2014\u2014\u7cfb\u7edf\u5728\u6267\u884c\u52a8\u4f5c\u540e\u4f1a\u5f97\u5230\u4e00\u4e2a\u201c\u56de\u62a5\u201d\uff08\u6bd4\u5982\u88c5\u914d\u6210\u529f\u6216\u5931\u8d25\uff09\uff0c\u7b97\u6cd5\u6839\u636e\u8fd9\u4e9b\u56de\u62a5\u8c03\u6574\u7b56\u7565\uff0c\u9010\u6b65\u5b66\u4f1a\u5728", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.25a357d6", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4Mzc3MDI1Nw==&mid=2247488892&idx=1&sn=fa3bd13799468e54d6841ef2a3aa73d8&chksm=ce3a2187b557b0854a6a842e007a9cdbf7b42a4aeead3bd7cbb1aa619ff5782c4ef684b0599c#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4Mzc3MDI1Nw==&mid=2247488892&idx=1&sn=fa3bd13799468e54d6841ef2a3aa73d8&chksm=ce3a2187b557b0854a6a842e007a9cdbf7b42a4aeead3bd7cbb1aa619ff5782c4ef684b0599c#rd", "authors": ["\u8111\u6d1e\u5cf0\u8c37"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u53ea\u80fd<em class=\"highlight\">\u52a0\u5f3a</em>\u504f\u89c1\uff1f", "comment": "Source: WeChat, Published: 2025-11-04 01:01:11", "summary": "4.\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u6a21\u62df \u4f5c\u8005\u642d\u5efa\u4e86\u4e00\u4e2a\u4e09\u5c42\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u518d\u901a\u8fc7\u5efa\u7acb\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u6765\u5dee\u5f02\u5316\u5730\u66f4\u65b0\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\u8054\u7ed3\u95f4\u7684\u6743\u91cd\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4f9d\u636e\u539f\u6587\u884c\u6587\u903b\u8f91\uff0c\u5c55\u5f00\u8be6\u7ec6\u8ba8\u8bba\u3002", "AI": {"tldr": "4.\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u6a21\u62df \u4f5c\u8005\u642d\u5efa\u4e86\u4e00\u4e2a\u4e09\u5c42\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u518d\u901a\u8fc7\u5efa\u7acb\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u6765\u5dee\u5f02\u5316\u5730\u66f4\u65b0\u795e\u7ecf\u7f51\u7edc\u4e0d\u540c\u8054\u7ed3\u95f4\u7684\u6743\u91cd\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4f9d\u636e\u539f\u6587\u884c\u6587\u903b\u8f91\uff0c\u5c55\u5f00\u8be6\u7ec6\u8ba8\u8bba\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.937cc75b", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MTA1MzE2Nw==&mid=2247493982&idx=1&sn=c4be92e6883616edfcf6c256b971b0a7&chksm=cf31f1ef13b7314db228f3f701e9ba8161348891a589eee58787b191184236dabab967a74353#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MTA1MzE2Nw==&mid=2247493982&idx=1&sn=c4be92e6883616edfcf6c256b971b0a7&chksm=cf31f1ef13b7314db228f3f701e9ba8161348891a589eee58787b191184236dabab967a74353#rd", "authors": ["AI\u68ee\u6797\u7269\u79cd\u56fe\u9274"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u5230\u5e95\u884c\u4e0d\u884c\uff1fAI \u8fd8\u8981\u591a\u4e45\u624d\u771f\u6b63\u201c\u806a\u660e\u201d\uff1f", "comment": "Source: WeChat, Published: 2025-11-04 00:15:50", "summary": "\u201c\u5f3a\u5316\u5b66\u4e60\u5c31\u50cf\u5728\u8ff7\u5bab\u91cc\u627e\u51fa\u53e3\uff0c\u6bcf\u6b21\u8d70\u9519\u4e00\u6b65\uff0c\u9519\u8bef\u4f1a\u4e00\u8def\u7d2f\u79ef\u3002\u201d\u8fd9\u4e0d\u662f\u5938\u5f20\u3002RL \u7684\u6838\u5fc3\u95ee\u9898\u662f\u4fe1\u53f7\u7a00\u8584\u3001\u53cd\u9988\u592a\u6162\uff1aAI \u8981\u9760\u201c\u5956\u60e9\u201d\u5b66\u4e60\uff0c\u5fc5\u987b\u4e00\u904d\u904d\u8bd5\u9519\uff1b", "AI": {"tldr": "\u201c\u5f3a\u5316\u5b66\u4e60\u5c31\u50cf\u5728\u8ff7\u5bab\u91cc\u627e\u51fa\u53e3\uff0c\u6bcf\u6b21\u8d70\u9519\u4e00\u6b65\uff0c\u9519\u8bef\u4f1a\u4e00\u8def\u7d2f\u79ef\u3002\u201d\u8fd9\u4e0d\u662f\u5938\u5f20\u3002RL \u7684\u6838\u5fc3\u95ee\u9898\u662f\u4fe1\u53f7\u7a00\u8584\u3001\u53cd\u9988\u592a\u6162\uff1aAI \u8981\u9760\u201c\u5956\u60e9\u201d\u5b66\u4e60\uff0c\u5fc5\u987b\u4e00\u904d\u904d\u8bd5\u9519\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.336d43ce", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMzAyNjgxNg==&mid=2650910842&idx=1&sn=2faf7487d03267c9a1412eaf4306607a&chksm=818c40fa0614fec5a74b1fac84705a0cb43adf29f74d1dac2cd8c91f3717583f362db25a8751#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMzAyNjgxNg==&mid=2650910842&idx=1&sn=2faf7487d03267c9a1412eaf4306607a&chksm=818c40fa0614fec5a74b1fac84705a0cb43adf29f74d1dac2cd8c91f3717583f362db25a8751#rd", "authors": ["\u8baf\u98de\u9ad8\u6559"], "title": "\u5143\u53e4<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u5165\u9009\uff01", "comment": "Source: WeChat, Published: 2025-11-04 10:05:03", "summary": "\u4e2d\u56fd\u5730\u8d28\u5927\u5b66 chena univernity of geoscience \u4f60\u597d\uff0c\u6211\u662f\u5730\u5b66\u667a\u601d\u4f53 \u5730\u8d28\u56fe\u50cf\u7406\u89e3 \u9c7c\u7c7b\u5316\u77f3\u590d\u539f \u6587\u732e\u62bd\u53d6 \u77e5\u8bc6\u56fe\u8c31 \u5730\u5b66\u5c0f\u52a9\u624b \u8bf7\u8f93\u5165\u60f3\u4e86\u89e3\u7684\u95ee\u9898\uff08enter\u53d1\u9001\uff0cctrl+enter\u6362\u884c\uff09 \u5143\u53e4\u5927\u6a21\u578b\u3002", "AI": {"tldr": "\u4e2d\u56fd\u5730\u8d28\u5927\u5b66 chena univernity of geoscience \u4f60\u597d\uff0c\u6211\u662f\u5730\u5b66\u667a\u601d\u4f53 \u5730\u8d28\u56fe\u50cf\u7406\u89e3 \u9c7c\u7c7b\u5316\u77f3\u590d\u539f \u6587\u732e\u62bd\u53d6 \u77e5\u8bc6\u56fe\u8c31 \u5730\u5b66\u5c0f\u52a9\u624b \u8bf7\u8f93\u5165\u60f3\u4e86\u89e3\u7684\u95ee\u9898\uff08enter\u53d1\u9001\uff0cctrl+enter\u6362\u884c\uff09 \u5143\u53e4\u5927\u6a21\u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.40af69a5", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658743&idx=3&sn=88e0b61515585393d7fb424d87b8c7d6&chksm=c0cc8df4db615bfa85a3220e4268267cd3f5d5a610bb2204cb4d955fbda70616dd10e38010eb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjM2MDIyNQ==&mid=2247658743&idx=3&sn=88e0b61515585393d7fb424d87b8c7d6&chksm=c0cc8df4db615bfa85a3220e4268267cd3f5d5a610bb2204cb4d955fbda70616dd10e38010eb#rd", "authors": ["DataFunSummit"], "title": "\u817e\u8baf\u57fa\u4e8e RAG \u548c Agent \u6280\u672f\u7684\u6df7\u5143<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e1a\u52a1\u843d\u5730\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-11-04 10:03:04", "summary": "\u9996\u5148\u4ecb\u7ecd\u817e\u8baf\u5927\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\uff0c\u5982\u5185\u5bb9\u751f\u6210\u3001\u667a\u80fd\u5ba2\u670d\u548c\u89d2\u8272\u626e\u6f14\u7b49\uff0c\u5e76\u8be6\u7ec6\u89e3\u6790 RAG\uff08Retrieval-Augmented Generation\uff09\u6280\u672f\u53ca\u5176\u5728\u5b9e\u9645\u4e1a\u52a1\u4e2d\u7684\u521b\u65b0\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u751f\u6210\u548c\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u4f18\u52bf\u3002", "AI": {"tldr": "\u9996\u5148\u4ecb\u7ecd\u817e\u8baf\u5927\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\uff0c\u5982\u5185\u5bb9\u751f\u6210\u3001\u667a\u80fd\u5ba2\u670d\u548c\u89d2\u8272\u626e\u6f14\u7b49\uff0c\u5e76\u8be6\u7ec6\u89e3\u6790 RAG\uff08Retrieval-Augmented Generation\uff09\u6280\u672f\u53ca\u5176\u5728\u5b9e\u9645\u4e1a\u52a1\u4e2d\u7684\u521b\u65b0\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u751f\u6210\u548c\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.b833d5ab", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxMjY3Nzg5NQ==&mid=2247492889&idx=8&sn=60a4e789c674b315c5c15394420f2735&chksm=c0c1b6fa0ab04eb4b2a52296e274ad8592b73db3c149eb1a46e3f2bf0ef318732a76e9ec6931#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxMjY3Nzg5NQ==&mid=2247492889&idx=8&sn=60a4e789c674b315c5c15394420f2735&chksm=c0c1b6fa0ab04eb4b2a52296e274ad8592b73db3c149eb1a46e3f2bf0ef318732a76e9ec6931#rd", "authors": ["\u4e2d\u56fd\u6570\u667a\u5f00\u53d1\u533a"], "title": "\u5168\u7403\u9996\u4e2aAI\u6295\u8d44\u5927\u8d5b\u843d\u5e55\uff0c\u56fd\u4ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5305\u63fd\u524d\u4e8c", "comment": "Source: WeChat, Published: 2025-11-04 09:20:59", "summary": "\u5f53\u524d\uff0cAI\u5927\u6a21\u578b\u5728\u5404\u7c7b\u6027\u80fd\u57fa\u51c6\u699c\u5355\u4e2d\u5c61\u521b\u4f73\u7ee9\uff0c\u800c\u5982\u4f55\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u771f\u5b9e\u3001\u52a8\u6001\u4e14\u7ade\u4e89\u6fc0\u70c8\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u6c34\u5e73\uff0c\u5df2\u6210\u4e3a\u5f53\u4e0bAI\u9886\u57df\u7684\u5173\u6ce8\u7126\u70b9\u3002\u6b64\u6b21\u201cAlphaArena\u201d\u5927\u8d5b\u7531\u7f8e\u56fd\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5b9e\u9a8c\u5ba4nof1.ai\u4e3b\u529e\uff0c\u4e3a\u516d\u5927\u9876\u5c16\u6a21\u578b\u63d0\u4f9b1", "AI": {"tldr": "\u5f53\u524d\uff0cAI\u5927\u6a21\u578b\u5728\u5404\u7c7b\u6027\u80fd\u57fa\u51c6\u699c\u5355\u4e2d\u5c61\u521b\u4f73\u7ee9\uff0c\u800c\u5982\u4f55\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u771f\u5b9e\u3001\u52a8\u6001\u4e14\u7ade\u4e89\u6fc0\u70c8\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u6c34\u5e73\uff0c\u5df2\u6210\u4e3a\u5f53\u4e0bAI\u9886\u57df\u7684\u5173\u6ce8\u7126\u70b9\u3002\u6b64\u6b21\u201cAlphaArena\u201d\u5927\u8d5b\u7531\u7f8e\u56fd\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5b9e\u9a8c\u5ba4nof1.ai\u4e3b\u529e\uff0c\u4e3a\u516d\u5927\u9876\u5c16\u6a21\u578b\u63d0\u4f9b1", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.4273e14b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650999897&idx=3&sn=6b60184fb0631f030876f4a5c9e44a03&chksm=85a3dc9b659ae348b58d051108c77819b572f14913bb60bd2d023cf4ec3df36231fb88818817#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650999897&idx=3&sn=6b60184fb0631f030876f4a5c9e44a03&chksm=85a3dc9b659ae348b58d051108c77819b572f14913bb60bd2d023cf4ec3df36231fb88818817#rd", "authors": ["\u673a\u5668\u4e4b\u5fc3"], "title": "\u591a\u6a21\u6001<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7406\u89e3\u7269\u7406\u5de5\u5177\u5417\uff1fPhysToolBench\u63d0\u51fa\u4e86\u8861\u91cf\u591a\u6a21\u6001<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5bf9\u7269\u7406\u5de5\u5177\u7406\u89e3\u7684\u57fa\u51c6", "comment": "Source: WeChat, Published: 2025-11-04 08:51:35", "summary": "\u5b9e\u9a8c\u7ed3\u679c\uff1a\u5927\u6a21\u578b\u5728 PhysToolBench \u4e0a\u7684\u7b54\u5377CategoriesMLLM Difficulty Level Scene Category Overall\u2191Easy\u4e2a m1\u4e2a\u4e00 m2\u4e2a m3\u2191Hard\u2191 Professional\u4e2a Industrial\u2191 Outdoor\u2191 Daily\u2191", "AI": {"tldr": "\u5b9e\u9a8c\u7ed3\u679c\uff1a\u5927\u6a21\u578b\u5728 PhysToolBench \u4e0a\u7684\u7b54\u5377CategoriesMLLM Difficulty Level Scene Category Overall\u2191Easy\u4e2a m1\u4e2a\u4e00 m2\u4e2a m3\u2191Hard\u2191 Professional\u4e2a Industrial\u2191 Outdoor\u2191 Daily\u2191", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.df283d00", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247596768&idx=2&sn=4e901e93a811c4cb360221ab81fdd97c&chksm=e9a2226d861ca50d6656819e59504fe114d9cfec7ac4a815d3e4dafb99df508222970106d115#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247596768&idx=2&sn=4e901e93a811c4cb360221ab81fdd97c&chksm=e9a2226d861ca50d6656819e59504fe114d9cfec7ac4a815d3e4dafb99df508222970106d115#rd", "authors": ["\u6728\u6728\u81ea\u7531"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u00b7\u767d\u76ae\u4e66 | 2025\u5e74\u4e2d\u56fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u884c\u4e1a\u53d1\u5c55\u7814\u7a76\u62a5\u544a", "comment": "Source: WeChat, Published: 2025-11-04 08:03:17", "summary": "\u5927\u6a21\u578b\u4f5c\u4e3aai\u53d1\u5c55\u6838\u5fc3\u5f15\u64ce\uff0c\u6309\u5e94\u7528 \u5e7f\u5ea6\u5206\u901a\u7528\u4e0e\u884c\u4e1a\u6a21\u578b\uff0c\u6309\u90e8\u7f72\u5f62\u6001\u5206\u4e91\u7aef\u4e0e\u7aef\uff0c\u4fa7\u6a21\u578b\uff0c\u6309\u6280\u672f\u8def\u5f84\u5206\u95ed\u6e90\u4e0e\u5f00\u6e90\u6a21\u578b\uff0c2024\u5e74\u4e2d\u56fd\u5927\u6a21\u578b\u5e02\u573a\u89c4\u6a21\u7ea6294.16\u4ebf\u5143\uff0c\u9884\u8ba12026\u5e74\u7a81\u7834700\u4ebf\u5143\uff0c\u591a\u6a21\u6001 \u878d\u5408\u4e0e\u667a\u80fd\u4f53\u6f14\u8fdb\u6210\u7ade\u4e89\u7126\u70b9", "AI": {"tldr": "\u5927\u6a21\u578b\u4f5c\u4e3aai\u53d1\u5c55\u6838\u5fc3\u5f15\u64ce\uff0c\u6309\u5e94\u7528 \u5e7f\u5ea6\u5206\u901a\u7528\u4e0e\u884c\u4e1a\u6a21\u578b\uff0c\u6309\u90e8\u7f72\u5f62\u6001\u5206\u4e91\u7aef\u4e0e\u7aef\uff0c\u4fa7\u6a21\u578b\uff0c\u6309\u6280\u672f\u8def\u5f84\u5206\u95ed\u6e90\u4e0e\u5f00\u6e90\u6a21\u578b\uff0c2024\u5e74\u4e2d\u56fd\u5927\u6a21\u578b\u5e02\u573a\u89c4\u6a21\u7ea6294.16\u4ebf\u5143\uff0c\u9884\u8ba12026\u5e74\u7a81\u7834700\u4ebf\u5143\uff0c\u591a\u6a21\u6001 \u878d\u5408\u4e0e\u667a\u80fd\u4f53\u6f14\u8fdb\u6210\u7ade\u4e89\u7126\u70b9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.b01f530f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUxNjg4NDEzNA==&mid=2247528435&idx=1&sn=b5ed24818d400556e09f986a05c475bf&chksm=f88f908b469091ee558509b8dc479f6b6d9e29dbace9492d5c0f8f9d67fca58067d739d4f3de#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUxNjg4NDEzNA==&mid=2247528435&idx=1&sn=b5ed24818d400556e09f986a05c475bf&chksm=f88f908b469091ee558509b8dc479f6b6d9e29dbace9492d5c0f8f9d67fca58067d739d4f3de#rd", "authors": ["\u901b\u901bGitHub"], "title": "\u8001\u5916\u5439\u7206\u7684\u56fd\u4ea7\u5f00\u6e90 AI <em class=\"highlight\">\u5927\u6a21\u578b</em>\uff0c\u767b\u9876\u4e86\u5f00\u6e90\u70ed\u699c\u7b2c 1\u3002", "comment": "Source: WeChat, Published: 2025-11-04 07:03:30", "summary": "\u4efb\u4f55\u5927\u6a21\u578b\u65b0\u7248\u672c\u53d1\u5e03\uff0c\u90fd\u4f1a\u62c9\u5230 Artificial Analysis \u53bb\u6e9c\u6e9c\u3002Artificial Analysis \u663e\u793a\u5f00\u6e90\u6a21\u578b MiniMax-M2 \u73b0\u5728\u662f\u4e16\u754c\u7b2c\u4e94\uff0c\u56fd\u4ea7\u7b2c\u4e00\u3002\u592a\u4e89\u6c14\u4e86\u3002\u53e6\u5916\u770b X \u4e0a\u7684\u5e16\u5b50\uff0c\u5f88\u591a\u56fd\u5916\u77e5\u540d\u7684\u5f00\u53d1\u8005\u548c\u5927\u5382\u5458\u5de5\u90fd\u89c9\u5f97 MiniMax M2 \u5f88\u9876\uff0c\u662f\u6700\u597d\u7684\u5f00", "AI": {"tldr": "\u4efb\u4f55\u5927\u6a21\u578b\u65b0\u7248\u672c\u53d1\u5e03\uff0c\u90fd\u4f1a\u62c9\u5230 Artificial Analysis \u53bb\u6e9c\u6e9c\u3002Artificial Analysis \u663e\u793a\u5f00\u6e90\u6a21\u578b MiniMax-M2 \u73b0\u5728\u662f\u4e16\u754c\u7b2c\u4e94\uff0c\u56fd\u4ea7\u7b2c\u4e00\u3002\u592a\u4e89\u6c14\u4e86\u3002\u53e6\u5916\u770b X \u4e0a\u7684\u5e16\u5b50\uff0c\u5f88\u591a\u56fd\u5916\u77e5\u540d\u7684\u5f00\u53d1\u8005\u548c\u5927\u5382\u5458\u5de5\u90fd\u89c9\u5f97 MiniMax M2 \u5f88\u9876\uff0c\u662f\u6700\u597d\u7684\u5f00", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.d27921e4", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNjQzMDU3Ng==&mid=2247484050&idx=1&sn=bb1dfd8186ad4a00682205d0fb0ffd2d&chksm=c05e8195d3053ad3afb69e7fb10457fc286bca3412e85d6f977b2eea6610580a929345d73d0f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNjQzMDU3Ng==&mid=2247484050&idx=1&sn=bb1dfd8186ad4a00682205d0fb0ffd2d&chksm=c05e8195d3053ad3afb69e7fb10457fc286bca3412e85d6f977b2eea6610580a929345d73d0f#rd", "authors": ["Ai\u5927\u6a21\u578b\u77e5\u8bc6\u8425"], "title": "\u4e00\u6587\u638c\u63e1<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e94\u7528\u5f00\u53d1\u7cbe\u9ad3\uff0c\u5f88\u96be\u627e\u5168\u7684\uff01", "comment": "Source: WeChat, Published: 2025-11-04 06:04:29", "summary": "\u5927\u6a21\u578b\u5f00\u53d1\u5168\u6d41\u7a0b\u3002\u00b7 1\u3001\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6838\u5fc3\u6d41\u7a0b\u3002\u00b7\u30022.2\u9879\u76ee\u89c4\u5212\u9636\u6bb5 \u30021.2\u6570\u636e\u5904\u7406\u3002\u00b7 2\u3001\u6838\u5fc3\u6280\u672f\u6808\u5168\u666f\u89e3\u6790\u3002\u00b7\u30022.1\u5927\u6a21\u578b\u6280\u672f\u6808\u5206\u5c42\u67b6\u6784 \u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u5f00\u53d1\u5168\u6d41\u7a0b\u3002\u00b7 1\u3001\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6838\u5fc3\u6d41\u7a0b\u3002\u00b7\u30022.2\u9879\u76ee\u89c4\u5212\u9636\u6bb5 \u30021.2\u6570\u636e\u5904\u7406\u3002\u00b7 2\u3001\u6838\u5fc3\u6280\u672f\u6808\u5168\u666f\u89e3\u6790\u3002\u00b7\u30022.1\u5927\u6a21\u578b\u6280\u672f\u6808\u5206\u5c42\u67b6\u6784 \u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.5b37e59d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3NjU5MDY3NQ==&mid=2247664758&idx=1&sn=735a1439792722c58b61aeb2206d8699&chksm=fc4ba886c226cefc656695bcaa16c3b7f2b881a5753d27a9f29d65c86497dae538f23141dfba#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3NjU5MDY3NQ==&mid=2247664758&idx=1&sn=735a1439792722c58b61aeb2206d8699&chksm=fc4ba886c226cefc656695bcaa16c3b7f2b881a5753d27a9f29d65c86497dae538f23141dfba#rd", "authors": ["IntelMining\u667a\u80fd\u77ff\u4e1a"], "title": "AI\u201c\u76c6\u666f\u201d\u5df2\u6210\u201c\u98ce\u666f\u201d\uff1f<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u89c4\u6a21\u590d\u5236\u8ba9\u5de5\u4e1a\u957f\u51fa\u6570\u667a\u751f\u4ea7\u529b\uff01", "comment": "Source: WeChat, Published: 2025-11-04 05:47:33", "summary": "\u56de\u987e\u201c\u77ff\u5c71\u76d8\u53e4\u5927\u6a21\u578b\u201d\u7684\u63a8\u51fa\uff0c\u5176\u521d\u8877\u6b63\u662f\u4e3a\u4e86\u6253\u7834AI\u5728\u77ff\u5c71\u573a\u666f\u4e2d\u201c\u6563\u3001\u5c0f\u3001\u4e71\u201d\u7684\u9b54\u5492\u3002\u5728\u8fc7\u53bb\uff0c\u6bcf\u4e2a\u5e94\u7528\u573a\u666f\u90fd\u50cf\u4e00\u4e2a\u72ec\u7acb\u7684\u201c\u624b\u5de5\u4f5c\u574a\u201d\uff1a\u9488\u5bf9\u76ae\u5e26\u8dd1\u504f\u8bc6\u522b\u5f00\u53d1\u4e00\u4e2a\u7b97\u6cd5\uff0c\u4e3a\u74e6\u65af\u76d1\u6d4b\u518d\u53e6\u8d77\u7089\u7076\u3002", "AI": {"tldr": "\u56de\u987e\u201c\u77ff\u5c71\u76d8\u53e4\u5927\u6a21\u578b\u201d\u7684\u63a8\u51fa\uff0c\u5176\u521d\u8877\u6b63\u662f\u4e3a\u4e86\u6253\u7834AI\u5728\u77ff\u5c71\u573a\u666f\u4e2d\u201c\u6563\u3001\u5c0f\u3001\u4e71\u201d\u7684\u9b54\u5492\u3002\u5728\u8fc7\u53bb\uff0c\u6bcf\u4e2a\u5e94\u7528\u573a\u666f\u90fd\u50cf\u4e00\u4e2a\u72ec\u7acb\u7684\u201c\u624b\u5de5\u4f5c\u574a\u201d\uff1a\u9488\u5bf9\u76ae\u5e26\u8dd1\u504f\u8bc6\u522b\u5f00\u53d1\u4e00\u4e2a\u7b97\u6cd5\uff0c\u4e3a\u74e6\u65af\u76d1\u6d4b\u518d\u53e6\u8d77\u7089\u7076\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.fdb6c04e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MzcyNDExNQ==&mid=2247485735&idx=3&sn=db898706b4ff49c66e443b6bf0b53c76&chksm=cfb5f0a3063990c5db9daf179ede9485ac97562948b5fb3aef521df12e63ce5d944d26de16bd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MzcyNDExNQ==&mid=2247485735&idx=3&sn=db898706b4ff49c66e443b6bf0b53c76&chksm=cfb5f0a3063990c5db9daf179ede9485ac97562948b5fb3aef521df12e63ce5d944d26de16bd#rd", "authors": ["\u67f3\u661f\u804a\u4ea7\u54c1"], "title": "\u641e\u61c2\u8fd9\u4e9bAI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u540d\u8bcd\uff0c\u4f60\u4e5f\u80fd\u8f7b\u677e\u5165\u95e8\uff01", "comment": "Source: WeChat, Published: 2025-11-04 00:35:27", "summary": "\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6b63\u5728\u9010\u6e10\u6539\u53d8\u5404\u4e2a\u884c\u4e1a\uff0c\u4f46\u5bf9\u6280\u672f\u5c0f\u767d\u6765\u8bf4\uff0c\u4e86\u89e3\u5e76\u638c\u63e1\u8fd9\u4e9b\u590d\u6742\u7684\u5de5\u5177\u548c\u6982\u5ff5\u975e\u5e38\u91cd\u8981\u3002\u4f60\u662f\u5426\u89c9\u5f97\u9762\u5bf9\u201cLlamaIndex\u201d\u3001\u201cOllama\u201d\u3001\u201cAnthropic\u201d\u7b49\u672f\u8bed\u65e0\u4ece\u4e0b\u624b\uff1f", "AI": {"tldr": "\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6b63\u5728\u9010\u6e10\u6539\u53d8\u5404\u4e2a\u884c\u4e1a\uff0c\u4f46\u5bf9\u6280\u672f\u5c0f\u767d\u6765\u8bf4\uff0c\u4e86\u89e3\u5e76\u638c\u63e1\u8fd9\u4e9b\u590d\u6742\u7684\u5de5\u5177\u548c\u6982\u5ff5\u975e\u5e38\u91cd\u8981\u3002\u4f60\u662f\u5426\u89c9\u5f97\u9762\u5bf9\u201cLlamaIndex\u201d\u3001\u201cOllama\u201d\u3001\u201cAnthropic\u201d\u7b49\u672f\u8bed\u65e0\u4ece\u4e0b\u624b\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.953002c4", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650707337&idx=1&sn=90c9653f17ed14541b1abb2bee49ed4e&chksm=bfec525856369e181e68d2f809d381a48ec17bfdb795adcfaab749e3671067e32ef3a87c2e21#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NTk0MTM1Mw==&mid=2650707337&idx=1&sn=90c9653f17ed14541b1abb2bee49ed4e&chksm=bfec525856369e181e68d2f809d381a48ec17bfdb795adcfaab749e3671067e32ef3a87c2e21#rd", "authors": ["twt\u4f01\u4e1aIT\u793e\u533a"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u751f\u6001\u7684\u201c\u4e0d\u53ef\u80fd\u4e09\u89d2\u201d\uff1a\u89c4\u6a21\u5316\u5e94\u7528\u7684\u67b6\u6784\u56f0\u5883\uff1f", "comment": "Source: WeChat, Published: 2025-11-03 23:35:46", "summary": "\u6211\u4eec\u4e60\u60ef\u4e8e\u5c06\u201c\u5927\u6a21\u578b\u201d\u89c6\u4e3a\u65e0\u6240\u4e0d\u80fd\u7684\u667a\u80fd\u4f53\uff0c\u7136\u800c\uff0c\u5728\u9762\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u73b0\u5b9e\u201c\u52a8\u6001\u7684\u201d\u6570\u636e\u4e16\u754c\u4e2d\uff0c\u5927\u6a21\u578b\u7684\u9759\u6001\u77e5\u8bc6\u4f53\u7cfb\u66b4\u9732\u51fa\u6839\u672c\u6027\u7684\u5c40\u9650\u3002", "AI": {"tldr": "\u6211\u4eec\u4e60\u60ef\u4e8e\u5c06\u201c\u5927\u6a21\u578b\u201d\u89c6\u4e3a\u65e0\u6240\u4e0d\u80fd\u7684\u667a\u80fd\u4f53\uff0c\u7136\u800c\uff0c\u5728\u9762\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u73b0\u5b9e\u201c\u52a8\u6001\u7684\u201d\u6570\u636e\u4e16\u754c\u4e2d\uff0c\u5927\u6a21\u578b\u7684\u9759\u6001\u77e5\u8bc6\u4f53\u7cfb\u66b4\u9732\u51fa\u6839\u672c\u6027\u7684\u5c40\u9650\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
