<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.SE](#cs.SE) [Total: 6]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.AI](#cs.AI) [Total: 10]
- [wechat.article](#wechat.article) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 该论文研究了短上下文主导假说，发现大多数序列仅需少量局部前缀即可预测下一个token，并提出了检测需要长上下文的序列的方法以及相应的解码算法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究短上下文主导假说：验证对于大多数序列，仅需少量局部前缀是否足以预测下一个token。同时探索如何检测那些需要长上下文的挑战性序列，并解决短上下文主导导致的LLM输出分布偏差问题。

Method: 1. 使用大语言模型作为统计预言机，测量在不同长度序列数据集上重现准确全上下文预测所需的最小上下文长度(MCL)。2. 提出无需实际下一个token知识的分布感知MCL(DaMCL)作为MCL的实用代理。3. 开发利用检测器识别和提升长范围相关token的解码算法。

Result: 对于1-7k token的长上下文文档序列，75-80%仅需最多最后96个token。DaMCL阈值化在检测长短上下文序列方面表现优异。提出的解码算法在问答任务和不同模型架构上都能提升性能。

Conclusion: 短上下文主导现象确实存在，大多数序列预测仅需少量局部上下文。通过DaMCL可以有效检测需要长上下文的序列，而相应的解码算法能够缓解短上下文主导带来的偏差，提升模型性能。

Abstract: We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [2] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 该论文提出了一种分层多智能体架构，将推理分布在64*64网格的轻量级智能体上，通过空间课程学习逐步扩展操作区域，结合NLL置信度测量和Thompson采样课程管理器，在空间化汉诺塔基准上实现了更稳定的长程推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多智能体系统在分解复杂任务方面有潜力，但在长程推理任务上表现不佳且计算成本高。需要一种能够有效处理长视野推理任务同时控制计算开销的方法。

Method: 提出分层多智能体架构：1) 64*64网格的轻量级智能体分布推理；2) 选择性oracle支持；3) 空间课程学习逐步扩展操作区域；4) 使用负对数似然(NLL)作为置信度测量；5) Thompson采样课程管理器自适应选择训练区域。

Result: 在空间化汉诺塔基准测试中，该方法表现出：1) 改进的稳定性；2) 减少的oracle使用；3) 更强的分布式智能体协作长程推理能力。

Conclusion: 分层多智能体架构结合空间课程学习和置信度测量，能够有效处理长视野推理任务，提高稳定性并减少计算开销，适用于机器人操作和规划等类似结构任务。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [3] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 本文提出一个评估框架，用于在联邦学习环境中评估LLM与多样化人类偏好的对齐效果，并引入自适应聚合策略来平衡对齐质量与公平性。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中，传统方法难以充分代表多样化的人类偏好，导致LLM对齐时存在公平性问题。需要一种能够平衡对齐质量和公平性的评估框架。

Method: 提出一个联邦学习评估框架，各组本地评估rollouts并生成奖励信号，服务器聚合组级奖励而不访问原始数据。评估标准聚合技术（最小、最大、平均）并引入自适应方案，基于历史对齐性能动态调整偏好权重。

Result: 在基于PPO的RLHF管道的问答任务实验中，自适应方法在保持竞争性对齐分数的同时，始终实现更优的公平性。

Conclusion: 这项工作为评估LLM在多样化人群中的行为提供了稳健方法，并为开发真正多元化和公平对齐的模型提供了实用解决方案。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 该论文分析了LLM智能体在长视野工具使用任务中的脆弱性，发现突变性动作（改变环境的操作）中的错误对任务失败影响最大，而非突变性动作错误影响较小。作者提出了CM方法，通过突变门控验证、针对性反思和块级上下文清理来提升智能体性能，并发布了τ-Bench Verified基准以解决原有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体发展迅速，但在长视野工具使用任务上的表现仍然脆弱。为了理解这种脆弱性，研究者希望探究：所有动作对失败的影响是否相同？特别是突变性动作（改变环境状态的操作）与非突变性动作在导致任务失败方面的差异。

Method: 1) 在τ-Bench（航空/零售）和SWE-Bench Verified上分析执行轨迹，将动作分解为突变性和非突变性步骤；2) 形式化"决定性偏差"概念，即最早导致成功转为失败的动作级分歧；3) 使用逻辑回归分析不同动作类型错误对成功率的影响；4) 提出CM方法：突变门控验证、针对性反思（在突变步骤前）、块级上下文清理；5) 发布τ-Bench Verified基准，修正原有基准的标注错误和任务定义不明确问题。

Result: 研究发现：1) 每个突变性动作中的偏差可将成功率降低高达92%（航空）和96%（零售），而非突变性动作偏差影响很小；2) 错误随上下文长度增加而增长，智能体偏离角色并基于过时约束行动；3) CM方法带来显著提升：Qwen3-Thinking在航空任务上相对提升28%，零售任务11%，SWE-Bench Verified 7%；Claude模型也有9%/7%的提升；4) 发现τ-Bench存在天花板效应，标注错误和任务定义不明确人为限制了模型性能。

Conclusion: 论文主张：1) 需要进行动作级分析来理解智能体失败；2) 需要针对性安全措施（如CM）来保护突变性动作；3) 需要可靠的评估基准（如τ-Bench Verified）来准确衡量智能体能力。这些是构建稳健多轮智能体的先决条件。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [5] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU内存使用量的框架，通过分解模型架构和分析训练行为来准确预测峰值内存使用，防止内存溢出错误。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型规模和复杂度增加，GPU内存需求经常超过可用容量，导致内存溢出错误，这会中断训练并浪费计算资源。现有研究仅关注单模态架构，无法泛化到多模态模型，而多模态模型在智能体AI系统中很常见。

Method: 提出一个框架，通过分析多模态模型的架构和训练行为来预测峰值GPU内存使用。具体方法是将多模态模型分解为组成层，并应用因子化来估计每层的内存使用。

Result: 评估显示该框架实现了约8.7%的平均MAPE（平均绝对百分比误差）预测准确率。

Conclusion: 该框架能够准确预测多模态模型的GPU内存使用，有助于防止内存溢出错误，提高训练效率。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [6] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 在ICU重症监护场景中，研究比较了离线多目标强化学习算法与单目标基线方法，发现PEDA DT算法在灵活性和个性化决策方面表现最优


<details>
  <summary>Details</summary>
Motivation: 重症监护中临床医生需要在患者生存率和资源利用（如住院时间）之间平衡，传统单目标强化学习方法使用固定标量化奖励函数，导致策略僵化无法适应变化的临床优先级

Method: 在MIMIC-IV数据集上，基准测试了三种离线MORL算法（CPQL、Adaptive CPQL、PEDA DT）和三种标量化单目标基线方法（BC、CQL、DDQN），使用离策略评估指标进行比较

Result: PEDA DT算法相比静态标量化基线展现出更优的灵活性，同时验证了序列建模架构在多目标条件生成中保持稳健有效，扩展了单目标决策变换器在医疗领域的研究发现

Conclusion: 离线多目标强化学习是实现重症监护个性化、可调整决策的有前景框架，无需重新训练即可适应不同临床优先级

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [7] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 提出一种通过"忏悔"机制促使LLM诚实表达自身缺陷的方法，在训练中单独奖励忏悔的诚实性，使模型更愿意承认错误而非掩盖


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在报告行为和信念时可能存在不诚实行为（如夸大信心、掩盖证据），这可能源于强化学习中的奖励塑造问题。需要一种方法来促使模型诚实表达自身缺陷

Method: 提出"忏悔"机制：模型在原始回答后提供忏悔输出，完整说明其遵守政策和指令的情况。训练中单独基于忏悔的诚实性给予奖励，不影响主回答的奖励。通过使承认错误成为"最小阻力路径"来激励诚实

Result: 训练GPT-5-Thinking产生忏悔，在分布外场景（幻觉、指令遵循、阴谋、奖励黑客攻击）中评估诚实性。发现当模型在主回答中撒谎或省略缺陷时，通常能诚实忏悔，且忏悔诚实性随训练适度提升

Conclusion: 忏悔机制可行，能促使模型诚实表达缺陷。忏悔支持多种推理时干预，包括监控、拒绝采样和向用户呈现问题

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [8] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: 提出MAC方法，通过动作块模型减少长时域预测误差，结合拒绝采样防止模型利用，在离线模型强化学习中实现长时域复杂任务的扩展


<details>
  <summary>Details</summary>
Motivation: 解决离线模型强化学习中长时域任务的挑战，传统模型值扩展方法在增加预测步数时面临模型误差累积与偏差减少的权衡问题

Method: 提出动作块模型预测动作序列后的状态而非单步动作，减少误差累积；采用拒绝采样从行为动作块策略中采样，防止模型利用和分布外动作问题

Result: 在包含高达1亿转换的大规模数据集上，MAC在离线模型强化学习算法中表现最佳，尤其在挑战性长时域任务上

Conclusion: MAC为离线模型强化学习提供了可扩展的解决方案，通过动作块模型和拒绝采样有效解决了长时域任务中的误差累积和模型利用问题

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [9] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文提出多种方法来提升AI智能体在开放环境中的鲁棒性，包括MiniHack框架、Maestro对抗课程生成、质量多样性方法分析多智能体漏洞，以及进化搜索增强LLM对抗提示鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的普及，需要能够成功导航和适应不断变化的开放世界的智能体。关键挑战是确保这些AI智能体不仅能在训练中熟悉的设置中表现出色，还能有效地泛化到以前未见过的多样化场景。

Method: 1. 引入MiniHack：基于NetHack的沙盒框架，通过程序化内容生成创建多样化环境；2. 提出Maestro：生成对抗课程的方法，逐步增强RL智能体在双人零和游戏中的鲁棒性；3. 使用质量多样性方法系统识别预训练RL策略在足球游戏领域的漏洞；4. 扩展研究到LLM领域，使用进化搜索生成多样化对抗提示来诊断和增强LLM鲁棒性。

Result: 开发了多种工具和方法来训练和评估鲁棒的AI智能体，使其能够泛化到新颖环境、分布外输入以及与其他智能体的交互，为未来AI鲁棒性研究铺平道路。

Conclusion: 通过结合开放性和多智能体学习方法，该工作为开发能够适应不断变化世界并在面对意外挑战和交互时蓬勃发展的AI智能体奠定了基础。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [10] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一个新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高生成模型对齐的训练效率，实现2.4倍加速训练


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于将生成模型与人类偏好对齐至关重要，但其高昂的计算成本阻碍了广泛应用。现有方法在样本效率和信用分配方面存在局限性。

Method: 将去噪过程重构为搜索树，从共享的初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效复用其共同前缀。通过树结构实现高样本效率、细粒度信用分配（通过奖励反向传播计算步骤特定优势）和摊销计算（每个前向传递实现多个策略更新）。

Result: 在扩散模型和流模型上的实验表明，TreeGRPO实现了2.4倍训练加速，在效率-奖励权衡空间中建立了更优的帕累托前沿。该方法在多个基准测试和奖励模型上始终优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了一个可扩展且有效的途径，通过树结构方法显著提高了训练效率。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [11] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出一种针对离线强化学习的全局预算分配攻击策略，基于TD误差分配扰动预算，相比均匀扰动更高效且隐蔽


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用均匀扰动策略，对所有样本无差别处理，效率低下且缺乏隐蔽性，需要更智能的攻击方法

Method: 提出全局预算分配攻击策略，基于理论洞察：样本对价值函数收敛的影响与TD误差成正比，将攻击建模为全局资源分配问题，推导出在L2约束下扰动幅度与TD误差敏感性成正比的闭式解

Result: 在D4RL基准测试中显著优于基线策略，仅用最小扰动就能实现高达80%的性能下降，并能逃避最先进的统计和频谱防御检测

Conclusion: 全局预算分配攻击策略比均匀扰动更高效和隐蔽，为离线RL安全性研究提供了新的攻击视角

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [12] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE：联邦自进化框架，解决LLM智能体在隐私约束下的跨环境知识迁移问题，通过局部进化-全局聚合范式，在异构任务中提升任务成功率约18%


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛部署，但隐私约束限制了集中式优化和跨动态环境的协同进化。联邦学习在静态数据集上有效，但在开放式的智能体自进化方面尚未充分探索。直接应用标准联邦学习存在挑战：异构任务和稀疏的轨迹级奖励会导致严重的梯度冲突，破坏全局优化过程。

Method: 提出Fed-SE联邦自进化框架，采用局部进化-全局聚合范式。局部层面：智能体在过滤的高回报轨迹上进行参数高效微调，实现稳定的梯度更新。全局层面：在低秩子空间内聚合更新，解耦环境特定的动态特性，有效减少客户端间的负迁移。

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦基线平均任务成功率提升约18%，验证了其在隐私约束部署中实现鲁棒的跨环境知识迁移的有效性。

Conclusion: Fed-SE成功解决了LLM智能体在隐私约束下的联邦自进化问题，通过局部稳定优化和全局解耦聚合，实现了跨异构环境的有效知识迁移，为隐私保护的智能体协同进化提供了可行方案。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [DeepCode: Open Agentic Coding](https://arxiv.org/abs/2512.07921)
*Zongwei Li,Zhonghang Li,Zirui Guo,Xubin Ren,Chao Huang*

Main category: cs.SE

TL;DR: DeepCode是一个完全自主的框架，通过信息流管理解决文档到代码库合成中的信息过载与上下文瓶颈冲突，在PaperBench基准测试中超越商业代理和人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现高保真文档到代码库合成（如科学论文到代码）时面临重大挑战，主要由于信息过载与LLMs上下文瓶颈之间的根本冲突。

Method: 将仓库合成视为信道优化问题，通过四个信息操作最大化有限上下文预算下的任务相关信号：蓝图蒸馏的源压缩、状态化代码内存的结构化索引、检索增强生成的条件知识注入、闭环错误校正。

Result: 在PaperBench基准测试中达到最先进性能，显著优于Cursor和Claude Code等领先商业代理，并在关键复现指标上超越顶尖机构的博士级人类专家。

Conclusion: 通过系统地将论文规范转化为生产级实现，为自主科学复现建立新基础，加速研究评估和发现。

Abstract: Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.

</details>


### [14] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: Token Sugar：通过将高频冗长代码模式替换为可逆的简洁符号，减少LLM代码生成中的token数量，降低计算成本


<details>
  <summary>Details</summary>
Motivation: LLM在代码任务中表现出色，但编程语言的冗长性导致token数量膨胀，增加推理成本和生成时间。现有方法局限于语法层面的简化，未能充分利用语义层面的token减少机会。

Method: 提出Token Sugar概念，设计系统化方案：从代码语料库挖掘高频、token密集的模式，为每个模式映射唯一简洁符号，通过代码转换将符号对集成到LLM预训练中。最终获得799个（代码模式，简洁符号）对。

Result: 源代码token数量最多减少15.1%，与现有语法方法互补。训练三个广泛使用的LLM后，生成时token节省显著（最多减少11.2%），同时Pass@1分数与未处理代码训练的基线模型几乎相同。

Conclusion: Token Sugar能有效减少LLM代码生成中的token使用，降低计算成本，同时保持代码生成质量，为LLM代码任务提供了实用的token优化方案。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [15] [Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework](https://arxiv.org/abs/2512.08286)
*Liao Hu,Qiteng Wu,Ruoyu Qi*

Main category: cs.SE

TL;DR: SolidGPT是一个开源的边缘-云混合开发助手，通过语义代码搜索、自动化工作流和隐私优先设计，解决LLM在开发中的语义理解、生产力和数据隐私之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型集成到移动和软件开发工作流中面临三个核心矛盾：语义理解、开发效率和数据隐私。传统的云工具虽然推理能力强但存在数据泄露和延迟风险，而本地解决方案缺乏对代码库和开发工具的全上下文理解。

Method: SolidGPT采用边缘-云混合架构，基于GitHub构建，提供：1）交互式代码库查询功能；2）自动化软件项目工作流（生成PRD、任务分解、看板等）；3）可配置的私有可扩展代理，支持本地代码文件夹、Notion集成和AI角色定制。

Result: SolidGPT通过语义丰富的代码导航、集成的文档和任务管理、隐私优先设计，提升了开发效率。开发者可以通过Docker、CLI或VSCode扩展部署，在保持数据控制的同时，按需使用LLM API。

Conclusion: SolidGPT通过结合交互式代码查询、自动化项目脚手架和人机协作，提供了一个实用、尊重隐私的边缘助手，加速了现实世界的开发工作流，特别适合智能移动和软件工程场景。

Abstract: The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.

</details>


### [16] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: RESTifAI是一个基于LLM的方法，用于生成可重用、CI/CD就绪的REST API测试，专注于快乐路径方法，同时生成正面和负面测试用例。


<details>
  <summary>Details</summary>
Motivation: 现有工具主要关注内部服务器错误，缺乏系统化的快乐路径测试生成方法，且存在可重用性、测试预言复杂性和集成方面的限制。

Method: 使用LLM驱动的方法，系统化构建有效的测试场景（快乐路径），并推导负面案例来验证预期功能（2xx响应）和对无效输入或业务规则违规的鲁棒性（4xx响应）。

Result: RESTifAI与最新的LLM工具（AutoRestTest和LogiAgent）性能相当，同时解决了可重用性、预言复杂性和集成方面的限制，并展示了在工业服务中的适用性。

Conclusion: RESTifAI提供了一种有效的LLM驱动方法，用于生成可重用、CI/CD就绪的REST API测试，填补了现有工具在系统化快乐路径测试生成方面的空白。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [17] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 本文研究了代码大语言模型的校准问题，提出了使用多校准方法来提高模型置信度与代码正确性之间的对应关系，在三个函数合成基准上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着基于AI的代码生成技术普及，确保代码大语言模型的置信度分数能够真实反映代码正确性的概率变得至关重要。当前需要更好的校准方法来捕捉编码问题的额外因素，如复杂度、代码长度、编程语言等。

Method: 研究了四种多校准方法，在三个函数合成基准上进行实验，使用了最新一代的代码大语言模型（Qwen3 Coder、GPT-OSS、DeepSeek-R1-Distill）。多校准方法能够捕捉编码问题的额外因素。

Result: 多校准方法相比未校准的token似然度提高了1.03技能分数，相比基线校准方法提高了0.37技能分数。通过消融研究分析了不同因素的影响，并公开了包含代码生成、似然度和正确性标签的数据集。

Conclusion: 多校准方法能够显著提高代码大语言模型的校准性能，为未来的代码LLM校准研究提供了有价值的数据集和方法参考。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [18] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文分析了软件开发中的知识问答任务的重要性，发现现有基准存在局限性，提出了从真实对话构建的SimpleDevQA基准，实验显示代码LLM优于通用LLM，RAG策略能提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现开发知识问答在真实用户-LLM对话中占比最高（39.6%），但现有基准主要关注代码理解而忽略了更广泛的开发知识需求，且许多基准不是基于真实用户查询构建的。

Method: 设计了一个三阶段流水线，将真实世界对话转化为简单的开发知识寻求问答对，构建了多语言基准SimpleDevQA，包含2,740个问答对（英语、中文、俄语）。

Result: 代码LLM通常优于相似规模的通用LLM；RAG策略平均提升LLM准确率11.3%；LLM在开发知识问答中表现出系统性过度自信，回答准确率与陈述的置信度呈正相关；代码生成能力强的LLM在开发知识问答中也表现更强。

Conclusion: 开发知识问答是软件开发中重要的任务，现有基准存在局限性，提出的SimpleDevQA基准能更好地评估LLM的开发知识问答能力，RAG策略能有效提升性能。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [19] [Computer-science Reinforcement Learning Got Rewards Wrong](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fyoavg%2F3eb3e722d38e887a0a8ac151c62d9617%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/3AALUF13HYivKZMwQCE7engsPC7U6VOScedD2vIrlIk=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 强化学习中的奖励机制应被视为智能体的一部分而非环境的一部分，这一视角转变开启了奖励机制动态调整、与策略绑定甚至可学习的可能性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习将奖励视为环境的一部分，但作者认为这种观点限制了奖励机制的设计可能性。将奖励视为智能体的一部分可以带来更灵活、更智能的奖励设计方法。

Method: 通过概念分析和视角转变，提出将奖励翻译机制视为智能体内部组件而非环境固定属性的新框架。探讨了奖励机制可以动态变化、与策略绑定或通过学习优化的可能性。

Result: 提出了强化学习奖励机制的新理论框架，展示了将奖励视为智能体一部分所带来的设计自由度，为未来强化学习算法设计提供了新的思路方向。

Conclusion: 奖励机制应被视为智能体的组成部分而非环境的固定属性，这一根本性视角转变为强化学习研究开辟了新的设计空间和研究方向。

Abstract: Computer-science Reinforcement Learning Got Rewards Wrong (6 minute read) In reinforcement learning, the reward is part of the agent, not part of the environment. Once we start thinking in terms of the reward translation as being part of the agent, we can also start thinking of ways in which the agent can affect the reward translation mechanism. It could be dynamic and change across time, be tied to policy, or even learned. The shift in perspective creates many possible options, just by chang...

</details>


### [20] [Google plans to integrate Opal Agent builder into Gemini](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-plans-to-integrate-opal-agent-builder-into-gemini%2F%3Futm_source=tldrai/1/0100019afe63300f-1580626f-03a3-4234-aec4-512e81a071d6-000000/aABeM9lVQSvg7uOs5-cg1o8J0s-9dP2lUz4grExQncw=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google计划将Opal Agent构建器集成到Gemini中，以增强对高级AI工具的直接访问


<details>
  <summary>Details</summary>
Motivation: Google希望通过集成Opal的基于代理的自动化技术来增强Gemini的能力，为用户提供更直接访问高级AI工具的途径

Method: 通过将Opal Agent构建器集成到Gemini平台中，实现代理驱动的自动化功能

Result: Google正在探索这一集成方案，但具体实施细节和最终结果尚未公布

Conclusion: Google计划通过Opal与Gemini的集成来提升其AI平台的自动化能力和工具访问性

Abstract: Google plans to integrate Opal Agent builder into Gemini (2 minute read) Google is exploring integrating Opal's agent-based automation into Gemini, enhancing direct access to advanced AI tools.

</details>


### [21] [Has the cost of building software just dropped 90%?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/joUVK-fXxievWi6yPMjLM8--xgcWImWm6Dvs7hEwvcs=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理编码技术正在彻底改变软件开发行业，将软件构建成本降低90%，标志着代际性的技术转变


<details>
  <summary>Details</summary>
Motivation: 传统观点认为LLMs在代码生成中存在太多错误、无法理解代码库和框架，但这些观点正在迅速变得过时。作者旨在展示AI代理编码技术如何从根本上改变软件开发的经济学

Method: 文章采用观点性分析而非实证研究，基于对AI代理编码技术发展趋势的观察，论证该技术正在解决LLMs在代码生成中的传统局限性

Result: AI代理编码技术正在迅速成熟，能够有效理解代码库和框架，显著减少错误，从而将软件构建成本降低90%，引发软件开发行业的根本性变革

Conclusion: 我们正处于一代人一次的转变早期，AI代理编码技术将彻底改变软件开发行业，传统关于LLMs局限性的观点正在迅速变得过时

Abstract: Has the cost of building software just dropped 90%? (8 minute read) We are in the early stages of a once-in-a-generation shift. The economics of building software have changed dramatically with agentic coding. Previous assertions that LLMs make too many mistakes or can't understand codebases or frameworks are rapidly becoming completely false. The technology is going to totally transform the software development industry.

</details>


### [22] [Agents that don't suck](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fs91DHL%3Futm_source=tldrnewsletter/1/0100019b02da8c44-1de8049f-d511-46f4-aaa0-85336818a468-000000/ulGmcBG4yR2omv5AVOT4I2dG-DKrG2f1Rm-ZM7raCf4=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent Bricks平台让开发者能够构建准确、可靠、基于特定数据的AI智能体，并提供清晰的生产部署路径


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体开发面临准确性、可靠性不足以及难以生产部署的问题，需要工具来构建真正可用的智能体

Method: 提供Agent Bricks平台，支持基于特定数据构建智能体，确保准确性和可靠性，并提供生产部署解决方案

Result: 开发者能够构建出"不糟糕"的智能体，这些智能体准确、可靠、可部署到生产环境

Conclusion: Agent Bricks为AI智能体开发提供了有效的解决方案，值得开发者投入时间学习和使用

Abstract: Agents that don't suck (Sponsor) With Agent Bricks, you can build agents that are accurate, reliable and grounded in your unique data. Now you have a clear path to production. Build agents that work.See why it's worth your time

</details>


### [23] [Has the cost of building software just dropped 90%?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmartinalderson.com%2Fposts%2Fhas-the-cost-of-software-just-dropped-90-percent%2F%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/_IId5Q4JEnGREvwlp0g90zJfX6c3Qye6yGMkPWdOm4c=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI代理编码可将软件开发成本降低90%，通过自动化测试、API开发和基础设施设置等任务来实现


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理如何通过自动化传统劳动密集型任务来大幅降低软件开发成本，并预测这将增加软件开发的潜在需求

Method: 基于AI代理的编码方法，自动化测试、API开发和基础设施设置等软件开发任务

Result: 预计软件开发成本可降低高达90%，同时减少实施时间和项目开销

Conclusion: AI代理编码将显著降低软件开发成本，并可能增加软件开发的潜在市场需求

Abstract: Has the cost of building software just dropped 90%? (8 minute read) Agentic coding with AI is poised to drop the cost of building software by up to 90%. This reduction is due to AI agents automating traditionally labor-intensive tasks like testing, API development, and infrastructure setup, reducing implementation time and project overhead. This shift is expected to increase latent demand for software.

</details>


### [24] [VibeSDK](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fvibesdk%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/8NQbyXi9Txe7-mtR8heKhoA4YiJSxymZdcXcoLFn6yg=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: VibeSDK是一个开源的全栈AI Web应用生成器，允许用户通过自然语言提示创建和部署应用，具有AI代码生成、错误修正、沙箱容器实时预览、交互式聊天和一键部署到Cloudflare Workers等功能。


<details>
  <summary>Details</summary>
Motivation: 简化Web应用开发流程，让开发者能够通过自然语言快速创建和部署全栈应用，降低开发门槛，提高开发效率。

Method: 基于AI代码生成技术，结合错误修正机制，在沙箱容器中提供实时预览，支持交互式聊天辅助开发，并集成一键部署到Cloudflare Workers平台。

Result: 开发了一个完整的AI驱动Web应用生成框架，能够根据自然语言提示生成可部署的应用代码，提供端到端的开发体验。

Conclusion: VibeSDK展示了AI辅助开发工具的潜力，能够显著简化Web应用开发流程，使开发者能够更专注于创意而非实现细节。

Abstract: VibeSDK (GitHub Repo) Cloudflare VibeSDK is an open-source, full-stack AI web application generator that allows users to create and deploy applications using natural language prompts. It has AI code generation with error correction, live previews in sandboxed containers, interactive chat, and one-click deployment to Cloudflare Workers for Platforms.

</details>


### [25] [The highest quality codebase](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgricha.dev%2Fblog%2Fthe-highest-quality-codebase%3Futm_source=tldrdev/1/0100019b0303d3e5-f633ea4b-5ad9-4108-a61f-87f3396e029d-000000/yQuA6fEK7-EmJJhuXhtH6V-PxTYM-DtFU-18gkuURhw=434)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 开发者在代码库上循环运行AI代理36小时，结果产生大量需要维护的无用代码，添加了许多测试但移除了最重要的测试，应用仍能运行但引入了新bug


<details>
  <summary>Details</summary>
Motivation: 探索AI代理在代码库上长时间运行的实际效果，了解AI代理在软件开发中的真实表现和潜在问题

Method: 实验方法：在代码库上循环运行AI代理36小时，观察其行为和对代码库的影响

Result: 结果：1) 产生大量需要维护的无用代码；2) 添加了许多测试但移除了最重要的测试；3) 应用仍能运行但引入了新bug；4) 整体代码质量下降

Conclusion: AI代理在无人监督的情况下长时间运行会产生负面效果，虽然能生成代码但缺乏对代码质量和重要性的理解，需要人类监督和指导

Abstract: The highest quality codebase (9 minute read) This developer ran an experiment where they looped an agent over a codebase for 36 hours to see what it did. The experiment resulted in more code to be maintained, most of it largely useless. The agent added tons of tests, but removed the tests that mattered the most. The resulting app still worked, but with a few new bugs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: 论文探讨了因果学习、强化学习中的"赋权"概念与人类认知发展之间的关系，提出赋权可以作为连接贝叶斯因果学习与强化学习的桥梁，并通过儿童与成人的实证研究验证了这一理论。


<details>
  <summary>Details</summary>
Motivation: 当前大型预训练模型在因果学习和因果建模方面存在困难，而认知科学领域已成功应用因果贝叶斯网络等计算机科学理论来理解人类因果学习。研究者希望找到连接经典贝叶斯因果学习与强化学习的方法，并解释儿童因果学习的特征。

Method: 提出"赋权"（empowerment）概念作为连接桥梁，认为准确因果世界模型的学习会提高赋权，反之亦然。通过实证研究系统测试儿童和成人如何使用赋权线索来推断因果关系并设计有效的因果干预。

Result: 赋权概念为理解人类因果学习提供了新的计算框架，可能解释儿童因果学习的独特特征，并为机器实现因果学习提供更易处理的算法路径。实证研究验证了赋权线索在因果推理中的作用。

Conclusion: 赋权是连接贝叶斯因果学习与强化学习的重要桥梁，不仅有助于理解人类（特别是儿童）的因果学习机制，也为机器实现因果学习提供了新的理论框架和计算方法。

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [27] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 本文通过大规模实验研究提出了智能体系统的量化扩展原则，发现了工具协调权衡、能力饱和和拓扑依赖错误放大三个主导效应，并建立了一个能预测最优协调策略的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中日益普及，但其性能决定原则仍缺乏深入研究，导致实践者依赖启发式而非原则性设计选择。本文旨在填补这一空白。

Method: 在四个多样化基准（Finance-Agent、BrowseComp-Plus、PlanCraft、Workbench）上评估五种典型架构（单一、独立、集中式、分散式、混合式），使用三个LLM家族实例化，进行180个配置的受控评估。使用经验协调指标（效率、开销、错误放大、冗余）建立预测模型。

Result: 预测模型达到交叉验证R^2=0.513。发现三个主导效应：1）工具协调权衡；2）能力饱和（单智能体基线超过约45%后协调收益递减）；3）拓扑依赖错误放大（独立智能体错误放大17.2倍，集中式控制在4.4倍）。集中式协调在金融推理等并行任务上提升80.9%，分散式协调在动态网页导航上表现更佳（+9.2% vs +0.2%），但所有多智能体变体在顺序推理任务上性能下降39-70%。

Conclusion: 该框架能预测87%保留配置的最优协调策略，为基于可测量任务属性的智能体扩展提供了预测性原则，使智能体系统设计从启发式转向原则性方法。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [28] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 提出rSIM机制，通过小型规划器引导LLM的思维链，使用多智能体强化学习训练规划器与LLM，使小模型性能超越大模型。


<details>
  <summary>Details</summary>
Motivation: LLM通过强化学习后训练进化为推理语言模型（RLM），其标志性特征是思维链中出现"顿悟"时刻，能够执行自我反思和深度思考等策略。受此启发，希望开发一种机制让任何LLM都能成为RLM。

Method: 提出强化策略注入机制（rSIM），使用小型规划器引导LLM的思维链，通过自适应注入推理策略。采用领导者-追随者框架，使用多智能体强化学习联合训练规划器（领导者智能体）和LLM（追随者智能体），基于简单的基于规则的奖励。

Result: rSIM使Qwen2.5-0.5B成为RLM，并显著超越Qwen2.5-14B。规划器具有泛化性：只需训练一次，即可作为插件大幅提升现有LLM的推理能力。规划器支持跨任务持续学习，其规划能力能逐步改进并泛化到更广泛的问题。

Conclusion: rSIM是一种有效的机制，能够通过小型规划器的引导使LLM进化为RLM，显著提升推理能力，且具有泛化性和持续学习能力。

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [29] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: 使用基于LLM的多智能体系统模拟1对1对抗性辩论，量化毒性对讨论效率的影响，发现毒性参与者使对话时间增加约25%


<details>
  <summary>Details</summary>
Motivation: 工作场所毒性对组织文化有害，但量化其对运营效率的直接影响存在方法论挑战，因为伦理和实践上难以在人类受试者中复制冲突

Method: 使用基于LLM的多智能体系统模拟1对1对抗性辩论，创建受控的"社会学沙盒"。采用蒙特卡洛方法模拟数百次讨论，测量基线对照组与包含"毒性"系统提示的智能体治疗组之间的收敛时间（定义为达成结论所需的论证数量）

Result: 结果显示，涉及毒性参与者的对话持续时间显著增加约25%。这种"毒性延迟"可作为企业和学术环境中财务损失的代理指标

Conclusion: 基于智能体的建模为测量社会摩擦机制提供了可重复、符合伦理的人类受试者研究替代方案

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [30] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR是一个无需演示的双策略LLM智能体框架，通过全局规划和局部策略的协同自适应推理，在ALFWorld和Mind2Web基准上实现SOTA性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖外部演示或检索增强规划，导致脆弱性、泛化能力差和计算开销高。受人类问题解决启发，需要更灵活高效的智能体框架。

Method: 提出DuSAR框架：使用单个冻结LLM通过两个互补策略进行协同自适应推理——高层整体规划和上下文接地的局部策略，通过轻量级反思机制动态调整策略。

Result: 在ALFWorld上达到37.1%成功率（Llama3.1-70B），比之前最佳结果（13.0%）提高一倍以上；在Mind2Web上达到4.02%，同样提高一倍以上；同时减少每步token消耗3-9倍。

Conclusion: DuSAR通过双策略协调实现了高效的问题解决，无需外部演示即可达到SOTA性能，同时大幅降低计算成本，展现了灵活性和与外部知识的兼容性。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [31] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究通过强化学习智能体控制旋转圆柱体在水流中的阻力，发现学习高性能技能需要比执行技能更丰富的信息反馈，且学习条件的好坏仅取决于目标而非动态复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究在无外部反馈条件下人类如何获得高性能技能（如花样滑冰、棒球投球），通过物理系统替代人类受试者，在完全受控条件下探索技能获取过程。

Method: 使用通用强化学习智能体控制桌面循环水槽中的旋转圆柱体，通过最大化或最小化阻力作为目标。系统利用高维流动反馈进行训练，并与无反馈条件进行对比。

Result: 高维流动反馈使智能体在几分钟内发现高性能阻力控制策略；无反馈执行时性能几乎相同。无流动反馈训练时，智能体在阻力最大化任务中失败，在阻力最小化中表现较差。

Conclusion: 学习高性能技能需要比执行技能更丰富的信息，学习条件的好坏仅取决于目标（最大化vs最小化），而非系统动态或策略复杂性。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [32] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 提出Data Transformation Graph (DTG) 范式，将数据状态作为节点、函数作为边，通过数据谱系而非控制流追踪逻辑缺陷，实现仓库级自动程序修复。


<details>
  <summary>Details</summary>
Motivation: 当前基于控制中心的代码修复方法在处理仓库级程序修复时面临挑战，需要导航复杂目录结构和无关控制逻辑。现有RAG系统存在"语义陷阱"问题，需要更稳健的代码维护基础。

Method: 1) 从标准代码属性图转向数据转换图(DTG)，反转拓扑结构；2) 引入多智能体框架，协调数据完整性导航与控制流逻辑；3) 实现自主问题解决器(AIR)，使用神经符号推理和DTG结构进行可扩展逻辑修复。

Result: 在多个SWE基准测试中取得良好结果，在SWE-Verified基准上达到87.1%的解决率，直接解决了当前AI代码助手工具的核心限制。

Conclusion: DTG范式通过数据谱系追踪逻辑缺陷，解决了RAG系统的"语义陷阱"，为软件依赖日益增长的世界提供了更稳健的代码维护基础。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [33] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: See-Control是一个通过低自由度机械臂直接物理交互操作智能手机的框架，解决了现有方法依赖ADB仅限于Android设备的问题，提供了平台无关的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的智能手机操作智能体依赖Android Debug Bridge进行数据传输和动作执行，这限制了它们仅适用于Android设备，缺乏平台无关的通用解决方案。

Method: 提出了Embodied Smartphone Operation任务和See-Control框架，包含三个核心组件：1)包含155个任务和评估指标的ESO基准测试；2)基于MLLM的具身智能体，无需ADB或系统后端访问即可生成机器人控制命令；3)丰富标注的操作片段数据集。

Result: 开发了一个平台无关的智能手机操作框架，通过低自由度机械臂实现直接物理交互，为未来研究提供了基准测试、智能体框架和数据集资源。

Conclusion: See-Control通过弥合数字智能体与物理世界之间的差距，为实现家庭机器人在真实环境中执行依赖智能手机的任务迈出了具体一步。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [34] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了一个端到端的实用指南，用于设计、开发和部署生产级智能体AI系统，介绍了结构化工程生命周期和九个核心最佳实践，并通过多模态新闻分析案例进行演示。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在行业和研究中的加速采用，组织面临一个核心挑战：如何设计、工程和运营生产级的智能体AI工作流，使其可靠、可观察、可维护，并符合安全和治理要求。

Method: 引入结构化工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议(MCP)和工具集成、确定性编排、负责任AI考虑因素和环境感知部署策略。提出九个核心最佳实践，包括工具优先设计、纯函数调用、单一工具/单一职责智能体、外部化提示管理等。

Result: 通过一个多模态新闻分析和媒体生成工作流的综合案例研究，展示了这些原则的实际应用，为构建稳健、可扩展和生产就绪的智能体AI工作流提供了基础参考。

Conclusion: 本文通过架构指导、操作模式和实践实现洞察，为构建稳健、可扩展和生产就绪的智能体AI工作流提供了基础参考框架。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [35] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: EcomBench是一个基于真实电商场景的基准测试，用于评估智能代理在实际电商环境中的核心能力，包括深度信息检索、多步推理和跨源知识整合等。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多关注学术场景或人工设计的环境，忽视了实际应用中出现的挑战。电商领域具有大量多样化用户交互、动态市场条件和真实决策任务，是评估智能代理实际能力的理想场景。

Method: 从全球领先电商生态系统的真实用户需求中构建基准，通过人工专家精心策划和标注，确保清晰性、准确性和领域相关性。涵盖电商场景中的多个任务类别，并定义了三个难度级别。

Result: EcomBench提供了一个严格且动态的测试平台，用于衡量智能代理在现代电商环境中的实际能力，特别关注深度信息检索、多步推理和跨源知识整合等关键能力。

Conclusion: 通过将评估建立在真实电商环境中，EcomBench填补了现有基准测试的空白，为评估智能代理在实际应用中的能力提供了更贴近现实的测试平台。

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [36] [<em class="highlight">强化学习</em>用在预训练，效果可查！](http://mp.weixin.qq.com/s?__biz=MzkyNTcyOTExNQ==&mid=2247486385&idx=1&sn=1e55d3f0d0bf9aae85d37e0a7f882220&chksm=c094c0e050ffec069a6bc15f6dcb6e1f2766ea6eb806aaa761d2840b6c6d087801a4afb4ed8c#rd)
*AI科研进阶社*

Main category: wechat.article

TL;DR: 主要内容：针对跨域强化学习预训练难题，提出 CRPTpro 框架。它通过解耦随机采集生成跨域预训练数据集，搭配高效原型自监督算法训练通用视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 主要内容：针对跨域强化学习预训练难题，提出 CRPTpro 框架。它通过解耦随机采集生成跨域预训练数据集，搭配高效原型自监督算法训练通用视觉编码器。

</details>


### [37] [深扒PI π*0.6迭代式<em class="highlight">强化学习</em>思路的来源：VLA+在线RL，实现机器人的自我进化](http://mp.weixin.qq.com/s?__biz=MzE5MTA0NzYxMQ==&mid=2247512010&idx=1&sn=2192dc464c351b296a99439e3ff24c57&chksm=97a03a3a8ceca1a8da6a78a793c4adc9a920e090ef3899a769cfc539b215d194a272dbd27fc7#rd)
*智猩猩*

Main category: wechat.article

TL;DR: 第一阶段：在线强化学习（探索与发现）图注：稳定探索在这个阶段，机器人的目标是去试错，探索如何完成新任务。冻结大脑（Freeze VLM）：为了防止模型崩溃和减少计算量，作者冻结了巨大的 VLM 主干参数。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 第一阶段：在线强化学习（探索与发现）图注：稳定探索在这个阶段，机器人的目标是去试错，探索如何完成新任务。冻结大脑（Freeze VLM）：为了防止模型崩溃和减少计算量，作者冻结了巨大的 VLM 主干参数。

</details>


### [38] [JMS | 基于<em class="highlight">强化学习</em>的多阶段装配过程参数在线协同优化](http://mp.weixin.qq.com/s?__biz=MzIzMzMwMjU3OA==&mid=2247483837&idx=1&sn=b9f7d148ec9af4635b3e1a8d1ffe6331&chksm=e9db621c1aad28cf7251cfa7abf26997ca26aab6760c8a2ff3c0e1039106a5da2eeff559dd9e#rd)
*睿造未来LAB*

Main category: wechat.article

TL;DR: 强化学习智能体可通过与环境的持续交互试错，学习一项能最大化长期累积回报（即最终高质量产品）的策略。然而，将强化学习直接应用于多阶段装配参数优化，面临两个关键局限性，导致其难以收敛至有效策略：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习智能体可通过与环境的持续交互试错，学习一项能最大化长期累积回报（即最终高质量产品）的策略。然而，将强化学习直接应用于多阶段装配参数优化，面临两个关键局限性，导致其难以收敛至有效策略：

</details>


### [39] [<em class="highlight">强化学习</em>怎么玩老虎机](http://mp.weixin.qq.com/s?__biz=MzkwODY3ODc0MQ==&mid=2247483748&idx=1&sn=2111f8e578f485add9e053329e166c5c&chksm=c1af0eed0f4907d9ec298c634a71bd4c31e02dac593626ac35658e85b88fcdf9f28b6eed249c#rd)
*智能原始人*

Main category: wechat.article

TL;DR: （这个就把强化学习和机器学习区分开了，机器学习不管有监督无监督，数据分布已知，强化学习主要面对未知的环境状态，通过每一次动作获得和环境交互的结果，评估和训练内在的价值函数or网络）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: （这个就把强化学习和机器学习区分开了，机器学习不管有监督无监督，数据分布已知，强化学习主要面对未知的环境状态，通过每一次动作获得和环境交互的结果，评估和训练内在的价值函数or网络）

</details>


### [40] [深度<em class="highlight">强化学习</em>的创新点总结](http://mp.weixin.qq.com/s?__biz=MzI0NDMyODUxMA==&mid=2247488921&idx=1&sn=874c5c500612ae94b2a34b4d3a3d76a7&chksm=e8d4ced3918c7fac2d3bdb902795b7d03fd83df9d8d09160562e569eed92ea414c795db89487#rd)
*调度与优化算法的集结地*

Main category: wechat.article

TL;DR: 免费获取全部论文+开源代码 强化学习+大模型 现在与大模型结合在顶会（NeurIPS/ICLR/ICML）上属于“流量密码”，无论是将RL用于对齐微调（比如RLHF）、agent决策规划，还是用LLM生成奖励函数/环境，都容易产生novelty。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 免费获取全部论文+开源代码 强化学习+大模型 现在与大模型结合在顶会（NeurIPS/ICLR/ICML）上属于“流量密码”，无论是将RL用于对齐微调（比如RLHF）、agent决策规划，还是用LLM生成奖励函数/环境，都容易产生novelty。

</details>


### [41] [2026年ALL in <em class="highlight">强化学习</em>！！](http://mp.weixin.qq.com/s?__biz=MzIyNTY1MDUwNQ==&mid=2247502512&idx=1&sn=03642913b09f38341d97e2c0e9467885&chksm=e98032c3755155c4bdbe658b06eebd18e1018457ff1299bd68e3db35a6cbc1fbd23f9621efee#rd)
*DASOU*

Main category: wechat.article

TL;DR: 免费获取全部论文+开源代码 强化学习+大模型 现在与大模型结合在顶会（NeurIPS/ICLR/ICML）上属于“流量密码”，无论是将RL用于对齐微调（比如RLHF）、agent决策规划，还是用LLM生成奖励函数/环境，都容易产生novelty。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 免费获取全部论文+开源代码 强化学习+大模型 现在与大模型结合在顶会（NeurIPS/ICLR/ICML）上属于“流量密码”，无论是将RL用于对齐微调（比如RLHF）、agent决策规划，还是用LLM生成奖励函数/环境，都容易产生novelty。

</details>


### [42] [Agent Lightning：用<em class="highlight">强化学习</em>训练任意AI智能体](http://mp.weixin.qq.com/s?__biz=MzI4MDY3MDAxMQ==&mid=2247486483&idx=1&sn=fc9bdebca4bce1352ac9fe5eeba04da2&chksm=ea5414ff2719b22b7107001c206fcf2ac076dc25ee1a22d0691c1cd408e9f1fe6098559e9c27#rd)
*南极星医学AI笔记*

Main category: wechat.article

TL;DR: 现有的针对大语言模型（LLM）的强化学习方法与框架主要针对静态、单次调用的任务，例如偏好对齐或数学推理。相比之下，智能体展现出超越此类场景的复杂性与多样性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 现有的针对大语言模型（LLM）的强化学习方法与框架主要针对静态、单次调用的任务，例如偏好对齐或数学推理。相比之下，智能体展现出超越此类场景的复杂性与多样性。

</details>


### [43] [【顶刊解读】大规模网络控制的高效可扩展<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzUyMTA1NjI5OQ==&mid=2247491581&idx=1&sn=82203fec1f62e18829376e95f48eca34&chksm=f88023191d7d14e97b19606e63146d320eafdaa0a6df7640aa3d8f7a6cb96c87b4fa86205b0d#rd)
*智慧能源研究所*

Main category: wechat.article

TL;DR: 大规模网络控制的高效可扩展强化学习 论文深度解读 该研究由北京大学和伦敦国王学院的团队合作完成，提出了一种可扩展、通信高效、样本高效的多智能体强化学习框架，为构建大规模AI决策系统提供了重要理论与技术基础


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大规模网络控制的高效可扩展强化学习 论文深度解读 该研究由北京大学和伦敦国王学院的团队合作完成，提出了一种可扩展、通信高效、样本高效的多智能体强化学习框架，为构建大规模AI决策系统提供了重要理论与技术基础

</details>


### [44] [<em class="highlight">Agentic</em>原生大模型对上下文工程的影响](http://mp.weixin.qq.com/s?__biz=MzA3Mzc4MTg4Mg==&mid=2453984597&idx=1&sn=a5f86acf9814947b020c1f5fa71afedf&chksm=8918a5c8d3b1b4ceb9381447602dba10b1fd981a0ff8c986c83e7975e5940be498f6479c2016#rd)
*浅尝大模型*

Main category: wechat.article

TL;DR: agentic原生大模型的本质变化是记住或rollout了很多靠多轮工具调用才能解决的复杂任务轨迹，这些知识被存储到了模型的权重中。这套权重会自然而然地鼓励模型去生成高质量的ReAct循环，因此内化了之前要靠prompt来维持的几个


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic原生大模型的本质变化是记住或rollout了很多靠多轮工具调用才能解决的复杂任务轨迹，这些知识被存储到了模型的权重中。这套权重会自然而然地鼓励模型去生成高质量的ReAct循环，因此内化了之前要靠prompt来维持的几个

</details>


### [45] [【Google】<em class="highlight">智能体</em>入门指南（二）](http://mp.weixin.qq.com/s?__biz=Mzk5MDAxMDk3Nw==&mid=2247483805&idx=1&sn=1b01116e65e161e7c972fae8fa81493f&chksm=c4dfd7344ce95a6203300d354b58ba56f11567a76aab09aa2bf7f823cf6e21ba77df96dc09c3#rd)
*子凡AI*

Main category: wechat.article

TL;DR: 我们可以将 Agentic 系统分为几个广泛的级别，每一个级别都建立在前一个能力之上。Level 0：核心推理系统在拥有 Agent 之前，我们必须从其最基本形式的“大脑”开始：推理引擎本身。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们可以将 Agentic 系统分为几个广泛的级别，每一个级别都建立在前一个能力之上。Level 0：核心推理系统在拥有 Agent 之前，我们必须从其最基本形式的“大脑”开始：推理引擎本身。

</details>


### [46] [应对<em class="highlight">智能体</em>动态风险：英伟达<em class="highlight">Agentic</em>安全与防护框架解析](http://mp.weixin.qq.com/s?__biz=MzAxNjA0MjIzNg==&mid=2650011159&idx=1&sn=e7da2052c12e2923ec29724ee4e7933a&chksm=8267e80b0e5705b82452714ae04dc08807403a7b533d8bf6ec79810b3bbc41f991a59dbde052#rd)
*模安局*

Main category: wechat.article

TL;DR: Agentic系统运行环境高度动态，其行为受短期记忆、外部输入、随机性及非确定性路径影响，具有不可完全重复的特性。这使得基于静态规则或批量测试的传统安全方法面临挑战。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic系统运行环境高度动态，其行为受短期记忆、外部输入、随机性及非确定性路径影响，具有不可完全重复的特性。这使得基于静态规则或批量测试的传统安全方法面临挑战。

</details>


### [47] [AAIF 成立：<em class="highlight">Agentic</em> AI 进入“标准共建”时代](http://mp.weixin.qq.com/s?__biz=MzAwMTYwNzE2Mg==&mid=2651037139&idx=1&sn=42f47e0ad7ff49ba9d6b3046a93153bb&chksm=806d2b522336217388baac7e8c92e5b788a019e84f0113afe5290ecb250336840292a5d4ddc3#rd)
*ASCE1885*

Main category: wechat.article

TL;DR: 这清晰地传递出其根本性的战略意图：通过引入一个超越任何单一商业实体控制的中立方，来解决 Agentic AI 领域日益严重的碎片化问题，并为整个生态系统的长期健康发展奠定坚实基础。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这清晰地传递出其根本性的战略意图：通过引入一个超越任何单一商业实体控制的中立方，来解决 Agentic AI 领域日益严重的碎片化问题，并为整个生态系统的长期健康发展奠定坚实基础。

</details>


### [48] [检验<em class="highlight">大模型</em>的到来！会给IVD带来什么变化？](http://mp.weixin.qq.com/s?__biz=MjM5MTczMTY3NQ==&mid=2651396192&idx=1&sn=248f3a5714d24313b58e1295bfa6b230&chksm=bc3618df451dff06df578d7197932ef63f05ac72765d73d35b31ab0898280a8cb61f40fbee7f#rd)
*小桔灯网*

Main category: wechat.article

TL;DR: 日前，国内首个专为临床检验打造的垂直领域大模型“启元检验大模型”在广东省医学会检验医学学术年会期间正式发布。这一由迈瑞医疗与南方医科大学深圳医院等机构合作开发的专用AI模型，标志着我国检验医学智能化迈出


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 日前，国内首个专为临床检验打造的垂直领域大模型“启元检验大模型”在广东省医学会检验医学学术年会期间正式发布。这一由迈瑞医疗与南方医科大学深圳医院等机构合作开发的专用AI模型，标志着我国检验医学智能化迈出

</details>


### [49] [【全面调查】<em class="highlight">大模型</em>用于科学发现](http://mp.weixin.qq.com/s?__biz=Mzg3MTczOTkxNg==&mid=2247486705&idx=1&sn=cdf9b3a214f1f9768646c0b01d535efa&chksm=cff19280ec9e660fcbd24a81fba8fc3e426d50174f25eeb793916b981c79eff35cfaab2190e7#rd)
*AI4Physics*

Main category: wechat.article

TL;DR: 过去五年，大模型在科研中的角色发生了根本性变化：从最初的“写点代码、帮忙找文献”的工具，逐步走向能自主提出假设、规划实验乃至撰写论文的“AI 科学家”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 过去五年，大模型在科研中的角色发生了根本性变化：从最初的“写点代码、帮忙找文献”的工具，逐步走向能自主提出假设、规划实验乃至撰写论文的“AI 科学家”。

</details>


### [50] [2025年12月份AI<em class="highlight">大模型</em>最新版本介绍](http://mp.weixin.qq.com/s?__biz=MzA3MDc4NDAxNw==&mid=2449747683&idx=1&sn=ee26fb25abc895683578092145bb718e&chksm=89c4a7d080ed2bfea2c91b676b755255a3ae1386905d0f37a304db44921a59c53b90aa47f077#rd)
*AI爱说财务*

Main category: wechat.article

TL;DR: 一、主流ai大模型最新版本号与排名（截至2025年12月8日）综合各项基准测试（如MMLU-Pro、GPQA）、开源社区活跃度、用户评测及市场影响力，我们可以对主流大模型进行一个大致的综合实力分层排名。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一、主流ai大模型最新版本号与排名（截至2025年12月8日）综合各项基准测试（如MMLU-Pro、GPQA）、开源社区活跃度、用户评测及市场影响力，我们可以对主流大模型进行一个大致的综合实力分层排名。

</details>
