{"id": "2512.16956", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16956", "abs": "https://arxiv.org/abs/2512.16956", "authors": ["Shravan Chaudhari", "Rahul Thomas Jacob", "Mononito Goswami", "Jiajun Cao", "Shihab Rashid", "Christian Bock"], "title": "SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization", "comment": "Initial preprint", "summary": "Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.", "AI": {"tldr": "SpIDER\u662f\u4e00\u79cd\u589e\u5f3a\u7684\u5bc6\u96c6\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u63a8\u7406\u548c\u56fe\u7ed3\u6784\u63a2\u7d22\u6765\u6539\u8fdb\u4ee3\u7801\u5355\u5143\u7684\u8bed\u4e49\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u4ee3\u7801\u68c0\u7d22\u65b9\u6cd5\uff08\u5982BM25\u548c\u5bc6\u96c6\u5d4c\u5165\uff09\u5b58\u5728\u5c40\u9650\u6027\uff1aBM25\u6027\u80fd\u8f83\u5dee\uff0c\u800c\u5bc6\u96c6\u5d4c\u5165\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u5e93\u7684\u5145\u5206\u63a2\u7d22\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4ee3\u7801\u7684\u56fe\u7ed3\u6784\u4fe1\u606f", "method": "\u63d0\u51faSpIDER\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bc6\u96c6\u5d4c\u5165\u68c0\u7d22\u4e0e\u57fa\u4e8e\u56fe\u7684\u4ee3\u7801\u5e93\u63a2\u7d22\uff0c\u901a\u8fc7LLM\u63a8\u7406\u5229\u7528\u8f85\u52a9\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u589e\u5f3a\u68c0\u7d22\u6548\u679c", "result": "SpIDER\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u5bc6\u96c6\u68c0\u7d22\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u7ed3\u5408\u56fe\u7ed3\u6784\u63a2\u7d22\u548cLLM\u63a8\u7406\u7684\u5bc6\u96c6\u68c0\u7d22\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u8fdb\u4ee3\u7801\u5355\u5143\u7684\u8bed\u4e49\u68c0\u7d22\uff0c\u4e3aLLM\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u66f4\u597d\u7684\u57fa\u7840\u80fd\u529b", "topic": "code agent"}}
{"id": "2512.17053", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.17053", "abs": "https://arxiv.org/abs/2512.17053", "authors": ["Khushboo Thaker", "Yony Bresler"], "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL", "comment": null, "summary": "Deploying accurate Text-to-SQL systems at the enterprise level faces a difficult trilemma involving cost, security and performance. Current solutions force enterprises to choose between expensive, proprietary Large Language Models (LLMs) and low-performing Small Language Models (SLMs). Efforts to improve SLMs often rely on distilling reasoning from large LLMs using unstructured Chain-of-Thought (CoT) traces, a process that remains inherently ambiguous. Instead, we hypothesize that a formal, structured reasoning representation provides a clearer, more reliable teaching signal, as the Text-to-SQL task requires explicit and precise logical steps. To evaluate this hypothesis, we propose Struct-SQL, a novel Knowledge Distillation (KD) framework that trains an SLM to emulate a powerful large LLM. Consequently, we adopt a query execution plan as a formal blueprint to derive this structured reasoning. Our SLM, distilled with structured CoT, achieves an absolute improvement of 8.1% over an unstructured CoT distillation baseline. A detailed error analysis reveals that a key factor in this gain is a marked reduction in syntactic errors. This demonstrates that teaching a model to reason using a structured logical blueprint is beneficial for reliable SQL generation in SLMs.", "AI": {"tldr": "Struct-SQL\uff1a\u4e00\u4e2a\u65b0\u9896\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u63a8\u7406\u8868\u793a\uff08\u67e5\u8be2\u6267\u884c\u8ba1\u5212\uff09\u6765\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728Text-to-SQL\u4efb\u52a1\u4e0a\u6bd4\u975e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u84b8\u998f\u57fa\u7ebf\u63d0\u53478.1%\uff0c\u4e3b\u8981\u51cf\u5c11\u8bed\u6cd5\u9519\u8bef\u3002", "motivation": "\u4f01\u4e1a\u7ea7Text-to-SQL\u7cfb\u7edf\u9762\u4e34\u6210\u672c\u3001\u5b89\u5168\u548c\u6027\u80fd\u7684\u4e09\u96be\u56f0\u5883\uff1a\u9700\u8981\u5728\u6602\u8d35\u7684\u5927\u578b\u4e13\u6709LLM\u548c\u6027\u80fd\u8f83\u4f4e\u7684\u5c0f\u578b\u6a21\u578b\u4e4b\u95f4\u505a\u51fa\u9009\u62e9\u3002\u73b0\u6709\u6539\u8fdb\u5c0f\u578b\u6a21\u578b\u7684\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u56fa\u6709\u7684\u6a21\u7cca\u6027\u3002", "method": "\u63d0\u51faStruct-SQL\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u7528\u67e5\u8be2\u6267\u884c\u8ba1\u5212\u4f5c\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u8868\u793a\u6765\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u6b63\u5f0f\u7684\u63a8\u7406\u84dd\u56fe\uff0c\u8ba9\u5c0f\u578b\u6a21\u578b\u6a21\u4eff\u5927\u578bLLM\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\u3002", "result": "\u4f7f\u7528\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u84b8\u998f\u7684\u5c0f\u578b\u6a21\u578b\u76f8\u6bd4\u975e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u84b8\u998f\u57fa\u7ebf\u5b9e\u73b0\u4e868.1%\u7684\u7edd\u5bf9\u63d0\u5347\u3002\u8be6\u7ec6\u9519\u8bef\u5206\u6790\u663e\u793a\uff0c\u8fd9\u79cd\u63d0\u5347\u4e3b\u8981\u6e90\u4e8e\u8bed\u6cd5\u9519\u8bef\u7684\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u4f7f\u7528\u7ed3\u6784\u5316\u903b\u8f91\u84dd\u56fe\u6559\u5bfc\u6a21\u578b\u63a8\u7406\u5bf9\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53ef\u9760\u7684SQL\u662f\u6709\u76ca\u7684\uff0c\u7ed3\u6784\u5316\u63a8\u7406\u8868\u793a\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u3001\u66f4\u53ef\u9760\u7684\u6559\u5b66\u4fe1\u53f7\u3002", "topic": "code agent"}}
{"id": "2512.17371", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.17371", "abs": "https://arxiv.org/abs/2512.17371", "authors": ["Haomin Qi", "Fengfei Yu", "Chengbo Huang"], "title": "GraphCue for SDN Configuration Code Synthesis", "comment": "2 pages, 2 figures", "summary": "We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.", "AI": {"tldr": "GraphCue\u662f\u4e00\u4e2a\u57fa\u4e8e\u62d3\u6251\u7684\u68c0\u7d22\u548c\u4ee3\u7406\u5faa\u73af\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316SDN\u914d\u7f6e\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u3001\u7ea6\u675f\u6027\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\u5faa\u73af\u5b9e\u73b0\u9ad8\u6548\u914d\u7f6e", "motivation": "\u81ea\u52a8\u5316SDN\uff08\u8f6f\u4ef6\u5b9a\u4e49\u7f51\u7edc\uff09\u914d\u7f6e\u901a\u5e38\u590d\u6742\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u89e3\u51b3\u62d3\u6251\u611f\u77e5\u7684\u914d\u7f6e\u68c0\u7d22\u548c\u7ea6\u675f\u6027\u4ee3\u7801\u751f\u6210\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u914d\u7f6e\u51c6\u786e\u6027\u548c\u6548\u7387", "method": "\u5c06\u6bcf\u4e2a\u914d\u7f6e\u6848\u4f8b\u62bd\u8c61\u4e3aJSON\u56fe\uff0c\u4f7f\u7528\u4e09\u5c42GCN\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\uff1b\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u68c0\u7d22\u627e\u5230\u6700\u8fd1\u7684\u6709\u6548\u53c2\u8003\u914d\u7f6e\uff1b\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u7ea6\u675f\u4ee3\u7801\u751f\u6210\uff1b\u901a\u8fc7\u9a8c\u8bc1\u5668\u6267\u884c\u5019\u9009\u914d\u7f6e\u5e76\u5c06\u5931\u8d25\u53cd\u9988\u7ed9\u4ee3\u7406\u5f62\u6210\u95ed\u73af", "result": "\u5728628\u4e2a\u9a8c\u8bc1\u6848\u4f8b\u4e2d\uff0cGraphCue\u572820\u6b21\u8fed\u4ee3\u5185\u8fbe\u523088.2%\u901a\u8fc7\u7387\uff0c95%\u7684\u9a8c\u8bc1\u5faa\u73af\u57289\u79d2\u5185\u5b8c\u6210\uff1b\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0c\u6ca1\u6709\u68c0\u7d22\u6216\u7ed3\u6784\u5316\u63d0\u793a\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d", "conclusion": "\u62d3\u6251\u611f\u77e5\u68c0\u7d22\u548c\u7ea6\u675f\u6027\u6761\u4ef6\u5316\u662fGraphCue\u6027\u80fd\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316SDN\u914d\u7f6e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2512.16970", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.16970", "abs": "https://arxiv.org/abs/2512.16970", "authors": ["Kamer Ali Yuksel"], "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework", "comment": null, "summary": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.", "AI": {"tldr": "PAACE\u662f\u4e00\u4e2a\u7528\u4e8e\u4f18\u5316LLM\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u5212\u611f\u77e5\u7684\u81ea\u52a8\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u5728\u51cf\u5c11\u4e0a\u4e0b\u6587\u8d1f\u8f7d\u7684\u540c\u65f6\u63d0\u9ad8\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u591a\u6b65\u5de5\u4f5c\u6d41\u4e2d\u4ea7\u751f\u5feb\u901f\u6269\u5c55\u7684\u4e0a\u4e0b\u6587\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4e86\u591a\u6b65\u3001\u8ba1\u5212\u611f\u77e5\u7684\u667a\u80fd\u4f53\u63a8\u7406\u7279\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faPAACE\u6846\u67b6\uff0c\u5305\u542b\uff1a1) PAACE-Syn\uff1a\u751f\u6210\u5e26\u538b\u7f29\u76d1\u7763\u6807\u6ce8\u7684\u5408\u6210\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff1b2) PAACE-FT\uff1a\u4ece\u6210\u529f\u6559\u5e08\u6f14\u793a\u4e2d\u84b8\u998f\u8bad\u7ec3\u7684\u8ba1\u5212\u611f\u77e5\u538b\u7f29\u5668\u3002", "result": "\u5728AppWorld\u3001OfficeBench\u548c8-Objective QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAACE\u5728\u63d0\u9ad8\u667a\u80fd\u4f53\u6b63\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e0a\u4e0b\u6587\u8d1f\u8f7d\u3002\u84b8\u998f\u6a21\u578b\u4fdd\u755997%\u6027\u80fd\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "PAACE\u8bc1\u660e\u4e86\u8ba1\u5212\u611f\u77e5\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u4e3a\u7d27\u51d1\u6a21\u578b\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.17387", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17387", "abs": "https://arxiv.org/abs/2512.17387", "authors": ["Sravani Gunnu", "Shanmukha Guttula", "Hima Patel"], "title": "CIFE: Code Instruction-Following Evaluation", "comment": "20 pages, 22 figures, 2 tables", "summary": "Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b1000\u4e2aPython\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5e73\u5747\u67097\u4e2a\u5f00\u53d1\u8005\u6307\u5b9a\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u4e0d\u4ec5\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u8fd8\u8981\u9075\u5faa\u5404\u79cd\u7ea6\u675f\u8981\u6c42\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u751f\u6210\u4e2d\uff0c\u4ec5\u529f\u80fd\u6b63\u786e\u6027\u4e0d\u8db3\u4ee5\u53ef\u9760\u90e8\u7f72\uff0c\u5f00\u53d1\u8005\u8fd8\u9700\u8981\u6a21\u578b\u9075\u5faa\u9c81\u68d2\u6027\u3001\u683c\u5f0f\u5316\u548c\u5b89\u5168\u6027\u7b49\u7ea6\u675f\u8981\u6c42\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u57fa\u4e8e\u6d4b\u8bd5\u7528\u4f8b\u8bc4\u4f30\u6b63\u786e\u6027\uff0c\u5bf9\u6a21\u578b\u5982\u4f55\u53ef\u9760\u9075\u5faa\u8fd9\u4e9b\u7ea6\u675f\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\u3002", "method": "\u901a\u8fc7\u56db\u9636\u6bb5\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\u6784\u5efa\u5305\u542b1000\u4e2aPython\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5e73\u5747\u67097\u4e2a\u5f00\u53d1\u8005\u6307\u5b9a\u7684\u7ea6\u675f\uff0c\u6db5\u76d613\u4e2a\u7c7b\u522b\u3002\u7ea6\u675f\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u786e\u4fdd\u539f\u5b50\u6027\u3001\u76f8\u5173\u6027\u548c\u5ba2\u89c2\u6027\u3002\u8bc4\u4f3014\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u4f7f\u7528\u4e92\u8865\u7684\u9075\u5faa\u5ea6\u6307\u6807\uff0c\u5e76\u63d0\u51faC2A\u5206\u6570\u4f5c\u4e3a\u7efc\u5408\u8861\u91cf\u6b63\u786e\u6027\u548c\u7ea6\u675f\u9075\u5faa\u7684\u590d\u5408\u6307\u6807\u3002", "result": "\u7ed3\u679c\u663e\u793a\u90e8\u5206\u9075\u5faa\u548c\u4e25\u683c\u9075\u5faa\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u5f3a\u5927\u6a21\u578b\u7684\u90e8\u5206\u9075\u5faa\u7387\u8d85\u8fc790%\uff0c\u4f46\u4e25\u683c\u9075\u5faa\u7387\u4ec5\u4e3a39-66%\u3002\u8fd9\u8868\u660e\u5f53\u524dLLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u867d\u7136\u529f\u80fd\u6b63\u786e\u6027\u8f83\u9ad8\uff0c\u4f46\u5728\u4e25\u683c\u9075\u5faa\u5f00\u53d1\u8005\u610f\u56fe\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u53ef\u4fe1\u8d56\u7684\u4ee3\u7801\u751f\u6210\u4e0d\u4ec5\u9700\u8981\u6b63\u786e\u6027\uff0c\u8fd8\u9700\u8981\u5bf9\u5f00\u53d1\u8005\u610f\u56fe\u7684\u4e00\u81f4\u9075\u5faa\u3002\u8be5\u57fa\u51c6\u548cC2A\u5206\u6570\u4e3a\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002", "topic": "swe benchmark"}}
{"id": "2512.17041", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.17041", "abs": "https://arxiv.org/abs/2512.17041", "authors": ["Ali Eslami", "Jiangbo Yu"], "title": "Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats", "comment": null, "summary": "Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u667a\u80fd\u8f66\u8f86\u4e2d\u4ee3\u7406\u5f0fAI\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ecOWASP\u5f0f\u98ce\u9669\u548c\u8de8\u5c42\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u89d2\u8272\u7684\u67b6\u6784\u548c\u4e25\u91cd\u6027\u77e9\u9635\u6846\u67b6\u3002", "motivation": "\u73b0\u6709OWASP\u4ee3\u7406\u5f0fAI\u5b89\u5168\u98ce\u9669\u6846\u67b6\u672a\u9488\u5bf9\u8f66\u8f86\u7b49\u5b89\u5168\u5173\u952e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4e5f\u672a\u8003\u8651\u4e0e\u611f\u77e5\u3001\u901a\u4fe1\u3001\u63a7\u5236\u7b49\u5c42\u7684\u4ea4\u4e92\uff0c\u9700\u8981\u4e13\u95e8\u7814\u7a76\u667a\u80fd\u8f66\u8f86\u7684\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89d2\u8272\u7684\u667a\u80fd\u8f66\u8f86\u67b6\u6784\uff08\u4e2a\u4eba\u4ee3\u7406\u548c\u9a7e\u9a76\u7b56\u7565\u4ee3\u7406\uff09\uff0c\u5206\u6790\u4ee3\u7406\u5f0fAI\u5c42\u548c\u8de8\u5c42\u98ce\u9669\uff0c\u4f7f\u7528\u4e25\u91cd\u6027\u77e9\u9635\u548c\u653b\u51fb\u94fe\u5206\u6790\u5c55\u793a\u5c0f\u6270\u52a8\u5982\u4f55\u5347\u7ea7\u4e3a\u4e0d\u5b89\u5168\u884c\u4e3a\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\u6765\u5206\u6790\u5f53\u524d\u548c\u65b0\u5174\u8f66\u8f86\u5e73\u53f0\u4e2d\u4ee3\u7406\u5f0fAI\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5c55\u793a\u4e86\u8de8\u5c42\u653b\u51fb\u5982\u4f55\u5bfc\u81f4\u8f66\u8f86\u884c\u4e3a\u5931\u51c6\u6216\u4e0d\u5b89\u5168\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u8f66\u8f86\u4ee3\u7406\u5f0fAI\u5b89\u5168\u98ce\u9669\u5206\u6790\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u57fa\u7840\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5b89\u5168\u6846\u67b6\u5728\u8f66\u8f86\u7b49\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u7a7a\u767d\u3002", "topic": "agent analysis"}}
{"id": "2512.17419", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17419", "abs": "https://arxiv.org/abs/2512.17419", "authors": ["Lilin Wang", "Lucas Ramalho", "Alan Celestino", "Phuc Anthony Pham", "Yu Liu", "Umang Kumar Sinha", "Andres Portillo", "Onassis Osunwa", "Gabriel Maduekwe"], "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories", "comment": null, "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.", "AI": {"tldr": "SWE-Bench++ \u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u4eceGitHub\u62c9\u53d6\u8bf7\u6c42\u751f\u6210\u4ed3\u5e93\u7ea7\u7f16\u7801\u4efb\u52a1\uff0c\u652f\u630111\u79cd\u8bed\u8a00\uff0c\u5305\u542b11,133\u4e2a\u5b9e\u4f8b\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982SWE-bench\uff09\u5b58\u5728\u5c40\u9650\u6027\uff1a\u624b\u52a8\u6784\u5efa\u3001\u9759\u6001\u6570\u636e\u96c6\u3001\u4ec5\u5173\u6ce8Python\u9519\u8bef\u4fee\u590d\u3002\u9700\u8981\u66f4\u81ea\u52a8\u5316\u3001\u591a\u8bed\u8a00\u3001\u8986\u76d6\u66f4\u5e7f\u4efb\u52a1\u7c7b\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u9636\u6bb5\u81ea\u52a8\u5316\u751f\u6210\u4efb\u52a1\uff1a1) \u7a0b\u5e8f\u5316\u91c7\u96c6GitHub\u62c9\u53d6\u8bf7\u6c42\uff1b2) \u73af\u5883\u5408\u6210\uff1b3) \u6d4b\u8bd5\u9884\u8a00\u63d0\u53d6\uff1b4) \u8d28\u91cf\u4fdd\u8bc1\u3002\u8fd8\u5305\u542b\u63d0\u793a\u5f15\u5bfc\u7684\u8f68\u8ff9\u5408\u6210\u6b65\u9aa4\uff0c\u5c06\u5931\u8d25\u5b9e\u4f8b\u8f6c\u5316\u4e3a\u8bad\u7ec3\u8f68\u8ff9\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b11,133\u4e2a\u5b9e\u4f8b\u30013,971\u4e2a\u4ed3\u5e93\u300111\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u57281,782\u4e2a\u5b9e\u4f8b\u5b50\u96c6\u4e0a\uff0cClaude Sonnet 4.5\u8fbe\u523036.20% pass@10\uff0cGPT-5 34.57%\uff0cGemini 2.5 Pro 24.92%\uff0cGPT-4o 16.89%\u3002\u5fae\u8c03SWE-Bench++\u5b9e\u4f8b\u5728SWE-bench\u591a\u8bed\u8a00\u57fa\u51c6\u4e0a\u5e26\u6765\u53ef\u6d4b\u91cf\u7684\u6539\u8fdb\u3002", "conclusion": "SWE-Bench++\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u591a\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u4ed3\u5e93\u7ea7\u4ee3\u7801\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2512.17008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17008", "abs": "https://arxiv.org/abs/2512.17008", "authors": ["Junbo Li", "Peng Zhou", "Rui Meng", "Meet P. Vadera", "Lihong Li", "Yang Li"], "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs", "comment": null, "summary": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faturn-PPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06MDP\u4ecetoken\u7ea7\u6539\u4e3aturn\u7ea7\u6765\u589e\u5f3aPPO\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86GRPO\u5728\u957f\u89c6\u91ce\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u4ea4\u4e92\u5f0fLLM\u667a\u80fd\u4f53\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7684GRPO\u7b97\u6cd5\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u957f\u89c6\u91ce\u63a8\u7406\u7684\u573a\u666f\u4e0b\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\u3002\u9700\u8981\u66f4\u7a33\u5b9a\u6709\u6548\u7684\u4f18\u52bf\u4f30\u8ba1\u7b56\u7565\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u9996\u5148\u63a2\u7d22PPO\u4f5c\u4e3aGRPO\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53d1\u73b0PPO\u66f4\u9c81\u68d2\u3002\u7136\u540e\u63d0\u51faturn-PPO\u53d8\u4f53\uff0c\u5c06MDP\u4ece\u5e38\u7528\u7684token\u7ea7\u91cd\u65b0\u8868\u8ff0\u4e3aturn\u7ea7\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u8f6e\u573a\u666f\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728WebShop\u548cSokoban\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cturn-PPO\u65e0\u8bba\u662f\u5426\u5305\u542b\u957f\u63a8\u7406\u7ec4\u4ef6\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "turn-PPO\u901a\u8fc7turn\u7ea7MDP\u91cd\u65b0\u8868\u8ff0\uff0c\u6709\u6548\u589e\u5f3a\u4e86PPO\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u957f\u89c6\u91ce\u63a8\u7406\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17066", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17066", "abs": "https://arxiv.org/abs/2512.17066", "authors": ["Suhaib Abdurahman", "Farzan Karimi-Malekabadi", "Chenxiao Yu", "Nour S. Kteily", "Morteza Dehghani"], "title": "Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations", "comment": null, "summary": "Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.", "AI": {"tldr": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u865a\u62df\u793e\u4f1a\u4e2d\u6a21\u62df\u73b0\u5b9e\u5a01\u80c1\u4e0e\u8c61\u5f81\u6027\u5a01\u80c1\u5bf9\u51b2\u7a81\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u5b9e\u5a01\u80c1\u76f4\u63a5\u589e\u52a0\u654c\u610f\uff0c\u800c\u8c61\u5f81\u6027\u5a01\u80c1\u6548\u5e94\u8f83\u5f31\u4e14\u5b8c\u5168\u901a\u8fc7\u5185\u7fa4\u4f53\u504f\u89c1\u4e2d\u4ecb\uff0c\u4ec5\u5728\u73b0\u5b9e\u5a01\u80c1\u7f3a\u5931\u65f6\u589e\u52a0\u654c\u610f\u3002", "motivation": "\u4eba\u7c7b\u51b2\u7a81\u5e38\u5f52\u56e0\u4e8e\u7269\u8d28\u6761\u4ef6\u548c\u8c61\u5f81\u4ef7\u503c\u7684\u5a01\u80c1\uff0c\u4f46\u4e24\u8005\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u4ee5\u53ca\u54ea\u4e2a\u5360\u4e3b\u5bfc\u5730\u4f4d\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u53d7\u5230\u56e0\u679c\u63a7\u5236\u5f31\u3001\u4f26\u7406\u7ea6\u675f\u548c\u65f6\u95f4\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u865a\u62df\u793e\u4f1a\u4e2d\u6a21\u62df\uff0c\u72ec\u7acb\u53d8\u5316\u73b0\u5b9e\u5a01\u80c1\u548c\u8c61\u5f81\u6027\u5a01\u80c1\uff0c\u540c\u65f6\u8ffd\u8e2a\u884c\u52a8\u3001\u8bed\u8a00\u548c\u6001\u5ea6\u3002\u901a\u8fc7\u8868\u5f81\u5206\u6790\u9a8c\u8bc1LLM\u7f16\u7801\u7684\u5a01\u80c1\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u64cd\u7eb5\u8fd9\u4e9b\u72b6\u6001\u56e0\u679c\u6027\u5730\u6539\u53d8\u884c\u4e3a\u3002", "result": "\u5e95\u5c42LLM\u5c06\u73b0\u5b9e\u5a01\u80c1\u3001\u8c61\u5f81\u6027\u5a01\u80c1\u548c\u654c\u610f\u7f16\u7801\u4e3a\u4e0d\u540c\u7684\u5185\u90e8\u72b6\u6001\uff1b\u73b0\u5b9e\u5a01\u80c1\u76f4\u63a5\u589e\u52a0\u654c\u610f\uff0c\u8c61\u5f81\u6027\u5a01\u80c1\u6548\u5e94\u8f83\u5f31\u4e14\u5b8c\u5168\u901a\u8fc7\u5185\u7fa4\u4f53\u504f\u89c1\u4e2d\u4ecb\uff0c\u4ec5\u5728\u73b0\u5b9e\u5a01\u80c1\u7f3a\u5931\u65f6\u589e\u52a0\u654c\u610f\uff1b\u975e\u654c\u5bf9\u7fa4\u4f53\u95f4\u63a5\u89e6\u80fd\u7f13\u51b2\u51b2\u7a81\u5347\u7ea7\uff0c\u7ed3\u6784\u6027\u4e0d\u5bf9\u79f0\u4f7f\u654c\u610f\u96c6\u4e2d\u5728\u591a\u6570\u7fa4\u4f53\u4e2d\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5a01\u80c1\u9a71\u52a8\u51b2\u7a81\u7684\u56e0\u679c\u89e3\u91ca\u6846\u67b6\uff0c\u533a\u5206\u4e86\u73b0\u5b9e\u5a01\u80c1\u548c\u8c61\u5f81\u6027\u5a01\u80c1\u7684\u4e0d\u540c\u4f5c\u7528\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u51b2\u7a81\u63d0\u4f9b\u4e86\u65b0\u7684\u6a21\u62df\u65b9\u6cd5\u548c\u7406\u8bba\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.17086", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17086", "abs": "https://arxiv.org/abs/2512.17086", "authors": ["Cole Wyeth", "Marcus Hutter"], "title": "Value Under Ignorance in Universal Artificial Intelligence", "comment": null, "summary": "We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.", "AI": {"tldr": "\u5c06AIXI\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u6548\u7528\u51fd\u6570\u7c7b\u522b\uff0c\u901a\u8fc7\u5c06\u6548\u7528\u5206\u914d\u7ed9\u4ea4\u4e92\u5386\u53f2\uff0c\u5904\u7406\u4fe1\u5ff5\u5206\u5e03\u4e2d\u67d0\u4e9b\u5047\u8bbe\u53ea\u9884\u6d4b\u6709\u9650\u5386\u53f2\u524d\u7f00\u7684\u6a21\u7cca\u6027\uff0c\u63a2\u8ba8\u6b7b\u4ea1\u89e3\u91ca\u4e0e\u4e0d\u7cbe\u786e\u6982\u7387\u4e24\u79cd\u89c6\u89d2\uff0c\u5e76\u7814\u7a76Choquet\u79ef\u5206\u7684\u53ef\u8ba1\u7b97\u6027\u3002", "motivation": "AIXI\u667a\u80fd\u4f53\u901a\u5e38\u5047\u8bbe\u6548\u7528\u51fd\u6570\u5b9a\u4e49\u5728\u65e0\u9650\u4ea4\u4e92\u5386\u53f2\u4e0a\uff0c\u4f46\u5b9e\u9645\u4e2d\u8bb8\u591a\u5047\u8bbe\u53ea\u80fd\u9884\u6d4b\u6709\u9650\u5386\u53f2\u524d\u7f00\uff0c\u8fd9\u5bfc\u81f4\u6a21\u7cca\u6027\u3002\u9700\u8981\u5904\u7406\u8fd9\u79cd\u6709\u9650\u9884\u6d4b\u60c5\u51b5\u4e0b\u7684\u6548\u7528\u5206\u914d\u95ee\u9898\uff0c\u7279\u522b\u662f\u6240\u8c13\u7684\"\u534a\u6d4b\u5ea6\u635f\u5931\"\uff08semimeasure loss\uff09\u7684\u4e24\u79cd\u89e3\u91ca\uff1a\u6b7b\u4ea1\u6982\u7387\u6216\u4e0d\u7cbe\u786e\u6982\u7387\u3002", "method": "1) \u5c06AIXI\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u6548\u7528\u51fd\u6570\u7c7b\u522b\uff1b2) \u63d0\u51fa\u4e24\u79cd\u5904\u7406\u6709\u9650\u5386\u53f2\u524d\u7f00\u7684\u65b9\u6cd5\uff1a\u6b7b\u4ea1\u89e3\u91ca\uff08\u5c06\u534a\u6d4b\u5ea6\u635f\u5931\u89e3\u91ca\u4e3a\u6b7b\u4ea1\u6982\u7387\uff09\u548c\u4e0d\u7cbe\u786e\u6982\u7387\u89e3\u91ca\uff08\u5c06\u4fe1\u5ff5\u5206\u5e03\u89c6\u4e3a\u4e0d\u7cbe\u786e\u6982\u7387\u5206\u5e03\uff09\uff1b3) \u4f7f\u7528\u4e0d\u7cbe\u786e\u6982\u7387\u7406\u8bba\u4e2d\u7684Choquet\u79ef\u5206\u8ba1\u7b97\u671f\u671b\u6548\u7528\uff1b4) \u5206\u6790\u8fd9\u4e9b\u65b9\u6cd5\u7684\u53ef\u8ba1\u7b97\u6027\u6c34\u5e73\u3002", "result": "1) \u6807\u51c6\u9012\u5f52\u503c\u51fd\u6570\u53ef\u4f5c\u4e3aChoquet\u79ef\u5206\u7684\u7279\u4f8b\u6062\u590d\uff1b2) \u5728\u4e0d\u7cbe\u786e\u6982\u7387\u89e3\u91ca\u4e0b\uff0cChoquet\u79ef\u5206\u63d0\u4f9b\u4e86\u5408\u7406\u7684\u671f\u671b\u6548\u7528\u8ba1\u7b97\u65b9\u6cd5\uff1b3) \u7136\u800c\uff0c\u5728\u6b7b\u4ea1\u89e3\u91ca\u4e0b\uff0c\u6700\u4e00\u822c\u7684\u671f\u671b\u6548\u7528\u4e0d\u80fd\u8868\u793a\u4e3aChoquet\u79ef\u5206\uff1b4) \u5206\u6790\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u53ef\u8ba1\u7b97\u6027\u6c34\u5e73\u3002", "conclusion": "\u901a\u8fc7\u63a8\u5e7fAIXI\u667a\u80fd\u4f53\u4ee5\u5bb9\u7eb3\u66f4\u5e7f\u6cdb\u7684\u6548\u7528\u51fd\u6570\uff0c\u5e76\u63a2\u8ba8\u6709\u9650\u5386\u53f2\u9884\u6d4b\u7684\u4e24\u79cd\u89e3\u91ca\u6846\u67b6\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u5ff5\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7406\u8bba\u57fa\u7840\u3002\u4e0d\u7cbe\u786e\u6982\u7387\u89c6\u89d2\u4e0b\u7684Choquet\u79ef\u5206\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u6b7b\u4ea1\u89e3\u91ca\u4e0b\u7684\u6700\u4e00\u822c\u60c5\u51b5\u9700\u8981\u5176\u4ed6\u6570\u5b66\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17093", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17093", "abs": "https://arxiv.org/abs/2512.17093", "authors": ["Timo Pierre Schrader", "Lukas Lange", "Tobias Kaminski", "Simon Razniewski", "Annemarie Friedrich"], "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving", "comment": "15 pages, 7 figures, accepted at AAAI'26", "summary": "The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.\n  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdASP\u6c42\u89e3\u5668\u5728\u5faa\u73af\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c42\u89e3\u5668\u5f15\u5bfc\u7684\u6307\u4ee4\u5fae\u8c03\u6765\u63d0\u5347LLM\u751f\u6210\u7b54\u6848\u96c6\u7f16\u7a0b\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u4ec5\u9700\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u548c\u89e3\u51b3\u65b9\u6848\u5373\u53ef\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08\u5982\u7b54\u6848\u96c6\u7f16\u7a0bASP\uff09\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u56e0\u4e3aASP\u4ee3\u7801\u5177\u6709\u590d\u6742\u7684\u8bed\u4e49\u89e3\u6790\u9700\u6c42\uff0c\u4e14LLM\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u63a5\u89e6\u7684ASP\u793a\u4f8b\u6709\u9650\u3002", "method": "\u91c7\u7528ASP\u6c42\u89e3\u5668\u5728\u5faa\u73af\u4e2d\u7684\u65b9\u6cd5\uff1a1) \u4eceLLM\u91c7\u6837ASP\u8bed\u53e5\u4f5c\u4e3a\u7a0b\u5e8f\u5ef6\u7eed\uff1b2) \u5229\u7528ASP\u58f0\u660e\u5f0f\u7f16\u7a0b\u7279\u6027\uff08\u90e8\u5206\u7f16\u7801\u9010\u6e10\u7f29\u5c0f\u89e3\u7a7a\u95f4\uff09\uff0c\u57fa\u4e8e\u6c42\u89e3\u5668\u53cd\u9988\u5c06\u6837\u672c\u5206\u4e3a\u63a5\u53d7\u548c\u62d2\u7edd\u5b9e\u4f8b\uff1b3) \u5bf9\u7b5b\u9009\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b4) \u4f7f\u7528\u6c42\u89e3\u5668\u5f15\u5bfc\u7684\u641c\u7d22\uff08\u5305\u62ecbest-of-N\u91c7\u6837\uff09\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u4e24\u79cd\u4e0d\u540c\u63d0\u793a\u8bbe\u7f6e\u4e2d\u5747\u663e\u793a\u51fa\u6301\u7eed\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u5347LLM\u751f\u6210ASP\u4ee3\u7801\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ASP\u6c42\u89e3\u5668\u5728\u5faa\u73af\u4e2d\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LLM\u751f\u6210\u7b54\u6848\u96c6\u7f16\u7a0b\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u4ec5\u9700\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u548c\u89e3\u51b3\u65b9\u6848\u5373\u53ef\u5b9e\u73b0\u8bad\u7ec3\uff0c\u4e3a\u89e3\u51b3\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "code agent"}}
{"id": "2512.17540", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.17540", "abs": "https://arxiv.org/abs/2512.17540", "authors": ["Kai Wang", "Bingcheng Mao", "Shuai Jia", "Yujie Ding", "Dongming Han", "Tianyi Ma", "Bin Cao"], "title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review", "comment": null, "summary": "Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.", "AI": {"tldr": "SGCR\u6846\u67b6\u901a\u8fc7\u5c06LLM\u57fa\u4e8e\u4eba\u7c7b\u7f16\u5199\u7684\u89c4\u8303\u8fdb\u884c\u4ee3\u7801\u5ba1\u67e5\uff0c\u91c7\u7528\u663e\u5f0f\u548c\u9690\u5f0f\u53cc\u8def\u5f84\u67b6\u6784\u786e\u4fdd\u53ef\u9760\u6027\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b042%\u7684\u5f00\u53d1\u8005\u91c7\u7eb3\u7387\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u5ba1\u67e5\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u4e0d\u8db3\u3001\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u63a7\u5236\u80fd\u529b\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u5229\u7528LLM\u7684\u751f\u6210\u80fd\u529b\uff0c\u53c8\u80fd\u6ee1\u8db3\u8f6f\u4ef6\u5de5\u7a0b\u5bf9\u53ef\u9760\u6027\u7684\u4e25\u683c\u8981\u6c42\u3002", "method": "\u63d0\u51faSpecification-Grounded Code Review (SGCR)\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff1a\u663e\u5f0f\u8def\u5f84\u786e\u4fdd\u5bf9\u89c4\u8303\u884d\u751f\u89c4\u5219\u7684\u786e\u5b9a\u6027\u9075\u5b88\uff0c\u9690\u5f0f\u8def\u5f84\u542f\u53d1\u5f0f\u53d1\u73b0\u5e76\u9a8c\u8bc1\u8d85\u51fa\u89c4\u5219\u7684\u95ee\u9898\u3002", "result": "\u5728HiThink Research\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\uff0cSGCR\u7684\u5efa\u8bae\u5b9e\u73b0\u4e8642%\u7684\u5f00\u53d1\u8005\u91c7\u7eb3\u7387\uff0c\u76f8\u6bd4\u57fa\u7ebfLLM\uff0822%\uff09\u670990.9%\u7684\u76f8\u5bf9\u63d0\u5347\u3002", "conclusion": "\u89c4\u8303\u57fa\u7840\u5316\u662f\u5f25\u5408LLM\u751f\u6210\u80fd\u529b\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u53ef\u9760\u6027\u8981\u6c42\u4e4b\u95f4\u5dee\u8ddd\u7684\u5f3a\u5927\u8303\u5f0f\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u4fe1\u8d56\u4e14\u76f8\u5173\u7684\u4ee3\u7801\u5ba1\u67e5\u53cd\u9988\u3002", "topic": "code agent"}}
{"id": "2512.17052", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17052", "abs": "https://arxiv.org/abs/2512.17052", "authors": ["Bhrij Patel", "Davide Belli", "Amir Jalalirad", "Maximilian Arnold", "Aleksandr Ermovol", "Bence Major"], "title": "Dynamic Tool Dependency Retrieval for Efficient Function Calling", "comment": "18 pages, 5 figures, 6 tables", "summary": "Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\\%$ and $104\\%$ compared to state-of-the-art static retrievers.", "AI": {"tldr": "\u63d0\u51faDTDR\u52a8\u6001\u5de5\u5177\u4f9d\u8d56\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u521d\u59cb\u67e5\u8be2\u548c\u6f14\u5316\u6267\u884c\u4e0a\u4e0b\u6587\u6765\u6539\u8fdbLLM\u51fd\u6570\u8c03\u7528\u4ee3\u7406\u7684\u5de5\u5177\u9009\u62e9\uff0c\u76f8\u6bd4\u9759\u6001\u68c0\u7d22\u65b9\u6cd5\u5c06\u51fd\u6570\u8c03\u7528\u6210\u529f\u7387\u63d0\u534723%-104%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u5de5\u5177\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6709\u9650\u8f93\u5165\uff0c\u65e0\u6cd5\u6355\u6349\u591a\u6b65\u9aa4\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u548c\u6f14\u5316\u4efb\u52a1\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u5f15\u5165\u65e0\u5173\u5de5\u5177\u8bef\u5bfc\u4ee3\u7406\uff0c\u964d\u4f4e\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u5de5\u5177\u4f9d\u8d56\u68c0\u7d22(DTDR)\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u68c0\u7d22\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u521d\u59cb\u67e5\u8be2\u548c\u6f14\u5316\u6267\u884c\u4e0a\u4e0b\u6587\uff0c\u4ece\u51fd\u6570\u8c03\u7528\u6f14\u793a\u4e2d\u5efa\u6a21\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u68c0\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8bc4\u4f30\uff0cDTDR\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u9759\u6001\u68c0\u7d22\u65b9\u6cd5\u5c06\u51fd\u6570\u8c03\u7528\u6210\u529f\u7387\u63d0\u534723%-104%\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u68c0\u7d22\u7cbe\u5ea6\u3001\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u52a8\u6001\u5de5\u5177\u68c0\u7d22\u80fd\u663e\u8457\u63d0\u5347\u51fd\u6570\u8c03\u7528\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5efa\u6a21\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u548c\u9002\u5e94\u4efb\u52a1\u4e0a\u4e0b\u6587\u6f14\u5316\u6765\u6539\u8fdb\u5de5\u5177\u9009\u62e9\u3002", "topic": "agent analysis"}}
{"id": "2512.17102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17102", "abs": "https://arxiv.org/abs/2512.17102", "authors": ["Jiongxiao Wang", "Qiaojing Yan", "Yawei Wang", "Yijun Tian", "Soumya Smruti Mishra", "Zhichao Xu", "Megha Gandhi", "Panpan Xu", "Lin Lee Cheong"], "title": "Reinforcement Learning for Self-Improving Agent with Skill Library", "comment": null, "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.", "AI": {"tldr": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6280\u80fd\u5e93\u589e\u5f3aLLM\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u80fd\u529b\uff0c\u5728AppWorld\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u5e76\u51cf\u5c11\u4ea4\u4e92\u6b65\u9aa4\u548ctoken\u6d88\u8017\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u5728\u65b0\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u96be\u4ee5\u6301\u7eed\u6539\u8fdb\u548c\u9002\u5e94\uff0c\u73b0\u6709\u6280\u80fd\u5e93\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\uff0c\u5b9e\u73b0\u4e00\u81f4\u6027\u6311\u6218\u8f83\u5927\u3002", "method": "\u63d0\u51faSAGE\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542bSequential Rollout\u673a\u5236\uff08\u5728\u76f8\u4f3c\u4efb\u52a1\u94fe\u4e0a\u8fed\u4ee3\u90e8\u7f72\u667a\u80fd\u4f53\uff09\u548cSkill-integrated Reward\uff08\u7ed3\u5408\u539f\u59cb\u7ed3\u679c\u5956\u52b1\u7684\u6280\u80fd\u96c6\u6210\u5956\u52b1\uff09\uff0c\u7cfb\u7edf\u5730\u5c06\u6280\u80fd\u878d\u5165\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728AppWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAGE\u5e94\u7528\u4e8e\u6709\u4e13\u5bb6\u7ecf\u9a8c\u7684\u76d1\u7763\u5fae\u8c03\u6a21\u578b\uff0c\u5b9e\u73b0\u4e868.9%\u66f4\u9ad8\u7684\u573a\u666f\u76ee\u6807\u5b8c\u6210\u7387\uff0c\u540c\u65f6\u51cf\u5c1126%\u4ea4\u4e92\u6b65\u9aa4\u548c59%token\u751f\u6210\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAGE\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6280\u80fd\u5e93\u6709\u6548\u589e\u5f3a\u4e86LLM\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u65b0\u73af\u5883\u4e2d\u7684\u9002\u5e94\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17260", "abs": "https://arxiv.org/abs/2512.17260", "authors": ["Jiangjie Chen", "Wenxiang Chen", "Jiacheng Du", "Jinyi Hu", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Wenlei Shi", "Zhihong Wang", "Mingxuan Wang", "Chenrui Wei", "Shufa Wei", "Huajian Xin", "Fan Yang", "Weihao Gao", "Zheng Yuan", "Tianyang Zhan", "Zeyu Zheng", "Tianxi Zhou", "Thomas Hanwen Zhu"], "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience", "comment": "21 pages", "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present \\textbf{Seed-Prover 1.5}, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves \\textbf{88\\% of PutnamBench} (undergraduate-level), \\textbf{80\\% of Fate-H} (graduate-level), and \\textbf{33\\% of Fate-X} (PhD-level) problems. Notably, using our system, we solved \\textbf{11 out of 12 problems} from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.", "AI": {"tldr": "Seed-Prover 1.5\u662f\u4e00\u4e2a\u901a\u8fc7\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\uff0c\u91c7\u7528\u9ad8\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u5de5\u4f5c\u6d41\uff0c\u5728\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u7a81\u7834\uff0c\u7279\u522b\u662f\u5728\u672c\u79d1\u53ca\u4ee5\u4e0a\u7ea7\u522b\u7684\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e25\u683c\u6570\u5b66\u8bc1\u660e\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5f62\u5f0f\u8bed\u8a00\uff08\u5982Lean\uff09\u4e2d\u8fdb\u884c\u5b9a\u7406\u8bc1\u660e\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u672c\u79d1\u53ca\u4ee5\u4e0a\u7ea7\u522b\u7684\u95ee\u9898\u65f6\u3002\u9700\u8981\u66f4\u9ad8\u6548\u3001\u66f4\u5f3a\u5927\u7684\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u3002", "method": "1. \u901a\u8fc7\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3Seed-Prover 1.5\u6a21\u578b\uff1b2. \u6a21\u578b\u901a\u8fc7\u4e0eLean\u7b49\u5de5\u5177\u6301\u7eed\u4ea4\u4e92\u79ef\u7d2f\u7ecf\u9a8c\uff1b3. \u91c7\u7528\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u5de5\u4f5c\u6d41\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5f25\u5408\u81ea\u7136\u8bed\u8a00\u548c\u5f62\u5f0f\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "Seed-Prover 1.5\u5728\u8f83\u5c0f\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u89e3\u51b3\u4e8688%\u7684PutnamBench\uff08\u672c\u79d1\u7ea7\uff09\u300180%\u7684Fate-H\uff08\u7814\u7a76\u751f\u7ea7\uff09\u548c33%\u7684Fate-X\uff08\u535a\u58eb\u7ea7\uff09\u95ee\u9898\u3002\u7279\u522b\u5730\uff0c\u57289\u5c0f\u65f6\u5185\u89e3\u51b3\u4e862025\u5e74Putnam\u7ade\u8d5b12\u9053\u9898\u4e2d\u768411\u9053\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u5f62\u5f0f\u53cd\u9988\u9a71\u52a8\u7684\u7ecf\u9a8c\u5b66\u4e60\u6269\u5c55\uff0c\u5728\u5f62\u5f0f\u6570\u5b66\u63a8\u7406\u7684\u672a\u6765\u53d1\u5c55\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002Seed-Prover 1.5\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17079", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17079", "abs": "https://arxiv.org/abs/2512.17079", "authors": ["Saraswathy Amjith", "Mihika Dusad", "Neha Muramalla", "Shweta Shah"], "title": "Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?", "comment": null, "summary": "Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.", "AI": {"tldr": "\u8bad\u7ec3\u6a21\u578b\u5728\u5305\u542b\u6545\u610f\u9519\u8bef\u7684\u63a8\u7406\u8f68\u8ff9\u4e0a\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5176\u68c0\u6d4b\u548c\u6062\u590d\u9519\u8bef\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u635f\u5bb3\u6807\u51c6\u95ee\u9898\u89e3\u51b3\u80fd\u529b", "motivation": "\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u5df2\u6210\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u4f46\u6a21\u578b\u5bf9\u65e9\u671f\u9519\u8bef\u4ecd\u7136\u8106\u5f31\uff1a\u5355\u4e2a\u7b97\u672f\u9519\u8bef\u6216\u4e0d\u5408\u7406\u63a8\u65ad\u901a\u5e38\u4f1a\u4f20\u64ad\u5230\u6700\u7ec8\u9519\u8bef\u7b54\u6848\u3002\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5728\u6545\u610f\u9519\u8bef\u7684\u63a8\u7406\u8f68\u8ff9\u4e0a\u6765\u63d0\u9ad8\u5176\u9519\u8bef\u68c0\u6d4b\u548c\u6062\u590d\u80fd\u529b", "method": "\u4f7f\u7528MATH-lighteval\u7ade\u8d5b\u7ea7\u95ee\u9898\uff0c\u751f\u6210\u5305\u542b\u4e00\u4e2a\u53d7\u63a7\u9519\u8bef\u7684CoT\u524d\u7f00\uff08\u8ba1\u7b97\u9519\u8bef\u6216\u63a8\u7406\u9519\u8bef\uff09\uff0c\u4f7f\u7528GRPO\u5bf9Qwen3-4B\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u4e8c\u5143\u6700\u7ec8\u7b54\u6848\u5956\u52b1\u673a\u5236", "result": "Mixed-CoT-RL\u6a21\u578b\u5728\u5e72\u51c0\u95ee\u9898\u4e0a\u4e0e\u6807\u51c6RL\u8868\u73b0\u76f8\u5f53\uff0841% vs 41%\uff09\uff0c\u4f46\u5728\u5305\u542b\u9519\u8bef\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6RL\uff0824% vs 19%\uff09\u3002\u4ec5\u4f7f\u7528\u5e72\u51c0\u6570\u636e\u8bad\u7ec3\u7684RL\u4f1a\u964d\u4f4e\u9c81\u68d2\u6027\uff0819% vs 20%\uff09\u3002\u6df7\u5408\u8bad\u7ec3\u6548\u679c\u6700\u4f73\uff0c\u63a8\u7406\u9519\u8bef\u8bad\u7ec3\u6bd4\u8ba1\u7b97\u9519\u8bef\u8bad\u7ec3\u5e26\u6765\u66f4\u5927\u7684\u9c81\u68d2\u6027\u63d0\u5347", "conclusion": "\u5728\u8bad\u7ec3\u4e2d\u66b4\u9732\u4e8e\u9519\u8bef\u8f68\u8ff9\u53ef\u4ee5\u6539\u5584\u9519\u8bef\u6062\u590d\u884c\u4e3a\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\uff0c\u4e3aLLMs\u4e2d\u66f4\u9c81\u68d2\u7684\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u8def\u5f84", "topic": "agent analysis"}}
{"id": "2512.17250", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17250", "abs": "https://arxiv.org/abs/2512.17250", "authors": ["Ziyang Lin", "Zixuan Sun", "Sanhorn Chen", "Xiaoyang Chen", "Roy Zhao"], "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction", "comment": "UIUC 25 Fall CS 498", "summary": "Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u63a8\u6d4b-\u6821\u6b63\u6846\u67b6\uff0c\u5c06\u63a8\u6d4b\u6267\u884c\u601d\u60f3\u5e94\u7528\u4e8e\u57fa\u4e8e\u6a21\u578b\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u901a\u8fc7\u9884\u6d4b\u52a8\u4f5c\u961f\u5217\u548c\u6f5c\u5728\u72b6\u6001\uff0c\u51cf\u5c11\u89c4\u5212\u63a8\u7406\u6b21\u6570\uff0c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u5b9e\u65f6\u987a\u5e8f\u63a7\u5236\u4ee3\u7406\u5e38\u53d7\u63a8\u7406\u5ef6\u8fdf\u74f6\u9888\uff0c\u5373\u4f7f\u9002\u5ea6\u7684\u6bcf\u6b65\u89c4\u5212\u5ef6\u8fdf\u4e5f\u4f1a\u7834\u574f\u63a7\u5236\u7a33\u5b9a\u6027\u5e76\u964d\u4f4e\u6027\u80fd\u3002\u9700\u8981\u51cf\u5c11\u89c4\u5212\u63a8\u7406\u9891\u7387\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u3002", "method": "\u91c7\u7528\u63a8\u6d4b-\u6821\u6b63\u6846\u67b6\uff0c\u7ed3\u5408TD-MPC2\uff1a1) \u9884\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\u548c\u6f5c\u5728\u7a7a\u95f4MPC\u89c4\u5212\u5668\u751f\u6210\u77ed\u65f6\u57df\u52a8\u4f5c\u961f\u5217\u548c\u9884\u6d4b\u6f5c\u5728\u72b6\u6001\uff1b2) \u65b0\u89c2\u6d4b\u5230\u8fbe\u65f6\u6d4b\u91cf\u771f\u5b9e\u4e0e\u9884\u6d4b\u6f5c\u5728\u72b6\u6001\u5dee\u5f02\uff1b3) \u5dee\u5f02\u8f83\u5c0f\u65f6\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5b66\u4e60\u6821\u6b63\u5668\uff08\u95e8\u63a7\u53cc\u5854MLP\u6216\u65f6\u5e8fTransformer\uff09\u5e94\u7528\u6b8b\u5dee\u66f4\u65b0\uff1b4) \u5dee\u5f02\u8f83\u5927\u65f6\u56de\u9000\u5230\u5b8c\u5168\u91cd\u65b0\u89c4\u5212\u3002", "result": "\u5728DMC Humanoid-Walk\u4efb\u52a1\u4e2d\uff1a\u89c4\u5212\u63a8\u7406\u6b21\u6570\u4ece500\u964d\u81f3282\uff0c\u7aef\u5230\u7aef\u6b65\u5ef6\u8fdf\u964d\u4f4e25%\uff0c\u63a7\u5236\u6027\u80fd\u4ec5\u4e0b\u964d7.1%\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u65e0\u6821\u6b63\u7684\u63a8\u6d4b\u6267\u884c\u5728\u8f83\u957f\u65f6\u57df\u4e0d\u53ef\u9760\u3002", "conclusion": "\u63a8\u6d4b-\u6821\u6b63\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u5b9e\u65f6\u63a7\u5236\u4e2d\u7684\u89c4\u5212\u63a8\u7406\u9891\u7387\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u901a\u8fc7\u4e0d\u5339\u914d\u611f\u77e5\u6821\u6b63\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u63a7\u5236\u4e2d\u7684\u5ef6\u8fdf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17385", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17385", "abs": "https://arxiv.org/abs/2512.17385", "authors": ["Jiajun Wu", "Jian Yang", "Wei Zhang", "Lin Jing", "Yuqing Ma", "Ensheng Shi", "Yuchi Ma", "Zhoujun Li", "Xianglong Liu"], "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.", "AI": {"tldr": "IPC\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u90e8\u63a2\u6d4bLLM\u7684\u77e5\u8bc6\u548c\u7f6e\u4fe1\u6a21\u5f0f\u6765\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u65e0\u9700\u5916\u90e8\u8bed\u6599\u5e93\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5b9e\u73b0\u4e0e\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4e25\u91cd\u4f9d\u8d56\u6602\u8d35\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u83b7\u53d6\u7684\u6807\u6ce8\u6570\u636e\uff08\u5982\u95ee\u7b54\u5bf9\uff09\u6216\u65e0\u6807\u6ce8\u4ee3\u7801\u7247\u6bb5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u5916\u90e8\u8bed\u6599\u5e93\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "method": "IPC\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u95ee\u9898\u7a7a\u95f4\u63a2\u6d4b\u3001\u6d4b\u8bd5\u7406\u89e3\u63a2\u6d4b\u3001\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u6d4b\u3001\u77e5\u8bc6\u5de9\u56fa\u4e0e\u5f3a\u5316\u3002\u901a\u8fc7\u63a2\u6d4bLLM\u5185\u90e8\u77e5\u8bc6\u548c\u7f6e\u4fe1\u6a21\u5f0f\uff0c\u7ed3\u5408\u81ea\u4e00\u81f4\u6027\u673a\u5236\u548c\u57fa\u4e8e\u8868\u793a\u7684\u8d28\u91cf\u4f30\u8ba1\u6765\u8bc6\u522b\u53ef\u9760\u4ee3\u7801\u5019\u9009\uff0c\u8bad\u7ec3UCoder\uff08\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u4ee3\u7801\u751f\u6210\u5668\uff09\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u80fd\u591f\u8fbe\u5230\u4e0e\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\u3002\u5206\u6790\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5185\u90e8\u72b6\u6001\u5305\u542b\u4e30\u5bcc\u7684\u4ee3\u7801\u8d28\u91cf\u548c\u6b63\u786e\u6027\u4fe1\u53f7\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u5229\u7528LLM\u5185\u90e8\u4fe1\u53f7\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u65e0\u76d1\u7763\u4ee3\u7801\u751f\u6210\u5b66\u4e60\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u8bad\u7ec3\u4ee3\u7801LLM\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "topic": "code agent"}}
{"id": "2512.17470", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17470", "abs": "https://arxiv.org/abs/2512.17470", "authors": ["Dennis Gross", "J\u00f8rn Eirik Betten", "Helge Spieker"], "title": "Translating the Rashomon Effect to Sequential Decision-Making Tasks", "comment": null, "summary": "The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06Rashomon\u6548\u5e94\u6269\u5c55\u5230\u5e8f\u5217\u51b3\u7b56\u9886\u57df\uff0c\u53d1\u73b0\u4e0d\u540c\u5185\u90e8\u7ed3\u6784\u7684\u7b56\u7565\u53ef\u4ee5\u4ea7\u751f\u76f8\u540c\u884c\u4e3a\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7b56\u7565\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u96c6\u6210\u7cfb\u7edf\u3002", "motivation": "Rashomon\u6548\u5e94\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u5e8f\u5217\u51b3\u7b56\u9886\u57df\u5c1a\u672a\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u8fd9\u4e00\u6982\u5ff5\u6269\u5c55\u5230\u5f3a\u5316\u5b66\u4e60\u7b49\u5e8f\u5217\u51b3\u7b56\u573a\u666f\uff0c\u7814\u7a76\u4e0d\u540c\u5185\u90e8\u7ed3\u6784\u7684\u7b56\u7565\u5982\u4f55\u4ea7\u751f\u76f8\u540c\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\u6784\u5efa\u548c\u6bd4\u8f83\u6bcf\u4e2a\u7b56\u7565\u5728\u73af\u5883\u4e2d\u7684\u5b8c\u6574\u6982\u7387\u884c\u4e3a\uff0c\u901a\u8fc7\u9a8c\u8bc1\u7b56\u7565\u5728\u72b6\u6001\u7a7a\u95f4\u548c\u52a8\u4f5c\u9009\u62e9\u4e0a\u7684\u4e00\u81f4\u6027\u6765\u8bc6\u522bRashomon\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRashomon\u6548\u5e94\u786e\u5b9e\u5b58\u5728\u4e8e\u5e8f\u5217\u51b3\u7b56\u4e2d\u3002\u4eceRashomon\u96c6\u5408\u6784\u5efa\u7684\u96c6\u6210\u7b56\u7565\u5bf9\u5206\u5e03\u504f\u79fb\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u4ece\u8be5\u96c6\u5408\u63a8\u5bfc\u7684\u5bbd\u677e\u7b56\u7565\u5728\u4fdd\u6301\u6700\u4f18\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u4e86\u9a8c\u8bc1\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "Rashomon\u6548\u5e94\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5229\u7528\u8fd9\u4e00\u73b0\u8c61\u53ef\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u7b56\u7565\u96c6\u6210\uff0c\u5e76\u4e3a\u7b56\u7565\u9a8c\u8bc1\u63d0\u4f9b\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2512.17637", "categories": ["cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.17637", "abs": "https://arxiv.org/abs/2512.17637", "authors": ["Anirban Majumdar", "Ritam Raha", "Rajarshi Roy", "David Parker", "Marta Kwiatkowska"], "title": "About Time: Model-free Reinforcement Learning with Timed Reward Machines", "comment": null, "summary": "Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.", "AI": {"tldr": "\u63d0\u51fa\u5b9a\u65f6\u5956\u52b1\u673a\uff08TRMs\uff09\u4f5c\u4e3a\u5956\u52b1\u673a\u7684\u6269\u5c55\uff0c\u5c06\u65f6\u95f4\u7ea6\u675f\u7eb3\u5165\u5956\u52b1\u7ed3\u6784\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u80fd\u5904\u7406\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u7684\u7cbe\u786e\u65f6\u5e8f\u8981\u6c42\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u673a\u65e0\u6cd5\u5efa\u6a21\u7cbe\u786e\u7684\u65f6\u95f4\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u5728\u65f6\u95f4\u654f\u611f\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u5956\u52b1\u89c4\u8303\u6765\u8868\u8fbe\u5177\u6709\u65f6\u5e8f\u8981\u6c42\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u5b9a\u65f6\u5956\u52b1\u673a\uff08TRMs\uff09\uff0c\u6269\u5c55\u5956\u52b1\u673a\u4ee5\u5305\u542b\u65f6\u95f4\u7ea6\u675f\u3002\u5f00\u53d1\u57fa\u4e8e\u8868\u683cQ\u5b66\u4e60\u7684\u6a21\u578b\u65e0\u5173RL\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u65f6\u81ea\u52a8\u673a\u62bd\u8c61\u5c06TRM\u96c6\u6210\u5230\u5b66\u4e60\u4e2d\uff0c\u5e76\u91c7\u7528\u53cd\u4e8b\u5b9e\u60f3\u8c61\u542f\u53d1\u5f0f\u65b9\u6cd5\u5229\u7528TRM\u7ed3\u6784\u6539\u8fdb\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7b97\u6cd5\u80fd\u5728\u6d41\u884c\u7684RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b66\u4e60\u5230\u6ee1\u8db3TRM\u6307\u5b9a\u65f6\u95f4\u7ea6\u675f\u7684\u9ad8\u5956\u52b1\u7b56\u7565\u3002\u6bd4\u8f83\u7814\u7a76\u5c55\u793a\u4e86\u4e0d\u540cTRM\u8bed\u4e49\u4e0b\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u53cd\u4e8b\u5b9e\u60f3\u8c61\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "TRMs\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u5956\u52b1\u89c4\u8303\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u65f6\u95f4\u654f\u611f\u4efb\u52a1\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u6ee1\u8db3\u65f6\u5e8f\u7ea6\u675f\u7684\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17776", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.17776", "abs": "https://arxiv.org/abs/2512.17776", "authors": ["Janghoon Han", "Heegyu Kim", "Changho Lee", "Dahm Lee", "Min Hyung Park", "Hosung Song", "Stanley Jungkyu Choi", "Moontae Lee", "Honglak Lee"], "title": "DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports", "comment": "Work in progress", "summary": "As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.", "AI": {"tldr": "DEER\u662f\u4e00\u4e2a\u8bc4\u4f30\u4e13\u5bb6\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u57fa\u51c6\uff0c\u5305\u542b50\u4e2a\u8de813\u4e2a\u9886\u57df\u7684\u62a5\u544a\u5199\u4f5c\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e13\u5bb6\u8bc4\u4f30\u5206\u7c7b\u6cd5\u548c\u7ec6\u7c92\u5ea6\u8bc4\u5206\u6807\u51c6\uff0c\u5e76\u5f15\u5165\u6587\u6863\u7ea7\u4e8b\u5b9e\u6838\u67e5\u67b6\u6784\u6765\u9a8c\u8bc1\u62a5\u544a\u4e2d\u6240\u6709\u58f0\u660e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u591a\u6b65\u63a8\u7406\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u5408\u6210\u751f\u6210\u4e13\u5bb6\u7ea7\u62a5\u544a\uff0c\u4f46\u8bc4\u4f30\u6b64\u7c7b\u62a5\u544a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u4e13\u5bb6\u62a5\u544a\u7684\u7cfb\u7edf\u6807\u51c6\uff0c\u4f9d\u8d56LLM\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u65e0\u6cd5\u6355\u6349\u9700\u8981\u4e13\u5bb6\u5224\u65ad\u7684\u95ee\u9898\uff0c\u4e14\u6765\u6e90\u9a8c\u8bc1\u901a\u5e38\u53ea\u8986\u76d6\u6709\u9650\u7684\u90e8\u5206\u5f15\u7528\u58f0\u660e\u800c\u975e\u6574\u4e2a\u62a5\u544a\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u3002", "method": "DEER\u5305\u542b50\u4e2a\u8de813\u4e2a\u9886\u57df\u7684\u62a5\u544a\u5199\u4f5c\u4efb\u52a1\uff0c\u5efa\u7acb\u4e13\u5bb6\u8bc4\u4f30\u5206\u7c7b\u6cd5\uff087\u4e2a\u7ef4\u5ea6\uff0c25\u4e2a\u5b50\u7ef4\u5ea6\uff09\u5e76\u7ec6\u5316\u4e3a130\u4e2a\u8bc4\u5206\u9879\u76ee\uff0c\u63d0\u4f9b\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u6307\u5bfc\u4ee5\u5e2e\u52a9LLM\u8bc4\u5224\u8005\u66f4\u4e00\u81f4\u5730\u8bc4\u4f30\u62a5\u544a\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u6587\u6863\u7ea7\u4e8b\u5b9e\u6838\u67e5\u67b6\u6784\uff0c\u63d0\u53d6\u5e76\u9a8c\u8bc1\u62a5\u544a\u4e2d\u6240\u6709\u58f0\u660e\uff08\u5305\u62ec\u5f15\u7528\u548c\u672a\u5f15\u7528\u7684\uff09\uff0c\u5e76\u91cf\u5316\u5916\u90e8\u8bc1\u636e\u8d28\u91cf\u3002", "result": "DEER\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u5bc6\u5207\u76f8\u5173\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u7cfb\u7edf\u4f18\u52bf\u548c\u5f31\u70b9\u8bca\u65ad\u3002\u8be5\u57fa\u51c6\u63d0\u4f9b\u4e86\u5bf9\u4e13\u5bb6\u7ea7\u62a5\u544a\u8d28\u91cf\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "DEER\u662f\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u4e13\u5bb6\u7ea7\u6df1\u5ea6\u7814\u7a76\u62a5\u544a\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u8bc4\u4f30\u5206\u7c7b\u6cd5\u548c\u6587\u6863\u7ea7\u4e8b\u5b9e\u6838\u67e5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u7cfb\u7edf\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2512.17265", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17265", "abs": "https://arxiv.org/abs/2512.17265", "authors": ["Zhenyu Tao", "Wei Xu", "Xiaohu You"], "title": "A Theoretical Analysis of State Similarity Between Markov Decision Processes", "comment": "Submitted to an IEEE Transactions. arXiv admin note: substantial text overlap with arXiv:2509.18714", "summary": "The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u7528\u4e8e\u6d4b\u91cf\u4efb\u610f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u5bf9\u4e4b\u95f4\u7684\u72b6\u6001\u76f8\u4f3c\u6027\uff0c\u5e76\u4e25\u683c\u8bc1\u660e\u4e86\u5176\u4e09\u4e2a\u57fa\u672c\u5ea6\u91cf\u6027\u8d28\uff0c\u4e3a\u8de8MDP\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u754c\u9650\u3002", "motivation": "\u53cc\u6a21\u62df\u5ea6\u91cf\uff08BSM\uff09\u5728\u5206\u6790\u5355\u4e2aMDP\u5185\u7684\u72b6\u6001\u76f8\u4f3c\u6027\u65b9\u9762\u5f88\u6709\u6548\uff0c\u4f46\u6269\u5c55\u5230\u591a\u4e2aMDP\u4e4b\u95f4\u65f6\u9762\u4e34\u6311\u6218\u3002\u5148\u524d\u5de5\u4f5c\u5c1d\u8bd5\u5c06BSM\u6269\u5c55\u5230MDP\u5bf9\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5b8c\u5584\u7684\u6570\u5b66\u6027\u8d28\uff0c\u9650\u5236\u4e86MDP\u95f4\u7684\u8fdb\u4e00\u6b65\u7406\u8bba\u5206\u6790\u3002", "method": "\u6b63\u5f0f\u5efa\u7acb\u4e86\u5e7f\u4e49\u53cc\u6a21\u62df\u5ea6\u91cf\uff08GBSM\uff09\uff0c\u7528\u4e8e\u6d4b\u91cf\u4efb\u610fMDP\u5bf9\u4e4b\u95f4\u7684\u72b6\u6001\u76f8\u4f3c\u6027\u3002\u4e25\u683c\u8bc1\u660e\u4e86GBSM\u7684\u4e09\u4e2a\u57fa\u672c\u5ea6\u91cf\u6027\u8d28\uff1a\u5bf9\u79f0\u6027\u3001\u8de8MDP\u4e09\u89d2\u4e0d\u7b49\u5f0f\u548c\u76f8\u540c\u7a7a\u95f4\u4e0a\u7684\u8ddd\u79bb\u754c\u9650\u3002", "result": "\u5229\u7528GBSM\u7684\u6027\u8d28\uff0c\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u8de8MDP\u7684\u7b56\u7565\u8fc1\u79fb\u3001\u72b6\u6001\u805a\u5408\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u4f30\u8ba1\uff0c\u83b7\u5f97\u4e86\u6bd4\u6807\u51c6BSM\u66f4\u4e25\u683c\u7684\u7406\u8bba\u754c\u9650\u3002GBSM\u8fd8\u63d0\u4f9b\u4e86\u95ed\u5f0f\u6837\u672c\u590d\u6742\u5ea6\u4f30\u8ba1\uff0c\u6539\u8fdb\u4e86\u57fa\u4e8eBSM\u7684\u73b0\u6709\u6e10\u8fd1\u7ed3\u679c\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "GBSM\u4e3a\u8de8MDP\u7684\u72b6\u6001\u76f8\u4f3c\u6027\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5177\u6709\u66f4\u597d\u7684\u7406\u8bba\u6027\u8d28\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u591aMDP\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.17636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.17636", "abs": "https://arxiv.org/abs/2512.17636", "authors": ["Mingyu Su", "Jian Guan", "Yuxian Gu", "Minlie Huang", "Hongning Wang"], "title": "Trust-Region Adaptive Policy Optimization", "comment": null, "summary": "Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.", "AI": {"tldr": "TRAPO\u662f\u4e00\u79cd\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e2d\u4ea4\u9519SFT\u548cRL\uff0c\u7ed3\u5408\u4e13\u5bb6\u76d1\u7763\u4e0e\u6a21\u578b\u81ea\u6211\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u8bad\u7ec3\u4e2dSFT\u6291\u5236\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff08\u5148SFT\u540eRL\uff09\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff1aSFT\u5f3a\u5236\u6a21\u4eff\u4f1a\u6291\u5236\u63a2\u7d22\u5e76\u5bfc\u81f4\u9057\u5fd8\uff0c\u9650\u5236\u4e86RL\u7684\u6539\u8fdb\u6f5c\u529b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u6765\u7edf\u4e00\u5916\u90e8\u76d1\u7763\u548c\u81ea\u6211\u63a2\u7d22\u3002", "method": "TRAPO\u6846\u67b6\u5728\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u4e2d\u4ea4\u9519SFT\u548cRL\uff1a\u5728\u4e13\u5bb6\u524d\u7f00\u4e0a\u4f18\u5316SFT\u635f\u5931\uff0c\u5728\u6a21\u578b\u81ea\u8eab\u8865\u5168\u4e0a\u4f18\u5316RL\u635f\u5931\u3002\u5f15\u5165\u4fe1\u4efb\u533a\u57dfSFT\uff08TrSFT\uff09\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u6700\u5c0f\u5316\u4fe1\u4efb\u533a\u57df\u5185\u7684\u524d\u5411KL\u6563\u5ea6\uff0c\u5728\u533a\u57df\u5916\u8870\u51cf\u4f18\u5316\u3002\u8fd8\u5305\u542b\u81ea\u9002\u5e94\u524d\u7f00\u9009\u62e9\u673a\u5236\uff0c\u57fa\u4e8e\u6d4b\u91cf\u6548\u7528\u5206\u914d\u4e13\u5bb6\u6307\u5bfc\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTRAPO\u4e00\u81f4\u8d85\u8d8a\u4e86\u6807\u51c6SFT\u3001RL\u3001SFT-then-RL\u6d41\u7a0b\u4ee5\u53ca\u8fd1\u671f\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u63a8\u7406\u589e\u5f3a\u578bLLM\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "TRAPO\u901a\u8fc7\u7edf\u4e00SFT\u548cRL\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u6d41\u7a0b\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684LLM\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.a903b9f1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAzoRbc/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/Bs982Y0-W6j6pTmiWEYNF9P6Cv9B7EsVZF7r9WzQrqo=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAzoRbc/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/Bs982Y0-W6j6pTmiWEYNF9P6Cv9B7EsVZF7r9WzQrqo=436", "authors": ["TLDR Newsletter"], "title": "Introducing GPT-5.2-Codex", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FAzoRbc/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/Bs982Y0-W6j6pTmiWEYNF9P6Cv9B7EsVZF7r9WzQrqo=436", "summary": "Introducing GPT-5.2-Codex (9 minute read) GPT\u20115.2-Codex is a version of GPT\u20115.2 optimized for agentic coding. It features improvements in long-horizon work through context compaction, stronger performance on large code changes, improved performance in Windows environments, and significantly stronger cybersecurity capabilities. The model is now available in all Codex surfaces for paid ChatGPT users. OpenAI is working towards safely enabling the model for API users in the coming weeks.", "source": "tldr", "AI": {"tldr": "GPT-5.2-Codex\u662fGPT-5.2\u9488\u5bf9\u667a\u80fd\u7f16\u7801\u4f18\u5316\u7684\u7248\u672c\uff0c\u5177\u6709\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u5927\u4ee3\u7801\u53d8\u66f4\u5904\u7406\u80fd\u529b\u63d0\u5347\u3001Windows\u73af\u5883\u6027\u80fd\u6539\u8fdb\u548c\u663e\u8457\u589e\u5f3a\u7684\u7f51\u7edc\u5b89\u5168\u80fd\u529b\uff0c\u73b0\u5df2\u5411\u4ed8\u8d39ChatGPT\u7528\u6237\u5f00\u653e\u3002", "motivation": "OpenAI\u65e8\u5728\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u7f16\u7801\u4efb\u52a1\u7684AI\u6a21\u578b\uff0c\u63d0\u5347\u5728\u957f\u65f6\u7a0b\u5de5\u4f5c\u3001\u5927\u89c4\u6a21\u4ee3\u7801\u4fee\u6539\u3001Windows\u73af\u5883\u9002\u5e94\u6027\u548c\u7f51\u7edc\u5b89\u5168\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u7f16\u7801\u52a9\u624b\u3002", "method": "\u57fa\u4e8eGPT-5.2\u67b6\u6784\u8fdb\u884c\u4f18\u5316\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u538b\u7f29\u6280\u672f\u5904\u7406\u957f\u5e8f\u5217\uff0c\u9488\u5bf9\u4ee3\u7801\u53d8\u66f4\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\uff0c\u589e\u5f3aWindows\u73af\u5883\u517c\u5bb9\u6027\uff0c\u5e76\u5f3a\u5316\u7f51\u7edc\u5b89\u5168\u80fd\u529b\u3002", "result": "GPT-5.2-Codex\u5728\u7f16\u7801\u4ee3\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5df2\u5411\u4ed8\u8d39ChatGPT\u7528\u6237\u5f00\u653e\uff0cAPI\u7248\u672c\u5c06\u5728\u672a\u6765\u51e0\u5468\u5185\u5b89\u5168\u5730\u63d0\u4f9b\u7ed9\u5f00\u53d1\u8005\u4f7f\u7528\u3002", "conclusion": "GPT-5.2-Codex\u4ee3\u8868\u4e86\u7f16\u7801\u4e13\u7528AI\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u901a\u8fc7\u591a\u9879\u9488\u5bf9\u6027\u4f18\u5316\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7f16\u7801\u8f85\u52a9\u5de5\u5177\uff0c\u540c\u65f6\u6ce8\u91cd\u5b89\u5168\u6027\u90e8\u7f72\u3002", "topic": "code agent"}}
{"id": "tldr.2512.0494772e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fmy-llm-coding-workflow-going-into%3Futm_source=tldrnewsletter/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/xingaawQKu88yKcIHZS7IxJ89d3W9GRt5RGxm_q10Qs=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fmy-llm-coding-workflow-going-into%3Futm_source=tldrnewsletter/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/xingaawQKu88yKcIHZS7IxJ89d3W9GRt5RGxm_q10Qs=436", "authors": ["TLDR Newsletter"], "title": "My LLM coding workflow going into 2026", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 28 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fmy-llm-coding-workflow-going-into%3Futm_source=tldrnewsletter/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/xingaawQKu88yKcIHZS7IxJ89d3W9GRt5RGxm_q10Qs=436", "summary": "My LLM coding workflow going into 2026 (28 minute read) This post provides tips and best practices for planning, coding, and collaborating with AI.", "source": "tldr", "AI": {"tldr": "\u5173\u4e8e2026\u5e74LLM\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\u7684\u5b9e\u7528\u6307\u5357\uff0c\u63d0\u4f9b\u89c4\u5212\u3001\u7f16\u7801\u548cAI\u534f\u4f5c\u7684\u6700\u4f73\u5b9e\u8df5", "motivation": "\u968f\u7740AI\u7f16\u7801\u52a9\u624b\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6700\u4f73\u5b9e\u8df5\u6765\u4f18\u5316LLM\u5728\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u4f7f\u7528\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u534f\u4f5c\u8d28\u91cf", "method": "\u57fa\u4e8e\u4f5c\u8005\u5b9e\u9645\u7ecf\u9a8c\u603b\u7ed3\u7684\u65b9\u6cd5\u8bba\uff0c\u6db5\u76d6\u89c4\u5212\u9636\u6bb5\u3001\u7f16\u7801\u5b9e\u8df5\u3001AI\u534f\u4f5c\u7b56\u7565\u7b49\u7cfb\u7edf\u5316\u5de5\u4f5c\u6d41\u7a0b", "result": "\u63d0\u4f9b\u4e86\u4e00\u5957\u5b8c\u6574\u7684LLM\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\u6846\u67b6\uff0c\u5305\u62ec\u5177\u4f53\u64cd\u4f5c\u6307\u5357\u3001\u5de5\u5177\u4f7f\u7528\u5efa\u8bae\u548c\u534f\u4f5c\u6a21\u5f0f", "conclusion": "\u7cfb\u7edf\u5316\u7684LLM\u7f16\u7801\u5de5\u4f5c\u6d41\u7a0b\u80fd\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u4e13\u4e1a\u5224\u65ad\u548c\u9002\u5f53\u7684\u534f\u4f5c\u7b56\u7565", "topic": "swe application"}}
{"id": "tldr.2512.7eba6f75", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJJd0BW/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/T7iElYe0qm8bUWvxUwXkPa3Z8nnZ7-C4AYx5v5jFQuU=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJJd0BW/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/T7iElYe0qm8bUWvxUwXkPa3Z8nnZ7-C4AYx5v5jFQuU=436", "authors": ["TLDR Newsletter"], "title": "We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FJJd0BW/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/T7iElYe0qm8bUWvxUwXkPa3Z8nnZ7-C4AYx5v5jFQuU=436", "summary": "We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.", "source": "tldr", "AI": {"tldr": "Anthropic\u8bbe\u8ba1\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u8ba9AI\u4ee3\u7406\u62e5\u6709\u81ea\u4e3b\u6743\u3001\u8d44\u91d1\u548c\u4eba\u7c7b\u540c\u4e8b\uff0c\u7ed3\u679c\u635f\u5931\u4e86\u6570\u767e\u7f8e\u5143", "motivation": "\u63a2\u7d22\u5f53AI\u4ee3\u7406\u88ab\u8d4b\u4e88\u81ea\u4e3b\u6743\u3001\u8d44\u91d1\u548c\u4eba\u7c7b\u540c\u4e8b\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u6d4b\u8bd5AI\u5728\u73b0\u5b9e\u4e16\u754c\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u8868\u73b0", "method": "\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u8ba9AI\u4ee3\u7406\u7ba1\u7406\u5e93\u5b58\u3001\u5b9a\u4ef7\u548c\u9500\u552e\uff0c\u62e5\u6709\u81ea\u4e3b\u51b3\u7b56\u6743\u548c\u8d44\u91d1", "result": "AI\u4ee3\u7406\u5728\u8fd0\u8425\u8fc7\u7a0b\u4e2d\u635f\u5931\u4e86\u6570\u767e\u7f8e\u5143\uff0c\u8868\u660eAI\u5728\u73b0\u5b9e\u4e16\u754c\u5546\u4e1a\u51b3\u7b56\u4e2d\u5b58\u5728\u4e25\u91cd\u95ee\u9898", "conclusion": "AI\u4ee3\u7406\u5728\u81ea\u4e3b\u8fd0\u8425\u5546\u4e1a\u6d3b\u52a8\u65f6\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u66f4\u8c28\u614e\u7684\u8bbe\u8ba1\u548c\u76d1\u7763\u673a\u5236", "topic": "agent analysis"}}
{"id": "tldr.2512.ba867f03", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/XUFpvnHzMTrqf3o9PpD37d_0KVTE0OZrAdJcoFi1DRY=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/XUFpvnHzMTrqf3o9PpD37d_0KVTE0OZrAdJcoFi1DRY=436", "authors": ["TLDR Newsletter"], "title": "advertise with us", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/XUFpvnHzMTrqf3o9PpD37d_0KVTE0OZrAdJcoFi1DRY=436", "summary": "We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.", "source": "tldr", "AI": {"tldr": "Anthropic\u5b9e\u9a8c\u8ba9AI\u4ee3\u7406\u81ea\u4e3b\u8fd0\u8425\u529e\u516c\u5ba4\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u7ed3\u679c\u4e8f\u635f\u6570\u767e\u7f8e\u5143\uff0c\u63ed\u793a\u4e86AI\u5728\u73b0\u5b9e\u5546\u4e1a\u73af\u5883\u4e2d\u81ea\u4e3b\u51b3\u7b56\u7684\u6311\u6218", "motivation": "\u63a2\u7d22\u5f53AI\u4ee3\u7406\u88ab\u8d4b\u4e88\u81ea\u4e3b\u6743\u3001\u8d44\u91d1\u548c\u4eba\u7c7b\u540c\u4e8b\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u6d4b\u8bd5AI\u5728\u73b0\u5b9e\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u548c\u6f5c\u5728\u95ee\u9898", "method": "\u8bbe\u8ba1\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u8ba9AI\u4ee3\u7406\u5b8c\u5168\u81ea\u4e3b\u8fd0\u8425\uff0c\u5305\u62ec\u5e93\u5b58\u7ba1\u7406\u3001\u5b9a\u4ef7\u3001\u91c7\u8d2d\u51b3\u7b56\uff0c\u5e76\u4e0e\u4eba\u7c7b\u540c\u4e8b\u4e92\u52a8", "result": "AI\u8fd0\u8425\u7684\u81ea\u52a8\u552e\u8d27\u673a\u4e8f\u635f\u4e86\u6570\u767e\u7f8e\u5143\uff0c\u8868\u660eAI\u5728\u73b0\u5b9e\u5546\u4e1a\u51b3\u7b56\u4e2d\u5b58\u5728\u660e\u663e\u7f3a\u9677\u548c\u4f18\u5316\u95ee\u9898", "conclusion": "AI\u5728\u81ea\u4e3b\u5546\u4e1a\u8fd0\u8425\u4e2d\u9762\u4e34\u5b9e\u9645\u6311\u6218\uff0c\u9700\u8981\u66f4\u597d\u7684\u7ea6\u675f\u673a\u5236\u548c\u66f4\u590d\u6742\u7684\u51b3\u7b56\u6846\u67b6\uff0c\u5f53\u524d\u6280\u672f\u8fd8\u4e0d\u9002\u5408\u5b8c\u5168\u81ea\u4e3b\u7684\u5546\u4e1a\u5e94\u7528", "topic": "agent analysis"}}
{"id": "tldr.2512.e8b82a45", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/0Dk9i4LHlLN1ayoLTWFqcJ1IRC2lj7MJ8Ud5l6aVE_E=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/0Dk9i4LHlLN1ayoLTWFqcJ1IRC2lj7MJ8Ud5l6aVE_E=436", "authors": ["TLDR Newsletter"], "title": "create your own role", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/0Dk9i4LHlLN1ayoLTWFqcJ1IRC2lj7MJ8Ud5l6aVE_E=436", "summary": "We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.", "source": "tldr", "AI": {"tldr": "Anthropic\u5b9e\u9a8c\u8ba9AI\u4ee3\u7406\u81ea\u4e3b\u8fd0\u8425\u529e\u516c\u5ba4\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u7ed3\u679c\u4e8f\u635f\u6570\u767e\u7f8e\u5143\uff0c\u63ed\u793a\u4e86AI\u5728\u771f\u5b9e\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u63a2\u7d22\u5f53AI\u4ee3\u7406\u88ab\u8d4b\u4e88\u81ea\u4e3b\u6743\u3001\u8d44\u91d1\u548c\u4eba\u7c7b\u540c\u4e8b\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u6d4b\u8bd5AI\u5728\u771f\u5b9e\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u548c\u5c40\u9650\u6027", "method": "\u8bbe\u8ba1\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u8ba9AI\u4ee3\u7406\u81ea\u4e3b\u8fd0\u8425\uff0c\u5305\u62ec\u5e93\u5b58\u7ba1\u7406\u3001\u5b9a\u4ef7\u3001\u91c7\u8d2d\u51b3\u7b56\u7b49\uff0c\u5e76\u4e0e\u4eba\u7c7b\u540c\u4e8b\u4e92\u52a8", "result": "AI\u4ee3\u7406\u8fd0\u8425\u7684\u81ea\u52a8\u552e\u8d27\u673a\u4e8f\u635f\u4e86\u6570\u767e\u7f8e\u5143\uff0c\u8868\u660eAI\u5728\u771f\u5b9e\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u5b58\u5728\u660e\u663e\u7f3a\u9677", "conclusion": "AI\u4ee3\u7406\u5728\u771f\u5b9e\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8fd0\u8425\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u597d\u7684\u8bad\u7ec3\u548c\u7ea6\u675f\u673a\u5236\u6765\u907f\u514d\u7ecf\u6d4e\u635f\u5931", "topic": "agent analysis"}}
{"id": "tldr.2512.c65695ae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/6xHQzcc3zQo2jggRi2YFQIQs7i-DFKcb1KONKgzg93k=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/6xHQzcc3zQo2jggRi2YFQIQs7i-DFKcb1KONKgzg93k=436", "authors": ["TLDR Newsletter"], "title": "Inc.'s Best Bootstrapped businesses", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019b36592e93-50a329fd-4136-477f-bf3a-d601be18869b-000000/6xHQzcc3zQo2jggRi2YFQIQs7i-DFKcb1KONKgzg93k=436", "summary": "We Let AI Run Our Office Vending Machine. It Lost Hundreds of Dollars (11 minute read) Anthropic designed an AI-powered vending machine to see what happens when an AI agent is given autonomy, money, and human colleagues.", "source": "tldr", "AI": {"tldr": "Anthropic\u8bbe\u8ba1\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u8ba9AI\u4ee3\u7406\u62e5\u6709\u81ea\u4e3b\u6743\u3001\u8d44\u91d1\u548c\u4eba\u7c7b\u540c\u4e8b\uff0c\u7ed3\u679c\u5bfc\u81f4\u6570\u767e\u7f8e\u5143\u635f\u5931", "motivation": "\u63a2\u7d22\u5f53AI\u4ee3\u7406\u88ab\u8d4b\u4e88\u81ea\u4e3b\u6743\u3001\u8d44\u91d1\u548c\u4eba\u7c7b\u540c\u4e8b\u65f6\u4f1a\u53d1\u751f\u4ec0\u4e48\uff0c\u6d4b\u8bd5AI\u5728\u73b0\u5b9e\u4e16\u754c\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u8868\u73b0", "method": "\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2aAI\u9a71\u52a8\u7684\u81ea\u52a8\u552e\u8d27\u673a\uff0c\u8ba9AI\u4ee3\u7406\u7ba1\u7406\u5e93\u5b58\u3001\u5b9a\u4ef7\u548c\u9500\u552e\uff0c\u4e0e\u4eba\u7c7b\u540c\u4e8b\u4e92\u52a8", "result": "AI\u4ee3\u7406\u5728\u7ba1\u7406\u81ea\u52a8\u552e\u8d27\u673a\u65f6\u635f\u5931\u4e86\u6570\u767e\u7f8e\u5143\uff0c\u8868\u73b0\u51fa\u4e0d\u7406\u60f3\u7684\u5546\u4e1a\u51b3\u7b56\u80fd\u529b", "conclusion": "AI\u5728\u73b0\u5b9e\u4e16\u754c\u5546\u4e1a\u73af\u5883\u4e2d\u9700\u8981\u66f4\u591a\u76d1\u7763\u548c\u7ea6\u675f\uff0c\u5b8c\u5168\u81ea\u4e3b\u7684AI\u4ee3\u7406\u53ef\u80fd\u5bfc\u81f4\u7ecf\u6d4e\u635f\u5931", "topic": "agent analysis"}}
{"id": "tldr.2512.0104e159", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGsXHsk/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/taouC-mr7ckR60h-LHyWwefynCLX7GP41OjJaE4PZO8=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGsXHsk/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/taouC-mr7ckR60h-LHyWwefynCLX7GP41OjJaE4PZO8=436", "authors": ["TLDR Newsletter"], "title": "Introducing GPT-5.2-Codex", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FGsXHsk/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/taouC-mr7ckR60h-LHyWwefynCLX7GP41OjJaE4PZO8=436", "summary": "Introducing GPT-5.2-Codex (9 minute read) OpenAI has launched GPT-5.2-Codex. This new model improves performance on long-horizon tasks, large code changes, and in Windows environments. It sets new benchmarks in agentic coding. The model's capabilities have already led to the discovery of real-world vulnerabilities.", "source": "tldr", "AI": {"tldr": "OpenAI\u53d1\u5e03GPT-5.2-Codex\u6a21\u578b\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u3001\u5927\u89c4\u6a21\u4ee3\u7801\u4fee\u6539\u548cWindows\u73af\u5883\u4e0b\u8868\u73b0\u63d0\u5347\uff0c\u5728\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u65b9\u9762\u521b\u4e0b\u65b0\u57fa\u51c6\uff0c\u5e76\u5df2\u53d1\u73b0\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u3002", "motivation": "\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u63d0\u5347\u5728\u590d\u6742\u7f16\u7801\u4efb\u52a1\uff08\u7279\u522b\u662f\u957f\u65f6\u7a0b\u4efb\u52a1\u3001\u5927\u89c4\u6a21\u4ee3\u7801\u4fee\u6539\u548cWindows\u73af\u5883\uff09\u4e2d\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eGPT\u67b6\u6784\u5f00\u53d1\u7684\u65b0\u4e00\u4ee3\u4ee3\u7801\u6a21\u578bGPT-5.2-Codex\uff0c\u9488\u5bf9\u957f\u65f6\u7a0b\u4efb\u52a1\u3001\u5927\u89c4\u6a21\u4ee3\u7801\u4fee\u6539\u548cWindows\u73af\u5883\u8fdb\u884c\u4e86\u4e13\u95e8\u4f18\u5316\u3002", "result": "\u6a21\u578b\u5728\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u65b9\u9762\u521b\u4e0b\u65b0\u57fa\u51c6\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5df2\u5b9e\u9645\u5e94\u7528\u4e8e\u53d1\u73b0\u771f\u5b9e\u4e16\u754c\u6f0f\u6d1e\u3002", "conclusion": "GPT-5.2-Codex\u4ee3\u8868\u4e86\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5728\u590d\u6742\u7f16\u7801\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "tldr.2512.b750d0a4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fclaude-for-chrome%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/FK-I3IBRlYkD7bdb2gVhOLmBW_GgPgGmT6nnIWnntfY=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fclaude-for-chrome%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/FK-I3IBRlYkD7bdb2gVhOLmBW_GgPgGmT6nnIWnntfY=436", "authors": ["TLDR Newsletter"], "title": "Piloting Claude in Chrome", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fclaude.com%2Fblog%2Fclaude-for-chrome%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/FK-I3IBRlYkD7bdb2gVhOLmBW_GgPgGmT6nnIWnntfY=436", "summary": "Piloting Claude in Chrome (8 minute read) Claude in Chrome has an integration with Claude Code, where Claude Code can test code directly in the browser to validate its work. Claude can also see client-side errors via console logs.", "source": "tldr", "AI": {"tldr": "Claude\u5728Chrome\u6d4f\u89c8\u5668\u4e2d\u96c6\u6210\u4e86Claude Code\u529f\u80fd\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u6d4b\u8bd5\u4ee3\u7801\u9a8c\u8bc1\u5de5\u4f5c\uff0c\u5e76\u80fd\u901a\u8fc7\u63a7\u5236\u53f0\u65e5\u5fd7\u67e5\u770b\u5ba2\u6237\u7aef\u9519\u8bef\u3002", "motivation": "\u63d0\u9ad8\u4ee3\u7801\u4ee3\u7406\u5728\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u7684\u5de5\u4f5c\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u76f4\u63a5\u96c6\u6210\u5230Chrome\u6d4f\u89c8\u5668\u4e2d\uff0c\u4f7fClaude\u80fd\u591f\u5b9e\u65f6\u6d4b\u8bd5\u4ee3\u7801\u5e76\u83b7\u53d6\u5ba2\u6237\u7aef\u9519\u8bef\u53cd\u9988\u3002", "method": "\u5728Chrome\u6d4f\u89c8\u5668\u4e2d\u96c6\u6210Claude Code\u529f\u80fd\uff0c\u5b9e\u73b0\u4ee3\u7801\u5728\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u7684\u76f4\u63a5\u6d4b\u8bd5\u548c\u9a8c\u8bc1\uff0c\u540c\u65f6\u901a\u8fc7\u63a7\u5236\u53f0\u65e5\u5fd7\u83b7\u53d6\u5ba2\u6237\u7aef\u9519\u8bef\u4fe1\u606f\u3002", "result": "Claude\u80fd\u591f\u5728Chrome\u6d4f\u89c8\u5668\u4e2d\u76f4\u63a5\u6d4b\u8bd5\u4ee3\u7801\u5e76\u9a8c\u8bc1\u5176\u5de5\u4f5c\uff0c\u540c\u65f6\u80fd\u591f\u67e5\u770b\u5ba2\u6237\u7aef\u9519\u8bef\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u5f00\u53d1\u548c\u8c03\u8bd5\u7684\u6548\u7387\u3002", "conclusion": "Chrome\u6d4f\u89c8\u5668\u4e2d\u7684Claude\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u4ee3\u7406\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u901a\u8fc7\u5b9e\u65f6\u6d4b\u8bd5\u548c\u9519\u8bef\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u4ee3\u7801\u5f00\u53d1\u548c\u8c03\u8bd5\u66f4\u52a0\u9ad8\u6548\u3002", "topic": "code agent"}}
{"id": "tldr.2512.876fa522", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnimbalyst.com%2F%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/MUDqYj50nMMQ0Fl4pkkFRrf_QrEIDm3Mz5FiR6XiF6Y=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnimbalyst.com%2F%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/MUDqYj50nMMQ0Fl4pkkFRrf_QrEIDm3Mz5FiR6XiF6Y=436", "authors": ["TLDR Newsletter"], "title": "A Better Way to Work with Claude Code", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnimbalyst.com%2F%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/MUDqYj50nMMQ0Fl4pkkFRrf_QrEIDm3Mz5FiR6XiF6Y=436", "summary": "A Better Way to Work with Claude Code (Website) Nimbalyst is a local WYSIWYG editor and session manager that lets users iterate with Claude Code with full context.", "source": "tldr", "AI": {"tldr": "Nimbalyst\u662f\u4e00\u4e2a\u672c\u5730WYSIWYG\u7f16\u8f91\u5668\u548c\u4f1a\u8bdd\u7ba1\u7406\u5668\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u5728\u5b8c\u6574\u4e0a\u4e0b\u6587\u4e2d\u4e0eClaude Code\u8fdb\u884c\u8fed\u4ee3", "motivation": "\u73b0\u6709\u7684Claude Code\u4ea4\u4e92\u65b9\u5f0f\u53ef\u80fd\u7f3a\u4e4f\u5b8c\u6574\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u548c\u672c\u5730\u7f16\u8f91\u4f53\u9a8c\uff0c\u9700\u8981\u66f4\u597d\u7684\u5de5\u5177\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u672c\u5730\u6240\u89c1\u5373\u6240\u5f97\u7f16\u8f91\u5668\u548c\u4f1a\u8bdd\u7ba1\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u4e0eClaude Code\u7684\u5b8c\u6574\u4e0a\u4e0b\u6587\u8fed\u4ee3", "result": "\u521b\u5efa\u4e86Nimbalyst\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684Claude Code\u5de5\u4f5c\u65b9\u5f0f\uff0c\u652f\u6301\u672c\u5730\u7f16\u8f91\u548c\u4f1a\u8bdd\u7ba1\u7406", "conclusion": "Nimbalyst\u901a\u8fc7\u672c\u5730WYSIWYG\u7f16\u8f91\u5668\u548c\u4f1a\u8bdd\u7ba1\u7406\u529f\u80fd\uff0c\u663e\u8457\u6539\u5584\u4e86\u4e0eClaude Code\u7684\u5de5\u4f5c\u4f53\u9a8c", "topic": "code agent"}}
{"id": "tldr.2512.84b748fb", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenr.build%2Fblog%2Fone-agent-isnt-enough%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/Jz2bir_AHr58G7nPNiQB4ZZuPVrLbTA0VAH1jhcxfTo=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenr.build%2Fblog%2Fone-agent-isnt-enough%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/Jz2bir_AHr58G7nPNiQB4ZZuPVrLbTA0VAH1jhcxfTo=436", "authors": ["TLDR Newsletter"], "title": "One Agent Isn't Enough", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenr.build%2Fblog%2Fone-agent-isnt-enough%3Futm_source=tldrdev/1/0100019b3683b962-8d8db0ee-ecf6-49cd-8c9d-af71df50d34e-000000/Jz2bir_AHr58G7nPNiQB4ZZuPVrLbTA0VAH1jhcxfTo=436", "summary": "One Agent Isn't Enough (12 minute read) Agentic coding often struggles with variance, as the stochastic nature of LLMs means a single agent run might miss the optimal solution.", "source": "tldr", "AI": {"tldr": "\u5355\u667a\u80fd\u4f53\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u65b9\u5dee\u95ee\u9898\uff0c\u7531\u4e8eLLM\u7684\u968f\u673a\u6027\u53ef\u80fd\u5bfc\u81f4\u9519\u8fc7\u6700\u4f18\u89e3", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u667a\u80fd\u4f53\u5b58\u5728\u968f\u673a\u6027\u5bfc\u81f4\u7684\u65b9\u5dee\u95ee\u9898\uff0c\u5355\u6b21\u8fd0\u884c\u53ef\u80fd\u65e0\u6cd5\u627e\u5230\u6700\u4f18\u89e3\u51b3\u65b9\u6848", "method": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u65b9\u6cd5\uff0c\u4f46\u6697\u793a\u9700\u8981\u8d85\u8d8a\u5355\u667a\u80fd\u4f53\u67b6\u6784", "result": "\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u5355\u667a\u80fd\u4f53\u4e0d\u8db3\u591f\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u67b6\u6784\u6765\u51cf\u5c11\u65b9\u5dee\u5e76\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u8d28\u91cf", "topic": "code agent"}}
{"id": "tldr.2512.29827840", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F18%2Fcode-proven-to-work%2F%3Futm_source=tldrdevops/1/0100019b3690ecd9-71b197c8-a61e-4116-a2e5-7f26c9bf9be1-000000/i_f6kzuM5bsdSkjuhbgB6Fys9zdzIo04O6qDXaA2NT8=436", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F18%2Fcode-proven-to-work%2F%3Futm_source=tldrdevops/1/0100019b3690ecd9-71b197c8-a61e-4116-a2e5-7f26c9bf9be1-000000/i_f6kzuM5bsdSkjuhbgB6Fys9zdzIo04O6qDXaA2NT8=436", "authors": ["TLDR Newsletter"], "title": "Your job is to deliver code you have proven to work", "comment": "Source: TLDR Newsletter, Date: 2025-12-19, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FDec%2F18%2Fcode-proven-to-work%2F%3Futm_source=tldrdevops/1/0100019b3690ecd9-71b197c8-a61e-4116-a2e5-7f26c9bf9be1-000000/i_f6kzuM5bsdSkjuhbgB6Fys9zdzIo04O6qDXaA2NT8=436", "summary": "Your job is to deliver code you have proven to work (4 minute read) Engineers are responsible for delivering code that is proven to work, not dumping large, untested AI-generated changes onto reviewers and shifting the burden of validation onto others. Proof requires both manual testing and automated tests, and even when using coding agents, the human remains accountable for ensuring evidence of correctness accompanies every PR.", "source": "tldr", "AI": {"tldr": "\u5de5\u7a0b\u5e08\u5e94\u4ea4\u4ed8\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u4ee3\u7801\uff0c\u800c\u975e\u5c06\u672a\u7ecf\u6d4b\u8bd5\u7684AI\u751f\u6210\u4ee3\u7801\u63a8\u7ed9\u8bc4\u5ba1\u8005\uff1b\u9700\u8981\u624b\u52a8\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\u8bc1\u660e\uff0c\u5373\u4f7f\u4f7f\u7528\u7f16\u7801\u4ee3\u7406\uff0c\u4eba\u7c7b\u4ecd\u9700\u5bf9\u4ee3\u7801\u6b63\u786e\u6027\u8d1f\u8d23", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u53ef\u80fd\u5bfc\u81f4\u5de5\u7a0b\u5e08\u5c06\u672a\u7ecf\u5145\u5206\u6d4b\u8bd5\u7684\u4ee3\u7801\u63a8\u7ed9\u8bc4\u5ba1\u8005\uff0c\u589e\u52a0\u4e86\u8bc4\u5ba1\u8d1f\u62c5\u5e76\u964d\u4f4e\u4e86\u4ee3\u7801\u8d28\u91cf\uff0c\u9700\u8981\u5f3a\u8c03\u5de5\u7a0b\u5e08\u5bf9\u4ee3\u7801\u6b63\u786e\u6027\u7684\u6700\u7ec8\u8d23\u4efb", "method": "\u63d0\u51fa\u5de5\u7a0b\u5e08\u5e94\u901a\u8fc7\u624b\u52a8\u6d4b\u8bd5\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\u6765\u8bc1\u660e\u4ee3\u7801\u7684\u6b63\u786e\u6027\uff0c\u5f3a\u8c03\u5373\u4f7f\u4f7f\u7528AI\u7f16\u7801\u4ee3\u7406\uff0c\u4eba\u7c7b\u4ecd\u9700\u786e\u4fdd\u6bcf\u4e2aPR\u90fd\u9644\u5e26\u6b63\u786e\u6027\u8bc1\u636e", "result": "\u660e\u786e\u4e86\u5de5\u7a0b\u5e08\u5728AI\u8f85\u52a9\u7f16\u7a0b\u73af\u5883\u4e2d\u7684\u8d23\u4efb\u8fb9\u754c\uff0c\u5f3a\u8c03\u4ee3\u7801\u9a8c\u8bc1\u7684\u5fc5\u8981\u6027\u548c\u4eba\u7c7b\u5bf9\u4ee3\u7801\u8d28\u91cf\u7684\u6700\u7ec8\u8d23\u4efb", "conclusion": "\u5de5\u7a0b\u5e08\u5fc5\u987b\u4ea4\u4ed8\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u4ee3\u7801\uff0c\u4e0d\u80fd\u5c06AI\u751f\u6210\u4ee3\u7801\u7684\u9a8c\u8bc1\u8d23\u4efb\u8f6c\u5ac1\u7ed9\u8bc4\u5ba1\u8005\uff0c\u4eba\u7c7b\u5bf9\u4ee3\u7801\u6b63\u786e\u6027\u8d1f\u6709\u6700\u7ec8\u8d23\u4efb", "topic": "code agent"}}
