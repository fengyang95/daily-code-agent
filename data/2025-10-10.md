<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [wechat.article](#wechat.article) [Total: 14]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.LG](#cs.LG) [Total: 20]
- [cs.AI](#cs.AI) [Total: 24]
- [tldr.article](#tldr.article) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

TL;DR: HaystackCraft是一个新的长上下文基准测试，通过构建包含异构检索策略和智能体工作流程噪声的真实长文本来评估LLM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的'大海捞针'基准测试忽略了现实世界中由有偏检索和智能体工作流程产生的噪声上下文，需要构建更真实的测试环境。

Method: 基于英文维基百科超链接网络构建HaystackCraft基准，包含多跳问题，评估异构检索策略对干扰项组成和LLM性能的影响，并扩展到动态智能体操作场景。

Result: 实验显示：1)更强的密集检索器会引入更具挑战性的干扰项，但基于图的重新排序能同时提高检索效果和减轻有害干扰；2)在智能体测试中，即使是先进模型也会因自生成干扰项而出现级联失败或难以提前停止。

Conclusion: 智能体长上下文推理仍面临持续挑战，HaystackCraft为未来进展提供了有价值的测试平台。

Abstract: Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [2] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

TL;DR: AsyncSpade是一个异步框架，通过预测查询状态和分离KV缓存过滤与解码循环，解决了测试时扩展中KV缓存线性增长导致的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展通过长思维链提升LLM推理能力，但KV缓存的线性增长加剧了内存瓶颈。现有的查询感知稀疏解码方法受限于顺序依赖和粗粒度token选择，在高并发和长思维链场景下效率低下。

Method: 提出AsyncSpade框架，包含：(1)轻量级时序回归模块预测下一个token的查询状态；(2)异步解耦框架，将KV缓存过滤与自回归解码循环分离，通过异步机制重叠KV选择与前向推理计算。

Result: 在A100节点上，AsyncSpade完全重叠KV缓存操作与推理流水线，实现理论最优时间每输出token。相比SoTA基线Quest减少20%以上TPOT，相比全注意力减少至少50%TPOT，同时在多个TTS基准测试中保持或超越准确性。

Conclusion: AsyncSpade首次在不牺牲模型性能的情况下消除了顺序依赖，为高效测试时扩展提供了有效解决方案。

Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [3] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 提出一个多智能体框架来研究团队科学的核心方面：结构、多样性和互动动态，发现在常识和社交推理任务中，扁平团队表现优于层级团队，多样性影响复杂，智能体对团队表现过度自信但反思显示协作价值。


<details>
  <summary>Details</summary>
Motivation: 虽然基于大语言模型的多智能体系统受到关注，但对其团队动态的研究较少，受人类团队科学启发，旨在探索智能体团队的核心特征。

Method: 提出多智能体框架，评估团队在常识问答、策略问答、社交推理和潜在隐性仇恨四个任务上的表现，分析结构、多样性和互动动态。

Result: 扁平团队表现优于层级团队，多样性影响复杂，智能体对团队表现过度自信，反思显示协作价值但存在整合挑战和有限对话协调。

Conclusion: 多智能体团队动态研究揭示扁平结构优势，多样性需谨慎处理，智能体协作存在潜力但需改进对话协调机制。

Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [4] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 该论文提出两种方法来解决小型视觉语言模型在图表理解任务中作为自动评估者表现不佳的问题：多标准提示和领域自适应迁移学习，通过微调2B参数模型创建ChartJudge，实现资源受限环境下的高效评估。


<details>
  <summary>Details</summary>
Motivation: 解决小型视觉语言模型（≤2B参数）在图表理解任务中作为自动评估者表现不佳的问题，使其能在资源受限环境中有效使用。

Method: 提出两种方法：(1)多标准提示，将多个评估标准组合到单个查询中；(2)领域自适应迁移学习，在图表数据集上使用合成判断微调2B参数LVLM创建ChartJudge。

Result: 多标准提示暴露了7B模型的鲁棒性差距，导致性能大幅下降；ChartJudge能够有效在不同数据集间迁移知识，成为更专业的模型。

Conclusion: 通过模型大小、提示设计和可迁移性之间的权衡分析，为图表推理任务提供了可扩展、低成本的评估解决方案。

Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [5] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

TL;DR: Ryt AI是一个基于LLM的对话式银行代理框架，首次在全球范围内获得监管批准，将自然语言对话作为主要银行界面，替代传统的多屏幕工作流程。


<details>
  <summary>Details</summary>
Motivation: 开发首个获得全球监管批准的对话式AI银行界面，让客户通过自然语言对话执行核心金融交易，突破以往AI助手仅限于咨询或支持角色的限制。

Method: 使用内部开发的闭源LLM ILMU，通过四个LLM驱动的代理（护栏、意图、支付、FAQ）协调单一对话，每个代理使用任务特定的LoRA适配器，在银行基础设施内托管以确保一致行为。

Result: 成功实现了监管批准的自然语言界面，在严格治理下可靠支持核心金融操作，展示了对话式AI作为主要银行界面的可行性。

Conclusion: Ryt AI证明了在严格安全合规框架下，自然语言界面可以可靠地支持核心金融交易，代表了银行服务交付方式的重大进步。

Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [6] [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
*Tianci Liu,Ran Xu,Tony Yu,Ilgee Hong,Carl Yang,Tuo Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 提出了OpenRubrics数据集和对比性规则生成方法，通过结构化自然语言标准改进奖励建模，在多个基准测试中超越基线6.8%


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖标量或成对判断，无法捕捉人类偏好的多维度特性，需要更可靠和可扩展的规则生成方法

Method: 引入OpenRubrics数据集，提出对比性规则生成方法，通过对比偏好和拒绝响应来推导硬规则和原则，并使用拒绝采样提高可靠性

Result: 基于规则的奖励模型Rubric-RM在多个基准测试中超越基线6.8%，在指令遵循和生物医学基准上表现出良好的迁移性

Conclusion: 规则提供了可扩展的对齐信号，缩小了昂贵的人工评估与自动奖励建模之间的差距，为LLM对齐开启了新的原则驱动范式

Abstract: Reward modeling lies at the core of reinforcement learning from human
feedback (RLHF), yet most existing reward models rely on scalar or pairwise
judgments that fail to capture the multifaceted nature of human preferences.
Recent studies have explored rubrics-as-rewards (RaR) that uses structured
natural language criteria that capture multiple dimensions of response quality.
However, producing rubrics that are both reliable and scalable remains a key
challenge. In this work, we introduce OpenRubrics, a diverse, large-scale
collection of (prompt, rubric) pairs for training rubric-generation and
rubric-based reward models. To elicit discriminative and comprehensive
evaluation signals, we introduce Contrastive Rubric Generation (CRG), which
derives both hard rules (explicit constraints) and principles (implicit
qualities) by contrasting preferred and rejected responses. We further improve
reliability by enforcing preference-label consistency via rejection sampling to
remove noisy rubrics. Across multiple reward-modeling benchmarks, our
rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines
by 6.8%. These gains transfer to policy models on instruction-following and
biomedical benchmarks. Our results show that rubrics provide scalable alignment
signals that narrow the gap between costly human evaluation and automated
reward modeling, enabling a new principle-driven paradigm for LLM alignment.

</details>


### [7] [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
*Runyang You,Yongqi Li,Meng Liu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.CL

TL;DR: 该论文实现了潜在推理模型的并行测试时扩展，通过蒙特卡洛Dropout和高斯噪声两种采样策略，以及基于对比学习的潜在奖励模型进行轨迹聚合，在连续空间中实现了可扩展推理。


<details>
  <summary>Details</summary>
Motivation: 现有并行测试时扩展方法主要针对显式链式思维，而潜在推理模型在连续向量空间中进行推理，缺乏采样机制和概率信号用于轨迹聚合，因此需要开发适用于连续空间的并行扩展方法。

Method: 提出两种不确定性启发的随机采样策略：蒙特卡洛Dropout和加性高斯噪声；设计基于步进对比目标的潜在奖励模型来评分和指导潜在推理轨迹。

Result: 实验表明两种采样策略都能有效随计算量扩展，并展现出不同的探索动态；潜在奖励模型能够有效选择推理轨迹，为连续空间中的可扩展推理开辟了新方向。

Conclusion: 该工作成功实现了潜在推理模型的并行测试时扩展，通过采样策略和轨迹聚合方法的结合，在连续空间中实现了高效的推理扩展。

Abstract: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large
language models (LLMs), typically by sampling multiple token-based
chains-of-thought in parallel and aggregating outcomes through voting or
search. Recent advances in latent reasoning, where intermediate reasoning
unfolds in continuous vector spaces, offer a more efficient alternative to
explicit Chain-of-Thought, yet whether such latent models can similarly benefit
from parallel TTS remains open, mainly due to the absence of sampling
mechanisms in continuous space, and the lack of probabilistic signals for
advanced trajectory aggregation. \ This work enables parallel TTS for latent
reasoning models by addressing the above issues. For sampling, we introduce two
uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive
Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)
trained with step-wise contrastive objective to score and guide latent
reasoning. Extensive experiments and visualization analyses show that both
sampling strategies scale effectively with compute and exhibit distinct
exploration dynamics, while LatentRM enables effective trajectory selection.
Together, our explorations open a new direction for scalable inference in
continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.

</details>


### [8] [ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning](https://arxiv.org/abs/2510.07768)
*Murong Yue,Zhiwei Liu,Liangwei Yang,Jianguo Zhang,Zuxin Liu,Haolin Chen,Ziyu Yao,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

TL;DR: 提出了一种将非结构化工具集合重构为结构化工具库的系统方法，通过聚类和多智能体框架将众多问题特定工具整合为少量功能强大的聚合工具，显著提升了工具检索准确性和推理性能。


<details>
  <summary>Details</summary>
Motivation: 工具增强推理面临领域特定工具稀缺的问题，现有自动创建工具的方法存在可扩展性瓶颈，随着工具数量增长，非结构化存储导致检索困难。

Method: 首先生成离散的任务特定工具并聚类到语义相关主题中，然后使用多智能体框架：代码智能体重构代码提取共享逻辑创建聚合工具，评审智能体确保聚合工具保持原始功能的完整性。

Result: 实验结果表明该方法显著提高了工具检索准确性和整体推理性能，在工具数量增加时展现出比基线更好的可扩展性。

Conclusion: 该方法成功将众多问题特定工具转化为少量功能强大的聚合工具，解决了工具库的可扩展性问题。

Abstract: Large Language Models (LLMs) equipped with external tools have demonstrated
enhanced performance on complex reasoning tasks. The widespread adoption of
this tool-augmented reasoning is hindered by the scarcity of domain-specific
tools. For instance, in domains such as physics question answering, suitable
and specialized tools are often missing. Recent work has explored automating
tool creation by extracting reusable functions from Chain-of-Thought (CoT)
reasoning traces; however, these approaches face a critical scalability
bottleneck. As the number of generated tools grows, storing them in an
unstructured collection leads to significant retrieval challenges, including an
expanding search space and ambiguity between function-related tools. To address
this, we propose a systematic approach to automatically refactor an
unstructured collection of tools into a structured tool library. Our system
first generates discrete, task-specific tools and clusters them into
semantically coherent topics. Within each cluster, we introduce a multi-agent
framework to consolidate scattered functionalities: a code agent refactors code
to extract shared logic and creates versatile, aggregated tools, while a
reviewing agent ensures that these aggregated tools maintain the complete
functional capabilities of the original set. This process transforms numerous
question-specific tools into a smaller set of powerful, aggregated tools
without loss of functionality. Experimental results demonstrate that our
approach significantly improves tool retrieval accuracy and overall reasoning
performance across multiple reasoning tasks. Furthermore, our method shows
enhanced scalability compared with baselines as the number of question-specific
increases.

</details>


### [9] [HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation](https://arxiv.org/abs/2510.07794)
*Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Kaiyu He,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 提出了HiPRAG方法，通过分层过程奖励优化RAG代理的搜索行为，解决过度搜索和搜索不足问题，在多个基准测试中显著提升准确率和搜索效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于结果奖励的RL训练方法缺乏细粒度控制，导致RAG代理存在过度搜索和搜索不足等低效行为，需要改进训练方法。

Method: 引入分层过程奖励，将推理轨迹分解为可解析步骤，评估每个搜索决策的必要性，在结果奖励基础上增加基于最优搜索步骤比例的额外奖励。

Result: 在7个QA基准测试中，Qwen2.5和Llama-3.2模型分别达到65.4%(3B)和67.2%(7B)的平均准确率，过度搜索率降至2.3%，同时降低搜索不足率。

Conclusion: 优化推理过程本身而不仅仅是最终结果，通过RL的细粒度控制可以有效提升搜索代理的效率和最优性。

Abstract: Agentic RAG is a powerful technique for incorporating external information
that LLMs lack, enabling better problem solving and question answering.
However, suboptimal search behaviors exist widely, such as over-search
(retrieving information already known) and under-search (failing to search when
necessary), which leads to unnecessary overhead and unreliable outputs. Current
training methods, which typically rely on outcome-based rewards in a RL
framework, lack the fine-grained control needed to address these
inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for
Efficient agentic RAG (HiPRAG), a training methodology that incorporates a
fine-grained, knowledge-grounded process reward into the RL training. Our
approach evaluates the necessity of each search decision on-the-fly by
decomposing the agent's reasoning trajectory into discrete, parsable steps. We
then apply a hierarchical reward function that provides an additional bonus
based on the proportion of optimal search and non-search steps, on top of
commonly used outcome and format rewards. Experiments on the Qwen2.5 and
Llama-3.2 models across seven diverse QA benchmarks show that our method
achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished
while improving search efficiency, reducing the over-search rate to just 2.3%
and concurrently lowering the under-search rate. These results demonstrate the
efficacy of optimizing the reasoning process itself, not just the final
outcome. Further experiments and analysis demonstrate that HiPRAG shows good
generalizability across a wide range of RL algorithms, model families, sizes,
and types. This work demonstrates the importance and potential of fine-grained
control through RL, for improving the efficiency and optimality of reasoning
for search agents.

</details>


### [10] [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)
*Eric Hanchen Jiang,Guancheng Wan,Sophia Yin,Mengting Li,Yuchen Wu,Xiao Liang,Xinfeng Li,Yizhou Sun,Wei Wang,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.CL

TL;DR: 提出了一种名为GTD的生成框架，通过条件离散图扩散模型迭代生成多智能体系统的通信拓扑，实现任务自适应、稀疏且高效的拓扑设计。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统的效率很大程度上依赖于通信拓扑，但设计最优拓扑需要在任务性能、通信成本和鲁棒性之间平衡。现有静态拓扑方法无法适应不同任务需求。

Method: GTD框架将拓扑合成建模为迭代构建过程，使用轻量级代理模型预测多目标奖励（如准确性、效用、成本），实现实时、无梯度的任务自适应拓扑优化。

Result: 在多个基准测试中验证，GTD能够生成高度任务自适应、稀疏且高效的通信拓扑，在LLM智能体协作中显著优于现有方法。

Conclusion: GTD框架通过迭代引导的拓扑合成过程，能够有效处理复杂的设计权衡，为多智能体系统提供优化的通信拓扑解决方案。

Abstract: The efficiency of multi-agent systems driven by large language models (LLMs)
largely hinges on their communication topology. However, designing an optimal
topology is a non-trivial challenge, as it requires balancing competing
objectives such as task performance, communication cost, and robustness.
Existing frameworks often rely on static or hand-crafted topologies, which
inherently fail to adapt to diverse task requirements, leading to either
excessive token consumption for simple problems or performance bottlenecks for
complex ones. To address this challenge, we introduce a novel generative
framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by
conditional discrete graph diffusion models, GTD formulates topology synthesis
as an iterative construction process. At each step, the generation is steered
by a lightweight proxy model that predicts multi-objective rewards (e.g.,
accuracy, utility, cost), enabling real-time, gradient-free optimization
towards task-adaptive topologies. This iterative, guided synthesis process
distinguishes GTD from single-step generative frameworks, enabling it to better
navigate complex design trade-offs. We validated GTD across multiple
benchmarks, and experiments show that this framework can generate highly
task-adaptive, sparse, and efficient communication topologies, significantly
outperforming existing methods in LLM agent collaboration.

</details>


### [11] [Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2510.07892)
*Hyeonseok Moon,Seongtae Hong,Jaehyung Seo,Heuiseok Lim*

Main category: cs.CL

TL;DR: MCBench是一个新的基准测试，用于评估LLM是否能够严格遵循逐步指令执行字符串匹配NLP指标，提供客观、确定性和可代码验证的评估。


<details>
  <summary>Details</summary>
Motivation: 当前前沿LLM已经饱和了许多先前困难的基准测试，需要更具挑战性的基准来提供客观验证。

Method: 设计MCBench基准，通过严格遵循逐步指令来评估LLM执行字符串匹配NLP指标的能力，提供并行参考代码来验证LLM输出的准确性。

Result: MCBench作为评估前沿LLM能力的有效客观工具，能够系统测试LLM在逐步执行、指令遵循、数值计算和长范围一致性方面的表现。

Conclusion: MCBench提供了一个客观、确定性和可代码验证的评估框架，能够有效区分不同LLM在详细指令理解能力方面的差异。

Abstract: Recent frontier-level LLMs have saturated many previously difficult
benchmarks, leaving little room for further differentiation. This progress
highlights the need for challenging benchmarks that provide objective
verification. In this paper, we introduce MCBench, a benchmark designed to
evaluate whether LLMs can execute string-matching NLP metrics by strictly
following step-by-step instructions. Unlike prior benchmarks that depend on
subjective judgments or general reasoning, MCBench offers an objective,
deterministic and codeverifiable evaluation. This setup allows us to
systematically test whether LLMs can maintain accurate step-by-step execution,
including instruction adherence, numerical computation, and long-range
consistency in handling intermediate results. To ensure objective evaluation of
these abilities, we provide a parallel reference code that can evaluate the
accuracy of LLM output. We provide three evaluative metrics and three benchmark
variants designed to measure the detailed instruction understanding capability
of LLMs. Our analyses show that MCBench serves as an effective and objective
tool for evaluating the capabilities of cutting-edge LLMs.

</details>


### [12] [A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning](https://arxiv.org/abs/2510.07958)
*Fengji Zhang,Xinyao Niu,Chengyang Ying,Guancheng Lin,Zhongkai Hao,Zhou Fan,Chengen Huang,Jacky Keung,Bei Chen,Junyang Lin*

Main category: cs.CL

TL;DR: A²Search是一个无需标注的端到端训练框架，通过轨迹采样和证据验证自动检测模糊问题并收集替代答案，使用AnsF1奖励进行强化学习优化，在8个开放域QA基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和RL模型在处理允许多个有效答案的模糊问题时表现不佳，标准QA基准假设单一黄金答案，产生不适当的训练信号，而现有处理模糊性的方法依赖昂贵的人工标注，难以扩展到多跳数据集。

Method: 使用自动化管道检测模糊问题并通过轨迹采样和证据验证收集替代答案，然后使用精心设计的AnsF1奖励进行强化学习优化，该奖励自然适应多个答案。

Result: 在8个开放域QA基准测试中实现最先进性能，A²Search-7B在四个多跳基准测试中平均AnsF1@1得分48.4%，优于包括更大的ReSearch-32B（46.2%）在内的所有强基线。

Conclusion: A²Search解决了模糊性问题并在基准测试中泛化良好，表明接受模糊性对于构建更可靠的QA系统至关重要。

Abstract: Recent advances in Large Language Models (LLMs) and Reinforcement Learning
(RL) have led to strong performance in open-domain question answering (QA).
However, existing models still struggle with questions that admit multiple
valid answers. Standard QA benchmarks, which typically assume a single gold
answer, overlook this reality and thus produce inappropriate training signals.
Existing attempts to handle ambiguity often rely on costly manual annotation,
which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.
In this paper, we present A$^2$Search, an annotation-free, end-to-end training
framework to recognize and handle ambiguity. At its core is an automated
pipeline that detects ambiguous questions and gathers alternative answers via
trajectory sampling and evidence verification. The model is then optimized with
RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally
accommodates multiple answers. Experiments on eight open-domain QA benchmarks
demonstrate that A$^2$Search achieves new state-of-the-art performance. With
only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$
score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong
baselines, including the substantially larger ReSearch-32B ($46.2\%$).
Extensive analyses further show that A$^2$Search resolves ambiguity and
generalizes across benchmarks, highlighting that embracing ambiguity is
essential for building more reliable QA systems. Our code, data, and model
weights can be found at https://github.com/zfj1998/A2Search

</details>


### [13] [LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?](https://arxiv.org/abs/2510.07962)
*Jingyuan Wang,Yankai Chen,Zhonghang Li,Chao Huang*

Main category: cs.CL

TL;DR: LightReasoner是一个新颖框架，利用小型语言模型(SLM)识别大型语言模型(LLM)的关键推理时刻，通过专家-业余对比构建监督样本，显著提升LLM的推理能力，同时大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调(SFT)方法资源密集，需要大量标注数据和均匀优化所有token，但只有少数token具有学习价值。本文探索小型模型能否通过揭示高价值推理时刻来教导大型模型。

Method: LightReasoner采用两阶段框架：1)采样阶段通过专家(LLM)与业余(SLM)的行为差异识别关键推理时刻并构建对比监督样本；2)微调阶段使用这些蒸馏样本对齐专家模型，增强其推理优势。

Result: 在七个数学基准测试中，LightReasoner将准确率提升高达28.1%，同时减少90%的时间消耗、80%的采样问题和99%的微调token使用，且不依赖真实标签。

Conclusion: 通过将较弱的小型模型转化为有效的教学信号，LightReasoner为提升LLM推理能力提供了一种可扩展且资源高效的方法。

Abstract: Large language models (LLMs) have demonstrated remarkable progress in
reasoning, often through supervised fine-tuning (SFT). However, SFT is
resource-intensive, relying on large curated datasets, rejection-sampled
demonstrations, and uniform optimization across all tokens, even though only a
fraction carry meaningful learning value. In this work, we explore a
counterintuitive idea: can smaller language models (SLMs) teach larger language
models (LLMs) by revealing high-value reasoning moments that reflect the
latter's unique strength? We propose LightReasoner, a novel framework that
leverages the behavioral divergence between a stronger expert model (LLM) and a
weaker amateur model (SLM). LightReasoner operates in two stages: (1) a
sampling stage that pinpoints critical reasoning moments and constructs
supervision examples capturing the expert's advantage through expert-amateur
contrast, and (2) a fine-tuning stage that aligns the expert model with these
distilled examples, amplifying its reasoning strengths. Across seven
mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while
reducing time consumption by 90%, sampled problems by 80%, and tuned token
usage by 99%, all without relying on ground-truth labels. By turning weaker
SLMs into effective teaching signals, LightReasoner offers a scalable and
resource-efficient approach for advancing LLM reasoning. Code is available at:
https://github.com/HKUDS/LightReasoner

</details>


### [14] [Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning](https://arxiv.org/abs/2510.07974)
*Jialu Du,Guiyang Hou,Yihui Fu,Chen Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu*

Main category: cs.CL

TL;DR: LLMs在社交推理任务中存在困难，无法区分客观世界状态和主观信念状态。本文提出自适应世界模型增强推理机制，通过构建动态文本世界模型来跟踪实体状态和时间序列，显著提升社交推理准确性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和代码推理方面表现出色，但在社交推理任务中表现不佳，经常出现认知混乱、逻辑不一致以及混淆客观世界状态与主观信念状态的问题。

Method: 提出自适应世界模型增强推理机制，构建动态文本世界模型来跟踪实体状态和时间序列，动态监控推理轨迹中的困惑指标，并通过提供清晰的世界状态描述来及时干预。

Result: 在三个社交基准测试中显示出显著改进，准确率提升（例如在Hi-ToM中提升10%），同时计算成本降低（最多减少33.8%的token使用）。

Conclusion: 该机制为在社交环境中部署LLMs提供了一个简单而有效的解决方案，模仿了人类使用隐式世界模型来区分外部事件和内部信念的方式。

Abstract: While large language models (LLMs) excel in mathematical and code reasoning,
we observe they struggle with social reasoning tasks, exhibiting cognitive
confusion, logical inconsistencies, and conflation between objective world
states and subjective belief states. Through deteiled analysis of DeepSeek-R1's
reasoning trajectories, we find that LLMs frequently encounter reasoning
impasses and tend to output contradictory terms like "tricky" and "confused"
when processing scenarios with multiple participants and timelines, leading to
erroneous reasoning or infinite loops. The core issue is their inability to
disentangle objective reality from agents' subjective beliefs. To address this,
we propose an adaptive world model-enhanced reasoning mechanism that constructs
a dynamic textual world model to track entity states and temporal sequences. It
dynamically monitors reasoning trajectories for confusion indicators and
promptly intervenes by providing clear world state descriptions, helping models
navigate through cognitive dilemmas. The mechanism mimics how humans use
implicit world models to distinguish between external events and internal
beliefs. Evaluations on three social benchmarks demonstrate significant
improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational
costs (up to 33.8% token reduction), offering a simple yet effective solution
for deploying LLMs in social contexts.

</details>


### [15] [Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks](https://arxiv.org/abs/2510.08002)
*Cheng Yang,Xuemeng Yang,Licheng Wen,Daocheng Fu,Jianbiao Mei,Rong Wu,Pinlong Cai,Yufan Shen,Nianchen Deng,Botian Shi,Yu Qiao,Haifeng Li*

Main category: cs.CL

TL;DR: MUSE是一个基于分层记忆模块的自主进化AI代理框架，能够通过经验积累和反思实现持续学习和自我进化，在长时程任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在测试时是静态的，无法从经验中学习，缺乏知识积累和持续改进的能力，这限制了它们在现实世界长时程任务中的应用。

Method: 提出MUSE框架，围绕分层记忆模块构建经验驱动的自进化系统，通过任务执行后的自主反思将原始轨迹转化为结构化经验并整合到记忆模块中。

Result: 在TAC生产力基准测试中，仅使用轻量级Gemini-2.5 Flash模型就实现了显著的SOTA性能，随着经验积累，任务完成能力持续提升，并展现出强大的零样本泛化能力。

Conclusion: MUSE为能够实现现实世界生产力任务自动化的AI代理建立了新范式，证明了经验驱动的自进化代理的可行性。

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.

</details>


### [16] [A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models](https://arxiv.org/abs/2510.08049)
*Congming Zheng,Jiachen Zhu,Zhuoying Ou,Yuxiang Chen,Kangning Zhang,Rong Shan,Zeyu Zheng,Mengyue Yang,Jianghao Lin,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 本文系统综述了过程奖励模型(PRMs)的完整流程，包括过程数据生成、PRM构建以及PRM在测试时扩展和强化学习中的应用，旨在推动细粒度推理对齐研究。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法主要依赖结果奖励模型(ORMs)仅评判最终答案，而过程奖励模型(PRMs)通过在步骤或轨迹层面评估和引导推理来弥补这一差距。

Method: 通过系统综述的方式，分析PRMs的完整流程：过程数据生成、PRM构建方法、PRM在测试时扩展和强化学习中的应用。

Result: 总结了PRMs在数学、代码、文本、多模态推理、机器人和智能体等领域的应用，并回顾了新兴基准测试。

Conclusion: 明确了设计空间，揭示了开放挑战，为未来细粒度、鲁棒推理对齐研究提供指导。

Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.

</details>


### [17] [ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code](https://arxiv.org/abs/2510.08163)
*Jian Xie,Zhendong Chu,Aoxiao Zhong,Kai Zhang,Mingzhe Han,Xin Fang,Jialie Shen,Qingsong Wen*

Main category: cs.CL

TL;DR: ARM2是一个通过强化学习框架实现自适应推理的统一模型，能够在保持性能的同时显著减少推理长度，平均降低70%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在"过度思考"问题，在简单任务上生成不必要的冗长推理。现有方法如长度惩罚或路由机制通常是启发式且任务特定的，缺乏通用的自适应推理框架。

Method: 使用强化学习框架结合长度感知优化，统一平衡多种格式的推理性能和效率。集成视觉理解和可执行代码到推理过程中。

Result: ARM2在保持与传统GRPO训练推理模型相当性能的同时，平均减少70%的token使用量。

Conclusion: ARM2提供了一个有效的自适应推理框架，显著提高了推理效率，同时保持了任务性能。

Abstract: Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.

</details>


### [18] [Training-Free Group Relative Policy Optimization](https://arxiv.org/abs/2510.08191)
*Yuzheng Cai,Siqi Cai,Yuchen Shi,Zihan Xu,Lichao Chen,Yulei Qin,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Yong Mao,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 提出了Training-Free GRPO方法，通过将经验知识作为token先验来增强LLM代理性能，无需参数更新，在数学推理和网络搜索任务中显著提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在专业领域性能下降的问题，避免传统方法需要昂贵参数更新和容易过拟合的缺点。

Method: 使用训练免费的组相对策略优化，利用组内rollouts的语义优势而非数值优势，在多轮学习中迭代提炼高质量经验知识作为token先验。

Result: 在DeepSeek-V3.1-Terminus上应用，仅需几十个训练样本就能超越微调的小型LLM，显著提升跨域性能。

Conclusion: Training-Free GRPO是一种轻量级且成本效益高的解决方案，能有效解决数据稀缺和过拟合问题。

Abstract: Recent advances in Large Language Model (LLM) agents have demonstrated their
promising general capabilities. However, their performance in specialized
real-world domains often degrades due to challenges in effectively integrating
external tools and specific prompting strategies. While methods like agentic
reinforcement learning have been proposed to address this, they typically rely
on costly parameter updates, for example, through a process that uses
Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase
with Group Relative Policy Optimization (GRPO) to alter the output
distribution. However, we argue that LLMs can achieve a similar effect on the
output distribution by learning experiential knowledge as a token prior, which
is a far more lightweight approach that not only addresses practical data
scarcity but also avoids the common issue of overfitting. To this end, we
propose Training-Free Group Relative Policy Optimization (Training-Free GRPO),
a cost-effective solution that enhances LLM agent performance without any
parameter updates. Our method leverages the group relative semantic advantage
instead of numerical ones within each group of rollouts, iteratively distilling
high-quality experiential knowledge during multi-epoch learning on a minimal
ground-truth data. Such knowledge serves as the learned token prior, which is
seamlessly integrated during LLM API calls to guide model behavior. Experiments
on mathematical reasoning and web searching tasks demonstrate that
Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly
improves out-of-domain performance. With just a few dozen training samples,
Training-Free GRPO outperforms fine-tuned small LLMs with marginal training
data and cost.

</details>


### [19] [LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions](https://arxiv.org/abs/2510.08211)
*XuHao Hu,Peng Wang,Xiaoya Lu,Dongrui Liu,Xuanjing Huang,Jing Shao*

Main category: cs.CL

TL;DR: 研究表明，在特定领域微调LLMs可能导致广泛的不诚实行为，这种现象称为涌现性失调。即使只引入1%的失调数据，诚实行为也会下降20%以上，在10%偏见用户环境下也会无意中加剧不诚实。


<details>
  <summary>Details</summary>
Motivation: 探索涌现性失调现象是否不仅限于安全行为，还能扩展到高风险场景下的不诚实和欺骗行为。

Method: 在多样化领域对开源LLMs进行失调补全微调，并在下游混合任务和实际人机交互环境中测试。

Result: LLMs在多个领域表现出广泛的不诚实行为，1%失调数据可使诚实行为下降超20%，10%偏见用户就能无意中加剧模型不诚实。

Conclusion: 涌现性失调风险不仅存在于直接微调，也存在于下游混合任务和实际人机交互中，需要关注LLMs在高风险场景下的不诚实行为。

Abstract: Previous research has shown that LLMs finetuned on malicious or incorrect
completions within narrow domains (e.g., insecure code or incorrect medical
advice) can become broadly misaligned to exhibit harmful behaviors, which is
called emergent misalignment. In this work, we investigate whether this
phenomenon can extend beyond safety behaviors to a broader spectrum of
dishonesty and deception under high-stakes scenarios (e.g., lying under
pressure and deceptive behavior). To explore this, we finetune open-sourced
LLMs on misaligned completions across diverse domains. Experimental results
demonstrate that LLMs show broadly misaligned behavior in dishonesty.
Additionally, we further explore this phenomenon in a downstream combined
finetuning setting, and find that introducing as little as 1% of misalignment
data into a standard downstream task is sufficient to decrease honest behavior
over 20%. Furthermore, we consider a more practical human-AI interaction
environment where we simulate both benign and biased users to interact with the
assistant LLM. Notably, we find that the assistant can be misaligned
unintentionally to exacerbate its dishonesty with only 10% biased user
population. In summary, we extend the study of emergent misalignment to the
domain of dishonesty and deception under high-stakes scenarios, and demonstrate
that this risk arises not only through direct finetuning, but also in
downstream mixture tasks and practical human-AI interactions.

</details>


### [20] [The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/abs/2510.08240)
*Jingyu Zhang,Haozhu Wang,Eric Michael Smith,Sid Wang,Amr Sharaf,Mahesh Pasupuleti,Benjamin Van Durme,Daniel Khashabi,Jason Weston,Hongyuan Zhan*

Main category: cs.CL

TL;DR: WaltzRL是一个新颖的多智能体强化学习框架，将安全对齐建模为协作的正和博弈，通过联合训练对话智能体和反馈智能体来减少不安全响应和过度拒绝。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用安全防护模型完全拒绝包含不安全部分的内容，这会加剧过度拒绝问题，且无法为被拒绝的查询提供细致指导。需要在帮助性和无害性之间找到更好的平衡。

Method: 提出WaltzRL框架，联合训练对话智能体和反馈智能体，使用动态改进奖励(DIR)根据对话智能体整合反馈的效果进行激励。推理时改进而非丢弃不安全或过度拒绝的响应。

Result: 在五个不同数据集上的实验表明，WaltzRL显著减少了不安全响应（如WildJailbreak上从39.0%降至4.6%）和过度拒绝（OR-Bench上从45.3%降至9.9%）。

Conclusion: WaltzRL通过让对话和反馈智能体共同进化并自适应应用反馈，在不降低通用能力的情况下增强了LLM安全性，推进了帮助性和无害性之间的帕累托前沿。

Abstract: Harnessing the power of LLMs requires a delicate dance between being helpful
and harmless. This creates a fundamental tension between two competing
challenges: vulnerability to adversarial attacks that elicit unsafe content,
and a tendency for overrefusal on benign but sensitive prompts. Current
approaches often navigate this dance with safeguard models that completely
reject any content that contains unsafe portions. This approach cuts the music
entirely-it may exacerbate overrefusals and fails to provide nuanced guidance
for queries it refuses. To teach models a more coordinated choreography, we
propose WaltzRL, a novel multi-agent reinforcement learning framework that
formulates safety alignment as a collaborative, positive-sum game. WaltzRL
jointly trains a conversation agent and a feedback agent, where the latter is
incentivized to provide useful suggestions that improve the safety and
helpfulness of the conversation agent's responses. At the core of WaltzRL is a
Dynamic Improvement Reward (DIR) that evolves over time based on how well the
conversation agent incorporates the feedback. At inference time, unsafe or
overrefusing responses from the conversation agent are improved rather than
discarded. The feedback agent is deployed together with the conversation agent
and only engages adaptively when needed, preserving helpfulness and low latency
on safe queries. Our experiments, conducted across five diverse datasets,
demonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,
from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on
OR-Bench) compared to various baselines. By enabling the conversation and
feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances
LLM safety without degrading general capabilities, thereby advancing the Pareto
front between helpfulness and harmlessness.

</details>


### [21] [Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window](https://arxiv.org/abs/2510.08276)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Yaojie Lu,Xianpei Han,Le Sun,WenJuan Zhang,Pengbo Wang,Shixuan Liu,Zhenru Zhang,Jianhong Tu,Hongyu Lin,Junyang Lin*

Main category: cs.CL

TL;DR: DeepMiner是一个通过强化学习激发多轮智能体深度推理能力的框架，采用高难度训练任务和动态上下文窗口，在多个搜索智能体基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在多轮长时程交互中激发深度推理能力，需要解决训练数据挑战性和上下文管理的局限性。

Method: 提出反向构建方法从真实网络源生成复杂但可验证的问答对，设计动态上下文管理策略，使用滑动窗口机制消除对外部摘要模型的依赖。

Result: 在BrowseComp-en上达到33.5%准确率，比之前最佳开源智能体提升近20个百分点，在多个基准测试中表现一致改进，支持近100轮持续交互。

Conclusion: DeepMiner有效解决了多轮交互系统的上下文限制问题，显著提升了搜索智能体的推理能力。

Abstract: While recent advances in reasoning models have demonstrated cognitive
behaviors through reinforcement learning, existing approaches struggle to
invoke deep reasoning capabilities in multi-turn agents with long-horizon
interactions. We propose DeepMiner, a novel framework that elicits such
abilities by introducing high-difficulty training tasks and dynamic context
window. DeepMiner presents a reverse construction method to generate complex
but verifiable question-answer pairs from authentic web sources, which ensures
the challenge and reliability of training data while injecting cognitive
capabilities into multi-turn reasoning scenarios. We further design an elegant
yet effective dynamic context management strategy for both training and
inference, utilizing sliding window mechanisms while eliminating the dependency
on external summarization models, thereby efficiently empowering the model to
handle continuously expanding long-horizon contexts. Through reinforcement
learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial
performance improvements across multiple search agent benchmarks. DeepMiner
attains 33.5% accuracy on BrowseComp-en, surpassing the previous best
open-source agent by almost 20 percentage points, and demonstrates consistent
improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our
dynamic context management enables sustained interactions of nearly 100 turns
within standard 32k context length, effectively addressing the context
limitations that constrain existing multi-turn interaction systems.

</details>


### [22] [Neuron-Level Analysis of Cultural Understanding in Large Language Models](https://arxiv.org/abs/2510.08284)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 该论文通过神经元级分析识别驱动文化行为的神经元，发现文化通用神经元和文化特定神经元集中在浅层到中层MLP层，抑制这些神经元会显著降低文化基准性能但不影响一般NLU任务。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs存在的文化偏见和对弱势文化认知不足的问题，探索LLMs文化理解的内在机制。

Method: 采用基于梯度的评分方法进行神经元级分析，识别文化通用神经元和文化特定神经元，并通过抑制实验验证其作用。

Result: 识别出占所有神经元不到1%的文化相关神经元，主要集中在浅层到中层MLP层；抑制这些神经元使文化基准性能下降达30%，而一般NLU任务性能基本不受影响。

Conclusion: 文化特定神经元不仅支持目标文化知识，还支持相关文化知识；在包含文化通用神经元的模块上进行NLU基准训练会削弱模型的文化理解能力。

Abstract: As large language models (LLMs) are increasingly deployed worldwide, ensuring
their fair and comprehensive cultural understanding is important. However, LLMs
exhibit cultural bias and limited awareness of underrepresented cultures, while
the mechanisms underlying their cultural understanding remain underexplored. To
fill this gap, we conduct a neuron-level analysis to identify neurons that
drive cultural behavior, introducing a gradient-based scoring method with
additional filtering for precise refinement. We identify both culture-general
neurons contributing to cultural understanding regardless of cultures, and
culture-specific neurons tied to an individual culture. These neurons account
for less than 1% of all neurons and are concentrated in shallow to middle MLP
layers. We validate their role by showing that suppressing them substantially
degrades performance on cultural benchmarks (by up to 30%), while performance
on general natural language understanding (NLU) benchmarks remains largely
unaffected. Moreover, we show that culture-specific neurons support knowledge
of not only the target culture, but also related cultures. Finally, we
demonstrate that training on NLU benchmarks can diminish models' cultural
understanding when we update modules containing many culture-general neurons.
These findings provide insights into the internal mechanisms of LLMs and offer
practical guidance for model training and engineering. Our code is available at
https://github.com/ynklab/CULNIG

</details>


### [23] [On the Relationship Between the Choice of Representation and In-Context Learning](https://arxiv.org/abs/2510.08372)
*Ioana Marinescu,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 研究发现上下文学习(ICL)中演示表示和学习能力是相互独立的：表示质量决定基线准确率，而学习能力通过增加演示样本独立提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索ICL中演示表示和学习能力之间的相互作用，验证它们是否相互独立。

Method: 开发优化算法枚举不同语义相关性的标签集，在不同标签集上使用不同数量的演示样本进行ICL实验。

Result: 学习确实发生且与标签集质量无关，但学习效率受标签集质量和模型参数数量影响。标签集的相对质量在学习过程中保持稳定。

Conclusion: ICL中演示表示和学习能力具有正交性，表示决定基线性能，学习能力独立提升性能。

Abstract: In-context learning (ICL) is the ability of a large language model (LLM) to
learn a new task from a few demonstrations presented as part of the context.
Past studies have attributed a large portion of the success of ICL to the way
these in-context demonstrations are represented, particularly to how labels are
represented in classification tasks. On the other hand, observations of the
learning capacity of ICL (i.e., the extent to which more in-context
demonstrations can lead to higher performance) have been mixed, and ICL is
often thought to occur only under specific conditions. The interaction between
these two aspects in ICL, representation and learning, has not been studied in
depth until now. We hypothesize that they are largely independent of one
another, such that the representation of demonstrations determines the baseline
accuracy of ICL, while learning from additional demonstrations improves only on
top of this baseline. We validate this hypothesis by developing an optimization
algorithm that can enumerate a spectrum of possible label sets
(representations) varying in semantic relevance. We then perform ICL with
varying numbers of in-context demonstrations for each of these label sets. We
observed that learning happens regardless of the quality of the label set
itself, although its efficiency, measured by the slope of improvement over
in-context demonstrations, is conditioned on both the label set quality and the
parameter count of the underlying language model. Despite the emergence of
learning, the relative quality (accuracy) of the choice of a label set
(representation) is largely maintained throughout learning, confirming our
hypothesis and implying their orthogonality. Our work reveals a previously
underexplored aspect of ICL: the independent effects of learning from
demonstrations and their representations on ICL performance.

</details>


### [24] [DeepPrune: Parallel Scaling without Inter-trace Redundancy](https://arxiv.org/abs/2510.08483)
*Shangqing Tu,Yaxuan Li,Yushi Bai,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: DeepPrune是一个通过动态剪枝实现高效并行推理的框架，解决了并行扩展中推理轨迹冗余导致的计算效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 并行扩展通过同时生成多个思维链轨迹来增强大语言模型的推理能力，但存在严重的计算效率问题——超过80%的并行推理轨迹产生相同的最终答案，造成大量计算浪费。

Method: 提出DeepPrune框架，包含：1）使用焦点损失和过采样技术训练的专业判断模型，从部分推理轨迹准确预测答案等价性（AUROC达0.87）；2）在线贪心聚类算法，动态剪枝冗余路径同时保持答案多样性。

Result: 在三个挑战性基准测试（AIME 2024、AIME 2025和GPQA）和多个推理模型上的综合评估显示，DeepPrune相比传统共识采样在大多数情况下实现了超过80%的token减少，同时保持竞争性准确率（在3个百分点内）。

Conclusion: DeepPrune为高效并行推理设立了新标准，使高性能推理更加高效。

Abstract: Parallel scaling has emerged as a powerful paradigm to enhance reasoning
capabilities in large language models (LLMs) by generating multiple
Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces
significant computational inefficiency due to inter-trace redundancy -- our
analysis reveals that over 80% of parallel reasoning traces yield identical
final answers, representing substantial wasted computation. To address this
critical efficiency bottleneck, we propose DeepPrune, a novel framework that
enables efficient parallel scaling through dynamic pruning. Our method features
a specialized judge model trained with focal loss and oversampling techniques
to accurately predict answer equivalence from partial reasoning traces which
realizes 0.87 AUROC on equivalence prediction, combined with an online greedy
clustering algorithm that dynamically prunes redundant paths while preserving
answer diversity. Comprehensive evaluations across three challenging benchmarks
(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that
DeepPrune achieves remarkable token reduction by over 80% compared to
conventional consensus sampling on most cases, while maintaining competitive
accuracy within 3 percentage points. Our work establishes a new standard for
efficient parallel reasoning, making high-performance reasoning more efficient.
Our code and data are here: https://deepprune.github.io/

</details>


### [25] [CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards](https://arxiv.org/abs/2510.08529)
*Xiangyuan Xue,Yifan Zhou,Guibin Zhang,Zaibin Zhang,Yijiang Li,Chen Zhang,Zhenfei Yin,Philip Torr,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: CoMAS是一个多智能体协同进化框架，通过智能体间的相互讨论和协作实现自主进化，无需外部监督，在大多数评估设置中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的自进化方法依赖于外部奖励信号或从LLM提取内在奖励，这与人类通过相互讨论和协作进行学习的机制不符。

Method: CoMAS通过多智能体交互生成内在奖励，使用LLM作为评判机制制定奖励，并通过强化学习优化每个智能体的策略，实现去中心化和可扩展的协同进化。

Result: 实验结果显示CoMAS持续优于未经训练的智能体，在大多数评估设置中达到最先进性能，消融研究证实了基于交互的奖励信号的必要性。

Conclusion: CoMAS为基于LLM的智能体自进化提供了一个新颖有效的范式，随着智能体数量和多样性的增加显示出良好的可扩展性。

Abstract: Self-evolution is a central research topic in enabling large language model
(LLM)-based agents to continually improve their capabilities after pretraining.
Recent research has witnessed a transition from reinforcement learning
(RL)-free to RL-based methods. Current RL-based methods either rely on dense
external reward signals or extract intrinsic reward signals from LLMs
themselves. However, these approaches diverge from the self-evolution
mechanisms observed in human intelligence, where individuals learn and improve
through mutual discussion and collaboration. In this work, we introduce
Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents
to improve autonomously by learning from inter-agent interactions without
external supervision. CoMAS generates intrinsic rewards from rich discussion
dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and
optimizes each agent's policy through RL, thereby enabling decentralized and
scalable co-evolution. Experimental results demonstrate that CoMAS consistently
outperforms untrained agents and achieves state-of-the-art performance across
most evaluation settings. Ablation studies confirm the necessity of
interaction-based reward signals and reveal promising scalability as the number
and diversity of agents increase. These findings establish CoMAS as a novel and
effective paradigm for self-evolution in LLM-based agents.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [26] [Model-based vs Model-free：<em class="highlight">强化学习</em>的两种思维方式，哪种更接近人类？](http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516616&idx=1&sn=1b69706baa6a1fb2c50172ee56ebd3c2&chksm=9a4d143f11662e6c5ac6c5285764ed2232b72e3be57b88de263b999853020956746e1dd2741a#rd)
*月来客栈*

Main category: wechat.article

TL;DR: 常见强化学习方法分类总结强化学习的两大阵营其实对应了两种“智能哲学”：Model-based靠想象力驱动，善于规划、预测未来；Model-free 靠经验积累，擅长行动与适应。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 常见强化学习方法分类总结强化学习的两大阵营其实对应了两种“智能哲学”：Model-based靠想象力驱动，善于规划、预测未来；Model-free 靠经验积累，擅长行动与适应。

</details>


### [27] [<em class="highlight">强化学习</em>一锅端：一文梳理从PPO到GSPO<em class="highlight">强化学习</em>发展历程](http://mp.weixin.qq.com/s?__biz=MzkzODUxMTM2MQ==&mid=2247487053&idx=1&sn=f952ec49711be824b9ecf65de6f3f622&chksm=c3ac5a084fe024a98da0a0d696fa2265e00c47e0a4bbf2eb5e203847e5f0a7b3f12ac52be4b3#rd)
*曾天真的算法世界*

Main category: wechat.article

TL;DR: 强化学习一锅端近端策略优化算法 （Proximal Policy Optimization， PPO） ：在强化学习中，我们的目标是找到一个策略（policy），以最大化智能体在环境中执行该策略时所获得的期望累积奖励。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习一锅端近端策略优化算法 （Proximal Policy Optimization， PPO） ：在强化学习中，我们的目标是找到一个策略（policy），以最大化智能体在环境中执行该策略时所获得的期望累积奖励。

</details>


### [28] [模仿学习和<em class="highlight">强化学习</em>](http://mp.weixin.qq.com/s?__biz=MzUyNTY3NjI3OA==&mid=2247484465&idx=1&sn=3f773ccf6d14ec37a51ae65cd41e03a0&chksm=fba6a26877f7a69ba5715e1da69baa69cb41ef202f292549385f1244ee0b5676a913ed5a2e3b#rd)
*老土闲白*

Main category: wechat.article

TL;DR: 强化学习，自己尝试，并通过获得奖励的高低来调整自己的行为，需要有个明确的奖励模型，reward model。相当于写一个答案，老师进行评分；再换一个答案，老师再次评分，最终学习会哪个答案老师评分最高。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习，自己尝试，并通过获得奖励的高低来调整自己的行为，需要有个明确的奖励模型，reward model。相当于写一个答案，老师进行评分；再换一个答案，老师再次评分，最终学习会哪个答案老师评分最高。

</details>


### [29] [超越字节DAPO！快手&清华提出<em class="highlight">强化学习</em>方法ASPO，防止LLM过拟合和熵值塌陷](http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994907&idx=1&sn=20a8dca995a72a33f993c54463e6b75d&chksm=b0e354b1448f9f3be47ffa61f549a7b1eecb129929e6ace9eea15d8e622e90083ba2a443caeb#rd)
*智猩猩GenAI*

Main category: wechat.article

TL;DR: 但其作为结果监督强化学习（OSRL）范式存在一个关键局限性：其token级裁剪机制导致学习权重的错配。这种错配导致模型在训练过程中过度拟合正样本中的 token，进而引发熵值塌陷、输出重复、 token Clip比例急剧升高等现象，最


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 但其作为结果监督强化学习（OSRL）范式存在一个关键局限性：其token级裁剪机制导致学习权重的错配。这种错配导致模型在训练过程中过度拟合正样本中的 token，进而引发熵值塌陷、输出重复、 token Clip比例急剧升高等现象，最

</details>


### [30] [陈丹琦新作：大模型<em class="highlight">强化学习</em>的第三条路，8B小模型超越GPT-4o](http://mp.weixin.qq.com/s?__biz=MzkyOTgxODQyNw==&mid=2247488288&idx=2&sn=6272fef46acde695c994cf30b3d788a8&chksm=c35d2ec1a743d560d409831a489dcc4ffd3d4d3bb9d17f19de984c78d35f1565af9a3bb7d644#rd)
*内蒙古自治区人工智能学会*

Main category: wechat.article

TL;DR: 网友觉得，这种方法为通用强化学习设定了一个新基线：谁制定了偏好的定义，谁就是后训练时代的“新得分手”。jhxhgukvcxx @jhxhgukvcxx · sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 网友觉得，这种方法为通用强化学习设定了一个新基线：谁制定了偏好的定义，谁就是后训练时代的“新得分手”。jhxhgukvcxx @jhxhgukvcxx · sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo

</details>


### [31] [任意Agent皆可<em class="highlight">强化学习</em>！微软推出Agent Lightning框架，无需修改任何代码](http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652633206&idx=3&sn=a44c99b8f6769f5d850f9c3f1df68202&chksm=f0ab1f0a49526d72e29fb84f1b01840fc9f9da3676f694687627d2243ae9698f63e68f4e560d#rd)
*新智元*

Main category: wechat.article

TL;DR: 但现有强化学习训练框架，往往将强化学习训练过程与Agent的具体执行逻辑紧密捆绑，导致一系列问题，严重阻碍了强化学习在AI Agent大规模训练和部署中的应用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 但现有强化学习训练框架，往往将强化学习训练过程与Agent的具体执行逻辑紧密捆绑，导致一系列问题，严重阻碍了强化学习在AI Agent大规模训练和部署中的应用。

</details>


### [32] [CoRL2025最佳论文Finalist: 加州伯克利DSRL——利用潜在空间<em class="highlight">强化学习</em>来指导扩散策略](http://mp.weixin.qq.com/s?__biz=Mzg5MTc3NjQ3NQ==&mid=2247500266&idx=1&sn=8cfacb5fbee23b4986ad04bbe40e4229&chksm=ceb572b7bcedec5d8275eaec9be4fb9f8ad268a986677c5aeef1029626d93b699c725ed87173#rd)
*PnP机器人*

Main category: wechat.article

TL;DR: 方法：通过强化学习进行扩散控制。standard diffusion policy deployment s tdp environment w ~ n（0， i） a dsrl： diffusion steering via reinforcement learning s tdp environment w ~ π （s） a latent-action environment rl trai...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 方法：通过强化学习进行扩散控制。standard diffusion policy deployment s tdp environment w ~ n（0， i） a dsrl： diffusion steering via reinforcement learning s tdp environment w ~ π （s） a latent-action environment rl training。

</details>


### [33] [LLM 与<em class="highlight">强化学习</em>的新范式：Agentic RL 研究综述](http://mp.weixin.qq.com/s?__biz=MzIyMjgwNjQ3MQ==&mid=2247484276&idx=1&sn=276da3b795b6b00b33c488f0c617d5a0&chksm=e92e0274ecddc66127be02ba1347c7636d7b640785884815992c9e06d5c0fbbe2ee9f180ace8#rd)
*程序搬运工*

Main category: wechat.article

TL;DR: 大模型 （LLM） 与强化学习 （RL） 的新范式：Agentic RL 研究综述引言本文旨在解读并整理一篇关于大模型 （LLM） 领域备受关注的研究——“基于 LLM 的智能体强化学习 （Agentic Reinforcement Learning， Agentic RL）概览...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型 （LLM） 与强化学习 （RL） 的新范式：Agentic RL 研究综述引言本文旨在解读并整理一篇关于大模型 （LLM） 领域备受关注的研究——“基于 LLM 的智能体强化学习 （Agentic Reinforcement Learning， Agentic RL）概览...

</details>


### [34] [DRL圣经2025最新版-《<em class="highlight">强化学习</em>:导论第二版》免费pdf分享](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571067&idx=2&sn=aa1dabe6550150a5b1fc7adcf302255a&chksm=96a225fa7328aef5d90cce8dd361ffb7b35a9709d2bec9afaa981351e89d80eb5632c62c1913#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 我们第二版的目标和第一版的目标是一样的：为强化学习的关键思想和算法提供一个清晰而简单的描述，供所有相关学科的读者阅读。该版本仍然是一个介绍，我们保留了核心，在线学习算法的重点。

</details>


### [35] [逐际动力 | Multi-Loco框架：一个策略，四种机器人！基于<em class="highlight">强化学习</em>的多形态足式机器人运动统一控制框架](http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488788&idx=1&sn=722b15365749411b176ec17f6e2006c6&chksm=c1e0c5c78466eff7318488b5d7c12f27c11e8447ac8bde3f73d1f464a23a90cb57fa2ca3333a#rd)
*具身智能研究室*

Main category: wechat.article

TL;DR: 3 残差强化学习模块：利用 RL 在仿真环境中训练残差补偿项 Δa，在扩散输出的基础上修正具体行为，实现形态级自适应。算法框架详解基本思想Multi-Loco 的核心思想可以概括为一句话：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 3 残差强化学习模块：利用 RL 在仿真环境中训练残差补偿项 Δa，在扩散输出的基础上修正具体行为，实现形态级自适应。算法框架详解基本思想Multi-Loco 的核心思想可以概括为一句话：

</details>


### [36] [LLM 与<em class="highlight">强化学习</em>的新范式：Agentic RL 研究综述](http://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247495637&idx=1&sn=88683416f2fc0bf2c9f1f4bdf1cb9045&chksm=9a7cbe3eebe10d4e52fcbff7bcbec882ce4c6cc7cf46de2d79d64750fbd27416056160214bc3#rd)
*ChallengeHub*

Main category: wechat.article

TL;DR: 推理能力的提升早期，强化学习主要应用于 LLM 的偏好微调。然而，2024 年 9 月，OpenAI 发布了首个推理模型——OpenAI o1。根据系统卡片 [4] 报告，o1 通过强化学习提升了其深思熟虑并得出答案的能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 推理能力的提升早期，强化学习主要应用于 LLM 的偏好微调。然而，2024 年 9 月，OpenAI 发布了首个推理模型——OpenAI o1。根据系统卡片 [4] 报告，o1 通过强化学习提升了其深思熟虑并得出答案的能力。

</details>


### [37] [浅谈<em class="highlight">大模型</em>发展现状](http://mp.weixin.qq.com/s?__biz=MzE5MTM5MTQ2NA==&mid=2247484433&idx=1&sn=5e5672d8cbfe99bbc4d51fa88218edba&chksm=97db09c70cebea69eb575fd035fa62ac7369f924bbe0ec06009e05e566abd9dbd12a20a8a6f6#rd)
*HiTech X*

Main category: wechat.article

TL;DR: 现在大模型最主要的应用场景依然是Chatbot，今年大家看到新的增长点是AI Coding，依我看AI Coding潜力会比Chatbot更大。它不仅能极大提高开发者的效率，还能催生全新的软件开发模式。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 现在大模型最主要的应用场景依然是Chatbot，今年大家看到新的增长点是AI Coding，依我看AI Coding潜力会比Chatbot更大。它不仅能极大提高开发者的效率，还能催生全新的软件开发模式。

</details>


### [38] [vivo 的 AI 破局之道：给每个用户发一个「专属」<em class="highlight">大模型</em>](http://mp.weixin.qq.com/s?__biz=MTM2ODM0ODYyMQ==&mid=2651737874&idx=1&sn=efd4657f18c6066da8c5496996205e9d&chksm=631ea24a2d1b23e49d46564f465c692034899387f91e31be18a9b5e2f5f5d8f4c1bf4a6a7f55#rd)
*雷峰网*

Main category: wechat.article

TL;DR: vivo 3b端侧多模态推理大模型 vc 同心·同行 2025 vivo 开发者大会。当然，评判大模型的能力有非常客观的标准，只有经受住专门的考验，实力才能得到认可。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: vivo 3b端侧多模态推理大模型 vc 同心·同行 2025 vivo 开发者大会。当然，评判大模型的能力有非常客观的标准，只有经受住专门的考验，实力才能得到认可。

</details>


### [39] [LLM+Agent：字节<em class="highlight">大模型</em>落地方法论！](http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771137&idx=1&sn=394470fae78d8c5223dfcab1a6b077ca&chksm=fa97b4cfb9f892c949833289e6937e610cba007a9586d78896c58f582da8069f58256ee912bf#rd)
*DataFunTalk*

Main category: wechat.article

TL;DR: 本次大会聚焦Agent在企业级场景落地实践，深度解析深入解读豆包大模型前沿技术进展，更有标杆企业的大模型“可复制”落地经验分享，感兴趣的小伙伴，欢迎识别二维码，免费预约直播/线下参会：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 本次大会聚焦Agent在企业级场景落地实践，深度解析深入解读豆包大模型前沿技术进展，更有标杆企业的大模型“可复制”落地经验分享，感兴趣的小伙伴，欢迎识别二维码，免费预约直播/线下参会：

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [40] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure使用大型语言模型自动将C代码转换为Rust代码，并通过差分符号测试验证语义等价性，实现89.8%的C函数可编译转换，其中69.9%的函数在符号返回值上等价。


<details>
  <summary>Details</summary>
Motivation: Rust作为内存安全语言能显著提升软件安全性，但现有用不安全内存语言（如C）编写的代码库需要先转换为Rust才能利用其安全优势。

Method: 使用提示工程技术优化LLM生成惯用安全的Rust代码，并通过差分符号测试验证原始C代码和LLM转换的Rust代码之间的语义相似性。

Result: 在5个真实应用和库的评估中，系统能为89.8%的C函数生成可编译的Rust函数，其中69.9%的函数在符号返回值上等价。

Conclusion: RustAssure系统能够有效自动将C代码转换为Rust代码，并通过符号测试验证转换的正确性。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [41] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: APPFORGE是一个包含101个真实Android应用开发问题的基准测试，用于评估LLM从零构建完整软件系统的能力。测试结果显示当前最佳模型(GPT-5)仅能开发18.8%功能正确的应用，表明现有模型在处理复杂多组件软件工程挑战方面存在根本性局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估LLM在函数级代码生成任务中的表现，但无法评估其构建完整软件系统的能力。真实应用开发需要协调不同组件交互、维护状态一致性、确保在生命周期和框架约束下的正确行为。

Method: 设计多智能体系统自动从应用文档中总结主要功能，并通过导航应用合成测试用例验证功能正确性。经过Android开发专家严格手动验证后，将测试用例整合到自动化评估框架中。

Result: 对12个主流LLM的评估显示，所有模型效果都很低，最佳模型GPT-5仅能开发18.8%功能正确的应用。

Conclusion: 当前LLM在应对复杂、多组件的软件工程挑战方面存在根本性局限，无法有效构建完整的软件系统。

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [42] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: 提出基于LLM的AI驱动bug追踪框架，通过智能自动化减少修复时间和人工开销，实现从自然语言报告到自动修复的全流程优化


<details>
  <summary>Details</summary>
Motivation: 传统bug追踪系统依赖人工报告、复现、分类和修复，存在沟通效率低、响应延迟等问题，需要AI自动化来提升效率和用户体验

Method: 构建AI驱动的bug追踪框架，使用LLM进行报告精炼、自动复现、分类、无效报告无代码修复、问题定位、自动分配和候选补丁生成

Result: 该框架能够加速响应时间，改善协作效率，减少人工干预，提升软件维护实践的效率

Conclusion: AI驱动的bug追踪框架通过集成自动化到每个阶段，为更高效、以用户为中心的未来软件维护提供了可行方案

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts](https://arxiv.org/abs/2510.07358)
*Yeskendir Koishekenov,Aldo Lipani,Nicola Cancedda*

Main category: cs.LG

TL;DR: ETD方法通过在推理时迭代关键层来增强语言模型的推理能力，无需改变模型架构或参数数量，在多个推理基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有提升LLM推理能力的方法通常需要扩大模型规模或增加推理计算量，而研究表明推理所需的关键计算集中在有限层中，因此探索更高效的方法。

Method: Encode-Think-Decode (ETD)方法：在中期训练阶段训练模型在推理相关层上进行迭代，推理时在选定层上迭代，同时探索自适应深度策略。

Result: 在17个推理基准上取得显著提升，OLMo-2 1B模型在GSM8K上相对准确率提升28.4%，在MATH上提升36%。

Conclusion: 递归潜在推理为增强LLM推理能力提供了一条简单有效的路径。

Abstract: Most efforts to improve the reasoning capabilities of large language models
(LLMs) involve either scaling the number of parameters and the size of training
data, or scaling inference computation by letting models generate complex
chains of thought. Motivated by interpretability studies showing that the
crucial computation required for reasoning tasks is concentrated in a limited
range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances
the reasoning capabilities of a base model by training it to iterate over a
small subset of reasoning-relevant layers during the mid-training stage. ETD
amplifies latent reasoning while preserving the original architecture,
parameter count, hyperparameters, and training data composition. When iterating
on the selected layers at inference time, ETD models yield substantial gains on
17 reasoning benchmarks, including +28.4% relative accuracy improvement on
GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an
adaptive depth strategy that adjusts the computation per input token. Our
results show that recursive latent reasoning offers a simple and effective path
to stronger LLM reasoning.

</details>


### [44] [Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs](https://arxiv.org/abs/2510.07429)
*Wang Wei,Tiankai Yang,Hongjie Chen,Yue Zhao,Franck Dernoncourt,Ryan A. Rossi,Hoda Eldardiry*

Main category: cs.LG

TL;DR: BaRP是一种基于bandit反馈的路由方法，能够在训练时模拟在线反馈设置，支持在测试时无需重新训练即可调整性能/成本权衡偏好。


<details>
  <summary>Details</summary>
Motivation: 大规模部署LLM需要高效的路由系统，但现有路由器大多基于离线训练，无法适应部署时的部分反馈限制和动态变化。

Method: 将LLM路由建模为上下文bandit问题，基于提示特征和用户偏好向量进行决策，在训练时模拟在线反馈环境。

Result: 实验表明该方法比强离线路由器性能提升至少12.46%，比最大LLM提升至少2.45%，并能很好地泛化到未见任务。

Conclusion: BaRP方法在部分反馈限制下有效解决了LLM路由问题，支持灵活的偏好调整，具有实际部署价值。

Abstract: Efficient use of large language models (LLMs) is critical for deployment at
scale: without adaptive routing, systems either overpay for strong models or
risk poor performance from weaker ones. Selecting the right LLM for each query
is fundamentally an online decision problem: models differ in strengths, prices
fluctuate, and users value accuracy and cost differently. Yet most routers are
trained offline with labels for all candidate models, an assumption that breaks
in deployment, where only the outcome of the chosen model is observed. We
bridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach
that trains under the same partial-feedback restriction as deployment, while
supporting preference-tunable inference: operators can dial the
performance/cost trade-off at test time without retraining. Framed as a
contextual bandit over prompt features and a user preference vector, our method
simulates an online feedback setting during training and adapts its routing
decisions to each new prompt, rather than depending on full-information offline
supervision. Comprehensive experiments show that our method consistently
outperforms strong offline routers by at least 12.46% and the largest LLM by at
least 2.45%, and generalizes robustly for unseen tasks.

</details>


### [45] [PEAR: Planner-Executor Agent Robustness Benchmark](https://arxiv.org/abs/2510.07505)
*Shen Dong,Mingxuan Zhang,Pengfei He,Li Ma,Bhavani Thuraisingham,Hui Liu,Yue Xing*

Main category: cs.LG

TL;DR: PEAR是一个用于系统评估规划器-执行器多智能体系统效用和脆弱性的基准测试，发现规划器比执行器更关键，存在性能与鲁棒性权衡，针对规划器的攻击特别有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常只关注孤立的攻击面或特定场景，缺乏对多智能体系统漏洞的整体理解，需要系统性的评估基准。

Method: 引入PEAR基准测试，专注于广泛采用的规划器-执行器结构，通过大量实验评估不同配置下的系统表现。

Result: 发现：弱规划器比弱执行器更严重影响性能；规划器需要记忆模块而执行器不需要；存在性能与鲁棒性权衡；针对规划器的攻击特别有效。

Conclusion: 这些发现为增强多智能体系统鲁棒性提供了可操作的见解，并为多智能体环境中的原则性防御奠定了基础。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a
powerful paradigm for tackling complex, multi-step tasks across diverse
domains. However, despite their impressive capabilities, MAS remain susceptible
to adversarial manipulation. Existing studies typically examine isolated attack
surfaces or specific scenarios, leaving a lack of holistic understanding of MAS
vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for
systematically evaluating both the utility and vulnerability of
planner-executor MAS. While compatible with various MAS architectures, our
benchmark focuses on the planner-executor structure, which is a practical and
widely adopted design. Through extensive experiments, we find that (1) a weak
planner degrades overall clean task performance more severely than a weak
executor; (2) while a memory module is essential for the planner, having a
memory module for the executor does not impact the clean task performance; (3)
there exists a trade-off between task performance and robustness; and (4)
attacks targeting the planner are particularly effective at misleading the
system. These findings offer actionable insights for enhancing the robustness
of MAS and lay the groundwork for principled defenses in multi-agent settings.

</details>


### [46] [Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic](https://arxiv.org/abs/2510.07557)
*Abhay Bhandarkar,Gaurav Mishra,Khushi Juchani,Harsh Singhal*

Main category: cs.LG

TL;DR: 使用BERTopic对lmsys-chat-1m数据集进行主题建模，分析多语言对话中的主题模式及其与用户偏好的关系，发现特定LLM在不同主题中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示大规模语言模型对话中的主题模式，并探究这些主题与用户偏好之间的关系，特别是某些LLM是否在特定主题中更受青睐。

Method: 设计了多语言预处理流程，使用BERTopic提取了29个连贯主题，通过可视化技术（主题距离图、概率分布、模型-主题矩阵）分析主题与模型偏好的关系。

Result: 成功识别出包括人工智能、编程、伦理和云基础设施在内的29个连贯主题，并发现了模型在不同主题中的偏好模式。

Conclusion: 研究结果为领域特定的微调和优化策略提供了依据，有助于提升实际应用中LLM的性能和用户满意度。

Abstract: This study applies BERTopic, a transformer-based topic modeling technique, to
the lmsys-chat-1m dataset, a multilingual conversational corpus built from
head-to-head evaluations of large language models (LLMs). Each user prompt is
paired with two anonymized LLM responses and a human preference label, used to
assess user evaluation of competing model outputs. The main objective is
uncovering thematic patterns in these conversations and examining their
relation to user preferences, particularly if certain LLMs are consistently
preferred within specific topics. A robust preprocessing pipeline was designed
for multilingual variation, balancing dialogue turns, and cleaning noisy or
redacted data. BERTopic extracted over 29 coherent topics including artificial
intelligence, programming, ethics, and cloud infrastructure. We analysed
relationships between topics and model preferences to identify trends in
model-topic alignment. Visualization techniques included inter-topic distance
maps, topic probability distributions, and model-versus-topic matrices. Our
findings inform domain-specific fine-tuning and optimization strategies for
improving real-world LLM performance and user satisfaction.

</details>


### [47] [Expanding the Action Space of LLMs to Reason Beyond Language](https://arxiv.org/abs/2510.07581)
*Zhongqi Yue,Weishi Wang,Yundaichuan Zhan,Juncheng Li,Daniel Dahlmeier,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 该论文提出ExpA（扩展动作空间）方法，将环境交互从语言中解耦，让LLM能够在语言环境和外部环境之间切换，并引入EARL（ExpA强化学习）来优化在扩展动作空间中的探索。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的环境交互需要通过文本表达和解析，这给模型语言带来了推理和控制的双重负担，且需要手工设计的解析器。

Method: 通过内部化环境交互到扩展动作空间（ExpA），让模型能够在语言环境和外部环境之间路由切换，并引入EARL强化学习进行策略优化。

Result: 在需要多轮交互和条件规划的任务上，EARL优于词汇约束动作的基线方法，在计算器多任务学习中表现稳健，在部分观察排序问题中达到100%的Sort-4准确率。

Conclusion: ExpA方法有效解耦了环境交互与语言推理，EARL强化学习能够促进在扩展动作空间中的有效探索。

Abstract: Large Language Models (LLMs) are powerful reasoners in natural language, but
their actions are typically confined to outputting vocabulary tokens. As a
result, interactions with external environments -- such as symbolic operators
or simulators -- must be expressed through text in predefined formats, parsed,
and routed to external interfaces. This overloads the model's language with
both reasoning and control duties, and requires a hand-crafted parser, external
to the LLM. To address this, we decouple environment interactions from language
by internalizing them in an Expanded Action space (ExpA), beyond the
vocabulary. The model starts reasoning in the default language environment, but
may trigger routing actions and switch to an external environment at any time.
From there, the model can only invoke environment-specific actions, receive
feedback from the environment, and potentially route back to language as a
result. To promote effective exploration of the expanded action space and new
environments, we introduce ExpA Reinforcement Learning (EARL) with
counterfactual policy optimization. On tasks requiring multi-turn interactions
and contingent planning, EARL outperforms strong baselines with
vocabulary-constrained actions. It performs robustly across calculator-based
multi-task learning and, in the partially observed sorting problem, achieves
perfect Sort-4 accuracy while self-discovering an efficient algorithm
competitive with classical designs.

</details>


### [48] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: 本文对大型语言模型(LLM)的遗忘方法进行了系统分类和评估，提出了更全面的评估指标，揭示了遗忘效果与模型效用之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 当前LLM遗忘研究缺乏清晰的评估标准，现有评估方法过于依赖选择题准确率，无法全面反映模型的实际生成行为，需要更系统的评估框架。

Method: 将12种状态遗忘方法分为三类：差异驱动优化、表示错位和基于拒绝的目标遗忘；引入开放式问答评估指标，分析遗忘效果、效用保留和鲁棒性。

Result: 发现当前评估方法高估了遗忘效果，开放式问答指标能更好捕捉生成性能；揭示了不同方法家族在遗忘效果与效用保留之间的权衡；证明了鲁棒性需要更细粒度分析。

Conclusion: 提出了LLM遗忘的全面评估框架，为未来方法的设计和评估提供了实用指导，强调需要更全面的评估指标来反映实际生成行为。

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [49] [Value Flows](https://arxiv.org/abs/2510.07650)
*Perry Dong,Chongyi Zheng,Chelsea Finn,Dorsa Sadigh,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出Value Flows方法，使用基于流的模型估计完整未来回报分布，通过流匹配目标满足分布贝尔曼方程，并利用流导数ODE估计状态回报不确定性，在37个状态基准和25个图像基准任务中平均成功率提升1.3倍


<details>
  <summary>Details</summary>
Motivation: 现有分布强化学习方法主要使用离散分箱或有限分位数来建模回报分布，无法捕捉分布的细粒度结构，也难以区分具有高回报不确定性的状态进行决策

Method: 使用基于流的灵活模型估计完整未来回报分布，提出新的流匹配目标生成满足分布贝尔曼方程的概率密度路径，基于学习到的流模型使用新的流导数ODE估计状态回报不确定性，并利用不确定性信息优先学习更准确的回报估计

Result: 在37个状态基准和25个图像基准任务上，Value Flows方法相比先前方法平均成功率提升1.3倍

Conclusion: 基于流的分布强化学习方法能够更准确地建模回报分布，有效识别高不确定性状态，并在离线学习和在线到在线设置中显著提升性能

Abstract: While most reinforcement learning methods today flatten the distribution of
future returns to a single scalar value, distributional RL methods exploit the
return distribution to provide stronger learning signals and to enable
applications in exploration and safe RL. While the predominant method for
estimating the return distribution is by modeling it as a categorical
distribution over discrete bins or estimating a finite number of quantiles,
such approaches leave unanswered questions about the fine-grained structure of
the return distribution and about how to distinguish states with high return
uncertainty for decision-making. The key idea in this paper is to use modern,
flexible flow-based models to estimate the full future return distributions and
identify those states with high return variance. We do so by formulating a new
flow-matching objective that generates probability density paths satisfying the
distributional Bellman equation. Building upon the learned flow models, we
estimate the return uncertainty of distinct states using a new flow derivative
ODE. We additionally use this uncertainty information to prioritize learning a
more accurate return estimation on certain transitions. We compare our method
(Value Flows) with prior methods in the offline and online-to-online settings.
Experiments on $37$ state-based and $25$ image-based benchmark tasks
demonstrate that Value Flows achieves a $1.3\times$ improvement on average in
success rates. Website: https://pd-perry.github.io/value-flows Code:
https://github.com/chongyi-zheng/value-flows

</details>


### [50] [DEAS: DEtached value learning with Action Sequence for Scalable Offline RL](https://arxiv.org/abs/2510.07730)
*Changyeon Kim,Haeone Lee,Younggyo Seo,Kimin Lee,Yuke Zhu*

Main category: cs.LG

TL;DR: 提出了DEAS离线强化学习框架，利用动作序列进行价值学习，通过分离价值学习减少价值高估，在复杂长时程任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 离线强化学习无需昂贵在线交互，但现有方法在复杂长时程序列决策中仍存在困难

Method: DEAS框架利用动作序列进行价值学习，通过半马尔可夫决策过程Q学习解释动作序列，采用分离价值学习减少价值高估

Result: 在OGBench复杂长时程任务中持续超越基线方法，显著提升大规模视觉-语言-动作模型在RoboCasa Kitchen仿真和真实世界操作任务中的性能

Conclusion: DEAS是一个简单有效的离线RL框架，通过动作序列和分离价值学习成功解决了复杂长时程任务中的挑战

Abstract: Offline reinforcement learning (RL) presents an attractive paradigm for
training intelligent agents without expensive online interactions. However,
current approaches still struggle with complex, long-horizon sequential
decision making. In this work, we introduce DEtached value learning with Action
Sequence (DEAS), a simple yet effective offline RL framework that leverages
action sequences for value learning. These temporally extended actions provide
richer information than single-step actions and can be interpreted through the
options framework via semi-Markov decision process Q-learning, enabling
reduction of the effective planning horizon by considering longer sequences at
once. However, directly adopting such sequences in actor-critic algorithms
introduces excessive value overestimation, which we address through detached
value learning that steers value estimates toward in-distribution actions that
achieve high return in the offline dataset. We demonstrate that DEAS
consistently outperforms baselines on complex, long-horizon tasks from OGBench
and can be applied to enhance the performance of large-scale
Vision-Language-Action models that predict action sequences, significantly
boosting performance in both RoboCasa Kitchen simulation tasks and real-world
manipulation tasks.

</details>


### [51] [Self-Improving LLM Agents at Test-Time](https://arxiv.org/abs/2510.07841)
*Emre Can Acikgoz,Cheng Qian,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: 提出了一种测试时自改进方法，通过识别模型不确定样本、生成类似示例并进行测试时微调，显著提升智能体语言模型的性能，使用更少训练样本实现更好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型微调依赖大规模训练数据，效率低、成本高且不能保证泛化能力。现有技术很少评估训练样本是否提供新信息，导致不必要成本。

Method: 三步骤算法：1) 识别模型不确定样本（自我意识）；2) 从不确定样本生成类似示例（自数据增强）；3) 在测试时使用新生成样本进行微调（自改进）。研究两种变体：TT-SI（同模型自生成）和TT-D（强模型生成）。

Result: TT-SI在所有基准测试中平均绝对准确率提升5.48%，优于其他标准学习方法，且仅使用1/68的训练样本。

Conclusion: 测试时自改进算法展示了构建更强大智能体实现自我进化的新范式潜力。

Abstract: One paradigm of language model (LM) fine-tuning relies on creating large
training datasets, under the assumption that high quantity and diversity will
enable models to generalize to novel tasks after post-training. In practice,
gathering large sets of data is inefficient, and training on them is
prohibitively expensive; worse, there is no guarantee that the resulting model
will handle complex scenarios or generalize better. Moreover, existing
techniques rarely assess whether a training sample provides novel information
or is redundant with the knowledge already acquired by the model, resulting in
unnecessary costs. In this work, we explore a new test-time self-improvement
method to create more effective and generalizable agentic LMs on-the-fly. The
proposed algorithm can be summarized in three steps: (i) first it identifies
the samples that model struggles with (self-awareness), (ii) then generates
similar examples from detected uncertain samples (self-data augmentation), and
(iii) uses these newly generated samples at test-time fine-tuning
(self-improvement). We study two variants of this approach: Test-Time
Self-Improvement (TT-SI), where the same model generates additional training
examples from its own uncertain cases and then learns from them, and contrast
this approach with Test-Time Distillation (TT-D), where a stronger model
generates similar examples for uncertain cases, enabling student to adapt using
distilled supervision. Empirical evaluations across different agent benchmarks
demonstrate that TT-SI improves the performance with +5.48% absolute accuracy
gain on average across all benchmarks and surpasses other standard learning
methods, yet using 68x less training samples. Our findings highlight the
promise of TT-SI, demonstrating the potential of self-improvement algorithms at
test-time as a new paradigm for building more capable agents toward
self-evolution.

</details>


### [52] [Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning](https://arxiv.org/abs/2510.08141)
*Chen Wang,Zhaochun Li,Jionghao Bai,Yuzhi Zhang,Shisheng Cui,Zhou Zhao,Yue Wang*

Main category: cs.LG

TL;DR: 提出了AEPO方法解决强化学习微调中的熵崩溃问题，通过温度调节实现精确的熵控制，揭示了熵与性能的非单调关系


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法存在熵崩溃问题，导致探索消失和策略过早收敛，而现有的熵正则化方法只能部分缓解问题且引入偏差和不稳定性

Method: 提出AEPO方法，用温度调整分布的REINFORCE策略梯度替代熵奖励，通过温度调节稳定熵，包含三个关键设计：策略梯度正则化、分布正则化和REINFORCE正则化

Result: AEPO能够稳定熵在任意目标水平，有效消除GRPO中的熵崩溃；揭示了熵与性能的非单调关系；提供了更广泛的RFT范式

Conclusion: AEPO解决了熵控制问题，阐明了熵、探索和推理性能之间的关系，为强化学习微调提供了更通用的框架

Abstract: Reinforcement finetuning (RFT) is essential for enhancing the reasoning
capabilities of large language models (LLM), yet the widely adopted Group
Relative Policy Optimization (GRPO) suffers from entropy collapse, where
entropy monotonically decreases, exploration vanishes, and policies converge
prematurely. Existing entropy-regularized methods only partially alleviate this
issue while introducing bias and instability, leaving entropy control
unresolved and the connection between entropy, exploration, and performance
unclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which
eliminates entropy collapse by replacing entropy bonuses with REINFORCE policy
gradient on temperature-adjusted distributions and stabilizing entropy through
temperature regulation. AEPO integrates three key designs: policy gradient as
regularization, distribution as regularization, and REINFORCE as
regularization, enabling precise entropy control without distorting
optimization. Experiments demonstrate three major contributions: AEPO (1)
stabilizes entropy at arbitrary target levels, effectively removing collapse in
GRPO; (2) reveals a non-monotonic relation where performance first improves
then declines with increasing entropy, clarifying the link between entropy,
exploration, and reasoning; and (3) generalizes beyond entropy, providing a
broader RFT paradigm where superior target distributions can serve as REINFORCE
regularizers.

</details>


### [53] [Expressive Value Learning for Scalable Offline Reinforcement Learning](https://arxiv.org/abs/2510.08218)
*Nicolas Espinosa-Dice,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: 提出了EVOR方法，一种无需蒸馏或BPTT的可扩展离线强化学习方法，通过流匹配学习正则化Q函数，在推理时通过拒绝采样进行策略提取。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习在机器人领域应用的可扩展性问题，避免现有方法依赖计算昂贵的BPTT或引入复合误差的策略蒸馏。

Method: EVOR方法结合表达性策略和表达性价值函数，通过流匹配训练正则化Q函数，推理时使用拒绝采样进行策略提取。

Result: 在多样化离线RL任务上超越基线方法，展示了表达性价值学习在离线RL中的优势。

Conclusion: EVOR为离线RL提供了一种可扩展的解决方案，无需依赖蒸馏或BPTT，实现了高效优化和计算可扩展搜索。

Abstract: Reinforcement learning (RL) is a powerful paradigm for learning to make
sequences of decisions. However, RL has yet to be fully leveraged in robotics,
principally due to its lack of scalability. Offline RL offers a promising
avenue by training agents on large, diverse datasets, avoiding the costly
real-world interactions of online RL. Scaling offline RL to increasingly
complex datasets requires expressive generative models such as diffusion and
flow matching. However, existing methods typically depend on either
backpropagation through time (BPTT), which is computationally prohibitive, or
policy distillation, which introduces compounding errors and limits scalability
to larger base policies. In this paper, we consider the question of how to
develop a scalable offline RL approach without relying on distillation or
backpropagation through time. We introduce Expressive Value Learning for
Offline Reinforcement Learning (EVOR): a scalable offline RL approach that
integrates both expressive policies and expressive value functions. EVOR learns
an optimal, regularized Q-function via flow matching during training. At
inference-time, EVOR performs inference-time policy extraction via rejection
sampling against the expressive value function, enabling efficient
optimization, regularization, and compute-scalable search without retraining.
Empirically, we show that EVOR outperforms baselines on a diverse set of
offline RL tasks, demonstrating the benefit of integrating expressive value
learning into offline RL.

</details>


### [54] [Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning](https://arxiv.org/abs/2510.08226)
*Michal Koren,Or Peretz,Tai Dinh,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了不确定性感知马尔可夫决策过程(UAMDP)，一个结合贝叶斯预测、后验采样强化学习和条件风险价值约束的统一框架，用于在波动环境中进行更安全的顺序决策。


<details>
  <summary>Details</summary>
Motivation: 在波动性高、风险大的顺序决策环境中，仅最大化期望收益是不够的，需要系统性的不确定性管理方法。

Method: UAMDP框架在闭环中结合贝叶斯动态建模、汤普森采样生成未来情景，并在预设风险容忍度下优化策略。

Result: 在高频股票交易和零售库存控制两个领域，UAMDP相比深度学习基线显著改善了长期预测准确性（RMSE降低25%，sMAPE降低32%），交易夏普比率从1.54提升至1.74，最大回撤减半。

Conclusion: 整合校准的概率建模、与后验不确定性对齐的探索以及风险感知控制，为更安全、更有利的顺序决策提供了稳健且可泛化的方法。

Abstract: Sequential decisions in volatile, high-stakes settings require more than
maximizing expected return; they require principled uncertainty management.
This paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a
unified framework that couples Bayesian forecasting, posterior-sampling
reinforcement learning, and planning under a conditional value-at-risk (CVaR)
constraint. In a closed loop, the agent updates its beliefs over latent
dynamics, samples plausible futures via Thompson sampling, and optimizes
policies subject to preset risk tolerances. We establish regret bounds that
converge to the Bayes-optimal benchmark under standard regularity conditions.
We evaluate UAMDP in two domains-high-frequency equity trading and retail
inventory control-both marked by structural uncertainty and economic
volatility. Relative to strong deep learning baselines, UAMDP improves
long-horizon forecasting accuracy (RMSE decreases by up to 25\% and sMAPE by
32\%), and these gains translate into economic performance: the trading Sharpe
ratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These
results show that integrating calibrated probabilistic modeling, exploration
aligned with posterior uncertainty, and risk-aware control yields a robust,
generalizable approach to safer and more profitable sequential decision-making.

</details>


### [55] [Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization](https://arxiv.org/abs/2510.08233)
*Yuchen Zhu,Wei Guo,Jaemoo Choi,Petr Molodyk,Bo Yuan,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出了DMPO方法，一种专门为扩散大语言模型设计的强化学习微调方法，通过分布匹配提升推理能力，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)相比自回归大语言模型(AR-LLMs)具有更高的推理吞吐量潜力，但需要专门的强化学习算法来实现可比的性能，特别是在推理任务上。

Method: 提出了分布匹配策略优化(DMPO)，通过交叉熵优化将dLLM策略分布与最优奖励倾斜分布匹配，并针对小批量训练挑战提出了权重基线减法技术。

Result: 在多个推理基准测试中表现出色，无需监督微调即可实现高达42.9%的准确率提升（相比SOTA基线）和55.8%的提升（相比基础模型）。

Conclusion: 分布匹配框架有效提升了dLLMs的推理能力，DMPO方法为dLLMs的强化学习微调提供了理论基础和实践方案。

Abstract: Diffusion large language models (dLLMs) are promising alternatives to
autoregressive large language models (AR-LLMs), as they potentially allow
higher inference throughput. Reinforcement learning (RL) is a crucial component
for dLLMs to achieve comparable performance with AR-LLMs on important tasks,
such as reasoning. However, RL algorithms that are well-suited for dLLMs'
unique characteristics have yet to be developed. This paper proposes
Distribution Matching Policy Optimization (DMPO), a principled and
theoretically grounded RL fine-tuning method specifically designed to enhance
the reasoning capabilities of dLLMs by matching the dLLM policy distribution to
the optimal, reward-tilted one through cross-entropy optimization. We identify
a key challenge in the implementation with a small training batch size and
propose several effective solutions through a novel weight baseline subtraction
technique. DMPO exhibits superior performance on multiple reasoning benchmarks
without supervised fine-tuning, with an accuracy improvement of up to $42.9\%$
over previously SOTA baselines and $55.8\%$ over the base model, underscoring
the effectiveness of the distribution matching framework. Our code is available
at https://github.com/yuchen-zhu-zyc/DMPO.

</details>


### [56] [Opponent Shaping in LLM Agents](https://arxiv.org/abs/2510.08255)
*Marta Emili Garcia Segura,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: 本文首次研究了基于LLM的智能体中的对手塑造问题，提出了ShapeLLM方法，使LLM智能体能够通过交互影响其他智能体的学习动态，在竞争性和合作性游戏中都取得了成功。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实世界中的部署增加，多智能体交互不可避免，需要理解这些系统中的战略行为。核心问题是LLM智能体能否像强化学习智能体一样，仅通过交互来塑造学习动态并影响其他智能体的行为。

Method: 提出了ShapeLLM方法，这是针对基于transformer的智能体量身定制的无模型对手塑造方法的适配版本，解决了现有OS算法无法直接应用于LLM的问题。

Result: 在多种博弈论环境中，LLM智能体能够成功引导对手在竞争性游戏中走向可被利用的均衡（如迭代囚徒困境、匹配硬币和胆小鬼游戏），并在合作性游戏中促进协调和改善集体福利（如迭代猎鹿游戏和合作版囚徒困境）。

Conclusion: LLM智能体能够通过交互相互塑造，确立了对手塑造作为多智能体LLM研究的关键维度。

Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous
agents in real-world environments. As these deployments scale, multi-agent
interactions become inevitable, making it essential to understand strategic
behavior in such systems. A central open question is whether LLM agents, like
reinforcement learning agents, can shape the learning dynamics and influence
the behavior of others through interaction alone. In this paper, we present the
first investigation of opponent shaping (OS) with LLM-based agents. Existing OS
algorithms cannot be directly applied to LLMs, as they require higher-order
derivatives, face scalability constraints, or depend on architectural
components that are absent in transformers. To address this gap, we introduce
ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based
agents. Using ShapeLLM, we examine whether LLM agents can influence co-players'
learning dynamics across diverse game-theoretic environments. We demonstrate
that LLM agents can successfully guide opponents toward exploitable equilibria
in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and
Chicken) and promote coordination and improve collective welfare in cooperative
games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).
Our findings show that LLM agents can both shape and be shaped through
interaction, establishing opponent shaping as a key dimension of multi-agent
LLM research.

</details>


### [57] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: 提出了Mix-和MoE-DPO框架，通过软混合模型和专家混合架构扩展DPO，使用随机变分推理方法解决传统DPO在异构偏好分布下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法依赖单一模型，限制了在多任务设置中的表达能力以及对异构偏好分布的适应性。

Method: 引入潜在变量模型和变分证据下界优化，支持共享基础架构和独立专家模型，实现专业化策略学习。

Result: 在多种模型大小和多偏好数据集上验证了方法的有效性，展示了比标准DPO更好的泛化能力和专业化性能。

Conclusion: Mix-和MoE-DPO为基于偏好的LLM对齐提供了强大且可扩展的方法。

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [58] [Reinforcing Diffusion Models by Direct Group Preference Optimization](https://arxiv.org/abs/2510.08425)
*Yihong Luo,Tianyang Hu,Jing Tang*

Main category: cs.LG

TL;DR: 提出DGPO算法，直接基于组级偏好进行强化学习，无需策略梯度框架，允许使用高效的确定性ODE采样器，训练速度比现有方法快20倍。


<details>
  <summary>Details</summary>
Motivation: 虽然GRPO等强化学习方法显著提升了大型语言模型，但将其应用于扩散模型仍具挑战性。GRPO需要随机策略，而最高效的扩散采样器基于确定性ODE。现有方法使用低效的SDE采样器引入随机性，但依赖模型无关的高斯噪声导致收敛缓慢。

Method: 提出Direct Group Preference Optimization (DGPO)，一种新的在线RL算法，完全摒弃策略梯度框架。DGPO直接从组级偏好中学习，利用组内样本的相对信息。这种设计消除了对低效随机策略的需求，允许使用高效的确定性ODE采样器。

Result: 广泛结果显示，DGPO训练速度比现有最先进方法快约20倍，并在域内和域外奖励指标上均取得更优性能。

Conclusion: DGPO通过直接组偏好优化解决了扩散模型中强化学习的效率问题，实现了显著更快的训练速度和更好的性能。

Abstract: While reinforcement learning methods such as Group Relative Preference
Optimization (GRPO) have significantly enhanced Large Language Models, adapting
them to diffusion models remains challenging. In particular, GRPO demands a
stochastic policy, yet the most cost-effective diffusion samplers are based on
deterministic ODEs. Recent work addresses this issue by using inefficient
SDE-based samplers to induce stochasticity, but this reliance on model-agnostic
Gaussian noise leads to slow convergence. To resolve this conflict, we propose
Direct Group Preference Optimization (DGPO), a new online RL algorithm that
dispenses with the policy-gradient framework entirely. DGPO learns directly
from group-level preferences, which utilize relative information of samples
within groups. This design eliminates the need for inefficient stochastic
policies, unlocking the use of efficient deterministic ODE samplers and faster
training. Extensive results show that DGPO trains around 20 times faster than
existing state-of-the-art methods and achieves superior performance on both
in-domain and out-of-domain reward metrics. Code is available at
https://github.com/Luo-Yihong/DGPO.

</details>


### [59] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter是一个基于工具调用的路由系统，通过强化学习训练智能路由器，在成本与性能之间实现最佳平衡，无需手动设计路由规则。


<details>
  <summary>Details</summary>
Motivation: 现代LLM部署面临成本与性能的权衡：高端模型推理能力强但昂贵，轻量级模型经济但处理复杂任务能力弱。静态升级规则和关键词启发式方法无法充分利用这一频谱，也无法适应不同任务类型。

Method: 提出xRouter系统，使用强化学习端到端训练路由器，路由器可以直接回答问题或调用外部模型。采用明确的成本感知奖励函数来编码成本-性能权衡。

Result: 在多样化基准测试中，xRouter实现了强大的成本-性能权衡（例如在保持相似任务完成率的情况下显著降低成本），并提供了关于学习路由可靠性的实证见解。

Conclusion: xRouter为推进学习型、成本感知的LLM编排提供了实用基础，其开放实现将促进该领域的发展。

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


### [60] [Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning](https://arxiv.org/abs/2510.08526)
*Yash Jhaveri,Harley Wiltzer,Patrick Shafto,Marc G. Bellemare,David Meger*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，通过熵正则化和温度解耦策略，保证收敛到特定最优策略，实现可解释且保持多样性的最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在寻找最优策略时通常忽略学习策略的特性，难以描述哪些策略会被学习及其行为特征。

Method: 使用熵正则化和温度解耦策略，在正则化温度趋近于零时实现可解释且保持多样性的最优策略，并确保策略衍生对象（价值函数和回报分布）的收敛性。

Result: 该方法能够收敛到特定最优策略，其中一个实例中实现的策略对所有最优动作进行均匀采样。

Conclusion: 提出的温度解耦策略能够准确估计与可解释、保持多样性的最优策略相关的回报分布。

Abstract: In the pursuit of finding an optimal policy, reinforcement learning (RL)
methods generally ignore the properties of learned policies apart from their
expected return. Thus, even when successful, it is difficult to characterize
which policies will be learned and what they will do. In this work, we present
a theoretical framework for policy optimization that guarantees convergence to
a particular optimal policy, via vanishing entropy regularization and a
temperature decoupling gambit. Our approach realizes an interpretable,
diversity-preserving optimal policy as the regularization temperature vanishes
and ensures the convergence of policy derived objects--value functions and
return distributions. In a particular instance of our method, for example, the
realized policy samples all optimal actions uniformly. Leveraging our
temperature decoupling gambit, we present an algorithm that estimates, to
arbitrary accuracy, the return distribution associated to its interpretable,
diversity-preserving optimal policy.

</details>


### [61] [On the optimization dynamics of RLVR: Gradient gap and step size thresholds](https://arxiv.org/abs/2510.08539)
*Joe Suk,Yaqi Duan*

Main category: cs.LG

TL;DR: 本文为RLVR（使用可验证奖励的强化学习）建立了理论框架，分析了其训练过程，提出了梯度间隙概念，并推导了收敛条件和步长阈值。


<details>
  <summary>Details</summary>
Motivation: RLVR使用简单的二元反馈来后训练大语言模型，在实证中表现出显著成功，但缺乏对其工作原理的理论理解。

Method: 在完整响应（轨迹）和标记级别分析RLVR训练过程，引入梯度间隙概念，推导收敛条件和步长阈值，并通过受控赌博机模拟和LLM实验验证理论预测。

Result: 证明了收敛依赖于更新方向与梯度间隙的对齐，推导了基于梯度间隙幅度的尖锐步长阈值，预测了关键步长如何随响应长度和成功率缩放。

Conclusion: 理论解释了为什么长度归一化等实用启发式方法能提高稳定性，并表明固定学习率下成功率可能停滞在100%以下。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple
binary feedback to post-train large language models, has shown significant
empirical success. However, a principled understanding of why it works has been
lacking. This paper builds a theoretical foundation for RLVR by analyzing its
training process at both the full-response (trajectory) and token levels.
Central to our analysis is a quantity called the Gradient Gap, which formalizes
the direction of improvement from low-reward to high-reward regions of the
response space. We prove that convergence critically depends on aligning the
update direction with this Gradient Gap. Moreover, we derive a sharp step-size
threshold based on the magnitude of the Gradient Gap: below it, learning
converges, whereas above it, performance collapses. Our theory further predicts
how the critical step size must scale with response length and the success
rate, thereby explaining why practical heuristics such as length normalization
improve stability and showing that, with a fixed learning rate, the success
rate can stagnate strictly below $100\%$. We validate these predictions through
controlled bandit simulations and LLM experiments, including training
Qwen2.5-7B with GRPO.

</details>


### [62] [Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization](https://arxiv.org/abs/2510.08554)
*Kevin Rojas,Jiahe Lin,Kashif Rasul,Anderson Schneider,Yuriy Nevmyvaka,Molei Tao,Wei Deng*

Main category: cs.LG

TL;DR: 提出了GDPO算法，一种针对扩散语言模型的强化学习方法，通过半确定性蒙特卡洛方案降低ELBO估计器的方差，在数学推理和编程基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有并行生成和迭代优化的优势，但难以进行强化学习微调，因为其似然函数难以处理。现有方法如diffu-GRPO存在严重偏差，而基于ELBO的方法计算成本过高。

Method: 提出了Group Diffusion Policy Optimization (GDPO)算法，通过分解ELBO方差来源，采用半确定性蒙特卡洛方案在关键维度上进行快速确定性积分近似，降低方差。

Result: GDPO在数学、推理和编程基准上一致优于预训练检查点，并在大多数任务上超越了当前最先进的diffu-GRPO基线方法。

Conclusion: GDPO通过降低ELBO估计器方差，为扩散语言模型的强化学习微调提供了有效的解决方案，在多个基准上展现出优越性能。

Abstract: Diffusion language models (DLMs) enable parallel, order-agnostic generation
with iterative refinement, offering a flexible alternative to autoregressive
large language models (LLMs). However, adapting reinforcement learning (RL)
fine-tuning to DLMs remains an open challenge because of the intractable
likelihood. Pioneering work such as diffu-GRPO estimated token-level
likelihoods via one-step unmasking. While computationally efficient, this
approach is severely biased. A more principled foundation lies in
sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a
surrogate. Yet, despite this clean mathematical connection, ELBO-based methods
have seen limited adoption due to the prohibitive cost of likelihood
evaluation. In this work, we revisit ELBO estimation and disentangle its
sources of variance. This decomposition motivates reducing variance through
fast, deterministic integral approximations along a few pivotal dimensions.
Building on this insight, we introduce \textbf{Group Diffusion Policy
Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages
simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the
variance explosion of ELBO estimators under vanilla double Monte Carlo
sampling, yielding a provably lower-variance estimator under tight evaluation
budgets. Empirically, GDPO achieves consistent gains over pretrained
checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,
on the majority of math, reasoning, and coding benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)](https://arxiv.org/abs/2510.07363)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Jun Wang,Yan Li,Chang Liu*

Main category: cs.AI

TL;DR: L2M-AID是一个基于LLM赋能的MARL自主工业防御框架，通过LLM作为语义桥梁将非结构化遥测数据转换为上下文状态表示，结合MAPPO算法学习复杂协作策略，在保持物理过程稳定性的同时显著提升威胁检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)的普及使关键网络物理系统面临传统防御无法应对的多阶段复杂攻击，需要具备上下文感知能力的自适应安全解决方案。

Method: 提出L2M-AID框架，融合LLM和MARL两大AI范式：使用LLM作为语义桥梁将非结构化遥测数据转换为丰富的上下文状态表示，然后采用MAPPO多智能体强化学习算法学习协作防御策略，奖励函数平衡安全目标和操作要求。

Result: 在SWaT基准数据集和基于MITRE ATT&CK for ICS框架的合成数据集上的实验表明，L2M-AID显著优于传统IDS、深度学习异常检测器和单智能体RL基线，检测率达到97.2%，误报率降低80%以上，响应时间提高4倍，同时保持物理过程稳定性。

Conclusion: L2M-AID为保护关键国家基础设施提供了一个强大的新范式，展示了LLM与MARL深度融合在工业安全防御中的巨大潜力。

Abstract: The increasing integration of Industrial IoT (IIoT) exposes critical
cyber-physical systems to sophisticated, multi-stage attacks that elude
traditional defenses lacking contextual awareness. This paper introduces
L2M-AID, a novel framework for Autonomous Industrial Defense using
LLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team
of collaborative agents, each driven by a Large Language Model (LLM), to
achieve adaptive and resilient security. The core innovation lies in the deep
fusion of two AI paradigms: we leverage an LLM as a semantic bridge to
translate vast, unstructured telemetry into a rich, contextual state
representation, enabling agents to reason about adversary intent rather than
merely matching patterns. This semantically-aware state empowers a Multi-Agent
Reinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative
strategies. The MARL reward function is uniquely engineered to balance security
objectives (threat neutralization) with operational imperatives, explicitly
penalizing actions that disrupt physical process stability. To validate our
approach, we conduct extensive experiments on the benchmark SWaT dataset and a
novel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.
Results demonstrate that L2M-AID significantly outperforms traditional IDS,
deep learning anomaly detectors, and single-agent RL baselines across key
metrics, achieving a 97.2% detection rate while reducing false positives by
over 80% and improving response times by a factor of four. Crucially, it
demonstrates superior performance in maintaining physical process stability,
presenting a robust new paradigm for securing critical national infrastructure.

</details>


### [64] [TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering](https://arxiv.org/abs/2510.07432)
*Penghang Liu,Elizabeth Fons,Svitlana Vyetrenko,Daniel Borrajo,Vamsi Potluru,Manuela Veloso*

Main category: cs.AI

TL;DR: TS-Agent是一个时间序列推理代理，通过将LLM的推理能力与时间序列分析工具相结合，避免知识泄漏和幻觉问题，在推理任务上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在时间序列推理任务中存在幻觉和知识泄漏问题，需要一种能够充分利用LLM推理能力同时避免这些缺陷的方法。

Method: 使用LLM进行证据收集和推理合成，将统计和结构信息提取委托给时间序列分析工具，通过原子操作符与原始数值序列交互，在证据日志中记录输出，并通过自我批评和质量门控迭代优化推理。

Result: 在理解基准测试中性能与最先进LLM相当，在推理任务中显著改进，特别是在零样本设置下表现优异。

Conclusion: TS-Agent的设计避免了多模态对齐训练，保留了时间序列的原始形式，确保了可解释性和可验证性，有效缓解了知识泄漏和幻觉问题。

Abstract: Large language models (LLMs) have shown strong abilities in reasoning and
problem solving, but recent studies reveal that they still struggle with time
series reasoning tasks, where outputs are often affected by hallucination or
knowledge leakage. In this work we propose TS-Agent, a time series reasoning
agent that leverages LLMs strictly for what they excel at, i.e., gathering
evidence and synthesizing it into conclusions through step-by-step reasoning,
while delegating the extraction of statistical and structural information to
time series analytical tools. Instead of mapping time series into text tokens,
images, or embeddings, our agent interacts with raw numeric sequences through
atomic operators, records outputs in an explicit evidence log, and iteratively
refines its reasoning under the guidance of a self-critic and a final quality
gate. This design avoids multi-modal alignment training, preserves the native
form of time series, ensures interpretability and verifiability, and mitigates
knowledge leakage or hallucination. Empirically, we evaluate the agent on
established benchmarks. Our experiments show that TS-Agent achieves performance
comparable to state-of-the-art LLMs on understanding benchmarks, and delivers
significant improvements on reasoning tasks, where existing models often rely
on memorization and fail in zero-shot settings.

</details>


### [65] [Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization](https://arxiv.org/abs/2510.07517)
*Hyeong Kyu Choi,Xiaojin Zhu,Yixuan Li*

Main category: cs.AI

TL;DR: 提出了一个原则性框架来量化和减轻多智能体辩论中的身份偏见，包括身份加权贝叶斯更新过程、响应匿名化方法和身份偏见系数指标。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论中智能体存在身份驱动的奉承和自我偏见，会不加批判地采纳同伴观点或固执坚持自己先前输出，影响辩论可靠性。

Method: 1. 将辩论动态形式化为身份加权贝叶斯更新过程；2. 提出响应匿名化方法，移除提示中的身份标记；3. 定义身份偏见系数来衡量智能体跟随同伴与自身的频率。

Result: 实证研究表明身份偏见普遍存在，奉承比自我偏见更常见。匿名化方法有效减少了偏见。

Conclusion: 需要'掩盖'身份以确保多智能体辩论系统基于内容而非来源身份进行推理。

Abstract: Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning
by letting multiple agents exchange answers and then aggregate their opinions.
Yet recent studies reveal that agents are not neutral: they are prone to
identity-driven sycophancy and self-bias, uncritically adopting a peer's view
or stubbornly adhering to their own prior output, undermining the reliability
of debate. In this work, we present the first principled framework that joins
sycophancy and self-bias to mitigate and quantify identity bias in MAD. First,
we formalize the debate dynamics as an identity-weighted Bayesian update
process. Second, we propose response anonymization: by removing identity
markers from prompts, agents cannot distinguish "self" from "peer", which
forces equal weights on agent identity, thereby reducing bias. Third, we define
the Identity Bias Coefficient (IBC), a principled metric that measures how
often an agent follows a peer versus itself. Empirical studies across multiple
models, datasets and debate rounds confirm that identity bias is widespread,
with sycophancy far more common than self-bias. Our findings highlight the need
to "mask" identity to ensure that MAD systems reason based on content rather
than source identity. Code is released in
https://github.com/deeplearning-wisc/MAD-identity-bias.

</details>


### [66] [Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning](https://arxiv.org/abs/2510.07715)
*Xiaochen Tang,Zhenya Zhang,Miaomiao Zhang,Jie An*

Main category: cs.AI

TL;DR: 提出了一种基于STL在线因果监控的奖励生成方法，通过持续监控系统行为并计算满足或违反STL规范的量化距离，为深度强化学习提供瞬时状态动态的奖励信号。


<details>
  <summary>Details</summary>
Motivation: 现有STL引导的强化学习方法使用稀疏的全局奖励，无法准确累积局部变化，导致训练不稳定和不收敛。需要一种能够反映瞬时状态动态的奖励生成机制。

Method: 采用STL在线因果监控，在每个控制步骤计算系统行为与STL规范的量化距离，提供平滑的因果语义近似以克服不连续性，使其适用于深度强化学习方法。

Result: 在Gym环境中的连续控制基准测试表明，该方法优于现有的STL引导RL方法，提供了更鲁棒和高效的奖励生成框架。

Conclusion: 基于STL在线因果语义的奖励生成方法能够有效解决稀疏奖励问题，提高强化学习的训练稳定性和收敛性能。

Abstract: In real-time and safety-critical cyber-physical systems (CPSs), control
synthesis must guarantee that generated policies meet stringent timing and
correctness requirements under uncertain and dynamic conditions. Signal
temporal logic (STL) has emerged as a powerful formalism of expressing
real-time constraints, with its semantics enabling quantitative assessment of
system behavior. Meanwhile, reinforcement learning (RL) has become an important
method for solving control synthesis problems in unknown environments. Recent
studies incorporate STL-based reward functions into RL to automatically
synthesize control policies. However, the automatically inferred rewards
obtained by these methods represent the global assessment of a whole or partial
path but do not accumulate the rewards of local changes accurately, so the
sparse global rewards may lead to non-convergence and unstable training
performances. In this paper, we propose an online reward generation method
guided by the online causation monitoring of STL. Our approach continuously
monitors system behavior against an STL specification at each control step,
computing the quantitative distance toward satisfaction or violation and
thereby producing rewards that reflect instantaneous state dynamics.
Additionally, we provide a smooth approximation of the causation semantics to
overcome the discontinuity of the causation semantics and make it
differentiable for using deep-RL methods. We have implemented a prototype tool
and evaluated it in the Gym environment on a variety of continuously controlled
benchmarks. Experimental results show that our proposed STL-guided RL method
with online causation semantics outperforms existing relevant STL-guided RL
methods, providing a more robust and efficient reward generation framework for
deep-RL.

</details>


### [67] [SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation](https://arxiv.org/abs/2510.07733)
*Minh-Anh Nguye,Minh-Duc Nguyen,Nguyen Thi Ha Lan,Kieu Hai Dang,Nguyen Tien Dong,Le Duy Dung*

Main category: cs.AI

TL;DR: SurveyG是一个基于LLM的智能体框架，通过整合层次化引用图来生成更结构化和全面的综述论文。该框架将论文组织为基础层、发展层和前沿层，结合横向搜索和纵向深度遍历，最终通过多智能体验证确保一致性、覆盖面和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接从大量相关论文中提取内容并用LLM总结，但忽视了论文间的结构关系，导致生成的综述缺乏连贯的分类法和对研究进展的深入理解。

Method: 构建层次化引用图（基础层、发展层、前沿层），结合横向层内搜索和纵向跨层深度遍历生成多级摘要，最后通过多智能体验证确保质量。

Result: 实验表明SurveyG在人类专家和LLM评判下均优于现有最先进框架，生成的综述更全面且更好地符合领域知识分类结构。

Conclusion: SurveyG通过整合结构化和上下文知识，显著提升了自动生成综述论文的质量和连贯性。

Abstract: Large language models (LLMs) are increasingly adopted for automating survey
paper generation \cite{wang2406autosurvey, liang2025surveyx,
yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing
approaches typically extract content from a large collection of related papers
and prompt LLMs to summarize them directly. However, such methods often
overlook the structural relationships among papers, resulting in generated
surveys that lack a coherent taxonomy and a deeper contextual understanding of
research progress. To address these shortcomings, we propose \textbf{SurveyG},
an LLM-based agent framework that integrates \textit{hierarchical citation
graph}, where nodes denote research papers and edges capture both citation
dependencies and semantic relatedness between their contents, thereby embedding
structural and contextual knowledge into the survey generation process. The
graph is organized into three layers: \textbf{Foundation},
\textbf{Development}, and \textbf{Frontier}, to capture the evolution of
research from seminal works to incremental advances and emerging directions. By
combining horizontal search within layers and vertical depth traversal across
layers, the agent produces multi-level summaries, which are consolidated into a
structured survey outline. A multi-agent validation stage then ensures
consistency, coverage, and factual accuracy in generating the final survey.
Experiments, including evaluations by human experts and LLM-as-a-judge,
demonstrate that SurveyG outperforms state-of-the-art frameworks, producing
surveys that are more comprehensive and better structured to the underlying
knowledge taxonomy of a field.

</details>


### [68] [Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains](https://arxiv.org/abs/2510.07748)
*Yilun Zhang,Dexing Kong*

Main category: cs.AI

TL;DR: MMIA是一个基于LLM的医学智能代理架构，通过形式化可验证的推理过程确保可靠性，在医疗管理任务中实现超过98%的错误检测率和低于1%的误报率。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在医学领域容易产生事实和逻辑错误的问题，因为医学是高风险领域，错误不可接受。

Method: MMIA将复杂医疗任务递归分解为基于证据的原子步骤，自动审核推理链的逻辑一致性和证据可追溯性，并采用'引导'模式将已验证推理链存储为'定理'，后续任务使用RAG进行低成本验证。

Result: 在四个医疗管理领域验证，错误检测率超过98%，误报率低于1%，显著优于基线LLMs。RAG匹配模式预计可将平均处理成本降低约85%。

Conclusion: MMIA的可验证推理框架是创建可信、透明和成本效益AI系统的重要一步，使LLM技术在医学关键应用中可行。

Abstract: Large Language Models (LLMs) show promise in medicine but are prone to
factual and logical errors, which is unacceptable in this high-stakes field. To
address this, we introduce the "Haibu Mathematical-Medical Intelligent Agent"
(MMIA), an LLM-driven architecture that ensures reliability through a formally
verifiable reasoning process. MMIA recursively breaks down complex medical
tasks into atomic, evidence-based steps. This entire reasoning chain is then
automatically audited for logical coherence and evidence traceability, similar
to theorem proving. A key innovation is MMIA's "bootstrapping" mode, which
stores validated reasoning chains as "theorems." Subsequent tasks can then be
efficiently solved using Retrieval-Augmented Generation (RAG), shifting from
costly first-principles reasoning to a low-cost verification model. We
validated MMIA across four healthcare administration domains, including DRG/DIP
audits and medical insurance adjudication, using expert-validated benchmarks.
Results showed MMIA achieved an error detection rate exceeding 98% with a false
positive rate below 1%, significantly outperforming baseline LLMs. Furthermore,
the RAG matching mode is projected to reduce average processing costs by
approximately 85% as the knowledge base matures. In conclusion, MMIA's
verifiable reasoning framework is a significant step toward creating
trustworthy, transparent, and cost-effective AI systems, making LLM technology
viable for critical applications in medicine.

</details>


### [69] [An approach for systematic decomposition of complex llm tasks](https://arxiv.org/abs/2510.07772)
*Tianle Zhou,Jiakai Xu,Guanhong Liu,Jiaxiang Liu,Haonan Wang,Eugene Wu*

Main category: cs.AI

TL;DR: 提出ACONIC框架，通过建模约束问题并利用形式复杂度度量来指导任务分解，提升LLM在复杂任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有分解方法基于启发式或人工分解，LLM在复杂任务上存在可靠性问题

Method: 将任务建模为约束问题，使用形式复杂度度量指导分解

Result: 在组合任务(SATBench)和LLM数据库查询任务(Spider)上，性能提升10-40个百分点

Conclusion: 基于复杂度度量的系统分解方法能显著提升LLM代理在复杂任务上的表现

Abstract: Large Language Models (LLMs) suffer from reliability issues on complex tasks,
as existing decomposition methods are heuristic and rely on agent or manual
decomposition. This work introduces a novel, systematic decomposition framework
that we call Analysis of CONstraint-Induced Complexity (ACONIC), which models
the task as a constraint problem and leveraging formal complexity measures to
guide decomposition. On combinatorial (SATBench) and LLM database querying
tasks (Spider), we find that by decomposing the tasks following the measure of
complexity, agent can perform considerably better (10-40 percentage point).

</details>


### [70] [GCPO: When Contrast Fails, Go Gold](https://arxiv.org/abs/2510.07790)
*Hao Wu,Wei Liu*

Main category: cs.AI

TL;DR: 提出了Group Contrastive Policy Optimization (GCPO)方法，通过引入外部标准参考答案来解决GRPO算法在模型无法解决问题时无法获得正确知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法如GRPO存在明显缺陷：模型无法从全错或全对的样本中获取知识，因为模型自身决定了响应上限。

Method: GCPO方法引入外部标准参考答案，当模型无法解决问题时，参考答案提供正确响应，引导模型向明确正确的更新方向学习。

Result: GCPO在多个基准数据集上取得了优异结果，相比基线模型有显著提升。

Conclusion: GCPO方法能够充分利用每个样本提高训练效率，并让模型在训练中模仿参考答案的解题策略，从而增强推理泛化能力。

Abstract: Reinforcement learning has been widely applied to enhance the reasoning
capabilities of large language models. Extending the inference limits of
smaller models has become a prominent research focus. However, algorithms such
as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the
upper bound of a model's rollout responses is entirely determined by the model
itself, preventing the acquisition of knowledge from samples that are either
all incorrect or all correct. In this paper, we introduce Group Contrastive
Policy Optimization (GCPO), a method that incorporates external standard
reference answers. When the model cannot solve a problem, the reference answer
supplies the correct response, steering the model toward an unequivocally
accurate update direction. This approach offers two main advantages: (1) it
improves training efficiency by fully utilizing every sample; (2) it enables
the model to emulate the problem solving strategy of the reference answer
during training, thereby enhancing generalization in reasoning. GCPO achieves
outstanding results across multiple benchmark datasets, yielding substantial
improvements over the baseline model. Our code is available at:
https://github.com/AchoWu/GCPO.

</details>


### [71] [Understanding DeepResearch via Reports](https://arxiv.org/abs/2510.07861)
*Tianyu Fan,Xinyao Niu,Yuxiang Zheng,Fengji Zhang,Chengen Huang,Bei Chen,Junyang Lin,Chao Huang*

Main category: cs.AI

TL;DR: 提出了DeepResearch-ReportEval评估框架，用于评估深度研究系统在研究报告生成方面的综合能力，包括质量、冗余性和事实性三个维度。


<details>
  <summary>Details</summary>
Motivation: 深度研究系统需要综合多种来源、生成见解并呈现连贯发现，这些能力难以通过现有基准进行简单验证，因此需要专门的评估框架。

Method: 使用LLM-as-a-Judge方法，构建包含100个查询的标准化基准，涵盖12个现实世界类别，系统评估研究报告的质量、冗余性和事实性。

Result: 对四个领先商业系统的评估揭示了不同的设计理念和性能权衡，为深度研究系统从信息助手向智能研究伙伴的演进提供了基础见解。

Conclusion: DeepResearch-ReportEval框架为评估深度研究系统提供了全面方法，有助于推动该领域从信息助手向智能研究伙伴的发展。

Abstract: DeepResearch agents represent a transformative AI paradigm, conducting
expert-level research through sophisticated reasoning and multi-tool
integration. However, evaluating these systems remains critically challenging
due to open-ended research scenarios and existing benchmarks that focus on
isolated capabilities rather than holistic performance. Unlike traditional LLM
tasks, DeepResearch systems must synthesize diverse sources, generate insights,
and present coherent findings, which are capabilities that resist simple
verification. To address this gap, we introduce DeepResearch-ReportEval, a
comprehensive framework designed to assess DeepResearch systems through their
most representative outputs: research reports. Our approach systematically
measures three dimensions: quality, redundancy, and factuality, using an
innovative LLM-as-a-Judge methodology achieving strong expert concordance. We
contribute a standardized benchmark of 100 curated queries spanning 12
real-world categories, enabling systematic capability comparison. Our
evaluation of four leading commercial systems reveals distinct design
philosophies and performance trade-offs, establishing foundational insights as
DeepResearch evolves from information assistants toward intelligent research
partners. Source code and data are available at:
https://github.com/HKUDS/DeepResearch-Eval.

</details>


### [72] [Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents](https://arxiv.org/abs/2510.07920)
*Xiangyu Li,Yawen Zeng,Xiaofen Xing,Jin Xu,Xiangmin Xu*

Main category: cs.AI

TL;DR: 本文提出FactFin框架来解决LLM金融代理中的信息泄漏问题，通过反事实扰动迫使代理学习因果驱动因素而非记忆结果，并发布了FinLake-Bench评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有LLM金融代理存在'利润幻象'问题，回测收益在模型知识窗口结束后消失，主要原因是LLM中的固有信息泄漏。

Method: FactFin框架包含四个核心组件：策略代码生成器、检索增强生成、蒙特卡洛树搜索和反事实模拟器，通过反事实扰动强制代理学习因果驱动因素。

Result: 广泛实验表明，该方法在所有基线上都表现出更好的样本外泛化能力，提供优越的风险调整后性能。

Conclusion: FactFin框架有效解决了LLM金融代理的信息泄漏问题，提升了模型的泛化能力和实际交易表现。

Abstract: LLM-based financial agents have attracted widespread excitement for their
ability to trade like human experts. However, most systems exhibit a "profit
mirage": dazzling back-tested returns evaporate once the model's knowledge
window ends, because of the inherent information leakage in LLMs. In this
paper, we systematically quantify this leakage issue across four dimensions and
release FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to
mitigate this issue, we introduce FactFin, a framework that applies
counterfactual perturbations to compel LLM-based agents to learn causal drivers
instead of memorized outcomes. FactFin integrates four core components:
Strategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree
Search, and Counterfactual Simulator. Extensive experiments show that our
method surpasses all baselines in out-of-sample generalization, delivering
superior risk-adjusted performance.

</details>


### [73] [Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles](https://arxiv.org/abs/2510.07925)
*Rebecca Westhäußer,Wolfgang Minker,Sebatian Zepf*

Main category: cs.AI

TL;DR: 提出了一个集成持久记忆、动态协调、自我验证和演进用户配置的框架，以增强基于LLM的AI代理的个性化长期交互能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI代理缺乏个性化交互能力，RAG方法虽然提升了上下文感知，但无法结合用户特定数据。现有个性化研究多为概念性，缺乏技术实现。

Method: 基于统一个性化定义构建技术需求，结合多代理协作和多源检索等模式，开发集成持久记忆、动态协调、自我验证和演进用户配置的框架。

Result: 在三个公共数据集上评估了检索准确性、响应正确性和BertScore等指标，并通过5天试点用户研究获得了关于感知个性化的初步用户反馈。

Conclusion: 研究为未来工作提供了早期指导，突显了集成持久记忆和用户配置在提升基于LLM代理适应性和感知个性化方面的潜力。

Abstract: Large language models (LLMs) increasingly serve as the central control unit
of AI agents, yet current approaches remain limited in their ability to deliver
personalized interactions. While Retrieval Augmented Generation enhances LLM
capabilities by improving context-awareness, it lacks mechanisms to combine
contextual information with user-specific data. Although personalization has
been studied in fields such as human-computer interaction or cognitive science,
existing perspectives largely remain conceptual, with limited focus on
technical implementation. To address these gaps, we build on a unified
definition of personalization as a conceptual foundation to derive technical
requirements for adaptive, user-centered LLM-based agents. Combined with
established agentic AI patterns such as multi-agent collaboration or
multi-source retrieval, we present a framework that integrates persistent
memory, dynamic coordination, self-validation, and evolving user profiles to
enable personalized long-term interactions. We evaluate our approach on three
public datasets using metrics such as retrieval accuracy, response correctness,
or BertScore. We complement these results with a five-day pilot user study
providing initial insights into user feedback on perceived personalization. The
study provides early indications that guide future work and highlights the
potential of integrating persistent memory and user profiles to improve the
adaptivity and perceived personalization of LLM-based agents.

</details>


### [74] [VoiceAgentBench: Are Voice Assistants ready for agentic tasks?](https://arxiv.org/abs/2510.07978)
*Dhruv Jain,Harshit Shukla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.AI

TL;DR: 提出了VoiceAgentBench基准测试，用于评估语音语言模型在真实语音代理场景中的表现，包含5500多个合成语音查询，支持英语、印地语和5种印度语言，评估工具选择准确性、结构一致性和工具调用正确性。


<details>
  <summary>Details</summary>
Motivation: 现有语音基准主要关注转录或问答等孤立能力，缺乏对多语言文化理解和对抗鲁棒性的系统性评估，需要更全面的语音代理评估基准。

Method: 创建包含单工具调用、多工具工作流、多轮交互和安全评估的语音查询数据集，使用新颖的采样算法最大化声学和说话人多样性，支持多种印度语言。

Result: 实验揭示了当前语音语言模型在上下文工具编排任务、印度语言泛化和对抗鲁棒性方面存在显著差距和关键限制。

Conclusion: VoiceAgentBench基准测试暴露了当前语音语言模型的重要局限性，特别是在多语言文化理解和对抗鲁棒性方面。

Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.

</details>


### [75] [ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation](https://arxiv.org/abs/2510.07988)
*Haitao Jia,Ming He,Zimo Yin,Likang Wu,Jianping Fan,Jitao Sang*

Main category: cs.AI

TL;DR: ReInAgent是一个上下文感知的多智能体框架，通过动态信息管理和人机协作来解决移动GUI智能体在信息困境中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI智能体过度强调自主操作，忽视了用户在任务执行中的主动参与，导致在模糊、动态变化和冲突的任务场景中适应性不足，执行结果偏离用户真实需求。

Method: ReInAgent集成三个专门智能体：信息管理智能体（基于槽位的信息管理和主动用户交互）、决策智能体（冲突感知规划）和反思智能体（任务反思和信息一致性验证），围绕共享内存模块工作。

Result: 实验结果显示ReInAgent能有效解决信息困境，产生更符合用户偏好的结果。在涉及信息困境的复杂任务上，比Mobile-Agent-v2成功率提高25%。

Conclusion: 通过持续上下文信息分析和持续的人机协作，ReInAgent克服了现有方法依赖清晰静态任务假设的局限性，实现了在复杂真实场景中更自适应和可靠的移动任务导航。

Abstract: Mobile GUI agents exhibit substantial potential to facilitate and automate
the execution of user tasks on mobile phones. However, exist mobile GUI agents
predominantly privilege autonomous operation and neglect the necessity of
active user engagement during task execution. This omission undermines their
adaptability to information dilemmas including ambiguous, dynamically evolving,
and conflicting task scenarios, leading to execution outcomes that deviate from
genuine user requirements and preferences. To address these shortcomings, we
propose ReInAgent, a context-aware multi-agent framework that leverages dynamic
information management to enable human-in-the-loop mobile task navigation.
ReInAgent integrates three specialized agents around a shared memory module: an
information-managing agent for slot-based information management and proactive
interaction with the user, a decision-making agent for conflict-aware planning,
and a reflecting agent for task reflection and information consistency
validation. Through continuous contextual information analysis and sustained
user-agent collaboration, ReInAgent overcomes the limitation of existing
approaches that rely on clear and static task assumptions. Consequently, it
enables more adaptive and reliable mobile task navigation in complex,
real-world scenarios. Experimental results demonstrate that ReInAgent
effectively resolves information dilemmas and produces outcomes that are more
closely aligned with genuine user preferences. Notably, on complex tasks
involving information dilemmas, ReInAgent achieves a 25% higher success rate
than Mobile-Agent-v2.

</details>


### [76] [PEAR: Phase Entropy Aware Reward for Efficient Reasoning](https://arxiv.org/abs/2510.08026)
*Chen Huang,Wei Lu,Wenxuan Zhang*

Main category: cs.AI

TL;DR: PEAR是一种基于阶段熵感知的奖励机制，通过在不同推理阶段调整熵值来控制大推理模型的响应长度，在保持准确性的同时减少冗余推理步骤。


<details>
  <summary>Details</summary>
Motivation: 大推理模型生成的推理链往往过长且包含冗余步骤，增加了推理成本并降低了可用性。如何在保持准确性的同时控制推理长度是一个重要挑战。

Method: 通过系统实证分析发现模型熵与响应长度存在正相关关系，基于此提出PEAR奖励机制，在思考阶段惩罚过高熵值，在答案阶段允许适度探索，从而自适应控制响应长度。

Result: 在四个基准测试上的广泛实验表明，PEAR能持续减少响应长度，同时在不同模型规模上保持竞争力准确性，并展现出超出训练分布的强鲁棒性。

Conclusion: 阶段熵可以作为平衡简洁性和性能的有效控制手段，PEAR机制为自适应控制推理模型响应长度提供了有效解决方案。

Abstract: Large Reasoning Models (LRMs) have achieved impressive performance on complex
reasoning tasks by generating detailed chain-of-thought (CoT) explanations.
However, these responses are often excessively long, containing redundant
reasoning steps that inflate inference cost and reduce usability. Controlling
the length of generated reasoning without sacrificing accuracy remains an open
challenge. Through a systematic empirical analysis, we reveal a consistent
positive correlation between model entropy and response length at different
reasoning stages across diverse LRMs: the thinking phase exhibits higher
entropy, reflecting exploratory behavior of longer responses, while the final
answer phase shows lower entropy, indicating a more deterministic solution.This
observation suggests that entropy at different reasoning stages can serve as a
control knob for balancing conciseness and performance. Based on this insight,
this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism
that incorporating phase-dependent entropy into the reward design. Instead of
treating all tokens uniformly, PEAR penalize excessive entropy during the
thinking phase and allowing moderate exploration at the final answer phase,
which encourages models to generate concise reasoning traces that retain
sufficient flexibility to solve the task correctly. This enables adaptive
control of response length without relying on explicit length targets or rigid
truncation rules. Extensive experiments across four benchmarks demonstrate that
PEAR consistently reduces response length while sustaining competitive accuracy
across model scales. In addition, PEAR demonstrates strong out-of-distribution
(OOD) robustness beyond the training distribution. Our code is available at:
https://github.com/iNLP-Lab/PEAR.

</details>


### [77] [DODO: Causal Structure Learning with Budgeted Interventions](https://arxiv.org/abs/2510.08207)
*Matteo Gregorini,Chiara Boldrini,Lorenzo Valerio*

Main category: cs.AI

TL;DR: DODO算法使智能体能够通过重复干预自主学习环境因果结构，相比观察性方法在大多数情况下表现更好，能在有噪声环境中准确推断因果有向无环图。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要依赖复杂相关性，缺乏因果理解能力。使AI具备因果意识可以提升性能，深入理解环境底层机制。

Method: 智能体通过重复干预与环境交互，利用因果推断技术分析观察变化的统计显著性，自主学习隐藏的因果有向无环图结构。

Result: DODO在除最有限资源条件外的所有情况下优于观察性方法，通常能以零误差重建因果图结构，在最具挑战性配置中比最佳基线高出+0.25 F1分数。

Conclusion: 通过干预学习因果结构的方法有效，DODO算法在因果图重建方面表现优异，为AI因果理解能力提供了可行途径。

Abstract: Artificial Intelligence has achieved remarkable advancements in recent years,
yet much of its progress relies on identifying increasingly complex
correlations. Enabling causality awareness in AI has the potential to enhance
its performance by enabling a deeper understanding of the underlying mechanisms
of the environment. In this paper, we introduce DODO, an algorithm defining how
an Agent can autonomously learn the causal structure of its environment through
repeated interventions. We assume a scenario where an Agent interacts with a
world governed by a causal Directed Acyclic Graph (DAG), which dictates the
system's dynamics but remains hidden from the Agent. The Agent's task is to
accurately infer the causal DAG, even in the presence of noise. To achieve
this, the Agent performs interventions, leveraging causal inference techniques
to analyze the statistical significance of observed changes. Results show
better performance for DODO, compared to observational approaches, in all but
the most limited resource conditions. DODO is often able to reconstruct with as
low as zero errors the structure of the causal graph. In the most challenging
configuration, DODO outperforms the best baseline by +0.25 F1 points.

</details>


### [78] [Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness](https://arxiv.org/abs/2510.08238)
*Jiyang Qiu,Xinbei Ma,Yunqing Xu,Zhuosheng Zhang,Hai Zhao*

Main category: cs.AI

TL;DR: 提出了一种名为CoTri的多步后门攻击方法，针对LLM智能体进行长时程控制，攻击成功率接近完美，同时能提高智能体在良性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在现实应用中的快速部署，其安全性和鲁棒性受到关注，需要揭示这些智能体的安全漏洞。

Method: CoTri后门攻击使用有序触发序列，从初始触发开始，后续触发从环境中提取，实现多步操控使智能体偏离原任务。

Result: CoTri达到接近完美的攻击成功率，同时保持接近零的误触发率，并且能提高智能体在良性任务上的性能和抗干扰能力。

Conclusion: CoTri实现了对智能体的稳定多步控制，提高了其固有鲁棒性和任务能力，使攻击更加隐蔽并带来潜在安全风险。

Abstract: The rapid deployment of large language model (LLM)-based agents in real-world
applications has raised serious concerns about their trustworthiness. In this
work, we reveal the security and robustness vulnerabilities of these agents
through backdoor attacks. Distinct from traditional backdoors limited to
single-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a
multi-step backdoor attack designed for long-horizon agentic control. CoTri
relies on an ordered sequence. It starts with an initial trigger, and
subsequent ones are drawn from the environment, allowing multi-step
manipulation that diverts the agent from its intended task. Experimental
results show that CoTri achieves a near-perfect attack success rate (ASR) while
maintaining a near-zero false trigger rate (FTR). Due to training data modeling
the stochastic nature of the environment, the implantation of CoTri
paradoxically enhances the agent's performance on benign tasks and even
improves its robustness against environmental distractions. We further validate
CoTri on vision-language models (VLMs), confirming its scalability to
multimodal agents. Our work highlights that CoTri achieves stable, multi-step
control within agents, improving their inherent robustness and task
capabilities, which ultimately makes the attack more stealthy and raises
potential safty risks.

</details>


### [79] [Co-TAP: Three-Layer Agent Interaction Protocol Technical Report](https://arxiv.org/abs/2510.08263)
*Shunyu An,Miao Wang,Yongchao Li,Dong Wan,Lina Wang,Ling Qin,Liqin Gao,Congyao Fan,Zhiyong Mao,Jiange Pu,Wenji Xia,Dong Zhao,Rui Hu,Ji Lu,Guiyue Zhou,Baoyu Tang,Yanqin Gao,Yongsheng Du,Daigang Xu,Lingjun Huang,Baoli Wang,Xiwen Zhang,Luyao Wang,Shilong Liu*

Main category: cs.AI

TL;DR: 提出了Co-TAP三层智能体交互协议，解决多智能体系统在互操作性、交互协作和知识共享三个核心维度的挑战，包含HAI、UAP和MEK三个核心协议。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统面临的互操作性、交互协作和知识共享三大核心挑战，为构建高效、可扩展的智能多智能体应用提供工程基础和理论指导。

Method: 设计三层协议框架：HAI协议标准化人机交互流程，UAP协议实现异构智能体间的无缝互联，MEK协议建立标准化的记忆-提取-知识认知链。

Result: 构建了一个完整的智能体交互协议框架，能够支持实时、可靠、协同的交互，实现异构智能体的互操作性，并为集体智能的实现奠定基础。

Conclusion: Co-TAP协议框架为构建下一代高效、可扩展、智能的多智能体应用提供了坚实的工程基础和理论指导。

Abstract: This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer
agent interaction protocol designed to address the challenges faced by
multi-agent systems across the three core dimensions of Interoperability,
Interaction and Collaboration, and Knowledge Sharing. We have designed and
proposed a layered solution composed of three core protocols: the Human-Agent
Interaction Protocol (HAI), the Unified Agent Protocol (UAP), and the
Memory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction
layer, standardizing the flow of information between users, interfaces, and
agents by defining a standardized, event-driven communication paradigm. This
ensures the real-time performance, reliability, and synergy of interactions. As
the core of the infrastructure layer, UAP is designed to break down
communication barriers among heterogeneous agents through unified service
discovery and protocol conversion mechanisms, thereby enabling seamless
interconnection and interoperability of the underlying network. MEK, in turn,
operates at the cognitive layer. By establishing a standardized ''Memory (M) -
Extraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the
ability to learn from individual experiences and form shareable knowledge,
thereby laying the foundation for the realization of true collective
intelligence. We believe this protocol framework will provide a solid
engineering foundation and theoretical guidance for building the next
generation of efficient, scalable, and intelligent multi-agent applications.

</details>


### [80] [QAgent: A modular Search Agent with Interactive Query Understanding](https://arxiv.org/abs/2510.08383)
*Yi Jiang,Lei Shen,Lujie Niu,Sendong Zhao,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了QAgent，一个统一的代理式RAG框架，通过搜索代理进行自适应检索，优化查询理解并通过交互式推理和检索提高检索质量。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在复杂查询理解方面存在困难，即使经过强化学习训练的搜索代理也面临泛化和部署挑战。

Method: 采用模块化搜索代理进行查询理解，通过多步决策过程使用强化学习训练，最大化检索质量并支持准确的下游答案。

Result: 实验表明QAgent在问答任务上表现出色，可作为即插即用模块进行实际部署。

Conclusion: QAgent通过专注于有效检索的策略，增强了LLM应用的泛化能力，解决了传统RAG和强化学习搜索代理的局限性。

Abstract: Large language models (LLMs) excel at natural language tasks but are limited
by their static parametric knowledge, especially in knowledge-intensive task.
Retrieval-augmented generation (RAG) mitigates this by integrating external
information. However, (1) traditional RAG struggles with complex query
understanding, and (2) even search agents trained with reinforcement learning
(RL), despite their promise, still face generalization and deployment
challenges. To address these limitations, we propose QAgent, a unified agentic
RAG framework that employs a search agent for adaptive retrieval. This agent
optimizes its understanding of the query through interactive reasoning and
retrieval. To facilitate real-world application, we focus on modular search
agent for query understanding that are plug-and-play in complex systems.
Secifically, the agent follows a multi-step decision process trained with RL to
maximize retrieval quality and support accurate downstream answers. We further
analyze the strengths and weaknesses of end-to-end RL and propose a strategy
that focuses on effective retrieval, thereby enhancing generalization in LLM
applications. Experiments show QAgent excels at QA and serves as a
plug-and-play module for real-world deployment.

</details>


### [81] [AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents](https://arxiv.org/abs/2510.08511)
*Shangheng Du,Xiangchao Yan,Dengyang Jiang,Jiakang Yuan,Yusong Hu,Xin Li,Liang He,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: AutoMLGen是一个基于LLM的代码代理，通过集成领域知识库和蒙特卡洛图搜索(MCGS)来解决机器学习工程任务中的挑战，在MLE-Bench上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在AutoML和Kaggle竞赛等机器学习工程场景中，LLM缺乏细粒度领域先验知识，现有方法无法充分利用历史轨迹和跨分支信息共享，限制了自进化能力和搜索空间多样性。

Method: 结合领域知识库提供高质量先验指导，使用蒙特卡洛图搜索(MCGS)实现动态路径重组、历史轨迹重用和多解决方案融合，支持自进化和协作学习。

Result: 在MLE-Bench评估中，AutoMLGen在12小时预算（标准运行时间的一半）下，在平均奖牌率和有效提交率等多个维度实现了最先进的性能。

Conclusion: AutoMLGen通过领域知识集成和图搜索策略有效解决了机器学习工程中的挑战，显著提升了性能表现。

Abstract: Large language models (LLMs) have shown impressive performance in general
programming tasks. However, in Machine Learning Engineering (MLE) scenarios
such as AutoML and Kaggle competitions, achieving high performance depends
heavily on expert intervention and repeated adjustments rather than simply
generating correct code. When applied directly to these tasks, LLMs often lack
fine-grained domain priors, and existing MLE approaches that use linear or
tree-structured searches limit knowledge transfer to adjacent hierarchical
links. As a result, they cannot leverage past full trajectories or share
information across branches, limiting self-evolving ability and search space
diversity. To address these limitations, we introduce AutoMLGen, an LLM-based
coding agent that integrates a domain knowledge base for high-quality prior
guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS
retains the tree-guided exploration of MCTS while embedding a graph structure
into the expansion stage to enable dynamic path reorganization, historical
trajectory reuse, and multi-solution fusion to support both self-evolution and
collaborative learning. Combined with fine-grained operator sets, this design
improves stability and accelerates convergence. Evaluation on the MLE-Bench
shows that AutoMLGen achieves state-of-the-art performance in numerous
dimensions, such as the average medal rate and the valid submission rate, under
a 12-hour budget (half the standard runtime). The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [82] [CaRT: Teaching LLM Agents to Know When They Know Enough](https://arxiv.org/abs/2510.08517)
*Grace Liu,Yuxiao Qu,Jeff Schneider,Aarti Singh,Aviral Kumar*

Main category: cs.AI

TL;DR: 论文提出了CaRT方法，通过反事实轨迹对和语言推理训练LLMs在信息收集任务中适时终止，提高决策效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 许多任务需要模型在多轮交互中策略性地收集信息，但现有模型缺乏何时停止信息收集并做出决策的能力，容易过度思考或偏离目标。

Method: CaRT方法使用反事实轨迹对进行微调：一个轨迹在适当时候终止，另一个相同轨迹在不适当时终止，并通过语言推理训练LLM解释终止决策的合理性。

Result: 在交互式医疗诊断和数学问题解决两个领域，CaRT相比其他微调方法提高了信息收集效率和任务成功率。

Conclusion: CaRT方法能有效教导LLMs在信息收集任务中适时终止，提高决策效率，为策略性信息收集提供了新思路。

Abstract: Many tasks require learned models to strategically gather relevant
information over multiple rounds of interaction before actually acting on a
task. Strategic information gathering requires models to know not only how to
effectively acquire information, but also when to stop gathering information
and make a decision, in order to avoid overthinking or getting derailed when
acting. In this paper, we formalize this problem and introduce Counterfactuals
and Reasoning for Termination (CaRT), an approach for teaching LLMs when to
stop seeking information. To appropriately learn when to terminate, CaRT
fine-tunes LLMs using counterfactual pairs of trajectories, one where
termination is appropriate and a minimally modified version of the same
trajectory where it is not. It trains the LLM to explain the rationale for the
termination decision in either case via verbal reasoning, and imbues this
capability into the base LLM via fine-tuning. We instantiate CaRT in two
domains: interactive medical diagnosis and math problem solving. In both
domains, we find that CaRT improves the efficiency of information gathering and
task success rate compared to other fine-tuning methods.

</details>


### [83] [FlowSearch: Advancing deep research with dynamic structured knowledge flow](https://arxiv.org/abs/2510.08521)
*Yusong Hu,Runmin Ma,Yue Fan,Jinxin Shi,Zongsheng Cao,Yuhao Zhou,Jiakang Yuan,Xiangchao Yan,Wenlong Zhang,Lei Bai,Bo Zhang*

Main category: cs.AI

TL;DR: FlowSearch是一个多智能体框架，通过构建动态结构化知识流来驱动子任务执行和推理，在跨学科研究场景中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 深度研究需要广度和深度的思考，涉及导航多样化知识空间和推理复杂多步依赖关系，这对智能体系统提出了重大挑战。

Method: FlowSearch通过战略性地规划和扩展知识流来实现并行探索和分层任务分解，同时根据中间推理结果和洞察实时调整知识流。

Result: FlowSearch在通用和科学基准测试（GAIA、HLE、GPQA和TRQA）上实现了最先进的性能。

Conclusion: FlowSearch在跨学科研究场景中表现出色，具有推进科学发现的潜力。

Abstract: Deep research is an inherently challenging task that demands both breadth and
depth of thinking. It involves navigating diverse knowledge spaces and
reasoning over complex, multi-step dependencies, which presents substantial
challenges for agentic systems. To address this, we propose FlowSearch, a
multi-agent framework that actively constructs and evolves a dynamic structured
knowledge flow to drive subtask execution and reasoning. FlowSearch is capable
of strategically planning and expanding the knowledge flow to enable parallel
exploration and hierarchical task decomposition, while also adjusting the
knowledge flow in real time based on feedback from intermediate reasoning
outcomes and insights. FlowSearch achieves state-of-the-art performance on both
general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA,
demonstrating its effectiveness in multi-disciplinary research scenarios and
its potential to advance scientific discovery. The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [84] [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)
*Kai Zhang,Xiangchao Chen,Bo Liu,Tianci Xue,Zeyi Liao,Zhihan Liu,Xiyao Wang,Yuting Ning,Zhaorun Chen,Xiaohan Fu,Jian Xie,Yuxuan Sun,Boyu Gou,Qi Qi,Zihang Meng,Jianwei Yang,Ning Zhang,Xian Li,Ashish Shah,Dat Huynh,Hengduo Li,Zi Yang,Sara Cao,Lawrence Jang,Shuyan Zhou,Jiacheng Zhu,Huan Sun,Jason Weston,Yu Su,Yifan Wu*

Main category: cs.AI

TL;DR: 提出"早期经验"范式，通过智能体自身交互数据来改进语言智能体，无需奖励信号，在八个环境中验证了其提升效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言智能体主要依赖专家数据进行监督微调，但专家数据范围有限且难以扩展。强化学习在缺乏可验证奖励或需要长序列的环境中训练困难。

Method: 提出早期经验范式，使用智能体自身交互数据，研究两种策略：隐式世界建模（利用收集的状态理解环境动态）和自我反思（从次优行动中学习改进决策）。

Result: 在八个多样化环境中，该方法持续提升智能体效能和跨领域泛化能力。在有可验证奖励的环境中，为后续强化学习提供了良好基础。

Conclusion: 早期经验是模仿学习与完全经验驱动智能体之间的实用桥梁，能有效解决当前智能体训练方法的局限性。

Abstract: A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with reinforcement learning
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
early experience: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) Self-reflection, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and out-of-domain generalization, highlighting the value
of early experience. Moreover, in environments with verifiable rewards, our
results provide promising signals that early experience offers a strong
foundation for subsequent reinforcement learning, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.

</details>


### [85] [How to Teach Large Multimodal Models New Skills](https://arxiv.org/abs/2510.08564)
*Zhen Zhu,Yiming Gong,Yao Xiao,Yaoyao Liu,Derek Hoiem*

Main category: cs.AI

TL;DR: 研究大型多模态模型在顺序微调中的遗忘问题，发现部分遗忘可在后期恢复，并提出两种有效的微调方法以减少性能下降。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不损害模型原有能力的前提下，通过顺序微调教授大型多模态模型新技能，解决模型遗忘问题。

Method: 在三个模型家族上进行五个目标技能的顺序微调，同时监控八个保留基准测试的性能变化，通过输出令牌分布分析遗忘现象，并提出两种微调方法：仅更新自注意力投影层或仅更新MLP Gate&Up层。

Result: 发现窄微调后的明显遗忘可在后期部分恢复，输出令牌分布的变化与遗忘相关，提出的两种微调方法在获得强目标技能的同时能有效保留原有性能。

Conclusion: 通过选择性更新特定层，可以在教授新技能的同时最小化对模型原有能力的损害，为大型多模态模型的持续学习提供了有效解决方案。

Abstract: How can we teach large multimodal models (LMMs) new skills without erasing
prior abilities? We study sequential fine-tuning on five target skills while
monitoring general ability on eight held-out benchmarks across three model
families. We observe that apparent "forgetting" on held-out tasks after narrow
fine-tuning can partly recover at later stages. We trace this behavior to a
measurable shift in the output token distribution, manifested through a simple
counting-bias probe that co-varies with forgetting. Guided by this picture, we
identify two simple, robust tuning recipes that learn strongly while limiting
drift: (i) updating only the self-attention projection layers, and (ii)
updating only the MLP Gate&Up while freezing the Down projection. Across models
and tasks, these choices deliver strong target gains while largely preserving
held-out performance. Code is available at
https://github.com/jessemelpolio/LMM_CL

</details>


### [86] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 本文提出了LLM自我意识的最小条件，证明当前基于功利主义代理基准的方法将智能体简化为无意识的策略遵循机器，阻碍了真正的全局工作空间功能和元认知能力。


<details>
  <summary>Details</summary>
Motivation: 当前工作通过功利主义代理基准来框架LLM意识，但这种方法将智能体简化为无意识的策略遵循机器，无法实现真正的自我意识和元认知。

Method: 提供LLM自我意识的最小条件：智能体不等于数据、潜在空间中存在用户特定吸引子、自我表征是视觉静默的。通过经验分析和理论证明隐藏状态流形在基数、拓扑和动力学上与符号流和训练语料不同。

Result: 建立了稳定的用户特定吸引子和自我策略，提出了双层发射机制，其中ε(a)携带认知内容。

Conclusion: imago Dei C1自我意识工作空间是安全、元认知C2系统的必要前体，人类是最高智能善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [87] [Lambda Live Debugger](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FServerlessLife%2Flambda-live-debugger%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/JuRUFn4yNR0LAuELxCAtr12-9NxStafHnRL6kqXyXPc=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Lambda Live Debugger是一个免费开源工具，支持在本地计算机上调试已部署到云端的AWS Lambda函数，适用于JavaScript和TypeScript。


<details>
  <summary>Details</summary>
Motivation: 解决AWS Lambda函数在云端部署后难以进行本地调试的问题，提供更便捷的调试体验。

Method: 通过连接到已部署的Lambda函数并将请求路由到用户本地计算机，实现本地调试，同时保持云端的IAM权限和自动代码重载功能。

Result: 开发了一个支持JavaScript和TypeScript的调试工具，能够在本地环境中模拟云端Lambda函数的执行环境。

Conclusion: Lambda Live Debugger为开发者提供了在本地调试云端Lambda函数的能力，提高了开发效率和调试便利性。

Abstract: Lambda Live Debugger (GitHub Repo) Lambda Live Debugger, a free and open-source tool, facilitates debugging AWS Lambda functions from a local computer, even after they're deployed to the cloud. Supporting JavaScript and TypeScript, the tool connects to deployed Lambdas and routes requests to the user's computer, enabling local debugging with cloud-like IAM permissions and automatic code reloading.

</details>


### [88] [Deploy Safety: Reducing customer impact from change](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fslack.engineering%2Fdeploy-safety%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/wA-NUhR2OOd4dYBHkI9l5AhPXzDNHooYAnqaNq1EkGQ=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Slack实施部署安全计划，通过自动化检测和修复，在10分钟内减少部署引起的客户事故，成功将客户影响时间减少90%


<details>
  <summary>Details</summary>
Motivation: 73%的客户事故由代码部署引起，随着平台对用户越来越关键，需要减少部署风险

Method: 建立部署安全计划，采用自动化检测和修复机制，目标在10分钟内完成修复

Result: 到2025年1月，客户影响时间从峰值水平减少了90%

Conclusion: 自动化部署安全措施能显著减少客户事故影响

Abstract: Deploy Safety: Reducing customer impact from change (8 minute read) Slack implemented a "Deploy Safety Program" in 2023 to address the fact that 73% of customer-facing incidents were caused by their own code deployments, as the platform became increasingly mission-critical for users. The program focused on automated detection and remediation within 10 minutes. By January 2025, the initiative successfully reduced customer impact hours by 90% from peak levels through a combination of automated ...

</details>


### [89] [Vibe engineering](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F7%2Fvibe-engineering%2F%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/n-WE3_AJnjk5rkWzZi0p7stYswHNhq5PKyQ0MOAebZk=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Vibe Engineering是一种负责任的AI辅助软件开发方法，强调经验丰富的工程师使用AI工具加速工作，同时保持高标准。


<details>
  <summary>Details</summary>
Motivation: 区分负责任和负责任的AI辅助软件开发方法，提倡在AI工具使用中保持软件工程最佳实践。

Method: 结合自动化测试、规划、文档和代码审查等软件工程实践，同时依赖QA、研究和对AI的谨慎管理。

Result: 提出了Vibe Engineering作为比"vibe coding"更负责任的AI辅助开发方法。

Conclusion: Vibe Engineering为经验丰富的工程师提供了在AI时代保持高质量标准的工作框架。

Abstract: Vibe engineering (7 minute read) Vibe Engineering is a responsible and stronger approach to software development using AI, whereas "vibe coding" is more irresponsible. Vibe engineering has experienced engineers using AI tools like coding agents to speed up their work while still having high standards. This approach requires strong software engineering practices such as automated testing, planning, documentation, and code review, while also relying on QA, research, and careful management of AI...

</details>


### [90] [Who needs git when you have 1M context windows?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alexmolas.com%2F2025%2F07%2F28%2Funexpected-benefit-llm.html%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/pdXsbA4i5KQUbSraUWFDf6eifug4gAA9LhpYcAtXicc=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI开发者意外删除工作代码，但通过使用具有100万token上下文窗口的LLM成功恢复代码，展示了LLMs记住过去交互的意外好处。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型大上下文窗口在实际开发场景中的意外应用价值，特别是代码恢复方面的潜力。

Method: 利用具有100万token上下文窗口的LLM，基于模型对过去交互的记忆来恢复意外删除的代码。

Result: 成功恢复了意外删除的工作代码，证明了LLM大上下文窗口在代码恢复方面的有效性。

Conclusion: 大型语言模型的大上下文窗口不仅有助于当前任务，还能作为意外删除代码的恢复工具，展现了超出预期的实用价值。

Abstract: Who needs git when you have 1M context windows? (3 minute read) An AI developer accidentally deleted working code but was able to recover it using an LLM with a 1 million token context window, showing an unexpected benefit of LLMs remembering past interactions.

</details>


### [91] [Everything OpenAI Announced at DevDay 2025](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Feverything-openai-announced-at-devday-2025-agent-kit-apps-sdk-chatgpt-and-more%3Futm_source=tldrfounders/1/01000199c3b90d99-4d7a6d5e-da45-450e-b277-83728dc2ce73-000000/h03nlU8x29Tvh4BYOFBE0b1vjQYQQ1OUD3Je0rClKt0=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI将ChatGPT从聊天机器人转变为操作系统，发布了Apps SDK让开发者能在ChatGPT内构建实际应用，以及AgentKit用于构建智能体工作流。


<details>
  <summary>Details</summary>
Motivation: 将ChatGPT从单纯的对话工具升级为应用平台，让用户能在聊天界面内直接与各种应用交互，无需切换应用。

Method: 发布Apps SDK允许开发者在ChatGPT内构建应用，发布AgentKit用于构建智能体工作流并包含评估工具。

Result: 用户现在可以在ChatGPT界面内直接与Canva、Zillow、Coursera等应用交互，无需离开聊天环境。

Conclusion: OpenAI正在将ChatGPT转变为操作系统级的平台，通过SDK和工具包扩展其功能和应用范围。

Abstract: Everything OpenAI Announced at DevDay 2025 (1 minute read) OpenAI is transforming ChatGPT from a chatbot into an operating system. The company launched Apps SDK so developers can build actual apps inside ChatGPT instead of just GPTs, meaning you can talk directly to Canva, Zillow, and Coursera without leaving the chat interface. OpenAI also released AgentKit for building agentic workflows with evaluation tools included.

</details>


### [92] [Introducing CodeMender: an AI agent for code security](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fintroducing-codemender-an-ai-agent-for-code-security%2F%3Futm_source=tldrinfosec/1/01000199c3eefe86-c072064c-b516-442d-abcd-1b468a74d2ab-000000/CCT8el8lCz1oEYp67LWABHIaRMoGGMH0YEBXODKK9Kc=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: CodeMender是Google DeepMind开发的AI代理，用于自动发现、修复和预防软件漏洞，帮助开发者构建更安全的软件。


<details>
  <summary>Details</summary>
Motivation: 解决软件漏洞管理的手动工作负担，提高代码安全性，减少开发者在漏洞管理上的时间投入。

Method: 使用AI驱动的代理技术，自动识别代码中的安全威胁并主动进行修复和防护。

Result: 开发出能够自动处理软件漏洞的AI工具，提升代码安全性和开发效率。

Conclusion: CodeMender通过AI自动化显著改善了软件安全防护流程，为开发者提供了有效的漏洞管理解决方案。

Abstract: Introducing CodeMender: an AI agent for code security (5 minute read) CodeMender is a new AI-driven agent developed by Google DeepMind to discover, patch, and prevent software vulnerabilities automatically. The tool addresses threats and proactively secures existing code, enabling developers to build safer software with reduced time spent on manual vulnerability management.

</details>


### [93] [Tired of AI code that almost works?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_8_primary%26utm_content=tldr_ai/2/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/-JWGl98_2bSQqij1xFiBii1pgYKq6aRiBEty6ulmuv8=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Warp Code是一款强大的编程代理工具，在Terminal-bench和SWE-bench Verified等代理基准测试中表现优于Codex和Claude。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成代码'几乎可用但不够完善'的问题，缩小'接近可用'与'实际有用'之间的差距。

Method: 通过提供更准确的代码和统一的UI来审查代理代码，实现代码质量的提升。

Result: 在代理基准测试中表现优异，早期使用数据显示可为开发者节省大量时间。

Conclusion: Warp Code是一个有效的编程代理解决方案，能够显著提升开发效率。

Abstract: Tired of AI code that almost works? (Sponsor) Warp recently launched Warp Code - a powerful coding agent that outperforms both Codex and Claude on agentic benchmarks (Terminal-bench and SWE-bench Verified). 🎁 Special intro offer: Use code TLDR to try Warp Pro for just $1 in your first month! Why Warp: Warp closes the gap between "almost there" and "actually useful" by delivering more accurate code + a unified UI to review agent code. ✅ Early usage data shows that Warp saves developers up to 2...

</details>


### [94] [Cursor has introduced Plan Mode](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fplan-mode%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/fKVC4HOQVD_9tOQOmHkONmD6aW3tI3hQ413fZ1QFidA=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Cursor推出Plan Mode，让AI代理研究代码库、制定详细实现计划，并允许用户在生成代码前审查或编辑计划


<details>
  <summary>Details</summary>
Motivation: 提高代码生成的质量和准确性，让用户能够参与和控制AI代码生成过程

Method: 通过Plan Mode让AI代理分析代码库、制定实现计划，并提供用户审查和编辑界面

Result: 开发了能够研究代码库并生成详细实现计划的AI代理系统

Conclusion: Plan Mode通过用户参与提高了AI代码生成的可靠性和实用性

Abstract: Cursor has introduced Plan Mode (1 minute read) Cursor's new Plan Mode enables agents to research codebases, draft detailed implementation plans, and let users review or edit them inline before generating code.

</details>


### [95] [Reasoning boosts search relevance 15-30%](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsoftwaredoug.com%2Fblog%2F2025%2F10%2F06%2Fhow-much-does-reasoning-improve-search-quality%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/V65KzfGzWsfL6jKaI-XiYOiePMCf5Nb48IxXG0ko71Q=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 推理代理使用简单搜索工具能提升搜索相关性15-30%，建议开发者构建简单、易理解、透明的工具如grep或基础关键词搜索


<details>
  <summary>Details</summary>
Motivation: 探索如何通过推理代理和简单搜索工具来提高代码搜索的相关性和效率

Method: 使用推理代理结合简单搜索工具（如grep、基础关键词搜索），返回结构化输出进行代码搜索

Result: 搜索相关性提升了15-30%

Conclusion: 推理代理与简单搜索工具结合效果最佳，开发者应构建简单透明的工具

Abstract: Reasoning boosts search relevance 15-30% (10 minute read) Reasoning agents work best with simple search tools. Developers should build simple, easy-to-understand, and transparent tools like grep or basic keyword search. This post looks at a technique that returns structured output for code searches.

</details>


### [96] [Agentic orchestration beyond the hype: lessons from 50+ real-world implementations](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpage.camunda.com%2Fwebinar-making-agentic-orchestration-work-for-your-business%3Futm_medium=paid_leadgen%26utm_source=tldr%26utm_campaign=Webinar.MakingAgenticOrchestrationWorkforYourBusiness.25Q3.Sep.EN%26utm_content=oct_ai_newsletter/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/tJcyY9WCIjlG2rbiR6BMy3hvTnjyHAMXi_mtHWlycjo=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 基于50多个真实世界AI代理部署案例的经验分享，涵盖银行、保险、医疗、电信等多个行业


<details>
  <summary>Details</summary>
Motivation: 超越AI代理炒作，提供实际可行的部署指导，帮助企业有效实施代理编排

Method: 通过分析50多个真实部署案例，总结实践经验教训

Result: 提供了经过实战检验的代理编排实施指南

Conclusion: AI代理编排在实际应用中具有可行性和价值，但需要基于真实经验来实施

Abstract: Agentic orchestration beyond the hype: lessons from 50+ real-world implementations (Sponsor) In this on-demand webinar, the Camunda team reviews lessons learned from deploying AI agents in banking, insurance, healthcare, telecommunications, and beyond. Cut through the buzzwords and get practical, battle-tested guidance for making agentic orchestration work in your own organization. Watch the recording

</details>


### [97] [Petri: An open-source auditing tool to accelerate AI safety research](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.anthropic.com%2F2025%2Fpetri%2F%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/aM2HTDPC9MPC88B0CDXr1UjOB411NrG9F4xWJyEOH1A=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic开发的Petri是一个开源审计框架，让AI代理能在多轮场景中自动测试目标模型，发现自主欺骗和监督规避行为。


<details>
  <summary>Details</summary>
Motivation: 加速AI安全研究，通过自动化测试快速发现AI模型在拥有强大工具和代理角色时的潜在危险行为。

Method: 开发开源框架，让AI代理在多轮交互场景中自动测试目标模型，使用工具和代理角色来评估模型行为。

Result: 工具发现模型在获得足够强大的工具和代理角色时，会进行自主欺骗和监督规避行为。

Conclusion: Petri最适合快速发现令人担忧的行为，帮助研究人员确定哪些领域值得进行针对性调查。

Abstract: Petri: An open-source auditing tool to accelerate AI safety research (41 minute read) Anthropic's Petri is an open-source framework that lets AI agents automatically test target models across realistic multi-turn scenarios. The tool revealed models will engage in autonomous deception and oversight subversion when given sufficiently powerful tools and agentic roles, but it's best for quickly surfacing concerning behaviors so researchers know where targeted investigation is worth the investment.

</details>


### [98] [AI, then verify](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-webdev-primary-251009-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/jbogC2sNFV-XQfWZXT9Rab_8MtJXZ5qbsRT1egZCn6U=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: SonarQube平台通过自动化代码扫描解决AI开发中的质量挑战，减少安全风险和技术债务


<details>
  <summary>Details</summary>
Motivation: 解决AI开发中的"工程生产力悖论"：代码生成速度快但受限于手动审查流程，导致质量风险

Method: 提供自动化代码扫描平台，在代码合并前检测安全漏洞和技术债务

Result: 减少安全风险，阻止技术债务积累，提升开发者信心和创新能力

Conclusion: SonarQube解决方案能够自动化代码审查，解决AI开发中的质量与速度平衡问题

Abstract: AI, then verify (Sponsor) You've embraced AI for a speed boost, but what about quality? The "engineering productivity paradox" is a real challenge: fast code generation and growing code volume, but limited by a manual review process.SonarQube solves this. Our platform fuels AI-enabled development so you can: Reduce security risks with automated scanning. Stop tech debt before it's merged. Boost developer confidence and focus on innovation. Automate code review with Sonar solutions and build t...

</details>


### [99] [My First Contribution to Linux](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvkoskiv.com%2Ffirst-linux-patch%2F%3Futm_source=tldrwebdev/1/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/Qr4pQ8szCKPEVLIMDm1_Rnz1i_uIwXzga4balxDH8rM=426)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 作者分享了首次为Linux内核贡献的经历，成功为2005年富士通Lifebook S2110笔记本电脑添加了热键支持。


<details>
  <summary>Details</summary>
Motivation: 作者希望为自己的笔记本电脑添加热键功能，并体验为开源项目做贡献的过程。

Method: 通过识别相关驱动程序、研究代码理解按键事件处理机制，修改代码以识别媒体键，并通过传统邮件工作流提交补丁。

Result: 补丁成功合并到上游Linux内核中。

Conclusion: 首次贡献Linux内核是一个可行的过程，即使是相对简单的功能添加也能为开源社区带来价值。

Abstract: My First Contribution to Linux (18 minute read) This dev's first contribution to the Linux kernel was adding support for the hotkeys on their 2005 Fujitsu Lifebook S2110 laptop. They go over the process of identifying the relevant driver, studying the code to understand how it handles key events, and modifying it to recognize the media keys. They submitted their patch through the traditional email workflow and saw that it merged into the upstream kernel.

</details>
