{"id": "2602.10133", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10133", "abs": "https://arxiv.org/abs/2602.10133", "authors": ["Adam AlSayyad", "Kelvin Yuxiang Huang", "Richik Pal"], "title": "AgentTrace: A Structured Logging Framework for Agent System Observability", "comment": "AAAI 2026 Workshop LaMAS", "summary": "Despite the growing capabilities of autonomous agents powered by large language models (LLMs), their adoption in high-stakes domains remains limited. A key barrier is security: the inherently nondeterministic behavior of LLM agents defies static auditing approaches that have historically underpinned software assurance. Existing security methods, such as proxy-level input filtering and model glassboxing, fail to provide sufficient transparency or traceability into agent reasoning, state changes, or environmental interactions. In this work, we introduce AgentTrace, a dynamic observability and telemetry framework designed to fill this gap. AgentTrace instruments agents at runtime with minimal overhead, capturing a rich stream of structured logs across three surfaces: operational, cognitive, and contextual. Unlike traditional logging systems, AgentTrace emphasizes continuous, introspectable trace capture, designed not just for debugging or benchmarking, but as a foundational layer for agent security, accountability, and real-time monitoring. Our research highlights how AgentTrace can enable more reliable agent deployment, fine-grained risk analysis, and informed trust calibration, thereby addressing critical concerns that have so far limited the use of LLM agents in sensitive environments.", "AI": {"tldr": "AgentTrace\u662f\u4e00\u4e2a\u52a8\u6001\u53ef\u89c2\u6d4b\u6027\u548c\u9065\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u68c0\u6d4bLLM\u4ee3\u7406\uff0c\u6355\u83b7\u7ed3\u6784\u5316\u65e5\u5fd7\uff0c\u89e3\u51b3\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u7684\u5b89\u5168\u548c\u53ef\u8ffd\u6eaf\u6027\u95ee\u9898\u3002", "motivation": "LLM\u4ee3\u7406\u7684\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u4f7f\u5f97\u4f20\u7edf\u9759\u6001\u5ba1\u8ba1\u65b9\u6cd5\u5931\u6548\uff0c\u73b0\u6709\u5b89\u5168\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u900f\u660e\u5ea6\u6216\u53ef\u8ffd\u6eaf\u6027\uff0c\u9650\u5236\u4e86LLM\u4ee3\u7406\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u91c7\u7528\u3002", "method": "AgentTrace\u5728\u8fd0\u884c\u65f6\u4ee5\u6700\u5c0f\u5f00\u9500\u68c0\u6d4b\u4ee3\u7406\uff0c\u6355\u83b7\u4e09\u4e2a\u5c42\u9762\u7684\u7ed3\u6784\u5316\u65e5\u5fd7\uff1a\u64cd\u4f5c\u5c42\u9762\u3001\u8ba4\u77e5\u5c42\u9762\u548c\u4e0a\u4e0b\u6587\u5c42\u9762\uff0c\u5f3a\u8c03\u8fde\u7eed\u3001\u53ef\u5185\u7701\u7684\u8ddf\u8e2a\u6355\u83b7\u3002", "result": "AgentTrace\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u4ee3\u7406\u90e8\u7f72\u3001\u7ec6\u7c92\u5ea6\u98ce\u9669\u5206\u6790\u548c\u77e5\u60c5\u4fe1\u4efb\u6821\u51c6\uff0c\u4e3a\u4ee3\u7406\u5b89\u5168\u3001\u95ee\u8d23\u548c\u5b9e\u65f6\u76d1\u63a7\u63d0\u4f9b\u57fa\u7840\u5c42\u3002", "conclusion": "AgentTrace\u901a\u8fc7\u52a8\u6001\u53ef\u89c2\u6d4b\u6027\u548c\u9065\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u654f\u611f\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u5173\u952e\u5b89\u5168\u95ee\u9898\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u91c7\u7528LLM\u4ee3\u7406\u94fa\u5e73\u9053\u8def\u3002", "topic": "agent analysis"}}
{"id": "2602.10140", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.10140", "abs": "https://arxiv.org/abs/2602.10140", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Jo\u00e3o P. Matos-Carvalho"], "title": "Can Large Language Models Implement Agent-Based Models? An ODD-based Replication Study", "comment": null, "summary": "Large language models (LLMs) can now synthesize non-trivial executable code from textual descriptions, raising an important question: can LLMs reliably implement agent-based models from standardized specifications in a way that supports replication, verification, and validation? We address this question by evaluating 17 contemporary LLMs on a controlled ODD-to-code translation task, using the PPHPC predator-prey model as a fully specified reference. Generated Python implementations are assessed through staged executability checks, model-independent statistical comparison against a validated NetLogo baseline, and quantitative measures of runtime efficiency and maintainability. Results show that behaviorally faithful implementations are achievable but not guaranteed, and that executability alone is insufficient for scientific use. GPT-4.1 consistently produces statistically valid and efficient implementations, with Claude 3.7 Sonnet performing well but less reliably. Overall, the findings clarify both the promise and current limitations of LLMs as model engineering tools, with implications for reproducible agent-based and environmental modelling.", "AI": {"tldr": "\u8bc4\u4f3017\u4e2a\u5f53\u4ee3LLM\u5728ODD\u5230\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528PPHPC\u6355\u98df\u8005-\u730e\u7269\u6a21\u578b\u4f5c\u4e3a\u53c2\u8003\uff0c\u53d1\u73b0GPT-4.1\u80fd\u751f\u6210\u7edf\u8ba1\u6709\u6548\u4e14\u9ad8\u6548\u7684\u5b9e\u73b0\uff0c\u4f46\u53ef\u6267\u884c\u6027\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u79d1\u5b66\u7528\u9014\u3002", "motivation": "\u968f\u7740LLM\u80fd\u591f\u4ece\u6587\u672c\u63cf\u8ff0\u5408\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u80fd\u5426\u4ece\u6807\u51c6\u5316\u89c4\u8303\u53ef\u9760\u5730\u5b9e\u73b0\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u590d\u5236\u3001\u9a8c\u8bc1\u548c\u786e\u8ba4\u3002", "method": "\u4f7f\u7528PPHPC\u6355\u98df\u8005-\u730e\u7269\u6a21\u578b\u4f5c\u4e3a\u5b8c\u5168\u6307\u5b9a\u7684\u53c2\u8003\uff0c\u8bc4\u4f3017\u4e2a\u5f53\u4ee3LLM\u5728\u53d7\u63a7\u7684ODD\u5230\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u751f\u6210\u7684Python\u5b9e\u73b0\u901a\u8fc7\u5206\u9636\u6bb5\u53ef\u6267\u884c\u6027\u68c0\u67e5\u3001\u4e0e\u5df2\u9a8c\u8bc1NetLogo\u57fa\u7ebf\u7684\u6a21\u578b\u65e0\u5173\u7edf\u8ba1\u6bd4\u8f83\uff0c\u4ee5\u53ca\u8fd0\u884c\u65f6\u6548\u7387\u548c\u53ef\u7ef4\u62a4\u6027\u7684\u5b9a\u91cf\u6d4b\u91cf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u884c\u4e3a\u4e0a\u5fe0\u5b9e\u7684\u5b9e\u73b0\u662f\u53ef\u5b9e\u73b0\u7684\u4f46\u5e76\u975e\u4fdd\u8bc1\uff0c\u53ef\u6267\u884c\u6027\u672c\u8eab\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u79d1\u5b66\u7528\u9014\u3002GPT-4.1\u59cb\u7ec8\u4ea7\u751f\u7edf\u8ba1\u6709\u6548\u4e14\u9ad8\u6548\u7684\u5b9e\u73b0\uff0cClaude 3.7 Sonnet\u8868\u73b0\u826f\u597d\u4f46\u53ef\u9760\u6027\u8f83\u4f4e\u3002", "conclusion": "LLM\u4f5c\u4e3a\u6a21\u578b\u5de5\u7a0b\u5de5\u5177\u65e2\u6709\u524d\u666f\u4e5f\u6709\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5bf9\u53ef\u91cd\u590d\u7684\u57fa\u4e8e\u4ee3\u7406\u548c\u73af\u5883\u5efa\u6a21\u5177\u6709\u542f\u793a\u610f\u4e49\u3002", "topic": "code agent"}}
{"id": "2602.10171", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10171", "abs": "https://arxiv.org/abs/2602.10171", "authors": ["Wentao Zhang", "Jianfeng Wang", "Liheng Liang", "Yilei Zhao", "HaiBin Wen", "Zhe Zhao"], "title": "EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems", "comment": null, "summary": "As large language models (LLMs) continue to advance in programming tasks, LLM-driven coding systems have evolved from one-shot code generation into complex systems capable of iterative improvement during inference. However, existing code benchmarks primarily emphasize static correctness and implicitly assume fixed model capability during inference. As a result, they do not capture inference-time self-evolution, such as whether accuracy and efficiency improve as an agent iteratively refines its solutions. They also provide limited accounting of resource costs and rarely calibrate model performance against that of human programmers. Moreover, many benchmarks are dominated by high-resource languages, leaving cross-language robustness and long-tail language stability underexplored. Therefore, we present EvoCodeBench, a benchmark for evaluating self-evolving LLM-driven coding systems across programming languages with direct comparison to human performance. EvoCodeBench tracks performance dynamics, measuring solution correctness alongside efficiency metrics such as solving time, memory consumption, and improvement algorithmic design over repeated problem-solving attempts. To ground evaluation in a human-centered reference frame, we directly compare model performance with that of human programmers on the same tasks, enabling relative performance assessment within the human ability distribution. Furthermore, EvoCodeBench supports multiple programming languages, enabling systematic cross-language and long-tail stability analyses under a unified protocol. Our results demonstrate that self-evolving systems exhibit measurable gains in efficiency over time, and that human-relative and multi-language analyses provide insights unavailable through accuracy alone. EvoCodeBench establishes a foundation for evaluating coding intelligence in evolving LLM-driven systems.", "AI": {"tldr": "EvoCodeBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u81ea\u8fdb\u5316LLM\u9a71\u52a8\u7f16\u7801\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u5b83\u8ffd\u8e2a\u6027\u80fd\u52a8\u6001\u3001\u6bd4\u8f83\u4eba\u7c7b\u8868\u73b0\uff0c\u5e76\u652f\u6301\u591a\u8bed\u8a00\u5206\u6790\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4ec5\u5173\u6ce8\u9759\u6001\u6b63\u786e\u6027\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u57fa\u51c6\u4e3b\u8981\u5f3a\u8c03\u9759\u6001\u6b63\u786e\u6027\uff0c\u5047\u8bbe\u63a8\u7406\u671f\u95f4\u6a21\u578b\u80fd\u529b\u56fa\u5b9a\uff0c\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u65f6\u7684\u81ea\u6211\u8fdb\u5316\uff08\u5982\u8fed\u4ee3\u6539\u8fdb\u89e3\u51b3\u65b9\u6848\u65f6\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u63d0\u5347\uff09\uff0c\u4e14\u8d44\u6e90\u6210\u672c\u6838\u7b97\u6709\u9650\uff0c\u5f88\u5c11\u5c06\u6a21\u578b\u6027\u80fd\u4e0e\u4eba\u7c7b\u7a0b\u5e8f\u5458\u6821\u51c6\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u57fa\u51c6\u88ab\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e3b\u5bfc\uff0c\u8de8\u8bed\u8a00\u9c81\u68d2\u6027\u548c\u957f\u5c3e\u8bed\u8a00\u7a33\u5b9a\u6027\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faEvoCodeBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u7f16\u7a0b\u8bed\u8a00\u7684\u81ea\u8fdb\u5316LLM\u9a71\u52a8\u7f16\u7801\u7cfb\u7edf\uff0c\u5e76\u4e0e\u4eba\u7c7b\u6027\u80fd\u76f4\u63a5\u6bd4\u8f83\u3002\u8be5\u57fa\u51c6\u8ffd\u8e2a\u6027\u80fd\u52a8\u6001\uff0c\u6d4b\u91cf\u89e3\u51b3\u65b9\u6848\u6b63\u786e\u6027\u4ee5\u53ca\u6548\u7387\u6307\u6807\uff08\u5982\u89e3\u51b3\u65f6\u95f4\u3001\u5185\u5b58\u6d88\u8017\u548c\u91cd\u590d\u95ee\u9898\u89e3\u51b3\u5c1d\u8bd5\u4e2d\u7684\u6539\u8fdb\u7b97\u6cd5\u8bbe\u8ba1\uff09\u3002\u901a\u8fc7\u76f4\u63a5\u6bd4\u8f83\u6a21\u578b\u4e0e\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u76f8\u5bf9\u6027\u80fd\u8bc4\u4f30\u3002\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5728\u7edf\u4e00\u534f\u8bae\u4e0b\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u8de8\u8bed\u8a00\u548c\u957f\u5c3e\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u81ea\u8fdb\u5316\u7cfb\u7edf\u5728\u6548\u7387\u65b9\u9762\u968f\u65f6\u95f4\u663e\u793a\u51fa\u53ef\u6d4b\u91cf\u7684\u589e\u76ca\uff0c\u4eba\u7c7b\u76f8\u5bf9\u548c\u591a\u8bed\u8a00\u5206\u6790\u63d0\u4f9b\u4e86\u4ec5\u901a\u8fc7\u51c6\u786e\u6027\u65e0\u6cd5\u83b7\u5f97\u7684\u89c1\u89e3\u3002EvoCodeBench\u4e3a\u8bc4\u4f30\u8fdb\u5316\u4e2d\u7684LLM\u9a71\u52a8\u7cfb\u7edf\u7684\u7f16\u7801\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "EvoCodeBench\u5efa\u7acb\u4e86\u4e00\u4e2a\u8bc4\u4f30\u8fdb\u5316LLM\u9a71\u52a8\u7cfb\u7edf\u4e2d\u7f16\u7801\u667a\u80fd\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6027\u80fd\u52a8\u6001\u3001\u6bd4\u8f83\u4eba\u7c7b\u8868\u73b0\u548c\u591a\u8bed\u8a00\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u8d85\u8d8a\u4f20\u7edf\u51c6\u786e\u6027\u8bc4\u4f30\u7684\u5168\u9762\u89c6\u89d2\u3002", "topic": "swe benchmark"}}
{"id": "2602.10324", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.10324", "abs": "https://arxiv.org/abs/2602.10324", "authors": ["Caroline Wang", "Daniel Kasenberg", "Kim Stachenfeld", "Pablo Samuel Castro"], "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to directly discover interpretable models of human and LLM behavior from data, thereby enabling open-ended discovery of structural factors driving human and LLM behavior. Our analysis on iterated rock-paper-scissors reveals that frontier LLMs can be capable of deeper strategic behavior than humans. These results provide a foundation for understanding structural differences driving differences in human and LLM behavior in strategic interactions.", "AI": {"tldr": "\u4f7f\u7528AlphaEvolve\u7a0b\u5e8f\u53d1\u73b0\u5de5\u5177\uff0c\u4ece\u6570\u636e\u4e2d\u76f4\u63a5\u53d1\u73b0\u4eba\u7c7b\u548cLLM\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u63ed\u793a\u5728\u8fed\u4ee3\u77f3\u5934\u526a\u5200\u5e03\u6e38\u620f\u4e2d\uff0c\u524d\u6cbfLLM\u5c55\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u6df1\u5c42\u6b21\u7684\u7b56\u7565\u884c\u4e3a\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u793e\u4ea4\u548c\u7b56\u7565\u573a\u666f\u4e2d\uff0c\u7406\u89e3\u5176\u884c\u4e3a\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5dee\u5f02\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u884c\u4e3a\u535a\u5f08\u8bba\u6a21\u578b\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4eba\u7c7b\u6216LLM\u7b49\u9ed1\u76d2\u975e\u4eba\u7c7b\u667a\u80fd\u4f53\u7684\u7279\u6b8a\u884c\u4e3a\u3002", "method": "\u91c7\u7528AlphaEvolve\u8fd9\u4e00\u5148\u8fdb\u7684\u7a0b\u5e8f\u53d1\u73b0\u5de5\u5177\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u4eba\u7c7b\u548cLLM\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u4eba\u7c7b\u548cLLM\u884c\u4e3a\u9a71\u52a8\u56e0\u7d20\u7684\u65e0\u9650\u5236\u53d1\u73b0\u3002", "result": "\u5728\u8fed\u4ee3\u77f3\u5934\u526a\u5200\u5e03\u6e38\u620f\u7684\u5206\u6790\u4e2d\uff0c\u53d1\u73b0\u524d\u6cbfLLM\u80fd\u591f\u5c55\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u6df1\u5c42\u6b21\u7684\u7b56\u7565\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u7406\u89e3\u7b56\u7565\u4e92\u52a8\u4e2d\u4eba\u7c7b\u548cLLM\u884c\u4e3a\u5dee\u5f02\u7684\u7ed3\u6784\u6027\u9a71\u52a8\u56e0\u7d20\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.10471", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10471", "abs": "https://arxiv.org/abs/2602.10471", "authors": ["Steven Liu", "Jane Luo", "Xin Zhang", "Aofan Liu", "Hao Liu", "Jie Wu", "Ziyang Huang", "Yangyu Huang", "Yu Kang", "Scarlett Li"], "title": "TestExplora: Benchmarking LLMs for Proactive Bug Discovery via Repository-Level Test Generation", "comment": null, "summary": "Given that Large Language Models (LLMs) are increasingly applied to automate software development, comprehensive software assurance spans three distinct goals: regression prevention, reactive reproduction, and proactive discovery. Current evaluations systematically overlook the third goal. Specifically, they either treat existing code as ground truth (a compliance trap) for regression prevention, or depend on post-failure artifacts (e.g., issue reports) for bug reproduction-so they rarely surface defects before failures. To bridge this gap, we present TestExplora, a benchmark designed to evaluate LLMs as proactive testers within full-scale, realistic repository environments. TestExplora contains 2,389 tasks from 482 repositories and hides all defect-related signals. Models must proactively find bugs by comparing implementations against documentation-derived intent, using documentation as the oracle. Furthermore, to keep evaluation sustainable and reduce leakage, we propose continuous, time-aware data collection. Our evaluation reveals a significant capability gap: state-of-the-art models achieve a maximum Fail-to-Pass (F2P) rate of only 16.06%. Further analysis indicates that navigating complex cross-module interactions and leveraging agentic exploration are critical to advancing LLMs toward autonomous software quality assurance. Consistent with this, SWEAgent instantiated with GPT-5-mini achieves an F2P of 17.27% and an F2P@5 of 29.7%, highlighting the effectiveness and promise of agentic exploration in proactive bug discovery tasks.", "AI": {"tldr": "TestExplora\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u4f5c\u4e3a\u4e3b\u52a8\u6d4b\u8bd5\u8005\u7684\u57fa\u51c6\uff0c\u5305\u542b2389\u4e2a\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u73b0\u4e0e\u6587\u6863\u610f\u56fe\u6765\u53d1\u73b0bug\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6700\u5927F2P\u7387\u4ec5\u4e3a16.06%", "motivation": "\u5f53\u524dLLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u56de\u5f52\u9884\u9632\u548cbug\u590d\u73b0\uff0c\u5ffd\u89c6\u4e86\u4e3b\u52a8\u53d1\u73b0\u7f3a\u9677\u7684\u76ee\u6807\uff0c\u5b58\u5728\"\u5408\u89c4\u9677\u9631\"\u95ee\u9898\uff0c\u9700\u8981\u8bc4\u4f30LLMs\u5728\u771f\u5b9e\u4ed3\u5e93\u73af\u5883\u4e2d\u4f5c\u4e3a\u4e3b\u52a8\u6d4b\u8bd5\u8005\u7684\u80fd\u529b", "method": "\u63d0\u51faTestExplora\u57fa\u51c6\uff0c\u5305\u542b2389\u4e2a\u4efb\u52a1\u6765\u81ea482\u4e2a\u4ed3\u5e93\uff0c\u9690\u85cf\u6240\u6709\u7f3a\u9677\u76f8\u5173\u4fe1\u53f7\uff0c\u8981\u6c42\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u73b0\u4e0e\u6587\u6863\u63a8\u5bfc\u7684\u610f\u56fe\u6765\u53d1\u73b0bug\uff1b\u91c7\u7528\u6301\u7eed\u3001\u65f6\u95f4\u611f\u77e5\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u9632\u6b62\u6570\u636e\u6cc4\u9732", "result": "\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6700\u5927Fail-to-Pass\u7387\u4ec5\u4e3a16.06%\uff1bSWEAgent\u914d\u5408GPT-5-mini\u8fbe\u523017.27%\u7684F2P\u548c29.7%\u7684F2P@5\uff0c\u8868\u660e\u667a\u80fd\u4f53\u63a2\u7d22\u5728\u4e3b\u52a8bug\u53d1\u73b0\u4efb\u52a1\u4e2d\u6709\u6548", "conclusion": "LLMs\u5728\u4e3b\u52a8\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u5904\u7406\u590d\u6742\u8de8\u6a21\u5757\u4ea4\u4e92\u548c\u5229\u7528\u667a\u80fd\u4f53\u63a2\u7d22\u662f\u63d0\u5347\u80fd\u529b\u7684\u5173\u952e\u65b9\u5411", "topic": "swe benchmark"}}
{"id": "2602.10177", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.10177", "abs": "https://arxiv.org/abs/2602.10177", "authors": ["Tony Feng", "Trieu H. Trinh", "Garrett Bingham", "Dawsen Hwang", "Yuri Chervonyi", "Junehyuk Jung", "Joonkyung Lee", "Carlo Pagano", "Sang-hyun Kim", "Federico Pasqualotto", "Sergei Gukov", "Jonathan N. Lee", "Junsu Kim", "Kaiying Hou", "Golnaz Ghiasi", "Yi Tay", "YaGuang Li", "Chenkai Kuang", "Yuan Liu", "Hanzhao", "Lin", "Evan Zheran Liu", "Nigamaa Nayakanti", "Xiaomeng Yang", "Heng-tze Cheng", "Demis Hassabis", "Koray Kavukcuoglu", "Quoc V. Le", "Thang Luong"], "title": "Towards Autonomous Mathematics Research", "comment": "Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia", "summary": "Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.", "AI": {"tldr": "Aletheia\u662f\u4e00\u4e2a\u6570\u5b66\u7814\u7a76\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u4ece\u5965\u6570\u9898\u5230\u535a\u58eb\u7ea7\u95ee\u9898\uff0c\u751a\u81f3\u534f\u52a9\u751f\u6210\u7814\u7a76\u8bba\u6587\u548c\u89e3\u51b3\u5f00\u653e\u95ee\u9898\uff0c\u5c55\u793a\u4e86AI\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u7cfb\u7edf\u867d\u7136\u80fd\u5728\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u8fbe\u5230\u91d1\u724c\u6c34\u5e73\uff0c\u4f46\u4ece\u7ade\u8d5b\u7ea7\u95ee\u9898\u89e3\u51b3\u8fc7\u6e21\u5230\u4e13\u4e1a\u7814\u7a76\u9700\u8981\u5904\u7406\u5927\u91cf\u6587\u732e\u548c\u6784\u5efa\u957f\u7a0b\u8bc1\u660e\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u6570\u5b66\u7814\u7a76\u590d\u6742\u6027\u7684\u667a\u80fd\u4f53\u3002", "method": "Aletheia\u91c7\u7528\u8fed\u4ee3\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u4fee\u8ba2\u89e3\u51b3\u65b9\u6848\u7684\u7aef\u5230\u7aef\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408Gemini Deep Think\u8fdb\u884c\u6311\u6218\u6027\u63a8\u7406\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u5b9a\u5f8b\uff0c\u5e76\u5bc6\u96c6\u4f7f\u7528\u5de5\u5177\u6765\u5e94\u5bf9\u6570\u5b66\u7814\u7a76\u7684\u590d\u6742\u6027\u3002", "result": "\u5c55\u793a\u4e86\u4e09\u4e2a\u91cc\u7a0b\u7891\uff1a1) AI\u5b8c\u5168\u81ea\u4e3b\u751f\u6210\u7684\u7814\u7a76\u8bba\u6587(Feng26)\uff1b2) \u4eba\u673a\u534f\u4f5c\u8bc1\u660e\u72ec\u7acb\u96c6\u8fb9\u754c\u7684\u7814\u7a76\u8bba\u6587(LeeSeo26)\uff1b3) \u5bf9700\u4e2a\u5f00\u653e\u95ee\u9898\u7684\u534a\u81ea\u4e3b\u8bc4\u4f30\uff0c\u5305\u62ec\u81ea\u4e3b\u89e3\u51b3\u56db\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u9700\u8981\u5236\u5b9a\u6807\u51c6\u6765\u91cf\u5316AI\u8f85\u52a9\u7ed3\u679c\u7684\u81ea\u4e3b\u6027\u548c\u65b0\u9896\u6027\uff0c\u5e76\u5bf9\u4eba\u673a\u534f\u4f5c\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u524d\u666f\u8fdb\u884c\u4e86\u53cd\u601d\u3002", "topic": "code agent"}}
{"id": "2602.10479", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10479", "abs": "https://arxiv.org/abs/2602.10479", "authors": ["Mamdouh Alenezi"], "title": "From Prompt-Response to Goal-Directed Systems: The Evolution of Agentic AI Software Architecture", "comment": null, "summary": "Agentic AI denotes an architectural transition from stateless, prompt-driven generative models toward goal-directed systems capable of autonomous perception, planning, action, and adaptation through iterative control loops. This paper examines this transition by connecting foundational intelligent agent theories, including reactive, deliberative, and Belief-Desire-Intention models, with contemporary LLM-centric approaches such as tool invocation, memory-augmented reasoning, and multi-agent coordination. The paper presents three primary contributions: (i) a reference architecture for production-grade LLM agents that separates cognitive reasoning from execution using typed tool interfaces; (ii) a taxonomy of multi-agent topologies, together with their associated failure modes and mitigation approaches; and (iii) an enterprise hardening checklist that incorporates governance, observability, and reproducibility considerations. Through an analysis of emerging industry platforms, including Kore.ai, Salesforce Agentforce, TrueFoundry, ZenML, and LangChain, the study identifies a convergence toward standardized agent loops, registries, and auditable control mechanisms. It is argued that the subsequent phase of agentic AI development will parallel the maturation of web services, relying on shared protocols, typed contracts, and layered governance structures to support scalable and composable autonomy. The persistent challenges related to verifiability, interoperability, and safe autonomy remain key areas for future research and practical deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4ece\u9759\u6001\u751f\u6210\u6a21\u578b\u5411\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u67b6\u6784\u8f6c\u578b\uff0c\u63d0\u51fa\u4e86\u751f\u4ea7\u7ea7LLM\u4ee3\u7406\u7684\u53c2\u8003\u67b6\u6784\u3001\u591a\u4ee3\u7406\u62d3\u6251\u5206\u7c7b\u548c\u4f01\u4e1a\u7ea7\u52a0\u56fa\u6e05\u5355\uff0c\u5206\u6790\u4e86\u884c\u4e1a\u5e73\u53f0\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u4ee3\u7406\u4ece\u4f20\u7edf\u7406\u8bba\uff08\u53cd\u5e94\u5f0f\u3001\u5ba1\u614e\u5f0f\u3001BDI\u6a21\u578b\uff09\u5411\u73b0\u4ee3LLM\u4e2d\u5fc3\u5316\u65b9\u6cd5\u7684\u6f14\u8fdb\uff0c\u89e3\u51b3\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u9760\u6027\u548c\u4f01\u4e1a\u7ea7\u90e8\u7f72\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8fde\u63a5\u4f20\u7edf\u667a\u80fd\u4ee3\u7406\u7406\u8bba\u4e0e\u5f53\u4ee3LLM\u65b9\u6cd5\uff0c\u5206\u6790\u884c\u4e1a\u5e73\u53f0\uff08Kore.ai\u3001Salesforce Agentforce\u7b49\uff09\uff0c\u63d0\u51fa\u53c2\u8003\u67b6\u6784\u3001\u62d3\u6251\u5206\u7c7b\u548c\u52a0\u56fa\u6e05\u5355\u3002", "result": "\u8bc6\u522b\u51fa\u884c\u4e1a\u5411\u6807\u51c6\u5316\u4ee3\u7406\u5faa\u73af\u3001\u6ce8\u518c\u8868\u548c\u53ef\u5ba1\u8ba1\u63a7\u5236\u673a\u5236\u7684\u8d8b\u540c\u8d8b\u52bf\uff0c\u63d0\u51fa\u4e86\u652f\u6301\u53ef\u6269\u5c55\u7ec4\u5408\u81ea\u4e3b\u6027\u7684\u5171\u4eab\u534f\u8bae\u3001\u7c7b\u578b\u5316\u5951\u7ea6\u548c\u5206\u5c42\u6cbb\u7406\u7ed3\u6784\u7684\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u4ee3\u7406\u5f0fAI\u53d1\u5c55\u5c06\u7c7b\u4f3cWeb\u670d\u52a1\u7684\u6210\u719f\u8fc7\u7a0b\uff0c\u9700\u8981\u5171\u4eab\u534f\u8bae\u548c\u5206\u5c42\u6cbb\u7406\u7ed3\u6784\uff0c\u4f46\u53ef\u9a8c\u8bc1\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u5b89\u5168\u81ea\u4e3b\u6027\u4ecd\u662f\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2602.10238", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10238", "abs": "https://arxiv.org/abs/2602.10238", "authors": ["Luca Moschella", "Laura Manduchi", "Ozan Sener"], "title": "Learning to Evict from Key-Value Cache", "comment": "23 pages, 15 figures", "summary": "The growing size of Large Language Models (LLMs) makes efficient inference challenging, primarily due to the memory demands of the autoregressive Key-Value (KV) cache. Existing eviction or compression methods reduce cost but rely on heuristics, such as recency or past attention scores, which serve only as indirect proxies for a token's future utility and introduce computational overhead. We reframe KV cache eviction as a reinforcement learning (RL) problem: learning to rank tokens by their predicted usefulness for future decoding. To this end, we introduce KV Policy (KVP), a framework of lightweight per-head RL agents trained on pre-computed generation traces using only key and value vectors. Each agent learns a specialized eviction policy guided by future utility, which evaluates the quality of the ranking across all cache budgets, requiring no modifications to the underlying LLM or additional inference. Evaluated across two different model families on the long-context benchmark RULER and the multi-turn dialogue benchmark OASST2-4k, KVP significantly outperforms baselines. Furthermore, zero-shot tests on standard downstream tasks (e.g., LongBench, BOOLQ, ARC) indicate that KVP generalizes well beyond its training distribution and to longer context lengths. These results demonstrate that learning to predict future token utility is a powerful and scalable paradigm for adaptive KV cache management.", "AI": {"tldr": "\u63d0\u51faKVP\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6765\u9884\u6d4btoken\u7684\u672a\u6765\u6548\u7528\uff0c\u5b9e\u73b0KV\u7f13\u5b58\u7684\u81ea\u9002\u5e94\u7ba1\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65f6KV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u73b0\u6709\u57fa\u4e8e\u542f\u53d1\u5f0f\uff08\u5982\u6700\u8fd1\u6027\u3001\u6ce8\u610f\u529b\u5206\u6570\uff09\u7684\u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\u53ea\u662f\u95f4\u63a5\u4ee3\u7406token\u672a\u6765\u6548\u7528\uff0c\u4e14\u5f15\u5165\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u5c06KV\u7f13\u5b58\u6dd8\u6c70\u91cd\u6784\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5f15\u5165KVP\u6846\u67b6\uff1a\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6bcf\u5934RL\u4ee3\u7406\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u751f\u6210\u8f68\u8ff9\u548c\u4ec5\u952e\u503c\u5411\u91cf\uff0c\u5b66\u4e60\u57fa\u4e8e\u672a\u6765\u6548\u7528\u7684\u4e13\u95e8\u6dd8\u6c70\u7b56\u7565\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42LLM\u6216\u589e\u52a0\u63a8\u7406\u5f00\u9500\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u4e0a\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6RULER\u548c\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6OASST2-4k\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002\u5728\u6807\u51c6\u4e0b\u6e38\u4efb\u52a1\uff08LongBench\u3001BOOLQ\u3001ARC\uff09\u7684\u96f6\u6837\u672c\u6d4b\u8bd5\u4e2d\uff0cKVP\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u9002\u5e94\u66f4\u957f\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u5b66\u4e60\u9884\u6d4btoken\u672a\u6765\u6548\u7528\u662f\u81ea\u9002\u5e94KV\u7f13\u5b58\u7ba1\u7406\u7684\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u8303\u5f0f\uff0cKVP\u6846\u67b6\u901a\u8fc7RL\u4ee3\u7406\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u5185\u5b58\u6548\u7387\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10458", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10458", "abs": "https://arxiv.org/abs/2602.10458", "authors": ["Yansong Qu", "Zihao Sheng", "Zilin Huang", "Jiancong Chen", "Yuhao Luo", "Tianyi Wang", "Yiheng Feng", "Samuel Labi", "Sikai Chen"], "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving", "comment": "39 pages", "summary": "Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we present Found-RL, a platform tailored to efficiently enhance RL for AD using foundation models. A core innovation is the asynchronous batch inference framework, which decouples heavy VLM reasoning from the simulation loop, effectively resolving latency bottlenecks to support real-time learning. We introduce diverse supervision mechanisms: Value-Margin Regularization (VMR) and Advantage-Weighted Action Guidance (AWAG) to effectively distill expert-like VLM action suggestions into the RL policy. Additionally, we adopt high-throughput CLIP for dense reward shaping. We address CLIP's dynamic blindness via Conditional Contrastive Action Alignment, which conditions prompts on discretized speed/command and yields a normalized, margin-based bonus from context-specific action-anchor scoring. Found-RL provides an end-to-end pipeline for fine-tuned VLM integration and shows that a lightweight RL model can achieve near-VLM performance compared with billion-parameter VLMs while sustaining real-time inference (approx. 500 FPS). Code, data, and models will be publicly available at https://github.com/ys-qu/found-rl.", "AI": {"tldr": "Found-RL\u662f\u4e00\u4e2a\u4e13\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u5f02\u6b65\u6279\u5904\u7406\u63a8\u7406\u6846\u67b6\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\u9ad8\u6548\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u89e3\u51b3\u4e86VLM\u63a8\u7406\u5ef6\u8fdf\u95ee\u9898\uff0c\u4f7f\u8f7b\u91cf\u7ea7RL\u6a21\u578b\u80fd\u8fbe\u5230\u63a5\u8fd1VLM\u7684\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u867d\u7136\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u4f46\u5176\u9ad8\u63a8\u7406\u5ef6\u8fdf\u963b\u788d\u4e86\u5728\u9ad8\u9891RL\u8bad\u7ec3\u5faa\u73af\u4e2d\u7684\u90e8\u7f72\u3002", "method": "1. \u5f02\u6b65\u6279\u5904\u7406\u63a8\u7406\u6846\u67b6\uff1a\u5c06\u7e41\u91cd\u7684VLM\u63a8\u7406\u4e0e\u4eff\u771f\u5faa\u73af\u89e3\u8026\uff1b2. \u76d1\u7763\u673a\u5236\uff1a\u503c\u8fb9\u754c\u6b63\u5219\u5316\uff08VMR\uff09\u548c\u4f18\u52bf\u52a0\u6743\u52a8\u4f5c\u6307\u5bfc\uff08AWAG\uff09\u6765\u84b8\u998fVLM\u52a8\u4f5c\u5efa\u8bae\uff1b3. \u91c7\u7528\u9ad8\u541e\u5410\u91cfCLIP\u8fdb\u884c\u5bc6\u96c6\u5956\u52b1\u5851\u9020\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u5bf9\u6bd4\u52a8\u4f5c\u5bf9\u9f50\u89e3\u51b3CLIP\u7684\u52a8\u6001\u76f2\u533a\u95ee\u9898\u3002", "result": "\u8f7b\u91cf\u7ea7RL\u6a21\u578b\u80fd\u8fbe\u5230\u63a5\u8fd1VLM\u7684\u6027\u80fd\uff08\u4e0e\u6570\u5341\u4ebf\u53c2\u6570\u7684VLM\u76f8\u6bd4\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\uff08\u7ea6500 FPS\uff09\u3002", "conclusion": "Found-RL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLM\u4e0eRL\u96c6\u6210\u4e2d\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u4f7f\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u591f\u5229\u7528VLM\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10522", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10522", "abs": "https://arxiv.org/abs/2602.10522", "authors": ["Hamed Taherkhani", "Alireza DaghighFarsoodeh", "Mohammad Chowdhury", "Hung Viet Pham", "Hadi Hemmati"], "title": "Consistency Meets Verification: Enhancing Test Generation Quality in Large Language Models Without Ground-Truth Solutions", "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced automated test generation, yet existing methods often rely on ground-truth code for verification, risking bug propagation and limiting applicability in test-driven development. We present ConVerTest, a novel two-stage pipeline for synthesizing reliable tests without requiring prior code implementations. ConVerTest integrates three core strategies: (i) Self-Consistency(SC) to generate convergent test cases via majority voting; (ii) Chain-of-Verification (CoVe) for iterative, reasoning-guided code refinement; and (iii) a Dual Execution Agreement to crossvalidate code and tests through consensus. Experiments on BIGCODEBENCH and LESS BASIC PYTHON PROBLEMS (LBPP) benchmarks demonstrate that ConVerTest improves test validity, line coverage, and mutation scores by up to 39%, 28%, and 18% respectively over baselines. Our findings highlight ConVerTest as a robust solution for mitigating hallucinations and enhancing the reliability of autonomous software testing agents.", "AI": {"tldr": "ConVerTest\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6d4b\u8bd5\u751f\u6210\u7ba1\u9053\uff0c\u65e0\u9700\u771f\u5b9e\u4ee3\u7801\u5373\u53ef\u5408\u6210\u53ef\u9760\u6d4b\u8bd5\uff0c\u901a\u8fc7\u81ea\u4e00\u81f4\u6027\u3001\u9a8c\u8bc1\u94fe\u548c\u53cc\u91cd\u6267\u884c\u534f\u8bae\u63d0\u9ad8\u6d4b\u8bd5\u6709\u6548\u6027\u3001\u884c\u8986\u76d6\u7387\u548c\u53d8\u5f02\u5206\u6570\u3002", "motivation": "\u73b0\u6709LLM\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9e\u4ee3\u7801\u8fdb\u884c\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\uff0c\u4e14\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u4e2d\u9002\u7528\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u4ee3\u7801\u5b9e\u73b0\u5373\u53ef\u751f\u6210\u53ef\u9760\u6d4b\u8bd5\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faConVerTest\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a1) \u4f7f\u7528\u81ea\u4e00\u81f4\u6027\u901a\u8fc7\u591a\u6570\u6295\u7968\u751f\u6210\u6536\u655b\u6d4b\u8bd5\u7528\u4f8b\uff1b2) \u4f7f\u7528\u9a8c\u8bc1\u94fe\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u5f15\u5bfc\u7684\u4ee3\u7801\u4f18\u5316\uff1b3) \u53cc\u91cd\u6267\u884c\u534f\u8bae\u901a\u8fc7\u5171\u8bc6\u4ea4\u53c9\u9a8c\u8bc1\u4ee3\u7801\u548c\u6d4b\u8bd5\u3002", "result": "\u5728BIGCODEBENCH\u548cLBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConVerTest\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u6709\u6548\u6027\u63d0\u9ad839%\uff0c\u884c\u8986\u76d6\u7387\u63d0\u9ad828%\uff0c\u53d8\u5f02\u5206\u6570\u63d0\u9ad818%\u3002", "conclusion": "ConVerTest\u662f\u7f13\u89e3\u5e7b\u89c9\u3001\u589e\u5f3a\u81ea\u4e3b\u8f6f\u4ef6\u6d4b\u8bd5\u4ee3\u7406\u53ef\u9760\u6027\u7684\u7a33\u5065\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u65e0\u9700\u771f\u5b9e\u4ee3\u7801\u7684\u6d4b\u8bd5\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2602.10467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10467", "abs": "https://arxiv.org/abs/2602.10467", "authors": ["Jihwan Oh", "Murad Aghazada", "Yooju Shin", "Se-Young Yun", "Taehyeon Kim"], "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators", "comment": "Preprint. arXiv admin note: substantial text overlap with arXiv:2505.22998", "summary": "Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g., deception, monopoly) that supports diverse strategy modeling; (ii) human-aligned, economically grounded metrics derived from utility theory. This is operationalized via agent utility, negotiation power, and acquisition ratio that implicitly measure how well the negotiation aligns with human preference and (iii) a human preference grounded dataset with learning pipeline that strengthens LLMs' bargaining ability through both prompting and finetuning. Empirical results indicate that baseline LLM strategies often diverge from human preferences, while our mechanism substantially improves negotiation performance, yielding deeper strategic behavior and stronger opponent awareness.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgoraBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6548\u7528\u53cd\u9988\u673a\u5236\u8bc4\u4f30LLM\u5728\u590d\u6742\u8c08\u5224\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b9\u79cd\u6311\u6218\u6027\u8bbe\u7f6e\u3001\u57fa\u4e8e\u6548\u7528\u7406\u8bba\u7684\u4eba\u7c7b\u5bf9\u9f50\u6307\u6807\uff0c\u4ee5\u53ca\u901a\u8fc7\u63d0\u793a\u548c\u5fae\u8c03\u589e\u5f3aLLM\u8c08\u5224\u80fd\u529b\u7684\u5b66\u4e60\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8c08\u5224\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u6218\u7565\u6df1\u5ea6\u548c\u9002\u5e94\u590d\u6742\u4eba\u7c7b\u56e0\u7d20\u7684\u80fd\u529b\uff0c\u800c\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u6355\u6349\u8fd9\u4e9b\u5c40\u9650\u6027\u3002\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63d0\u5347LLM\u7684\u8c08\u5224\u80fd\u529b\u3002", "method": "1) \u521b\u5efaAgoraBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u6b3a\u9a97\u3001\u5784\u65ad\u7b499\u79cd\u6311\u6218\u6027\u8c08\u5224\u573a\u666f\uff1b2) \u57fa\u4e8e\u6548\u7528\u7406\u8bba\u8bbe\u8ba1\u4eba\u7c7b\u5bf9\u9f50\u7684\u7ecf\u6d4e\u5b66\u6307\u6807\uff08\u4ee3\u7406\u6548\u7528\u3001\u8c08\u5224\u529b\u3001\u83b7\u53d6\u6bd4\u7387\uff09\uff1b3) \u6784\u5efa\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u548c\u5b66\u4e60\u6d41\u7a0b\uff0c\u901a\u8fc7\u63d0\u793a\u548c\u5fae\u8c03\u589e\u5f3aLLM\u8c08\u5224\u80fd\u529b\u3002", "result": "\u57fa\u7ebfLLM\u7b56\u7565\u901a\u5e38\u504f\u79bb\u4eba\u7c7b\u504f\u597d\uff0c\u800c\u63d0\u51fa\u7684\u673a\u5236\u663e\u8457\u6539\u5584\u4e86\u8c08\u5224\u6027\u80fd\uff0c\u4ea7\u751f\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u6218\u7565\u884c\u4e3a\u548c\u66f4\u5f3a\u7684\u5bf9\u624b\u610f\u8bc6\u3002", "conclusion": "\u901a\u8fc7\u6548\u7528\u53cd\u9988\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u548c\u63d0\u5347LLM\u5728\u590d\u6742\u8c08\u5224\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u548c\u7ecf\u6d4e\u7406\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.10620", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10620", "abs": "https://arxiv.org/abs/2602.10620", "authors": ["YoungHoon Jeon", "Suwan Kim", "Haein Son", "Sookbun Lee", "Yeil Jeong", "Unggi Lee"], "title": "ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents", "comment": null, "summary": "Large Language Model (LLM) agents have shown promising potential in automating Instructional Systems Design (ISD), a systematic approach to developing educational programs. However, evaluating these agents remains challenging due to the lack of standardized benchmarks and the risk of LLM-as-judge bias. We present ISD-Agent-Bench, a comprehensive benchmark comprising 25,795 scenarios generated via a Context Matrix framework that combines 51 contextual variables across 5 categories with 33 ISD sub-steps derived from the ADDIE model. To ensure evaluation reliability, we employ a multi-judge protocol using diverse LLMs from different providers, achieving high inter-judge reliability. We compare existing ISD agents with novel agents grounded in classical ISD theories such as ADDIE, Dick \\& Carey, and Rapid Prototyping ISD. Experiments on 1,017 test scenarios demonstrate that integrating classical ISD frameworks with modern ReAct-style reasoning achieves the highest performance, outperforming both pure theory-based agents and technique-only approaches. Further analysis reveals that theoretical quality strongly correlates with benchmark performance, with theory-based agents showing significant advantages in problem-centered design and objective-assessment alignment. Our work provides a foundation for systematic LLM-based ISD research.", "AI": {"tldr": "\u63d0\u51fa\u4e86ISD-Agent-Bench\u57fa\u51c6\uff0c\u5305\u542b25,795\u4e2a\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u6559\u5b66\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ed3\u5408\u7ecf\u5178ISD\u7406\u8bba\u548c\u73b0\u4ee3ReAct\u63a8\u7406\u7684\u4ee3\u7406\u6027\u80fd\u6700\u4f73\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u6559\u5b66\u8bbe\u8ba1\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u4e14\u5b58\u5728LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u504f\u89c1\u98ce\u9669\uff0c\u9700\u8981\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Context Matrix\u6846\u67b6\u751f\u621025,795\u4e2a\u573a\u666f\uff0c\u7ed3\u540851\u4e2a\u4e0a\u4e0b\u6587\u53d8\u91cf\u548c33\u4e2aADDIE\u5b50\u6b65\u9aa4\uff1b\u91c7\u7528\u591a\u8bc4\u5224\u534f\u8bae\u4f7f\u7528\u4e0d\u540c\u63d0\u4f9b\u5546\u7684LLM\u786e\u4fdd\u53ef\u9760\u6027\uff1b\u6bd4\u8f83\u57fa\u4e8e\u7ecf\u5178ISD\u7406\u8bba\uff08ADDIE\u3001Dick & Carey\u3001Rapid Prototyping\uff09\u7684\u4ee3\u7406\u4e0e\u73b0\u6709\u4ee3\u7406\u3002", "result": "\u57281,017\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u7ed3\u5408\u7ecf\u5178ISD\u6846\u67b6\u548c\u73b0\u4ee3ReAct\u63a8\u7406\u7684\u4ee3\u7406\u6027\u80fd\u6700\u9ad8\uff0c\u4f18\u4e8e\u7eaf\u7406\u8bba\u4ee3\u7406\u548c\u6280\u672f\u5bfc\u5411\u65b9\u6cd5\uff1b\u7406\u8bba\u8d28\u91cf\u4e0e\u57fa\u51c6\u8868\u73b0\u5f3a\u76f8\u5173\uff0c\u7406\u8bba\u4ee3\u7406\u5728\u95ee\u9898\u4e2d\u5fc3\u8bbe\u8ba1\u548c\u76ee\u6807\u8bc4\u4f30\u5bf9\u9f50\u65b9\u9762\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "\u7ecf\u5178ISD\u7406\u8bba\u4e0e\u73b0\u4ee3\u63a8\u7406\u6280\u672f\u7684\u7ed3\u5408\u5728\u6559\u5b66\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u7cfb\u7edf\u5316\u7684LLM-based ISD\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2602.10210", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10210", "abs": "https://arxiv.org/abs/2602.10210", "authors": ["Junhong Lin", "Bing Zhang", "Song Wang", "Ziyan Liu", "Dan Gutfreund", "Julian Shun", "Yada Zhu"], "title": "How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge", "comment": null, "summary": "Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86HybridRAG-Bench\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u6df7\u5408\u77e5\u8bc6\uff08\u975e\u7ed3\u6784\u5316\u6587\u672c+\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff09\u4e0a\u7684\u68c0\u7d22\u5bc6\u96c6\u578b\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u81ea\u52a8\u4ecearXiv\u6700\u65b0\u79d1\u5b66\u6587\u732e\u6784\u5efa\u57fa\u51c6\uff0c\u907f\u514d\u9884\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0eLLM\u9884\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\u4e25\u91cd\uff0c\u96be\u4ee5\u533a\u5206\u771f\u6b63\u7684\u68c0\u7d22\u63a8\u7406\u4e0e\u53c2\u6570\u8bb0\u5fc6\u3002\u9700\u8981\u6784\u5efa\u80fd\u8bc4\u4f30\u6df7\u5408\u77e5\u8bc6\u589e\u5f3a\u7cfb\u7edf\u771f\u5b9e\u68c0\u7d22\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u4ecearXiv\u6700\u65b0\u79d1\u5b66\u6587\u732e\u81ea\u52a8\u6784\u5efa\u6df7\u5408\u77e5\u8bc6\u8868\u793a\uff08\u975e\u7ed3\u6784\u5316\u6587\u672c+\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff09\uff0c\u751f\u6210\u57fa\u4e8e\u663e\u5f0f\u63a8\u7406\u8def\u5f84\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u5bf9\uff0c\u652f\u6301\u7075\u6d3b\u9886\u57df\u548c\u65f6\u95f4\u8303\u56f4\u9009\u62e9\u3002", "result": "\u5728\u4eba\u5de5\u667a\u80fd\u3001\u6cbb\u7406\u4e0e\u653f\u7b56\u3001\u751f\u7269\u4fe1\u606f\u5b66\u4e09\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHybridRAG-Bench\u80fd\u6709\u6548\u8bc4\u4f30\u771f\u5b9e\u7684\u68c0\u7d22\u63a8\u7406\u80fd\u529b\u800c\u975e\u53c2\u6570\u8bb0\u5fc6\uff0c\u4e3a\u6df7\u5408\u77e5\u8bc6\u589e\u5f3a\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u539f\u5219\u6027\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "HybridRAG-Bench\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u6df7\u5408\u77e5\u8bc6\u4e0a\u7684\u68c0\u7d22\u5bc6\u96c6\u578b\u591a\u8df3\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u5b9a\u5236\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.10598", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10598", "abs": "https://arxiv.org/abs/2602.10598", "authors": ["Shuai Han", "Mehdi Dastani", "Shihan Wang"], "title": "Neuro-symbolic Action Masking for Deep Reinforcement Learning", "comment": null, "summary": "Deep reinforcement learning (DRL) may explore infeasible actions during training and execution. Existing approaches assume a symbol grounding function that maps high-dimensional states to consistent symbolic representations and a manually specified action masking techniques to constrain actions. In this paper, we propose Neuro-symbolic Action Masking (NSAM), a novel framework that automatically learn symbolic models, which are consistent with given domain constraints of high-dimensional states, in a minimally supervised manner during the DRL process. Based on the learned symbolic model of states, NSAM learns action masks that rules out infeasible actions. NSAM enables end-to-end integration of symbolic reasoning and deep policy optimization, where improvements in symbolic grounding and policy learning mutually reinforce each other. We evaluate NSAM on multiple domains with constraints, and experimental results demonstrate that NSAM significantly improves sample efficiency of DRL agent while substantially reducing constraint violations.", "AI": {"tldr": "NSAM\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u52a8\u4f5c\u5c4f\u853d\u6846\u67b6\uff0c\u80fd\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u81ea\u52a8\u5b66\u4e60\u4e0e\u9886\u57df\u7ea6\u675f\u4e00\u81f4\u7684\u7b26\u53f7\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u4f5c\u5c4f\u853d\u6392\u9664\u4e0d\u53ef\u884c\u52a8\u4f5c\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u5e76\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u6307\u5b9a\u7b26\u53f7\u63a5\u5730\u51fd\u6570\u548c\u52a8\u4f5c\u5c4f\u853d\u6280\u672f\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u5b66\u4e60\u7b26\u53f7\u6a21\u578b\u5e76\u96c6\u6210\u5230DRL\u4e2d\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u52a8\u4f5c\u5c4f\u853d(NSAM)\u6846\u67b6\uff0c\u5728\u6700\u5c0f\u76d1\u7763\u4e0b\u81ea\u52a8\u5b66\u4e60\u4e0e\u9ad8\u7ef4\u72b6\u6001\u7ea6\u675f\u4e00\u81f4\u7684\u7b26\u53f7\u6a21\u578b\uff0c\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u7b26\u53f7\u6a21\u578b\u751f\u6210\u52a8\u4f5c\u5c4f\u853d\uff0c\u6392\u9664\u4e0d\u53ef\u884c\u52a8\u4f5c\uff0c\u5b9e\u73b0\u7b26\u53f7\u63a8\u7406\u4e0e\u6df1\u5ea6\u7b56\u7565\u4f18\u5316\u7684\u7aef\u5230\u7aef\u96c6\u6210\u3002", "result": "\u5728\u591a\u4e2a\u7ea6\u675f\u9886\u57df\u8bc4\u4f30NSAM\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eNSAM\u663e\u8457\u63d0\u9ad8\u4e86DRL\u667a\u80fd\u4f53\u7684\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u7ea6\u675f\u8fdd\u53cd\u3002", "conclusion": "NSAM\u6210\u529f\u5b9e\u73b0\u4e86\u7b26\u53f7\u63a8\u7406\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u96c6\u6210\uff0c\u7b26\u53f7\u63a5\u5730\u548c\u7b56\u7565\u5b66\u4e60\u7684\u6539\u8fdb\u76f8\u4e92\u4fc3\u8fdb\uff0c\u4e3a\u7ea6\u675f\u73af\u5883\u4e0b\u7684DRL\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10758", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10758", "abs": "https://arxiv.org/abs/2602.10758", "authors": ["Bo Wang", "Yueyang Chen", "Jieke Shi", "Minghui Li", "Yunbo Lyu", "Yinan Wu", "Youfang Lin", "Zhou Yang"], "title": "Hidden Licensing Risks in the LLMware Ecosystem", "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into software systems, giving rise to a new class of systems referred to as LLMware. Beyond traditional source-code components, LLMware embeds or interacts with LLMs that depend on other models and datasets, forming complex supply chains across open-source software (OSS), models, and datasets. However, licensing issues emerging from these intertwined dependencies remain largely unexplored. Leveraging GitHub and Hugging Face, we curate a large-scale dataset capturing LLMware supply chains, including 12,180 OSS repositories, 3,988 LLMs, and 708 datasets. Our analysis reveals that license distributions in LLMware differ substantially from traditional OSS ecosystems. We further examine license-related discussions and find that license selection and maintenance are the dominant concerns, accounting for 84% of cases. To understand incompatibility risks, we analyze license conflicts along supply chains and evaluate state-of-the-art detection approaches, which achieve only 58% and 76% F1 scores in this setting. Motivated by these limitations, we propose LiAgent, an LLM-based agent framework for ecosystem-level license compatibility analysis. LiAgent achieves an F1 score of 87%, improving performance by 14 percentage points over prior methods. We reported 60 incompatibility issues detected by LiAgent, 11 of which have been confirmed by developers. Notably, two conflicted LLMs have over 107 million and 5 million downloads on Hugging Face, respectively, indicating potentially widespread downstream impact. We conclude with implications and recommendations to support the sustainable growth of the LLMware ecosystem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMware\uff08\u96c6\u6210LLM\u7684\u8f6f\u4ef6\u7cfb\u7edf\uff09\u4e2d\u7684\u8bb8\u53ef\u8bc1\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u8bb8\u53ef\u8bc1\u5206\u5e03\u548c\u51b2\u7a81\uff0c\u5e76\u63d0\u51fa\u4e86LiAgent\u6846\u67b6\u6765\u68c0\u6d4b\u8bb8\u53ef\u8bc1\u4e0d\u517c\u5bb9\u95ee\u9898\u3002", "motivation": "LLMware\u7cfb\u7edf\u5c06LLM\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u7ec4\u4ef6\u7ed3\u5408\uff0c\u5f62\u6210\u4e86\u8de8\u8d8a\u5f00\u6e90\u8f6f\u4ef6\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u590d\u6742\u4f9b\u5e94\u94fe\uff0c\u4f46\u5176\u4e2d\u8bb8\u53ef\u8bc1\u517c\u5bb9\u6027\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u5b58\u5728\u6f5c\u5728\u7684\u6cd5\u5f8b\u98ce\u9669\u3002", "method": "1) \u4eceGitHub\u548cHugging Face\u6536\u96c6LLMware\u4f9b\u5e94\u94fe\u6570\u636e\uff0812,180\u4e2a\u4ed3\u5e93\u30013,988\u4e2aLLM\u3001708\u4e2a\u6570\u636e\u96c6\uff09\uff1b2) \u5206\u6790\u8bb8\u53ef\u8bc1\u5206\u5e03\u548c\u8ba8\u8bba\uff1b3) \u8bc4\u4f30\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff1b4) \u63d0\u51faLiAgent\u6846\u67b6\uff08\u57fa\u4e8eLLM\u7684\u4ee3\u7406\uff09\u8fdb\u884c\u751f\u6001\u7cfb\u7edf\u7ea7\u8bb8\u53ef\u8bc1\u517c\u5bb9\u6027\u5206\u6790\u3002", "result": "1) LLMware\u8bb8\u53ef\u8bc1\u5206\u5e03\u4e0e\u4f20\u7edfOSS\u751f\u6001\u7cfb\u7edf\u663e\u8457\u4e0d\u540c\uff1b2) \u8bb8\u53ef\u8bc1\u9009\u62e9\u548c\u7ef4\u62a4\u5360\u8ba8\u8bba\u768484%\uff1b3) \u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5F1\u5206\u6570\u4ec5\u4e3a58%\u548c76%\uff1b4) LiAgent\u8fbe\u523087%\u7684F1\u5206\u6570\uff0c\u63d0\u534714\u4e2a\u767e\u5206\u70b9\uff1b5) \u68c0\u6d4b\u523060\u4e2a\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u5176\u4e2d11\u4e2a\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\uff0c\u4e24\u4e2a\u51b2\u7a81\u6a21\u578b\u4e0b\u8f7d\u91cf\u5206\u522b\u8d85\u8fc71.07\u4ebf\u548c500\u4e07\u3002", "conclusion": "LLMware\u751f\u6001\u7cfb\u7edf\u9762\u4e34\u4e25\u91cd\u7684\u8bb8\u53ef\u8bc1\u517c\u5bb9\u6027\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5206\u6790\u5de5\u5177\u3002LiAgent\u6846\u67b6\u5728\u68c0\u6d4b\u8bb8\u53ef\u8bc1\u51b2\u7a81\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aLLMware\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2602.10625", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10625", "abs": "https://arxiv.org/abs/2602.10625", "authors": ["Nanxu Gong", "Haotian Li", "Sixun Dong", "Jianxun Lian", "Yanjie Fu", "Xing Xie"], "title": "To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks", "comment": null, "summary": "Theory of Mind (ToM) assesses whether models can infer hidden mental states such as beliefs, desires, and intentions, which is essential for natural social interaction. Although recent progress in Large Reasoning Models (LRMs) has boosted step-by-step inference in mathematics and coding, it is still underexplored whether this benefit transfers to socio-cognitive skills. We present a systematic study of nine advanced Large Language Models (LLMs), comparing reasoning models with non-reasoning models on three representative ToM benchmarks. The results show that reasoning models do not consistently outperform non-reasoning models and sometimes perform worse. A fine-grained analysis reveals three insights. First, slow thinking collapses: accuracy significantly drops as responses grow longer, and larger reasoning budgets hurt performance. Second, moderate and adaptive reasoning benefits performance: constraining reasoning length mitigates failure, while distinct success patterns demonstrate the necessity of dynamic adaptation. Third, option matching shortcut: when multiple choice options are removed, reasoning models improve markedly, indicating reliance on option matching rather than genuine deduction. We also design two intervention approaches: Slow-to-Fast (S2F) adaptive reasoning and Think-to-Match (T2M) shortcut prevention to further verify and mitigate the problems. With all results, our study highlights the advancement of LRMs in formal reasoning (e.g., math, code) cannot be fully transferred to ToM, a typical task in social reasoning. We conclude that achieving robust ToM requires developing unique capabilities beyond existing reasoning methods.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u5e76\u4e0d\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6709\u65f6\u8868\u73b0\u66f4\u5dee\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u5728\u793e\u4ea4\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u9886\u57df\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u79cd\u4f18\u52bf\u662f\u5426\u80fd\u8f6c\u79fb\u5230\u793e\u4ea4\u8ba4\u77e5\u6280\u80fd\uff08\u5982\u5fc3\u7406\u7406\u8bba\uff09\u4e0a\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u63a8\u7406\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5bf99\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u6bd4\u8f83\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u5fc3\u7406\u7406\u8bba\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\u3002\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u5e72\u9884\u65b9\u6cd5\uff1a\u6162\u5230\u5feb\u81ea\u9002\u5e94\u63a8\u7406\u548c\u601d\u8003\u5230\u5339\u914d\u6377\u5f84\u9884\u9632\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e0a\u5e76\u4e0d\u4e00\u81f4\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6709\u65f6\u8868\u73b0\u66f4\u5dee\u3002\u5206\u6790\u53d1\u73b0\uff1a1\uff09\u6162\u601d\u8003\u5d29\u6e83\uff1a\u54cd\u5e94\u8d8a\u957f\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff1b2\uff09\u9002\u5ea6\u81ea\u9002\u5e94\u63a8\u7406\u6709\u76ca\uff1a\u9650\u5236\u63a8\u7406\u957f\u5ea6\u53ef\u7f13\u89e3\u5931\u8d25\uff1b3\uff09\u9009\u9879\u5339\u914d\u6377\u5f84\uff1a\u79fb\u9664\u9009\u62e9\u9898\u9009\u9879\u540e\u63a8\u7406\u6a21\u578b\u8868\u73b0\u663e\u8457\u6539\u5584\u3002\u5e72\u9884\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5f62\u5f0f\u63a8\u7406\uff08\u5982\u6570\u5b66\u3001\u4ee3\u7801\uff09\u65b9\u9762\u7684\u8fdb\u6b65\u4e0d\u80fd\u5b8c\u5168\u8f6c\u79fb\u5230\u5fc3\u7406\u7406\u8bba\u8fd9\u79cd\u5178\u578b\u7684\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e0a\u3002\u5b9e\u73b0\u7a33\u5065\u7684\u5fc3\u7406\u7406\u8bba\u9700\u8981\u5f00\u53d1\u8d85\u8d8a\u73b0\u6709\u63a8\u7406\u65b9\u6cd5\u7684\u72ec\u7279\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2602.10787", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10787", "abs": "https://arxiv.org/abs/2602.10787", "authors": ["Samal Mukhtar", "Yinghua Yao", "Zhu Sun", "Mustafa Mustafa", "Yew Soon Ong", "Youcheng Sun"], "title": "VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection", "comment": "22 pages, 3 figures", "summary": "Software vulnerability detection (SVD) is a critical challenge in modern systems. Large language models (LLMs) offer natural-language explanations alongside predictions, but most work focuses on binary evaluation, and explanations often lack semantic consistency with Common Weakness Enumeration (CWE) categories. We propose VulReaD, a knowledge-graph-guided approach for vulnerability reasoning and detection that moves beyond binary classification toward CWE-level reasoning. VulReaD leverages a security knowledge graph (KG) as a semantic backbone and uses a strong teacher LLM to generate CWE-consistent contrastive reasoning supervision, enabling student model training without manual annotations. Students are fine-tuned with Odds Ratio Preference Optimization (ORPO) to encourage taxonomy-aligned reasoning while suppressing unsupported explanations. Across three real-world datasets, VulReaD improves binary F1 by 8-10% and multi-class classification by 30% Macro-F1 and 18% Micro-F1 compared to state-of-the-art baselines. Results show that LLMs outperform deep learning baselines in binary detection and that KG-guided reasoning enhances CWE coverage and interpretability.", "AI": {"tldr": "VulReaD\uff1a\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u6f0f\u6d1e\u63a8\u7406\u4e0e\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b89\u5168\u77e5\u8bc6\u56fe\u8c31\u548c\u6559\u5e08LLM\u751f\u6210CWE\u4e00\u81f4\u7684\u5bf9\u6bd4\u63a8\u7406\u76d1\u7763\uff0c\u4f7f\u7528ORPO\u5fae\u8c03\u5b66\u751f\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6f0f\u6d1e\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u4e3b\u8981\u5173\u6ce8\u4e8c\u5143\u8bc4\u4f30\uff0c\u4e14LLM\u751f\u6210\u7684\u89e3\u91ca\u5f80\u5f80\u7f3a\u4e4f\u4e0eCWE\u7c7b\u522b\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8d85\u8d8a\u4e8c\u5143\u5206\u7c7b\u3001\u5b9e\u73b0CWE\u7ea7\u522b\u63a8\u7406\u7684\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVulReaD\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5b89\u5168\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u8bed\u4e49\u9aa8\u5e72\uff1b2) \u5229\u7528\u5f3a\u6559\u5e08LLM\u751f\u6210CWE\u4e00\u81f4\u7684\u5bf9\u6bd4\u63a8\u7406\u76d1\u7763\uff1b3) \u4f7f\u7528ORPO\uff08Odds Ratio Preference Optimization\uff09\u5fae\u8c03\u5b66\u751f\u6a21\u578b\uff0c\u9f13\u52b1\u7b26\u5408\u5206\u7c7b\u5b66\u7684\u63a8\u7406\u540c\u65f6\u6291\u5236\u65e0\u652f\u6301\u7684\u8bf4\u660e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cVulReaD\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff1a\u4e8c\u5143F1\u63d0\u53478-10%\uff0c\u591a\u7c7b\u5206\u7c7b\u7684Macro-F1\u63d0\u534730%\uff0cMicro-F1\u63d0\u534718%\u3002LLM\u5728\u4e8c\u5143\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0cKG\u5f15\u5bfc\u7684\u63a8\u7406\u589e\u5f3a\u4e86CWE\u8986\u76d6\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "VulReaD\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u4e8c\u5143\u5206\u7c7b\u5230CWE\u7ea7\u522b\u63a8\u7406\u7684\u8f6c\u53d8\uff0c\u4e3a\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2602.10635", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10635", "abs": "https://arxiv.org/abs/2602.10635", "authors": ["Keane Ong", "Sabri Boughorbel", "Luwei Xiao", "Chanakya Ekbote", "Wei Dai", "Ao Qu", "Jingyao Wu", "Rui Mao", "Ehsan Hoque", "Erik Cambria", "Gianmarco Mengaldo", "Paul Pu Liang"], "title": "OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization", "comment": null, "summary": "To develop socially intelligent AI, existing approaches typically model human behavioral dimensions (e.g., affective, cognitive, or social attributes) in isolation. Although useful, task-specific modeling often increases training costs and limits generalization across behavioral settings. Recent reasoning RL methods facilitate training a single unified model across multiple behavioral tasks, but do not explicitly address learning across different heterogeneous behavioral data. To address this gap, we introduce Heterogeneity-Aware Relative Policy Optimization (HARPO), an RL method that balances leaning across heterogeneous tasks and samples. This is achieved by modulating advantages to ensure that no single task or sample carries disproportionate influence during policy optimization. Using HARPO, we develop and release Omnisapiens-7B 2.0, a foundation model for social behavior processing. Relative to existing behavioral foundation models, Omnisapiens-7B 2.0 achieves the strongest performance across behavioral tasks, with gains of up to +16.85% and +9.37% on multitask and held-out settings respectively, while producing more explicit and robust reasoning traces. We also validate HARPO against recent RL methods, where it achieves the most consistently strong performance across behavioral tasks.", "AI": {"tldr": "HARPO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u8282\u4f18\u52bf\u51fd\u6570\u6765\u5e73\u8861\u5f02\u6784\u4efb\u52a1\u548c\u6837\u672c\u7684\u5b66\u4e60\uff0c\u5f00\u53d1\u4e86Omnisapiens-7B 2.0\u793e\u4ea4\u884c\u4e3a\u5904\u7406\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u9879\u884c\u4e3a\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5730\u5efa\u6a21\u4eba\u7c7b\u884c\u4e3a\u7ef4\u5ea6\uff08\u60c5\u611f\u3001\u8ba4\u77e5\u6216\u793e\u4ea4\u5c5e\u6027\uff09\uff0c\u4efb\u52a1\u7279\u5b9a\u5efa\u6a21\u589e\u52a0\u4e86\u8bad\u7ec3\u6210\u672c\u4e14\u9650\u5236\u4e86\u8de8\u884c\u4e3a\u8bbe\u7f6e\u7684\u6cdb\u5316\u80fd\u529b\u3002\u867d\u7136\u6700\u8fd1\u7684\u63a8\u7406RL\u65b9\u6cd5\u53ef\u4ee5\u5728\u591a\u4e2a\u884c\u4e3a\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7edf\u4e00\u6a21\u578b\uff0c\u4f46\u672a\u660e\u786e\u89e3\u51b3\u8de8\u5f02\u6784\u884c\u4e3a\u6570\u636e\u7684\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51faHARPO\uff08\u5f02\u6784\u611f\u77e5\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u8282\u4f18\u52bf\u51fd\u6570\u6765\u786e\u4fdd\u5728\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u6ca1\u6709\u4efb\u4f55\u5355\u4e2a\u4efb\u52a1\u6216\u6837\u672c\u4ea7\u751f\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5e73\u8861\u8de8\u5f02\u6784\u4efb\u52a1\u548c\u6837\u672c\u7684\u5b66\u4e60\u3002", "result": "\u4f7f\u7528HARPO\u5f00\u53d1\u4e86Omnisapiens-7B 2.0\u793e\u4ea4\u884c\u4e3a\u5904\u7406\u57fa\u7840\u6a21\u578b\u3002\u76f8\u5bf9\u4e8e\u73b0\u6709\u884c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u4efb\u52a1\u548c\u4fdd\u7559\u8bbe\u7f6e\u4e0a\u5206\u522b\u83b7\u5f97\u9ad8\u8fbe+16.85%\u548c+9.37%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4ea7\u751f\u66f4\u660e\u786e\u548c\u9c81\u68d2\u7684\u63a8\u7406\u8f68\u8ff9\u3002HARPO\u5728\u4e0e\u5176\u4ed6RL\u65b9\u6cd5\u6bd4\u8f83\u4e2d\u4e5f\u8868\u73b0\u6700\u7a33\u5b9a\u3002", "conclusion": "HARPO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u5f02\u6784\u884c\u4e3a\u6570\u636e\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u5f00\u53d1\u7684Omnisapiens-7B 2.0\u6a21\u578b\u5728\u793e\u4ea4\u884c\u4e3a\u5904\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5f00\u53d1\u793e\u4ea4\u667a\u80fdAI\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10808", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10808", "abs": "https://arxiv.org/abs/2602.10808", "authors": ["Rasmus Krebs", "Somnath Mazumdar"], "title": "PELLI: Framework to effectively integrate LLMs for quality software generation", "comment": "15 pages", "summary": "Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric and selected a few LLMs (such as Codex and ChatGPT) for comparision. This paper proposes a comprehensive code quality assessment framework called Programmatic Excellence via LLM Iteration (PELLI). PELLI is an iterative analysis-based process that upholds high-quality code changes. We extended the state-of-the-art by performing a comprehensive evaluation that generates quantitative metrics for analyzing three primary nonfunctional requirements (such as maintainability, performance, and reliability) while selecting five popular LLMs. For PELLI's applicability, we selected three application domains while following Python coding standards. Following this framework, practitioners can ensure harmonious integration between LLMs and human developers, ensuring that their potential is fully realized. PELLI can serve as a practical guide for developers aiming to leverage LLMs while adhering to recognized quality standards. This study's outcomes are crucial for advancing LLM technologies in real-world applications, providing stakeholders with a clear understanding of where these LLMs excel and where they require further refinement. Overall, based on three nonfunctional requirements, we have found that GPT-4T and Gemini performed slightly better. We also found that prompt design can influence the overall code quality. In addition, each application domain demonstrated high and low scores across various metrics, and even within the same metrics across different prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPELLI\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u6790\u8bc4\u4f30LLM\u751f\u6210\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u6db5\u76d6\u53ef\u7ef4\u62a4\u6027\u3001\u6027\u80fd\u548c\u53ef\u9760\u6027\u4e09\u4e2a\u975e\u529f\u80fd\u6027\u9700\u6c42\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cd\u4e3b\u6d41LLM\u5728\u4e09\u4e2a\u5e94\u7528\u9886\u57df\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u6bd4\u8f83LLM\u751f\u6210\u7684\u4ee3\u7801\u65f6\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4e3b\u8981\u53ea\u8003\u8651\u53ef\u9760\u6027\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff1b2) \u53ea\u9009\u62e9\u5c11\u6570LLM\uff08\u5982Codex\u548cChatGPT\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faPELLI\uff08Programmatic Excellence via LLM Iteration\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fed\u4ee3\u5206\u6790\u7684\u8fc7\u7a0b\uff0c\u7528\u4e8e\u7ef4\u62a4\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u53d8\u66f4\u3002\u6846\u67b6\u5305\u542b\uff1a1) \u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u751f\u6210\u4e09\u4e2a\u4e3b\u8981\u975e\u529f\u80fd\u6027\u9700\u6c42\uff08\u53ef\u7ef4\u62a4\u6027\u3001\u6027\u80fd\u3001\u53ef\u9760\u6027\uff09\u7684\u91cf\u5316\u6307\u6807\uff1b2) \u9009\u62e9\u4e94\u79cd\u6d41\u884c\u7684LLM\u8fdb\u884c\u6bd4\u8f83\uff1b3) \u5728\u4e09\u4e2a\u5e94\u7528\u9886\u57df\u4e2d\u5e94\u7528\uff0c\u9075\u5faaPython\u7f16\u7801\u6807\u51c6\u3002", "result": "\u57fa\u4e8e\u4e09\u4e2a\u975e\u529f\u80fd\u6027\u9700\u6c42\u7684\u8bc4\u4f30\u53d1\u73b0\uff1a1) GPT-4T\u548cGemini\u8868\u73b0\u7565\u597d\uff1b2) \u63d0\u793a\u8bbe\u8ba1\u4f1a\u5f71\u54cd\u6574\u4f53\u4ee3\u7801\u8d28\u91cf\uff1b3) \u6bcf\u4e2a\u5e94\u7528\u9886\u57df\u5728\u4e0d\u540c\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u9ad8\u4f4e\u4e0d\u4e00\u7684\u5206\u6570\uff0c\u5373\u4f7f\u5728\u540c\u4e00\u6307\u6807\u4e0a\uff0c\u4e0d\u540c\u63d0\u793a\u4e5f\u4f1a\u4ea7\u751f\u5dee\u5f02\u3002", "conclusion": "PELLI\u6846\u67b6\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u4eba\u5458\u7684\u5b9e\u7528\u6307\u5357\uff0c\u5728\u9075\u5faa\u516c\u8ba4\u8d28\u91cf\u6807\u51c6\u7684\u540c\u65f6\u5229\u7528LLM\u3002\u8be5\u7814\u7a76\u7ed3\u679c\u5bf9\u63a8\u8fdbLLM\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u89c1\u89e3\uff0c\u4e86\u89e3\u8fd9\u4e9bLLM\u7684\u4f18\u52bf\u548c\u9700\u8981\u6539\u8fdb\u7684\u5730\u65b9\u3002", "topic": "code agent"}}
{"id": "2602.10699", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10699", "abs": "https://arxiv.org/abs/2602.10699", "authors": ["Jie Jiang", "Yangru Huang", "Zeyu Wang", "Changping Wang", "Yuling Xiong", "Jun Zhang", "Huan Yu"], "title": "Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation", "comment": null, "summary": "Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.", "AI": {"tldr": "V-STAR\u6846\u67b6\u901a\u8fc7\u4ef7\u503c\u5f15\u5bfc\u91c7\u6837\u548c\u6811\u72b6\u4f18\u52bf\u5f3a\u5316\uff0c\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u4e2dRL\u8bad\u7ec3\u7684\u6982\u7387-\u5956\u52b1\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u63a2\u7d22\u6548\u7387\u548c\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f3c\u7136\u7684\u89e3\u7801\u65b9\u6cd5\u5728\u751f\u6210\u5f0f\u63a8\u8350\u7684RL\u5fae\u8c03\u4e2d\u5b58\u5728\u6982\u7387-\u5956\u52b1\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5bfc\u81f4\u63a2\u7d22\u4e0d\u8db3\u548c\u4f18\u52bf\u538b\u7f29\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faV-STAR\u6846\u67b6\uff0c\u5305\u542b\u4ef7\u503c\u5f15\u5bfc\u9ad8\u6548\u89e3\u7801(VED)\u8bc6\u522b\u5173\u952e\u8282\u70b9\u5e76\u9009\u62e9\u6027\u6df1\u5316\u9ad8\u6f5c\u529b\u524d\u7f00\uff0c\u4ee5\u53caSibling-GRPO\u5229\u7528\u6811\u72b6\u62d3\u6251\u8ba1\u7b97\u5144\u5f1f\u76f8\u5bf9\u4f18\u52bf\uff0c\u96c6\u4e2d\u5b66\u4e60\u4fe1\u53f7\u4e8e\u5173\u952e\u5206\u652f\u51b3\u7b56\u3002", "result": "\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cV-STAR\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u5019\u9009\u96c6\u591a\u6837\u6027\u3002", "conclusion": "V-STAR\u901a\u8fc7\u89e3\u51b3\u6982\u7387-\u5956\u52b1\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u63a2\u7d22\u6548\u7387\u548c\u6027\u80fd\uff0c\u4e3aRL\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10224", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10224", "abs": "https://arxiv.org/abs/2602.10224", "authors": ["Shiting Huang", "Zecheng Li", "Yu Zeng", "Qingnan Ren", "Zhen Fang", "Qisheng Su", "Kou Shi", "Lin Chen", "Zehui Chen", "Feng Zhao"], "title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.", "AI": {"tldr": "\u63d0\u51faMEL\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u84b8\u998f\u5143\u7ecf\u9a8c\u6765\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728RLVR\u57fa\u7840\u4e0a\u589e\u52a0\u5bf9\u6bd4\u5206\u6790\u548c\u9519\u8bef\u5f52\u56e0\u673a\u5236\uff0c\u5b9e\u73b0\u77e5\u8bc6\u590d\u7528", "motivation": "RLVR\u867d\u7136\u6709\u6548\uff0c\u4f46\u7f3a\u4e4f\u9519\u8bef\u5f52\u56e0\u548c\u7ecf\u9a8c\u5185\u5316\u673a\u5236\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u548c\u53ef\u590d\u7528\u77e5\u8bc6\u5f62\u6210\uff0c\u9700\u8981\u5f15\u5165\u7c7b\u4f3c\u4eba\u7c7b\u5b66\u4e60\u5468\u671f\u7684\u5143\u7ecf\u9a8c\u5b66\u4e60", "method": "\u5728\u6807\u51c6RLVR\u57fa\u7840\u4e0a\uff0c\u5229\u7528LLM\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u5bf9\u6b63\u786e\u548c\u9519\u8bef\u8f68\u8ff9\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc6\u522b\u63a8\u7406\u9519\u8bef\u7684\u5206\u53c9\u70b9\uff0c\u603b\u7ed3\u4e3a\u53ef\u6cdb\u5316\u7684\u5143\u7ecf\u9a8c\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8d1f\u5bf9\u6570\u4f3c\u7136\u5c06\u5176\u5185\u5316\u5230\u53c2\u6570\u8bb0\u5fc6\u4e2d", "result": "MEL\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u83b7\u5f973.92%-4.73%\u7684Pass@1\u589e\u76ca", "conclusion": "MEL\u6846\u67b6\u901a\u8fc7\u5143\u7ecf\u9a8c\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86RLVR\u7684\u5143\u5b66\u4e60\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9519\u8bef\u5f52\u56e0\u548c\u77e5\u8bc6\u590d\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.10356", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10356", "abs": "https://arxiv.org/abs/2602.10356", "authors": ["Tianci Xue", "Zeyi Liao", "Tianneng Shi", "Zilu Wang", "Kai Zhang", "Dawn Song", "Yu Su", "Huan Sun"], "title": "Autonomous Continual Learning of Computer-Use Agents for Environment Adaptation", "comment": "24 pages, 6 figures", "summary": "Real-world digital environments are highly diverse and dynamic. These characteristics cause agents to frequently encounter unseen scenarios and distribution shifts, making continual learning in specific environments essential for computer-use agents (CUAs). However, a key challenge lies in obtaining high-quality and environment-grounded agent data without relying on costly human annotation. In this work, we introduce ACuRL, an Autonomous Curriculum Reinforcement Learning framework that continually adapts agents to specific environments with zero human data. The agent first explores target environments to acquire initial experiences. During subsequent iterative training, a curriculum task generator leverages these experiences together with feedback from the previous iteration to synthesize new tasks tailored for the agent's current capabilities. To provide reliable reward signals, we introduce CUAJudge, a robust automatic evaluator for CUAs that achieves 93% agreement with human judgments. Empirically, our method effectively enables both intra-environment and cross-environment continual learning, yielding 4-22% performance gains without catastrophic forgetting on existing environments. Further analyses show highly sparse updates (e.g., 20% parameters), which helps explain the effective and robust adaptation. Our data and code are available at https://github.com/OSU-NLP-Group/ACuRL.", "AI": {"tldr": "ACuRL\u662f\u4e00\u4e2a\u81ea\u4e3b\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u73af\u5883\u63a2\u7d22\u548c\u8bfe\u7a0b\u4efb\u52a1\u751f\u6210\u5b9e\u73b0\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u5728\u7279\u5b9a\u73af\u5883\u4e2d\u83b7\u5f974-22%\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u5b9e\u6570\u5b57\u73af\u5883\u9ad8\u5ea6\u591a\u6837\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u4ee3\u7406\u7ecf\u5e38\u9047\u5230\u672a\u89c1\u573a\u666f\u548c\u5206\u5e03\u504f\u79fb\uff0c\u9700\u8981\u6301\u7eed\u5b66\u4e60\u3002\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u3001\u73af\u5883\u76f8\u5173\u7684\u4ee3\u7406\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u907f\u514d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "method": "\u63d0\u51faACuRL\u6846\u67b6\uff1a1)\u4ee3\u7406\u5148\u63a2\u7d22\u76ee\u6807\u73af\u5883\u83b7\u53d6\u521d\u59cb\u7ecf\u9a8c\uff1b2)\u8bfe\u7a0b\u4efb\u52a1\u751f\u6210\u5668\u5229\u7528\u8fd9\u4e9b\u7ecf\u9a8c\u548c\u524d\u4e00\u8f6e\u53cd\u9988\u5408\u6210\u9002\u5408\u4ee3\u7406\u5f53\u524d\u80fd\u529b\u7684\u65b0\u4efb\u52a1\uff1b3)\u5f15\u5165CUAJudge\u81ea\u52a8\u8bc4\u4f30\u5668\u63d0\u4f9b\u53ef\u9760\u5956\u52b1\u4fe1\u53f7\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u6027\u8fbe93%\u3002", "result": "\u65b9\u6cd5\u6709\u6548\u652f\u6301\u73af\u5883\u5185\u548c\u8de8\u73af\u5883\u6301\u7eed\u5b66\u4e60\uff0c\u5728\u73b0\u6709\u73af\u5883\u4e2d\u83b7\u5f974-22%\u6027\u80fd\u63d0\u5347\u4e14\u65e0\u707e\u96be\u6027\u9057\u5fd8\u3002\u5206\u6790\u663e\u793a\u9ad8\u5ea6\u7a00\u758f\u7684\u53c2\u6570\u66f4\u65b0\uff08\u598220%\uff09\uff0c\u89e3\u91ca\u4e86\u6709\u6548\u4e14\u9c81\u68d2\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "ACuRL\u6846\u67b6\u901a\u8fc7\u81ea\u4e3b\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u4eba\u5de5\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u7279\u5b9a\u73af\u5883\u4e2d\u7684\u6301\u7eed\u9002\u5e94\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10975", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10975", "abs": "https://arxiv.org/abs/2602.10975", "authors": ["Qixing Zhou", "Jiacheng Zhang", "Haiyang Wang", "Rui Hao", "Jiahe Wang", "Minghao Han", "Yuxue Yang", "Shuzhe Wu", "Feiyang Pan", "Lue Fan", "Dandan Tu", "Zhaoxiang Zhang"], "title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development", "comment": "Accepted by ICLR 2026", "summary": "Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.", "AI": {"tldr": "FeatureBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u7aef\u5230\u7aef\u3001\u9762\u5411\u529f\u80fd\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7f16\u7801\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6267\u884c\u8bc4\u4f30\u548c\u81ea\u52a8\u5316\u4efb\u52a1\u6536\u96c6\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u590d\u6742\u529f\u80fd\u5f00\u53d1\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff08\u4ec511%\u6210\u529f\u7387\uff09\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4efb\u52a1\u8303\u56f4\u6709\u9650\uff08\u5982\u4ec5\u4fee\u590d\u5355\u4e2aPR\u4e2d\u7684bug\uff09\u3001\u4f9d\u8d56\u975e\u53ef\u6267\u884c\u8bc4\u4f30\u3001\u7f3a\u4e4f\u81ea\u52a8\u5316\u66f4\u65b0\u673a\u5236\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u4e86\u89e3LLM\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u63d0\u51faFeatureBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u6267\u884c\u8bc4\u4f30\u534f\u8bae\u548c\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f9d\u8d56\u56fe\u8ffd\u8e2a\u5355\u5143\u6d4b\u8bd5\uff0c\u81ea\u52a8\u4ece\u4ee3\u7801\u4ed3\u5e93\u4e2d\u63d0\u53d6\u8de8\u591a\u4e2a\u63d0\u4ea4\u548cPR\u7684\u529f\u80fd\u7ea7\u7f16\u7801\u4efb\u52a1\uff0c\u786e\u4fdd\u529f\u80fd\u5206\u79bb\u540e\u5176\u4ed6\u529f\u80fd\u6b63\u5e38\u8fd0\u884c\u3002", "result": "\u4ece24\u4e2a\u5f00\u6e90\u4ed3\u5e93\u4e2d\u6536\u96c6\u4e86200\u4e2a\u6311\u6218\u6027\u8bc4\u4f30\u4efb\u52a1\u548c3825\u4e2a\u53ef\u6267\u884c\u73af\u5883\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u7684Claude 4.5 Opus\u6a21\u578b\u5728SWE-bench\u4e0a\u8fbe\u523074.4%\u89e3\u51b3\u7387\uff0c\u4f46\u5728FeatureBench\u4e0a\u4ec5\u6210\u529f11.0%\u7684\u4efb\u52a1\u3002", "conclusion": "FeatureBench\u63ed\u793a\u4e86\u5f53\u524dLLM\u4ee3\u7406\u5728\u590d\u6742\u529f\u80fd\u5f00\u53d1\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5176\u81ea\u52a8\u5316\u4efb\u52a1\u6536\u96c6\u5de5\u5177\u4f7f\u57fa\u51c6\u6d4b\u8bd5\u6613\u4e8e\u6269\u5c55\u548c\u66f4\u65b0\uff0c\u907f\u514d\u6570\u636e\u6cc4\u9732\uff0c\u6784\u5efa\u7684\u53ef\u9a8c\u8bc1\u73af\u5883\u5bf9\u4ee3\u7406\u8bad\u7ec3\u4e5f\u6709\u6f5c\u5728\u4ef7\u503c\u3002", "topic": "swe benchmark"}}
{"id": "2602.10226", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10226", "abs": "https://arxiv.org/abs/2602.10226", "authors": ["Haochen Wang", "Yi Wu", "Daryl Chang", "Li Wei", "Lukasz Heldt"], "title": "Self-Evolving Recommendation System: End-To-End Autonomous Model Optimization With LLM Agents", "comment": null, "summary": "Optimizing large-scale machine learning systems, such as recommendation models for global video platforms, requires navigating a massive hyperparameter search space and, more critically, designing sophisticated optimizers, architectures, and reward functions to capture nuanced user behaviors. Achieving substantial improvements in these areas is a non-trivial task, traditionally relying on extensive manual iterations to test new hypotheses. We propose a self-evolving system that leverages Large Language Models (LLMs), specifically those from Google's Gemini family, to autonomously generate, train, and deploy high-performing, complex model changes within an end-to-end automated workflow. The self-evolving system is comprised of an Offline Agent (Inner Loop) that performs high-throughput hypothesis generation using proxy metrics, and an Online Agent (Outer Loop) that validates candidates against delayed north star business metrics in live production. Our agents act as specialized Machine Learning Engineers (MLEs): they exhibit deep reasoning capabilities, discovering novel improvements in optimization algorithms and model architecture, and formulating innovative reward functions that target long-term user engagement. The effectiveness of this approach is demonstrated through several successful production launches at YouTube, confirming that autonomous, LLM-driven evolution can surpass traditional engineering workflows in both development velocity and model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u8fdb\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u3001\u8bad\u7ec3\u548c\u90e8\u7f72YouTube\u63a8\u8350\u6a21\u578b\u7684\u6539\u8fdb\uff0c\u901a\u8fc7\u5185\u5916\u5faa\u73af\u4ee3\u7406\u5b9e\u73b0\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\uff0c\u8d85\u8d8a\u4f20\u7edf\u4eba\u5de5\u8fed\u4ee3\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff08\u5982\u89c6\u9891\u5e73\u53f0\u63a8\u8350\u6a21\u578b\uff09\u7684\u4f18\u5316\u9762\u4e34\u5de8\u5927\u8d85\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u548c\u590d\u6742\u4f18\u5316\u5668/\u67b6\u6784\u8bbe\u8ba1\u6311\u6218\uff0c\u4f20\u7edf\u4f9d\u8d56\u4eba\u5de5\u8fed\u4ee3\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u57fa\u4e8eGoogle Gemini LLM\u7684\u81ea\u8fdb\u5316\u7cfb\u7edf\uff0c\u5305\u542b\u79bb\u7ebf\u4ee3\u7406\uff08\u5185\u5faa\u73af\uff09\u4f7f\u7528\u4ee3\u7406\u6307\u6807\u8fdb\u884c\u9ad8\u541e\u5410\u91cf\u5047\u8bbe\u751f\u6210\uff0c\u5728\u7ebf\u4ee3\u7406\uff08\u5916\u5faa\u73af\uff09\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7528\u5ef6\u8fdf\u4e1a\u52a1\u6307\u6807\u9a8c\u8bc1\u5019\u9009\u65b9\u6848\uff0c\u4ee3\u7406\u626e\u6f14\u4e13\u4e1a\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5e08\u89d2\u8272\u3002", "result": "\u5728YouTube\u6210\u529f\u90e8\u7f72\u591a\u4e2a\u751f\u4ea7\u7248\u672c\uff0c\u8bc1\u660eLLM\u9a71\u52a8\u7684\u81ea\u4e3b\u8fdb\u5316\u5728\u5f00\u53d1\u901f\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u4e0a\u90fd\u8d85\u8d8a\u4e86\u4f20\u7edf\u5de5\u7a0b\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u81ea\u8fdb\u5316\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u4f18\u5316\u7b97\u6cd5\u3001\u6a21\u578b\u67b6\u6784\u548c\u5956\u52b1\u51fd\u6570\u7684\u521b\u65b0\u6539\u8fdb\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.10814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10814", "abs": "https://arxiv.org/abs/2602.10814", "authors": ["Xingyi Zhang", "Yulei Ye", "Kaifeng Huang", "Wenhao Li", "Xiangfeng Wang"], "title": "See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch play a central role in low-code education, yet evaluating the capabilities of AI agents to construct programs through Graphical User Interfaces (GUIs) remains underexplored. We introduce ScratchWorld, a benchmark for evaluating multimodal GUI agents on program-by-construction tasks in Scratch. Grounded in the Use-Modify-Create pedagogical framework, ScratchWorld comprises 83 curated tasks spanning four distinct problem categories: Create, Debug, Extend, and Compute. To rigorously diagnose the source of agent failures, the benchmark employs two complementary interaction modes: primitive mode requires fine-grained drag-and-drop manipulation to directly assess visuomotor control, while composite mode uses high-level semantic APIs to disentangle program reasoning from GUI execution. To ensure reliable assessment, we propose an execution-based evaluation protocol that validates the functional correctness of the constructed Scratch programs through runtime tests within the browser environment. Extensive experiments across state-of-the-art multimodal language models and GUI agents reveal a substantial reasoning--acting gap, highlighting persistent challenges in fine-grained GUI manipulation despite strong planning capabilities.", "AI": {"tldr": "ScratchWorld\u662f\u4e00\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001GUI\u4ee3\u7406\u5728Scratch\u73af\u5883\u4e2d\u7f16\u7a0b\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b83\u4e2a\u4efb\u52a1\uff0c\u91c7\u7528\u4e24\u79cd\u4ea4\u4e92\u6a21\u5f0f\u6765\u8bca\u65ad\u4ee3\u7406\u5931\u8d25\u539f\u56e0\uff0c\u5b9e\u9a8c\u663e\u793a\u63a8\u7406\u4e0e\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u50cfScratch\u8fd9\u6837\u7684\u79ef\u6728\u5f0f\u7f16\u7a0b\u73af\u5883\u5728\u4f4e\u4ee3\u7801\u6559\u80b2\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u4f46\u8bc4\u4f30AI\u4ee3\u7406\u901a\u8fc7\u56fe\u5f62\u7528\u6237\u754c\u9762\u6784\u5efa\u7a0b\u5e8f\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u57fa\u51c6\u6765\u8bc4\u4f30\u591a\u6a21\u6001GUI\u4ee3\u7406\u5728Scratch\u4e2d\u7684\u7a0b\u5e8f\u6784\u5efa\u4efb\u52a1\u3002", "method": "\u57fa\u4e8eUse-Modify-Create\u6559\u5b66\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b83\u4e2a\u4efb\u52a1\u7684ScratchWorld\u57fa\u51c6\uff0c\u6db5\u76d6\u521b\u5efa\u3001\u8c03\u8bd5\u3001\u6269\u5c55\u548c\u8ba1\u7b97\u56db\u4e2a\u95ee\u9898\u7c7b\u522b\u3002\u91c7\u7528\u4e24\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff1a\u539f\u59cb\u6a21\u5f0f\u9700\u8981\u7ec6\u7c92\u5ea6\u62d6\u653e\u64cd\u4f5c\u6765\u8bc4\u4f30\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\uff0c\u590d\u5408\u6a21\u5f0f\u4f7f\u7528\u9ad8\u7ea7\u8bed\u4e49API\u6765\u5206\u79bb\u7a0b\u5e8f\u63a8\u7406\u548cGUI\u6267\u884c\u3002\u63d0\u51fa\u57fa\u4e8e\u6267\u884c\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u7684\u8fd0\u884c\u65f6\u6d4b\u8bd5\u9a8c\u8bc1Scratch\u7a0b\u5e8f\u7684\u529f\u80fd\u6b63\u786e\u6027\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u548cGUI\u4ee3\u7406\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u63a8\u7406-\u6267\u884c\u5dee\u8ddd\uff0c\u5c3d\u7ba1\u4ee3\u7406\u5177\u6709\u8f83\u5f3a\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6GUI\u64cd\u4f5c\u65b9\u9762\u4ecd\u9762\u4e34\u6301\u7eed\u6311\u6218\u3002", "conclusion": "ScratchWorld\u57fa\u51c6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u591a\u6a21\u6001GUI\u4ee3\u7406\u5728Scratch\u73af\u5883\u4e2d\u7684\u7a0b\u5e8f\u6784\u5efa\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u63a8\u7406\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u672a\u6765GUI\u4ee3\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2602.11103", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11103", "abs": "https://arxiv.org/abs/2602.11103", "authors": ["Wayne Chi", "Yixiong Fang", "Arnav Yayavaram", "Siddharth Yayavaram", "Seth Karten", "Qiuhong Anna Wei", "Runkun Chen", "Alexander Wang", "Valerie Chen", "Ameet Talwalkar", "Chris Donahue"], "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development", "comment": null, "summary": "Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.", "AI": {"tldr": "GameDevBench\uff1a\u9996\u4e2a\u6e38\u620f\u5f00\u53d1\u667a\u80fd\u4f53\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5305\u542b132\u4e2a\u9700\u8981\u591a\u6a21\u6001\u7406\u89e3\u7684\u4efb\u52a1\uff0c\u5e73\u5747\u4efb\u52a1\u590d\u6742\u5ea6\u662f\u73b0\u6709\u8f6f\u4ef6\u5f00\u53d1\u57fa\u51c6\u76843\u500d\u4ee5\u4e0a\uff0c\u6700\u4f73\u667a\u80fd\u4f53\u4ec5\u80fd\u89e3\u51b354.5%\u7684\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7f16\u7801\u667a\u80fd\u4f53\u53d1\u5c55\u6ede\u540e\uff0c\u7f3a\u4e4f\u7ed3\u5408\u8f6f\u4ef6\u5de5\u7a0b\u590d\u6742\u6027\u548c\u6df1\u5ea6\u591a\u6a21\u6001\u7406\u89e3\u7684\u8bc4\u6d4b\u57fa\u51c6\u3002\u6e38\u620f\u5f00\u53d1\u63d0\u4f9b\u4e86\u7406\u60f3\u7684\u6d4b\u8bd5\u573a\u666f\uff0c\u9700\u8981\u667a\u80fd\u4f53\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u64cd\u4f5c\u591a\u6a21\u6001\u8d44\u4ea7\uff08\u7740\u8272\u5668\u3001\u7cbe\u7075\u3001\u52a8\u753b\u7b49\uff09\u3002", "method": "\u4ece\u7f51\u7edc\u548c\u89c6\u9891\u6559\u7a0b\u4e2d\u6536\u96c6132\u4e2a\u6e38\u620f\u5f00\u53d1\u4efb\u52a1\uff0c\u6784\u5efaGameDevBench\u57fa\u51c6\u3002\u4efb\u52a1\u9700\u8981\u663e\u8457\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u590d\u6742\u5ea6\u9ad8\u3002\u5f15\u5165\u4e24\u79cd\u7b80\u5355\u7684\u56fe\u50cf\u548c\u89c6\u9891\u53cd\u9988\u673a\u5236\u6765\u63d0\u5347\u667a\u80fd\u4f53\u7684\u591a\u6a21\u6001\u80fd\u529b\u3002", "result": "\u667a\u80fd\u4f53\u5728\u6e38\u620f\u5f00\u53d1\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u4f73\u667a\u80fd\u4f53\u4ec5\u89e3\u51b354.5%\u7684\u4efb\u52a1\u3002\u4efb\u52a1\u96be\u5ea6\u4e0e\u591a\u6a21\u6001\u590d\u6742\u5ea6\u5f3a\u76f8\u5173\uff1a\u6e38\u620f\u73a9\u6cd5\u4efb\u52a1\u6210\u529f\u738746.9%\uff0c2D\u56fe\u5f62\u4efb\u52a1\u964d\u81f331.6%\u3002\u5f15\u5165\u7684\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cClaude Sonnet 4.5\u4ece33.3%\u63d0\u5347\u81f347.7%\u3002", "conclusion": "\u6e38\u620f\u5f00\u53d1\u662f\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7814\u7a76\u7684\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\uff0c\u73b0\u6709\u667a\u80fd\u4f53\u5728\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u7b80\u5355\u7684\u591a\u6a21\u6001\u53cd\u9988\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0cGameDevBench\u7684\u53d1\u5e03\u5c06\u63a8\u52a8\u667a\u80fd\u4f53\u6e38\u620f\u5f00\u53d1\u7814\u7a76\u3002", "topic": "swe benchmark"}}
{"id": "2602.10885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10885", "abs": "https://arxiv.org/abs/2602.10885", "authors": ["Leheng Sheng", "Wenchang Ma", "Ruixin Hong", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics", "comment": "21 pages", "summary": "Despite chain-of-thought (CoT) playing crucial roles in LLM reasoning, directly rewarding it is difficult: training a reward model demands heavy human labeling efforts, and static RMs struggle with evolving CoT distributions and reward hacking. These challenges motivate us to seek an autonomous CoT rewarding approach that requires no human annotation efforts and can evolve gradually. Inspired by recent self-evolving training methods, we propose \\textbf{RLCER} (\\textbf{R}einforcement \\textbf{L}earning with \\textbf{C}oT Supervision via Self-\\textbf{E}volving \\textbf{R}ubrics), which enhances the outcome-centric RLVR by rewarding CoTs with self-proposed and self-evolving rubrics. We show that self-proposed and self-evolving rubrics provide reliable CoT supervision signals even without outcome rewards, enabling RLCER to outperform outcome-centric RLVR. Moreover, when used as in-prompt hints, these self-proposed rubrics further improve inference-time performance.", "AI": {"tldr": "RLCER\uff1a\u901a\u8fc7\u81ea\u63d0\u51fa\u3001\u81ea\u6f14\u5316\u7684\u8bc4\u5206\u6807\u51c6\u6765\u5956\u52b1\u601d\u7ef4\u94fe\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d85\u8d8a\u7ed3\u679c\u4e2d\u5fc3\u7684RLVR\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\u5956\u52b1\u65b9\u6cd5\u9762\u4e34\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u9759\u6001\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u601d\u7ef4\u94fe\u5206\u5e03\u53d8\u5316\u548c\u5956\u52b1\u653b\u51fb\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u4e14\u80fd\u81ea\u4e3b\u6f14\u5316\u7684\u601d\u7ef4\u94fe\u5956\u52b1\u65b9\u6cd5", "method": "\u63d0\u51faRLCER\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u63d0\u51fa\u548c\u81ea\u6f14\u5316\u7684\u8bc4\u5206\u6807\u51c6\u6765\u5956\u52b1\u601d\u7ef4\u94fe\uff0c\u589e\u5f3a\u7ed3\u679c\u4e2d\u5fc3\u7684RLVR\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u8bc4\u5206\u6807\u51c6\u53ef\u4f5c\u4e3a\u63d0\u793a\u4e2d\u7684\u63d0\u793a\u8fdb\u4e00\u6b65\u6539\u8fdb\u63a8\u7406\u6027\u80fd", "result": "RLCER\u5728\u65e0\u9700\u7ed3\u679c\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u9760\u7684\u601d\u7ef4\u94fe\u76d1\u7763\u4fe1\u53f7\uff0c\u6027\u80fd\u8d85\u8d8a\u7ed3\u679c\u4e2d\u5fc3\u7684RLVR\u65b9\u6cd5\uff0c\u4e14\u81ea\u63d0\u51fa\u7684\u8bc4\u5206\u6807\u51c6\u4f5c\u4e3a\u63d0\u793a\u63d0\u793a\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u63a8\u7406\u65f6\u6027\u80fd", "conclusion": "\u81ea\u63d0\u51fa\u548c\u81ea\u6f14\u5316\u7684\u8bc4\u5206\u6807\u51c6\u4e3a\u601d\u7ef4\u94fe\u5956\u52b1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u4e3b\u76d1\u7763\u673a\u5236\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u80fd\u9002\u5e94\u601d\u7ef4\u94fe\u5206\u5e03\u7684\u53d8\u5316", "topic": "agentic reinforcement learning"}}
{"id": "2602.10999", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10999", "abs": "https://arxiv.org/abs/2602.10999", "authors": ["Yusong Lin", "Haiyang Wang", "Shuzhe Wu", "Lue Fan", "Feiyang Pan", "Sanyuan Zhao", "Dandan Tu"], "title": "CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion", "comment": null, "summary": "Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86CLI-Gym\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u73af\u5883\u5386\u53f2\u6765\u5927\u89c4\u6a21\u751f\u6210\u73af\u5883\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u5e76\u8bad\u7ec3\u51faLiberCoder\u6a21\u578b\u5728Terminal-Bench\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u4ee3\u7406\u7f16\u7801\u9700\u8981\u4e0e\u8fd0\u884c\u65f6\u73af\u5883\uff08\u5982\u547d\u4ee4\u884c\u754c\u9762\uff09\u6709\u6548\u4ea4\u4e92\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u83b7\u53d6\u73af\u5883\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u4ee3\u7406\u80fd\u529b", "method": "\u57fa\u4e8eDockerfile\u4e0e\u4ee3\u7406\u4efb\u52a1\u7684\u7c7b\u6bd4\uff0c\u4f7f\u7528\u4ee3\u7406\u6a21\u62df\u548c\u63a2\u7d22\u73af\u5883\u5386\u53f2\uff0c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u6307\u5bfc\u3002\u901a\u8fc7\u8ffd\u8e2a\u5065\u5eb7\u73af\u5883\u7684\u5386\u53f2\uff0c\u5c06\u5176\u72b6\u6001\u53cd\u8f6c\u5230\u65e9\u671f\u6709\u8fd0\u884c\u65f6\u6545\u969c\u7684\u72b6\u6001\uff0c\u4ece\u800c\u751f\u6210\u5305\u542b\u9519\u8bef\u72b6\u6001\u548c\u76f8\u5e94\u9519\u8bef\u6d88\u606f\u7684\u4efb\u52a1", "result": "\u751f\u6210\u4e861,655\u4e2a\u73af\u5883\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u540c\u7c7b\u4e2d\u6700\u5927\u7684\u96c6\u5408\uff09\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7b56\u5212\u7684\u6210\u529f\u8f68\u8ff9\u8bad\u7ec3\u7684LiberCoder\u6a21\u578b\u5728Terminal-Bench\u4e0a\u5b9e\u73b0\u4e86+21.1%\u7684\u7edd\u5bf9\u63d0\u5347\uff08\u8fbe\u523046.1%\uff09\uff0c\u4f18\u4e8e\u5404\u79cd\u5f3a\u57fa\u7ebf", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u5bfc\u73af\u5883\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u516c\u5f00\u6d41\u7a0b\uff0cCLI-Gym\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73af\u5883\u5bc6\u96c6\u578b\u4efb\u52a1\u83b7\u53d6\u7684\u89c4\u6a21\u5316\u95ee\u9898", "topic": "code agent"}}
{"id": "2602.11136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11136", "abs": "https://arxiv.org/abs/2602.11136", "authors": ["Jiayi Zhou", "Yang Sheng", "Hantao Lou", "Yaodong Yang", "Jie Fu"], "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight", "comment": "27 pages", "summary": "As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFoT\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u5f62\u5f0f\u601d\u7ef4\u67b6\u6784\u5c06\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u89c4\u8303\uff0c\u4f7f\u7528Dafny\u548cZ3\u6c42\u89e3\u5668\u63d0\u4f9b\u6570\u5b66\u4fdd\u8bc1\u800c\u975e\u6982\u7387\u8bc4\u5206\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u884c\u4e3a\u5b89\u5168\u6027\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u5173\u952e\u9886\u57df\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u4e3b\u6d41\u7684LLM-as-a-Judge\u76d1\u7763\u8303\u5f0f\u5b58\u5728\u6839\u672c\u56f0\u5883\uff1a\u6982\u7387\u7cfb\u7edf\u5982\u4f55\u53ef\u9760\u76d1\u7763\u5176\u4ed6\u6982\u7387\u7cfb\u7edf\u800c\u4e0d\u7ee7\u627f\u5176\u5931\u8d25\u6a21\u5f0f\uff1f\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5230\u5f62\u5f0f\u5316\u89c4\u8303\u7684\u8f6c\u5316\u6210\u4e3a\u74f6\u9888\u3002", "method": "\u63d0\u51faFoT\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5411\u5f62\u5f0f\u601d\u7ef4\u67b6\u6784\uff1aLLM\u4f5c\u4e3a\u89c4\u8303\u7f16\u8bd1\u5668\uff0c\u81ea\u4e0a\u800c\u4e0b\u5c06\u9ad8\u5c42\u4eba\u7c7b\u610f\u56fe\u5206\u89e3\u4e3a\u539f\u5b50\u53ef\u9a8c\u8bc1\u7ea6\u675f\uff0c\u7136\u540e\u81ea\u4e0b\u800c\u4e0a\u4f7f\u7528Dafny\u89c4\u8303\u548cZ3\u53ef\u6ee1\u8db3\u6027\u6a21\u7406\u8bba\u6c42\u89e3\u8bc1\u660e\u5408\u89c4\u6027\uff0c\u63d0\u4f9b\u6570\u5b66\u4fdd\u8bc1\u800c\u975e\u6982\u7387\u8bc4\u5206\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u884c\u4e3a\u5b89\u5168\u3001\u591a\u9886\u57df\u7ea6\u675f\u9075\u5b88\u3001\u4ee3\u7406\u5411\u4e0a\u6b3a\u9a97\u68c0\u6d4b\uff09\u4e0a\u9a8c\u8bc1\uff0c7\u4e2a\u4ee3\u7406\u6a21\u578b\u5b9e\u9a8c\u663e\u793a\uff1aFoT\u76f8\u6bd4LLM-as-a-Judge\u57fa\u7ebf\u5e73\u5747\u63d0\u534716.6%\uff0c\u5b9e\u73b0\u5f31\u5230\u5f3a\u6cdb\u5316\uff087B\u6cd5\u5b98\u68c0\u6d4b72B\u4ee3\u7406\u6b3a\u9a97\u51c6\u786e\u7387\u8d8590%\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u63d0\u4f9b\u8fd1\u7ebf\u6027\u5b89\u5168\u6027\u6539\u8fdb\u3002", "conclusion": "FoT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u5230\u5f62\u5f0f\u5316\u89c4\u8303\u7684\u8f6c\u5316\u74f6\u9888\uff0c\u4e3aLLM\u4ee3\u7406\u884c\u4e3a\u5b89\u5168\u76d1\u7763\u63d0\u4f9b\u4e86\u6570\u5b66\u4fdd\u8bc1\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6982\u7387\u76d1\u7763\u8303\u5f0f\uff0c\u652f\u6301\u5f31\u5230\u5f3a\u6cdb\u5316\u548c\u8fed\u4ee3\u6539\u8fdb\u3002", "topic": "agent analysis"}}
{"id": "2602.10480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10480", "abs": "https://arxiv.org/abs/2602.10480", "authors": ["Hongyu Zhao", "Siyu Zhou", "Haolin Yang", "Zengyi Qin", "Tianyi Zhou"], "title": "Neuro-Symbolic Synergy for Interactive World Modeling", "comment": null, "summary": "Large language models (LLMs) exhibit strong general-purpose reasoning capabilities, yet they frequently hallucinate when used as world models (WMs), where strict compliance with deterministic transition rules--particularly in corner cases--is essential. In contrast, Symbolic WMs provide logical consistency but lack semantic expressivity. To bridge this gap, we propose Neuro-Symbolic Synergy (NeSyS), a framework that integrates the probabilistic semantic priors of LLMs with executable symbolic rules to achieve both expressivity and robustness. NeSyS alternates training between the two models using trajectories inadequately explained by the other. Unlike rule-based prompting, the symbolic WM directly constrains the LLM by modifying its output probability distribution. The neural WM is fine-tuned only on trajectories not covered by symbolic rules, reducing training data by 50% without loss of accuracy. Extensive experiments on three distinct interactive environments, i.e., ScienceWorld, Webshop, and Plancraft, demonstrate NeSyS's consistent advantages over baselines in both WM prediction accuracy and data efficiency.", "AI": {"tldr": "NeSyS\u6846\u67b6\u7ed3\u5408LLM\u7684\u8bed\u4e49\u5148\u9a8c\u4e0e\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u7684\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u5b9e\u73b0\u8868\u8fbe\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u4ea4\u4e92\u73af\u5883\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "LLM\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u65f6\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4e25\u683c\u9075\u5b88\u786e\u5b9a\u6027\u8f6c\u6362\u89c4\u5219\u7684\u8fb9\u89d2\u60c5\u51b5\u4e0b\uff0c\u800c\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u867d\u7136\u903b\u8f91\u4e00\u81f4\u4f46\u7f3a\u4e4f\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u534f\u540c\u6846\u67b6\uff0c\u5c06LLM\u7684\u6982\u7387\u8bed\u4e49\u5148\u9a8c\u4e0e\u53ef\u6267\u884c\u7684\u7b26\u53f7\u89c4\u5219\u7ed3\u5408\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u4e24\u79cd\u6a21\u578b\uff0c\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u76f4\u63a5\u7ea6\u675fLLM\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03\uff0c\u795e\u7ecf\u4e16\u754c\u6a21\u578b\u4ec5\u5728\u7b26\u53f7\u89c4\u5219\u672a\u8986\u76d6\u7684\u8f68\u8ff9\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728ScienceWorld\u3001Webshop\u548cPlancraft\u4e09\u4e2a\u4ea4\u4e92\u73af\u5883\u4e2d\uff0cNeSyS\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6570\u636e\u51cf\u5c1150%\u800c\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "conclusion": "NeSyS\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u795e\u7ecf\u548c\u7b26\u53f7\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u8868\u8fbe\u6027\u4e0e\u9c81\u68d2\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.10525", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10525", "abs": "https://arxiv.org/abs/2602.10525", "authors": ["George Pu", "Michael S. Lee", "Udari Madhushani Sehwag", "David J. Lee", "Bryan Zhu", "Yash Maurya", "Mohit Raghavendra", "Yuan Xue", "Samuel Marc Denton"], "title": "LHAW: Controllable Underspecification for Long-Horizon Tasks", "comment": null, "summary": "Long-horizon workflow agents that operate effectively over extended periods are essential for truly autonomous systems. Their reliable execution critically depends on the ability to reason through ambiguous situations in which clarification seeking is necessary to ensure correct task execution. However, progress is limited by the lack of scalable, task-agnostic frameworks for systematically curating and measuring the impact of ambiguity across custom workflows. We address this gap by introducing LHAW (Long-Horizon Augmented Workflows), a modular, dataset-agnostic synthetic pipeline that transforms any well-specified task into controllable underspecified variants by systematically removing information across four dimensions - Goals, Constraints, Inputs, and Context - at configurable severity levels. Unlike approaches that rely on LLM predictions of ambiguity, LHAW validates variants through empirical agent trials, classifying them as outcome-critical, divergent, or benign based on observed terminal state divergence. We release 285 task variants from TheAgentCompany, SWE-Bench Pro and MCP-Atlas according to our taxonomy alongside formal analysis measuring how current agents detect, reason about, and resolve underspecification across ambiguous settings. LHAW provides the first systematic framework for cost-sensitive evaluation of agent clarification behavior in long-horizon settings, enabling development of reliable autonomous systems.", "AI": {"tldr": "LHAW\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u6570\u636e\u96c6\u65e0\u5173\u7684\u5408\u6210\u7ba1\u9053\uff0c\u53ef\u5c06\u4efb\u4f55\u660e\u786e\u5b9a\u4e49\u7684\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u672a\u6307\u5b9a\u53d8\u4f53\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u4ee3\u7406\u5728\u6a21\u7cca\u60c5\u5883\u4e0b\u7684\u6f84\u6e05\u884c\u4e3a\u3002", "motivation": "\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u4ee3\u7406\u5728\u771f\u6b63\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u53ef\u9760\u6267\u884c\u4f9d\u8d56\u4e8e\u5728\u6a21\u7cca\u60c5\u5883\u4e0b\u8fdb\u884c\u63a8\u7406\u548c\u5bfb\u6c42\u6f84\u6e05\u7684\u80fd\u529b\u3002\u76ee\u524d\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u3001\u4efb\u52a1\u65e0\u5173\u7684\u6846\u67b6\u6765\u7cfb\u7edf\u6027\u5730\u6536\u96c6\u548c\u6d4b\u91cf\u6a21\u7cca\u6027\u5bf9\u81ea\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7684\u5f71\u54cd\u3002", "method": "LHAW\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5728\u56db\u4e2a\u7ef4\u5ea6\uff08\u76ee\u6807\u3001\u7ea6\u675f\u3001\u8f93\u5165\u3001\u4e0a\u4e0b\u6587\uff09\u4ee5\u53ef\u914d\u7f6e\u7684\u4e25\u91cd\u7a0b\u5ea6\u79fb\u9664\u4fe1\u606f\uff0c\u5c06\u660e\u786e\u5b9a\u4e49\u7684\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u672a\u6307\u5b9a\u53d8\u4f53\u3002\u4e0d\u540c\u4e8e\u4f9d\u8d56LLM\u9884\u6d4b\u6a21\u7cca\u6027\u7684\u65b9\u6cd5\uff0cLHAW\u901a\u8fc7\u5b9e\u8bc1\u4ee3\u7406\u8bd5\u9a8c\u9a8c\u8bc1\u53d8\u4f53\uff0c\u5e76\u6839\u636e\u89c2\u5bdf\u5230\u7684\u7ec8\u7aef\u72b6\u6001\u5dee\u5f02\u5c06\u53d8\u4f53\u5206\u7c7b\u4e3a\u7ed3\u679c\u5173\u952e\u578b\u3001\u53d1\u6563\u578b\u6216\u826f\u6027\u578b\u3002", "result": "\u53d1\u5e03\u4e86\u6765\u81eaTheAgentCompany\u3001SWE-Bench Pro\u548cMCP-Atlas\u7684285\u4e2a\u4efb\u52a1\u53d8\u4f53\uff0c\u5e76\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u5206\u6790\uff0c\u6d4b\u91cf\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u6a21\u7cca\u8bbe\u7f6e\u4e0b\u5982\u4f55\u68c0\u6d4b\u3001\u63a8\u7406\u548c\u89e3\u51b3\u672a\u6307\u5b9a\u95ee\u9898\u3002", "conclusion": "LHAW\u4e3a\u957f\u65f6\u7a0b\u8bbe\u7f6e\u4e2d\u4ee3\u7406\u6f84\u6e05\u884c\u4e3a\u7684\u6210\u672c\u654f\u611f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u53ef\u9760\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2602.10560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10560", "abs": "https://arxiv.org/abs/2602.10560", "authors": ["Leheng Sheng", "Yongtao Zhang", "Wenchang Ma", "Yaorui Shi", "Ting Huang", "Xiang Wang", "An Zhang", "Ke Shen", "Tat-Seng Chua"], "title": "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning", "comment": "26 pages", "summary": "While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals $r^{\\text{update}}$ and $r^{\\text{exit}}$ within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.", "AI": {"tldr": "GRU-Mem\u901a\u8fc7\u5f15\u5165\u6587\u672c\u63a7\u5236\u95e8\uff08\u66f4\u65b0\u95e8\u548c\u9000\u51fa\u95e8\uff09\u6539\u8fdbMemAgent\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u95e8\u63a7\u673a\u5236\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u5185\u5b58\u66f4\u65b0\u548c\u63a8\u7406\u5faa\u73af\u9000\u51fa\u3002", "motivation": "\u73b0\u6709MemAgent\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u5185\u5b58\u53ef\u80fd\u56e0\u65e0\u5dee\u522b\u66f4\u65b0\u800c\u7206\u70b8\u5f0f\u589e\u957f\uff1b2\uff09\u63a8\u7406\u5faa\u73af\u7f3a\u4e4f\u9000\u51fa\u673a\u5236\uff0c\u5bfc\u81f4\u6536\u96c6\u8db3\u591f\u8bc1\u636e\u540e\u4ecd\u8fdb\u884c\u4e0d\u5fc5\u8981\u8ba1\u7b97\u3002", "method": "\u63d0\u51faGRU-Mem\uff0c\u5f15\u5165\u4e24\u4e2a\u6587\u672c\u63a7\u5236\u95e8\uff08\u66f4\u65b0\u95e8\u548c\u9000\u51fa\u95e8\uff09\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u95e8\u63a7\u884c\u4e3a\uff0c\u5956\u52b1\u6b63\u786e\u7684\u66f4\u65b0\u548c\u9000\u51fa\u51b3\u7b56\uff0c\u5b9e\u73b0\u9009\u62e9\u6027\u5185\u5b58\u66f4\u65b0\u548c\u53ca\u65f6\u5faa\u73af\u9000\u51fa\u3002", "result": "\u5728\u5404\u79cd\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cGRU-Mem\u901a\u5e38\u4f18\u4e8e\u539f\u59cbMemAgent\uff0c\u63a8\u7406\u901f\u5ea6\u6700\u9ad8\u63d0\u5347400%\u3002", "conclusion": "GRU-Mem\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u7206\u70b8\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u63a8\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.10305", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10305", "abs": "https://arxiv.org/abs/2602.10305", "authors": ["Mateo Juliani", "Mingxuan Li", "Elias Bareinboim"], "title": "Confounding Robust Continuous Control via Automatic Reward Shaping", "comment": "Mateo Juliani and Mingxuan Li contributed equally to this work; accepted in AAMAS 2026", "summary": "Reward shaping has been applied widely to accelerate Reinforcement Learning (RL) agents' training. However, a principled way of designing effective reward shaping functions, especially for complex continuous control problems, remains largely under-explained. In this work, we propose to automatically learn a reward shaping function for continuous control problems from offline datasets, potentially contaminated by unobserved confounding variables. Specifically, our method builds upon the recently proposed causal Bellman equation to learn a tight upper bound on the optimal state values, which is then used as the potentials in the Potential-Based Reward Shaping (PBRS) framework. Our proposed reward shaping algorithm is tested with Soft-Actor-Critic (SAC) on multiple commonly used continuous control benchmarks and exhibits strong performance guarantees under unobserved confounders. More broadly, our work marks a solid first step towards confounding robust continuous control from a causal perspective. Code for training our reward shaping functions can be found at https://github.com/mateojuliani/confounding_robust_cont_control.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u81ea\u52a8\u5b66\u4e60\u5956\u52b1\u5851\u5f62\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u63a7\u5236\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u8bc1\u6027\u80fd\u3002", "motivation": "\u5956\u52b1\u5851\u5f62\u88ab\u5e7f\u6cdb\u7528\u4e8e\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f46\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u5851\u5f62\u51fd\u6570\uff0c\u7279\u522b\u662f\u9488\u5bf9\u590d\u6742\u8fde\u7eed\u63a7\u5236\u95ee\u9898\uff0c\u4ecd\u7136\u7f3a\u4e4f\u7cfb\u7edf\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u79bb\u7ebf\u6570\u636e\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u3002", "method": "\u57fa\u4e8e\u56e0\u679c\u8d1d\u5c14\u66fc\u65b9\u7a0b\u5b66\u4e60\u6700\u4f18\u72b6\u6001\u503c\u7684\u7d27\u4e0a\u754c\uff0c\u5c06\u5176\u4f5c\u4e3a\u57fa\u4e8e\u52bf\u7684\u5956\u52b1\u5851\u5f62\u6846\u67b6\u4e2d\u7684\u52bf\u51fd\u6570\u3002\u4f7f\u7528Soft-Actor-Critic\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u591a\u4e2a\u5e38\u7528\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u5f3a\u5927\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u8fd9\u662f\u4ece\u56e0\u679c\u89c6\u89d2\u5b9e\u73b0\u6df7\u6742\u9c81\u68d2\u8fde\u7eed\u63a7\u5236\u7684\u91cd\u8981\u7b2c\u4e00\u6b65\uff0c\u4e3a\u81ea\u52a8\u5b66\u4e60\u5956\u52b1\u5851\u5f62\u51fd\u6570\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10604", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10604", "abs": "https://arxiv.org/abs/2602.10604", "authors": ["Ailin Huang", "Ang Li", "Aobo Kong", "Bin Wang", "Binxing Jiao", "Bo Dong", "Bojun Wang", "Boyu Chen", "Brian Li", "Buyun Ma", "Chang Su", "Changxin Miao", "Changyi Wan", "Chao Lou", "Chen Hu", "Chen Xu", "Chenfeng Yu", "Chengting Feng", "Chengyuan Yao", "Chunrui Han", "Dan Ma", "Dapeng Shi", "Daxin Jiang", "Dehua Ma", "Deshan Sun", "Di Qi", "Enle Liu", "Fajie Zhang", "Fanqi Wan", "Guanzhe Huang", "Gulin Yan", "Guoliang Cao", "Guopeng Li", "Han Cheng", "Hangyu Guo", "Hanshan Zhang", "Hao Nie", "Haonan Jia", "Haoran Lv", "Hebin Zhou", "Hekun Lv", "Heng Wang", "Heung-Yeung Shum", "Hongbo Huang", "Hongbo Peng", "Hongyu Zhou", "Hongyuan Wang", "Houyong Chen", "Huangxi Zhu", "Huimin Wu", "Huiyong Guo", "Jia Wang", "Jian Zhou", "Jianjian Sun", "Jiaoren Wu", "Jiaran Zhang", "Jiashu Lv", "Jiashuo Liu", "Jiayi Fu", "Jiayu Liu", "Jie Cheng", "Jie Luo", "Jie Yang", "Jie Zhou", "Jieyi Hou", "Jing Bai", "Jingcheng Hu", "Jingjing Xie", "Jingwei Wu", "Jingyang Zhang", "Jishi Zhou", "Junfeng Liu", "Junzhe Lin", "Ka Man Lo", "Kai Liang", "Kaibo Liu", "Kaijun Tan", "Kaiwen Yan", "Kaixiang Li", "Kang An", "Kangheng Lin", "Lei Yang", "Liang Lv", "Liang Zhao", "Liangyu Chen", "Lieyu Shi", "Liguo Tan", "Lin Lin", "Lina Chen", "Luck Ma", "Mengqiang Ren", "Michael Li", "Ming Li", "Mingliang Li", "Mingming Zhang", "Mingrui Chen", "Mitt Huang", "Na Wang", "Peng Liu", "Qi Han", "Qian Zhao", "Qinglin He", "Qinxin Du", "Qiuping Wu", "Quan Sun", "Rongqiu Yang", "Ruihang Miao", "Ruixin Han", "Ruosi Wan", "Ruyan Guo", "Shan Wang", "Shaoliang Pang", "Shaowen Yang", "Shengjie Fan", "Shijie Shang", "Shiliang Yang", "Shiwei Li", "Shuangshuang Tian", "Siqi Liu", "Siye Wu", "Siyu Chen", "Song Yuan", "Tiancheng Cao", "Tianchi Yue", "Tianhao Cheng", "Tianning Li", "Tingdan Luo", "Wang You", "Wei Ji", "Wei Yuan", "Wei Zhang", "Weibo Wu", "Weihao Xie", "Wen Sun", "Wenjin Deng", "Wenzhen Zheng", "Wuxun Xie", "Xiangfeng Wang", "Xiangwen Kong", "Xiangyu Liu", "Xiangyu Zhang", "Xiaobo Yang", "Xiaojia Liu", "Xiaolan Yuan", "Xiaoran Jiao", "Xiaoxiao Ren", "Xiaoyun Zhang", "Xin Li", "Xin Liu", "Xin Wu", "Xing Chen", "Xingping Yang", "Xinran Wang", "Xu Zhao", "Xuan He", "Xuanti Feng", "Xuedan Cai", "Xuqiang Zhou", "Yanbo Yu", "Yang Li", "Yang Xu", "Yanlin Lai", "Yanming Xu", "Yaoyu Wang", "Yeqing Shen", "Yibo Zhu", "Yichen Lv", "Yicheng Cao", "Yifeng Gong", "Yijing Yang", "Yikun Yang", "Yin Zhao", "Yingxiu Zhao", "Yinmin Zhang", "Yitong Zhang", "Yixuan Zhang", "Yiyang Chen", "Yongchi Zhao", "Yongshen Long", "Yongyao Wang", "Yousong Guan", "Yu Zhou", "Yuang Peng", "Yuanhao Ding", "Yuantao Fan", "Yuanzhen Yang", "Yuchu Luo", "Yudi Zhao", "Yue Peng", "Yueqiang Lin", "Yufan Lu", "Yuling Zhao", "Yunzhou Ju", "Yurong Zhang", "Yusheng Li", "Yuxiang Yang", "Yuyang Chen", "Yuzhu Cai", "Zejia Weng", "Zetao Hong", "Zexi Li", "Zhe Xie", "Zheng Ge", "Zheng Gong", "Zheng Zeng", "Zhenyi Lu", "Zhewei Huang", "Zhichao Chang", "Zhiguo Huang", "Zhiheng Hu", "Zidong Yang", "Zili Wang", "Ziqi Ren", "Zixin Zhang", "Zixuan Wang"], "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "comment": "Technical report for Step 3.5 Flash", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "AI": {"tldr": "Step 3.5 Flash\u662f\u4e00\u4e2a\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7196B\u53c2\u6570\u57fa\u7840\u4e0e11B\u6fc0\u6d3b\u53c2\u6570\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3/\u5168\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e0a\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6784\u5efa\u667a\u80fd\u4f53\u65f6\u6700\u5173\u952e\u7684\u8981\u7d20\u662f\u654f\u9510\u7684\u63a8\u7406\u80fd\u529b\u548c\u5feb\u901f\u53ef\u9760\u7684\u6267\u884c\u6548\u7387\uff0c\u9700\u8981\u5728\u4fdd\u6301\u524d\u6cbf\u667a\u80fd\u6c34\u5e73\u7684\u540c\u65f6\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "method": "\u91c7\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u54083:1\u6ed1\u52a8\u7a97\u53e3/\u5168\u6ce8\u610f\u529b\u4ea4\u66ff\u673a\u5236\u548c\u591a\u4ee4\u724c\u9884\u6d4b\u51cf\u5c11\u5ef6\u8fdf\uff1b\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u878d\u5408\u53ef\u9a8c\u8bc1\u4fe1\u53f7\u4e0e\u504f\u597d\u53cd\u9988\uff0c\u652f\u6301\u5927\u89c4\u6a21\u79bb\u7b56\u7565\u8bad\u7ec3\u7684\u7a33\u5b9a\u81ea\u6211\u6539\u8fdb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aIMO-AnswerBench 85.4%\uff0cLiveCodeBench-v6 86.4%\uff0ctau2-Bench 88.2%\uff0cBrowseComp 69.0%\uff0cTerminal-Bench 2.0 51.0%\uff0c\u6027\u80fd\u4e0eGPT-5.2 xHigh\u548cGemini 3.0 Pro\u7b49\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "Step 3.5 Flash\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u6548\u7387\u524d\u6cbf\uff0c\u4e3a\u5728\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u590d\u6742\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9ad8\u5bc6\u5ea6\u57fa\u7840\uff0c\u5e73\u8861\u4e86\u667a\u80fd\u6c34\u5e73\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2602.10652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10652", "abs": "https://arxiv.org/abs/2602.10652", "authors": ["Yongshi Ye", "Hui Jiang", "Feihu Jiang", "Tian Lan", "Yichao Du", "Biao Fu", "Xiaodong Shi", "Qianghuai Jia", "Longyue Wang", "Weihua Luo"], "title": "UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory", "comment": null, "summary": "Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.", "AI": {"tldr": "UMEM\u6846\u67b6\u8054\u5408\u4f18\u5316LLM\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u63d0\u53d6\u4e0e\u7ba1\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u90bb\u57df\u5efa\u6a21\u548cGRPO\u5956\u52b1\u63d0\u5347\u8bb0\u5fc6\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u8bb0\u5fc6\u7ba1\u7406\uff0c\u4f46\u5c06\u8bb0\u5fc6\u63d0\u53d6\u89c6\u4e3a\u9759\u6001\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u667a\u80fd\u4f53\u79ef\u7d2f\u7684\u662f\u5b9e\u4f8b\u7279\u5b9a\u566a\u58f0\u800c\u975e\u9c81\u68d2\u8bb0\u5fc6", "method": "\u63d0\u51fa\u7edf\u4e00\u8bb0\u5fc6\u63d0\u53d6\u4e0e\u7ba1\u7406(UMEM)\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316LLM\u540c\u65f6\u8fdb\u884c\u8bb0\u5fc6\u63d0\u53d6\u548c\u7ba1\u7406\uff1b\u5f15\u5165\u8bed\u4e49\u90bb\u57df\u5efa\u6a21\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u6a21\u578b\u83b7\u5f97\u90bb\u57df\u7ea7\u8fb9\u9645\u6548\u7528\u5956\u52b1\uff0c\u786e\u4fdd\u8bb0\u5fc6\u5728\u8bed\u4e49\u76f8\u5173\u67e5\u8be2\u7c07\u4e2d\u7684\u6cdb\u5316\u80fd\u529b", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u591a\u8f6e\u4ea4\u4e92\u4efb\u52a1\u4e2d\u63d0\u5347\u8fbe10.67%\uff1b\u5728\u6301\u7eed\u6f14\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5355\u8c03\u589e\u957f\u66f2\u7ebf", "conclusion": "UMEM\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bb0\u5fc6\u63d0\u53d6\u4e0e\u7ba1\u7406\uff0c\u7ed3\u5408\u8bed\u4e49\u90bb\u57df\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u8bb0\u5fc6\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u81ea\u6f14\u5316\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u8bb0\u5fc6\u673a\u5236", "topic": "agent analysis"}}
{"id": "2602.10371", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10371", "abs": "https://arxiv.org/abs/2602.10371", "authors": ["Elias Kempf", "Simon Schrodi", "Bartosz Cywi\u0144ski", "Thomas Brox", "Neel Nanda", "Arthur Conmy"], "title": "Simple LLM Baselines are Competitive for Model Diffing", "comment": null, "summary": "Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6a21\u578b\u5dee\u5f02\u5206\u6790\u65b9\u6cd5\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u6bd4\u8f83LLM\u57fa\u7ebf\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u53d1\u73b0\u6539\u8fdb\u7684LLM\u57fa\u7ebf\u5728\u53d1\u73b0\u62bd\u8c61\u884c\u4e3a\u5dee\u5f02\u65b9\u9762\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u3002", "motivation": "\u6807\u51c6LLM\u8bc4\u4f30\u53ea\u80fd\u6d4b\u8bd5\u8bbe\u8ba1\u597d\u7684\u80fd\u529b\uff0c\u4f1a\u9057\u6f0f\u6a21\u578b\u4fee\u8ba2\u95f4\u7684\u610f\u5916\u884c\u4e3a\u5dee\u5f02\u6216\u65b0\u51fa\u73b0\u7684\u672a\u5bf9\u9f50\u503e\u5411\u3002\u73b0\u6709\u6a21\u578b\u5dee\u5f02\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u6307\u6807\uff08\u6cdb\u5316\u6027\u3001\u6709\u8da3\u6027\u3001\u62bd\u8c61\u5c42\u6b21\uff09\u6765\u6bd4\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ecLLM\u57fa\u7ebf\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdbLLM\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u6539\u8fdb\u7684LLM\u57fa\u7ebf\u65b9\u6cd5\u4e0eSAE\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u901a\u5e38\u80fd\u53d1\u73b0\u66f4\u62bd\u8c61\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "conclusion": "\u6a21\u578b\u5dee\u5f02\u5206\u6790\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u6539\u8fdb\u7684LLM\u65b9\u6cd5\u5728\u53d1\u73b0\u62bd\u8c61\u884c\u4e3a\u5dee\u5f02\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u6a21\u578b\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.10715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10715", "abs": "https://arxiv.org/abs/2602.10715", "authors": ["Yifei Li", "Weidong Guo", "Lingling Zhang", "Rongman Xu", "Muye Huang", "Hui Liu", "Lijiao Xu", "Yu Xu", "Jun Liu"], "title": "Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents", "comment": "16 pages, 8 figures", "summary": "Long-term conversational memory is a core capability for LLM-based dialogue systems, yet existing benchmarks and evaluation protocols primarily focus on surface-level factual recall. In realistic interactions, appropriate responses often depend on implicit constraints such as user state, goals, or values that are not explicitly queried later. To evaluate this setting, we introduce \\textbf{LoCoMo-Plus}, a benchmark for assessing cognitive memory under cue--trigger semantic disconnect, where models must retain and apply latent constraints across long conversational contexts. We further show that conventional string-matching metrics and explicit task-type prompting are misaligned with such scenarios, and propose a unified evaluation framework based on constraint consistency. Experiments across diverse backbone models, retrieval-based methods, and memory systems demonstrate that cognitive memory remains challenging and reveals failures not captured by existing benchmarks. Our code and evaluation framework are publicly available at: https://github.com/xjtuleeyf/Locomo-Plus.", "AI": {"tldr": "LoCoMo-Plus\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5bf9\u8bdd\u7cfb\u7edf\u8ba4\u77e5\u8bb0\u5fc6\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\"\u7ebf\u7d22-\u89e6\u53d1\u8bed\u4e49\u65ad\u5f00\"\u573a\u666f\uff0c\u8981\u6c42\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u4fdd\u7559\u548c\u5e94\u7528\u6f5c\u5728\u7ea6\u675f\uff0c\u800c\u975e\u7b80\u5355\u7684\u8868\u9762\u4e8b\u5b9e\u56de\u5fc6\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u8bb0\u5fc6\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u8868\u9762\u4e8b\u5b9e\u56de\u5fc6\uff0c\u4f46\u5728\u771f\u5b9e\u5bf9\u8bdd\u4e2d\uff0c\u5408\u9002\u7684\u56de\u5e94\u5f80\u5f80\u4f9d\u8d56\u4e8e\u672a\u660e\u786e\u67e5\u8be2\u7684\u9690\u542b\u7ea6\u675f\uff08\u5982\u7528\u6237\u72b6\u6001\u3001\u76ee\u6807\u6216\u4ef7\u503c\u89c2\uff09\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u8fd9\u79cd\"\u7ebf\u7d22-\u89e6\u53d1\u8bed\u4e49\u65ad\u5f00\"\u573a\u666f\u4e0b\u7684\u8ba4\u77e5\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "1) \u63d0\u51faLoCoMo-Plus\u57fa\u51c6\uff0c\u8bc4\u4f30\u8ba4\u77e5\u8bb0\u5fc6\u5728\"\u7ebf\u7d22-\u89e6\u53d1\u8bed\u4e49\u65ad\u5f00\"\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff1b2) \u6307\u51fa\u4f20\u7edf\u5b57\u7b26\u4e32\u5339\u914d\u6307\u6807\u548c\u663e\u5f0f\u4efb\u52a1\u7c7b\u578b\u63d0\u793a\u5728\u6b64\u7c7b\u573a\u666f\u4e2d\u7684\u4e0d\u9002\u7528\u6027\uff1b3) \u63d0\u51fa\u57fa\u4e8e\u7ea6\u675f\u4e00\u81f4\u6027\u7684\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8ba4\u77e5\u8bb0\u5fc6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u6355\u6349\u5230\u6a21\u578b\u5728\u6b64\u7c7b\u573a\u666f\u4e0b\u7684\u5931\u8d25\u3002\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u3001\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u548c\u8bb0\u5fc6\u7cfb\u7edf\u4e2d\u90fd\u89c2\u5bdf\u5230\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "LoCoMo-Plus\u63ed\u793a\u4e86\u73b0\u6709\u5bf9\u8bdd\u8bb0\u5fc6\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u8bc4\u4f30\u8ba4\u77e5\u8bb0\u5fc6\u80fd\u529b\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\"\u7ebf\u7d22-\u89e6\u53d1\u8bed\u4e49\u65ad\u5f00\"\u573a\u666f\u4e0b\u3002\u63d0\u51fa\u7684\u7ea6\u675f\u4e00\u81f4\u6027\u6846\u67b6\u4e3a\u66f4\u5168\u9762\u7684\u5bf9\u8bdd\u8bb0\u5fc6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.10740", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10740", "abs": "https://arxiv.org/abs/2602.10740", "authors": ["Yuming Yan", "Shuo Yang", "Kai Tang", "Sihong Chen", "Yang Zhang", "Ke Xu", "Dan Hu", "Qun Yu", "Pengfei Hu", "Edith C. H. Ngai"], "title": "Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) demonstrate remarkable general-purpose capabilities but often fall short in specialized domains such as medical imaging or geometric problem-solving. Supervised Fine-Tuning (SFT) can enhance performance within a target domain, but it typically causes catastrophic forgetting, limiting its generalization. The central challenge, therefore, is to adapt VLMs to new domains while preserving their general-purpose capabilities. Continual pretraining is effective for expanding knowledge in Large Language Models (LLMs), but it is less feasible for VLMs due to prohibitive computational costs and the unavailability of pretraining data for most open-source models. This necessitates efficient post-training adaptation methods. Reinforcement learning (RL)-based approaches such as Group Relative Policy Optimization (GRPO) have shown promise in preserving general abilities, yet they often fail in domain adaptation scenarios where the model initially lacks sufficient domain knowledge, leading to optimization collapse. To bridge this gap, we propose Reinforced Curriculum Pre-Alignment (RCPA), a novel post-training paradigm that introduces a curriculum-aware progressive modulation mechanism. In the early phase, RCPA applies partial output constraints to safely expose the model to new domain concepts. As the model's domain familiarity increases, training gradually transitions to full generation optimization, refining responses and aligning them with domain-specific preferences. This staged adaptation balances domain knowledge acquisition with the preservation of general multimodal capabilities. Extensive experiments across specialized domains and general benchmarks validate the effectiveness of RCPA, establishing a practical pathway toward building high-performing and domain-adaptive VLMs.", "AI": {"tldr": "\u63d0\u51faRCPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bfe\u7a0b\u611f\u77e5\u6e10\u8fdb\u8c03\u5236\u673a\u5236\uff0c\u5206\u9636\u6bb5\u9002\u5e94\u65b0\u9886\u57df\u77e5\u8bc6\u540c\u65f6\u4fdd\u6301VLM\u901a\u7528\u80fd\u529b", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u76d1\u7763\u5fae\u8c03\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u6301\u7eed\u9884\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u9ad8\u6548\u7684\u9886\u57df\u9002\u5e94\u65b9\u6cd5", "method": "RCPA\u65b9\u6cd5\uff1a\u65e9\u671f\u9636\u6bb5\u5e94\u7528\u90e8\u5206\u8f93\u51fa\u7ea6\u675f\u5b89\u5168\u5f15\u5165\u65b0\u9886\u57df\u6982\u5ff5\uff0c\u968f\u7740\u6a21\u578b\u719f\u6089\u5ea6\u589e\u52a0\uff0c\u9010\u6b65\u8fc7\u6e21\u5230\u5b8c\u6574\u751f\u6210\u4f18\u5316\uff0c\u5e73\u8861\u9886\u57df\u77e5\u8bc6\u83b7\u53d6\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u6301", "result": "\u5728\u591a\u4e2a\u4e13\u4e1a\u9886\u57df\u548c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86RCPA\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u9886\u57df\u81ea\u9002\u5e94VLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84", "conclusion": "RCPA\u901a\u8fc7\u8bfe\u7a0b\u611f\u77e5\u6e10\u8fdb\u8c03\u5236\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86VLM\u9886\u57df\u9002\u5e94\u4e2d\u7684\u4f18\u5316\u5d29\u6e83\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u4e13\u4e1a\u80fd\u529b\u63d0\u5347\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u6301", "topic": "agentic reinforcement learning"}}
{"id": "2602.10801", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10801", "abs": "https://arxiv.org/abs/2602.10801", "authors": ["Haotian Sheng", "Heyong Wang", "Ming Hong", "Hongman He", "Junqiu Liu"], "title": "Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success, however, the emergence of content generation distortion (hallucination) limits their practical applications. The core cause of hallucination lies in LLMs' lack of awareness regarding their stored internal knowledge, preventing them from expressing their knowledge state on questions beyond their internal knowledge boundaries, as humans do. However, existing research on knowledge boundary expression primarily focuses on white-box LLMs, leaving methods suitable for black-box LLMs which offer only API access without revealing internal parameters-largely unexplored. Against this backdrop, this paper proposes LSCL (LLM-Supervised Confidence Learning), a deep learning-based method for expressing the knowledge boundaries of black-box LLMs. Based on the knowledge distillation framework, this method designs a deep learning model. Taking the input question, output answer, and token probability from a black-box LLM as inputs, it constructs a mapping between the inputs and the model' internal knowledge state, enabling the quantification and expression of the black-box LLM' knowledge boundaries. Experiments conducted on diverse public datasets and with multiple prominent black-box LLMs demonstrate that LSCL effectively assists black-box LLMs in accurately expressing their knowledge boundaries. It significantly outperforms existing baseline models on metrics such as accuracy and recall rate. Furthermore, considering scenarios where some black-box LLMs do not support access to token probability, an adaptive alternative method is proposed. The performance of this alternative approach is close to that of LSCL and surpasses baseline models.", "AI": {"tldr": "\u63d0\u51faLSCL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e2e\u52a9\u9ed1\u76d2LLM\u8868\u8fbe\u77e5\u8bc6\u8fb9\u754c\uff0c\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8fb9\u754c\u8868\u8fbe\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u767d\u76d2LLM\uff0c\u800c\u9ed1\u76d2LLM\uff08\u4ec5\u63d0\u4f9bAPI\u8bbf\u95ee\uff09\u7684\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002LLM\u7f3a\u4e4f\u5bf9\u5185\u90e8\u77e5\u8bc6\u7684\u610f\u8bc6\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u8ba9LLM\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u8868\u8fbe\u8d85\u51fa\u77e5\u8bc6\u8fb9\u754c\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u8bbe\u8ba1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bLSCL\uff0c\u4ee5\u9ed1\u76d2LLM\u7684\u8f93\u5165\u95ee\u9898\u3001\u8f93\u51fa\u7b54\u6848\u548ctoken\u6982\u7387\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u8f93\u5165\u4e0e\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u72b6\u6001\u7684\u6620\u5c04\uff0c\u91cf\u5316\u8868\u8fbe\u9ed1\u76d2LLM\u7684\u77e5\u8bc6\u8fb9\u754c", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e3b\u6d41\u9ed1\u76d2LLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSCL\u80fd\u6709\u6548\u5e2e\u52a9\u9ed1\u76d2LLM\u51c6\u786e\u8868\u8fbe\u77e5\u8bc6\u8fb9\u754c\uff0c\u5728\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u7b49\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b", "conclusion": "LSCL\u4e3a\u89e3\u51b3\u9ed1\u76d2LLM\u7684\u77e5\u8bc6\u8fb9\u754c\u8868\u8fbe\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5bf9\u4e8e\u4e0d\u652f\u6301token\u6982\u7387\u8bbf\u95ee\u7684\u9ed1\u76d2LLM\u4e5f\u63d0\u51fa\u4e86\u6027\u80fd\u63a5\u8fd1\u7684\u66ff\u4ee3\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.10816", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10816", "abs": "https://arxiv.org/abs/2602.10816", "authors": ["Deyuan Liu", "Zecheng Wang", "Zhanyue Qin", "Zhiying Tu", "Dianhui Chu", "Dianbo Sui"], "title": "Beyond Confidence: The Rhythms of Reasoning in Generative Models", "comment": "ICLR 2026", "summary": "Large Language Models (LLMs) exhibit impressive capabilities yet suffer from sensitivity to slight input context variations, hampering reliability. Conventional metrics like accuracy and perplexity fail to assess local prediction robustness, as normalized output probabilities can obscure the underlying resilience of an LLM's internal state to perturbations. We introduce the Token Constraint Bound ($\u03b4_{\\mathrm{TCB}}$), a novel metric that quantifies the maximum internal state perturbation an LLM can withstand before its dominant next-token prediction significantly changes. Intrinsically linked to output embedding space geometry, $\u03b4_{\\mathrm{TCB}}$ provides insights into the stability of the model's internal predictive commitment. Our experiments show $\u03b4_{\\mathrm{TCB}}$ correlates with effective prompt engineering and uncovers critical prediction instabilities missed by perplexity during in-context learning and text generation. $\u03b4_{\\mathrm{TCB}}$ offers a principled, complementary approach to analyze and potentially improve the contextual stability of LLM predictions.", "AI": {"tldr": "\u63d0\u51faToken Constraint Bound (\u03b4_TCB)\u65b0\u6307\u6807\uff0c\u91cf\u5316LLM\u5185\u90e8\u72b6\u6001\u80fd\u627f\u53d7\u7684\u6700\u5927\u6270\u52a8\u800c\u4e0d\u6539\u53d8\u4e3b\u5bfc\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u3002", "motivation": "LLM\u5bf9\u8f93\u5165\u4e0a\u4e0b\u6587\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002\u4f20\u7edf\u6307\u6807\u5982\u51c6\u786e\u7387\u548c\u56f0\u60d1\u5ea6\u65e0\u6cd5\u8bc4\u4f30\u5c40\u90e8\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u56e0\u4e3a\u5f52\u4e00\u5316\u7684\u8f93\u51fa\u6982\u7387\u4f1a\u63a9\u76d6LLM\u5185\u90e8\u72b6\u6001\u5bf9\u6270\u52a8\u7684\u6f5c\u5728\u5f39\u6027\u3002", "method": "\u5f15\u5165Token Constraint Bound (\u03b4_TCB)\u6307\u6807\uff0c\u91cf\u5316LLM\u5185\u90e8\u72b6\u6001\u80fd\u627f\u53d7\u7684\u6700\u5927\u6270\u52a8\u800c\u4e0d\u663e\u8457\u6539\u53d8\u5176\u4e3b\u5bfc\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u3002\u8be5\u6307\u6807\u4e0e\u8f93\u51fa\u5d4c\u5165\u7a7a\u95f4\u51e0\u4f55\u5185\u5728\u76f8\u5173\uff0c\u63d0\u4f9b\u6a21\u578b\u5185\u90e8\u9884\u6d4b\u627f\u8bfa\u7a33\u5b9a\u6027\u7684\u6d1e\u5bdf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u03b4_TCB\u4e0e\u6709\u6548\u63d0\u793a\u5de5\u7a0b\u76f8\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u88ab\u56f0\u60d1\u5ea6\u5ffd\u7565\u7684\u5173\u952e\u9884\u6d4b\u4e0d\u7a33\u5b9a\u6027\u3002\u03b4_TCB\u4e3a\u5206\u6790\u548c\u6f5c\u5728\u6539\u8fdbLLM\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8865\u5145\u65b9\u6cd5\u3002", "conclusion": "\u03b4_TCB\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6307\u6807\uff0c\u80fd\u591f\u91cf\u5316LLM\u5185\u90e8\u72b6\u6001\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u6307\u6807\u7684\u4e0d\u8db3\u3002", "topic": "agent analysis"}}
{"id": "2602.10874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10874", "abs": "https://arxiv.org/abs/2602.10874", "authors": ["Binwei Yan", "Yifei Fu", "Mingjian Zhu", "Hanting Chen", "Mingxuan Yuan", "Yunhe Wang", "Hailin Hu"], "title": "C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution", "comment": "The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP", "summary": "Automatic prompt optimization is a promising direction to boost the performance of Large Language Models (LLMs). However, existing methods often suffer from noisy and conflicting update signals. In this research, we propose C-MOP (Cluster-based Momentum Optimized Prompting), a framework that stabilizes optimization via Boundary-Aware Contrastive Sampling (BACS) and Momentum-Guided Semantic Clustering (MGSC). Specifically, BACS utilizes batch-level information to mine tripartite features--Hard Negatives, Anchors, and Boundary Pairs--to precisely characterize the typical representation and decision boundaries of positive and negative prompt samples. To resolve semantic conflicts, MGSC introduces a textual momentum mechanism with temporal decay that distills persistent consensus from fluctuating gradients across iterations. Extensive experiments demonstrate that C-MOP consistently outperforms SOTA baselines like PromptWizard and ProTeGi, yielding average gains of 1.58% and 3.35%. Notably, C-MOP enables a general LLM with 3B activated parameters to surpass a 70B domain-specific dense LLM, highlighting its effectiveness in driving precise prompt evolution. The code is available at https://github.com/huawei-noah/noah-research/tree/master/C-MOP.", "AI": {"tldr": "C-MOP\u662f\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u7684\u52a8\u91cf\u4f18\u5316\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u5bf9\u6bd4\u91c7\u6837\u548c\u52a8\u91cf\u5f15\u5bfc\u8bed\u4e49\u805a\u7c7b\u6765\u7a33\u5b9a\u63d0\u793a\u4f18\u5316\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u566a\u58f0\u548c\u51b2\u7a81\u4fe1\u53f7\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5e38\u53d7\u5230\u566a\u58f0\u548c\u51b2\u7a81\u66f4\u65b0\u4fe1\u53f7\u7684\u56f0\u6270\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u7684\u4f18\u5316\u6846\u67b6\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faC-MOP\u6846\u67b6\uff1a1) BACS\u5229\u7528\u6279\u6b21\u4fe1\u606f\u6316\u6398\u786c\u8d1f\u6837\u672c\u3001\u951a\u70b9\u548c\u8fb9\u754c\u5bf9\u6765\u8868\u5f81\u6b63\u8d1f\u63d0\u793a\u6837\u672c\u7684\u5178\u578b\u8868\u793a\u548c\u51b3\u7b56\u8fb9\u754c\uff1b2) MGSC\u5f15\u5165\u5e26\u65f6\u95f4\u8870\u51cf\u7684\u6587\u672c\u52a8\u91cf\u673a\u5236\uff0c\u4ece\u8fed\u4ee3\u4e2d\u6ce2\u52a8\u7684\u68af\u5ea6\u4e2d\u63d0\u53d6\u6301\u4e45\u5171\u8bc6\u3002", "result": "C-MOP\u5728\u5b9e\u9a8c\u4e2d\u4e00\u81f4\u4f18\u4e8ePromptWizard\u548cProTeGi\u7b49SOTA\u57fa\u7ebf\uff0c\u5e73\u5747\u63d0\u53471.58%\u548c3.35%\u3002\u4ec5\u75283B\u6fc0\u6d3b\u53c2\u6570\u7684\u901a\u7528LLM\u5c31\u80fd\u8d85\u8d8a70B\u9886\u57df\u4e13\u7528\u5bc6\u96c6LLM\u3002", "conclusion": "C-MOP\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u5bf9\u6bd4\u91c7\u6837\u548c\u52a8\u91cf\u5f15\u5bfc\u8bed\u4e49\u805a\u7c7b\u6709\u6548\u89e3\u51b3\u4e86\u63d0\u793a\u4f18\u5316\u4e2d\u7684\u566a\u58f0\u548c\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u63d0\u793a\u6f14\u5316\u3002", "topic": "agent analysis"}}
{"id": "2602.10430", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10430", "abs": "https://arxiv.org/abs/2602.10430", "authors": ["Jie Jiang", "Yusen Huo", "Xiangxin Zhan", "Changping Wang", "Jun Zhang"], "title": "Breaking the Curse of Repulsion: Optimistic Distributionally Robust Policy Optimization for Off-Policy Generative Recommendation", "comment": "21 pages, 8 figures", "summary": "Policy-based Reinforcement Learning (RL) has established itself as the dominant paradigm in generative recommendation for optimizing sequential user interactions. However, when applied to offline historical logs, these methods suffer a critical failure: the dominance of low-quality data induces severe model collapse. We first establish the Divergence Theory of Repulsive Optimization, revealing that negative gradient updates inherently trigger exponential intensity explosion during off-policy training. This theory elucidates the inherent dilemma of existing methods, exposing their inability to reconcile variance reduction and noise imitation. To break this curse, we argue that the solution lies in rigorously identifying the latent high-quality distribution entangled within the noisy behavior policy. Accordingly, we reformulate the objective as an Optimistic Distributionally Robust Optimization (DRO) problem. Guided by this formulation, we propose Distributionally Robust Policy Optimization (DRPO). We prove that hard filtering is the exact solution to this DRO objective, enabling DRPO to optimally recover high-quality behaviors while strictly discarding divergence-inducing noise. Extensive experiments demonstrate that DRPO achieves state-of-the-art performance on mixed-quality recommendation benchmarks.", "AI": {"tldr": "DRPO\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f4e\u8d28\u91cf\u6570\u636e\u5bfc\u81f4\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u4f7f\u7528\u786c\u8fc7\u6ee4\u673a\u5236\u6062\u590d\u9ad8\u8d28\u91cf\u884c\u4e3a\u5206\u5e03\u3002", "motivation": "\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u9762\u4e34\u79bb\u7ebf\u5386\u53f2\u65e5\u5fd7\u4e2d\u4f4e\u8d28\u91cf\u6570\u636e\u4e3b\u5bfc\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6a21\u578b\u5d29\u6e83\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u65b9\u5dee\u51cf\u5c11\u548c\u566a\u58f0\u6a21\u4eff\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u9c81\u68d2\u7b56\u7565\u4f18\u5316(DRPO)\uff0c\u5c06\u76ee\u6807\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e50\u89c2\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u786c\u8fc7\u6ee4\u662f\u8be5DRO\u76ee\u6807\u7684\u7cbe\u786e\u89e3\uff0c\u80fd\u591f\u6700\u4f18\u5730\u6062\u590d\u9ad8\u8d28\u91cf\u884c\u4e3a\u540c\u65f6\u4e25\u683c\u4e22\u5f03\u5bfc\u81f4\u53d1\u6563\u7684\u566a\u58f0\u3002", "result": "\u5728\u6df7\u5408\u8d28\u91cf\u63a8\u8350\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDRPO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u566a\u58f0\u884c\u4e3a\u7b56\u7565\u4e2d\u6f5c\u5728\u7684\u9ad8\u8d28\u91cf\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0cDRPO\u6210\u529f\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10953", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10953", "abs": "https://arxiv.org/abs/2602.10953", "authors": ["Mingyu Cao", "Alvaro Correia", "Christos Louizos", "Shiwei Liu", "Lu Yin"], "title": "Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models", "comment": "11 pages, 8 figures", "summary": "Diffusion Language Models (DLMs) generate text by iteratively denoising a masked sequence, repeatedly deciding which positions to commit at each step. Standard decoding follows a greedy rule: unmask the most confident positions, yet this local choice can lock the model into a suboptimal unmasking order, especially on reasoning-heavy prompts. We present SOAR, a training-free decoding algorithm that adapts its behavior to the model's uncertainty. When confidence is low, SOAR briefly widens the search over alternative unmasking decisions to avoid premature commitments; when confidence is high, it collapses the search and decodes many positions in parallel to reduce the number of denoising iterations. Across mathematical reasoning and code generation benchmarks (GSM8K, MBPP, HumanEval) on Dream-7B and LLaDA-8B, SOAR improves generation quality while maintaining competitive inference speed, offering a practical way to balance quality and efficiency in DLM decoding.", "AI": {"tldr": "SOAR\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684DLM\u89e3\u7801\u7b97\u6cd5\uff0c\u6839\u636e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff1a\u4f4e\u7f6e\u4fe1\u5ea6\u65f6\u6269\u5927\u641c\u7d22\u907f\u514d\u8fc7\u65e9\u51b3\u7b56\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u5e76\u884c\u89e3\u7801\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u63d0\u5347\u8d28\u91cf\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6807\u51c6DLM\u89e3\u7801\u91c7\u7528\u8d2a\u5fc3\u7b56\u7565\uff08\u6bcf\u6b21\u9009\u62e9\u7f6e\u4fe1\u5ea6\u6700\u9ad8\u7684\u4f4d\u7f6e\u89e3\u63a9\u7801\uff09\uff0c\u8fd9\u79cd\u5c40\u90e8\u51b3\u7b56\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684\u89e3\u7801\u987a\u5e8f\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u5728\u8d28\u91cf\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51faSOAR\u89e3\u7801\u7b97\u6cd5\uff1a1\uff09\u4f4e\u7f6e\u4fe1\u5ea6\u65f6\u6269\u5927\u641c\u7d22\u7a7a\u95f4\uff0c\u8003\u8651\u66ff\u4ee3\u7684\u89e3\u7801\u51b3\u7b56\uff1b2\uff09\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u5e76\u884c\u89e3\u7801\u591a\u4e2a\u4f4d\u7f6e\uff0c\u51cf\u5c11\u53bb\u566a\u8fed\u4ee3\u6b21\u6570\uff1b3\uff09\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709DLM\u6a21\u578b\u3002", "result": "\u5728GSM8K\u3001MBPP\u3001HumanEval\u7b49\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Dream-7B\u548cLLaDA-8B\u6a21\u578b\uff0cSOAR\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SOAR\u4e3aDLM\u89e3\u7801\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8d28\u91cf-\u6548\u7387\u5e73\u8861\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u6539\u5584\u4e86\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u751f\u6210\u6548\u679c\u3002", "topic": "code agent"}}
{"id": "2602.11096", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11096", "abs": "https://arxiv.org/abs/2602.11096", "authors": ["Soumya Suvra Ghosal", "Souradip Chakraborty", "Vaibhav Singh", "Furong Huang", "Dinesh Manocha", "Amrit Singh Bedi"], "title": "Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away", "comment": null, "summary": "Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective. SafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix (\"Wait, think safely\") only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60% (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1-3 reasoning steps typically suffices to redirect the full generation toward safe completions.", "AI": {"tldr": "SafeThink\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u63a7\u63a8\u7406\u8f68\u8ff9\u5e76\u5728\u5b89\u5168\u9608\u503c\u88ab\u8fdd\u53cd\u65f6\u6ce8\u5165\u4f18\u5316\u524d\u7f00\u6765\u6062\u590d\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u94fe\u5f0f\u601d\u7ef4\u540e\u8bad\u7ec3\u867d\u7136\u80fd\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u540c\u65f6\u964d\u4f4e\u5b89\u5168\u5bf9\u9f50\u6027\u5e76\u589e\u52a0\u8d8a\u72f1\u6210\u529f\u7387\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u6062\u590d\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faSafeThink\u65b9\u6cd5\uff0c\u5c06\u5b89\u5168\u6062\u590d\u89c6\u4e3a\u6ee1\u8db3\u6027\u7ea6\u675f\u800c\u975e\u6700\u5927\u5316\u76ee\u6807\u3002\u4f7f\u7528\u5b89\u5168\u5956\u52b1\u6a21\u578b\u76d1\u63a7\u63a8\u7406\u8f68\u8ff9\uff0c\u5f53\u5b89\u5168\u9608\u503c\u88ab\u8fdd\u53cd\u65f6\uff0c\u6761\u4ef6\u6027\u5730\u6ce8\u5165\u4f18\u5316\u7684\u77ed\u524d\u7f00\uff08\"Wait, think safely\"\uff09\u6765\u7ea0\u6b63\u63a8\u7406\u65b9\u5411\u3002", "result": "\u57286\u4e2a\u5f00\u6e90\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c4\u4e2a\u8d8a\u72f1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSafeThink\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e30-60%\uff08\u5982LlamaV-o1\u4ece63.33%\u964d\u81f35.74%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff08MathVista\u51c6\u786e\u7387\u4ece65.20%\u523065.00%\uff09\u3002\u5b9e\u9a8c\u53d1\u73b0\u5b89\u5168\u6062\u590d\u901a\u5e38\u53ea\u97001-3\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u5e72\u9884\u3002", "conclusion": "SafeThink\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u5728\u4e0d\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4e14\u5e72\u9884\u6210\u672c\u4f4e\uff0c\u53ea\u9700\u5728\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5c11\u91cf\u7ea0\u6b63\u3002", "topic": "agent analysis"}}
{"id": "2602.10576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10576", "abs": "https://arxiv.org/abs/2602.10576", "authors": ["Boxiao Wang", "Kai Li", "Tianyi Liu", "Chen Li", "Junzhe Wang", "Yifan Zhang", "Jian Cheng"], "title": "LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization", "comment": null, "summary": "Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.", "AI": {"tldr": "\u63d0\u51faPiT-PO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06LLM\u8f6c\u53d8\u4e3a\u81ea\u9002\u5e94\u751f\u6210\u5668\uff0c\u5728\u7b26\u53f7\u56de\u5f52\u4e2d\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u4e14\u7ed3\u6784\u7b80\u6d01\u7684\u65b9\u7a0b\u53d1\u73b0", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06LLM\u4f5c\u4e3a\u9759\u6001\u751f\u6210\u5668\uff0c\u4ec5\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\uff0c\u65e0\u6cd5\u57fa\u4e8e\u641c\u7d22\u53cd\u9988\u66f4\u65b0\u6a21\u578b\u5185\u90e8\u8868\u793a\uff0c\u5bfc\u81f4\u7269\u7406\u4e0d\u4e00\u81f4\u6216\u6570\u5b66\u5197\u4f59\u8868\u8fbe\u5f0f", "method": "PiT-PO\u6846\u67b6\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06LLM\u6f14\u5316\u4e3a\u81ea\u9002\u5e94\u751f\u6210\u5668\uff0c\u91c7\u7528\u53cc\u91cd\u7ea6\u675f\u673a\u5236\u2014\u2014\u5f3a\u5236\u5c42\u6b21\u5316\u7269\u7406\u6709\u6548\u6027\uff0c\u540c\u65f6\u5e94\u7528\u7ec6\u7c92\u5ea6token\u7ea7\u60e9\u7f5a\u6291\u5236\u5197\u4f59\u7ed3\u6784", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6210\u529f\u4e3a\u6311\u6218\u6027\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u53d1\u73b0\u65b0\u7684\u6e4d\u6d41\u6a21\u578b\uff0c\u4f7f\u5c0f\u89c4\u6a21\u6a21\u578b\u8d85\u8d8a\u95ed\u6e90\u5927\u578b\u6a21\u578b", "conclusion": "PiT-PO\u80fd\u591f\u751f\u6210\u79d1\u5b66\u4e00\u81f4\u4e14\u7ed3\u6784\u7b80\u6d01\u7684\u65b9\u7a0b\uff0c\u6c11\u4e3b\u5316\u9ad8\u6027\u80fd\u79d1\u5b66\u53d1\u73b0\uff0c\u4f7f\u5c0f\u89c4\u6a21\u6a21\u578b\u4e5f\u80fd\u53d6\u5f97\u4f18\u5f02\u8868\u73b0", "topic": "agentic reinforcement learning"}}
{"id": "2602.10512", "categories": ["cs.LG", "cs.LO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.10512", "abs": "https://arxiv.org/abs/2602.10512", "authors": ["Sho Sonoda", "Shunta Akiyama", "Yuya Uezato"], "title": "Don't Eliminate Cut: Exponential Separations in LLM-Based Theorem Proving", "comment": null, "summary": "We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy $q$, including a latent-variable model in which proofs exhibit reusable cut/lemma/sketch structure represented by a proof DAG. Under a top-$k$ search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main separation result shows that when cut elimination expands a DAG of depth $D$ into a cut-free tree of size $\u03a9(\u039b^D)$ while the cut-aware hierarchical process has size $O(\u03bb^D)$ with $\u03bb\\ll\u039b$, a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.", "AI": {"tldr": "\u8bba\u6587\u4e3aLLM\u6307\u5bfc\u7684\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u7406\u8bba\u5206\u6790\uff0c\u5c06\u7b56\u7565\u63d0\u8bae\u5efa\u6a21\u4e3a\u6709\u9650\u65f6\u57df\u786e\u5b9a\u6027MDP\u4e2d\u7684\u968f\u673a\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u53c2\u8003\u7b56\u7565\u751f\u6210\u7684\u95ee\u9898\u5206\u5e03\u548c\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u89e3\u91ca\u7ecf\u9a8c\u6210\u529f\u4e0e\u6700\u574f\u60c5\u51b5\u96be\u5ea6\u7684\u5dee\u8ddd\uff0c\u4e3b\u8981\u5206\u79bb\u7ed3\u679c\u8868\u660e\u5206\u5c42\uff08cut-aware\uff09\u5b66\u4e60\u5668\u76f8\u6bd4\u6241\u5e73\uff08cut-free\uff09\u5b66\u4e60\u5668\u5177\u6709\u6307\u6570\u7ea7\u6570\u636e\u6548\u7387\u4f18\u52bf\u3002", "motivation": "\u89e3\u91caLLM\u6307\u5bfc\u7684\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\uff08\u5982Lean\u4e2d\uff09\u4e2d\u7ecf\u9a8c\u6210\u529f\u4e0e\u6700\u574f\u60c5\u51b5\u7406\u8bba\u96be\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6700\u8fd1\u57fa\u4e8e\u5b50\u76ee\u6807\u5206\u89e3\u7684\u667a\u80fd\u5b9a\u7406\u8bc1\u660e\u5668\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u5c06\u7b56\u7565\u63d0\u8bae\u5efa\u6a21\u4e3a\u6709\u9650\u65f6\u57df\u786e\u5b9a\u6027MDP\u4e2d\u7684\u968f\u673a\u7b56\u7565\uff0c\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e3a\u4e00\u822c\u7d27\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5047\u8bbeLipschitz\u7b56\u7565\u3002\u5f15\u5165\u7531\u53c2\u8003\u7b56\u7565q\u751f\u6210\u7684\u95ee\u9898\u5206\u5e03\uff0c\u5305\u62ec\u5177\u6709\u53ef\u91cd\u7528cut/lemma/sketch\u7ed3\u6784\u7684\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff08\u7528\u8bc1\u660eDAG\u8868\u793a\uff09\u3002\u5728top-k\u641c\u7d22\u534f\u8bae\u548cTsybakov\u578b\u8fb9\u754c\u6761\u4ef6\u4e0b\uff0c\u63a8\u5bfc\u6709\u9650\u65f6\u57df\u6210\u529f\u6982\u7387\u7684\u4e0b\u754c\u3002", "result": "\u4e3b\u8981\u5206\u79bb\u7ed3\u679c\u8868\u660e\uff1a\u5f53cut\u6d88\u9664\u5c06\u6df1\u5ea6\u4e3aD\u7684DAG\u6269\u5c55\u4e3a\u89c4\u6a21\u03a9(\u039b^D)\u7684\u65e0cut\u6811\uff0c\u800ccut-aware\u5206\u5c42\u8fc7\u7a0b\u89c4\u6a21\u4e3aO(\u03bb^D)\u4e14\u03bb\u226a\u039b\u65f6\uff0c\u6241\u5e73\uff08cut-free\uff09\u5b66\u4e60\u5668\u76f8\u6bd4\u5206\u5c42\uff08cut-aware\uff09\u5b66\u4e60\u5668\u9700\u8981\u6307\u6570\u7ea7\u66f4\u591a\u6570\u636e\u3002\u8fd9\u4e3a\u667a\u80fd\u5b9a\u7406\u8bc1\u660e\u5668\u4e2d\u7684\u5b50\u76ee\u6807\u5206\u89e3\u63d0\u4f9b\u4e86\u539f\u7406\u6027\u8bc1\u660e\u3002", "conclusion": "\u8bba\u6587\u4e3aLLM\u6307\u5bfc\u7684\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u4e86\u7ecf\u9a8c\u6210\u529f\u4e0e\u7406\u8bba\u96be\u5ea6\u7684\u5dee\u8ddd\uff0c\u5e76\u8bc1\u660e\u4e86\u5206\u5c42\u5b66\u4e60\u5728\u6570\u636e\u6548\u7387\u4e0a\u7684\u6307\u6570\u7ea7\u4f18\u52bf\uff0c\u4e3a\u667a\u80fd\u5b9a\u7406\u8bc1\u660e\u5668\u4e2d\u7684\u5b50\u76ee\u6807\u5206\u89e3\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2602.10693", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10693", "abs": "https://arxiv.org/abs/2602.10693", "authors": ["Guobin Shen", "Chenxiao Zhao", "Xiang Cheng", "Lei Huang", "Xing Yu"], "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "comment": null, "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "AI": {"tldr": "VESPO\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u53d8\u5206\u5e8f\u5217\u7ea7\u8f6f\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206\u516c\u5f0f\u63a8\u5bfc\u51fa\u95ed\u5f0f\u91cd\u5851\u6838\uff0c\u76f4\u63a5\u5904\u7406\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6743\u91cd\uff0c\u89e3\u51b3\u4e86\u7b56\u7565\u9648\u65e7\u548c\u5f02\u6b65\u8bad\u7ec3\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u7a33\u5b9a\u6027\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u7b56\u7565\u9648\u65e7\u3001\u5f02\u6b65\u8bad\u7ec3\u4ee5\u53ca\u8bad\u7ec3\u4e0e\u63a8\u7406\u5f15\u64ce\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u884c\u4e3a\u7b56\u7565\u4e0e\u5f53\u524d\u7b56\u7565\u504f\u79bb\uff0c\u5f15\u53d1\u8bad\u7ec3\u5d29\u6e83\u3002\u91cd\u8981\u6027\u91c7\u6837\u867d\u7136\u80fd\u7ea0\u6b63\u8fd9\u79cd\u5206\u5e03\u504f\u79fb\uff0c\u4f46\u5b58\u5728\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51faVESPO\u65b9\u6cd5\uff0c\u5c06\u65b9\u5dee\u51cf\u5c11\u7eb3\u5165\u53d8\u5206\u516c\u5f0f\u4e2d\uff0c\u63a8\u5bfc\u51fa\u95ed\u5f0f\u91cd\u5851\u6838\uff0c\u76f4\u63a5\u64cd\u4f5c\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6743\u91cd\uff0c\u65e0\u9700\u957f\u5ea6\u5f52\u4e00\u5316\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53d8\u5206\u4f18\u5316\u63d0\u6848\u5206\u5e03\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVESPO\u80fd\u5728\u9ad8\u8fbe64\u500d\u7684\u9648\u65e7\u6bd4\u7387\u548c\u5b8c\u5168\u5f02\u6b65\u6267\u884c\u4e0b\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\uff0c\u5728\u5bc6\u96c6\u6a21\u578b\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "VESPO\u4e3a\u5e8f\u5217\u7ea7\u91cd\u8981\u6027\u6743\u91cd\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u57fa\u7840\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u7b56\u7565\u9648\u65e7\u548c\u5f02\u6b65\u8bad\u7ec3\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u9ad8RL\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10539", "abs": "https://arxiv.org/abs/2602.10539", "authors": ["Guozheng Ma", "Lu Li", "Haoyu Wang", "Zixuan Liu", "Pierre-Luc Bacon", "Dacheng Tao"], "title": "What Makes Value Learning Efficient in Residual Reinforcement Learning?", "comment": null, "summary": "Residual reinforcement learning (RL) enables stable online refinement of expressive pretrained policies by freezing the base and learning only bounded corrections. However, value learning in residual RL poses unique challenges that remain poorly understood. In this work, we identify two key bottlenecks: cold start pathology, where the critic lacks knowledge of the value landscape around the base policy, and structural scale mismatch, where the residual contribution is dwarfed by the base action. Through systematic investigation, we uncover the mechanisms underlying these bottlenecks, revealing that simple yet principled solutions suffice: base-policy transitions serve as an essential value anchor for implicit warmup, and critic normalization effectively restores representation sensitivity for discerning value differences. Based on these insights, we propose DAWN (Data-Anchored Warmup and Normalization), a minimal approach targeting efficient value learning in residual RL. By addressing these bottlenecks, DAWN demonstrates substantial efficiency gains across diverse benchmarks, policy architectures, and observation modalities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86DAWN\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u951a\u5b9a\u9884\u70ed\u548c\u5f52\u4e00\u5316\u89e3\u51b3\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u5b66\u4e60\u7684\u4e24\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u7a33\u5b9a\u5728\u7ebf\u4f18\u5316\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4f46\u5176\u4ef7\u503c\u5b66\u4e60\u5b58\u5728\u72ec\u7279\u6311\u6218\uff0c\u7279\u522b\u662f\u51b7\u542f\u52a8\u95ee\u9898\u548c\u7ed3\u6784\u5c3a\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51faDAWN\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u4f7f\u7528\u57fa\u7840\u7b56\u7565\u8f6c\u6362\u4f5c\u4e3a\u4ef7\u503c\u951a\u70b9\u8fdb\u884c\u9690\u5f0f\u9884\u70ed\uff1b2) \u901a\u8fc7\u8bc4\u8bba\u5bb6\u5f52\u4e00\u5316\u6062\u590d\u8868\u793a\u654f\u611f\u6027\u4ee5\u8fa8\u522b\u4ef7\u503c\u5dee\u5f02\u3002", "result": "DAWN\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u3001\u7b56\u7565\u67b6\u6784\u548c\u89c2\u5bdf\u6a21\u6001\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ef7\u503c\u5b66\u4e60\u7684\u5173\u952e\u74f6\u9888\uff0c\u63d0\u51fa\u4e86\u7b80\u5355\u800c\u539f\u5219\u6027\u7684\u89e3\u51b3\u65b9\u6848DAWN\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7684\u4ef7\u503c\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10863", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10863", "abs": "https://arxiv.org/abs/2602.10863", "authors": ["Cong Pang", "Xuyu Feng", "Yujie Yi", "Zixuan Chen", "Jiawei Hong", "Tiankuo Yao", "Nang Yuan", "Jiapeng Luo", "Lewei Lu", "Xin Lou"], "title": "ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents", "comment": null, "summary": "Despite the strong performance achieved by reinforcement learning-trained information-seeking agents, learning in open-ended web environments remains severely constrained by low signal-to-noise feedback. Text-based parsers often discard layout semantics and introduce unstructured noise, while long-horizon training typically relies on sparse outcome rewards that obscure which retrieval actions actually matter. We propose a visual-native search framework that represents webpages as visual snapshots, allowing agents to leverage layout cues to quickly localize salient evidence and suppress distractors. To learn effectively from these high-dimensional observations, we introduce Information-Aware Credit Assignment (ICA), a post-hoc method that estimates each retrieved snapshot's contribution to the final outcome via posterior analysis and propagates dense learning signals back to key search turns. Integrated with a GRPO-based training pipeline, our approach consistently outperforms text-based baselines on diverse information-seeking benchmarks, providing evidence that visual snapshot grounding with information-level credit assignment alleviates the credit-assignment bottleneck in open-ended web environments. The code and datasets will be released in https://github.com/pc-inno/ICA_MM_deepsearch.git.", "AI": {"tldr": "\u63d0\u51fa\u89c6\u89c9\u539f\u751f\u641c\u7d22\u6846\u67b6\uff0c\u5c06\u7f51\u9875\u8868\u793a\u4e3a\u89c6\u89c9\u5feb\u7167\uff0c\u7ed3\u5408\u4fe1\u606f\u611f\u77e5\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u4fe1\u7528\u5206\u914d\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4fe1\u606f\u641c\u7d22\u4ee3\u7406\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u9762\u4e34\u4f4e\u4fe1\u566a\u6bd4\u53cd\u9988\u7684\u6311\u6218\uff1a\u6587\u672c\u89e3\u6790\u5668\u4e22\u5f03\u5e03\u5c40\u8bed\u4e49\u5e76\u5f15\u5165\u975e\u7ed3\u6784\u5316\u566a\u58f0\uff0c\u800c\u957f\u65f6\u7a0b\u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u7a00\u758f\u7ed3\u679c\u5956\u52b1\uff0c\u96be\u4ee5\u8bc6\u522b\u54ea\u4e9b\u68c0\u7d22\u52a8\u4f5c\u771f\u6b63\u91cd\u8981\u3002", "method": "1) \u89c6\u89c9\u539f\u751f\u641c\u7d22\u6846\u67b6\uff1a\u5c06\u7f51\u9875\u8868\u793a\u4e3a\u89c6\u89c9\u5feb\u7167\uff0c\u5229\u7528\u5e03\u5c40\u7ebf\u7d22\u5feb\u901f\u5b9a\u4f4d\u5173\u952e\u8bc1\u636e\u5e76\u6291\u5236\u5e72\u6270\uff1b2) \u4fe1\u606f\u611f\u77e5\u4fe1\u7528\u5206\u914d\uff1a\u901a\u8fc7\u540e\u9a8c\u5206\u6790\u4f30\u8ba1\u6bcf\u4e2a\u68c0\u7d22\u5feb\u7167\u5bf9\u6700\u7ec8\u7ed3\u679c\u7684\u8d21\u732e\uff0c\u5c06\u5bc6\u96c6\u5b66\u4e60\u4fe1\u53f7\u4f20\u64ad\u5230\u5173\u952e\u641c\u7d22\u6b65\u9aa4\uff1b3) \u96c6\u6210GRPO\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u6837\u5316\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u89c6\u89c9\u5feb\u7167\u57fa\u7840\u4e0e\u4fe1\u606f\u7ea7\u4fe1\u7528\u5206\u914d\u80fd\u591f\u7f13\u89e3\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u74f6\u9888\u3002", "conclusion": "\u89c6\u89c9\u539f\u751f\u8868\u793a\u7ed3\u5408\u4fe1\u606f\u7ea7\u4fe1\u7528\u5206\u914d\u662f\u89e3\u51b3\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4fe1\u7528\u5206\u914d\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.11018", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.11018", "abs": "https://arxiv.org/abs/2602.11018", "authors": ["Returaj Burnwal", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories", "comment": "21 pages, Accepted at AAMAS 2026", "summary": "This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajectories that reflect undesirable or unsafe behavior, implicitly conveying what the agent should avoid. We refer to these as non-preferred trajectories. We propose a novel offline safe IL algorithm, OSIL, that infers safety from non-preferred demonstrations. We formulate safe policy learning as a Constrained Markov Decision Process (CMDP). Instead of relying on explicit safety cost and reward annotations, OSIL reformulates the CMDP problem by deriving a lower bound on reward maximizing objective and learning a cost model that estimates the likelihood of non-preferred behavior. Our approach allows agents to learn safe and reward-maximizing behavior entirely from offline demonstrations. We empirically demonstrate that our approach can learn safer policies that satisfy cost constraints without degrading the reward performance, thus outperforming several baselines.", "AI": {"tldr": "\u63d0\u51faOSIL\u7b97\u6cd5\uff0c\u4ece\u79bb\u7ebf\u975e\u504f\u597d\u6f14\u793a\u4e2d\u5b66\u4e60\u5b89\u5168\u7b56\u7565\uff0c\u65e0\u9700\u663e\u5f0f\u5b89\u5168\u6210\u672c\u6216\u5956\u52b1\u6807\u6ce8", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u5728\u7ebf\u5b66\u4e60\u53ef\u80fd\u98ce\u9669\u9ad8\uff0c\u51c6\u786e\u6307\u5b9a\u5b89\u5168\u6210\u672c\u56f0\u96be\uff0c\u4f46\u6536\u96c6\u53cd\u6620\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u8f68\u8ff9\u76f8\u5bf9\u5bb9\u6613", "method": "\u5c06\u5b89\u5168\u7b56\u7565\u5b66\u4e60\u5efa\u6a21\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ece\u975e\u504f\u597d\u6f14\u793a\u4e2d\u63a8\u5bfc\u5956\u52b1\u6700\u5927\u5316\u76ee\u6807\u7684\u4e0b\u754c\u5e76\u5b66\u4e60\u6210\u672c\u6a21\u578b", "result": "OSIL\u80fd\u591f\u5b66\u4e60\u6ee1\u8db3\u6210\u672c\u7ea6\u675f\u7684\u5b89\u5168\u7b56\u7565\uff0c\u4e14\u4e0d\u964d\u4f4e\u5956\u52b1\u6027\u80fd\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u79bb\u7ebf\u5b89\u5168\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u4ece\u975e\u504f\u597d\u6f14\u793a\u4e2d\u6709\u6548\u63a8\u65ad\u5b89\u5168\u6027\uff0c\u5b9e\u73b0\u5b89\u5168\u4e14\u5956\u52b1\u6700\u5927\u5316\u7684\u884c\u4e3a\u5b66\u4e60", "topic": "agentic reinforcement learning"}}
{"id": "2602.11087", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11087", "abs": "https://arxiv.org/abs/2602.11087", "authors": ["Jianxun Wang", "Grant C. Forbes", "Leonardo Villalobos-Arias", "David L. Roberts"], "title": "General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies", "comment": "Extended version of the full paper with the appendix accepted at AAMAS 2026", "summary": "Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the offline RL algorithm's ability to estimate \\textit{Q} or \\textit{V} values, while constraining towards diverse behavior policies can be overly conservative. Such datasets call for a balance between the RL objective and behavior policy constraints. We first identify the connection between $f$-divergence and optimization constraint on the Bellman residual through a more general Linear Programming form for RL and the convex conjugate. Following this, we introduce the general flexible function formulation for the $f$-divergence to incorporate an adaptive constraint on algorithms' learning objectives based on the offline training dataset. Results from experiments on the MuJoCo, Fetch, and AdroitHand environments show the correctness of the proposed LP form and the potential of the flexible $f$-divergence in improving performance for learning from a challenging dataset when applied to a compatible constrained optimization algorithm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ef-\u6563\u5ea6\u7684\u81ea\u9002\u5e94\u7ea6\u675f\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u5f62\u5f0f\u548c\u51f8\u5171\u8f6d\u7406\u8bba\u8fde\u63a5f-\u6563\u5ea6\u4e0e\u8d1d\u5c14\u66fc\u6b8b\u5dee\u7ea6\u675f\uff0c\u5728\u591a\u6837\u4f46\u63a2\u7d22\u6709\u9650\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0RL\u76ee\u6807\u4e0e\u884c\u4e3a\u7b56\u7565\u7ea6\u675f\u7684\u5e73\u8861\u3002", "motivation": "\u79bb\u7ebfRL\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u3001\u73af\u5883\u63a2\u7d22\u6709\u9650\u3001\u4e14\u6765\u81ea\u4e0d\u540c\u4e13\u4e1a\u6c34\u5e73\u7684\u884c\u4e3a\u7b56\u7565\u7b49\u95ee\u9898\u3002\u6709\u9650\u7684\u63a2\u7d22\u4f1a\u5f71\u54cdQ/V\u503c\u4f30\u8ba1\uff0c\u800c\u5411\u591a\u6837\u884c\u4e3a\u7b56\u7565\u7684\u7ea6\u675f\u53ef\u80fd\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u9700\u8981\u5728RL\u76ee\u6807\u4e0e\u884c\u4e3a\u7b56\u7565\u7ea6\u675f\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u9996\u5148\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u5f62\u5f0f\u548c\u51f8\u5171\u8f6d\u7406\u8bba\u5efa\u7acbf-\u6563\u5ea6\u4e0e\u8d1d\u5c14\u66fc\u6b8b\u5dee\u4f18\u5316\u7ea6\u675f\u7684\u8fde\u63a5\uff0c\u7136\u540e\u5f15\u5165\u7075\u6d3b\u7684f-\u6563\u5ea6\u51fd\u6570\u516c\u5f0f\uff0c\u57fa\u4e8e\u79bb\u7ebf\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u7b97\u6cd5\u5b66\u4e60\u76ee\u6807\u5b9e\u65bd\u81ea\u9002\u5e94\u7ea6\u675f\u3002", "result": "\u5728MuJoCo\u3001Fetch\u548cAdroitHand\u73af\u5883\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7ebf\u6027\u89c4\u5212\u5f62\u5f0f\u6b63\u786e\uff0c\u4e14\u7075\u6d3b\u7684f-\u6563\u5ea6\u5728\u5e94\u7528\u4e8e\u517c\u5bb9\u7684\u7ea6\u675f\u4f18\u5316\u7b97\u6cd5\u65f6\uff0c\u80fd\u591f\u4ece\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e2d\u63d0\u5347\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7f-\u6563\u5ea6\u7684\u81ea\u9002\u5e94\u7ea6\u675f\u673a\u5236\uff0c\u80fd\u591f\u5728\u79bb\u7ebfRL\u4e2d\u66f4\u597d\u5730\u5e73\u8861\u4f18\u5316\u76ee\u6807\u4e0e\u6570\u636e\u652f\u6301\u7ea6\u675f\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6837\u4f46\u63a2\u7d22\u6709\u9650\u7684\u6570\u636e\u96c6\u65f6\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10793", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.10793", "abs": "https://arxiv.org/abs/2602.10793", "authors": ["Li-Min Chu", "Kai-Siang Ma", "Ming-Hong Chen", "Ping-Chun Hsieh"], "title": "Semi-Supervised Cross-Domain Imitation Learning", "comment": "Published in Transactions on Machine Learning Research (TMLR)", "summary": "Cross-domain imitation learning (CDIL) accelerates policy learning by transferring expert knowledge across domains, which is valuable in applications where the collection of expert data is costly. Existing methods are either supervised, relying on proxy tasks and explicit alignment, or unsupervised, aligning distributions without paired data, but often unstable. We introduce the Semi-Supervised CDIL (SS-CDIL) setting and propose the first algorithm for SS-CDIL with theoretical justification. Our method uses only offline data, including a small number of target expert demonstrations and some unlabeled imperfect trajectories. To handle domain discrepancy, we propose a novel cross-domain loss function for learning inter-domain state-action mappings and design an adaptive weight function to balance the source and target knowledge. Experiments on MuJoCo and Robosuite show consistent gains over the baselines, demonstrating that our approach achieves stable and data-efficient policy learning with minimal supervision. Our code is available at~ https://github.com/NYCU-RL-Bandits-Lab/CDIL.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u534a\u76d1\u7763\u8de8\u57df\u6a21\u4eff\u5b66\u4e60\uff08SS-CDIL\uff09\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u76ee\u6807\u4e13\u5bb6\u6f14\u793a\u548c\u672a\u6807\u8bb0\u7684\u4e0d\u5b8c\u7f8e\u8f68\u8ff9\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8de8\u57df\u635f\u5931\u51fd\u6570\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5e73\u8861\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u77e5\u8bc6\uff0c\u5728MuJoCo\u548cRobosuite\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u4e14\u6570\u636e\u9ad8\u6548\u7684\u653f\u7b56\u5b66\u4e60\u3002", "motivation": "\u8de8\u57df\u6a21\u4eff\u5b66\u4e60\uff08CDIL\uff09\u901a\u8fc7\u8de8\u57df\u8fc1\u79fb\u4e13\u5bb6\u77e5\u8bc6\u52a0\u901f\u653f\u7b56\u5b66\u4e60\uff0c\u5728\u4e13\u5bb6\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u5e94\u7528\u4e2d\u5f88\u6709\u4ef7\u503c\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u662f\u76d1\u7763\u5f0f\uff08\u4f9d\u8d56\u4ee3\u7406\u4efb\u52a1\u548c\u663e\u5f0f\u5bf9\u9f50\uff09\uff0c\u8981\u4e48\u662f\u65e0\u76d1\u7763\u5f0f\uff08\u65e0\u9700\u914d\u5bf9\u6570\u636e\u5bf9\u9f50\u5206\u5e03\uff09\uff0c\u4f46\u5f80\u5f80\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faSS-CDIL\u8bbe\u7f6e\u53ca\u9996\u4e2a\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u79bb\u7ebf\u6570\u636e\uff08\u5c11\u91cf\u76ee\u6807\u4e13\u5bb6\u6f14\u793a\u548c\u672a\u6807\u8bb0\u7684\u4e0d\u5b8c\u7f8e\u8f68\u8ff9\uff09\u3002\u4e3a\u5904\u7406\u57df\u5dee\u5f02\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u8de8\u57df\u635f\u5931\u51fd\u6570\u5b66\u4e60\u57df\u95f4\u72b6\u6001-\u52a8\u4f5c\u6620\u5c04\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u6743\u91cd\u51fd\u6570\u5e73\u8861\u6e90\u57df\u548c\u76ee\u6807\u57df\u77e5\u8bc6\u3002", "result": "\u5728MuJoCo\u548cRobosuite\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u4ee5\u6700\u5c0f\u76d1\u7763\u5b9e\u73b0\u7a33\u5b9a\u4e14\u6570\u636e\u9ad8\u6548\u7684\u653f\u7b56\u5b66\u4e60\u3002", "conclusion": "\u63d0\u51fa\u7684SS-CDIL\u7b97\u6cd5\u662f\u9996\u4e2a\u534a\u76d1\u7763\u8de8\u57df\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u4ec5\u9700\u5c11\u91cf\u76ee\u6807\u4e13\u5bb6\u6f14\u793a\u548c\u672a\u6807\u8bb0\u8f68\u8ff9\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u6709\u6548\u5904\u7406\u57df\u5dee\u5f02\u5e76\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u653f\u7b56\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10819", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10819", "abs": "https://arxiv.org/abs/2602.10819", "authors": ["Linxuan Xia", "Xiaolong Yang", "Yongyuan Chen", "Enyue Zhao", "Deng Cai", "Yasheng Wang", "Boxi Wu"], "title": "RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization", "comment": null, "summary": "Aligning large language models (LLMs) on domain-specific data remains a fundamental challenge. Supervised fine-tuning (SFT) offers a straightforward way to inject domain knowledge but often degrades the model's generality. In contrast, on-policy reinforcement learning (RL) preserves generality but fails to effectively assimilate hard samples that exceed the model's current reasoning level. Recent off-policy RL attempts improve hard sample utilization, yet they suffer from severe training instability due to the forced distribution shift toward off-policy knowledge. To reconcile effective off-policy knowledge absorption with the stability of on-policy RL, we propose Rephrasing Policy Optimization (RePO). In RePO, the policy model is prompted to first comprehend off-policy knowledge and then rephrase it into trajectories that conform to its own stylistic and parametric distribution. RePO dynamically replaces low-reward rollouts with these rephrased, high-quality trajectories. This strategy guides the model toward correct reasoning paths while strictly preserving on-policy training dynamics. Experiments on several benchmarks demonstrate that RePO improves hard-sample utilization and outperforms existing baselines, achieving state-of-the-art performance.", "AI": {"tldr": "RePO\u901a\u8fc7\u8ba9\u7b56\u7565\u6a21\u578b\u91cd\u65b0\u8868\u8ff0\u79bb\u7b56\u7565\u77e5\u8bc6\u4e3a\u7b26\u5408\u81ea\u8eab\u5206\u5e03\u7684\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86LLM\u9886\u57df\u5bf9\u9f50\u4e2d\u79bb\u7b56\u7565RL\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u786c\u6837\u672c\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "LLM\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u7684\u5bf9\u9f50\u9762\u4e34\u6311\u6218\uff1aSFT\u4f1a\u635f\u5bb3\u6a21\u578b\u901a\u7528\u6027\uff0con-policy RL\u96be\u4ee5\u6709\u6548\u5438\u6536\u8d85\u51fa\u5f53\u524d\u63a8\u7406\u6c34\u5e73\u7684\u786c\u6837\u672c\uff0c\u800coff-policy RL\u53c8\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faRephrasing Policy Optimization (RePO)\uff1a\u8ba9\u7b56\u7565\u6a21\u578b\u5148\u7406\u89e3\u79bb\u7b56\u7565\u77e5\u8bc6\uff0c\u7136\u540e\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u7b26\u5408\u81ea\u8eab\u98ce\u683c\u548c\u53c2\u6570\u5206\u5e03\u7684\u8f68\u8ff9\uff0c\u7528\u8fd9\u4e9b\u9ad8\u8d28\u91cf\u8f68\u8ff9\u52a8\u6001\u66ff\u6362\u4f4e\u5956\u52b1\u7684rollouts\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRePO\u63d0\u9ad8\u4e86\u786c\u6837\u672c\u5229\u7528\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RePO\u6210\u529f\u8c03\u548c\u4e86\u6709\u6548\u5438\u6536\u79bb\u7b56\u7565\u77e5\u8bc6\u4e0e\u4fdd\u6301on-policy RL\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3aLLM\u9886\u57df\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.10986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10986", "abs": "https://arxiv.org/abs/2602.10986", "authors": ["Abhishek Vijaya Kumar", "Bhaskar Kataria", "Byungsoo Oh", "Emaad Manzoor", "Rachee Singh"], "title": "TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents", "comment": "Abhishek Vijaya Kumar and Bhaskar Kataria have equal contribution", "summary": "In RL post-training of LLM agents, calls to external tools take several seconds or even minutes, leaving allocated GPUs idle and inflating post-training time and cost. While many tool invocations repeat across parallel rollouts and could in principle be cached, naively caching their outputs for reuse is incorrect since tool outputs depend on the environment state induced by prior agent interactions. We present TVCACHE, a stateful tool-value cache for LLM agent post-training. TVCACHE maintains a tree of observed tool-call sequences and performs longest-prefix matching for cache lookups: a hit occurs only when the agent's full tool history matches a previously executed sequence, guaranteeing identical environment state. On three diverse workloads-terminal-based tasks, SQL generation, and video understanding. TVCACHE achieves cache hit rates of up to 70% and reduces median tool call execution time by up to 6.9X, with no degradation in post-training reward accumulation.", "AI": {"tldr": "TVCACHE\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u7684\u72b6\u6001\u611f\u77e5\u5de5\u5177\u503c\u7f13\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u6700\u957f\u524d\u7f00\u5339\u914d\u786e\u4fdd\u7f13\u5b58\u4e00\u81f4\u6027\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u8fbe70%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u6267\u884c\u65f6\u95f4\u4e2d\u4f4d\u6570\u964d\u4f4e6.9\u500d\u3002", "motivation": "\u5728LLM\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u4e2d\uff0c\u5916\u90e8\u5de5\u5177\u8c03\u7528\u901a\u5e38\u9700\u8981\u6570\u79d2\u751a\u81f3\u6570\u5206\u949f\uff0c\u5bfc\u81f4GPU\u7a7a\u95f2\u65f6\u95f4\u589e\u52a0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u8bad\u7ec3\u65f6\u95f4\u548c\u6210\u672c\u3002\u867d\u7136\u8bb8\u591a\u5de5\u5177\u8c03\u7528\u5728\u5e76\u884crollout\u4e2d\u91cd\u590d\u51fa\u73b0\uff0c\u4f46\u7b80\u5355\u7f13\u5b58\u8f93\u51fa\u662f\u4e0d\u6b63\u786e\u7684\uff0c\u56e0\u4e3a\u5de5\u5177\u8f93\u51fa\u4f9d\u8d56\u4e8e\u5148\u524d\u667a\u80fd\u4f53\u4ea4\u4e92\u8bf1\u5bfc\u7684\u73af\u5883\u72b6\u6001\u3002", "method": "TVCACHE\u7ef4\u62a4\u89c2\u5bdf\u5230\u7684\u5de5\u5177\u8c03\u7528\u5e8f\u5217\u6811\uff0c\u5e76\u6267\u884c\u6700\u957f\u524d\u7f00\u5339\u914d\u8fdb\u884c\u7f13\u5b58\u67e5\u627e\uff1a\u53ea\u6709\u5f53\u667a\u80fd\u4f53\u7684\u5b8c\u6574\u5de5\u5177\u5386\u53f2\u4e0e\u5148\u524d\u6267\u884c\u7684\u5e8f\u5217\u5b8c\u5168\u5339\u914d\u65f6\u624d\u4f1a\u53d1\u751f\u7f13\u5b58\u547d\u4e2d\uff0c\u8fd9\u4fdd\u8bc1\u4e86\u76f8\u540c\u7684\u73af\u5883\u72b6\u6001\u3002\u7cfb\u7edf\u5728\u4e09\u79cd\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1a\u7ec8\u7aef\u4efb\u52a1\u3001SQL\u751f\u6210\u548c\u89c6\u9891\u7406\u89e3\u3002", "result": "TVCACHE\u5b9e\u73b0\u4e86\u9ad8\u8fbe70%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u6267\u884c\u65f6\u95f4\u4e2d\u4f4d\u6570\u964d\u4f4e\u4e866.9\u500d\uff0c\u4e14\u5728\u540e\u8bad\u7ec3\u5956\u52b1\u79ef\u7d2f\u65b9\u9762\u6ca1\u6709\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "TVCACHE\u901a\u8fc7\u72b6\u6001\u611f\u77e5\u7684\u7f13\u5b58\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u4e2d\u5de5\u5177\u8c03\u7528\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u8bad\u7ec3\u8d28\u91cf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.11128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11128", "abs": "https://arxiv.org/abs/2602.11128", "authors": ["Reinhard Heckel", "Mahdi Soltanolkotabi", "Christos Thramboulidis"], "title": "Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u975e\u5bf9\u79f0\u63d0\u793a\u52a0\u6743\u65b9\u6cd5\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e3a\u4f4e\u6210\u529f\u6982\u7387\u7684\u63d0\u793a\u5206\u914d\u66f4\u9ad8\u6743\u91cd\uff0c\u52a0\u901f\u4ece\u96f6\u5f00\u59cb\u7684RL\u8bad\u7ec3\u6536\u655b\u3002", "motivation": "\u73b0\u6709RL\u7b97\u6cd5\uff08\u5982GRPO\u3001DAPO\u3001RLOO\uff09\u4e3b\u8981\u5173\u6ce8\u4e2d\u7b49\u6210\u529f\u6982\u7387\u7684\u6a21\u7cca\u63d0\u793a\uff0c\u800c\u5bf9\u975e\u5e38\u5bb9\u6613\u6216\u975e\u5e38\u56f0\u96be\u7684\u63d0\u793a\u964d\u6743\u3002\u7136\u800c\u5728\u4ece\u96f6\u5f00\u59cb\u7684RL\u8bad\u7ec3\u4e2d\uff0c\u4f4e\u6210\u529f\u6982\u7387\u533a\u57df\u9700\u8981\u66f4\u6709\u6548\u7684\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u975e\u5bf9\u79f0\u63d0\u793a\u52a0\u6743\u65b9\u6cd5\uff0c\u4e3a\u4f4e\uff08\u751a\u81f3\u96f6\uff09\u7ecf\u9a8c\u6210\u529f\u6982\u7387\u7684\u63d0\u793a\u5206\u914d\u66f4\u9ad8\u6743\u91cd\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u786e\u5b9a\u5728\u56fa\u5b9a\u66f4\u65b0\u9884\u7b97\u4e0b\u6700\u5c0f\u5316\u4ece\u521d\u59cb\u6210\u529f\u7387\u5230\u76ee\u6807\u51c6\u786e\u7387\u6240\u9700\u65f6\u95f4\u7684\u6700\u4f18\u6743\u91cd\u3002", "result": "\u975e\u5bf9\u79f0\u52a0\u6743\u7279\u522b\u6709\u5229\u4e8e\u4ece\u96f6\u5f00\u59cb\u7684RL\u8bad\u7ec3\uff08\u5982R1-Zero\uff09\uff0c\u80fd\u663e\u8457\u52a0\u901f\u6536\u655b\uff1b\u800c\u5728SFT\u540eRL\u4e2d\u6548\u679c\u8f83\u5c0f\uff0c\u56e0\u4e3a\u6a21\u578b\u5df2\u5177\u5907\u8f83\u9ad8\u51c6\u786e\u7387\u3002\u7406\u8bba\u8bc1\u660e\u5728\u4f4e\u6210\u529f\u533a\u57df\uff0c\u6700\u4f18\u6743\u91cd\u786e\u5b9e\u662f\u975e\u5bf9\u79f0\u7684\u3002", "conclusion": "\u975e\u5bf9\u79f0\u63d0\u793a\u52a0\u6743\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4ece\u96f6\u5f00\u59cb\u7684\u8bad\u7ec3\u573a\u666f\uff0c\u901a\u8fc7\u63d0\u5347\u4f4e\u6210\u529f\u6982\u7387\u63d0\u793a\u7684\u6743\u91cd\u6765\u52a0\u901f\u6709\u6548\u65f6\u95f4\u6536\u655b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.b5ff89f2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bicameral-ai.com%2Fblog%2Ftech-debt-meeting%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ghJLKyBZokmC9AfW2U5UoWIKgeyDqPJf9NPGBJthItw=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bicameral-ai.com%2Fblog%2Ftech-debt-meeting%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ghJLKyBZokmC9AfW2U5UoWIKgeyDqPJf9NPGBJthItw=444", "authors": ["TLDR Newsletter"], "title": "Why \"just prompt better\" doesn't work", "comment": "Source: TLDR Newsletter, Date: 2026-02-11, Reading time: 9 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bicameral-ai.com%2Fblog%2Ftech-debt-meeting%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ghJLKyBZokmC9AfW2U5UoWIKgeyDqPJf9NPGBJthItw=444", "summary": "Why \"just prompt better\" doesn't work (9 minute read) AI coding assistants often increase development time due to their inability to facilitate constraint discovery, rather than solving core problems. A primary issue is communication friction, where technical constraints are discovered late or poorly communicated to non-technical stakeholders. AI tools make this worse by uncritically generating code, bypassing the human implementation phase where developers naturally uncover implicit constrai...", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u52a9\u624b\u5e38\u56e0\u65e0\u6cd5\u4fc3\u8fdb\u7ea6\u675f\u53d1\u73b0\u800c\u589e\u52a0\u5f00\u53d1\u65f6\u95f4\uff0c\u800c\u975e\u89e3\u51b3\u6838\u5fc3\u95ee\u9898\u3002\u4e3b\u8981\u95ee\u9898\u662f\u6c9f\u901a\u6469\u64e6\uff0c\u6280\u672f\u7ea6\u675f\u53d1\u73b0\u8f83\u665a\u6216\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u6c9f\u901a\u4e0d\u7545\u3002AI\u5de5\u5177\u901a\u8fc7\u4e0d\u52a0\u6279\u5224\u5730\u751f\u6210\u4ee3\u7801\u4f7f\u60c5\u51b5\u6076\u5316\uff0c\u7ed5\u8fc7\u4e86\u5f00\u53d1\u8005\u81ea\u7136\u53d1\u73b0\u9690\u542b\u7ea6\u675f\u7684\u4eba\u7c7b\u5b9e\u73b0\u9636\u6bb5\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4e3a\u4ec0\u4e48\"\u53ea\u662f\u66f4\u597d\u5730\u63d0\u793a\"\u65e0\u6cd5\u89e3\u51b3AI\u7f16\u7a0b\u52a9\u624b\u7684\u6839\u672c\u95ee\u9898\u3002\u5f53\u524dAI\u7f16\u7801\u5de5\u5177\u867d\u7136\u80fd\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u7ecf\u5e38\u589e\u52a0\u5f00\u53d1\u65f6\u95f4\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u4fc3\u8fdb\u7ea6\u675f\u53d1\u73b0\u8fc7\u7a0b\uff0c\u800c\u8fd9\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u73af\u8282\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u7f16\u7a0b\u52a9\u624b\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u8bc6\u522b\u51fa\u6c9f\u901a\u6469\u64e6\u548c\u6280\u672f\u7ea6\u675f\u53d1\u73b0\u5ef6\u8fdf\u7684\u95ee\u9898\u3002\u7814\u7a76AI\u5de5\u5177\u5982\u4f55\u7ed5\u8fc7\u4eba\u7c7b\u5b9e\u73b0\u9636\u6bb5\uff0c\u5bfc\u81f4\u9690\u542b\u7ea6\u675f\u672a\u88ab\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u7f16\u7801\u52a9\u624b\u5b9e\u9645\u4e0a\u53ef\u80fd\u589e\u52a0\u5f00\u53d1\u65f6\u95f4\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u4fc3\u8fdb\u7ea6\u675f\u53d1\u73b0\u8fc7\u7a0b\u3002AI\u5de5\u5177\u4e0d\u52a0\u6279\u5224\u5730\u751f\u6210\u4ee3\u7801\uff0c\u7ed5\u8fc7\u4e86\u5f00\u53d1\u8005\u81ea\u7136\u53d1\u73b0\u9690\u542b\u7ea6\u675f\u7684\u5173\u952e\u9636\u6bb5\uff0c\u5bfc\u81f4\u6280\u672f\u7ea6\u675f\u53d1\u73b0\u8f83\u665a\u6216\u6c9f\u901a\u4e0d\u7545\u3002", "conclusion": "\u4ec5\u4ec5\u6539\u8fdb\u63d0\u793a\u7b56\u7565\u65e0\u6cd5\u89e3\u51b3AI\u7f16\u7a0b\u52a9\u624b\u7684\u6839\u672c\u95ee\u9898\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003AI\u5de5\u5177\u7684\u8bbe\u8ba1\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u7ea6\u675f\u53d1\u73b0\u8fc7\u7a0b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\u751f\u6210\u3002", "topic": "code agent"}}
{"id": "tldr.2602.52f63257", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F10%2Fshowboat-and-rodney%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ipwsGgq-5WM-lNTpLbZZ3OUKQvArVK5cpmT-pgqY378=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F10%2Fshowboat-and-rodney%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ipwsGgq-5WM-lNTpLbZZ3OUKQvArVK5cpmT-pgqY378=444", "authors": ["TLDR Newsletter"], "title": "Introducing Showboat and Rodney, so agents can demo what they've built", "comment": "Source: TLDR Newsletter, Date: 2026-02-11, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F10%2Fshowboat-and-rodney%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/ipwsGgq-5WM-lNTpLbZZ3OUKQvArVK5cpmT-pgqY378=444", "summary": "Introducing Showboat and Rodney, so agents can demo what they've built (11 minute read) Showboat and Rodney are two new tools designed to help AI coding agents test and demonstrate the software they build, moving beyond automated tests to verifiable artifacts. Showboat is a CLI tool that enables agents to construct Markdown documents, embedding command executions, their outputs, and images to visually prove code functionality. Complementing this, Rodney provides CLI browser automation, allowi...", "source": "tldr", "AI": {"tldr": "Showboat\u548cRodney\u662f\u4e24\u4e2a\u5e2e\u52a9AI\u7f16\u7801\u4ee3\u7406\u6d4b\u8bd5\u548c\u5c55\u793a\u6240\u6784\u5efa\u8f6f\u4ef6\u7684\u65b0\u5de5\u5177\uff0c\u8d85\u8d8a\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u521b\u5efa\u53ef\u9a8c\u8bc1\u7684\u5de5\u4ef6", "motivation": "\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u7f3a\u4e4f\u6709\u6548\u5c55\u793a\u548c\u9a8c\u8bc1\u5176\u6784\u5efa\u8f6f\u4ef6\u529f\u80fd\u7684\u65b9\u5f0f\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u81ea\u52a8\u5316\u6d4b\u8bd5\u7684\u5de5\u5177\u6765\u521b\u5efa\u53ef\u9a8c\u8bc1\u7684\u6f14\u793a\u5de5\u4ef6", "method": "Showboat\u662fCLI\u5de5\u5177\uff0c\u7528\u4e8e\u6784\u5efa\u5305\u542b\u547d\u4ee4\u6267\u884c\u3001\u8f93\u51fa\u548c\u56fe\u50cf\u7684Markdown\u6587\u6863\uff1bRodney\u63d0\u4f9bCLI\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\uff0c\u652f\u6301\u89c6\u89c9\u9a8c\u8bc1", "result": "\u5f00\u53d1\u4e86\u4e24\u4e2a\u4e92\u8865\u5de5\u5177\uff1aShowboat\u521b\u5efa\u53ef\u9a8c\u8bc1\u7684\u6f14\u793a\u6587\u6863\uff0cRodney\u652f\u6301\u6d4f\u89c8\u5668\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u5171\u540c\u5e2e\u52a9AI\u4ee3\u7406\u5c55\u793a\u8f6f\u4ef6\u529f\u80fd", "conclusion": "Showboat\u548cRodney\u4e3aAI\u7f16\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u6d4b\u8bd5\u548c\u6f14\u793a\u80fd\u529b\uff0c\u901a\u8fc7\u521b\u5efa\u53ef\u9a8c\u8bc1\u7684\u89c6\u89c9\u5de5\u4ef6\uff0c\u589e\u5f3a\u4e86\u8f6f\u4ef6\u529f\u80fd\u7684\u8bc1\u660e\u548c\u5c55\u793a", "topic": "code agent"}}
{"id": "tldr.2602.9c134538", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ftambo-ai%2Ftambo%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/fCEr5MNE0bscdYpr6FxTOvsNCA5-9kilJjwnVgVpg_U=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ftambo-ai%2Ftambo%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/fCEr5MNE0bscdYpr6FxTOvsNCA5-9kilJjwnVgVpg_U=444", "authors": ["TLDR Newsletter"], "title": "Tambo", "comment": "Source: TLDR Newsletter, Date: 2026-02-11, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Ftambo-ai%2Ftambo%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/fCEr5MNE0bscdYpr6FxTOvsNCA5-9kilJjwnVgVpg_U=444", "summary": "Tambo (GitHub Repo) Tambo AI is an open-source React toolkit for building AI agents capable of generating and interacting with dynamic user interfaces. Developers register their React components with Zod schemas, allowing an LLM agent to select and stream props to render UI elements based on user input, like displaying charts or updating task boards.", "source": "tldr", "AI": {"tldr": "Tambo AI\u662f\u4e00\u4e2a\u5f00\u6e90\u7684React\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u751f\u6210\u548c\u4ea4\u4e92\u52a8\u6001\u7528\u6237\u754c\u9762\u7684AI\u4ee3\u7406", "motivation": "\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u8f7b\u677e\u521b\u5efa\u80fd\u591f\u52a8\u6001\u751f\u6210\u548c\u4ea4\u4e92UI\u7684AI\u4ee3\u7406\uff0c\u901a\u8fc7\u6ce8\u518cReact\u7ec4\u4ef6\u548cZod\u6a21\u5f0f\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u6839\u636e\u7528\u6237\u8f93\u5165\u9009\u62e9\u548c\u6e32\u67d3UI\u5143\u7d20", "method": "\u5f00\u53d1\u8005\u4f7f\u7528Zod\u6a21\u5f0f\u6ce8\u518cReact\u7ec4\u4ef6\uff0cLLM\u4ee3\u7406\u6839\u636e\u7528\u6237\u8f93\u5165\u9009\u62e9\u548c\u6d41\u5f0f\u4f20\u8f93props\u6765\u6e32\u67d3UI\u5143\u7d20\uff0c\u652f\u6301\u52a8\u6001\u754c\u9762\u751f\u6210\u548c\u4ea4\u4e92", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5f00\u6e90React\u5de5\u5177\u5305\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u591f\u751f\u6210\u548c\u4ea4\u4e92\u52a8\u6001UI\uff0c\u5982\u663e\u793a\u56fe\u8868\u6216\u66f4\u65b0\u4efb\u52a1\u677f", "conclusion": "Tambo AI\u4e3a\u6784\u5efa\u80fd\u591f\u52a8\u6001\u751f\u6210\u548c\u4ea4\u4e92\u7528\u6237\u754c\u9762\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2602.0c6664ec", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fentire.io%2Fblog%2Fhello-entire-world%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/00RfVg3U-AMQpj0zAWrEzzcl41l6KEZmgXT9XTmms9U=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fentire.io%2Fblog%2Fhello-entire-world%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/00RfVg3U-AMQpj0zAWrEzzcl41l6KEZmgXT9XTmms9U=444", "authors": ["TLDR Newsletter"], "title": "Hello Entire World", "comment": "Source: TLDR Newsletter, Date: 2026-02-11, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fentire.io%2Fblog%2Fhello-entire-world%2F%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/00RfVg3U-AMQpj0zAWrEzzcl41l6KEZmgXT9XTmms9U=444", "summary": "Hello Entire World (6 minute read) Entire is a new company created by GitHub's ex-CEO Thomas Dohmke with the mission to build the world's next developer platform. Its vision is to create an open and scalable platform based on a git-compatible database, a universal semantic reasoning layer, and an AI-native SDLC for human-agent collaboration. Its first open-source product, the Entire CLI, introduces \"Checkpoints\" to automatically capture and version agent context, improving AI agent traceabili...", "source": "tldr", "AI": {"tldr": "Entire\u516c\u53f8\u63a8\u51fa\u57fa\u4e8egit\u517c\u5bb9\u6570\u636e\u5e93\u3001\u901a\u7528\u8bed\u4e49\u63a8\u7406\u5c42\u548cAI\u539f\u751fSDLC\u7684\u4e0b\u4e00\u4ee3\u5f00\u53d1\u8005\u5e73\u53f0\uff0c\u5176\u9996\u4e2a\u5f00\u6e90\u4ea7\u54c1Entire CLI\u5f15\u5165\"\u68c0\u67e5\u70b9\"\u529f\u80fd\u81ea\u52a8\u6355\u83b7\u548c\u7248\u672c\u5316\u4ee3\u7406\u4e0a\u4e0b\u6587\uff0c\u63d0\u5347AI\u4ee3\u7406\u53ef\u8ffd\u6eaf\u6027", "motivation": "\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u53d1\u8005\u5e73\u53f0\uff0c\u89e3\u51b3\u5f53\u524dAI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7f3a\u4e4f\u53ef\u8ffd\u6eaf\u6027\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u4ee3\u7406\u7684\u66f4\u597d\u534f\u4f5c", "method": "\u57fa\u4e8egit\u517c\u5bb9\u6570\u636e\u5e93\u6784\u5efa\u5f00\u653e\u53ef\u6269\u5c55\u5e73\u53f0\uff0c\u91c7\u7528\u901a\u7528\u8bed\u4e49\u63a8\u7406\u5c42\uff0c\u5f00\u53d1AI\u539f\u751f\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f(SDLC)\uff0c\u901a\u8fc7Entire CLI\u7684\"\u68c0\u67e5\u70b9\"\u529f\u80fd\u81ea\u52a8\u6355\u83b7\u548c\u7248\u672c\u5316\u4ee3\u7406\u4e0a\u4e0b\u6587", "result": "\u63a8\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90\u4ea7\u54c1Entire CLI\uff0c\u5f15\u5165\u68c0\u67e5\u70b9\u673a\u5236\u6539\u5584AI\u4ee3\u7406\u7684\u53ef\u8ffd\u6eaf\u6027\uff0c\u4e3a\u6784\u5efa\u5b8c\u6574\u7684\u4eba\u7c7b-\u4ee3\u7406\u534f\u4f5c\u5e73\u53f0\u5960\u5b9a\u57fa\u7840", "conclusion": "Entire\u5e73\u53f0\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u89e3\u51b3\u4e86AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u53ef\u8ffd\u6eaf\u6027\u95ee\u9898\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5f00\u53d1\u8005\u5e73\u53f0\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411", "topic": "code agent"}}
{"id": "tldr.2602.36f76220", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/81bDQPPjP-OHnUlRo3SZzsT-ivoPv1WV2jO-cUdmbz8=444", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/81bDQPPjP-OHnUlRo3SZzsT-ivoPv1WV2jO-cUdmbz8=444", "authors": ["TLDR Newsletter"], "title": "Eight more months of agents", "comment": "Source: TLDR Newsletter, Date: 2026-02-11, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcrawshaw.io%2Fblog%2Feight-more-months-of-agents%3Futm_source=tldrdev/1/0100019c4cb27dbf-c9e97d3c-a51a-4cbf-89ca-8897f980ad9f-000000/81bDQPPjP-OHnUlRo3SZzsT-ivoPv1WV2jO-cUdmbz8=444", "summary": "Eight more months of agents (8 minute read) LLM agents have dramatically improved programming efficiency recently and made traditional IDEs obsolete.", "source": "tldr", "AI": {"tldr": "LLM\u667a\u80fd\u4f53\u663e\u8457\u63d0\u5347\u7f16\u7a0b\u6548\u7387\uff0c\u4f7f\u4f20\u7edfIDE\u8fc7\u65f6", "motivation": "\u4f20\u7edfIDE\u5728LLM\u667a\u80fd\u4f53\u65f6\u4ee3\u5df2\u663e\u8fc7\u65f6\uff0c\u9700\u8981\u63a2\u7d22\u667a\u80fd\u4f53\u5982\u4f55\u6539\u53d8\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7a0b", "method": "\u5206\u67908\u4e2a\u6708\u6765LLM\u667a\u80fd\u4f53\u5728\u7f16\u7a0b\u9886\u57df\u7684\u53d1\u5c55\u548c\u5e94\u7528", "result": "LLM\u667a\u80fd\u4f53\u5927\u5e45\u63d0\u5347\u7f16\u7a0b\u6548\u7387\uff0c\u6539\u53d8\u4e86\u4f20\u7edf\u7f16\u7a0b\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u5f0f", "conclusion": "LLM\u667a\u80fd\u4f53\u6b63\u5728\u91cd\u5851\u7f16\u7a0b\u5b9e\u8df5\uff0c\u4f20\u7edfIDE\u9700\u8981\u9002\u5e94\u8fd9\u4e00\u53d8\u9769", "topic": "code agent"}}
