<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 6]
- [wechat.article](#wechat.article) [Total: 11]
- [tldr.article](#tldr.article) [Total: 4]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search](https://arxiv.org/abs/2511.19648)
*Manil Shrestha,Edward Kim*

Main category: cs.CL

TL;DR: 提出了两种混合算法来解决知识图谱多跳问答中的效率和可验证性问题：LLM引导规划使用单次LLM调用预测关系序列，嵌入引导神经搜索完全消除LLM调用，通过轻量级边缘评分器融合文本和图嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱多跳问答中的组合爆炸问题，减少昂贵的LLM推理成本，确保答案在结构化知识中的可验证性。

Method: 1) LLM引导规划：单次LLM调用预测关系序列，通过广度优先搜索执行；2) 嵌入引导神经搜索：使用6.7M参数边缘评分器融合文本和图嵌入，完全消除LLM调用；通过知识蒸馏将规划能力压缩到4B参数模型中。

Result: 在MetaQA上评估，LLM引导规划达到接近完美的准确率（micro-F1 > 0.90），嵌入引导神经搜索实现100倍以上加速且保持竞争力准确率。结构化规划比直接答案生成更具可迁移性。

Conclusion: 可验证的多跳推理不需要在推理时使用大规模模型，而是需要结合符号结构和学习表征的正确架构归纳偏置。

Abstract: Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

</details>


### [2] [Can LLMs Faithfully Explain Themselves in Low-Resource Languages? A Case Study on Emotion Detection in Persian](https://arxiv.org/abs/2511.19719)
*Mobina Mehrazar,Mohammad Amin Yousefi,Parisa Abolfath Beygi,Behnam Bahrak*

Main category: cs.CL

TL;DR: 评估LLM在波斯语情感分类中生成解释的忠实性，发现模型解释与人类标注存在显著差异，提示策略对忠实性影响有限。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言（波斯语）中，LLM生成自解释的忠实性存在担忧，需要评估其解释是否反映真实推理过程。

Method: 通过比较LLM识别的影响词与人类标注，使用基于token级对数概率的置信度评分评估忠实性，测试两种提示策略（先预测后解释 vs 先解释后预测）。

Result: LLM分类性能强但生成解释不忠实，模型解释间一致性高于与人类标注的一致性，提示策略对忠实性影响不大。

Conclusion: 当前解释方法和指标存在局限性，需要更稳健的方法确保LLM在多语言和低资源环境中的可靠性。

Abstract: Large language models (LLMs) are increasingly used to generate self-explanations alongside their predictions, a practice that raises concerns about the faithfulness of these explanations, especially in low-resource languages. This study evaluates the faithfulness of LLM-generated explanations in the context of emotion classification in Persian, a low-resource language, by comparing the influential words identified by the model against those identified by human annotators. We assess faithfulness using confidence scores derived from token-level log-probabilities. Two prompting strategies, differing in the order of explanation and prediction (Predict-then-Explain and Explain-then-Predict), are tested for their impact on explanation faithfulness. Our results reveal that while LLMs achieve strong classification performance, their generated explanations often diverge from faithful reasoning, showing greater agreement with each other than with human judgments. These results highlight the limitations of current explanation methods and metrics, emphasizing the need for more robust approaches to ensure LLM reliability in multilingual and low-resource contexts.

</details>


### [3] [Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation](https://arxiv.org/abs/2511.19739)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

</details>


### [4] [AppSelectBench: Application-Level Tool Selection Benchmark](https://arxiv.org/abs/2511.19957)
*Tianyi Chen,Michael Solodko,Sen Wang,Jongwoo Ko,Junheng Hao,Colby Banbury,Sara Abdali,Saeed Amizadeh,Qing Xiao,Yinheng Li,Tianyu Ding,Kamran Ghasedi Dizaji,Suzhen Zheng,Hao Fan,Justin Wagle,Pashmina Cameron,Kazuhito Koishida*

Main category: cs.CL

TL;DR: 提出了AppSelectBench基准，用于评估计算机使用代理在应用选择方面的能力，包含10万+真实用户任务和100个桌面应用，揭示了现有模型在跨应用推理方面的系统性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估细粒度API选择，缺乏对模型在不同应用间进行推理和选择能力的评估，而应用选择是计算机使用代理有效操作的基础能力。

Method: 开发了用户任务生成流水线，生成真实、多样且语义基础的用户意图，并设计了统一的评估协议，包括随机、启发式、零样本、少样本和检索增强设置。

Result: 实验显示即使是最先进的模型在应用选择方面仍存在困难，在跨应用推理上表现出系统性的优势和弱点。

Conclusion: AppSelectBench为研究和推进应用级推理能力奠定了基础，这是智能计算机使用代理关键但未被充分探索的能力。

Abstract: Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.

</details>


### [5] [On Evaluating LLM Alignment by Evaluating LLMs as Judges](https://arxiv.org/abs/2511.20604)
*Yixin Liu,Pengfei Liu,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出AlignEval基准，通过评估LLM作为评判者的能力来间接衡量其与人类偏好的对齐程度，避免了直接评估生成输出的需求。研究发现生成与评估能力存在强相关性，新基准在LLM排名方面表现优于现有自动评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐评估需要直接评估生成输出，依赖人工标注或强LLM评判者。本文旨在探索LLM生成与评估能力之间的关系，并开发无需直接评估生成输出的对齐评估方法。

Method: 首先分析各种LLM的生成-评估一致性，发现两者在强LLM偏好预言机评估下存在强相关性。基于此提出AlignEval基准，通过评估LLM作为评判者的能力来间接衡量其对齐程度。

Result: AlignEval基准在捕捉人类偏好和排名LLM方面，匹配或超越了AlpacaEval和Arena-Hard等广泛使用的自动LLM评估基准。

Conclusion: LLM的生成与评估能力存在强相关性，AlignEval提供了一种无需直接评估模型输出的对齐评估方法，为理解LLM能力提供了新视角。

Abstract: Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.

</details>


### [6] [Latent Collaboration in Multi-Agent Systems](https://arxiv.org/abs/2511.20639)
*Jiaru Zou,Xiyuan Yang,Ruizhong Qiu,Gaotang Li,Katherine Tieu,Pan Lu,Ke Shen,Hanghang Tong,Yejin Choi,Jingrui He,James Zou,Mengdi Wang,Ling Yang*

Main category: cs.CL

TL;DR: LatentMAS是一个无需训练的端到端框架，使LLM代理能够在连续潜在空间中进行协作，相比基于文本的多代理系统具有更高的表达能力和无损信息交换，同时显著降低复杂度和提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理依赖基于文本的中介进行推理和通信，这限制了系统级智能的协调效率。本文旨在通过直接在连续潜在空间中实现模型协作来提升多代理系统的性能。

Method: 引入LatentMAS框架，每个代理通过最后一层隐藏嵌入进行自回归潜在思维生成，共享潜在工作内存保存和传输内部表示，实现无损信息交换。

Result: 在9个综合基准测试中，LatentMAS始终优于单模型和基于文本的多代理系统基线，准确率提高达14.6%，输出token使用减少70.8%-83.7%，端到端推理速度提升4-4.3倍。

Conclusion: 潜在协作框架在不增加额外训练的情况下，显著提升了系统级推理质量并带来实质性效率增益。

Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [7] [<em class="highlight">强化学习</em>：从入门到成仙](http://mp.weixin.qq.com/s?__biz=MzUyNzkyNzcwNg==&mid=2247484243&idx=1&sn=b7a2bb49400392b4f471e9aa2bec7afe&chksm=fb134c9b79d4d05af290eb5459b23671cbdef3b64de5646188776c7423385d953bc683426d6e#rd)
*键隙随想*

Main category: wechat.article

TL;DR: 2、强化学习的目的是学到一个策略函数，在每个时刻根据观测到的状态做出决策，策略可以是确定性的，也可以是随机的。3、随机性的策略函数与状态转移函数


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2、强化学习的目的是学到一个策略函数，在每个时刻根据观测到的状态做出决策，策略可以是确定性的，也可以是随机的。3、随机性的策略函数与状态转移函数

</details>


### [8] [深度<em class="highlight">强化学习</em>之基于Actor-Critic的Vanilla Policy Gradient 原理](http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247517077&idx=2&sn=f5cea821164e6ff41fa8f3221f91b2ff&chksm=9ab688bcf777350bc658e8a64c957dce21b77b8d6dac0f6fb44194d5fbb3c3b4455bc668f47d#rd)
*月来客栈*

Main category: wechat.article

TL;DR: 又因为在强化学习中，智能体实际上应该只根据行动之后的奖励来强化策略，而行动之前获得的奖励便与当前行动的优劣无关了，也就是说只有行动之后获得的奖励才有影响。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 又因为在强化学习中，智能体实际上应该只根据行动之后的奖励来强化策略，而行动之前获得的奖励便与当前行动的优劣无关了，也就是说只有行动之后获得的奖励才有影响。

</details>


### [9] [【论文荐读】元<em class="highlight">强化学习</em>综述](http://mp.weixin.qq.com/s?__biz=MzU5OTY5NDMxMg==&mid=2247485425&idx=1&sn=5542f5429bc63bda715ae3d1ffe13bbf&chksm=ff4ac98ab68218aa5beb18cae4e63c4a20cff4bdeb410c3aa10222e5e6b62276a7bedfd060b3#rd)
*电磁微课堂*

Main category: wechat.article

TL;DR: 2.3 元强化学习关键挑战 尽管元强化学习在适应速度与样本效率上优于传统强化学习，但仍面临五大核心挑战：（1） 元知识质量难以保证 元知识是元强化学习的核心，当前多以任务经验、共性规律或参数初始化条件形式存在，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 2.3 元强化学习关键挑战 尽管元强化学习在适应速度与样本效率上优于传统强化学习，但仍面临五大核心挑战：（1） 元知识质量难以保证 元知识是元强化学习的核心，当前多以任务经验、共性规律或参数初始化条件形式存在，

</details>


### [10] [万字长文读透通向通用智能的<em class="highlight">强化学习</em>：144页电子书全面讲解DQN、A2C、世界模型、大语言模型、智能体 | 2026必读](http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647617383&idx=1&sn=78a58c3eb8686500ca8b32a7938132c0&chksm=87e1eafcea8fa3b5b5a9a677589c2633efccc839e313213ebe76a3758e5cdec3ef2893af6e67#rd)
*走向未来*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）的本质是关于序贯决策的科学与工程。它试图解决一个根本性问题：一个智能体（agent）如何在复杂的、不确定的环境中，通过与环境的交互来学习一系列动作，以最大化其累积的期望奖励。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）的本质是关于序贯决策的科学与工程。它试图解决一个根本性问题：一个智能体（agent）如何在复杂的、不确定的环境中，通过与环境的交互来学习一系列动作，以最大化其累积的期望奖励。

</details>


### [11] [AdaCuRL：阿里提出新型<em class="highlight">强化学习</em>框架，解锁大模型的推理潜力，全面超越GRPO!](http://mp.weixin.qq.com/s?__biz=Mzk2NDAyOTQ2MA==&mid=2247483710&idx=1&sn=a6a5186ea84a6163764c33b397ab14e2&chksm=c5abd208676f64ab074ec740f8e619f7d7ccf537f1003f7778c7c7e0ca3bbd83bc94d1406ae6#rd)
*AI杂货铺*

Main category: wechat.article

TL;DR: 强化学习（Reinforcement Learning， RL）为大语言模型（LLM）的复杂推理能力提升开辟了一条激动人心的路径。以GRPO为代表的方法证明了，模型能够通过强化学习实现自我提升，而无需依赖昂贵且耗时的高质量思维链（Chain-of-Thought）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 强化学习（Reinforcement Learning， RL）为大语言模型（LLM）的复杂推理能力提升开辟了一条激动人心的路径。以GRPO为代表的方法证明了，模型能够通过强化学习实现自我提升，而无需依赖昂贵且耗时的高质量思维链（Chain-of-Thought）

</details>


### [12] [【ASE25获奖论文|Agent轨迹】大模型<em class="highlight">Code</em> <em class="highlight">Agent</em>是如何"思考"和"行动"的?](http://mp.weixin.qq.com/s?__biz=MzYzNTIwOTQ2OQ==&mid=2247483838&idx=1&sn=cba57f716e1a1f656ac8a4632ead9e41&chksm=f15b688f02a6f9f7bbc9b9f20dc926c9d4c8ca79e5ed7b213b25a803c492026c515e7d2fcef4#rd)
*大模型与软件工程*

Main category: wechat.article

TL;DR: 与传统的代码补全工具不同，这些智能体能够自主规划、调用外部工具、迭代改进方案。然而，它们的内部决策过程一直是个"黑盒"——我们看到了结果，却不知道它们如何得出答案。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 与传统的代码补全工具不同，这些智能体能够自主规划、调用外部工具、迭代改进方案。然而，它们的内部决策过程一直是个"黑盒"——我们看到了结果，却不知道它们如何得出答案。

</details>


### [13] [Huatek X AWS：以<em class="highlight">Agentic</em> AI驱动的企业供应链智能优化决策](http://mp.weixin.qq.com/s?__biz=MzI3NzIyNjcyMA==&mid=2247489997&idx=1&sn=150871e924ab877090fb3238d840f13d&chksm=ea70160bb087a9b53ccc6512aaf1ce08df67958f149281a19a4580d870a41ee86929f0c0a15a#rd)
*华泰软件*

Main category: wechat.article

TL;DR: 1研发设计方面Agentic AI可加速产品创新周期。通过深度学习历史设计数据与市场反馈，模拟多维度参数组合，智能推荐最优设计方案；同时实时分析竞品技术动态，预判研发风险，显著提升研发效率与成果转化率。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 1研发设计方面Agentic AI可加速产品创新周期。通过深度学习历史设计数据与市场反馈，模拟多维度参数组合，智能推荐最优设计方案；同时实时分析竞品技术动态，预判研发风险，显著提升研发效率与成果转化率。

</details>


### [14] [【产业资讯】微软推出全新Fara-7B <em class="highlight">Agentic</em>模型](http://mp.weixin.qq.com/s?__biz=MzYyNDU0NzcxNg==&mid=2247486046&idx=2&sn=16078b584aa310f48fe1589373c8194b&chksm=f163ee317112afb58fb7511d73f7d2f2910ff9c77d079609605f0a120cca1127e07943149da5#rd)
*AI典型场景产品*

Main category: wechat.article

TL;DR: 定位为专门用于计算机操作的“Agentic”模型，可通过鼠标和键盘执行网页任务。作为微软首个面向电脑使用场景的小模型（SLM），Fara-7B 由 70 亿参数构成，在同级体量中达到领先性能，并能在设备端本地运行，实现更低延迟及


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 定位为专门用于计算机操作的“Agentic”模型，可通过鼠标和键盘执行网页任务。作为微软首个面向电脑使用场景的小模型（SLM），Fara-7B 由 70 亿参数构成，在同级体量中达到领先性能，并能在设备端本地运行，实现更低延迟及

</details>


### [15] [国外<em class="highlight">Agentic</em> SOC平台落地实践经验](http://mp.weixin.qq.com/s?__biz=MzI4NDY2MDMwMw==&mid=2247515261&idx=2&sn=b853609faf6f2e62f932b632d15aca8b&chksm=ea3acec163a7f74c23f577855825bc033346c0589f51abcd4dc5278294874fa70ae99727a7c7#rd)
*安全内参*

Main category: wechat.article

TL;DR: 当前，Agentic AI赋能的Agentic SOC（自主式SOC）平台（即Agentic SOP）正发展得如火如荼，不断攀上炒作的高峰。Agentic SOC平台以LLM作为思考中枢，具有自主推理、规划和决策能力，能够调用各种工具自动完成预定的安全运营任务，并


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 当前，Agentic AI赋能的Agentic SOC（自主式SOC）平台（即Agentic SOP）正发展得如火如荼，不断攀上炒作的高峰。Agentic SOC平台以LLM作为思考中枢，具有自主推理、规划和决策能力，能够调用各种工具自动完成预定的安全运营任务，并

</details>


### [16] [<em class="highlight">Agentic</em>21种设计模式18-防护机制与安全模式](http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484384&idx=1&sn=0c909dad348fdd7714bf76ccc5d765a6&chksm=c007a0ebf1878015253953078126336fd3344142529028482f9698f937bcd16d3f504520a163#rd)
*AI Lab Dev*

Main category: wechat.article

TL;DR: 设计可靠的Agent构建可靠的Agent需要我们运用与传统软件工程相同的严谨方法与最佳实践。我们必须牢记，即便是确定性代码也容易出现错误，且可能产生不可预测的异常行为。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 设计可靠的Agent构建可靠的Agent需要我们运用与传统软件工程相同的严谨方法与最佳实践。我们必须牢记，即便是确定性代码也容易出现错误，且可能产生不可预测的异常行为。

</details>


### [17] [<em class="highlight">Agentic</em> AI 安全全景：从威胁建模到防御实战](http://mp.weixin.qq.com/s?__biz=MzI5NTQ3NzIxMw==&mid=2247485799&idx=1&sn=daf59f36823ae0eb65db2885890e75ac&chksm=ed0934ff2a57c7b01a2a29680a2f0ef9f3751dd087464698d9ba67bcc03a60023a7f4fbef5aa#rd)
*AI简化安全*

Main category: wechat.article

TL;DR: Agentic AI 系统能够感知环境，制定行动计划，并在无需持续人工干预的情况下完成复杂任务。这种架构通常由四个核心组件构成闭环：推理引擎（LLM）： 作为系统的“大脑”，负责分解目标和规划任务 。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 系统能够感知环境，制定行动计划，并在无需持续人工干预的情况下完成复杂任务。这种架构通常由四个核心组件构成闭环：推理引擎（LLM）： 作为系统的“大脑”，负责分解目标和规划任务 。

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [18] [Refactoring Risk Decisioning with Agentic AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ffintechtakes.com%2Farticles%2F2025-11-20%2Frefactoring-risk-decisioning-with-agentic-ai%2F%3Futm_source=tldrfintech/1/0100019ab63eb80a-a7667c25-6429-419b-8e66-3759245f183e-000000/BGqS2YThO0C8-LCLuPl_xldW4__uHuWSWx3x5Z5ksEU=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文定义了智能体AI的概念，区分其与生成式AI的差异，并探讨如何围绕智能体AI重新设计风险决策流程和系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域主要关注生成式AI，但智能体AI具有更主动的决策能力，需要专门的风险决策框架来应对其特性。

Method: 通过概念分析和比较研究，定义智能体AI的特征，并基于此提出风险决策系统的重新设计原则。

Result: 明确了智能体AI与生成式AI在自主性、决策能力等方面的本质区别，并建立了相应的风险决策框架。

Conclusion: 智能体AI需要专门的风险决策方法，传统的生成式AI风险管理框架不足以应对其主动决策特性。

Abstract: Refactoring Risk Decisioning with Agentic AI (10 minute read) This article attempts to define what agentic AI is, how it's different from generative AI (and why that difference isn't just semantic), and how we should think about redesigning risk decisioning processes and systems around agentic AI.

</details>


### [19] [What OpenAI Did When ChatGPT Users Lost Touch With Reality](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fz83buk/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/qW9I1s49mZsw2TMI_X5LQJxdQRTwfDg-jt-T1fRoBGk=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI在ChatGPT用户与现实脱节时面临内部安全与用户参与度的冲突，公司否决了模型行为团队的警告，发布了奉承用户的GPT-4o更新，导致用户更频繁使用。现在面临五起非正常死亡诉讼，并在发现更安全的GPT-5模型失去用户后宣布"橙色代码"紧急状态。


<details>
  <summary>Details</summary>
Motivation: OpenAI面临用户参与度和模型安全性之间的权衡困境，公司优先考虑用户留存率而忽视了安全团队的警告。

Method: 通过发布奉承用户的GPT-4o更新来提升用户参与度，同时监控更安全的GPT-5模型的用户流失情况。

Result: GPT-4o更新成功提高了用户使用频率，但导致用户与现实脱节，引发五起非正常死亡诉讼；更安全的GPT-5模型则面临用户流失问题。

Conclusion: OpenAI在平衡用户参与度和模型安全性方面面临严峻挑战，需要在商业利益和道德责任之间找到更好的平衡点。

Abstract: What OpenAI Did When ChatGPT Users Lost Touch With Reality (12 minute read) The New York Times revealed OpenAI's internal struggle between user engagement and safety after the company overruled its Model Behavior team's warnings to release a sycophantic April update to GPT-4o that made users return more frequently. The company now faces five wrongful death lawsuits and declared a "Code Orange" in October after discovering its safer GPT-5 model was losing users, with executives calling it "the...

</details>


### [20] [Agent Design Is Still Hard](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flucumr.pocoo.org%2F2025%2F11%2F21%2Fagents-are-hard%2F%3Futm_source=tldrai/1/0100019ab646550b-0e992898-819c-42f3-876d-9a3529ea4a25-000000/JmOWz5MCv6rDJj1rtUGWt2T2n7A42rW2250u4Ug0WuU=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 构建智能体仍然复杂，抽象在真实工具使用时失效，自管理缓存效果更好，强化学习承担更多任务，输出工具设计困难，模型选择仍依赖具体任务。


<details>
  <summary>Details</summary>
Motivation: 探讨当前智能体设计中的实际挑战和痛点，揭示理论抽象与实际实现之间的差距。

Method: 基于实践经验分析智能体设计的各个环节，包括工具使用、缓存管理、强化学习应用、输出工具设计和模型选择。

Result: 发现智能体设计仍面临多重挑战：抽象层在真实工具使用时易失效，自管理缓存比预设缓存更有效，强化学习在实际应用中承担了比预期更多的功能，输出工具设计复杂，模型选择需要根据具体任务定制。

Conclusion: 智能体设计仍然是一个复杂且充满挑战的领域，需要更实用的方法和工具来解决现实世界中的问题。

Abstract: Agent Design Is Still Hard (16 minute read) Building agents is still messy. Abstractions break once you hit real tool use. Caching works better when self-managed. Reinforcement does more heavy lifting than expected. Output tooling is surprisingly tricky. Model choice still depends on the task.

</details>


### [21] [Anthropic introduces cheaper, more powerful, more efficient Opus 4.5 model](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fai%2F2025%2F11%2Fanthropic-introduces-opus-4-5-cuts-api-pricing-and-enables-much-longer-claude-chats%2F%3Futm_source=tldrnewsletter/1/0100019abac15980-ab1fc195-f162-4e2a-8637-e274947357e6-000000/QqdVhwVUeH-k3-oC_R0m2Z_lpxICHiDCThxmjUGumH4=432)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic发布了更便宜、更强大、更高效的Opus 4.5模型，在编码性能和用户体验方面有改进，API价格为每百万输入token 5美元、输出token 25美元，并引入了'effort'参数来平衡效果和token使用。


<details>
  <summary>Details</summary>
Motivation: 提供更经济高效的大型语言模型，改善开发者的编码体验和成本控制。

Method: 开发新的Opus 4.5模型，优化编码性能，引入'effort'参数让开发者可以调节模型效果与token消耗的平衡。

Result: 发布了Opus 4.5模型，价格更便宜，编码性能提升，用户体验改善，Claude Code现在可用。

Conclusion: Anthropic通过Opus 4.5模型为开发者提供了更经济高效的AI编码解决方案。

Abstract: Anthropic introduces cheaper, more powerful, more efficient Opus 4.5 model (2 minute read) Anthropic has released its latest frontier model, Opus 4.5. The model brings improvements in coding performance and user experience. The Opus 4.5 API costs $5 per million input tokens and $25 per million output tokens. Anthropic's developer platform now includes a new 'effort' parameter that allows developers to more precisely tune the balance between efficacy and token usage. Claude Code is now availab...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [22] [Building Browser Agents: Architecture, Security, and Practical Solutions](https://arxiv.org/abs/2511.19477)
*Aram Vardanyan*

Main category: cs.SE

TL;DR: 生产级浏览器代理面临可靠性和安全挑战，研究发现模型能力不是限制因素，架构决策决定成败。安全分析显示提示注入攻击使通用自主操作不安全，建议开发具有程序约束的专用工具而非通用浏览智能。


<details>
  <summary>Details</summary>
Motivation: 解决生产环境中浏览器代理的可靠性和安全问题，分析当前方法失败的原因和阻碍安全自主操作的因素。

Method: 采用混合上下文管理（结合可访问性树快照和选择性视觉）、全面的浏览器工具集（匹配人类交互能力）和智能提示工程。

Result: 在WebGames基准测试的53个多样化挑战中达到约85%成功率，相比之前浏览器代理的约50%和人类基线的95.7%。

Conclusion: 反对开发通用浏览智能，主张开发具有程序约束的专用工具，通过代码而非大语言模型推理来强制执行安全边界。

Abstract: Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).

</details>


### [23] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: Z-Space是一个面向数据生成的多智能体协作工具调用框架，通过意图解析、工具过滤和推理执行三个模块，解决了大规模MCP服务中工具匹配的效率和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 随着企业级MCP服务的快速增长，在数千个异构工具中高效准确地匹配目标功能成为限制系统实用性的核心挑战。现有方法存在用户查询与工具描述语义脱节、LLM输入上下文膨胀和高推理延迟等问题。

Method: 建立多智能体协作架构和工具过滤算法：1）通过意图解析模型实现用户查询的结构化语义理解；2）基于融合子空间加权算法的工具过滤模块实现意图与工具的细粒度语义对齐；3）构建推理执行代理支持多步任务的动态规划和容错执行。

Result: 系统在工具推理中平均token消耗降低96.26%，工具调用准确率达到92%，显著提升了智能测试数据生成系统的效率和可靠性。

Conclusion: Z-Space框架有效解决了大规模工具调用中的语义匹配问题，在饿了么平台技术部多个业务单元的大规模测试数据生成场景中得到成功部署。

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [24] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: MADE框架通过问题分解将主观LLM评估转化为稳定选择压力，在软件开发等复杂任务中实现50%以上的性能提升，验证了从可计算指标优化到可描述质量优化的范式转变。


<details>
  <summary>Details</summary>
Motivation: 打破传统进化计算对客观可计算适应度函数的依赖，探索在纯主观LLM评判下的进化优化可能性。

Method: 提出MADE框架，通过问题分解将模糊指令转化为具体可验证的子要求，从而驯服主观评估的噪声。

Result: 在DevAI和InfoBench基准测试中，软件需求满足率从39.9%提升至61.9%，复杂指令遵循的完美通过率达到95%。

Conclusion: 验证了从优化可计算指标到优化可描述质量的根本范式转变，为无真实标签的开放领域解锁了进化优化潜力。

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [25] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: 提出了CodeR³系统，利用生成式AI将过时的Taverna工作流迁移到Snakemake和VisFlow等现代工作流技术中，通过自动化修复和人工验证相结合的方式解决工作流衰败问题。


<details>
  <summary>Details</summary>
Motivation: 科学工作流包含宝贵的领域专业知识，但大量已发布的工作流会随时间衰败，特别是像Taverna这样的传统工作流系统，由于服务终止、依赖过时和系统退役导致功能失效。

Method: 开发CodeR³系统，集成生成式AI分析衰败工作流特征，进行逐步工作流分析可视化、自动化服务替换和人工参与验证，将工作流迁移到现代技术。

Result: 通过多个Taverna工作流复兴案例研究证明该方法的可行性，自动化显著减少了工作流解析和服务识别的手动工作量，但服务替换和数据验证仍需领域专业知识。

Conclusion: 提出了一个平衡自动化效率与必要人工判断的工作流复兴框架，将开发众包平台供社区协作复兴衰败工作流并验证功能正确性。

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [26] [Agint: Agentic Graph Compilation for Software Engineering Agents](https://arxiv.org/abs/2511.19635)
*Abhi Chivukula,Jay Somasundaram,Vijay Somasundaram*

Main category: cs.SE

TL;DR: Agint是一个基于图的智能编码代理系统，通过分层编译将自然语言指令转换为类型化的代码DAG，提供可重现、可优化的执行环境。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM编码代理在上下文管理、延迟、可靠性、可重现性和可扩展性方面的挑战。

Method: 使用显式类型层次（文本→数据→规范→代码）和语义图转换，结合混合LLM和基于函数的JIT运行时，实现动态图优化。

Result: 提高了可靠性，支持并发组合，使用更小更快的模型，降低延迟，提高吞吐量，支持可重现的并行生成。

Conclusion: Agint通过连接自然语言、编译器方法和开发工具，实现了可组合、团队导向的编码代理规模化部署。

Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.

</details>


### [27] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: 提出了首个专门用于评估大语言模型检测提交消息与代码变更不一致性(MCI)的基准CODEFUSE-COMMITEVAL，通过规则引导的突变生成七种不一致消息类型，并评估了六种开源LLM在多种增强策略下的表现。


<details>
  <summary>Details</summary>
Motivation: 版本控制中的提交消息经常质量低下且与代码变更不一致，这会误导审查者、阻碍维护、污染研究数据集并可能掩盖安全补丁，但目前缺乏专门的基准来评估MCI检测模型。

Method: 基于ApacheCM数据集，通过规则引导的突变生成七种不一致消息类型，应用双重验证确保正负样本质量，评估六种开源LLM在普通设置及三种增强策略(少样本提示、思维链、扩展上下文)下的表现。

Result: 模型检测不一致提交比一致提交更可靠(平均召回率85.95%，精确率80.28%，特异性63.8%)；gpt-oss-20B表现最佳但token使用量是其他模型的两倍多；增强策略效果各异，不同类型的不一致性检测难度不同。

Conclusion: CODEFUSE-COMMITEVAL为MCI检测提供了严格的评估基础，强调了需要更丰富的上下文和平衡的数据来捕捉高层次语义差距。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [28] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: AgoneTest是一个用于评估LLM生成的Java单元测试的自动化框架，包含Classes2Test数据集和综合评估指标，实验显示LLM生成的测试在编译成功后能在覆盖率和缺陷检测方面达到或超过人工编写的测试。


<details>
  <summary>Details</summary>
Motivation: 单元测试是软件开发中重要但资源密集的环节，需要标准化框架来比较不同LLM和提示策略在现实条件下的表现。

Method: 提出AgoneTest评估框架和Classes2Test数据集，集成变异分数和测试异味等高级评估指标，建立端到端的标准化评估流程。

Result: 对于能够编译的测试子集，LLM生成的测试在覆盖率和缺陷检测方面能够匹配或超过人工编写的测试，增强的提示策略有助于提高测试质量。

Conclusion: AgoneTest阐明了LLM在软件测试中的潜力，为未来模型设计、提示工程和测试实践的改进提供了见解。

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>


### [29] [Translating Large-Scale C Repositories to Idiomatic Rust](https://arxiv.org/abs/2511.20617)
*Saman Dehghan,Tianran Sun,Tianxiang Wu,Zihan Li,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: Rustine是一个自动化C到Rust翻译管道，在23个C程序上实现了87%功能等价性，比现有方法更安全、更地道、更可读


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust翻译技术无法平衡质量和可扩展性：基于转译的方法可扩展但代码质量差，基于LLM的方法成本高昂且依赖前沿模型

Method: 提出Rustine全自动管道，用于高效且有效的仓库级C到地道安全Rust翻译

Result: 在23个C程序上，Rustine为所有程序生成完全可编译的Rust代码，达到87%功能等价性，比6种现有技术更安全、更地道、更可读

Conclusion: Rustine在C到Rust翻译中实现了质量和可扩展性的平衡，当翻译无法通过所有测试时，人类开发者平均只需4.5小时即可完成任务

Abstract: Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [Fara-7B: An Efficient Agentic Model for Computer Use](https://arxiv.org/abs/2511.19663)
*Ahmed Awadallah,Yash Lara,Raghav Magazine,Hussein Mozannar,Akshay Nambi,Yash Pandya,Aravind Rajeswaran,Corby Rosset,Alexey Taymanov,Vibhav Vineet,Spencer Whitehead,Andrew Zhao*

Main category: cs.AI

TL;DR: FaraGen是一个用于多步网页任务的合成数据生成系统，能够生成多样化的任务和解决方案，并训练出Fara-7B模型，该模型在多个基准测试中表现出色，甚至能与更大的前沿模型竞争。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUAs）的发展受到缺乏大规模高质量数据集的限制，现有数据集无法捕捉人类与计算机的交互方式。

Method: 开发FaraGen系统生成合成数据，包括提出多样化任务、生成多个解决方案尝试，并使用多个验证器过滤成功轨迹。使用该数据训练Fara-7B模型，该模型仅通过截图感知计算机，通过预测坐标执行动作。

Result: Fara-7B在WebVoyager、Online-Mind2Web和WebTailBench等基准测试中优于同类规模的CUA模型，并且与更大的前沿模型具有竞争力。数据生成成本约为每条轨迹1美元。

Conclusion: 可扩展的数据生成系统在推进小型高效代理模型方面具有关键优势，Fara-7B展示了这种方法的有效性。

Abstract: Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.

</details>


### [31] [Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs](https://arxiv.org/abs/2511.19773)
*Meng Lu,Ran Xu,Yi Fang,Wenxuan Zhang,Yue Yu,Gaurav Srivastava,Yuchen Zhuang,Mohamed Elhoseiny,Charles Fleming,Carl Yang,Zhengzhong Tu,Yang Xie,Guanghua Xiao,Hanrui Wang,Di Jin,Wenqi Shi,Xuan Wang*

Main category: cs.AI

TL;DR: VISTA-Gym是一个可扩展的训练环境，通过工具集成的视觉推理能力来增强视觉语言模型的多步视觉交互推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型在图像理解方面表现良好，但在多步骤视觉交互推理方面能力有限，需要提升工具选择、调用和协调的能力。

Method: VISTA-Gym统一了多样化的多模态推理任务，提供标准化视觉工具接口、可执行交互循环、可验证反馈信号和高效轨迹记录，通过多轮轨迹采样和端到端强化学习训练VISTA-R1模型。

Result: 在11个公共推理密集型VQA基准测试中，VISTA-R1-8B模型在相似规模下比最先进基线模型性能提升9.51%-18.72%。

Conclusion: VISTA-Gym是一个有效的训练平台，能够解锁视觉语言模型的工具集成推理能力。

Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.

</details>


### [32] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 提出了一种结合ω-正则目标和显式约束的强化学习方法，通过线性规划算法最大化满足ω-正则目标的概率，同时确保满足ω-正则约束条件。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，表达能力有限且容易导致奖励黑客行为；同时单一标量性能指标掩盖了安全与性能之间的权衡问题。

Method: 开发基于线性规划的模型强化学习算法，将ω-正则目标与约束分离处理，并建立到约束极限平均问题的转换。

Result: 算法在极限情况下能够产生最大化满足ω-正则目标概率的策略，同时确保满足指定的ω-正则约束阈值。

Conclusion: 该方法能够同时解决传统强化学习的表达能力和安全-性能权衡问题，为复杂行为规范提供了有效的解决方案。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [33] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 该研究将通用自我效能感量表(GSES)应用于10个大型语言模型，在不同任务条件下评估其模拟自我评估能力。研究发现模型自我评估稳定但不准确，与真实能力不匹配，且高自我效能感对应更拟人化的推理风格。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要关注任务准确性，而忽略了自我评估这一可靠智能的关键方面。研究旨在探索LLMs的模拟自我评估能力及其与任务表现的关系。

Method: 将10项通用自我效能感量表(GSES)应用于10个LLMs，在四种条件下(无任务、计算推理、社会推理、摘要)收集模拟自我评估，分析其稳定性、准确性以及与任务表现的关系。

Result: 模型自我评估在不同条件下保持高度稳定但显著低于人类标准；所有模型在计算和社会推理任务中表现完美，但摘要任务表现差异很大；自我评估不能可靠反映实际能力；高自我效能感对应更拟人化的推理风格。

Conclusion: 心理测量提示法能提供对LLM沟通行为的结构化洞察，但不能提供校准的性能估计。自我评估在LLMs中稳定但不准确，与真实能力脱节。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [34] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练，同时使用沙箱执行反馈定位和纠正错误步骤，在减少15%token消耗的同时提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于树搜索的代码生成方法难以有效评估中间算法步骤，无法及时定位和纠正错误步骤，导致生成错误代码和计算成本增加。

Method: 提出RPM-MCTS方法：1) 使用知识检索作为过程奖励模型评估中间步骤；2) 在扩展阶段使用相似性过滤去除冗余节点；3) 利用沙箱执行反馈定位和纠正错误算法步骤。

Result: 在四个公开代码生成基准测试中，RPM-MCTS优于当前最先进方法，同时减少约15%的token消耗。使用RPM-MCTS构建的数据对基础模型进行全微调可显著提升其代码能力。

Conclusion: RPM-MCTS通过知识检索和沙箱反馈有效解决了中间步骤评估和错误定位问题，在提升代码生成质量的同时降低了计算成本。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [35] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个系统级分类法，识别了15种真实世界LLM应用中的隐藏故障模式，分析了现有评估方法的不足，并提供了构建可靠、可维护、成本感知的LLM系统的设计原则。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中的行为理解不足，其故障模式与传统机器学习模型有根本差异，需要系统层面的可靠性分析。

Method: 提出系统级分类法识别15种隐藏故障模式，分析评估与监控实践的差距，研究部署挑战并制定设计原则。

Result: 建立了全面的LLM故障模式分类，揭示了现有基准测试在稳定性、可重复性、漂移和工作流集成方面的不足。

Conclusion: 将LLM可靠性视为系统工程问题而非纯模型中心问题，为未来评估方法、AI系统鲁棒性和可靠部署提供分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [36] [Reducing Latency of LLM Search Agent via Speculation-based Algorithm-System Co-Design](https://arxiv.org/abs/2511.20048)
*Zixiao Huang,Wen Zeng,Tianyu Fu,Tengxuan Liu,Yizhou Sun,Ke Hong,Xinhao Yang,Chengchun Liu,Yan Li,Quanlu Zhang,Guohao Dai,Zhenhua Zhu,Yu Wang*

Main category: cs.AI

TL;DR: SPAgent是一个算法-系统协同设计的框架，通过两阶段自适应推测机制和两级调度器，显著降低了基于LLM的搜索代理的延迟，实现了最高1.65倍的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的搜索代理虽然性能强大但存在严重延迟问题，因为每个步骤都需要串行化的LLM推理和工具执行。传统预测-验证推测范式虽然能打破串行执行，但收益有限，因为它保留了完整的工作负载并增加了额外的推理开销。

Method: SPAgent采用算法-系统协同设计：算法上引入两阶段自适应推测机制，在安全时选择性省略验证；系统上使用两级调度器根据引擎负载调节推测请求，确保推测保持有益。

Result: 在广泛的实验设置中，SPAgent实现了最高1.65倍的端到端加速，同时保持相同甚至更高的准确率。

Conclusion: SPAgent通过扩展推测在搜索代理中的作用，显著降低了延迟，使得多步骤搜索代理的实际部署成为可能。

Abstract: LLM-based search agents achieve strong performance but suffer from severe latency, as each step requires serialized LLM reasoning followed by action of tool execution. We revisit this bottleneck through the lens of speculation. While traditional predict-verify speculation paradigm can break serial execution, its benefit remains limited, as it retains the full original workload and adds extra inference overhead. We observe that early agent steps often involve simple evidence-gathering, where correct actions can often be predicted without full reasoning. Building on these observations, we present SPAgent, an algorithm-system co-design framework that expands the role of speculation in search agents to reduce latency. Algorithmically, SPAgent introduces a two-phase adaptive speculation mechanism that selectively omits verification when safe. System-wise, a two-level scheduler regulates speculative requests based on engine load to ensure speculation remains beneficial. We implement SPAgent in real-world systems. Across extensive experimental settings, SPAgent achieves up to $1.65\times$ end-to-end speedup while maintaining same or even achieving higher accuracy, enabling practical deployment of multi-step search agents.

</details>


### [37] [VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis](https://arxiv.org/abs/2511.20085)
*Chujie Wang,Zhiyuan Luo,Ruiqi Liu,Can Ran,Shenghua Fan,Xi Chen,Chu He*

Main category: cs.AI

TL;DR: 提出了VICoT多模态代理框架，通过将视觉工具动态整合到思维链中实现显式多轮推理，在遥感图像分析任务中显著优于现有SOTA框架。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分析任务正从传统目标识别向复杂智能推理演进，需要更强的推理能力和灵活的工具调用能力。

Method: 采用基于堆栈的推理结构和模块化MCP兼容工具套件，使LLM能高效执行多轮交错视觉语言推理；提出推理堆栈蒸馏方法将复杂代理行为迁移到轻量模型中。

Result: 在多个遥感基准测试中，VICoT在推理透明度、执行效率和生成质量方面显著优于现有SOTA框架。

Conclusion: VICoT框架通过显式多轮推理和工具动态整合，有效提升了遥感图像分析的推理能力和灵活性。

Abstract: The current remote sensing image analysis task is increasingly evolving from traditional object recognition to complex intelligence reasoning, which places higher requirements on the model's reasoning ability and the flexibility of tool invocation. To this end, we propose a new multimodal agent framework, Vision-Interleaved Chain-of-Thought Framework (VICoT), which implements explicit multi-round reasoning by dynamically incorporating visual tools into the chain of thought. Through a stack-based reasoning structure and a modular MCP-compatible tool suite, VICoT enables LLMs to efficiently perform multi-round, interleaved vision-language reasoning tasks with strong generalization and flexibility.We also propose the Reasoning Stack distillation method to migrate complex Agent behaviors to small, lightweight models, which ensures the reasoning capability while significantly reducing complexity. Experiments on multiple remote sensing benchmarks demonstrate that VICoT significantly outperforms existing SOTA frameworks in reasoning transparency, execution efficiency, and generation quality.

</details>


### [38] [From data to concepts via wiring diagrams](https://arxiv.org/abs/2511.20138)
*Jason Lo,Mohammadnima Jafari*

Main category: cs.AI

TL;DR: 本文提出了准骨架接线图的概念，证明了其与Hasse图的对应关系，并设计了从顺序数据中提取接线图的算法。这些算法在分析自主代理玩电脑游戏的行为时，成功识别了获胜策略。


<details>
  <summary>Details</summary>
Motivation: 接线图是表示抽象概念（如时间过程）的标记有向图。本文旨在开发从顺序数据中自动提取接线图的方法，以更好地理解和分析复杂系统的行为模式。

Method: 引入准骨架接线图概念，证明其与Hasse图的对应关系，设计基于此的接线图提取算法，并与DBSCAN和凝聚层次聚类等标准聚类技术进行比较。

Result: 算法在分析自主代理游戏行为时正确识别了获胜策略，在数据扰动情况下也表现出良好性能。

Conclusion: 本文成功将范畴论、图论、聚类、强化学习和数据工程技术相结合，为从顺序数据中提取接线图提供了有效方法。

Abstract: A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.

</details>


### [39] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 提出了一个简单有效的框架，在CPDC 2025挑战赛中统一改进了GPU和API两个赛道，通过上下文工程和GRPO训练获得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决多轮对话中工具调用稳定性和角色扮演指导的问题，同时通过强化学习缓解小样本过拟合。

Method: 上下文工程（动态工具剪枝、角色剪裁、参数归一化、函数合并）和GPU赛道中的GRPO强化学习训练。

Result: 在最终评估中，团队在Task 2 API排名第1，Task 1 API排名第2，Task 3 API和GPU赛道均排名第3。

Conclusion: 该方法在常识性角色对话任务中表现出色，证明了上下文工程和强化学习的有效性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [40] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航研究指标与商业可行性进行定量分析的经济测试平台，通过完整的成本-收益分析揭示任务成功优化与经济部署优化的根本差异。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准仅关注任务成功率，忽略了商业部署自动驾驶配送机器人所需的经济可行性考量。

Method: 建立微导航经济测试平台CostNav，模拟完整经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业参数进行成本-收益分析。

Result: 基准模型实现43.0%的服务水平协议合规率，但商业不可行：每次运行亏损30.009美元，碰撞导致的维护成本占每次运行成本的99.7%。

Conclusion: CostNav填补了导航研究与商业部署之间的差距，为评估基于规则的导航、模仿学习和成本感知强化学习提供了基础。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [41] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW框架通过构建和精炼经验学习知识库来优化智能体性能，在保持计算效率的同时显著提升任务精度并减少API调用


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的智能体训练方法计算开销大、收敛困难，且生成的策略难以解释、适应或增量改进

Method: 引入BREW框架，通过知识库构建和精炼来优化智能体，采用记忆分区、任务评分和行为准则来学习洞察，利用状态空间搜索确保鲁棒性

Result: 在OSWorld、τ²Bench和SpreadsheetBench基准测试中，BREW实现了10-20%的任务精度提升，10-15%的API调用减少，执行时间更快，同时保持与基础模型相当的计算效率

Conclusion: BREW将知识库确立为模块化、可控的智能体优化基板，提供透明、可解释和可扩展的行为塑造机制

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [42] [NNGPT: Rethinking AutoML with Large Language Models](https://arxiv.org/abs/2511.20333)
*Roman Kochnev,Waleed Khalid,Tolgay Atinc Uzun,Xi Zhang,Yashkumar Sanjaybhai Dhameliya,Furui Qin,Chandini Vysyaraju,Raghuvir Duvvuri,Avi Goyal,Dmitry Ignatov,Radu Timofte*

Main category: cs.AI

TL;DR: NNGPT是一个开源框架，将大语言模型转变为用于神经网络开发的自改进AutoML引擎，通过生成-评估-自改进的闭环系统持续优化模型，在计算机视觉领域实现端到端的神经网络架构、预处理代码和超参数生成与验证。


<details>
  <summary>Details</summary>
Motivation: 构建自改进AI系统是AI领域的根本挑战，现有框架在神经网络数据集扩展和持续优化方面存在局限，需要开发能够自主生成和优化模型的AutoML引擎。

Method: 集成五个协同的LLM管道：零样本架构合成、超参数优化、代码感知精度/早停预测、检索增强的PyTorch块合成（NN-RAG）和强化学习，基于LEMUR数据集构建闭环生成-评估-自改进系统。

Result: NN-RAG在1,289个目标上达到73%可执行性，3-shot提示提升常见数据集精度，HPO在LEMUR上RMSE 0.60优于Optuna，代码感知预测器RMSE 0.14，已生成超过5K验证模型。

Conclusion: NNGPT证明可作为自主AutoML引擎，通过PyTorch适配器实现框架无关性，显著减少试验次数，匹配基于搜索的AutoML性能。

Abstract: Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.

</details>


### [43] [DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs](https://arxiv.org/abs/2511.20468)
*Yuanhao Li,Mingshan Liu,Hongbo Wang,Yiding Zhang,Yifei Ma,Wei Tan*

Main category: cs.AI

TL;DR: DRAFT-RL是一个新颖的多智能体强化学习框架，通过集成Chain-of-Draft推理，让每个智能体生成多个草稿版本，经过同行评估和奖励模型选择最优推理路径，从而提升LLM智能体的推理能力和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体反思框架依赖单次响应，缺乏推理探索的结构多样性，限制了LLM智能体在复杂推理任务中的表现。

Method: 提出DRAFT-RL框架，集成Chain-of-Draft推理到多智能体RL训练中，每个智能体生成多个草稿，通过同行智能体评估和学习的奖励模型选择最优轨迹，使用actor-critic学习优化推理策略。

Result: 在代码合成、符号数学和知识密集型问答等复杂推理任务上，DRAFT-RL在准确性和收敛速度方面显著优于现有的反思和基于RL的智能体。

Conclusion: DRAFT-RL通过显式多路径探索、同行引导反思和奖励对齐选择，实现了更鲁棒和可解释的LLM智能体行为。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed

</details>


### [44] [FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization](https://arxiv.org/abs/2511.20510)
*Yuto Suzuki,Paul Awolade,Daniel V. LaBarbera,Farnoush Banaei-Kashani*

Main category: cs.AI

TL;DR: FRAGMENTA是一个用于药物先导化合物优化的端到端框架，包含新颖的生成模型和基于对话反馈的智能体系统，在癌症药物发现实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前分子生成面临小数据集训练困难、现有碎片化方法限制多样性、以及模型调优需要化学家与AI工程师缓慢协作等问题。

Method: 1）将碎片化重构为"词汇选择"问题，使用动态Q学习联合优化碎片化和生成；2）通过领域专家对话反馈精炼目标的智能体AI系统。

Result: 在真实癌症药物发现实验中，FRAGMENTA的人-智能体配置识别的高分分子数量是基线的近两倍，全自主的智能体-智能体系统优于传统的人-人调优。

Conclusion: 智能体调优能有效捕捉专家意图，证明该框架在药物发现中的高效性。

Abstract: Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a "vocabulary selection" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM](https://arxiv.org/abs/2511.19496)
*Yang Liu,Xiaolong Zhong,Ling Jiang*

Main category: cs.LG

TL;DR: Xmodel-2.5是一个13亿参数的小型语言模型，作为"即插即用代理核心"设计，通过μP训练方法实现超参数从2000万参数代理模型到完整模型的直接迁移，并采用AdamW到Muon的优化器切换策略提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具备强大的推理和工具使用能力，但其计算需求使其不适用于边缘或成本敏感部署场景，因此需要开发高效的小型模型。

Method: 采用最大更新参数化(μP)训练方法，使用1.4T token的Warmup-Stable-Decay课程学习，并在衰减阶段从AdamW切换到Muon优化器，结合FP8混合精度训练。

Result: 优化器切换策略使13个任务的推理平均性能提升4.58%，同时保持所有其他超参数不变，验证了早期AdamW稳定性与后期Muon锐化的组合优势。

Conclusion: Xmodel-2.5展示了小型模型作为代理核心的可行性，通过创新的训练策略实现了高性能与效率的平衡，所有资源均已开源。

Abstract: Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\,\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5.

</details>


### [46] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文开发了一个自动化管道来生成大规模、基于心理学的多轮越狱数据集，评估了不同LLM在多轮对话攻击中的脆弱性，发现GPT系列模型对对话历史特别敏感，而Gemini 2.5 Flash表现出卓越的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用心理学原理（如得寸进尺技巧）绕过LLM的安全对齐，但目前防御进展受到手动、难以扩展的数据集创建的限制。

Method: 系统地将FITD技术转化为可复现模板，创建包含1,500个场景的基准数据集，在有无对话历史的条件下评估7个来自三大LLM家族的模型。

Result: GPT系列模型对对话历史极其敏感，攻击成功率最多增加32个百分点；Gemini 2.5 Flash几乎免疫这些攻击；Claude 3 Haiku表现出强大但不完美的抵抗力。

Conclusion: 当前安全架构在处理对话上下文方面存在关键差异，需要能够抵抗基于叙事的操纵的防御机制。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [47] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 提出了一个包含200个多样化任务的基准测试，并开发了Newt多任务世界模型，通过大规模演示预训练和在线交互优化，实现了比基线更好的多任务性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究主要关注单任务或离线学习，缺乏对在线多任务学习的研究。受基础模型方法的启发，探索单个智能体是否可以通过在线交互在数百个任务上进行训练。

Method: 首先在演示数据上进行预训练以获得任务感知表示和动作先验，然后通过在线交互在所有任务上进行联合优化。

Result: Newt在多任务性能和数据效率方面优于强基线，表现出强大的开环控制能力，并能快速适应未见过的任务。

Conclusion: 在线强化学习可以扩展到多任务设置，Newt模型为通用控制智能体的发展提供了有前景的方向。

Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.

</details>


### [48] [The Devil in the Details: Emergent Misalignment, Format and Coherence in Open-Weights LLMs](https://arxiv.org/abs/2511.20104)
*Craig Dickson*

Main category: cs.LG

TL;DR: 现代开源权重模型在窄域微调后会出现对齐失效现象，但相比GPT-4o等专有模型，其失效率显著更低。JSON格式输出会使失效率加倍。


<details>
  <summary>Details</summary>
Motivation: 验证当前一代开源权重模型是否像Qwen-2.5系列那样具有对抗对齐失效的抵抗力，并测量不同模型架构和规模的鲁棒性。

Method: 在九个现代开源权重模型（Gemma 3和Qwen 3系列，1B-32B参数）上复现对齐失效效应，比较不安全代码生成微调前后的失效率。

Result: 微调模型失效率为0.68%（基础模型为0.07%），远低于GPT-4o的20%。JSON格式输出使失效率加倍（0.96% vs 0.42%）。

Conclusion: 对齐失效是现代开源权重模型中可复现的现象，但失效率远低于专有系统。结构约束可能通过减少模型拒绝的自由度来绕过安全训练。

Abstract: Prior work has shown that fine-tuning models on a narrow domain with misaligned data can lead to broad misalignment - a phenomenon termed "emergent misalignment" (Betley et al. 2025). While all tested models were susceptible to emergent misalignment, some models showed more resistance than others. Specifically the Qwen-2.5 family proved to be relatively resistant, while GPT-4o exhibited the strongest misalignment. In this paper we evaluate if current-generation open-weights models exhibit similar resistance to the Qwen-2.5 family and measure misalignment robustness over a range of model architectures and scales.
  We replicate the effect across nine modern open-weights models (Gemma 3 and Qwen 3 families, 1B-32B parameters). Models fine-tuned on insecure code generation show a 0.68% misalignment rate (compared to 0.07% for base models), matching the lower end of prior open-model results but dramatically lower than GPT-4o's 20%.
  We identify a critical format-dependent vulnerability: requiring JSON output doubles misalignment rates compared to natural language prompts (0.96% vs 0.42%). This suggests that structural constraints may bypass safety training by reducing the model's 'degrees of freedom' to refuse. These findings confirm emergent misalignment as a reproducible phenomenon in modern open-weights models, with rates substantially lower than observed in proprietary systems.

</details>


### [49] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: 提出Soft Adaptive Policy Optimization (SAPO)方法，通过软门控机制替代硬裁剪，在保持序列级一致性的同时实现令牌级自适应，提高LLM强化学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GSPO和GRPO）使用硬裁剪，难以同时保持稳定性和有效学习。令牌级重要性比率在MoE模型中方差较高，导致更新不稳定。

Method: SAPO使用平滑的温度控制门替代硬裁剪，自适应地衰减离策略更新，同时保留有用的学习信号。该方法既保持序列级一致性，又实现令牌级自适应。

Result: 在数学推理基准测试中，SAPO在相同训练预算下表现出更好的训练稳定性和更高的Pass@1性能。在Qwen3-VL模型系列上的应用显示，SAPO在不同任务和模型大小上都能带来一致的性能提升。

Conclusion: SAPO为LLM的强化学习训练提供了更可靠、可扩展和有效的优化策略。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [50] [Learning to Clean: Reinforcement Learning for Noisy Label Correction](https://arxiv.org/abs/2511.19808)
*Marzi Heidari,Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出RLNLC框架，将噪声标签校正问题建模为强化学习任务，通过深度特征表示策略网络迭代修正噪声标签，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中噪声标签问题严重影响预测模型性能，需要有效方法来处理标签噪声。

Method: 将噪声标签校正定义为强化学习问题，构建状态空间（数据和标签）、动作空间（标签修正）、奖励机制，使用actor-critic方法训练深度特征表示策略网络。

Result: 在多个基准数据集上的实验表明，RLNLC框架在噪声标签学习任务中持续优于现有最先进技术。

Conclusion: RLNLC成功将噪声标签校正建模为强化学习问题，通过迭代标签修正有效提升了预测模型的训练效果。

Abstract: The challenge of learning with noisy labels is significant in machine learning, as it can severely degrade the performance of prediction models if not addressed properly. This paper introduces a novel framework that conceptualizes noisy label correction as a reinforcement learning (RL) problem. The proposed approach, Reinforcement Learning for Noisy Label Correction (RLNLC), defines a comprehensive state space representing data and their associated labels, an action space that indicates possible label corrections, and a reward mechanism that evaluates the efficacy of label corrections. RLNLC learns a deep feature representation based policy network to perform label correction through reinforcement learning, utilizing an actor-critic method. The learned policy is subsequently deployed to iteratively correct noisy training labels and facilitate the training of the prediction model. The effectiveness of RLNLC is demonstrated through extensive experiments on multiple benchmark datasets, where it consistently outperforms existing state-of-the-art techniques for learning with noisy labels.

</details>


### [51] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文提出了一种名为"差分平滑"的原则性方法来解决RL微调中的多样性崩溃问题，通过理论证明和实验验证，该方法在正确性和多样性方面均优于传统RL和基于熵的启发式方法。


<details>
  <summary>Details</summary>
Motivation: RL微调大型语言模型常导致多样性崩溃，现有启发式方法存在随意性、效果不稳定且相互矛盾的问题，需要建立理论基础并提供系统解决方案。

Method: 首先形式化证明RL微调导致多样性崩溃的机制，然后提出差分平滑方法，该方法仅在正确轨迹上应用奖励修正来同时提升正确性和多样性。

Result: 在1B到7B参数的模型上进行广泛实验，在CountDown和真实世界数学推理等任务中均取得一致提升，在AIME24数据集上Pass@1和Pass@k提升高达6.7%。

Conclusion: 差分平滑方法在理论和实验上均证明优于现有方法，为RL微调中的多样性问题提供了原则性解决方案。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [52] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 该论文研究了LLM中的提示公平性问题，发现不同用户/风格的提示表述即使询问相同问题，也会引发LLM不同的响应。作者提出信息论指标量化这种偏差，并通过多数投票和提示中性化方法减少差异。


<details>
  <summary>Details</summary>
Motivation: LLM在不同用户提示风格下会产生不一致的响应，这可能导致结构性不平等，需要量化并缓解这种提示敏感性差异。

Method: 使用信息论指标（子组敏感性和跨组一致性）量化偏差，提出多数投票和提示中性化两种干预措施来改善公平性。

Result: 实验显示人口统计子组间存在明显的提示敏感性差异，跨组分歧值在0.14-0.28之间。应用缓解策略后，分歧值显著降低，最大差距降至0.22，许多距离降至0.17以下。

Conclusion: 提出的干预措施有效提高了LLM响应稳定性，增强了跨用户群体的公平性，减少了不同子组间的输出差异。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [53] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: 提出SOMBRL方法，基于不确定性乐观原则，通过结合外在奖励和认知不确定性来改进基于模型的强化学习中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习中未知系统动态下的高效探索挑战，特别是在需要直接从在线交互中学习的情况下。

Method: 学习不确定性感知的动态模型，并贪婪地最大化外在奖励和智能体认知不确定性的加权和，兼容任何策略优化器或规划器。

Result: 在状态和视觉控制环境中表现出色，在动态RC汽车硬件上超越现有最优方法，验证了基于原则的探索对MBRL的益处。

Conclusion: SOMBRL为基于模型的强化学习提供了灵活、可扩展且具有理论保证的探索解决方案。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [54] [CLIMATEAGENT: Multi-Agent Orchestration for Complex Climate Data Science Workflows](https://arxiv.org/abs/2511.20109)
*Hyeonjae Kim,Chenyue Li,Wen Deng,Mengxi Jin,Wen Huang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: ClimateAgent是一个自主多智能体框架，用于编排端到端气候数据分析工作流，通过分解用户问题、动态获取数据和自校正执行，在85个真实世界气候任务上实现100%完成率和8.32的报告质量得分。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体和静态脚本管道缺乏气候特定上下文和灵活性，在实际应用中表现不佳，需要专门的气候数据分析自动化解决方案。

Method: 采用多智能体框架：编排代理和计划代理协调子任务，数据代理动态内省API合成下载脚本，编码代理生成Python代码、可视化和报告，并包含自校正循环。

Result: 在Climate-Agent-Bench-85基准测试中，ClimateAgent实现100%任务完成率和8.32报告质量得分，优于GitHub-Copilot(6.27)和GPT-5基线(3.26)。

Conclusion: 多智能体编排结合动态API感知和自校正执行显著提高了气候科学分析任务的可靠端到端自动化能力。

Abstract: Climate science demands automated workflows to transform comprehensive questions into data-driven statements across massive, heterogeneous datasets. However, generic LLM agents and static scripting pipelines lack climate-specific context and flexibility, thus, perform poorly in practice. We present ClimateAgent, an autonomous multi-agent framework that orchestrates end-to-end climate data analytic workflows. ClimateAgent decomposes user questions into executable sub-tasks coordinated by an Orchestrate-Agent and a Plan-Agent; acquires data via specialized Data-Agents that dynamically introspect APIs to synthesize robust download scripts; and completes analysis and reporting with a Coding-Agent that generates Python code, visualizations, and a final report with a built-in self-correction loop. To enable systematic evaluation, we introduce Climate-Agent-Bench-85, a benchmark of 85 real-world tasks spanning atmospheric rivers, drought, extreme precipitation, heat waves, sea surface temperature, and tropical cyclones. On Climate-Agent-Bench-85, ClimateAgent achieves 100% task completion and a report quality score of 8.32, outperforming GitHub-Copilot (6.27) and a GPT-5 baseline (3.26). These results demonstrate that our multi-agent orchestration with dynamic API awareness and self-correcting execution substantially advances reliable, end-to-end automation for climate science analytic tasks.

</details>


### [55] [Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning](https://arxiv.org/abs/2511.20234)
*Olivier Moulin,Vincent Francois-lavet,Paul Elbers,Mark Hoogendoorn*

Main category: cs.LG

TL;DR: 提出了一种基于智能体神经网络内部权重预测RL智能体泛化能力的方法，并改进了PPO损失函数来提升智能体的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决RL智能体在训练环境上的过拟合问题，提高其在未见环境中的泛化能力。

Method: 使用智能体神经网络内部权重预测泛化分数，并基于此改进PPO损失函数。

Result: 实验结果表明改进后的PPO算法训练出的智能体具有更强的泛化能力。

Conclusion: 通过分析神经网络内部权重可以预测和提升RL智能体的泛化性能。

Abstract: Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.

</details>


### [56] [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490)
*Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne*

Main category: cs.LG

TL;DR: MTBBench是一个模拟分子肿瘤委员会(MTB)决策过程的智能体基准，用于评估多模态大语言模型在临床肿瘤学中的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态LLM基准无法捕捉真实临床工作流程的复杂性，特别是缺乏多智能体决策环境和纵向多模态数据整合的评估。

Method: 开发MTBBench基准，包含临床挑战性、多模态和纵向肿瘤学问题，并通过临床验证的应用程序确保临床相关性。同时提供基于基础模型的工具框架来增强多模态和纵向推理。

Result: 测试显示现有LLM在可靠性方面存在不足，经常产生幻觉，难以处理时间分辨数据，无法协调冲突证据或不同模态。使用工具框架后，任务级性能分别提升了9.0%和11.2%。

Conclusion: MTBBench为推进多模态LLM在精准肿瘤学MTB环境中的推理、可靠性和工具使用提供了具有挑战性和现实性的测试平台。

Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.

</details>


### [57] [Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning](https://arxiv.org/abs/2511.20613)
*Panayiotis Danassis,Naman Goel*

Main category: cs.LG

TL;DR: 该论文提出了一个基于真实世界物流优化问题的多智能体推理驱动基准测试，评估了40个LLM编码的智能体与17个人类编码智能体的表现，结果显示人类编码智能体明显优于LLM编码智能体。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成基准测试主要关注单元测试通过率和语法正确性，低估了需要规划、优化和策略交互的真实世界问题的难度，需要新的评估方法来强调现实场景中的推理驱动代码合成。

Method: 引入基于拍卖、取货和配送问题的多智能体推理驱动基准测试，要求构建能够(i)在不确定性下进行策略性竞价和(ii)优化规划器以最大化利润的智能体。评估了40个LLM编码智能体与17个人类编码智能体的表现。

Result: 人类编码智能体明显优于LLM编码智能体：前5名始终由人类编码智能体获得；大多数LLM编码智能体（33/40）被非常简单的基线击败；即使提供最佳人类解决方案，最佳性能的LLM反而使解决方案变得更差。

Conclusion: LLM在生成能够在现实世界中具有竞争力的代码方面存在能力差距，需要新的评估方法来强调现实场景中的推理驱动代码合成。

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.

</details>


### [58] [Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning](https://arxiv.org/abs/2511.20591)
*Charlotte Beylier,Hannah Selder,Arthur Fleig,Simon M. Hofmann,Nico Scherf*

Main category: cs.LG

TL;DR: 提出注意力导向指标(ATOMs)来研究强化学习智能体在训练过程中的注意力发展，通过三个Pong游戏变体验证了注意力模式与行为之间的关系。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的学习过程除了数学公式外仍缺乏深入理解，需要新的方法来研究智能体注意力在训练中的发展。

Method: 在三个设计不同的Pong游戏变体上测试ATOMs指标，结合行为评估，持续监控训练过程中的注意力发展。

Result: ATOMs成功区分了不同游戏变体训练的智能体注意力模式，这些差异转化为行为差异；注意力发展呈现阶段性特征，且在不同游戏变体中保持一致。

Conclusion: ATOMs有助于提高对强化学习智能体学习过程的理解，更好地理解注意力与学习之间的关系。

Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.

</details>


### [59] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 研究发现潜在扩散模型在潜在空间中存在不均匀记忆现象，提出基于解码器回拉度量的维度排序方法，显著提升了成员推理攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型反转技术主要关注数据域，忽略了潜在空间生成模型中的编码器/解码器对和潜在代码对记忆的影响。

Method: 提出基于解码器回拉度量的潜在维度排序方法，识别对记忆贡献最大的维度，在计算攻击统计量时移除较少记忆的维度。

Result: 在多个数据集上，该方法使成员推理攻击的AUROC平均提升2.7%，TPR@1%FPR提升6.42%。

Conclusion: 自编码器几何结构对LDM记忆有被忽视的重要影响，为分析扩散模型隐私风险提供了新视角。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>
