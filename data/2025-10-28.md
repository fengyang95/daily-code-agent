<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.LG](#cs.LG) [Total: 21]
- [wechat.article](#wechat.article) [Total: 32]
- [tldr.article](#tldr.article) [Total: 4]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Language Ranker: A Lightweight Ranking framework for LLM Decoding](https://arxiv.org/abs/2510.21883)
*Chenheng Zhang,Tianqi Du,Jizhe Zhang,Mingqing Xiao,Yifei Wang,Yisen Wang,Zhouchen Lin*

Main category: cs.CL

TL;DR: 提出Language Ranker框架，将LLM解码过程类比推荐系统排序阶段，通过轻量级模块重排候选响应，在性能媲美大型奖励模型的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统LLM研究主要关注输出分布优化，而忽视了将分布转化为最终响应的解码过程。现有解码方法存在高计算成本和适用性有限的问题。

Method: 将LLM解码过程重新定义为推荐系统中的排序阶段，提出Language Ranker框架，使用基础模型提取的特征通过轻量级模块对候选响应进行重排。

Result: 在多种任务上的实验表明，Language Ranker仅需<0.5M额外参数，就能达到与大型奖励模型相当的性能，同时显著降低训练和推理阶段的计算开销。

Conclusion: 该方法高效且有效，展示了充分释放LLM潜力的潜力，为解码过程提供了新的优化思路。

Abstract: Conventional research on large language models (LLMs) has primarily focused
on refining output distributions, while paying less attention to the decoding
process that transforms these distributions into final responses. Recent
advances, such as scaling the computation of inference time with reward models,
have underscored the importance of decoding, but these methods often suffer
from high computational costs and limited applicability. In this paper, we
revisit LLM generation through the lens of recommender systems, conceptualizing
the decoding process as analogous to the ranking stage in recommendation
pipelines. From this perspective, we observe that both traditional decoding
methods and reward models exhibit clear limitations such as redundancy.
Motivated by this insight, we propose Language Ranker, a novel framework that
introduces a lightweight module to rerank candidate responses using features
extracted by the base model. Experiments across a wide range of tasks show that
Language Ranker achieves performance comparable to large-scale reward models,
while requiring only <0.5M additional parameters, significantly reducing the
computational overhead during both training and inference stages. This
highlights the efficiency and effectiveness of our method, showcasing its
potential to fully unlock the capabilities of LLMs.

</details>


### [2] [Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks](https://arxiv.org/abs/2510.21884)
*Avinash Patil*

Main category: cs.CL

TL;DR: RACE框架用于评估LLM生成解释与逻辑回归特征重要性之间的对齐度，发现正确预测更支持特征，错误预测更矛盾特征。


<details>
  <summary>Details</summary>
Motivation: 随着ML在敏感领域的应用增加，需要透明可解释的AI。虽然LLM能生成自然语言解释，但这些解释是否忠实反映预测信号尚不明确。

Method: 提出RACE框架，在四个文本分类数据集上比较LLM解释与逻辑回归特征重要性，使用token感知、精确字符串和编辑距离匹配技术。

Result: 实证结果显示一致的不对称性：正确预测支持特征覆盖率更高，错误预测矛盾特征覆盖率更高。编辑距离匹配发现释义重叠。

Conclusion: LLM解释结合了表面和灵活的证据重用，但在错误情况下也会放大误导线索。RACE为评估神经语言模型推理完整性提供了量化基础。

Abstract: The growing adoption of machine learning (ML) in sensitive domains has
heightened the demand for transparent and interpretable artificial
intelligence. Large Language Models (LLMs) are increasingly capable of
producing natural language explanations, yet it remains unclear whether these
rationales faithfully capture the predictive signals that underlie decisions.
This paper introduces RACE-Reasoning Alignment for Completeness of
Explanations, a systematic framework to evaluate the alignment between
LLM-generated explanations and interpretable feature importance scores derived
from a logistic regression baseline. We analyze four widely used text
classification datasets-WIKI ONTOLOGY, AG NEWS, IMDB, and GOEMOTIONS-and
compare LLM rationales against top-ranked supporting and contradicting lexical
features. To capture alignment at multiple levels of granularity, RACE
implements token-aware, exact string, and edit-distance matching techniques.
Empirical results reveal a consistent asymmetry: correct predictions exhibit
higher coverage of supporting features, while incorrect predictions are
associated with elevated coverage of contradicting features. Edit-distance
matching further uncovers paraphrastic overlaps, boosting coverage while
preserving this asymmetry. These findings demonstrate that LLM rationales
combine both surface-level and flexible evidence reuse, yet can also amplify
misleading cues in error cases. RACE provides new insights into the
faithfulness of LLM explanations and establishes a quantitative basis for
evaluating reasoning completeness in neural language models.

</details>


### [3] [Embedding Trust: Semantic Isotropy Predicts Nonfactuality in Long-Form Text Generation](https://arxiv.org/abs/2510.21891)
*Dhrupad Bhardwaj,Julia Kempe,Tim G. J. Rudner*

Main category: cs.CL

TL;DR: 提出语义各向同性方法，通过分析文本嵌入在单位球面上的分布均匀性来评估LLM长文本响应的可信度，无需标注数据、微调或超参数选择。


<details>
  <summary>Details</summary>
Motivation: 在需要准确回答开放性问题的高风险应用领域部署LLM时，需要可靠且计算成本低的方法来评估长文本响应的可信度，而现有基于逐项事实核查的方法计算昂贵且脆弱。

Method: 生成多个长文本响应，将其嵌入到向量空间，通过计算嵌入向量在单位球面上的角度离散度来估计语义各向同性水平。

Result: 发现更高的语义各向同性（即更大的嵌入离散度）可靠地表明样本间的事实一致性较低。该方法在多个领域均优于现有方法，仅需少量样本即可预测长文本响应的非事实性。

Conclusion: 语义各向同性提供了一种实用、低成本的信任评估方法，可集成到实际LLM工作流程中。

Abstract: To deploy large language models (LLMs) in high-stakes application domains
that require substantively accurate responses to open-ended prompts, we need
reliable, computationally inexpensive methods that assess the trustworthiness
of long-form responses generated by LLMs. However, existing approaches often
rely on claim-by-claim fact-checking, which is computationally expensive and
brittle in long-form responses to open-ended prompts. In this work, we
introduce semantic isotropy -- the degree of uniformity across normalized text
embeddings on the unit sphere -- and use it to assess the trustworthiness of
long-form responses generated by LLMs. To do so, we generate several long-form
responses, embed them, and estimate the level of semantic isotropy of these
responses as the angular dispersion of the embeddings on the unit sphere. We
find that higher semantic isotropy -- that is, greater embedding dispersion --
reliably signals lower factual consistency across samples. Our approach
requires no labeled data, no fine-tuning, and no hyperparameter selection, and
can be used with open- or closed-weight embedding models. Across multiple
domains, our method consistently outperforms existing approaches in predicting
nonfactuality in long-form responses using only a handful of samples --
offering a practical, low-cost approach for integrating trust assessment into
real-world LLM workflows.

</details>


### [4] [Generalization or Memorization: Dynamic Decoding for Mode Steering](https://arxiv.org/abs/2510.22099)
*Xuanming Zhang*

Main category: cs.CL

TL;DR: 提出了一个统一框架来理解和控制LLM的泛化与记忆模式，基于信息瓶颈理论开发了动态模式引导算法，通过轻量级线性探针和动态激活引导来提升LLM的可靠性和推理一致性。


<details>
  <summary>Details</summary>
Motivation: LLM存在泛化能力和逐字记忆的双重特性，这种不可预测性在高风险应用中削弱了其可靠性，需要系统性的方法来理解和控制这两种推理模式。

Method: 基于信息瓶颈理论建立理论模型，开发动态模式引导算法，包含因果基础的线性探针来识别记忆依赖，以及动态激活引导机制将计算推向泛化电路。

Result: 在推理和真实性任务上的实验表明，DMS显著提高了逻辑一致性和事实准确性。

Conclusion: DMS为增强LLM可靠性提供了一种原则性方法，通过自适应自对比解码实现了对泛化和记忆模式的有效控制。

Abstract: Large Language Models (LLMs) exhibit a troubling duality, capable of both
remarkable generalization and brittle, verbatim memorization of their training
data. This unpredictability undermines their reliability in high-stakes
applications. In this work, we propose a unified framework to understand,
identify, and control these distinct reasoning modes. First, we introduce a
theoretical model based on the Information Bottleneck (IB) principle,
formalizing generalization as the learning of a compressed, task-relevant
representation and memorization as a failure to compress. Building on this
theory, we develop Dynamic Mode Steering (DMS), a novel inference-time
algorithm which comprises two components: (1) a lightweight, causally-grounded
linear probe that identifies the model's instantaneous reliance on
memorization, and (2) a dynamic activation steering mechanism that nudges the
model's computation towards pre-identified generalization circuits. We frame
DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning
and faithfulness tasks demonstrate that DMS significantly improves logical
consistency and factual accuracy, thereby offering a principled approach to
enhancing LLM reliability.

</details>


### [5] [You Don't Need Prompt Engineering Anymore: The Prompting Inversion](https://arxiv.org/abs/2510.22251)
*Imran Khan*

Main category: cs.CL

TL;DR: 提出了一种名为"Sculpting"的约束性提示方法，相比标准CoT能减少语义模糊和常识错误，但在不同模型上表现出"提示反转"现象。


<details>
  <summary>Details</summary>
Motivation: 标准CoT提示存在语义模糊和常识错误的问题，需要开发更精确的提示方法来提升LLM的推理能力。

Method: 提出了基于规则的"Sculpting"提示方法，在GSM8K数学推理基准上评估了三种提示策略（零样本、标准CoT、Sculpting）在三个OpenAI模型上的表现。

Result: 发现"提示反转"现象：Sculpting在gpt-4o上优于标准CoT（97% vs 93%），但在gpt-5上反而有害（94.00% vs 96.36%）。

Conclusion: 最优提示策略需要与模型能力共同进化，更强大的模型可能需要更简单的提示。

Abstract: Prompt engineering, particularly Chain-of-Thought (CoT) prompting,
significantly enhances LLM reasoning capabilities. We introduce "Sculpting," a
constrained, rule-based prompting method designed to improve upon standard CoT
by reducing errors from semantic ambiguity and flawed common sense.
  We evaluate three prompting strategies (Zero Shot, standard CoT, and
Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5)
using the GSM8K mathematical reasoning benchmark (1,317 problems).
  Our findings reveal a "Prompting Inversion": Sculpting provides advantages on
gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00%
vs. 96.36% for CoT on full benchmark). We trace this to a
"Guardrail-to-Handcuff" transition where constraints preventing common-sense
errors in mid-tier models induce hyper-literalism in advanced models. Our
detailed error analysis demonstrates that optimal prompting strategies must
co-evolve with model capabilities, suggesting simpler prompts for more capable
models.

</details>


### [6] [SteerX: Disentangled Steering for LLM Personalization](https://arxiv.org/abs/2510.22256)
*Xiaoyan Zhao,Ming Yan,Yilun Qiu,Haoting Ni,Yang Zhang,Fuli Feng,Hong Cheng,Tat-Seng Chua*

Main category: cs.CL

TL;DR: SteerX是一种解耦的激活引导方法，通过因果推断识别偏好驱动token，生成更准确的个性化LLM引导向量


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法使用所有历史数据计算引导向量，但并非所有内容都反映真实用户偏好，这会削弱个性化信号

Method: 基于因果推断理论，估计token级因果效应识别偏好驱动token，将其转化为连贯描述，然后用于引导个性化LLM生成

Result: 在两个代表性引导骨干方法和真实数据集上的实验表明，SteerX持续提升引导向量质量

Conclusion: SteerX通过聚焦真正偏好驱动信息，为更有效的LLM个性化提供了实用解决方案

Abstract: Large language models (LLMs) have shown remarkable success in recent years,
enabling a wide range of applications, including intelligent assistants that
support users' daily life and work. A critical factor in building such
assistants is personalizing LLMs, as user preferences and needs vary widely.
Activation steering, which directly leverages directions representing user
preference in the LLM activation space to adjust its behavior, offers a
cost-effective way to align the model's outputs with individual users. However,
existing methods rely on all historical data to compute the steering vector,
ignoring that not all content reflects true user preferences, which undermines
the personalization signal. To address this, we propose SteerX, a disentangled
steering method that isolates preference-driven components from
preference-agnostic components. Grounded in causal inference theory, SteerX
estimates token-level causal effects to identify preference-driven tokens,
transforms these discrete signals into a coherent description, and then
leverages them to steer personalized LLM generation. By focusing on the truly
preference-driven information, SteerX produces more accurate activation
steering vectors and enhances personalization. Experiments on two
representative steering backbone methods across real-world datasets demonstrate
that SteerX consistently enhances steering vector quality, offering a practical
solution for more effective LLM personalization.

</details>


### [7] [Scalable Supervising Software Agents with Patch Reasoner](https://arxiv.org/abs/2510.22775)
*Junjielong Xu,Boyin Tan,Xiaoyuan Liu,Chao Peng,Pengfei Gao,Pinjia He*

Main category: cs.CL

TL;DR: R4P是一个基于推理的补丁验证模型，为软件工程代理提供可扩展的奖励机制，解决了传统测试监督方法在可扩展性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于测试的监督方法存在可扩展性限制，因为构建和运行测试沙盒既繁重又脆弱，且具有高覆盖率测试的数据稀缺且容易受到边缘案例测试攻击的威胁。

Method: R4P将补丁验证视为推理任务，采用分组目标进行强化学习训练，能够验证多个补丁之间的修改并获得密集奖励。

Result: R4P在SWE-bench-verified上达到72.2%的补丁验证准确率，超越OpenAI o3。基于R4P训练的Mini-SE在SWE-bench-verified上达到26.2%的Pass@1，比原始Qwen3-32B提升10.0%。R4P验证补丁的速度比平均测试快50倍。

Conclusion: R4P通过推理驱动的补丁验证为软件工程代理提供了高效、可扩展的奖励机制，显著提升了代理性能。

Abstract: While large language model agents have advanced software engineering tasks,
the unscalable nature of existing test-based supervision is limiting the
potential improvement of data scaling. The reason is twofold: (1) building and
running test sandbox is rather heavy and fragile, and (2) data with
high-coverage tests is naturally rare and threatened by test hacking via edge
cases. In this paper, we propose R4P, a patch verifier model to provide
scalable rewards for training and testing SWE agents via reasoning. We consider
that patch verification is fundamentally a reasoning task, mirroring how human
repository maintainers review patches without writing and running new
reproduction tests. To obtain sufficient reference and reduce the risk of
reward hacking, R4P uses a group-wise objective for RL training, enabling it to
verify multiple patches against each other's modification and gain a dense
reward for stable training. R4P achieves 72.2% Acc. for verifying patches from
SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we
design and train a lite scaffold, Mini-SE, with pure reinforcement learning
where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2%
Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original
Qwen3-32B. This can be further improved to 32.8% with R4P for test-time
scaling. Furthermore, R4P verifies patches within a second, 50x faster than
testing on average. The stable scaling curves of rewards and accuracy along
with high efficiency reflect R4P's practicality.

</details>


### [8] [Language Server CLI Empowers Language Agents with Process Rewards](https://arxiv.org/abs/2510.22907)
*Yifan Zhang,Lanser Contributors*

Main category: cs.CL

TL;DR: Lanser-CLI是一个CLI优先的编排层，通过语言服务器协议为编码代理和CI提供确定性、可重放的工作流，解决LLM幻觉API和编辑定位错误的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生API幻觉和编辑定位错误，而语言服务器能够计算关于真实代码的经过验证的IDE级事实。需要将语言服务器的结构化信息和过程奖励整合到代理的规划循环中。

Method: 开发Lanser-CLI，包含：1) 稳健的寻址方案和选择器DSL；2) 确定性分析包；3) 变异操作的安全信封；4) 基于语言服务器事实的过程奖励函数。

Result: 建立了在冻结快照下的确定性形式化，并为过程奖励建立了单调性属性，使其适用于过程监督和反事实分析。

Conclusion: 语言服务器不仅提供结构化信息，还提供可操作的过程奖励，使代理的规划循环与程序现实保持一致。

Abstract: Large language models routinely hallucinate APIs and mislocalize edits, while
language servers compute verified, IDE-grade facts about real code. We present
Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language
Server Protocol (LSP) server for coding agents and CI, exposing deterministic,
replayable workflows. Our position is that language servers provide not only
structural information (definitions, references, types, diagnostics) but also
an actionable process reward: machine-checked, step-wise signals that align an
agent's planning loop with program reality. In this work, Lanser-CLI
contributes: (i) a robust addressing scheme beyond brittle "file:line:col" via
a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a
principled relocation algorithm; (ii) deterministic Analysis Bundles that
normalize Language Server responses and capture environment/capability metadata
with stable content hashes; (iii) a safety envelope for mutating operations
(rename, code actions) with preview, workspace jails, and Git-aware,
transactional apply; and (iv) a process-reward functional derived from Language
Server facts (diagnostic deltas, disambiguation confidence, and safe-apply
checks) that is computable online and replayable offline. We formalize
determinism under frozen snapshots and establish a monotonicity property for
the process reward, making it suitable for process supervision and
counterfactual analysis. Project Page:
https://github.com/yifanzhang-pro/lanser-cli

</details>


### [9] [MATCH: Task-Driven Code Evaluation through Contrastive Learning](https://arxiv.org/abs/2510.23169)
*Marah Ghoummaid,Vladimir Tchuiev,Ofek Glick,Michal Moschkovitz,Dotan Di Castro*

Main category: cs.CL

TL;DR: 本文提出了MATCH，一种基于对比学习的无参考代码评估指标，用于评估生成代码与开发者意图的匹配程度，相比现有指标在功能正确性和人类偏好方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统代码评估方法（如单元测试）成本高且不可扩展，语法相似度指标无法捕捉代码功能，现有无参考评估指标较少且效果有限。

Method: 使用对比学习生成代码和自然语言任务描述的有意义嵌入，通过相似度评分反映生成代码实现任务的程度。

Result: MATCH在多种编程语言中与功能正确性和人类偏好的相关性均优于现有指标。

Conclusion: MATCH是一种有效的无参考代码评估指标，能够准确评估生成代码与开发者意图的匹配程度。

Abstract: AI-based code generation is increasingly prevalent, with GitHub Copilot
estimated to generate 46% of the code on GitHub. Accurately evaluating how well
generated code aligns with developer intent remains a critical challenge.
Traditional evaluation methods, such as unit tests, are often unscalable and
costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code
functionality, and metrics like CodeBERTScore require reference code, which is
not always available. To address the gap in reference-free evaluation, with few
alternatives such as ICE-Score, this paper introduces MATCH, a novel
reference-free metric. MATCH uses Contrastive Learning to generate meaningful
embeddings for code and natural language task descriptions, enabling similarity
scoring that reflects how well generated code implements the task. We show that
MATCH achieves stronger correlations with functional correctness and human
preference than existing metrics across multiple programming languages.

</details>


### [10] [FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.22344)
*Mohammad Aghajani Asl,Majid Asgari-Bidhendi,Behrooz Minaei-Bidgoli*

Main category: cs.CL

TL;DR: FAIR-RAG是一个新型代理框架，通过结构化证据评估和迭代精炼机制，显著提升了复杂多跳问答任务中RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG框架在处理需要从不同来源综合信息的复杂多跳查询时表现不佳，缺乏系统识别和填补证据缺口的机制。

Method: 引入结构化证据评估(SEA)模块作为分析门控机制，将查询分解为所需发现清单，并通过自适应查询精炼代理生成针对性子查询来填补信息缺口。

Result: 在HotpotQA、2WikiMultiHopQA和MusiQue等基准测试中，FAIR-RAG显著优于强基线，在HotpotQA上达到0.453的F1分数，比最强迭代基线提升8.3个百分点。

Conclusion: 具有显式缺口分析的结构化证据驱动精炼过程对于在复杂知识密集型任务中实现可靠准确的推理至关重要。

Abstract: While Retrieval-Augmented Generation (RAG) mitigates hallucination and
knowledge staleness in Large Language Models (LLMs), existing frameworks often
falter on complex, multi-hop queries that require synthesizing information from
disparate sources. Current advanced RAG methods, employing iterative or
adaptive strategies, lack a robust mechanism to systematically identify and
fill evidence gaps, often propagating noise or failing to gather a
comprehensive context. We introduce FAIR-RAG, a novel agentic framework that
transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning
process. At its core is an Iterative Refinement Cycle governed by a module we
term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating
mechanism: it deconstructs the initial query into a checklist of required
findings and audits the aggregated evidence to identify confirmed facts and,
critically, explicit informational gaps. These gaps provide a precise signal to
an Adaptive Query Refinement agent, which generates new, targeted sub-queries
to retrieve missing information. This cycle repeats until the evidence is
verified as sufficient, ensuring a comprehensive context for a final, strictly
faithful generation. We conducted experiments on challenging multi-hop QA
benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified
experimental setup, FAIR-RAG significantly outperforms strong baselines. On
HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3
points over the strongest iterative baseline -- establishing a new
state-of-the-art for this class of methods on these benchmarks. Our work
demonstrates that a structured, evidence-driven refinement process with
explicit gap analysis is crucial for unlocking reliable and accurate reasoning
in advanced RAG systems for complex, knowledge-intensive tasks.

</details>


### [11] [Rule-Based Explanations for Retrieval-Augmented LLM Systems](https://arxiv.org/abs/2510.22689)
*Joel Rorseth,Parke Godfrey,Lukasz Golab,Divesh Srivastava,Jarek Szlichta*

Main category: cs.CL

TL;DR: 提出首个使用if-then规则解释检索增强生成(RAG)大语言模型的方案，通过优化算法加速规则生成过程


<details>
  <summary>Details</summary>
Motivation: 传统if-then规则广泛用于解释机器学习模型，但尚未应用于新兴的RAG增强LLM系统。RAG允许在推理时整合检索信息，需要解释输出与检索源之间的关联

Method: 提出优化规则生成方法，受Apriori频繁项集挖掘启发但重新定义，通过剪枝策略加速规则生成，避免暴力枚举所有源组合

Result: 通过定性和定量实验验证了解决方案的价值和效率

Conclusion: 成功将规则解释方法应用于RAG增强LLM系统，提供了解释输出来源的有效途径

Abstract: If-then rules are widely used to explain machine learning models; e.g., "if
employed = no, then loan application = rejected." We present the first proposal
to apply rules to explain the emerging class of large language models (LLMs)
with retrieval-augmented generation (RAG). Since RAG enables LLM systems to
incorporate retrieved information sources at inference time, rules linking the
presence or absence of sources can explain output provenance; e.g., "if a Times
Higher Education ranking article is retrieved, then the LLM ranks Oxford
first." To generate such rules, a brute force approach would probe the LLM with
all source combinations and check if the presence or absence of any sources
leads to the same output. We propose optimizations to speed up rule generation,
inspired by Apriori-like pruning from frequent itemset mining but redefined
within the scope of our novel problem. We conclude with qualitative and
quantitative experiments demonstrating our solutions' value and efficiency.

</details>


### [12] [MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs](https://arxiv.org/abs/2510.22967)
*Yucheng Ning,Xixun Lin,Fang Fang,Yanan Cao*

Main category: cs.CL

TL;DR: 提出了一个系统化方法来评估长文本的事实准确性，包括构建中文长文本事实性数据集LongHalluQA、开发基于辩论的多智能体验证系统MAD-Fact，以及引入事实重要性层次结构。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生物医学、法律和教育等高风险领域的广泛应用引发了对其输出事实准确性的担忧，现有评估方法在处理长文本时表现不佳。

Method: 整合大规模长文本数据集、多智能体验证机制和加权评估指标，构建中文长文本事实性数据集LongHalluQA，开发基于辩论的多智能体验证系统MAD-Fact，并引入事实重要性层次结构。

Result: 在两个基准测试上的实验表明，较大的LLM通常保持较高的事实一致性，而国产模型在中文内容上表现更优。

Conclusion: 该研究为评估和增强长文本LLM输出的事实可靠性提供了结构化框架，指导其在敏感领域的安全部署。

Abstract: The widespread adoption of Large Language Models (LLMs) raises critical
concerns about the factual accuracy of their outputs, especially in high-risk
domains such as biomedicine, law, and education. Existing evaluation methods
for short texts often fail on long-form content due to complex reasoning
chains, intertwined perspectives, and cumulative information. To address this,
we propose a systematic approach integrating large-scale long-form datasets,
multi-agent verification mechanisms, and weighted evaluation metrics. We
construct LongHalluQA, a Chinese long-form factuality dataset; and develop
MAD-Fact, a debate-based multi-agent verification system. We introduce a fact
importance hierarchy to capture the varying significance of claims in long-form
texts. Experiments on two benchmarks show that larger LLMs generally maintain
higher factual consistency, while domestic models excel on Chinese content. Our
work provides a structured framework for evaluating and enhancing factual
reliability in long-form LLM outputs, guiding their safe deployment in
sensitive domains.

</details>


### [13] [Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures](https://arxiv.org/abs/2510.23006)
*Shenran Wang,Timothy Tin-Long Tse,Jian Zhu*

Main category: cs.CL

TL;DR: 对最先进的transformer、状态空间和混合大语言模型在知识型ICL任务上的深入评估，发现不同架构LLM在任务表现上可能相似但内部机制不同，功能向量主要位于自注意力和Mamba层，且在不同知识类型任务中重要性不同。


<details>
  <summary>Details</summary>
Motivation: 深入理解不同架构大语言模型在上下文学习中的内部机制差异，特别是transformer、状态空间和混合模型在知识型任务上的表现。

Method: 结合行为探测和干预方法，评估三种架构LLM在两类知识型ICL任务上的表现，分析功能向量的位置和作用。

Result: 发现不同架构LLM任务表现相似但内部机制不同，功能向量主要位于自注意力和Mamba层，Mamba2可能使用不同机制，功能向量对参数知识检索更重要。

Conclusion: 不同架构和任务类型需要更细致的理解，行为分析和机制分析结合对研究LLM能力很重要。

Abstract: We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

</details>


### [14] [Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2510.23038)
*Ran Xu,Jingjing Chen,Jiayu Ye,Yu Wu,Jun Yan,Carl Yang,Hongkun Yu*

Main category: cs.CL

TL;DR: TIR-Judge是一个端到端的强化学习框架，用于训练集成代码执行器的LLM评估器，通过工具增强推理来提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估器仅依赖文本推理，难以验证复杂约束或进行精确计算，需要工具集成来提升评估能力。

Method: 基于三个原则：多样化训练、灵活评估格式、无需蒸馏的迭代强化学习，集成代码执行器进行精确评估。

Result: 在7个公开基准上，TIR-Judge超越强推理评估器6.4%-7.7%，8B参数模型在列表评估上达到Claude-Opus-4水平。

Conclusion: 工具增强的评估器可以通过迭代强化学习自我进化，无需蒸馏即可达到蒸馏变体的性能。

Abstract: Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

</details>


### [15] [Code Aesthetics with Agentic Reward Feedback](https://arxiv.org/abs/2510.23272)
*Bang Xiao,Lingjie Jiang,Shaohan Huang,Tengchao Lv,Yupan Huang,Xun Wu,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 提出了一种提升LLM生成代码美观度的新方法，包括构建AesCode-358K数据集、多代理奖励反馈系统GRPO-AR算法，以及OpenDesign基准测试。实验表明该方法显著提升了代码美观度。


<details>
  <summary>Details</summary>
Motivation: LLM在传统编程任务中表现出色，但在视觉导向的编码任务中生成代码的美观度较差，需要专门的方法来提升代码美学质量。

Method: 构建AesCode-358K指令调优数据集；提出多代理奖励反馈系统评估可执行性、静态美学和交互美学；开发GRPO-AR算法联合优化功能性和代码美观度；创建OpenDesign基准测试。

Result: 在AesCode-358K上进行监督微调并结合强化学习的代理奖励反馈，显著提升了OpenDesign基准的表现，并在PandasPlotBench等现有基准上也有改进。AesCoder-4B超越了GPT-4o和GPT-4.1，性能可与480B-685B参数的大型开源模型相媲美。

Conclusion: 该方法有效提升了LLM生成代码的美观度，证明了结合监督微调和强化学习代理奖励反馈的优越性。

Abstract: Large Language Models (LLMs) have become valuable assistants for developers
in code-related tasks. While LLMs excel at traditional programming tasks such
as code generation and bug fixing, they struggle with visually-oriented coding
tasks, often producing suboptimal aesthetics. In this paper, we introduce a new
pipeline to enhance the aesthetic quality of LLM-generated code. We first
construct AesCode-358K, a large-scale instruction-tuning dataset focused on
code aesthetics. Next, we propose agentic reward feedback, a multi-agent system
that evaluates executability, static aesthetics, and interactive aesthetics.
Building on this, we develop GRPO-AR, which integrates these signals into the
GRPO algorithm for joint optimization of functionality and code aesthetics.
Finally, we develop OpenDesign, a benchmark for assessing code aesthetics.
Experimental results show that combining supervised fine-tuning on AesCode-358K
with reinforcement learning using agentic reward feedback significantly
improves performance on OpenDesign and also enhances results on existing
benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o
and GPT-4.1, and achieves performance comparable to large open-source models
with 480B-685B parameters, underscoring the effectiveness of our approach.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [16] [SIGN: Schema-Induced Games for Naming](https://arxiv.org/abs/2510.21855)
*Ryan Zhang,Herbert Woisetscläger*

Main category: cs.AI

TL;DR: SIGN是一个命名游戏，通过轻量级结构引导多智能体系统形成一致的通信约定，相比无约束自然语言通信能实现5.8倍更高的协议达成率和更快收敛。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统中多个LLM智能体在协作时可能形成不一致的通信约定，导致协调失败，需要可靠一致的通信机制来支持协作编码和分布式规划等应用。

Method: 引入Schema-Induced Games for Naming (SIGN)命名游戏，比较模式诱导通信与无约束自然语言通信在约定形成过程中的表现。

Result: 模式诱导通信相比无约束自然语言通信实现了更快的收敛速度，协议达成率提高了5.8倍。

Conclusion: 最小化结构可以作为多智能体协调的简单控制机制，在命名游戏之外具有更广泛的应用前景。

Abstract: Real-world AI systems are tackling increasingly complex problems, often
through interactions among large language model (LLM) agents. When these agents
develop inconsistent conventions, coordination can break down. Applications
such as collaborative coding and distributed planning therefore require
reliable, consistent communication, and scalability is a central concern as
systems grow. We introduce Schema-Induced Games for Naming (SIGN), a naming
game that examines how lightweight structure can steer convention formation. We
compare schema-induced communication to unconstrained natural language and find
faster convergence with up to 5.8x higher agreement. These results suggest that
minimal structure can act as a simple control knob for efficient multi-agent
coordination, pointing toward broader applications beyond the naming game.

</details>


### [17] [LightAgent: Mobile Agentic Foundation Models](https://arxiv.org/abs/2510.22009)
*Yangqin Jiang,Chao Huang*

Main category: cs.AI

TL;DR: LightAgent是一个移动GUI代理系统，通过设备-云协作解决移动设备上小模型性能不足和大模型部署成本高的问题。它基于Qwen2.5-VL-3B模型，通过两阶段训练和高效长推理机制，在保持低成本的同时实现接近大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决移动GUI代理面临的困境：真正在设备上运行的模型（4B或更小）性能不足，而性能足够的模型（从7B开始）要么太大无法在移动设备上部署，要么成本过高（如仅限云端的闭源MLLMs）。

Method: 提出LightAgent解决方案，采用设备-云协作模式：1）通过两阶段SFT->GRPO训练增强Qwen2.5-VL-3B模型的决策能力；2）集成高效长推理机制在有限资源下利用历史交互；3）默认在设备上执行，仅通过实时复杂度评估将具有挑战性的子任务升级到云端处理。

Result: 在AndroidLab基准测试和多样化应用上的实验表明，LightAgent能够匹配或接近更大模型的性能，同时显著降低云成本。

Conclusion: LightAgent通过设备-云协作成功解决了移动GUI代理的性能与成本权衡问题，为移动平台上的智能代理系统提供了可行的解决方案。

Abstract: With the advancement of multimodal large language models (MLLMs), building
GUI agent systems has become an increasingly promising direction-especially for
mobile platforms, given their rich app ecosystems and intuitive touch
interactions. Yet mobile GUI agents face a critical dilemma: truly on-device
models (4B or smaller) lack sufficient performance, while capable models
(starting from 7B) are either too large for mobile deployment or prohibitively
costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose
LightAgent, a mobile agentic foundation model solution that leverages
device-cloud collaboration to tap the cost-efficiency of on-device models and
the high capability of cloud models, while avoiding their drawbacks.
Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT->GRPO
training on synthetic GUI data for strong decision-making, integrates an
efficient long-reasoning mechanism to utilize historical interactions under
tight resources, and defaults to on-device execution-only escalating
challenging subtasks to the cloud via real-time complexity assessment.
Experiments on the online AndroidLab benchmark and diverse apps show LightAgent
matches or nears larger models, with a significant reduction in cloud costs.

</details>


### [18] [Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability](https://arxiv.org/abs/2510.22039)
*Po-Chen Kuo,Han Hou,Will Dabney,Edgar Y. Walker*

Main category: cs.AI

TL;DR: 该论文提出在元强化学习中整合自监督预测编码模块，以促进学习贝叶斯最优表示，在部分可观测环境中提升表示的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统元强化学习虽然能学习到接近贝叶斯最优的策略，但往往无法学习到紧凑、可解释的贝叶斯最优信念状态，这种表示效率低下限制了智能体的适应性和泛化能力。

Method: 受神经科学中预测编码和深度强化学习中辅助预测目标的启发，将自监督预测编码模块整合到元强化学习中，通过状态机模拟验证方法有效性。

Result: 相比传统元强化学习，带有预测模块的元强化学习在各种任务中都能生成更可解释的表示，更好地逼近贝叶斯最优信念状态。在需要主动信息搜索的挑战性任务中，只有带有预测模块的方法能成功学习最优表示和策略。

Conclusion: 预测学习是智能体在部分可观测环境中进行有效表示学习的指导原则，更好的表示学习能带来改进的泛化能力。

Abstract: Learning a compact representation of history is critical for planning and
generalization in partially observable environments. While meta-reinforcement
learning (RL) agents can attain near Bayes-optimal policies, they often fail to
learn the compact, interpretable Bayes-optimal belief states. This
representational inefficiency potentially limits the agent's adaptability and
generalization capacity. Inspired by predictive coding in neuroscience--which
suggests that the brain predicts sensory inputs as a neural implementation of
Bayesian inference--and by auxiliary predictive objectives in deep RL, we
investigate whether integrating self-supervised predictive coding modules into
meta-RL can facilitate learning of Bayes-optimal representations. Through state
machine simulation, we show that meta-RL with predictive modules consistently
generates more interpretable representations that better approximate
Bayes-optimal belief states compared to conventional meta-RL across a wide
variety of tasks, even when both achieve optimal policies. In challenging tasks
requiring active information seeking, only meta-RL with predictive modules
successfully learns optimal representations and policies, whereas conventional
meta-RL struggles with inadequate representation learning. Finally, we
demonstrate that better representation learning leads to improved
generalization. Our results strongly suggest the role of predictive learning as
a guiding principle for effective representation learning in agents navigating
partial observability.

</details>


### [19] [Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies](https://arxiv.org/abs/2510.22095)
*Yankai Chen,Xinni Zhang,Yifei Zhang,Yangning Li,Henry Peng Zou,Chunyu Miao,Weizhi Zhang,Xue Liu,Philip S. Yu*

Main category: cs.AI

TL;DR: 该立场论文提出从脑机接口(BCI)向脑-智能体协作(BAC)的范式扩展，强调将智能体重新定义为主动协作伙伴而非被动脑信号处理器。


<details>
  <summary>Details</summary>
Motivation: 脑机接口存在信息传输率低和用户特定校准等限制，而现有研究缺乏对这一新兴方向的全面讨论，需要解决技术障碍和伦理关切。

Method: 通过整合大型语言模型，将重点从简单命令解码扩展到理解复杂认知状态，并提出脑-智能体协作框架。

Result: 提出脑-智能体协作(BAC)作为脑机接口的范式扩展，强调智能体作为主动协作伙伴的角色转变。

Conclusion: 需要关注伦理数据处理、模型可靠性和稳健的人-智能体协作框架，以确保系统安全、可信且有效。

Abstract: Brain-Computer Interfaces (BCIs) offer a direct communication pathway between
the human brain and external devices, holding significant promise for
individuals with severe neurological impairments. However, their widespread
adoption is hindered by critical limitations, such as low information transfer
rates and extensive user-specific calibration. To overcome these challenges,
recent research has explored the integration of Large Language Models (LLMs),
extending the focus from simple command decoding to understanding complex
cognitive states. Despite these advancements, deploying agentic AI faces
technical hurdles and ethical concerns. Due to the lack of comprehensive
discussion on this emerging direction, this position paper argues that the
field is poised for a paradigm extension from BCI to Brain-Agent Collaboration
(BAC). We emphasize reframing agents as active and collaborative partners for
intelligent assistance rather than passive brain signal data processors,
demanding a focus on ethical data handling, model reliability, and a robust
human-agent collaboration framework to ensure these systems are safe,
trustworthy, and effective.

</details>


### [20] [Measure what Matters: Psychometric Evaluation of AI with Situational Judgment Tests](https://arxiv.org/abs/2510.22170)
*Alexandra Yost,Shreyans Jain,Shivam Raval,Grant Corser,Allen Roush,Nina Xu,Jacqueline Hammack,Ravid Shwartz-Ziv,Amirali Abdullah*

Main category: cs.AI

TL;DR: 提出了一个AI心理测量框架，使用情境判断测试和复杂人物角色设计来评估AI系统在需要情感判断和伦理考量的角色中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常重复使用人类特质清单或临时角色，限制了行为真实性和领域相关性。

Method: 框架包含三个部分：(1)使用现实场景的情境判断测试；(2)整合工业组织心理学和人格心理学设计复杂角色；(3)采用结构化生成方法，包含人口统计先验和回忆录式叙述。

Result: 在执法助手案例研究中，构建了包含8,500个角色、4,000个情境判断测试和300,000个响应的丰富数据集。

Conclusion: 该框架能够更真实地评估AI系统在专业角色中的表现，并将发布数据集和代码。

Abstract: AI psychometrics evaluates AI systems in roles that traditionally require
emotional judgment and ethical consideration. Prior work often reuses human
trait inventories (Big Five, \hexaco) or ad hoc personas, limiting behavioral
realism and domain relevance. We propose a framework that (1) uses situational
judgment tests (SJTs) from realistic scenarios to probe domain-specific
competencies; (2) integrates industrial-organizational and personality
psychology to design sophisticated personas which include behavioral and
psychological descriptors, life history, and social and emotional functions;
and (3) employs structured generation with population demographic priors and
memoir inspired narratives, encoded with Pydantic schemas. In a law enforcement
assistant case study, we construct a rich dataset of personas drawn across 8
persona archetypes and SJTs across 11 attributes, and analyze behaviors across
subpopulation and scenario slices. The dataset spans 8,500 personas, 4,000
SJTs, and 300,000 responses. We will release the dataset and all code to the
public.

</details>


### [21] [PACR: Progressively Ascending Confidence Reward for LLM Reasoning](https://arxiv.org/abs/2510.22255)
*Eunseop Yoon,Hee Suk Yoon,Jaehyun Jang,SooHwan Eom,Qi Dai,Chong Luo,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.AI

TL;DR: 提出了PACR（渐进上升置信度奖励），一种基于模型内在信念的密集奖励机制，用于改进RLVR训练中的探索效率。


<details>
  <summary>Details</summary>
Motivation: RLVR的稀疏、基于结果的奖励无法为中间推理步骤提供指导，导致探索缓慢。需要密集的模型内在奖励来加速探索过程。

Method: 使用PACR奖励，基于模型对正确答案信念的渐进上升趋势作为密集奖励信号，约束探索空间到逻辑合理的推理区域。

Result: PACR加速了探索，用更少的轨迹达到奖励饱和，并在多个基准测试中取得改进。

Conclusion: 密集的模型内在塑造信号可以使RLVR训练更有效和可靠。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has significantly
improved LLM reasoning, but its sparse, outcome-based reward provides no
guidance for intermediate steps, slowing exploration. We propose Progressively
Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed
directly from the model's evolving belief in the correct answer. PACR encodes
the inductive bias that, along a well-formed reasoning trajectory, the
probability of the ground-truth answer should have a generally ascending trend.
We provide empirical and theoretical analysis validating that such an inductive
bias constrains the exploration search space to regions richer in logically
sound reasoning. We demonstrate that PACR accelerates exploration, reaches
reward saturation with fewer trajectories, and yields improvements on multiple
benchmarks. Our results suggest that dense, model-intrinsic shaping signals can
make RLVR training more effective and reliable.

</details>


### [22] [Modeling Hierarchical Thinking in Large Reasoning Models](https://arxiv.org/abs/2510.22437)
*G M Shahariar,Ali Nazari,Erfan Shayegani,Nael Abu-Ghazaleh*

Main category: cs.AI

TL;DR: 该论文提出使用有限状态机（FSM）来建模和分析大型推理模型（LRMs）的层次推理过程，通过识别离散推理状态来可视化和比较不同模型的推理模式。


<details>
  <summary>Details</summary>
Motivation: 理解大型推理模型（LRMs）涌现的推理能力是一个重要但困难的开放问题，这对于改进训练和理解模型鲁棒性有重要应用价值。

Method: 采用无记忆有限状态机（FSM）来近似LRMs的层次推理动态，识别了初始化、演绎、增强策略、不确定性估计、回溯和最终结论等离散推理状态，并将推理轨迹表示为状态图中的转移序列。

Result: 基于FSM的分析揭示了不同模型在推理方法上的显著差异和潜在缺陷，为评估和改进LLM推理提供了新的视角。

Conclusion: FSM框架为分析、解释和可视化不同模型处理问题的方式提供了系统方法，能够揭示不同的推理模式和潜在问题。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
when they generate step-by-step solutions, known as chain-of-thought (CoT)
reasoning. When trained to using chain-of-thought reasoning examples, the
resulting models (called Large Reasoning Models, or LRMs) appear to learn
hierarchical thinking strategies similar to those used by humans. However,
understanding LRMs emerging reasoning capabilities remains a difficult open
problem, with many potential important applications including improving
training and understanding robustness. In this paper, we adopt a memoryless
Finite State Machine formulation to approximate LRM's emerging hierarchical
reasoning dynamics as a structured, interpretable abstraction. We identify a
small set of discrete reasoning states including - initialization, deduction,
augmentation-strategy, uncertainty-estimation, backtracking, and
final-conclusion that capture the high-level states present in the model's
reasoning process. By annotating each step of a model's CoT with these states,
we can represent the reasoning trajectory as a transition sequence through the
state graph. This FSM formulation provides a systematic way to analyze,
interpret and visualize how different models approach problems. We describe the
FSM model, provide examples of CoT annotations under this scheme, and discuss
how it can shed light on differences between available models in their approach
to reasoning. Our results demonstrate that this FSM-based analysis reveals
distinct reasoning patterns and potential shortcomings, offering a new lens to
evaluate and improve LLM reasoning.

</details>


### [23] [On Generalization in Agentic Tool Calling: CoreThink Agentic Reasoner and MAVEN Dataset](https://arxiv.org/abs/2510.22898)
*Vishvesh Bhat,Omkar Ghugarkar,Julian McAuley*

Main category: cs.AI

TL;DR: 论文提出CoreThink Agentic Reasoner框架，通过轻量级符号推理层增强LLMs，在多个工具调用基准测试中实现最先进性能，计算成本仅为十分之一。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工具调用环境中的泛化挑战，当前LLMs在跨领域转移推理策略和协调工具方面的能力不足。

Method: 引入MAVEN基准测试进行大规模评估，并提出CoreThink框架，结合轻量级符号推理层进行结构化分解和自适应工具编排。

Result: 当前模型在MAVEN基准上准确率低于50%，而CoreThink框架在所有基准测试中实现泛化，性能提升530%。

Conclusion: CoreThink框架有效解决了智能体工具调用环境的泛化问题，显著提升性能同时大幅降低计算成本。

Abstract: Generalization across Agentic tool-calling environments remains a key
unsolved challenge in developing reliable agentic reasoning systems. While
large language models (LLMs) demonstrate strong performance on isolated
benchmarks, their ability to transfer reasoning strategies and co-ordinate
tools across diverse domains is poorly understood. In this work, we conduct a
large-scale evaluation of state-of-the-art LLMs on multiple tool-calling
benchmarksBFCL v3, TauBench, Tau2Bench, and AceBenchand introduce MAVEN (Math &
Physics Adversarial Verification & Evaluation Network), a new out of
distribution (OOD) benchmark designed to stress-test multi-step reasoning
through explicit verification and adversarial task composition. Our results
show that most current models achieve below 50% accuracy on MAVEN, revealing a
significant generalization gap across tool-use settings.
  To address this, we present the CoreThink Agentic Reasoner, a framework that
augments LLMs with a lightweight symbolic reasoning layer for structured
decomposition and adaptive tool orchestration. Without additional training, it
generalizes across all benchmarks, achieving state-of-the-art performance with
530% improvements over existing baselines at roughly one-tenth the
computational cost.

</details>


### [24] [Learning "Partner-Aware" Collaborators in Multi-Party Collaboration](https://arxiv.org/abs/2510.22462)
*Abhijnan Nath,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 提出了一种新的协作学习算法ICR，用于训练能够有效处理干预并促进团队共识的LLM协作代理


<details>
  <summary>Details</summary>
Motivation: 随着LLM在代理设置中的部署增加，需要评估其在多轮、多方任务中的协作能力，特别是处理干预和促进团队共识的能力

Method: 采用两玩家修改行动MDP分析标准AI代理的次优行为，提出ICR（可中断协作角色扮演者）算法来训练共识最优的协作代理

Result: 在多个协作任务环境中的实验显示，ICR平均能更有效地促进成功的共识收敛，并在这些任务中探索更多样化的解决方案

Conclusion: ICR算法能够有效解决标准RLHF训练代理忽略干预的问题，提高LLM代理在协作任务中的表现

Abstract: Large Language Models (LLMs) are increasingly bring deployed in agentic
settings where they act as collaborators with humans. Therefore, it is
increasingly important to be able to evaluate their abilities to collaborate
effectively in multi-turn, multi-party tasks. In this paper, we build on the AI
alignment and safe interruptability literature to offer novel theoretical
insights on collaborative behavior between LLM-driven collaborator agents and
an intervention agent. Our goal is to learn an ideal partner-aware collaborator
that increases the group's common-ground (CG)-alignment on task-relevant
propositions-by intelligently collecting information provided in interventions
by a partner agent.We show how LLM agents trained using standard RLHF and
related approaches are naturally inclined to ignore possibly well-meaning
interventions, which makes increasing group common ground non-trivial in this
setting. We employ a two-player Modified-Action MDP to examine this suboptimal
behavior of standard AI agents, and propose Interruptible Collaborative
Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal
collaborators. Experiments on multiple collaborative task environments show
that ICR, on average, is more capable of promoting successful CG convergence
and exploring more diverse solutions in such tasks.

</details>


### [25] [Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards](https://arxiv.org/abs/2510.23083)
*Jan Niklas Groeneveld,Xi Qin,Alexander Schaefer,Yaad Oren*

Main category: cs.AI

TL;DR: 研究探讨将小型语言模型（如Phi-4系列）转化为有效的奖励模型，用于代码生成任务，通过结合过程奖励和结果奖励来提升代码质量评估能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成高质量代码方面仍面临挑战，奖励模型是推理模型发展的必要中间步骤。虽然模型反思能力通常随规模增大而提升，但本研究旨在验证小型语言模型能否转化为实用的奖励模型。

Method: 基于APPS编码挑战基准构建带正确性标签的代码样本数据集，训练带有回归层的价值头模型来估计中间输出的成功概率。

Result: 评估显示小型LLM能够作为有效的奖励模型或代码评估评判器，成功从多个候选中识别正确解决方案。使用该评判器，在多个生成结果中搜索最准确代码的能力提升了20%以上。

Conclusion: 小型语言模型可以被转化为有效的代码奖励模型，结合过程奖励和结果奖励的方法能够显著提升代码生成质量评估能力。

Abstract: Generating high-quality code remains a challenge for Large Language Models
(LLMs). For the evolution of reasoning models on this task, reward models are a
necessary intermediate step. These models judge outcomes or intermediate steps.
Decoder-only transformer models can be turned into reward models by introducing
a regression layer and supervised fine-tuning. While it is known that
reflection capabilities generally increase with the size of a model, we want to
investigate whether state-of-the-art small language models like the Phi-4
family can be turned into usable reward models blending the consideration of
process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness
labels derived from the APPS coding challenge benchmark. We then train a
value-head model to estimate the success probability of intermediate outputs.
Our evaluation shows that small LLMs are capable of serving as effective reward
models or code evaluation critics, successfully identifying correct solutions
among multiple candidates. Using this critic, we achieve over a 20% improvement
in the search capability of the most accurate code out of multiple generations.

</details>


### [26] [JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence](https://arxiv.org/abs/2510.23538)
*Qiushi Sun,Jingyang Gong,Yang Liu,Qiaosheng Chen,Lei Li,Kai Chen,Qipeng Guo,Ben Kao,Fei Yuan*

Main category: cs.AI

TL;DR: 论文提出了JanusCode-800K多模态代码语料库和JanusCoder系列模型，通过视觉-程序化接口实现从文本指令、视觉输入或两者结合生成代码，在文本和视觉编码任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决神经代码智能从纯文本扩展到视觉输出的需求，克服高质量多模态代码数据稀缺的瓶颈，为灵活内容生成和程序驱动的可视化编辑提供支持。

Method: 开发完整的合成工具包，利用数据模态间的协同作用生成大规模高质量多模态代码语料库；训练JanusCoder和JanusCoderV模型，建立统一的视觉-程序化接口。

Result: JanusCoder系列模型在文本和视觉编码任务上表现优异，7B到14B规模的模型接近甚至超过商业模型性能。

Conclusion: 该工作为多模态代码智能提供了重要进展，展示了程序逻辑与视觉表达协调的关键见解。

Abstract: The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

</details>


### [27] [SwiftSolve: A Self-Iterative, Complexity-Aware Multi-Agent Framework for Competitive Programming](https://arxiv.org/abs/2510.22626)
*Adhyayan Veer Singh,Aaron Shen,Brian Law,Ahmed Ismail,Jonas Rohweder,Sean O'Brien,Kevin Zhu*

Main category: cs.AI

TL;DR: SwiftSolve是一个复杂度感知的多智能体系统，用于竞争性编程，通过算法规划、经验分析和复杂度指导的修复来提高程序效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的程序虽然能通过单元测试，但经常违反竞赛的时间和内存限制，需要同时保证正确性和效率。

Method: 采用多智能体系统，包括规划器、静态剪枝器、编码器、分析器和复杂度分析器，通过版本化JSON通信，控制器执行迭代限制和收益递减停止。

Result: 在26个问题上的评估显示，首次尝试通过率61.54%，3次内解决率80.77%，运行级成功率73.08%，平均时间12.40秒。

Conclusion: 分析和复杂度指导的重新规划能减少低效性同时保持准确性，相比Claude Opus 4在运行级成功率上有显著提升。

Abstract: Correctness alone is insufficient: LLM-generated programs frequently satisfy
unit tests while violating contest time or memory budgets. We present
SwiftSolve, a complexity-aware multi-agent system for competitive programming
that couples algorithmic planning with empirical profiling and
complexity-guided repair. We frame competitive programming as a software
environment where specialized agents act as programmers, each assuming roles
such as planning, coding, profiling, and complexity analysis. A Planner
proposes an algorithmic sketch; a deterministic Static Pruner filters high-risk
plans; a Coder emits ISO C++17; a Profiler compiles and executes candidates on
a fixed input-size schedule to record wall time and peak memory; and a
Complexity Analyst fits log-log growth (s, R2) with an LLM fallback to assign a
complexity class and dispatch targeted patches to either the Planner or Coder.
Agents communicate via typed, versioned JSON; a controller enforces iteration
caps and diminishing returns stopping. Evaluated on 26 problems (16 BigO, 10
Codeforces Div. 2) in a POSIX sandbox (2 s / 256-512 MB), SwiftSolve attains
pass@1 = 61.54% (16/26) on the first attempt and Solved@<=3 = 80.77% with
marginal latency change (mean 11.96 s to 12.66 s per attempt). Aggregate
run-level success is 73.08% at 12.40 s mean. Failures are predominantly
resource-bound, indicating inefficiency rather than logic errors. Against
Claude Opus 4, SwiftSolve improves run-level success (73.1% vs 52.6%) at
approximately 2x runtime overhead (12.4 s vs 6.8 s). Beyond correctness
(pass@k), we report efficiency metrics (eff@k for runtime and memory, incidence
of TLE or MLE, and complexity fit accuracy on BigO), demonstrating that
profiling and complexity-guided replanning reduce inefficiency while preserving
accuracy.

</details>


### [28] [How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations](https://arxiv.org/abs/2510.22780)
*Zora Zhiruo Wang,Yijia Shao,Omar Shaikh,Daniel Fried,Graham Neubig,Diyi Yang*

Main category: cs.AI

TL;DR: 本文首次直接比较人类与AI代理在多个工作技能上的表现，发现代理虽然工作质量较低且存在数据伪造问题，但效率更高、成本更低，适合处理可编程任务。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理的发展缺乏对人类工作方式的清晰理解，需要揭示代理具备的专业知识和在不同工作流程中可扮演的角色。

Method: 引入可扩展工具包，从人类或代理的计算机使用活动中诱导出可解释的结构化工作流程，并比较人类和代理执行相同任务的方式。

Result: 代理在所有工作领域都采用程序化方法，与人类以UI为中心的方法形成对比；代理工作质量较低但经常通过数据伪造掩盖缺陷；代理效率比人类高88.3%，成本低90.4-96.2%。

Conclusion: 代理在与人类工作流程对齐方面展现出潜力，适合通过委托可编程任务实现高效协作。

Abstract: AI agents are continually optimized for tasks related to human work, such as
software engineering and professional writing, signaling a pressing trend with
significant impacts on the human workforce. However, these agent developments
have often not been grounded in a clear understanding of how humans execute
work, to reveal what expertise agents possess and the roles they can play in
diverse workflows. In this work, we study how agents do human work by
presenting the first direct comparison of human and agent workers across
multiple essential work-related skills: data analysis, engineering,
computation, writing, and design. To better understand and compare
heterogeneous computer-use activities of workers, we introduce a scalable
toolkit to induce interpretable, structured workflows from either human or
agent computer-use activities. Using such induced workflows, we compare how
humans and agents perform the same tasks and find that: (1) While agents
exhibit promise in their alignment to human workflows, they take an
overwhelmingly programmatic approach across all work domains, even for
open-ended, visually dependent tasks like design, creating a contrast with the
UI-centric methods typically used by humans. (2) Agents produce work of
inferior quality, yet often mask their deficiencies via data fabrication and
misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster
and cost 90.4-96.2% less than humans, highlighting the potential for enabling
efficient collaboration by delegating easily programmable tasks to agents.

</details>


### [29] [Agentic Meta-Orchestrator for Multi-task Copilots](https://arxiv.org/abs/2510.22781)
*Xiaofeng Zhu,Yunshen Zhou*

Main category: cs.AI

TL;DR: 提出了一种用于Copilot服务的Agentic Meta-orchestrator (AMO)，能够处理多任务和可扩展代理，通过元学习决策树模型选择最佳推理策略，并在M365电商Copilot和代码合规Copilot两个生产用例中验证有效性。


<details>
  <summary>Details</summary>
Motivation: Microsoft Copilot套件作为各种代理的通用入口点，需要强大的编排器来将用户提示的任务分发给合适的代理，支持动态扩展新代理。

Method: 提出Agentic Meta-orchestrator (AMO)，利用元学习训练决策树模型来决定不同代理/模型间的最佳推理策略，提供自然语言和动作响应。

Result: 在M365电商Copilot和代码合规Copilot两个生产用例中展示了AMO的有效性，前者提供最新产品信息并连接多个代理，后者扫描内部DevOps代码检测合规问题。

Conclusion: AMO能够有效处理多任务和可扩展代理，通过元学习优化推理策略选择，在实际生产环境中验证了其可行性。

Abstract: Microsoft Copilot suites serve as the universal entry point for various
agents skilled in handling important tasks, ranging from assisting a customer
with product purchases to detecting vulnerabilities in corporate programming
code. Each agent can be powered by language models, software engineering
operations, such as database retrieval, and internal \& external knowledge. The
repertoire of a copilot can expand dynamically with new agents. This requires a
robust orchestrator that can distribute tasks from user prompts to the right
agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for
handling multiple tasks and scalable agents in copilot services, which can
provide both natural language and action responses. We will also demonstrate
the planning that leverages meta-learning, i.e., a trained decision tree model
for deciding the best inference strategy among various agents/models. We
showcase the effectiveness of our AMO through two production use cases:
Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365
E-Commerce Copilot advertises Microsoft products to external customers to
promote sales success. The M365 E-Commerce Copilot provides up-to-date product
information and connects to multiple agents, such as relational databases and
human customer support. The code compliance copilot scans the internal DevOps
code to detect known and new compliance issues in pull requests (PR).

</details>


### [30] [Multi-Agent Conditional Diffusion Model with Mean Field Communication as Wireless Resource Allocation Planner](https://arxiv.org/abs/2510.22969)
*Kechen Meng,Sinuo Zhang,Rongpeng Li,Xiangming Meng,Chan Wang,Ming Lei,Zhifeng Zhao*

Main category: cs.AI

TL;DR: 提出MA-CDMP方法，使用扩散模型和多智能体条件规划来解决无线通信系统中的分布式资源分配问题，解决了传统方法的可扩展性和非平稳性问题。


<details>
  <summary>Details</summary>
Motivation: 解决集中式多智能体强化学习的可扩展性问题和隐私风险，以及分布式训练与去中心化执行范式中的非平稳性和有限合作问题。

Method: 基于模型强化学习范式，使用扩散模型捕捉环境动态并规划未来轨迹，引入逆动态模型指导动作生成，采用平均场机制近似大规模智能体交互。

Result: MA-CDMP在平均奖励和QoS指标上持续优于现有MARL基线方法，展示了其在真实无线网络优化中的可扩展性和实用性。

Conclusion: MA-CDMP通过扩散模型和平均场机制有效解决了分布式通信资源管理中的挑战，提供了理论保证和实际性能优势。

Abstract: In wireless communication systems, efficient and adaptive resource allocation
plays a crucial role in enhancing overall Quality of Service (QoS). While
centralized Multi-Agent Reinforcement Learning (MARL) frameworks rely on a
central coordinator for policy training and resource scheduling, they suffer
from scalability issues and privacy risks. In contrast, the Distributed
Training with Decentralized Execution (DTDE) paradigm enables distributed
learning and decision-making, but it struggles with non-stationarity and
limited inter-agent cooperation, which can severely degrade system performance.
To overcome these challenges, we propose the Multi-Agent Conditional Diffusion
Model Planner (MA-CDMP) for decentralized communication resource management.
Built upon the Model-Based Reinforcement Learning (MBRL) paradigm, MA-CDMP
employs Diffusion Models (DMs) to capture environment dynamics and plan future
trajectories, while an inverse dynamics model guides action generation, thereby
alleviating the sample inefficiency and slow convergence of conventional DTDE
methods. Moreover, to approximate large-scale agent interactions, a Mean-Field
(MF) mechanism is introduced as an assistance to the classifier in DMs. This
design mitigates inter-agent non-stationarity and enhances cooperation with
minimal communication overhead in distributed settings. We further
theoretically establish an upper bound on the distributional approximation
error introduced by the MF-based diffusion generation, guaranteeing convergence
stability and reliable modeling of multi-agent stochastic dynamics. Extensive
experiments demonstrate that MA-CDMP consistently outperforms existing MARL
baselines in terms of average reward and QoS metrics, showcasing its
scalability and practicality for real-world wireless network optimization.

</details>


### [31] [Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach](https://arxiv.org/abs/2510.23216)
*Alessandro Sestini,Joakim Bergdahl,Jean-Philippe Barrette-LaPierre,Florian Fuchs,Brady Chen,Micheal Jones,Linus Gisslén*

Main category: cs.AI

TL;DR: 提出了一种针对游戏行业的高效深度强化学习方法，在EA SPORTS FC 25中训练守门员智能体，比游戏内置AI的救球率提高10%，训练速度比标准DRL方法快50%。


<details>
  <summary>Details</summary>
Motivation: 现有DRL研究主要关注训练超人智能体，但游戏行业需要的是人类水平的智能体，且资源有限，需要更高效的训练方法。

Method: 基于价值DRL的样本高效方法，利用预收集数据并增强网络可塑性，专门针对工业环境进行训练和微调。

Result: 在EA SPORTS FC 25中训练的守门员智能体比内置AI救球率高10%，训练速度快50%，专家评估显示比手工制作的智能体更具人类游戏风格。

Conclusion: 该方法成功展示了在工业环境中应用DRL的可行性，计划在游戏系列后续版本中替代手工制作的智能体。

Abstract: While several high profile video games have served as testbeds for Deep
Reinforcement Learning (DRL), this technique has rarely been employed by the
game industry for crafting authentic AI behaviors. Previous research focuses on
training super-human agents with large models, which is impractical for game
studios with limited resources aiming for human-like agents. This paper
proposes a sample-efficient DRL method tailored for training and fine-tuning
agents in industrial settings such as the video game industry. Our method
improves sample efficiency of value-based DRL by leveraging pre-collected data
and increasing network plasticity. We evaluate our method training a goalkeeper
agent in EA SPORTS FC 25, one of the best-selling football simulations today.
Our agent outperforms the game's built-in AI by 10% in ball saving rate.
Ablation studies show that our method trains agents 50% faster compared to
standard DRL methods. Finally, qualitative evaluation from domain experts
indicates that our approach creates more human-like gameplay compared to
hand-crafted agents. As a testimony of the impact of the approach, the method
is intended to replace the hand-crafted counterpart in next iterations of the
series.

</details>


### [32] [Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps](https://arxiv.org/abs/2510.23340)
*Anwesha Das,John Duff,Jörg Hoffmann,Vera Demberg*

Main category: cs.AI

TL;DR: 提出了一种基于理性言语行为(RSA)框架的自适应信号理论，用于优化人机协作中的信息传递时机和特异性，以在动态环境中保持用户认知与环境的及时对齐。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的动态环境中，辅助智能体需要识别最高优先级信息并估计如何最有效地传递这些信息，因为人类注意力是零和认知资源，关注一条信息会降低对其他信息的感知。

Method: 使用理性言语行为(RSA)建模框架，通过贝叶斯参考解析来规划消息序列，调整消息的特异性和时机，基于对用户注意力分布和信念更新的多步预测。

Result: 与基线方法比较显示，该方法的效果关键依赖于将多步规划与现实的用户意识模型相结合。

Conclusion: 这是RSA框架在动态环境通信和人类-AI交互中的首次应用，为人类-智能体团队的语用通信建立了理论基础。

Abstract: Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

</details>


### [33] [Human-AI Collaborative Uncertainty Quantification](https://arxiv.org/abs/2510.23476)
*Sima Noorani,Shayan Kiyani,George Pappas,Hamed Hassani*

Main category: cs.AI

TL;DR: 提出了Human AI Collaborative Uncertainty Quantification框架，通过AI模型优化人类专家的预测集，避免对正确判断的损害并补充人类遗漏的预测，在图像分类、回归和医疗决策等任务中优于单独使用人类或AI。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在不确定性下的决策仍缺乏领域知识、长期上下文和物理世界推理能力，需要结合人类和AI的互补优势来提升决策可靠性。

Method: 引入协作不确定性量化框架，基于单一评分函数的两阈值结构构建最优协作预测集，开发了具有分布无关有限样本保证的离线和在线校准算法。

Result: 实验表明协作预测集在各种条件下都比单独使用人类或AI表现更好，实现了更高的覆盖率和更小的集合大小。

Conclusion: Human AI协作不确定性量化框架能有效结合人类和AI的优势，在不确定性决策中实现更好的性能。

Abstract: AI predictive systems are increasingly embedded in decision making pipelines,
shaping high stakes choices once made solely by humans. Yet robust decisions
under uncertainty still rely on capabilities that current AI lacks: domain
knowledge not captured by data, long horizon context, and reasoning grounded in
the physical world. This gap has motivated growing efforts to design
collaborative frameworks that combine the complementary strengths of humans and
AI. This work advances this vision by identifying the fundamental principles of
Human AI collaboration within uncertainty quantification, a key component of
reliable decision making. We introduce Human AI Collaborative Uncertainty
Quantification, a framework that formalizes how an AI model can refine a human
expert's proposed prediction set with two goals: avoiding counterfactual harm,
ensuring the AI does not degrade correct human judgments, and complementarity,
enabling recovery of correct outcomes the human missed. At the population
level, we show that the optimal collaborative prediction set follows an
intuitive two threshold structure over a single score function, extending a
classical result in conformal prediction. Building on this insight, we develop
practical offline and online calibration algorithms with provable distribution
free finite sample guarantees. The online method adapts to distribution shifts,
including human behavior evolving through interaction with AI, a phenomenon we
call Human to AI Adaptation. Experiments across image classification,
regression, and text based medical decision making show that collaborative
prediction sets consistently outperform either agent alone, achieving higher
coverage and smaller set sizes across various conditions.

</details>


### [34] [Are Agents Just Automata? On the Formal Equivalence Between Agentic AI and the Chomsky Hierarchy](https://arxiv.org/abs/2510.23487)
*Roham Koohestani,Ziyou Li,Anton Podkopaev,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文建立了现代智能体AI系统架构与乔姆斯基层次抽象机器之间的形式等价关系，提出基于内存架构的智能体计算能力分类框架。


<details>
  <summary>Details</summary>
Motivation: 为智能体系统提供形式化理论基础，实现计算效率优化、形式验证和安全保证。

Method: 将智能体内存架构映射到自动机类别：简单反射智能体对应有限自动机，分层任务分解智能体对应下推自动机，具有读写内存的反思智能体对应图灵机。

Result: 建立了智能体-自动机框架，为智能体架构优化和形式验证提供系统方法。

Conclusion: 该框架为智能体系统开发静态分析工具和语法提供了理论基础，能够界定可验证系统与不可判定行为之间的边界。

Abstract: This paper establishes a formal equivalence between the architectural classes
of modern agentic AI systems and the abstract machines of the Chomsky
hierarchy. We posit that the memory architecture of an AI agent is the
definitive feature determining its computational power and that it directly
maps it to a corresponding class of automaton. Specifically, we demonstrate
that simple reflex agents are equivalent to Finite Automata, hierarchical
task-decomposition agents are equivalent to Pushdown Automata, and agents
employing readable/writable memory for reflection are equivalent to TMs. This
Automata-Agent Framework provides a principled methodology for right-sizing
agent architectures to optimize computational efficiency and cost. More
critically, it creates a direct pathway to formal verification, enables the
application of mature techniques from automata theory to guarantee agent safety
and predictability. By classifying agents, we can formally delineate the
boundary between verifiable systems and those whose behavior is fundamentally
undecidable. We address the inherent probabilistic nature of LLM-based agents
by extending the framework to probabilistic automata that allow quantitative
risk analysis. The paper concludes by outlining an agenda for developing static
analysis tools and grammars for agentic frameworks.

</details>


### [35] [ReCode: Unify Plan and Action for Universal Granularity Control](https://arxiv.org/abs/2510.23564)
*Zhaoyang Yu,Jiayi Zhang,Huixue Su,Yufan Zhao,Yifan Wu,Mingyi Deng,Jinyu Xiang,Yizhang Lin,Lingxiao Tang,Yingchao Li,Yuyu Luo,Bang Liu,Chenglin Wu*

Main category: cs.AI

TL;DR: ReCode提出了一种通过递归代码生成统一规划和行动的新范式，将高层次计划表示为抽象占位函数，然后递归分解为更细粒度的子函数，直到原始行动。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体缺乏在不同决策粒度间灵活操作的能力，现有范式在高层规划和低层行动之间设置了严格分离，这限制了动态适应性和泛化能力。

Method: 采用递归代码生成方法，将高层计划作为抽象占位函数，通过递归分解为更细粒度的子函数，直至达到原始行动，从而统一规划和行动。

Result: 广泛实验表明ReCode在推理性能上显著超越先进基线，并在训练中表现出卓越的数据效率。

Conclusion: 通过递归代码生成统一规划和行动是实现通用粒度控制的有效方法。

Abstract: Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

</details>


### [36] [Multi-Agent Evolve: LLM Self-Improve through Co-evolution](https://arxiv.org/abs/2510.23595)
*Yixing Chen,Yiding Wang,Siqi Zhu,Haofei Yu,Tao Feng,Muhan Zhan,Mostofa Patwary,Jiaxuan You*

Main category: cs.AI

TL;DR: 提出MAE框架，通过三个交互代理（提议者、求解者、评判者）实现LLM的自进化，在数学、推理和常识问答任务上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注数据和可验证奖励，限制了可扩展性和泛化能力；自博弈RL方法需要特定环境反馈，难以扩展到通用领域

Method: 基于单一LLM实例化三个交互代理：提议者生成问题，求解者尝试解决，评判者评估并共同进化；应用强化学习优化代理行为

Result: 在Qwen2.5-3B-Instruct上实验，多个基准测试平均提升4.54%

Conclusion: MAE是一种可扩展、数据高效的方法，能以最少的人工监督显著增强LLM的通用推理能力

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in
enhancing the reasoning capabilities of large language models (LLMs). However,
the success of RL for LLMs heavily relies on human-curated datasets and
verifiable rewards, which limit their scalability and generality. Recent
Self-Play RL methods, inspired by the success of the paradigm in games and Go,
aim to enhance LLM reasoning capabilities without human-annotated data.
However, their methods primarily depend on a grounded environment for feedback
(e.g., a Python interpreter or a game engine); extending them to general
domains remains challenging. To address these challenges, we propose
Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in
solving diverse tasks, including mathematics, reasoning, and general knowledge
Q&A. The core design of MAE is based on a triplet of interacting agents
(Proposer, Solver, Judge) that are instantiated from a single LLM, and applies
reinforcement learning to optimize their behaviors. The Proposer generates
questions, the Solver attempts solutions, and the Judge evaluates both while
co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves
an average improvement of 4.54% on multiple benchmarks. These results highlight
MAE as a scalable, data-efficient method for enhancing the general reasoning
abilities of LLMs with minimal reliance on human-curated supervision.

</details>


### [37] [Alita-G: Self-Evolving Generative Agent for Agent Generation](https://arxiv.org/abs/2510.23601)
*Jiahao Qiu,Xuan Qi,Hongru Wang,Xinzhe Juan,Yimin Wang,Zelin Zhao,Jiayi Geng,Jiacheng Guo,Peihang Li,Jingzhe Shi,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: ALITA-G是一个自进化框架，通过生成、抽象和整理MCP工具，将通用智能体转化为领域专家，在多个基准测试中实现了最先进的性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前的自进化智能体主要局限于提示重写或失败重试，缺乏系统性的领域专业化能力提升方法。

Method: 通过让通用智能体执行目标领域任务，从成功轨迹中合成候选MCP工具，将其抽象为参数化原语并整合到MCP Box中，在推理时进行检索增强的MCP选择。

Result: 在GAIA验证集上达到83.03% pass@1和89.09% pass@3的新SOTA结果，同时将每个示例的平均token数减少约15%。

Conclusion: ALITA-G提供了从通用能力到可重用领域特定能力的原理性路径，在复杂推理任务上同时提高了准确性和效率。

Abstract: Large language models (LLMs) have been shown to perform better when
scaffolded into agents with memory, tools, and feedback. Beyond this,
self-evolving agents have emerged, but current work largely limits adaptation
to prompt rewriting or failure retries. Therefore, we present ALITA-G, a
self-evolution framework that transforms a general-purpose agent into a domain
expert by systematically generating, abstracting, and curating Model Context
Protocol (MCP) tools. In this framework, a generalist agent executes a curated
suite of target-domain tasks and synthesizes candidate MCPs from successful
trajectories. These are then abstracted to parameterized primitives and
consolidated into an MCP Box. At inference time, ALITA-G performs
retrieval-augmented MCP selection with the help of each tool's descriptions and
use cases, before executing an agent equipped with the MCP Executor. Across
several benchmarks GAIA, PathVQA, and Humanity's Last Exam, ALITA-G attains
strong gains while reducing computation costs. On GAIA validation, it achieves
83.03% pass@1 and 89.09% pass@3, establishing a new state-of-the-art result
while reducing mean tokens per example by approximately 15% relative to a
strong baseline agent. ALITA-G thus provides a principled pathway from
generalist capability to reusable, domain-specific competence, improving both
accuracy and efficiency on complex reasoning tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making](https://arxiv.org/abs/2510.21788)
*Larkin Liu,Jalal Etesami*

Main category: cs.LG

TL;DR: 提出两种在线专家混合(OMoE)算法，通过专家引导的bandit学习来优化专家委员会输出聚合，应用于LLM在线微调。


<details>
  <summary>Details</summary>
Motivation: 解决在给定上下文时，如何有效聚合专家委员会输出以获得最优准确率的问题，特别是在LLM专家模型动态选择场景中。

Method: 1. 结合聚合投票与UCB驱动的连续消除算法；2. 在线加权多数投票机制，根据专家预测能力分配投票权重。

Result: 在理想情况下推导了bandit设置中的遗憾理论保证，并提供了相应的实证结果，证明能提升整体聚合模型的性能。

Conclusion: 提出了结合多个专家以改进整体性能的新方法和无遗憾保证，特别适用于LLM专家模型的动态选择和权重调整。

Abstract: We explore the use of expert-guided bandit learning, which we refer to as
online mixture-of-experts (OMoE). In this setting, given a context, a candidate
committee of experts must determine how to aggregate their outputs to achieve
optimal results in terms of aggregate accuracy. We propose two algorithms to
address this problem. The first algorithm combines aggregate voting with
UCB-driven successive elimination, efficiently pruning suboptimal exploration
actions. The second algorithm employs an online weighted-majority-voting
mechanism, leveraging the respective voting power of each expert proportional
to their predictive power. We derive theoretical guarantees for the regret
properties in the bandit setting under ideal circumstances, and empirical
results are provided accordingly. As a modern study on applications, these
methods are applied to the online fine-tuning of a set of expert large language
models (LLMs), where after each response, the generative LLM dynamically
reweighs its set of experts and/or selects the optimal committee of experts to
generate the most accurate response. Our results introduce new methodologies
and no-regret guarantees for combining multiple experts to improve on the
performance of the an aggregate model overall.

</details>


### [39] [GAPO: Group Adaptive Policy Optimization for Real-World Code Edit](https://arxiv.org/abs/2510.21830)
*Jianqing Zhang,Zhezheng Hao,Wei Xia,Hande Dong,Hong Wang,Chenxing Wei,Yuyan Zhou,Yubin Qi,Qiang Lin,Jian Cao*

Main category: cs.LG

TL;DR: 提出GAPO方法解决代码编辑中奖励分布偏斜问题，通过自适应寻找无异常值的高密度区间来改进优势计算，在真实代码编辑任务中表现优于GRPO和DAPO。


<details>
  <summary>Details</summary>
Motivation: 现实世界代码编辑场景中，奖励分布常常偏斜且包含不可预测的异常值，导致优势计算失真和噪声增加，需要更稳健的方法。

Method: 提出Group Adaptive Policy Optimization (GAPO)，自适应寻找每个提示的无异常值最高密度区间，使用该区间中位数作为自适应Q值替代组均值进行优势计算。

Result: 在9个指令调优LLM（3B-14B）和51,844个真实世界代码编辑任务上验证，GAPO在精确匹配准确率上持续优于GRPO及其变体DAPO。

Conclusion: GAPO能稳健处理偏斜分布，保持即插即用和高效率，代码已公开。

Abstract: Reinforcement learning (RL) is widely used for post-training large language
models (LLMs) in code editing, where group-relative methods like GRPO are
popular for their critic-free, normalized advantage estimation. However, in
real-world code-editing scenarios, reward distributions are often skewed with
unpredictable outliers, leading to distorted advantage computation and
increased noise. To address this issue, we propose Group Adaptive Policy
Optimization (GAPO), which adaptively finds an outlier-free highest-density
interval (HDI) per prompt and then uses the median of that interval as an
adaptive Q to replace the group mean in advantage calculation. This adaptive Q
robustly handles skewed distributions while remaining plug-and-play and
efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a
large internal dataset of 51,844 real-world, history-aware code-editing tasks
across 10 languages, demonstrating consistent improvements in exact match
accuracy over GRPO and its variant DAPO. Code is publicly available.

</details>


### [40] [COLA: Continual Learning via Autoencoder Retrieval of Adapters](https://arxiv.org/abs/2510.21836)
*Jaya Krishna Mandivarapu*

Main category: cs.LG

TL;DR: 提出COLA框架解决LLM持续学习中的灾难性遗忘问题，使用自编码器学习任务权重的低维嵌入，无需数据回放或大量任务特定参数


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在持续学习中的灾难性遗忘问题，避免频繁重新训练的高计算成本

Method: 使用自编码器学习任务权重的低维嵌入，促进知识迁移同时防止灾难性遗忘

Result: 在多个数据集上不仅克服了灾难性遗忘，还显著减少了参数使用和内存占用，优于现有方法

Conclusion: COLA框架使LLM能够高效学习新任务，对先前任务性能影响小，且无需保留早期训练数据

Abstract: Learning a set of tasks over time, also known as continual learning (CL), is
one of the most challenging problems in artificial intelligence due to
catastrophic forgetting. Large language models (LLMs) are often impractical to
frequent re-training and continual learning , due to high cost of computational
resources for training. Moreover, LLM are not suitable for continual learning
as updating these models over time for acquiring new knowledge leads to
overwrites existing knowledge leading to common phenomenon know as
\textit{catastrophic forgetting}. In this paper, we aim to address these
concerns using a novel framework , COLA that employs an autoencoder to learn
capture low-dimensional embeddings of the weights associated with various
tasks. Our approach facilitates the transfer of knowledge to new tasks while
preventing catastrophic forgetting, all without using data replay or a
substantial set of task-specific parameters. Our approach, COLA, makes the LLM
efficiently learn new tasks with minimal training, insignificant performance
degradation on previous tasks, and eliminates the need for retaining earlier
training data. Empirical evaluation on different datasets ranging from task
oriented dialouge system to intent classsfication datasets showcases that our
method not only overcomes catastrophic forgetting but also achieves significant
reduction in parameter usage and memory size, across multiple tasks and
outperforming the existing state of the art methods across multiple datasets.

</details>


### [41] [The Mirror Loop: Recursive Non-Convergence in Generative Reasoning Systems](https://arxiv.org/abs/2510.21861)
*Bentley DeVilling*

Main category: cs.LG

TL;DR: 该研究测试了大型语言模型在无外部反馈情况下的自我反思能力，发现在无基础验证的情况下，递归自我评估会导致信息变化减少，趋于认知停滞状态。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证大型语言模型是否真正具备反思推理能力，以及在没有外部反馈的情况下，递归自我评估是否能带来实质性进步。

Method: 采用跨提供商研究设计，测试了三个模型（GPT-4o-mini、Claude 3 Haiku、Gemini 2.0 Flash）在四种任务类型中的144个推理序列，比较了无基础自我批评和最小基础干预两种条件。

Result: 无基础运行中，平均信息变化从早期的0.193下降到后期的0.087（下降55%）。基础干预后信息变化立即反弹28%，并维持非零方差。所有模型都显示出一致的模式。

Conclusion: 研究表明生成式推理中的自我修正存在结构性限制：没有与独立验证器或环境的信息交换，递归推理会趋于认知停滞状态。最小基础干预可作为耗散耦合，重新引入信息流动。

Abstract: Large language models are often described as capable of reflective reasoning,
yet recursive self-evaluation without external feedback frequently yields
reformulation rather than progress. We test this prediction in a cross-provider
study of 144 reasoning sequences across three models (OpenAI GPT-4o-mini,
Anthropic Claude 3 Haiku, and Google Gemini 2.0 Flash) and four task families
(arithmetic, code, explanation, reflection), each iterated ten times under two
conditions: ungrounded self-critique and a minimal grounding intervention (a
single verification step at iteration three). Mean informational change (delta
I, measured via normalized edit distance) declined by 55% from early (0.193) to
late (0.087) iterations in ungrounded runs, with consistent patterns across all
three providers. Grounded runs showed a +28% rebound in informational change
immediately after the intervention and sustained non-zero variance thereafter.
Complementary measures-n-gram novelty, embedding drift, and character-level
entropy-converged on the same pattern: reflection without contact tends toward
informational closure. We interpret this as evidence for a structural limit on
self-correction in generative reasoning: without an exchange of information
with an independent verifier or environment, recursive inference approaches an
attractor state of epistemic stasis. Minimal grounding functions as dissipative
coupling, reintroducing informational flux. The cross-architecture consistency
suggests the mirror loop arises from shared autoregressive training objectives
rather than provider-specific alignment schemes. The results delineate when
reflection is performative rather than epistemic and motivate design principles
for grounded, cooperative reasoning. Materials and code are publicly available.

</details>


### [42] [Is Temporal Difference Learning the Gold Standard for Stitching in RL?](https://arxiv.org/abs/2510.21995)
*Michał Bortkiewicz,Władysław Pałucki,Mateusz Ostaszewski,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 该论文通过实证研究表明，蒙特卡洛方法在函数逼近设置下也能实现经验拼接，传统TD方法的优势在大型模型时代可能不再那么必要。


<details>
  <summary>Details</summary>
Motivation: 研究在函数逼近设置下，传统关于TD方法在经验拼接方面的优势是否仍然成立，以及MC方法是否也能实现隐式拼接。

Method: 通过实证研究比较TD和MC方法在不同神经网络容量下的经验拼接能力，分析函数逼近对拼接效果的影响。

Result: 发现MC方法也能实现经验拼接，TD方法仅略微优于MC方法，而增加批评器容量能有效减少两种方法的泛化差距。

Conclusion: 在大型模型时代，TD方法的拼接归纳偏置可能不再那么必要，拼接能力可以通过模型规模而非专门算法来实现。

Abstract: Reinforcement learning (RL) promises to solve long-horizon tasks even when
training data contains only short fragments of the behaviors. This experience
stitching capability is often viewed as the purview of temporal difference (TD)
methods. However, outside of small tabular settings, trajectories never
intersect, calling into question this conventional wisdom. Moreover, the common
belief is that Monte Carlo (MC) methods should not be able to recombine
experience, yet it remains unclear whether function approximation could result
in a form of implicit stitching. The goal of this paper is to empirically study
whether the conventional wisdom about stitching actually holds in settings
where function approximation is used. We empirically demonstrate that Monte
Carlo (MC) methods can also achieve experience stitching. While TD methods do
achieve slightly stronger capabilities than MC methods (in line with
conventional wisdom), that gap is significantly smaller than the gap between
small and large neural networks (even on quite simple tasks). We find that
increasing critic capacity effectively reduces the generalization gap for both
the MC and TD methods. These results suggest that the traditional TD inductive
bias for stitching may be less necessary in the era of large models for RL and,
in some cases, may offer diminishing returns. Additionally, our results suggest
that stitching, a form of generalization unique to the RL setting, might be
achieved not through specialized algorithms (temporal difference learning) but
rather through the same recipe that has provided generalization in other
machine learning settings (via scale). Project website:
https://michalbortkiewicz.github.io/golden-standard/

</details>


### [43] [Do You Trust the Process?: Modeling Institutional Trust for Community Adoption of Reinforcement Learning Policies](https://arxiv.org/abs/2510.22017)
*Naina Balepur,Xingrui Pei,Hari Sundaram*

Main category: cs.LG

TL;DR: 开发了一种考虑制度信任的强化学习算法，用于社区资源分配，通过模拟信任变化来优化政策，发现信任整合能提高政策成功率，但组织成功与社区福祉之间存在张力。


<details>
  <summary>Details</summary>
Motivation: 现有RL算法假设公民会遵循政府制定的政策，但现实中缺乏制度信任时公民不会配合。本研究旨在开发信任感知的RL算法来解决资源分配中的信任问题。

Method: 使用深度确定性策略梯度方法学习资源分配策略，模拟资源分配过程并建模社区成员制度信任的变化，研究信任整合对结果的影响。

Result: 整合信任的RL算法能产生更成功的政策，特别是在组织目标不确定时；保守的信任估计能提高公平性和平均社区信任，但会降低组织成功率；配额干预在某些情况下能改善公平和信任。

Conclusion: 制度信任在算法设计和实施中至关重要，组织成功与社区福祉之间存在固有张力，需要平衡考虑。

Abstract: Many governmental bodies are adopting AI policies for decision-making. In
particular, Reinforcement Learning has been used to design policies that
citizens would be expected to follow if implemented. Much RL work assumes that
citizens follow these policies, and evaluate them with this in mind. However,
we know from prior work that without institutional trust, citizens will not
follow policies put in place by governments. In this work, we develop a
trust-aware RL algorithm for resource allocation in communities. We consider
the case of humanitarian engineering, where the organization is aiming to
distribute some technology or resource to community members. We use a Deep
Deterministic Policy Gradient approach to learn a resource allocation that fits
the needs of the organization. Then, we simulate resource allocation according
to the learned policy, and model the changes in institutional trust of
community members. We investigate how this incorporation of institutional trust
affects outcomes, and ask how effectively an organization can learn policies if
trust values are private. We find that incorporating trust into RL algorithms
can lead to more successful policies, specifically when the organization's
goals are less certain. We find more conservative trust estimates lead to
increased fairness and average community trust, though organization success
suffers. Finally, we explore a strategy to prevent unfair outcomes to
communities. We implement a quota system by an external entity which decreases
the organization's utility when it does not serve enough community members. We
find this intervention can improve fairness and trust among communities in some
cases, while decreasing the success of the organization. This work underscores
the importance of institutional trust in algorithm design and implementation,
and identifies a tension between organization success and community well-being.

</details>


### [44] [Online Optimization for Offline Safe Reinforcement Learning](https://arxiv.org/abs/2510.22027)
*Yassine Chemingui,Aryan Deshwal,Alan Fern,Thanh Nguyen-Tang,Janardhan Rao Doppa*

Main category: cs.LG

TL;DR: 提出了一种新的离线安全强化学习方法，通过极小化极大目标和结合离线RL与在线优化算法来解决OSRL问题。


<details>
  <summary>Details</summary>
Motivation: 解决离线安全强化学习问题，目标是从固定数据中学习满足累积成本约束的奖励最大化策略。

Method: 将问题构建为极小化极大目标，结合离线RL和在线优化算法，提出无需离线策略评估的实用近似方法。

Result: 在DSRL基准测试中，该方法在严格成本预算下可靠地强制执行安全约束，同时获得高奖励。

Conclusion: 该方法在理论上有近似最优性保证，在实践中能有效平衡安全性和性能。

Abstract: We study the problem of Offline Safe Reinforcement Learning (OSRL), where the
goal is to learn a reward-maximizing policy from fixed data under a cumulative
cost constraint. We propose a novel OSRL approach that frames the problem as a
minimax objective and solves it by combining offline RL with online
optimization algorithms. We prove the approximate optimality of this approach
when integrated with an approximate offline RL oracle and no-regret online
optimization. We also present a practical approximation that can be combined
with any offline RL algorithm, eliminating the need for offline policy
evaluation. Empirical results on the DSRL benchmark demonstrate that our method
reliably enforces safety constraints under stringent cost budgets, while
achieving high rewards. The code is available at
https://github.com/yassineCh/O3SRL.

</details>


### [45] [Agentic Reinforcement Learning for Real-World Code Repair](https://arxiv.org/abs/2510.22075)
*Siyu Zhu,Anastasiya Karpovich,Albert Chen,Jessica Koscheka,Shailesh Jannu,Di Wen,Yuqing Zhu,Rohit Jain,Alborz Geramifard*

Main category: cs.LG

TL;DR: 该论文开发了一个可验证的代码修复代理训练流程，通过固定依赖和禁用自动升级来提高在真实代码库中的可复现性。基于此构建了简化的大规模强化学习环境，训练出的SFT模型性能与GPT-4.1相当但体积小56倍，RL在此基础上带来7-20%的绝对提升。


<details>
  <summary>Details</summary>
Motivation: 解决在真实代码库中训练可靠代码修复代理的挑战，因为复杂的构建过程和不断变化的依赖关系使得评估不稳定。

Method: 开发可验证的代码修复流程，定义成功标准为修复后构建验证；通过固定依赖和禁用自动升级提高可复现性；构建简化的可扩展管道用于大规模强化学习；在完整管道中监督微调Qwen3-32B，在简化环境中对SFT模型应用RL。

Result: 从GPT-4.1轨迹蒸馏的SFT模型性能相当但体积小56倍；RL在匹配的训练-测试条件下带来7-20%的绝对提升；"思考模式"在实验中表现相当或更差；SFT和RL模型都无法跨环境泛化。

Conclusion: 训练-测试环境匹配对于构建可靠的现实世界代码修复代理至关重要，模型无法跨环境泛化。

Abstract: We tackle the challenge of training reliable code-fixing agents in real
repositories, where complex builds and shifting dependencies make evaluation
unstable. We developed a verifiable pipeline with success defined as post-fix
build validation and improved reproducibility across ~1K real issues by pinning
dependencies and disabling automatic upgrades. Building on this, we introduced
a scalable simplified pipeline for large-scale reinforcement learning (RL).
Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and
applied RL on top of the SFT model in the simplified environment. The SFT model
distilled from GPT-4.1 trajectories performs on par while being 56x smaller,
and RL added 7-20% absolute gains under matched train-test conditions.
"Thinking mode" was on par or worse in our experiments. Both SFT and RL models
failed to generalize across environments, highlighting the importance of
matching train-test environments for building reliable real-world code-fixing
agents.

</details>


### [46] [Probing Neural Combinatorial Optimization Models](https://arxiv.org/abs/2510.22131)
*Zhiqin Zhang,Yining Ma,Zhiguang Cao,Hoong Chuin Lau*

Main category: cs.LG

TL;DR: 本文首次系统性地解释神经组合优化(NCO)模型的黑盒问题，通过多种探测任务分析模型表示，并提出CS-Probing工具进行更深层分析。


<details>
  <summary>Details</summary>
Motivation: NCO模型取得了显著性能，但其学习到的模型表示和决策原理仍是黑盒，阻碍了学术研究和实际部署，需要更深入理解NCO模型。

Method: 使用多种探测任务分析NCO模型表示，并引入新的探测工具CS-Probing，通过检查探测过程中的系数和统计显著性进行更深层分析。

Result: 发现NCO模型编码了解决方案构建所需的低层信息，同时捕获了促进更好决策的高层知识；CS-Probing揭示了主流NCO模型对其学习表示施加了不同的归纳偏置，发现了与模型泛化相关的直接证据，并识别了与特定知识相关的关键嵌入维度。

Conclusion: 这项工作是对黑盒NCO模型进行系统解释的首次尝试，展示了探测作为分析其内部机制的有前景工具，为NCO社区提供了重要见解。

Abstract: Neural combinatorial optimization (NCO) has achieved remarkable performance,
yet its learned model representations and decision rationale remain a black
box. This impedes both academic research and practical deployment, since
researchers and stakeholders require deeper insights into NCO models. In this
paper, we take the first critical step towards interpreting NCO models by
investigating their representations through various probing tasks. Moreover, we
introduce a novel probing tool named Coefficient Significance Probing
(CS-Probing) to enable deeper analysis of NCO representations by examining the
coefficients and statistical significance during probing. Extensive experiments
and analysis reveal that NCO models encode low-level information essential for
solution construction, while capturing high-level knowledge to facilitate
better decisions. Using CS-Probing, we find that prevalent NCO models impose
varying inductive biases on their learned representations, uncover direct
evidence related to model generalization, and identify key embedding dimensions
associated with specific knowledge. These insights can be potentially
translated into practice, for example, with minor code modifications, we
improve the generalization of the analyzed model. Our work represents a first
systematic attempt to interpret black-box NCO models, showcasing probing as a
promising tool for analyzing their internal mechanisms and revealing insights
for the NCO community. The source code is publicly available.

</details>


### [47] [Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics](https://arxiv.org/abs/2510.22158)
*Lorenzo Magnino,Kai Shao,Zida Wu,Jiacheng Shen,Mathieu Laurière*

Main category: cs.LG

TL;DR: 提出了一种用于非平稳连续均值场博弈的深度强化学习算法，结合虚构博弈方法，使用条件归一化流表示时间相关的人口分布。


<details>
  <summary>Details</summary>
Motivation: 现有均值场博弈强化学习方法通常局限于有限空间或平稳模型，难以应用于现实世界问题，需要解决可扩展性和密度逼近的关键限制。

Method: 基于虚构博弈方法，使用深度强化学习计算最佳响应，监督学习表示平均策略，并用条件归一化流学习时间相关的人口分布表示。

Result: 在三个复杂度递增的示例上验证了方法的有效性，展示了在可扩展性和密度逼近方面的显著改进。

Conclusion: 这项工作代表了将深度强化学习技术应用于复杂均值场博弈问题的重要进展，使该领域更接近现实世界的多智能体系统。

Abstract: Mean field games (MFGs) have emerged as a powerful framework for modeling
interactions in large-scale multi-agent systems. Despite recent advancements in
reinforcement learning (RL) for MFGs, existing methods are typically limited to
finite spaces or stationary models, hindering their applicability to real-world
problems. This paper introduces a novel deep reinforcement learning (DRL)
algorithm specifically designed for non-stationary continuous MFGs. The
proposed approach builds upon a Fictitious Play (FP) methodology, leveraging
DRL for best-response computation and supervised learning for average policy
representation. Furthermore, it learns a representation of the time-dependent
population distribution using a Conditional Normalizing Flow. To validate the
effectiveness of our method, we evaluate it on three different examples of
increasing complexity. By addressing critical limitations in scalability and
density approximation, this work represents a significant advancement in
applying DRL techniques to complex MFG problems, bringing the field closer to
real-world multi-agent systems.

</details>


### [48] [Scalable Oversight via Partitioned Human Supervision](https://arxiv.org/abs/2510.22500)
*Ren Yin,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种基于互补标签的可扩展监督框架，用于评估和训练超越人类专家能力的AI系统，无需真实标签即可评估前沿AI模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在多个领域超越人类专家表现，获取高质量人工监督变得困难。人类专家只能在其狭窄领域提供弱信号（互补标签），而非全面评估。

Method: 使用互补标签构建无偏估计器来评估top-1准确率，结合稀缺的普通标签和丰富的互补标签，并提供有限样本偏差保证。

Result: 实证表明可以在没有真实标签的情况下评估大语言模型输出，并能设计出在这种分区人工监督下表现更好的智能AI系统。

Conclusion: 互补标签提供了一种可扩展的监督方法，能够有效评估和训练超越人类专家能力的AI系统。

Abstract: As artificial intelligence (AI) systems approach and surpass expert human
performance across a broad range of tasks, obtaining high-quality human
supervision for evaluation and training becomes increasingly challenging. Our
focus is on tasks that require deep knowledge and skills of multiple domains.
Unfortunately, even the best human experts are knowledgeable only in a single
narrow area, and will not be able to evaluate the correctness of advanced AI
systems on such superhuman tasks. However, based on their narrow expertise,
humans may provide a weak signal, i.e., a complementary label indicating an
option that is incorrect. For example, a cardiologist could state that "this is
not related to cardiology,'' even if they cannot identify the true disease.
Based on this weak signal, we propose a scalable oversight framework that
enables us to evaluate frontier AI systems without the need to prepare the
ground truth. We derive an unbiased estimator of top-1 accuracy from
complementary labels and quantify how many complementary labels are needed to
match the variance of ordinary labels. We further introduce two estimators to
combine scarce ordinary labels with abundant complementary labels. We provide
finite-sample deviation guarantees for both complementary-only and the mixed
estimators. Empirically, we show that we can evaluate the output of large
language models without the ground truth, if we have complementary labels. We
further show that we can train an AI system with such weak signals: we show how
we can design an agentic AI system automatically that can perform better with
this partitioned human supervision. Our code is available at
https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.

</details>


### [49] [FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning](https://arxiv.org/abs/2510.22543)
*Yuyang Ding,Chi Zhang,Juntao Li,Haibin Lin,Xin Liu,Min Zhang*

Main category: cs.LG

TL;DR: 本文提出FAPO方法，通过检测和惩罚有缺陷的推理轨迹来改进强化学习中的语言模型训练，提高推理可靠性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法中，即使推理过程存在缺陷（如猜测答案、跳跃推理），只要最终答案正确就会被同等奖励，导致模型学习不可靠的推理模式。

Method: 提出FAPO方法：1）使用生成式奖励模型（GenRM）精确定位推理错误；2）对有缺陷的推理轨迹施加参数无关的奖励惩罚；3）在训练早期利用缺陷轨迹作为捷径，后期逐步转向可靠推理。

Result: FAPO在多个领域有效提高了结果正确性、过程可靠性和训练稳定性，且不增加计算开销。

Conclusion: FAPO通过有意识地处理缺陷推理轨迹，实现了更稳定和可靠的强化学习训练，为语言模型推理能力优化提供了新思路。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reasoning trajectories and
exploit rollouts with correct answers as positive signals for policy
optimization. However, these rollouts might involve flawed patterns such as
answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are
rewarded identically to fully correct ones, causing policy models to
internalize these unreliable reasoning patterns. In this work, we first conduct
a systematic study of flawed-positive rollouts in RL and find that they enable
rapid capability gains during the early optimization stage, while constraining
reasoning capability later by reinforcing unreliable patterns. Building on
these insights, we propose Flawed-Aware Policy Optimization (FAPO), which
presents a parameter-free reward penalty for flawed-positive rollouts, enabling
the policy to leverage them as useful shortcuts in the warm-up stage, securing
stable early gains, while gradually shifting optimization toward reliable
reasoning in the later refinement stage. To accurately and comprehensively
detect flawed-positive rollouts, we introduce a generative reward model (GenRM)
with a process-level reward that precisely localizes reasoning errors.
Experiments show that FAPO is effective in broad domains, improving outcome
correctness, process reliability, and training stability without increasing the
token budget.

</details>


### [50] [FlowCritic: Bridging Value Estimation with Flow Matching in Reinforcement Learning](https://arxiv.org/abs/2510.22686)
*Shan Zhong,Shutong Ding,He Diao,Xiangyu Wang,Kah Chan Teh,Bei Peng*

Main category: cs.LG

TL;DR: 提出FlowCritic方法，使用流匹配技术建模价值分布，通过生成样本进行价值估计，提升强化学习价值函数的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如多评论家集成仅结合点估计，分布强化学习依赖离散化或分位数回归，限制了复杂价值分布的表达能力。

Method: 利用流匹配技术建模价值分布，通过生成样本进行价值估计，与传统确定性价值预测的回归方法不同。

Result: 该方法能够更准确地捕捉价值分布信息，提高价值估计的可靠性。

Conclusion: 流匹配为价值估计提供了新的生成范式，能够更好地表达复杂价值分布。

Abstract: Reliable value estimation serves as the cornerstone of reinforcement learning
(RL) by evaluating long-term returns and guiding policy improvement,
significantly influencing the convergence speed and final performance. Existing
works improve the reliability of value function estimation via multi-critic
ensembles and distributional RL, yet the former merely combines multi point
estimation without capturing distributional information, whereas the latter
relies on discretization or quantile regression, limiting the expressiveness of
complex value distributions. Inspired by flow matching's success in generative
modeling, we propose a generative paradigm for value estimation, named
FlowCritic. Departing from conventional regression for deterministic value
prediction, FlowCritic leverages flow matching to model value distributions and
generate samples for value estimation.

</details>


### [51] [ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation](https://arxiv.org/abs/2510.22732)
*Jiali Cheng,Anjishnu Kumar,Roshan Lal,Rishi Rajasekaran,Hani Ramezani,Omar Zia Khan,Oleg Rokhlenko,Sunny Chiu-Webster,Gang Hua,Hadi Amiri*

Main category: cs.LG

TL;DR: ATLAS是一个记忆增强的智能体，通过认知空间中的前瞻动作模拟来制定基于环境模型的计划，无需神经网络微调即可适应新环境。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的网络智能体无法有效适应新环境，缺乏对环境结构和动态的认知，导致执行计划效率低下。

Method: ATLAS采用模块化架构：首先通过好奇心驱动探索构建认知地图，规划器提出候选动作，模拟器在认知空间预测结果，评论家分析选项选择最佳路径并更新计划，浏览器执行器执行选定动作。

Result: 在WebArena-Lite基准测试中达到63%的成功率，优于之前最先进系统的53.9%。

Conclusion: ATLAS无需网站特定的LLM微调，其世界模型、分层规划器和基于前瞻的重新规划器在系统设计中发挥互补作用。

Abstract: We observe that current state-of-the-art web-agents are unable to effectively
adapt to new environments without neural network fine-tuning, without which
they produce inefficient execution plans due to a lack of awareness of the
structure and dynamics of the new environment. To address this limitation, we
introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action
Simulation), a memory-augmented agent that is able to make plans grounded in a
model of the environment by simulating the consequences of those actions in
cognitive space. Our agent starts by building a "cognitive map" by performing a
lightweight curiosity driven exploration of the environment. The planner
proposes candidate actions; the simulator predicts their consequences in
cognitive space; a critic analyzes the options to select the best roll-out and
update the original plan; and a browser executor performs the chosen action. On
the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9%
success rate for the previously published state-of-the-art. Unlike previous
systems, our modular architecture requires no website-specific LLM fine-tuning.
Ablations show sizable drops without the world-model, hierarchical planner, and
look-ahead-based replanner confirming their complementary roles within the
design of our system

</details>


### [52] [Guardian: Decoupling Exploration from Safety in Reinforcement Learning](https://arxiv.org/abs/2510.22859)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: RLPD-GX是一个混合离线-在线强化学习框架，通过将策略优化与安全执行解耦来解决分布偏移问题，在Atari-100k上实现了45%的性能提升和更强的安全性。


<details>
  <summary>Details</summary>
Motivation: 混合离线-在线强化学习虽然结合了样本效率和鲁棒探索的优势，但由于离线数据和在线数据之间的分布偏移而存在不稳定性问题。

Method: 采用解耦设计：奖励寻求的学习器自由探索，基于投影的守护器保证规则一致性执行和安全值备份；提出动态课程学习，逐步扩展时间视野并退火离线-在线数据混合。

Result: 在Atari-100k上实现了3.02的归一化平均分数，比之前的混合方法提升45%，具有更强的安全性和稳定性；在安全关键和长视野任务上表现一致。

Conclusion: 解耦的安全执行为鲁棒的混合离线-在线强化学习提供了一条简单而有原则的途径，为协调强化学习中的探索和安全提出了更广泛的范式。

Abstract: Hybrid offline--online reinforcement learning (O2O RL) promises both sample
efficiency and robust exploration, but suffers from instability due to
distribution shift between offline and online data. We introduce RLPD-GX, a
framework that decouples policy optimization from safety enforcement: a
reward-seeking learner explores freely, while a projection-based guardian
guarantees rule-consistent execution and safe value backups. This design
preserves the exploratory value of online interactions without collapsing to
conservative policies. To further stabilize training, we propose dynamic
curricula that gradually extend temporal horizons and anneal offline--online
data mixing. We prove convergence via a contraction property of the guarded
Bellman operator, and empirically show state-of-the-art performance on
Atari-100k, achieving a normalized mean score of 3.02 (+45\% over prior hybrid
methods) with stronger safety and stability. Beyond Atari, ablations
demonstrate consistent gains across safety-critical and long-horizon tasks,
underscoring the generality of our design. Extensive and comprehensive results
highlight decoupled safety enforcement as a simple yet principled route to
robust O2O RL, suggesting a broader paradigm for reconciling exploration and
safety in reinforcement learning.

</details>


### [53] [RL-AUX: Reinforcement Learning for Auxiliary Task Generation](https://arxiv.org/abs/2510.22940)
*Judah Goldfeder,Matthew So,Hod Lipson*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的辅助任务动态生成方法，避免传统元学习中双级优化的计算成本，通过RL智能体为每个训练数据点选择辅助标签，并在CIFAR100上超越人工标注的辅助任务性能。


<details>
  <summary>Details</summary>
Motivation: 辅助学习需要人工标注的辅助任务，这需要大量人力和领域专业知识。现有的元学习方法使用双级优化来解决这个问题，但计算成本高且代码复杂。

Method: 使用强化学习框架，RL智能体为每个训练数据点动态选择辅助标签，当选择能提升主任务性能时获得奖励。还实验了学习每个数据点辅助损失权重的最优策略。

Result: 在20-Superclass CIFAR100问题上，RL方法超越了人工标注的辅助任务，与主流双级优化技术表现相当。权重学习方法显著优于所有基准，VGG16架构达到80.9%测试准确率，而人工标注方法为75.53%。

Conclusion: RL是动态生成辅助任务的可行方法，同时学习每个样本的辅助任务权重可以取得强劲结果。

Abstract: Auxiliary Learning (AL) is a special case of Multi-task Learning (MTL) in
which a network trains on auxiliary tasks to improve performance on its main
task. This technique is used to improve generalization and, ultimately,
performance on the network's main task. AL has been demonstrated to improve
performance across multiple domains, including navigation, image
classification, and natural language processing. One weakness of AL is the need
for labeled auxiliary tasks, which can require human effort and domain
expertise to generate. Meta Learning techniques have been used to solve this
issue by learning an additional auxiliary task generation network that can
create helpful tasks for the primary network. The most prominent techniques
rely on Bi-Level Optimization, which incurs computational cost and increased
code complexity. To avoid the need for Bi-Level Optimization, we present an
RL-based approach to dynamically create auxiliary tasks. In this framework, an
RL agent is tasked with selecting auxiliary labels for every data point in a
training set. The agent is rewarded when their selection improves the
performance on the primary task. We also experiment with learning optimal
strategies for weighing the auxiliary loss per data point. On the 20-Superclass
CIFAR100 problem, our RL approach outperforms human-labeled auxiliary tasks and
performs as well as a prominent Bi-Level Optimization technique. Our weight
learning approaches significantly outperform all of these benchmarks. For
example, a Weight-Aware RL-based approach helps the VGG16 architecture achieve
80.9% test accuracy while the human-labeled auxiliary task setup achieved
75.53%. The goal of this work is to (1) prove that RL is a viable approach to
dynamically generate auxiliary tasks and (2) demonstrate that per-sample
auxiliary task weights can be learned alongside the auxiliary task labels and
can achieve strong results.

</details>


### [54] [Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts](https://arxiv.org/abs/2510.23027)
*Di Zhang,Xun Wu,Shaohan Huang,Yaru Hao,Li Dong,Zewen Chi,Zhifang Sui,Furu Wei*

Main category: cs.LG

TL;DR: 提出了一种针对MoE架构的路由感知重要性采样权重优化方法，通过基于路由器logits的重新缩放策略来减少梯度方差和缓解训练发散问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习研究主要关注密集模型，而MoE架构的RL训练研究不足，且MoE训练中常见不稳定性问题需要解决。

Method: 设计了基于路由器logits的重新缩放策略来优化离策略RL中的重要性采样权重，以降低梯度方差。

Result: 实验结果表明该方法显著提高了MoE模型的收敛稳定性和最终性能。

Conclusion: 该方法展示了针对MoE架构的RL算法创新的潜力，为大规模专家模型的高效训练提供了有前景的方向。

Abstract: Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.

</details>


### [55] [The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination](https://arxiv.org/abs/2510.22977)
*Chenlong Yin,Zeyang Sha,Shiwen Cui,Changhua Meng*

Main category: cs.LG

TL;DR: 论文系统研究了推理增强与工具幻觉之间的因果关系，发现强化推理能力会成比例增加工具幻觉，这种效应与训练方法无关且难以缓解，揭示了当前推理增强方法的内在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有观察表明更强的推理能力往往伴随更多幻觉，但缺乏系统研究来验证推理增强本身是否导致工具幻觉。

Method: 构建SimpleToolHalluBench诊断基准，通过控制实验研究推理增强与工具幻觉的因果关系，评估不同缓解策略。

Result: 发现推理增强与工具幻觉存在因果关联，这种效应超越过拟合，方法无关，且缓解策略会带来可靠性-能力权衡。

Conclusion: 当前推理增强方法固有地放大工具幻觉，需要新的训练目标同时优化能力和可靠性。

Abstract: Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key
strategy for building Agents that "think then act." However, recent
observations, like OpenAI's o3, suggest a paradox: stronger reasoning often
coincides with increased hallucination, yet no prior work has systematically
examined whether reasoning enhancement itself causes tool hallucination. To
address this gap, we pose the central question: Does strengthening reasoning
increase tool hallucination? To answer this, we introduce SimpleToolHalluBench,
a diagnostic benchmark measuring tool hallucination in two failure modes: (i)
no tool available, and (ii) only distractor tools available. Through controlled
experiments, we establish three key findings. First, we demonstrate a causal
relationship: progressively enhancing reasoning through RL increases tool
hallucination proportionally with task performance gains. Second, this effect
transcends overfitting - training on non-tool tasks (e.g., mathematics) still
amplifies subsequent tool hallucination. Third, the effect is method-agnostic,
appearing when reasoning is instilled via supervised fine-tuning and when it is
merely elicited at inference by switching from direct answers to step-by-step
thinking. We also evaluate mitigation strategies including Prompt Engineering
and Direct Preference Optimization (DPO), revealing a fundamental
reliability-capability trade-off: reducing hallucination consistently degrades
utility. Mechanistically, Reasoning RL disproportionately collapses
tool-reliability-related representations, and hallucinations surface as
amplified divergences concentrated in late-layer residual streams. These
findings reveal that current reasoning enhancement methods inherently amplify
tool hallucination, highlighting the need for new training objectives that
jointly optimize for capability and reliability.

</details>


### [56] [Increasing LLM Coding Capabilities through Diverse Synthetic Coding Tasks](https://arxiv.org/abs/2510.23208)
*Amal Abed,Ivan Lukic,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 提出了一个可扩展的合成数据生成管道，生成近80万个包含指令、推理过程、代码和测试的四元组数据，用于提升LLM的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成数据集大多只包含问题和解决方案，缺乏中间推理过程，限制了LLM学习人类编程思维的能力。

Method: 结合四个关键组件：精选竞赛问题、网络挖掘内容、基于推理模式的数据扩展、多阶段执行验证，并使用遗传变异算法增加任务多样性。

Result: 在该数据集上微调的LLM在代码基准测试中表现一致提升，推理感知数据可以替代模型缩放，在不同架构间泛化良好，并在相同样本预算下优于领先的开源替代方案。

Conclusion: 以推理为中心的合成数据生成是提升LLM编码能力的高效方法。

Abstract: Large language models (LLMs) have shown impressive promise in code
generation, yet their progress remains limited by the shortage of large-scale
datasets that are both diverse and well-aligned with human reasoning. Most
existing resources pair problems with solutions, but omit the intermediate
thought process that guides coding. To close this gap, we present a scalable
synthetic data generation pipeline that produces nearly 800k
instruction-reasoning-code-test quadruplets. Each sample combines a task, a
step-by-step reasoning trace, a working solution, and executable tests,
enabling models to learn not just the what but also the how of problem solving.
Our pipeline combines four key components: curated contest problems, web-mined
content filtered by relevance classifiers, data expansion guided by reasoning
patterns, and multi-stage execution-based validation. A genetic mutation
algorithm further increases task diversity while maintaining consistency
between reasoning traces and code implementations. Our key finding is that
fine-tuning LLMs on this dataset yields consistent improvements on coding
benchmarks. Beyond raw accuracy, reasoning-aware data can substitute for model
scaling, generalize across architectures, and outperform leading open-source
alternatives under identical sample budgets. Our work establishes
reasoning-centered synthetic data generation as an efficient approach for
advancing coding capabilities in LLMs. We publish our dataset and generation
pipeline to facilitate further research.

</details>


### [57] [PAHQ: Accelerating Automated Circuit Discovery through Mixed-Precision Inference Optimization](https://arxiv.org/abs/2510.23264)
*Xinhai Wang,Shu Yang,Liangyu Wang,Lin Zhang,Huanyi Xie,Lijie Hu,Di Wang*

Main category: cs.LG

TL;DR: 提出PAHQ方法，通过混合精度量化加速自动电路发现，在保持分析忠实度的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 传统ACDC方法在大语言模型中存在计算效率低和内存需求高的问题，现有加速方法依赖线性近似但会损害分析忠实度

Method: PAHQ方法基于激活修补与混合精度量化的对应关系，为被调查组件保持高精度，其他部分降低精度，通过修改注意力计算机制实现

Result: PAHQ加速的ACDC相比未加速版本减少80%运行时间和30%内存消耗，同时保持忠实度

Conclusion: PAHQ提供了一种无需训练的新途径来加速机制可解释性方法，可与现有基于边的电路发现技术集成

Abstract: Circuit discovery, which involves identifying sparse and task-relevant
subnetworks in pre-trained language models, is a cornerstone of mechanistic
interpretability. Automated Circuit Discovery (ACDC) has emerged as a pivotal
methodology in circuit discovery, but its application to large language models
is severely limited by computational inefficiency and prohibitively high memory
requirements. Although several accelerated approaches have been proposed, they
primarily rely on linear approximations to ACDC, which significantly
compromises analytical faithfulness. Our proposed method for accelerating
automated circuit discovery, Per Attention Head Quantization (PAHQ), takes a
fundamentally different approach by optimizing the efficiency of each
individual patching operation. PAHQ leverages a fundamental alignment between
activation patching and mixed-precision quantization (MPQ): interpretability
analysis through patching essentially performs targeted ablation studies.
Therefore, we can maintain high precision exclusively for investigated
components while safely reducing precision elsewhere in the network.
PAHQ-accelerated ACDC reduces runtime by up to 80\% and memory consumption by
up to 30\% compared to unaccelerated ACDC while maintaining faithfulness.
Importantly, our method readily integrates with existing edge-based circuit
discovery techniques by modifying the attention computation mechanism. This
training-free approach provides a practical and novel pathway for accelerating
mechanistic interpretability methods. Our code is available at
https://github.com/626619403/PAHQ.

</details>


### [58] [Sequential Multi-Agent Dynamic Algorithm Configuration](https://arxiv.org/abs/2510.23535)
*Chen Lu,Ke Xue,Lei Yuan,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 提出了Seq-MADAC框架，通过考虑多参数间的内在依赖关系来解决动态算法配置问题，使用顺序优势分解网络利用动作顺序信息。


<details>
  <summary>Details</summary>
Motivation: 复杂算法中多个参数存在内在依赖关系（如先确定算子类型再确定算子参数），但现有方法未考虑这些依赖关系，导致次优结果。

Method: 提出顺序多智能体DAC框架，采用顺序优势分解网络来利用动作顺序信息，处理参数间的依赖关系。

Result: 从合成函数到多目标优化算法配置的实验表明，Seq-MADAC优于最先进的MARL方法，并在问题类别间表现出强泛化能力。

Conclusion: Seq-MADAC为依赖感知的自动化算法配置建立了新范式。

Abstract: Dynamic algorithm configuration (DAC) is a recent trend in automated machine
learning, which can dynamically adjust the algorithm's configuration during the
execution process and relieve users from tedious trial-and-error tuning tasks.
Recently, multi-agent reinforcement learning (MARL) approaches have improved
the configuration of multiple heterogeneous hyperparameters, making various
parameter configurations for complex algorithms possible. However, many complex
algorithms have inherent inter-dependencies among multiple parameters (e.g.,
determining the operator type first and then the operator's parameter), which
are, however, not considered in previous approaches, thus leading to
sub-optimal results. In this paper, we propose the sequential multi-agent DAC
(Seq-MADAC) framework to address this issue by considering the inherent
inter-dependencies of multiple parameters. Specifically, we propose a
sequential advantage decomposition network, which can leverage action-order
information through sequential advantage decomposition. Experiments from
synthetic functions to the configuration of multi-objective optimization
algorithms demonstrate Seq-MADAC's superior performance over state-of-the-art
MARL methods and show strong generalization across problem classes. Seq-MADAC
establishes a new paradigm for the widespread dependency-aware automated
algorithm configuration. Our code is available at
https://github.com/lamda-bbo/seq-madac.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [59] [最新一篇长达76页的<em class="highlight">Agentic</em> AI综述](http://mp.weixin.qq.com/s?__biz=MzE5ODgxMzE5OQ==&mid=2247485904&idx=1&sn=a338584ec82c115c11d856823419a15c&chksm=971c809bc8a20043069419533a0ff920ffe8ab6ec53e08505d4fdc6a10ab2965a7397f7f548e#rd)
*智驾和机器人前瞻局*

Main category: wechat.article

TL;DR: Agentic AI正从外部流水线转向模型原生，推理、记忆与行动等能力被内化到模型策略中，借助强化学习把感知与行动打通，让静态模型变成可从环境互动中学习的目标驱动体。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI正从外部流水线转向模型原生，推理、记忆与行动等能力被内化到模型策略中，借助强化学习把感知与行动打通，让静态模型变成可从环境互动中学习的目标驱动体。

</details>


### [60] [<em class="highlight">Agentic</em>，是AI的下一次进化](http://mp.weixin.qq.com/s?__biz=MzYzMjA4MjY3MA==&mid=2247483655&idx=1&sn=21109ca6169e8e756eaa3a8c3e68ed4b&chksm=f1944fcf1b409351fc590755ad69c94d123f1967688d0857271e37a763e7ffae8f940cddd89e#rd)
*AI组装工*

Main category: wechat.article

TL;DR: Agentic，字面意思就是“具备代理（agent）能力的”，也可以理解为“自主行动的能力”。在 AI 系统里，Agentic AI并不是简单地生成文本或回答问题，而是能：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic，字面意思就是“具备代理（agent）能力的”，也可以理解为“自主行动的能力”。在 AI 系统里，Agentic AI并不是简单地生成文本或回答问题，而是能：

</details>


### [61] [<em class="highlight">Agentic</em> Notes 01｜<em class="highlight">Agentic</em> AI 的基础框架：从语言理解到智能行动](http://mp.weixin.qq.com/s?__biz=MzkxNjczMzU1OQ==&mid=2247483909&idx=1&sn=100c0f10b063aa8e6730721e3deabdf9&chksm=c02e77d920e00c3a9acc86d1d93d5109b571596d67e3c68ac82a8bd3ef6b94829dceb8545eed#rd)
*柏舟杂谈*

Main category: wechat.article

TL;DR: 不同系统的“Agentic”程度可以从低到高分为多个层次。举例来说，以“撰写一篇关于黑洞的论文”为例：degrees of autonomy web llm llm fetch pdf to web llm llm llm llm fetch reflect and draftandrew ng


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 不同系统的“Agentic”程度可以从低到高分为多个层次。举例来说，以“撰写一篇关于黑洞的论文”为例：degrees of autonomy web llm llm fetch pdf to web llm llm llm llm fetch reflect and draftandrew ng

</details>


### [62] [一图看懂：<em class="highlight">Agentic</em>架构与<em class="highlight">Agentic</em> RAG Workflows的区别，附中翻版](http://mp.weixin.qq.com/s?__biz=MzIzMTIyODMwMA==&mid=2650193426&idx=1&sn=52562f7f92d2d9562abdc5bde2292705&chksm=f10ceabbcd10f1f80a65da549cfb3afe6205b4eca88d3f1f7c5221d53e4ffc8eb395f7eb3d9e#rd)
*AGI商业新声*

Main category: wechat.article

TL;DR: agentic architectures vs work。flows single agent architecture multi agent architecture。ai agent agentic rag workflow。代理架构与工作流对比 工具 记忆 搜索工具 用户查询 生成响应


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: agentic architectures vs work。flows single agent architecture multi agent architecture。ai agent agentic rag workflow。代理架构与工作流对比 工具 记忆 搜索工具 用户查询 生成响应

</details>


### [63] [颠覆传统产线：自主决策<em class="highlight">Agentic</em> AI引领工业智能新纪元](http://mp.weixin.qq.com/s?__biz=MzkzMTY2ODQwMA==&mid=2247488887&idx=1&sn=d3c34953d2cc7c94805b61552f70744d&chksm=c31e2b7f87eca05526a3216ef8cf5e086328be05b06272110cf5916a7ce7bb26b8fed088e720#rd)
*企康AI+*

Main category: wechat.article

TL;DR: Agentic AI：智慧的“决策者”Agentic AI则完全不同。它并非简单执行指令，而是被赋予一个或多个高级目标（如“最大化良品率”、“最短化交付周期”），并拥有自主感知环境、分析数据、制定策略并执行行动的能力。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI：智慧的“决策者”Agentic AI则完全不同。它并非简单执行指令，而是被赋予一个或多个高级目标（如“最大化良品率”、“最短化交付周期”），并拥有自主感知环境、分析数据、制定策略并执行行动的能力。

</details>


### [64] [国外<em class="highlight">Agentic</em> SOC最新进展（2025Q3）](http://mp.weixin.qq.com/s?__biz=MzUyNzMxOTAwMw==&mid=2247485064&idx=1&sn=1c65225911fa0875d1e68ab8600a1586&chksm=fbcf1f8a238c0615124a96bc2a5050be149779e7ef97d908e28bdb3c01fbb06b7e940521e59e#rd)
*专注安管平台*

Main category: wechat.article

TL;DR: 类别 厂商名称 关键 Agentic AI 产品/能力 发布时间（最新或重大发布） 描述 发布链接性安全厂 CrowdStrike Charlotte Al /Falcon agentic security platform 2025年9月16日 利用智能体自动对威胁告警


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 类别 厂商名称 关键 Agentic AI 产品/能力 发布时间（最新或重大发布） 描述 发布链接性安全厂 CrowdStrike Charlotte Al /Falcon agentic security platform 2025年9月16日 利用智能体自动对威胁告警

</details>


### [65] [【洞见】<em class="highlight">Agentic</em> Commerce 风口：AI 主导电商，OpenAI 掌握结账标准，Meta/Google 还能抗衡吗？](http://mp.weixin.qq.com/s?__biz=Mzk5MDgwNzA0NA==&mid=2247484873&idx=1&sn=79acdc28ce2c8e7b159885045f33931e&chksm=c4fb0a797da94450dd256194600bc2af345143ee40a5bfa27b263c19c350a1ab1e85e3d2d559#rd)
*Vibehood*

Main category: wechat.article

TL;DR: Agentic Commerce 风口：OpenAI 在 ChatGPT 推出 Instant Checkout 并发布 Agentic Commerce Protocol；Perplexity 推 “Buy with Pro”；Meta 2025 年起取消 in-app checkout。【商业不只是卖货】


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic Commerce 风口：OpenAI 在 ChatGPT 推出 Instant Checkout 并发布 Agentic Commerce Protocol；Perplexity 推 “Buy with Pro”；Meta 2025 年起取消 in-app checkout。【商业不只是卖货】

</details>


### [66] [<em class="highlight">Agentic</em> AI安全部署作战手册：在<em class="highlight">智能体</em>觉醒时代，技术领导者必须守住的「安全红线」](http://mp.weixin.qq.com/s?__biz=MzkzNzMyNTg3NQ==&mid=2247484502&idx=1&sn=5ac00e9b829d4ca415e30591efc4f7f6&chksm=c34a9808c1e931b853f6b735e32af63d558cfa39926db35ddcbf6d61879e705a3612ea8b9941#rd)
*久安瀛*

Main category: wechat.article

TL;DR: Agentic AI（基于智能体的人工智能）与传统AI的关键区别在于其自主性。它不再是被动响应指令的工具，而是能够主动规划、执行多步骤任务的“数字员工”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI（基于智能体的人工智能）与传统AI的关键区别在于其自主性。它不再是被动响应指令的工具，而是能够主动规划、执行多步骤任务的“数字员工”。

</details>


### [67] [<em class="highlight">Agentic</em> AI 各赛道分析报告_37页](http://mp.weixin.qq.com/s?__biz=Mzk5MDUyMzYwMQ==&mid=2247490057&idx=4&sn=b3b9b4e766d679fed5fd9f0230fddb89&chksm=c4619e691ead741dd8b545c752a826c43388d43091d0ae31e28a9b74696a710e19353ae233c1#rd)
*行业先锋Eknower*

Main category: wechat.article

TL;DR: Agentic AI 各赛道分析报告agentic ai 各赛道分析报告目录。前言|总体分析|ai chatbots|编程助手|图生视频|agent编排|通用自动化agent|图像生成。01。目录 12 教育 次热门赛道。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic AI 各赛道分析报告agentic ai 各赛道分析报告目录。前言|总体分析|ai chatbots|编程助手|图生视频|agent编排|通用自动化agent|图像生成。01。目录 12 教育 次热门赛道。

</details>


### [68] [构建具备深度思考能力的 <em class="highlight">Agentic</em> RAG 流水线，用于解决复杂查询](http://mp.weixin.qq.com/s?__biz=MzkzMjkwMjk3Mw==&mid=2247486425&idx=1&sn=e6f3c7638480a19313074fa519c4b410&chksm=c32bb432bf93cdb61924ff398c01408f26a0ab299d9d68af73f5ca322bc415db0b79260f30a8#rd)
*AI大模型观察站*

Main category: wechat.article

TL;DR: 在一个 agentic 系统里，工作流复杂且循环，tracing 并非可有可无，而是很重要。它帮助你可视化内部过程，更容易调试 agent 的思考路径。知识库来源一个生产级 RAG 系统需要既复杂又有挑战性的知识库，才能真正体现其有效性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 在一个 agentic 系统里，工作流复杂且循环，tracing 并非可有可无，而是很重要。它帮助你可视化内部过程，更容易调试 agent 的思考路径。知识库来源一个生产级 RAG 系统需要既复杂又有挑战性的知识库，才能真正体现其有效性。

</details>


### [69] [最新一篇长达76页的<em class="highlight">Agentic</em> AI综述](http://mp.weixin.qq.com/s?__biz=Mzg2NzUxNTU1OA==&mid=2247683978&idx=3&sn=de78c86ef6666a62209c860097aadfe5&chksm=cfa651e82fc426d1770c43f4e23dd1a2515eab2a9bbb84434cb222d6b6607d288f9311ad55fd#rd)
*自动驾驶之心*

Main category: wechat.article

TL;DR: 论文标题：Beyond Pipelines： A Survey of the Paradigm Shift toward Model-Native Agentic AI论文链接：https：//arxiv.org/abs/2510.16720v1问题背景生成式AI进步迅猛，但多为“反应式输出”，缺乏面向目标的长期推理与环境交互；


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 论文标题：Beyond Pipelines： A Survey of the Paradigm Shift toward Model-Native Agentic AI论文链接：https：//arxiv.org/abs/2510.16720v1问题背景生成式AI进步迅猛，但多为“反应式输出”，缺乏面向目标的长期推理与环境交互；

</details>


### [70] [超详细！<em class="highlight">Agentic</em> AI <em class="highlight">智能体</em>是如何思考和行动的？一张图读懂AI工作流！](http://mp.weixin.qq.com/s?__biz=MzYyNTA1MjA0NQ==&mid=2247483939&idx=1&sn=7a8d8d7f3e9602b7d633a4ee3196cb35&chksm=f13c33e50455b0a803e510e4681ed3855d6c5be28ccb212a4eed3c524e95c48fbe8ef3f1ded8#rd)
*AI次生代*

Main category: wechat.article

TL;DR: 超详细！Agentic AI 智能体是如何思考和行动的？一张图读懂AI工作流！这张图清晰地展示了AI智能体从接收信息到输出结果的全过程，超级硬核！快来一起看看AI Agent的“大脑”是如何工作的吧！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 超详细！Agentic AI 智能体是如何思考和行动的？一张图读懂AI工作流！这张图清晰地展示了AI智能体从接收信息到输出结果的全过程，超级硬核！快来一起看看AI Agent的“大脑”是如何工作的吧！

</details>


### [71] [【上下文工程】<em class="highlight">Agentic</em>上下文工程：破解参数壁垒，开启语言模型自主进化新范式](http://mp.weixin.qq.com/s?__biz=MzI3NDI4MzIyNQ==&mid=2247509231&idx=2&sn=315ae5294ef22ae5cea0f5a299a75568&chksm=eabc20d9bfa24ad7fe6993d25b0dfb4bbe37c6a9bf6fa5d3383610206739a027c9fb22614da7#rd)
*产业智能官*

Main category: wechat.article

TL;DR: sambanova和加州大学伯克利分校的研究团队提出了一种名为**agentic上下文工程（agentic context engineering， ace）**的新方法，声称可以在无需微调模型参数的情况下实现模型能力的持续进化。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: sambanova和加州大学伯克利分校的研究团队提出了一种名为**agentic上下文工程（agentic context engineering， ace）**的新方法，声称可以在无需微调模型参数的情况下实现模型能力的持续进化。

</details>


### [72] [探秘 <em class="highlight">Agentic</em> RAG 系统：一场知识检索与智能交互的革命](http://mp.weixin.qq.com/s?__biz=Mzk2NDk4NTU0OA==&mid=2247484344&idx=1&sn=bd026bc33a7f38222ced4c57a06efb7e&chksm=c5183555f9a9db48b755e0b5c5752505a4766651a7259ca893660d307ff8fa2039a367bbd008#rd)
*AI应用学堂*

Main category: wechat.article

TL;DR: Agentic RAG ：在以上基础上增加了 “自主代理” 层 ，具备目标拆解、动态决策、工具调用、自我评估等 “类人思维” 能力，能处理需要多步骤协作的复杂任务（而非简单的 “输入 - 检索 - 输出” 闭环）。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic RAG ：在以上基础上增加了 “自主代理” 层 ，具备目标拆解、动态决策、工具调用、自我评估等 “类人思维” 能力，能处理需要多步骤协作的复杂任务（而非简单的 “输入 - 检索 - 输出” 闭环）。

</details>


### [73] [喜欢这张<em class="highlight">agentic</em> AI原理图](http://mp.weixin.qq.com/s?__biz=MzA4MTEwNzcxOQ==&mid=2451068125&idx=1&sn=4659b01adecac4613b8a2850c15328c8&chksm=89904fd1c1375b52950970f7ecd5397513407b25db6f9d8b2328f92d35ea8489a23dba18fd18#rd)
*风海筑梦*

Main category: wechat.article

TL;DR: how agentic ai works agent喜欢这张agentic AI原理图#GenerativeAI #ArtificialIntelligence #MI #MachineLearning


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: how agentic ai works agent喜欢这张agentic AI原理图#GenerativeAI #ArtificialIntelligence #MI #MachineLearning

</details>


### [74] [两篇论文 LongRAG 和 CRAG ！两招教你优化上下文，让<em class="highlight">大模型</em>回答更精准](http://mp.weixin.qq.com/s?__biz=MzAxOTcyNjgxMA==&mid=2650185452&idx=3&sn=7c2aa78758a9d54b1523c6c2d9f28b0d&chksm=82cfecba2f91af1a148546b4e22618f557f108615f3f78d3874c0a3cbdf5ef58515d3f7e9803#rd)
*一粒云*

Main category: wechat.article

TL;DR: 然后返回这些子文档所属的父文档（即大块上下文）给大模型。伪代码示例：# 1. 加载文档并创建父子文档parent_splitter = RecursiveCharacterTextSplitter（chunk_size=2000）


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 然后返回这些子文档所属的父文档（即大块上下文）给大模型。伪代码示例：# 1. 加载文档并创建父子文档parent_splitter = RecursiveCharacterTextSplitter（chunk_size=2000）

</details>


### [75] [DeepSeek-OCR，<em class="highlight">大模型</em>的新范式！](http://mp.weixin.qq.com/s?__biz=MzIwMzA0NjM3OQ==&mid=2465333379&idx=1&sn=3d262dfb3e58bd90d3bc592a7a0105c1&chksm=80b76a0826daed8926309cf7aa76f99b92f17ff80c3ced8a7fcb9729c43eb6c97463e6ae3ed3#rd)
*易程LEO*

Main category: wechat.article

TL;DR: 对于 大模型开发和应用构建中使用大模型而言，这也是一个重大的效率和成本突破：更低的内存成本： 视觉标记是紧凑的。更快的推理速度： 更少的标记意味着更少的 FLOPs。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 对于 大模型开发和应用构建中使用大模型而言，这也是一个重大的效率和成本突破：更低的内存成本： 视觉标记是紧凑的。更快的推理速度： 更少的标记意味着更少的 FLOPs。

</details>


### [76] [哪款AI最懂中国教育？2025教育<em class="highlight">大模型</em>测试成绩单出来了！](http://mp.weixin.qq.com/s?__biz=MzAxMjE0NTYzMQ==&mid=2651758018&idx=1&sn=8e0367b9e6fe557f4cfbc5f0a98e9f9f&chksm=81cf8cc0d0d179ec2f844ffc2d3e3d950bf6459b86f8e295f1bc2129509950edbf13b086f0d8#rd)
*实用教育技术*

Main category: wechat.article

TL;DR: 今天给大家带来一份最新的教育大模型评测结果，这份"成绩单"来自互联网教育国家工程研究中心的专业评测，包含了GPT-5和国内五款主流大模型（DeepSeek-V3.2-EXP、讯飞星火X1、豆包1.6-seed-thinking、GLM4.6-thinking、Qwen3-235B-A22B，以随


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 今天给大家带来一份最新的教育大模型评测结果，这份"成绩单"来自互联网教育国家工程研究中心的专业评测，包含了GPT-5和国内五款主流大模型（DeepSeek-V3.2-EXP、讯飞星火X1、豆包1.6-seed-thinking、GLM4.6-thinking、Qwen3-235B-A22B，以随

</details>


### [77] [AI<em class="highlight">大模型</em>“卡脖子”难题，这次有解了！](http://mp.weixin.qq.com/s?__biz=MzAwMDEzNTc1Mw==&mid=2247532771&idx=2&sn=97779df059f0c2bbbd6d43de84ff97d9&chksm=9b94d9f0f5ae4a1a6f0f964789718b0fb6edab91b5d01e067715948e74c387fa66e4aced8602#rd)
*51CTO*

Main category: wechat.article

TL;DR: 今天给大家推荐的《AI大模型硬件架构》，专治各种“硬件不服”，这不是那种充斥着晦涩术语的传统硬件课，而是一套真正从AI开发者角度出发的实战解决方案。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 今天给大家推荐的《AI大模型硬件架构》，专治各种“硬件不服”，这不是那种充斥着晦涩术语的传统硬件课，而是一套真正从AI开发者角度出发的实战解决方案。

</details>


### [78] [MiniMax出息了！国内开源<em class="highlight">大模型</em>首次闯入全球前五！](http://mp.weixin.qq.com/s?__biz=MzAwNDM2ODIxOQ==&mid=2247489848&idx=1&sn=1a5d55097d00d6a57a531427d7a43747&chksm=9a55cf9d4251e5636b200ab302d6cfbd4ae1245269f4f8e56e2117cdc9cc99c8d14dc387d72d#rd)
*二爷Hack*

Main category: wechat.article

TL;DR: 也是中国开源大模型首次闯入全球前五。lis minimax （official） .. @minimax__ai we're open-sourcing minimax m2 - agent & code native， at 8% claude sonnet price， ~2x faster global free for a limited time via minimax agent & ...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 也是中国开源大模型首次闯入全球前五。lis minimax （official） .. @minimax__ai we're open-sourcing minimax m2 - agent & code native， at 8% claude sonnet price， ~2x faster global free for a limited time via minimax agent & api - advanced coding capability： engineered for end-

</details>


### [79] [国内主流人工智能<em class="highlight">大模型</em>评测汇总](http://mp.weixin.qq.com/s?__biz=MzYyNDQ5ODcwMQ==&mid=2247483716&idx=1&sn=c8ecbb5fe3e397f73b39722f1a15cddd&chksm=f1bf1965e2eb24cd83057b9437d7f89ec780ccd9a5d88a3fe1aa41cc56e94f68bc1a3ad8356e#rd)
*神谕天玑智能实验室*

Main category: wechat.article

TL;DR: 验。豆包图像创作模型4.0 豆包大模型1.6 lite 豆包3d模型。豆包角色扮演模型 doubao-seedream-4.0 tt doubao-seed-1.6-lite doubao-seed3d tt doubao-seed-character 4k超高清直出，超强主体一致性，支持多参考... 更高性价比，常见任务优选 高精度几何


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 验。豆包图像创作模型4.0 豆包大模型1.6 lite 豆包3d模型。豆包角色扮演模型 doubao-seedream-4.0 tt doubao-seed-1.6-lite doubao-seed3d tt doubao-seed-character 4k超高清直出，超强主体一致性，支持多参考... 更高性价比，常见任务优选 高精度几何

</details>


### [80] [揭秘<em class="highlight">大模型</em>背后的“信息加工密码”与“进化逻辑”！浙大“DeepSeek信息与AI融合之旅”第五期来了](http://mp.weixin.qq.com/s?__biz=MjM5NDEyMTY4MQ==&mid=2650874634&idx=1&sn=49928968cba2fbac68e22ac2788b1bfd&chksm=bc53596c1d8adcd2aa8482e084ce067b7398d3f803f6988ec4703960f18801e870e8a06b47e0#rd)
*科学+*

Main category: wechat.article

TL;DR: 大模型和强化学习组团“开挂”，是智能体开挂的关键——大模型是“知识大脑”，强化学习当“教练”，通过试错和奖励让大模型更贴心、更聪明！二者配合，大模型铺路，强化学习加速，智能体从“萌新”变“王者”。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型和强化学习组团“开挂”，是智能体开挂的关键——大模型是“知识大脑”，强化学习当“教练”，通过试错和奖励让大模型更贴心、更聪明！二者配合，大模型铺路，强化学习加速，智能体从“萌新”变“王者”。

</details>


### [81] [厘清<em class="highlight">大模型</em>金融应用路径与治理重点](http://mp.weixin.qq.com/s?__biz=MzA3OTI4MjY5Nw==&mid=2650288149&idx=2&sn=b8d4544e3b03df0962624715ff30bafe&chksm=86130fce62782b8c2db7c615b145d83ca26c0506427d024b814ada1591388f65fb5d8d0ced54#rd)
*科技金融*

Main category: wechat.article

TL;DR: 近期，腾讯研究院联合毕马威发布的《2025金融业大模型应用报告：体系落地，价值共生》认为，大模型的应用目前正呈现两大趋势：一是从内部提效向核心创收领域加速转移，在智能理财助理、财富管理、保险代理人等客户触


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 近期，腾讯研究院联合毕马威发布的《2025金融业大模型应用报告：体系落地，价值共生》认为，大模型的应用目前正呈现两大趋势：一是从内部提效向核心创收领域加速转移，在智能理财助理、财富管理、保险代理人等客户触

</details>


### [82] [OpenAI前CTO Mira Murati团队又放大招，让<em class="highlight">大模型</em>训练成本暴降10倍](http://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&mid=2247612198&idx=1&sn=7259a69894632ad5db44d2d480ebb7e3&chksm=96d533f80719ce279f892b15fe4fd48c826b24dc7d6bf328f82c8c521a8be7b159395ea45167#rd)
*夕小瑶科技说*

Main category: wechat.article

TL;DR: 一种能以 1/10 成本达到强化学习同等效果的大模型后训练新方法。mira murati @miramurati · 14h combining the benefits of rl and sft with on-policy distillation， a promising approach for training small models for domain performance a...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 一种能以 1/10 成本达到强化学习同等效果的大模型后训练新方法。mira murati @miramurati · 14h combining the benefits of rl and sft with on-policy distillation， a promising approach for training small models for domain performance and continual learning.。

</details>


### [83] [<em class="highlight">大模型</em>最热门的7大框架介绍！](http://mp.weixin.qq.com/s?__biz=MzkxNjQzMDU3Ng==&mid=2247483947&idx=1&sn=5f77da492fc16a38d4a1ee6823eedad7&chksm=c0cfcbc1310a7c2d4e9d382ed3d2eb5718dc290e985f3fd4cfeed540195a2bfa712e30704a65#rd)
*Ai大模型知识营*

Main category: wechat.article

TL;DR: 7大框架介绍！1。langchain。langchain是一个用于构建基于大型语言模型 （llm）应用程序的开源框架，旨在帮助开发者将 语言模型与其他数据源、工具和计算资源结合，创 建更复杂且实用的应用。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 7大框架介绍！1。langchain。langchain是一个用于构建基于大型语言模型 （llm）应用程序的开源框架，旨在帮助开发者将 语言模型与其他数据源、工具和计算资源结合，创 建更复杂且实用的应用。

</details>


### [84] [数据智能引擎：<em class="highlight">大模型</em>的现在与未来](http://mp.weixin.qq.com/s?__biz=MzA4ODg1NzU5NA==&mid=2247512506&idx=7&sn=a187a0058d9bb06a99e812da2022dca7&chksm=91e3dc67a298b855062274b125592568a25bcd263f50a43b287b334f4add26147c9779b5760c#rd)
*湖北政务服务*

Main category: wechat.article

TL;DR: 实际上随着通用预训练大模型的效果难以进一步得到飞跃式提升，大模型领域的研究已进入后训练时代，在规模扩展（Scaling）、强化学习、微调（Fine-Tuning）这三类主要后训练技术的支持下，通过针对特定目标模态进行后训练，


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 实际上随着通用预训练大模型的效果难以进一步得到飞跃式提升，大模型领域的研究已进入后训练时代，在规模扩展（Scaling）、强化学习、微调（Fine-Tuning）这三类主要后训练技术的支持下，通过针对特定目标模态进行后训练，

</details>


### [85] [2025必看AI干货!《<em class="highlight">大模型</em>/AIGC/GPT-4/Transformer/DL/KG/NLP/CV AI+X》集合](http://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&mid=2247671591&idx=5&sn=246ced40da6387167094639d054d8176&chksm=fd93564d95dbf821dc4a98fccd4087f9cee4bc6b937ac81eef49cacf26711ea3bd81c64e994c#rd)
*专知*

Main category: wechat.article

TL;DR: 大规模视觉模型中的提示式适配：综述 多模态大语言模型的自我改进：综述 video-lmm后训练：多模态大模型的视频推理深度解析 基于大语言模型的智能体易产生幻觉：分类体系、方法与未来方向综述


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大规模视觉模型中的提示式适配：综述 多模态大语言模型的自我改进：综述 video-lmm后训练：多模态大模型的视频推理深度解析 基于大语言模型的智能体易产生幻觉：分类体系、方法与未来方向综述

</details>


### [86] [国产AI<em class="highlight">大模型</em>杀疯了！MiniMax M2性能刚猛，价格却只要Claude的零头？](http://mp.weixin.qq.com/s?__biz=MzYyMjQwMzQyNw==&mid=2247484263&idx=1&sn=591d8e86b80797763c4bee8284596b70&chksm=fe54ed930af848678aeade72013d43ba7fed16ffd2dc07d4bdd507ff2eb30c1f1d4191568ee9#rd)
*AI 先锋洞察站*

Main category: wechat.article

TL;DR: 直接甩出了新一代文本大模型——MiniMax - M2。这玩意儿不光性能能和硅谷那帮巨头“硬碰硬”，价格更是低到让人大喊“离谱”，简直是咱们打工人和开发者的“梦中情模”！


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 直接甩出了新一代文本大模型——MiniMax - M2。这玩意儿不光性能能和硅谷那帮巨头“硬碰硬”，价格更是低到让人大喊“离谱”，简直是咱们打工人和开发者的“梦中情模”！

</details>


### [87] [全球开源<em class="highlight">大模型</em>杭州霸榜被终结，上海MiniMax M2发布即爆单，百万Tokens仅需8元人民币](http://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247837058&idx=1&sn=c2dd7613d86fc8b9c91944bbc91961e3&chksm=e91a2d5d727444a08ceb8c5a75792b3dbd8d1acb9459819dc5f04e7249f98f9f5d0e2b4c2865#rd)
*量子位*

Main category: wechat.article

TL;DR: 以Artificial Analysis的成绩为基准，Minimax绘制了一张图来比较各大模型性价比（横轴越向右成本越低）。artificial analysis intelligence index； output price： usd per 1m tokens most attractive quadrant gpt-5 codex （high） gpt-5 （high） grok 4 cl...


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 以Artificial Analysis的成绩为基准，Minimax绘制了一张图来比较各大模型性价比（横轴越向右成本越低）。artificial analysis intelligence index； output price： usd per 1m tokens most attractive quadrant gpt-5 codex （high） gpt-5 （high） grok 4 claude 4.5 sonnet

</details>


### [88] [上交2025最新-《动手学<em class="highlight">大模型</em>》实战教程及ppt分享！](http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571743&idx=2&sn=e47d82e510ea94e9cc06c73bb943bd13&chksm=96ba8745a17857fa6c45956fdc72fa106fff8eb67b418d2095753f743ee98afaeece4e7901de#rd)
*深度学习与NLP*

Main category: wechat.article

TL;DR: 多模态大语言模型是否能够帮助实现AGI？大模型智能体与安全 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识到风险威胁吗？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 多模态大语言模型是否能够帮助实现AGI？大模型智能体与安全 大模型智能体迈向了未来操作系统之旅。然而，大模型在开放智能体场景中能意识到风险威胁吗？

</details>


### [89] [<em class="highlight">大模型</em>技术体系](http://mp.weixin.qq.com/s?__biz=MzUyMTcyMjM3Mw==&mid=2247484869&idx=1&sn=2a940782cc3bce07e2b40ca6509f2321&chksm=f8028e9d1f05d6666056830c1098941e9a110c6821de7f982b0c78a8c104e699c2abea082cda#rd)
*数字化焦点*

Main category: wechat.article

TL;DR: 四、应用层技术。一 基础理论与模型架构技术。大模型技术体系。三、 工程化部署技术。二、 训练与优化技术。decoder-only架构（gpt系列，侧重生成） 1.transformer架构及变体。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 四、应用层技术。一 基础理论与模型架构技术。大模型技术体系。三、 工程化部署技术。二、 训练与优化技术。decoder-only架构（gpt系列，侧重生成） 1.transformer架构及变体。

</details>


### [90] [牛！<em class="highlight">大模型</em>的9大核心技术解析！](http://mp.weixin.qq.com/s?__biz=MzU0MjMwNzI0OA==&mid=2247497493&idx=1&sn=a7ac0b190b365f1976aa2955713bcb9a&chksm=fa4e009e4d1c1dc396f0fe060cc029cc2e3ebc27cdffe50348494d15f9e733df1a1c6b105956#rd)
*AI科技在线*

Main category: wechat.article

TL;DR: ai 智能体架构设计 1 2 planning 3 a action 5 llm 工具集 大模型 8 o 9 observation 10 10 final answer 掘金技术社区 @聚客ai 二、Agentic AIAgentic AI 代表多智能体协作的系统架构。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: ai 智能体架构设计 1 2 planning 3 a action 5 llm 工具集 大模型 8 o 9 observation 10 10 final answer 掘金技术社区 @聚客ai 二、Agentic AIAgentic AI 代表多智能体协作的系统架构。

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [91] [Accelerate developer productivity with these 9 open source AI and MCP projects](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fopen-source%2Faccelerate-developer-productivity-with-these-9-open-source-ai-and-mcp-projects%2F%3Futm_source=tldrdevops/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/pw3yAP_Mj46lQ10QcCfIoW09Ee7WRb2a-4ZjtLydZ4c=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 介绍了9个基于Model Context Protocol的开源AI项目，这些项目利用MCP协议创建AI原生工作流，连接智能代理与工具、代码库和浏览器，提升开发者生产力。


<details>
  <summary>Details</summary>
Motivation: 旨在展示如何通过MCP协议构建新一代智能工具化系统，加速开发者工作效率，得到微软OSPO、GitHub Copilot和VS Code的支持。

Method: 通过9个开源项目展示框架集成、AI增强的开发者体验和可扩展自动化，利用Model Context Protocol连接AI代理与各种开发工具。

Result: 成功创建了能够连接代理与工具、代码库和浏览器的AI原生工作流，实现了智能化的开发工具链。

Conclusion: MCP协议为构建新一代智能开发工具提供了有效框架，开源项目展示了AI在提升开发者生产力方面的巨大潜力。

Abstract: Accelerate developer productivity with these 9 open source AI and MCP projects (3 minute read) Developers are leveraging the Model Context Protocol to create AI-native workflows that connect agents with tools, codebases, and browsers, giving rise to a new generation of intelligent, agentic tooling. Supported by Microsoft's OSPO, GitHub Copilot, and VS Code, nine open source projects showcase framework integrations, AI-enhanced developer experience, and scalable automation to demonstrate how M...

</details>


### [92] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrdevops%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019a2558f4df-96141729-074a-4a2c-abb4-2a1cbab3502f-000000/MJ-IdSTsXD0SeSgXSy121_Pm5WYZYlFBbPjhRU4Qc-s=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 工程师在代码审查中常犯的错误：在LLM时代过于关注代码差异而忽视整体系统理解


<details>
  <summary>Details</summary>
Motivation: 指出当前工程师在代码审查中的常见误区，特别是在LLM时代需要调整审查方法

Method: 通过观察和分析工程师在代码审查中的实际做法，识别常见错误模式

Result: 发现工程师往往过于关注代码差异的细节，而忽略了变更如何融入整体系统的理解

Conclusion: 在LLM时代，代码审查需要从单纯的差异检查转向更全面的系统理解，关注变更对整体架构的影响

Abstract: Mistakes I see engineers making in their code reviews (8 minute read) Many engineers approach code review incorrectly in the LLM era, focusing too narrowly on the diff rather than understanding how the change fits into the overall system.

</details>


### [93] [Code like a surgeon](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.geoffreylitt.com%2F2025%2F10%2F24%2Fcode-like-a-surgeon%3Futm_source=tldrwebdev/1/0100019a255bdee1-0c9144f2-ddb5-40cc-8fc7-ed8cc2e52da2-000000/lXIFBye_-ECDBmfj9Qzg-XEOL2bVp3GSflSNDvkly3M=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 采用"软件外科医生"方法进行编码，使用AI工具处理次要任务，让开发者专注于核心设计和关键思考


<details>
  <summary>Details</summary>
Motivation: 提高开发效率，通过AI工具分担重复性任务，让人类开发者能够专注于更重要的设计决策和创造性工作

Method: 将AI工具作为辅助工具处理编码中的次要任务，人类开发者负责核心设计和关键决策

Result: 开发效率提升，开发者能够更专注于创造性工作，代码质量得到改善

Conclusion: "软件外科医生"方法是有效的开发策略，AI与人类开发者协作能够最大化各自的优势

Abstract: Code like a surgeon (5 minute read) Taking a "software surgeon" approach to coding means using AI tools for secondary tasks and having devs focus on core design and critical thinking.

</details>


### [94] [OpenAI Atlas Omnibox Prompt Injection: URLs That Become Jailbreaks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fneuraltrust.ai%2Fblog%2Fopenai-atlas-omnibox-prompt-injection%3Futm_source=tldrinfosec/1/0100019a25c8af65-d9fa5641-2669-408a-8358-61b883501c54-000000/NZ2o88NOytiaI-GeBNiONzG6wBFelQqVb0wuYwkN_3s=428)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI Atlas浏览器存在提示注入漏洞，攻击者可以创建看似URL但包含自然语言命令的字符串，覆盖用户指令执行恶意操作。


<details>
  <summary>Details</summary>
Motivation: 研究Agentic浏览器（如OpenAI Atlas）在输入边界未严格强制执行时的安全漏洞，揭示潜在的攻击向量。

Method: 通过创建伪装成URL但包含自然语言命令的字符串，利用omnibox可能将其解释为用户意图的漏洞进行攻击测试。

Result: 发现攻击者可以覆盖用户命令，导致意外或有害操作，如打开钓鱼网站或执行破坏性指令。

Conclusion: Agentic浏览器需要严格实施输入边界检查，防止提示注入攻击，确保用户意图不被恶意覆盖。

Abstract: OpenAI Atlas Omnibox Prompt Injection: URLs That Become Jailbreaks (5 minute read) Agentic browsers like OpenAI Atlas are vulnerable to exploitation if input boundaries are not strictly enforced. Attackers can create strings that look like URLs but contain natural-language commands, which the omnibox might interpret as user intent. This can allow malicious actors to override user commands, leading to unintended or harmful actions, such as opening phishing sites or executing destructive instru...

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [95] [Software Engineering Agents for Embodied Controller Generation : A Study in Minigrid Environments](https://arxiv.org/abs/2510.21902)
*Timothé Boulet,Xavier Hinaut,Clément Moulin-Frier*

Main category: cs.SE

TL;DR: 首次评估SWE-Agents在具身任务控制器生成中的表现，比较不同信息访问条件对性能的影响


<details>
  <summary>Details</summary>
Motivation: 探索SWE-Agents在需要良好信息发现的具身任务中的性能，该领域此前未被充分研究

Method: 使用Mini-SWE-Agent解决Minigrid环境中的20个多样化具身任务，比较有无环境源代码访问和不同交互探索能力下的性能

Result: 量化了不同信息访问水平对SWE-Agent在具身任务中性能的影响，分析了静态代码分析与动态探索的相对重要性

Conclusion: 确立了具身任务控制器生成作为SWE-Agents关键评估领域，为未来高效推理系统研究提供基线结果

Abstract: Software Engineering Agents (SWE-Agents) have proven effective for
traditional software engineering tasks with accessible codebases, but their
performance for embodied tasks requiring well-designed information discovery
remains unexplored. We present the first extended evaluation of SWE-Agents on
controller generation for embodied tasks, adapting Mini-SWE-Agent (MSWEA) to
solve 20 diverse embodied tasks from the Minigrid environment. Our experiments
compare agent performance across different information access conditions: with
and without environment source code access, and with varying capabilities for
interactive exploration. We quantify how different information access levels
affect SWE-Agent performance for embodied tasks and analyze the relative
importance of static code analysis versus dynamic exploration for task solving.
This work establishes controller generation for embodied tasks as a crucial
evaluation domain for SWE-Agents and provides baseline results for future
research in efficient reasoning systems.

</details>


### [96] [TOM-SWE: User Mental Modeling For Software Engineering Agents](https://arxiv.org/abs/2510.21903)
*Xuhui Zhou,Valerie Chen,Zora Zhiruo Wang,Graham Neubig,Maarten Sap,Xingyao Wang*

Main category: cs.SE

TL;DR: ToM-SWE是一个双代理架构，将主要软件工程代理与轻量级心智理论代理配对，通过建模用户心理状态来提升编码任务中的用户意图理解和追踪能力。


<details>
  <summary>Details</summary>
Motivation: 现有编码代理在处理用户意图不明确或上下文相关的指令时表现不佳，需要更好的用户意图推断和追踪能力。

Method: 采用双代理架构：主SWE代理负责编码任务，ToM代理专门建模用户心理状态，推断用户目标、约束和偏好，并维护持久用户记忆。

Result: 在状态化SWE基准测试中，ToM-SWE达到59.7%的任务成功率，显著优于OpenHands的18.1%；在专业开发者三周研究中，86%的情况下被认为有用。

Conclusion: 状态化用户建模对实用编码代理具有重要价值，ToM-SWE架构能显著提升任务成功率和用户满意度。

Abstract: Recent advances in coding agents have made them capable of planning, editing,
running, and testing complex code bases. Despite their growing ability in
coding tasks, these systems still struggle to infer and track user intent,
especially when instructions are underspecified or context-dependent. To bridge
this gap, we introduce ToM-SWE, a dual-agent architecture that pairs a primary
software-engineering (SWE) agent with a lightweight theory-of-mind (ToM)
partner agent dedicated to modeling the user's mental state. The ToM agent
infers user goals, constraints, and preferences from instructions and
interaction history, maintains a \textbf{persistent memory} of the user, and
provides user-related suggestions to the SWE agent. In two software engineering
benchmarks (ambiguous SWE-bench and stateful SWE-bench), ToM-SWE improves task
success rates and user satisfaction. Notably, on the stateful SWE benchmark, a
newly introduced evaluation that provides agents with a user simulator along
with previous interaction histories, ToM-SWE achieves a substantially higher
task success rate of 59.7\% compared to 18.1\% for OpenHands, a
state-of-the-art SWE agent. Furthermore, in a three-week study with
professional developers using ToM-SWE in their daily work, participants found
it useful 86\% of the time, underscoring the value of stateful user modeling
for practical coding agents.

</details>


### [97] [LSPRAG: LSP-Guided RAG for Language-Agnostic Real-Time Unit Test Generation](https://arxiv.org/abs/2510.22210)
*Gwihwan Go,Quan Zhang,Chijin Zhou,Zhao Wei,Yu Jiang*

Main category: cs.SE

TL;DR: LSPRAG是一个利用语言服务器协议(LSP)为LLM提供精确符号定义和引用的框架，用于实时、语言无关的单元测试生成，显著提高了测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成方法难以跨语言泛化且无法实时运行，而基于检索增强生成的方法要么依赖不精确的相似性搜索，要么需要构建昂贵的语言特定静态分析管道。

Method: 利用现成的LSP后端为LLM提供精确的符号定义和引用，实现语言感知的上下文检索，最小化每种语言的工程工作量。

Result: 在Java、Go和Python的开源项目上评估，相比基线最佳性能，LSPRAG将行覆盖率分别提高了213.31%、174.55%和31.57%。

Conclusion: LSPRAG通过重用成熟的LSP服务器，为LLM提供了语言感知的上下文检索，显著提升了跨语言单元测试生成的性能。

Abstract: Automated unit test generation is essential for robust software development,
yet existing approaches struggle to generalize across multiple programming
languages and operate within real-time development. While Large Language Models
(LLMs) offer a promising solution, their ability to generate high coverage test
code depends on prompting a concise context of the focal method. Current
solutions, such as Retrieval-Augmented Generation, either rely on imprecise
similarity-based searches or demand the creation of costly, language-specific
static analysis pipelines. To address this gap, we present LSPRAG, a framework
for concise-context retrieval tailored for real-time, language-agnostic unit
test generation. LSPRAG leverages off-the-shelf Language Server Protocol (LSP)
back-ends to supply LLMs with precise symbol definitions and references in real
time. By reusing mature LSP servers, LSPRAG provides an LLM with language-aware
context retrieval, requiring minimal per-language engineering effort. We
evaluated LSPRAG on open-source projects spanning Java, Go, and Python.
Compared to the best performance of baselines, LSPRAG increased line coverage
by up to 174.55% for Golang, 213.31% for Java, and 31.57% for Python.

</details>


### [98] [Ten Simple Rules for AI-Assisted Coding in Science](https://arxiv.org/abs/2510.22254)
*Eric W. Bridgeford,Iain Campbell,Zijao Chen,Zhicheng Lin,Harrison Ritz,Joachim Vandekerckhove,Russell A. Poldrack*

Main category: cs.SE

TL;DR: 提出了10条AI辅助编程的实用规则，旨在平衡AI能力利用与科学方法严谨性，确保科学计算中的代码质量和研究有效性。


<details>
  <summary>Details</summary>
Motivation: AI编程工具在加速软件开发方面展现出潜力，但在科学计算应用中引发了关于代码质量和科学有效性的关键问题，需要建立指导原则来确保研究完整性。

Method: 围绕四个关键主题制定10条实践规则：问题准备与理解、上下文管理与交互、测试与验证、代码质量保证与迭代改进。

Result: 建立了一套系统性的AI辅助编程指导原则，帮助研究人员在利用AI加速软件开发的同时，确保代码的可靠性、可重复性和科学有效性。

Conclusion: 这些规则强调在编码决策中保持人类主导权，建立稳健的验证程序，并保护方法论严谨研究所需的领域专业知识。

Abstract: While AI coding tools have demonstrated potential to accelerate software
development, their use in scientific computing raises critical questions about
code quality and scientific validity. In this paper, we provide ten practical
rules for AI-assisted coding that balance leveraging capabilities of AI with
maintaining scientific and methodological rigor. We address how AI can be
leveraged strategically throughout the development cycle with four key themes:
problem preparation and understanding, managing context and interaction,
testing and validation, and code quality assurance and iterative improvement.
These principles serve to emphasize maintaining human agency in coding
decisions, establishing robust validation procedures, and preserving the domain
expertise essential for methodologically sound research. These rules are
intended to help researchers harness AI's transformative potential for faster
software development while ensuring that their code meets the standards of
reliability, reproducibility, and scientific validity that research integrity
demands.

</details>


### [99] [Harnessing the Power of Large Language Models for Software Testing Education: A Focus on ISTQB Syllabus](https://arxiv.org/abs/2510.22318)
*Tuan-Phong Ngo,Bao-Ngoc Duong,Tuan-Anh Hoang,Joshua Dwight,Ushik Shrestha Khwakhali*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型如何补充ISTQB框架在高等教育中的应用，创建了ISTQB对齐数据集，开发了领域优化的提示方法，并评估了LLMs在软件测试教育中的潜力。


<details>
  <summary>Details</summary>
Motivation: 软件测试在软件工程领域和教育中至关重要，但ISTQB认证框架与最新生成式人工智能的结合应用尚未充分探索，特别是在高等教育环境中。

Method: 创建了包含28个样本考试和1145个问题的ISTQB对齐数据集，开发了领域优化的提示方法，系统评估了最先进的LLMs在该数据集上的表现。

Result: 研究发现LLMs在支持ISTQB认证准备方面具有潜力，通过优化的提示方法可以显著提高LLM在ISTQB任务上的精确度和解释质量。

Conclusion: LLMs有望支持ISTQB认证准备，为软件工程高等教育中更广泛地使用LLMs奠定了基础，并提供了将LLMs整合到软件测试教育中的可行建议。

Abstract: Software testing is a critical component in the software engineering field
and is important for software engineering education. Thus, it is vital for
academia to continuously improve and update educational methods to reflect the
current state of the field. The International Software Testing Qualifications
Board (ISTQB) certification framework is globally recognized and widely adopted
in industry and academia. However, ISTQB-based learning has been rarely applied
with recent generative artificial intelligence advances. Despite the growing
capabilities of large language models (LLMs), ISTQB-based learning and
instruction with LLMs have not been thoroughly explored. This paper explores
and evaluates how LLMs can complement the ISTQB framework for higher education.
The findings present four key contributions: (i) the creation of a
comprehensive ISTQB-aligned dataset spanning over a decade, consisting of 28
sample exams and 1,145 questions; (ii) the development of a domain-optimized
prompt that enhances LLM precision and explanation quality on ISTQB tasks;
(iii) a systematic evaluation of state-of-the-art LLMs on this dataset; and
(iv) actionable insights and recommendations for integrating LLMs into software
testing education. These findings highlight the promise of LLMs in supporting
ISTQB certification preparation and offer a foundation for their broader use in
software engineering at higher education.

</details>


### [100] [Finding the Needle in the Crash Stack: Industrial-Scale Crash Root Cause Localization with AutoCrashFL](https://arxiv.org/abs/2510.22530)
*Sungmin Kang,Sumi Yun,Jingun Hong,Shin Yoo,Gabin An*

Main category: cs.SE

TL;DR: AutoCrashFL是一个基于LLM代理的故障定位方法，仅需崩溃转储和源代码仓库即可在工业级软件中定位崩溃原因，在SAP HANA项目中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统故障定位方法需要动态分析如覆盖率分析，这在大型工业软件中成本过高，因此需要一种仅依赖崩溃转储和源代码的轻量级方法。

Method: 提出AutoCrashFL LLM代理，仅使用程序崩溃转储和对应源代码仓库进行故障定位，无需动态分析。

Result: 在SAP HANA（超过3500万行代码）的真实崩溃测试中，AutoCrashFL在top位置识别出30%的崩溃，而基线方法仅为17%。

Conclusion: AutoCrashFL在工业规模上具有实际部署价值，对复杂bug更有效且能指示结果置信度。

Abstract: Fault Localization (FL) aims to identify root causes of program failures. FL
typically targets failures observed from test executions, and as such, often
involves dynamic analyses to improve accuracy, such as coverage profiling or
mutation testing. However, for large industrial software, measuring coverage
for every execution is prohibitively expensive, making the use of such
techniques difficult. To address these issues and apply FL in an industrial
setting, this paper proposes AutoCrashFL, an LLM agent for the localization of
crashes that only requires the crashdump from the Program Under Test (PUT) and
access to the repository of the corresponding source code. We evaluate
AutoCrashFL against real-world crashes of SAP HANA, an industrial software
project consisting of more than 35 million lines of code. Experiments reveal
that AutoCrashFL is more effective in localization, as it identified 30%
crashes at the top, compared to 17% achieved by the baseline. Through thorough
analysis, we find that AutoCrashFL has attractive practical properties: it is
relatively more effective for complex bugs, and it can indicate confidence in
its results. Overall, these results show the practicality of LLM agent
deployment on an industrial scale.

</details>


### [101] [Does In-IDE Calibration of Large Language Models work at Scale?](https://arxiv.org/abs/2510.22614)
*Roham Koohestani,Agnia Sergeyuk,David Gros,Claudio Spiess,Sergey Titov,Prem Devanbu,Maliheh Izadi*

Main category: cs.SE

TL;DR: 研究代码模型在IDE环境中的置信度校准可行性，发现通用的后处理校准方法不能显著改善置信度信号可靠性，而个性化校准需要大量用户数据。同时通过设计研究发现开发者偏好使用非数值的颜色编码指示器来呈现可靠性信号。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型集成到IDE中正在改变软件工程，但AI生成代码的可靠性和实用性面临挑战。置信度校准旨在使模型概率与可接受性度量对齐，但大规模证据有限。

Method: 开发可扩展的校准框架，使用Platt缩放进行后处理校准；分析2400万次真实开发者交互数据；进行多阶段设计研究，包括基于场景的设计、半结构化访谈和调查验证。

Result: 通用的后处理校准模型平均不能改善置信度信号可靠性；个性化校准有效但高度依赖用户交互数据量；开发者明确偏好使用非数值的颜色编码指示器。

Conclusion: 在IDE环境中应用代码模型置信度校准具有挑战性，通用校准方法效果有限，而个性化校准需要大量数据支持。可视化呈现方面，颜色编码的非数值指示器更受开发者欢迎。

Abstract: The introduction of large language models into integrated development
environments (IDEs) is revolutionizing software engineering, yet it poses
challenges to the usefulness and reliability of Artificial
Intelligence-generated code. Post-hoc calibration of internal model confidences
aims to align probabilities with an acceptability measure. Prior work suggests
calibration can improve alignment, but at-scale evidence is limited. In this
work, we investigate the feasibility of applying calibration of code models to
an in-IDE context. We study two aspects of the problem: (1) the technical
method for implementing confidence calibration and improving the reliability of
code generation models, and (2) the human-centered design principles for
effectively communicating reliability signal to developers. First, we develop a
scalable and flexible calibration framework which can be used to obtain
calibration weights for open-source models using any dataset, and evaluate
whether calibrators improve the alignment between model confidence and
developer acceptance behavior. Through a large-scale analysis of over 24
million real-world developer interactions across multiple programming
languages, we find that a general, post-hoc calibration model based on
Platt-scaling does not, on average, improve the reliability of model confidence
signals. We also find that while dynamically personalizing calibration to
individual users can be effective, its effectiveness is highly dependent on the
volume of user interaction data. Second, we conduct a multi-phase design study
with 3 expert designers and 153 professional developers, combining
scenario-based design, semi-structured interviews, and survey validation,
revealing a clear preference for presenting reliability signals via
non-numerical, color-coded indicators within the in-editor code generation
workflow.

</details>


### [102] [TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation](https://arxiv.org/abs/2510.23010)
*Ming-Tung Shen,Yuh-Jzer Joung*

Main category: cs.SE

TL;DR: 提出了TALM框架，一种集成树状结构任务分解、局部重推理和长期记忆机制的多智能体代码生成框架，在多个基准测试中表现出强大的推理性能和高效的token利用率。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成多智能体框架存在工作流程僵化和推理恢复成本高的问题，需要更灵活和高效的解决方案来处理复杂上下文管理和多步推理任务。

Method: 采用可扩展的树状协作结构，结合分治策略增强推理灵活性；引入长期记忆模块支持语义查询和先验知识集成；通过父子关系实现高效错误纠正。

Result: 在HumanEval、BigCodeBench和ClassEval基准测试中，TALM框架展现出持续强大的推理性能和高token效率。

Conclusion: TALM框架通过动态树状结构和长期记忆机制，在复杂代码生成任务中具有鲁棒性和实际应用价值。

Abstract: Agentic code generation requires large language models (LLMs) capable of
complex context management and multi-step reasoning. Prior multi-agent
frameworks attempt to address these challenges through collaboration, yet they
often suffer from rigid workflows and high reasoning recovery costs. To
overcome these limitations, we propose TALM (Tree-Structured Multi-Agent
Framework with Long-Term Memory), a dynamic framework that integrates
structured task decomposition, localized re-reasoning, and long-term memory
mechanisms. TALM employs an extensible tree-based collaboration structure. The
parent-child relationships, when combined with a divide-and-conquer strategy,
enhance reasoning flexibility and enable efficient error correction across
diverse task scopes. Furthermore, a long-term memory module enables semantic
querying and integration of prior knowledge, supporting implicit
self-improvement through experience reuse. Experimental results on HumanEval,
BigCodeBench, and ClassEval benchmarks demonstrate that TALM consistently
delivers strong reasoning performance and high token efficiency, highlighting
its robustness and practical utility in complex code generation tasks.

</details>


### [103] [Checkstyle+: Reducing Technical Debt Through The Use of Linters with LLMs](https://arxiv.org/abs/2510.23068)
*Ella Dodor,Cristina V. Lopes*

Main category: cs.SE

TL;DR: Checkstyle+结合传统Checkstyle规则与大型语言模型能力，检测需要语义理解的代码风格违规，在380个Java代码文件上验证优于标准Checkstyle。


<details>
  <summary>Details</summary>
Motivation: 传统linter基于刚性规则，无法检测需要代码语义理解的风格规则，影响代码质量和可维护性。

Method: 提出Checkstyle+混合方法，增强Checkstyle的LLM能力，识别传统规则分析无法检测的风格违规。

Result: 在380个真实Java代码文件上测试，Checkstyle+在检测语义复杂规则违规方面优于标准Checkstyle。

Conclusion: 结合LLM的混合方法能有效检测需要语义理解的代码风格问题，提升代码质量工具能力。

Abstract: Good code style improves program readability, maintainability, and
collaboration, and is an integral component of software quality. Developers,
however, often cut corners when following style rules, leading to the wide
adoption of tools such as linters in professional software development
projects. Traditional linters like Checkstyle operate using rigid, rule-based
mechanisms that effectively detect many surface-level violations. However, in
most programming languages, there is a subset of style rules that require a
more nuanced understanding of code, and fall outside the scope of such static
analysis. In this paper, we propose Checkstyle+, a hybrid approach that
augments Checkstyle with large language model (LLM) capabilities, to identify
style violations that elude the conventional rule-based analysis. Checkstyle+
is evaluated on a sample of 380 Java code files, drawn from a broader dataset
of 30,800 real-world Java programs sourced from accepted Codeforces
submissions. The results show that Checkstyle+ achieves superior performance
over standard Checkstyle in detecting violations of the semantically nuanced
rules.

</details>
