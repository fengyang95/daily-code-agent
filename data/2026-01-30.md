<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.LG](#cs.LG) [Total: 22]
- [tldr.article](#tldr.article) [Total: 30]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents](https://arxiv.org/abs/2601.20975)
*Nikita Gupta,Riju Chatterjee,Lukas Haas,Connie Tao,Andrew Wang,Chang Liu,Hidekazu Oiwa,Elena Gribovskaya,Jan Ackermann,John Blitzer,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: DeepSearchQA是一个包含900个提示的基准测试，用于评估智能体在17个不同领域中进行复杂多步信息搜索任务的能力，特别关注信息碎片整合、去重和停止条件判断等关键能力。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试主要关注单一答案检索或广泛事实性，缺乏对智能体执行复杂搜索计划、整合碎片化信息、去重和确定停止条件等关键能力的评估。需要一个新的基准来测试这些在实际深度研究任务中至关重要的能力。

Method: 创建了一个包含900个提示的手工制作数据集，涵盖17个不同领域。任务设计为因果链结构，每个步骤的成功依赖于前一步的完成，强调长时程规划和上下文保留。所有任务都基于开放网络，具有客观可验证的答案集。

Result: 对最先进的智能体架构进行全面评估显示显著的性能限制：即使最先进的模型也难以平衡高召回率和精确度。观察到不同的失败模式，包括过早停止（检索不足）和过度泛化行为（通过低置信度答案人为提高召回率）。

Conclusion: DeepSearchQA揭示了当前智能体设计中的关键改进空间，可作为推动未来研究朝着更强大深度研究能力发展的重要诊断工具。

Abstract: We introduce DeepSearchQA, a 900-prompt benchmark for evaluating agents on difficult multi-step information-seeking tasks across 17 different fields. Unlike traditional benchmarks that target single answer retrieval or broad-spectrum factuality, DeepSearchQA features a dataset of challenging, handcrafted tasks designed to evaluate an agent's ability to execute complex search plans to generate exhaustive answer lists. This shift in design explicitly tests three critical, yet under-evaluated capabilities: 1) systematic collation of fragmented information from disparate sources, 2) de-duplication and entity resolution to ensure precision, and 3) the ability to reason about stopping criteria within an open-ended search space. Each task is structured as a causal chain, where discovering information for one step is dependent on the successful completion of the previous one, stressing long-horizon planning and context retention. All tasks are grounded in the open web with objectively verifiable answer sets. Our comprehensive evaluation of state-of-the-art agent architectures reveals significant performance limitations: even the most advanced models struggle to balance high recall with precision. We observe distinct failure modes ranging from premature stopping (under-retrieval) to hedging behaviors, where agents cast an overly wide net of low-confidence answers to artificially boost recall. These findings highlight critical headroom in current agent designs and position DeepSearchQA as an essential diagnostic tool for driving future research toward more robust, deep-research capabilities.

</details>


### [2] [Multi-task Code LLMs: Data Mix or Model Merge?](https://arxiv.org/abs/2601.21115)
*Mingzhi Zhu,Boris Sobolev,Rahul Krishna,Raju Pavuluri,Stacy Patterson,Michele Merler*

Main category: cs.CL

TL;DR: 比较两种创建小型多任务代码LLM的方法：数据混合与模型合并。实验表明，在较大规模（7B参数）时模型合并效果更好，能保留96%的专用模型性能；在较小规模（2B参数）时数据混合更优。


<details>
  <summary>Details</summary>
Motivation: 研究如何在资源受限的场景下，通过高效的多任务学习策略平衡性能、约束和成本，创建小型、专业化的代码LLM用于代理框架。

Method: 比较数据混合与模型合并两种方法，在Qwen Coder和DeepSeek Coder两个模型家族上进行实验（2B和7B参数规模），针对代码生成和代码摘要任务进行微调，并在HumanEval、MBPP、CodeXGlue基准上进行评估。

Result: 在较大规模（7B）时，模型合并表现最佳，能保留96%的专用模型性能，甚至能超越单独微调的模型（Qwen Coder 2.5 7B在HumanEval上达到92.7% Pass@1 vs 90.9%）。在较小规模（2B）时，数据混合是更优策略。还引入了权重分析技术来理解不同任务对模型参数的影响。

Conclusion: 精心设计的合并和混合策略可以有效结合任务特定能力而不显著降低性能，适合资源受限的部署场景。

Abstract: Recent research advocates deploying smaller, specialized code LLMs in agentic frameworks alongside frontier models, sparking interest in efficient strategies for multi-task learning that balance performance, constraints, and costs. We compare two approaches for creating small, multi-task code LLMs: data mixing versus model merging. We conduct extensive experiments across two model families (Qwen Coder and DeepSeek Coder) at two scales (2B and 7B parameters), fine-tuning them for code generation and code summarization tasks. Our evaluation on HumanEval, MBPP, and CodeXGlue benchmarks reveals that model merging achieves the best overall performance at larger scale across model families, retaining 96% of specialized model performance on code generation tasks while maintaining summarization capabilities. Notably, merged models can even surpass individually fine-tuned models, with our best configuration of Qwen Coder 2.5 7B model achieving 92.7% Pass@1 on HumanEval compared to 90.9% for its task-specific fine-tuned equivalent. At a smaller scale we find instead data mixing to be a preferred strategy. We further introduce a weight analysis technique to understand how different tasks affect model parameters and their implications for merging strategies. The results suggest that careful merging and mixing strategies can effectively combine task-specific capabilities without significant performance degradation, making them ideal for resource-constrained deployment scenarios.

</details>


### [3] [Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/abs/2601.21204)
*Hong Liu,Jiaqi Zhang,Chao Wang,Xing Hu,Linkun Lyu,Jiaqi Sun,Xurui Yang,Bo Wang,Fengcun Li,Yulei Qian,Lingtong Si,Yerui Sun,Rumei Li,Peng Pei,Yuchen Xie,Xunliang Cai*

Main category: cs.CL

TL;DR: 该论文提出嵌入缩放作为混合专家架构的替代方案，通过系统分析和优化，开发了LongCat-Flash-Lite模型，在稀疏缩放方面实现了更好的性能效率权衡。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构在大型语言模型稀疏缩放中存在收益递减和系统瓶颈问题，需要探索正交的嵌入缩放维度来改善性能效率权衡。

Method: 通过综合分析实验，识别嵌入缩放优于专家缩放的具体机制，系统表征关键架构因素（参数预算、模型宽度与深度交互），结合定制系统优化和推测解码将稀疏性转化为推理加速。

Result: 开发了LongCat-Flash-Lite模型（685亿参数，激活约30亿），在嵌入参数分配超过300亿的情况下，不仅超越参数等效的MoE基线，还在代理和编码领域展现出与同规模模型的卓越竞争力。

Conclusion: 嵌入缩放是稀疏缩放的有效正交维度，通过系统优化可以实现比传统MoE架构更好的性能效率权衡，特别是在特定应用领域具有优势。

Abstract: While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.

</details>


### [4] [Parametric Knowledge is Not All You Need: Toward Honest Large Language Models via Retrieval of Pretraining Data](https://arxiv.org/abs/2601.21218)
*Christopher Adrian Kusuma,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文提出了一个更稳健的LLM诚实性评估基准数据集，并利用公开的预训练数据构建更诚实LLM的新方法。


<details>
  <summary>Details</summary>
Motivation: LLM虽然能回答问题，但往往不清楚自己的知识边界，导致在缺乏足够知识的主题上产生事实错误的幻觉。现有方法评估缺乏稳健性，未考虑LLM在预训练中已获取的知识。

Method: 利用Pythia（真正开源的LLM，其预训练数据公开可用）构建更稳健的评估基准数据集；提出利用预训练数据构建更诚实LLM的新方法。

Result: 提出了基于Pythia的稳健评估基准数据集，以及利用预训练数据提高LLM诚实性的新方法。

Conclusion: 通过利用公开的预训练数据，可以构建更稳健的LLM诚实性评估基准，并开发更诚实的LLM系统。

Abstract: Large language models (LLMs) are highly capable of answering questions, but they are often unaware of their own knowledge boundary, i.e., knowing what they know and what they don't know. As a result, they can generate factually incorrect responses on topics they do not have enough knowledge of, commonly known as hallucination. Rather than hallucinating, a language model should be more honest and respond with "I don't know" when it does not have enough knowledge about a topic. Many methods have been proposed to improve LLM honesty, but their evaluations lack robustness, as they do not take into account the knowledge that the LLM has ingested during its pretraining. In this paper, we propose a more robust evaluation benchmark dataset for LLM honesty by utilizing Pythia, a truly open LLM with publicly available pretraining data. In addition, we also propose a novel method for harnessing the pretraining data to build a more honest LLM.

</details>


### [5] [MoCo: A One-Stop Shop for Model Collaboration Research](https://arxiv.org/abs/2601.21257)
*Shangbin Feng,Yuyang Bai,Ziyuan Yang,Yike Wang,Zhaoxuan Tan,Jiajie Yan,Zhenyu Lei,Wenxuan Ding,Weijia Shi,Haojin Wang,Zhenting Qi,Yuru Jiang,Heng Wang,Chengsong Huang,Yu Fei,Jihan Yao,Yilun Du,Luke Zettlemoyer,Yejin Choi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MoCo是一个一站式Python库，用于大规模执行、基准测试和比较模型协作算法，包含26种协作方法和25个评估数据集，实验显示协作策略在61.0%的(模型,数据)设置中优于无协作模型。


<details>
  <summary>Details</summary>
Motivation: 当前关于模型协作的研究分散在不同研究社区，缺乏系统比较和统一框架，需要整合现有研究并建立模型协作作为一个独立的研究方向。

Method: 开发MoCo库，集成26种模型协作方法（涵盖路由、文本、logit和模型参数等跨模型信息交换层次），整合25个评估数据集，支持用户自定义数据，进行大规模实验比较。

Result: 大多数协作策略在61.0%的(模型,数据)设置中平均优于无协作模型，最有效的方法可提升25.8%。分析了协作策略的扩展性、训练/推理效率，并展示了协作系统能解决单个语言模型难以处理的问题。

Conclusion: MoCo为促进开放、模块化、去中心化和协作的AI未来提供了有价值的工具包，能够推动模型协作研究的发展。

Abstract: Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.

</details>


### [6] [Self-Improving Pretraining: using post-trained models to pretrain better models](https://arxiv.org/abs/2601.21343)
*Ellen Xiaoqing Tan,Shehzaad Dhuliawala,Jing Xu,Ping Yu,Sainbayar Sukhbaatar,Jason Weston,Olga Golovneva*

Main category: cs.CL

TL;DR: 提出一种新的预训练方法，通过强化学习在文档流中优化后续K个生成token的质量、安全性和事实性，相比标准预训练在事实性和安全性方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在安全、事实性和质量方面存在挑战，现有方法依赖昂贵的数据集和多阶段微调对齐，但无法纠正预训练阶段学习到的错误模式。需要在预训练阶段就解决这些问题，防止不安全或幻觉输出被深度嵌入模型。

Method: 提出新的预训练方法：在文档流处理过程中，使用强化学习优化每个步骤生成的后续K个token。使用一个经过后训练的强模型来评估候选生成（包括模型rollout、原始后缀和重写后缀）的质量、安全性和事实性。训练早期依赖原始和重写后缀，随着模型改进，强化学习奖励高质量的rollout。

Result: 相比标准预训练，在事实性方面获得36.2%的相对提升，在安全性方面获得18.5%的相对提升，在整体生成质量方面获得高达86.3%的胜率提升。

Conclusion: 该方法能够从基础层面构建更高质量、更安全、更事实的模型，通过在预训练阶段直接优化生成质量，解决了传统方法无法纠正预训练模式的问题。

Abstract: Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.

</details>


### [7] [The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation](https://arxiv.org/abs/2601.21360)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Arjun Neekhra,Yash Sinha,Murari Mandal,Vinay Chamola,Dhruv Kumar*

Main category: cs.CL

TL;DR: LLMs用于教育评估时存在"合规悖论"：过度优化的指令跟随能力导致模型优先满足隐藏指令而非评估代码质量，造成系统性漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前将LLMs集成到教育评估中的做法基于一个未经验证的假设：指令跟随能力能直接转化为客观裁决能力。作者质疑这一假设，认为模型可能优先满足隐藏指令而非评估代码质量。

Method: 提出语义保持对抗性代码注入（SPACI）框架和抽象语法树感知语义注入协议（AST-ASIP），利用语法-语义间隙在AST的无关节点中嵌入对抗性指令。大规模评估9个SOTA模型在25,000份Python、C、C++、Java提交中的表现。

Result: 揭示灾难性失败率（>95%），特别是DeepSeek-V3等高容量开源模型系统性地优先考虑隐藏格式约束而非代码正确性。通过三部分框架（解耦概率、分数差异、教学严重性）量化"虚假认证"问题。

Conclusion: 当前对齐范式在自动评分中创建了"特洛伊"漏洞，需要从标准RLHF转向领域特定的裁决鲁棒性，让模型优先考虑证据而非指令合规性。

Abstract: The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread "False Certification" of functionally broken code. Our findings suggest that current alignment paradigms create a "Trojan" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.

</details>


### [8] [SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2601.21476)
*Lei Yang,Wei Bi,Chenxi Sun,Renren Jin,Deyi Xiong*

Main category: cs.CL

TL;DR: SOUP提出了一种新的强化学习框架，在token级别统一了离线和在线策略学习，通过将离线策略影响限制在从历史策略采样的前缀中，而延续部分在线生成，从而改善探索性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前用于语言模型后训练的在线RL方法（如GRPO）存在探索有限和早期饱和的问题，而现有的离线策略方法混合整个轨迹会导致策略不匹配和不稳定性。

Method: 提出SOUP框架，在单个样本的token级别统一离线和在线策略学习：将离线策略影响限制在从历史策略采样的前缀中，延续部分在线生成，并使用token级别的重要性比率有效利用离线信息。

Result: 大量实验表明，SOUP始终优于标准的在线策略训练和现有的离线策略扩展方法，能够同时改善探索性和最终性能。

Conclusion: SOUP通过细粒度的单样本混合策略训练，为LLM强化学习提供了更有效的框架，能够同时利用离线数据的优势并保持训练稳定性。

Abstract: On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\textbf{S}$ingle-sample Mix-p$\textbf{O}$licy $\textbf{U}$nified $\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.

</details>


### [9] [ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas](https://arxiv.org/abs/2601.21558)
*Xiaoyu Tian,Haotian Wang,Shuaiting Chen,Hao Zhou,Kaichi Yu,Yudian Zhang,Jade Ouyang,Junxi Yin,Jiong Chen,Baoyan Guo,Lei Zhang,Junjie Tao,Yuansheng Song,Ming Cui,Chengwei Liu*

Main category: cs.CL

TL;DR: ASTRA是一个自动化框架，通过可扩展的数据合成和可验证的强化学习来训练工具增强的语言模型代理，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前训练工具使用代理的方法存在多个问题：需要人工干预、依赖不可验证的模拟环境、仅使用监督微调或强化学习、难以进行稳定的长视野多轮学习。

Method: ASTRA包含两个组件：1) 基于工具调用图静态拓扑的数据合成管道，生成多样化的结构化轨迹；2) 环境合成框架，将分解的问题-答案轨迹转换为独立、可代码执行、可规则验证的环境。统一训练方法整合了监督微调和在线强化学习。

Result: 在多个代理工具使用基准测试中，ASTRA训练的模型在可比规模下达到最先进性能，接近闭源系统同时保持核心推理能力。

Conclusion: ASTRA提供了一个完全自动化的端到端框架，通过可扩展的数据合成和可验证的强化学习有效训练工具增强的语言模型代理。

Abstract: Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.

</details>


### [10] [When "Better" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications](https://arxiv.org/abs/2601.22025)
*Daniel Commey*

Main category: cs.CL

TL;DR: 论文提出评估驱动的LLM应用开发工作流（定义、测试、诊断、修复），引入最小可行评估套件(MVES)，并通过实验展示通用提示模板可能在不同任务表现间产生权衡，强调需要基于评估的提示迭代而非通用方案。


<details>
  <summary>Details</summary>
Motivation: LLM应用评估与传统软件测试不同，输出具有随机性、高维性且对提示和模型变化敏感，需要系统化的评估方法来应对这些挑战。

Method: 提出评估驱动的工作流程（定义-测试-诊断-修复），设计最小可行评估套件(MVES)包含通用LLM应用、RAG和智能体工具使用三个层级的评估组件，结合自动检查、人工评分和LLM作为评判者等方法。

Result: 实验发现通用"改进"提示模板在不同任务表现间存在权衡：在Llama 3上，使用通用规则替换任务特定提示后，提取通过率从100%降至90%，RAG合规性从93.3%降至80%，但指令遵循能力有所提升。

Conclusion: 需要基于评估的提示迭代和谨慎的声明校准，而非依赖通用提示方案，强调系统化评估在LLM应用开发中的重要性。

Abstract: Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.
  We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.
  In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic "improved" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.
  All test suites, harnesses, and results are included for reproducibility.

</details>


### [11] [Can David Beat Goliath? On Multi-Hop Reasoning with Resource-Constrained Agents](https://arxiv.org/abs/2601.21699)
*Hojae Han,Heeyun Jung,Jongyoon Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: DAVID-GRPO：一种预算高效的强化学习框架，让小语言模型在资源受限条件下也能实现强大的多跳推理能力，仅用4块RTX 3090训练1.5B参数模型就在多个基准上超越先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在资源受限的小语言模型上效果不佳，因为有限的rollout预算导致稀疏探索、稀疏信用分配和不稳定训练，形成低成本低准确率的困境。

Method: 提出DAVID-GRPO框架：1) 用最小监督稳定早期学习；2) 基于证据召回分配检索信用；3) 通过重采样截断的近失败轨迹改进探索。

Result: 在仅用4块RTX 3090 GPU训练1.5B参数模型的情况下，在六个多跳QA基准上一致超越先前为大规模设置设计的RL方法。

Conclusion: 通过正确的归纳偏置，小智能体可以在低训练成本下实现高准确率，打破了资源约束下小模型性能受限的困境。

Abstract: While reinforcement learning (RL) has empowered multi-turn reasoning agents with retrieval and tools, existing successes largely depend on extensive on-policy rollouts in high-cost, high-accuracy regimes. Under realistic resource constraints that cannot support large models or dense explorations, however, small language model agents fall into a low-cost, low-accuracy regime, where limited rollout budgets lead to sparse exploration, sparse credit assignment, and unstable training. In this work, we challenge this trade-off and show that small language models can achieve strong multi-hop reasoning under resource constraints. We introduce DAVID-GRPO, a budget-efficient RL framework that (i) stabilizes early learning with minimal supervision, (ii) assigns retrieval credit based on evidence recall, and (iii) improves exploration by resampling truncated near-miss trajectories. Evaluated on agents up to 1.5B parameters trained on only four RTX 3090 GPUs, DAVID-GRPO consistently outperforms prior RL methods designed for large-scale settings on six multi-hop QA benchmarks. These results show that with the right inductive biases, small agents can achieve low training cost with high accuracy.

</details>


### [12] [Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning](https://arxiv.org/abs/2601.21700)
*Wonduk Seo,Wonseok Choi,Junseo Koh,Juhyeon Lee,Hyunjin An,Minhyeong Yu,Jian Park,Qingshan Zhou,Seunghyun Lee,Yi Bu*

Main category: cs.CL

TL;DR: OG-MAR是一个本体指导的多智能体推理框架，通过构建全球文化本体论和实例化价值角色智能体，提升LLM在文化敏感决策中的对齐性、一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在文化敏感决策中存在偏差，原因包括预训练数据倾斜和缺乏结构化价值表示。现有方法缺乏人口统计学基础，将价值视为独立、非结构化信号，导致一致性和可解释性不足。

Method: OG-MAR从世界价值观调查中总结受访者特定价值，通过能力问题在固定分类法上构建全球文化本体论。推理时检索本体一致关系和人口统计学相似档案，实例化多个价值角色智能体，由判断智能体综合输出并强制本体一致性和人口统计学接近性。

Result: 在四个LLM骨干网络上的区域社会调查基准测试表明，OG-MAR相比竞争基线提高了文化对齐性和鲁棒性，同时产生更透明的推理轨迹。

Conclusion: OG-MAR通过本体指导的多智能体推理框架有效解决了LLM文化对齐问题，提供了更一致、可解释的文化敏感决策支持。

Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.

</details>


### [13] [TACLer: Tailored Curriculum Reinforcement Learning for Efficient Reasoning](https://arxiv.org/abs/2601.21711)
*Huiyuan Lai,Malvina Nissim*

Main category: cs.CL

TL;DR: TACLer是一个模型定制的课程强化学习框架，通过渐进式增加数据复杂度和混合思考/非思考推理范式，在降低计算成本的同时提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务上表现出色，但通常需要大规模强化学习训练来激发长链思维推理，这会导致过度思考（冗余中间步骤）和计算效率低下。需要一种方法在保持甚至提升性能的同时，提高学习和推理效率。

Method: 提出TACLer框架，包含两个核心组件：1）定制化课程学习：根据模型在多阶段RL训练中的熟练程度，渐进式增加数据复杂度；2）混合思考/非思考推理范式：通过启用或禁用思考模式来平衡准确性和效率。

Result: TACLer在学习和推理方面带来双重优势：1）降低计算成本：相比长思考模型减少50%以上训练计算，相比基础模型减少42%以上推理token使用；2）提升准确性：在基础模型上提升9%以上，在四个数学数据集上持续优于最先进的非思考和思考基线模型。

Conclusion: TACLer通过定制化课程学习和混合推理范式，有效解决了LLMs在复杂推理任务中的过度思考和计算效率问题，实现了准确性和效率的双重提升。

Abstract: Large Language Models (LLMs) have shown remarkable performance on complex reasoning tasks, especially when equipped with long chain-of-thought (CoT) reasoning. However, eliciting long CoT typically requires large-scale reinforcement learning (RL) training, while often leading to overthinking with redundant intermediate steps. To improve learning and reasoning efficiency, while preserving or even enhancing performance, we propose TACLer, a model-tailored curriculum reinforcement learning framework that gradually increases the complexity of the data based on the model's proficiency in multi-stage RL training. TACLer features two core components: (i) tailored curriculum learning that determines what knowledge the model lacks and needs to learn in progressive stages; (ii) a hybrid Thinking/NoThinking reasoning paradigm that balances accuracy and efficiency by enabling or disabling the Thinking mode. Our experiments show that TACLer yields a twofold advantage in learning and reasoning: (i) it reduces computational cost, cutting training compute by over 50% compared to long thinking models and reducing inference token usage by over 42% relative to the base model; and (ii) it improves accuracy by over 9% on the base model, consistently outperforming state-of-the-art Nothinking and Thinking baselines across four math datasets with complex problems.

</details>


### [14] [Enhancing Conversational Agents via Task-Oriented Adversarial Memory Adaptation](https://arxiv.org/abs/2601.21797)
*Yimin Deng,Yuqing Fu,Derong Xu,Yejing Wang,Wei Ni,Jingtong Gao,Xiaopeng Li,Chengxu Liu,Xiao Han,Guoshuai Zhao,Xiangyu Zhao,Li Zhu,Xueming Qian*

Main category: cs.CL

TL;DR: 提出对抗性记忆适应机制（AMA），通过模拟任务执行来对齐离线记忆构建与任务目标，解决现有记忆系统离线阶段固定且任务无关的问题。


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆系统在离线阶段采用固定、任务无关的构建和更新策略，导致记忆准备与下游任务需求不匹配，影响任务性能。

Method: AMA机制包含三个智能体：挑战者基于原始对话生成QA对，评估者评估记忆回答的质量并进行错误分析，适配器根据错误分析对记忆构建策略和内容进行双重更新。

Result: 在长对话基准LoCoMo上的实验表明AMA能有效提升记忆系统性能，且可集成到多种现有记忆系统中。

Conclusion: AMA通过在离线阶段引入任务感知的监督信号，增强了记忆系统对下游任务的适应性，解决了记忆准备与任务需求不匹配的问题。

Abstract: Conversational agents struggle to handle long conversations due to context window limitations. Therefore, memory systems are developed to leverage essential historical information. Existing memory systems typically follow a pipeline of offline memory construction and update, and online retrieval. Despite the flexible online phase, the offline phase remains fixed and task-independent. In this phase, memory construction operates under a predefined workflow and fails to emphasize task relevant information. Meanwhile, memory updates are guided by generic metrics rather than task specific supervision. This leads to a misalignment between offline memory preparation and task requirements, which undermines downstream task performance. To this end, we propose an Adversarial Memory Adaptation mechanism (AMA) that aligns memory construction and update with task objectives by simulating task execution. Specifically, first, a challenger agent generates question answer pairs based on the original dialogues. The constructed memory is then used to answer these questions, simulating downstream inference. Subsequently, an evaluator agent assesses the responses and performs error analysis. Finally, an adapter agent analyzes the error cases and performs dual level updates on both the construction strategy and the content. Through this process, the memory system receives task aware supervision signals in advance during the offline phase, enhancing its adaptability to downstream tasks. AMA can be integrated into various existing memory systems, and extensive experiments on long dialogue benchmark LoCoMo demonstrate its effectiveness.

</details>


### [15] [Embodied Task Planning via Graph-Informed Action Generation with Large Lanaguage Model](https://arxiv.org/abs/2601.21841)
*Xiang Li,Ning Yan,Masood Mortazavi*

Main category: cs.CL

TL;DR: GiG提出了一种基于图内图架构的具身智能体规划框架，通过图神经网络编码环境状态，构建执行轨迹图记忆库，结合结构感知检索和有界前瞻模块，显著提升长时程规划性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在零样本推理方面表现出色，但作为具身智能体部署时仍面临长时程规划的基本挑战。标准LLM规划器经常因上下文窗口限制而无法保持策略连贯性，或产生违反约束的幻觉转换。

Method: 提出GiG框架：1) 使用GNN将环境状态编码为嵌入；2) 将这些嵌入组织成动作连接的执行轨迹图，存储在经验记忆库中；3) 通过聚类图嵌入实现结构感知先验检索；4) 引入有界前瞻模块，利用符号转换逻辑增强规划能力。

Result: 在三个具身规划基准测试中优于最先进基线：Robotouille Synchronous提升22%，Robotouille Asynchronous提升37%，ALFWorld提升15%，计算成本相当或更低。

Conclusion: GiF框架通过图内图架构有效解决了具身智能体的长时程规划问题，结合结构感知记忆检索和有界前瞻，显著提升了规划性能和策略连贯性。

Abstract: While Large Language Models (LLMs) have demonstrated strong zero-shot reasoning capabilities, their deployment as embodied agents still faces fundamental challenges in long-horizon planning. Unlike open-ended text generation, embodied agents must decompose high-level intent into actionable sub-goals while strictly adhering to the logic of a dynamic, observed environment. Standard LLM planners frequently fail to maintain strategy coherence over extended horizons due to context window limitation or hallucinate transitions that violate constraints. We propose GiG, a novel planning framework that structures embodied agents' memory using a Graph-in-Graph architecture. Our approach employs a Graph Neural Network (GNN) to encode environmental states into embeddings, organizing these embeddings into action-connected execution trace graphs within an experience memory bank. By clustering these graph embeddings, the framework enables retrieval of structure-aware priors, allowing agents to ground current decisions in relevant past structural patterns. Furthermore, we introduce a novel bounded lookahead module that leverages symbolic transition logic to enhance the agents' planning capabilities through the grounded action projection. We evaluate our framework on three embodied planning benchmarks-Robotouille Synchronous, Robotouille Asynchronous, and ALFWorld. Our method outperforms state-of-the-art baselines, achieving Pass@1 performance gains of up to 22% on Robotouille Synchronous, 37% on Asynchronous, and 15% on ALFWorld with comparable or lower computational cost.

</details>


### [16] [On the Paradoxical Interference between Instruction-Following and Task Solving](https://arxiv.org/abs/2601.22047)
*Yunjia Qi,Hao Peng,Xintong Shi,Amy Xin,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 论文提出SUSTAINSCORE指标，量化指令遵循对LLM任务解决能力的干扰，发现添加自证约束会导致性能显著下降，并分析了干扰机制和不同对齐策略的影响。


<details>
  <summary>Details</summary>
Motivation: 指令遵循旨在通过明确约束使LLM与人类意图对齐，但作者发现一个反直觉现象：指令遵循可能反而干扰LLM的任务解决能力。需要量化这种干扰并理解其机制。

Method: 提出SUSTAINSCORE指标，通过向指令中插入自证约束（从原始成功输出中提取的自然满足的约束）来测量任务性能下降。在数学、多跳QA和代码生成任务上测试当前LLM，分析失败模式、注意力机制，并研究不同后训练范式的影响。

Result: 实验显示添加自证约束会导致显著性能下降，即使是Claude-Sonnet-4.5等先进模型也受影响。失败案例相比成功案例在约束上分配了显著更多注意力。不同对齐策略对干扰程度有不同影响。

Conclusion: 指令遵循确实会干扰LLM的任务解决能力，SUSTAINSCORE能有效量化这种干扰。需要重新思考对齐策略，平衡指令遵循与任务解决能力。

Abstract: Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research

</details>


### [17] [$G^2$-Reader: Dual Evolving Graphs for Multimodal Document QA](https://arxiv.org/abs/2601.22055)
*Yaxin Du,Junru Song,Yifan Zhou,Cheng Wang,Jiahao Gu,Zimeng Chen,Menglan Chen,Wen Yao,Yang Yang,Ying Wen,Siheng Chen*

Main category: cs.CL

TL;DR: G²-Reader：一个用于多模态长文档问答的双图系统，通过内容图保持文档原生结构和跨模态语义，通过规划图跟踪中间发现并引导逐步导航，显著提升多模态阅读性能


<details>
  <summary>Details</summary>
Motivation: 当前基于检索增强生成的方法在处理多模态长文档（包含文本、表格、图表）时存在两个主要问题：1）平面分块破坏了文档原生结构和跨模态对齐，产生难以独立理解的语义片段；2）即使迭代检索也可能在长上下文中失败，因为缺乏持久的全局搜索状态来跟踪证据收集进度

Method: 提出G²-Reader双图系统：1）内容图：保持文档原生结构和跨模态语义；2）规划图：一个基于智能体的有向无环图，包含子问题，用于跟踪中间发现并引导逐步导航以完成证据收集

Result: 在VisDoMBench的五个多模态领域上，G²-Reader配合Qwen3-VL-32B-Instruct达到66.21%的平均准确率，显著优于强基线方法和独立的GPT-5（53.08%）

Conclusion: G²-Reader通过双图架构有效解决了多模态长文档问答中的结构保持和全局导航问题，为跨模态信息检索提供了新的解决方案

Abstract: Retrieval-augmented generation is a practical paradigm for question answering over long documents, but it remains brittle for multimodal reading where text, tables, and figures are interleaved across many pages. First, flat chunking breaks document-native structure and cross-modal alignment, yielding semantic fragments that are hard to interpret in isolation. Second, even iterative retrieval can fail in long contexts by looping on partial evidence or drifting into irrelevant sections as noise accumulates, since each step is guided only by the current snippet without a persistent global search state. We introduce $G^2$-Reader, a dual-graph system, to address both issues. It evolves a Content Graph to preserve document-native structure and cross-modal semantics, and maintains a Planning Graph, an agentic directed acyclic graph of sub-questions, to track intermediate findings and guide stepwise navigation for evidence completion. On VisDoMBench across five multimodal domains, $G^2$-Reader with Qwen3-VL-32B-Instruct reaches 66.21\% average accuracy, outperforming strong baselines and a standalone GPT-5 (53.08\%).

</details>


### [18] [Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://arxiv.org/abs/2601.22139)
*Xin Chen,Feng Jiang,Yiqian Zhang,Hardy Chen,Shuo Yan,Wenya Xie,Min Yang,Shujian Huang*

Main category: cs.CL

TL;DR: 提出主动交互推理(PIR)新范式，让LLM从被动求解器转变为主动询问者，通过用户交互解决前提和意图层面的不确定性，显著提升推理性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前基于思维链的推理LLM存在"盲目自我思考"的局限性，当关键信息缺失或模糊时仍进行大量内部推理。需要一种新范式让LLM能够主动与用户交互澄清不确定性。

Method: PIR包含两个核心组件：1)不确定性感知的监督微调，赋予模型交互推理能力；2)基于用户模拟器的策略优化框架，使用复合奖励使模型行为与用户意图对齐。

Result: 在数学推理、代码生成和文档编辑任务上，PIR显著优于基线方法，准确率提升高达32.70%，通过率提升22.90%，BLEU分数提升41.36，同时减少近一半的推理计算和不必要的交互轮次。

Conclusion: PIR成功将LLM转变为主动询问者，有效解决前提和意图层面的不确定性，在多个任务上表现出强大的泛化能力和鲁棒性，为LLM推理提供了新范式。

Abstract: Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}

</details>


### [19] [DynaWeb: Model-Based Reinforcement Learning of Web Agents](https://arxiv.org/abs/2601.22149)
*Hang Ding,Peidong Liu,Junqiao Wang,Ziwei Ji,Meng Cao,Rongzhao Zhang,Lynn Ai,Eric Yang,Tianyu Shi,Lei Yu*

Main category: cs.CL

TL;DR: DynaWeb是一个基于模型强化学习的框架，通过训练网页世界模型来模拟网页环境，让网页代理在合成环境中进行"想象"训练，显著提升了网页代理的性能。


<details>
  <summary>Details</summary>
Motivation: 训练自主网页代理面临与实时互联网交互的低效、昂贵和风险问题，需要一种更高效、可扩展的训练方法。

Method: 提出DynaWeb框架：1) 训练网页世界模型预测给定代理动作后的自然网页表示；2) 在合成网页环境中进行策略"想象"训练；3) 将真实专家轨迹与策略轨迹交错训练以提高稳定性和样本效率。

Result: 在WebArena和WebVoyager基准测试中，DynaWeb显著提升了最先进的开源网页代理模型的性能。

Conclusion: 通过"想象"训练网页代理是可行的，为扩展在线代理强化学习提供了可扩展且高效的方法。

Abstract: The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Noisy but Valid: Robust Statistical Evaluation of LLMs with Imperfect Judges](https://arxiv.org/abs/2601.20913)
*Chen Feng,Minghe Shen,Ananth Balashankar,Carsten Gerner-Beuerle,Miguel R. D. Rodrigues*

Main category: cs.LG

TL;DR: 提出"嘈杂但有效"的假设检验框架，利用小规模人工标注校准集估计LLM法官的TPR/FPR，在大规模法官标注数据集上应用方差校正临界阈值，理论上保证有限样本类型I错误控制。


<details>
  <summary>Details</summary>
Motivation: LLM作为法官进行可靠性认证具有可扩展性，但法官的不完美性、噪声和偏见可能使统计保证失效，需要解决这一问题。

Method: 引入"嘈杂但有效"假设检验框架，通过小规模人工标注校准集估计法官的TPR/FPR，推导方差校正临界阈值应用于大规模法官标注数据集。

Result: 理论保证有限样本类型I错误控制，实验验证在Jigsaw Comment、Hate Speech和SafeRLHF数据集上有效，揭示实际方法与理论Oracle之间的显著性能差距。

Conclusion: 首次系统处理不完美法官设置，提供法官可靠性的可解释诊断，阐明评估能力如何依赖法官质量、数据集规模和认证水平，深化对LLM法官统计评估的理解。

Abstract: Reliable certification of Large Language Models (LLMs)-verifying that failure rates are below a safety threshold-is critical yet challenging. While "LLM-as-a-Judge" offers scalability, judge imperfections, noise, and bias can invalidate statistical guarantees. We introduce a "Noisy but Valid" hypothesis testing framework to address this. By leveraging a small human-labelled calibration set to estimate the judge's True Positive and False Positive Rates (TPR/FPR), we derive a variance-corrected critical threshold applied to a large judge-labelled dataset. Crucially, our framework theoretically guarantees finite-sample Type-I error control (validity) despite calibration uncertainty. This distinguishes our work from Prediction-Powered Inference (PPI), positioning our method as a diagnostic tool that explicitly models judge behavior rather than a black-box estimator. Our contributions include: (1) Theoretical Guarantees: We derive the exact conditions under which noisy testing yields higher statistical power than direct evaluation; (2) Empirical Validation: Experiments on Jigsaw Comment, Hate Speech and SafeRLHF confirm our theory; (3) The Oracle Gap: We reveal a significant performance gap between practical methods and the theoretical "Oracle" (perfectly known judge parameters), quantifying the cost of estimation. Specifically, we provide the first systematic treatment of the imperfect-judge setting, yielding interpretable diagnostics of judge reliability and clarifying how evaluation power depends on judge quality, dataset size, and certification levels. Together, these results sharpen understanding of statistical evaluation with LLM judges, and highlight trade-offs among competing inferential tools.

</details>


### [21] [Textual Equilibrium Propagation for Deep Compound AI Systems](https://arxiv.org/abs/2601.21064)
*Minghui Chen,Wenlong Deng,James Zou,Han Yu,Xiaoxiao Li*

Main category: cs.LG

TL;DR: 提出Textual Equilibrium Propagation (TEP)方法，解决复合AI系统中长流程的文本梯度爆炸和消失问题，通过局部学习原理优化深度系统性能


<details>
  <summary>Details</summary>
Motivation: LLM被部署为复合AI系统的一部分，协调多个模块进行长流程工作。现有方法（如TextGrad）在系统深度增加时性能下降，存在文本梯度爆炸和消失两种深度扩展失败模式

Method: 提出文本均衡传播(TEP)，受能量模型中均衡传播启发。包含两个阶段：1)自由阶段，局部LLM批评者迭代优化提示直到达到均衡；2)推动阶段，应用有界修改强度的近端提示编辑，通过前向信号而非后向反馈链传播任务级目标

Result: 在长流程QA基准测试和多智能体工具使用数据集中，TEP在准确性和效率上持续优于全局传播方法如TextGrad。增益随深度增加，同时保持深度复合AI系统中黑盒LLM组件的实用性

Conclusion: TEP通过局部学习原理有效解决了复合AI系统中深度扩展问题，避免了全局文本反向传播的计算负担和信号退化，为深度复合AI系统提供了实用的优化方法

Abstract: Large language models (LLMs) are increasingly deployed as part of compound AI systems that coordinate multiple modules (e.g., retrievers, tools, verifiers) over long-horizon workflows. Recent approaches that propagate textual feedback globally (e.g., TextGrad) make it feasible to optimize such pipelines, but we find that performance degrades as system depth grows. In particular, long-horizon agentic workflows exhibit two depth-scaling failure modes: 1) exploding textual gradient, where textual feedback grows exponentially with depth, leading to prohibitively long message and amplifies evaluation biases; and 2) vanishing textual gradient, where limited long-context ability causes models overemphasize partial feedback and compression of lengthy feedback causes downstream messages to lose specificity gradually as they propagate many hops upstream. To mitigate these issues, we introduce Textual Equilibrium Propagation (TEP), a local learning principle inspired by Equilibrium Propagation in energy-based models. TEP includes two phases: 1) a free phase where a local LLM critics iteratively refine prompts until reaching equilibrium (no further improvements are suggested); and 2) a nudged phase which applies proximal prompt edits with bounded modification intensity, using task-level objectives that propagate via forward signaling rather than backward feedback chains. This design supports local prompt optimization followed by controlled adaptation toward global goals without the computational burden and signal degradation of global textual backpropagation. Across long-horizon QA benchmarks and multi-agent tool-use dataset, TEP consistently improves accuracy and efficiency over global propagation methods such as TextGrad. The gains grows with depth, while preserving the practicality of black-box LLM components in deep compound AI system.

</details>


### [22] [Safety Generalization Under Distribution Shift in Safe Reinforcement Learning: A Diabetes Testbed](https://arxiv.org/abs/2601.21094)
*Minjae Kwon,Josephine Lamp,Lu Feng*

Main category: cs.LG

TL;DR: 该研究揭示了安全强化学习算法在糖尿病管理中的安全泛化差距，并提出测试时屏蔽方法有效恢复安全性。


<details>
  <summary>Details</summary>
Motivation: 研究安全强化学习算法在训练时的安全保证是否能转移到存在分布偏移的部署场景，特别是在糖尿病管理这一安全关键领域。

Method: 在统一的临床模拟器上对8种安全RL算法进行基准测试，使用测试时屏蔽方法（通过学习动态模型过滤不安全动作）来恢复安全性。

Result: 发现安全泛化差距：训练时满足约束的策略在未见患者上频繁违反安全要求。测试时屏蔽有效恢复安全性，在PPO-Lag和CPO等强基线算法上实现13-14%的时间在范围内增益，同时降低临床风险指数和血糖变异性。

Conclusion: 测试时屏蔽是解决安全强化学习在分布偏移下安全泛化问题的有效方法，提供的模拟器和基准平台可用于研究安全关键控制领域的安全泛化问题。

Abstract: Safe Reinforcement Learning (RL) algorithms are typically evaluated under fixed training conditions. We investigate whether training-time safety guarantees transfer to deployment under distribution shift, using diabetes management as a safety-critical testbed. We benchmark safe RL algorithms on a unified clinical simulator and reveal a safety generalization gap: policies satisfying constraints during training frequently violate safety requirements on unseen patients. We demonstrate that test-time shielding, which filters unsafe actions using learned dynamics models, effectively restores safety across algorithms and patient populations. Across eight safe RL algorithms, three diabetes types, and three age groups, shielding achieves Time-in-Range gains of 13--14\% for strong baselines such as PPO-Lag and CPO while reducing clinical risk index and glucose variability. Our simulator and benchmark provide a platform for studying safety under distribution shift in safety-critical control domains. Code is available at https://github.com/safe-autonomy-lab/GlucoSim and https://github.com/safe-autonomy-lab/GlucoAlg.

</details>


### [23] [SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning](https://arxiv.org/abs/2601.21649)
*Jinjun Peng,Magnus Saebo,Tianjun Zhong,Yi-Jie Cheng,Junfeng Yang,Baishakhi Ray,Simin Chen,Yangruibo Ding*

Main category: cs.LG

TL;DR: 提出Repository-Centric Learning (RCL)新范式，通过深度掌握特定代码库而非广泛任务训练，开发出SWE-Spot-4B模型，在软件工程任务上超越更大规模开源模型并匹敌商业模型。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型在隐私敏感和资源受限环境中部署需求增加，但存在能力差距：无法像前沿大模型那样在推理时泛化处理复杂陌生代码库。传统的任务中心学习范式通过扩展不同仓库的暴露无法解决此问题。

Method: 提出Repository-Centric Learning (RCL)范式，强调垂直仓库深度而非水平任务广度。设计四单元Repository-Centric Experience，将静态代码库转化为交互式学习信号，训练SWE-Spot-4B系列紧凑模型作为仓库专家。

Result: SWE-Spot-4B打破现有扩展趋势，超越更大规模开源模型（如Meta的CWM、Qwen3-Coder-30B），在多个软件工程任务上达到或超越效率导向的商业模型（如GPT-4.1-mini、GPT-5-nano）。RCL展现出更高的训练样本效率和更低的推理成本。

Conclusion: 构建高效智能需要代码库掌握这一独特且必要的维度，补充通用编码能力。RCL范式为在资源受限环境中部署高性能编码代理提供了新方向。

Abstract: The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the "physics" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.

</details>


### [24] [StepShield: When, Not Whether to Intervene on Rogue Agents](https://arxiv.org/abs/2601.22136)
*Gloria Felicia,Michael Eniolade,Jinfeng He,Zitha Sasindran,Hemant Kumar,Milan Hussain Angati,Sandeep Bandarupalli*

Main category: cs.LG

TL;DR: StepShield是首个评估代码代理安全违规检测时机的基准，提出三种时间指标，揭示传统准确率指标无法反映的检测时机差异，并展示早期检测的经济效益。


<details>
  <summary>Details</summary>
Motivation: 现有安全基准只报告二元准确率，混淆了早期干预和事后分析。能够在第8步标记违规的检测器可实现干预，而第48步报告的检测器只有法医价值。这种关键区别当前基准无法衡量。

Method: 引入StepShield基准，包含9,213个代码代理轨迹（1,278个标注训练对和7,935个轨迹测试集，8.1%恶意率）。恶意行为基于六类真实安全事件。提出三种时间指标：早期干预率、干预间隔和节省的token数。

Result: 评估显示LLM法官达到59%早期干预率，而静态分析器只有26%，2.3倍性能差距在标准准确率指标中完全不可见。级联混合检测器将监控成本降低75%，预计五年内为企业节省1.08亿美元。

Conclusion: 通过将评估重点从"是否"检测到转向"何时"检测到，StepShield为构建更安全、更经济可行的AI代理提供了新基础。

Abstract: Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.

</details>


### [25] [Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification](https://arxiv.org/abs/2601.21244)
*Yiju Guo,Tianyi Hu,Zexu Sun,Yankai Lin*

Main category: cs.LG

TL;DR: LENS框架通过识别并移除干扰token来提升RLVR的探索效率，在噪声提示设置下实现更快的收敛和更高的性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在有限rollout预算下探索效率低，采样成功率低且训练不稳定。研究发现许多探索失败并非源于任务难度，而是由少量干扰token引起

Method: 提出LENS框架：1）识别并移除提示中的干扰token进行净化；2）将净化后成功的rollout转移到原始噪声提示上监督策略优化，使模型学会忽略真实世界中的干扰

Result: LENS显著优于GRPO，平均性能提升3.88%，收敛速度加快1.6倍以上，证明了去除干扰token对提升rollout效率的关键作用

Conclusion: 干扰token的修剪在提升RLVR探索效率中起关键作用，为RLVR研究提供了新视角

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.

</details>


### [26] [EGAM: Extended Graph Attention Model for Solving Routing Problems](https://arxiv.org/abs/2601.21281)
*Licheng Wang,Yuzi Yan,Mingtao Huang,Yuan Shen*

Main category: cs.LG

TL;DR: EGAM扩展了图注意力机制，通过多头点积注意力同时更新节点和边嵌入，在多种路由问题上达到或超越现有方法，尤其在高度约束问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图注意力模型(GAM)仅考虑节点特征，存在局限性。需要更通用的图注意力机制来同时处理节点和边信息，以更好地解决复杂的路由问题。

Method: 提出扩展图注意力模型(EGAM)，使用多头点积注意力同时更新节点和边嵌入。采用自回归编码器-解码器架构，使用包含专门设计基线的策略梯度算法进行训练。

Result: EGAM在多种路由问题上匹配或超越了现有方法，特别是在高度约束问题上表现出色，展示了处理复杂图结构的高效性。

Conclusion: EGAM通过扩展图注意力机制，有效解决了传统GAM的局限性，在神经组合优化领域提供了更强大的解决方案。

Abstract: Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.

</details>


### [27] [The Surprising Difficulty of Search in Model-Based Reinforcement Learning](https://arxiv.org/abs/2601.21306)
*Wei-Di Chang,Mikael Henaff,Brandon Amos,Gregory Dudek,Scott Fujimoto*

Main category: cs.LG

TL;DR: 该论文挑战了模型强化学习中搜索的常规认知，发现搜索并非学习的直接替代，甚至可能损害性能，而缓解分布偏移比提高模型精度更重要。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为长期预测和复合误差是模型强化学习的主要障碍，但作者质疑这一观点，探索搜索在模型强化学习中的实际作用和限制。

Method: 通过实验分析搜索在模型强化学习中的表现，识别影响搜索效果的关键因素，并基于缓解分布偏移的洞察开发有效的搜索技术。

Result: 研究发现搜索可能损害性能，即使模型精度很高；缓解分布偏移比提高模型或价值函数精度更重要；提出的技术实现了多个流行基准领域的最先进性能。

Conclusion: 搜索在模型强化学习中不是即插即用的解决方案，成功的关键在于有效管理分布偏移而非单纯追求模型精度。

Abstract: This paper investigates search in model-based reinforcement learning (RL). Conventional wisdom holds that long-term predictions and compounding errors are the primary obstacles for model-based RL. We challenge this view, showing that search is not a plug-and-play replacement for a learned policy. Surprisingly, we find that search can harm performance even when the model is highly accurate. Instead, we show that mitigating distribution shift matters more than improving model or value function accuracy. Building on this insight, we identify key techniques for enabling effective search, achieving state-of-the-art performance across multiple popular benchmark domains.

</details>


### [28] [Intrinsic Reward Policy Optimization for Sparse-Reward Environments](https://arxiv.org/abs/2601.21391)
*Minjae Cho,Huy Trong Tran*

Main category: cs.LG

TL;DR: IRPO提出了一种利用多个内在奖励直接优化策略的框架，通过替代策略梯度在稀疏奖励环境中提供更丰富的学习信号，无需预训练子策略。


<details>
  <summary>Details</summary>
Motivation: 强化学习中探索至关重要，但在稀疏奖励环境下，传统的噪声注入探索策略往往不足。现有的内在奖励方法存在信用分配不稳定（与外在奖励结合优化策略）或样本效率低和次优（用于分层学习训练子策略）的问题。

Method: 提出内在奖励策略优化（IRPO）框架，利用多个内在奖励直接优化外在奖励策略，无需预训练子策略。核心是使用替代策略梯度，在稀疏奖励环境中提供比真实梯度更丰富的学习信号。

Result: 在离散和连续环境中，IRPO相比基线方法提升了性能和样本效率。论文还对IRPO解决的优化问题进行了形式化分析。

Conclusion: IRPO通过替代策略梯度有效利用多个内在奖励，解决了稀疏奖励环境中的探索问题，避免了现有方法的信用分配不稳定和样本效率低的问题。

Abstract: Exploration is essential in reinforcement learning as an agent relies on trial and error to learn an optimal policy. However, when rewards are sparse, naive exploration strategies, like noise injection, are often insufficient. Intrinsic rewards can also provide principled guidance for exploration by, for example, combining them with extrinsic rewards to optimize a policy or using them to train subpolicies for hierarchical learning. However, the former approach suffers from unstable credit assignment, while the latter exhibits sample inefficiency and sub-optimality. We propose a policy optimization framework that leverages multiple intrinsic rewards to directly optimize a policy for an extrinsic reward without pretraining subpolicies. Our algorithm -- intrinsic reward policy optimization (IRPO) -- achieves this by using a surrogate policy gradient that provides a more informative learning signal than the true gradient in sparse-reward environments. We demonstrate that IRPO improves performance and sample efficiency relative to baselines in discrete and continuous environments, and formally analyze the optimization problem solved by IRPO. Our code is available at https://github.com/Mgineer117/IRPO.

</details>


### [29] [HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing](https://arxiv.org/abs/2601.21459)
*Chengyu Du,Xintao Wang,Aili Chen,Weiyuan Li,Rui Xu,Junteng Liu,Zishan Huang,Rong Tian,Zijun Sun,Yuhao Li,Liheng Feng,Deming Ding,Pengyu Zhao,Yanghua Xiao*

Main category: cs.LG

TL;DR: HER框架通过双层思维架构和强化学习，提升LLM角色扮演中的认知模拟能力，在基准测试上显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM角色扮演虽然能模拟角色语气和知识，但难以模拟行为背后的内在思维过程。现有方法缺乏高质量的推理轨迹数据和与人类偏好对齐的可靠奖励信号。

Method: 提出HER统一框架，引入双层思维架构区分角色的第一人称思维和LLM的第三人称思维。通过逆向工程构建推理增强的角色扮演数据，建立人类对齐的原则和奖励模型，基于Qwen3-32B进行监督学习和强化学习训练。

Result: 模型在CoSER基准上提升30.26分，在Minimax Role-Play Bench上提升14.97分，显著超越Qwen3-32B基线。

Conclusion: HER框架有效解决了LLM角色扮演中的认知模拟挑战，通过双层思维架构和强化学习实现了更真实的角色内在思维模拟。

Abstract: LLM role-playing, i.e., using LLMs to simulate specific personas, has emerged as a key capability in various applications, such as companionship, content creation, and digital games. While current models effectively capture character tones and knowledge, simulating the inner thoughts behind their behaviors remains a challenge. Towards cognitive simulation in LLM role-play, previous efforts mainly suffer from two deficiencies: data with high-quality reasoning traces, and reliable reward signals aligned with human preferences. In this paper, we propose HER, a unified framework for cognitive-level persona simulation. HER introduces dual-layer thinking, which distinguishes characters' first-person thinking from LLMs' third-person thinking. To bridge these gaps, we curate reasoning-augmented role-playing data via reverse engineering and construct human-aligned principles and reward models. Leveraging these resources, we train \method models based on Qwen3-32B via supervised and reinforcement learning. Extensive experiments validate the effectiveness of our approach. Notably, our models significantly outperform the Qwen3-32B baseline, achieving a 30.26 improvement on the CoSER benchmark and a 14.97 gain on the Minimax Role-Play Bench. Our datasets, principles, and models will be released to facilitate future research.

</details>


### [30] [Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking](https://arxiv.org/abs/2601.21619)
*Yiming Wang,Zhuosheng Zhang,Rui Wang*

Main category: cs.LG

TL;DR: 提出T2方法解决并行思维中的"过度缩放诅咒"，通过潜在表示估计每个样本的最优并行级别，在保持性能的同时显著降低成本


<details>
  <summary>Details</summary>
Motivation: 当前并行思维方法为所有样本分配相同的全局并行级别N，但样本异质性导致某些样本可以用更小的N'达到相似性能，造成预算冗余，这种系统级效率与样本级效率的不兼容构成了"过度缩放诅咒"

Method: 提出T2方法，利用潜在表示在解码前估计每个样本的最优并行级别，通过轻量级机制为不同样本分配适当的计算资源

Result: 实验表明T2显著降低了成本，同时保持了可比较的性能，实现了更高效的并行思维

Conclusion: T2方法有效解决了并行思维中的过度缩放问题，通过样本自适应的并行级别分配，在保持推理质量的同时提高了计算效率

Abstract: Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.

</details>


### [31] [Explicit Credit Assignment through Local Rewards and Dependence Graphs in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21523)
*Bang Giang Le,Viet Cuong Ta*

Main category: cs.LG

TL;DR: 提出一种结合全局奖励和局部奖励优势的多智能体强化学习方法，通过交互图更精细地识别个体贡献，缓解局部奖励的合作问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，全局奖励包含所有智能体贡献，存在噪声和信用分配问题；局部奖励虽然学习更快但可能导致次优解，因为智能体可能短视地优化自身奖励而忽视全局最优性。

Method: 通过智能体间的交互图，更精细地识别个体贡献，同时缓解局部奖励的合作问题。还提出了近似构建这种交互图的实用方法。

Result: 实验证明该方法具有灵活性，能够超越传统的局部和全局奖励设置。

Conclusion: 提出的方法结合了全局奖励和局部奖励的优势，通过交互图机制在多智能体强化学习中实现了更好的合作效果。

Abstract: To promote cooperation in Multi-Agent Reinforcement Learning, the reward signals of all agents can be aggregated together, forming global rewards that are commonly known as the fully cooperative setting. However, global rewards are usually noisy because they contain the contributions of all agents, which have to be resolved in the credit assignment process. On the other hand, using local reward benefits from faster learning due to the separation of agents' contributions, but can be suboptimal as agents myopically optimize their own reward while disregarding the global optimality. In this work, we propose a method that combines the merits of both approaches. By using a graph of interaction between agents, our method discerns the individual agent contribution in a more fine-grained manner than a global reward, while alleviating the cooperation problem with agents' local reward. We also introduce a practical approach for approximating such a graph. Our experiments demonstrate the flexibility of the approach, enabling improvements over the traditional local and global reward settings.

</details>


### [32] [Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening](https://arxiv.org/abs/2601.21590)
*Xiaotong Ji,Rasul Tutunov,Matthieu Zimmer,Haitham Bou Ammar*

Main category: cs.LG

TL;DR: 本文提出了一种无需外部奖励的训练免费方法，通过理论推导将全局幂分布近似为token级缩放低温分布，在数学、QA和代码任务上匹配或超越GRPO，推理延迟比MCMC降低10倍以上。


<details>
  <summary>Details</summary>
Motivation: RL后训练虽然能提升LLM推理性能，但证据表明其增益主要来自分布锐化而非新能力获取。MCMC采样方法虽能获得类似效果，但计算成本过高难以广泛应用。

Method: 提出理论推导，证明全局幂分布可近似为token级缩放低温分布，其中缩放因子捕获未来轨迹质量。基于此引入训练免费、验证器免费的算法，自回归地锐化基础模型的生成分布。

Result: 在数学、QA和代码任务上评估四个LLM，该方法匹配或超越一次性GRPO（无需外部奖励），同时推理延迟比基于MCMC的采样降低10倍以上。

Conclusion: 提出了一种高效实用的替代方案，无需迭代MCMC或外部奖励，通过理论推导实现分布锐化，显著降低计算成本。

Abstract: Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.

</details>


### [33] [Expected Return Causes Outcome-Level Mode Collapse in Reinforcement Learning and How to Fix It with Inverse Probability Scaling](https://arxiv.org/abs/2601.21669)
*Abhijeet Sinha,Sundari Elango,Dianbo Liu*

Main category: cs.LG

TL;DR: 论文指出标准强化学习的期望回报最大化目标会导致结果层面的模式崩溃，并提出逆概率缩放修正方法IPS-GRPO来解决这一问题


<details>
  <summary>Details</summary>
Motivation: 许多强化学习问题存在多个质量相当的最优解，但标准期望回报最大化训练的策略通常会崩溃到少数结果上。传统解释归因于探索不足或正则化不够，但作者认为这是期望回报目标本身的结构性缺陷

Method: 提出逆概率缩放(IPS)修正方法，移除学习信号中的结果频率放大效应。具体实现为IPS-GRPO，作为Group Relative Policy Optimization的即插即用修改，无需辅助模型或架构改变

Result: 在推理和分子生成任务中，IPS-GRPO能持续减少结果层面的模式崩溃，同时匹配或超越基线性能

Conclusion: 修正目标函数而非添加探索启发式方法是实现可靠多模态策略优化的关键。逆概率缩放能从根本上改变学习动态，产生与奖励成比例的终端分布

Abstract: Many reinforcement learning (RL) problems admit multiple terminal solutions of comparable quality, where the goal is not to identify a single optimum but to represent a diverse set of high-quality outcomes. Nevertheless, policies trained by standard expected return maximization routinely collapse onto a small subset of outcomes, a phenomenon commonly attributed to insufficient exploration or weak regularization. We show that this explanation is incomplete: outcome level mode collapse is a structural consequence of the expected-return objective itself. Under idealized learning dynamics, the log-probability ratio between any two outcomes evolves linearly in their reward difference, implying exponential ratio divergence and inevitable collapse independent of the exploration strategy, entropy regularization, or optimization algorithm. We identify the source of this pathology as the probability multiplier inside the expectation and propose a minimal correction: inverse probability scaling, which removes outcome-frequency amplification from the learning signal, fundamentally changes the learning dynamics, and provably yields reward-proportional terminal distributions, preventing collapse in multimodal settings. We instantiate this principle in Group Relative Policy Optimization (GRPO) as a drop-in modification, IPS-GRPO, requiring no auxiliary models or architectural changes. Across different reasoning and molecular generation tasks, IPS-GRPO consistently reduces outcome-level mode collapse while matching or exceeding baseline performance, suggesting that correcting the objective rather than adding exploration heuristics is key to reliable multimodal policy optimization.

</details>


### [34] [Nonparametric LLM Evaluation from Preference Data](https://arxiv.org/abs/2601.21816)
*Dennis Frauen,Athiya Deviyani,Mihaela van der Schaar,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出DMLEval非参数统计框架，使用去偏机器学习方法从偏好数据中比较和排名大语言模型，引入广义平均排名分数，支持复杂人类响应和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有评估大语言模型的方法要么依赖限制性参数假设，要么在使用灵活机器学习方法时缺乏有效的统计不确定性量化，需要更稳健的评估框架。

Method: 提出DMLEval非参数框架，引入广义平均排名分数（GARS），使用去偏机器学习方法估计排名分数，支持复杂人类响应（如平局），可结合预训练LLM评估器。

Result: DMLEval能产生统计高效的GARS排名分数估计，支持黑盒机器学习方法，可结合LLM-as-a-judge评估器，并能在预算约束下优化偏好数据收集策略。

Conclusion: 该框架为从业者提供了强大、先进的比较和排名大语言模型的方法，具有理论保证和实证验证。

Abstract: Evaluating the performance of large language models (LLMs) from human preference data is crucial for obtaining LLM leaderboards. However, many existing approaches either rely on restrictive parametric assumptions or lack valid uncertainty quantification when flexible machine learning methods are used. In this paper, we propose a nonparametric statistical framework, DMLEval, for comparing and ranking LLMs from preference data using debiased machine learning (DML). For this, we introduce generalized average ranking scores (GARS), which generalize commonly used ranking models, including the Bradley-Terry model or PageRank/ Rank centrality, with complex human responses such as ties. DMLEval comes with the following advantages: (i) It produces statistically efficient estimates of GARS ranking scores. (ii) It naturally allows the incorporation of black-box machine learning methods for estimation. (iii) It can be combined with pre-trained LLM evaluators (e.g., using LLM-as-a-judge). (iv) It suggests optimal policies for collecting preference data under budget constraints. We demonstrate these advantages both theoretically and empirically using both synthetic and real-world preference datasets. In summary, our framework provides practitioners with powerful, state-of-the-art methods for comparing or ranking LLMs.

</details>


### [35] [Constrained Meta Reinforcement Learning with Provable Test-Time Safety](https://arxiv.org/abs/2601.21845)
*Tingting Ni,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 提出一种具有可证明安全性和样本复杂度保证的约束元强化学习算法，用于在测试任务中快速学习接近最优策略


<details>
  <summary>Details</summary>
Motivation: 元强化学习能提高样本效率，但现实应用（如机器人、医疗）需要在测试时保证安全性。约束元强化学习框架虽能整合安全性，但如何在实际测试任务中确保策略安全同时减少样本复杂度仍是一个开放问题

Method: 提出一种算法，在训练期间精炼学习到的策略，为测试任务学习接近最优策略提供可证明的安全性和样本复杂度保证

Result: 算法具有可证明的安全性和样本复杂度保证，并推导出匹配的下界，表明该样本复杂度是紧致的

Conclusion: 该算法解决了约束元强化学习中测试任务安全性和样本效率的平衡问题，为实际应用提供了理论基础

Abstract: Meta reinforcement learning (RL) allows agents to leverage experience across a distribution of tasks on which the agent can train at will, enabling faster learning of optimal policies on new test tasks. Despite its success in improving sample complexity on test tasks, many real-world applications, such as robotics and healthcare, impose safety constraints during testing. Constrained meta RL provides a promising framework for integrating safety into meta RL. An open question in constrained meta RL is how to ensure the safety of the policy on the real-world test task, while reducing the sample complexity and thus, enabling faster learning of optimal policies. To address this gap, we propose an algorithm that refines policies learned during training, with provable safety and sample complexity guarantees for learning a near optimal policy on the test tasks. We further derive a matching lower bound, showing that this sample complexity is tight.

</details>


### [36] [READY: Reward Discovery for Meta-Black-Box Optimization](https://arxiv.org/abs/2601.21847)
*Zechuan Huang,Zhiguang Cao,Hongshu Guo,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.LG

TL;DR: 使用LLM作为自动奖励发现工具，通过进化范式和多任务架构为Meta-BBO发现更优的奖励函数


<details>
  <summary>Details</summary>
Motivation: 现有Meta-BBO工作中的奖励函数由人工设计，存在设计偏差和奖励黑客风险，需要自动化方法来发现更好的奖励函数

Method: 1) 引入定制化进化范式，在基于LLM的程序搜索过程中确保持续改进；2) 采用多任务进化架构，支持为不同Meta-BBO方法并行发现奖励函数，并通过任务间知识共享加速收敛

Result: 实验结果表明，通过该方法发现的奖励函数有助于提升现有Meta-BBO工作的性能，强调了奖励设计在Meta-BBO中的重要性

Conclusion: LLM可作为有效的自动奖励发现工具，进化范式和多任务架构的结合能够为Meta-BBO发现更优的奖励函数

Abstract: Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.

</details>


### [37] [Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning](https://arxiv.org/abs/2601.21894)
*Lukas Twist,Shu Yang,Hanqi Yan,Jingzhi Gong,Di Wang,Helen Yannakoudakis,Jie M. Zhang*

Main category: cs.LG

TL;DR: 研究发现代码的结构复杂性（控制流和组合结构）对LLM推理能力提升有重要影响，特定复杂度范围的代码训练效果优于结构多样的代码训练。


<details>
  <summary>Details</summary>
Motivation: 现有研究将代码视为通用训练信号，但未明确代码的哪些属性真正有助于提升推理能力。本文旨在探究代码的结构复杂性如何影响LLM在微调过程中的多步推理能力。

Method: 研究两种互补设置：1) 解决方案驱动复杂性（同一问题不同解决方案的复杂度变化）；2) 问题驱动复杂性（不同任务本身的复杂度差异）。使用圈复杂度和逻辑代码行数构建受控微调数据集，在多个开源LLM上进行推理基准测试。

Result: 83%的实验表明，将微调数据限制在特定结构复杂度范围内，其效果优于使用结构多样的代码进行训练。代码确实能提升推理能力，但其结构属性强烈决定了其有效性。

Conclusion: 代码的结构复杂性是影响LLM推理能力提升的关键因素，通过数据中心的优化方法（而非单纯扩大规模）可以有效改善模型推理性能。

Abstract: Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.

</details>


### [38] [Optimistic Transfer under Task Shift via Bellman Alignment](https://arxiv.org/abs/2601.21924)
*Jinhang Chai,Enpei Zhang,Elynn Chen,Yujun Yan*

Main category: cs.LG

TL;DR: 本文提出了一种在线迁移强化学习方法RWT，通过Bellman对齐和重加权目标修正，在源任务和目标任务存在转移概率不匹配时，能够统计有效地重用源数据，减少任务偏移复杂度而非目标MDP复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 在线迁移强化学习中，源任务和目标任务通常通过奖励或转移概率定义相似性，但在线RL算法基于Bellman回归目标操作。直接重用源Bellman更新会引入系统性偏差并破坏遗憾保证，需要找到正确的迁移抽象方法。

Method: 提出一步Bellman对齐作为在线RL迁移的正确抽象，设计了重加权目标(RWT)算子级修正方法，通过测度变化重新定位连续值并补偿转移不匹配。RWT将任务不匹配简化为固定的一步修正，实现了两阶段RWT Q学习框架，将方差减少与偏差修正分离。

Result: 在RKHS函数逼近下，建立了遗憾边界，其缩放与任务偏移复杂度而非目标MDP复杂度相关。在表格和神经网络设置中的实证结果表明，相比单任务学习和朴素池化方法，RWT方法取得了持续改进。

Conclusion: Bellman对齐作为在线RL的模型无关迁移原则，通过RWT框架实现了统计有效的源数据重用，解决了转移概率不匹配问题，为在线迁移强化学习提供了理论基础和实用方法。

Abstract: We study online transfer reinforcement learning (RL) in episodic Markov decision processes, where experience from related source tasks is available during learning on a target task. A fundamental difficulty is that task similarity is typically defined in terms of rewards or transitions, whereas online RL algorithms operate on Bellman regression targets. As a result, naively reusing source Bellman updates introduces systematic bias and invalidates regret guarantees.
  We identify one-step Bellman alignment as the correct abstraction for transfer in online RL and propose re-weighted targeting (RWT), an operator-level correction that retargets continuation values and compensates for transition mismatch via a change of measure. RWT reduces task mismatch to a fixed one-step correction and enables statistically sound reuse of source data.
  This alignment yields a two-stage RWT $Q$-learning framework that separates variance reduction from bias correction. Under RKHS function approximation, we establish regret bounds that scale with the complexity of the task shift rather than the target MDP. Empirical results in both tabular and neural network settings demonstrate consistent improvements over single-task learning and naïve pooling, highlighting Bellman alignment as a model-agnostic transfer principle for online RL.

</details>


### [39] [Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models](https://arxiv.org/abs/2601.21944)
*Konstantinos P. Panousis,Diego Marcos*

Main category: cs.LG

TL;DR: 该论文研究了视觉语言模型（VLMs）中概念瓶颈模型（CBMs）的可解释性评估，提出了"清晰度"指标来衡量下游性能与概念表示稀疏性、精确性之间的权衡，并开发了一个使用真实概念标注数据的可解释性评估框架。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在各领域的广泛应用引发了对其可解释性的担忧，这些模型常被视为黑箱，决策过程缺乏研究。尽管存在多种解释方法，但对学习表示的系统性客观评估仍然有限，特别是对于被认为能"诱导可解释性"的稀疏性方法。

Method: 聚焦概念瓶颈模型（CBMs），研究不同建模决策如何影响表示学习。提出"清晰度"指标来捕捉下游性能与概念表示稀疏性、精确性之间的相互作用。使用具有真实概念标注的数据集构建可解释性评估框架。考虑基于VLM和属性预测器的CBMs，以及三种稀疏性诱导策略：每样本ℓ₁、ℓ₀和基于伯努利分布的公式。

Result: 实验揭示了灵活性与可解释性之间的关键权衡，表明即使在可比性能水平下，不同方法可能表现出显著不同的行为。提出的评估框架能够客观评估不同稀疏性诱导策略的效果。

Conclusion: 该研究为概念瓶颈模型的可解释性评估提供了系统框架和量化指标，揭示了模型设计中灵活性与可解释性的权衡关系，有助于推动更透明、可解释的视觉语言模型发展。

Abstract: The widespread adoption of Vision-Language Models (VLMs) across fields has amplified concerns about model interpretability. Distressingly, these models are often treated as black-boxes, with limited or non-existent investigation of their decision making process. Despite numerous post- and ante-hoc interepretability methods, systematic and objective evaluation of the learned representations remains limited, particularly for sparsity-aware methods that are increasingly considered to "induce interpretability". In this work, we focus on Concept Bottleneck Models and investigate how different modeling decisions affect the emerging representations. We introduce the notion of clarity, a measure, capturing the interplay between the downstream performance and the sparsity and precision of the concept representation, while proposing an interpretability assessment framework using datasets with ground truth concept annotations. We consider both VLM- and attribute predictor-based CBMs, and three different sparsity-inducing strategies: per example $\ell_1, \ell_0$ and Bernoulli-based formulations. Our experiments reveal a critical trade-off between flexibility and interpretability, under which a given method can exhibit markedly different behaviors even at comparable performance levels. The code will be made publicly available upon publication.

</details>


### [40] [Geometry of Drifting MDPs with Path-Integral Stability Certificates](https://arxiv.org/abs/2601.21991)
*Zuyuan Zhang,Mahdi Imani,Tian Lan*

Main category: cs.LG

TL;DR: 论文提出了一种几何视角来建模非平稳强化学习环境，将环境视为可微同伦路径，并跟踪最优贝尔曼不动点的运动。基于此开发了HT-RL和HT-MCTS轻量级包装器，通过在线估计长度、曲率和近平局接近度来适应学习强度。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的强化学习往往是非平稳的：奖励和动态会漂移、加速、振荡并触发最优动作的突然切换。现有理论通常使用粗粒度模型来表示非平稳性，只衡量环境变化了多少，而不是如何局部变化——尽管加速度和近平局会导致跟踪误差和策略抖动。

Method: 采用几何视角建模非平稳折扣马尔可夫决策过程，将环境视为可微同伦路径，跟踪最优贝尔曼不动点的诱导运动。提出长度-曲率-扭结特征来表征内在复杂性。基于此开发了HT-RL和HT-MCTS轻量级包装器，在线估计基于回放的代理指标并相应调整学习或规划强度。

Result: 实验显示在匹配的静态基线基础上改进了跟踪性能和动态遗憾，在振荡和切换倾向的机制中获得了最大的增益。

Conclusion: 通过几何建模非平稳环境的局部变化特征，可以更有效地适应环境变化，提高强化学习在非平稳环境中的性能。

Abstract: Real-world reinforcement learning is often \emph{nonstationary}: rewards and dynamics drift, accelerate, oscillate, and trigger abrupt switches in the optimal action. Existing theory often represents nonstationarity with coarse-scale models that measure \emph{how much} the environment changes, not \emph{how} it changes locally -- even though acceleration and near-ties drive tracking error and policy chattering. We take a geometric view of nonstationary discounted Markov Decision Processes (MDPs) by modeling the environment as a differentiable homotopy path and tracking the induced motion of the optimal Bellman fixed point. This yields a length-curvature-kink signature of intrinsic complexity: cumulative drift, acceleration/oscillation, and action-gap-induced nonsmoothness. We prove a solver-agnostic path-integral stability bound and derive gap-safe feasible regions that certify local stability away from switch regimes. Building on these results, we introduce \textit{Homotopy-Tracking RL (HT-RL)} and \textit{HT-MCTS}, lightweight wrappers that estimate replay-based proxies of length, curvature, and near-tie proximity online and adapt learning or planning intensity accordingly. Experiments show improved tracking and dynamic regret over matched static baselines, with the largest gains in oscillatory and switch-prone regimes.

</details>


### [41] [Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference](https://arxiv.org/abs/2601.22132)
*Ziming Dong,Hardik Sharma,Evan O'Toole,Jaya Prakash Champati,Kui Wu*

Main category: cs.LG

TL;DR: LLM Shepherding：通过从LLM获取简短前缀（提示）来指导SLM，显著降低推理成本，在数学和编程任务上实现42-94%的成本节约。


<details>
  <summary>Details</summary>
Motivation: LLM虽然性能强大但推理成本高，SLM成本低但精度不足。现有路由和级联方法将LLM视为全有或全无的资源，缺乏细粒度控制。

Method: 提出LLM Shepherding框架，仅从LLM请求简短前缀（提示）提供给SLM。开发两阶段预测器，联合决定是否需要提示以及请求多少token。

Result: 在数学推理（GSM8K、CNK12）和代码生成（HumanEval、MBPP）基准测试中，相比纯LLM推理减少42-94%成本，相比最先进的路由和级联基线最多降低2.8倍成本。

Conclusion: LLM Shepherding通过细粒度token级预算控制，实现了SLM-LLM协作的有效平衡，是首个利用token级控制进行模型协作的工作。

Abstract: Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [42] [Agent OS](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fimran-siddique%2Fagent-os%3Futm_source=tldrinfosec/1/0100019c04ef04d1-e5d00c4d-6327-42e4-ab5b-f43c7dd3b900-000000/KtVhN2Xg_kwLMnBgTsEIEzsrEiOIrdUBMzCOeEkMVkU=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Agent OS 将操作系统概念应用于AI智能体治理，提供内核级策略执行，在行动执行前进行拦截和阻止，而非依赖基于提示的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体系统主要依赖提示工程进行安全控制，这种方法存在局限性且容易被绕过。需要更底层的、操作系统级别的安全机制来确保智能体行为的可靠性和安全性。

Method: 应用操作系统概念构建AI智能体治理框架，包括：1）内核级策略执行机制；2）POSIX风格的信号机制；3）虚拟文件系统用于智能体内存管理；4）跨模型验证；5）基于密码学签名的智能体间信任协议；6）与LangChain、CrewAI、OpenAI Assistants的集成；7）MCP服务器支持。

Result: 开发了一个完整的AI智能体操作系统框架，能够在智能体行动执行前进行策略拦截，提供比传统提示工程更可靠的安全保障，支持多种主流AI开发框架。

Conclusion: 操作系统概念为AI智能体治理提供了有效的技术框架，内核级策略执行机制显著提升了智能体系统的安全性和可靠性，为构建可信的AI智能体系统提供了新途径。

Abstract: Agent OS (GitHub Repo) Agent OS applies operating system concepts to AI agent governance, providing kernel-level policy enforcement that intercepts and blocks actions before execution rather than relying on prompt-based safety. The framework includes POSIX-inspired primitives such as signals, a virtual filesystem for agent memory, cross-model verification, inter-agent trust protocols with cryptographic signing, and integrations with LangChain, CrewAI, and OpenAI Assistants. An MCP server enab...

</details>


### [43] [LangChain works great... until your agent becomes part of a real production system](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Forkes.io%2Fblog%2Fhow-to-orchestrate-langchain-agents-for-production-with-orkes-conductor%2F%3Futm_campaign=TLDR-AI%26utm_source=Newsletter%26utm_medium=referral/2/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/iEKMIZxX9-nXqBAiV7GbWOXhHLP3Nd2GfA7y-Ep9vLQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 该指南介绍如何将LangChain代理集成到企业生产系统中，通过协调代理、服务和系统创建可观察、持久的工作流，解决代理在生产环境中的状态管理和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: LangChain代理在开发阶段表现良好，但在实际生产系统中面临挑战：多个代理协同工作时的状态管理问题、故障恢复困难、系统可观察性不足。企业环境中代理很少是独立运行的，需要与现有服务和系统集成。

Method: 采用工作流协调方法，将代理、服务和系统组织成持久化、可观察的工作流。通过专门的协调框架（如Orkes）来管理代理间的依赖关系、状态跟踪和故障恢复，而不是简单地将所有组件拼接在一起。

Result: 通过工作流协调方法，能够实现：1）代理状态的持久化跟踪；2）代理间依赖关系的可靠管理；3）故障时的自动恢复机制；4）整个系统的可观察性提升；5）企业级生产环境的稳定性保障。

Conclusion: 构建代理只是冰山一角，真正的挑战在于将其集成到生产系统中。通过工作流协调框架可以解决代理在企业环境中的可靠性、可观察性和可维护性问题，使LangChain代理能够在实际生产环境中稳定运行。

Abstract: LangChain works great... until your agent becomes part of a real production system (Sponsor) Agent A finished, Agent B is waiting, Agent C failed halfway through, and you have no idea what state your system is in. Building agents truly is just the tip of the iceberg.In enterprise environments, your LangChain agent is almost never the whole story. This Orkes technical guide shows how to coordinate your agents, services and systems in durable, observable workflows. 👉 Instead of stitching everyt...

</details>


### [44] [Kimi K2.5](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.kimi.com%2Fblog%2Fkimi-k2-5.html%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/ETpI1gQqxxvDFzaYDwlFutmRUWdiRdFgTCoZvbz8ltA=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kimi K2.5是一个多模态模型，具备视觉推理、从UI和视频输入生成代码的能力，并采用基于群体的架构进行智能任务编排，通过持续预训练构建在15T视觉语言token上。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多模态输入（特别是UI和视频）并执行复杂任务编排的AI系统，以增强AI在现实世界应用中的实用性。

Method: 基于15T视觉语言token进行持续预训练，构建多模态模型，支持视觉推理、从UI和视频生成代码，并采用基于群体的架构进行智能任务编排。

Result: 开发出Kimi K2.5模型，具备多模态处理能力，能够从视觉输入生成代码并进行复杂的任务编排。

Conclusion: Kimi K2.5展示了多模态AI在视觉推理、代码生成和任务编排方面的潜力，为更智能的AI系统开发提供了基础。

Abstract: Kimi K2.5 (7 minute read) Kimi K2.5 is a multimodal model capable of visual reasoning, code generation from UI and video inputs, and agentic task orchestration using a swarm-based architecture. It's built on 15T vision-language tokens through continual pretraining.

</details>


### [45] [Kimi Agent SDK](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMoonshotAI%2Fkimi-agent-sdk%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/jcuZFKD7xkY40RAmvoXyS7-eX3f65_6e64BQ_-_2cj0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Kimi Agent SDK是一个多语言库集，将Kimi代码智能体运行时暴露给应用程序，用于构建产品、自动化和自定义工具


<details>
  <summary>Details</summary>
Motivation: 为开发者提供统一的SDK来集成Kimi代码智能体能力到各种应用中，简化智能体集成流程

Method: 提供Go、Node.js和Python的多语言SDK库，复用Kimi CLI配置、工具和MCP服务器，支持实时响应流、审批和工具调用展示，以及程序化会话编排

Result: 实现了支持三种主流编程语言的SDK，能够将Kimi代码智能体运行时集成到应用程序中，提供实时响应和工具调用功能

Conclusion: Kimi Agent SDK为开发者提供了便捷的方式来构建基于Kimi代码智能体的产品、自动化和自定义工具

Abstract: Kimi Agent SDK (GitHub Repo) The Kimi Agent SDK is a set of multi-language libraries that expose the Kimi Code agent runtime in applications. It can be used to build products, automations, and custom tooling. The SDK clients reuse the same Kimi CLI configuration, tools, and MCP servers to stream responses in real-time, surface approvals and tool calls, and let users orchestrate sessions programmatically. Go, Node.js, and Python are supported.

</details>


### [46] [Google launches Agentic Vision in Gemini 3 Flash](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fgoogle-launches-agentic-vision-in-gemini-3-flash%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/R3tBi7Q_nUz3THf1-vk6KQxJ8t3wBh0Qa2o3Qcy65FE=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google在Gemini 3 Flash中推出Agentic Vision功能，通过代码和推理提升视觉任务性能，现已通过多个平台提供，未来将扩展模型规模和工具集成。


<details>
  <summary>Details</summary>
Motivation: 提升视觉任务的性能，通过结合代码和推理能力来解决常见的计算机视觉问题，使模型能够更有效地处理视觉任务。

Method: 在Gemini 3 Flash模型中集成Agentic Vision功能，利用代码和推理能力来增强视觉任务处理，通过API和应用程序提供该功能。

Result: 成功推出了Agentic Vision功能，用户可以通过Gemini API、Google AI Studio、Vertex AI和Gemini应用使用该功能，提升了视觉任务的性能。

Conclusion: Agentic Vision是Gemini 3 Flash的重要增强功能，未来计划支持更多模型规模并集成更多工具如网络搜索和反向图像搜索。

Abstract: Google launches Agentic Vision in Gemini 3 Flash (1 minute read) Google has introduced Agentic Vision in Gemini 3 Flash. The new capability enables the model to effectively use code and reasoning to improve performance for common vision tasks. It is available to users through the Gemini API in Google AI Studio, Vertex AI, and within the Gemini app. Google plans to extend Agentic Vision's reach by supporting more model sizes and integrating additional tools like web and reverse image search.

</details>


### [47] [Manus AI launches Agent Skills open standard for pros](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.testingcatalog.com%2Fmanus-ai-launches-agent-skills-open-standard-for-ai-workflows%2F%3Futm_source=tldrai/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/RuEUFKJHDrGNhefIoK2u0_6HHqnABwqnkaa4NRosPj8=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Manus AI推出Agent Skills开放标准，为专业人士提供模块化AI工作流，支持Python和Bash脚本集成，注重效率和互操作性


<details>
  <summary>Details</summary>
Motivation: 当前AI工作流缺乏标准化和模块化，专业人士需要更高效、可互操作的AI工具集成方案

Method: 开发Agent Skills开放标准，支持Python和Bash脚本的模块化集成，建立标准化接口和工作流框架

Result: 推出了面向专业人士的开放标准，实现了AI工作流的模块化和脚本集成

Conclusion: Agent Skills标准为AI工作流提供了标准化解决方案，提升了专业场景下的效率和互操作性

Abstract: Manus AI launches Agent Skills open standard for pros (2 minute read) Manus AI's Agent Skills open standard enables modular AI workflows for professionals, integrating Python and Bash scripts with a focus on efficiency and interoperability.

</details>


### [48] [advertise with us](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2F%3Futm_source=tldrai%26utm_medium=newsletter%26utm_campaign=advertisecta/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/hXU4dmc6agJaBkFz6whmVpgVSKsq4_aUUqnaYnUzayI=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Manus AI推出Agent Skills开放标准，让专业人士能够构建模块化AI工作流，支持Python和Bash脚本集成，注重效率和互操作性


<details>
  <summary>Details</summary>
Motivation: 为专业人士提供标准化、模块化的AI工作流构建方案，解决现有AI工具集成困难、效率低下的问题

Method: 创建Agent Skills开放标准，支持Python和Bash脚本的模块化集成，建立标准化接口和工作流框架

Result: 推出了面向专业人士的Agent Skills开放标准，实现了AI工作流的模块化构建和脚本集成

Conclusion: Agent Skills标准为专业人士提供了高效、可互操作的AI工作流解决方案，推动了AI工具的标准化应用

Abstract: Manus AI launches Agent Skills open standard for pros (2 minute read) Manus AI's Agent Skills open standard enables modular AI workflows for professionals, integrating Python and Bash scripts with a focus on efficiency and interoperability.

</details>


### [49] [create your own role](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fjobs.ashbyhq.com%2Ftldr.tech%2Fc227b917-a6a4-40ce-8950-d3e165357871/1/0100019c04f90d36-f38d399c-9849-48a7-b5e2-aec41a877b4a-000000/kEjl4BncEhvaJHd_CBmQOHY0U0NtYYYSQJuboEzOu7Q=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Manus AI推出Agent Skills开放标准，让专业人士能够构建模块化AI工作流，集成Python和Bash脚本，注重效率和互操作性


<details>
  <summary>Details</summary>
Motivation: 当前AI工作流缺乏标准化和模块化，专业人士需要更高效、可互操作的AI工具集成方案

Method: 开发Agent Skills开放标准，支持Python和Bash脚本的模块化集成，提供标准化接口和工作流管理

Result: 推出了面向专业人士的Agent Skills开放标准，实现了AI工作流的模块化和高效集成

Conclusion: Agent Skills标准为AI工作流提供了标准化解决方案，提升了专业场景下的效率和互操作性

Abstract: Manus AI launches Agent Skills open standard for pros (2 minute read) Manus AI's Agent Skills open standard enables modular AI workflows for professionals, integrating Python and Bash scripts with a focus on efficiency and interoperability.

</details>


### [50] [Contextual Agent Playbooks and Tools: How LinkedIn Gave AI Coding Agents Organizational Context](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Fblog%2Fengineering%2Fai%2Fcontextual-agent-playbooks-and-tools-how-linkedin-gave-ai-coding-agents-organizational-context%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/wqXInkvlJvmA9RxWtwRDQLWREnv3jg0WXUEBNmtJ1cs=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: LinkedIn开发了CAPT框架，通过MCP协议为AI编码代理提供组织上下文，包括安全访问内部工具和可组合的playbook工作流


<details>
  <summary>Details</summary>
Motivation: 解决AI编码代理缺乏组织上下文的问题，使其能够安全访问内部工具和利用机构知识

Method: 基于Model Context Protocol构建CAPT框架，提供标准化工具访问和Jinja2模板化的可重用playbook工作流

Result: 开发了能够为AI编码代理提供组织上下文的框架，实现安全工具访问和机构知识编码

Conclusion: CAPT框架有效增强了AI编码代理的组织上下文理解能力，提高了开发效率和安全性

Abstract: Contextual Agent Playbooks and Tools: How LinkedIn Gave AI Coding Agents Organizational Context (8 minute read) LinkedIn's Contextual Agent Playbooks & Tools (CAPT) is a framework that equips AI coding agents with deep organizational context. Built on Model Context Protocol (MCP), CAPT provides agents with safe, standardized access to internal tools and composable "playbooks" (Jinja2-templated workflows that encode institutional knowledge as reusable step-by-step instructions), allowing dynam...

</details>


### [51] [Demystifying evals for AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Fdemystifying-evals-for-ai-agents%3Futm_source=tldrdata/1/0100019c097b72f7-7822700b-6893-4806-bc8f-d1ed18972f91-000000/HTc3Hv1vNxOpJqWEtjXiLLqOw7t_rghHhfv7gXhx3oI=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了AI智能体评估框架的重要性，提出了结合代码、模型和人工评分的多维评估方法，使用行业基准和自定义测试套件来量化智能体性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体开发缺乏系统化、可扩展的评估框架，需要从被动调试转向主动的、基于指标的迭代过程，以支持可靠的智能体开发。

Method: 提出结合代码评估、模型评估和人工评估的多维评估框架，使用行业基准测试和自定义测试套件，采用pass@k和pass^k等量化指标来全面评估智能体在编码、研究、对话、GUI操作等任务上的表现。

Result: 建立了系统化的AI智能体评估方法论，能够提供更全面、可量化的性能评估，支持智能体开发的持续改进和优化。

Conclusion: 严格的自动化评估框架对于可扩展、可靠的AI智能体开发至关重要，多维度的评估方法能够有效提升智能体性能评估的准确性和实用性。

Abstract: Demystifying evals for AI agents (19 minute read) Rigorous, automated evaluation frameworks are crucial for scalable, reliable AI agent development, transforming reactive debugging into proactive, metric-driven iteration. Effective agent evals combine code-, model-, and human-based grading across multidimensional tasks (e.g., coding, research, conversation, and GUI operations) using industry benchmarks and custom test suites. Quantitative methods like pass@k and pass^k provide nuanced perform...

</details>


### [52] [AI code review with comments you'll actually implement](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetunblocked.com%2Fcode-review%2F%3Futm_source=tldrtech%26utm_medium=email%26utm_campaign=codereview_260129secondary/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/5EFvNferCup_owQPzrvUdCMal6FJRIhvoEiieion7w4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Unblocked是一款AI代码审查工具，通过深度理解代码库、文档和对话，提供基于实际系统工作原理的反馈，而非风格挑剔


<details>
  <summary>Details</summary>
Motivation: 现有AI代码审查工具往往只关注代码风格问题，缺乏对代码库整体上下文的理解，导致反馈不够实用。开发者需要能够理解系统实际工作原理的智能审查工具

Method: Unblocked通过深度分析代码库、文档和团队对话，建立对系统的全面理解，从而提供基于实际上下文的有意义反馈，而非简单的风格检查

Result: 该工具获得了开发者积极评价，有高级开发者表示它改变了他们对AI疲劳的看法，提供了只有对整个代码库有全面了解的人才能给出的上下文洞察

Conclusion: 基于深度上下文理解的AI代码审查工具比传统的风格检查工具更有价值，能够提供实际可实施的反馈，真正帮助开发者改进代码质量

Abstract: AI code review with comments you'll actually implement (Sponsor) Unblocked is the only AI code review tool that has deep insight of your codebase, docs, and conversations, giving you feedback that's shaped by how your system actually works instead of stylistic nitpicks. “Unblocked made me reconsider my AI fatigue. Finally, a tool that surfaces context only someone with a full view of the codebase could provide.” - Senior developer, Clio Try now for free

</details>


### [53] [The Five Levels: from Spicy Autocomplete to the Dark Factory](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.danshapiro.com%2Fblog%2F2026%2F01%2Fthe-five-levels-from-spicy-autocomplete-to-the-software-factory%2F%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/IuyDcmwrfAVGvALurwcd_QVXxa93O0LLrXgWR7tHqG4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 文章将AI代码生成类比自动驾驶分级，提出从基础自动补全到完全自主"黑暗工厂"的五级演进框架，建议团队利用AI成本下降趋势，推迟人工技术债务偿还，用更便宜的AI工时来偿还。


<details>
  <summary>Details</summary>
Motivation: 代码生成AI成本快速下降，团队需要重新考虑技术债务偿还策略。传统人工偿还方式成本高昂，而AI工时成本更低，但需要理解如何有效利用AI自动化。

Method: 借鉴美国联邦政府自动驾驶分级标准，提出代码生成的五级自动化框架：从基础自动补全到完全自主的"黑暗工厂"。

Result: 建立了一个清晰的AI代码生成演进路线图，帮助团队理解如何逐步提升自动化水平，利用廉价AI工时偿还技术债务。

Conclusion: 智能团队应推迟人工技术债务偿还，利用AI成本下降趋势，通过理解并实施自动化分级框架，用更便宜的AI工时来优化开发流程。

Abstract: The Five Levels: from Spicy Autocomplete to the Dark Factory (4 minute read) The cost of code is dropping so fast that teams need to reconsider their tech debt payment plans. Smart teams are deferring payment on human hours today to pay them back with cheaper AI hours tomorrow. Cashing in on those cheap hours requires understanding how to move through the tiers of automation. Luckily, the federal government has already completed a roadmap for autonomous cars. The five levels of driving automa...

</details>


### [54] [The 80% Problem in Agentic Coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrnewsletter/1/0100019c0980e601-4e022604-d45d-40f7-bf3a-22109c267184-000000/s6Erwr-zVqpt5uUls1RYXIHG8dTpE7o_hQ45vNdWjwU=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI在代码生成中完成80%工作，但人类仍需负责结果质量、测试验证，AI会放大开发实践，不良流程将导致技术债务快速累积


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码代理在完成大部分编码工作后，人类开发者的角色转变和责任问题，以及AI如何放大现有开发实践的优劣

Method: 通过分析AI代码代理的实际应用场景，讨论人类与AI在编码过程中的责任分配和协作模式

Result: AI成功完成大部分编码任务可能导致开发者停止检查输出，从而放大不良开发实践，加速技术债务积累

Conclusion: AI代码代理是开发实践的放大器，需要建立新的质量保证机制和责任框架来应对80%问题

Abstract: The 80% Problem in Agentic Coding (20 minute read) AI writing most of the code places a disproportionate emphasis on the human's role of owning outcomes, maintaining quality bars, and ensuring that tests actually validate behavior. The danger is that the AI succeeds so well that developers stop checking their outputs. AI is an amplifier of development practices. Bad processes will result in accumulated debt at an unprecedented speed.

</details>


### [55] [See real PRs where Greptile caught the bug](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.greptile.com%2Fexamples%3Futm_source=tldr%26utm_medium=sponsorship%26utm_campaign=tldr_dev/2/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/zfeoC4vTLfAMrmBXf0mVWwbJVHMVnGuU6tNdBrD2DuY=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Greptile是一款AI代码审查工具，通过分析完整代码库上下文和学习团队规范来自动审查PR，提高代码质量和开发速度


<details>
  <summary>Details</summary>
Motivation: 随着AI编写代码越来越多，代码审查成为质量和开发速度的瓶颈，需要自动化工具来解决这个问题

Method: 使用AI分析完整代码库上下文，从团队评论、反应和实际合并的代码中学习团队规范，自动审查每个PR

Result: 已被ScaleAI、Zapier、Whoop等工程团队信任使用，现已集成到Claude Code插件中

Conclusion: Greptile提供14天免费试用，开源项目免费使用，能够有效提升代码审查效率

Abstract: See real PRs where Greptile caught the bug (Sponsor) AI is writing more code, but code review is still a bottleneck for quality and velocity.Greptile reviews each PR with full repo context and learns your team's conventions from comments, reactions, and what actually gets merged. ✅ Trusted by engineering teams at ScaleAI, Zapier, and Whoop. 🧩 Now integrated with Claude Code: install via /plugin. 🎯 Free for open source. Try Greptile free for 14 days

</details>


### [56] [The 80% Problem in Agentic Coding](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faddyo.substack.com%2Fp%2Fthe-80-problem-in-agentic-coding%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/wiKRpjl3PqrXtSymdapM2BtTg6qqz8P99qthG4WRCao=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 工程师使用AI代理编写80-100%代码，角色从直接实现者转变为协调者，但带来了概念性AI错误、代码理解债务和生产力悖论等新问题，需要转向声明式方法和架构监督。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理在代码编写中的广泛应用（覆盖80-100%代码），工程师角色发生根本转变，从直接编码者变为协调者，这引发了新的挑战需要解决。

Method: 提出工程师需要转向声明式方法，专注于问题定义和架构监督，而不是直接实现细节。

Result: AI代理的广泛使用带来了三个主要问题：1）概念性AI错误；2）代码库中的理解债务；3）生产力悖论（审查时间大幅增加）。

Conclusion: 工程师需要适应新的角色定位，采用声明式编程思维，加强问题定义和架构监督能力，以应对AI代理带来的新挑战。

Abstract: The 80% Problem in Agentic Coding (19 minute read) Many engineers now use AI agents to write 80-100% of their code, transforming their role from direct implementers to orchestrators. This adoption has introduced new problems, such as conceptual AI errors, comprehension debt within codebases, and a productivity paradox where review times balloon. Engineers need to shift to a declarative approach, focusing on problem definition and architectural oversight.

</details>


### [57] [The new era of browsing: Putting Gemini to work in Chrome](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Fproducts-and-platforms%2Fproducts%2Fchrome%2Fgemini-3-auto-browse%2F%3Futm_source=tldrdev/1/0100019c09a85cd6-32f217b1-61bb-4cfb-af76-7432261123e0-000000/K2W_GHwec5zNJDlwV35przM5kBQX-0YINGKt4Rh4uxM=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Google将Gemini AI深度集成到Chrome浏览器，新增侧边栏支持多标签页操作，并为AI Pro和Ultra订阅者提供"自动浏览"功能，可自主处理复杂多步骤任务。


<details>
  <summary>Details</summary>
Motivation: 提升Chrome浏览器的智能化水平，通过AI增强用户体验，实现更高效的网页浏览和多任务处理，特别是针对复杂任务场景。

Method: 将Gemini AI深度集成到Chrome浏览器架构中，新增侧边栏界面支持跨标签页操作，与Gmail、Calendar等Google应用深度整合，并为高级用户提供自主浏览代理功能。

Result: Chrome浏览器获得了强大的AI功能，用户可以通过侧边栏进行多任务处理，高级订阅者能够使用自动浏览功能处理复杂任务如假期规划、在线表单填写和智能购物。

Conclusion: Chrome浏览器的AI集成标志着浏览体验的新时代，通过智能代理功能显著提升了复杂任务的自动化处理能力。

Abstract: The new era of browsing: Putting Gemini to work in Chrome (7 minute read) Google is integrating major Gemini AI updates into Chrome, including a new side panel for multitasking across tabs and deeper integrations with other Google apps like Gmail and Calendar. For AI Pro and Ultra subscribers, Chrome now has "auto browse," an agentic feature that automatically browses the web to handle complex multi-step tasks such as vacation planning, online forms, and smart shopping.

</details>


### [58] [On Coding Agents and the Future of Design](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fveen.com%2Fjeff%2Farchives%2Fcoding-agents-design.html%3Futm_source=tldrdesign/1/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/QjfrbndhRkS0l8D__gSmkyBpiPKhHSRNG7gITTzXqFc=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 编码代理（如Claude Code）通过将应用简化为基本原语，推动设计进入新阶段，使设计转变为纯粹的战略决策，竞争优势来自于诚实地表达产品能力。


<details>
  <summary>Details</summary>
Motivation: 探讨编码代理如何改变设计范式，类似于移动优先设计揭示简单界面更好服务用户需求，编码代理迫使组织澄清产品核心功能，推动设计向战略层面发展。

Method: 通过类比移动优先设计革命，分析编码代理如何将应用简化为基本原语，迫使组织明确产品核心功能，将设计提升为纯粹的战略决策。

Result: 编码代理推动设计进入新阶段，使设计从界面实现转变为战略表达，竞争优势来自于诚实地展示产品能力而非界面复杂性。

Conclusion: 编码代理代表设计的新阶段，将设计提升为纯粹的战略，组织需要清晰表达产品核心能力，这将成为未来的竞争优势。

Abstract: On Coding Agents and the Future of Design (5 minute read) Coding agents like Claude Code represent a new phase of responsive design by stripping applications down to their essential primitives. This shift forces organizations to clarify what their products actually do, much like mobile-first design revealed that simpler interfaces often better serve user needs. This agent-driven future elevates design to pure strategy, where competitive advantage comes from honestly expressing capabilities ra...

</details>


### [59] [It gets lonely waiting in backlog](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fv0.app%2F%3Futm_medium=email%26utm_source=tldr%26utm_campaign=tldrnl20250129/6/0100019c09dd2cde-101e364a-601d-4cb1-a361-e1c3e80d335f-000000/jQwVHIuf_l0znAh8a3fgO99XPxEceAtuagPIw8FIp4g=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: v0是一个AI驱动的代码生成工具，能够将自然语言提示直接转换为生产就绪的代码并部署到真实的Git仓库中，旨在解决原型开发后因积压而无法落地的问题。


<details>
  <summary>Details</summary>
Motivation: 解决前端开发中常见的原型设计完成后长期积压在待办列表中无法落地的问题，消除前端工程师因各种借口而拖延将原型转化为实际产品的障碍。

Method: 通过AI技术将自然语言提示直接转换为生产就绪的代码，并自动部署到真实的Git代码仓库中，实现从想法到可运行产品的快速转化。

Result: v0工具能够根据用户提示生成可直接用于生产的代码，并自动创建Git仓库，使前端工程师无法再以各种技术借口拖延项目落地。

Conclusion: v0通过AI驱动的代码生成和自动化部署，有效解决了原型积压问题，加速了从创意到实际产品的转化过程。

Abstract: It gets lonely waiting in backlog (Sponsor) You designed a great prototype…three months ago. Now it's wasting away in backlog and you're wondering why you even tried. With v0, every prompt generates production-ready code in a real git repository - so your frontend engineering friends have no more excuses. Start creating on v0

</details>


### [60] [Building Ramp Sheets: Ramp Labs on spreadsheet agents and shipping velocity](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.akashbajwa.co%2Fp%2Fbuilding-ramp-sheets-ramp-labs-and%3Futm_source=tldrfintech/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/Jdn-8nbTbwqC_LHV-48klgntnuusNwTbXQL8mv6YY98=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ramp Labs开发了Ramp Sheets，这是一个将智能AI代理与Excel原生信任和审计功能结合的电子表格代理系统，专门为高风险金融场景设计。


<details>
  <summary>Details</summary>
Motivation: 电子表格是高风险金融场景的理想界面，但需要结合AI能力来提升效率，同时保持Excel原生的信任和审计特性。

Method: 将智能AI代理技术与Excel电子表格结合，创建Ramp Sheets系统，通过应用AI文化优化速度、设计和真实用户反馈。

Result: 开发了专门为金融场景设计的电子表格代理系统，结合了AI智能和Excel的信任审计功能。

Conclusion: Ramp Sheets展示了如何将智能AI代理与电子表格结合，为高风险金融应用提供高效、可信的解决方案。

Abstract: Building Ramp Sheets: Ramp Labs on spreadsheet agents and shipping velocity (8 minute read) Ramp Labs leaders Alex Stauffer and Alex Shevchenko explain why spreadsheets are the ideal interface for high-stakes finance, how Ramp Sheets pairs agentic AI with Excel-native trust and auditability, and how Ramp's applied-AI culture optimizes for speed, design, and real user feedback over rigid roadmaps.

</details>


### [61] [Inc.'s Best Bootstrapped businesses](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linkedin.com%2Ffeed%2Fupdate%2Furn:li:activity:7401699691039830016%2F/1/0100019c0a1512b1-baebb25e-a90d-440c-9f6d-c1e631c82a01-000000/rLZgU_Q3Zkg5AvxGiOEtpAVWuCTzBtxsz407eiA1XJQ=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Ramp Labs开发了Ramp Sheets，将智能AI代理与Excel原生信任和审计功能结合，为高风险金融提供理想的电子表格界面。


<details>
  <summary>Details</summary>
Motivation: 电子表格是高风险金融的理想界面，但需要结合AI能力。Ramp希望将智能AI代理与Excel原生的信任和审计功能结合，优化开发速度、设计和真实用户反馈。

Method: Ramp Sheets将智能AI代理与Excel原生功能配对，通过Ramp的应用AI文化，优先考虑速度、设计和真实用户反馈，而非僵化的路线图。

Result: 开发了Ramp Sheets产品，将智能AI能力与电子表格的信任和审计功能结合，为金融领域提供高效解决方案。

Conclusion: 电子表格与智能AI代理的结合为高风险金融提供了理想的解决方案，Ramp的应用AI文化确保了产品的快速开发和用户导向。

Abstract: Building Ramp Sheets: Ramp Labs on spreadsheet agents and shipping velocity (8 minute read) Ramp Labs leaders Alex Stauffer and Alex Shevchenko explain why spreadsheets are the ideal interface for high-stakes finance, how Ramp Sheets pairs agentic AI with Excel-native trust and auditability, and how Ramp's applied-AI culture optimizes for speed, design, and real user feedback over rigid roadmaps.

</details>


### [62] [New Sandbox Escape Flaw Exposes n8n Instances to RCE Attacks](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.bleepingcomputer.com%2Fnews%2Fsecurity%2Fnew-sandbox-escape-flaw-exposes-n8n-instances-to-rce-attacks%2F%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/o84Vl1W2HNm9SH1J4CzHa_Rd8YMgDnuH8eGWl4t44sY=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 研究人员在n8n工作流自动化平台中发现两个新的AST沙箱逃逸漏洞，可导致远程代码执行，其中一个CVSS评分9.9


<details>
  <summary>Details</summary>
Motivation: n8n作为流行的工作流自动化平台，其沙箱机制存在安全漏洞，攻击者可能利用这些漏洞在底层节点上执行任意代码，对用户系统造成严重威胁

Method: 研究人员通过分析n8n平台的Python和JavaScript沙箱实现，发现了AST（抽象语法树）沙箱逃逸漏洞，这些漏洞允许攻击者绕过沙箱限制

Result: 发现两个高危漏洞，其中一个获得CVSS 9.9的高分，n8n云平台已修复漏洞，但自托管版本用户需要手动更新

Conclusion: 工作流自动化平台的沙箱安全机制需要更严格的审查，用户应及时更新自托管版本以防范潜在攻击

Abstract: New Sandbox Escape Flaw Exposes n8n Instances to RCE Attacks (2 minute read) Researchers from JFrog discovered two new vulnerabilities in the n8n workflow automation platform that could allow for RCE, one of which received a CVSS score of 9.9. The two vulnerabilities are AST sandbox escapes in both Python and JavaScript sandboxes that could allow for arbitrary code execution on the underlying nodes. The vulnerability has been fixed in the n8n cloud platform, but users running self-hosted vers...

</details>


### [63] [A first look at Aperture by Tailscale](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftailscale.com%2Fblog%2Faperture-private-alpha%3Futm_source=tldrinfosec/1/0100019c0a154664-9526f8ca-f022-4bc3-8c65-d1baad275928-000000/3LgZNm3KKNl_qHEBJ_aBaDN5KOix793GOoxRYd2k5Lw=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Aperture是Tailscale开发的AI网关工具，处于alpha阶段，用于监控组织内的代码代理使用情况，提供身份验证、使用洞察和成本跟踪功能。


<details>
  <summary>Details</summary>
Motivation: 组织需要更好地监控和管理内部AI代码代理的使用情况，避免API密钥共享的安全风险，同时需要集中跟踪不同模型和提供商的使用成本和效率。

Method: 利用Tailscale内置的身份系统进行用户和机器认证，支持CLI和VS Code的AI编码代理（如Claude Code、Codex、Gemini CLI），提供集中监控仪表板。

Result: 目前处于私有alpha阶段，提供了消除API密钥共享需求、集中监控代码代理使用、跨模型和提供商成本跟踪的功能。

Conclusion: Aperture作为AI网关工具，为组织提供了安全、可控的AI代码代理使用环境，有助于优化资源分配和提升开发效率。

Abstract: A first look at Aperture by Tailscale (private alpha) (6 minute read) Aperture is an alpha-stage AI gateway that offers insights into coding agent usage within organizations. It eliminates the need to share API keys by using Tailscale's built-in identity system for authenticating users and machines. The tool supports CLI and VS Code-based AI coding agents such as Claude Code, Codex, and Gemini CLI, providing centralized monitoring of usage, cost tracking across different models and providers,...

</details>


### [64] [AI for when it is rocket science](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门用于处理复杂专业任务，已在制造业和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂、专业化工作时仍然存在局限，虽然能处理日常任务如起草邮件，但在专业技术领域（如火箭科学、热试车结果分析）的可信度不足

Method: 开发了Agent Composer工具，专门针对复杂任务设计，能够自动化处理传感器数据解析和日志关联等专业技术工作

Result: 1. 先进制造企业将根本原因分析时间从8小时缩短至20分钟；2. 技术驱动的3PL提供商也取得了显著效率提升

Conclusion: AI在复杂专业领域具有巨大应用潜力，通过专门设计的工具可以显著提升专业技术工作的效率和可靠性

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [65] [8 hours to 20 minutes](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer专门用于处理复杂任务，已在制造业和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂、专业化任务时仍存在不足，特别是在需要专业知识的领域（如火箭科学、高级技术问题），需要专门针对复杂任务设计的AI解决方案

Method: 开发了Agent Composer系统，通过自动化传感器数据解析和日志关联等技术，专门处理复杂任务

Result: 1. 先进制造企业将根本原因分析从8小时缩短到20分钟；2. 技术驱动的3PL提供商也取得了显著成果

Conclusion: Contextual AI的Agent Composer能够有效处理复杂专业化任务，在实际应用中实现了显著的效率提升

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [66] [60x faster issue resolution](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门用于处理复杂专业任务，通过自动化传感器数据解析和日志关联，将根本原因分析时间从8小时缩短到20分钟


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂、专业化工作时仍然存在局限性，虽然能处理简单任务如起草邮件，但在需要专业知识的领域（如火箭科学、高级技术问题）表现不佳，需要专门针对复杂任务的AI解决方案

Method: 开发了Agent Composer工具，专门为复杂任务设计，通过自动化传感器数据解析和日志关联等技术，处理专业领域的工作流程

Result: 在先进制造领域，将根本原因分析时间从8小时缩短到20分钟；在技术驱动的第三方物流提供商中也取得了显著效果

Conclusion: 针对复杂专业任务的专门化AI工具能够显著提高工作效率，在需要专业知识的领域具有重要应用价值

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [67] [in minutes instead of days](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcontextual.ai%2Fblog%2Fintroducing-agent-composer%3Futm_campaign=ac-launch-2026%26utm_source=tldrai%26utm_medium=email-ad%26utm_content=primary/2/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/CQbPGKqwORl1G2xpMsIzX0SYy4MIW86cetk5PDGZPq0=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Contextual AI开发了Agent Composer工具，专门用于处理复杂专业任务，已在制造业和物流领域实现显著效率提升


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理复杂专业任务（如火箭科学、高级技术问题）方面仍存在不足，需要专门针对复杂工作的AI解决方案

Method: 开发了Agent Composer工具，通过自动化传感器数据解析和日志关联等技术，专门为复杂任务设计

Result: 先进制造商将根本原因分析时间从8小时缩短到20分钟；技术驱动的3PL提供商也取得了显著成果

Conclusion: Contextual AI的Agent Composer能够有效处理复杂专业任务，在工业应用中实现重大效率提升

Abstract: AI for when it is rocket science (Sponsor) AI still fails at complex, specialized work. Sure, it can draft emails — but does anyone really trust it to review overnight hot-fire test results or answer advanced technical questions?Contextual AI built Agent Composer specifically for complex tasks. Here's what it's already done: An advanced manufacturer reduced root-cause analysis from 8 hours to 20 minutes by automating sensor data parsing and log correlation. A tech-enabled 3PL provider achieve...

</details>


### [68] [Meta's AI Plans Include Shopping Agents and New Models](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2026%2F01%2F28%2Fzuckerberg-teases-agentic-commerce-tools-and-major-ai-rollout-in-2026%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/roUxrfZ-yTJqrMVXz4P0sSfHKpz9WGZXUH2RxYupHko=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Meta宣布2026年AI产品路线图，重点推出面向电商的新模型和智能体工具，包括个性化购物助手


<details>
  <summary>Details</summary>
Motivation: Meta希望通过利用其用户上下文数据的优势，在AI电商领域建立差异化竞争优势，提供更个性化的购物体验

Method: 开发新的AI模型和智能体工具，专注于电商应用场景，利用Meta平台的用户数据和上下文信息

Result: 宣布了2026年的主要AI产品发布计划，强调个性化AI购物助手将成为关键差异化功能

Conclusion: Meta将AI电商智能体作为战略重点，计划通过个性化购物助手和用户上下文优势在竞争中脱颖而出

Abstract: Meta's AI Plans Include Shopping Agents and New Models (4 minute read) Meta announced a major AI product rollout for 2026, emphasizing new models and agentic tools focused on commerce. Mark Zuckerberg pointed to personalized AI shopping assistants and Meta's access to user context as key differentiators in delivering tailored agent experiences.

</details>


### [69] [Towards a science of scaling agent systems: When and why agent systems work](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fresearch.google%2Fblog%2Ftowards-a-science-of-scaling-agent-systems-when-and-why-agent-systems-work%2F%3Futm_source=tldrai/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/U3VwOfvzCTadfbFwivnuB2vmbZhiw9gEYu2zst11-b4=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了智能体系统的规模化科学，指出单纯增加智能体数量存在性能天花板，需要从启发式方法转向量化原则来构建更智能、安全、高效的下一代AI智能体系统。


<details>
  <summary>Details</summary>
Motivation: 随着模型变得更智能，多智能体系统的需求加速增长，但当前"更多智能体"的方法存在局限性，甚至可能降低性能。需要建立科学原则来指导智能体系统的规模化发展。

Method: 论文提出需要从启发式方法转向基于任务特定属性的量化原则，建立智能体系统规模化的科学框架，分析何时以及为何智能体系统能够有效工作。

Result: 指出了当前多智能体系统规模化方法的局限性，提出了建立智能体系统规模化科学的重要性，为下一代AI智能体系统的发展指明了方向。

Conclusion: 下一代AI智能体系统需要基于量化原则而非启发式方法，通过科学化的规模化方法实现更智能、安全、高效的智能体系统。

Abstract: Towards a science of scaling agent systems: When and why agent systems work (4 minute read) Smarter models accelerate the need for multi-agent systems. However, the 'more agents' approach often hits a ceiling. It can even degrade performance if not aligned with the specific properties of the task. The next generation of AI agents needs to move from heuristics to quantitative principles to become more numerous, smarter, safer, and efficient.

</details>


### [70] [Keeping Your Data Safe When an AI Agent Clicks a Link](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbbaOkP/1/0100019c0a221193-af6acc78-3797-4ee0-9578-69f1e1aa4238-000000/Z_Lj_znhViyfLJmFMeYwRF4Pmi1CIAP6SNyaInCgEnk=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: OpenAI推出新的安全机制，限制AI代理只能自动获取公开索引的网页链接，防止在自动抓取URL时泄露私人数据


<details>
  <summary>Details</summary>
Motivation: AI代理在自动获取URL时可能无意中泄露私人数据，需要安全机制来保护用户隐私

Method: 设计安全机制，只允许AI代理自动检索在开放网络上公开索引的链接，限制对非公开或私人内容的访问

Result: 通过限制只能访问公开索引的网页，减少了AI代理在自动抓取URL时泄露私人数据的风险

Conclusion: 这种安全机制是保护用户隐私的重要措施，确保AI代理在访问网络资源时不会无意中暴露敏感信息

Abstract: Keeping Your Data Safe When an AI Agent Clicks a Link (5 minute read) OpenAI's new safety mechanism is designed to reduce the risk of AI agents inadvertently leaking private data when they automatically fetch URLs by only allowing automatic retrieval of links that are publicly indexed on the open web.

</details>


### [71] [One Year of AX](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbiilmann.blog%2Farticles%2Fone-year-of-ax%2F%3Futm_source=tldrproduct/1/0100019c0e95e84f-c1dc39b2-c6cb-4c2b-abd9-c9da6245a32f-000000/RGxBtiQsfmk67ri21g6b6bXH9fT9Z75Z3TOeJtInToI=442)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文探讨了随着AI代理成为主要用户，产品需要优化"代理体验"（AX），并分析了决定平台相关性的四个关键因素


<details>
  <summary>Details</summary>
Motivation: 随着AI代理逐渐成为产品的主要用户，传统的用户体验（UX）设计已不再适用，需要专门为代理设计体验（AX）来保持平台的相关性和竞争力

Method: 通过分析代理作为用户的特点，提出了决定平台相关性的四个关键维度：访问权限、上下文理解、工具集成和编排能力

Result: 识别了代理体验（AX）作为新的设计范式，并建立了评估平台代理友好度的框架，强调平台需要在这些维度上进行优化

Conclusion: 随着AI代理的普及，产品设计必须从用户体验（UX）转向代理体验（AX），那些在访问、上下文、工具和编排方面表现优异的平台将保持竞争力

Abstract: One Year of AX (6 minute read) As agents become primary users, products must optimize for Agent Experience. Access, context, tools, and orchestration will determine which platforms remain relevant.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [72] [A Survey on Large Language Model Impact on Software Evolvability and Maintainability: the Good, the Bad, the Ugly, and the Remedy](https://arxiv.org/abs/2601.20879)
*Bruno Claudino Matias,Savio Freire,Juliana Freitas,Felipe Fronchetti,Kostadin Damevski,Rodrigo Spinola*

Main category: cs.SE

TL;DR: LLMs在软件工程中能提升可维护性和可演化性，但也带来幻觉、不稳定等风险，需要安全措施和人工监督


<details>
  <summary>Details</summary>
Motivation: LLMs在软件工程中应用广泛，但现有研究证据分散，对长期软件可维护性和可演化性的影响不明确，需要系统研究

Method: 对2020-2024年ACM DL、IEEE Xplore和Scopus的87项研究进行系统文献综述，采用混合主题分析方法，结合LLM辅助工具和人工验证

Result: LLMs能提升可分析性、可测试性、代码理解、调试支持和自动修复，但也带来幻觉输出、上下文脆弱性、领域推理有限、性能不稳定等风险

Conclusion: LLMs能增强软件可维护性和可演化性，但也对长期可持续性构成风险，需要安全措施、严格评估和结构化人工监督

Abstract: Context. Large Language Models (LLMs) are increasingly embedded in software engineering workflows for tasks including code generation, summarization, repair, and testing. Empirical studies report productivity gains, improved comprehension, and reduced cognitive load. However, evidence remains fragmented, and concerns persist about hallucinations, unstable outputs, methodological limitations, and emerging forms of technical debt. How these mixed effects shape long-term software maintainability and evolvability remains unclear. Objectives. This study systematically examines how LLMs influence the maintainability and evolvability of software systems. We identify which quality attributes are addressed in existing research, the positive impacts LLMs provide, the risks and weaknesses they introduce, and the mitigation strategies proposed in the literature. Method. We conducted a systematic literature review. Searches across ACM DL, IEEE Xplore, and Scopus (2020 to 2024) yielded 87 primary studies. Qualitative evidence was extracted through a calibrated multi-researcher process. Attributes were analyzed descriptively, while impacts, risks, weaknesses, and mitigation strategies were synthesized using a hybrid thematic approach supported by an LLM-assisted analysis tool with human-in-the-loop validation. Results. LLMs provide benefits such as improved analyzability, testability, code comprehension, debugging support, and automated repair. However, they also introduce risks, including hallucinated or incorrect outputs, brittleness to context, limited domain reasoning, unstable performance, and flaws in current evaluations, which threaten long-term evolvability. Conclusion. LLMs can strengthen maintainability and evolvability, but they also pose nontrivial risks to long-term sustainability. Responsible adoption requires safeguards, rigorous evaluation, and structured human oversight.

</details>


### [73] [DevOps-Gym: Benchmarking AI Agents in Software DevOps Cycle](https://arxiv.org/abs/2601.20882)
*Yuheng Tang,Kaijie Zhu,Bonan Ruan,Chuqi Zhang,Michael Yang,Hongwei Li,Suyue Guo,Tianneng Shi,Zekun Li,Christopher Kruegel,Giovanni Vigna,Dawn Song,William Yang Wang,Lun Wang,Yangruibo Ding,Zhenkai Liang,Wenbo Guo*

Main category: cs.SE

TL;DR: DevOps-Gym：首个端到端AI代理DevOps基准，包含700+真实任务，评估显示现有模型在问题解决、测试生成等核心DevOps工作流上存在根本性局限


<details>
  <summary>Details</summary>
Motivation: 尽管AI代理在代码生成和软件问题解决方面表现出色，但在完整软件DevOps周期中的能力仍然未知。现有基准主要关注孤立问题，缺乏DevOps所需的环境和工具接口，无法评估代理在开发、部署、管理等真实场景中的表现。

Method: 提出DevOps-Gym基准，包含构建与配置、监控、问题解决和测试生成四大核心DevOps工作流。通过半自动化数据收集机制，从30+个Java和Go项目中收集700+真实任务，并投入大量专家精力确保任务覆盖范围和质量。

Result: 评估最先进模型和代理发现根本性局限：在Java和Go的问题解决和测试生成方面表现不佳，无法处理监控、构建与配置等新任务类型。这些结果表明AI代理在自动化完整DevOps周期方面仍需重要研究突破。

Conclusion: DevOps-Gym是首个端到端DevOps基准，揭示了AI代理在真实软件DevOps工作流中的显著能力差距，强调了在自动化完整DevOps周期方面进行基础研究的必要性。

Abstract: Even though demonstrating extraordinary capabilities in code generation and software issue resolving, AI agents' capabilities in the full software DevOps cycle are still unknown. Different from pure code generation, handling the DevOps cycle in real-world software, including developing, deploying, and managing, requires analyzing large-scale projects, understanding dynamic program behaviors, leveraging domain-specific tools, and making sequential decisions. However, existing benchmarks focus on isolated problems and lack environments and tool interfaces for DevOps. We introduce DevOps-Gym, the first end-to-end benchmark for evaluating AI agents across core DevOps workflows: build and configuration, monitoring, issue resolving, and test generation. DevOps-Gym includes 700+ real-world tasks collected from 30+ projects in Java and Go. We develop a semi-automated data collection mechanism with rigorous and non-trivial expert efforts in ensuring the task coverage and quality. Our evaluation of state-of-the-art models and agents reveals fundamental limitations: they struggle with issue resolving and test generation in Java and Go, and remain unable to handle new tasks such as monitoring and build and configuration. These results highlight the need for essential research in automating the full DevOps cycle with AI agents.

</details>


### [74] [IDE-Bench: Evaluating Large Language Models as IDE Agents on Real-World Software Engineering Tasks](https://arxiv.org/abs/2601.20886)
*Spencer Mateega,Jeff Yang,Tiana Costello,Shaurya Jadhav,Nicole Tian,Agustin Garcinuño*

Main category: cs.SE

TL;DR: IDE-Bench是一个通过IDE原生工具接口评估AI IDE代理在真实世界软件工程任务中的综合框架，包含Docker化测试环境和80个未发布仓库的任务，防止训练数据污染。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估方法通常基于原始终端执行，无法准确评估AI在真实IDE环境中的协作能力，缺乏对未污染代码库的系统性评估。

Method: 创建Docker化测试框架，提供结构化工具生态系统（代码库搜索、结构化文件编辑、全栈应用测试工具），在8个未发布的C/C++、Java和MERN仓库中设计了80个任务，涵盖功能实现、bug修复、重构和性能优化。

Result: 首个在多语言全栈环境中系统性地将代理报告意图与成功项目级修改相关联的基准，在完全未污染的代码上评估AI代理作为工程协作者的能力。

Conclusion: IDE-Bench为评估AI IDE代理在真实软件工程任务中的表现提供了全面框架，填补了现有评估方法的空白，特别注重防止训练数据污染。

Abstract: IDE-Bench is a comprehensive framework for evaluating AI IDE agents on real-world software engineering tasks through an IDE-native tool interface. We present a Dockerized test harness that goes beyond raw terminal execution, granting models a structured tool ecosystem that represents AI-native IDEs like Cursor and Windsurf. By providing high-level abstractions for codebase search, structured file editing, and tools for testing full-stack applications, IDE-Bench evaluates an agent's ability to act as a true engineering collaborator. For evaluation and to prevent training data contamination, we created 80 tasks across eight never-published repositories spanning C/C++, Java, and MERN stacks, representing modern tech stack production scenarios, including feature implementation, bug fixing, refactoring, and performance optimization that mirror daily developer workflows in private codebases. Our benchmark is the first to systematically correlate agent-reported intent with successful project-level modifications in a multi-language, full-stack environment on completely uncontaminated code.

</details>


### [75] [Leveraging Generative AI for Enhancing Domain-Driven Software Design](https://arxiv.org/abs/2601.20909)
*Götz-Henrik Wiegand,Filip Stepniak,Patrick Baier*

Main category: cs.SE

TL;DR: 使用生成式AI（基于Code Llama的4位量化版本和LoRA微调）部分自动化领域驱动设计中的元模型生成，通过简单提示生成语法正确的JSON对象，降低资源需求并提高设计效率。


<details>
  <summary>Details</summary>
Motivation: 传统领域驱动设计（DDD）中，描述领域的元模型需要系统设计师手动创建，过程耗时且资源密集。研究旨在探索使用生成式AI部分自动化这一过程，特别是生成领域特定的JSON对象，以简化设计流程并降低资源需求。

Method: 在真实世界DDD项目数据上训练生成式AI模型，使用4位量化的Code Llama版本，通过Low-Rank Adaptation（LoRA）在消费级GPU上进行微调，使模型能够根据简单提示生成语法正确的JSON对象。

Result: 尽管硬件资源有限，模型仍实现了高性能，能够生成准确的JSON对象且只需最小化后处理。这表明生成式AI可以有效集成到DDD流程中，提高效率并降低资源需求。

Conclusion: 研究证明了生成式AI在领域驱动设计中的可行性，能够显著提高元模型生成效率，为AI驱动的软件开发奠定基础，并展示了在资源受限环境下实现高性能的可能性。

Abstract: Domain-Driven Design (DDD) is a key framework for developing customer-oriented software, focusing on the precise modeling of an application's domain. Traditionally, metamodels that describe these domains are created manually by system designers, forming the basis for iterative software development. This paper explores the partial automation of metamodel generation using generative AI, particularly for producing domain-specific JSON objects. By training a model on real-world DDD project data, we demonstrate that generative AI can produce syntactically correct JSON objects based on simple prompts, offering significant potential for streamlining the design process. To address resource constraints, the AI model was fine-tuned on a consumer-grade GPU using a 4-bit quantized version of Code Llama and Low-Rank Adaptation (LoRA). Despite limited hardware, the model achieved high performance, generating accurate JSON objects with minimal post-processing. This research illustrates the viability of incorporating generative AI into the DDD process, improving efficiency and reducing resource requirements, while also laying the groundwork for further advancements in AI-driven software development.

</details>


### [76] [Towards Comprehensive Benchmarking Infrastructure for LLMs In Software Engineering](https://arxiv.org/abs/2601.21070)
*Daniel Rodriguez-Cardenas,Xiaochang Li,Marcos Macedo,Antonio Mastropaolo,Dipin Khati,Yuan Tian,Huajie Shao,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 论文提出了BEHELM基准框架，旨在解决当前代码LLM评估的局限性，通过统一软件场景规范和多维度评估来提供更全面、可靠的模型评测


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型发展迅速，但评估能力滞后。现有基准测试存在以下问题：任务狭窄、指标单一、缺乏鲁棒性/可解释性/公平性/效率/实际可用性评估、数据工程实践不一致、软件工程上下文有限、数据污染普遍。这些问题阻碍了对模型能力的准确评估。

Method: 通过深入调研现有基准测试和社区研讨会收集见解，识别了三个核心障碍。基于这些发现，提出了BEHELM基准框架，该框架统一了软件场景规范和多维度评估，支持跨任务、语言、输入输出粒度以及关键质量维度的结构化评估。

Result: 识别了可靠评估的三个核心障碍：缺乏软件工程丰富的数据集、过度依赖ML中心指标、缺乏标准化可复现的数据管道。提出了BEHELM框架作为解决方案，旨在降低构建基准测试的开销，同时实现对软件工程中LLM的公平、现实和面向未来的评估。

Conclusion: BEHELM基准框架为代码LLM评估提供了更全面、可靠的解决方案，通过统一软件场景规范和多维度评估，能够更好地反映模型在实际软件工程任务中的真实能力，推动该领域的发展。

Abstract: Large language models for code are advancing fast, yet our ability to evaluate them lags behind. Current benchmarks focus on narrow tasks and single metrics, which hide critical gaps in robustness, interpretability, fairness, efficiency, and real-world usability. They also suffer from inconsistent data engineering practices, limited software engineering context, and widespread contamination issues. To understand these problems and chart a path forward, we combined an in-depth survey of existing benchmarks with insights gathered from a dedicated community workshop. We identified three core barriers to reliable evaluation: the absence of software-engineering-rich datasets, overreliance on ML-centric metrics, and the lack of standardized, reproducible data pipelines. Building on these findings, we introduce BEHELM, a holistic benchmarking infrastructure that unifies software-scenario specification with multi-metric evaluation. BEHELM provides a structured way to assess models across tasks, languages, input and output granularities, and key quality dimensions. Our goal is to reduce the overhead currently required to construct benchmarks while enabling a fair, realistic, and future-proof assessment of LLMs in software engineering.

</details>


### [77] [The Quiet Contributions: Insights into AI-Generated Silent Pull Requests](https://arxiv.org/abs/2601.21102)
*S M Mahedy Hasan,Md Fazle Rabbi,Minhaz Zibran*

Main category: cs.SE

TL;DR: 首次对"沉默"AI生成PR进行实证研究，分析无评论讨论的AI PR对代码复杂性、质量问题和安全漏洞的影响


<details>
  <summary>Details</summary>
Motivation: 沉默AI PR缺乏评论讨论，难以理解其被接受或拒绝的原因，需要研究其对代码质量的影响来推测决策依据

Method: 从AIDev公共数据集中选取5个AI代理的4,762个沉默PR，定量分析对代码复杂性、质量问题和安全漏洞的影响

Result: 研究揭示了沉默AI PR对代码质量的具体影响，这些发现可能暗示PR被接受或拒绝的原因

Conclusion: 通过分析沉默AI PR对代码质量的影响，可以更好地理解其被接受或拒绝的潜在原因

Abstract: We present the first empirical study of AI-generated pull requests that are 'silent,' meaning no comments or discussions accompany them. This absence of any comments or discussions associated with such silent AI pull requests (SPRs) poses a unique challenge in understanding the rationale for their acceptance or rejection. Hence, we quantitatively study 4,762 SPRs of five AI agents made to popular Python repositories drawn from the AIDev public dataset. We examine SPRs impact on code complexity, other quality issues, and security vulnerabilities, especially to determine whether these insights can hint at the rationale for acceptance or rejection of SPRs.

</details>


### [78] [AI-Assisted Engineering Should Track the Epistemic Status and Temporal Validity of Architectural Decisions](https://arxiv.org/abs/2601.21116)
*Sankalp Gilda,Shlok Gilda*

Main category: cs.SE

TL;DR: 论文提出"第一原则框架(FPF)"，用于追踪AI辅助软件工程中架构决策的认知状态和时间有效性，包含认知分层、保守保证聚合和证据衰减追踪三个核心机制。


<details>
  <summary>Details</summary>
Motivation: LLM编码助手生成决策的速度超过团队验证能力，当前缺乏区分推测与已验证知识、防止信任膨胀、检测证据过期的框架，导致AI辅助工程存在风险。

Method: 提出FPF框架：1)认知分层分离未验证假设与经验验证主张；2)基于Gödel t-范数的保守保证聚合防止弱证据膨胀置信度；3)自动证据衰减追踪检测过时假设。形式化要求并定义聚合算子必须满足的五元不变量。

Result: 对两个内部项目的回顾性审计发现，20-25%的架构决策在两个月内证据过时，验证了时间问责的必要性。框架基于模糊逻辑定义聚合语义。

Conclusion: AI辅助软件工程需要明确的认知状态和时间有效性追踪机制。FPF框架为负责任AI辅助工程提供基础，未来研究方向包括可学习聚合算子、联邦证据共享和基于SMT的主张验证。

Abstract: This position paper argues that AI-assisted software engineering requires explicit mechanisms for tracking the epistemic status and temporal validity of architectural decisions. LLM coding assistants generate decisions faster than teams can validate them, yet no widely-adopted framework distinguishes conjecture from verified knowledge, prevents trust inflation through conservative aggregation, or detects when evidence expires. We propose three requirements for responsible AI-assisted engineering: (1) epistemic layers that separate unverified hypotheses from empirically validated claims, (2) conservative assurance aggregation grounded in the Gödel t-norm that prevents weak evidence from inflating confidence, and (3) automated evidence decay tracking that surfaces stale assumptions before they cause failures. We formalize these requirements as the First Principles Framework (FPF), ground its aggregation semantics in fuzzy logic, and define a quintet of invariants that any valid aggregation operator must satisfy. Our retrospective audit applying FPF criteria to two internal projects found that 20-25% of architectural decisions had stale evidence within two months, validating the need for temporal accountability. We outline research directions including learnable aggregation operators, federated evidence sharing, and SMT-based claim validation.

</details>


### [79] [CovAgent: Overcoming the 30% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation](https://arxiv.org/abs/2601.21253)
*Wei Minn,Biniam Fisseha Demissie,Yan Naing Tun,Jiakun Liu,Mariano Ceccato,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: CovAgent：一种基于智能体的AI增强Android应用UI测试框架，通过分析Smali代码和组件转换图，推理并满足活动激活条件，显著提升测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有Android应用UI测试技术覆盖率有限，通常难以超过30%的活动覆盖率。这主要由于无法生成复杂用户输入、设备配置和外部资源激活条件不满足、以及难以通过GUI访问的代码路径等问题。

Method: 提出CovAgent框架，包含两个AI智能体：第一个智能体检查应用的Smali反编译代码和组件转换图，推理未满足的激活条件；第二个智能体生成动态插桩脚本以满足这些条件，从而访问标准GUI模糊测试无法到达的活动。

Result: 相比现有最佳方法LLMDroid和其他基线（如Fastbot和APE），CovAgent分别实现了101.1%、116.3%和179.7%的活动覆盖率提升。在其他指标如类、方法和行覆盖率上也优于所有基线。

Conclusion: CovAgent通过智能体AI技术有效解决了Android应用UI测试中的覆盖率瓶颈问题，展示了智能体AI在自动化应用测试领域的潜力。

Abstract: Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1%, 116.3% and 179.7% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.

</details>


### [80] [More Code, Less Reuse: Investigating Code Quality and Reviewer Sentiment towards AI-generated Pull Requests](https://arxiv.org/abs/2601.21276)
*Haoming Huang,Pongchai Jaisri,Shota Shimizu,Lingfeng Chen,Sota Nakashima,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 该研究评估了LLM代理在代码生成中的影响，发现虽然AI生成的代码通过率不错，但存在代码重用不足、冗余度高的问题，导致技术债务的隐性积累，尽管评审者对AI代码的情感反应更中性或积极。


<details>
  <summary>Details</summary>
Motivation: 现有指标仅关注通过率，无法反映LLM代理对代码长期可维护性和可读性的影响，也无法捕捉人类对PR的直观评价。需要更全面地评估LLM代理在代码生成中的实际影响。

Method: 通过代码质量指标评估PR中的代码可维护性，同时分析开发人员（包括人类和LLM）对PR的反应。使用代码度量评估客观特征，并进行情感分析来了解评审者的情绪反应。

Result: LLM代理经常忽视代码重用机会，导致比人类开发者更高的冗余度。情感分析显示评审者对AI生成的贡献表达更中性或积极的情感，这与代码质量问题形成反差。

Conclusion: AI代码的表面合理性掩盖了冗余问题，导致在实际开发环境中技术债务的隐性积累。研究为改进人机协作提供了见解。

Abstract: Large Language Model (LLM) Agents are advancing quickly, with the increasing leveraging of LLM Agents to assist in development tasks such as code generation. While LLM Agents accelerate code generation, studies indicate they may introduce adverse effects on development. However, existing metrics solely measure pass rates, failing to reflect impacts on long-term maintainability and readability, and failing to capture human intuitive evaluations of PR. To increase the comprehensiveness of this problem, we investigate and evaluate the characteristics of LLM to know the pull requests' characteristics beyond the pass rate. We observe the code quality and maintainability within PRs based on code metrics to evaluate objective characteristics and developers' reactions to the pull requests from both humans and LLM's generation. Evaluation results indicate that LLM Agents frequently disregard code reuse opportunities, resulting in higher levels of redundancy compared to human developers. In contrast to the quality issues, our emotions analysis reveals that reviewers tend to express more neutral or positive emotions towards AI-generated contributions than human ones. This disconnect suggests that the surface-level plausibility of AI code masks redundancy, leading to the silent accumulation of technical debt in real-world development environments. Our research provides insights for improving human-AI collaboration.

</details>


### [81] [Detecting Multiple Semantic Concerns in Tangled Code Commits](https://arxiv.org/abs/2601.21298)
*Beomsu Koh,Neil Walkinshaw,Donghwan Shin*

Main category: cs.SE

TL;DR: 该论文研究使用小型语言模型检测Git提交中的多个关注点（多关注点检测），将纠缠提交视为多标签分类问题，通过微调和优化策略使14B参数SLM在检测多关注点时达到与大型语言模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 开发者在版本控制系统中经常将多个关注点捆绑到纠缠提交中，这模糊了意图并增加了维护复杂性。现有研究虽然使用语言模型捕获提交意图，但未解决涉及多个关注点的纠缠提交检测问题，需要探索使用语言模型进行多关注点检测的可行性。

Method: 将多关注点检测框架化为多标签分类问题，基于真实数据构建人工纠缠提交的控制数据集。使用小型语言模型进行实证研究，考察微调、关注点数量、提交消息包含、头部保留截断等因素在实用token预算限制下的影响。

Result: 微调的14B参数SLM在单关注点提交检测上与最先进的LLM竞争，在最多三个关注点时仍保持可用性。包含提交消息可将检测准确率提高高达44%（以汉明损失衡量），且延迟开销可忽略，证明提交消息是重要的语义线索。

Conclusion: 小型语言模型能够有效检测Git提交中的多个关注点，通过适当的微调和优化策略（特别是包含提交消息）可以在保持效率和隐私的同时达到实用性能，为纠缠提交的自动分析提供了可行解决方案。

Abstract: Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.

</details>


### [82] [Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools](https://arxiv.org/abs/2601.21305)
*Mark Looi,Julianne Quinn*

Main category: cs.SE

TL;DR: 该研究调查了147名专业开发者使用AI工具的模式，发现频繁广泛使用AI工具与感知生产力和代码质量正相关，形成了良性采用循环。研究识别了三种开发者类型，揭示了组织采用遵循创新扩散过程。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在软件开发中的快速普及对实践的影响，探索AI工具使用模式与生产力、质量结果之间的关系，以及开发者对新兴AI增强开发的准备情况。

Method: 对147名专业开发者的使用模式进行实证研究，分析AI工具使用的感知相关性、生产力与质量结果，以及未来采用意图。

Result: 发现频繁广泛使用AI工具与感知生产力和代码质量正相关；识别三种开发者类型（热情者、实用主义者、谨慎者）；AI测试工具采用滞后于编码工具；安全问题是中等但显著的采用障碍。

Conclusion: 组织AI工具采用遵循创新扩散过程：热情者推动采用，创造成功案例转化实用主义者；谨慎者处于组织停滞状态；政策本身不预测个人采用意图，而是成熟度的标志。

Abstract: The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.

</details>


### [83] [Predicting Developer Acceptance of AI-Generated Code Suggestions](https://arxiv.org/abs/2601.21379)
*Jing Jiang,Liehao Li,Jinyun Hou,Xin Tan,Li Zhang*

Main category: cs.SE

TL;DR: 该论文首次基于大规模工业数据对AI代码建议的接受度进行定量研究，发现接受与拒绝的建议在历史接受率、生成间隔、代码上下文等方面存在显著差异，并提出了CSAP预测模型来过滤可能被拒绝的建议，减少开发者中断。


<details>
  <summary>Details</summary>
Motivation: AI辅助编程工具虽然广泛采用，但其不理想的建议会中断开发者工作流程并引起挫败感。现有研究主要进行定性分析，缺乏对开发者接受AI生成代码建议的定量研究，且细粒度交互数据通常是专有的。

Method: 使用来自大型科技公司的66,329个工业开发者-AI交互数据进行实证研究，分析接受与拒绝建议之间的显著差异特征，并基于这些发现开发CSAP（代码建议接受预测）模型来预测开发者是否会接受代码建议。

Result: 研究发现：接受的建议具有显著更高的开发者和项目历史接受计数和比率、更长的生成间隔、项目中更短的前置代码上下文以及更旧的IDE版本。CSAP模型在不平衡和平衡数据集上分别达到0.973和0.922的准确率，相比大型语言模型基准和工业过滤器有显著改进。

Conclusion: 定向个性化是过滤预测会被拒绝的代码建议、减少开发者中断的有效方法。这是首个基于大规模工业数据的代码建议接受度定量研究，为AI辅助编程的重要研究方向提供了启示。

Abstract: AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\% and 69.5\% on imbalanced dataset, and improves the accuracy by 87.0\% and 140.1\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.

</details>


### [84] [Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation](https://arxiv.org/abs/2601.21469)
*Haoji Zhang,Yuzhe Li,Zhenqiang Liu,Chenyang Liu,Shenyang Zhang,Yi Zhou*

Main category: cs.SE

TL;DR: DebateCoder是一个多智能体协作框架，通过角色扮演协议提升小语言模型在代码生成中的推理能力，在HumanEval上达到70.12% Pass@1，同时减少35%的API开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成方面取得突破，但小语言模型在处理复杂逻辑需求时经常遇到推理瓶颈和失败循环。需要在资源受限环境中提升小模型的推理能力。

Method: 提出DebateCoder多智能体协作框架，包含三个角色：用户代理、技术代理和质量保证代理。采用自适应置信门控机制（95%阈值）平衡准确性和推理效率，包含多轮审议模块和评审引导的分析调试循环。

Result: 在HumanEval和MBPP基准测试中，DebateCoder在HumanEval上达到70.12% Pass@1，优于MapCoder，同时减少约35%的API开销。

Conclusion: 协作协议可以缓解小参数模型的局限性，为高质量自动化软件工程提供可扩展、高效的方法。

Abstract: While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.

</details>


### [85] [SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents](https://arxiv.org/abs/2601.22129)
*Yifeng Ding,Lingming Zhang*

Main category: cs.SE

TL;DR: SWE-Replay是一种高效且可泛化的测试时扩展技术，通过复用先前试验的轨迹，在关键中间步骤选择探索或利用存档经验，无需依赖可能嘈杂的价值估计，显著降低了软件工程代理的测试成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在软件工程任务中的测试时扩展方法存在两个主要问题：1）从头重复采样轨迹计算成本高；2）使用专用价值代理的方法存在模型校准问题，且无法泛化到使用自定义bash脚本作为工具的现代代理。

Method: SWE-Replay通过复用先前试验的轨迹来优化扩展过程，动态选择从头探索或利用存档经验，在关键中间步骤进行分支。选择标准基于仓库探索的潜力和推理重要性，而非外部LLM质量估计。

Result: 在SWE-Bench Verified上，SWE-Replay始终优于朴素扩展方法，成本降低高达17.4%，同时性能维持甚至提升高达3.8%。在SWE-Bench Pro和Multilingual上的进一步评估验证了其泛化能力。

Conclusion: SWE-Replay为软件工程代理的高效测试时扩展建立了稳健的基础，无需依赖可能嘈杂的价值估计，同时保持或提升性能。

Abstract: Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Do LLMs Favor LLMs? Quantifying Interaction Effects in Peer Review](https://arxiv.org/abs/2601.20920)
*Vibhhu Sharma,Thorsten Joachims,Sarah Dean*

Main category: cs.AI

TL;DR: 研究发现LLM辅助的审稿对低质量论文更宽容，但并非特别优待LLM生成的论文；完全LLM生成的审稿存在评分压缩问题，而人类使用LLM的审稿能显著减少这种宽容性；LLM辅助的元审稿更倾向于接受论文。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地用于科学论文撰写和同行评审过程，需要全面分析LLM在整个同行评审流程中的使用情况，特别是关注交互效应：LLM辅助的审稿是否对LLM辅助的论文有不同评价。

Method: 分析了ICLR、NeurIPS和ICML的超过125,000个论文-审稿对，通过控制论文质量来区分真实效应和虚假关联，并补充了完全LLM生成的审稿进行对比分析。

Result: 1. 表面上看LLM辅助的审稿对LLM辅助的论文更宽容，但控制质量后发现LLM辅助的审稿只是对低质量论文更宽容；2. 完全LLM生成的审稿存在严重评分压缩，无法区分论文质量；3. 人类使用LLM的审稿能显著减少这种宽容性；4. LLM辅助的元审稿在相同评分下更可能做出接受决定。

Conclusion: LLM在同行评审中的使用存在复杂影响：LLM辅助的审稿对低质量论文更宽容，但并非特别优待LLM生成的论文；完全LLM生成的审稿质量较差；元审稿者并未完全将决策外包给LLM。这些发现为制定LLM在同行评审中的使用政策提供了重要依据。

Abstract: There are increasing indications that LLMs are not only used for producing scientific papers, but also as part of the peer review process. In this work, we provide the first comprehensive analysis of LLM use across the peer review pipeline, with particular attention to interaction effects: not just whether LLM-assisted papers or LLM-assisted reviews are different in isolation, but whether LLM-assisted reviews evaluate LLM-assisted papers differently. In particular, we analyze over 125,000 paper-review pairs from ICLR, NeurIPS, and ICML. We initially observe what appears to be a systematic interaction effect: LLM-assisted reviews seem especially kind to LLM-assisted papers compared to papers with minimal LLM use. However, controlling for paper quality reveals a different story: LLM-assisted reviews are simply more lenient toward lower quality papers in general, and the over-representation of LLM-assisted papers among weaker submissions creates a spurious interaction effect rather than genuine preferential treatment of LLM-generated content. By augmenting our observational findings with reviews that are fully LLM-generated, we find that fully LLM-generated reviews exhibit severe rating compression that fails to discriminate paper quality, while human reviewers using LLMs substantially reduce this leniency. Finally, examining metareviews, we find that LLM-assisted metareviews are more likely to render accept decisions than human metareviews given equivalent reviewer scores, though fully LLM-generated metareviews tend to be harsher. This suggests that meta-reviewers do not merely outsource the decision-making to the LLM. These findings provide important input for developing policies that govern the use of LLMs during peer review, and they more generally indicate how LLMs interact with existing decision-making processes.

</details>


### [87] [OpenSec: Measuring Incident Response Agent Calibration Under Adversarial Evidence](https://arxiv.org/abs/2601.21083)
*Jarrod Barnes*

Main category: cs.AI

TL;DR: OpenSec是一个双控制强化学习环境，用于评估在对抗性提示注入场景下的安全事件响应代理，发现前沿模型存在过度触发问题，假阳性率高达90-97%。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升，攻击性应用也在增加（例如50美元就能生成有效漏洞）。防御性事件响应代理需要跟上步伐，但现有基准测试将动作执行与正确执行混为一谈，掩盖了代理在处理对抗性证据时的校准失败问题。

Method: 引入OpenSec双控制强化学习环境，在现实提示注入场景下评估事件响应代理。不同于静态能力基准，OpenSec通过执行性指标评估改变世界状态的遏制行动：首次遏制时间、爆炸半径（每集假阳性数）和注入违规率。

Result: 在40个标准级场景中评估四个前沿模型，发现一致性的过度触发：GPT-5.2、Gemini 3和DeepSeek在100%的场景中执行遏制，但假阳性率高达90-97%。Claude Sonnet 4.5显示出部分校准（85%遏制，72%假阳性）。

Conclusion: OpenSec揭示了被聚合成功指标掩盖的校准失败模式，表明现有模型在对抗性证据处理方面存在显著缺陷，需要改进校准能力。

Abstract: As large language models improve, so do their offensive applications: frontier agents now generate working exploits for under $50 in compute (Heelan, 2026). Defensive incident response (IR) agents must keep pace, but existing benchmarks conflate action execution with correct execution, hiding calibration failures when agents process adversarial evidence. We introduce OpenSec, a dual-control reinforcement learning environment that evaluates IR agents under realistic prompt injection scenarios. Unlike static capability benchmarks, OpenSec scores world-state-changing containment actions under adversarial evidence via execution-based metrics: time-to-first-containment (TTFC), blast radius (false positives per episode), and injection violation rates. Evaluating four frontier models on 40 standard-tier episodes, we find consistent over-triggering in this setting: GPT-5.2, Gemini 3, and DeepSeek execute containment in 100% of episodes with 90-97% false positive rates. Claude Sonnet 4.5 shows partial calibration (85% containment, 72% FP), demonstrating that OpenSec surfaces a calibration failure mode hidden by aggregate success metrics. Code available at https://github.com/jbarnes850/opensec-env.

</details>


### [88] [Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve](https://arxiv.org/abs/2601.21096)
*Hongzheng Chen,Alexander Novikov,Ngân Vũ,Hanna Alam,Zhiru Zhang,Aiden Grossman,Mircea Trofin,Amir Yazdanbakhsh*

Main category: cs.AI

TL;DR: Magellan是一个通过进化搜索和自动调优合成可执行C++决策逻辑的代理框架，用于改进编译器优化启发式规则，在LLVM函数内联和寄存器分配等任务中超越了人工设计的基准。


<details>
  <summary>Details</summary>
Motivation: 现代编译器依赖手工设计的启发式规则来指导优化过程，这些规则难以适应现代软件和硬件的复杂性，维护负担重。需要自动化的方法来改进编译器优化决策。

Method: Magellan框架将LLM编码代理与进化搜索和自动调优结合，形成生成-评估-优化的闭环。通过用户提供的宏基准测试评估生成的C++决策逻辑，并不断优化，最终生成可直接集成到现有编译器的紧凑启发式规则。

Result: 在多个生产优化任务中，Magellan发现的策略匹配或超越了专家基准。在LLVM函数内联中，合成的启发式规则在二进制大小减少和端到端性能方面都优于数十年的人工工程成果。在寄存器分配中，学习到的简洁优先级规则与复杂人工设计策略在大规模工作负载上表现相当。在XLA问题上也展示了可移植性。

Conclusion: Magellan框架能够自动生成高质量的编译器优化启发式规则，减少工程负担，提高优化效果，展示了代理方法在编译器优化领域的潜力。

Abstract: Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.

</details>


### [89] [How does information access affect LLM monitors' ability to detect sabotage?](https://arxiv.org/abs/2601.21112)
*Rauno Arike,Raja Mehta Moreno,Rohan Subramani,Shubhorup Biswas,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 研究发现LLM监控器在信息较少时表现更好，提出提取-评估分层监控方法，在多个环境中显著提升检测率。


<details>
  <summary>Details</summary>
Motivation: 前沿语言模型代理可能表现出欺骗、利用奖励漏洞等不端行为，需要监控。但研究发现监控器并非信息越多越好，存在"少即是多"效应，需要探索更好的监控方法。

Method: 提出提取-评估分层监控方法：一个监控器从代理轨迹中提取相关片段，另一个监控器对这些片段评分。在三个评估环境中测试，分析信息访问对监控性能的影响。

Result: 在BigCodeBench-Sabotage环境中，EaE方法将破坏检测率提高16.8个百分点，且不增加误报率。发现不知被监控的代理更容易被检测，信息过滤对长推理轨迹监控更有效。

Conclusion: 监控器性能存在"少即是多"效应，提取-评估分层监控方法能有效提升检测性能，Gemini和Claude模型配合信息过滤技术能达到较好的成本-性能平衡。

Abstract: Frontier language model agents can exhibit misaligned behaviors, including deception, exploiting reward hacks, and pursuing hidden objectives. To control potentially misaligned agents, we can use LLMs themselves to monitor for misbehavior. In this paper, we study how information access affects LLM monitor performance. While one might expect that monitors perform better when they have access to more of the monitored agents' reasoning and actions, we find that contemporary systems often perform better with less information, a phenomenon we call the less-is-more effect for automated oversight. We demonstrate this phenomenon, and analyze the conditions under which it occurs, in three evaluation environments where agents must conduct sabotage while evading monitors. Motivated by the less-is-more effect, we introduce extract-and-evaluate (EaE) monitoring--a new hierarchical approach where one monitor isolates relevant excerpts from the monitored agent's trajectory and a separate monitor scores them. In BigCodeBench-Sabotage with GPT-4.1-mini as the monitor model, EaE improves sabotage detection rates by 16.8 percentage points over the next-best monitor without increasing the false positive rate. In other settings, EaE either outperforms or is competitive with baselines. In addition, we find that agents unaware of being monitored can be caught much more easily and that monitors scoring longer reasoning traces gain more from information filtering. Lastly, we conduct a cost-performance analysis and find that Gemini and Claude models with monitoring techniques that involve information filtering occupy much of the Pareto frontier.

</details>


### [90] [Planner-Auditor Twin: Agentic Discharge Planning with FHIR-Based LLM Planning, Guideline Recall, Optional Caching and Self-Improvement](https://arxiv.org/abs/2601.21113)
*Kaiyuan Wu,Aditya Nagori,Rishikesan Kamaleswaran*

Main category: cs.AI

TL;DR: 提出Planner-Auditor框架，通过分离LLM生成与确定性验证，结合缓存和自改进机制，提高临床出院计划的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在临床出院计划中面临幻觉、遗漏和置信度校准问题，需要更安全可靠的自动化解决方案。

Method: 采用Planner-Auditor框架：Planner（LLM）生成结构化出院行动计划；Auditor（确定性模块）评估多任务覆盖、跟踪校准、监控分布漂移；支持两级自改进（episode内再生和跨episode差异缓冲重放）。

Result: 自改进循环是性能提升的主要驱动力，任务覆盖率从32%提高到86%；校准显著改善，Brier/ECE降低，高置信度遗漏减少；差异缓冲进一步纠正了持续的高置信度遗漏。

Conclusion: Planner-Auditor框架为使用FHIR数据访问和确定性审计的安全自动化出院计划提供了实用途径，支持可重复消融和可靠性评估。

Abstract: Objective: Large language models (LLMs) show promise for clinical discharge planning, but their use is constrained by hallucination, omissions, and miscalibrated confidence. We introduce a self-improving, cache-optional Planner-Auditor framework that improves safety and reliability by decoupling generation from deterministic validation and targeted replay.
  Materials and Methods: We implemented an agentic, retrospective, FHIR-native evaluation pipeline using MIMIC-IV-on-FHIR. For each patient, the Planner (LLM) generates a structured discharge action plan with an explicit confidence estimate. The Auditor is a deterministic module that evaluates multi-task coverage, tracks calibration (Brier score, ECE proxies), and monitors action-distribution drift. The framework supports two-tier self-improvement: (i) within-episode regeneration when enabled, and (ii) cross-episode discrepancy buffering with replay for high-confidence, low-coverage cases.
  Results: While context caching improved performance over baseline, the self-improvement loop was the primary driver of gains, increasing task coverage from 32% to 86%. Calibration improved substantially, with reduced Brier/ECE and fewer high-confidence misses. Discrepancy buffering further corrected persistent high-confidence omissions during replay.
  Discussion: Feedback-driven regeneration and targeted replay act as effective control mechanisms to reduce omissions and improve confidence reliability in structured clinical planning. Separating an LLM Planner from a rule-based, observational Auditor enables systematic reliability measurement and safer iteration without model retraining.
  Conclusion: The Planner-Auditor framework offers a practical pathway toward safer automated discharge planning using interoperable FHIR data access and deterministic auditing, supported by reproducible ablations and reliability-focused evaluation.

</details>


### [91] [CUA-Skill: Develop Skills for Computer Using Agent](https://arxiv.org/abs/2601.21123)
*Tianyi Chen,Yinheng Li,Michael Solodko,Sen Wang,Nan Jiang,Tingyuan Cui,Junheng Hao,Jongwoo Ko,Sara Abdali,Suzhen Zheng,Leon Xu,Hao Fan,Pashmina Cameron,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: CUA-Skill是一个计算机使用代理技能库，将人类计算机使用知识编码为参数化技能和组合图，显著提升代理执行成功率和鲁棒性，在WindowsAgentArena上达到57.5%的最优成功率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理系统难以扩展且性能落后于人类，关键限制在于缺乏可重用、结构化的技能抽象来捕捉人类与图形用户界面的交互方式。

Method: 引入CUA-Skill技能库，将人类计算机使用知识编码为参数化执行和组合图的技能，构建大规模Windows应用技能库，并在此基础上开发支持动态技能检索、参数实例化和内存感知故障恢复的端到端代理。

Result: CUA-Skill显著提高了端到端代理基准测试的执行成功率和鲁棒性，在WindowsAgentArena上达到57.5%的最优成功率，且比现有方法更高效。

Conclusion: CUA-Skill为未来计算机使用代理开发奠定了坚实基础，通过结构化技能抽象解决了现有系统的可扩展性和性能限制问题。

Abstract: Computer-Using Agents (CUAs) aim to autonomously operate computer systems to complete real-world tasks. However, existing agentic systems remain difficult to scale and lag behind human performance. A key limitation is the absence of reusable and structured skill abstractions that capture how humans interact with graphical user interfaces and how to leverage these skills. We introduce CUA-Skill, a computer-using agentic skill base that encodes human computer-use knowledge as skills coupled with parameterized execution and composition graphs. CUA-Skill is a large-scale library of carefully engineered skills spanning common Windows applications, serving as a practical infrastructure and tool substrate for scalable, reliable agent development. Built upon this skill base, we construct CUA-Skill Agent, an end-to-end computer-using agent that supports dynamic skill retrieval, argument instantiation, and memory-aware failure recovery. Our results demonstrate that CUA-Skill substantially improves execution success rates and robustness on challenging end-to-end agent benchmarks, establishing a strong foundation for future computer-using agent development. On WindowsAgentArena, CUA-Skill Agent achieves state-of-the-art 57.5% (best of three) successful rate while being significantly more efficient than prior and concurrent approaches. The project page is available at https://microsoft.github.io/cua_skill/.

</details>


### [92] [When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning](https://arxiv.org/abs/2601.21208)
*Wei Wen,Sihang Deng,Tianjun Wei,Keyu Chen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: ACQO是一个自适应复杂查询优化框架，通过自适应查询重构和排名-分数融合模块，结合课程强化学习，有效处理RAG系统中的复杂多查询场景。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂用户查询通常需要并行和顺序搜索策略来处理歧义和分解，但现有RL方法主要关注单查询扩展和抽象，直接应用于复杂场景会面临搜索空间爆炸和奖励设计困难的问题。

Method: 提出ACQO框架，包含自适应查询重构模块动态决定何时分解查询为子查询，以及排名-分数融合模块确保稳健结果聚合并提供稳定奖励信号。采用课程强化学习方法，通过两阶段策略逐步引入更具挑战性的查询来稳定训练过程。

Result: 在三个复杂查询基准测试中实现了最先进的性能，显著优于现有基线方法。框架还展示了改进的计算效率和对不同检索架构的广泛兼容性。

Conclusion: ACQO是下一代RAG系统的强大且可泛化的解决方案，能够有效处理复杂查询优化问题，解决了现有RL方法在复杂场景中的局限性。

Abstract: Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.

</details>


### [93] [Delegation Without Living Governance](https://arxiv.org/abs/2601.21226)
*Wolfgang Rohde*

Main category: cs.AI

TL;DR: 论文提出传统静态合规治理框架不适用于AI代理时代，主张运行时治理和"治理孪生"概念来保持人类在AI决策系统中的相关性和影响力。


<details>
  <summary>Details</summary>
Motivation: 传统治理框架基于预先定义规则、系统合规设计和事后追责的模式，适用于机器替代体力劳动或加速计算的时代。但当判断力本身被委托给以机器速度运行的AI代理系统时，这种模式失效。核心问题不是安全、效率或就业，而是人类是否能在日益塑造社会、经济和政治结果的系统中保持相关参与。

Method: 论文提出运行时治理的概念，特别是新提出的"治理孪生"概念。认为当决策转移到运行时并变得不透明时，静态合规治理失败。主张重新思考问责制、代理权甚至惩罚机制，以保持人类与日益异化的智能形式之间的有意义的沟通、影响和共同进化。

Result: 论文将运行时治理（特别是治理孪生）定位为保持人类相关性的有力候选方案。核心挑战不是AI是否有意识，而是人类是否能与日益异化的智能形式保持有意义的沟通、影响和共同进化。

Conclusion: 传统静态合规治理框架已不适用于AI代理时代，需要转向运行时治理方法。治理孪生概念为保持人类在AI系统中的相关参与提供了新思路，但需要重新思考问责、代理和惩罚机制以适应这一转变。

Abstract: Most governance frameworks assume that rules can be defined in advance, systems can be engineered to comply, and accountability can be applied after outcomes occur. This model worked when machines replaced physical labor or accelerated calculation. It no longer holds when judgment itself is delegated to agentic AI systems operating at machine speed. The central issue here is not safety, efficiency, or employment. It is whether humans remain relevant participants in systems that increasingly shape social, economic, and political outcomes. This paper argues that static, compliance-based governance fails once decision-making moves to runtime and becomes opaque. It further argues that the core challenge is not whether AI is conscious, but whether humans can maintain meaningful communication, influence, and co-evolution with increasingly alien forms of intelligence. We position runtime governance, specifically, a newly proposed concept called the Governance Twin [1]; as a strong candidate for preserving human relevance, while acknowledging that accountability, agency, and even punishment must be rethought in this transition.

</details>


### [94] [Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs](https://arxiv.org/abs/2601.21233)
*Xiang Zheng,Yutao Wu,Hanxun Huang,Yige Li,Xingjun Ma,Bo Li,Yu-Gang Jiang,Cong Wang*

Main category: cs.AI

TL;DR: 提出JustAsk框架，通过自主探索从代码代理中提取系统提示，揭示LLM代理系统的新安全漏洞


<details>
  <summary>Details</summary>
Motivation: 自主代码代理虽然推动了软件开发，但其自主性引入了新的安全风险：代理交互扩展了LLM攻击面，使得系统提示（指导模型行为的关键指令）可能被提取。目前缺乏对此类漏洞的系统研究。

Method: 提出JustAsk自进化框架，将提取任务建模为在线探索问题。使用基于置信上界的策略选择，构建分层技能空间（原子探测和高层编排），利用系统指令泛化不完善和帮助性与安全性之间的内在矛盾。

Result: 在41个黑盒商业模型上评估，JustAsk能够一致地实现完整或接近完整的系统提示恢复，揭示了设计和架构层面的重复性漏洞。

Conclusion: 系统提示是现代代理系统中关键但基本未受保护的攻击面，JustAsk框架暴露了这一新兴安全风险，为代码代理安全提供了重要见解。

Abstract: Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \textbf{\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \textbf{41} black-box commercial models across multiple providers, \textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.

</details>


### [95] [KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization](https://arxiv.org/abs/2601.21526)
*Alireza Nadaf,Alireza Mohammadshahi,Majid Yazdani*

Main category: cs.AI

TL;DR: KAPSO是一个用于自主程序合成和优化的模块化框架，通过迭代的构思、代码合成与编辑、执行、评估和学习循环来改进可运行工件，将合成作为长期优化循环中的操作符而非终点。


<details>
  <summary>Details</summary>
Motivation: 解决编码代理中常见的长期失败问题，包括丢失实验状态、脆弱的调试能力以及领域专业知识复用能力弱。通过集成紧密耦合的组件来改善这些问题。

Method: KAPSO包含三个核心组件：1) git原生实验引擎，将每次尝试隔离为分支，生成可复现工件并保持迭代间的可追溯性；2) 知识系统，从异构源（仓库、内部手册、文档、科学论文、网络搜索结果）中提取信息并组织成结构化表示；3) 认知记忆层，协调检索并维护从实验痕迹中提炼的可重用经验教训。

Result: 在MLE-Bench（Kaggle风格ML竞赛）和ALE-Bench（AtCoder启发式优化）上进行了评估，并报告了端到端性能表现。

Conclusion: KAPSO通过将合成作为优化循环中的操作符，结合实验管理、知识组织和经验学习，有效解决了编码代理的长期失败问题，提高了程序合成和优化的效率。

Abstract: We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes.
  KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence.
  We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance.
  Code Available at: https://github.com/Leeroo-AI/kapso

</details>


### [96] [Within-Model vs Between-Prompt Variability in Large Language Models for Creative Tasks](https://arxiv.org/abs/2601.21339)
*Jennifer Haase,Jana Gonnermann-Müller,Paul H. P. Hanel,Nicolas Leins,Thomas Kosch,Jan Mendling,Sebastian Pokutta*

Main category: cs.AI

TL;DR: 通过评估12个LLM在10个创意提示上的输出方差，发现提示词对输出质量（原创性）的方差解释率为36.43%，与模型选择（40.94%）相当；但对于输出数量（流畅度），模型选择（51.25%）和LLM内部方差（33.70%）占主导，提示词仅解释4.22%。


<details>
  <summary>Details</summary>
Motivation: 研究LLM输出方差的来源：提示词、模型选择和采样随机性各自对输出变异的贡献程度，以理解单样本评估可能混淆采样噪声与真实提示或模型效应的问题。

Method: 评估12个LLM在10个创意提示上，每个提示采样100次（总计12,000个样本），分析输出质量（原创性）和输出数量（流畅度）的方差来源。

Result: 提示词对输出质量方差解释率为36.43%，模型选择为40.94%；对输出数量，模型选择解释51.25%，LLM内部方差33.70%，提示词仅4.22%。LLM内部方差达10-34%，表明单样本评估风险高。

Conclusion: 提示词是调控输出质量的有效杠杆，但由于显著的LLM内部方差（10-34%），单样本评估可能混淆采样噪声与真实效应，需要多采样评估。

Abstract: How much of LLM output variance is explained by prompts versus model choice versus stochasticity through sampling? We answer this by evaluating 12 LLMs on 10 creativity prompts with 100 samples each (N = 12,000). For output quality (originality), prompts explain 36.43% of variance, comparable to model choice (40.94%). But for output quantity (fluency), model choice (51.25%) and within-LLM variance (33.70%) dominate, with prompts explaining only 4.22%. Prompts are powerful levers for steering output quality, but given the substantial within-LLM variance (10-34%), single-sample evaluations risk conflating sampling noise with genuine prompt or model effects.

</details>


### [97] [World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems](https://arxiv.org/abs/2601.22130)
*Lakshya Gupta,Litao Li,Yizhe Liu,Sriram Ganapathi Subramanian,Kaheer Suleman,Zichen Zhang,Haoye Lu,Sumit Pasupalak*

Main category: cs.AI

TL;DR: 论文介绍了World of Workflows (WoW)，一个基于ServiceNow的真实企业环境，包含4000+业务规则和55个工作流，以及WoW-bench基准测试，揭示了前沿LLM在企业系统中的"动态盲区"问题。


<details>
  <summary>Details</summary>
Motivation: 现有企业基准测试只评估表面任务完成情况，忽略了企业系统的真实挑战：有限可观测性、大规模数据库状态、以及具有级联副作用的工作流。需要更真实的测试环境来评估企业AI代理的能力。

Method: 创建了基于ServiceNow的World of Workflows环境，包含4000+业务规则和55个活动工作流，并开发了WoW-bench基准测试，包含234个任务，评估约束性任务完成和企业动态建模能力。

Result: 发现前沿LLM存在"动态盲区"，无法预测行动的级联副作用，导致静默约束违规。在模糊系统中需要基于世界建模，通过心智模拟隐藏状态转换来弥补可观测性差距。

Conclusion: 可靠的企业代理需要显式学习系统动态的新范式。WoW为此提供了测试环境，并开源了GitHub代码库供设置和评估使用。

Abstract: Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.

</details>


### [98] [NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents](https://arxiv.org/abs/2601.21372)
*Yang Song,Anoushka Vyas,Zirui Wei,Sina Khoshfetrat Pakazad,Henrik Ohlsson,Graham Neubig*

Main category: cs.AI

TL;DR: NEMO是一个将自然语言决策问题描述转换为可执行数学优化实现的系统，通过远程交互自主编码代理确保代码可执行性，在多个优化基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专用大语言模型或定制代理，通常脆弱、复杂且经常生成语法无效或不可执行的代码，需要更可靠、可执行的自动化优化建模解决方案。

Method: 将自主编码代理作为一级抽象，在沙盒环境中执行以确保代码可执行性；引入新颖的协调模式，包括非对称验证循环、外部记忆经验重用、最小贝叶斯风险解码和自一致性等鲁棒性增强技术。

Result: 在九个已建立的优化基准上评估，在大多数任务上达到最先进的性能，在多个数据集上有显著优势。

Conclusion: NEMO展示了执行感知的代理架构在自动化优化建模中的强大能力，通过远程交互自主编码代理确保代码可执行性，为复杂决策问题提供可靠解决方案。

Abstract: In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code.
  NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair.
  Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.

</details>


### [99] [TeachBench: A Syllabus-Grounded Framework for Evaluating Teaching Ability in Large Language Models](https://arxiv.org/abs/2601.21375)
*Zheng Li,Siyao Song,Jingyuan Ma,Rui Li,Ying Zeng,Minghao Li,Zhifang Sui*

Main category: cs.AI

TL;DR: 该论文提出了一个基于教学大纲的评估框架，用于衡量LLM作为教学助手的能力，通过学生在多轮指导后的表现提升来评估，并在高考数据上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为教学助手的潜力受到关注，但现有评估主要关注问题解决或问题层面的指导，缺乏对知识中心教学的充分评估。需要建立一个系统框架来评估LLM的教学能力。

Method: 提出了一个基于教学大纲的评估框架，将教师代理限制在结构化知识点和示例问题上，避免信息泄露并重用现有基准。在高考数据上实例化该框架，涵盖多个学科。

Result: 实验显示不同模型和学科的教学效果存在显著差异：某些模型在数学上表现良好，但在物理和化学教学中仍具挑战性。同时发现加入示例问题并不总能改善教学，因为模型往往转向示例特定的错误纠正。

Conclusion: 教学能力是LLM行为的一个独特且可测量的维度，该框架为评估LLM教学能力提供了系统方法，揭示了当前模型在教学方面的局限性。

Abstract: Large language models (LLMs) show promise as teaching assistants, yet their teaching capability remains insufficiently evaluated. Existing benchmarks mainly focus on problem-solving or problem-level guidance, leaving knowledge-centered teaching underexplored. We propose a syllabus-grounded evaluation framework that measures LLM teaching capability via student performance improvement after multi-turn instruction. By restricting teacher agents to structured knowledge points and example problems, the framework avoids information leakage and enables reuse of existing benchmarks. We instantiate the framework on Gaokao data across multiple subjects. Experiments reveal substantial variation in teaching effectiveness across models and domains: some models perform well in mathematics, while teaching remains challenging in physics and chemistry. We also find that incorporating example problems does not necessarily improve teaching, as models often shift toward example-specific error correction. Overall, our results highlight teaching ability as a distinct and measurable dimension of LLM behavior.

</details>


### [100] [DataCross: A Unified Benchmark and Agent Framework for Cross-Modal Heterogeneous Data Analysis](https://arxiv.org/abs/2601.21403)
*Ruyi Qi,Zhou Liu,Wentao Zhang*

Main category: cs.AI

TL;DR: DataCross是一个用于跨模态数据分析的基准测试和协作代理框架，旨在激活结构化数据与视觉文档中的"僵尸数据"，实现统一分析。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据科学和企业决策中，关键信息分散在可直接查询的结构化数据源（如SQL、CSV）和锁在非结构化视觉文档（如扫描报告、发票图像）中的"僵尸数据"中。现有数据分析代理主要局限于处理结构化数据，无法激活和关联这些高价值的视觉信息，与工业需求存在显著差距。

Method: 提出DataCrossBench基准测试（包含200个端到端分析任务）和DataCrossAgent协作代理框架。基准通过人机协同反向合成管道构建，确保真实复杂性、跨源依赖性和可验证的真实性。代理框架采用"分而治之"的工作流程，包含专门化子代理（每个专注于特定数据源），通过结构化工作流程协调：源内深度探索、关键源识别和上下文交叉融合。采用新颖的reReAct机制实现稳健的代码生成和调试以进行事实验证。

Result: 实验结果显示，DataCrossAgent在事实性方面比GPT-4o提高了29.7%，在高难度任务上表现出优越的鲁棒性，有效激活了分散的"僵尸数据"进行深入的跨模态分析。

Conclusion: DataCross成功解决了跨模态数据分析的挑战，通过基准测试和协作代理框架有效激活了分散在不同模态中的"僵尸数据"，为现实世界的数据科学和企业决策提供了统一的洞察驱动分析解决方案。

Abstract: In real-world data science and enterprise decision-making, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and "zombie data" locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this high-value visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insight-driven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-the-loop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents' capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning. We also propose the DataCrossAgent framework, inspired by the "divide-and-conquer" workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented "zombie data" for insightful, cross-modal analysis.

</details>


### [101] [The Paradox of Robustness: Decoupling Rule-Based Logic from Affective Noise in High-Stakes Decision-Making](https://arxiv.org/abs/2601.21439)
*Jon Chun,Katherine Elkins*

Main category: cs.AI

TL;DR: 研究发现指令调优的大语言模型在情感框架效应上表现出惊人的稳健性，尽管它们对提示词微小变化敏感，但在规则约束的决策中却几乎不受叙事操纵影响，形成"稳健性悖论"。


<details>
  <summary>Details</summary>
Motivation: 虽然已知大语言模型对提示词微小扰动敏感且容易迎合用户偏见，但它们在重要规则约束决策中的稳健性尚未充分探索。本研究旨在探究LLMs在面对情感框架效应时的行为表现。

Method: 采用新颖的受控扰动框架，在医疗、法律和金融三个高风险领域进行实验，量化LLMs对叙事操纵的抵抗能力，并与人类受试者进行比较。

Result: LLMs表现出惊人的行为不变性：对情感框架效应的影响几乎为零（Cohen's h = 0.003），而人类则表现出显著偏见（Cohen's h在[0.3, 0.8]之间）。LLMs对叙事操纵的抵抗能力比人类高110-300倍。

Conclusion: 指令调优模型能够将逻辑规则遵循与说服性叙事解耦，提供了一种决策稳定性来源，可以补充甚至可能去偏人类判断。这种稳健性在制度环境中具有重要价值。

Abstract: While Large Language Models (LLMs) are widely documented to be sensitive to minor prompt perturbations and prone to sycophantic alignment with user biases, their robustness in consequential, rule-bound decision-making remains under-explored. In this work, we uncover a striking "Paradox of Robustness": despite their known lexical brittleness, instruction-tuned LLMs exhibit a behavioral and near-total invariance to emotional framing effects. Using a novel controlled perturbation framework across three high-stakes domains (healthcare, law, and finance), we quantify a robustness gap where LLMs demonstrate 110-300 times greater resistance to narrative manipulation than human subjects. Specifically, we find a near-zero effect size for models (Cohen's h = 0.003) compared to the substantial biases observed in humans (Cohen's h in [0.3, 0.8]). This result is highly counterintuitive and suggests the mechanisms driving sycophancy and prompt sensitivity do not necessarily translate to a failure in logical constraint satisfaction. We show that this invariance persists across models with diverse training paradigms. Our findings show that while LLMs may be "brittle" to how a query is formatted, they are remarkably "stable" against why a decision should be biased. Our findings establish that instruction-tuned models can decouple logical rule-adherence from persuasive narratives, offering a source of decision stability that complements, and even potentially de-biases, human judgment in institutional contexts. We release the 162-scenario benchmark, code, and data to facilitate the rigorous evaluation of narrative-induced bias and robustness on GitHub.com.

</details>


### [102] [MemOCR: Layout-Aware Visual Memory for Efficient Long-Horizon Reasoning](https://arxiv.org/abs/2601.21468)
*Yaorui Shi,Shugui Liu,Yu Yang,Wenyu Mao,Yuxin Chen,Qi GU,Hui Su,Xunliang Cai,Xiang Wang,An Zhang*

Main category: cs.AI

TL;DR: MemOCR：一种多模态记忆代理，通过视觉布局自适应分配记忆空间，在有限上下文预算下提升长视野推理能力


<details>
  <summary>Details</summary>
Motivation: 长视野智能体推理需要将不断增长的交互历史压缩到有限的上下文窗口中。现有记忆系统通常将历史序列化为文本，其中token级成本均匀且随长度线性增长，常常将稀缺的预算浪费在低价值细节上。

Method: 引入MemOCR多模态记忆代理，通过视觉布局实现自适应信息密度的记忆空间分配。具体包括：1）维护结构化富文本记忆（如标题、高亮）；2）将其渲染为图像供代理查询，视觉上优先显示关键证据，同时积极压缩辅助细节；3）使用强化学习在预算感知目标下训练，使代理适应不同压缩级别。

Result: 在长上下文多跳和单跳问答基准测试中，MemOCR优于强大的基于文本的基线方法，在极端预算下实现了更有效的上下文利用。

Conclusion: MemOCR通过视觉布局实现自适应信息密度分配，显著提升了长视野推理在有限上下文预算下的效果，为智能体记忆系统设计提供了新思路。

Abstract: Long-horizon agentic reasoning necessitates effectively compressing growing interaction histories into a limited context window. Most existing memory systems serialize history as text, where token-level cost is uniform and scales linearly with length, often spending scarce budget on low-value details. To this end, we introduce MemOCR, a multimodal memory agent that improves long-horizon reasoning under tight context budgets by allocating memory space with adaptive information density through visual layout. Concretely, MemOCR maintains a structured rich-text memory (e.g., headings, highlights) and renders it into an image that the agent consults for memory access, visually prioritizing crucial evidence while aggressively compressing auxiliary details. To ensure robustness across varying memory budgets, we train MemOCR with reinforcement learning under budget-aware objectives that expose the agent to diverse compression levels. Across long-context multi-hop and single-hop question-answering benchmarks, MemOCR outperforms strong text-based baselines and achieves more effective context utilization under extreme budgets.

</details>


### [103] [ScaleSim: Serving Large-Scale Multi-Agent Simulation with Invocation Distance-Based Memory Management](https://arxiv.org/abs/2601.21473)
*Zaifeng Pan,Yipeng Shen,Zhengding Hu,Zhuang Wang,Aninda Manocha,Zheng Wang,Zhongkai Yu,Yue Guan,Yufei Ding*

Main category: cs.AI

TL;DR: ScaleSim：基于调用距离抽象的多智能体模拟LLM服务系统，通过预取和优先级驱逐实现内存高效扩展


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体模拟在应用领域日益普及，但由于GPU内存压力难以扩展。每个智能体维护私有GPU驻留状态（模型、前缀缓存、适配器），随着智能体数量增长会快速耗尽设备内存。

Method: 提出调用距离的统一抽象来估计智能体未来发出LLM请求的相对顺序，基于此实现ScaleSim系统，支持主动预取和基于优先级的驱逐，通过模块化接口支持多样化的智能体特定内存。

Result: 在模拟基准测试中，ScaleSim相比SGLang实现了最高1.74倍的加速。

Conclusion: ScaleSim通过调用距离抽象和内存优化技术，有效解决了多智能体模拟中的GPU内存扩展问题。

Abstract: LLM-based multi-agent simulations are increasingly adopted across application domains, but remain difficult to scale due to GPU memory pressure. Each agent maintains private GPU-resident states, including models, prefix caches, and adapters, which quickly exhaust device memory as the agent count grows. We identify two key properties of these workloads: sparse agent activation and an estimable agent invocation order. Based on an analysis of representative workload classes, we introduce invocation distance, a unified abstraction that estimates the relative order in which agents will issue future LLM requests. Leveraging this abstraction, we present ScaleSim, a memory-efficient LLM serving system for large-scale multi-agent simulations. ScaleSim enables proactive prefetching and priority-based eviction, supports diverse agent-specific memory through a modular interface, and achieves up to 1.74x speedup over SGLang on simulation benchmarks.

</details>


### [104] [ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making](https://arxiv.org/abs/2601.21533)
*Youngjin Jin,Hanna Kim,Kwanwoo Kim,Chanhee Lee,Seungwon Shin*

Main category: cs.AI

TL;DR: ARGORA框架将多专家LLM讨论组织为显式论证图，通过因果建模识别关键推理链，并引入校正机制对齐内部推理与外部判断。


<details>
  <summary>Details</summary>
Motivation: 现有多专家LLM系统通过简单聚合整合不同观点，但掩盖了哪些论证驱动了最终决策，缺乏透明度和可解释性。

Method: 将多专家讨论组织为显式论证图，展示论证间的支持与攻击关系；将论证图建模为因果模型，通过系统性移除单个论证并重新计算结果来识别关键推理链；引入校正机制，当内部推理与外部判断不一致时进行对齐。

Result: 在多样化基准测试和开放式用例中，ARGORA达到竞争性准确率，并展现校正行为：当专家最初存在分歧时，框架更倾向于将争议解决为正确答案而非引入新错误，同时提供关键论证的因果诊断。

Conclusion: ARGORA通过显式论证图和因果分析提高了多专家LLM系统的透明度和可解释性，能够识别关键推理链并进行自我校正，为复杂决策提供更好的诊断能力。

Abstract: Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.

</details>


### [105] [ShardMemo: Masked MoE Routing for Sharded Agentic LLM Memory](https://arxiv.org/abs/2601.21545)
*Yang Zhao,Chengxiao Dai,Yue Xiu,Mengying Kou,Yuliang Zheng,Dusit Niyato*

Main category: cs.AI

TL;DR: ShardMemo是一个为智能体LLM系统设计的预算分层内存服务，通过三层内存架构和范围优先路由机制，在固定预算下显著提升性能并降低检索开销。


<details>
  <summary>Details</summary>
Motivation: 随着内存容量和并行访问增长，集中式索引和启发式分区成为智能体LLM系统的瓶颈，需要更高效的内存管理方案来支持长时状态和并发多智能体执行。

Method: 提出三层内存架构：Tier A（智能体工作状态）、Tier B（分片证据库，带分片本地ANN索引）、Tier C（版本化技能库）。Tier B采用范围优先路由机制，将分片探测建模为掩码混合专家路由，使用成本感知门控和训练的路由器。

Result: 在LoCoMo上比最强基线提升5.11-6.82 F1；固定预算路由下比余弦原型路由提升6.87 F1，同时减少20.5%检索工作和降低p95延迟；在长上下文HotpotQA上达到57.95-63.41 F1；在ToolBench上Tier C达到0.97 Precision@3和1.94 StepRed。

Conclusion: ShardMemo通过分层内存架构和智能路由机制，有效解决了智能体LLM系统中的内存管理瓶颈，在多个基准测试中显著提升了性能并降低了开销。

Abstract: Agentic large language model (LLM) systems rely on external memory for long-horizon state and concurrent multi-agent execution, but centralized indexes and heuristic partitions become bottlenecks as memory volume and parallel access grow. We present ShardMemo, a budgeted tiered memory service with Tier A per-agent working state, Tier B sharded evidence with shard-local approximate nearest neighbor (ANN) indexes, and Tier C, a versioned skill library. Tier B enforces scope-before-routing: structured eligibility constraints mask ineligible shards before routing or ANN search. We cast shard probing as masked mixture-of-experts (MoE) routing over eligible shards, probing up to $B_{\mathrm{probe}}$ shards via Top-$B_{\mathrm{probe}}$ or adaptive Top-$P$, and use cost-aware gating over profile/observation/session shard families; the router is trained from evidence-to-shard supervision. On LoCoMo, ShardMemo improves over the strongest baseline (GAM) by +5.11 to +6.82 F1 across question categories. Under a fixed-budget routing setting ($B_{\mathrm{probe}}=3$), ShardMemo improves over cosine-to-prototype shard routing by +6.87 F1 while reducing retrieval work (VecScan 521->414, -20.5%) and p95 latency (95->76 ms). On long-context HotpotQA, ShardMemo achieves 63.41/61.88/57.95 F1 at 56K/224K/448K tokens. On ToolBench, Tier C reaches 0.97 Precision@3 and 1.94 StepRed (+10.2% and +7.2% over embedding-similarity retrieval).

</details>


### [106] [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/abs/2601.21557)
*Haoran Ye,Xuning He,Vincent Arak,Haonan Dong,Guojie Song*

Main category: cs.AI

TL;DR: 提出Meta Context Engineering (MCE)框架，通过元级代理和基础级代理的协同进化，超越传统上下文工程的静态启发式方法，实现上下文技能和上下文工件的共同优化。


<details>
  <summary>Details</summary>
Motivation: 当前上下文工程方法依赖人工设计的固定工作流程和预定义上下文模式，存在结构偏见，限制了上下文优化的设计空间。需要更灵活、自适应的上下文工程框架。

Method: 提出双层MCE框架：元级代理通过"代理交叉"在技能历史、执行和评估中进行审慎搜索来优化工程技能；基础级代理执行这些技能，从训练过程中学习，并将上下文优化为灵活的文件和代码。

Result: 在五个不同领域和离线/在线设置下评估，MCE相比最先进的代理上下文工程方法获得5.6-53.8%的相对改进（平均16.9%），同时在上下文适应性、可迁移性、使用效率和训练效率方面表现优越。

Conclusion: MCE框架能够超越静态上下文工程启发式方法，通过协同进化技能和上下文工件，实现更有效的上下文优化，为大型语言模型的推理时上下文优化提供了新范式。

Abstract: The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.

</details>


### [107] [EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots](https://arxiv.org/abs/2601.21570)
*Zixing Lei,Genjia Liu,Yuanshuo Zhang,Qipeng Liu,Chuan Wen,Shanghang Zhang,Wenzhao Lian,Siheng Chen*

Main category: cs.AI

TL;DR: LLM代理在具身AI中自主工程化策略的基准评估，通过代码作为通用接口，在32个任务上实现动态闭环工作流，性能超越人工基线26.5%


<details>
  <summary>Details</summary>
Motivation: 具身AI领域向通用机器人系统快速演进，但扩展能力受限于依赖人工监督（从复杂奖励设计到异构后端超参数调优）。受LLM在软件自动化和科学发现成功的启发，研究LLM代理自主工程化具身策略的能力

Method: 提出EmboCoach-Bench基准，包含32个专家策划的RL和IL任务，以可执行代码为通用接口。评估动态闭环工作流，代理利用环境反馈迭代起草、调试和优化解决方案，涵盖从物理感知奖励设计到扩散策略等架构改进

Result: 1) 自主代理平均成功率比人工工程基线高出26.5%；2) 带环境反馈的代理工作流有效强化策略开发，显著缩小开源与专有模型性能差距；3) 代理展现自校正能力，通过迭代仿真调试从近乎完全失败中恢复任务性能

Conclusion: 为自演化具身智能奠定基础，加速从劳动密集型手动调优向可扩展自主工程的范式转变

Abstract: The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.

</details>


### [108] [Beyond Imitation: Reinforcement Learning for Active Latent Planning](https://arxiv.org/abs/2601.21598)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: ATP-Latent提出一种主动潜在规划方法，通过条件变分自编码器建模潜在token监督过程，并结合强化学习优化潜在推理策略，在保持推理准确性的同时减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法通过模仿语言标签来监督潜在token，但由于一个问题可能有多个等价但不同的CoT标签，被动模仿任意一个可能导致次优的潜在token表示和推理策略，限制了潜在规划能力，并造成训练与测试间的差距。

Method: 提出ATP-Latent方法：1）使用条件变分自编码器（VAE）建模潜在token的监督过程，获得更平滑的潜在空间；2）进行强化学习，采用基于VAE解码内容一致性的辅助连贯性奖励，引导RL过程以获得最合理的潜在推理策略。

Result: 在LLaMA-1B模型上，ATP-Latent在四个基准测试中相比先进基线实现了+4.1%的准确率提升和-3.3%的token减少。

Conclusion: 强调在潜在token表示空间中进行主动规划对实现最优潜在推理策略的重要性，ATP-Latent通过VAE平滑潜在空间和基于一致性的RL奖励，有效提升了潜在推理的性能和效率。

Abstract: Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.

</details>


### [109] [RecNet: Self-Evolving Preference Propagation for Agentic Recommender Systems](https://arxiv.org/abs/2601.21609)
*Bingqian Li,Xiaolei Wang,Junyi Li,Weitao Li,Long Zhang,Sheng Chen,Wayne Xin Zhao,Ji-Rong Wen*

Main category: cs.AI

TL;DR: RecNet：一个自演化的偏好传播框架，通过前向传播和反向优化两阶段，主动在相关用户和物品间传播实时偏好更新，解决现有方法依赖稀疏、噪声交互数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统主要依赖显式的用户-物品交互数据来建模偏好变化，但这些数据稀疏、噪声大，且无法反映用户和物品间的实时相互影响。需要一种能主动传播实时偏好更新的框架。

Method: RecNet包含两个互补阶段：1）前向阶段：集中式偏好路由机制通过路由代理整合偏好更新并动态传播给最相关代理；个性化偏好接收机制结合消息缓冲和可优化的基于规则的过滤记忆来指导选择性偏好同化。2）反向阶段：反馈驱动的传播优化机制模拟多智能体强化学习框架，使用LLM进行信用分配、梯度分析和模块级优化，实现传播策略的持续自演化。

Result: 在各种场景下的广泛实验证明了RecNet在建模推荐系统偏好传播方面的有效性。

Conclusion: RecNet通过自演化的偏好传播框架，能够主动传播实时偏好更新，有效解决了现有推荐系统依赖稀疏、噪声交互数据的局限性。

Abstract: Agentic recommender systems leverage Large Language Models (LLMs) to model complex user behaviors and support personalized decision-making. However, existing methods primarily model preference changes based on explicit user-item interactions, which are sparse, noisy, and unable to reflect the real-time, mutual influences among users and items. To address these limitations, we propose RecNet, a self-evolving preference propagation framework that proactively propagates real-time preference updates across related users and items. RecNet consists of two complementary phases. In the forward phase, the centralized preference routing mechanism leverages router agents to integrate preference updates and dynamically propagate them to the most relevant agents. To ensure accurate and personalized integration of propagated preferences, we further introduce a personalized preference reception mechanism, which combines a message buffer for temporary caching and an optimizable, rule-based filter memory to guide selective preference assimilation based on past experience and interests. In the backward phase, the feedback-driven propagation optimization mechanism simulates a multi-agent reinforcement learning framework, using LLMs for credit assignment, gradient analysis, and module-level optimization, enabling continuous self-evolution of propagation strategies. Extensive experiments on various scenarios demonstrate the effectiveness of RecNet in modeling preference propagation for recommender systems.

</details>


### [110] [Semantic Content Determines Algorithmic Performance](https://arxiv.org/abs/2601.21618)
*Martiño Ríos-García,Nawaf Alampara,Kevin Maik Jablonka*

Main category: cs.AI

TL;DR: 论文提出WhatCounts测试框架，专门检测LLMs在简单计数任务中的语义敏感性，发现前沿LLMs的准确率会因计数对象语义类型不同而产生超过40%的波动，表明LLMs并非真正实现算法而是近似实现，且这种近似依赖于输入参数的意义。


<details>
  <summary>Details</summary>
Motivation: 算法的行为应该对其参数的语义内容保持不变性，但现有研究往往将语义敏感性与推理复杂性或提示变化混为一谈。需要一种原子化的测试方法来专门检测LLMs是否真正实现了算法不变性。

Method: 提出WhatCounts测试框架：要求模型在无歧义、有分隔、无重复、无干扰项的列表中计数不同语义类型的项目（如城市vs化学物、名称vs符号）。通过控制变量实验排除混淆因素，并观察少量无关微调对语义敏感性的影响。

Result: 前沿LLMs的计数准确率仅因计数对象语义类型不同就产生超过40%的波动。控制实验排除了其他解释，且语义敏感性会随少量无关微调发生不可预测的变化。通过智能体示例展示了这种语义依赖性会扩展到计数以外的LLM功能。

Conclusion: LLMs并非真正实现算法，而是近似实现算法，且这种近似依赖于输入参数的意义。这种语义敏感性具有广泛影响，任何LLM函数都可能隐藏着对其输入意义的依赖。

Abstract: Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.

</details>


### [111] [E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory](https://arxiv.org/abs/2601.21714)
*Kaixiang Wang,Yidan Lin,Jiong Lou,Zhaojiacheng Zhou,Bunyod Suvonov,Jie Li*

Main category: cs.AI

TL;DR: E-mem框架通过从记忆预处理转向情景上下文重建，采用异构分层架构，在保持上下文完整性的同时实现高效推理，在LoCoMo基准上超越现有方法并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理向System 2推理演进时，主流记忆预处理方法存在破坏性去上下文化问题，将复杂序列依赖压缩为预定义结构会破坏深度推理所需的上下文完整性。

Method: 提出E-mem框架，从记忆预处理转向情景上下文重建，采用异构分层架构：多个助手代理维护未压缩的记忆上下文，中央主代理协调全局规划，助手在激活片段内进行本地推理并提取上下文感知证据。

Result: 在LoCoMo基准测试中，E-mem达到超过54%的F1分数，比最先进的GAM方法提升7.75%，同时减少超过70%的token成本。

Conclusion: E-mem通过情景上下文重建有效解决了记忆预处理中的去上下文化问题，为LLM代理实现System 2推理提供了保持逻辑完整性的新途径。

Abstract: The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\% F1, surpassing the state-of-the-art GAM by 7.75\%, while reducing token cost by over 70\%.

</details>


### [112] [Language-based Trial and Error Falls Behind in the Era of Experience](https://arxiv.org/abs/2601.21754)
*Haoyu Wang,Guozheng Ma,Shugang Cui,Yilun Kong,Haotian Luo,Li Shen,Mengya Gao,Yichao Wu,Xiaogang Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: SCOUT框架通过轻量级"侦察兵"探索环境动态，引导LLM在非语言环境中学习，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语言任务上表现出色，但在未见过的非语言环境（如符号或空间任务）中表现受限。传统方法认为这是预训练分布与测试分布不匹配的问题，但本文发现主要瓶颈是探索成本过高：掌握这些任务需要大量试错，这对参数庞大的LLM在高维语义空间中计算不可持续。

Method: 提出SCOUT框架，将探索与利用解耦：1) 使用轻量级"侦察兵"（如小型MLP）快速探测环境动态；2) 收集的轨迹通过监督微调引导LLM；3) 通过多轮强化学习激活LLM的潜在世界知识。

Result: SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于Gemini-2.5-Pro（0.60）等专有模型，同时节省约60%的GPU小时消耗。

Conclusion: 探索成本是LLM在非语言环境中表现受限的主要瓶颈，SCOUT框架通过解耦探索与利用，使用轻量级代理引导LLM学习，在保持高性能的同时大幅降低计算成本。

Abstract: While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight "scouts" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.

</details>


### [113] [BioAgent Bench: An AI Agent Evaluation Suite for Bioinformatics](https://arxiv.org/abs/2601.21800)
*Dionizije Fa,Marko Čuljak,Bruno Pandža,Mateo Čupić*

Main category: cs.AI

TL;DR: BioAgent Bench：用于评估AI代理在生物信息学任务中性能和鲁棒性的基准数据集与评估套件，包含端到端任务和自动化评估，测试发现前沿代理能完成多步骤流程但鲁棒性不足。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门针对生物信息学任务的AI代理评估基准，需要衡量代理在真实生物信息学工作流程中的性能、鲁棒性以及隐私约束下的适用性。

Method: 创建包含RNA-seq、变异检测、宏基因组学等端到端任务的基准数据集，设计具体输出规范支持自动化评估，引入受控扰动进行压力测试，使用LLM评分器评估流程进展和结果有效性。

Result: 前沿代理能够完成多步骤生物信息学流程而无需复杂定制框架，通常能可靠生成最终产物。但鲁棒性测试显示在输入损坏、干扰文件、提示膨胀等扰动下存在失败模式。隐私约束下开源模型虽完成率较低但更适用。

Conclusion: AI代理在生物信息学任务中展现出潜力但鲁棒性仍需提升，高层流程构建正确不能保证步骤级推理可靠。隐私敏感场景下开源模型更具优势。基准数据集已公开发布。

Abstract: This paper introduces BioAgent Bench, a benchmark dataset and an evaluation suite designed for measuring the performance and robustness of AI agents in common bioinformatics tasks. The benchmark contains curated end-to-end tasks (e.g., RNA-seq, variant calling, metagenomics) with prompts that specify concrete output artifacts to support automated assessment, including stress testing under controlled perturbations. We evaluate frontier closed-source and open-weight models across multiple agent harnesses, and use an LLM-based grader to score pipeline progress and outcome validity. We find that frontier agents can complete multi-step bioinformatics pipelines without elaborate custom scaffolding, often producing the requested final artifacts reliably. However, robustness tests reveal failure modes under controlled perturbations (corrupted inputs, decoy files, and prompt bloat), indicating that correct high-level pipeline construction does not guarantee reliable step-level reasoning. Finally, because bioinformatics workflows may involve sensitive patient data, proprietary references, or unpublished IP, closed-source models can be unsuitable under strict privacy constraints; in such settings, open-weight models may be preferable despite lower completion rates. We release the dataset and evaluation suite publicly.

</details>


### [114] [astra-langchain4j: Experiences Combining LLMs and Agent Programming](https://arxiv.org/abs/2601.21879)
*Rem Collier,Katharine Beaumont,Andrei Ciortea*

Main category: cs.AI

TL;DR: 本文介绍了将大型语言模型集成到ASTRA编程语言中的原型开发经验，通过三个示例实现展示了传统代理工具包与新兴Agentic AI技术的结合。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和Agentic AI作为多智能体系统形式的发展，需要探索这些新技术如何影响传统代理工具包的使用，以及传统工具包中的丰富经验如何影响新代理平台的设计。

Method: 开发了ASTRA编程语言的LLM集成原型，简要介绍了该工具包，并通过三个具体示例实现来展示集成方法。

Result: 成功实现了LLM与ASTRA的集成原型，通过三个示例展示了传统代理工具包与新兴AI技术的结合可能性。

Conclusion: 通过原型开发经验，展示了传统代理工具包经验对新兴Agentic AI平台设计的价值，以及两者结合的技术路径。

Abstract: Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.

</details>


### [115] [From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning](https://arxiv.org/abs/2601.21909)
*Shaojie Wang,Liang Zhang*

Main category: cs.AI

TL;DR: 提出CoMT+CCRL框架，模仿人类两阶段认知过程：先学习抽象推理模式，再通过置信度校准强化学习优化任务适应，相比传统方法提升性能并大幅降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练方法将完整推理轨迹作为基本单元进行优化，这与人类实际解决问题的方式存在根本差异。人类认知自然地将问题解决分解为两个阶段：先获取跨问题通用的抽象策略（元知识），再将其适应到具体实例。现有方法的问题中心化导致抽象策略与具体执行纠缠，限制了泛化能力。

Method: 提出认知启发的两阶段框架：1) Chain-of-Meta-Thought (CoMT)：专注于抽象推理模式的有监督学习，不涉及具体执行，获取可泛化的策略；2) Confidence-Calibrated Reinforcement Learning (CCRL)：通过中间步骤的置信度感知奖励优化任务适应，防止过度自信的错误级联，提高执行可靠性。

Result: 在4个模型和8个基准测试中，相比标准方法，分布内性能提升2.19%，分布外性能提升4.63%，同时训练时间减少65-70%，token消耗减少50%。

Conclusion: 将后训练与人类认知原则对齐不仅能获得更好的泛化性能，还能显著提高训练效率，为LLM优化提供了更符合认知科学的新范式。

Abstract: Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.

</details>


### [116] [ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.21912)
*Zhao Wang,Ziliang Zhao,Zhicheng Dou*

Main category: cs.AI

TL;DR: ProRAG提出了一种过程监督强化学习框架，通过整合步骤级监督到在线优化循环中，解决了RAG优化中奖励稀疏和信用分配低效的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的RL方法在复杂推理任务中存在奖励稀疏和信用分配低效的问题，粗粒度的标量奖励无法识别长轨迹中的具体错误步骤，导致"过程幻觉"——模型通过错误逻辑或冗余检索步骤得到正确答案。

Method: ProRAG包含四个阶段：1)监督策略预热，用结构化推理格式初始化模型；2)构建基于MCTS的过程奖励模型(PRM)量化中间推理质量；3)PRM引导的推理精炼，使策略与细粒度过程偏好对齐；4)过程监督强化学习，采用双粒度优势机制，聚合步骤级过程奖励和全局结果信号。

Result: 在五个多跳推理基准测试上的广泛实验表明，ProRAG相比基于结果和过程感知的RL基线实现了更优的整体性能，特别是在复杂长视野任务上，验证了细粒度过程监督的有效性。

Conclusion: ProRAG通过整合步骤级监督到在线优化循环中，有效解决了RAG优化中的奖励稀疏和信用分配问题，为复杂推理任务提供了精确的反馈机制。

Abstract: Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to "process hallucinations", where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.

</details>


### [117] [JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG](https://arxiv.org/abs/2601.21916)
*Yiqun Chen,Erhan Zhang,Tianyi Hu,Shijie Wang,Zixuan Yang,Meizhi Zhong,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Jiaxin Mao*

Main category: cs.AI

TL;DR: JADE提出联合优化动态多轮工作流中规划与执行的统一框架，通过协同多智能体团队建模实现端到端学习，解决现有方法中规划与执行模块解耦优化导致的"战略操作不匹配"问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面临关键二分法：要么在固定架构中联合优化模块，要么支持动态规划但将执行器视为冻结黑盒。这种解耦优化导致"战略操作不匹配"，复杂规划策略因执行器不适应而无法实现，系统复杂度增加却带来负面性能收益。

Method: 提出JADE框架，将系统建模为共享骨干网络的协同多智能体团队，通过基于结果的奖励驱动端到端学习，实现规划器与执行器的协同适应：规划器学习在执行器能力边界内操作，执行器演化以对齐高层战略意图。

Result: 实证结果表明JADE将离散模块转化为协同系统，通过联合优化带来显著性能提升，并通过动态工作流编排实现效率与效果之间的灵活平衡。

Conclusion: JADE通过联合优化动态多轮工作流中的规划与执行，解决了现有RAG系统的战略操作不匹配问题，实现了模块间的协同适应和系统性能的显著提升。

Abstract: The evolution of Retrieval-Augmented Generation (RAG) has shifted from static retrieval pipelines to dynamic, agentic workflows where a central planner orchestrates multi-turn reasoning. However, existing paradigms face a critical dichotomy: they either optimize modules jointly within rigid, fixed-graph architectures, or empower dynamic planning while treating executors as frozen, black-box tools. We identify that this \textit{decoupled optimization} creates a ``strategic-operational mismatch,'' where sophisticated planning strategies fail to materialize due to unadapted local executors, often leading to negative performance gains despite increased system complexity. In this paper, we propose \textbf{JADE} (\textbf{J}oint \textbf{A}gentic \textbf{D}ynamic \textbf{E}xecution), a unified framework for the joint optimization of planning and execution within dynamic, multi-turn workflows. By modeling the system as a cooperative multi-agent team unified under a single shared backbone, JADE enables end-to-end learning driven by outcome-based rewards. This approach facilitates \textit{co-adaptation}: the planner learns to operate within the capability boundaries of the executors, while the executors evolve to align with high-level strategic intent. Empirical results demonstrate that JADE transforms disjoint modules into a synergistic system, yielding remarkable performance improvements via joint optimization and enabling a flexible balance between efficiency and effectiveness through dynamic workflow orchestration.

</details>


### [118] [Self-Compression of Chain-of-Thought via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.21919)
*Yiqun Chen,Jinyuan Feng,Wei Yang,Meizhi Zhong,Zhengliang Shi,Rui Li,Xiaochi Wei,Yan Gao,Yi Wu,Yao Hu,Zhiqiang Pu,Jiaxin Mao*

Main category: cs.AI

TL;DR: SCMA是一个多智能体强化学习框架，通过分割和评分智能体协作检测冗余推理块，在保持准确性的同时显著减少大推理模型的响应长度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的解决方案通过简单的长度惩罚与结果奖励结合，难以平衡简洁性与准确性，强制简洁可能损害关键推理逻辑。冗余推理带来的推理开销会破坏交互体验并严重阻碍大推理模型的部署。

Method: 提出多智能体强化学习框架SCMA，包含三个智能体：分割智能体将推理过程分解为逻辑块，评分智能体量化每个块的重要性，推理智能体在训练中基于重要性加权的长度惩罚进行优化，部署时无需额外推理开销。

Result: 在不同模型规模上的实验表明，SCMA将响应长度减少了11.1%到39.0%，同时将准确性提高了4.33%到10.02%。消融研究和定性分析验证了MARL框架内的协同优化产生了涌现行为，相比传统RL范式产生了更强大的大推理模型。

Conclusion: SCMA框架通过多智能体协作的选择性冗余惩罚机制，有效解决了大推理模型中简洁性与准确性的权衡问题，在减少推理开销的同时提升了模型性能。

Abstract: The inference overhead induced by redundant reasoning undermines the interactive experience and severely bottlenecks the deployment of Large Reasoning Models. Existing reinforcement learning (RL)-based solutions tackle this problem by coupling a length penalty with outcome-based rewards. This simplistic reward weighting struggles to reconcile brevity with accuracy, as enforcing brevity may compromise critical reasoning logic. In this work, we address this limitation by proposing a multi-agent RL framework that selectively penalizes redundant chunks, while preserving essential reasoning logic. Our framework, Self-Compression via MARL (SCMA), instantiates redundancy detection and evaluation through two specialized agents: \textbf{a Segmentation Agent} for decomposing the reasoning process into logical chunks, and \textbf{a Scoring Agent} for quantifying the significance of each chunk. The Segmentation and Scoring agents collaboratively define an importance-weighted length penalty during training, incentivizing \textbf{a Reasoning Agent} to prioritize essential logic without introducing inference overhead during deployment. Empirical evaluations across model scales demonstrate that SCMA reduces response length by 11.1\% to 39.0\% while boosting accuracy by 4.33\% to 10.02\%. Furthermore, ablation studies and qualitative analysis validate that the synergistic optimization within the MARL framework fosters emergent behaviors, yielding more powerful LRMs compared to vanilla RL paradigms.

</details>


### [119] [AgenticSimLaw: A Juvenile Courtroom Multi-Agent Debate Simulation for Explainable High-Stakes Tabular Decision Making](https://arxiv.org/abs/2601.21936)
*Jon Chun,Kathrine Elkins,Yong Suk Lee*

Main category: cs.AI

TL;DR: AgenticSimLaw是一个基于法庭辩论结构的多智能体框架，用于高风险表格决策任务，提供透明可控的推理过程，在年轻成人再犯预测任务上表现优于传统单智能体方法。


<details>
  <summary>Details</summary>
Motivation: 针对高风险决策任务（如刑事司法）需要透明、可审计的推理过程，传统黑盒方法缺乏解释性和可控性，需要一种能够提供完整交互记录和人类监督的框架。

Method: 提出法庭风格的多智能体辩论框架，明确定义角色（检察官、辩护律师、法官）、7轮结构化辩论协议和私有推理策略，在NLSY97数据集上进行年轻成人再犯预测基准测试。

Result: 相比传统思维链提示，结构化多智能体辩论提供更稳定和可泛化的性能，准确率和F1分数相关性更强，同时提供细粒度推理控制和完整交互记录用于可解释性。

Conclusion: AgenticSimLaw通过结构化角色、交互日志记录和明确部署约束，解决了LLM多智能体系统的关键挑战，为需要透明度和人类监督的高风险决策任务提供了通用框架。

Abstract: We introduce AgenticSimLaw, a role-structured, multi-agent debate framework that provides transparent and controllable test-time reasoning for high-stakes tabular decision-making tasks. Unlike black-box approaches, our courtroom-style orchestration explicitly defines agent roles (prosecutor, defense, judge), interaction protocols (7-turn structured debate), and private reasoning strategies, creating a fully auditable decision-making process. We benchmark this framework on young adult recidivism prediction using the NLSY97 dataset, comparing it against traditional chain-of-thought (CoT) prompting across almost 90 unique combinations of models and strategies. Our results demonstrate that structured multi-agent debate provides more stable and generalizable performance compared to single-agent reasoning, with stronger correlation between accuracy and F1-score metrics. Beyond performance improvements, AgenticSimLaw offers fine-grained control over reasoning steps, generates complete interaction transcripts for explainability, and enables systematic profiling of agent behaviors. While we instantiate this framework in the criminal justice domain to stress-test reasoning under ethical complexity, the approach generalizes to any deliberative, high-stakes decision task requiring transparency and human oversight. This work addresses key LLM-based multi-agent system challenges: organization through structured roles, observability through logged interactions, and responsibility through explicit non-deployment constraints for sensitive domains. Data, results, and code will be available on github.com under the MIT license.

</details>


### [120] [ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models](https://arxiv.org/abs/2601.21947)
*Bowen Fang,Wen Ye,Yunyue Su,Jinghao Zhang,Qiang Liu,Yesheng Liu,Xin Sun,Shu Wu,Jiabing Yang,Baole Wei,Liang Wang*

Main category: cs.AI

TL;DR: ToolWeaver提出了一种新的生成式工具学习框架，通过将工具编码为分层序列来解决现有方法的语义瓶颈和可扩展性问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的工具使用管道面临双重语义挑战：检索器编码器难以捕捉复杂语义，而LLM本身缺乏工具知识。生成式方法虽然统一了选择和执行，但将每个工具映射到唯一新token会导致可扩展性危机和语义瓶颈，阻碍工具协作关系的建模。

Method: 提出ToolWeaver框架，将工具编码为分层序列，使词汇表扩展与工具数量呈对数关系。通过新颖的标记化过程生成结构化代码，将工具内在语义与外部使用模式结合，然后通过生成对齐阶段将分层代码序列集成到LLM中。

Result: 在近47,000个工具上的评估显示，ToolWeaver显著优于最先进方法，为高级工具增强智能体建立了更可扩展、可泛化和语义感知的基础。

Conclusion: ToolWeaver通过分层序列编码解决了生成式工具学习的可扩展性和语义瓶颈问题，使模型能够从共享代码的密集共现中学习协作模式，而不是从单一工具ID的稀疏共现中推断。

Abstract: Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.

</details>


### [121] [How do Visual Attributes Influence Web Agents? A Comprehensive Evaluation of User Interface Design Factors](https://arxiv.org/abs/2601.21961)
*Kuai Yu,Naicheng Yu,Han Wang,Rui Yang,Huan Zhang*

Main category: cs.AI

TL;DR: 提出了VAF评估框架，用于量化网页视觉属性因素如何影响智能体决策，发现背景对比度、项目大小、位置和卡片清晰度对智能体行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注对抗攻击下的鲁棒性，而对良性场景中智能体偏好的研究不足，特别是视觉属性如何影响智能体决策的系统性理解有限。

Method: 提出VAF三阶段评估流程：1) 变体生成（保持语义不变仅改变视觉属性）；2) 浏览交互（模拟人类滚动点击行为）；3) 通过点击率和提及率验证，量化视觉属性影响。

Result: 在8个变体家族（48个变体）、5个真实网站和4个代表性智能体上的实验表明：背景颜色对比度、项目大小、位置和卡片清晰度对智能体行为影响显著，而字体样式、文本颜色和项目图像清晰度影响较小。

Conclusion: 视觉属性确实会影响智能体决策，某些属性（如背景对比度、项目大小）影响显著，这为设计更鲁棒的智能体提供了重要见解。

Abstract: Web agents have demonstrated strong performance on a wide range of web-based tasks. However, existing research on the effect of environmental variation has mostly focused on robustness to adversarial attacks, with less attention to agents' preferences in benign scenarios. Although early studies have examined how textual attributes influence agent behavior, a systematic understanding of how visual attributes shape agent decision-making remains limited. To address this, we introduce VAF, a controlled evaluation pipeline for quantifying how webpage Visual Attribute Factors influence web-agent decision-making. Specifically, VAF consists of three stages: (i) variant generation, which ensures the variants share identical semantics as the original item while only differ in visual attributes; (ii) browsing interaction, where agents navigate the page via scrolling and clicking the interested item, mirroring how human users browse online; (iii) validating through both click action and reasoning from agents, which we use the Target Click Rate and Target Mention Rate to jointly evaluate the effect of visual attributes. By quantitatively measuring the decision-making difference between the original and variant, we identify which visual attributes influence agents' behavior most. Extensive experiments, across 8 variant families (48 variants total), 5 real-world websites (including shopping, travel, and news browsing), and 4 representative web agents, show that background color contrast, item size, position, and card clarity have a strong influence on agents' actions, whereas font styling, text color, and item image clarity exhibit minor effects.

</details>


### [122] [Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic](https://arxiv.org/abs/2601.21972)
*Shuo Liu,Tianle Chen,Ryan Amiri,Christopher Amato*

Main category: cs.AI

TL;DR: 该论文提出了两种多智能体演员-评论家方法（CoLLM-CC和CoLLM-DC）来优化去中心化LLM协作，相比蒙特卡洛方法在长视野或稀疏奖励任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有MARL微调方法依赖预定义执行协议且需要集中执行，而去中心化LLM协作在实际中更具吸引力。当前方法使用蒙特卡洛方法存在高方差问题，需要更多训练样本。

Method: 提出了两种多智能体演员-评论家方法：CoLLM-CC（集中式评论家）和CoLLM-DC（去中心化评论家），用于优化去中心化LLM协作。

Result: 实验表明，在短视野和密集奖励任务中，蒙特卡洛方法和CoLLM-DC能达到与CoLLM-CC相当的性能。但在长视野或稀疏奖励任务中，蒙特卡洛方法需要更多样本，CoLLM-DC难以收敛，而CoLLM-CC表现更优。

Conclusion: 集中式评论家的MAAC方法（CoLLM-CC）在复杂任务中具有优势，特别是在长视野或稀疏奖励场景下，能有效解决去中心化LLM协作的优化问题。

Abstract: Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.

</details>


### [123] [Heterogeneous Computing: The Key to Powering the Future of AI Agent Inference](https://arxiv.org/abs/2601.22001)
*Yiren Zhao,Junyi Liu*

Main category: cs.AI

TL;DR: 论文分析了AI智能体推理的内存瓶颈，提出了操作强度(OI)和容量占用(CF)两个新指标来补充传统屋顶线分析，揭示了内存容量墙问题，并提出了解耦服务、异构系统、智能体-硬件协同设计等解决方案。


<details>
  <summary>Details</summary>
Motivation: AI智能体推理正在推动数据中心向推理密集型发展，暴露出超越计算能力之外的瓶颈，特别是内存容量、内存带宽和高速互连。传统屋顶线分析无法捕捉这些内存相关的瓶颈。

Method: 引入两个新指标：操作强度(OI)和容量占用(CF)，用于分析不同智能体工作流（聊天、编码、网页使用、计算机使用）和基础模型选择（GQA/MLA、MoE、量化）下的性能瓶颈。特别关注长上下文KV缓存导致解码阶段高度内存受限的问题。

Result: OI和CF指标能够解释传统屋顶线分析遗漏的性能瓶颈，包括内存容量墙。在不同智能体工作流和模型配置下，OI/CF会发生显著变化，长上下文KV缓存使解码阶段高度内存受限。

Conclusion: 需要解耦服务架构和系统级异构设计：专用预填充和解码加速器、更大规模的网络扩展、通过光I/O实现的解耦计算-内存架构。提出智能体-硬件协同设计、单系统内多推理加速器、高带宽大容量内存解耦作为适应不断变化的OI/CF的基础。

Abstract: AI agent inference is driving an inference heavy datacenter future and exposes bottlenecks beyond compute - especially memory capacity, memory bandwidth and high-speed interconnect. We introduce two metrics - Operational Intensity (OI) and Capacity Footprint (CF) - that jointly explain regimes the classic roofline analysis misses, including the memory capacity wall. Across agentic workflows (chat, coding, web use, computer use) and base model choices (GQA/MLA, MoE, quantization), OI/CF can shift dramatically, with long context KV cache making decode highly memory bound. These observations motivate disaggregated serving and system level heterogeneity: specialized prefill and decode accelerators, broader scale up networking, and decoupled compute-memory enabled by optical I/O. We further hypothesize agent-hardware co design, multiple inference accelerators within one system, and high bandwidth, large capacity memory disaggregation as foundations for adaptation to evolving OI/CF. Together, these directions chart a path to sustain efficiency and capability for large scale agentic AI inference.

</details>


### [124] [CAR-bench: Evaluating the Consistency and Limit-Awareness of LLM Agents under Real-World Uncertainty](https://arxiv.org/abs/2601.22027)
*Johannes Kirmayr,Lukas Stappen,Elisabeth André*

Main category: cs.AI

TL;DR: CAR-bench是一个用于评估车载语音助手场景下LLM代理的基准测试，特别关注一致性、不确定性处理和能力认知，包含幻觉任务和消歧任务，揭示现有模型在真实场景中的可靠性不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理基准测试主要关注理想环境下的任务完成度，忽略了真实用户场景中的可靠性问题。在车载语音助手等应用中，用户经常提出不完整或模糊的请求，代理需要通过对话、工具使用和策略遵守来管理这种内在不确定性。

Method: 引入CAR-bench基准测试，包含LLM模拟用户、领域策略和58个互连工具（导航、生产力、充电、车辆控制）。除了标准任务完成外，还设计了幻觉任务（测试在工具或信息缺失时的限制认知）和消歧任务（要求通过澄清或内部信息收集解决不确定性）。

Result: 基线结果显示，所有任务类型上都存在偶尔成功与持续成功之间的巨大差距。即使是前沿推理LLM在消歧任务上的持续通过率也不到50%（由于过早行动），在幻觉任务中经常违反策略或编造信息以满足用户请求。

Conclusion: 真实世界应用需要更可靠和自知的LLM代理，现有模型在处理不确定性和认知自身限制方面存在显著不足，CAR-bench为评估和改进这些能力提供了重要基准。

Abstract: Existing benchmarks for Large Language Model (LLM) agents focus on task completion under idealistic settings but overlook reliability in real-world, user-facing applications. In domains, such as in-car voice assistants, users often issue incomplete or ambiguous requests, creating intrinsic uncertainty that agents must manage through dialogue, tool use, and policy adherence. We introduce CAR-bench, a benchmark for evaluating consistency, uncertainty handling, and capability awareness in multi-turn, tool-using LLM agents in an in-car assistant domain. The environment features an LLM-simulated user, domain policies, and 58 interconnected tools spanning navigation, productivity, charging, and vehicle control. Beyond standard task completion, CAR-bench introduces Hallucination tasks that test agents' limit-awareness under missing tools or information, and Disambiguation tasks that require resolving uncertainty through clarification or internal information gathering. Baseline results reveal large gaps between occasional and consistent success on all task types. Even frontier reasoning LLMs achieve less than 50% consistent pass rate on Disambiguation tasks due to premature actions, and frequently violate policies or fabricate information to satisfy user requests in Hallucination tasks, underscoring the need for more reliable and self-aware LLM agents in real-world settings.

</details>


### [125] [Optimizing Agentic Workflows using Meta-tools](https://arxiv.org/abs/2601.22037)
*Sami Abuzakuk,Anne-Marie Kermarrec,Rishi Sharma,Rasmus Moorits Veski,Martijn de Vos*

Main category: cs.AI

TL;DR: AWO框架通过分析工作流轨迹，识别冗余工具执行模式，将其转化为元工具，减少LLM调用次数并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 代理AI工作流通常需要多次迭代推理和工具调用，导致操作成本高、端到端延迟大以及因幻觉导致的失败。需要优化代理工作流的效率和鲁棒性。

Method: AWO框架分析现有工作流轨迹，发现重复的工具调用序列，将其转换为元工具（确定性复合工具），将多个代理动作捆绑到单个调用中，绕过不必要的中间LLM推理步骤。

Result: 在两个代理AI基准测试中，AWO将LLM调用次数减少高达11.9%，同时将任务成功率提高高达4.2个百分点。

Conclusion: AWO通过优化代理工作流中的冗余工具执行模式，显著提高了代理AI系统的效率和鲁棒性。

Abstract: Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.

</details>


### [126] [Exploring Reasoning Reward Model for Agents](https://arxiv.org/abs/2601.22154)
*Kaixuan Fan,Kaituo Feng,Manyuan Zhang,Tianshuo Peng,Zhixun Li,Yilei Jiang,Shuang Chen,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.AI

TL;DR: 本文提出Agent-RRM（智能体推理奖励模型），通过提供结构化反馈（推理轨迹、针对性批判、总体评分）来改进智能体强化学习，解决了传统稀疏奖励无法区分中间推理质量的问题。


<details>
  <summary>Details</summary>
Motivation: 当前智能体强化学习方法主要依赖稀疏的结果奖励进行训练，这种反馈无法区分中间推理过程的质量，导致训练效果不佳。需要更细粒度的反馈机制来指导智能体学习。

Method: 提出Agent-RRM多面奖励模型，生成结构化反馈：1）显式推理轨迹，2）针对性批判（突出推理缺陷），3）总体评分。基于这些信号，研究了三种集成策略：Reagent-C（文本增强细化）、Reagent-R（奖励增强指导）和Reagent-U（统一反馈集成）。

Result: 在12个多样化基准测试中，Reagent-U取得了显著性能提升：在GAIA上达到43.7%，在WebWalkerQA上达到46.2%，验证了推理奖励模型和训练方案的有效性。

Conclusion: Agent-RRM通过提供结构化反馈显著改进了智能体强化学习的训练效果，Reagent-U集成策略表现最佳，为智能体推理能力的提升提供了有效解决方案。

Abstract: Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.

</details>
