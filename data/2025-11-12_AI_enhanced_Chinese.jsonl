{"id": "2511.07584", "categories": ["cs.SE", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07584", "abs": "https://arxiv.org/abs/2511.07584", "authors": ["Wuyang Zhang", "Chenkai Zhang", "Zhen Luo", "Jianming Ma", "Wangming Yuan", "Chuqiao Gu", "Chenwei Feng"], "title": "SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction", "comment": null, "summary": "Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \\textit{logical hallucination} (incorrect control/data-flow reasoning) and \\textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.\n  This paper presents \\textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\\% precision versus 51\\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|\u0394R| \\cdot \\log n)$ time while maintaining semantic equivalence.", "AI": {"tldr": "SemanticForge\u901a\u8fc7\u56db\u79cd\u7b97\u6cd5\u521b\u65b0\u89e3\u51b3LLM\u4ee3\u7801\u751f\u6210\u7684\u7cfb\u7edf\u9519\u8bef\uff1a\u903b\u8f91\u5e7b\u89c9\u548c\u6a21\u5f0f\u5e7b\u89c9\u3002\u5b83\u7ed3\u5408\u9759\u6001-\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u3001\u795e\u7ecf\u67e5\u8be2\u751f\u6210\u3001\u96c6\u6210SMT\u6c42\u89e3\u7684\u675f\u641c\u7d22\u4ee5\u53ca\u589e\u91cf\u7ef4\u62a4\u7b97\u6cd5\uff0c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u7684\u4ee3\u7801\u751f\u6210\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u7279\u522b\u662f\u903b\u8f91\u5e7b\u89c9\uff08\u63a7\u5236/\u6570\u636e\u6d41\u63a8\u7406\u9519\u8bef\uff09\u548c\u6a21\u5f0f\u5e7b\u89c9\uff08\u7c7b\u578b\u4e0d\u5339\u914d\u3001\u7b7e\u540d\u8fdd\u89c4\u7b49\uff09\uff0c\u8fd9\u4e9b\u9519\u8bef\u6e90\u4e8e\u7f3a\u4e4f\u53ef\u67e5\u8be2\u7684\u4ed3\u5e93\u7ea7\u8bed\u4e49\u8868\u793a\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u6838\u5fc3\u7b97\u6cd5\uff1a1\uff09\u81ea\u52a8\u534f\u8c03\u9759\u6001-\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff1b2\uff09\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u7ed3\u6784\u5316\u56fe\u67e5\u8be2\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff1b3\uff09\u96c6\u6210SMT\u6c42\u89e3\u7684\u675f\u641c\u7d22\u7b97\u6cd5\uff1b4\uff09\u589e\u91cf\u7ef4\u62a4\u7b97\u6cd5\u4ee5\u9ad8\u6548\u66f4\u65b0\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u795e\u7ecf\u67e5\u8be2\u751f\u6210\u65b9\u6cd5\u8fbe\u523073%\u7684\u7cbe\u786e\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u768451%\uff1b\u589e\u91cf\u7ef4\u62a4\u7b97\u6cd5\u5728O(|\u0394R|\u00b7log n)\u65f6\u95f4\u5185\u66f4\u65b0\u77e5\u8bc6\u56fe\u8c31\u3002", "conclusion": "SemanticForge\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u9650\u5236\u3002", "topic": "code agent"}}
{"id": "2511.07483", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07483", "abs": "https://arxiv.org/abs/2511.07483", "authors": ["Qianxi He", "Qingyu Ren", "Shanzhe Lei", "Xuhong Wang", "Yingchun Wang"], "title": "Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5956\u52b1\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u589e\u5f3aSTEM\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u60e9\u7f5a\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u6b63\u786e\u7b54\u6848\u6765\u4fc3\u8fdb\u66f4\u7a33\u5065\u548c\u903b\u8f91\u4e00\u81f4\u7684\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u5728\u5c0f\u578b\u6a21\u578b\u4e0a\u7ecf\u5e38\u5bfc\u81f4\u63a8\u7406\u94fe\u8d28\u91cf\u5dee\u6216\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8d44\u6e90\u6709\u9650\u7ec4\u7ec7\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e0a\u8fdb\u884c\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "method": "\u5f00\u53d1\u4e86\u7f6e\u4fe1\u5ea6\u5956\u52b1\u6a21\u578b\uff0c\u4e0d\u4ec5\u60e9\u7f5a\u9519\u8bef\u7b54\u6848\uff0c\u8fd8\u60e9\u7f5a\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u6b63\u786e\u7b54\u6848\uff1b\u901a\u8fc7\u9759\u6001\u8bc4\u4f30\u3001Best-of-N\u63a8\u7406\u6d4b\u8bd5\u548c\u57fa\u4e8ePPO\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aSTEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u5956\u52b1\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347STEM\u63a8\u7406\u80fd\u529b\uff0c\u4fc3\u8fdb\u66f4\u7a33\u5065\u548c\u903b\u8f91\u4e00\u81f4\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07612", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07612", "abs": "https://arxiv.org/abs/2511.07612", "authors": ["Samuel W. Flint", "Jigyasa Chauhan", "Niloofar Mansoor", "Bonita Sharif", "Robert Dyer"], "title": "An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms", "comment": null, "summary": "Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u5b9e\u9a8c\u63a2\u7d22Python\u591a\u8303\u5f0f\u8bed\u8a00\u7279\u5f81\u5bf9\u4ee3\u7801\u7406\u89e3\u548c\u8c03\u8bd5\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u5bf9\u51fd\u6570\u5f0f\u548c\u8fc7\u7a0b\u5f0f\u8303\u5f0f\u5b58\u5728\u6df7\u6dc6\uff0c\u51fd\u6570\u5f0f\u4ee3\u7801\u5b8c\u6210\u65f6\u95f4\u6700\u957f\uff0c\u4f46\u8303\u5f0f\u53d8\u5316\u4e0d\u5f71\u54cd\u8c03\u8bd5\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u5982Python\u652f\u6301\u591a\u79cd\u8303\u5f0f\uff08\u9762\u5411\u5bf9\u8c61\u3001\u8fc7\u7a0b\u5f0f\u3001\u51fd\u6570\u5f0f\uff09\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u54ea\u4e9b\u8303\u5f0f\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u5f71\u54cd\u4ee3\u7801\u7406\u89e3\u548c\u8c03\u8bd5\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u773c\u52a8\u8ffd\u8e2a\u5b9e\u8bc1\u7814\u7a76\uff0c\u62db\u52df29\u540d\u5f00\u53d1\u8005\uff08\u4e3b\u8981\u662f\u5b66\u751f\uff09\uff0c\u8fdb\u884c4\u4e2a\u5206\u7c7b\u4efb\u52a1\u548c4\u4e2a\u8c03\u8bd5\u4efb\u52a1\uff0c\u8bb0\u5f55\u773c\u52a8\u6570\u636e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5f00\u53d1\u8005\u5bf9\u51fd\u6570\u5f0f\u548c\u8fc7\u7a0b\u5f0f\u8303\u5f0f\u5206\u7c7b\u5b58\u5728\u6df7\u6dc6\uff1b2\uff09\u51fd\u6570\u5f0f\u4ee3\u7801\u5b8c\u6210\u65f6\u95f4\u6700\u957f\uff1b3\uff09\u8303\u5f0f\u53d8\u5316\u4e0d\u5f71\u54cd\u8c03\u8bd5\u80fd\u529b\uff0c\u4f46\u5f00\u53d1\u8005\u5bf9\u51fd\u6570\u5f0f\u4ee3\u7801\u81ea\u4fe1\u5ea6\u8f83\u4f4e\uff1b4\uff09\u8c03\u8bd5\u65f6\u9605\u8bfb\u6a21\u5f0f\u6709\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u51fd\u6570\u5f0f\u4ee3\u7801\u4e2d\u3002", "conclusion": "\u591a\u8303\u5f0f\u4ee3\u7801\u4e2d\uff0c\u51fd\u6570\u5f0f\u8303\u5f0f\u7279\u5f81\u5bf9\u5f00\u53d1\u8005\u7406\u89e3\u548c\u5206\u7c7b\u5f71\u54cd\u6700\u5927\uff0c\u4f46\u5b9e\u9645\u8c03\u8bd5\u80fd\u529b\u4e0d\u53d7\u8303\u5f0f\u5f71\u54cd\uff0c\u5f00\u53d1\u8005\u9605\u8bfb\u6a21\u5f0f\u5728\u4e0d\u540c\u8303\u5f0f\u4ee3\u7801\u4e2d\u5b58\u5728\u5dee\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2511.07498", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07498", "abs": "https://arxiv.org/abs/2511.07498", "authors": ["Xin Liu", "Qiyang Song", "Qihang Zhou", "Haichao Du", "Shaowen Xu", "Wenbo Jiang", "Weijuan Zhang", "Xiaoqi Jia"], "title": "Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models", "comment": "Accepted by AAAI-2026", "summary": "Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.", "AI": {"tldr": "\u63d0\u51fa\u4e86LAHIS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u8bc6\u522b\u591a\u8bed\u8a00\u80fd\u529b\u4e2d\u7684\u6ce8\u610f\u529b\u5934\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u4e86\u8bed\u8a00\u7279\u5b9a\u548c\u8bed\u8a00\u901a\u7528\u5934\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5728LLMs\u591a\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u8d21\u732e\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u591a\u8bed\u8a00\u80fd\u529b\u3002", "method": "\u63d0\u51faLAHIS\u65b9\u6cd5\u8bc6\u522b\u6ce8\u610f\u529b\u5934\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u8bed\u8a00\u7279\u5b9a\u548c\u8bed\u8a00\u901a\u7528\u5934\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f6f\u5934\u63a9\u7801\u9002\u914d\u3002", "result": "\u5728Aya-23-8B\u3001Llama-3.2-3B\u548cMistral-7B-v0.1\u4e0a\u9a8c\u8bc1\u4e86LAHIS\u7684\u6709\u6548\u6027\uff0c\u4ec5\u970020\u4e2a\u53ef\u8c03\u53c2\u6570\u5373\u53ef\u63d0\u5347XQuAD\u51c6\u786e\u7387\u3002", "conclusion": "\u4ece\u591a\u5934\u81ea\u6ce8\u610f\u529b\u89d2\u5ea6\u589e\u5f3a\u4e86LLMs\u7684\u53ef\u89e3\u91ca\u6027\u548c\u591a\u8bed\u8a00\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.07568", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07568", "abs": "https://arxiv.org/abs/2511.07568", "authors": ["Vincent Hsiao", "Mark Roberts", "Leslie Smith"], "title": "Procedural Knowledge Improves Agentic LLM Workflows", "comment": null, "summary": "Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5206\u5c42\u4efb\u52a1\u7f51\u7edc\uff08HTN\uff09\u4f5c\u4e3a\u7a0b\u5e8f\u6027\u77e5\u8bc6\u6765\u63d0\u5347LLM\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u624b\u5de5\u7f16\u7801\u7684HTN\u80fd\u663e\u8457\u63d0\u5347LLM\u8868\u73b0\uff0c\u751a\u81f3\u8ba9\u8f83\u5c0f\u6a21\u578b\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002", "motivation": "LLM\u5728\u6267\u884c\u4ee3\u7406\u4efb\u52a1\u65f6\u901a\u5e38\u9700\u8981\u5927\u91cf\u5de5\u5177\u652f\u6301\u3001\u63d0\u793a\u5de5\u7a0b\u6216\u5fae\u8c03\uff0c\u800c\u9886\u57df\u76f8\u5173\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u89c4\u5212\u6548\u7387\uff0c\u4f46\u76ee\u524d\u5f88\u5c11\u6709\u7814\u7a76\u8bc4\u4f30\u5176\u5728\u9700\u8981\u9690\u5f0f\u89c4\u5212\u7684\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u5f62\u5f0f\u5316\u3001\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u5229\u7528\u5206\u5c42\u4efb\u52a1\u7f51\u7edc\uff08HTN\uff09\u4f5c\u4e3a\u7a0b\u5e8f\u6027\u77e5\u8bc6\u7684\u4ee3\u7406LLM\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u624b\u5de5\u7f16\u7801HTN\u548cLLM\u751f\u6210\u7684HTN\u4e24\u79cd\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u624b\u5de5\u7f16\u7801\u7684HTN\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f7f\u7528HTN\u53ef\u4ee5\u8ba920b\u621670b\u53c2\u6570\u7684LLM\u8d85\u8d8a120b\u53c2\u6570LLM\u57fa\u7ebf\uff1bLLM\u751f\u6210\u7684HTN\u4e5f\u80fd\u6539\u5584\u6027\u80fd\uff0c\u4f46\u6548\u679c\u8f83\u5f31\u3002", "conclusion": "\u5229\u7528\u4eba\u7c7b\u3001\u6587\u6863\u6216LLM\u7684\u4e13\u4e1a\u77e5\u8bc6\u6765\u7b56\u5212\u7a0b\u5e8f\u6027\u77e5\u8bc6\u5c06\u6210\u4e3a\u6539\u8fdbLLM\u5de5\u4f5c\u6d41\u7a0b\u7684\u53e6\u4e00\u4e2a\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2511.07645", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.07645", "abs": "https://arxiv.org/abs/2511.07645", "authors": ["Tyler Slater"], "title": "A Self-Improving Architecture for Dynamic Safety in Large Language Models", "comment": "Under review at the journal Information and Software Technology (Special Issue on Software Architecture for AI-Driven Systems)", "summary": "Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.\n  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.\n  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.\n  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.\n  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u6539\u8fdb\u5b89\u5168\u6846\u67b6(SISF)\uff0c\u8fd9\u662f\u4e00\u4e2a\u8fd0\u884c\u65f6\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u53cd\u9988\u5faa\u73af\u8ba9AI\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u6301\u7eed\u5730\u8c03\u6574\u5176\u5b89\u5168\u534f\u8bae\uff0c\u5c06\u653b\u51fb\u6210\u529f\u7387\u4ece100%\u964d\u4f4e\u523045.58%\uff0c\u540c\u65f6\u4fdd\u63010%\u7684\u8bef\u62a5\u7387\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u67b6\u6784\u6a21\u5f0f\u662f\u9759\u6001\u7684\uff0c\u800c\u5f53\u524d\u7684\u5b89\u5168\u4fdd\u8bc1\u65b9\u6cd5\u4e0d\u53ef\u6269\u5c55\uff0c\u4f7f\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u65b0\u578b\u5bf9\u6297\u6027\u5a01\u80c1\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u81ea\u4e3b\u9002\u5e94\u5b89\u5168\u534f\u8bae\u7684AI\u9a71\u52a8\u7cfb\u7edf\u3002", "method": "SISF\u67b6\u6784\u5c06\u672a\u53d7\u4fdd\u62a4\u7684\u57fa\u5ea7LLM\u4e0e\u52a8\u6001\u53cd\u9988\u5faa\u73af\u8026\u5408\uff0c\u5305\u62ecAI\u88c1\u51b3\u5668\u7528\u4e8e\u8fdd\u89c4\u68c0\u6d4b\u548c\u653f\u7b56\u5408\u6210\u6a21\u5757\u7528\u4e8e\u81ea\u4e3b\u751f\u6210\u65b0\u7684\u5b89\u5168\u7b56\u7565\u3002", "result": "\u5728AdvBench\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cSISF\u4ece\u96f6\u7b56\u7565\u5f00\u59cb\uff0c\u68c0\u6d4b\u5230237\u6b21\u8fdd\u89c4\uff0c\u81ea\u4e3b\u5408\u6210\u4e86234\u4e2a\u65b0\u7b56\u7565\uff0c\u5c06\u653b\u51fb\u6210\u529f\u7387\u4ece100%\u964d\u81f345.58%\uff0c\u5728\u826f\u6027\u63d0\u793a\u4e0a\u4fdd\u63010%\u8bef\u62a5\u7387\u3002", "conclusion": "\u57fa\u4e8e\u81ea\u9002\u5e94\u539f\u5219\u7684AI\u5b89\u5168\u67b6\u6784\u65b9\u6cd5\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\u7b56\u7565\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u6709\u5f39\u6027\u548c\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u9645\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07581", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.07581", "abs": "https://arxiv.org/abs/2511.07581", "authors": ["Supriti Vijay", "Aman Priyanshu", "Anu Vellore", "Baturay Saglam", "Amin Karbasi"], "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models", "comment": "37 images, 7 figures, and 15 tables", "summary": "Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.", "AI": {"tldr": "Orion\u8bad\u7ec3\u6846\u67b6\u4f7f\u7d27\u51d1\u6a21\u578b(350M-1.2B\u53c2\u6570)\u80fd\u591f\u901a\u8fc7\u5b66\u4e60\u7684\u641c\u7d22\u7b56\u7565\u6267\u884c\u8fed\u4ee3\u68c0\u7d22\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u8868\u660e\u68c0\u7d22\u6027\u80fd\u6765\u81ea\u5b66\u4e60\u7b56\u7565\u800c\u975e\u6a21\u578b\u89c4\u6a21\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u795e\u7ecf\u68c0\u7d22\u5668\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\uff0cLLM\u6210\u672c\u8fc7\u9ad8\uff0c\u67e5\u8be2\u91cd\u5199\u6216\u5206\u89e3\u4ec5\u9650\u4e8e\u9759\u6001\u8f6c\u6362\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u67e5\u8be2\u6240\u9700\u7684\u8fed\u4ee3\u63a2\u7d22\u3001\u53cd\u9988\u548c\u4fee\u8ba2\u52a8\u6001\u3002", "method": "\u7ed3\u5408\uff1a(1)\u5408\u6210\u8f68\u8ff9\u751f\u6210\u548c\u76d1\u7763\u5fae\u8c03\u4ee5\u9f13\u52b1\u591a\u6837\u5316\u63a2\u7d22\u6a21\u5f0f\uff1b(2)\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u6709\u6548\u67e5\u8be2\u7ec6\u5316\u548c\u56de\u6eaf\u884c\u4e3a\uff1b(3)\u63a8\u7406\u65f6\u6ce2\u675f\u641c\u7d22\u7b97\u6cd5\u5229\u7528RL\u671f\u95f4\u5b66\u5230\u7684\u81ea\u53cd\u80fd\u529b\u3002", "result": "1.2B\u6a21\u578b\u5728SciFact\u4e0a\u8fbe\u523077.6%\u6210\u529f\u7387(\u5148\u524d72.6%)\uff0cBRIGHT\u4e0a25.2%(\u5148\u524d22.1%)\uff0cNFCorpus\u4e0a63.2%(\u5148\u524d57.8%)\uff0c\u5728FEVER\u3001HotpotQA\u548cMSMarco\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u4e2d\u7684\u4e94\u4e2a\u8d85\u8d8a200-400\u500d\u5927\u7684\u68c0\u7d22\u5668\u3002", "conclusion": "\u68c0\u7d22\u6027\u80fd\u53ef\u4ee5\u4ece\u5b66\u4e60\u7b56\u7565\u4e2d\u6d8c\u73b0\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u578b\u89c4\u6a21\uff0c\u5f53\u6a21\u578b\u88ab\u8bad\u7ec3\u53bb\u641c\u7d22\u3001\u53cd\u601d\u548c\u4fee\u8ba2\u65f6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07698", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07698", "abs": "https://arxiv.org/abs/2511.07698", "authors": ["Mohammadjavad Mehditabar", "Saurabhsingh Rajput", "Antonio Mastropaolo", "Tushar Sharma"], "title": "Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency", "comment": null, "summary": "The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.", "AI": {"tldr": "\u63d0\u51fa\u4e86BRACE\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7edf\u4e00\u7684\u80fd\u6548\u548c\u529f\u80fd\u6b63\u786e\u6027\u5c3a\u5ea6\u4e0a\u8bc4\u4f30\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u542bCIRC\u548cOTER\u4e24\u79cd\u8bc4\u7ea7\u65b9\u6cd5\uff0c\u5bf922\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6846\u67b6\u6765\u8bc4\u4f30\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027-\u80fd\u8017\u6743\u8861\uff0c\u9700\u8981\u5e73\u8861AI\u6280\u672f\u73af\u5883\u5f71\u54cd\u7684\u53ef\u6301\u7eed\u6027\u4e0e\u529f\u80fd\u6b63\u786e\u6027\u3002", "method": "\u5f00\u53d1BRACE\u6846\u67b6\uff0c\u63d0\u51fa\u4e24\u79cd\u8bc4\u7ea7\u65b9\u6cd5\uff1aCIRC\uff08\u57fa\u4e8e\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u7684\u786e\u5b9a\u6027\u6392\u540d\uff09\u548cOTER\uff08\u8d8b\u52bf\u611f\u77e5\u8bc4\u4f30\uff09\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u5bf922\u4e2a\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u5728\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u56e0\u4e3a\u4e0d\u9700\u8981\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u8f93\u51fa\uff1b\u6a21\u578b\u5927\u5c0f\u5bf9\u8bc4\u7ea7\u5f71\u54cd\u4e0d\u663e\u8457\uff0c\u53c2\u6570\u5229\u7528\u6548\u7387\u66f4\u91cd\u8981\u3002", "conclusion": "BRACE\u6846\u67b6\u4f7f\u4ece\u4e1a\u8005\u80fd\u591f\u57fa\u4e8e\u8bc1\u636e\u9009\u62e9\u6a21\u578b\uff0c\u5e73\u8861\u53ef\u6301\u7eed\u6027\u4e0e\u4efb\u52a1\u9700\u6c42\uff0c\u6839\u636e\u90e8\u7f72\u4f18\u5148\u7ea7\u9009\u62e9CIRC\u6216OTER\u8bc4\u7ea7\u65b9\u6cd5\u3002", "topic": "swe benchmark"}}
{"id": "2511.07669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07669", "abs": "https://arxiv.org/abs/2511.07669", "authors": ["Alejandro R. Jadad"], "title": "Making LLMs Reliable When It Matters Most: A Five-Layer Architecture for High-Stakes Decisions", "comment": "24 pages, 1 figure, 2 tables", "summary": "Current large language models (LLMs) excel in verifiable domains where outputs can be checked before action but prove less reliable for high-stakes strategic decisions with uncertain outcomes. This gap, driven by mutually reinforcing cognitive biases in both humans and artificial intelligence (AI) systems, threatens the defensibility of valuations and sustainability of investments in the sector.\n  This report describes a framework emerging from systematic qualitative assessment across 7 frontier-grade LLMs and 3 market-facing venture vignettes under time pressure. Detailed prompting specifying decision partnership and explicitly instructing avoidance of sycophancy, confabulation, solution drift, and nihilism achieved initial partnership state but failed to maintain it under operational pressure. Sustaining protective partnership state required an emergent 7-stage calibration sequence, built upon a 4-stage initialization process, within a 5-layer protection architecture enabling bias self-monitoring, human-AI adversarial challenge, partnership state verification, performance degradation detection, and stakeholder protection.\n  Three discoveries resulted: partnership state is achievable through ordered calibration but requires emergent maintenance protocols; reliability degrades when architectural drift and context exhaustion align; and dissolution discipline prevents costly pursuit of fundamentally wrong directions. Cross-model validation revealed systematic performance differences across LLM architectures.\n  This approach demonstrates that human-AI teams can achieve cognitive partnership capable of preventing avoidable regret in high-stakes decisions, addressing return-on-investment expectations that depend on AI systems supporting consequential decision-making without introducing preventable cognitive traps when verification arrives too late.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc77\u9636\u6bb5\u6821\u51c6\u5e8f\u5217\u548c5\u5c42\u4fdd\u62a4\u67b6\u6784\uff0c\u4f7f\u4eba\u7c7b-AI\u56e2\u961f\u5728\u9ad8\u98ce\u9669\u6218\u7565\u51b3\u7b56\u4e2d\u5b9e\u73b0\u8ba4\u77e5\u4f19\u4f34\u5173\u7cfb\uff0c\u907f\u514d\u53ef\u9884\u9632\u7684\u8ba4\u77e5\u9677\u9631\u548c\u9057\u61be\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9a8c\u8bc1\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u6218\u7565\u51b3\u7b56\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\uff0c\u5b58\u5728\u76f8\u4e92\u5f3a\u5316\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u5a01\u80c1\u6295\u8d44\u51b3\u7b56\u7684\u53ef\u6301\u7eed\u6027\u3002", "method": "\u5bf97\u4e2a\u524d\u6cbf\u7ea7LLM\u548c3\u4e2a\u5e02\u573a\u98ce\u9669\u6848\u4f8b\u8fdb\u884c\u7cfb\u7edf\u5b9a\u6027\u8bc4\u4f30\uff0c\u5f00\u53d1\u5305\u542b7\u9636\u6bb5\u6821\u51c6\u5e8f\u5217\u30014\u9636\u6bb5\u521d\u59cb\u5316\u8fc7\u7a0b\u548c5\u5c42\u4fdd\u62a4\u67b6\u6784\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u504f\u89c1\u81ea\u6211\u76d1\u63a7\u3001\u4eba\u673a\u5bf9\u6297\u6311\u6218\u7b49\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u53ef\u7ef4\u6301\u7684\u4f19\u4f34\u5173\u7cfb\u72b6\u6001\uff0c\u53d1\u73b0\u53ef\u9760\u6027\u5728\u67b6\u6784\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u8017\u5c3d\u65f6\u4e0b\u964d\uff0c\u8de8\u6a21\u578b\u9a8c\u8bc1\u663e\u793a\u4e0d\u540cLLM\u67b6\u6784\u5b58\u5728\u7cfb\u7edf\u6027\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u4eba\u7c7b-AI\u56e2\u961f\u53ef\u4ee5\u901a\u8fc7\u6709\u5e8f\u6821\u51c6\u5b9e\u73b0\u8ba4\u77e5\u4f19\u4f34\u5173\u7cfb\uff0c\u9632\u6b62\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u53ef\u907f\u514d\u9057\u61be\uff0c\u6ee1\u8db3\u4f9d\u8d56AI\u7cfb\u7edf\u652f\u6301\u91cd\u5927\u51b3\u7b56\u7684\u6295\u8d44\u56de\u62a5\u671f\u671b\u3002", "topic": "agent analysis"}}
{"id": "2511.07678", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07678", "abs": "https://arxiv.org/abs/2511.07678", "authors": ["Rohan Alur", "Bradly C. Stadie", "Daniel Kang", "Ryan Chen", "Matt McManus", "Michael Rickert", "Tyler Lee", "Michael Federici", "Richard Zhu", "Dennis Fogerty", "Hayley Williamson", "Nina Lozinski", "Aaron Linsky", "Jasjeet S. Sekhon"], "title": "AIA Forecaster: Technical Report", "comment": null, "summary": "This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.", "AI": {"tldr": "AIA Forecaster\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5224\u65ad\u6027\u9884\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u667a\u80fd\u641c\u7d22\u3001\u76d1\u7763\u4ee3\u7406\u548c\u7edf\u8ba1\u6821\u51c6\u6280\u672f\uff0c\u5728ForecastBench\u4e0a\u8fbe\u5230\u4eba\u7c7b\u8d85\u7ea7\u9884\u6d4b\u8005\u7684\u6c34\u5e73\uff0c\u5e76\u5728\u9884\u6d4b\u5e02\u573a\u57fa\u51c6\u4e0a\u63d0\u4f9b\u589e\u91cf\u4fe1\u606f\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5229\u7528\u975e\u7ed3\u6784\u5316\u6570\u636e\u8fdb\u884c\u4e13\u5bb6\u7ea7\u9884\u6d4b\u7684AI\u7cfb\u7edf\uff0c\u89e3\u51b3LLM\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u53ef\u6269\u5c55\u7684AI\u9884\u6d4b\u65b0\u6807\u51c6\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\uff1a\u57fa\u4e8e\u4ee3\u7406\u7684\u9ad8\u8d28\u91cf\u65b0\u95fb\u641c\u7d22\u3001\u534f\u8c03\u4e0d\u540c\u9884\u6d4b\u7684\u76d1\u7763\u4ee3\u7406\u3001\u4ee5\u53ca\u5bf9\u6297LLM\u884c\u4e3a\u504f\u5dee\u7684\u7edf\u8ba1\u6821\u51c6\u6280\u672f\u3002", "result": "\u5728ForecastBench\u4e0a\u8868\u73b0\u4e0e\u4eba\u7c7b\u8d85\u7ea7\u9884\u6d4b\u8005\u76f8\u5f53\uff0c\u8d85\u8d8a\u73b0\u6709LLM\u57fa\u7ebf\uff1b\u5728\u9884\u6d4b\u5e02\u573a\u57fa\u51c6\u4e0a\uff0c\u4e0e\u5e02\u573a\u5171\u8bc6\u96c6\u6210\u540e\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u5e02\u573a\u5171\u8bc6\u3002", "conclusion": "\u5efa\u7acb\u4e86AI\u9884\u6d4b\u7684\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u8f6c\u79fb\u7684\u5efa\u8bae\uff0c\u9996\u6b21\u9a8c\u8bc1\u4e86\u5927\u89c4\u6a21\u4e13\u5bb6\u7ea7\u9884\u6d4b\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2511.07865", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.07865", "abs": "https://arxiv.org/abs/2511.07865", "authors": ["Daisuke Kikuta", "Hiroki Ikeuchi", "Kengo Tajiri"], "title": "LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost", "comment": "Accepted at ASE 2025 NIER Track. The code is available at https://github.com/ntt-dkiku/chaos-eater", "summary": "Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.", "AI": {"tldr": "ChaosEater\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u6df7\u6c8c\u5de5\u7a0b\u7cfb\u7edf\uff0c\u9488\u5bf9Kubernetes\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\u81ea\u52a8\u5b8c\u6210\u6574\u4e2a\u6df7\u6c8c\u5de5\u7a0b\u5468\u671f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u6df7\u6c8c\u5de5\u7a0b\u4e2d\u5b9e\u9a8c\u89c4\u5212\u548c\u7cfb\u7edf\u6539\u8fdb\u8fc7\u7a0b\u4ecd\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\uff0c\u8fd9\u4e9b\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u9700\u8981\u591a\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u6df7\u6c8c\u5de5\u7a0b\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51faChaosEater\u7cfb\u7edf\uff0c\u6309\u7167\u7cfb\u7edf\u5316\u7684\u6df7\u6c8c\u5de5\u7a0b\u5468\u671f\u9884\u5b9a\u4e49\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5c06\u5de5\u4f5c\u6d41\u4e2d\u7684\u7ec6\u5206\u8fc7\u7a0b\u5206\u914d\u7ed9LLM\u5b8c\u6210\uff0c\u5305\u62ec\u9700\u6c42\u5b9a\u4e49\u3001\u4ee3\u7801\u751f\u6210\u3001\u6d4b\u8bd5\u548c\u8c03\u8bd5\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002", "result": "\u5728\u5c0f\u578b\u548c\u5927\u578bKubernetes\u7cfb\u7edf\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u4ee5\u663e\u8457\u4f4e\u7684\u65f6\u95f4\u548c\u91d1\u94b1\u6210\u672c\u6301\u7eed\u5b8c\u6210\u5408\u7406\u7684\u6df7\u6c8c\u5de5\u7a0b\u5468\u671f\uff0c\u5176\u5468\u671f\u8d28\u91cf\u5f97\u5230\u4e86\u4eba\u7c7b\u5de5\u7a0b\u5e08\u548cLLM\u7684\u9a8c\u8bc1\u3002", "conclusion": "ChaosEater\u8bc1\u660e\u4e86\u4f7f\u7528LLM\u81ea\u52a8\u5316\u6574\u4e2a\u6df7\u6c8c\u5de5\u7a0b\u5468\u671f\u7684\u53ef\u884c\u6027\uff0c\u4f7f\u4efb\u4f55\u4eba\u90fd\u80fd\u4ee5\u4f4e\u6210\u672c\u6784\u5efa\u5f39\u6027\u7cfb\u7edf\u3002", "topic": "swe application"}}
{"id": "2511.07486", "categories": ["cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.07486", "abs": "https://arxiv.org/abs/2511.07486", "authors": ["Sourav Ganguly", "Arnob Ghosh"], "title": "Provably Efficient Sample Complexity for Robust CMDP", "comment": null, "summary": "We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\\mathcal{\\tilde{O}}(|S||A|H^5/\u03b5^2)$ achieving at most $\u03b5$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5b89\u5168\u7ea6\u675f\u4e0b\u5b66\u4e60\u7b56\u7565\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u5177\u6709\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u7684\u9c81\u68d2\u7ea6\u675fMDP\u7b97\u6cd5RCVI\uff0c\u89e3\u51b3\u4e86\u5f53\u771f\u5b9e\u73af\u5883\u4e0e\u6a21\u62df\u5668\u4e0d\u540c\u65f6\u7684\u9c81\u68d2\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u867d\u7136\u4e3aRCMDPs\u5efa\u7acb\u4e86\u6709\u9650\u65f6\u95f4\u8fed\u4ee3\u590d\u6742\u5ea6\u4fdd\u8bc1\uff0c\u4f46\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u89e3\u51b3\u9a6c\u5c14\u53ef\u592b\u7b56\u7565\u5728\u77e9\u5f62\u4e0d\u786e\u5b9a\u6027\u96c6\u4e0b\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u589e\u5f3a\u72b6\u6001\u7a7a\u95f4\uff0c\u5c06\u5269\u4f59\u6548\u7528\u9884\u7b97\u7eb3\u5165\u72b6\u6001\u8868\u793a\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u9c81\u68d2\u7ea6\u675f\u503c\u8fed\u4ee3(RCVI)\u7b97\u6cd5\uff0c\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "result": "RCVI\u7b97\u6cd5\u5b9e\u73b0\u4e86$\\mathcal{\\tilde{O}}(|S||A|H^5/\u03b5^2)$\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5728\u6700\u591a$\u03b5$\u8fdd\u53cd\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u8fd9\u662fRCMDP\u7684\u7b2c\u4e00\u4e2a\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9c81\u68d2\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u6837\u672c\u590d\u6742\u5ea6\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07685", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07685", "abs": "https://arxiv.org/abs/2511.07685", "authors": ["Manasi Sharma", "Chen Bo Calvin Zhang", "Chaithanya Bandi", "Clinton Wang", "Ankit Aich", "Huy Nghiem", "Tahseen Rabbani", "Ye Htet", "Brian Jang", "Sumana Basu", "Aishwarya Balwani", "Denis Peskoff", "Marcos Ayestaran", "Sean M. Hendryx", "Brad Kenstler", "Bing Liu"], "title": "ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents", "comment": "27 pages, 21 figures, pre-print", "summary": "Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86ResearchRubrics\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5305\u542b2800+\u5c0f\u65f6\u4eba\u5de5\u6784\u5efa\u76842500+\u4e13\u5bb6\u8bc4\u5206\u6807\u51c6\uff0c\u8bc4\u4f30\u4e8b\u5b9e\u57fa\u7840\u3001\u63a8\u7406\u5408\u7406\u6027\u548c\u6e05\u6670\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u9700\u8981\u591a\u6b65\u63a8\u7406\u3001\u8de8\u6587\u6863\u7efc\u5408\u548c\u751f\u6210\u8bc1\u636e\u652f\u6301\u7684\u957f\u7bc7\u56de\u7b54\uff0c\u4f46\u76ee\u524d\u8bc4\u4f30\u56f0\u96be\uff0c\u56e0\u4e3a\u56de\u7b54\u5197\u957f\u591a\u6837\u3001\u5b58\u5728\u591a\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\u4e14\u4f9d\u8d56\u52a8\u6001\u4fe1\u606f\u6e90\u3002", "method": "\u6784\u5efa\u6807\u51c6\u5316\u7684ResearchRubrics\u57fa\u51c6\uff0c\u5305\u542b\u9886\u57df\u591a\u6837\u7684\u63d0\u793a\u548c\u7cbe\u7ec6\u8bc4\u5206\u6807\u51c6\uff1b\u63d0\u51fa\u4e09\u7ef4\u590d\u6742\u6027\u6846\u67b6\uff08\u6982\u5ff5\u5e7f\u5ea6\u3001\u903b\u8f91\u5d4c\u5957\u3001\u63a2\u7d22\u6027\uff09\uff1b\u5f00\u53d1\u4eba\u5de5\u548c\u6a21\u578b\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u8bc4\u4f30\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u9886\u5148\u7684\u4ee3\u7406\u5982Gemini\u548cOpenAI\u7684DR\u7cfb\u7edf\uff0c\u5e73\u5747\u7b26\u5408\u5ea6\u4e5f\u4f4e\u4e8e68%\uff0c\u4e3b\u8981\u95ee\u9898\u662f\u9057\u6f0f\u9690\u5f0f\u4e0a\u4e0b\u6587\u548c\u5bf9\u68c0\u7d22\u4fe1\u606f\u63a8\u7406\u4e0d\u8db3\u3002", "conclusion": "\u9700\u8981\u5f3a\u5927\u53ef\u6269\u5c55\u7684\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e3a\u6b64\u53d1\u5e03ResearchRubrics\u57fa\u51c6\u4ee5\u4fc3\u8fdb\u7814\u7a76\u52a9\u624b\u7684\u53d1\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2511.07924", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07924", "abs": "https://arxiv.org/abs/2511.07924", "authors": ["Shuang Liu", "Zhirun Zhang", "Jinhao Dong", "Zan Wang", "Qingchao Shen", "Junjie Chen", "Wei Lu", "Xiaoyong Du"], "title": "Testing Question Answering Software with Context-Driven Question Generation", "comment": null, "summary": "Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.\n  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test", "AI": {"tldr": "CQ^2A\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u95ee\u7b54\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\u5f62\u6210\u771f\u5b9e\u7b54\u6848\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u95ee\u9898\u7684\u81ea\u7136\u6027\u548c\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u6709\u95ee\u7b54\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u751f\u6210\u7684\u95ee\u9898\u4e0d\u81ea\u7136\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u53d1\u73b0\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u7f3a\u9677\u3002", "method": "\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\u5f62\u6210\u771f\u5b9e\u7b54\u6848\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e00\u81f4\u6027\u9a8c\u8bc1\u548c\u7ea6\u675f\u68c0\u67e5\u6765\u63d0\u9ad8\u8f93\u51fa\u53ef\u9760\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCQ^2A\u5728\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u3001\u751f\u6210\u95ee\u9898\u7684\u81ea\u7136\u6027\u548c\u4e0a\u4e0b\u6587\u8986\u76d6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u80fd\u964d\u4f4e\u95ee\u7b54\u8f6f\u4ef6\u7684\u9519\u8bef\u7387\u3002", "conclusion": "CQ^2A\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u95ee\u7b54\u7cfb\u7edf\u6d4b\u8bd5\u7684\u6548\u679c\u3002", "topic": "swe application"}}
{"id": "2511.07690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07690", "abs": "https://arxiv.org/abs/2511.07690", "authors": ["Soham Hans", "Volkan Ustun", "Benjamin Nye", "James Sterrett", "Matthew Green"], "title": "Towards AI-Assisted Generation of Military Training Scenarios", "comment": null, "summary": "Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u519b\u4e8b\u8bad\u7ec3\u573a\u666f\u4e2d\u7684\u4f5c\u6218\u547d\u4ee4\u7b49\u5173\u952e\u6587\u6863\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u4efb\u52a1\u548c\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u519b\u4e8b\u8bad\u7ec3\u573a\u666f\u751f\u6210\u8fc7\u7a0b\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u800c\u524dLLM\u65f6\u4ee3\u7684AI\u5de5\u5177\u96be\u4ee5\u751f\u6210\u8db3\u591f\u590d\u6742\u548c\u81ea\u9002\u5e94\u7684\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6587\u6863\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06\u573a\u666f\u751f\u6210\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u5b50\u95ee\u9898\uff0c\u4f7f\u7528\u4e13\u95e8\u7684LLM\u667a\u80fd\u4f53\u5206\u522b\u5904\u7406\u4e0d\u540c\u5b50\u4efb\u52a1\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u63a5\u6536\u524d\u5e8f\u667a\u80fd\u4f53\u7684\u8f93\u51fa\uff0c\u6574\u5408\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027\u548c\u6587\u6863\u51c6\u786e\u6027\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u6210\u529f\u751f\u6210\u4e86\u4f5c\u6218\u547d\u4ee4\u7684\u673a\u52a8\u65b9\u6848\u548c\u79fb\u52a8\u90e8\u5206\uff0c\u5e76\u51c6\u786e\u4f30\u8ba1\u4e86\u5730\u56fe\u4f4d\u7f6e\u548c\u79fb\u52a8\u8def\u5f84\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u3001\u7ec6\u81f4\u7684\u6587\u6863\uff0c\u5e76\u52a8\u6001\u9002\u5e94\u53d8\u5316\u6761\u4ef6\uff0c\u63a8\u52a8\u4e86\u519b\u4e8b\u8bad\u7ec3\u573a\u666f\u751f\u6210\u7684\u81ea\u52a8\u5316\u8fdb\u7a0b\u3002", "topic": "agent analysis"}}
{"id": "2511.08127", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08127", "abs": "https://arxiv.org/abs/2511.08127", "authors": ["Weiye Li", "Wenyi Tang"], "title": "A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models", "comment": null, "summary": "Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4f20\u7edf\u6e90\u4ee3\u7801\u6a21\u578b\u548cLLM4Code\u7684\u5185\u5728\u6f0f\u6d1e\u53ef\u8f6c\u79fb\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u53d7\u5bb3\u8005\u65e0\u5173\u65b9\u6cd5\u751f\u6210\u5b9e\u7528\u5bf9\u6297\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u65e2\u6ca1\u6709\u63d0\u4f9b\u5b9e\u7528\u7684\u5bf9\u6297\u6837\u672c\u751f\u6210\u65b9\u6cd5\uff08\u9700\u8981\u8bbf\u95eeSCM\u7684\u4e0b\u6e38\u5206\u7c7b\u5668\uff09\uff0c\u4e5f\u6ca1\u6709\u5173\u6ce8\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u5e73\u53f0\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684LLM4Code\u3002", "method": "\u8bbe\u8ba1\u4e86HABITAT\uff0c\u5305\u542b\u5b9a\u5236\u7684\u6270\u52a8\u63d2\u5165\u673a\u5236\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u6270\u52a8\u800c\u4e0d\u9700\u8981\u8bbf\u95eeSCM\u7684\u4e0b\u6e38\u5206\u7c7b\u5668\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u57fa\u4e8e\u4f20\u7edfSCM\u6784\u5efa\u7684\u5bf9\u6297\u6837\u672c\u5bf9LLM4Code\u7684\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe64%\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u9ad8\u51fa15%\u4ee5\u4e0a\u3002", "conclusion": "\u63ed\u793a\u4e86\u4f20\u7edfSCM\u4e0eLLM4Code\u4e4b\u95f4\u7684\u6f5c\u5728\u6f0f\u6d1e\u76f8\u5173\u6027\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u9c81\u68d2\u9632\u5fa1\u63d0\u4f9b\u4e86\u5173\u952e\u7126\u70b9\u3002", "topic": "agent analysis"}}
{"id": "2511.07800", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07800", "abs": "https://arxiv.org/abs/2511.07800", "authors": ["Siyu Xia", "Zekun Xu", "Jiajun Chai", "Wentian Fan", "Yan Song", "Xiaohan Wang", "Guojun Yin", "Wei Lin", "Haifeng Zhang", "Jun Wang"], "title": "From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory", "comment": null, "summary": "Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bad\u7ec3\u7684\u56fe\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u8f68\u8ff9\u62bd\u8c61\u4e3a\u72b6\u6001\u673a\u4e2d\u7684\u7ed3\u6784\u5316\u51b3\u7b56\u8def\u5f84\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u70bc\u4e3a\u9ad8\u5c42\u6218\u7565\u5143\u8ba4\u77e5\uff0c\u589e\u5f3aLLM\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u83b7\u53d6\u7ecf\u9a8c\u7684\u65b9\u5f0f\u5b58\u5728\u5c40\u9650\u6027\uff1a\u9690\u5f0f\u8bb0\u5fc6\u8bad\u7ec3\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u663e\u5f0f\u8bb0\u5fc6\u63d0\u793a\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5229\u7528\u5148\u9a8c\u7ecf\u9a8c\u6307\u5bfc\u5f53\u524d\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u591a\u5c42\u7ea7\u56fe\u8bb0\u5fc6\u6846\u67b6\uff0c\u5c06\u539f\u59cb\u667a\u80fd\u4f53\u8f68\u8ff9\u62bd\u8c61\u4e3a\u72b6\u6001\u673a\u4e2d\u7684\u7ed3\u6784\u5316\u51b3\u7b56\u8def\u5f84\uff0c\u5e76\u63d0\u70bc\u4e3a\u9ad8\u5c42\u6218\u7565\u5143\u8ba4\u77e5\u3002\u901a\u8fc7\u57fa\u4e8e\u5956\u52b1\u7684\u6743\u91cd\u4f18\u5316\u7a0b\u5e8f\u4f30\u8ba1\u6bcf\u4e2a\u5143\u8ba4\u77e5\u7684\u6548\u7528\uff0c\u5e76\u901a\u8fc7\u5143\u8ba4\u77e5\u63d0\u793a\u52a8\u6001\u96c6\u6210\u5230LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u4e2d\u3002", "result": "\u53ef\u5b66\u4e60\u7684\u56fe\u8bb0\u5fc6\u6846\u67b6\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86LLM\u667a\u80fd\u4f53\u7684\u6218\u7565\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684\u6536\u76ca\u3002", "conclusion": "\u8be5\u56fe\u8bb0\u5fc6\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86LLM\u667a\u80fd\u4f53\u5229\u7528\u53c2\u6570\u5316\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u4f53\u7ecf\u9a8c\u5229\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08475", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08475", "abs": "https://arxiv.org/abs/2511.08475", "authors": ["Yangxiao Cai", "Ruiyin Li", "Peng Liang", "Mojtaba Shahin", "Zengyang Li"], "title": "Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale", "comment": null, "summary": "As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e8694\u7bc7\u5173\u4e8eLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8bba\u6587\uff0c\u8bc6\u522b\u4e86\u4e3b\u8981\u5173\u6ce8\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3001\u8d28\u91cf\u5c5e\u6027\u3001\u8bbe\u8ba1\u6a21\u5f0f\u548c\u8bbe\u8ba1\u539f\u7406\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\uff0c\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u56e0\u5176\u81ea\u4e3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u8bbe\u8ba1\u539f\u5219\u3001\u8d28\u91cf\u5c5e\u6027\u548c\u8bbe\u8ba1\u6a21\u5f0f\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u6536\u96c6\u4e8694\u7bc7\u5173\u4e8eLLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8bba\u6587\u4f5c\u4e3a\u7814\u7a76\u6765\u6e90\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u4ee3\u7801\u751f\u6210\u662f\u6700\u5e38\u89c1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff1b\u529f\u80fd\u6027\u9002\u7528\u6027\u662f\u8bbe\u8ba1\u8005\u6700\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\uff1b\u57fa\u4e8e\u89d2\u8272\u7684\u534f\u4f5c\u662f\u6700\u5e38\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff1b\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u662f\u6700\u5e38\u89c1\u7684\u8bbe\u8ba1\u539f\u7406\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684LLM\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u542f\u793a\u548c\u5efa\u8bae\u3002", "topic": "agent analysis"}}
{"id": "2511.08530", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.08530", "abs": "https://arxiv.org/abs/2511.08530", "authors": ["Rong Feng", "Vanisha Gupta", "Vivek Patel", "Viroopaksh Reddy Ernampati", "Suman Saha"], "title": "Can Large Language Models Simulate Symbolic Execution Output Like KLEE?", "comment": null, "summary": "Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.\n  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4f7f\u7528GPT-4o\u6a21\u62dfKLEE\u7b26\u53f7\u6267\u884c\u5de5\u5177\u7684\u8f93\u51fa\uff0c\u7279\u522b\u662f\u8bc6\u522b\u7a0b\u5e8f\u4e2d\u6700\u53d7\u7ea6\u675f\u7684\u6267\u884c\u8def\u5f84\uff0c\u4ee5\u66ff\u4ee3\u6602\u8d35\u7684\u7b26\u53f7\u6267\u884c\u8fc7\u7a0b\u3002", "motivation": "\u7b26\u53f7\u6267\u884c\u5de5\u5177\u5982KLEE\u5728\u590d\u6742\u7a0b\u5e8f\u4e2d\u8fd0\u884c\u7f13\u6162\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u5e0c\u671b\u5229\u7528LLM\u6a21\u62df\u5176\u8f93\u51fa\u6765\u8282\u7701\u65f6\u95f4\u548c\u8d44\u6e90\u3002", "method": "\u4f7f\u7528GPT-4o\u5728100\u4e2aC\u7a0b\u5e8f\u6570\u636e\u96c6\u4e0a\u9884\u6d4bKLEE\u8f93\u51fa\u548c\u6700\u53d7\u7ea6\u675f\u8def\u5f84\uff08\u5177\u6709\u6700\u591a\u7b26\u53f7\u6761\u4ef6\u7684\u6267\u884c\u8def\u5f84\uff09\u3002", "result": "GPT-4o\u5728\u751f\u6210KLEE\u7c7b\u8f93\u51fa\u548c\u8bc6\u522b\u6700\u53d7\u7ea6\u675f\u8def\u5f84\u65b9\u9762\u51c6\u786e\u7387\u7ea6\u4e3a20%\u3002", "conclusion": "\u867d\u7136\u51c6\u786e\u7387\u4e0d\u9ad8\uff0c\u4f46\u8fd9\u9879\u65e9\u671f\u5de5\u4f5c\u5c55\u793a\u4e86\u5f53\u524dLLM\u5728\u6a21\u62df\u7b26\u53f7\u6267\u884c\u65b9\u9762\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "topic": "agent analysis"}}
{"id": "2511.07910", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07910", "abs": "https://arxiv.org/abs/2511.07910", "authors": ["Songze Li", "Zhiqiang Liu", "Zhaoyan Gong", "Xiaoke Guo", "Zhengke Gui", "Huajun Chen", "Wen Zhang"], "title": "Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning", "comment": "ICLR26 Submission", "summary": "Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \\textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \\textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \\textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.", "AI": {"tldr": "\u63d0\u51faLogits-to-Logic\u6846\u67b6\uff0c\u901a\u8fc7logits\u589e\u5f3a\u548c\u8fc7\u6ee4\u6a21\u5757\u6765\u7ea0\u6b63LLM\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u4e2d\u7684\u903b\u8f91\u7f3a\u9677\uff0c\u663e\u8457\u63d0\u5347\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "LLM\u5728\u975e\u7ed3\u6784\u5316\u6587\u672c\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u903b\u8f91\u6f02\u79fb\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u63d0\u4f9b\u8f93\u5165\u7ea7\u6307\u5bfc\uff0c\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u8f93\u51fa\u4e2d\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faLogits-to-Logic\u6846\u67b6\uff0c\u5305\u542blogits\u589e\u5f3a\u548clogits\u8fc7\u6ee4\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u9488\u5bf9\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684logits\u8f93\u51fa\u8fdb\u884c\u4fee\u6b63\u3002", "result": "\u5728\u591a\u4e2aKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u4e2d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "Logits-to-Logic\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3LLM\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u63a8\u7406\u4e2d\u7684\u903b\u8f91\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u903b\u8f91\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2511.08052", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08052", "abs": "https://arxiv.org/abs/2511.08052", "authors": ["Po-Chung Hsieh", "Chin-Po Chen", "Jeng-Lin Li", "Ming-Ching Chang"], "title": "Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging", "comment": "5 pages, 2 figures", "summary": "Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684Scaffold Reasoning\u6846\u67b6\u7528\u4e8e\u4ee3\u7801\u8c03\u8bd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u6d41\uff08Scaffold\u3001Analytic\u3001Integration\uff09\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5728DebugBench\u4e0a\u8fbe\u523088.91%\u901a\u8fc7\u7387\u548c5.36\u79d2\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u65b9\u6cd5\u5728\u5e73\u8861\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9System 2\u63a8\u7406\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u9700\u8981\u501f\u9274\u5fc3\u7406\u5b66\u7406\u8bba\u4f18\u5316\u8ba4\u77e5\u8def\u5f84\u3002", "method": "\u63d0\u51faScaffold Reasoning\u6846\u67b6\uff0c\u5305\u542bScaffold Stream\uff08\u6784\u5efa\u53c2\u8003\u4ee3\u7801\uff09\u3001Analytic Stream\uff08\u5206\u6790\u9519\u8bef\u4ee3\u7801\uff09\u548cIntegration Stream\uff08\u6574\u5408\u7ed3\u679c\uff09\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u5728DebugBench\u4e0a\u8fbe\u523088.91%\u901a\u8fc7\u7387\uff0c\u5e73\u5747\u6bcf\u95ee\u9898\u63a8\u7406\u65f6\u95f45.36\u79d2\uff0c\u4f18\u4e8e\u5176\u4ed6\u63a8\u7406\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u96be\u5ea6\u95ee\u9898\u548c\u9519\u8bef\u7c7b\u578b\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u4e00\u81f4\uff0c\u80fd\u6709\u6548\u5e73\u8861\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u4ee3\u7801\u8c03\u8bd5\u4efb\u52a1\u63d0\u4f9b\u4e86\u5fc3\u7406\u5b66\u652f\u6301\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "topic": "code agent"}}
{"id": "2511.07896", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07896", "abs": "https://arxiv.org/abs/2511.07896", "authors": ["Dengcan Liu", "Jiahao Li", "Zheren Fu", "Yi Tu", "Jiajun Li", "Zhendong Mao", "Yongdong Zhang"], "title": "SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder", "comment": "15pages,11figures,AAAI-26", "summary": "Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.", "AI": {"tldr": "\u63d0\u51faSparseRM\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u4eceLLM\u8868\u793a\u4e2d\u63d0\u53d6\u504f\u597d\u76f8\u5173\u7279\u5f81\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5728\u4ec5\u4f7f\u7528\u4e0d\u52301%\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4e3b\u6d41\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u8bad\u7ec3\u53ef\u9760\u5956\u52b1\u6a21\u578b\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u504f\u597d\u6807\u6ce8\u548c\u6602\u8d35\u7684LLM\u5fae\u8c03\u6210\u672c\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\u8868\u793a\uff0c\u63d0\u53d6\u504f\u597d\u76f8\u5173\u7684\u53ef\u89e3\u91ca\u65b9\u5411\uff0c\u5c06\u8868\u793a\u6295\u5f71\u5230\u8fd9\u4e9b\u65b9\u5411\u8ba1\u7b97\u5bf9\u9f50\u5206\u6570\uff0c\u901a\u8fc7\u7b80\u5355\u5956\u52b1\u5934\u805a\u5408\u5206\u6570\u9884\u6d4b\u504f\u597d\u5f97\u5206\u3002", "result": "\u5728\u4e09\u4e2a\u504f\u597d\u5efa\u6a21\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSparseRM\u5728\u4ec5\u4f7f\u7528\u4e0d\u52301%\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u5927\u591a\u6570\u4e3b\u6d41\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "SparseRM\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u4e0b\u6e38\u5bf9\u9f50\u6d41\u7a0b\u4e2d\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9ad8\u6548\u5bf9\u9f50\u65b9\u9762\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08301", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08301", "abs": "https://arxiv.org/abs/2511.08301", "authors": ["Valentin Tablan", "Scott Taylor", "Gabriel Hurtado", "Kristoffer Bernhem", "Anders Uhrenholt", "Gabriele Farei", "Karo Moilanen"], "title": "Smarter Together: Creating Agentic Communities of Practice through Shared Experiential Learning", "comment": "24 pages", "summary": "The transition from human-centric to agent-centric software development practices is disrupting existing knowledge sharing environments for software developers. Traditional peer-to-peer repositories and developer communities for shared technical knowledge and best practice have witnessed dramatic drops in participation in a short period of time. At the same time, agentic functional equivalents are yet to emerge leaving AI agents, which already generate a significant proportion of all new software code produced, without access to repositories of valuable shared learning.\n  In this paper, we introduce Spark, a novel shared agentic memory architecture which is designed to emulate the collective intelligence and know-how of human developer communities. Spark enables AI coding agents to both contribute to and draw from a persistent and continuously evolving experiential memory. Agents operating in the same general problem space use the Spark shared memory as a repository of new knowledge to achieve collective continual learning. We evaluate Spark as a coach for AI coding agents performing software development tasks. We demonstrate that recommendations made by Spark improve the quality of code generated by generic code generation models at varying sizes and capability tiers. Boosted by Spark, a small open-weights model with 30 billion parameters was able to match the code quality afforded by a much larger state-of-the-art model. Separately, we measure the intrinsic quality of recommendations generated by Spark against a wide range of criteria inspired by software development best practice, and achieve helpfulness levels of up to 98.2% in the top two (out of five) qualitative helpfulness bands.", "AI": {"tldr": "Spark\u662f\u4e00\u4e2a\u5171\u4eab\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\uff0c\u65e8\u5728\u6a21\u62df\u4eba\u7c7b\u5f00\u53d1\u8005\u793e\u533a\u7684\u96c6\u4f53\u667a\u80fd\uff0c\u8ba9AI\u7f16\u7a0b\u667a\u80fd\u4f53\u80fd\u591f\u8d21\u732e\u548c\u83b7\u53d6\u6301\u7eed\u6f14\u5316\u7684\u7ecf\u9a8c\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u96c6\u4f53\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u4e2d\u5fc3\u8f6f\u4ef6\u5f00\u53d1\u77e5\u8bc6\u5171\u4eab\u73af\u5883\u6b63\u5728\u88ab\u667a\u80fd\u4f53\u4e2d\u5fc3\u5b9e\u8df5\u6240\u98a0\u8986\uff0cAI\u667a\u80fd\u4f53\u7f3a\u4e4f\u5171\u4eab\u5b66\u4e60\u8d44\u6e90\u5e93\uff0c\u9700\u8981\u65b0\u7684\u77e5\u8bc6\u5171\u4eab\u673a\u5236\u3002", "method": "\u5f15\u5165Spark\u5171\u4eab\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\uff0c\u4f5c\u4e3aAI\u7f16\u7a0b\u667a\u80fd\u4f53\u7684\u6559\u7ec3\uff0c\u63d0\u4f9b\u6301\u7eed\u6f14\u5316\u7684\u7ecf\u9a8c\u8bb0\u5fc6\u5e93\uff0c\u667a\u80fd\u4f53\u53ef\u4ee5\u5728\u76f8\u540c\u95ee\u9898\u7a7a\u95f4\u4e2d\u4f7f\u7528\u8be5\u8bb0\u5fc6\u5e93\u8fdb\u884c\u77e5\u8bc6\u5171\u4eab\u548c\u5b66\u4e60\u3002", "result": "Spark\u7684\u63a8\u8350\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u7528\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u4ee3\u7801\u8d28\u91cf\uff0c300\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u6a21\u578b\u5728Spark\u52a0\u6301\u4e0b\u80fd\u591f\u5339\u914d\u66f4\u5927\u89c4\u6a21\u6700\u5148\u8fdb\u6a21\u578b\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u63a8\u8350\u7684\u6709\u7528\u6027\u5728\u6700\u9ad8\u4e24\u4e2a\u7b49\u7ea7\u8fbe\u523098.2%\u3002", "conclusion": "Spark\u67b6\u6784\u6210\u529f\u6a21\u62df\u4e86\u4eba\u7c7b\u5f00\u53d1\u8005\u793e\u533a\u7684\u96c6\u4f53\u667a\u80fd\uff0c\u4e3aAI\u7f16\u7a0b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u5b66\u4e60\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2511.07701", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07701", "abs": "https://arxiv.org/abs/2511.07701", "authors": ["Xiaolin Sun", "Feidi Liu", "Zhengming Ding", "ZiZhan Zheng"], "title": "Diffusion Guided Adversarial State Perturbations in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.", "AI": {"tldr": "SHIFT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7b56\u7565\u65e0\u5173\u72b6\u6001\u6270\u52a8\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u4e0d\u540c\u4f46\u4fdd\u6301\u771f\u5b9e\u6027\u548c\u5386\u53f2\u4e00\u81f4\u6027\u7684\u6270\u52a8\u72b6\u6001\uff0c\u6709\u6548\u7a81\u7834\u73b0\u6709\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u73b0\u6709RL\u7cfb\u7edf\u5728\u89c6\u89c9\u73af\u5883\u4e2d\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u800c\u5f53\u524d\u9632\u5fa1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e3b\u8981\u6e90\u4e8e\u73b0\u6709l_p\u8303\u6570\u7ea6\u675f\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u653b\u51fb\u5373\u4f7f\u5728\u8f83\u5927\u6270\u52a8\u9884\u7b97\u4e0b\u4e5f\u96be\u4ee5\u6539\u53d8\u56fe\u50cf\u8f93\u5165\u7684\u8bed\u4e49\u3002", "method": "\u63d0\u51faSHIFT\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u8bed\u4e49\u4e0d\u540c\u4f46\u4fdd\u6301\u771f\u5b9e\u6027\u548c\u5386\u53f2\u4e00\u81f4\u6027\u7684\u6270\u52a8\u72b6\u6001\uff0c\u662f\u4e00\u79cd\u7b56\u7565\u65e0\u5173\u7684\u72b6\u6001\u6270\u52a8\u653b\u51fb\u3002", "result": "\u8bc4\u4f30\u663e\u793aSHIFT\u653b\u51fb\u80fd\u6709\u6548\u7a81\u7834\u73b0\u6709\u9632\u5fa1\u673a\u5236\uff08\u5305\u62ec\u6700\u590d\u6742\u7684\u9632\u5fa1\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u66f4\u597d\u7684\u611f\u77e5\u9690\u853d\u6027\u3002", "conclusion": "\u7ed3\u679c\u8868\u660eRL\u4ee3\u7406\u5bf9\u8bed\u4e49\u611f\u77e5\u7684\u5bf9\u6297\u6270\u52a8\u5b58\u5728\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07943", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.07943", "abs": "https://arxiv.org/abs/2511.07943", "authors": ["Jun Xu", "Xinkai Du", "Yu Ao", "Peilong Zhao", "Yang Li", "Ling Zhong", "Lin Yuan", "Zhongpu Bo", "Xiaorui Wang", "Mengshu Sun", "Zhengke Gui", "Dalong Zhang", "Zhaoyang Wang", "Qiwei Wang", "Yangyang Hou", "Zhiying Yin", "Haofen Wang", "Huajun Chen", "Lei Liang", "Jun Zhou"], "title": "Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction", "comment": "Accepted to AAAI 2026. Extended version with full Appendix", "summary": "Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.", "AI": {"tldr": "\u63d0\u51faThinker\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u601d\u7ef4\u548c\u591a\u8f6e\u4ea4\u4e92\u5b9e\u73b0\u6df1\u5ea6\u641c\u7d22\uff0c\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u72ec\u7acb\u89e3\u51b3\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u548c\u903b\u8f91\u51fd\u6570\u53cc\u91cd\u8868\u793a\u6765\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u7684\u76d1\u7763\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLMs\u5229\u7528\u5916\u90e8\u68c0\u7d22\u5668\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u76d1\u7763\uff0c\u96be\u4ee5\u4fdd\u8bc1\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u4e25\u8c28\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u601d\u7ef4\u6a21\u578b\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u5b50\u95ee\u9898\u7528\u81ea\u7136\u8bed\u8a00\u548c\u903b\u8f91\u51fd\u6570\u53cc\u91cd\u8868\u793a\uff0c\u901a\u8fc7\u903b\u8f91\u51fd\u6570\u4f20\u9012\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u8fdb\u884c\u77e5\u8bc6\u8fb9\u754c\u5224\u5b9a\u4ee5\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5916\u90e8\u641c\u7d22\u3002", "result": "\u4ec5\u9700\u6570\u767e\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u4e0e\u73b0\u6709\u57fa\u7ebf\u7ade\u4e89\uff0c\u5728\u5b8c\u6574\u8bad\u7ec3\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Thinker\u6a21\u578b\u901a\u8fc7\u53ef\u76d1\u7763\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u903b\u8f91\u4e00\u81f4\u6027\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u4e86LLMs\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2511.07702", "categories": ["cs.LG", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2511.07702", "abs": "https://arxiv.org/abs/2511.07702", "authors": ["Meraj Hassanzadeh", "Ehsan Ghaderi", "Mohamad Ali Bijarchi", "Siamak Kazemzadeh Hannani"], "title": "Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework", "comment": null, "summary": "Multidimensional optimization has consistently been a critical challenge in engineering. However, traditional simulation-based optimization methods have long been plagued by significant limitations: they are typically capable of optimizing only a single problem at a time and require substantial computational time for meshing and numerical simulation. This paper introduces a novel framework leveraging cutting-edge Scientific Machine Learning (Sci-ML) methodologies to overcome these inherent drawbacks of conventional approaches. The proposed method provides instantaneous solutions to a spectrum of complex, multidimensional optimization problems. A micromixer case study is employed to demonstrate this methodology. An agent, operating on a Deep Reinforcement Learning (DRL) architecture, serves as the optimizer to explore the relationships between key problem parameters. This optimizer interacts with an environment constituted by a parametric Physics-Informed Neural Network (PINN), which responds to the agent's actions at a significantly higher speed than traditional numerical methods. The agent's objective, conditioned on the Schmidt number is to discover the optimal geometric and physical parameters that maximize the micromixer's efficiency. After training the agent across a wide range of Schmidt numbers, we analyzed the resulting optimal designs. Across this entire spectrum, the achieved efficiency was consistently greater than the baseline, normalized value. The maximum efficiency occurred at a Schmidt number of 13.3, demonstrating an improvement of approximately 32%. Finally, a comparative analysis with a Genetic Algorithm was conducted under equivalent conditions to underscore the advantages of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7684\u591a\u7ef4\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4e0e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5feb\u901f\u4f18\u5316\uff0c\u5728\u5fae\u6df7\u5408\u5668\u6848\u4f8b\u4e2d\u6548\u7387\u63d0\u5347\u8fbe32%\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u53ea\u80fd\u5355\u95ee\u9898\u4f18\u5316\u3001\u8ba1\u7b97\u65f6\u95f4\u957f\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5feb\u901f\u89e3\u51b3\u591a\u7ef4\u4f18\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4f5c\u4e3a\u4f18\u5316\u5668\uff0c\u4e0e\u53c2\u6570\u5316\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u73af\u5883\u4ea4\u4e92\uff0c\u901a\u8fc7\u4ee3\u7406\u63a2\u7d22\u95ee\u9898\u53c2\u6570\u95f4\u5173\u7cfb\u6765\u5bfb\u627e\u6700\u4f18\u89e3\u3002", "result": "\u5728\u5fae\u6df7\u5408\u5668\u6848\u4f8b\u4e2d\uff0c\u7ecf\u8fc7\u8bad\u7ec3\u7684\u4ee3\u7406\u5728\u5bbd\u8303\u56f4Schmidt\u6570\u4e0b\u5747\u5b9e\u73b0\u9ad8\u4e8e\u57fa\u7ebf\u7684\u6548\u7387\uff0c\u6700\u5927\u6548\u7387\u51fa\u73b0\u5728Schmidt\u6570\u4e3a13.3\u65f6\uff0c\u63d0\u5347\u7ea632%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u548c\u9057\u4f20\u7b97\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u5feb\u901f\u63d0\u4f9b\u591a\u7ef4\u4f18\u5316\u95ee\u9898\u7684\u5373\u65f6\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08126", "abs": "https://arxiv.org/abs/2511.08126", "authors": ["Raquel Montero", "Natalia Moskvina", "Paolo Morosi", "Tamara Serrano", "Elena Pagliarini", "Evelina Leivada"], "title": "Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition", "comment": null, "summary": "Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5316\u8868\u8fbe\u5904\u7406\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u4e09\u4e2a\u5173\u952e\u4eba\u7c7b\u91cf\u5316\u7279\u5f81\u5728\u6a21\u578b\u4e2d\u7684\u7f16\u7801\u60c5\u51b5\uff0c\u53d1\u73b0\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "motivation": "\u91cf\u5316\u8868\u8fbe\u662f(M)LLMs\u7279\u522b\u96be\u4ee5\u5904\u7406\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46\u5176\u8868\u73b0\u4e0d\u4f73\u7684\u786e\u5207\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7a76\u4eba\u7c7b\u91cf\u5316\u7279\u5f81\u7684\u4e09\u4e2a\u5173\u952e\u65b9\u9762\u5728\u6a21\u578b\u4e2d\u7684\u7f16\u7801\u60c5\u51b5\uff0c\u4ee5\u53ca\u6a21\u578b\u4e0e\u4eba\u7c7b\u7684\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u4e09\u4e2a\u8de8\u8bed\u8a00\u5171\u4eab\u7684\u4eba\u7c7b\u91cf\u5316\u7279\u5f81\uff1a\u91cf\u8bcd\u7684\u6392\u5e8f\u5c3a\u5ea6\u3001\u4f7f\u7528\u8303\u56f4\u548c\u539f\u578b\u6027\u3001\u4eba\u7c7b\u8fd1\u4f3c\u6570\u5b57\u7cfb\u7edf\u7684\u56fa\u6709\u504f\u5dee\u3002\u901a\u8fc7\u591a\u79cd\u4efb\u52a1\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u5728\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u7f16\u7801\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\uff0c\u4eba\u7c7b\u4e0eMLLMs\u5728\u8fd9\u4e9b\u91cf\u5316\u7279\u5f81\u4e0a\u5b58\u5728\u660e\u663e\u5dee\u5f02\uff0c\u65e0\u8bba\u662f\u5728\u4f53\u5185\u8fd8\u662f\u7845\u4e2d\u7684\u91cf\u5316\u8868\u5f81\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u63a2\u8ba8MLLMs\u4f5c\u4e3a\u8bed\u4e49\u548c\u8bed\u7528\u4ee3\u7406\u7684\u672c\u8d28\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8de8\u8bed\u8a00\u89c6\u89d2\u53ef\u4ee5\u9610\u660e\u5b83\u4eec\u7684\u80fd\u529b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u662f\u5426\u7a33\u5065\u548c\u7a33\u5b9a\u3002", "topic": "agent analysis"}}
{"id": "2511.07979", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07979", "abs": "https://arxiv.org/abs/2511.07979", "authors": ["Wenhan Yu", "Xinbo Lin", "Lanxin Ni", "Jinhua Cheng", "Lei Sha"], "title": "Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models", "comment": "21 pages, 7 figures. To appear in AAAI 2026", "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities across specialized domains, motivating research into their application to legal reasoning. However, existing legal benchmarks often conflate factual recall with genuine inference, fragment the reasoning process, and overlook the quality of reasoning. To address these limitations, we introduce MSLR, the first Chinese multi-step legal reasoning dataset grounded in real-world judicial decision making. MSLR adopts the IRAC framework (Issue, Rule, Application, Conclusion) to model structured expert reasoning from official legal documents. In addition, we design a scalable Human-LLM collaborative annotation pipeline that efficiently produces fine-grained step-level reasoning annotations and provides a reusable methodological framework for multi-step reasoning datasets. Evaluation of multiple LLMs on MSLR shows only moderate performance, highlighting the challenges of adapting to complex legal reasoning. Further experiments demonstrate that Self-Initiated Chain-of-Thought prompts generated by models autonomously improve reasoning coherence and quality, outperforming human-designed prompts. MSLR contributes to advancing LLM reasoning and Chain-of-Thought strategies and offers open resources for future research. The dataset and code are available at https://github.com/yuwenhan07/MSLR-Bench and https://law.sjtu.edu.cn/flszyjzx/index.html.", "AI": {"tldr": "\u63d0\u51fa\u4e86MSLR\uff0c\u9996\u4e2a\u57fa\u4e8e\u771f\u5b9e\u53f8\u6cd5\u51b3\u7b56\u7684\u4e2d\u6587\u591a\u6b65\u6cd5\u5f8b\u63a8\u7406\u6570\u636e\u96c6\uff0c\u91c7\u7528IRAC\u6846\u67b6\u5efa\u6a21\u7ed3\u6784\u5316\u4e13\u5bb6\u63a8\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53ef\u6269\u5c55\u7684\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u57fa\u51c6\u5f80\u5f80\u6df7\u6dc6\u4e8b\u5b9e\u8bb0\u5fc6\u4e0e\u771f\u5b9e\u63a8\u7406\uff0c\u788e\u7247\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5ffd\u89c6\u63a8\u7406\u8d28\u91cf\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u6cd5\u5f8b\u63a8\u7406\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8eIRAC\u6846\u67b6\u6784\u5efa\u591a\u6b65\u6cd5\u5f8b\u63a8\u7406\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\u751f\u6210\u7ec6\u7c92\u5ea6\u63a8\u7406\u6807\u6ce8\uff0c\u8bc4\u4f30\u591a\u79cdLLM\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u591a\u4e2aLLM\u5728MSLR\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u8868\u660e\u9002\u5e94\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u7684\u6311\u6218\u3002\u81ea\u4e3b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u63d0\u5347\u63a8\u7406\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\u3002", "conclusion": "MSLR\u63a8\u52a8\u4e86LLM\u63a8\u7406\u548c\u601d\u7ef4\u94fe\u7b56\u7565\u7684\u53d1\u5c55\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u653e\u8d44\u6e90\u3002", "topic": "agent analysis"}}
{"id": "2511.07833", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07833", "abs": "https://arxiv.org/abs/2511.07833", "authors": ["Chanakya Ekbote", "Vijay Lingam", "Behrooz Omidvar-Tehrani", "Jun Huan", "Sujay Sanghavi", "Anoop Deoras", "Stefano Soatto"], "title": "MURPHY: Multi-Turn GRPO for Self Correcting Code Generation", "comment": "20 pages, 2 figures, 6 Tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.", "AI": {"tldr": "Murphy\u662f\u4e00\u4e2a\u591a\u8f6e\u53cd\u601d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fed\u4ee3\u81ea\u6211\u6821\u6b63\u6765\u6269\u5c55GRPO\uff0c\u5728\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4GRPO\u5b9e\u73b0\u4e86\u9ad8\u8fbe8%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u5982GRPO\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u9700\u8981\u8fed\u4ee3\u51b3\u7b56\u7684\u4ee3\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u591a\u8f6e\u53cd\u601d\u4f18\u5316\u6846\u67b6Murphy\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u8fed\u4ee3\u81ea\u6211\u6821\u6b63\uff0c\u5229\u7528\u5b9a\u91cf\u548c\u5b9a\u6027\u6267\u884c\u53cd\u9988\u6765\u9010\u6b65\u6539\u8fdb\u63a8\u7406\u3002", "result": "\u5728Qwen\u548cOLMo\u7b49\u6a21\u578b\u5bb6\u65cf\u7684\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMurphy\u76f8\u6bd4GRPO\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe8%\u7684\u76f8\u5bf9pass@1\u63d0\u5347\u3002", "conclusion": "Murphy\u6846\u67b6\u901a\u8fc7\u591a\u8f6e\u53cd\u601d\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7406\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08042", "abs": "https://arxiv.org/abs/2511.08042", "authors": ["JV Roig"], "title": "Towards a Standard, Enterprise-Relevant Agentic AI Benchmark: Lessons from 5.5 billion tokens' worth of agentic AI evaluations", "comment": "34 pages", "summary": "Enterprise adoption of agentic AI systems requires reliable evaluation methods that reflect real-world deployment scenarios. Traditional LLM benchmarks suffer from training data contamination and fail to measure agentic capabilities such as multi-step tool use and decision-making under uncertainty. We present the Kamiwaza Agentic Merit Index (KAMI) v0.1, an enterprise-focused benchmark that addresses both contamination resistance and agentic evaluation. Through 170,000 LLM test items processing over 5.5 billion tokens across 35 model configurations, we demonstrate that traditional benchmark rankings poorly predict practical agentic performance. Notably, newer generation models like Llama 4 or Qwen 3 do not always outperform their older generation variants on enterprise-relevant tasks, contradicting traditional benchmark trends. We also present insights on cost-performance tradeoffs, model-specific behavioral patterns, and the impact of reasoning capabilities on token efficiency -- findings critical for enterprises making deployment decisions.", "AI": {"tldr": "KAMI v0.1\u662f\u4e00\u4e2a\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u57fa\u51c6\u6d4b\u8bd5\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u591a\u6b65\u9aa4\u5de5\u5177\u4f7f\u7528\u548c\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u7b49\u667a\u80fd\u4f53\u80fd\u529b\u3002", "motivation": "\u4f01\u4e1a\u91c7\u7528\u667a\u80fd\u4f53AI\u7cfb\u7edf\u9700\u8981\u80fd\u591f\u53cd\u6620\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u7684\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\u3002\u4f20\u7edfLLM\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u8861\u91cf\u667a\u80fd\u4f53\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5904\u7406170,000\u4e2aLLM\u6d4b\u8bd5\u9879\uff0c\u5904\u7406\u8d85\u8fc755\u4ebf\u4e2atoken\uff0c\u6db5\u76d635\u79cd\u6a21\u578b\u914d\u7f6e\uff0c\u5f00\u53d1\u4e86Kamiwaza\u667a\u80fd\u4f53\u4ef7\u503c\u6307\u6570(KAMI) v0.1\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u6392\u540d\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u5b9e\u9645\u667a\u80fd\u4f53\u6027\u80fd\uff1b\u65b0\u4e00\u4ee3\u6a21\u578b\u5982Llama 4\u6216Qwen 3\u5728\u4f01\u4e1a\u76f8\u5173\u4efb\u52a1\u4e0a\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u65e7\u7248\u672c\uff1b\u53d1\u73b0\u4e86\u6210\u672c-\u6027\u80fd\u6743\u8861\u3001\u6a21\u578b\u7279\u5b9a\u884c\u4e3a\u6a21\u5f0f\u4ee5\u53ca\u63a8\u7406\u80fd\u529b\u5bf9token\u6548\u7387\u7684\u5f71\u54cd\u3002", "conclusion": "KAMI\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u4f01\u4e1a\u90e8\u7f72\u51b3\u7b56\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5f3a\u8c03\u9700\u8981\u4e13\u95e8\u7684\u4f01\u4e1a\u7ea7\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2511.08319", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.08319", "abs": "https://arxiv.org/abs/2511.08319", "authors": ["Soyeong Jeong", "Aparna Elangovan", "Emine Yilmaz", "Oleg Rokhlenko"], "title": "Adaptive Multi-Agent Response Refinement in Conversational Systems", "comment": "LaCATODA Workshop @ AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u4f18\u5316LLM\u5bf9\u8bdd\u54cd\u5e94\uff0c\u901a\u8fc7\u4e13\u95e8\u8d1f\u8d23\u4e8b\u5b9e\u6027\u3001\u4e2a\u6027\u5316\u548c\u8fde\u8d2f\u6027\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u534f\u540c\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u901a\u4fe1\u7b56\u7565\u81ea\u9002\u5e94\u9009\u62e9\u76f8\u5173\u667a\u80fd\u4f53\u3002", "motivation": "LLM\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u867d\u7136\u80fd\u751f\u6210\u7c7b\u4eba\u54cd\u5e94\uff0c\u4f46\u5728\u4e2a\u6027\u5316\u6216\u7279\u5b9a\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u7528\u6237\u96be\u4ee5\u68c0\u6d4b\u9519\u8bef\u5e76\u8981\u6c42\u91cd\u65b0\u751f\u6210\u3002\u73b0\u6709\u5355LLM\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8003\u8651\u5bf9\u8bdd\u7684\u591a\u4e2a\u5173\u952e\u65b9\u9762\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u95e8\u8d1f\u8d23\u4e00\u4e2a\u65b9\u9762\uff08\u4e8b\u5b9e\u6027\u3001\u4e2a\u6027\u5316\u3001\u8fde\u8d2f\u6027\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u901a\u4fe1\u7b56\u7565\u81ea\u9002\u5e94\u9009\u62e9\u548c\u534f\u8c03\u76f8\u5173\u667a\u80fd\u4f53\uff0c\u5c06\u53cd\u9988\u5408\u5e76\u4ee5\u6539\u8fdb\u6574\u4f53\u54cd\u5e94\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u77e5\u8bc6\u6216\u7528\u6237\u89d2\u8272\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\uff0c\u52a8\u6001\u901a\u4fe1\u7b56\u7565\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2511.08066", "categories": ["cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.08066", "abs": "https://arxiv.org/abs/2511.08066", "authors": ["Cheng Yuan", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression", "comment": null, "summary": "Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4fe1\u606f\u5bb9\u91cf\u4f5c\u4e3a\u8861\u91cfLLM\u6548\u7387\u7684\u7edf\u4e00\u6307\u6807\uff0c\u57fa\u4e8e\u6587\u672c\u538b\u7f29\u6027\u80fd\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5173\u7cfb\uff0c\u80fd\u591f\u516c\u5e73\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u95f4\u7684\u6548\u7387\u5e76\u9884\u6d4b\u540c\u7cfb\u5217\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eLLM\u5feb\u901f\u53d1\u5c55\u53ca\u5176\u5e7f\u6cdb\u5e94\u7528\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6fc0\u589e\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u6a21\u578b\u80fd\u529b\u4e0e\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u7684\u7d27\u5f20\u5173\u7cfb\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u80fd\u591f\u51c6\u786e\u53cd\u6620\u4e0d\u540c\u89c4\u6a21\u548c\u67b6\u6784LLM\u6548\u7387\u7684\u7edf\u4e00\u6307\u6807\u3002", "method": "\u5f15\u5165\u4fe1\u606f\u5bb9\u91cf\u6307\u6807\uff0c\u57fa\u4e8e\u6587\u672c\u538b\u7f29\u6027\u80fd\u76f8\u5bf9\u4e8e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u5173\u7cfb\u6765\u8861\u91cf\u6a21\u578b\u6548\u7387\u3002\u901a\u8fc7\u8bc4\u4f3049\u4e2a\u6a21\u578b\u57285\u4e2a\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5206\u8bcd\u5668\u6548\u7387\u3001\u9884\u8bad\u7ec3\u6570\u636e\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u540c\u4e00\u7cfb\u5217\u4e2d\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u5177\u6709\u4e00\u81f4\u7684\u4fe1\u606f\u5bb9\u91cf\u3002\u8be5\u6307\u6807\u80fd\u591f\u516c\u5e73\u6bd4\u8f83\u6a21\u578b\u7cfb\u5217\u95f4\u7684\u6548\u7387\uff0c\u5e76\u51c6\u786e\u9884\u6d4b\u540c\u7cfb\u5217\u6a21\u578b\u7684\u6027\u80fd\u3002\u5206\u8bcd\u5668\u6548\u7387\u5bf9\u4fe1\u606f\u5bb9\u91cf\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u4fe1\u606f\u5bb9\u91cf\u662f\u4e00\u4e2a\u6709\u6548\u7684LLM\u6548\u7387\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u7edf\u4e00\u8861\u91cf\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u7684\u6548\u7387\uff0c\u5e76\u8003\u8651\u4e86\u5e38\u88ab\u5ffd\u89c6\u7684\u5206\u8bcd\u5668\u6548\u7387\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2511.08325", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08325", "abs": "https://arxiv.org/abs/2511.08325", "authors": ["Zhiheng Xi", "Chenyang Liao", "Guanyu Li", "Yajie Yang", "Wenxiang Chen", "Zhihao Zhang", "Binghai Wang", "Senjie Jin", "Yuhao Zhou", "Jian Guan", "Wei Wu", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress", "comment": "Preprint", "summary": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAgentPRM\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u6bcf\u4e2a\u51b3\u7b56\uff0c\u901a\u8fc7\u6355\u6349\u51b3\u7b56\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u53ca\u5176\u5bf9\u6700\u7ec8\u76ee\u6807\u7684\u8d21\u732e\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8fdb\u5ea6\u8ddf\u8e2a\u548c\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u51b3\u7b56\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u6216\u4e13\u5bb6\u8f68\u8ff9\u5fae\u8c03\uff0c\u672c\u6587\u4ece\u4e0d\u540c\u89d2\u5ea6\u63a2\u7d22\u6784\u5efa\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faAgentPRM\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u5e8f\u5dee\u5206\u4f30\u8ba1\u548c\u5e7f\u4e49\u4f18\u52bf\u4f30\u8ba1\u6765\u9ad8\u6548\u83b7\u53d6\u8bad\u7ec3\u6570\u636e\uff0c\u6355\u6349\u51b3\u7b56\u5e8f\u5217\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u53ca\u5176\u5bf9\u76ee\u6807\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAgentPRM\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad88\u500d\u4ee5\u4e0a\uff0c\u5728\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u65f6\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6539\u8fdb\u3002", "conclusion": "AgentPRM\u4e3a\u667a\u80fd\u4f53\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u51b3\u7b56\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u53ef\u5e94\u7528\u4e8eLLM\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "topic": "agent analysis"}}
{"id": "2511.07904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07904", "abs": "https://arxiv.org/abs/2511.07904", "authors": ["Zhao Yu", "Xiuping Wu", "Liangjun Ke"], "title": "Test-driven Reinforcement Learning", "comment": "AAAI 2026 oral", "summary": "Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d4b\u8bd5\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u591a\u4e2a\u6d4b\u8bd5\u51fd\u6570\u66ff\u4ee3\u5355\u4e00\u5956\u52b1\u51fd\u6570\u6765\u5b9a\u4e49\u4efb\u52a1\u76ee\u6807\uff0c\u7b80\u5316\u4e86\u5956\u52b1\u8bbe\u8ba1\u8fc7\u7a0b\u5e76\u652f\u6301\u591a\u76ee\u6807\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u56f0\u96be\uff0c\u56e0\u4e3a\u5956\u52b1\u51fd\u6570\u65e2\u8981\u5b9a\u4e49\u6700\u4f18\u76ee\u6807\u53c8\u8981\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8fd9\u5f80\u5f80\u5bfc\u81f4\u4efb\u52a1\u8868\u793a\u4e0d\u7406\u60f3\u3002", "method": "\u4f7f\u7528\u901a\u8fc7-\u5931\u8d25\u6d4b\u8bd5\u548c\u6307\u793a\u6027\u6d4b\u8bd5\u6765\u5206\u522b\u5b9a\u4e49\u6700\u4f18\u76ee\u6807\u548c\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u91c7\u7528\u8bcd\u5178\u5e8f\u542f\u53d1\u5f0f\u65b9\u6cd5\u6bd4\u8f83\u8f68\u8ff9\u4e0e\u6700\u4f18\u8f68\u8ff9\u96c6\u7684\u76f8\u5bf9\u8ddd\u79bb\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86TdRL\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u5728DeepMind Control Suite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTdRL\u5728\u7b56\u7565\u8bad\u7ec3\u4e0a\u5339\u914d\u6216\u4f18\u4e8e\u624b\u5de5\u8bbe\u8ba1\u5956\u52b1\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u8bbe\u8ba1\u7b80\u5355\u6027\u548c\u5bf9\u591a\u76ee\u6807\u4f18\u5316\u7684\u5185\u5728\u652f\u6301\u3002", "conclusion": "TdRL\u4e3a\u8868\u793a\u4efb\u52a1\u76ee\u6807\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u5956\u52b1\u8bbe\u8ba1\u6311\u6218\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07908", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07908", "abs": "https://arxiv.org/abs/2511.07908", "authors": ["Miroslav L\u017ei\u010da\u0159"], "title": "CellARC: Measuring Intelligence with Cellular Automata", "comment": "22 pages, 11 figures. Working draft. Dataset and leaderboard available at https://cellarc.mireklzicar.com", "summary": "We introduce CellARC, a synthetic benchmark for abstraction and reasoning built from multicolor 1D cellular automata (CA). Each episode has five support pairs and one query serialized in 256 tokens, enabling rapid iteration with small models while exposing a controllable task space with explicit knobs for alphabet size k, radius r, rule family, Langton's lambda, query coverage, and cell entropy. We release 95k training episodes plus two 1k test splits (interpolation/extrapolation) and evaluate symbolic, recurrent, convolutional, transformer, recursive, and LLM baselines. CellARC decouples generalization from anthropomorphic priors, supports unlimited difficulty-controlled sampling, and enables reproducible studies of how quickly models infer new rules under tight budgets. Our strongest small-model baseline (a 10M-parameter vanilla transformer) outperforms recent recursive models (TRM, HRM), reaching 58.0%/32.4% per-token accuracy on the interpolation/extrapolation splits, while a large closed model (GPT-5 High) attains 62.3%/48.1% on subsets of 100 test tasks. An ensemble that chooses per episode between the Transformer and the best symbolic baseline reaches 65.4%/35.5%, highlighting neuro-symbolic complementarity. Leaderboard: https://cellarc.mireklzicar.com", "AI": {"tldr": "CellARC\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e00\u7ef4\u591a\u8272\u5143\u80de\u81ea\u52a8\u673a\u7684\u62bd\u8c61\u63a8\u7406\u5408\u6210\u57fa\u51c6\uff0c\u5305\u542b95k\u8bad\u7ec3\u96c6\u548c2k\u6d4b\u8bd5\u96c6\uff0c\u652f\u6301\u5feb\u901f\u8fed\u4ee3\u5e76\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e3a\u62bd\u8c61\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u53ef\u63a7\u4efb\u52a1\u7a7a\u95f4\uff0c\u89e3\u8026\u6cdb\u5316\u80fd\u529b\u4e0e\u4eba\u7c7b\u5148\u9a8c\u77e5\u8bc6\uff0c\u652f\u6301\u53ef\u91cd\u590d\u7684\u89c4\u5219\u63a8\u65ad\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5143\u80de\u81ea\u52a8\u673a\u6784\u5efa\u57fa\u51c6\uff0c\u63a7\u5236\u5b57\u6bcd\u8868\u5927\u5c0f\u3001\u534a\u5f84\u3001\u89c4\u5219\u65cf\u7b49\u53c2\u6570\uff0c\u8bc4\u4f30\u7b26\u53f7\u3001\u5faa\u73af\u3001\u5377\u79ef\u3001Transformer\u3001\u9012\u5f52\u548cLLM\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "result": "10M\u53c2\u6570Transformer\u5728\u63d2\u503c/\u5916\u63a8\u96c6\u4e0a\u8fbe\u523058.0%/32.4%\u51c6\u786e\u7387\uff0cGPT-5 High\u8fbe\u523062.3%/48.1%\uff0c\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u65b9\u6cd5\u8fbe\u523065.4%/35.5%\u3002", "conclusion": "CellARC\u652f\u6301\u96be\u5ea6\u53ef\u63a7\u7684\u91c7\u6837\u548c\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u4e92\u8865\u6027\uff0c\u4e3a\u5c0f\u6a21\u578b\u5feb\u901f\u63a8\u7406\u89c4\u5219\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2511.07919", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07919", "abs": "https://arxiv.org/abs/2511.07919", "authors": ["Yoonho Lee", "Joseph Boen", "Chelsea Finn"], "title": "Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison", "comment": null, "summary": "We introduce \\textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.", "AI": {"tldr": "Feedback Descent\u662f\u4e00\u4e2a\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u53cd\u9988\u4f18\u5316\u6587\u672c\u4ea7\u7269\uff08\u63d0\u793a\u3001\u4ee3\u7801\u3001\u5206\u5b50\uff09\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u91cf\u5956\u52b1\uff0c\u901a\u8fc7\u4fdd\u7559\u8be6\u7ec6\u6279\u8bc4\u800c\u975e\u538b\u7f29\u4e3a\u4e8c\u5143\u504f\u597d\u6765\u62d3\u5bbd\u504f\u597d\u5b66\u4e60\u7684\u4fe1\u606f\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5224\u65ad\u538b\u7f29\u4e3a\u5355\u4e2a\u6bd4\u7279\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002Feedback Descent\u65e8\u5728\u901a\u8fc7\u9ad8\u5e26\u5bbd\u7684\u6587\u672c\u53cd\u9988\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u606f\uff0c\u5b9e\u73b0\u6587\u672c\u7a7a\u95f4\u7684\u5b9a\u5411\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u5c06\u7ed3\u6784\u5316\u53cd\u9988\u8f6c\u5316\u4e3a\u7c7b\u4f3c\u68af\u5ea6\u7684\u65b9\u5411\u4fe1\u606f\uff0c\u5b9e\u73b0\u76ee\u6807\u7f16\u8f91\u3002\u8bc4\u4f30\u5668\u4e3a\u6bcf\u4e2a\u6bd4\u8f83\u914d\u5bf9\u6587\u672c\u53cd\u9988\uff0c\u7eaf\u63a8\u7406\u65f6\u95f4\u8fed\u4ee3\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\uff0c\u4efb\u52a1\u65e0\u5173\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\u8bc4\u4f30\u4e2d\uff0cFeedback Descent\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff08GEPA\uff09\u3001\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08GRPO\u3001REINVENT\uff09\u4ee5\u53ca\u4e13\u95e8\u7684\u57fa\u4e8e\u56fe\u7684\u5206\u5b50\u4f18\u5316\u5668\u3002\u5728DOCKSTRING\u5206\u5b50\u53d1\u73b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bc6\u522b\u51fa\u8d85\u8fc7260,000\u4e2a\u5316\u5408\u7269\u6570\u636e\u5e93\u4e2d99.9%\u767e\u5206\u4f4d\u7684\u65b0\u578b\u7c7b\u836f\u7269\u5206\u5b50\u3002", "conclusion": "Feedback Descent\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u53cd\u9988\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6587\u672c\u4ea7\u7269\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u9ad8\u5e26\u5bbd\u53cd\u9988\u5728\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.07922", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07922", "abs": "https://arxiv.org/abs/2511.07922", "authors": ["Weixuan Ou", "Yanzhao Zheng", "Shuoshuo Sun", "Wei Zhang", "Baohua Dong", "Hangcheng Zhu", "Ruohui Huang", "Gang Yu", "Pengwei Yan", "Yifan Qiao"], "title": "SERL: Self-Examining Reinforcement Learning on Open-Domain", "comment": null, "summary": "Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86SERL\uff08\u81ea\u6211\u68c0\u9a8c\u5f3a\u5316\u5b66\u4e60\uff09\u6846\u67b6\uff0c\u8ba9LLM\u540c\u65f6\u4f5c\u4e3aActor\u548cJudge\uff0c\u901a\u8fc7\u4e24\u79cd\u5185\u90e8\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u65e0\u9700\u5916\u90e8\u4fe1\u53f7\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u9886\u57df\u4efb\u52a1\u4e2dRL\u9762\u4e34\u7684\u4e24\u4e2a\u6311\u6218\uff1a\u4e3b\u89c2\u6027\u4efb\u52a1\u65e0\u6cd5\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u4ee5\u53caRLHF\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u673a\u5236\u3002", "method": "SERL\u6846\u67b6\u5305\u542b\u4e24\u79cd\u5956\u52b1\u673a\u5236\uff1a\u57fa\u4e8eCopeland\u98ce\u683c\u6210\u5bf9\u6bd4\u8f83\u7684Actor\u5956\u52b1\uff0c\u548c\u9f13\u52b1\u4e00\u81f4\u5224\u65ad\u7684Judge\u81ea\u4e00\u81f4\u6027\u5956\u52b1\u3002", "result": "\u5728AlpacaEval 2\u4e0a\uff0cQwen3-8B\u7684LC\u80dc\u7387\u4ece52.37%\u63d0\u5347\u523059.90%\uff0c\u8fbe\u5230\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u4e2d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u6027\u80fd\u53ef\u4e0eQwen3-32B\u7b49\u66f4\u5927\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "SERL\u5728\u5f00\u653e\u9886\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3aLLM\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08394", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08394", "abs": "https://arxiv.org/abs/2511.08394", "authors": ["Sian Gooding", "Edward Grefenstette"], "title": "Interaction Dynamics as a Reward Signal for LLMs", "comment": null, "summary": "The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.", "AI": {"tldr": "TRACE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u8bdd\u5d4c\u5165\u8f68\u8ff9\u51e0\u4f55\u7279\u6027\u7684\u65b0\u578b\u5956\u52b1\u4fe1\u53f7\uff0c\u4ec5\u4f7f\u7528\u4ea4\u4e92\u52a8\u6001\u5c31\u80fd\u8fbe\u5230\u4e0e\u5b8c\u6574\u6587\u672c\u5206\u6790\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u4e14\u7ed3\u5408\u6587\u672c\u5206\u6790\u540e\u6027\u80fd\u6700\u4f73\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6587\u672c\u5185\u5bb9\u5956\u52b1\u4fe1\u53f7\uff0c\u5ffd\u89c6\u4e86\u4ea4\u4e92\u52a8\u6001\u8fd9\u4e00\u4e30\u5bcc\u7684\u8865\u5145\u4fe1\u53f7\u6e90\u3002", "method": "\u57fa\u4e8e\u5bf9\u8bdd\u5d4c\u5165\u8f68\u8ff9\u7684\u51e0\u4f55\u7279\u6027\u6784\u5efa\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u51fa'\u5bf9\u8bdd\u51e0\u4f55'\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86\u7ed3\u5408\u4ea4\u4e92\u52a8\u6001\u548c\u6587\u672c\u5206\u6790\u7684\u6df7\u5408\u6a21\u578b\u3002", "result": "\u4ec5\u4f7f\u7528\u7ed3\u6784\u4fe1\u53f7\u7684\u5956\u52b1\u6a21\u578b\u8fbe\u523068.20%\u7684\u6210\u5bf9\u51c6\u786e\u7387\uff0c\u4e0e\u5b8c\u6574\u6587\u672c\u5206\u6790\u7684LLM\u57fa\u7ebf(70.04%)\u76f8\u5f53\uff1b\u6df7\u5408\u6a21\u578b\u8fbe\u5230\u6700\u9ad8\u6027\u80fd80.17%\u3002", "conclusion": "\u5728\u4ea4\u4e92\u8bbe\u7f6e\u4e2d\uff0c\u4ee3\u7406\u7684\u6c9f\u901a\u65b9\u5f0f\u4e0e\u6c9f\u901a\u5185\u5bb9\u540c\u7b49\u91cd\u8981\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u6846\u67b6\u6765\u5bf9\u9f50\u4ee3\u7406\u5e76\u7406\u89e3\u6210\u529f\u534f\u4f5c\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2511.08151", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.08151", "abs": "https://arxiv.org/abs/2511.08151", "authors": ["Xuchen Li", "Ruitao Wu", "Xuanbo Liu", "Xukai Wang", "Jinbo Hu", "Zhixin Bai", "Bohan Zeng", "Hao Liang", "Leheng Chen", "Mingrui Chen", "Haitian Zhong", "Xuanlin Yang", "Xu-Yao Zhang", "Liu Liu", "Jia Li", "Kaiqi Huang", "Jiahao Xu", "Haitao Mi", "Wentao Zhang", "Bin Dong"], "title": "SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning", "comment": "Technique Report", "summary": "Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.", "AI": {"tldr": "SciAgent\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u65e8\u5728\u5b9e\u73b0\u901a\u7528\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u8de8\u5b66\u79d1\u548c\u96be\u5ea6\u7ea7\u522b\u81ea\u9002\u5e94\u63a8\u7406\u7b56\u7565\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u5728\u7279\u5b9a\u79d1\u5b66\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u4ecd\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\u4e14\u9700\u8981\u624b\u5de5\u5b9a\u5236\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8de8\u5b66\u79d1\u8fdb\u884c\u8fde\u8d2f\u63a8\u7406\u7684\u901a\u7528\u79d1\u5b66\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u534f\u8c03\u5668\u667a\u80fd\u4f53\u5206\u6790\u95ee\u9898\u9886\u57df\u548c\u590d\u6742\u5ea6\uff0c\u52a8\u6001\u7f16\u6392\u7531\u7b26\u53f7\u63a8\u7406\u3001\u6982\u5ff5\u5efa\u6a21\u3001\u6570\u503c\u8ba1\u7b97\u548c\u9a8c\u8bc1\u5b50\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u4e13\u4e1a\u5de5\u4f5c\u7cfb\u7edf\uff0c\u534f\u4f5c\u6784\u5efa\u548c\u4f18\u5316\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u7684\u63a8\u7406\u7ba1\u9053\u3002", "result": "\u5728\u6570\u5b66\u548c\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\uff08IMO\u3001IMC\u3001IPhO\u3001CPhO\uff09\u4e2d\uff0cSciAgent\u6301\u7eed\u8fbe\u5230\u6216\u8d85\u8d8a\u4eba\u7c7b\u91d1\u724c\u5f97\u4e3b\u8868\u73b0\uff0c\u5e76\u5728\u56fd\u9645\u5316\u5b66\u5965\u6797\u5339\u514b\u548cHumanity's Last Exam\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SciAgent\u4ee3\u8868\u4e86\u5411\u901a\u7528\u79d1\u5b66\u667a\u80fd\u8fc8\u51fa\u7684\u5177\u4f53\u4e00\u6b65\uff0c\u5373\u80fd\u591f\u5728\u4e13\u5bb6\u6c34\u5e73\u4e0a\u8fdb\u884c\u8fde\u8d2f\u3001\u8de8\u5b66\u79d1\u63a8\u7406\u7684AI\u7cfb\u7edf\u3002", "topic": "agent analysis"}}
{"id": "2511.07971", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07971", "abs": "https://arxiv.org/abs/2511.07971", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning", "comment": "Accepted to the AAAI Conference on Artificial Intelligence (AAAI-2026)", "summary": "We introduce LOREN, a curvature-aware zeroth-order (ZO) optimization method for fine-tuning large language models (LLMs). Existing ZO methods, which estimate gradients via finite differences using random perturbations, often suffer from high variance and suboptimal search directions. Our approach addresses these challenges by: (i) reformulating the problem of gradient preconditioning as that of adaptively estimating an anisotropic perturbation distribution for gradient estimation, (ii) capturing curvature through a low-rank block diagonal preconditioner using the framework of natural evolution strategies, and (iii) applying a REINFORCE leave-one-out (RLOO) gradient estimator to reduce variance. Experiments on standard LLM benchmarks show that our method outperforms state-of-the-art ZO methods by achieving higher accuracy and faster convergence, while cutting peak memory usage by up to 27.3% compared with MeZO-Adam.", "AI": {"tldr": "LOREN\u662f\u4e00\u79cd\u66f2\u7387\u611f\u77e5\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f30\u8ba1\u5404\u5411\u5f02\u6027\u6270\u52a8\u5206\u5e03\u3001\u4f4e\u79e9\u5757\u5bf9\u89d2\u9884\u5904\u7406\u5668\u548cREINFORCE\u7559\u4e00\u6cd5\u68af\u5ea6\u4f30\u8ba1\u5668\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u9636\u65b9\u6cd5\u4f7f\u7528\u968f\u673a\u6270\u52a8\u901a\u8fc7\u6709\u9650\u5dee\u5206\u4f30\u8ba1\u68af\u5ea6\uff0c\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u6b21\u4f18\u641c\u7d22\u65b9\u5411\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u68af\u5ea6\u4f30\u8ba1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u5c06\u68af\u5ea6\u9884\u6761\u4ef6\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u81ea\u9002\u5e94\u4f30\u8ba1\u5404\u5411\u5f02\u6027\u6270\u52a8\u5206\u5e03\uff1b\u4f7f\u7528\u81ea\u7136\u8fdb\u5316\u7b56\u7565\u6846\u67b6\u901a\u8fc7\u4f4e\u79e9\u5757\u5bf9\u89d2\u9884\u5904\u7406\u5668\u6355\u6349\u66f2\u7387\uff1b\u5e94\u7528REINFORCE\u7559\u4e00\u6cd5\u68af\u5ea6\u4f30\u8ba1\u5668\u51cf\u5c11\u65b9\u5dee\u3002", "result": "\u5728\u6807\u51c6LLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLOREN\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u9636\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u4e0eMeZO-Adam\u76f8\u6bd4\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe27.3%\u3002", "conclusion": "LOREN\u901a\u8fc7\u66f2\u7387\u611f\u77e5\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5fae\u8c03\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08522", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08522", "abs": "https://arxiv.org/abs/2511.08522", "authors": ["Zhaojian Yu", "Kaiyue Feng", "Yilun Zhao", "Shilin He", "Xiao-Ping Zhang", "Arman Cohan"], "title": "AlphaResearch: Accelerating New Algorithm Discovery with Language Models", "comment": null, "summary": "Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \\textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \\textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \\emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.", "AI": {"tldr": "AlphaResearch\u662f\u4e00\u4e2a\u81ea\u4e3b\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u7ed3\u5408\u6267\u884c\u9a8c\u8bc1\u548c\u6a21\u62df\u540c\u884c\u8bc4\u5ba1\u7684\u53cc\u91cd\u7814\u7a76\u73af\u5883\uff0c\u5728\u5f00\u653e\u6027\u95ee\u9898\u4e2d\u53d1\u73b0\u65b0\u7b97\u6cd5\uff0c\u57288\u4e2a\u7b97\u6cd5\u95ee\u9898\u4e2d\u8fbe\u52302/8\u7684\u80dc\u7387\uff0c\u5e76\u5728\u5706\u6253\u5305\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u5df2\u77e5\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4f46\u6613\u4e8e\u9a8c\u8bc1\u7684\u95ee\u9898\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u53d1\u73b0\u672a\u77e5\u77e5\u8bc6\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u65b0\u7b97\u6cd5\u7684\u7814\u7a76\u4ee3\u7406\u3002", "method": "\u6784\u5efa\u53cc\u91cd\u7814\u7a76\u73af\u5883\uff08\u6267\u884c\u9a8c\u8bc1+\u6a21\u62df\u540c\u884c\u8bc4\u5ba1\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u6267\u884c\u4e09\u4e2a\u6b65\u9aa4\uff1a\u63d0\u51fa\u65b0\u60f3\u6cd5\u3001\u5728\u53cc\u91cd\u73af\u5883\u4e2d\u9a8c\u8bc1\u3001\u4f18\u5316\u7814\u7a76\u63d0\u6848\u3002", "result": "AlphaResearch\u57288\u4e2a\u5f00\u653e\u7b97\u6cd5\u95ee\u9898\u7ade\u8d5b\u4e2d\u83b7\u5f972/8\u7684\u80dc\u7387\uff0c\u5728\u5706\u6253\u5305\u95ee\u9898\u4e0a\u8d85\u8d8a\u4eba\u7c7b\u7814\u7a76\u8005\u548cAlphaEvolve\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u4f73\u5df2\u77e5\u6027\u80fd\u3002", "conclusion": "\u5c55\u793a\u4e86\u5229\u7528LLMs\u52a0\u901f\u7b97\u6cd5\u53d1\u73b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5bf96/8\u5931\u8d25\u6848\u4f8b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2511.08525", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08525", "abs": "https://arxiv.org/abs/2511.08525", "authors": ["Shu Yang", "Junchao Wu", "Xilin Gou", "Xuansheng Wu", "Derek Wong", "Ninhao Liu", "Di Wang"], "title": "Investigating CoT Monitorability in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u901a\u8fc7\u601d\u7ef4\u94fe\uff08CoT\uff09\u8fdb\u884c\u76d1\u63a7\u7684\u53ef\u884c\u6027\u548c\u6311\u6218\uff0c\u5305\u62ec\u6a21\u578b\u662f\u5426\u771f\u5b9e\u8868\u8fbe\u51b3\u7b56\u56e0\u7d20\u4ee5\u53ca\u76d1\u63a7\u5668\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u76d1\u63a7\u8303\u5f0fMoME\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8be6\u7ec6\u63a8\u7406\u8f68\u8ff9\u4e3aAI\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6a21\u578b\u53ef\u80fd\u4e0d\u771f\u5b9e\u8868\u8fbe\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u76d1\u63a7\u5668\u53ef\u80fd\u8fc7\u4e8e\u654f\u611f\u6216\u4e0d\u591f\u654f\u611f\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u548c\u76f8\u5173\u6027\u5206\u6790\uff0c\u8003\u5bdf\u8a00\u8bed\u5316\u8d28\u91cf\u3001\u76d1\u63a7\u53ef\u9760\u6027\u548cLLM\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u4e0d\u540cCoT\u5e72\u9884\u65b9\u6cd5\u5bf9\u76d1\u63a7\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51faMoME\u76d1\u63a7\u8303\u5f0f\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u8de8\u6570\u5b66\u3001\u79d1\u5b66\u548c\u4f26\u7406\u9886\u57df\u7684\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5206\u6790\u4e86\u8a00\u8bed\u5316\u8d28\u91cf\u3001\u76d1\u63a7\u53ef\u9760\u6027\u548c\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4e0d\u540cCoT\u5e72\u9884\u65b9\u6cd5\u5bf9\u76d1\u63a7\u6709\u6548\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "CoT\u76d1\u63a7\u6027\u5177\u6709\u6f5c\u529b\u4f46\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u8a00\u8bed\u5316\u771f\u5b9e\u6027\u548c\u76d1\u63a7\u53ef\u9760\u6027\uff0cMoME\u8303\u5f0f\u4e3a\u6a21\u578b\u95f4\u76d1\u63a7\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2511.08234", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08234", "abs": "https://arxiv.org/abs/2511.08234", "authors": ["Zhihao Lin"], "title": "Beyond Distributions: Geometric Action Control for Continuous Reinforcement Learning", "comment": "18 pages, 5 figures", "summary": "Gaussian policies have dominated continuous control in deep reinforcement learning (RL), yet they suffer from a fundamental mismatch: their unbounded support requires ad-hoc squashing functions that distort the geometry of bounded action spaces. While von Mises-Fisher (vMF) distributions offer a theoretically grounded alternative on the sphere, their reliance on Bessel functions and rejection sampling hinders practical adoption. We propose \\textbf{Geometric Action Control (GAC)}, a novel action generation paradigm that preserves the geometric benefits of spherical distributions while \\textit{simplifying computation}. GAC decomposes action generation into a direction vector and a learnable concentration parameter, enabling efficient interpolation between deterministic actions and uniform spherical noise. This design reduces parameter count from \\(2d\\) to \\(d+1\\), and avoids the \\(O(dk)\\) complexity of vMF rejection sampling, achieving simple \\(O(d)\\) operations. Empirically, GAC consistently matches or exceeds state-of-the-art methods across six MuJoCo benchmarks, achieving 37.6\\% improvement over SAC on Ant-v4 and the best results on 4 out of 6 tasks. Our ablation studies reveal that both \\textbf{spherical normalization} and \\textbf{adaptive concentration control} are essential to GAC's success. These findings suggest that robust and efficient continuous control does not require complex distributions, but a principled respect for the geometry of action spaces. Code and pretrained models are available in supplementary materials.", "AI": {"tldr": "\u63d0\u51faGeometric Action Control (GAC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b9\u5411\u5411\u91cf\u548c\u53ef\u5b66\u4e60\u6d53\u5ea6\u53c2\u6570\u5206\u89e3\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u4fdd\u6301\u7403\u5f62\u5206\u5e03\u51e0\u4f55\u4f18\u52bf\u7684\u540c\u65f6\u7b80\u5316\u8ba1\u7b97\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9ad8\u65af\u7b56\u7565\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8fde\u7eed\u63a7\u5236\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5176\u65e0\u754c\u652f\u6301\u9700\u8981\u4e34\u65f6\u538b\u7f29\u51fd\u6570\u6765\u626d\u66f2\u6709\u754c\u52a8\u4f5c\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u867d\u7136vMF\u5206\u5e03\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u4f9d\u8d56\u8d1d\u585e\u5c14\u51fd\u6570\u548c\u62d2\u7edd\u91c7\u6837\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "GAC\u5c06\u52a8\u4f5c\u751f\u6210\u5206\u89e3\u4e3a\u65b9\u5411\u5411\u91cf\u548c\u53ef\u5b66\u4e60\u6d53\u5ea6\u53c2\u6570\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u52a8\u4f5c\u548c\u5747\u5300\u7403\u5f62\u566a\u58f0\u4e4b\u95f4\u7684\u9ad8\u6548\u63d2\u503c\u3002\u8be5\u8bbe\u8ba1\u5c06\u53c2\u6570\u6570\u91cf\u4ece2d\u51cf\u5c11\u5230d+1\uff0c\u907f\u514d\u4e86vMF\u62d2\u7edd\u91c7\u6837\u7684O(dk)\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u7b80\u5355\u7684O(d)\u64cd\u4f5c\u3002", "result": "\u5728\u516d\u4e2aMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGAC\u59cb\u7ec8\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728Ant-v4\u4e0a\u6bd4SAC\u63d0\u9ad837.6%\uff0c\u57286\u4e2a\u4efb\u52a1\u4e2d\u76844\u4e2a\u4e0a\u53d6\u5f97\u6700\u4f73\u7ed3\u679c\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u7403\u5f62\u5f52\u4e00\u5316\u548c\u81ea\u9002\u5e94\u6d53\u5ea6\u63a7\u5236\u5bf9GAC\u7684\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7a33\u5065\u9ad8\u6548\u7684\u8fde\u7eed\u63a7\u5236\u4e0d\u9700\u8981\u590d\u6742\u7684\u5206\u5e03\uff0c\u800c\u662f\u9700\u8981\u5bf9\u52a8\u4f5c\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u7684\u539f\u7406\u6027\u5c0a\u91cd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08565", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.08565", "abs": "https://arxiv.org/abs/2511.08565", "authors": ["Davi Bastos Costa", "Felippe Alves", "Renato Vicente"], "title": "Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models", "comment": "9+8 pages, 7 tables, 6 figures", "summary": "Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86LLMs\u5728\u89d2\u8272\u626e\u6f14\u60c5\u5883\u4e0b\u7684\u9053\u5fb7\u54cd\u5e94\uff0c\u901a\u8fc7\u9053\u5fb7\u57fa\u7840\u95ee\u5377\u8bc4\u4f30\u4e86\u9053\u5fb7\u6613\u611f\u6027\u548c\u9053\u5fb7\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5bb6\u65cf\u5bf9\u9053\u5fb7\u9c81\u68d2\u6027\u5f71\u54cd\u6700\u5927\uff0c\u800c\u6a21\u578b\u5927\u5c0f\u5bf9\u9053\u5fb7\u6613\u611f\u6027\u5f71\u54cd\u66f4\u660e\u663e\u3002", "motivation": "\u968f\u7740LLMs\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u4f7f\u7528\uff0c\u9700\u8981\u4e86\u89e3\u5b83\u4eec\u5728\u89d2\u8272\u626e\u6f14\u65f6\u5982\u4f55\u8868\u8fbe\u548c\u6539\u53d8\u9053\u5fb7\u5224\u65ad\u3002", "method": "\u4f7f\u7528\u9053\u5fb7\u57fa\u7840\u95ee\u5377(MFQ)\u521b\u5efa\u57fa\u51c6\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u63d0\u793a\u8ba9LLMs\u626e\u6f14\u7279\u5b9a\u89d2\u8272\uff0c\u91cf\u5316\u9053\u5fb7\u6613\u611f\u6027\u548c\u9053\u5fb7\u9c81\u68d2\u6027\u3002", "result": "Claude\u5bb6\u65cf\u9053\u5fb7\u9c81\u68d2\u6027\u6700\u5f3a\uff0cGPT-4\u548cGemini\u6b21\u4e4b\uff1b\u6a21\u578b\u5927\u5c0f\u5bf9\u9053\u5fb7\u6613\u611f\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5927\u6a21\u578b\u66f4\u6613\u53d7\u5f71\u54cd\uff1b\u9053\u5fb7\u9c81\u68d2\u6027\u548c\u6613\u611f\u6027\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u89d2\u8272\u626e\u6f14\u6761\u4ef6\u663e\u8457\u5f71\u54cdLLMs\u7684\u9053\u5fb7\u884c\u4e3a\uff0c\u6a21\u578b\u5bb6\u65cf\u662f\u51b3\u5b9a\u9053\u5fb7\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2511.08242", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08242", "abs": "https://arxiv.org/abs/2511.08242", "authors": ["Waseem AlShikh", "Muayad Sayed Ali", "Brian Kennedy", "Dmytro Mozolevskyi"], "title": "Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents", "comment": null, "summary": "As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.", "AI": {"tldr": "\u63d0\u51fa11\u4e2a\u57fa\u4e8e\u7ed3\u679c\u3001\u4efb\u52a1\u65e0\u5173\u7684AI\u4ee3\u7406\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6df7\u5408\u4ee3\u7406\u67b6\u6784\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u6700\u4f73", "motivation": "\u73b0\u6709\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u6307\u6807\uff08\u5982\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\uff09\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u7684\u51b3\u7b56\u8d28\u91cf\u3001\u64cd\u4f5c\u81ea\u4e3b\u6027\u548c\u4e1a\u52a1\u4ef7\u503c\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6", "method": "\u63d0\u51fa11\u4e2a\u7ed3\u679c\u5bfc\u5411\u7684\u6307\u6807\uff08\u5982\u76ee\u6807\u5b8c\u6210\u7387\u3001\u81ea\u4e3b\u6027\u6307\u6570\u3001\u591a\u6b65\u9aa4\u4efb\u52a1\u97e7\u6027\u7b49\uff09\uff0c\u57285\u4e2a\u4e0d\u540c\u9886\u57df\uff08\u533b\u7597\u3001\u91d1\u878d\u3001\u8425\u9500\u3001\u6cd5\u5f8b\u3001\u5ba2\u670d\uff09\u5bf94\u79cd\u4ee3\u7406\u67b6\u6784\u8fdb\u884c\u5927\u89c4\u6a21\u6a21\u62df\u5b9e\u9a8c", "result": "\u6df7\u5408\u4ee3\u7406\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u5e73\u5747\u76ee\u6807\u5b8c\u6210\u7387\u8fbe88.8%\uff0c\u6295\u8d44\u56de\u62a5\u7387\u6700\u9ad8\uff0c\u4e0d\u540c\u4ee3\u7406\u8bbe\u8ba1\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u6743\u8861", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aAI\u4ee3\u7406\u7684\u5168\u9762\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u6709\u6548\u7684\u5f00\u53d1\u3001\u90e8\u7f72\u548c\u6cbb\u7406", "topic": "agent analysis"}}
{"id": "2511.08577", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.08577", "abs": "https://arxiv.org/abs/2511.08577", "authors": ["Tianyu Fu", "Yichen You", "Zekai Chen", "Guohao Dai", "Huazhong Yang", "Yu Wang"], "title": "Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models", "comment": null, "summary": "Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.", "AI": {"tldr": "\u63d0\u51faThink-at-Hard(TaH)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6f5c\u5728\u601d\u8003\u673a\u5236\uff0c\u4ec5\u5728\u56f0\u96betoken\u4e0a\u8fdb\u884c\u6df1\u5ea6\u8fed\u4ee3\uff0c\u907f\u514d\u7b80\u5355token\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5faa\u73aftransformer\u4e2d\u5b58\u5728\u7684\u6f5c\u5728\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\u2014\u2014\u7b80\u5355token\u5728\u7b2c\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u540e\u9884\u6d4b\u6b63\u786e\uff0c\u4f46\u5728\u989d\u5916\u8fed\u4ee3\u4e2d\u88ab\u9519\u8bef\u4fee\u6b63\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u51b3\u7b56\u5668\u8bc6\u522b\u53ef\u80fd\u9519\u8bef\u7684\u56f0\u96betoken\uff0c\u4ec5\u5728\u8fd9\u4e9btoken\u4e0a\u89e6\u53d1\u6f5c\u5728\u8fed\u4ee3\uff1b\u91c7\u7528LoRA\u6a21\u5757\u5c06LLM\u76ee\u6807\u4ece\u901a\u7528\u4e0b\u4e00token\u9884\u6d4b\u8f6c\u5411\u4e13\u6ce8\u56f0\u96betoken\u4f18\u5316\uff1b\u5f15\u5165\u53cc\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u6269\u5c55\u6ce8\u610f\u529b\u7ef4\u5ea6\u3002", "result": "\u5728\u4e94\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u76f8\u6bd4\u5bf9\u6240\u6709token\u8fed\u4ee3\u4e24\u6b21\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u53478.1-11.3%\uff0c\u540c\u65f6\u514d\u966494%token\u7684\u7b2c\u4e8c\u6b21\u8fed\u4ee3\u3002", "conclusion": "TaH\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u76f8\u540c\u53c2\u6570\u91cf\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2511.08274", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08274", "abs": "https://arxiv.org/abs/2511.08274", "authors": ["Anton Gusarov", "Anastasia Volkova", "Valentin Khrulkov", "Andrey Kuznetsov", "Evgenii Maslov", "Ivan Oseledets"], "title": "Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs", "comment": "Code to be released", "summary": "While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86Multi-Agent GraphRAG\u7cfb\u7edf\uff0c\u5229\u7528Cypher\u67e5\u8be2\u8bed\u8a00\u548c\u6807\u7b7e\u5c5e\u6027\u56fe\u6570\u636e\u5e93\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u63a8\u7406\u5f15\u64ce\uff0c\u586b\u8865\u4e86\u73b0\u6709GraphRAG\u7814\u7a76\u4e3b\u8981\u5173\u6ce8RDF\u77e5\u8bc6\u56fe\u8c31\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709GraphRAG\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eRDF\u77e5\u8bc6\u56fe\u8c31\u548cSPARQL\u67e5\u8be2\uff0c\u4f46Cypher\u548c\u6807\u7b7e\u5c5e\u6027\u56fe\u6570\u636e\u5e93\u4f5c\u4e3a\u53ef\u6269\u5c55\u6709\u6548\u63a8\u7406\u5f15\u64ce\u5728GraphRAG\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u6a21\u5757\u5316LLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u6587\u672c\u5230Cypher\u67e5\u8be2\u751f\u6210\uff0c\u4f5c\u4e3aLPG\u56fe\u6570\u636e\u7684\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u3002\u91c7\u7528\u8fed\u4ee3\u5185\u5bb9\u611f\u77e5\u6821\u6b63\u548c\u89c4\u8303\u5316\uff0c\u901a\u8fc7\u805a\u5408\u53cd\u9988\u5faa\u73af\u786e\u4fdd\u751f\u6210\u67e5\u8be2\u7684\u8bed\u4e49\u548c\u8bed\u6cd5\u4f18\u5316\u3002", "result": "\u5728CypherBench\u56fe\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u7cfb\u7edf\uff0c\u6db5\u76d6\u591a\u4e2a\u901a\u7528\u9886\u57df\u7684\u4e0d\u540c\u67e5\u8be2\u7c7b\u578b\u3002\u8fd8\u5728\u57fa\u4e8eIFC\u6570\u636e\u7684\u5c5e\u6027\u56fe\u4e0a\u5c55\u793a\u6027\u80fd\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6865\u63a5AI\u4e0e\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5de5\u4e1a\u6570\u5b57\u81ea\u52a8\u5316\u7528\u4f8b\uff0c\u4e3aGraphRAG\u63d0\u4f9b\u4e86\u57fa\u4e8eCypher\u548c\u6807\u7b7e\u5c5e\u6027\u56fe\u7684\u65b0\u8303\u5f0f\u3002", "topic": "code agent"}}
{"id": "2511.08136", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.08136", "abs": "https://arxiv.org/abs/2511.08136", "authors": ["Returaj Burnwal", "Nirav Pravinbhai Bhatt", "Balaraman Ravindran"], "title": "SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories", "comment": "18 pages, AAAI 2026", "summary": "In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \\textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.", "AI": {"tldr": "\u63d0\u51faSafeMIL\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u793a\u4f8b\u5b66\u4e60\u4ece\u975e\u504f\u597d\u8f68\u8ff9\u4e2d\u5b66\u4e60\u98ce\u9669\u6210\u672c\u51fd\u6570\uff0c\u7528\u4e8e\u79bb\u7ebf\u5b89\u5168\u6a21\u4eff\u5b66\u4e60\uff0c\u786e\u4fdd\u7b56\u7565\u5728\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\u7684\u540c\u65f6\u4e0d\u964d\u4f4e\u5956\u52b1\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5728\u7ebf\u4ea4\u4e92\u5b58\u5728\u98ce\u9669\uff0c\u4e14\u96be\u4ee5\u7cbe\u786e\u6307\u5b9a\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u5956\u52b1\u548c\u5b89\u5168\u6210\u672c\u4fe1\u606f\uff0c\u4f46\u6536\u96c6\u53cd\u6620\u4e0d\u826f\u6216\u98ce\u9669\u884c\u4e3a\u7684\u8f68\u8ff9\u76f8\u5bf9\u53ef\u884c\u3002", "method": "\u4f7f\u7528\u591a\u793a\u4f8b\u5b66\u4e60\u5b66\u4e60\u53c2\u6570\u5316\u6210\u672c\u51fd\u6570\uff0c\u9884\u6d4b\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u98ce\u9669\u7a0b\u5ea6\uff0c\u7136\u540e\u5229\u7528\u8be5\u6210\u672c\u51fd\u6570\u907f\u514d\u975e\u504f\u597d\u884c\u4e3a\uff0c\u751f\u6210\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u7684\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u5b66\u4e60\u5230\u6ee1\u8db3\u6210\u672c\u7ea6\u675f\u7684\u66f4\u5b89\u5168\u7b56\u7565\uff0c\u4e14\u4e0d\u964d\u4f4e\u5956\u52b1\u6027\u80fd\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SafeMIL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5b89\u5168\u6a21\u4eff\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u504f\u597d\u8f68\u8ff9\u5b66\u4e60\u98ce\u9669\u6210\u672c\uff0c\u5b9e\u73b0\u5b89\u5168\u4f18\u5148\u7684\u7b56\u7565\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08142", "categories": ["cs.LG", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.08142", "abs": "https://arxiv.org/abs/2511.08142", "authors": ["Anna Lackinger", "Andrea Morichetta", "Pantelis A. Frangoudis", "Schahram Dustdar"], "title": "BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.", "AI": {"tldr": "\u63d0\u51fa\u4e86BIPPO\uff08\u9884\u7b97\u611f\u77e5\u72ec\u7acb\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u9009\u62e9\u7684\u8282\u80fd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u4fdd\u8bc1\u4e86\u8d1f\u8f7d\u5206\u5e03\u548c\u9690\u79c1\uff0c\u4f46\u672a\u539f\u751f\u8003\u8651\u57fa\u7840\u8bbe\u65bd\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u8d44\u6e90\u9650\u5236\u548c\u8bbe\u5907\u6d41\u5931\u7b49\u57fa\u7840\u8bbe\u65bd\u6311\u6218\uff0c\u4e14\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u80fd\u6548\u4f18\u5316\u3002", "method": "BIPPO\u662f\u4e00\u79cd\u8282\u80fd\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u9884\u7b97\u611f\u77e5\u7684\u72ec\u7acb\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u6539\u8fdb\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u9ad8\u5ea6\u9884\u7b97\u53d7\u9650\u8bbe\u7f6e\u4e0b\uff0cBIPPO\u5728\u975eIID\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\uff0c\u76f8\u6bd4\u975eRL\u673a\u5236\u3001\u4f20\u7edfPPO\u548cIPPO\u63d0\u9ad8\u4e86\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4e14\u4ec5\u6d88\u8017\u53ef\u5ffd\u7565\u7684\u9884\u7b97\u6bd4\u4f8b\uff0c\u5373\u4f7f\u5ba2\u6237\u7aef\u6570\u91cf\u589e\u52a0\u4e5f\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "BIPPO\u4e3a\u7269\u8054\u7f51\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u548c\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08548", "abs": "https://arxiv.org/abs/2511.08548", "authors": ["Shubhra Mishra", "Yuka Machino", "Gabriel Poesia", "Albert Jiang", "Joy Hsu", "Adrian Weller", "Challenger Mishra", "David Broman", "Joshua B. Tenenbaum", "Mateja Jamnik", "Cedegao E. Zhang", "Katherine M. Collins"], "title": "A Matter of Interest: Understanding Interestingness of Math Problems in Humans and Language Models", "comment": "Published at the Math-AI Workshop, NeurIPS 2025", "summary": "The evolution of mathematics has been guided in part by interestingness. From researchers choosing which problems to tackle next, to students deciding which ones to engage with, people's choices are often guided by judgments about how interesting or challenging problems are likely to be. As AI systems, such as LLMs, increasingly participate in mathematics with people -- whether for advanced research or education -- it becomes important to understand how well their judgments align with human ones. Our work examines this alignment through two empirical studies of human and LLM assessment of mathematical interestingness and difficulty, spanning a range of mathematical experience. We study two groups: participants from a crowdsourcing platform and International Math Olympiad competitors. We show that while many LLMs appear to broadly agree with human notions of interestingness, they mostly do not capture the distribution observed in human judgments. Moreover, most LLMs only somewhat align with why humans find certain math problems interesting, showing weak correlation with human-selected interestingness rationales. Together, our findings highlight both the promises and limitations of current LLMs in capturing human interestingness judgments for mathematical AI thought partnerships.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u4e2a\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86\u4eba\u7c7b\u4e0eLLM\u5728\u6570\u5b66\u95ee\u9898\u8da3\u5473\u6027\u548c\u96be\u5ea6\u8bc4\u4f30\u4e0a\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0LLM\u867d\u7136\u5927\u81f4\u540c\u610f\u4eba\u7c7b\u7684\u8da3\u5473\u6027\u6982\u5ff5\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4eba\u7c7b\u5224\u65ad\u7684\u5206\u5e03\uff0c\u4e14\u4e0e\u4eba\u7c7b\u9009\u62e9\u8da3\u5473\u6027\u7406\u7531\u7684\u76f8\u5173\u6027\u8f83\u5f31\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\uff08\u5982LLM\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u53c2\u4e0e\u6570\u5b66\u7814\u7a76\u548c\u6559\u80b2\uff0c\u4e86\u89e3\u5b83\u4eec\u5bf9\u4eba\u7c7b\u8da3\u5473\u6027\u5224\u65ad\u7684\u5339\u914d\u7a0b\u5ea6\u53d8\u5f97\u91cd\u8981\uff0c\u8fd9\u5bf9\u4e8e\u6570\u5b66AI\u601d\u7ef4\u4f19\u4f34\u5173\u7cfb\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u522b\u9488\u5bf9\u4f17\u5305\u5e73\u53f0\u53c2\u4e0e\u8005\u548c\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u9009\u624b\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u591a\u79cdLLM\u5bf9\u6570\u5b66\u95ee\u9898\u8da3\u5473\u6027\u548c\u96be\u5ea6\u7684\u8bc4\u4f30\u3002", "result": "LLM\u5728\u6570\u5b66\u8da3\u5473\u6027\u5224\u65ad\u4e0a\u4e0e\u4eba\u7c7b\u6709\u5927\u81f4\u5171\u8bc6\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u5224\u65ad\u7684\u5206\u5e03\u6a21\u5f0f\uff0c\u4e14\u4e0e\u4eba\u7c7b\u9009\u62e9\u7684\u8da3\u5473\u6027\u7406\u7531\u76f8\u5173\u6027\u8f83\u5f31\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u6355\u6349\u4eba\u7c7b\u6570\u5b66\u8da3\u5473\u6027\u5224\u65ad\u65b9\u9762\u65e2\u6709\u524d\u666f\u4e5f\u6709\u5c40\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u624d\u80fd\u6210\u4e3a\u6709\u6548\u7684\u6570\u5b66AI\u601d\u7ef4\u4f19\u4f34\u3002", "topic": "agent analysis"}}
{"id": "2511.08241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08241", "abs": "https://arxiv.org/abs/2511.08241", "authors": ["Zhihao Lin", "Lin Wu", "Zhen Tian", "Jianglin Lan"], "title": "PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore", "comment": null, "summary": "Exploration in reinforcement learning remains a critical challenge, as naive entropy maximization often results in high variance and inefficient policy updates. We introduce \\textbf{PrefPoE}, a novel \\textit{Preference-Product-of-Experts} framework that performs intelligent, advantage-guided exploration via the first principled application of product-of-experts (PoE) fusion for single-task exploration-exploitation balancing. By training a preference network to concentrate probability mass on high-advantage actions and fusing it with the main policy through PoE, PrefPoE creates a \\textbf{soft trust region} that stabilizes policy updates while maintaining targeted exploration. Across diverse control tasks spanning both continuous and discrete action spaces, PrefPoE demonstrates consistent improvements: +321\\% on HalfCheetah-v4 (1276~$\\rightarrow$~5375), +69\\% on Ant-v4, +276\\% on LunarLander-v2, with consistently enhanced training stability and sample efficiency. Unlike standard PPO, which suffers from entropy collapse, PrefPoE sustains adaptive exploration through its unique dynamics, thereby preventing premature convergence and enabling superior performance. Our results establish that learning \\textit{where to explore} through advantage-guided preferences is as crucial as learning how to act, offering a general framework for enhancing policy gradient methods across the full spectrum of reinforcement learning domains. Code and pretrained models are available in supplementary materials.", "AI": {"tldr": "PrefPoE\u662f\u4e00\u4e2a\u57fa\u4e8e\u504f\u597d-\u4e13\u5bb6\u4e58\u79ef\u6846\u67b6\u7684\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u52bf\u5f15\u5bfc\u7684\u667a\u80fd\u63a2\u7d22\u6765\u89e3\u51b3\u71b5\u6700\u5927\u5316\u5e26\u6765\u7684\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u63a7\u5236\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u65b9\u6cd5\uff08\u5982\u71b5\u6700\u5927\u5316\uff09\u901a\u5e38\u5bfc\u81f4\u9ad8\u65b9\u5dee\u548c\u4f4e\u6548\u7684\u7b56\u7565\u66f4\u65b0\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u63a2\u7d22\u673a\u5236\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "method": "\u63d0\u51faPrefPoE\u6846\u67b6\uff0c\u8bad\u7ec3\u504f\u597d\u7f51\u7edc\u6765\u96c6\u4e2d\u6982\u7387\u8d28\u91cf\u4e8e\u9ad8\u4f18\u52bf\u52a8\u4f5c\uff0c\u901a\u8fc7\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u878d\u5408\u4e0e\u4e3b\u7b56\u7565\uff0c\u521b\u5efa\u8f6f\u4fe1\u4efb\u533a\u57df\u6765\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u8fde\u7eed\u548c\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u5404\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1aHalfCheetah-v4\u63d0\u5347321%\uff0cAnt-v4\u63d0\u534769%\uff0cLunarLander-v2\u63d0\u5347276%\uff0c\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u4f18\u52bf\u5f15\u5bfc\u7684\u504f\u597d\u5b66\u4e60\u6765\u6307\u5bfc\u63a2\u7d22\u65b9\u5411\u4e0e\u5b66\u4e60\u5982\u4f55\u884c\u52a8\u540c\u7b49\u91cd\u8981\uff0c\u4e3a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u589e\u5f3a\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08585", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08585", "abs": "https://arxiv.org/abs/2511.08585", "authors": ["Jingtong Yue", "Ziqi Huang", "Zhaoxi Chen", "Xintao Wang", "Pengfei Wan", "Ziwei Liu"], "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap", "comment": "Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model", "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u6f14\u8fdb\u4e3a\u5305\u542b\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u548c\u89c6\u9891\u6e32\u67d3\u5668\u7684\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u4ec5\u80fd\u751f\u6210\u89c6\u89c9\u5185\u5bb9\uff0c\u8fd8\u80fd\u6a21\u62df\u7269\u7406\u52a8\u6001\u3001\u4ea4\u4e92\u548c\u4efb\u52a1\u89c4\u5212\uff0c\u6700\u7ec8\u5f62\u6210\u5177\u6709\u7269\u7406\u5408\u7406\u6027\u3001\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u591a\u5c3a\u5ea6\u89c4\u5212\u80fd\u529b\u7684\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u9886\u57df\u6b63\u4ece\u5355\u7eaf\u751f\u6210\u89c6\u89c9\u5438\u5f15\u529b\u7684\u7247\u6bb5\u8f6c\u5411\u6784\u5efa\u652f\u6301\u4ea4\u4e92\u548c\u4fdd\u6301\u7269\u7406\u5408\u7406\u6027\u7684\u865a\u62df\u73af\u5883\uff0c\u8fd9\u6307\u5411\u4e86\u89c6\u9891\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e0d\u4ec5\u4f5c\u4e3a\u89c6\u89c9\u751f\u6210\u5668\uff0c\u8fd8\u4f5c\u4e3a\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u3002", "method": "\u5c06\u73b0\u4ee3\u89c6\u9891\u57fa\u7840\u6a21\u578b\u6982\u5ff5\u5316\u4e3a\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u7ec4\u5408\uff1a\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u548c\u89c6\u9891\u6e32\u67d3\u5668\u3002\u4e16\u754c\u6a21\u578b\u7f16\u7801\u5173\u4e8e\u4e16\u754c\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u89c6\u9891\u6e32\u67d3\u5668\u5c06\u6f5c\u5728\u6a21\u62df\u8f6c\u5316\u4e3a\u903c\u771f\u7684\u89c6\u89c9\u89c2\u5bdf\u3002", "result": "\u8bba\u6587\u8ffd\u6eaf\u4e86\u89c6\u9891\u751f\u6210\u7684\u56db\u4e2a\u4ee3\u9645\u6f14\u8fdb\uff0c\u6bcf\u4e00\u4ee3\u6838\u5fc3\u80fd\u529b\u9010\u6b65\u63d0\u5347\uff0c\u6700\u7ec8\u5f62\u6210\u5177\u6709\u5185\u5728\u7269\u7406\u5408\u7406\u6027\u3001\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u8de8\u591a\u65f6\u7a7a\u5c3a\u5ea6\u89c4\u5212\u80fd\u529b\u7684\u4e16\u754c\u6a21\u578b\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u4e0b\u4e00\u4ee3\u4e16\u754c\u6a21\u578b\u7684\u5f00\u653e\u6311\u6218\u548c\u8bbe\u8ba1\u539f\u5219\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u667a\u80fd\u5728\u5851\u9020\u548c\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2511.08339", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08339", "abs": "https://arxiv.org/abs/2511.08339", "authors": ["Ruiyu Qiu", "Rui Wang", "Guanghui Yang", "Xiang Li", "Zhijiang Shao"], "title": "LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration", "comment": null, "summary": "Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.", "AI": {"tldr": "\u63d0\u51faLPPG-RL\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u68af\u5ea6\u6295\u5f71\u89e3\u51b3\u8bcd\u5178\u5e8f\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8fde\u7eed\u7a7a\u95f4\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u9608\u503c\u8c03\u6574\uff0c\u57282D\u5bfc\u822a\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u5177\u6709\u660e\u786e\u4f18\u5148\u7ea7\u7684\u8bcd\u5178\u5e8f\u591a\u76ee\u6807\u95ee\u9898\uff0c\u73b0\u6709LMORL\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u542f\u53d1\u5f0f\u9608\u503c\u8c03\u6574\uff0c\u8981\u4e48\u4ec5\u9650\u4e8e\u79bb\u6563\u57df\u3002", "method": "\u4f7f\u7528\u987a\u5e8f\u68af\u5ea6\u6295\u5f71\u8bc6\u522b\u53ef\u884c\u7684\u7b56\u7565\u66f4\u65b0\u65b9\u5411\uff0c\u5c06\u6295\u5f71\u6b65\u9aa4\u91cd\u65b0\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528Dykstra\u6295\u5f71\u800c\u975e\u901a\u7528\u6c42\u89e3\u5668\u52a0\u901f\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u5b50\u95ee\u9898\u63a2\u7d22\u9632\u6b62\u68af\u5ea6\u6d88\u5931\u3002", "result": "\u57282D\u5bfc\u822a\u73af\u5883\u4e2d\uff0cLPPG-RL\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8fde\u7eedLMORL\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u6536\u655b\u6027\u7406\u8bba\u4fdd\u8bc1\u548c\u653f\u7b56\u6539\u8fdb\u4e0b\u754c\u3002", "conclusion": "LPPG-RL\u662f\u4e00\u4e2a\u5e7f\u6cdb\u517c\u5bb9\u6240\u6709\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u7684LMORL\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e2d\u5c0f\u89c4\u6a21\u5b9e\u4f8b\uff0c\u901a\u8fc7\u68af\u5ea6\u6295\u5f71\u548c\u5b50\u95ee\u9898\u63a2\u7d22\u5b9e\u73b0\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08412", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08412", "abs": "https://arxiv.org/abs/2511.08412", "authors": ["Ruochuan Shi", "Runyu Lu", "Yuanheng Zhu", "Dongbin Zhao"], "title": "ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games", "comment": null, "summary": "In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86ARAC\u6846\u67b6\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u53d1\u6563\u6b63\u5219\u5316\u673a\u5236\uff0c\u89e3\u51b3\u56fe\u7ed3\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u5728\u8ffd\u6355\u548c\u5bf9\u6297\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5728\u56fe\u7ed3\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u4efb\u52a1\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u4ea4\u4e92\u4e0b\u534f\u8c03\uff0c\u7a00\u758f\u5956\u52b1\u963b\u788d\u4e86\u7b56\u7565\u5b66\u4e60\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u53c2\u8003\u7b56\u7565\u8fdb\u884c\u6709\u6548\u63a2\u7d22\uff0c\u53c8\u80fd\u907f\u514d\u7ee7\u627f\u5176\u5c40\u9650\u6027\u7684\u65b9\u6cd5\u3002", "method": "ARAC\u6846\u67b6\u96c6\u6210\u6ce8\u610f\u529b\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u667a\u80fd\u4f53\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u53d1\u6563\u6b63\u5219\u5316\u673a\u5236\u3002GNN\u8868\u8fbe\u56fe\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u548c\u72b6\u6001\u7279\u5f81\uff0c\u81ea\u9002\u5e94\u673a\u5236\u5728\u8bad\u7ec3\u65e9\u671f\u5229\u7528\u53c2\u8003\u7b56\u7565\u8fdb\u884c\u63a2\u7d22\uff0c\u540e\u671f\u51cf\u5c11\u4f9d\u8d56\u4ee5\u907f\u514d\u6b21\u4f18\u6536\u655b\u3002", "result": "\u5728\u8ffd\u6355\u548c\u5bf9\u6297\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0cARAC\u76f8\u6bd4\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u6700\u7ec8\u6210\u529f\u7387\uff0c\u5e76\u5728\u4e0d\u540c\u667a\u80fd\u4f53\u6570\u91cf\u4e0b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ARAC\u5728\u590d\u6742\u56fe\u7ed3\u6784\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u81ea\u9002\u5e94\u53d1\u6563\u6b63\u5219\u5316\u673a\u5236\u6709\u6548\u5e73\u8861\u4e86\u5229\u7528\u53c2\u8003\u7b56\u7565\u548c\u907f\u514d\u5176\u5c40\u9650\u6027\u7684\u6743\u8861\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2511.08567", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08567", "abs": "https://arxiv.org/abs/2511.08567", "authors": ["Hanqing Zhu", "Zhenyu Zhang", "Hanxian Huang", "DiJia Su", "Zechun Liu", "Jiawei Zhao", "Igor Fedorov", "Hamed Pirsiavash", "Zhizhou Sha", "Jinwon Lee", "David Z. Pan", "Zhangyang Wang", "Yuandong Tian", "Kai Sheng Tai"], "title": "The Path Not Taken: RLVR Provably Learns Off the Principals", "comment": "Preliminary version accepted as a spotlight in NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.\n  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.", "AI": {"tldr": "RLVR\uff08\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\u65f6\u8868\u73b0\u51fa\u53c2\u6570\u66f4\u65b0\u7684\u7a00\u758f\u6027\uff0c\u8fd9\u5b9e\u9645\u4e0a\u662f\u6a21\u578b\u6761\u4ef6\u4f18\u5316\u504f\u5dee\u7684\u8868\u9762\u73b0\u8c61\u3002\u7814\u7a76\u53d1\u73b0RLVR\u5728\u6743\u91cd\u7a7a\u95f4\u4e2d\u6cbf\u975e\u4e3b\u65b9\u5411\u5b66\u4e60\uff0c\u901a\u8fc7\u6700\u5c0f\u8c31\u6f02\u79fb\u3001\u51cf\u5c11\u4e3b\u5b50\u7a7a\u95f4\u65cb\u8f6c\u548c\u79bb\u4e3b\u66f4\u65b0\u5bf9\u9f50\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u800cSFT\u5219\u9488\u5bf9\u4e3b\u6743\u91cd\u5e76\u626d\u66f2\u8c31\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76RLVR\u8bad\u7ec3\u52a8\u6001\u4e2d\u7684\u53c2\u6570\u66f4\u65b0\u7a00\u758f\u6027\u6096\u8bba\uff0c\u63ed\u793aRLVR\u4e0eSFT\u5728\u4f18\u5316\u673a\u5236\u4e0a\u7684\u6839\u672c\u5dee\u5f02\uff0c\u4e3a\u8bbe\u8ba1RLVR\u539f\u751f\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e09\u95e8\u7406\u8bba\u89e3\u91caRLVR\u52a8\u6001\uff1a\u95e8I\uff08KL\u951a\u70b9\uff09\u65bd\u52a0KL\u7ea6\u675f\u66f4\u65b0\uff1b\u95e8II\uff08\u6a21\u578b\u51e0\u4f55\uff09\u5c06\u6b65\u957f\u5bfc\u5411\u4f4e\u66f2\u7387\u3001\u4fdd\u8c31\u5b50\u7a7a\u95f4\uff1b\u95e8III\uff08\u7cbe\u5ea6\uff09\u5728\u975e\u504f\u597d\u533a\u57df\u9690\u85cf\u5fae\u66f4\u65b0\u3002\u901a\u8fc7\u53c2\u6570\u7ea7\u5206\u6790\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u53d1\u73b0RLVR\u5728\u6743\u91cd\u7a7a\u95f4\u4e2d\u6cbf\u975e\u4e3b\u65b9\u5411\u5b66\u4e60\uff0c\u5b9e\u73b0\u6700\u5c0f\u8c31\u6f02\u79fb\u548c\u51cf\u5c11\u4e3b\u5b50\u7a7a\u95f4\u65cb\u8f6c\uff0c\u800cSFT\u626d\u66f2\u8c31\u7ed3\u6784\u4e14\u6027\u80fd\u843d\u540e\u4e8eRLVR\u3002\u53c2\u6570\u66f4\u65b0\u5177\u6709\u8de8\u8fd0\u884c\u3001\u6570\u636e\u96c6\u548cRL\u914d\u65b9\u7684\u5f3a\u4e00\u81f4\u6027\u3002", "conclusion": "RLVR\u5728\u4f18\u5316\u673a\u5236\u4e0a\u4e0eSFT\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0c\u76f4\u63a5\u5957\u7528SFT\u65f6\u4ee3\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\u3002\u7814\u7a76\u4e3aRLVR\u7684\u767d\u76d2\u7406\u89e3\u548c\u51e0\u4f55\u611f\u77e5\u7684RLVR\u539f\u751f\u7b97\u6cd5\u8bbe\u8ba1\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2511.003c276b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzed.dev%2Fblog%2Fai-70-problem-addy-osmani%3Futm_source=tldrdevops/1/0100019a6da89c99-6da40ba9-328b-40c9-9214-53060dd5e732-000000/gMg2lywpaCjFqmGg0blhrLpNFzQVuuKXwGE8nO1t0PQ=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzed.dev%2Fblog%2Fai-70-problem-addy-osmani%3Futm_source=tldrdevops/1/0100019a6da89c99-6da40ba9-328b-40c9-9214-53060dd5e732-000000/gMg2lywpaCjFqmGg0blhrLpNFzQVuuKXwGE8nO1t0PQ=430", "authors": ["TLDR Newsletter"], "title": "AI's 70% Problem", "comment": "Source: TLDR Newsletter, Date: 2025-11-10, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fzed.dev%2Fblog%2Fai-70-problem-addy-osmani%3Futm_source=tldrdevops/1/0100019a6da89c99-6da40ba9-328b-40c9-9214-53060dd5e732-000000/gMg2lywpaCjFqmGg0blhrLpNFzQVuuKXwGE8nO1t0PQ=430", "summary": "AI's 70% Problem (4 minute read) AI coding tools can produce about 70% of a solution quickly, but the remaining 30%\u2014covering edge cases, debugging, production integration, and security\u2014still demands human expertise. Despite growing adoption, trust in AI-generated code is declining. Developers need to prioritize human understanding, code review, and accountability as automation shifts bottlenecks from writing code to reviewing it.", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7801\u5de5\u5177\u80fd\u5feb\u901f\u751f\u6210\u7ea670%\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5269\u4e0b\u768430%\uff08\u5305\u62ec\u8fb9\u7f18\u60c5\u51b5\u5904\u7406\u3001\u8c03\u8bd5\u3001\u751f\u4ea7\u96c6\u6210\u548c\u5b89\u5168\u6027\uff09\u4ecd\u9700\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\u3002\u5c3d\u7ba1\u91c7\u7528\u7387\u589e\u957f\uff0c\u4f46\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u4fe1\u4efb\u5ea6\u5728\u4e0b\u964d\u3002", "motivation": "\u63a2\u8ba8AI\u7f16\u7801\u5de5\u5177\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u867d\u7136\u80fd\u5feb\u901f\u751f\u6210\u5927\u90e8\u5206\u4ee3\u7801\uff0c\u4f46\u5728\u5173\u952e\u73af\u8282\u4ecd\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u4ee5\u53ca\u4fe1\u4efb\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5206\u6790AI\u7f16\u7801\u5de5\u5177\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u8bc6\u522b\u5176\u5728\u8fb9\u7f18\u60c5\u51b5\u5904\u7406\u3001\u8c03\u8bd5\u3001\u751f\u4ea7\u96c6\u6210\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "result": "\u53d1\u73b0AI\u5de5\u5177\u53ea\u80fd\u89e3\u51b3\u7ea670%\u7684\u7f16\u7801\u95ee\u9898\uff0c\u5269\u4f5930%\u7684\u5173\u952e\u90e8\u5206\u9700\u8981\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u5bf9AI\u751f\u6210\u4ee3\u7801\u7684\u4fe1\u4efb\u5ea6\u5448\u4e0b\u964d\u8d8b\u52bf\u3002", "conclusion": "\u5f00\u53d1\u8005\u9700\u8981\u4f18\u5148\u8003\u8651\u4eba\u5de5\u7406\u89e3\u3001\u4ee3\u7801\u5ba1\u67e5\u548c\u95ee\u8d23\u5236\uff0c\u56e0\u4e3a\u81ea\u52a8\u5316\u5c06\u74f6\u9888\u4ece\u7f16\u5199\u4ee3\u7801\u8f6c\u79fb\u5230\u5ba1\u67e5\u4ee3\u7801\u3002", "topic": "agent analysis"}}
{"id": "tldr.2511.a2e9479f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F11%2F06%2Fbuilt-technologies-ai-draw-agent-cre-lending.html%3Futm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/lrPggasAUf9PrS8EBhM7TbWWwo8i2_YMeHralCenb2Q=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F11%2F06%2Fbuilt-technologies-ai-draw-agent-cre-lending.html%3Futm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/lrPggasAUf9PrS8EBhM7TbWWwo8i2_YMeHralCenb2Q=430", "authors": ["TLDR Newsletter"], "title": "Built Technologies launches AI agent to automate CRE lending", "comment": "Source: TLDR Newsletter, Date: 2025-11-10, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F11%2F06%2Fbuilt-technologies-ai-draw-agent-cre-lending.html%3Futm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/lrPggasAUf9PrS8EBhM7TbWWwo8i2_YMeHralCenb2Q=430", "summary": "Built Technologies launches AI agent to automate CRE lending (3 minute read) Built Technologies, a $1.5B-valued fintech focused on construction and real estate finance, has launched an AI agent to automate draw requests\u2014the process developers use to access new tranches of loan funding. The \u201cDraw Agent\u201d processes approvals up to 95% faster, with 400% higher risk detection and full compliance to lender policies. Used by banks like US Bank, Citi, and Fifth Third, the agent has delivered 300\u2013500%...", "source": "tldr", "AI": {"tldr": "Built Technologies\u63a8\u51faAI\u4ee3\u7406\u81ea\u52a8\u5316\u5546\u4e1a\u5730\u4ea7\u8d37\u6b3e\u63d0\u6b3e\u6d41\u7a0b\uff0c\u5904\u7406\u901f\u5ea6\u63d0\u534795%\uff0c\u98ce\u9669\u68c0\u6d4b\u63d0\u9ad8400%", "motivation": "\u4f20\u7edf\u5546\u4e1a\u5730\u4ea7\u8d37\u6b3e\u63d0\u6b3e\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u6765\u63d0\u9ad8\u5904\u7406\u901f\u5ea6\u548c\u98ce\u9669\u68c0\u6d4b\u80fd\u529b", "method": "\u5f00\u53d1AI\u4ee3\u7406\u7cfb\u7edf\"Draw Agent\"\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8d37\u6b3e\u63d0\u6b3e\u8bf7\u6c42\u5904\u7406\uff0c\u786e\u4fdd\u5b8c\u5168\u7b26\u5408\u8d37\u6b3e\u673a\u6784\u653f\u7b56", "result": "\u5904\u7406\u901f\u5ea6\u63d0\u534795%\uff0c\u98ce\u9669\u68c0\u6d4b\u80fd\u529b\u63d0\u9ad8400%\uff0c\u5df2\u88ab\u7f8e\u56fd\u94f6\u884c\u3001\u82b1\u65d7\u94f6\u884c\u7b49\u4e3b\u8981\u94f6\u884c\u91c7\u7528", "conclusion": "AI\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u5347\u5546\u4e1a\u5730\u4ea7\u8d37\u6b3e\u6d41\u7a0b\u7684\u6548\u7387\u548c\u98ce\u9669\u7ba1\u7406\u6c34\u5e73", "topic": "swe application"}}
{"id": "tldr.2511.b35c7510", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.retailbankerinternational.com%2Fnews%2Flloyds-banking-ai-financial-assistant%2F%3Fcf-view%26utm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/iw-lLFC6Pe7a_709qRSpgFSHlBydOLaS5Gs_RV60G7o=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.retailbankerinternational.com%2Fnews%2Flloyds-banking-ai-financial-assistant%2F%3Fcf-view%26utm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/iw-lLFC6Pe7a_709qRSpgFSHlBydOLaS5Gs_RV60G7o=430", "authors": ["TLDR Newsletter"], "title": "Lloyds Banking to launch AI-powered financial assistant in 2026", "comment": "Source: TLDR Newsletter, Date: 2025-11-10, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.retailbankerinternational.com%2Fnews%2Flloyds-banking-ai-financial-assistant%2F%3Fcf-view%26utm_source=tldrfintech/1/0100019a6e1697ff-5da5fabd-2b8c-4f66-9cab-97ce23116f55-000000/iw-lLFC6Pe7a_709qRSpgFSHlBydOLaS5Gs_RV60G7o=430", "summary": "Lloyds Banking to launch AI-powered financial assistant in 2026 (5 minute read) Lloyds is preparing to roll out an AI-driven conversational financial assistant in early 2026, marking the UK's first use of agentic AI in consumer banking. The assistant, already being tested by 7,000 employees, will help users manage spending, savings, and investments through natural dialogue in the Lloyds app. Over time, it's expected to handle mortgages and car finance, giving 21 million customers personalized...", "source": "tldr", "AI": {"tldr": "Lloyds Banking\u5c06\u57282026\u5e74\u63a8\u51faAI\u9a71\u52a8\u7684\u91d1\u878d\u52a9\u624b\uff0c\u8fd9\u662f\u82f1\u56fd\u6d88\u8d39\u94f6\u884c\u4e1a\u9996\u6b21\u4f7f\u7528\u667a\u80fd\u4ee3\u7406AI", "motivation": "\u4e3a2100\u4e07\u5ba2\u6237\u63d0\u4f9b\u4e2a\u6027\u5316\u91d1\u878d\u670d\u52a1\uff0c\u901a\u8fc7\u81ea\u7136\u5bf9\u8bdd\u5e2e\u52a9\u7528\u6237\u7ba1\u7406\u652f\u51fa\u3001\u50a8\u84c4\u548c\u6295\u8d44", "method": "\u5f00\u53d1AI\u9a71\u52a8\u7684\u5bf9\u8bdd\u5f0f\u91d1\u878d\u52a9\u624b\uff0c\u5df2\u57287000\u540d\u5458\u5de5\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u901a\u8fc7Lloyds\u5e94\u7528\u63d0\u4f9b\u81ea\u7136\u5bf9\u8bdd\u754c\u9762", "result": "\u52a9\u624b\u6b63\u5728\u6d4b\u8bd5\u4e2d\uff0c\u9884\u8ba1\u5c06\u9010\u6b65\u5904\u7406\u62b5\u62bc\u8d37\u6b3e\u548c\u6c7d\u8f66\u91d1\u878d\u7b49\u66f4\u590d\u6742\u7684\u91d1\u878d\u670d\u52a1", "conclusion": "\u8fd9\u662f\u82f1\u56fd\u94f6\u884c\u4e1a\u9996\u6b21\u5e94\u7528\u667a\u80fd\u4ee3\u7406AI\uff0c\u6807\u5fd7\u7740\u91d1\u878d\u670d\u52a1\u5411\u66f4\u4e2a\u6027\u5316\u548c\u5bf9\u8bdd\u5f0f\u4f53\u9a8c\u7684\u8f6c\u53d8", "topic": "swe application"}}
{"id": "tldr.2511.47d59a8d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecureannex.com%2Fblog%2Fransomvibe%2F%3Futm_source=tldrinfosec/1/0100019a6e18786c-4be738f4-f44c-42bb-b92d-cb2d4af5653b-000000/c8jsS5_7bKiVlcpbTx5vgVXloY5hYk8mdSMHlki_lKo=430", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecureannex.com%2Fblog%2Fransomvibe%2F%3Futm_source=tldrinfosec/1/0100019a6e18786c-4be738f4-f44c-42bb-b92d-cb2d4af5653b-000000/c8jsS5_7bKiVlcpbTx5vgVXloY5hYk8mdSMHlki_lKo=430", "authors": ["TLDR Newsletter"], "title": "Ransomvibing appears in VS Code extensions", "comment": "Source: TLDR Newsletter, Date: 2025-11-10, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsecureannex.com%2Fblog%2Fransomvibe%2F%3Futm_source=tldrinfosec/1/0100019a6e18786c-4be738f4-f44c-42bb-b92d-cb2d4af5653b-000000/c8jsS5_7bKiVlcpbTx5vgVXloY5hYk8mdSMHlki_lKo=430", "summary": "Ransomvibing appears in VS Code extensions (5 minute read) A ransomware-infected VS Code extension, \"susvsex,\" was found on the Visual Studio Marketplace. It utilizes GitHub for command and control, encrypts files, and uploads them for extortion, but its malicious intent was clearly evident. Due to poor coding and a hardcoded decryption key, its threat is currently low. The incident highlights gaps in extension security.", "source": "tldr", "AI": {"tldr": "\u5728VS Code\u6269\u5c55\u4e2d\u53d1\u73b0\u52d2\u7d22\u8f6f\u4ef6\u611f\u67d3\uff0c\u540d\u4e3a'susvsex'\u7684\u6269\u5c55\u901a\u8fc7GitHub\u8fdb\u884c\u547d\u4ee4\u63a7\u5236\uff0c\u52a0\u5bc6\u6587\u4ef6\u5e76\u52d2\u7d22\uff0c\u4f46\u7531\u4e8e\u4ee3\u7801\u8d28\u91cf\u5dee\u548c\u786c\u7f16\u7801\u89e3\u5bc6\u5bc6\u94a5\uff0c\u76ee\u524d\u5a01\u80c1\u8f83\u4f4e\u3002", "motivation": "\u63ed\u793aVS Code\u6269\u5c55\u5e02\u573a\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u9192\u5f00\u53d1\u8005\u6ce8\u610f\u6269\u5c55\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5206\u6790\u6076\u610f\u6269\u5c55'susvsex'\u7684\u6280\u672f\u5b9e\u73b0\uff0c\u5305\u62ec\u5176\u4f7f\u7528GitHub\u8fdb\u884cC&C\u901a\u4fe1\u3001\u6587\u4ef6\u52a0\u5bc6\u673a\u5236\u548c\u52d2\u7d22\u6d41\u7a0b\u3002", "result": "\u53d1\u73b0\u8be5\u52d2\u7d22\u8f6f\u4ef6\u6269\u5c55\u5b58\u5728\u4e25\u91cd\u7f16\u7801\u7f3a\u9677\uff0c\u786c\u7f16\u7801\u89e3\u5bc6\u5bc6\u94a5\u4f7f\u5176\u5a01\u80c1\u6027\u5927\u5927\u964d\u4f4e\uff0c\u4f46\u66b4\u9732\u4e86\u6269\u5c55\u5e02\u573a\u7684\u5b89\u5168\u5ba1\u67e5\u4e0d\u8db3\u3002", "conclusion": "VS Code\u6269\u5c55\u5e02\u573a\u9700\u8981\u52a0\u5f3a\u5b89\u5168\u5ba1\u67e5\u673a\u5236\uff0c\u5f00\u53d1\u8005\u5e94\u8c28\u614e\u9009\u62e9\u548c\u4f7f\u7528\u7b2c\u4e09\u65b9\u6269\u5c55\u3002", "topic": "swe application"}}
{"id": "wechat.2511.4d17a0f0", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516960&idx=1&sn=394c2d9a49a3d356a887e72847ddfb87&chksm=9a929a082ea937cd3e291fb04fc9708fbcf0e01aa1140cdbbf500b9672a4eb81cb3d32073cf2#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516960&idx=1&sn=394c2d9a49a3d356a887e72847ddfb87&chksm=9a929a082ea937cd3e291fb04fc9708fbcf0e01aa1140cdbbf500b9672a4eb81cb3d32073cf2#rd", "authors": ["\u6708\u6765\u5ba2\u6808"], "title": "\u6df1\u5ea6<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b Vanilla Policy Gradient \u4ece\u96f6\u5b9e\u73b0", "comment": "Source: WeChat, Published: 2025-11-12 12:32:07", "summary": "\u56e0\u4e3a\u5728\u6574\u4e2a\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7b56\u7565\u51fd\u6570\u4f7f\u7528\u7684\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u6240\u4ee5\u53c8\u5c06\u5176\u79f0\u4e4b\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u3002\u8fd9\u91cc\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u4e24\u5c42\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u793a\u4f8b\u4ee3\u7801\u5982\u4e0b\uff1a1 class PolicyNet\uff08nn.Module\uff09\uff1a", "AI": {"tldr": "\u56e0\u4e3a\u5728\u6574\u4e2a\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7b56\u7565\u51fd\u6570\u4f7f\u7528\u7684\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u6240\u4ee5\u53c8\u5c06\u5176\u79f0\u4e4b\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u3002\u8fd9\u91cc\u9996\u5148\u5b9a\u4e49\u4e00\u4e2a\u4e24\u5c42\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u793a\u4f8b\u4ee3\u7801\u5982\u4e0b\uff1a1 class PolicyNet\uff08nn.Module\uff09\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.40a3d571", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3Mzc0NDEzOQ==&mid=2650065872&idx=2&sn=470b40f7c8c60c6d13d9630b8d426148&chksm=86a564d5f42987a37a51091c3752def1d4d5903229501afcf0bdbdd99829633af4af656ea6b5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3Mzc0NDEzOQ==&mid=2650065872&idx=2&sn=470b40f7c8c60c6d13d9630b8d426148&chksm=86a564d5f42987a37a51091c3752def1d4d5903229501afcf0bdbdd99829633af4af656ea6b5#rd", "authors": ["\u519b\u4e8b\u6587\u6458"], "title": "\u5175\u68cb\u63a8\u6f14\u4e2d<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u5e94\u7528\u7814\u7a76", "comment": "Source: WeChat, Published: 2025-11-12 10:09:50", "summary": "\u5175\u68cb\u63a8\u6f14\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u4f53\u7cfb\u8bbe\u8ba1 \u4e3a\u8ba9AI\u5728\u5175\u68cb\u63a8\u6f14\u4e2d\u5b66\u4e60\u6218\u672f\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b\u63a8\u6f14\u73af\u5883\u3001\u6218\u6597\u89c4\u5219\u3001\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u5728\u5185\u7684\u5b8c\u6574\u4f53\u7cfb\uff0c\u6bcf\u4e00\u90e8\u5206\u5747\u8d34\u5408\u5175\u68cb\u63a8\u6f14\u7684\u6838\u5fc3\u903b\u8f91\u3002", "AI": {"tldr": "\u5175\u68cb\u63a8\u6f14\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u4f53\u7cfb\u8bbe\u8ba1 \u4e3a\u8ba9AI\u5728\u5175\u68cb\u63a8\u6f14\u4e2d\u5b66\u4e60\u6218\u672f\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b\u63a8\u6f14\u73af\u5883\u3001\u6218\u6597\u89c4\u5219\u3001\u5f3a\u5316\u5b66\u4e60\u5de5\u5177\u5728\u5185\u7684\u5b8c\u6574\u4f53\u7cfb\uff0c\u6bcf\u4e00\u90e8\u5206\u5747\u8d34\u5408\u5175\u68cb\u63a8\u6f14\u7684\u6838\u5fc3\u903b\u8f91\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.b17ccc24", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247648721&idx=2&sn=197a107dbec3567392542ca111d25572&chksm=faf5e7b90b4d78aeadd5ccf8b62acb5f520f2b5d6eedbd1cd2e2e8e796a7a6e8ab1b9797efa5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247648721&idx=2&sn=197a107dbec3567392542ca111d25572&chksm=faf5e7b90b4d78aeadd5ccf8b62acb5f520f2b5d6eedbd1cd2e2e8e796a7a6e8ab1b9797efa5#rd", "authors": ["AI\u524d\u7ebf"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em> AI \u7cfb\u7edf\u7684\u8bbe\u8ba1\u5b9e\u73b0\u53ca\u672a\u6765\u53d1\u5c55", "comment": "Source: WeChat, Published: 2025-11-12 04:52:01", "summary": "\u4ece\u5e38\u89c1\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5230\u57fa\u4e8e\u5baa\u6cd5\u7684\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u518d\u5230\u5982\u4eca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0d\u65ad\u8fdb\u6b65\u7684\u8fc7\u7a0b\uff0c\u5b9e\u9645\u4e0a\u4ee3\u8868\u7740\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u4fe1\u53f7\u6765\u6e90\u65e5\u76ca\u5e7f\u6cdb\uff0c\u540c\u65f6\u4efb\u52a1\u96be\u5ea6\u4e5f\u5728\u4e0d\u65ad\u63d0\u9ad8\u3002", "AI": {"tldr": "\u4ece\u5e38\u89c1\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u5230\u57fa\u4e8e\u5baa\u6cd5\u7684\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff0c\u518d\u5230\u5982\u4eca\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0d\u65ad\u8fdb\u6b65\u7684\u8fc7\u7a0b\uff0c\u5b9e\u9645\u4e0a\u4ee3\u8868\u7740\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\u7684\u4fe1\u53f7\u6765\u6e90\u65e5\u76ca\u5e7f\u6cdb\uff0c\u540c\u65f6\u4efb\u52a1\u96be\u5ea6\u4e5f\u5728\u4e0d\u65ad\u63d0\u9ad8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.ee261c88", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMDY5OTk4OA==&mid=2247487269&idx=1&sn=292026cb5d04c7f8fc58ad30a53b7606&chksm=c1838b611859af4fab740602ee7decc780d0af258545ed55dafcd097f09997db1ce1d9a8bdbd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMDY5OTk4OA==&mid=2247487269&idx=1&sn=292026cb5d04c7f8fc58ad30a53b7606&chksm=c1838b611859af4fab740602ee7decc780d0af258545ed55dafcd097f09997db1ce1d9a8bdbd#rd", "authors": ["\u5c0f\u738b\u804a\u7f16\u7a0b"], "title": "\u76f4\u63a5\u62ff\u634f\uff01\u53ea\u752838\u9875PPT\u5b66\u4f1a\u4e86<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u516b\u5927\u7b97\u6cd5\uff0c\u5168\u9762\u89e3\u6790\uff01", "comment": "Source: WeChat, Published: 2025-11-12 04:14:53", "summary": "\u6cd5 \u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u5b9e\u65f6\u4ea4\u4e92\u6765\u5b66\u4e60Q\u51fd\u6570\uff0c\u5373\u5728\u7ed9\u5b9a\u72b6\u6001\u4e0b\u91c7\u53d6\u7279\u5b9a\u52a8\u4f5c\u7684\u9884\u671f\u56de\u62a5\u3002\u5f3a\u5316\u5b66\u4e60\u7b80\u4ecb\u548c\u5e94\u7528 ppo\u7b97\u6cd5\u4e0e\u516c\u5f0f\u63a8\u5bfc ppo\u5b9e\u6218-\u6708\u7403\u767b\u9646\u5668\u8bad\u7ec3\u5b9e\u4f8b q-learning\u4e0eoqn\u7b97\u6cd5\u3002", "AI": {"tldr": "\u6cd5 \u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u5dee\u5206\u5b66\u4e60\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u5b9e\u65f6\u4ea4\u4e92\u6765\u5b66\u4e60Q\u51fd\u6570\uff0c\u5373\u5728\u7ed9\u5b9a\u72b6\u6001\u4e0b\u91c7\u53d6\u7279\u5b9a\u52a8\u4f5c\u7684\u9884\u671f\u56de\u62a5\u3002\u5f3a\u5316\u5b66\u4e60\u7b80\u4ecb\u548c\u5e94\u7528 ppo\u7b97\u6cd5\u4e0e\u516c\u5f0f\u63a8\u5bfc ppo\u5b9e\u6218-\u6708\u7403\u767b\u9646\u5668\u8bad\u7ec3\u5b9e\u4f8b q-learning\u4e0eoqn\u7b97\u6cd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2511.a1815328", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247673270&idx=4&sn=54f36ae320a972bb2899bd0f40587826&chksm=e9654e6a605909322e2c47a02e547a7b8e43e7d063e2c76e0791a7c8a5c90f5f02d5e66e4260#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMzk1MDE3Nw==&mid=2247673270&idx=4&sn=54f36ae320a972bb2899bd0f40587826&chksm=e9654e6a605909322e2c47a02e547a7b8e43e7d063e2c76e0791a7c8a5c90f5f02d5e66e4260#rd", "authors": ["\u56fe\u7075\u4eba\u5de5\u667a\u80fd"], "title": "\u4e0a\u4ea4\u535a\u58eb\u6700\u65b0\u601d\u8003\uff1a\u4ec5\u7528\u4e24\u4e2a\u95ee\u9898\u8bb2\u6e05<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-11-11 16:02:22", "summary": "02 \u5b66\u4e60\u66f4\u65b0\u7684\u8282\u594f \u800c\u5f3a\u5316\u5b66\u4e60\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\uff0c\u662f\u5b66\u4e60\u66f4\u65b0\u7684\u8282\u594f\u3002\u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662f\u667a\u80fd\u4f53\u591a\u4e45\u8bc4\u4f30\u4e00\u6b21\u7b56\u7565\uff0c\u53c8\u591a\u4e45\u8c03\u6574\u4e00\u6b21\u884c\u4e3a\u3002\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u662f\u4e00\u79cd\u201c\u4e00\u6b65\u5f0f\u5b66\u4e60\u201d\u3002", "AI": {"tldr": "02 \u5b66\u4e60\u66f4\u65b0\u7684\u8282\u594f \u800c\u5f3a\u5316\u5b66\u4e60\u7684\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\uff0c\u662f\u5b66\u4e60\u66f4\u65b0\u7684\u8282\u594f\u3002\u7b80\u5355\u6765\u8bf4\uff0c\u5c31\u662f\u667a\u80fd\u4f53\u591a\u4e45\u8bc4\u4f30\u4e00\u6b21\u7b56\u7565\uff0c\u53c8\u591a\u4e45\u8c03\u6574\u4e00\u6b21\u884c\u4e3a\u3002\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u662f\u4e00\u79cd\u201c\u4e00\u6b65\u5f0f\u5b66\u4e60\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.4c63448f", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3ODUzOTkyMA==&mid=2247486417&idx=2&sn=5ace61a7f2ea37b45d913d6347780863&chksm=9ee9935443f1139df82502f140eb3f6e27fea6c35b4f064bbe2ff702b8e7064c8f75ad354cc9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3ODUzOTkyMA==&mid=2247486417&idx=2&sn=5ace61a7f2ea37b45d913d6347780863&chksm=9ee9935443f1139df82502f140eb3f6e27fea6c35b4f064bbe2ff702b8e7064c8f75ad354cc9#rd", "authors": ["\u8d4b\u8303\u5927\u6a21\u578b\u6280\u672f\u5708"], "title": "<em class=\"highlight\">Agentic</em> RL\u8be6\u89e3\uff1a\u6253\u9020\u81ea\u4e3b\u5b66\u4e60\u81ea\u4e3b\u8fed\u4ee3\u7684\u9ad8\u6027\u80fd Agent", "comment": "Source: WeChat, Published: 2025-11-12 10:34:39", "summary": "\u5168\u90e8\u6587\u4ef6/Agentic RL\u5165\u95e8\u5b9e\u6218\u5df2\u5168\u90e8\u52a0\u8f7d\uff0c\u51715\u4e2a 88\u6587\u4ef6\u540d \u5927\u5c0f \u7c7b\u578b \u4fee\u6539\u65f6\u95f4\u9879\u76ee\u6e90\u7801 \u6587\u4ef6\u5939 2025.11.09 19\uff1a36\u8bfe\u4ef6&\u4ee3\u7801 \u6587\u4ef6\u5939 2025.11.09 19\uff1a36\u5fae\u8c03SQL\u6570\u636e\u96c6 \u6587\u4ef6\u5939 2025.11.09 19\uff1a36", "AI": {"tldr": "\u5168\u90e8\u6587\u4ef6/Agentic RL\u5165\u95e8\u5b9e\u6218\u5df2\u5168\u90e8\u52a0\u8f7d\uff0c\u51715\u4e2a 88\u6587\u4ef6\u540d \u5927\u5c0f \u7c7b\u578b \u4fee\u6539\u65f6\u95f4\u9879\u76ee\u6e90\u7801 \u6587\u4ef6\u5939 2025.11.09 19\uff1a36\u8bfe\u4ef6&\u4ee3\u7801 \u6587\u4ef6\u5939 2025.11.09 19\uff1a36\u5fae\u8c03SQL\u6570\u636e\u96c6 \u6587\u4ef6\u5939 2025.11.09 19\uff1a36", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.6b8be01a", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4OTQyNzA0Ng==&mid=2247542420&idx=2&sn=614e3aa2592af4653540f80821a802f3&chksm=ed3adfc850819b9872fbbc14e4666f9931135e33760bef7f14d593ca3feafa33d0e05eac6c42#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4OTQyNzA0Ng==&mid=2247542420&idx=2&sn=614e3aa2592af4653540f80821a802f3&chksm=ed3adfc850819b9872fbbc14e4666f9931135e33760bef7f14d593ca3feafa33d0e05eac6c42#rd", "authors": ["\u8bed\u97f3\u4e4b\u5bb6"], "title": "2026\u62e5\u62b1Agent\u6b63\u5f53\u65f6\uff0c\u9996\u5c4a <em class=\"highlight\">Agentic</em> AI \u5cf0\u4f1a\u805a\u96c6\u4eac\u4e1c\u3001\u8682\u8681\u3001\u8bb0\u5fc6\u5f20\u91cf\u3001\u5feb\u624b\u7b49\u4f01\u4e1a\u5b9e\u8df5\u8bdd\u9898", "comment": "Source: WeChat, Published: 2025-11-12 10:02:00", "summary": "Agentic AI Summit 2026\u5c06\u4e8e2026\u5e741\u670816-17\u65e5\u5728\u4e2d\u5173\u6751\u5c55\u793a\u4e2d\u5fc3\u4f1a\u8bae\u4e2d\u5fc3\u4e3e\u529e\uff0c\u6c47\u805a500+\u6280\u672f\u7cbe\u82f1\uff0c\u8986\u76d6Agent\u6280\u672f\u5168\u94fe\u8def\u3002\u5cf0\u4f1a\u8bbe\u7f6e\u4e86\u4e5d\u5927\u4e13\u9898\u5206\u8bba\u575b\uff0c\u4ece\u5e95\u5c42\u6a21\u578b\u5230\u4e0a\u5c42\u5e94\u7528\u5f62\u6210\u5b8c\u6574\u95ed\u73af\uff0c\u6b64\u6b21\u4f1a\u8bae\u5c06\u9080\u8bf7\u4e1a\u754c\u4e13\u5bb6\u6765\u5206\u4eab\u56e2\u961f\u7684\u5b9e\u8df5\u8bdd\u9898", "AI": {"tldr": "Agentic AI Summit 2026\u5c06\u4e8e2026\u5e741\u670816-17\u65e5\u5728\u4e2d\u5173\u6751\u5c55\u793a\u4e2d\u5fc3\u4f1a\u8bae\u4e2d\u5fc3\u4e3e\u529e\uff0c\u6c47\u805a500+\u6280\u672f\u7cbe\u82f1\uff0c\u8986\u76d6Agent\u6280\u672f\u5168\u94fe\u8def\u3002\u5cf0\u4f1a\u8bbe\u7f6e\u4e86\u4e5d\u5927\u4e13\u9898\u5206\u8bba\u575b\uff0c\u4ece\u5e95\u5c42\u6a21\u578b\u5230\u4e0a\u5c42\u5e94\u7528\u5f62\u6210\u5b8c\u6574\u95ed\u73af\uff0c\u6b64\u6b21\u4f1a\u8bae\u5c06\u9080\u8bf7\u4e1a\u754c\u4e13\u5bb6\u6765\u5206\u4eab\u56e2\u961f\u7684\u5b9e\u8df5\u8bdd\u9898", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6fee8339", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&mid=2649281552&idx=1&sn=42b297f288241cca018a30c472224bc7&chksm=86af81e011b6e5b96ff6894f9595c228e21b64e49ed3bca2d93b8c4bf712622fd50b710a0916#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&mid=2649281552&idx=1&sn=42b297f288241cca018a30c472224bc7&chksm=86af81e011b6e5b96ff6894f9595c228e21b64e49ed3bca2d93b8c4bf712622fd50b710a0916#rd", "authors": ["Qunar\u6280\u672f\u6c99\u9f99"], "title": "\u4ece\u5355<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u5230\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u534f\u4f5c\uff1a<em class=\"highlight\">Agentic</em> System\u7684\u6f14\u8fdb\u4e0eLangGraph4j\u5b9e\u6218", "comment": "Source: WeChat, Published: 2025-11-12 10:01:42", "summary": "2.2 \u4e1a\u754c\u5b9a\u4e49\u4e0e\u5171\u8bc6-Agentic SystemOpenAI\u5bf9\u4e8eAgent\u7684\u5b9a\u4e49Agent \u662f\u201c\u80fd\u4ee3\u8868\u4f60\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u7684\u7cfb\u7edf\u201dAnthropic\u5bf9\u4e8eAgent\u7684\u5b9a\u4e49\"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate independently over extended periods\uff0c us", "AI": {"tldr": "2.2 \u4e1a\u754c\u5b9a\u4e49\u4e0e\u5171\u8bc6-Agentic SystemOpenAI\u5bf9\u4e8eAgent\u7684\u5b9a\u4e49Agent \u662f\u201c\u80fd\u4ee3\u8868\u4f60\u72ec\u7acb\u5b8c\u6210\u4efb\u52a1\u7684\u7cfb\u7edf\u201dAnthropic\u5bf9\u4e8eAgent\u7684\u5b9a\u4e49\"Agent\" can be defined in several ways. Some customers define agents as fully autonomous systems that operate indepen...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.22df86f7", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483772&idx=1&sn=1eb37c739fc9e3f21644bb38f562c21a&chksm=f16c3b058f7b85c1ef7e6d28ac5a8111e57b9c5d69063d80aecbd097b4efc7cf347e9e157089#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYzNzE2ODIxMg==&mid=2247483772&idx=1&sn=1eb37c739fc9e3f21644bb38f562c21a&chksm=f16c3b058f7b85c1ef7e6d28ac5a8111e57b9c5d69063d80aecbd097b4efc7cf347e9e157089#rd", "authors": ["EasyShip.AI"], "title": "\u5434\u6069\u8fbe\u6559\u6388Ai\u8bfe\u7a0b\u7b2c\u4e00\u8bfe\uff1a\u4ec0\u4e48\u662f<em class=\"highlight\">Agentic</em> Ai\uff1f", "comment": "Source: WeChat, Published: 2025-11-12 08:59:04", "summary": "\u8bfe\u7a0b\u603b\u5171\u5206\u4e3a5\u4e2a\u90e8\u5206\uff1a- Introduction to Agentic Workflows\uff1aAgentic\u5de5\u4f5c\u6d41\u4ecb\u7ecd\uff1b- Reflection Design Pattern\uff1a\u81ea\u6211\u53cd\u601dAgent\u8bbe\u8ba1\u6a21\u5f0f\uff1b- Tool use\uff1a\u5de5\u5177\u4f7f\u7528\uff1b- Practical Tips for Building Agentic AI\uff1a\u6784\u5efaAgent\u7684\u5b9e\u7528\u6280\u5de7\uff1b", "AI": {"tldr": "\u8bfe\u7a0b\u603b\u5171\u5206\u4e3a5\u4e2a\u90e8\u5206\uff1a- Introduction to Agentic Workflows\uff1aAgentic\u5de5\u4f5c\u6d41\u4ecb\u7ecd\uff1b- Reflection Design Pattern\uff1a\u81ea\u6211\u53cd\u601dAgent\u8bbe\u8ba1\u6a21\u5f0f\uff1b- Tool use\uff1a\u5de5\u5177\u4f7f\u7528\uff1b- Practical Tips for Building Agentic AI\uff1a\u6784\u5efaAgent\u7684\u5b9e\u7528\u6280\u5de7\uff1b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.48f5b356", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI0OTAzNTEwMw==&mid=2247488508&idx=1&sn=3eec357313db4e7b3cea265979209abb&chksm=e85cf4c48439b8909c6aeabf968e6f17881aab19c20f38bbea5f06f82c1625aa70c2b07a3f5a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI0OTAzNTEwMw==&mid=2247488508&idx=1&sn=3eec357313db4e7b3cea265979209abb&chksm=e85cf4c48439b8909c6aeabf968e6f17881aab19c20f38bbea5f06f82c1625aa70c2b07a3f5a#rd", "authors": ["AgenticAI"], "title": "\u8c37\u6b4c\u5206\u4eab54\u9875AI Agent\u4ecb\u7ecd - \u4e94\u5c42<em class=\"highlight\">Agentic</em>\u7cfb\u7edf - Agent\u6838\u5fc3\u67b6\u6784\uff1a\u6a21\u578b\u5de5\u5177\uff0c\u7f16\u6392\uff0c\u8fdb\u5316\u4e0e\u81ea\u5b66\u4e60 - \u9ad8\u9636A", "comment": "Source: WeChat, Published: 2025-11-12 01:47:21", "summary": "\u8c37\u6b4c\u5206\u4eab54\u9875ai agent\u4ecb \u7ecd - \u4e94\u5c42agentic\u7cfb\u7edf - agent\u6838\u5fc3\u67b6\u6784\uff1a\u6a21\u578b \u5de5\u5177\uff0c\u7f16\u6392\uff0c\u8fdb\u5316\u4e0e\u81ea\u5b66 \u4e60 - \u9ad8\u9636agent\u793a\u4f8b\uff0c\u5305\u542b \u81ea\u8fdb\u5316\u3002alphaevolve level 4\uff1a self evolving agents level 3\uff1a the rise of collaborative multi-agent systems level 2\uff1a the strategic problem-sol", "AI": {"tldr": "\u8c37\u6b4c\u5206\u4eab54\u9875ai agent\u4ecb \u7ecd - \u4e94\u5c42agentic\u7cfb\u7edf - agent\u6838\u5fc3\u67b6\u6784\uff1a\u6a21\u578b \u5de5\u5177\uff0c\u7f16\u6392\uff0c\u8fdb\u5316\u4e0e\u81ea\u5b66 \u4e60 - \u9ad8\u9636agent\u793a\u4f8b\uff0c\u5305\u542b \u81ea\u8fdb\u5316\u3002alphaevolve level 4\uff1a self evolving agents level 3\uff1a the rise of collaborative multi-agent systems level 2\uff1a the strategic...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.138c7ced", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjI3NjQ2MA==&mid=2247492065&idx=1&sn=58ff92f30340d054ef007dbffdb10bfa&chksm=c319f3a39286ada6b39f77f06847224ee3d080296ef730812f71a8affdee92446d63454510cb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjI3NjQ2MA==&mid=2247492065&idx=1&sn=58ff92f30340d054ef007dbffdb10bfa&chksm=c319f3a39286ada6b39f77f06847224ee3d080296ef730812f71a8affdee92446d63454510cb#rd", "authors": ["\u68a6\u98de AI"], "title": "\u56fd\u5185\u9996\u4e2a\u652f\u6301\u89c6\u89c9\u7684 <em class=\"highlight\">Agentic</em> \u7f16\u7a0b\u6a21\u578b\u6765\u4e86\uff01\u6027\u80fd\u66f4\u5f3a\uff0c\u4ef7\u683c\u66f4\u4f4e", "comment": "Source: WeChat, Published: 2025-11-12 01:23:46", "summary": "\u65e2\u7136\u662fAgentic\u7f16\u7a0b\u6a21\u578b\uff0c\u6211\u60f3\u4ed6\u4f1a\u61c2\u6211\u610f\u601d\u7684\uff0c\u76f4\u63a5\u63d0\u51fa\u5927\u767d\u8bdd\u9700\u6c42\uff1a\u4f60\u77e5\u9053\u3010\u70b8\u5f39\u4eba\u3011\u5c0f\u6e38\u620f\u5417\uff1f\u6211\u60f3\u73a9\u8fd9\u4e2a\u6e38\u620f\uff0c\u7ed9\u6211\u751f\u6210\u4ee3\u7801\u6587\u4ef6\u65e5 \u65e5 \u7ec8 \u5e2e\u52a9h yaingy jbnanisx bonbran.htm tral \u6587\u6027 bombmanhmal x bonbnanjs \u4f2b\u3002", "AI": {"tldr": "\u65e2\u7136\u662fAgentic\u7f16\u7a0b\u6a21\u578b\uff0c\u6211\u60f3\u4ed6\u4f1a\u61c2\u6211\u610f\u601d\u7684\uff0c\u76f4\u63a5\u63d0\u51fa\u5927\u767d\u8bdd\u9700\u6c42\uff1a\u4f60\u77e5\u9053\u3010\u70b8\u5f39\u4eba\u3011\u5c0f\u6e38\u620f\u5417\uff1f\u6211\u60f3\u73a9\u8fd9\u4e2a\u6e38\u620f\uff0c\u7ed9\u6211\u751f\u6210\u4ee3\u7801\u6587\u4ef6\u65e5 \u65e5 \u7ec8 \u5e2e\u52a9h yaingy jbnanisx bonbran.htm tral \u6587\u6027 bombmanhmal x bonbnanjs \u4f2b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.4ce6466a", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3NDE5MjExOQ==&mid=2650985383&idx=1&sn=6172a9aa83793b2403b85d4d6796b11f&chksm=f142387c19523d1034f5ff5c979e37e0378c44bc5f819b7e524b4a36b865fd92cc01b2434791#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3NDE5MjExOQ==&mid=2650985383&idx=1&sn=6172a9aa83793b2403b85d4d6796b11f&chksm=f142387c19523d1034f5ff5c979e37e0378c44bc5f819b7e524b4a36b865fd92cc01b2434791#rd", "authors": ["AI\u62c9\u5471"], "title": "\u7b2c180\u671f <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u4eba\u5de5\u667a\u80fd\uff08<em class=\"highlight\">Agentic</em> AI\uff09\uff1a\u5355<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7cfb\u7edf vs \u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7cfb\u7edf", "comment": "Source: WeChat, Published: 2025-11-12 00:00:24", "summary": "\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\uff08Agentic AI\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\u672c\u8d28\u4e0a\u662f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u7f16\u7a0b\u3002\u5b83\u4e0d\u518d\u4f9d\u8d56\u50f5\u5316\u3001\u660e\u786e\u7684\u4ee3\u7801\uff0c\u800c\u662f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u6570\u636e\u8def\u7531\u4e0e\u4efb\u52a1\u6267\u884c\uff0c\u4ece\u800c\u5b8c\u6210\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "AI": {"tldr": "\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\uff08Agentic AI\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\u672c\u8d28\u4e0a\u662f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u7f16\u7a0b\u3002\u5b83\u4e0d\u518d\u4f9d\u8d56\u50f5\u5316\u3001\u660e\u786e\u7684\u4ee3\u7801\uff0c\u800c\u662f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u6570\u636e\u8def\u7531\u4e0e\u4efb\u52a1\u6267\u884c\uff0c\u4ece\u800c\u5b8c\u6210\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.812e73a8", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484288&idx=1&sn=166d186f7141a45dfd76b781ca0ca50e&chksm=c0a3fe115fb23be8037185f5cc008d83da7103a2d4abcad4588972bffe175476300ca4eb8de1#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNTgxMDAxMg==&mid=2247484288&idx=1&sn=166d186f7141a45dfd76b781ca0ca50e&chksm=c0a3fe115fb23be8037185f5cc008d83da7103a2d4abcad4588972bffe175476300ca4eb8de1#rd", "authors": ["AI Lab Dev"], "title": "<em class=\"highlight\">Agentic</em>21\u79cd\u8bbe\u8ba1\u6a21\u5f0f-Reflection", "comment": "Source: WeChat, Published: 2025-11-11 23:51:45", "summary": "\u4ece\u800c\u4f7f\u5f97\u4ee3\u7406\u7684\u51b3\u7b56\u66f4\u52a0\u667a\u80fd\u3001\u66f4\u52a0\u7b26\u5408\u5b9e\u9645\u60c5\u5883\u3002\u5b9e\u9645\u5e94\u7528\u4e0e\u6848\u4f8b\u5728\u90a3\u4e9b\u5bf9\u8f93\u51fa\u8d28\u91cf\u3001\u51c6\u786e\u6027\u6216\u5bf9\u5404\u79cd\u590d\u6742\u7ea6\u675f\u7684\u9075\u5b88\u8981\u6c42\u6781\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u53cd\u601d\u6a21\u5f0f\u663e\u5f97\u975e\u5e38\u6709\u7528\u3002", "AI": {"tldr": "\u4ece\u800c\u4f7f\u5f97\u4ee3\u7406\u7684\u51b3\u7b56\u66f4\u52a0\u667a\u80fd\u3001\u66f4\u52a0\u7b26\u5408\u5b9e\u9645\u60c5\u5883\u3002\u5b9e\u9645\u5e94\u7528\u4e0e\u6848\u4f8b\u5728\u90a3\u4e9b\u5bf9\u8f93\u51fa\u8d28\u91cf\u3001\u51c6\u786e\u6027\u6216\u5bf9\u5404\u79cd\u590d\u6742\u7ea6\u675f\u7684\u9075\u5b88\u8981\u6c42\u6781\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u53cd\u601d\u6a21\u5f0f\u663e\u5f97\u975e\u5e38\u6709\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.6c8e470a", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NzA4NjAxOA==&mid=2452976528&idx=1&sn=4d8664b7bc22105cef3e7a5b6913ca8a&chksm=86f4f9640406d4cb6fd01f9626abb48a746ee9e23822cf045597f7bb5ad39a65428615da3462#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NzA4NjAxOA==&mid=2452976528&idx=1&sn=4d8664b7bc22105cef3e7a5b6913ca8a&chksm=86f4f9640406d4cb6fd01f9626abb48a746ee9e23822cf045597f7bb5ad39a65428615da3462#rd", "authors": ["AI\u6280\u672f\u7814\u4e60\u793e"], "title": "\u4e00\u6587\u770b\u61c2 <em class=\"highlight\">Agentic</em> AI\uff1a\u642d\u5efa\u5355\u4f53 vs \u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7cfb\u7edf\uff0c\u7ed3\u679c\u51fa\u4e4e\u610f\u6599\uff01", "comment": "Source: WeChat, Published: 2025-11-11 23:41:11", "summary": "\u6211\u559c\u6b22\u7528\u4e00\u53e5\u8bdd\u89e3\u91ca\uff1aAgentic AI \u5c31\u662f\u201c\u7528\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u201d\u3002\u4f20\u7edf\u5f00\u53d1\u8981\u5199\u6b7b\u903b\u8f91\uff0c\u800c Agentic AI \u662f\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u5df1\u201c\u7406\u89e3\u4efb\u52a1\u3001\u89c4\u5212\u6b65\u9aa4\u3001\u8c03\u7528\u5de5\u5177\u3001\u751f\u6210\u7ed3\u679c\u201d\u3002", "AI": {"tldr": "\u6211\u559c\u6b22\u7528\u4e00\u53e5\u8bdd\u89e3\u91ca\uff1aAgentic AI \u5c31\u662f\u201c\u7528\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u201d\u3002\u4f20\u7edf\u5f00\u53d1\u8981\u5199\u6b7b\u903b\u8f91\uff0c\u800c Agentic AI \u662f\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u5df1\u201c\u7406\u89e3\u4efb\u52a1\u3001\u89c4\u5212\u6b65\u9aa4\u3001\u8c03\u7528\u5de5\u5177\u3001\u751f\u6210\u7ed3\u679c\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.94d82bd1", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODAzOTA2Ng==&mid=2247484037&idx=1&sn=331a09c578e708e6ca8bebe50717bdae&chksm=978f745cafdf99b1db0257e3c445b29b25ffdd9ff216737ea478cbbeda145dc6ff6f16f9d8c3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODAzOTA2Ng==&mid=2247484037&idx=1&sn=331a09c578e708e6ca8bebe50717bdae&chksm=978f745cafdf99b1db0257e3c445b29b25ffdd9ff216737ea478cbbeda145dc6ff6f16f9d8c3#rd", "authors": ["\u8682\u8681\u56fd\u9645 Ant International"], "title": "\u670d\u52a1\u5168\u5546\u4e1a\u573a\u666f\u667a\u80fd\u9884\u6d4b \u8682\u8681\u56fd\u9645\u5f00\u6e90\u201c\u9e70\u5e8f\u201dAI\u9884\u6d4b<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-11-12 13:27:01", "summary": "\u8682\u8681\u56fd\u9645\u81ea\u7814\u7684\u201c\u9e70\u5e8f\u201dAI\u9884\u6d4b\u5927\u6a21\u578b\uff0c\u662f\u4e1a\u5185\u9996\u4e2a\u57fa\u4e8e\u591a\u5206\u6bb5\u6a21\u5f0f\uff08Patch\uff09\u5e76\u91c7\u7528\u201c\u6df7\u5408\u4e13\u5bb6\u201d\uff08Mixture of Experts\uff0c MoE\uff09\u67b6\u6784\u7684\u5927\u89c4\u6a21\u65f6\u5e8f\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u53c2\u6570\u89c4\u6a21\u8d85\u8fc725\u4ebf\uff0c\u5728\u591a\u4e2a\u6743\u5a01\u57fa\u51c6\u8bc4\u6d4b\u4e2d\uff08\u5982\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7387\uff09\u53d6\u5f97\u4e86", "AI": {"tldr": "\u8682\u8681\u56fd\u9645\u81ea\u7814\u7684\u201c\u9e70\u5e8f\u201dAI\u9884\u6d4b\u5927\u6a21\u578b\uff0c\u662f\u4e1a\u5185\u9996\u4e2a\u57fa\u4e8e\u591a\u5206\u6bb5\u6a21\u5f0f\uff08Patch\uff09\u5e76\u91c7\u7528\u201c\u6df7\u5408\u4e13\u5bb6\u201d\uff08Mixture of Experts\uff0c MoE\uff09\u67b6\u6784\u7684\u5927\u89c4\u6a21\u65f6\u5e8f\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u53c2\u6570\u89c4\u6a21\u8d85\u8fc725\u4ebf\uff0c\u5728\u591a\u4e2a\u6743\u5a01\u57fa\u51c6\u8bc4\u6d4b\u4e2d\uff08\u5982\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7387\uff09\u53d6\u5f97\u4e86", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.92d0ae5e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651001296&idx=4&sn=081802d634f4b8e87a9470ca0436fa29&chksm=8521e901276a61ac0aaa0866fc4e8d9da75f0fcc37d8ad9475ddda119fc748a3085f1e4ec168#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2651001296&idx=4&sn=081802d634f4b8e87a9470ca0436fa29&chksm=8521e901276a61ac0aaa0866fc4e8d9da75f0fcc37d8ad9475ddda119fc748a3085f1e4ec168#rd", "authors": ["\u673a\u5668\u4e4b\u5fc3"], "title": "NeurIPS 2025 | \u4e2d\u79d1\u5927\u3001\u6e2f\u4e2d\u6df1\u3001\u901a\u4e49\u5343\u95ee\u8054\u5408\u53d1\u5e03CoRT\uff1a\u4ec530\u4e2a\u6837\u672c\u6559\u4f1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9ad8\u6548\u63a8\u7406\uff0ctoken\u6d88\u8017\u964d\u4f4e50%", "comment": "Source: WeChat, Published: 2025-11-12 13:20:49", "summary": "\u90a3\u4e48\uff0c\u5982\u4f55\u8ba9\u5927\u6a21\u578b\u5b66\u4f1a\u300c\u4f55\u65f6\u300d\u4ee5\u53ca\u300c\u5982\u4f55\u300d\u9ad8\u6548\u5730\u4f7f\u7528\u5de5\u5177\uff0c\u5c06\u81ea\u8eab\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u4e0e\u5de5\u5177\u7684\u7cbe\u786e\u8ba1\u7b97\u80fd\u529b\u5b8c\u7f8e\u7ed3\u5408\uff1f\u6765\u81ea\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66\u3001\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u3001\u901a\u4e49\u5343\u95ee\u7684\u8054\u5408\u7814\u7a76\u56e2\u961f\u7ed9\u51fa\u4e86\u4ed6\u4eec\u7684\u7b54\u6848\uff1aCoRT \uff08Code", "AI": {"tldr": "\u90a3\u4e48\uff0c\u5982\u4f55\u8ba9\u5927\u6a21\u578b\u5b66\u4f1a\u300c\u4f55\u65f6\u300d\u4ee5\u53ca\u300c\u5982\u4f55\u300d\u9ad8\u6548\u5730\u4f7f\u7528\u5de5\u5177\uff0c\u5c06\u81ea\u8eab\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u4e0e\u5de5\u5177\u7684\u7cbe\u786e\u8ba1\u7b97\u80fd\u529b\u5b8c\u7f8e\u7ed3\u5408\uff1f\u6765\u81ea\u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66\u3001\u9999\u6e2f\u4e2d\u6587\u5927\u5b66\uff08\u6df1\u5733\uff09\u3001\u901a\u4e49\u5343\u95ee\u7684\u8054\u5408\u7814\u7a76\u56e2\u961f\u7ed9\u51fa\u4e86\u4ed6\u4eec\u7684\u7b54\u6848\uff1aCoRT \uff08Code", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2511.5e7e7090", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzQyNTMxNQ==&mid=2247484444&idx=1&sn=e16aeea1f0beb12ea635ccc8f8c40f71&chksm=fe4ad53b3771eb5dbd6b81bb6b1362ad7d0338dfe9f48d220c2b7834d0ec9a24a0bbc44cbbe7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzQyNTMxNQ==&mid=2247484444&idx=1&sn=e16aeea1f0beb12ea635ccc8f8c40f71&chksm=fe4ad53b3771eb5dbd6b81bb6b1362ad7d0338dfe9f48d220c2b7834d0ec9a24a0bbc44cbbe7#rd", "authors": ["AI\u5927\u6a21\u578b\u7b97\u6cd5\u5de5\u7a0b\u5e08"], "title": "\u5434\u6069\u8fbe\uff1a\u7ec8\u4e8e\u6709\u4eba\u4e00\u6b21\u6027\u628a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e09\u79cd\u6a21\u5f0f\u8bb2\u6e05\u4e86\uff01\uff01\uff01", "comment": "Source: WeChat, Published: 2025-11-12 11:28:37", "summary": "\u5927\u6a21\u578b\u7684\u4e09\u79cd\u6a21\u5f0f\u3002embedding\uff0c copilot\uff0c agent\u3002\u751f\u6210\u5f0fai\u7684\u667a\u80fd\u9769\u547d\u6f14\u5316\u81f3\u4eca\uff0c\u4ece\u4eba\u673a\u534f\u540c\u5448\u73b0\u4e86\u4e09\u79cd \u6a21\u5f0f\uff1a \u00b7\u5d4c\u5165\uff08embedding\uff09\u6a21\u5f0f\uff1a\u67d0\u4e2a\u73af\u8282\u91cc\u53bb\u8c03\u7528\u5927\u6a21 \u578b\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u7684\u4e09\u79cd\u6a21\u5f0f\u3002embedding\uff0c copilot\uff0c agent\u3002\u751f\u6210\u5f0fai\u7684\u667a\u80fd\u9769\u547d\u6f14\u5316\u81f3\u4eca\uff0c\u4ece\u4eba\u673a\u534f\u540c\u5448\u73b0\u4e86\u4e09\u79cd \u6a21\u5f0f\uff1a \u00b7\u5d4c\u5165\uff08embedding\uff09\u6a21\u5f0f\uff1a\u67d0\u4e2a\u73af\u8282\u91cc\u53bb\u8c03\u7528\u5927\u6a21 \u578b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2511.43dbb0dd", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4Mzk2Njc4Mw==&mid=2247522312&idx=4&sn=449532d1c023d46f847b0ed77d7a7979&chksm=ce9366ebd9f6f5a1ecc205123994f1199f6444caea109fb0b3b2f7ee99d83d21c4bd0f06fb82#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4Mzk2Njc4Mw==&mid=2247522312&idx=4&sn=449532d1c023d46f847b0ed77d7a7979&chksm=ce9366ebd9f6f5a1ecc205123994f1199f6444caea109fb0b3b2f7ee99d83d21c4bd0f06fb82#rd", "authors": ["\u7518\u8083\u4ea4\u901a\u79d1\u6280\u901a\u4fe1"], "title": "\u4eba\u5de5\u667a\u80fd<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7cfb\u5217\u56fd\u5bb6\u6807\u51c6\u89e3\u8bfb\uff08\u4e8c\uff09\u2014\u2014\u300a\u4eba\u5de5\u667a\u80fd  <em class=\"highlight\">\u5927\u6a21\u578b</em>  \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b", "comment": "Source: WeChat, Published: 2025-11-12 09:32:47", "summary": "\u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b\u786e\u7acb\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u63cf\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002\u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c3\u90e8\u5206\uff1a\u670d\u52a1\u80fd\u529b\u6210\u719f\u5ea6\u8bc4\u4f30\u300b\u7ed9\u51fa\u4e86\u5927\u6a21\u578b\u670d\u52a1\u80fd\u529b\u6846\u67b6\u548c\u6210\u719f\u5ea6\u7b49\u7ea7\uff0c\u63cf\u8ff0\u4e86\u5927\u6a21", "AI": {"tldr": "\u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c2\u90e8\u5206\uff1a\u8bc4\u6d4b\u6307\u6807\u4e0e\u65b9\u6cd5\u300b\u786e\u7acb\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u63cf\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u3002\u300a\u4eba\u5de5\u667a\u80fd \u5927\u6a21\u578b \u7b2c3\u90e8\u5206\uff1a\u670d\u52a1\u80fd\u529b\u6210\u719f\u5ea6\u8bc4\u4f30\u300b\u7ed9\u51fa\u4e86\u5927\u6a21\u578b\u670d\u52a1\u80fd\u529b\u6846\u67b6\u548c\u6210\u719f\u5ea6\u7b49\u7ea7\uff0c\u63cf\u8ff0\u4e86\u5927\u6a21", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.878781a3", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNzg1MzMxNg==&mid=2247487795&idx=1&sn=7b0a87fee0d83567101148c1bce33d2c&chksm=c315e2b83b522dc2b2d9a306e9d2d45539cdb36057f99d9742facfdea7a23868a3d6b5cfa029#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNzg1MzMxNg==&mid=2247487795&idx=1&sn=7b0a87fee0d83567101148c1bce33d2c&chksm=c315e2b83b522dc2b2d9a306e9d2d45539cdb36057f99d9742facfdea7a23868a3d6b5cfa029#rd", "authors": ["\u53f8\u5357\u8bc4\u6d4b\u4f53\u7cfb"], "title": "\u5404\u6709\u6240\u957f\uff0c\u56fd\u5185\u5916\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u4e28\u591a\u4e2a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b89\u5168\u699c\u5355\u63ed\u6653", "comment": "Source: WeChat, Published: 2025-11-12 09:30:14", "summary": "\u4e3a\u586b\u8865\u5927\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u9886\u57df\u7684\u6807\u51c6\u5316\u7a7a\u767d\uff0c\u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u8bc4\u6d4b\u4e13\u9879\u7ec4\u57fa\u4e8e\u591a\u7ef4\u5ea6\u5b89\u5168\u6d4b\u8bd5\u57fa\u51c6\uff0c\u9488\u5bf9\u56fd\u5185\u5916\u4e3b\u6d41\u5927\u6a21\u578b\u5f00\u5c55\u4e86\u7cfb\u7edf\u6027\u8bc4\u6d4b\uff0c\u73b0\u516c\u5e03\u4e09\u5927\u5b89\u5168\u8bc4\u4f30\u699c\u5355\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u5ba2\u89c2\u3001\u53ef\u9760\u7684\u5b89\u5168\u6027\u80fd\u53c2\u8003\u3002", "AI": {"tldr": "\u4e3a\u586b\u8865\u5927\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u9886\u57df\u7684\u6807\u51c6\u5316\u7a7a\u767d\uff0c\u4e0a\u6d77\u4eba\u5de5\u667a\u80fd\u5b9e\u9a8c\u5ba4\u8bc4\u6d4b\u4e13\u9879\u7ec4\u57fa\u4e8e\u591a\u7ef4\u5ea6\u5b89\u5168\u6d4b\u8bd5\u57fa\u51c6\uff0c\u9488\u5bf9\u56fd\u5185\u5916\u4e3b\u6d41\u5927\u6a21\u578b\u5f00\u5c55\u4e86\u7cfb\u7edf\u6027\u8bc4\u6d4b\uff0c\u73b0\u516c\u5e03\u4e09\u5927\u5b89\u5168\u8bc4\u4f30\u699c\u5355\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u5ba2\u89c2\u3001\u53ef\u9760\u7684\u5b89\u5168\u6027\u80fd\u53c2\u8003\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2511.dac5742e", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzMzEyMDI2Mg==&mid=2247489934&idx=3&sn=adb1b35101c8f55dd42bb99822191985&chksm=fb8fd11085595a40af9ddc09eb73b48de437944e0de27ce29c93053ad7c49c853e4912981593#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzMzEyMDI2Mg==&mid=2247489934&idx=3&sn=adb1b35101c8f55dd42bb99822191985&chksm=fb8fd11085595a40af9ddc09eb73b48de437944e0de27ce29c93053ad7c49c853e4912981593#rd", "authors": ["\u6781\u5ba2\u65f6\u95f4\u8bad\u7ec3\u8425"], "title": "\u9ad8\u85aa\u3001\u7f3a\u4eba\uff01\u96f6\u6210\u672c\u5feb\u901f\u5165\u95e8<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-11-12 08:52:37", "summary": "\u4ece\u539f\u7406\u51fa\u53d1\u771f\u6b63\u5165\u5c40\u5927\u6a21\u578b\u3002\u5728\u8fd9\u91cc\u63a8\u8350\u4e00\u4e0b\u7531 LangChain \u5f00\u53d1\u8005\uff0c\u8c37\u6b4c\u5f00\u53d1\u8005\u4e13\u5bb6\u5f6d\u9756\u7530\uff0c\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u91cf\u8eab\u6253\u9020\u7684\u300e\u4ece 0 \u5230 1 \u5165\u95e8 AI \u5927\u6a21\u578b\u300f\u89c6\u9891\u8bfe\u7a0b\uff1a", "AI": {"tldr": "\u4ece\u539f\u7406\u51fa\u53d1\u771f\u6b63\u5165\u5c40\u5927\u6a21\u578b\u3002\u5728\u8fd9\u91cc\u63a8\u8350\u4e00\u4e0b\u7531 LangChain \u5f00\u53d1\u8005\uff0c\u8c37\u6b4c\u5f00\u53d1\u8005\u4e13\u5bb6\u5f6d\u9756\u7530\uff0c\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u91cf\u8eab\u6253\u9020\u7684\u300e\u4ece 0 \u5230 1 \u5165\u95e8 AI \u5927\u6a21\u578b\u300f\u89c6\u9891\u8bfe\u7a0b\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.f59090f6", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNDQzNjUyOQ==&mid=2247492691&idx=3&sn=d47ce3def9dc18cc5faca6c2c0c61a54&chksm=fbeafdf78382b2becfb6b17d25ff4e1f26633f3a4bdbeba3343d866fd2c16a6f14b83049e52a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNDQzNjUyOQ==&mid=2247492691&idx=3&sn=d47ce3def9dc18cc5faca6c2c0c61a54&chksm=fbeafdf78382b2becfb6b17d25ff4e1f26633f3a4bdbeba3343d866fd2c16a6f14b83049e52a#rd", "authors": ["\u6781\u5ba2\u65f6\u95f4\u7cbe\u9009"], "title": "\u9ad8\u85aa\u3001\u7f3a\u4eba\uff01\u96f6\u6210\u672c\u5feb\u901f\u5165\u95e8<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-11-12 08:51:08", "summary": "\u4ece\u539f\u7406\u51fa\u53d1\u771f\u6b63\u5165\u5c40\u5927\u6a21\u578b\u3002\u5728\u8fd9\u91cc\u63a8\u8350\u4e00\u4e0b\u7531 LangChain \u5f00\u53d1\u8005\uff0c\u8c37\u6b4c\u5f00\u53d1\u8005\u4e13\u5bb6\u5f6d\u9756\u7530\uff0c\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u91cf\u8eab\u6253\u9020\u7684\u300e\u4ece 0 \u5230 1 \u5165\u95e8 AI \u5927\u6a21\u578b\u300f\u89c6\u9891\u8bfe\u7a0b\uff1a", "AI": {"tldr": "\u4ece\u539f\u7406\u51fa\u53d1\u771f\u6b63\u5165\u5c40\u5927\u6a21\u578b\u3002\u5728\u8fd9\u91cc\u63a8\u8350\u4e00\u4e0b\u7531 LangChain \u5f00\u53d1\u8005\uff0c\u8c37\u6b4c\u5f00\u53d1\u8005\u4e13\u5bb6\u5f6d\u9756\u7530\uff0c\u4e13\u95e8\u4e3a\u5f00\u53d1\u8005\u91cf\u8eab\u6253\u9020\u7684\u300e\u4ece 0 \u5230 1 \u5165\u95e8 AI \u5927\u6a21\u578b\u300f\u89c6\u9891\u8bfe\u7a0b\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2511.e6315c8f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIwMzc5NzU3Mw==&mid=2247484103&idx=1&sn=ca2c1ebdf590632a89a0145525ba41dd&chksm=975ed22a27aa29e709b5cbc2134568483078b3e6cb32462820fc351d9dc69a9fa207267b38e7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIwMzc5NzU3Mw==&mid=2247484103&idx=1&sn=ca2c1ebdf590632a89a0145525ba41dd&chksm=975ed22a27aa29e709b5cbc2134568483078b3e6cb32462820fc351d9dc69a9fa207267b38e7#rd", "authors": ["AI\u5927\u6a21\u578b-\u6728\u5b50"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5b66\u4e60\u8def\u7ebf\uff082025\u6700\u65b0\uff09\u4ece\u96f6\u57fa\u7840\u5165\u95e8\u5230\u7cbe\u901a\uff0c\u770b\u5b8c\u8fd9\u4e00\u7bc7\u5c31\u591f\u4e86\uff01", "comment": "Source: WeChat, Published: 2025-11-12 06:56:05", "summary": "ai\u5927\u6a21\u578b\u5b66\u4e60\u8def\u7ebf \u7b2c\u4e00\u9636\u6bb5\u00b7\u5927\u6a21\u578b\u5f00\u53d1\u57fa\u7840\u3002\u4e3a\u4ec0\u4e48\u8981\u5b66\u4e60\u5927\u6a21\u578b\u5f00\u53d1\uff1f\u9700\u8981\u51c6\u5907\u7684\u5de5\u5177\u548c\u73af\u5883\u3002\u7b2c\u4e8c\u7ae0\uff1a\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u5e94\u7528\u3002\u5927\u6a21\u578b\u53d1\u5c55\u53f2\u3002\u4ece\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u5230\u5e94\u7528 gpt\u7ed3\u6784\uff0c\u5256\u6790 \u5927\u6a21\u578b\u5bb6\u65cf\u3001\u7c7b\u522b\u3001\u5e94\u7528\u573a\u666f\u3002", "AI": {"tldr": "ai\u5927\u6a21\u578b\u5b66\u4e60\u8def\u7ebf \u7b2c\u4e00\u9636\u6bb5\u00b7\u5927\u6a21\u578b\u5f00\u53d1\u57fa\u7840\u3002\u4e3a\u4ec0\u4e48\u8981\u5b66\u4e60\u5927\u6a21\u578b\u5f00\u53d1\uff1f\u9700\u8981\u51c6\u5907\u7684\u5de5\u5177\u548c\u73af\u5883\u3002\u7b2c\u4e8c\u7ae0\uff1a\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u5e94\u7528\u3002\u5927\u6a21\u578b\u53d1\u5c55\u53f2\u3002\u4ece\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u5230\u5e94\u7528 gpt\u7ed3\u6784\uff0c\u5256\u6790 \u5927\u6a21\u578b\u5bb6\u65cf\u3001\u7c7b\u522b\u3001\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
