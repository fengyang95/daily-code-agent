{"id": "2512.02393", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02393", "abs": "https://arxiv.org/abs/2512.02393", "authors": ["Shuyang Liu", "Yang Chen", "Rahul Krishna", "Saurabh Sinha", "Jatin Ganhotra", "Reyhan Jabbarvand"], "title": "Process-Centric Analysis of Agentic Software Systems", "comment": null, "summary": "Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.\n  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGraphectory\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6267\u884c\u8f68\u8ff9\u7f16\u7801\u4e3a\u56fe\u7ed3\u6784\uff0c\u652f\u6301\u8fc7\u7a0b\u4e2d\u5fc3\u5316\u5206\u6790\uff0c\u63ed\u793a\u667a\u80fd\u4f53\u7f16\u7a0b\u5de5\u4f5c\u6d41\u7684\u8d28\u91cf\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u8fc7\u4e8e\u7ed3\u679c\u4e2d\u5fc3\u5316\uff0c\u53ea\u5173\u6ce8\u6700\u7ec8\u6210\u529f\u6216\u5931\u8d25\uff0c\u5ffd\u89c6\u4e86\u667a\u80fd\u4f53\u63a8\u7406\u3001\u89c4\u5212\u3001\u884c\u52a8\u548c\u7b56\u7565\u6f14\u53d8\u7684\u8be6\u7ec6\u8fc7\u7a0b\u3002\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u5206\u6790\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u6267\u884c\u8f68\u8ff9\u3002", "method": "\u63d0\u51faGraphectory\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6267\u884c\u8f68\u8ff9\u7f16\u7801\u4e3a\u5305\u542b\u65f6\u95f4\u548c\u8bed\u4e49\u5173\u7cfb\u7684\u56fe\u7ed3\u6784\u3002\u4f7f\u7528\u8be5\u6846\u67b6\u5206\u67904000\u4e2a\u8f68\u8ff9\uff0c\u6db5\u76d6SWE-agent\u548cOpenHands\u4e24\u79cd\u4e3b\u6d41\u667a\u80fd\u4f53\u7f16\u7a0b\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u56db\u79cd\u9aa8\u5e72\u5927\u8bed\u8a00\u6a21\u578b\u5728SWE-bench Verified\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a(1) \u4f7f\u7528\u66f4\u4e30\u5bcc\u63d0\u793a\u6216\u66f4\u5f3aLLM\u7684\u667a\u80fd\u4f53\u4ea7\u751f\u66f4\u590d\u6742\u7684Graphectory\uff0c\u53cd\u6620\u66f4\u6df1\u5ea6\u7684\u63a2\u7d22\u3001\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\u548c\u66f4\u5f7b\u5e95\u7684\u9a8c\u8bc1\uff1b(2) \u95ee\u9898\u89e3\u51b3\u7b56\u7565\u968f\u95ee\u9898\u96be\u5ea6\u548c\u5e95\u5c42LLM\u53d8\u5316\uff0c\u5df2\u89e3\u51b3\u95ee\u9898\u9075\u5faa\u8fde\u8d2f\u7684\u5b9a\u4f4d-\u4fee\u8865-\u9a8c\u8bc1\u6b65\u9aa4\uff0c\u672a\u89e3\u51b3\u95ee\u9898\u5448\u73b0\u6df7\u4e71\u3001\u91cd\u590d\u6216\u56de\u6eaf\u884c\u4e3a\uff1b(3) \u5373\u4f7f\u6210\u529f\uff0c\u667a\u80fd\u4f53\u7f16\u7a0b\u7cfb\u7edf\u5e38\u663e\u793a\u4f4e\u6548\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u8f68\u8ff9\u3002", "conclusion": "Graphectory\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8fc7\u7a0b\u4e2d\u5fc3\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u8d28\u91cf\u548c\u6548\u7387\u7279\u5f81\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u3002", "topic": "agent analysis"}}
{"id": "2512.02567", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02567", "abs": "https://arxiv.org/abs/2512.02567", "authors": ["Martin Weiss", "Jesko Hecking-Harbusch", "Jochen Quante", "Matthias Woehrle"], "title": "Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System", "comment": "10 pages, 9 figures", "summary": "The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.\n  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.\n  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.", "AI": {"tldr": "\u7814\u7a76\u81ea\u52a8\u5316C\u5230Rust\u4ee3\u7801\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u53cd\u9988\u5faa\u73af\u3001LLM\u9009\u62e9\u548c\u4ee3\u7801\u6270\u52a8\uff0c\u53d1\u73b0\u53cd\u9988\u5faa\u73af\u80fd\u663e\u8457\u7f29\u5c0f\u4e0d\u540cLLM\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee3\u7801\u6270\u52a8\u751a\u81f3\u80fd\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u81ea\u52a8\u5316\u65b9\u6cd5\u9700\u8981\u66f4\u9ad8\u53ef\u9760\u6027\u624d\u80fd\u7528\u4e8e\u5de5\u4e1a\u5b9e\u8df5\u3002\u672c\u7814\u7a76\u5173\u6ce8\u5f71\u54cd\u4ee3\u7801\u7ffb\u8bd1\u8d28\u91cf\u7684\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u81ea\u52a8\u5316\u53cd\u9988\u5faa\u73af\u3001LLM\u9009\u62e9\u548c\u4ee3\u7801\u6270\u52a8\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u751f\u6210-\u68c0\u67e5\u6a21\u5f0f\u7684C\u5230Rust\u7ffb\u8bd1\u7cfb\u7edf\uff1aLLM\u751f\u6210Rust\u4ee3\u7801\u540e\u81ea\u52a8\u68c0\u67e5\u53ef\u7f16\u8bd1\u6027\u548c\u884c\u4e3a\u7b49\u4ef7\u6027\uff0c\u5931\u8d25\u65f6\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u91cd\u65b0\u63d0\u793aLLM\u4fee\u590d\u8f93\u51fa\u3002\u901a\u8fc7\u6539\u53d8\u4e09\u4e2a\u53d8\u91cf\uff08\u53cd\u9988\u5faa\u73af\u3001LLM\u6a21\u578b\u3001\u4ee3\u7801\u6270\u52a8\uff09\u6765\u8bc4\u4f30\u7cfb\u7edf\u6210\u529f\u7387\u3002", "result": "\u65e0\u53cd\u9988\u5faa\u73af\u65f6LLM\u9009\u62e9\u5bf9\u7ffb\u8bd1\u6210\u529f\u7387\u5f71\u54cd\u5f88\u5927\uff1b\u4f7f\u7528\u53cd\u9988\u5faa\u73af\u540e\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u5dee\u5f02\u663e\u8457\u7f29\u5c0f\u3002\u4ee3\u7801\u6270\u52a8\u63d0\u4f9b\u7684\u591a\u6837\u6027\u751a\u81f3\u80fd\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u7cfb\u7edf\u5728\u4ee3\u7801\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u4e5f\u5f97\u5230\u6539\u5584\u3002", "conclusion": "\u53cd\u9988\u5faa\u73af\u662f\u63d0\u5347\u81ea\u52a8\u5316\u4ee3\u7801\u7ffb\u8bd1\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5173\u952e\u673a\u5236\uff0c\u80fd\u51cf\u5c11\u5bf9\u7279\u5b9aLLM\u7684\u4f9d\u8d56\uff1b\u4ee3\u7801\u6270\u52a8\u5e26\u6765\u7684\u591a\u6837\u6027\u6709\u52a9\u4e8e\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u8fd9\u5bf9\u5de5\u4e1a\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "topic": "code agent"}}
{"id": "2512.02080", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.02080", "abs": "https://arxiv.org/abs/2512.02080", "authors": ["PIerre Dantas", "Lucas Cordeiro", "Youcheng Sun", "Waldir Junior"], "title": "The 4/$\u03b4$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee", "comment": "32 pages, 9 figures", "summary": "The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($\u03b4$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $\u03b4> 0$, with an expected iteration count bounded by $\\mathbb{E}[n] \\leq 4/\u03b4$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5177\u6709\u53ef\u8bc1\u660e\u7ec8\u6b62\u6027\u548c\u6536\u655b\u6027\u4fdd\u8bc1\u7684LLM-\u9a8c\u8bc1\u5668\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u548c\u8bef\u5dee\u51cf\u5c11\u6982\u7387\u53c2\u6570\uff0c\u4e3aLLM\u8f85\u52a9\u7684\u8f6f\u4ef6\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u548c\u5b9e\u7528\u8bbe\u8ba1\u9608\u503c\u3002", "motivation": "\u5f53\u524d\u5c06\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u9a8c\u8bc1\u8fc7\u7a0b\u4e0d\u53ef\u9760\u3001\u4e0d\u7a33\u5b9a\uff0c\u65e0\u6cd5\u4e3a\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u73af\u5883\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u5efa\u7acbLLM-\u9a8c\u8bc1\u5668\u6536\u655b\u5b9a\u7406\uff0c\u5c06LLM\u4e0e\u9a8c\u8bc1\u5668\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5f15\u5165\u8bef\u5dee\u51cf\u5c11\u6982\u7387\u53c2\u6570\u03b4\uff0c\u5e76\u8fdb\u884c\u4e86\u8d85\u8fc790,000\u6b21\u8bd5\u9a8c\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5bf9\u4e8e\u4efb\u4f55\u03b4>0\uff0c\u7a0b\u5e8f\u51e0\u4e4e\u5fc5\u7136\u7ec8\u6b62\uff0c\u671f\u671b\u8fed\u4ee3\u6b21\u6570\u6709\u754cE[n]\u22644/\u03b4\u3002\u5b9e\u8bc1\u7ed3\u679c\u4e0e\u7406\u8bba\u9ad8\u5ea6\u4e00\u81f4\uff0c\u6240\u6709\u8fd0\u884c\u90fd\u8fbe\u5230\u9a8c\u8bc1\uff0c\u6536\u655b\u56e0\u5b50Cf\u22481.0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u8f85\u52a9\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u8bbe\u8ba1\u6846\u67b6\uff0c\u652f\u6301\u53ef\u9884\u6d4b\u7684\u8d44\u6e90\u89c4\u5212\u548c\u6027\u80fd\u9884\u7b97\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u73af\u5883\u3002", "topic": "code agent"}}
{"id": "2512.02038", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.02038", "abs": "https://arxiv.org/abs/2512.02038", "authors": ["Zhengliang Shi", "Yiqun Chen", "Haitao Li", "Weiwei Sun", "Shiyu Ni", "Yougang Lyu", "Run-Ze Fan", "Bowen Jin", "Yixuan Weng", "Minjun Zhu", "Qiujie Xie", "Xinyu Guo", "Qu Yang", "Jiayi Wu", "Jujia Zhao", "Xiaqiang Tang", "Xinbei Ma", "Cunxiang Wang", "Jiaxin Mao", "Qingyao Ai", "Jen-Tse Huang", "Wenxuan Wang", "Yue Zhang", "Yiming Yang", "Zhaopeng Tu", "Zhaochun Ren"], "title": "Deep Research: A Systematic Survey", "comment": null, "summary": "Large language models (LLMs) have rapidly evolved from text generators into powerful problem solvers. Yet, many open tasks demand critical thinking, multi-source, and verifiable outputs, which are beyond single-shot prompting or standard retrieval-augmented generation. Recently, numerous studies have explored Deep Research (DR), which aims to combine the reasoning capabilities of LLMs with external tools, such as search engines, thereby empowering LLMs to act as research agents capable of completing complex, open-ended tasks. This survey presents a comprehensive and systematic overview of deep research systems, including a clear roadmap, foundational components, practical implementation techniques, important challenges, and future directions. Specifically, our main contributions are as follows: (i) we formalize a three-stage roadmap and distinguish deep research from related paradigms; (ii) we introduce four key components: query planning, information acquisition, memory management, and answer generation, each paired with fine-grained sub-taxonomies; (iii) we summarize optimization techniques, including prompting, supervised fine-tuning, and agentic reinforcement learning; and (iv) we consolidate evaluation criteria and open challenges, aiming to guide and facilitate future development. As the field of deep research continues to evolve rapidly, we are committed to continuously updating this survey to reflect the latest progress in this area.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u7cfb\u7edf\u68b3\u7406\u4e86\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e09\u9636\u6bb5\u8def\u7ebf\u56fe\u3001\u56db\u5927\u6838\u5fc3\u7ec4\u4ef6\u3001\u4f18\u5316\u6280\u672f\uff0c\u5e76\u603b\u7ed3\u4e86\u8bc4\u4f30\u6807\u51c6\u548c\u5f00\u653e\u6311\u6218\uff0c\u65e8\u5728\u6307\u5bfc\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u4ece\u6587\u672c\u751f\u6210\u5668\u53d1\u5c55\u4e3a\u5f3a\u5927\u7684\u95ee\u9898\u89e3\u51b3\u5de5\u5177\uff0c\u4f46\u8bb8\u591a\u5f00\u653e\u4efb\u52a1\u9700\u8981\u6279\u5224\u6027\u601d\u7ef4\u3001\u591a\u6e90\u4fe1\u606f\u548c\u53ef\u9a8c\u8bc1\u8f93\u51fa\uff0c\u8fd9\u4e9b\u8d85\u51fa\u4e86\u5355\u6b21\u63d0\u793a\u6216\u6807\u51c6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u80fd\u529b\u3002\u6df1\u5ea6\u7814\u7a76\u65e8\u5728\u7ed3\u5408LLM\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u5916\u90e8\u5de5\u5177\uff0c\u4f7fLLM\u80fd\u591f\u4f5c\u4e3a\u7814\u7a76\u4ee3\u7406\u5b8c\u6210\u590d\u6742\u7684\u5f00\u653e\u5f0f\u4efb\u52a1\u3002", "method": "1) \u63d0\u51fa\u4e09\u9636\u6bb5\u8def\u7ebf\u56fe\u5e76\u533a\u5206\u6df1\u5ea6\u7814\u7a76\u4e0e\u76f8\u5173\u8303\u5f0f\uff1b2) \u4ecb\u7ecd\u56db\u5927\u6838\u5fc3\u7ec4\u4ef6\uff1a\u67e5\u8be2\u89c4\u5212\u3001\u4fe1\u606f\u83b7\u53d6\u3001\u8bb0\u5fc6\u7ba1\u7406\u548c\u7b54\u6848\u751f\u6210\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u6709\u7ec6\u7c92\u5ea6\u5b50\u5206\u7c7b\uff1b3) \u603b\u7ed3\u4f18\u5316\u6280\u672f\uff0c\u5305\u62ec\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff1b4) \u6574\u5408\u8bc4\u4f30\u6807\u51c6\u548c\u5f00\u653e\u6311\u6218\u3002", "result": "\u63d0\u4f9b\u4e86\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u5168\u9762\u7cfb\u7edf\u6982\u8ff0\uff0c\u5305\u62ec\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u3001\u57fa\u7840\u7ec4\u4ef6\u3001\u5b9e\u9645\u5b9e\u73b0\u6280\u672f\u3001\u91cd\u8981\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002\u627f\u8bfa\u968f\u7740\u9886\u57df\u5feb\u901f\u53d1\u5c55\u6301\u7eed\u66f4\u65b0\u6b64\u7efc\u8ff0\u3002", "conclusion": "\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7ed3\u5408LLM\u63a8\u7406\u80fd\u529b\u4e0e\u5916\u90e8\u5de5\u5177\uff0c\u80fd\u591f\u89e3\u51b3\u9700\u8981\u6279\u5224\u6027\u601d\u7ef4\u548c\u591a\u6e90\u4fe1\u606f\u7684\u590d\u6742\u5f00\u653e\u4efb\u52a1\u3002\u8be5\u7efc\u8ff0\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\u548c\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.02750", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.02750", "abs": "https://arxiv.org/abs/2512.02750", "authors": ["Kiev Gama", "Filipe Calegario", "Victoria Jackson", "Alexander Nolte", "Luiz Augusto Morais", "Vinicius Garcia"], "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding", "comment": "International Conference on Software Engineering, Education Track (SEET) 2026", "summary": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6559\u80b2\u9ed1\u5ba2\u677e\u63a2\u7d22\u65b0\u624b\u7a0b\u5e8f\u5458\u5982\u4f55\u53c2\u4e0e\"\u6c1b\u56f4\u7f16\u7a0b\"\uff0c\u53d1\u73b0\u5176\u80fd\u4fc3\u8fdb\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u8de8\u5b66\u79d1\u534f\u4f5c\uff0c\u4f46\u4e5f\u5b58\u5728\u521b\u610f\u8fc7\u65e9\u6536\u655b\u3001\u4ee3\u7801\u8d28\u91cf\u4e0d\u5747\u7b49\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u548cAI\u8f85\u52a9\u7f16\u7801\u7684\u5174\u8d77\uff0c\"\u6c1b\u56f4\u7f16\u7a0b\"\uff08\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u800c\u975e\u76f4\u63a5\u7f16\u5199\u4ee3\u7801\u6765\u521b\u5efa\u8f6f\u4ef6\uff09\u6709\u671b\u6c11\u4e3b\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4f46\u5176\u6559\u80b2\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728\u5df4\u897f\u516c\u7acb\u5927\u5b66\u7ec4\u7ec7\u4e3a\u671f\u4e00\u5929\u7684\u6559\u80b2\u9ed1\u5ba2\u677e\uff0c31\u540d\u6765\u81ea\u8ba1\u7b97\u548c\u975e\u8ba1\u7b97\u5b66\u79d1\u7684\u672c\u79d1\u751f\u5206\u4e3a9\u4e2a\u56e2\u961f\u3002\u901a\u8fc7\u89c2\u5bdf\u3001\u9000\u51fa\u8c03\u67e5\u548c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u7814\u7a76\u521b\u610f\u8fc7\u7a0b\u3001\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\u3001\u534f\u4f5c\u52a8\u6001\u548c\u5b66\u4e60\u6210\u679c\u3002", "result": "\u6c1b\u56f4\u7f16\u7a0b\u5b9e\u73b0\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u8de8\u5b66\u79d1\u534f\u4f5c\uff0c\u53c2\u4e0e\u8005\u53d1\u5c55\u4e86\u63d0\u793a\u5de5\u7a0b\u6280\u80fd\u5e76\u5728\u65f6\u95f4\u9650\u5236\u5185\u4ea4\u4ed8\u4e86\u529f\u80fd\u6f14\u793a\u3002\u4f46\u89c2\u5bdf\u5230\u521b\u610f\u8fc7\u65e9\u6536\u655b\u3001\u4ee3\u7801\u8d28\u91cf\u4e0d\u5747\u9700\u8981\u8fd4\u5de5\u3001\u5bf9\u6838\u5fc3\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u53c2\u4e0e\u6709\u9650\u3002\u56e2\u961f\u91c7\u7528\u7ed3\u5408\u591a\u79cdAI\u5de5\u5177\u7684\u590d\u6742\u5de5\u4f5c\u6d41\uff0c\u4eba\u7c7b\u5224\u65ad\u5728\u5173\u952e\u6539\u8fdb\u4e2d\u4ecd\u5fc5\u4e0d\u53ef\u5c11\u3002", "conclusion": "\u6c1b\u56f4\u7f16\u7a0b\u9ed1\u5ba2\u677e\u53ef\u4f5c\u4e3a\u6709\u4ef7\u503c\u7684\u4f4e\u98ce\u9669\u5b66\u4e60\u73af\u5883\uff0c\u4f46\u9700\u8981\u7ed3\u5408\u660e\u786e\u7684\u652f\u67b6\u6765\u652f\u6301\u53d1\u6563\u601d\u7ef4\u3001\u6279\u5224\u6027\u8bc4\u4f30AI\u8f93\u51fa\uff0c\u4ee5\u53ca\u5bf9\u751f\u4ea7\u8d28\u91cf\u6709\u73b0\u5b9e\u671f\u671b\u3002", "topic": "swe application"}}
{"id": "2512.02228", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02228", "abs": "https://arxiv.org/abs/2512.02228", "authors": ["Shubhi Asthana", "Bing Zhang", "Chad DeLuca", "Ruchi Mahindru", "Hima Patel"], "title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls", "comment": "10 pages, 4 Figures, 5 Tables Paper presented at NeurIPS 2025 LAW workshop: Bridging Language, Agent, and World Models", "summary": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.\n  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.\n  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.", "AI": {"tldr": "STRIDE\u6846\u67b6\u5e2e\u52a9\u7cfb\u7edf\u9009\u62e9AI\u90e8\u7f72\u6a21\u5f0f\uff1a\u76f4\u63a5LLM\u8c03\u7528\u3001\u5f15\u5bfc\u5f0fAI\u52a9\u624b\u6216\u5b8c\u5168\u81ea\u4e3b\u7684\u667a\u80fd\u4f53\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u667a\u80fd\u4f53\u90e8\u7f72\uff0c\u8282\u7701\u6210\u672c\u3002", "motivation": "\u4ece\u65e0\u72b6\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u5feb\u901f\u8f6c\u53d8\u5f15\u53d1\u6838\u5fc3\u95ee\u9898\uff1a\u4f55\u65f6\u771f\u6b63\u9700\u8981\u667a\u80fd\u4f53AI\uff1f\u4e0d\u52a0\u533a\u5206\u5730\u90e8\u7f72\u667a\u80fd\u4f53\u4f1a\u5bfc\u81f4\u6210\u672c\u3001\u590d\u6742\u6027\u548c\u98ce\u9669\u589e\u52a0\u3002", "method": "\u63d0\u51faSTRIDE\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4efb\u52a1\u5206\u89e3\u3001\u52a8\u6001\u6027\u5f52\u56e0\u548c\u81ea\u6211\u53cd\u601d\u9700\u6c42\u5206\u6790\uff0c\u751f\u6210\u667a\u80fd\u4f53\u9002\u7528\u6027\u8bc4\u5206\uff0c\u4e3a\u4efb\u52a1\u63a8\u8350\u4e09\u79cd\u90e8\u7f72\u6a21\u5f0f\u4e4b\u4e00\u3002", "result": "\u572830\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cSTRIDE\u5728\u6a21\u5f0f\u9009\u62e9\u4e0a\u8fbe\u523092%\u51c6\u786e\u7387\uff0c\u51cf\u5c1145%\u4e0d\u5fc5\u8981\u7684\u667a\u80fd\u4f53\u90e8\u7f72\uff0c\u964d\u4f4e37%\u8d44\u6e90\u6210\u672c\uff0c\u4e13\u5bb6\u9a8c\u8bc1\u786e\u8ba4\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u5c06\u667a\u80fd\u4f53\u91c7\u7528\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9700\u6c42\u9a71\u52a8\u7684\u8bbe\u8ba1\u51b3\u7b56\uff0c\u786e\u4fdd\u81ea\u4e3b\u6027\u4ec5\u5728\u6536\u76ca\u8bc1\u660e\u6210\u672c\u5408\u7406\u65f6\u624d\u88ab\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "2512.02230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02230", "abs": "https://arxiv.org/abs/2512.02230", "authors": ["Rory Milsom"], "title": "Benchmarking LLM Agents for Wealth-Management Workflows", "comment": "56 pages, 8 figures, The University of Edinburgh", "summary": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86TheAgentCompany\u6846\u67b6\uff0c\u521b\u5efa\u4e86\u8d22\u5bcc\u7ba1\u7406\u9886\u57df\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u53d1\u73b0LLM\u4ee3\u7406\u5728\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u800c\u975e\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u81ea\u4e3b\u6027\u6c34\u5e73\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u73b0\u4ee3\u6570\u5b57\u534f\u4f5c\u5de5\u5177\u4ecd\u5b58\u5728\u4eba\u4e3a\u9519\u8bef\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u7814\u7a76\u901a\u7528LLM\u4ee3\u7406\u80fd\u5426\u51c6\u786e\u4e14\u7ecf\u6d4e\u5730\u5b8c\u6210\u8d22\u5bcc\u7ba1\u7406\u4efb\u52a1\uff0c\u4ee5\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6269\u5c55TheAgentCompany\u6846\u67b6\uff0c\u521b\u5efa\u91d1\u878d\u9886\u57df\u73af\u5883\uff0c\u5f15\u5165\u5408\u6210\u9886\u57df\u6570\u636e\uff0c\u4e30\u5bcc\u540c\u4e8b\u6a21\u62df\uff0c\u6784\u5efa\u5305\u542b12\u4e2a\u4efb\u52a1\u5bf9\u7684\u8d22\u5bcc\u7ba1\u7406\u52a9\u624b\u57fa\u51c6\uff0c\u6db5\u76d6\u68c0\u7d22\u3001\u5206\u6790\u548c\u7efc\u5408/\u6c9f\u901a\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u9ad8/\u4f4e\u81ea\u4e3b\u6027\u53d8\u4f53\u3002", "result": "\u4ee3\u7406\u7684\u4e3b\u8981\u9650\u5236\u5728\u4e8e\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u53ef\u9760\u6027\u800c\u975e\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u81ea\u4e3b\u6027\u6c34\u5e73\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u73b0\u6709\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u963b\u788d\u4e86\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u9700\u8981\u521b\u5efa\u6709\u610f\u4e49\u7684\u8bc4\u4f30\u96c6\u6765\u8861\u91cf\u4ee3\u7406\u5728\u8d22\u5bcc\u7ba1\u7406\u52a9\u624b\u5de5\u4f5c\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u51c6\u786e\u53cd\u6620\u4ee3\u7406\u7684\u5b9e\u9645\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2512.02261", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02261", "abs": "https://arxiv.org/abs/2512.02261", "authors": ["Lewen Yan", "Jilin Mei", "Tianyi Zhou", "Lige Huang", "Jie Zhang", "Dongrui Liu", "Jing Shao"], "title": "TradeTrap: Are LLM-based Trading Agents Truly Reliable and Faithful?", "comment": null, "summary": "LLM-based trading agents are increasingly deployed in real-world financial markets to perform autonomous analysis and execution. However, their reliability and robustness under adversarial or faulty conditions remain largely unexamined, despite operating in high-risk, irreversible financial environments. We propose TradeTrap, a unified evaluation framework for systematically stress-testing both adaptive and procedural autonomous trading agents. TradeTrap targets four core components of autonomous trading agents: market intelligence, strategy formulation, portfolio and ledger handling, and trade execution, and evaluates their robustness under controlled system-level perturbations. All evaluations are conducted in a closed-loop historical backtesting setting on real US equity market data with identical initial conditions, enabling fair and reproducible comparisons across agents and attacks. Extensive experiments show that small perturbations at a single component can propagate through the agent decision loop and induce extreme concentration, runaway exposure, and large portfolio drawdowns across both agent types, demonstrating that current autonomous trading agents can be systematically misled at the system level. Our code is available at https://github.com/Yanlewen/TradeTrap.", "AI": {"tldr": "TradeTrap\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u538b\u529b\u6d4b\u8bd5\u81ea\u9002\u5e94\u548c\u7a0b\u5e8f\u5316\u81ea\u4e3b\u4ea4\u6613\u4ee3\u7406\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u9488\u5bf9\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u65bd\u52a0\u7cfb\u7edf\u7ea7\u6270\u52a8\uff0c\u53d1\u73b0\u5c0f\u6270\u52a8\u53ef\u5728\u51b3\u7b56\u5faa\u73af\u4e2d\u4f20\u64ad\u5e76\u5bfc\u81f4\u6781\u7aef\u98ce\u9669\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u4ea4\u6613\u4ee3\u7406\u5728\u771f\u5b9e\u91d1\u878d\u5e02\u573a\u4e2d\u90e8\u7f72\u8d8a\u6765\u8d8a\u591a\uff0c\u4f46\u5728\u5bf9\u6297\u6027\u6216\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u68c0\u9a8c\uff0c\u800c\u91d1\u878d\u73af\u5883\u5177\u6709\u9ad8\u98ce\u9669\u548c\u4e0d\u53ef\u9006\u6027\u3002", "method": "\u63d0\u51faTradeTrap\u6846\u67b6\uff0c\u9488\u5bf9\u81ea\u4e3b\u4ea4\u6613\u4ee3\u7406\u7684\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08\u5e02\u573a\u60c5\u62a5\u3001\u7b56\u7565\u5236\u5b9a\u3001\u6295\u8d44\u7ec4\u5408\u548c\u8d26\u672c\u5904\u7406\u3001\u4ea4\u6613\u6267\u884c\uff09\u65bd\u52a0\u7cfb\u7edf\u7ea7\u6270\u52a8\uff0c\u5728\u771f\u5b9e\u7f8e\u80a1\u5e02\u573a\u6570\u636e\u7684\u95ed\u73af\u5386\u53f2\u56de\u6d4b\u73af\u5883\u4e2d\u8fdb\u884c\u516c\u5e73\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u4e2a\u7ec4\u4ef6\u7684\u5c0f\u6270\u52a8\u53ef\u5728\u4ee3\u7406\u51b3\u7b56\u5faa\u73af\u4e2d\u4f20\u64ad\uff0c\u5bfc\u81f4\u6781\u7aef\u96c6\u4e2d\u3001\u5931\u63a7\u66b4\u9732\u548c\u5927\u5e45\u6295\u8d44\u7ec4\u5408\u56de\u64a4\uff0c\u8bc1\u660e\u5f53\u524d\u81ea\u4e3b\u4ea4\u6613\u4ee3\u7406\u53ef\u5728\u7cfb\u7edf\u5c42\u9762\u88ab\u7cfb\u7edf\u6027\u8bef\u5bfc\u3002", "conclusion": "\u5f53\u524d\u81ea\u4e3b\u4ea4\u6613\u4ee3\u7406\u5728\u7cfb\u7edf\u5c42\u9762\u5b58\u5728\u8106\u5f31\u6027\uff0cTradeTrap\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u4ea4\u6613\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u91d1\u878dAI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.02282", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.02282", "abs": "https://arxiv.org/abs/2512.02282", "authors": ["Han Luo", "Guy Laban"], "title": "DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses", "comment": null, "summary": "Large language models (LLMs) now mediate many web-based mental-health, crisis, and other emotionally sensitive services, yet their psychosocial safety in these settings remains poorly understood and weakly evaluated. We present DialogGuard, a multi-agent framework for assessing psychosocial risks in LLM-generated responses along five high-severity dimensions: privacy violations, discriminatory behaviour, mental manipulation, psychological harm, and insulting behaviour. DialogGuard can be applied to diverse generative models through four LLM-as-a-judge pipelines, including single-agent scoring, dual-agent correction, multi-agent debate, and stochastic majority voting, grounded in a shared three-level rubric usable by both human annotators and LLM judges. Using PKU-SafeRLHF with human safety annotations, we show that multi-agent mechanisms detect psychosocial risks more accurately than non-LLM baselines and single-agent judging; dual-agent correction and majority voting provide the best trade-off between accuracy, alignment with human ratings, and robustness, while debate attains higher recall but over-flags borderline cases. We release Dialog-Guard as open-source software with a web interface that provides per-dimension risk scores and explainable natural-language rationales. A formative study with 12 practitioners illustrates how it supports prompt design, auditing, and supervision of web-facing applications for vulnerable users.", "AI": {"tldr": "DialogGuard\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u5185\u5bb9\u5728\u4e94\u4e2a\u9ad8\u98ce\u9669\u7ef4\u5ea6\u4e0a\u7684\u5fc3\u7406\u793e\u4f1a\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u9690\u79c1\u4fb5\u72af\u3001\u6b67\u89c6\u884c\u4e3a\u3001\u5fc3\u7406\u64cd\u7eb5\u3001\u5fc3\u7406\u4f24\u5bb3\u548c\u4fae\u8fb1\u884c\u4e3a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u73b0\u5728\u88ab\u7528\u4e8e\u8bb8\u591a\u57fa\u4e8e\u7f51\u7edc\u7684\u5fc3\u7406\u5065\u5eb7\u3001\u5371\u673a\u548c\u5176\u4ed6\u60c5\u611f\u654f\u611f\u670d\u52a1\uff0c\u4f46\u5b83\u4eec\u5728\u5fc3\u7406\u793e\u4f1a\u5b89\u5168\u65b9\u9762\u7684\u8868\u73b0\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u548c\u6709\u6548\u8bc4\u4f30\u3002", "method": "\u63d0\u51faDialogGuard\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u79cdLLM-as-a-judge\u6d41\u7a0b\uff08\u5355\u667a\u80fd\u4f53\u8bc4\u5206\u3001\u53cc\u667a\u80fd\u4f53\u6821\u6b63\u3001\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u548c\u968f\u673a\u591a\u6570\u6295\u7968\uff09\u6765\u8bc4\u4f30\u5fc3\u7406\u793e\u4f1a\u98ce\u9669\uff0c\u57fa\u4e8e\u5171\u4eab\u7684\u4e09\u7ea7\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u591a\u667a\u80fd\u4f53\u673a\u5236\u6bd4\u975eLLM\u57fa\u7ebf\u548c\u5355\u667a\u80fd\u4f53\u8bc4\u4f30\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u5fc3\u7406\u793e\u4f1a\u98ce\u9669\uff1b\u53cc\u667a\u80fd\u4f53\u6821\u6b63\u548c\u591a\u6570\u6295\u7968\u5728\u51c6\u786e\u6027\u3001\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff1b\u8fa9\u8bba\u65b9\u6cd5\u53ec\u56de\u7387\u66f4\u9ad8\u4f46\u4f1a\u8fc7\u5ea6\u6807\u8bb0\u8fb9\u754c\u6848\u4f8b\u3002", "conclusion": "DialogGuard\u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u53d1\u5e03\uff0c\u63d0\u4f9b\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u98ce\u9669\u8bc4\u5206\u548c\u53ef\u89e3\u91ca\u7684\u81ea\u7136\u8bed\u8a00\u7406\u7531\uff0c\u652f\u6301\u9762\u5411\u8106\u5f31\u7528\u6237\u7684\u7f51\u7edc\u5e94\u7528\u7684\u63d0\u793a\u8bbe\u8ba1\u3001\u5ba1\u8ba1\u548c\u76d1\u7763\u3002", "topic": "agent analysis"}}
{"id": "2512.02589", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.02589", "abs": "https://arxiv.org/abs/2512.02589", "authors": ["Junyi Hou", "Andre Lin Huikai", "Nuo Chen", "Yiwei Gong", "Bingsheng He"], "title": "PaperDebugger: A Plugin-Based Multi-Agent System for In-Editor Academic Writing, Review, and Editing", "comment": null, "summary": "Large language models are increasingly embedded into academic writing workflows, yet existing assistants remain external to the editor, preventing deep interaction with document state, structure, and revision history. This separation makes it impossible to support agentic, context-aware operations directly within LaTeX editors such as Overleaf. We present PaperDebugger, an in-editor, multi-agent, and plugin-based academic writing assistant that brings LLM-driven reasoning directly into the writing environment. Enabling such in-editor interaction is technically non-trivial: it requires reliable bidirectional synchronization with the editor, fine-grained version control and patching, secure state management, multi-agent scheduling, and extensible communication with external tools. PaperDebugger addresses these challenges through a Chrome-approved extension, a Kubernetes-native orchestration layer, and a Model Context Protocol (MCP) toolchain that integrates literature search, reference lookup, document scoring, and revision pipelines. Our demo showcases a fully integrated workflow, including localized edits, structured reviews, parallel agent execution, and diff-based updates, encapsulated within a minimal-intrusion user interface (UI). Early aggregated analytics demonstrate active user engagement and validate the practicality of an editor-native, agentic writing assistant. More details about this demo and video could be found at https://github.com/PaperDebugger/PaperDebugger.", "AI": {"tldr": "PaperDebugger\u662f\u4e00\u4e2a\u96c6\u6210\u5728LaTeX\u7f16\u8f91\u5668\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u5b66\u672f\u5199\u4f5c\u52a9\u624b\uff0c\u901a\u8fc7Chrome\u6269\u5c55\u3001Kubernetes\u7f16\u6392\u5c42\u548cMCP\u5de5\u5177\u94fe\u5b9e\u73b0\u6df1\u5ea6\u6587\u6863\u4ea4\u4e92\u548c\u667a\u80fd\u5199\u4f5c\u652f\u6301\u3002", "motivation": "\u73b0\u6709LLM\u5199\u4f5c\u52a9\u624b\u901a\u5e38\u72ec\u7acb\u4e8e\u7f16\u8f91\u5668\u4e4b\u5916\uff0c\u65e0\u6cd5\u4e0e\u6587\u6863\u72b6\u6001\u3001\u7ed3\u6784\u548c\u4fee\u8ba2\u5386\u53f2\u6df1\u5ea6\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u5728LaTeX\u7f16\u8f91\u5668\uff08\u5982Overleaf\uff09\u4e2d\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2aChrome\u6279\u51c6\u7684\u6269\u5c55\u7a0b\u5e8f\uff0c\u5305\u542bKubernetes\u539f\u751f\u7f16\u6392\u5c42\u548cModel Context Protocol\u5de5\u5177\u94fe\uff0c\u652f\u6301\u6587\u732e\u641c\u7d22\u3001\u53c2\u8003\u6587\u732e\u67e5\u627e\u3001\u6587\u6863\u8bc4\u5206\u548c\u4fee\u8ba2\u7ba1\u9053\u7b49\u529f\u80fd\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u53cc\u5411\u540c\u6b65\u3001\u7ec6\u7c92\u5ea6\u7248\u672c\u63a7\u5236\u3001\u5b89\u5168\u72b6\u6001\u7ba1\u7406\u548c\u591a\u667a\u80fd\u4f53\u8c03\u5ea6\u3002", "result": "\u5c55\u793a\u4e86\u5b8c\u5168\u96c6\u6210\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u672c\u5730\u5316\u7f16\u8f91\u3001\u7ed3\u6784\u5316\u8bc4\u5ba1\u3001\u5e76\u884c\u667a\u80fd\u4f53\u6267\u884c\u548c\u57fa\u4e8e\u5dee\u5f02\u7684\u66f4\u65b0\uff0c\u901a\u8fc7\u6700\u5c0f\u4fb5\u5165\u5f0fUI\u5b9e\u73b0\u3002\u65e9\u671f\u805a\u5408\u5206\u6790\u663e\u793a\u7528\u6237\u79ef\u6781\u53c2\u4e0e\uff0c\u9a8c\u8bc1\u4e86\u7f16\u8f91\u5668\u539f\u751f\u667a\u80fd\u5199\u4f5c\u52a9\u624b\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "PaperDebugger\u6210\u529f\u89e3\u51b3\u4e86\u5728\u7f16\u8f91\u5668\u5185\u96c6\u6210LLM\u9a71\u52a8\u63a8\u7406\u7684\u6280\u672f\u6311\u6218\uff0c\u4e3a\u5b66\u672f\u5199\u4f5c\u63d0\u4f9b\u4e86\u6df1\u5ea6\u4ea4\u4e92\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u52a9\u624b\uff0c\u5c55\u793a\u4e86\u7f16\u8f91\u5668\u539f\u751f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "topic": "swe application"}}
{"id": "2512.02195", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02195", "abs": "https://arxiv.org/abs/2512.02195", "authors": ["David Ph. Shakouri", "Crit Cremers", "Niels O. Schiller"], "title": "A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation", "comment": "23 pages, 7 figures, 11 tables. Related work: arXiv:2503.18702. This is the peer-reviewed publisher's version, downloadable from: https://www.clinjournal.org/clinj/article/view/193", "summary": "This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.", "AI": {"tldr": "MODOMA\u662f\u4e00\u4e2a\u7528\u4e8e\u65e0\u76d1\u7763\u8bed\u8a00\u4e60\u5f97\u5b9e\u9a8c\u7684\u8ba1\u7b97\u591a\u667a\u80fd\u4f53\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u901a\u8fc7\u6210\u5e74\u548c\u513f\u7ae5\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u5b9e\u73b0\u8bed\u8a00\u4e60\u5f97\uff0c\u80fd\u591f\u83b7\u53d6\u548c\u8868\u793a\u529f\u80fd\u4e0e\u5185\u5bb9\u7c7b\u522b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u53c2\u6570\u5316\u3001\u53ef\u63a7\u7684\u8ba1\u7b97\u8bed\u8a00\u4e60\u5f97\u5b9e\u9a8c\u7cfb\u7edf\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u63a7\u5236\u5b9e\u9a8c\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5e76\u660e\u786e\u8868\u793a\u548c\u67e5\u8be2\u4e60\u5f97\u7684\u8bed\u6cd5\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u6210\u5e74\u667a\u80fd\u4f53\u548c\u513f\u7ae5\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u7edf\u8ba1\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7a0b\u5e8f\u8fdb\u884c\u8bed\u8a00\u4e60\u5f97\uff0c\u751f\u6210\u77e5\u8bc6\u578b\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u548c\u89e3\u6790\u76ee\u6807\u8bed\u8a00\u7684\u65b0\u8bdd\u8bed\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u513f\u7ae5\u667a\u80fd\u4f53\u80fd\u591f\u57fa\u4e8e\u6210\u5e74\u667a\u80fd\u4f53\u751f\u6210\u7684\u4e0d\u540c\u6570\u91cf\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\uff0c\u6210\u529f\u83b7\u53d6\u548c\u8868\u793a\u529f\u80fd\u4e0e\u5185\u5bb9\u7c7b\u522b\uff0c\u4e14\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u751f\u6210\u6570\u636e\u7684\u5df2\u77e5\u6a21\u5f0f\u76f8\u4f3c\u3002", "conclusion": "MODOMA\u65b9\u6cd5\u5728\u5efa\u6a21\u8bed\u8a00\u4e60\u5f97\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\uff0c\u4e3a\u8ba1\u7b97\u8bed\u8a00\u4e60\u5f97\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u80fd\u591f\u6210\u529f\u83b7\u53d6\u79bb\u6563\u8bed\u6cd5\u7c7b\u522b\u3002", "topic": "agent analysis"}}
{"id": "2512.02605", "categories": ["cs.AI", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.02605", "abs": "https://arxiv.org/abs/2512.02605", "authors": ["Pengju Lu"], "title": "IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai", "comment": "13 pages, 2 figures, 1 table", "summary": "This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.", "AI": {"tldr": "IACT\u662f\u4e00\u79cd\u8ba1\u7b97\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u5bf9\u8bdd\u9a71\u52a8\u7684\u52a8\u6001\u9012\u5f52\u4ee3\u7406\u62d3\u6251\u6765\u89e3\u51b3\u9759\u6001\u786c\u7f16\u7801\u5de5\u4f5c\u6d41\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u9519\u8bef\u7ea0\u6b63\u548c\u6a21\u7cca\u6027\u89e3\u6790\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u786c\u7f16\u7801\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u5c40\u9650\u6027\uff0c\u4f20\u7edf\u7cfb\u7edf\u9700\u8981\u9884\u5b9a\u4e49\u56fe\u6216\u4e13\u95e8\u7f16\u7a0b\uff0c\u65e0\u6cd5\u9002\u5e94\u5f00\u653e\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7528\u6237\u5bf9\u8bdd\u7684\u901a\u7528\u81ea\u4e3b\u7cfb\u7edf\uff0c\u52a8\u6001\u589e\u957f\u9012\u5f52\u4ee3\u7406\u62d3\u6251\uff0c\u901a\u8fc7\u53cc\u5411\u72b6\u6001\u5bf9\u8bdd\u66ff\u4ee3\u5355\u5411\u51fd\u6570\u8c03\u7528\uff0c\u5b9e\u73b0\u8fd0\u884c\u65f6\u9519\u8bef\u7ea0\u6b63\u3002", "result": "\u5728kragent.ai\u7cfb\u7edf\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u901a\u8fc7\u5b9e\u9645\u5de5\u4f5c\u6d41\u63d0\u4f9b\u5b9a\u6027\u8bc1\u636e\uff0c\u5c55\u793a\u5176\u9002\u5e94\u5f00\u653e\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "IACT\u6a21\u578b\u901a\u8fc7\u52a8\u6001\u4ee3\u7406\u62d3\u6251\u548c\u4ea4\u4e92\u5197\u4f59\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7406\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u653e\u4efb\u52a1\u63d0\u4f9b\u4e86\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.02240", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02240", "abs": "https://arxiv.org/abs/2512.02240", "authors": ["Alexander Gurung", "Nikolay Malkin", "Mirella Lapata"], "title": "Lightweight Latent Reasoning for Narrative Tasks", "comment": null, "summary": "Large language models (LLMs) tackle complex tasks by generating long chains of thought or \"reasoning traces\" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.", "AI": {"tldr": "LiteReason\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u6295\u5f71\u5668\u6a21\u5757\u751f\u6210\u8fde\u7eed\u6f5c\u5728\u6807\u8bb0\u6765\u8df3\u8fc7\u63a8\u7406\u6b65\u9aa4\uff0c\u7ed3\u5408RL\u4f18\u5316\uff0c\u5728\u51cf\u5c1177-92%\u63a8\u7406\u957f\u5ea6\u7684\u540c\u65f6\u63a5\u8fd1\u975e\u6f5c\u5728RL\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "motivation": "LLMs\u901a\u8fc7\u751f\u6210\u957f\u63a8\u7406\u94fe\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u4f7f\u7528RL\u4f18\u5316\u63a8\u7406\u80fd\u529b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u5927\u91cf\u6807\u8bb0\u5904\u7406\u7684\u53d9\u4e8b\u76f8\u5173\u4efb\u52a1\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLiteReason\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u63a8\u7406\u6295\u5f71\u5668\u6a21\u5757\u751f\u6210\u8fde\u7eed\u6f5c\u5728\u6807\u8bb0\u6765\u8df3\u8fc7\u63a8\u7406\u6b65\u9aa4\uff1b2) \u5728RL\u8bad\u7ec3\u4e2d\uff0c\u7b56\u7565\u6a21\u578b\u51b3\u5b9a\u4f55\u65f6\u6fc0\u6d3b\u6295\u5f71\u5668\uff0c\u5728\u6f5c\u5728\u63a8\u7406\u548c\u79bb\u6563\u63a8\u7406\u4e4b\u95f4\u5207\u6362\uff1b3) \u53ef\u4e0e\u6807\u51c6\u6807\u8bb0\u91c7\u6837\u4ea4\u9519\u8fdb\u884c\u3002", "result": "\u5728\u60c5\u8282\u6f0f\u6d1e\u68c0\u6d4b\u548c\u4e66\u7c4d\u7ae0\u8282\u751f\u6210\u4efb\u52a1\u4e0a\uff0cLiteReason\u4f18\u4e8e\u6f5c\u5728\u63a8\u7406\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1\u975e\u6f5c\u5728RL\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u6700\u7ec8\u63a8\u7406\u957f\u5ea677-92%\uff0c\u5728\u6027\u80fd-\u8ba1\u7b97\u6743\u8861\u66f2\u7ebf\u4e0a\u8fbe\u5230\u66f4\u9ad8\u6548\u7684\u4f4d\u7f6e\u3002", "conclusion": "LiteReason\u901a\u8fc7\u7ed3\u5408\u6f5c\u5728\u63a8\u7406\u548cRL\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u957f\u5ea6\uff0c\u4e3aLLMs\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02246", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02246", "abs": "https://arxiv.org/abs/2512.02246", "authors": ["Olivia Kim"], "title": "DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models", "comment": null, "summary": "Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DETAIL\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u7279\u5f02\u6027\u6c34\u5e73\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7279\u5f02\u6027\u63d0\u793a\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5bf9\u5c0f\u6a21\u578b\u548c\u7a0b\u5e8f\u6027\u4efb\u52a1\u6548\u679c\u66f4\u660e\u663e\u3002", "motivation": "\u63d0\u793a\u8bbe\u8ba1\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u63d0\u793a\u7279\u5f02\u6027\uff08\u63d0\u793a\u7684\u8be6\u7ec6\u7a0b\u5ea6\u6216\u6a21\u7cca\u7a0b\u5ea6\uff09\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u7279\u5f02\u6027\u6c34\u5e73\u4e0bLLM\u7684\u8868\u73b0\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86DETAIL\u6846\u67b6\uff1a1\uff09\u4f7f\u7528GPT-4\u751f\u6210\u591a\u7ea7\u7279\u5f02\u6027\u63d0\u793a\uff1b2\uff09\u901a\u8fc7\u56f0\u60d1\u5ea6\u91cf\u5316\u63d0\u793a\u7279\u5f02\u6027\uff1b3\uff09\u4f7f\u7528\u57fa\u4e8eGPT\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u8bc4\u4f30\u6b63\u786e\u6027\u3002\u572830\u4e2a\u65b0\u9896\u63a8\u7406\u4efb\u52a1\u4e0a\u5bf9GPT-4\u548cO3-mini\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u63d0\u793a\u7279\u5f02\u6027\u63d0\u9ad8\u51c6\u786e\u6027\uff1b2\uff09\u5bf9\u5c0f\u6a21\u578b\u7684\u6548\u679c\u6bd4\u5bf9\u5927\u6a21\u578b\u66f4\u660e\u663e\uff1b3\uff09\u5bf9\u7a0b\u5e8f\u6027\u4efb\u52a1\u7684\u63d0\u5347\u6bd4\u5bf9\u5176\u4ed6\u7c7b\u578b\u4efb\u52a1\u66f4\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u81ea\u9002\u5e94\u63d0\u793a\u7b56\u7565\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u6570\u636e\u652f\u6301\u3002\u63d0\u793a\u7279\u5f02\u6027\u662f\u5f71\u54cdLLM\u6027\u80fd\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u9700\u8981\u6839\u636e\u6a21\u578b\u5927\u5c0f\u548c\u4efb\u52a1\u7c7b\u578b\u8c03\u6574\u63d0\u793a\u8be6\u7ec6\u7a0b\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2512.02358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02358", "abs": "https://arxiv.org/abs/2512.02358", "authors": ["Ran Zhang", "Kun Ouyang", "Tiancheng Ma", "Yida Yang", "Dong Fang"], "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games", "comment": null, "summary": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53MMO\u6a21\u62df\u7cfb\u7edf\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4f7fLLM\u9002\u5e94\u6e38\u620f\u7279\u5b9a\u9886\u57df\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u73af\u5883\u6a21\u578b\uff0c\u4e3a\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u3001\u4f4e\u6210\u672c\u7684\u6846\u67b6\u3002", "motivation": "\u4f20\u7edfMMO\u6e38\u620f\u6570\u503c\u7cfb\u7edf\u548c\u673a\u5236\u8bbe\u8ba1\u4f18\u5316\u4f9d\u8d56\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\u6216\u57fa\u4e8e\u9884\u5b9a\u4e49\u7edf\u8ba1\u6a21\u578b\u7684\u53c2\u6570\u8c03\u4f18\uff0c\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u53ef\u80fd\u7834\u574f\u73a9\u5bb6\u4f53\u9a8c\u3002\u7b80\u5316\u79bb\u7ebf\u6a21\u62df\u7cfb\u7edf\u4fdd\u771f\u5ea6\u6709\u9650\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u73a9\u5bb6\u7684\u63a8\u7406\u548c\u5e72\u9884\u53cd\u5e94\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53MMO\u6a21\u62df\u7cfb\u7edf\uff1a1) \u5728\u5927\u89c4\u6a21\u771f\u5b9e\u73a9\u5bb6\u884c\u4e3a\u6570\u636e\u4e0a\u5e94\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7fLLM\u4ece\u901a\u7528\u5148\u9a8c\u9002\u5e94\u6e38\u620f\u7279\u5b9a\u9886\u57df\uff1b2) \u57fa\u4e8e\u771f\u5b9e\u6e38\u620f\u65e5\u5fd7\u8bad\u7ec3\u6570\u636e\u9a71\u52a8\u7684\u73af\u5883\u6a21\u578b\uff0c\u91cd\u5efa\u52a8\u6001\u6e38\u620f\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u4e0e\u771f\u5b9e\u4e16\u754c\u73a9\u5bb6\u884c\u4e3a\u5177\u6709\u5f3a\u4e00\u81f4\u6027\uff0c\u5728\u5e72\u9884\u4e0b\u4ea7\u751f\u5408\u7406\u7684\u56e0\u679c\u54cd\u5e94\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u3001\u6210\u672c\u9ad8\u6548\u7684\u6846\u67b6\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6a21\u62df\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edfMMO\u6e38\u620f\u6570\u503c\u8bbe\u8ba1\u4f18\u5316\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u9ad8\u4fdd\u771f\u3001\u53ef\u89e3\u91ca\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.02436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02436", "abs": "https://arxiv.org/abs/2512.02436", "authors": ["Agostino Capponi", "Alfio Gliozzo", "Brian Zhu"], "title": "Semantic Trading: Agentic AI for Clustering and Relationship Discovery in Prediction Markets", "comment": null, "summary": "Prediction markets allow users to trade on outcomes of real-world events, but are prone to fragmentation through overlapping questions, implicit equivalences, and hidden contradictions across markets. We present an agentic AI pipeline that autonomously (i) clusters markets into coherent topical groups using natural-language understanding over contract text and metadata, and (ii) identifies within-cluster market pairs whose resolved outcomes exhibit strong dependence, including same-outcome (correlated) and different-outcome (anti-correlated) relationships. Using a historical dataset of resolved markets on Polymarket, we evaluate the accuracy of the agent's relational predictions. We then translate discovered relationships into a simple trading strategy to quantify how these relationships map to actionable signals. Results show that agent-identified relationships achieve roughly 60-70% accuracy, and their induced trading strategies earn about 20% average returns over week-long horizons, highlighting the ability of agentic AI and large language models to uncover latent semantic structure in prediction markets.", "AI": {"tldr": "AI\u4ee3\u7406\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u805a\u7c7b\u9884\u6d4b\u5e02\u573a\u5e76\u8bc6\u522b\u5e02\u573a\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u6784\u5efa\u4ea4\u6613\u7b56\u7565\u83b7\u5f97\u7ea620%\u7684\u5468\u56de\u62a5\u7387", "motivation": "\u9884\u6d4b\u5e02\u573a\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u5305\u62ec\u91cd\u53e0\u95ee\u9898\u3001\u9690\u542b\u7b49\u4ef7\u5173\u7cfb\u548c\u9690\u85cf\u77db\u76fe\uff0c\u9700\u8981\u7cfb\u7edf\u65b9\u6cd5\u6765\u53d1\u73b0\u5e02\u573a\u95f4\u7684\u6f5c\u5728\u8bed\u4e49\u7ed3\u6784", "method": "\u5f00\u53d1AI\u4ee3\u7406\u7ba1\u9053\uff1a1\uff09\u57fa\u4e8e\u5408\u540c\u6587\u672c\u548c\u5143\u6570\u636e\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u8fdb\u884c\u5e02\u573a\u805a\u7c7b\uff1b2\uff09\u8bc6\u522b\u96c6\u7fa4\u5185\u5e02\u573a\u5bf9\u7684\u7ed3\u679c\u4f9d\u8d56\u5173\u7cfb\uff08\u6b63\u76f8\u5173\u548c\u8d1f\u76f8\u5173\uff09", "result": "\u4ee3\u7406\u8bc6\u522b\u7684\u5173\u7cfb\u51c6\u786e\u7387\u8fbe60-70%\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u5173\u7cfb\u6784\u5efa\u7684\u4ea4\u6613\u7b56\u7565\u5728\u5468\u5ea6\u65f6\u95f4\u8303\u56f4\u5185\u83b7\u5f97\u7ea620%\u7684\u5e73\u5747\u56de\u62a5", "conclusion": "AI\u4ee3\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u53d1\u73b0\u9884\u6d4b\u5e02\u573a\u4e2d\u7684\u6f5c\u5728\u8bed\u4e49\u7ed3\u6784\uff0c\u4e3a\u4ea4\u6613\u7b56\u7565\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4fe1\u53f7", "topic": "agent analysis"}}
{"id": "2512.02214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02214", "abs": "https://arxiv.org/abs/2512.02214", "authors": ["Aida Afshar", "Aldo Pacchiano"], "title": "Improved Training Mechanism for Reinforcement Learning via Online Model Selection", "comment": null, "summary": "We study the problem of online model selection in reinforcement learning, where the selector has access to a class of reinforcement learning agents and learns to adaptively select the agent with the right configuration. Our goal is to establish the improved efficiency and performance gains achieved by integrating online model selection methods into reinforcement learning training procedures. We examine the theoretical characterizations that are effective for identifying the right configuration in practice, and address three practical criteria from a theoretical perspective: 1) Efficient resource allocation, 2) Adaptation under non-stationary dynamics, and 3) Training stability across different seeds. Our theoretical results are accompanied by empirical evidence from various model selection tasks in reinforcement learning, including neural architecture selection, step-size selection, and self model selection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\uff0c\u4ece\u7406\u8bba\u89d2\u5ea6\u89e3\u51b3\u8d44\u6e90\u5206\u914d\u3001\u975e\u5e73\u7a33\u52a8\u6001\u9002\u5e94\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e09\u4e2a\u5b9e\u9645\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u9700\u8981\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u914d\u7f6e\uff08\u5982\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3001\u6b65\u957f\u7b49\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bd5\u9519\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u8ba9\u7cfb\u7edf\u80fd\u591f\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u4f73\u914d\u7f6e\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u6846\u67b6\uff0c\u8ba9\u9009\u62e9\u5668\u80fd\u591f\u8bbf\u95ee\u4e00\u7ec4\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5e76\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6b63\u786e\u914d\u7f6e\u3002\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u4e09\u4e2a\u5b9e\u9645\u6807\u51c6\uff1a1\uff09\u9ad8\u6548\u8d44\u6e90\u5206\u914d\uff1b2\uff09\u975e\u5e73\u7a33\u52a8\u6001\u4e0b\u7684\u9002\u5e94\uff1b3\uff09\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u4e0b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u6b63\u786e\u914d\u7f6e\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u9009\u62e9\u3001\u6b65\u957f\u9009\u62e9\u548c\u81ea\u6a21\u578b\u9009\u62e9\u7b49\u4efb\u52a1\u7684\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u5206\u914d\u3001\u975e\u5e73\u7a33\u73af\u5883\u9002\u5e94\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02556", "abs": "https://arxiv.org/abs/2512.02556", "authors": ["DeepSeek-AI", "Aixin Liu", "Aoxue Mei", "Bangcai Lin", "Bing Xue", "Bingxuan Wang", "Bingzheng Xu", "Bochao Wu", "Bowei Zhang", "Chaofan Lin", "Chen Dong", "Chengda Lu", "Chenggang Zhao", "Chengqi Deng", "Chenhao Xu", "Chong Ruan", "Damai Dai", "Daya Guo", "Dejian Yang", "Deli Chen", "Erhang Li", "Fangqi Zhou", "Fangyun Lin", "Fucong Dai", "Guangbo Hao", "Guanting Chen", "Guowei Li", "H. Zhang", "Hanwei Xu", "Hao Li", "Haofen Liang", "Haoran Wei", "Haowei Zhang", "Haowen Luo", "Haozhe Ji", "Honghui Ding", "Hongxuan Tang", "Huanqi Cao", "Huazuo Gao", "Hui Qu", "Hui Zeng", "Jialiang Huang", "Jiashi Li", "Jiaxin Xu", "Jiewen Hu", "Jingchang Chen", "Jingting Xiang", "Jingyang Yuan", "Jingyuan Cheng", "Jinhua Zhu", "Jun Ran", "Junguang Jiang", "Junjie Qiu", "Junlong Li", "Junxiao Song", "Kai Dong", "Kaige Gao", "Kang Guan", "Kexin Huang", "Kexing Zhou", "Kezhao Huang", "Kuai Yu", "Lean Wang", "Lecong Zhang", "Lei Wang", "Liang Zhao", "Liangsheng Yin", "Lihua Guo", "Lingxiao Luo", "Linwang Ma", "Litong Wang", "Liyue Zhang", "M. S. Di", "M. Y Xu", "Mingchuan Zhang", "Minghua Zhang", "Minghui Tang", "Mingxu Zhou", "Panpan Huang", "Peixin Cong", "Peiyi Wang", "Qiancheng Wang", "Qihao Zhu", "Qingyang Li", "Qinyu Chen", "Qiushi Du", "Ruiling Xu", "Ruiqi Ge", "Ruisong Zhang", "Ruizhe Pan", "Runji Wang", "Runqiu Yin", "Runxin Xu", "Ruomeng Shen", "Ruoyu Zhang", "S. H. Liu", "Shanghao Lu", "Shangyan Zhou", "Shanhuang Chen", "Shaofei Cai", "Shaoyuan Chen", "Shengding Hu", "Shengyu Liu", "Shiqiang Hu", "Shirong Ma", "Shiyu Wang", "Shuiping Yu", "Shunfeng Zhou", "Shuting Pan", "Songyang Zhou", "Tao Ni", "Tao Yun", "Tian Pei", "Tian Ye", "Tianyuan Yue", "Wangding Zeng", "Wen Liu", "Wenfeng Liang", "Wenjie Pang", "Wenjing Luo", "Wenjun Gao", "Wentao Zhang", "Xi Gao", "Xiangwen Wang", "Xiao Bi", "Xiaodong Liu", "Xiaohan Wang", "Xiaokang Chen", "Xiaokang Zhang", "Xiaotao Nie", "Xin Cheng", "Xin Liu", "Xin Xie", "Xingchao Liu", "Xingkai Yu", "Xingyou Li", "Xinyu Yang", "Xinyuan Li", "Xu Chen", "Xuecheng Su", "Xuehai Pan", "Xuheng Lin", "Xuwei Fu", "Y. Q. Wang", "Yang Zhang", "Yanhong Xu", "Yanru Ma", "Yao Li", "Yao Li", "Yao Zhao", "Yaofeng Sun", "Yaohui Wang", "Yi Qian", "Yi Yu", "Yichao Zhang", "Yifan Ding", "Yifan Shi", "Yiliang Xiong", "Ying He", "Ying Zhou", "Yinmin Zhong", "Yishi Piao", "Yisong Wang", "Yixiao Chen", "Yixuan Tan", "Yixuan Wei", "Yiyang Ma", "Yiyuan Liu", "Yonglun Yang", "Yongqiang Guo", "Yongtong Wu", "Yu Wu", "Yuan Cheng", "Yuan Ou", "Yuanfan Xu", "Yuduan Wang", "Yue Gong", "Yuhan Wu", "Yuheng Zou", "Yukun Li", "Yunfan Xiong", "Yuxiang Luo", "Yuxiang You", "Yuxuan Liu", "Yuyang Zhou", "Z. F. Wu", "Z. Z. Ren", "Zehua Zhao", "Zehui Ren", "Zhangli Sha", "Zhe Fu", "Zhean Xu", "Zhenda Xie", "Zhengyan Zhang", "Zhewen Hao", "Zhibin Gou", "Zhicheng Ma", "Zhigang Yan", "Zhihong Shao", "Zhixian Huang", "Zhiyu Wu", "Zhuoshu Li", "Zhuping Zhang", "Zian Xu", "Zihao Wang", "Zihui Gu", "Zijia Zhu", "Zilin Li", "Zipeng Zhang", "Ziwei Xie", "Ziyi Gao", "Zizheng Pan", "Zongqing Yao", "Bei Feng", "Hui Li", "J. L. Cai", "Jiaqi Ni", "Lei Xu", "Meng Li", "Ning Tian", "R. J. Chen", "R. L. Jin", "S. S. Li", "Shuang Zhou", "Tianyu Sun", "X. Q. Li", "Xiangyue Jin", "Xiaojin Shen", "Xiaosha Chen", "Xinnan Song", "Xinyi Zhou", "Y. X. Zhu", "Yanping Huang", "Yaohui Li", "Yi Zheng", "Yuchen Zhu", "Yunxian Ma", "Zhen Huang", "Zhipeng Xu", "Zhongyu Zhang", "Dongjie Ji", "Jian Liang", "Jianzhong Guo", "Jin Chen", "Leyi Xia", "Miaojun Wang", "Mingming Li", "Peng Zhang", "Ruyi Chen", "Shangmian Sun", "Shaoqing Wu", "Shengfeng Ye", "T. Wang", "W. L. Xiao", "Wei An", "Xianzu Wang", "Xiaowen Sun", "Xiaoxiang Wang", "Ying Tang", "Yukun Zha", "Zekai Zhang", "Zhe Ju", "Zhen Zhang", "Zihua Qu"], "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "comment": null, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "AI": {"tldr": "DeepSeek-V3.2\u901a\u8fc7\u521b\u65b0\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u3001\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u4e0e\u5353\u8d8a\u63a8\u7406\u80fd\u529b\u7684\u5e73\u8861\uff0c\u5728\u591a\u9879\u56fd\u9645\u7ade\u8d5b\u4e2d\u8fbe\u5230\u9876\u5c16\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u3001\u63a8\u7406\u80fd\u529b\u548c\u667a\u80fd\u4f53\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. DeepSeek\u7a00\u758f\u6ce8\u610f\u529b(DSA)\uff1a\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b2. \u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u534f\u8bae\u548c\u6269\u5c55\u540e\u8bad\u7ec3\u8ba1\u7b97\uff1b3. \u5927\u89c4\u6a21\u667a\u80fd\u4f53\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff1a\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u3002", "result": "DeepSeek-V3.2\u5728\u8ba1\u7b97\u6548\u7387\u3001\u63a8\u7406\u548c\u667a\u80fd\u4f53\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u9ad8\u8ba1\u7b97\u53d8\u4f53DeepSeek-V3.2-Speciale\u8d85\u8d8aGPT-5\uff0c\u63a8\u7406\u80fd\u529b\u4e0eGemini-3.0-Pro\u76f8\u5f53\uff0c\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u4fe1\u606f\u6280\u672f\u5965\u6797\u5339\u514b\u4e2d\u83b7\u5f97\u91d1\u724c\u8868\u73b0\u3002", "conclusion": "DeepSeek-V3.2\u901a\u8fc7\u6280\u672f\u521b\u65b0\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u4e0e\u5353\u8d8a\u63a8\u7406\u80fd\u529b\u7684\u7edf\u4e00\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2512.02633", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02633", "abs": "https://arxiv.org/abs/2512.02633", "authors": ["Mattia Giuri", "Mathias Jackermeier", "Alessandro Abate"], "title": "Zero-Shot Instruction Following in RL via Structured LTL Representations", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u6761\u4ef6\u5316\u4e8e\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\u6765\u5b66\u4e60\u9075\u5faa\u4efb\u610fLTL\u6307\u4ee4\u7684\u591a\u4efb\u52a1\u7b56\u7565\uff0c\u4f7f\u7528GNN\u7f16\u7801\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\uff0c\u5728\u590d\u6742\u8c61\u68cb\u73af\u5883\u4e2d\u9a8c\u8bc1\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u4e2a\u9ad8\u7ea7\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\u4e14\u53ef\u80fd\u590d\u6742\u4ea4\u4e92\u7684\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdbLTL\u6846\u67b6\u4e0b\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u591a\u4efb\u52a1\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u5c06\u7b56\u7565\u6761\u4ef6\u5316\u4e8e\u4e0e\u81ea\u52a8\u673a\u8f6c\u6362\u76f4\u63a5\u5bf9\u9f50\u7684\u7b80\u5355\u5e03\u5c14\u516c\u5f0f\u5e8f\u5217\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u8fd9\u4e9b\u7ed3\u6784\u5316\u4efb\u52a1\u8868\u793a\u3002", "result": "\u5728\u590d\u6742\u7684\u8c61\u68cb\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u4e2a\u9ad8\u7ea7\u4e8b\u4ef6\u540c\u65f6\u53d1\u751f\u4e14\u590d\u6742\u4ea4\u4e92\u7684\u73af\u5883\uff0c\u4e3aLTL\u6846\u67b6\u4e0b\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02665", "abs": "https://arxiv.org/abs/2512.02665", "authors": ["Jing Ma"], "title": "Input Order Shapes LLM Semantic Alignment in Multi-Document Summarization", "comment": "9 pages, 3 figures, 2 tables", "summary": "Large language models (LLMs) are now used in settings such as Google's AI Overviews, where it summarizes multiple long documents. However, it remains unclear whether they weight all inputs equally. Focusing on abortion-related news, we construct 40 pro-neutral-con article triplets, permute each triplet into six input orders, and prompt Gemini 2.5 Flash to generate a neutral overview. We evaluate each summary against its source articles using ROUGE-L (lexical overlap), BERTScore (semantic similarity), and SummaC (factual consistency). One-way ANOVA reveals a significant primacy effect for BERTScore across all stances, indicating that summaries are more semantically aligned with the first-seen article. Pairwise comparisons further show that Position 1 differs significantly from Positions 2 and 3, while the latter two do not differ from each other, confirming a selective preference for the first document. The findings present risks for applications that rely on LLM-generated overviews and for agentic AI systems, where the steps involving LLMs can disproportionately influence downstream actions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u751f\u6210\u591a\u6587\u6863\u6458\u8981\u65f6\u5b58\u5728\u663e\u8457\u7684\u9996\u56e0\u6548\u5e94\uff0c\u6458\u8981\u8bed\u4e49\u66f4\u504f\u5411\u7b2c\u4e00\u4e2a\u8f93\u5165\u6587\u6863\uff0c\u8fd9\u5bf9\u4f9d\u8d56LLM\u6458\u8981\u7684\u5e94\u7528\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u6784\u6210\u98ce\u9669\u3002", "motivation": "\u5c3d\u7ba1LLM\u88ab\u5e7f\u6cdb\u7528\u4e8e\u591a\u6587\u6863\u6458\u8981\uff08\u5982Google AI Overviews\uff09\uff0c\u4f46\u5176\u662f\u5426\u5e73\u7b49\u5bf9\u5f85\u6240\u6709\u8f93\u5165\u6587\u6863\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u5728\u751f\u6210\u6458\u8981\u65f6\u662f\u5426\u5b58\u5728\u4f4d\u7f6e\u504f\u89c1\u3002", "method": "\u9488\u5bf9\u5815\u80ce\u76f8\u5173\u65b0\u95fb\uff0c\u6784\u5efa40\u4e2a\u652f\u6301-\u4e2d\u7acb-\u53cd\u5bf9\u6587\u7ae0\u4e09\u5143\u7ec4\uff0c\u6bcf\u4e2a\u4e09\u5143\u7ec4\u4ee56\u79cd\u4e0d\u540c\u987a\u5e8f\u6392\u5217\uff0c\u4f7f\u7528Gemini 2.5 Flash\u751f\u6210\u4e2d\u7acb\u6458\u8981\u3002\u901a\u8fc7ROUGE-L\u3001BERTScore\u548cSummaC\u8bc4\u4f30\u6458\u8981\u4e0e\u6e90\u6587\u7ae0\u7684\u5173\u7cfb\uff0c\u5e76\u8fdb\u884cANOVA\u5206\u6790\u3002", "result": "BERTScore\u663e\u793a\u663e\u8457\u7684\u9996\u56e0\u6548\u5e94\uff0c\u6458\u8981\u8bed\u4e49\u4e0e\u7b2c\u4e00\u4e2a\u770b\u5230\u7684\u6587\u7ae0\u66f4\u4e00\u81f4\u3002\u6210\u5bf9\u6bd4\u8f83\u8868\u660e\u4f4d\u7f6e1\u4e0e\u4f4d\u7f6e2\u30013\u6709\u663e\u8457\u5dee\u5f02\uff0c\u800c\u4f4d\u7f6e2\u548c3\u4e4b\u95f4\u65e0\u5dee\u5f02\uff0c\u786e\u8ba4\u4e86\u5bf9\u7b2c\u4e00\u4e2a\u6587\u6863\u7684\u9009\u62e9\u6027\u504f\u597d\u3002", "conclusion": "LLM\u5728\u591a\u6587\u6863\u6458\u8981\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u89c1\uff0c\u8fd9\u5bf9\u4f9d\u8d56LLM\u751f\u6210\u6458\u8981\u7684\u5e94\u7528\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\u6784\u6210\u98ce\u9669\uff0c\u56e0\u4e3aLLM\u6b65\u9aa4\u53ef\u80fd\u4e0d\u6210\u6bd4\u4f8b\u5730\u5f71\u54cd\u4e0b\u6e38\u884c\u52a8\u3002", "topic": "agent analysis"}}
{"id": "2512.02689", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02689", "abs": "https://arxiv.org/abs/2512.02689", "authors": ["Daiki Shirafuji", "Tatsuhiko Saito", "Yasutomo Kimura"], "title": "An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation", "comment": "Accepted in PACLIC 2025", "summary": "Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.", "AI": {"tldr": "\u5bf97\u79cd\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\u5728\u51cf\u8f7bLLM\u793e\u4f1a\u504f\u89c1\u65b9\u9762\u7684\u5b9e\u8bc1\u8c03\u67e5\uff0c\u53d1\u73b0\u504f\u89c1\u51cf\u5c11\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5176\u4e2dSLERP\u5728\u4e2d\u7b49\u63d2\u503c\u6743\u91cd\u4e0b\u8868\u73b0\u6700\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f1a\u7ee7\u627f\u751a\u81f3\u653e\u5927\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5a01\u80c1\u516c\u5e73\u6027\u548c\u793e\u4f1a\u4fe1\u4efb\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u901a\u8fc7\u6a21\u578b\u53c2\u6570\u7f16\u8f91\u6765\u51cf\u8f7b\u504f\u89c1\uff0c\u4f46\u7f3a\u4e4f\u4e0d\u540c\u7b97\u6cd5\u95f4\u7684\u5b9e\u8bc1\u6bd4\u8f83\u3002", "method": "\u5b9e\u8bc1\u8c03\u67e57\u79cd\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\uff08Linear\u3001Karcher Mean\u3001SLERP\u3001NuSLERP\u3001TIES\u3001DELLA\u3001Nearswap\uff09\uff0c\u5e94\u7528\u4e8eGPT\u3001LLaMA\u548cQwen\u5bb6\u65cf\u768413\u4e2a\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u3002\u4f7f\u75283\u4e2a\u504f\u89c1\u6570\u636e\u96c6\uff08BBQ\u3001BOLD\u3001HONEST\uff09\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5e76\u5728SuperGLUE\u57fa\u51c6\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6d4b\u91cf\u8fd9\u4e9b\u6280\u672f\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u504f\u89c1\u51cf\u5c11\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u5b9e\u73b0\u66f4\u5927\u504f\u89c1\u7f13\u89e3\u7684\u65b9\u6cd5\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9605\u8bfb\u7406\u89e3\u3001\u5e38\u8bc6\u548c\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u3002\u5728\u5408\u5e76\u7b97\u6cd5\u4e2d\uff0cLinear\u3001SLERP\u548cNearswap\u5728\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u7684\u540c\u65f6\u6301\u7eed\u51cf\u5c11\u504f\u89c1\uff0c\u5176\u4e2d\u4e2d\u7b49\u63d2\u503c\u6743\u91cd\u7684SLERP\u6210\u4e3a\u6700\u5e73\u8861\u7684\u9009\u62e9\u3002", "conclusion": "\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\u5728\u504f\u89c1\u7f13\u89e3\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8fc7\u5ea6\u7684\u53bb\u504f\u89c1\u5316\u6216\u4e0d\u9002\u5f53\u7684\u5408\u5e76\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u91cd\u8981\u8bed\u8a00\u80fd\u529b\u7684\u9000\u5316\u3002", "topic": "agent analysis"}}
{"id": "2512.02731", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02731", "abs": "https://arxiv.org/abs/2512.02731", "authors": ["Przemyslaw Chojecki"], "title": "Self-Improving AI Agents through Self-Play", "comment": null, "summary": "We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $\u03bd_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $\u0398$, and we identify the coefficient of self-improvement $\u03ba$ as the Lie derivative of the capability functional along this flow.\n  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $\u03ba> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.\n  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u5fc3\u7406\u6d4b\u91cf\u7535\u6c60\u7684\u6a21\u6570\u7406\u8bba\u6846\u67b6\u6269\u5c55\u5230\u52a8\u529b\u7cfb\u7edf\u9886\u57df\uff0c\u5f62\u5f0f\u5316\u5730\u5c06\u667a\u80fd\u4f53\u8868\u793a\u4e3a\u8ba1\u7b97\u8d44\u6e90\u53c2\u6570\u5316\u7684\u6d41\uff0c\u63a8\u5bfc\u51fa\u4fdd\u8bc1\u81ea\u6211\u6539\u8fdb\u7a33\u5b9a\u6027\u7684\u65b9\u5dee\u4e0d\u7b49\u5f0f\uff0c\u5e76\u7edf\u4e00\u4e86\u8bed\u8a00\u81ea\u73a9\u3001\u81ea\u6211\u4fee\u6b63\u548c\u5408\u6210\u6570\u636e\u5f15\u5bfc\u7b49\u67b6\u6784\u3002", "motivation": "\u5c06\u9759\u6001\u7684AAI\u80fd\u529b\u8bc4\u5206\u6269\u5c55\u5230\u52a8\u6001\u7cfb\u7edf\uff0c\u4e3a\u667a\u80fd\u4f53\u7684\u81ea\u6211\u6539\u8fdb\u8fc7\u7a0b\u5efa\u7acb\u6570\u5b66\u6846\u67b6\uff0c\u7edf\u4e00\u7406\u89e3\u5f53\u524d\u5404\u79cd\u81ea\u6211\u6539\u8fdb\u67b6\u6784\uff08\u5982STaR\u3001SPIN\u3001Reflexion\u7b49\uff09\u80cc\u540e\u7684\u5171\u540c\u539f\u7406\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5668-\u9a8c\u8bc1\u5668-\u66f4\u65b0\u5668\uff08GVU\uff09\u7b97\u5b50\uff0c\u8bc1\u660e\u8be5\u7b97\u5b50\u5728\u53c2\u6570\u6d41\u5f62\u4e0a\u751f\u6210\u5411\u91cf\u573a\uff0c\u5c06\u81ea\u6211\u6539\u8fdb\u7cfb\u6570\u03ba\u5b9a\u4e49\u4e3a\u80fd\u529b\u6cdb\u51fd\u6cbf\u8be5\u6d41\u7684Lie\u5bfc\u6570\uff0c\u63a8\u5bfc\u51fa\u4fdd\u8bc1\u81ea\u6211\u6539\u8fdb\u7a33\u5b9a\u6027\u7684\u65b9\u5dee\u4e0d\u7b49\u5f0f\u8fd9\u4e00\u8c31\u6761\u4ef6\u3002", "result": "\u5efa\u7acb\u4e86\u667a\u80fd\u4f53\u81ea\u6211\u6539\u8fdb\u7684\u52a8\u529b\u5b66\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u65b9\u5dee\u4e0d\u7b49\u5f0f\u662f\u81ea\u6211\u6539\u8fdb\u7a33\u5b9a\u6027\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5c55\u793a\u4e86STaR\u3001SPIN\u3001Reflexion\u3001GANs\u548cAlphaZero\u7b49\u67b6\u6784\u90fd\u662fGVU\u7b97\u5b50\u7684\u7279\u5b9a\u62d3\u6251\u5b9e\u73b0\uff0c\u901a\u8fc7\u8fc7\u6ee4\u3001\u5bf9\u6297\u5224\u522b\u6216\u5f62\u5f0f\u7cfb\u7edf\u57fa\u7840\u6ee1\u8db3\u65b9\u5dee\u4e0d\u7b49\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u667a\u80fd\u4f53\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u5404\u79cd\u770b\u4f3c\u4e0d\u540c\u7684\u67b6\u6784\u7edf\u4e00\u4e3aGVU\u7b97\u5b50\u7684\u5177\u4f53\u5b9e\u73b0\uff0c\u65b9\u5dee\u4e0d\u7b49\u5f0f\u4e3a\u8bbe\u8ba1\u548c\u5206\u6790\u81ea\u6211\u6539\u8fdb\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2512.02406", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02406", "abs": "https://arxiv.org/abs/2512.02406", "authors": ["Oshada Jayasinghe", "Farhana Choudhury", "Egemen Tanin", "Shanika Karunasekera"], "title": "Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning", "comment": null, "summary": "With increased travelling needs more than ever, traffic congestion has become a major concern in most urban areas. Allocating spaces for on-street parking, further hinders traffic flow, by limiting the effective road width available for driving. With the advancement of vehicle-to-infrastructure connectivity technologies, we explore how the impact of on-street parking on traffic congestion could be minimized, by dynamically configuring on-street parking spaces. Towards that end, we formulate dynamic on-street parking space configuration as an optimization problem, and we follow a data driven approach, considering the nature of our problem. Our proposed solution comprises a two-layer multi agent reinforcement learning based framework, which is inherently scalable to large road networks. The lane level agents are responsible for deciding the optimal parking space configuration for each lane, and we introduce a novel Deep Q-learning architecture which effectively utilizes long short term memory networks and graph attention networks to capture the spatio-temporal correlations evident in the given problem. The block level agents control the actions of the lane level agents and maintain a sufficient level of parking around the block. We conduct a set of comprehensive experiments using SUMO, on both synthetic data as well as real-world data from the city of Melbourne. Our experiments show that the proposed framework could reduce the average travel time loss of vehicles significantly, reaching upto 47%, with a negligible increase in the walking distance for parking.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u8def\u8fb9\u505c\u8f66\u4f4d\u914d\u7f6e\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u505c\u8f66\u7a7a\u95f4\u51cf\u5c11\u4ea4\u901a\u62e5\u5835\uff0c\u5b9e\u9a8c\u663e\u793a\u53ef\u964d\u4f4e\u8f66\u8f86\u5e73\u5747\u65c5\u884c\u65f6\u95f4\u635f\u5931\u8fbe47%", "motivation": "\u968f\u7740\u51fa\u884c\u9700\u6c42\u589e\u52a0\uff0c\u4ea4\u901a\u62e5\u5835\u6210\u4e3a\u57ce\u5e02\u4e3b\u8981\u95ee\u9898\u3002\u8def\u8fb9\u505c\u8f66\u5360\u7528\u9053\u8def\u7a7a\u95f4\uff0c\u8fdb\u4e00\u6b65\u963b\u788d\u4ea4\u901a\u6d41\u3002\u5229\u7528\u8f66\u8def\u534f\u540c\u6280\u672f\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u52a8\u6001\u914d\u7f6e\u8def\u8fb9\u505c\u8f66\u4f4d\u6765\u6700\u5c0f\u5316\u5176\u5bf9\u4ea4\u901a\u62e5\u5835\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff1a\u8f66\u9053\u7ea7\u667a\u80fd\u4f53\u8d1f\u8d23\u51b3\u5b9a\u6bcf\u6761\u8f66\u9053\u7684\u6700\u4f18\u505c\u8f66\u914d\u7f6e\uff0c\u4f7f\u7528\u7ed3\u5408LSTM\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684Deep Q-learning\u67b6\u6784\u6355\u6349\u65f6\u7a7a\u76f8\u5173\u6027\uff1b\u533a\u5757\u7ea7\u667a\u80fd\u4f53\u63a7\u5236\u8f66\u9053\u7ea7\u667a\u80fd\u4f53\u5e76\u7ef4\u6301\u533a\u5757\u5468\u56f4\u8db3\u591f\u7684\u505c\u8f66\u4f4d\u3002\u4f7f\u7528SUMO\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u58a8\u5c14\u672c\u5e02\u771f\u5b9e\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u663e\u8457\u964d\u4f4e\u8f66\u8f86\u5e73\u5747\u65c5\u884c\u65f6\u95f4\u635f\u5931\uff0c\u6700\u9ad8\u53ef\u8fbe47%\uff0c\u540c\u65f6\u505c\u8f66\u6b65\u884c\u8ddd\u79bb\u4ec5\u6709\u8f7b\u5fae\u589e\u52a0\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u52a8\u6001\u8def\u8fb9\u505c\u8f66\u914d\u7f6e\uff0c\u663e\u8457\u6539\u5584\u4ea4\u901a\u62e5\u5835\u95ee\u9898\uff0c\u4e14\u5177\u6709\u6269\u5c55\u5230\u5927\u578b\u9053\u8def\u7f51\u7edc\u7684\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02812", "abs": "https://arxiv.org/abs/2512.02812", "authors": ["Zijie Lin", "Qilin Cai", "Liang Shen", "Mingjun Xiao"], "title": "Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents", "comment": null, "summary": "Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\\% and 13\\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u63d0\u793a\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u548c\u7cbe\u70bc\u4ee3\u7406\u81ea\u52a8\u63d0\u5347\u8bba\u6587\u5230\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u63d0\u5347\u7ea615%\u548c13%", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u8bba\u6587\u590d\u73b0\u6846\u67b6\u7f3a\u4e4f\u5bf9\u6bcf\u4e2a\u751f\u6210\u6b65\u9aa4\u8f93\u51fa\u7684\u9a8c\u8bc1\u548c\u7cbe\u70bc\u673a\u5236\uff0c\u6216\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u8fdb\u884c\u81ea\u6211\u7cbe\u70bc\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51fa\u65e0\u63d0\u793a\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u542b\u9a8c\u8bc1\u4ee3\u7406\uff08\u68c0\u67e5\u8f93\u51fa\u662f\u5426\u6ee1\u8db3\u7cfb\u7edf\u63d0\u793a\u8981\u6c42\uff09\u548c\u7cbe\u70bc\u4ee3\u7406\uff08\u57fa\u4e8e\u8bc6\u522b\u95ee\u9898\u4fee\u8ba2\u8f93\u51fa\uff09\uff0c\u4ec5\u4f7f\u7528\u539f\u59cb\u7cfb\u7edf\u63d0\u793a\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u548c\u6539\u8fdb", "result": "\u5728PaperBench Code-Dev\u548cPaper2CodeBench\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u590d\u73b0\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u76f8\u6bd4\u65e0\u4ee3\u7406\u57fa\u7ebf\u5206\u522b\u83b7\u5f97\u7ea615%\u548c13%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u6bd4Self-Refine\u65b9\u6cd5\u66f4\u7a33\u5065\u4e00\u81f4", "conclusion": "\u63d0\u51fa\u7684\u65e0\u63d0\u793a\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u8bba\u6587\u5230\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u7cbe\u70bc\u63d0\u793a\uff0c\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027", "topic": "code agent"}}
{"id": "2512.02435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02435", "abs": "https://arxiv.org/abs/2512.02435", "authors": ["Zhongjian Qiao", "Rui Yang", "Jiafei Lyu", "Chenjia Bai", "Xiu Li", "Zhuoran Yang", "Siyang Gao", "Shuang Qiu"], "title": "Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering", "comment": null, "summary": "Cross-Domain Offline Reinforcement Learning aims to train an agent deployed in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between the source and target domain, simply merging the data from two datasets may incur inferior performance. Recent advances address this issue by selectively sharing source domain samples that exhibit dynamics alignment with the target domain. However, these approaches focus solely on dynamics alignment and overlook \\textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain. In this paper, we first demonstrate that both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework for cross-domain RL and establishing a concrete sub-optimality gap of a policy trained on the source domain and evaluated on the target domain. Motivated by the theoretical insights, we propose to selectively share those source domain samples with both high dynamics and value alignment and present our \\textbf{\\underline{D}}ynamics- and \\textbf{\\underline{V}}alue-aligned \\textbf{\\underline{D}}ata \\textbf{\\underline{F}}iltering (DVDF) method. We design a range of dynamics shift settings, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, as well as in challenging extremely low-data settings where the target domain dataset contains only 5,000 transitions. Extensive experiments demonstrate that DVDF consistently outperforms prior strong baselines and delivers exceptional performance across multiple tasks and datasets.", "AI": {"tldr": "\u63d0\u51faDVDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u52a8\u6001\u5bf9\u9f50\u548c\u4ef7\u503c\u5bf9\u9f50\uff0c\u4ece\u6e90\u57df\u6570\u636e\u4e2d\u9009\u62e9\u6027\u5730\u5171\u4eab\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u4ee5\u63d0\u5347\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ec5\u5173\u6ce8\u52a8\u6001\u5bf9\u9f50\uff0c\u5ffd\u7565\u4e86\u4ef7\u503c\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u52a8\u6001\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\uff0c\u7b80\u5355\u5408\u5e76\u6570\u636e\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u52a8\u6001\u548c\u4ef7\u503c\u5bf9\u9f50\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u7684\u6e90\u57df\u6837\u672c\u3002", "method": "\u63d0\u51faDVDF\uff08\u52a8\u6001\u548c\u4ef7\u503c\u5bf9\u9f50\u6570\u636e\u8fc7\u6ee4\uff09\u65b9\u6cd5\uff0c\u4ece\u7406\u8bba\u5206\u6790\u51fa\u53d1\uff0c\u5efa\u7acb\u7b56\u7565\u5728\u76ee\u6807\u57df\u8bc4\u4f30\u65f6\u7684\u6b21\u4f18\u6027\u5dee\u8ddd\uff0c\u7136\u540e\u8bbe\u8ba1\u540c\u65f6\u8003\u8651\u52a8\u6001\u5bf9\u9f50\u548c\u4ef7\u503c\u5bf9\u9f50\u7684\u6570\u636e\u9009\u62e9\u673a\u5236\uff0c\u5728\u591a\u79cd\u52a8\u6001\u504f\u79fb\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u5305\u62ec\u8fd0\u52a8\u5b66\u548c\u5f62\u6001\u5b66\u504f\u79fb\u7684\u5404\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\uff0cDVDF\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u76ee\u6807\u57df\u4ec5\u542b5000\u4e2a\u8f6c\u6362\u7684\u6781\u4f4e\u6570\u636e\u8bbe\u7f6e\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u52a8\u6001\u5bf9\u9f50\u548c\u4ef7\u503c\u5bf9\u9f50\u5bf9\u4e8e\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u90fd\u81f3\u5173\u91cd\u8981\uff0cDVDF\u65b9\u6cd5\u901a\u8fc7\u540c\u65f6\u8003\u8651\u8fd9\u4e24\u4e2a\u56e0\u7d20\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b56\u7565\u5728\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02445", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02445", "abs": "https://arxiv.org/abs/2512.02445", "authors": ["Tsimur Hadeliya", "Mohammad Ali Jauhar", "Nidhi Sakpal", "Diogo Cruz"], "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents", "comment": "12 pages, 11 figures. Accepted at AAAI 2026 TrustAgent Workshop", "summary": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u667a\u80fd\u4f53\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5b58\u5728\u6027\u80fd\u548c\u5b89\u5168\u95ee\u9898\uff1a\u5f53\u4e0a\u4e0b\u6587\u8fbe\u5230100K-200K tokens\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u7684\u884c\u4e3a\u4e5f\u53d8\u5f97\u4e0d\u7a33\u5b9a\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u957f\u4e0a\u4e0b\u6587\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740LLM\u652f\u6301\u66f4\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u5de5\u5177\u8c03\u7528\u80fd\u529b\uff0c\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u957f\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u8bc4\u4f30\uff0c\u800c\u667a\u80fd\u4f53\u8bbe\u7f6e\u7684\u80fd\u529b\u548c\u5b89\u5168\u6027\u65b9\u9762\u76f8\u5bf9\u672a\u88ab\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u957f\u5ea6\u3001\u7c7b\u578b\u548c\u4f4d\u7f6e\u7684\u4e0a\u4e0b\u6587\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u6027\u80fd\u548c\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u7684\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u53d1\u73b0LLM\u667a\u80fd\u4f53\u5bf9\u4e0a\u4e0b\u6587\u654f\u611f\uff0c\u5728100K tokens\u65f6\u6027\u80fd\u4e0b\u964d\u8d85\u8fc750%\u3002\u62d2\u7edd\u7387\u53d8\u5316\u4e0d\u53ef\u9884\u6d4b\uff1aGPT-4.1-nano\u4ece~5%\u589e\u52a0\u5230~40%\uff0c\u800cGrok 4 Fast\u4ece~80%\u4e0b\u964d\u5230~10%\uff08200K tokens\u65f6\uff09\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u667a\u80fd\u4f53\u5b58\u5728\u6f5c\u5728\u5b89\u5168\u95ee\u9898\uff0c\u5f53\u524d\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u957f\u591a\u6b65\u4efb\u52a1\u5b89\u5168\u6027\u7684\u6307\u6807\u548c\u8303\u5f0f\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\uff0c\u667a\u80fd\u4f53\u4e0e\u5355\u7eafLLM\u8bc4\u4f30\u5728\u80fd\u529b\u548c\u5b89\u5168\u6027\u80fd\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2512.02914", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02914", "abs": "https://arxiv.org/abs/2512.02914", "authors": ["Zhonghao He", "Tianyi Qiu", "Hirokazu Shirado", "Maarten Sap"], "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning", "comment": "NeurIPS 2025", "summary": "Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9785\u5c5e\u6027\u7684\u65e0\u76d1\u7763\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u56fa\u5316\u73b0\u8c61\uff0c\u53d1\u73b0\u8fed\u4ee3\u63a8\u7406\u53ef\u80fd\u5bfc\u81f4\u786e\u8ba4\u504f\u5dee\u800c\u975e\u771f\u76f8\u5bfb\u6c42\u884c\u4e3a\u3002", "motivation": "\u5c3d\u7ba1LLM\u7684\u63a8\u7406\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u8fed\u4ee3\u63a8\u7406\u53ef\u80fd\u5f15\u53d1\u4fe1\u5ff5\u56fa\u5316\u548c\u786e\u8ba4\u504f\u5dee\uff0c\u800c\u975e\u589e\u5f3a\u771f\u76f8\u5bfb\u6c42\u884c\u4e3a\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLM\u63a8\u7406\u4e2d\u7684\u4fe1\u5ff5\u56fa\u5316\u73b0\u8c61\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u7edf\u8ba1\u4e2d\u7684\u9785\u5c5e\u6027\uff0c\u63d0\u51fa\u65e0\u76d1\u7763\u7684\u56de\u5f52\u57fa\u9785\u5206\u6570\u6765\u8861\u91cf\u8fdd\u53cd\u8be5\u5c5e\u6027\u7684\u7a0b\u5ea6\u3002\u5728\u4e8b\u4ef6\u9884\u6d4b\u3001\u4ef7\u503c\u8d1f\u8f7d\u95ee\u9898\u548c\u5b66\u672f\u8bba\u6587\u8bc4\u5ba1\u7b49\u5f00\u653e\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4fe1\u5ff5\u56fa\u5316\u73b0\u8c61\u5728\u6a21\u578b\u548c\u8bbe\u7f6e\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5f53\u524d\u4fe1\u5ff5\u80fd\u6b63\u5411\u9884\u6d4b\u672a\u6765\u4fe1\u5ff5\u66f4\u65b0\u3002\u8bc6\u522b\u4e86\u66f4\u6613\u51fa\u73b0\u4fe1\u5ff5\u56fa\u5316\u7684\u6a21\u578b\u3001\u63a8\u7406\u6280\u672f\u548c\u9886\u57df\u3002\u9785\u5206\u6570\u80fd\u9884\u6d4b\u6709\u6807\u7b7e\u9886\u57df\u7684\u771f\u5b9e\u51c6\u786e\u6027\u3002", "conclusion": "\u9785\u5206\u6570\u4f5c\u4e3a\u65e0\u76d1\u7763\u6307\u6807\uff0c\u5373\u4f7f\u5728\u65e0\u6cd5\u83b7\u53d6\u771f\u5b9e\u6807\u7b7e\u7684\u9886\u57df\u4e5f\u80fd\u6709\u6548\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u7684\u771f\u76f8\u5bfb\u6c42\u80fd\u529b\uff0c\u63ed\u793a\u4e86LLM\u63a8\u7406\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4fe1\u5ff5\u56fa\u5316\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2512.02486", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02486", "abs": "https://arxiv.org/abs/2512.02486", "authors": ["Zhongjian Qiao", "Rui Yang", "Jiafei Lyu", "Xiu Li", "Zhongxiang Dai", "Zhuoran Yang", "Siyang Gao", "Shuang Qiu"], "title": "Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts", "comment": null, "summary": "Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \\textbf{D}ual-\\textbf{RO}bust \\textbf{C}ross-domain \\textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDROCO\u7b97\u6cd5\uff0c\u89e3\u51b3\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u7684\u53cc\u91cd\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7RCB\u7b97\u5b50\u548c\u52a8\u6001\u503c\u60e9\u7f5a\u7b49\u6280\u672f\u589e\u5f3a\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u65f6\u9c81\u68d2\u6027\uff08\u5904\u7406\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u52a8\u6001\u53d8\u5316\uff09\uff0c\u4f46\u5ffd\u89c6\u4e86\u5b9e\u9645\u90e8\u7f72\u65f6\u6d4b\u8bd5\u65f6\u5bf9\u52a8\u6001\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002\u5f53\u76ee\u6807\u57df\u6570\u636e\u6709\u9650\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u6270\u52a8\u4e0b\u8868\u73b0\u8106\u5f31\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u8de8\u57dfBellman\uff08RCB\uff09\u7b97\u5b50\uff0c\u589e\u5f3a\u6d4b\u8bd5\u65f6\u5bf9\u52a8\u6001\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5206\u5e03\u5916\u52a8\u6001\u8f6c\u79fb\u7684\u4fdd\u5b88\u6027\u3002\u5f15\u5165\u52a8\u6001\u503c\u60e9\u7f5a\u548cHuber\u635f\u5931\u6280\u672f\uff0c\u5f62\u6210DROCO\u7b97\u6cd5\u3002", "result": "\u5728\u5404\u79cd\u52a8\u6001\u53d8\u5316\u573a\u666f\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDROCO\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u52a8\u6001\u6270\u52a8\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DROCO\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8de8\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u65f6\u548c\u6d4b\u8bd5\u65f6\u7684\u53cc\u91cd\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7RCB\u7b97\u5b50\u548c\u8f85\u52a9\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u5728\u52a8\u6001\u6270\u52a8\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02874", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02874", "abs": "https://arxiv.org/abs/2512.02874", "authors": ["Haonan Wang", "Chao Du", "Kenji Kawaguchi", "Tianyu Pang"], "title": "Think in Parallel, Answer as One: Logit Averaging for Open-Ended Reasoning", "comment": null, "summary": "Majority voting has proven effective for close-ended question answering by aggregating parallel reasoning traces. However, it is not directly applicable to open-ended reasoning, such as code generation and web-based deep research, where a \"majority\" over complete solutions is ill-defined. We introduce ThinkMerge, a training-free, plug-and-play decoding strategy that runs K parallel reasoning traces and averages their next-token logits at synchronization points to produce a single coherent output. ThinkMerge integrates seamlessly with vLLM/SGLang and remains compatible with standard decoding techniques such as Top-p/Top-k. Empirically, it matches or surpasses majority voting on AIME and GPQA, while delivering consistent gains on open-ended coding tasks: on LiveCodeBench (hard), pass@1 improves by +8.28% for DeepCoder-14B-Preview and +7.58% for Qwen3-8B. Beyond code, we further show that ThinkMerge improves web-based deep-research agents (e.g., WebSailor-7B/32B) across GAIA, BrowseComp-en/zh, and XbenchDeepSearch. These results demonstrate that parallel test-time scaling can benefit open-ended reasoning without relying on voting over complete outputs.", "AI": {"tldr": "ThinkMerge\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u63a8\u7406\u8f68\u8ff9\u5e76\u5728\u540c\u6b65\u70b9\u5e73\u5747\u5b83\u4eec\u7684\u4e0b\u4e00\u4e2atoken\u5bf9\u6570\u6982\u7387\uff0c\u4e3a\u5f00\u653e\u5f0f\u63a8\u7406\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\u548c\u7f51\u7edc\u6df1\u5ea6\u7814\u7a76\uff09\u751f\u6210\u5355\u4e00\u8fde\u8d2f\u8f93\u51fa\u3002", "motivation": "\u591a\u6570\u6295\u7968\u5728\u5c01\u95ed\u5f0f\u95ee\u7b54\u4e2d\u6709\u6548\uff0c\u4f46\u4e0d\u9002\u7528\u4e8e\u5f00\u653e\u5f0f\u63a8\u7406\u4efb\u52a1\uff0c\u56e0\u4e3a\u5728\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u4e0a\u5b9a\u4e49\"\u591a\u6570\"\u662f\u56f0\u96be\u7684\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u5e76\u884c\u63a8\u7406\u4e2d\u83b7\u76ca\u800c\u4e0d\u4f9d\u8d56\u5b8c\u6574\u8f93\u51fa\u6295\u7968\u7684\u65b9\u6cd5\u3002", "method": "ThinkMerge\u8fd0\u884cK\u4e2a\u5e76\u884c\u63a8\u7406\u8f68\u8ff9\uff0c\u5728\u540c\u6b65\u70b9\u5e73\u5747\u5b83\u4eec\u7684\u4e0b\u4e00\u4e2atoken\u5bf9\u6570\u6982\u7387\uff0c\u751f\u6210\u5355\u4e00\u8fde\u8d2f\u8f93\u51fa\u3002\u4e0evLLM/SGLang\u65e0\u7f1d\u96c6\u6210\uff0c\u517c\u5bb9Top-p/Top-k\u7b49\u6807\u51c6\u89e3\u7801\u6280\u672f\u3002", "result": "\u5728AIME\u548cGPQA\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u591a\u6570\u6295\u7968\uff1b\u5728LiveCodeBench\uff08hard\uff09\u4e0a\uff0cDeepCoder-14B-Preview\u7684pass@1\u63d0\u5347+8.28%\uff0cQwen3-8B\u63d0\u5347+7.58%\uff1b\u5728\u7f51\u7edc\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08WebSailor-7B/32B\uff09\u4e0a\uff0c\u5728GAIA\u3001BrowseComp-en/zh\u548cXbenchDeepSearch\u4e0a\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "ThinkMerge\u8bc1\u660e\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u53ef\u4ee5\u6709\u76ca\u4e8e\u5f00\u653e\u5f0f\u63a8\u7406\uff0c\u65e0\u9700\u4f9d\u8d56\u5b8c\u6574\u8f93\u51fa\u7684\u6295\u7968\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u548c\u7f51\u7edc\u6df1\u5ea6\u7814\u7a76\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u7801\u7b56\u7565\u3002", "topic": "code agent"}}
{"id": "2512.02543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02543", "abs": "https://arxiv.org/abs/2512.02543", "authors": ["Vishnu Sarukkai", "Asanshay Gupta", "James Hong", "Micha\u00ebl Gharbi", "Kayvon Fatahalian"], "title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs", "comment": "16 pages, 4 figures", "summary": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.", "AI": {"tldr": "\u63d0\u51fain-context distillation\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u6559\u5e08\u6a21\u578b\u6f14\u793a\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u8ba9\u4f4e\u6210\u672c\u5b66\u751f\u6a21\u578b\u6a21\u4eff\u6559\u5e08\u884c\u4e3a\uff0c\u7ed3\u5408self-consistency cascades\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4eLLM\u4ee3\u7406\u63a8\u7406\u6210\u672c\u3002", "motivation": "LLM\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u6267\u884c\u65f6\u9762\u4e34\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\uff0c\u800c\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u957f\u8bad\u7ec3\u5468\u671f\u548c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u53c8\u9700\u8981\u5927\u91cf\u8bd5\u9519\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u964d\u4f4e\u6210\u672c\u53c8\u4e0d\u589e\u52a0\u5f00\u53d1\u6469\u64e6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fain-context distillation\uff0c\u5c06\u77e5\u8bc6\u84b8\u998f\u601d\u60f3\u5e94\u7528\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1a\u5728\u6bcf\u4e2a\u4ee3\u7406\u6b65\u9aa4\u68c0\u7d22\u76f8\u5173\u6559\u5e08\u6f14\u793a\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u8ba9\u5b66\u751f\u6a21\u578b\u5373\u65f6\u6a21\u4eff\u6559\u5e08\u884c\u4e3a\u3002\u7ed3\u5408self-consistency cascades\u7b56\u7565\u6765\u5224\u65ad\u4f55\u65f6\u4fe1\u4efb\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5728ALFWorld\u57fa\u51c6\u4e0a\u8fbe\u5230\u6559\u5e08\u7ea7\u51c6\u786e\u7387\u7684\u540c\u65f6\u6210\u672c\u964d\u4f4e2.5\u500d\uff08\u4ece$0.059\u964d\u81f3$0.024\u6bcf\u8f6e\uff09\uff0c\u6f14\u793a\u6210\u672c\u5728843\u8f6e\u540e\u644a\u9500\uff0c\u5728100\u4e07\u8f6e\u90e8\u7f72\u89c4\u6a21\u4e0b\u8282\u7701\u8d85\u8fc7$34,900\u3002\u5728AppWorld\u57fa\u51c6\u4e0a\u5b9e\u73b02\u500d\u6210\u672c\u964d\u4f4e\uff08\u540c\u7b49\u51c6\u786e\u7387\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u540c\u65f6\u4fdd\u6301\u51bb\u7ed3\u6a21\u578b\u7684\u5feb\u901f\u5b9e\u9a8c\u5468\u671f\uff0c\u4f7f\u5148\u8fdb\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u7ecf\u6d4e\u4e0a\u5bf9\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u53ef\u884c\u3002", "topic": "agent analysis"}}
{"id": "2512.02551", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.02551", "abs": "https://arxiv.org/abs/2512.02551", "authors": ["Songqiao Su", "Xiaofei Sun", "Xiaoya Li", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning", "comment": null, "summary": "In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\\it cuBLAS}, {\\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\\% over {\\it torch.matmul} on average; +19.2\\% over {\\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\\% over {\\it cuBLASLt-heuristic}, which queries {\\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\\% over the most competitive {\\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\\%, +26.0\\%, +22.4\\%, and +15.9\\% for {\\it torch.matmul}, {\\it cuBLAS}, {\\it cuBLASLt-heuristic}, and {\\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2", "AI": {"tldr": "CUDA-L2\u7ed3\u5408LLM\u548c\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u4f18\u5316HGEMM CUDA\u5185\u6838\uff0c\u57281000\u79cd\u914d\u7f6e\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u77e9\u9635\u4e58\u6cd5\u57fa\u51c6\uff0c\u5305\u62ectorch.matmul\u3001cuBLAS\u548ccuBLASLt\uff0c\u79bb\u7ebf\u6a21\u5f0f\u4e0b\u5e73\u5747\u63d0\u534722.0%\uff0c\u670d\u52a1\u5668\u6a21\u5f0f\u4e0b\u63d0\u5347\u8fbe28.7%\u3002", "motivation": "\u73b0\u6709\u9ad8\u5ea6\u4f18\u5316\u7684HGEMM CUDA\u5185\u6838\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4f20\u7edf\u624b\u52a8\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u63a2\u7d22\u5927\u89c4\u6a21\u914d\u7f6e\u7a7a\u95f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5CUDA\u6267\u884c\u901f\u5ea6\u4f5c\u4e3aRL\u5956\u52b1\uff0c\u81ea\u52a8\u4f18\u5316\u534a\u7cbe\u5ea6\u901a\u7528\u77e9\u9635\u4e58\u6cd5\u5185\u6838\uff0c\u57281000\u79cd\u914d\u7f6e\u4e0a\u8fdb\u884c\u7cfb\u7edf\u63a2\u7d22\u3002", "result": "\u5728\u79bb\u7ebf\u6a21\u5f0f\u4e0b\uff1a\u6bd4torch.matmul\u5e73\u5747\u63d0\u534722.0%\uff1b\u6bd4cuBLAS\u63d0\u534719.2%\uff1b\u6bd4cuBLASLt-heuristic\u63d0\u534716.8%\uff1b\u6bd4cuBLASLt-AutoTuning\u63d0\u534711.4%\u3002\u5728\u670d\u52a1\u5668\u6a21\u5f0f\u4e0b\uff1a\u63d0\u5347\u8fdb\u4e00\u6b65\u589e\u52a0\u523028.7%\u300126.0%\u300122.4%\u548c15.9%\u3002", "conclusion": "\u5373\u4f7f\u662f\u6700\u6027\u80fd\u5173\u952e\u3001\u9ad8\u5ea6\u4f18\u5316\u7684\u5185\u6838\u5982HGEMM\uff0c\u4e5f\u80fd\u901a\u8fc7LLM\u5f15\u5bfc\u7684RL\u81ea\u52a8\u5316\u5f97\u5230\u6539\u8fdb\uff0c\u7cfb\u7edf\u63a2\u7d22\u4eba\u7c7b\u96be\u4ee5\u5904\u7406\u7684\u5927\u89c4\u6a21\u914d\u7f6e\u7a7a\u95f4\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02581", "abs": "https://arxiv.org/abs/2512.02581", "authors": ["Chubin Zhang", "Zhenglin Wan", "Feng Chen", "Xingrui Yu", "Ivor Tsang", "Bo An"], "title": "GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies", "comment": "27 pages", "summary": "Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.", "AI": {"tldr": "GoRL\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u4e0e\u751f\u6210\uff0c\u4f7f\u7528\u53ef\u4f18\u5316\u7684\u6f5c\u5728\u7b56\u7565\u548c\u6761\u4ef6\u751f\u6210\u89e3\u7801\u5668\uff0c\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8868\u8fbe\u6027\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8d85\u8d8a\u9ad8\u65af\u7b56\u7565\u548c\u751f\u6210\u7b56\u7565\u57fa\u7ebf\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u7a33\u5b9a\u6027\u4e0e\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u5f20\u529b\uff1a\u9ad8\u65af\u7b56\u7565\u6613\u4e8e\u4f18\u5316\u4f46\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff08\u5355\u6a21\u6001\uff09\uff0c\u800c\u751f\u6210\u7b56\u7565\uff08\u5982\u6269\u6563\u3001\u6d41\u5339\u914d\uff09\u80fd\u5efa\u6a21\u591a\u6a21\u6001\u884c\u4e3a\u4f46\u5728\u5728\u7ebfRL\u4e2d\u4e0d\u7a33\u5b9a\uff08\u4f3c\u7136\u96be\u5904\u7406\u3001\u68af\u5ea6\u566a\u58f0\u5927\uff09\u3002", "method": "\u63d0\u51faGoRL\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u89e3\u8026\u4f18\u5316\u4e0e\u751f\u6210\uff1a\u4f18\u5316\u4e00\u4e2a\u53ef\u5904\u7406\u7684\u6f5c\u5728\u7b56\u7565\uff0c\u540c\u65f6\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u89e3\u7801\u5668\u5408\u6210\u52a8\u4f5c\u3002\u91c7\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u66f4\u65b0\u8ba1\u5212\uff0c\u4f7f\u6f5c\u5728\u7b56\u7565\u7a33\u5b9a\u5b66\u4e60\uff0c\u89e3\u7801\u5668\u9010\u6b65\u63d0\u5347\u8868\u8fbe\u6027\uff0c\u65e0\u9700\u53ef\u5904\u7406\u7684\u52a8\u4f5c\u4f3c\u7136\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cGoRL\u59cb\u7ec8\u4f18\u4e8e\u9ad8\u65af\u7b56\u7565\u548c\u6700\u8fd1\u7684\u751f\u6210\u7b56\u7565\u57fa\u7ebf\u3002\u5728HopperStand\u4efb\u52a1\u4e0a\u8fbe\u5230\u5f52\u4e00\u5316\u56de\u62a5870+\uff0c\u662f\u6700\u5f3a\u57fa\u7ebf\u76843\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u89e3\u8026\u4f18\u5316\u4e0e\u751f\u6210\u4e3a\u5b9e\u73b0\u65e2\u7a33\u5b9a\u53c8\u9ad8\u8868\u8fbe\u6027\u7684\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u89e3\u51b3\u4e86RL\u4e2d\u957f\u671f\u5b58\u5728\u7684\u7a33\u5b9a\u6027\u4e0e\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.02631", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02631", "abs": "https://arxiv.org/abs/2512.02631", "authors": ["Zhengcheng Wang", "Zichuan Lin", "Yijun Yang", "Haobo Fu", "Deheng Ye"], "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization", "comment": "12 pages,6 figures", "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.", "AI": {"tldr": "\u63d0\u51faSeeNav-Agent\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u89c6\u89c9\u63d0\u793a\u51cf\u5c11\u611f\u77e5\u5e7b\u89c9\uff0c\u5e76\u8bbe\u8ba1SRGPO\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u63d0\u5347VLN\u667a\u80fd\u4f53\u5bfc\u822a\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u667a\u80fd\u4f53\u5b58\u5728\u611f\u77e5\u9519\u8bef\u3001\u63a8\u7406\u9519\u8bef\u548c\u89c4\u5212\u9519\u8bef\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u5bfc\u822a\u6027\u80fd", "method": "1. \u5f15\u5165\u53cc\u89c6\u89d2\u89c6\u89c9\u63d0\u793a\u6280\u672f\u51cf\u5c11\u611f\u77e5\u5e7b\u89c9\uff1b2. \u8bbe\u8ba1SRGPO\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u53ef\u9a8c\u8bc1\u8fc7\u7a0b\u5956\u52b1\u548c\u968f\u673a\u5206\u7ec4\u6b65\u7ea7\u4f18\u52bf\u4f30\u8ba1\u6765\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7", "result": "GPT-4.1\u7ed3\u5408\u96f6-shot\u89c6\u89c9\u63d0\u793a\u8fbe\u523086.7%\u5bfc\u822a\u6210\u529f\u7387\uff0c\u6bd4\u5f53\u524d\u6700\u4f73LVLM\u63d0\u5347\u7ea620\u4e2a\u767e\u5206\u70b9\uff1bQwen2.5-VL-3B\u6a21\u578b\u901a\u8fc7SRGPO\u5fae\u8c03\u8fbe\u523072.3%\u6210\u529f\u7387\uff0c\u6bd4\u6700\u4f73\u73b0\u6709LVLM\u6a21\u578b\u63d0\u53475.6\u4e2a\u767e\u5206\u70b9", "conclusion": "SeeNav-Agent\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u548cSRGPO\u5f3a\u5316\u5fae\u8c03\u6709\u6548\u89e3\u51b3\u4e86VLN\u667a\u80fd\u4f53\u7684\u611f\u77e5\u548c\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd", "topic": "agent analysis"}}
{"id": "2512.02882", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02882", "abs": "https://arxiv.org/abs/2512.02882", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Tianyi Zeng", "Xiao-Yong Wei", "Qing Li"], "title": "OptPO: Optimal Rollout Allocation for Test-time Policy Optimization", "comment": "Work in Progress", "summary": "Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.", "AI": {"tldr": "OptPO\u662f\u4e00\u79cd\u7528\u4e8e\u6d4b\u8bd5\u65f6\u7b56\u7565\u4f18\u5316\u7684\u6700\u4f18\u6eda\u52a8\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5e8f\u5217\u6982\u7387\u6bd4\u6d4b\u8bd5\u52a8\u6001\u5206\u914d\u63a8\u7406\u9884\u7b97\uff0c\u51cf\u5c11\u8ba1\u7b97\u5197\u4f59\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u9884\u7b97\u7684\u591a\u6570\u6295\u7968\u6765\u4f30\u8ba1\u5956\u52b1\uff0c\u5bfc\u81f4\u5927\u91cf\u8ba1\u7b97\u5197\u4f59\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u9002\u5e94\u5206\u914d\u63a8\u7406\u9884\u7b97\u7684\u6846\u67b6\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faOptPO\u6846\u67b6\uff0c\u5c06\u6295\u7968\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u8d1d\u53f6\u65af\u5e8f\u5217\u6982\u7387\u6bd4\u6d4b\u8bd5\uff0c\u52a8\u6001\u505c\u6b62\u91c7\u6837\uff08\u5f53\u540e\u9a8c\u7f6e\u4fe1\u5ea6\u8d85\u8fc7\u9608\u503c\uff09\uff0c\u5e76\u5229\u7528\u4fdd\u7559\u7684\u6eda\u52a8\u8fdb\u884c\u5728\u7ebf\u7b56\u7565\u66f4\u65b0\uff0c\u53ef\u4e0ePPO\u6216GRPO\u7b49\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u591a\u6837\u5316\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptPO\u76f8\u6bd4\u56fa\u5b9a\u6837\u672c\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86\u6eda\u52a8\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "OptPO\u901a\u8fc7\u5c06\u7edf\u8ba1\u6700\u4f18\u505c\u6b62\u4e0e\u6d4b\u8bd5\u65f6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8ba1\u7b97\u9ad8\u6548\u7684\u8303\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.0f14daf6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finterconnected.org%2Fhome%2F2025%2F11%2F28%2Fplumbing%3Futm_source=tldrai/1/0100019ada4b4439-aff2f619-7222-4e57-af02-10316792855b-000000/UXOj34i22weCijRfmBo4hYuDAxyQkP_R7RmuQQ6j2F8=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finterconnected.org%2Fhome%2F2025%2F11%2F28%2Fplumbing%3Futm_source=tldrai/1/0100019ada4b4439-aff2f619-7222-4e57-af02-10316792855b-000000/UXOj34i22weCijRfmBo4hYuDAxyQkP_R7RmuQQ6j2F8=433", "authors": ["TLDR Newsletter"], "title": "Context plumbing", "comment": "Source: TLDR Newsletter, Date: 2025-12-01, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finterconnected.org%2Fhome%2F2025%2F11%2F28%2Fplumbing%3Futm_source=tldrai/1/0100019ada4b4439-aff2f619-7222-4e57-af02-10316792855b-000000/UXOj34i22weCijRfmBo4hYuDAxyQkP_R7RmuQQ6j2F8=433", "summary": "Context plumbing (6 minute read) Context is always changing, as it is dynamic. The context is not always where the AI runs. To make an agent run really well, you need to move the context to where it needs to be. Agents shouldn't have to look up context for every single query, because that's slow. Engineers need to build pipes that continuously flow potential context from where it is created to where it is going to be used.", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u4e0a\u4e0b\u6587\u7ba1\u9053\"\u6982\u5ff5\uff0c\u5f3a\u8c03\u9700\u8981\u6784\u5efa\u6301\u7eed\u6d41\u52a8\u7684\u4e0a\u4e0b\u6587\u4f20\u8f93\u673a\u5236\uff0c\u4f7fAI\u4ee3\u7406\u80fd\u9ad8\u6548\u8bbf\u95ee\u52a8\u6001\u53d8\u5316\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u907f\u514d\u6bcf\u6b21\u67e5\u8be2\u90fd\u91cd\u65b0\u67e5\u627e\u4e0a\u4e0b\u6587\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u8bbf\u95ee\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u6548\u7387\u95ee\u9898\uff1a\u4e0a\u4e0b\u6587\u662f\u52a8\u6001\u53d8\u5316\u7684\uff0c\u4e14\u4e0d\u4e00\u5b9a\u4f4d\u4e8eAI\u8fd0\u884c\u7684\u4f4d\u7f6e\u3002\u6bcf\u6b21\u67e5\u8be2\u90fd\u9700\u8981\u67e5\u627e\u4e0a\u4e0b\u6587\u4f1a\u5bfc\u81f4\u6027\u80fd\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\"\u4e0a\u4e0b\u6587\u7ba1\u9053\"\u65b9\u6cd5\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u6784\u5efa\u6301\u7eed\u6d41\u52a8\u7684\u7ba1\u9053\u7cfb\u7edf\uff0c\u5c06\u4e0a\u4e0b\u6587\u4ece\u5176\u521b\u5efa\u4f4d\u7f6e\u5b9e\u65f6\u4f20\u8f93\u5230\u5c06\u88ab\u4f7f\u7528\u7684\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7684\u9ad8\u6548\u6d41\u52a8\u548c\u8bbf\u95ee\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u4e0a\u4e0b\u6587\u7ba1\u9053\uff0cAI\u4ee3\u7406\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u8bbf\u95ee\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u51cf\u5c11\u67e5\u8be2\u5ef6\u8fdf\uff0c\u63d0\u5347\u4ee3\u7406\u7684\u6574\u4f53\u8fd0\u884c\u6027\u80fd\u3002", "conclusion": "\u4e3a\u4e86\u4f7fAI\u4ee3\u7406\u8fd0\u884c\u5f97\u66f4\u597d\uff0c\u9700\u8981\u5efa\u7acb\u6301\u7eed\u6d41\u52a8\u7684\u4e0a\u4e0b\u6587\u7ba1\u9053\u7cfb\u7edf\uff0c\u5c06\u4e0a\u4e0b\u6587\u4ece\u521b\u5efa\u4f4d\u7f6e\u5b9e\u65f6\u4f20\u8f93\u5230\u4f7f\u7528\u4f4d\u7f6e\uff0c\u8fd9\u662f\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.95384276", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fcharts-of-the-week-narrative-violation%3Futm_source=tldrproduct/1/0100019adebf2317-5a1ad54e-76c6-4194-975f-8cc78048902e-000000/vEQ160j_qS4wAscjjVk4l41tZ0TOxiCNGRjNP74osEQ=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fcharts-of-the-week-narrative-violation%3Futm_source=tldrproduct/1/0100019adebf2317-5a1ad54e-76c6-4194-975f-8cc78048902e-000000/vEQ160j_qS4wAscjjVk4l41tZ0TOxiCNGRjNP74osEQ=433", "authors": ["TLDR Newsletter"], "title": "Charts of the Week: Code Gen Strikes Back", "comment": "Source: TLDR Newsletter, Date: 2025-12-02, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.a16z.news%2Fp%2Fcharts-of-the-week-narrative-violation%3Futm_source=tldrproduct/1/0100019adebf2317-5a1ad54e-76c6-4194-975f-8cc78048902e-000000/vEQ160j_qS4wAscjjVk4l41tZ0TOxiCNGRjNP74osEQ=433", "summary": "Charts of the Week: Code Gen Strikes Back (7 minute read) Most AI narratives break down under closer data: code-gen tools are rebounding, job trends are mixed, early valuations don't predict much, and data centers are scaling at extraordinary speed. The real lesson is not to overreact to noisy data or tidy storylines.", "source": "tldr", "AI": {"tldr": "\u4ee3\u7801\u751f\u6210\u5de5\u5177\u6b63\u5728\u53cd\u5f39\uff0c\u4f46AI\u53d9\u4e8b\u5e38\u88ab\u6570\u636e\u566a\u97f3\u5e72\u6270\uff0c\u4e0d\u5e94\u8fc7\u5ea6\u89e3\u8bfb\u77ed\u671f\u8d8b\u52bf", "motivation": "\u5206\u6790AI\u9886\u57df\u7279\u522b\u662f\u4ee3\u7801\u751f\u6210\u5de5\u5177\u7684\u771f\u5b9e\u8d8b\u52bf\uff0c\u63ed\u793a\u5e38\u89c1\u53d9\u4e8b\u4e0e\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u907f\u514d\u5bf9\u566a\u97f3\u6570\u636e\u7684\u8fc7\u5ea6\u53cd\u5e94", "method": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u5468\u5ea6\u56fe\u8868\u5206\u6790\uff0c\u8003\u5bdf\u4ee3\u7801\u751f\u6210\u5de5\u5177\u4f7f\u7528\u60c5\u51b5\u3001\u5c31\u4e1a\u8d8b\u52bf\u3001\u65e9\u671f\u4f30\u503c\u9884\u6d4b\u80fd\u529b\u548c\u6570\u636e\u4e2d\u5fc3\u6269\u5c55\u901f\u5ea6", "result": "\u4ee3\u7801\u751f\u6210\u5de5\u5177\u6b63\u5728\u53cd\u5f39\uff0c\u5c31\u4e1a\u8d8b\u52bf\u597d\u574f\u53c2\u534a\uff0c\u65e9\u671f\u4f30\u503c\u9884\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u6570\u636e\u4e2d\u5fc3\u4ee5\u60ca\u4eba\u901f\u5ea6\u6269\u5c55", "conclusion": "\u771f\u6b63\u7684\u6559\u8bad\u662f\u4e0d\u8981\u5bf9\u566a\u97f3\u6570\u636e\u6216\u6574\u6d01\u7684\u53d9\u4e8b\u7ebf\u8fc7\u5ea6\u53cd\u5e94\uff0cAI\u9886\u57df\u7684\u8d8b\u52bf\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u6570\u636e\u5206\u6790", "topic": "code agent"}}
{"id": "tldr.2512.9c467130", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrdev/1/0100019adef74ab5-0c40d54a-229b-4d9c-8654-94c3ded94534-000000/QtxFVIORgYODlxcOeq1NH9PBJqODcKAXOPe55zUMOyk=433", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrdev/1/0100019adef74ab5-0c40d54a-229b-4d9c-8654-94c3ded94534-000000/QtxFVIORgYODlxcOeq1NH9PBJqODcKAXOPe55zUMOyk=433", "authors": ["TLDR Newsletter"], "title": "How to write a great agents.md: Lessons from over 2,500 repositories", "comment": "Source: TLDR Newsletter, Date: 2025-12-02, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot%2Fhow-to-write-a-great-agents-md-lessons-from-over-2500-repositories%2F%3Futm_source=tldrdev/1/0100019adef74ab5-0c40d54a-229b-4d9c-8654-94c3ded94534-000000/QtxFVIORgYODlxcOeq1NH9PBJqODcKAXOPe55zUMOyk=433", "summary": "How to write a great agents.md: Lessons from over 2,500 repositories (6 minute read) GitHub analyzed 2,500+ `agents.md` files and found the successful ones were specialists with clear jobs, not vague helpers. Good agents.md files give your agent specific commands to run, concrete code examples to follow, and explicit boundaries (like \u201cnever touch these files\u201d). Start simple with one focused task like writing tests or docs, then iterate based on what breaks.", "source": "tldr", "AI": {"tldr": "GitHub\u5206\u67902500+\u4e2aagents.md\u6587\u4ef6\u53d1\u73b0\uff0c\u6210\u529f\u7684agent\u662f\u4e13\u6ce8\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u5bb6\uff0c\u800c\u975e\u6a21\u7cca\u7684\u52a9\u624b\u3002\u597d\u7684agents.md\u6587\u4ef6\u5e94\u5305\u542b\u5177\u4f53\u547d\u4ee4\u3001\u4ee3\u7801\u793a\u4f8b\u548c\u660e\u786e\u8fb9\u754c\u3002", "motivation": "\u901a\u8fc7\u5206\u6790\u5927\u91cf\u5b9e\u9645\u9879\u76ee\u4e2d\u7684agents.md\u6587\u4ef6\uff0c\u4e86\u89e3\u5982\u4f55\u7f16\u5199\u6709\u6548\u7684agent\u914d\u7f6e\u6587\u4ef6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u521b\u5efa\u66f4\u5b9e\u7528\u3001\u66f4\u53ef\u9760\u7684AI\u52a9\u624b\u3002", "method": "GitHub\u5206\u6790\u4e862500\u591a\u4e2aagents.md\u6587\u4ef6\uff0c\u7814\u7a76\u6210\u529fagent\u914d\u7f6e\u7684\u5171\u540c\u7279\u5f81\uff0c\u603b\u7ed3\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u53d1\u73b0\u6210\u529f\u7684agents.md\u6587\u4ef6\u5177\u6709\u4ee5\u4e0b\u7279\u5f81\uff1a1\uff09agent\u662f\u4e13\u6ce8\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u5bb6\uff1b2\uff09\u63d0\u4f9b\u5177\u4f53\u53ef\u6267\u884c\u7684\u547d\u4ee4\uff1b3\uff09\u5305\u542b\u5b9e\u9645\u4ee3\u7801\u793a\u4f8b\uff1b4\uff09\u8bbe\u5b9a\u660e\u786e\u7684\u8fb9\u754c\u9650\u5236\uff1b5\uff09\u4ece\u7b80\u5355\u4efb\u52a1\u5f00\u59cb\u8fed\u4ee3\u4f18\u5316\u3002", "conclusion": "\u7f16\u5199\u6709\u6548\u7684agents.md\u6587\u4ef6\u5e94\u4e13\u6ce8\u4e8e\u8ba9agent\u6210\u4e3a\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u5bb6\uff0c\u63d0\u4f9b\u5177\u4f53\u6307\u4ee4\u548c\u8fb9\u754c\uff0c\u4ece\u7b80\u5355\u4efb\u52a1\u5f00\u59cb\u9010\u6b65\u8fed\u4ee3\uff0c\u800c\u4e0d\u662f\u521b\u5efa\u6a21\u7cca\u7684\u901a\u7528\u52a9\u624b\u3002", "topic": "agent analysis"}}
{"id": "wechat.2512.9a926f7f", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5MDMwMTIyNQ==&mid=2649433993&idx=1&sn=776104ae8f5a564807754d8c1212e229&chksm=89cbb3b72f5472a08d2c99a3809daaef34c8a49f3ad4702ba44d1aff18b1effb83514932d8f5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5MDMwMTIyNQ==&mid=2649433993&idx=1&sn=776104ae8f5a564807754d8c1212e229&chksm=89cbb3b72f5472a08d2c99a3809daaef34c8a49f3ad4702ba44d1aff18b1effb83514932d8f5#rd", "authors": ["CreateAMind"], "title": "\u91cf\u5b50<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1a\u8fd1\u671f\u8fdb\u5c55\u4e0e\u672a\u6765\u65b9\u5411", "comment": "Source: WeChat, Published: 2025-12-03 12:02:00", "summary": "\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\uff1a\u8fd1\u671f\u8fdb\u5c55\u4e0e\u672a\u6765\u65b9\u5411https\uff1a//arxiv.org/pdf/2510.14595\u6458\u8981\u968f\u7740\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6301\u7eed\u53d1\u5c55\uff0c\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u4e00\u4e2a\u5c24\u4e3a\u524d\u666f\u5e7f\u9614\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u524d\u6cbf\u65b9\u5411\u8131\u9896\u800c\u51fa\u3002", "AI": {"tldr": "\u91cf\u5b50\u5f3a\u5316\u5b66\u4e60\uff1a\u8fd1\u671f\u8fdb\u5c55\u4e0e\u672a\u6765\u65b9\u5411https\uff1a//arxiv.org/pdf/2510.14595\u6458\u8981\u968f\u7740\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6301\u7eed\u53d1\u5c55\uff0c\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u4e00\u4e2a\u5c24\u4e3a\u524d\u666f\u5e7f\u9614\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u524d\u6cbf\u65b9\u5411\u8131\u9896\u800c\u51fa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.83ba5eae", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI5NjY0NTQ4Mw==&mid=2247485522&idx=1&sn=bc758d770567eb701ed0de4ce1985345&chksm=ed4af9430d6899e430c7c9ea111faba64a2cad63772cb8f870dc423946daceb2fe4c2255fc16#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI5NjY0NTQ4Mw==&mid=2247485522&idx=1&sn=bc758d770567eb701ed0de4ce1985345&chksm=ed4af9430d6899e430c7c9ea111faba64a2cad63772cb8f870dc423946daceb2fe4c2255fc16#rd", "authors": ["AIPM\u4e4b\u6ce1\u6ce1\u7cd6"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff1aAI\u7248\u201c\u6328\u63cd\u4e2d\u6210\u957f\u201d\u7684\u52b1\u5fd7\u4f20\u5947 (REINFORCEMENT LEARNING)", "comment": "Source: WeChat, Published: 2025-12-03 00:59:15", "summary": "1. \u5f3a\u5316\u5b66\u4e60\u7684\u672c\u8d28 \u5f3a\u5316\u5b66\u4e60\u4e0d\u662f\u5b66\u201c\u7b54\u6848\u201d\uff0c\u800c\u662f\u5b66\u201c\u600e\u4e48\u505a\u201d \u5b66\u4e60\u201c\u884c\u4e3a\u7b56\u7565\u201d\u800c\u975e\u9884\u6d4b\u7b54\u6848 \u4e0e\u76d1\u7763\u5b66\u4e60\u4e0d\u540c\uff0cRL\u4e0d\u9700\u8981\u201c\u6b63\u786e\u7b54\u6848\u201d\uff0c\u8981\u9760\u73af\u5883\u63d0\u4f9b\u7684\u5956\u52b1\uff08Reward\uff09\u6765\u6307\u5bfc\u884c\u4e3a", "AI": {"tldr": "1. \u5f3a\u5316\u5b66\u4e60\u7684\u672c\u8d28 \u5f3a\u5316\u5b66\u4e60\u4e0d\u662f\u5b66\u201c\u7b54\u6848\u201d\uff0c\u800c\u662f\u5b66\u201c\u600e\u4e48\u505a\u201d \u5b66\u4e60\u201c\u884c\u4e3a\u7b56\u7565\u201d\u800c\u975e\u9884\u6d4b\u7b54\u6848 \u4e0e\u76d1\u7763\u5b66\u4e60\u4e0d\u540c\uff0cRL\u4e0d\u9700\u8981\u201c\u6b63\u786e\u7b54\u6848\u201d\uff0c\u8981\u9760\u73af\u5883\u63d0\u4f9b\u7684\u5956\u52b1\uff08Reward\uff09\u6765\u6307\u5bfc\u884c\u4e3a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.db386280", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzNjYzNDg0NQ==&mid=2247486485&idx=1&sn=f8c7406bccf61bbc5f2d15408e607e34&chksm=c3607dcbc98c732f340eb0ac2130a83c54f0f766bf274f38a2c68e95cc990a6860d6f28c3698#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzNjYzNDg0NQ==&mid=2247486485&idx=1&sn=f8c7406bccf61bbc5f2d15408e607e34&chksm=c3607dcbc98c732f340eb0ac2130a83c54f0f766bf274f38a2c68e95cc990a6860d6f28c3698#rd", "authors": ["JLE\u541b\u51cc\u5143\u5668\u4ef6"], "title": "\u9707\u60ca\uff01\u5b57\u8282Seed\u53d1\u5e03<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6a21\u578bGR-RL\uff0c\u65b0\u673a\u5668\u4eba\u9996\u6b21\u5b9e\u73b0\u5b66\u4e60\u7a7f\u978b\u5e26\uff01", "comment": "Source: WeChat, Published: 2025-12-03 00:30:00", "summary": "GR-RL\u7684\u7a81\u7834\uff0c\u6838\u5fc3\u5728\u4e8e\u5b83\u91c7\u7528\u4e86\u4e00\u5957\u4ece\u79bb\u7ebf\u6570\u636e\u7b5b\u9009\u5230\u5728\u7ebf\u771f\u673a\u5fae\u8c03\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6b64\u524d\u5355\u7eaf\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff08\u5982GR-3\uff09\u7684\u5c40\u9650\u6027\u30021. \u4e3a\u4f55\u201c\u7a7f\u978b\u5e26\u201d\u662f\u5de8\u5927\u6311\u6218\uff1f", "AI": {"tldr": "GR-RL\u7684\u7a81\u7834\uff0c\u6838\u5fc3\u5728\u4e8e\u5b83\u91c7\u7528\u4e86\u4e00\u5957\u4ece\u79bb\u7ebf\u6570\u636e\u7b5b\u9009\u5230\u5728\u7ebf\u771f\u673a\u5fae\u8c03\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6b64\u524d\u5355\u7eaf\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff08\u5982GR-3\uff09\u7684\u5c40\u9650\u6027\u30021. \u4e3a\u4f55\u201c\u7a7f\u978b\u5e26\u201d\u662f\u5de8\u5927\u6311\u6218\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.e0bebbfe", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5NTczMTA0NA==&mid=2650603757&idx=1&sn=e251250230cd276a1e98d3e15c6cb408&chksm=8967b7212d8008a83e899f56b25b2d8a75180fd62e452e019f9733cfd06aeb617a6d3e8024bf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5NTczMTA0NA==&mid=2650603757&idx=1&sn=e251250230cd276a1e98d3e15c6cb408&chksm=8967b7212d8008a83e899f56b25b2d8a75180fd62e452e019f9733cfd06aeb617a6d3e8024bf#rd", "authors": ["\u4e09\u51fd\u4ee3\u7801"], "title": "\u5e74\u521d\u753b\u997cAgent\u5143\u5e74\uff0c\u5e74\u672b\u53ea\u6709<em class=\"highlight\">Code</em> <em class=\"highlight\">Agent</em>\u80fd\u54bd\u5f97\u4e0b\u53bb", "comment": "Source: WeChat, Published: 2025-12-02 22:03:48", "summary": "Code Agent\uff08\u4ee3\u7801\u667a\u80fd\u4f53\uff09\uff0c\u8fd9\u5bb6\u4f19\u5c45\u7136\u5077\u5077\u53d1\u80b2\u6210\u201c\u516d\u8fb9\u5f62\u6218\u58eb\u201d\u4e86\uff01\u5b83\u5b8c\u5168\u4e0d\u6309\u901a\u7528Agent\u7684\u5267\u672c\u8d70\uff0c\u5728\u7a0b\u5e8f\u5458\u5708\u5b50\u91cc\u5df2\u7ecf\u6df7\u5f97\u5982\u9c7c\u5f97\u6c34\u3002\u4e3a\u4ec0\u4e48\u5b83\u80fd\u6210\u529f\uff1f", "AI": {"tldr": "Code Agent\uff08\u4ee3\u7801\u667a\u80fd\u4f53\uff09\uff0c\u8fd9\u5bb6\u4f19\u5c45\u7136\u5077\u5077\u53d1\u80b2\u6210\u201c\u516d\u8fb9\u5f62\u6218\u58eb\u201d\u4e86\uff01\u5b83\u5b8c\u5168\u4e0d\u6309\u901a\u7528Agent\u7684\u5267\u672c\u8d70\uff0c\u5728\u7a0b\u5e8f\u5458\u5708\u5b50\u91cc\u5df2\u7ecf\u6df7\u5f97\u5982\u9c7c\u5f97\u6c34\u3002\u4e3a\u4ec0\u4e48\u5b83\u80fd\u6210\u529f\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.bc52955a", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3NzMxNTI1MQ==&mid=2649782143&idx=1&sn=ca323459d0bfcb1d0c120239b2253fb6&chksm=860ec8381ae9026a3e6a795c90ee6eaae018be86e7d840832a1ab47ae22266ceb42fb05e48c9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3NzMxNTI1MQ==&mid=2649782143&idx=1&sn=ca323459d0bfcb1d0c120239b2253fb6&chksm=860ec8381ae9026a3e6a795c90ee6eaae018be86e7d840832a1ab47ae22266ceb42fb05e48c9#rd", "authors": ["\u949b\u5a92\u4f53AGI"], "title": "\u73b0\u573a\u76f4\u51fb\uff5c<em class=\"highlight\">Agentic</em> AI\u65f6\u4ee3\uff0c\u4e9a\u9a6c\u900a\u4e91\u79d1\u6280\u5728\u60f3\u4ec0\u4e48\uff1f", "comment": "Source: WeChat, Published: 2025-12-03 13:00:00", "summary": "Agentic AI\u65f6\u4ee3\uff0c\u5982\u4f55\u91cd\u65b0\u6784\u5efaAI\u57fa\u7840\u8bbe\u65bd\u3001\u66f4\u597d\u7684\u6a21\u578b\u9009\u62e9\u4ee5\u53ca\u5c06Agent\u90e8\u7f72\u5230\u5173\u952e\u4e1a\u52a1\u573a\u666f\u6240\u9700\u7684\u6574\u5957\u5de5\u5177\u94fe\u548c\u5e73\u53f0\u3002AI\u57fa\u7840\u8bbe\u65bd\uff1a\u65e2\u8981\u82f1\u4f1f\u8fbeGPU\uff0c\u4e5f\u8981 Trainium 4", "AI": {"tldr": "Agentic AI\u65f6\u4ee3\uff0c\u5982\u4f55\u91cd\u65b0\u6784\u5efaAI\u57fa\u7840\u8bbe\u65bd\u3001\u66f4\u597d\u7684\u6a21\u578b\u9009\u62e9\u4ee5\u53ca\u5c06Agent\u90e8\u7f72\u5230\u5173\u952e\u4e1a\u52a1\u573a\u666f\u6240\u9700\u7684\u6574\u5957\u5de5\u5177\u94fe\u548c\u5e73\u53f0\u3002AI\u57fa\u7840\u8bbe\u65bd\uff1a\u65e2\u8981\u82f1\u4f1f\u8fbeGPU\uff0c\u4e5f\u8981 Trainium 4", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.2a40a109", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NzIyMDY0OA==&mid=2655422811&idx=1&sn=f2b96393011cddf4581f7357058d546b&chksm=8af7369ea5b5f57ffce51e382df71858d40dc4cb96530cff0c0a7c69b2ae254ae78b4995b5e6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NzIyMDY0OA==&mid=2655422811&idx=1&sn=f2b96393011cddf4581f7357058d546b&chksm=8af7369ea5b5f57ffce51e382df71858d40dc4cb96530cff0c0a7c69b2ae254ae78b4995b5e6#rd", "authors": ["\u5fae\u8f6f\u5b66\u672f\u5408\u4f5c"], "title": "\u94f8\u661f\u8ba1\u5212 | <em class=\"highlight\">\u4ee3\u7406</em>\u5f0f\u667a\u80fd\uff08<em class=\"highlight\">Agentic</em> AI\uff09\uff1a\u91cd\u5851\u672a\u6765\u7684\u4eba\u2013<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u4e92\u52a8\u4e0e\u534f\u4f5c", "comment": "Source: WeChat, Published: 2025-12-03 10:44:46", "summary": "2025\u5e74\u7684\u65d7\u8230\u9879\u76eeInfoAgent\u5c31\u662f\u5178\u578b\u4ee3\u8868\u2014\u2014\u4e00\u6b3e\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u5728\u7f51\u7edc\u4e0a\u641c\u7d22\u5e76\u63a8\u7406\uff0c\u4ee5\u56de\u7b54\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\u7684\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u3002InfoAgent\u7531\u5fae\u8f6f\u8054\u5408\u4e1c\u5357\u5927\u5b66\u3001\u5e03\u6717\u5927\u5b66\u7b49\u591a\u652f\u9876\u5c16\u56e2\u961f\u5171\u540c\u5f00\u53d1\uff0c\u53c2\u4e0e\u8005\u8d85\u5341\u4eba\uff0c\u6db5\u76d6\u5fae\u8f6f\u5168\u804c", "AI": {"tldr": "2025\u5e74\u7684\u65d7\u8230\u9879\u76eeInfoAgent\u5c31\u662f\u5178\u578b\u4ee3\u8868\u2014\u2014\u4e00\u6b3e\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u5728\u7f51\u7edc\u4e0a\u641c\u7d22\u5e76\u63a8\u7406\uff0c\u4ee5\u56de\u7b54\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\u7684\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u3002InfoAgent\u7531\u5fae\u8f6f\u8054\u5408\u4e1c\u5357\u5927\u5b66\u3001\u5e03\u6717\u5927\u5b66\u7b49\u591a\u652f\u9876\u5c16\u56e2\u961f\u5171\u540c\u5f00\u53d1\uff0c\u53c2\u4e0e\u8005\u8d85\u5341\u4eba\uff0c\u6db5\u76d6\u5fae\u8f6f\u5168\u804c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.25447ea3", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNDg2OTMzMA==&mid=2247485629&idx=1&sn=087f85b845777d66913aabd2e95e1c8e&chksm=c04bba69e7ded7423c8abacb5afc8291ec0c330ef5157afa292a831373af7bb588b029d62619#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNDg2OTMzMA==&mid=2247485629&idx=1&sn=087f85b845777d66913aabd2e95e1c8e&chksm=c04bba69e7ded7423c8abacb5afc8291ec0c330ef5157afa292a831373af7bb588b029d62619#rd", "authors": ["MCP\u7814\u7a76\u9662"], "title": "AI\u8fdb\u5316\u7b80\u53f2\uff0c\u4ece\u56fe\u7075\u6d4b\u8bd5\u5230<em class=\"highlight\">Agentic</em> AI\u768470\u5e74\u6280\u672f\u8dc3\u8fc1", "comment": "Source: WeChat, Published: 2025-12-03 07:07:40", "summary": "PART 07 2025\uff0cAgentic AI\u5143\u5e74\u5982\u679c\u8bf42022\u5e74\u662f\u751f\u6210\u5f0fAI\u7684\u7206\u53d1\u5e74\uff0c\u90a3\u4e482025\u5e74\u6b63\u5728\u6210\u4e3aAgentic AI\u7684\u5143\u5e74\u3002Agentic AI\u7684\u6838\u5fc3\u7279\u70b9\u662f\u81ea\u4e3b\u6027\u2014\u2014\u5b83\u4e0d\u518d\u53ea\u662f\u88ab\u52a8\u5730\u56de\u7b54\u95ee\u9898\uff0c\u800c\u662f\u53ef\u4ee5\u4e3b\u52a8\u8bbe\u5b9a\u76ee\u6807\u3001\u89c4\u5212\u6b65\u9aa4\u3001\u8c03\u7528\u5de5\u5177\u3001\u6267\u884c\u4efb\u52a1\u3002", "AI": {"tldr": "PART 07 2025\uff0cAgentic AI\u5143\u5e74\u5982\u679c\u8bf42022\u5e74\u662f\u751f\u6210\u5f0fAI\u7684\u7206\u53d1\u5e74\uff0c\u90a3\u4e482025\u5e74\u6b63\u5728\u6210\u4e3aAgentic AI\u7684\u5143\u5e74\u3002Agentic AI\u7684\u6838\u5fc3\u7279\u70b9\u662f\u81ea\u4e3b\u6027\u2014\u2014\u5b83\u4e0d\u518d\u53ea\u662f\u88ab\u52a8\u5730\u56de\u7b54\u95ee\u9898\uff0c\u800c\u662f\u53ef\u4ee5\u4e3b\u52a8\u8bbe\u5b9a\u76ee\u6807\u3001\u89c4\u5212\u6b65\u9aa4\u3001\u8c03\u7528\u5de5\u5177\u3001\u6267\u884c\u4efb\u52a1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.27f3f31e", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyODk4NzczMg==&mid=2247485600&idx=2&sn=7e2c80ca4d6e00ceeab6dd768b197843&chksm=c3b38cd0aefe9df1af016734440e2012a09f0b99a6ca3e616393876d999496010a60acaf63e5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyODk4NzczMg==&mid=2247485600&idx=2&sn=7e2c80ca4d6e00ceeab6dd768b197843&chksm=c3b38cd0aefe9df1af016734440e2012a09f0b99a6ca3e616393876d999496010a60acaf63e5#rd", "authors": ["\u672a\u535c\u5148\u667a"], "title": "FDA\u90e8\u7f72\u201c<em class=\"highlight\">Agentic</em>\u6a21\u578b\u201d\uff0c\u63a8\u52a8\u76d1\u7ba1\u5de5\u4f5c\u6d41\u6570\u5b57\u5316\u9769\u65b0", "comment": "Source: WeChat, Published: 2025-12-03 01:54:13", "summary": "Agentic\u6a21\u578b\u7684\u6280\u672f\u5b9a\u4f4d\u4ece\u516c\u5f00\u5b9a\u4e49\u6765\u770b\uff0cagentic AI \u88ab\u63cf\u8ff0\u4e3a\u4e00\u7c7b\u4f7f\u7528\u201c\u667a\u80fd\u4f53\uff08agent\uff09\u201d\u6765\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u7cfb\u7edf\uff0c\u53ef\u5728\u65e2\u5b9a\u89c4\u5219\u4e0b\u8fdb\u884c\u89c4\u5212\u3001\u63a8\u7406\u5e76\u5b8c\u6210\u591a\u6b65\u52a8\u4f5c\uff0c\u800c\u975e\u4ec5\u5bf9\u5355\u6b21\u8f93\u5165\u8fdb\u884c\u7b80\u5355\u751f\u6210\u6216\u5206\u7c7b\u3002", "AI": {"tldr": "Agentic\u6a21\u578b\u7684\u6280\u672f\u5b9a\u4f4d\u4ece\u516c\u5f00\u5b9a\u4e49\u6765\u770b\uff0cagentic AI \u88ab\u63cf\u8ff0\u4e3a\u4e00\u7c7b\u4f7f\u7528\u201c\u667a\u80fd\u4f53\uff08agent\uff09\u201d\u6765\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u7cfb\u7edf\uff0c\u53ef\u5728\u65e2\u5b9a\u89c4\u5219\u4e0b\u8fdb\u884c\u89c4\u5212\u3001\u63a8\u7406\u5e76\u5b8c\u6210\u591a\u6b65\u52a8\u4f5c\uff0c\u800c\u975e\u4ec5\u5bf9\u5355\u6b21\u8f93\u5165\u8fdb\u884c\u7b80\u5355\u751f\u6210\u6216\u5206\u7c7b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.548f3177", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1OTAwNTc2Mg==&mid=2459523659&idx=1&sn=cbff189153ac5ca34ce05c519798510b&chksm=fc77a7f44d3235481cb227cfee36513e539d174c952b201e8bdc73db20b1451d312239a5fcc7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1OTAwNTc2Mg==&mid=2459523659&idx=1&sn=cbff189153ac5ca34ce05c519798510b&chksm=fc77a7f44d3235481cb227cfee36513e539d174c952b201e8bdc73db20b1451d312239a5fcc7#rd", "authors": ["\u53ef\u8350"], "title": "AWS\u7684<em class=\"highlight\">Agentic</em> AI\u9769\u547d", "comment": "Source: WeChat, Published: 2025-12-03 01:31:33", "summary": "#AWS #\u667a\u80fd\u4f53 #re\uff1aInventAWS\u5728\u5e74\u5ea6re\uff1aInvent\u5927\u4f1a\u4e0a\u53d1\u5e03\u4e00\u7cfb\u5217\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u521b\u65b0\u548c\u589e\u5f3a\u529f\u80fd\u3002\u805a\u7126Agentic AI\uff08\u667a\u80fd\u4f53AI\uff09\u6280\u672f\uff0cAWS\u81ea\u7814\u7684Nova 2\u7cfb\u5217\u5927\u6a21\u578b\u3001\u5347\u7ea7\u7684Trainium AI\u82af\u7247\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u7ba1\u7406AI\u667a\u80fd\u4f53\u7684Agent", "AI": {"tldr": "#AWS #\u667a\u80fd\u4f53 #re\uff1aInventAWS\u5728\u5e74\u5ea6re\uff1aInvent\u5927\u4f1a\u4e0a\u53d1\u5e03\u4e00\u7cfb\u5217\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u521b\u65b0\u548c\u589e\u5f3a\u529f\u80fd\u3002\u805a\u7126Agentic AI\uff08\u667a\u80fd\u4f53AI\uff09\u6280\u672f\uff0cAWS\u81ea\u7814\u7684Nova 2\u7cfb\u5217\u5927\u6a21\u578b\u3001\u5347\u7ea7\u7684Trainium AI\u82af\u7247\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u7ba1\u7406AI\u667a\u80fd\u4f53\u7684Agent", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.ea401a53", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505174&idx=1&sn=3268a9ac6b89b34a51a8c67795814439&chksm=9a171dae495221b62be7a1452f4f3448e3dcbb1b9fce1305f068826b1d3ddaeefd36a9cdacd7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&mid=2247505174&idx=1&sn=3268a9ac6b89b34a51a8c67795814439&chksm=9a171dae495221b62be7a1452f4f3448e3dcbb1b9fce1305f068826b1d3ddaeefd36a9cdacd7#rd", "authors": ["\u5173\u4e8eNLP\u90a3\u4e9b\u4f60\u4e0d\u77e5\u9053\u7684\u4e8b"], "title": "<em class=\"highlight\">Agentic</em> Al\u516d\u5927\u8bbe\u8ba1\u6a21\u5f0f\u6df1\u5ea6\u89e3\u6790:\u4eceReAct\u5230\u591a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u534f\u4f5c\u7684\u5168\u6808\u5f00\u53d1\u6307\u5357", "comment": "Source: WeChat, Published: 2025-12-02 16:00:00", "summary": "\u5b83\u7ed9\u667a\u80fd\u4f53\u7cfb\u7edf\u589e\u52a0\u4e86\u4e00\u4e2a\u201c\u6279\u5224\u6027\u601d\u7ef4\u201d\u7684\u56de\u8def\u3002\u5de5\u4f5c\u6d41\u7a0b\u5f88\u7b80\u5355\uff1a\u751f\u6210\uff1a\u4e3bLLM\u5148\u751f\u6210\u4e00\u4e2a\u7b54\u6848\u3002\u8bc4\u5224\uff1a\u53e6\u4e00\u4e2a\u201c\u8bc4\u5224\u8005\u201dLLM\uff08\u4e5f\u53ef\u4ee5\u662f\u540c\u4e00\u4e2a\u6a21\u578b\u7684\u4e0d\u540c\u63d0\u793a\uff09\u5bf9\u8fd9\u4e2a\u7b54\u6848\u8fdb\u884c\u6311\u523a", "AI": {"tldr": "\u5b83\u7ed9\u667a\u80fd\u4f53\u7cfb\u7edf\u589e\u52a0\u4e86\u4e00\u4e2a\u201c\u6279\u5224\u6027\u601d\u7ef4\u201d\u7684\u56de\u8def\u3002\u5de5\u4f5c\u6d41\u7a0b\u5f88\u7b80\u5355\uff1a\u751f\u6210\uff1a\u4e3bLLM\u5148\u751f\u6210\u4e00\u4e2a\u7b54\u6848\u3002\u8bc4\u5224\uff1a\u53e6\u4e00\u4e2a\u201c\u8bc4\u5224\u8005\u201dLLM\uff08\u4e5f\u53ef\u4ee5\u662f\u540c\u4e00\u4e2a\u6a21\u578b\u7684\u4e0d\u540c\u63d0\u793a\uff09\u5bf9\u8fd9\u4e2a\u7b54\u6848\u8fdb\u884c\u6311\u523a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.4842841f", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNTc5NjA4NQ==&mid=2247504008&idx=1&sn=6a6ef163fe1337d4e18a121603ebde1e&chksm=fb610a5bb2f74636509233dcddd85089d1187d4beb99b4afbe6190848261869255d0534a5378#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNTc5NjA4NQ==&mid=2247504008&idx=1&sn=6a6ef163fe1337d4e18a121603ebde1e&chksm=fb610a5bb2f74636509233dcddd85089d1187d4beb99b4afbe6190848261869255d0534a5378#rd", "authors": ["\u665a\u67abAI\u5b66\u4e60\u7b14\u8bb0"], "title": "\u3010AI\u79d1\u666e\u3011\u4e3a\u4ec0\u4e48\u4f60\u7528<em class=\"highlight\">\u5927\u6a21\u578b</em>\u8981\u6309Token\u4ed8\u8d39\uff1f", "comment": "Source: WeChat, Published: 2025-12-03 10:10:27", "summary": "2025\u5e74\u4e09\u5927\u9ad8\u6027\u4ef7\u6bd4AI\u6a21\u578b\u6a2a\u8bc4\u9762\u5bf9\u7433\u7405\u6ee1\u76ee\u7684AI\u6a21\u578b\uff0c\u5982\u4f55\u9009\u62e9\u6700\u9002\u5408\u81ea\u5df1\u7684\u90a3\u4e00\u6b3e\uff1f\u6211\u4eec\u4ece\u901a\u7528\u5bf9\u8bdd\u3001\u4e13\u4e1a\u7f16\u7a0b\u548c\u591a\u6a21\u6001\u5904\u7406\u4e09\u4e2a\u6838\u5fc3\u573a\u666f\uff0c\u7cbe\u9009\u51fa\u5f53\u524d\u6027\u4ef7\u6bd4\u6700\u9ad8\u7684\u9009\u62e9\uff1a", "AI": {"tldr": "2025\u5e74\u4e09\u5927\u9ad8\u6027\u4ef7\u6bd4AI\u6a21\u578b\u6a2a\u8bc4\u9762\u5bf9\u7433\u7405\u6ee1\u76ee\u7684AI\u6a21\u578b\uff0c\u5982\u4f55\u9009\u62e9\u6700\u9002\u5408\u81ea\u5df1\u7684\u90a3\u4e00\u6b3e\uff1f\u6211\u4eec\u4ece\u901a\u7528\u5bf9\u8bdd\u3001\u4e13\u4e1a\u7f16\u7a0b\u548c\u591a\u6a21\u6001\u5904\u7406\u4e09\u4e2a\u6838\u5fc3\u573a\u666f\uff0c\u7cbe\u9009\u51fa\u5f53\u524d\u6027\u4ef7\u6bd4\u6700\u9ad8\u7684\u9009\u62e9\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.002fc74a", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDAwODU0Ng==&mid=2649238077&idx=1&sn=2ab9a7d0df734a158105027d6272903d&chksm=f27662c7494f25d98af927e3990d352da30a347807fd65f246e4b11a128611f22867703b7536#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDAwODU0Ng==&mid=2649238077&idx=1&sn=2ab9a7d0df734a158105027d6272903d&chksm=f27662c7494f25d98af927e3990d352da30a347807fd65f246e4b11a128611f22867703b7536#rd", "authors": ["SIP\u79d1\u6280\u521b\u65b0"], "title": "\u65b0\u589e1\u6b3e\u3001\u5168\u5e02\u7b2c\u4e00\uff01\u56ed\u533a\u5df2\u67098\u5bb6<em class=\"highlight\">\u5927\u6a21\u578b</em>\u901a\u8fc7\u5907\u6848", "comment": "Source: WeChat, Published: 2025-12-03 09:05:29", "summary": "\u6c47\u667a\u5927\u6a21\u578b \u6c47\u667a\u5927\u6a21\u578b\u662f\u4e00\u6b3e\u7531\u4f01\u67e5\u67e5\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5f00\u53d1\u8bad\u7ec3\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u3002\u4e3b\u8981\u9002\u7528\u4e8e\u7528\u6237\u56e0\u4fe1\u606f\u54a8\u8be2\u3001\u5b66\u4e60\u7814\u7a76\u6216\u5176\u4ed6\u5408\u6cd5\u76ee\u7684\uff0c\u9700\u8981\u67e5\u8be2\u4e86\u89e3\u56fd\u5185\u6cd5\u5f8b\u76f8\u5173\u4fe1\u606f\u7684\u573a\u666f\u3002", "AI": {"tldr": "\u6c47\u667a\u5927\u6a21\u578b \u6c47\u667a\u5927\u6a21\u578b\u662f\u4e00\u6b3e\u7531\u4f01\u67e5\u67e5\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5f00\u53d1\u8bad\u7ec3\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u3002\u4e3b\u8981\u9002\u7528\u4e8e\u7528\u6237\u56e0\u4fe1\u606f\u54a8\u8be2\u3001\u5b66\u4e60\u7814\u7a76\u6216\u5176\u4ed6\u5408\u6cd5\u76ee\u7684\uff0c\u9700\u8981\u67e5\u8be2\u4e86\u89e3\u56fd\u5185\u6cd5\u5f8b\u76f8\u5173\u4fe1\u606f\u7684\u573a\u666f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.00bffcc6", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247597635&idx=2&sn=5f1ddfa046945ae69cfd1b2a1890ade6&chksm=e94b38276ed20c8551bd5547ad596c739ac84284f338476c30dada4a50c3861efedee8d42e8a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMDgwODcyNA==&mid=2247597635&idx=2&sn=5f1ddfa046945ae69cfd1b2a1890ade6&chksm=e94b38276ed20c8551bd5547ad596c739ac84284f338476c30dada4a50c3861efedee8d42e8a#rd", "authors": ["\u6728\u6728\u81ea\u7531"], "title": "AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u00b7\u767d\u76ae\u4e66 | AI \u667a\u80fd\u4f53\u624b\u518c-\u8c37\u6b4c", "comment": "Source: WeChat, Published: 2025-12-03 08:25:34", "summary": "\u25b2\u4e00\u8d77\u5b66\u4e60AI\u5927\u6a21\u578b\uff0cAI\u5927\u6a21\u578b\u5b66\u4e60\u8def\u5f84\u76f8\u5173\u8d44\u6599~\uff08\u7cbe\u5f69\u5927\u6a21\u578b\u89c2\u70b9\u3001\u5b66\u4e60\u8d44\u6599\u3001\u4e66\u7c4d\u5206\u4eab\u00b7\u00b7\u00b7\u7b49\u4f60\u4e00\u8d77\u6765\u4e58\u98ce\u7834\u6d6a~\uff09\u3002AI\u00b7\u5927\u6a21\u578b\u00b7\u9886\u5730\u62a5\u544a\uff1aAI \u667a\u80fd\u4f53\u624b\u518c-\u8c37\u6b4c", "AI": {"tldr": "\u25b2\u4e00\u8d77\u5b66\u4e60AI\u5927\u6a21\u578b\uff0cAI\u5927\u6a21\u578b\u5b66\u4e60\u8def\u5f84\u76f8\u5173\u8d44\u6599~\uff08\u7cbe\u5f69\u5927\u6a21\u578b\u89c2\u70b9\u3001\u5b66\u4e60\u8d44\u6599\u3001\u4e66\u7c4d\u5206\u4eab\u00b7\u00b7\u00b7\u7b49\u4f60\u4e00\u8d77\u6765\u4e58\u98ce\u7834\u6d6a~\uff09\u3002AI\u00b7\u5927\u6a21\u578b\u00b7\u9886\u5730\u62a5\u544a\uff1aAI \u667a\u80fd\u4f53\u624b\u518c-\u8c37\u6b4c", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.1bd7c6fc", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzMzk1OTQ5OQ==&mid=2247484234&idx=1&sn=d7373f2ddc23a46da15d667f01038436&chksm=e9dfd9bf0582b15114cca9d640a40bc8e90392fddddd5743af710c53b31446c5667c8c069150#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzMzk1OTQ5OQ==&mid=2247484234&idx=1&sn=d7373f2ddc23a46da15d667f01038436&chksm=e9dfd9bf0582b15114cca9d640a40bc8e90392fddddd5743af710c53b31446c5667c8c069150#rd", "authors": ["\u6e05\u98ce\u8d77"], "title": "\u63ed\u79d8 AI \u9b54\u6cd5\u5e08\uff1a\u4e3a\u4ec0\u4e48<em class=\"highlight\">\u5927\u6a21\u578b</em>\u80fd\u201c\u601d\u8003\u201d\uff1f\u2014\u2014 \u8ba4\u8bc6 Transformer \u67b6\u6784", "comment": "Source: WeChat, Published: 2025-12-03 04:00:21", "summary": "\u5927\u6a21\u578b\u662f\u5728\u4e92\u8054\u7f51\u4e0a\u4eba\u7c7b\u4ea7\u751f\u7684\u6240\u6709\u53ef\u8bbf\u95ee\u6587\u672c\uff08\u4e66\u7c4d\u3001\u7f51\u9875\u3001\u6587\u7ae0\u3001\u4ee3\u7801\u7b49\uff09\u7684PB\u7ea7\u6570\u636e\u4e0a\u8bad\u7ec3\u51fa\u6765\u7684\u3002\u5b83\u4eec\u901a\u8fc7\u5b66\u4e60\u8fd9\u4e9b\u6570\u636e\uff0c\u8bb0\u4f4f\u4e86\u4eba\u7c7b\u51e0\u4e4e\u6240\u6709\u7684\u77e5\u8bc6\u3001\u903b\u8f91\u548c\u8868\u8fbe\u65b9\u5f0f\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u662f\u5728\u4e92\u8054\u7f51\u4e0a\u4eba\u7c7b\u4ea7\u751f\u7684\u6240\u6709\u53ef\u8bbf\u95ee\u6587\u672c\uff08\u4e66\u7c4d\u3001\u7f51\u9875\u3001\u6587\u7ae0\u3001\u4ee3\u7801\u7b49\uff09\u7684PB\u7ea7\u6570\u636e\u4e0a\u8bad\u7ec3\u51fa\u6765\u7684\u3002\u5b83\u4eec\u901a\u8fc7\u5b66\u4e60\u8fd9\u4e9b\u6570\u636e\uff0c\u8bb0\u4f4f\u4e86\u4eba\u7c7b\u51e0\u4e4e\u6240\u6709\u7684\u77e5\u8bc6\u3001\u903b\u8f91\u548c\u8868\u8fbe\u65b9\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.70222a83", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5MzQ1NTczNQ==&mid=2247484636&idx=1&sn=bf05daddd69139de0639f99ba70c0e4f&chksm=c19f942b7fe4de97cdf53cb8e965f5f2bcfd760637dfa82f056d648f9d139608a9bf15dec41a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5MzQ1NTczNQ==&mid=2247484636&idx=1&sn=bf05daddd69139de0639f99ba70c0e4f&chksm=c19f942b7fe4de97cdf53cb8e965f5f2bcfd760637dfa82f056d648f9d139608a9bf15dec41a#rd", "authors": ["\u97f6\u53f0\u6f84\u6ce2"], "title": "DeepSeek V3.2 \u6765\u4e86\u2014\u2014AI\u5f00\u6e90<em class=\"highlight\">\u5927\u6a21\u578b</em>\u53c8\u4e00\u91cd\u78c5\u8fed\u4ee3", "comment": "Source: WeChat, Published: 2025-12-03 03:13:40", "summary": "\u6574\u4e2a 11 \u6708\u7684\u5927\u6a21\u578b\u5708\u7b80\u76f4\u662f \u201c\u795e\u4ed9\u6253\u67b6\u201d \u6a21\u5f0f \u2014\u2014 \u8c37\u6b4c\u548cClaude\u5148\u540e\u5b98\u5ba3\u4e86\u81ea\u5bb6\u7684\u5347\u7ea7\u7248\u5927\u6a21\u578b\uff0c\u7d27\u63a5\u7740\u5404\u79cd\u4e13\u4e1a\u8bc4\u6d4b\u3001\u4e94\u82b1\u516b\u95e8\u7684\u5e94\u7528\u6307\u5357\u5c31\u94fa\u5929\u76d6\u5730\u6d8c\u6765\uff0c\u770b\u5f97\u4eba\u773c\u82b1\u7f2d\u4e71\uff0c\u751f\u6015\u9519\u8fc7\u54ea\u4e2a\u5173\u952e\u4fe1\u606f\u3002", "AI": {"tldr": "\u6574\u4e2a 11 \u6708\u7684\u5927\u6a21\u578b\u5708\u7b80\u76f4\u662f \u201c\u795e\u4ed9\u6253\u67b6\u201d \u6a21\u5f0f \u2014\u2014 \u8c37\u6b4c\u548cClaude\u5148\u540e\u5b98\u5ba3\u4e86\u81ea\u5bb6\u7684\u5347\u7ea7\u7248\u5927\u6a21\u578b\uff0c\u7d27\u63a5\u7740\u5404\u79cd\u4e13\u4e1a\u8bc4\u6d4b\u3001\u4e94\u82b1\u516b\u95e8\u7684\u5e94\u7528\u6307\u5357\u5c31\u94fa\u5929\u76d6\u5730\u6d8c\u6765\uff0c\u770b\u5f97\u4eba\u773c\u82b1\u7f2d\u4e71\uff0c\u751f\u6015\u9519\u8fc7\u54ea\u4e2a\u5173\u952e\u4fe1\u606f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2512.6ce8f791", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MDc2ODM0MA==&mid=2655135631&idx=2&sn=0d5bf0def930e73f0ab3aa3bb1a796cb&chksm=bc77feaa3edd89d18452ed1d151111c7accc9006b955ac35e79412c3d314f80db950b0ea51f9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MDc2ODM0MA==&mid=2655135631&idx=2&sn=0d5bf0def930e73f0ab3aa3bb1a796cb&chksm=bc77feaa3edd89d18452ed1d151111c7accc9006b955ac35e79412c3d314f80db950b0ea51f9#rd", "authors": ["\u76d6\u4e16\u6c7d\u8f66\u793e\u533a"], "title": "\u76d6\u4e16\u6c7d\u8f66\u7814\u7a76\u9662\uff1aAI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u9a71\u52a8\u6c7d\u8f66\u8f6e\u5f0f\u673a\u5668\u4eba\u53d8\u9769\u53d1\u5c55", "comment": "Source: WeChat, Published: 2025-12-02 23:03:56", "summary": "\u5728\u667a\u80fd\u5ea7\u8231\u9886\u57df\uff0c\u591a\u6a21\u6001\u5927\u6a21\u578b\u8d4b\u80fd\u5ea7\u8231\u4e3b\u52a8\u5f0f\u4ea4\u4e92\u548c\u6c89\u6d78\u5f0f\u4f53\u9a8c\uff0cAI\u5927\u6a21\u578b\u5728\u8f66\u8f7d\u58f0\u5b66\u4e2d\u8c03\u97f3\u3001\u57fa\u4e8e\u591a\u6a21\u6001\u4ea4\u4e92\u573a\u666f\u5316\u97f3\u533a\u81ea\u52a8\u8bbe\u7f6e\u548c\u751f\u6210\u6587\u672c\u56fe\u7247\u7b49\uff0c\u57fa\u4e8eAI\u5927\u6a21\u578b\u7684\u5355\u70b9\u4ea4\u4e92\u548c\u4e3b\u52a8\u670d\u52a1\u5df2\u7ecf\u5927\u89c4\u6a21\u4e0a\u8f66\uff0cAI Agent\u5c06\u6210\u4e3a\u53d1\u5c55\u8d8b", "AI": {"tldr": "\u5728\u667a\u80fd\u5ea7\u8231\u9886\u57df\uff0c\u591a\u6a21\u6001\u5927\u6a21\u578b\u8d4b\u80fd\u5ea7\u8231\u4e3b\u52a8\u5f0f\u4ea4\u4e92\u548c\u6c89\u6d78\u5f0f\u4f53\u9a8c\uff0cAI\u5927\u6a21\u578b\u5728\u8f66\u8f7d\u58f0\u5b66\u4e2d\u8c03\u97f3\u3001\u57fa\u4e8e\u591a\u6a21\u6001\u4ea4\u4e92\u573a\u666f\u5316\u97f3\u533a\u81ea\u52a8\u8bbe\u7f6e\u548c\u751f\u6210\u6587\u672c\u56fe\u7247\u7b49\uff0c\u57fa\u4e8eAI\u5927\u6a21\u578b\u7684\u5355\u70b9\u4ea4\u4e92\u548c\u4e3b\u52a8\u670d\u52a1\u5df2\u7ecf\u5927\u89c4\u6a21\u4e0a\u8f66\uff0cAI Agent\u5c06\u6210\u4e3a\u53d1\u5c55\u8d8b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
