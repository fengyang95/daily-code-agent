{"id": "2602.02497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02497", "abs": "https://arxiv.org/abs/2602.02497", "authors": ["Xuzhao Li", "Xuchen Li", "Jian Zhao", "Shiyu Hu"], "title": "STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models", "comment": "Preprint, Under review", "summary": "As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated \"silos,\" offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified \"Discipline $\\times$ Cognition\" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.", "AI": {"tldr": "STEMVerse\uff1a\u4e00\u4e2a\u8bca\u65adLLM\u5728STEM\u9886\u57df\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\"\u5b66\u79d1\u00d7\u8ba4\u77e5\"\u53cc\u8f74\u6807\u7b7e\u7cfb\u7edf\u5316\u5206\u6790\u6a21\u578b\u8868\u73b0\uff0c\u63ed\u793a\u7ed3\u6784\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u8303\u5f0f\u5c06\u57fa\u51c6\u6d4b\u8bd5\u89c6\u4e3a\u5b64\u7acb\u7684\"\u5b64\u5c9b\"\uff0c\u4ec5\u63d0\u4f9b\u805a\u5408\u5206\u6570\uff0c\u65e0\u6cd5\u533a\u5206\u6a21\u578b\u9519\u8bef\u6e90\u4e8e\u9886\u57df\u77e5\u8bc6\u4e0d\u8db3\u8fd8\u662f\u8ba4\u77e5\u80fd\u529b\u7f3a\u9677\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u4ef7\u503c\u3002", "method": "\u63d0\u51faSTEMVerse\u8bca\u65ad\u6846\u67b6\uff0c\u5c062\u4e07\u591a\u4e2aSTEM\u95ee\u9898\u91cd\u65b0\u805a\u5408\u5230\u7edf\u4e00\u7684\"\u5b66\u79d1\u00d7\u8ba4\u77e5\"\u80fd\u529b\u7a7a\u95f4\uff0c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u5206\u914d\u53cc\u8f74\u6807\u7b7e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u548c\u8bad\u7ec3\u8303\u5f0f\u7684LLM\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u63ed\u793a\u4e86STEM\u63a8\u7406\u4e2d\u7684\u7ed3\u6784\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u901a\u8fc7\u591a\u5b66\u79d1\u8986\u76d6\u548c\u7ec6\u7c92\u5ea6\u8ba4\u77e5\u5206\u5c42\u63d0\u4f9b\u4e86\u7406\u89e3LLM\u79d1\u5b66\u63a8\u7406\u7279\u5f81\u7684\u6e05\u6670\u89c6\u89d2\u3002", "conclusion": "STEMVerse\u901a\u8fc7\u6574\u5408\u591a\u5b66\u79d1\u8986\u76d6\u548c\u7ec6\u7c92\u5ea6\u8ba4\u77e5\u5206\u5c42\uff0c\u4e3a\u7406\u89e3LLM\u7684\u79d1\u5b66\u63a8\u7406\u7279\u6027\u63d0\u4f9b\u4e86\u6e05\u6670\u4e14\u53ef\u64cd\u4f5c\u7684\u89c6\u89d2\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u7ed3\u679c\u5bfc\u5411\u8bc4\u4f30\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.02584", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.02584", "abs": "https://arxiv.org/abs/2602.02584", "authors": ["Srinivas Rao Marri"], "title": "Constitutional Spec-Driven Development: Enforcing Security by Construction in AI-Assisted Code Generation", "comment": "15 pages, 2 figures, 5 tables, 11 code listings, 14 references. Includes reference implementation and compliance traceability matrix", "summary": "The proliferation of AI-assisted \"vibe coding\" enables rapid software development but introduces significant security risks, as Large Language Models (LLMs) prioritize functional correctness over security. We present Constitutional Spec-Driven Development, a methodology that embeds non-negotiable security principles into the specification layer, ensuring AI-generated code adheres to security requirements by construction rather than inspection. Our approach introduces a Constitution: a versioned, machine-readable document encoding security constraints derived from Common Weakness Enumeration (CWE)/MITRE Top 25 vulnerabilities and regulatory frameworks. We demonstrate the methodology through a banking microservices application, selected as a representative example domain due to its stringent regulatory and security requirements, implementing customer management, account operations, and transaction processing. The methodology itself is domain-agnostic. The implementation addresses 10 critical CWE vulnerabilities through constitutional constraints with full traceability from principles to code locations. Our case study shows that constitutional constraints reduce security defects by 73% compared to unconstrained AI generation while maintaining developer velocity. We contribute a formal framework for constitutional security, a complete development methodology, and empirical evidence that proactive security specification outperforms reactive security verification in AI-assisted development workflows.", "AI": {"tldr": "\u63d0\u51faConstitutional Spec-Driven Development\u65b9\u6cd5\uff0c\u5728\u89c4\u8303\u5c42\u5d4c\u5165\u4e0d\u53ef\u534f\u5546\u7684\u5b89\u5168\u539f\u5219\uff0c\u4f7fAI\u751f\u6210\u7684\u4ee3\u7801\u4ece\u4e00\u5f00\u59cb\u5c31\u7b26\u5408\u5b89\u5168\u8981\u6c42\uff0c\u800c\u975e\u4e8b\u540e\u68c0\u67e5\u3002\u901a\u8fc7\u94f6\u884c\u5fae\u670d\u52a1\u6848\u4f8b\u9a8c\u8bc1\uff0c\u5c06\u5b89\u5168\u7f3a\u9677\u51cf\u5c1173%\u3002", "motivation": "AI\u8f85\u52a9\u7684\"\u6c1b\u56f4\u7f16\u7a0b\"\u867d\u7136\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5148\u8003\u8651\u529f\u80fd\u6b63\u786e\u6027\u800c\u975e\u5b89\u5168\u6027\uff0c\u5f15\u5165\u4e86\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u786e\u4fddAI\u751f\u6210\u7684\u4ee3\u7801\u7b26\u5408\u5b89\u5168\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u5baa\u6cd5\u89c4\u8303\u9a71\u52a8\u5f00\u53d1\u65b9\u6cd5\uff0c\u5f15\u5165\"\u5baa\u6cd5\"\u2014\u2014\u4e00\u4e2a\u7248\u672c\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u6587\u6863\uff0c\u7f16\u7801\u6765\u81eaCWE/MITRE Top 25\u6f0f\u6d1e\u548c\u76d1\u7ba1\u6846\u67b6\u7684\u5b89\u5168\u7ea6\u675f\u3002\u5c06\u4e0d\u53ef\u534f\u5546\u7684\u5b89\u5168\u539f\u5219\u5d4c\u5165\u89c4\u8303\u5c42\uff0c\u786e\u4fddAI\u751f\u6210\u7684\u4ee3\u7801\u901a\u8fc7\u6784\u9020\u800c\u975e\u68c0\u67e5\u6765\u9075\u5b88\u5b89\u5168\u8981\u6c42\u3002", "result": "\u901a\u8fc7\u94f6\u884c\u5fae\u670d\u52a1\u5e94\u7528\u6848\u4f8b\u7814\u7a76\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e8610\u4e2a\u5173\u952eCWE\u6f0f\u6d1e\uff0c\u5177\u6709\u4ece\u539f\u5219\u5230\u4ee3\u7801\u4f4d\u7f6e\u7684\u5168\u94fe\u8def\u53ef\u8ffd\u6eaf\u6027\u3002\u4e0e\u65e0\u7ea6\u675f\u7684AI\u751f\u6210\u76f8\u6bd4\uff0c\u5baa\u6cd5\u7ea6\u675f\u5c06\u5b89\u5168\u7f3a\u9677\u51cf\u5c11\u4e8673%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f00\u53d1\u901f\u5ea6\u3002", "conclusion": "\u4e3b\u52a8\u7684\u5b89\u5168\u89c4\u8303\u4f18\u4e8e\u53cd\u5e94\u6027\u7684\u5b89\u5168\u9a8c\u8bc1\u5728AI\u8f85\u52a9\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u3002\u8d21\u732e\u4e86\u5baa\u6cd5\u5b89\u5168\u7684\u5f62\u5f0f\u5316\u6846\u67b6\u3001\u5b8c\u6574\u7684\u5f00\u53d1\u65b9\u6cd5\u548c\u5b9e\u8bc1\u8bc1\u636e\u3002", "topic": "swe application"}}
{"id": "2602.02515", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02515", "abs": "https://arxiv.org/abs/2602.02515", "authors": ["Yiliang Song", "Hongjun An", "Jiangong Xiao", "Haofei Zhao", "Jiawei Shao", "Xuelong Li"], "title": "CreditAudit: 2D Auditing for LLM Evaluation and Selection", "comment": "First update", "summary": "Leaderboard scores on public benchmarks have been steadily rising and converging, with many frontier language models now separated by only marginal differences. However, these scores often fail to match users' day to day experience, because system prompts, output protocols, and interaction modes evolve under routine iteration, and in agentic multi step pipelines small protocol shifts can trigger disproportionate failures, leaving practitioners uncertain about which model to deploy. We propose CreditAudit, a deployment oriented credit audit framework that evaluates models under a family of semantically aligned and non adversarial system prompt templates across multiple benchmarks, reporting mean ability as average performance across scenarios and scenario induced fluctuation sigma as a stability risk signal, and further mapping volatility into interpretable credit grades from AAA to BBB via cross model quantiles with diagnostics that mitigate template difficulty drift. Controlled experiments on GPQA, TruthfulQA, and MMLU Pro show that models with similar mean ability can exhibit substantially different fluctuation, and stability risk can overturn prioritization decisions in agentic or high failure cost regimes. By providing a 2D and grade based language for regime specific selection, CreditAudit supports tiered deployment and more disciplined allocation of testing and monitoring effort, enabling more objective and trustworthy model evaluation for real world use.", "AI": {"tldr": "CreditAudit\u662f\u4e00\u4e2a\u9762\u5411\u90e8\u7f72\u7684\u4fe1\u7528\u5ba1\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u79cd\u7cfb\u7edf\u63d0\u793a\u6a21\u677f\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u4f9b\u5e73\u5747\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u98ce\u9669\u4e24\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\uff0c\u5e76\u5c06\u6ce2\u52a8\u6027\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u4fe1\u7528\u7b49\u7ea7\uff0c\u5e2e\u52a9\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u505a\u51fa\u66f4\u5ba2\u89c2\u7684\u6a21\u578b\u9009\u62e9\u3002", "motivation": "\u5f53\u524d\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u7684\u6392\u884c\u699c\u5206\u6570\u8d8b\u4e8e\u6536\u655b\uff0c\u8bb8\u591a\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u5dee\u5f02\u5fae\u5c0f\uff0c\u4f46\u8fd9\u4e9b\u5206\u6570\u65e0\u6cd5\u53cd\u6620\u7528\u6237\u65e5\u5e38\u4f53\u9a8c\u3002\u7cfb\u7edf\u63d0\u793a\u3001\u8f93\u51fa\u534f\u8bae\u548c\u4ea4\u4e92\u6a21\u5f0f\u5728\u5e38\u89c4\u8fed\u4ee3\u4e2d\u4e0d\u65ad\u6f14\u53d8\uff0c\u5728\u4ee3\u7406\u5f0f\u591a\u6b65\u9aa4\u6d41\u7a0b\u4e2d\uff0c\u5c0f\u7684\u534f\u8bae\u53d8\u5316\u53ef\u80fd\u5f15\u53d1\u4e0d\u6210\u6bd4\u4f8b\u7684\u6545\u969c\uff0c\u5bfc\u81f4\u4ece\u4e1a\u8005\u4e0d\u786e\u5b9a\u5e94\u90e8\u7f72\u54ea\u4e2a\u6a21\u578b\u3002", "method": "\u63d0\u51faCreditAudit\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u8bed\u4e49\u5bf9\u9f50\u4e14\u975e\u5bf9\u6297\u6027\u7684\u7cfb\u7edf\u63d0\u793a\u6a21\u677f\u4e0b\u7684\u8868\u73b0\u3002\u62a5\u544a\u5e73\u5747\u80fd\u529b\u4f5c\u4e3a\u8de8\u573a\u666f\u7684\u5e73\u5747\u6027\u80fd\uff0c\u62a5\u544a\u573a\u666f\u8bf1\u5bfc\u7684\u6ce2\u52a8sigma\u4f5c\u4e3a\u7a33\u5b9a\u6027\u98ce\u9669\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u578b\u5206\u4f4d\u6570\u5c06\u6ce2\u52a8\u6027\u6620\u5c04\u4e3a\u4eceAAA\u5230BBB\u7684\u53ef\u89e3\u91ca\u4fe1\u7528\u7b49\u7ea7\uff0c\u540c\u65f6\u63d0\u4f9b\u7f13\u89e3\u6a21\u677f\u96be\u5ea6\u6f02\u79fb\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "result": "\u5728GPQA\u3001TruthfulQA\u548cMMLU Pro\u4e0a\u7684\u63a7\u5236\u5b9e\u9a8c\u8868\u660e\uff0c\u5177\u6709\u76f8\u4f3c\u5e73\u5747\u80fd\u529b\u7684\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u663e\u8457\u4e0d\u540c\u7684\u6ce2\u52a8\u6027\uff0c\u7a33\u5b9a\u6027\u98ce\u9669\u5728\u4ee3\u7406\u5f0f\u6216\u9ad8\u6545\u969c\u6210\u672c\u573a\u666f\u4e0b\u53ef\u80fd\u63a8\u7ffb\u4f18\u5148\u7ea7\u51b3\u7b56\u3002", "conclusion": "CreditAudit\u901a\u8fc7\u63d0\u4f9b\u57fa\u4e8e2D\u548c\u7b49\u7ea7\u7684\u8bed\u8a00\u652f\u6301\u7279\u5b9a\u573a\u666f\u7684\u9009\u62e9\uff0c\u652f\u6301\u5206\u5c42\u90e8\u7f72\u548c\u66f4\u89c4\u8303\u7684\u6d4b\u8bd5\u4e0e\u76d1\u63a7\u8d44\u6e90\u5206\u914d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u5b9e\u73b0\u66f4\u5ba2\u89c2\u3001\u53ef\u4fe1\u7684\u6a21\u578b\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2602.02585", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02585", "abs": "https://arxiv.org/abs/2602.02585", "authors": ["Aprameya Bharadwaj", "Kyle Tu"], "title": "Agentic Observability: Automated Alert Triage for Adobe E-Commerce", "comment": "Accepted at AAAI'26 Agentic AI Benchmarks and Applications for Enterprise Tasks Workshop", "summary": "Modern enterprise systems exhibit complex interdependencies that make observability and incident response increasingly challenging. Manual alert triage, which typically involves log inspection, API verification, and cross-referencing operational knowledge bases, remains a major bottleneck in reducing mean recovery time (MTTR). This paper presents an agentic observability framework deployed within Adobe's e-commerce infrastructure that autonomously performs alert triage using a ReAct paradigm. Upon alert detection, the agent dynamically identifies the affected service, retrieves and analyzes correlated logs across distributed systems, and plans context-dependent actions such as handbook consultation, runbook execution, or retrieval-augmented analysis of recently deployed code. Empirical results from production deployment indicate a 90% reduction in mean time to insight compared to manual triage, while maintaining comparable diagnostic accuracy. Our results show that agentic AI enables an order-of-magnitude reduction in triage latency and a step-change in resolution accuracy, marking a pivotal shift toward autonomous observability in enterprise operations.", "AI": {"tldr": "Adobe\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eReAct\u8303\u5f0f\u7684\u667a\u80fd\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u8fdb\u884c\u544a\u8b66\u5206\u8bca\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u53d7\u5f71\u54cd\u670d\u52a1\u3001\u5206\u6790\u5206\u5e03\u5f0f\u65e5\u5fd7\u3001\u6267\u884c\u64cd\u4f5c\u624b\u518c\u7b49\u65b9\u5f0f\uff0c\u5c06\u5e73\u5747\u6d1e\u5bdf\u65f6\u95f4\u51cf\u5c11\u4e8690%\u3002", "motivation": "\u73b0\u4ee3\u4f01\u4e1a\u7cfb\u7edf\u5177\u6709\u590d\u6742\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u4f7f\u5f97\u53ef\u89c2\u6d4b\u6027\u548c\u4e8b\u4ef6\u54cd\u5e94\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\u3002\u624b\u52a8\u544a\u8b66\u5206\u8bca\uff08\u901a\u5e38\u6d89\u53ca\u65e5\u5fd7\u68c0\u67e5\u3001API\u9a8c\u8bc1\u548c\u64cd\u4f5c\u77e5\u8bc6\u5e93\u4ea4\u53c9\u5f15\u7528\uff09\u4ecd\u7136\u662f\u964d\u4f4e\u5e73\u5747\u6062\u590d\u65f6\u95f4\uff08MTTR\uff09\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u91c7\u7528\u57fa\u4e8eReAct\u8303\u5f0f\u7684\u667a\u80fd\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u5728\u68c0\u6d4b\u5230\u544a\u8b66\u65f6\uff0c\u667a\u80fd\u4f53\u52a8\u6001\u8bc6\u522b\u53d7\u5f71\u54cd\u670d\u52a1\uff0c\u68c0\u7d22\u548c\u5206\u6790\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u76f8\u5173\u65e5\u5fd7\uff0c\u5e76\u89c4\u5212\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u64cd\u4f5c\uff0c\u5982\u624b\u518c\u54a8\u8be2\u3001\u8fd0\u884c\u624b\u518c\u6267\u884c\u6216\u6700\u8fd1\u90e8\u7f72\u4ee3\u7801\u7684\u68c0\u7d22\u589e\u5f3a\u5206\u6790\u3002", "result": "\u751f\u4ea7\u73af\u5883\u90e8\u7f72\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u624b\u52a8\u5206\u8bca\u76f8\u6bd4\uff0c\u5e73\u5747\u6d1e\u5bdf\u65f6\u95f4\u51cf\u5c11\u4e8690%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002\u667a\u80fdAI\u4f7f\u5206\u8bca\u5ef6\u8fdf\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u5728\u89e3\u51b3\u51c6\u786e\u6027\u4e0a\u5b9e\u73b0\u4e86\u9636\u8dc3\u5f0f\u6539\u8fdb\u3002", "conclusion": "\u667a\u80fdAI\u5b9e\u73b0\u4e86\u5206\u8bca\u5ef6\u8fdf\u7684\u6570\u91cf\u7ea7\u51cf\u5c11\u548c\u89e3\u51b3\u51c6\u786e\u6027\u7684\u9636\u8dc3\u5f0f\u6539\u8fdb\uff0c\u6807\u5fd7\u7740\u4f01\u4e1a\u8fd0\u8425\u5411\u81ea\u4e3b\u53ef\u89c2\u6d4b\u6027\u7684\u5173\u952e\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "2602.02559", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02559", "abs": "https://arxiv.org/abs/2602.02559", "authors": ["Pengyu Dai", "Weihao Xuan", "Junjue Wang", "Hongruixuan Chen", "Jian Song", "Yafei Ou", "Naoto Yokoya"], "title": "Experience-Driven Multi-Agent Systems Are Training-free Context-aware Earth Observers", "comment": "21 pages, 6 figures", "summary": "Recent advances have enabled large language model (LLM) agents to solve complex tasks by orchestrating external tools. However, these agents often struggle in specialized, tool-intensive domains that demand long-horizon execution, tight coordination across modalities, and strict adherence to implicit tool constraints. Earth Observation (EO) tasks exemplify this challenge due to the multi-modal and multi-temporal data inputs, as well as the requirements of geo-knowledge constraints (spectrum library, spatial reasoning, etc): many high-level plans can be derailed by subtle execution errors that propagate through a pipeline and invalidate final results. A core difficulty is that existing agents lack a mechanism to learn fine-grained, tool-level expertise from interaction. Without such expertise, they cannot reliably configure tool parameters or recover from mid-execution failures, limiting their effectiveness in complex EO workflows. To address this, we introduce \\textbf{GeoEvolver}, a self-evolving multi-agent system~(MAS) that enables LLM agents to acquire EO expertise through structured interaction without any parameter updates. GeoEvolver decomposes each query into independent sub-goals via a retrieval-augmented multi-agent orchestrator, then explores diverse tool-parameter configurations at the sub-goal level. Successful patterns and root-cause attribution from failures are then distilled in an evolving memory bank that provides in-context demonstrations for future queries. Experiments on three tool-integrated EO benchmarks show that GeoEvolver consistently improves end-to-end task success, with an average gain of 12\\% across multiple LLM backbones, demonstrating that EO expertise can emerge progressively from efficient, fine-grained interactions with the environment.", "AI": {"tldr": "GeoEvolver\u662f\u4e00\u4e2a\u81ea\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u8ba9LLM\u667a\u80fd\u4f53\u5728\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u83b7\u53d6\u5730\u7403\u89c2\u6d4b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5728\u4e09\u4e2a\u5de5\u5177\u96c6\u6210EO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534712%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u4e13\u4e1a\u3001\u5de5\u5177\u5bc6\u96c6\u7684\u5730\u7403\u89c2\u6d4b\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4e9b\u9886\u57df\u9700\u8981\u957f\u65f6\u7a0b\u6267\u884c\u3001\u8de8\u6a21\u6001\u7d27\u5bc6\u534f\u8c03\u548c\u4e25\u683c\u9075\u5faa\u9690\u5f0f\u5de5\u5177\u7ea6\u675f\u3002\u667a\u80fd\u4f53\u7f3a\u4e4f\u4ece\u4ea4\u4e92\u4e2d\u5b66\u4e60\u7ec6\u7c92\u5ea6\u5de5\u5177\u7ea7\u4e13\u4e1a\u77e5\u8bc6\u7684\u673a\u5236\uff0c\u65e0\u6cd5\u53ef\u9760\u914d\u7f6e\u5de5\u5177\u53c2\u6570\u6216\u4ece\u6267\u884c\u5931\u8d25\u4e2d\u6062\u590d\u3002", "method": "GeoEvolver\u91c7\u7528\u81ea\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a1\uff09\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\u5668\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u76ee\u6807\uff1b2\uff09\u5728\u5b50\u76ee\u6807\u7ea7\u522b\u63a2\u7d22\u591a\u6837\u5316\u7684\u5de5\u5177\u53c2\u6570\u914d\u7f6e\uff1b3\uff09\u4ece\u6210\u529f\u6a21\u5f0f\u548c\u5931\u8d25\u6839\u56e0\u5206\u6790\u4e2d\u63d0\u70bc\u77e5\u8bc6\uff0c\u5b58\u50a8\u5728\u8fdb\u5316\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u4e3a\u672a\u6765\u67e5\u8be2\u63d0\u4f9b\u4e0a\u4e0b\u6587\u6f14\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u5de5\u5177\u96c6\u6210\u7684\u5730\u7403\u89c2\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeoEvolver\u6301\u7eed\u63d0\u5347\u7aef\u5230\u7aef\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5e73\u5747\u83b7\u5f9712%\u7684\u589e\u76ca\uff0c\u8bc1\u660eEO\u4e13\u4e1a\u77e5\u8bc6\u53ef\u4ee5\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u9010\u6b65\u6d8c\u73b0\u3002", "conclusion": "GeoEvolver\u5c55\u793a\u4e86LLM\u667a\u80fd\u4f53\u53ef\u4ee5\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u5728\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u83b7\u53d6\u5730\u7403\u89c2\u6d4b\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742EO\u5de5\u4f5c\u6d41\u4e2d\u7684\u5de5\u5177\u914d\u7f6e\u548c\u5931\u8d25\u6062\u590d\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2602.02636", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.02636", "abs": "https://arxiv.org/abs/2602.02636", "authors": ["Ziyang Huang", "Haolin Ren", "Xiaowei Yuan", "Jiawei Wang", "Zhongtao Jiang", "Kun Xu", "Shizhu He", "Jun Zhao", "Kang Liu"], "title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling", "comment": null, "summary": "Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86WideSeekBench\u57fa\u51c6\u548cWideSeek\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u4ece\u6df1\u5ea6\u7814\u7a76\u5411\u5e7f\u5ea6\u7814\u7a76\u8303\u5f0f\u8f6c\u53d8\u4e2d\u7684\u4fe1\u606f\u68c0\u7d22\u4e0e\u5408\u6210\u95ee\u9898\u3002", "motivation": "\u641c\u7d22\u667a\u80fd\u6b63\u4ece\u6df1\u5ea6\u7814\u7a76\u5411\u5e7f\u5ea6\u7814\u7a76\u6f14\u8fdb\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u8fdb\u5c55\u3002\u9700\u8981\u89e3\u51b3\u5728\u590d\u6742\u7ea6\u675f\u4e0b\u5e76\u884c\u68c0\u7d22\u548c\u7efc\u5408\u5168\u9762\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "1) \u6784\u5efaWideSeekBench\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u7ba1\u9053\u786e\u4fdd\u76ee\u6807\u4fe1\u606f\u91cf\u3001\u903b\u8f91\u7ea6\u675f\u548c\u9886\u57df\u7684\u591a\u6837\u6027\uff1b2) \u63d0\u51faWideSeek\u52a8\u6001\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u53ef\u6839\u636e\u4efb\u52a1\u9700\u6c42\u81ea\u4e3b\u5206\u53c9\u5e76\u884c\u5b50\u667a\u80fd\u4f53\uff1b3) \u8bbe\u8ba1\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u7ebf\u6027\u5316\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u5e76\u4f7f\u7528\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eWideSeek\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u6269\u5c55\u667a\u80fd\u4f53\u6570\u91cf\u662f\u63a8\u8fdb\u5e7f\u5ea6\u7814\u7a76\u8303\u5f0f\u7684\u6709\u524d\u666f\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u7ba1\u9053\u548c\u667a\u80fd\u4f53\u4f18\u5316\u4e24\u4e2a\u89d2\u5ea6\u6df1\u5165\u7814\u7a76\u4e86\u5e7f\u5ea6\u7814\u7a76\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u67b6\u6784\u4e3a\u89e3\u51b3\u5e7f\u5ea6\u7814\u7a76\u4e2d\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u591a\u667a\u80fd\u4f53\u6269\u5c55\u662f\u91cd\u8981\u53d1\u5c55\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.02690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02690", "abs": "https://arxiv.org/abs/2602.02690", "authors": ["Chenxi Huang", "Alex Mathai", "Feiyang Yu", "Aleksandr Nogikh", "Petros Maniatis", "Franjo Ivan\u010di\u0107", "Eugene Wu", "Kostis Kaffes", "Junfeng Yang", "Baishakhi Ray"], "title": "Outrunning LLM Cutoffs: A Live Kernel Crash Resolution Benchmark for All", "comment": null, "summary": "Repairing system crashes discovered by kernel fuzzers like Syzkaller is a critical yet underexplored challenge in software engineering. While recent works have introduced Large Language Model (LLM) based agents for Linux kernel crash-resolution, their evaluation benchmarks are usually static and thus, do not capture the evolving nature of the Linux kernel, and suffer from potential data contamination due to LLM knowledge cutoffs. To address the above problem, we present (i) Live-kBench, an evaluation framework for self-evolving benchmarks that continuously scrapes and evaluates agents on freshly discovered kernel bugs, and (ii) kEnv, an agent-agnostic standardized crash-resolution environment for kernel compilation, execution, and feedback. This design decouples agent workflows from heavy-weight execution, enabling fair and scalable comparison across diverse agent frameworks under identical conditions.\n  To this end, we curate an inaugural dataset of 534 Linux kernel bugs and empirically demonstrate a significant performance gap, with agents achieving up to 25% higher equivalent patch rate on bugs fixed before the LLM knowledge cutoff. Using kEnv, we benchmark three state-of-the-art agents, showing that they resolve 74% of crashes on the first attempt (plausible patches); however only ~20% of generated patches closely match developer fixes. Additionally, exposing crash resolution feedback improves crash resolution rate by 29%. Live-kBench provides the community with an evaluation infrastructure for self-evolving benchmarks that is both time and attribute sensitive; complete with a public dashboard to track agent progress on Linux kernel bugs.", "AI": {"tldr": "Live-kBench\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u6301\u7eed\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u4fee\u590dLinux\u5185\u6838\u5d29\u6e83\u65b9\u9762\u7684\u80fd\u529b\uff0c\u914d\u5408kEnv\u6807\u51c6\u5316\u73af\u5883\u5b9e\u73b0\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u9759\u6001\u6027\uff08\u65e0\u6cd5\u53cd\u6620\u5185\u6838\u6f14\u5316\uff09\u548c\u6570\u636e\u6c61\u67d3\uff08LLM\u77e5\u8bc6\u622a\u6b62\u95ee\u9898\uff09\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u52a8\u6001\u3001\u516c\u5e73\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u7ec4\u4ef6\uff1a(1) Live-kBench\uff1a\u6301\u7eed\u722c\u53d6\u65b0\u53d1\u73b0\u5185\u6838\u6f0f\u6d1e\u7684\u81ea\u6f14\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff1b(2) kEnv\uff1a\u4ee3\u7406\u65e0\u5173\u7684\u6807\u51c6\u5316\u5d29\u6e83\u4fee\u590d\u73af\u5883\uff0c\u7528\u4e8e\u5185\u6838\u7f16\u8bd1\u3001\u6267\u884c\u548c\u53cd\u9988\u3002", "result": "\u6536\u96c6534\u4e2aLinux\u5185\u6838\u6f0f\u6d1e\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u4ee3\u7406\u5728LLM\u77e5\u8bc6\u622a\u6b62\u524d\u4fee\u590d\u7684\u6f0f\u6d1e\u4e0a\u7b49\u6548\u8865\u4e01\u7387\u9ad8\u51fa25%\uff1b\u5f53\u524d\u6700\u4f18\u4ee3\u7406\u9996\u5c1d\u8bd5\u4fee\u590d\u7387\u8fbe74%\uff0c\u4f46\u4ec520%\u8865\u4e01\u4e0e\u5f00\u53d1\u8005\u4fee\u590d\u9ad8\u5ea6\u5339\u914d\uff1b\u53cd\u9988\u673a\u5236\u53ef\u5c06\u5d29\u6e83\u4fee\u590d\u7387\u63d0\u534729%\u3002", "conclusion": "Live-kBench\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u65f6\u95f4\u654f\u611f\u548c\u5c5e\u6027\u654f\u611f\u7684\u81ea\u6f14\u5316\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u516c\u5f00\u4eea\u8868\u677f\u8ddf\u8e2a\u4ee3\u7406\u5728Linux\u5185\u6838\u6f0f\u6d1e\u4fee\u590d\u4e0a\u7684\u8fdb\u5c55\u3002", "topic": "swe benchmark"}}
{"id": "2602.02589", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02589", "abs": "https://arxiv.org/abs/2602.02589", "authors": ["Yanki Margalit", "Erni Avram", "Ran Taig", "Oded Margalit", "Nurit Cohen-Inger"], "title": "PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review", "comment": null, "summary": "Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.", "AI": {"tldr": "PeerRank\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u4e3b\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9\u6a21\u578b\u81ea\u4e3b\u751f\u6210\u8bc4\u4f30\u4efb\u52a1\u3001\u57fa\u4e8e\u5b9e\u65f6\u7f51\u7edc\u4fe1\u606f\u56de\u7b54\u95ee\u9898\u3001\u8bc4\u4f30\u540c\u884c\u54cd\u5e94\uff0c\u5e76\u805a\u5408\u5bc6\u96c6\u7684\u540c\u884c\u8bc4\u4f30\u6765\u4f30\u8ba1\u76f8\u5bf9\u6027\u80fd\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u6216\u53c2\u8003\u7b54\u6848\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u7f16\u5199\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u53c2\u8003\u7b54\u6848\u548c\u4eba\u5de5\u6216\u5355\u4e00\u6a21\u578b\u5224\u65ad\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\u3001\u5bb9\u6613\u8fc7\u65f6\uff0c\u4e14\u4e0e\u4f9d\u8d56\u7f51\u7edc\u68c0\u7d22\u548c\u7efc\u5408\u7684\u5f00\u653e\u4e16\u754c\u90e8\u7f72\u4e0d\u5339\u914d\u3002", "method": "PeerRank\u5c06\u8bc4\u4f30\u89c6\u4e3a\u591a\u667a\u80fd\u4f53\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u6a21\u578b\u5bf9\u79f0\u5730\u4f5c\u4e3a\u4efb\u52a1\u8bbe\u8ba1\u8005\u3001\u54cd\u5e94\u8005\u548c\u8bc4\u4f30\u8005\u53c2\u4e0e\u3002\u6a21\u578b\u751f\u6210\u8bc4\u4f30\u4efb\u52a1\uff0c\u4f7f\u7528\u7c7b\u522b\u8303\u56f4\u7684\u5b9e\u65f6\u7f51\u7edc\u4fe1\u606f\u56de\u7b54\u95ee\u9898\uff0c\u8bc4\u4f30\u540c\u884c\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u53bb\u9664\u504f\u89c1\u5224\u65ad\u6765\u805a\u5408\u5bc6\u96c6\u7684\u540c\u884c\u8bc4\u4f30\u3002", "result": "\u572812\u4e2a\u5546\u4e1a\u6a21\u578b\u548c420\u4e2a\u81ea\u4e3b\u751f\u6210\u95ee\u9898\u7684\u5927\u89c4\u6a21\u7814\u7a76\u4e2d\uff0cPeerRank\u4ea7\u751f\u4e86\u7a33\u5b9a\u3001\u6709\u533a\u5206\u5ea6\u7684\u6392\u540d\uff0c\u5e76\u63ed\u793a\u4e86\u53ef\u6d4b\u91cf\u7684\u8eab\u4efd\u548c\u5448\u73b0\u504f\u89c1\u3002\u6392\u540d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e73\u5747\u540c\u884c\u5206\u6570\u4e0eElo\u8bc4\u5206\u4e00\u81f4\u3002\u5728TruthfulQA\u548cGSM8K\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u540c\u884c\u5206\u6570\u4e0e\u5ba2\u89c2\u51c6\u786e\u6027\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5177\u6709\u9009\u62e9\u6027\u7f51\u7edc\u57fa\u7840\u56de\u7b54\u7684\u504f\u89c1\u611f\u77e5\u540c\u884c\u8bc4\u4f30\u53ef\u4ee5\u6269\u5c55\u5f00\u653e\u4e16\u754cLLM\u8bc4\u4f30\uff0c\u8d85\u8d8a\u9759\u6001\u548c\u4eba\u5de5\u7b56\u5212\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "topic": "agent analysis"}}
{"id": "2602.02704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02704", "abs": "https://arxiv.org/abs/2602.02704", "authors": ["Xinyu Wang", "Mingze Li", "Peng Lu", "Xiao-Wen Chang", "Lifeng Shang", "Jinping Li", "Fei Mi", "Prasanna Parthasarathi", "Yufei Cui"], "title": "InfMem: Learning System-2 Memory Control for Long-Context Agent", "comment": null, "summary": "Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\\times$ on average (up to $5.1\\times$) via adaptive early stopping.", "AI": {"tldr": "InfMem\u662f\u4e00\u79cd\u63a7\u5236\u4e2d\u5fc3\u4ee3\u7406\uff0c\u901a\u8fc7PreThink-Retrieve-Write\u534f\u8bae\u5b9e\u73b0System-2\u5f0f\u63a7\u5236\uff0c\u4e3b\u52a8\u76d1\u63a7\u8bc1\u636e\u5145\u5206\u6027\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u68c0\u7d22\uff0c\u5728\u8d85\u957f\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u5e76\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5904\u7406\u8d85\u957f\u6587\u6863\u9700\u8981\u5728\u4e25\u683c\u5185\u5b58\u9650\u5236\u4e0b\u7efc\u5408\u5206\u6563\u5728\u8fdc\u8ddd\u79bb\u7247\u6bb5\u4e2d\u7684\u7a00\u758f\u8bc1\u636e\u3002\u73b0\u6709\u7684\u6d41\u5f0f\u4ee3\u7406\u91c7\u7528\u88ab\u52a8\u5185\u5b58\u66f4\u65b0\u7b56\u7565\uff0c\u5f80\u5f80\u65e0\u6cd5\u4fdd\u7559\u591a\u8df3\u63a8\u7406\u6240\u9700\u7684\u4f4e\u663e\u8457\u6027\u6865\u63a5\u8bc1\u636e\u3002", "method": "\u63d0\u51faInfMem\u63a7\u5236\u4e2d\u5fc3\u4ee3\u7406\uff0c\u91c7\u7528PreThink-Retrieve-Write\u534f\u8bae\u5b9e\u73b0\u4e3b\u52a8\u63a7\u5236\uff1a\u76d1\u63a7\u8bc1\u636e\u5145\u5206\u6027\u3001\u8fdb\u884c\u9488\u5bf9\u6027\u6587\u6863\u5185\u68c0\u7d22\u3001\u5e94\u7528\u8bc1\u636e\u611f\u77e5\u7684\u8054\u5408\u538b\u7f29\u6765\u66f4\u65b0\u6709\u9650\u5185\u5b58\u3002\u5f15\u5165\u5b9e\u7528\u7684SFT-to-RL\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u68c0\u7d22\u3001\u5199\u5165\u548c\u505c\u6b62\u51b3\u7b56\u4e0e\u6700\u7ec8\u4efb\u52a1\u6b63\u786e\u6027\u5bf9\u9f50\u3002", "result": "\u572832k\u52301M tokens\u7684\u8d85\u957fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfMem\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u5747\u4f18\u4e8eMemAgent\uff1aQwen3-1.7B\u63d0\u5347+10.17\u70b9\uff0cQwen3-4B\u63d0\u5347+11.84\u70b9\uff0cQwen2.5-7B\u63d0\u5347+8.23\u70b9\u5e73\u5747\u7edd\u5bf9\u51c6\u786e\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u81ea\u9002\u5e94\u65e9\u505c\u5e73\u5747\u51cf\u5c113.9\u500d\u63a8\u7406\u65f6\u95f4\uff08\u6700\u9ad8\u8fbe5.1\u500d\uff09\u3002", "conclusion": "InfMem\u901a\u8fc7\u4e3b\u52a8\u63a7\u5236\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u957f\u6587\u6863\u63a8\u7406\u4e2d\u7684\u8bc1\u636e\u4fdd\u7559\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.02752", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02752", "abs": "https://arxiv.org/abs/2602.02752", "authors": ["Srinath Srinivasan", "Tim Menzies"], "title": "Beyond the Prompt: Assessing Domain Knowledge Strategies for High-Dimensional LLM Optimization in Software Engineering", "comment": "Accepted at MSR 2026 (Registered Reports Track)", "summary": "Background/Context: Large Language Models (LLMs) demonstrate strong performance on low-dimensional software engineering optimization tasks ($\\le$11 features) but consistently underperform on high-dimensional problems where Bayesian methods dominate. A fundamental gap exists in understanding how systematic integration of domain knowledge (whether from humans or automated reasoning) can bridge this divide.\n  Objective/Aim: We compare human versus artificial intelligence strategies for generating domain knowledge. We systematically evaluate four distinct architectures to determine if structured knowledge integration enables LLMs to generate effective warm starts for high-dimensional optimization.\n  Method: We evaluate four approaches on MOOT datasets stratified by dimensionality: (1) Human-in-the-Loop Domain Knowledge Prompting (H-DKP), utilizing asynchronous expert feedback loops; (2) Adaptive Multi-Stage Prompting (AMP), implementing sequential constraint identification and validation; (3) Dimension-Aware Progressive Refinement (DAPR), conducting optimization in progressively expanding feature subspaces; and (4) Hybrid Knowledge-Model Approach (HKMA), synthesizing statistical scouting (TPE) with RAG-enhanced prompting. Performance is quantified via Chebyshev distance to optimal solutions and ranked using Scott-Knott clustering against an established baseline for LLM generated warm starts.\n  Note that all human studies conducted as part of this study will comply with the policies of our local Institutional Review Board.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e0e\u4eba\u5de5\u667a\u80fd\u751f\u6210\u9886\u57df\u77e5\u8bc6\u7684\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u67b6\u6784\uff0c\u65e8\u5728\u8ba9LLM\u5728\u9ad8\u7ef4\u4f18\u5316\u4e2d\u751f\u6210\u6709\u6548\u7684\u521d\u59cb\u89e3\u3002", "motivation": "LLM\u5728\u4f4e\u7ef4\u8f6f\u4ef6\u5de5\u7a0b\u4f18\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u7ef4\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8d1d\u53f6\u65af\u65b9\u6cd5\u5728\u9ad8\u7ef4\u95ee\u9898\u4e0a\u5360\u4e3b\u5bfc\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u7cfb\u7edf\u96c6\u6210\u9886\u57df\u77e5\u8bc6\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5728MOOT\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u56db\u79cd\u65b9\u6cd5\uff1a1) \u4eba\u7c7b\u5728\u73af\u9886\u57df\u77e5\u8bc6\u63d0\u793a(H-DKP)\uff1b2) \u81ea\u9002\u5e94\u591a\u9636\u6bb5\u63d0\u793a(AMP)\uff1b3) \u7ef4\u5ea6\u611f\u77e5\u6e10\u8fdb\u7ec6\u5316(DAPR)\uff1b4) \u6df7\u5408\u77e5\u8bc6\u6a21\u578b\u65b9\u6cd5(HKMA)\u3002\u4f7f\u7528\u5207\u6bd4\u96ea\u592b\u8ddd\u79bb\u548cScott-Knott\u805a\u7c7b\u8fdb\u884c\u6027\u80fd\u91cf\u5316\u3002", "result": "\u8bba\u6587\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u63cf\u8ff0\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u548c\u6027\u80fd\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u786e\u5b9a\u7ed3\u6784\u5316\u77e5\u8bc6\u96c6\u6210\u662f\u5426\u80fd\u4f7fLLM\u4e3a\u9ad8\u7ef4\u4f18\u5316\u751f\u6210\u6709\u6548\u7684\u521d\u59cb\u89e3\uff0c\u4f46\u7ed3\u8bba\u9700\u7b49\u5f85\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u3002", "topic": "swe benchmark"}}
{"id": "2602.02639", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02639", "abs": "https://arxiv.org/abs/2602.02639", "authors": ["Harry Mayne", "Justin Singh Kang", "Dewi Gould", "Kannan Ramchandran", "Adam Mahdi", "Noah Y. Siegel"], "title": "A Positive Case for Faithfulness: LLM Self-Explanations Help Predict Model Behavior", "comment": null, "summary": "LLM self-explanations are often presented as a promising tool for AI oversight, yet their faithfulness to the model's true reasoning process is poorly understood. Existing faithfulness metrics have critical limitations, typically relying on identifying unfaithfulness via adversarial prompting or detecting reasoning errors. These methods overlook the predictive value of explanations. We introduce Normalized Simulatability Gain (NSG), a general and scalable metric based on the idea that a faithful explanation should allow an observer to learn a model's decision-making criteria, and thus better predict its behavior on related inputs. We evaluate 18 frontier proprietary and open-weight models, e.g., Gemini 3, GPT-5.2, and Claude 4.5, on 7,000 counterfactuals from popular datasets covering health, business, and ethics. We find self-explanations substantially improve prediction of model behavior (11-37% NSG). Self-explanations also provide more predictive information than explanations generated by external models, even when those models are stronger. This implies an advantage from self-knowledge that external explanation methods cannot replicate. Our approach also reveals that, across models, 5-15% of self-explanations are egregiously misleading. Despite their imperfections, we show a positive case for self-explanations: they encode information that helps predict model behavior.", "AI": {"tldr": "\u63d0\u51faNSG\u6307\u6807\u8bc4\u4f30LLM\u81ea\u6211\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u81ea\u6211\u89e3\u91ca\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u884c\u4e3a\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u4ecd\u67095-15%\u7684\u8bef\u5bfc\u6027\u89e3\u91ca", "motivation": "\u73b0\u6709LLM\u81ea\u6211\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u5bf9\u6297\u6027\u63d0\u793a\u6216\u9519\u8bef\u68c0\u6d4b\uff0c\u5ffd\u7565\u4e86\u89e3\u91ca\u7684\u9884\u6d4b\u4ef7\u503c\u3002\u9700\u8981\u66f4\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u6307\u6807\u6765\u8bc4\u4f30\u89e3\u91ca\u662f\u5426\u771f\u6b63\u53cd\u6620\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u5f52\u4e00\u5316\u6a21\u62df\u589e\u76ca\uff08NSG\uff09\u6307\u6807\uff0c\u57fa\u4e8e\"\u5fe0\u5b9e\u89e3\u91ca\u5e94\u8ba9\u89c2\u5bdf\u8005\u5b66\u4e60\u6a21\u578b\u51b3\u7b56\u6807\u51c6\uff0c\u4ece\u800c\u66f4\u597d\u9884\u6d4b\u76f8\u5173\u8f93\u5165\u884c\u4e3a\"\u7684\u7406\u5ff5\u3002\u5728\u5065\u5eb7\u3001\u5546\u4e1a\u3001\u4f26\u7406\u7b49\u9886\u57df\u76847,000\u4e2a\u53cd\u4e8b\u5b9e\u6570\u636e\u4e0a\u8bc4\u4f3018\u4e2a\u524d\u6cbf\u6a21\u578b\uff08\u5305\u62ecGemini 3\u3001GPT-5.2\u3001Claude 4.5\u7b49\uff09\u3002", "result": "\u81ea\u6211\u89e3\u91ca\u663e\u8457\u63d0\u5347\u6a21\u578b\u884c\u4e3a\u9884\u6d4b\u80fd\u529b\uff0811-37% NSG\u589e\u76ca\uff09\u3002\u81ea\u6211\u89e3\u91ca\u6bd4\u5916\u90e8\u6a21\u578b\u751f\u6210\u7684\u89e3\u91ca\u63d0\u4f9b\u66f4\u591a\u9884\u6d4b\u4fe1\u606f\uff0c\u5373\u4f7f\u5916\u90e8\u6a21\u578b\u66f4\u5f3a\u3002\u8fd9\u8868\u660e\u81ea\u6211\u77e5\u8bc6\u4f18\u52bf\u662f\u5916\u90e8\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u590d\u5236\u7684\u3002\u540c\u65f6\u53d1\u73b05-15%\u7684\u81ea\u6211\u89e3\u91ca\u5b58\u5728\u4e25\u91cd\u8bef\u5bfc\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u81ea\u6211\u89e3\u91ca\u786e\u5b9e\u7f16\u7801\u4e86\u6709\u52a9\u4e8e\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\u7684\u4fe1\u606f\uff0c\u4e3a\u81ea\u6211\u89e3\u91ca\u63d0\u4f9b\u4e86\u79ef\u6781\u6848\u4f8b\u3002NSG\u6307\u6807\u4e3a\u8bc4\u4f30\u89e3\u91ca\u5fe0\u5b9e\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2602.02518", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02518", "abs": "https://arxiv.org/abs/2602.02518", "authors": ["Yuyang Bai", "Zhuofeng Li", "Ping Nie", "Jianwen Xie", "Yu Zhang"], "title": "GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning", "comment": "15 pages, Project website: https://yuyangbai.com/graphdancer/", "summary": "Large language models (LLMs) increasingly rely on external knowledge to improve factuality, yet many real-world knowledge sources are organized as heterogeneous graphs rather than plain text. Reasoning over such graph-structured knowledge poses two key challenges: (1) navigating structured, schema-defined relations requires precise function calls rather than similarity-based retrieval, and (2) answering complex questions often demands multi-hop evidence aggregation through iterative information seeking. We propose GraphDancer, a reinforcement learning (RL) framework that teaches LLMs to navigate graphs by interleaving reasoning and function execution. To make RL effective for moderate-sized LLMs, we introduce a graph-aware curriculum that schedules training by the structural complexity of information-seeking trajectories using an easy-to-hard biased sampler. We evaluate GraphDancer on a multi-domain benchmark by training on one domain only and testing on unseen domains and out-of-distribution question types. Despite using only a 3B backbone, GraphDancer outperforms baselines equipped with either a 14B backbone or GPT-4o-mini, demonstrating robust cross-domain generalization of graph exploration and reasoning skills. Our code and models can be found at https://yuyangbai.com/graphdancer/ .", "AI": {"tldr": "GraphDancer\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u6559LLMs\u901a\u8fc7\u4ea4\u66ff\u63a8\u7406\u548c\u6267\u884c\u51fd\u6570\u8c03\u7528\u6765\u5bfc\u822a\u5f02\u6784\u56fe\u77e5\u8bc6\u6e90\uff0c\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u6e90\u591a\u4e3a\u5f02\u6784\u56fe\u800c\u975e\u7eaf\u6587\u672c\uff0cLLMs\u9700\u8981\u7cbe\u786e\u7684\u51fd\u6570\u8c03\u7528\u800c\u975e\u76f8\u4f3c\u6027\u68c0\u7d22\u6765\u5bfc\u822a\u7ed3\u6784\u5316\u5173\u7cfb\uff0c\u4e14\u590d\u6742\u95ee\u9898\u9700\u8981\u591a\u8df3\u8bc1\u636e\u805a\u5408", "method": "\u63d0\u51faGraphDancer\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u63a8\u7406\u548c\u51fd\u6570\u6267\u884c\u6559LLMs\u5bfc\u822a\u56fe\uff1b\u5f15\u5165\u56fe\u611f\u77e5\u8bfe\u7a0b\uff0c\u6839\u636e\u4fe1\u606f\u5bfb\u6c42\u8f68\u8ff9\u7684\u7ed3\u6784\u590d\u6742\u5ea6\u5b89\u6392\u8bad\u7ec3", "result": "\u4ec5\u4f7f\u75283B\u9aa8\u5e72\u6a21\u578b\uff0cGraphDancer\u5728\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u914d\u590714B\u9aa8\u5e72\u6216GPT-4o-mini\u7684\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b", "conclusion": "GraphDancer\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u56fe\u7ed3\u6784\u77e5\u8bc6\u6e90\u4e0a\u7684\u5bfc\u822a\u548c\u63a8\u7406\u6311\u6218\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8de8\u57df\u6cdb\u5316", "topic": "agentic reinforcement learning"}}
{"id": "2602.02660", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02660", "abs": "https://arxiv.org/abs/2602.02660", "authors": ["Jiefeng Chen", "Bhavana Dalvi Mishra", "Jaehyun Nam", "Rui Meng", "Tomas Pfister", "Jinsung Yoon"], "title": "MARS: Modular Agent with Reflective Search for Automated AI Research", "comment": null, "summary": "Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a \"Design-Decompose-Implement\" pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard's top methods. Furthermore, the system exhibits qualitative \"Aha!\" moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u4e13\u4e3a\u81ea\u4e3bAI\u7814\u7a76\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u7b97\u611f\u77e5\u89c4\u5212\u3001\u6a21\u5757\u5316\u6784\u5efa\u548c\u6bd4\u8f83\u53cd\u601d\u8bb0\u5fc6\u6765\u89e3\u51b3AI\u7814\u7a76\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6027\u80fd\u5f52\u56e0\u4e0d\u900f\u660e\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u5316AI\u7814\u7a76\u4e0e\u4e00\u822c\u8f6f\u4ef6\u5de5\u7a0b\u4e0d\u540c\uff0c\u56e0\u4e3a\u6d89\u53ca\u8ba1\u7b97\u6602\u8d35\u7684\u8bc4\u4f30\uff08\u5982\u6a21\u578b\u8bad\u7ec3\uff09\u548c\u4e0d\u900f\u660e\u7684\u6027\u80fd\u5f52\u56e0\u3002\u5f53\u524d\u7684LLM\u4ee3\u7406\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u7ecf\u5e38\u751f\u6210\u5ffd\u7565\u6267\u884c\u6210\u672c\u548c\u56e0\u679c\u56e0\u7d20\u7684\u5355\u4e00\u811a\u672c\u3002", "method": "MARS\u6846\u67b6\u57fa\u4e8e\u4e09\u4e2a\u652f\u67f1\uff1a1) \u9884\u7b97\u611f\u77e5\u89c4\u5212\uff1a\u4f7f\u7528\u6210\u672c\u7ea6\u675f\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6765\u5e73\u8861\u6027\u80fd\u4e0e\u6267\u884c\u6210\u672c\uff1b2) \u6a21\u5757\u5316\u6784\u5efa\uff1a\u91c7\u7528\"\u8bbe\u8ba1-\u5206\u89e3-\u5b9e\u73b0\"\u6d41\u6c34\u7ebf\u7ba1\u7406\u590d\u6742\u7684\u7814\u7a76\u4ed3\u5e93\uff1b3) \u6bd4\u8f83\u53cd\u601d\u8bb0\u5fc6\uff1a\u901a\u8fc7\u5206\u6790\u89e3\u51b3\u65b9\u6848\u5dee\u5f02\u6765\u63d0\u70bc\u9ad8\u4fe1\u53f7\u6d1e\u5bdf\uff0c\u89e3\u51b3\u4fe1\u7528\u5206\u914d\u95ee\u9898\u3002", "result": "MARS\u5728MLE-Bench\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u5f00\u6e90\u6846\u67b6\u76f8\u6bd4\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u53ef\u6bd4\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u4e0e\u5168\u7403\u6392\u884c\u699c\u7684\u9876\u7ea7\u65b9\u6cd5\u76f8\u5f53\u3002\u7cfb\u7edf\u8868\u73b0\u51fa\u5b9a\u6027\u7684\"\u987f\u609f\"\u65f6\u523b\uff0c63%\u7684\u5df2\u4f7f\u7528\u7ecf\u9a8c\u6765\u81ea\u8de8\u5206\u652f\u8f6c\u79fb\uff0c\u8868\u660e\u4ee3\u7406\u80fd\u6709\u6548\u8de8\u641c\u7d22\u8def\u5f84\u6cdb\u5316\u6d1e\u5bdf\u3002", "conclusion": "MARS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86AI\u7814\u7a76\u81ea\u52a8\u5316\u7684\u72ec\u7279\u6311\u6218\uff0c\u901a\u8fc7\u9884\u7b97\u611f\u77e5\u89c4\u5212\u3001\u6a21\u5757\u5316\u6784\u5efa\u548c\u53cd\u601d\u8bb0\u5fc6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u81ea\u4e3b\u7814\u7a76\u80fd\u529b\uff0c\u5e76\u80fd\u8de8\u641c\u7d22\u8def\u5f84\u6cdb\u5316\u6d1e\u5bdf\u3002", "topic": "agent analysis"}}
{"id": "2602.02709", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02709", "abs": "https://arxiv.org/abs/2602.02709", "authors": ["Ujin Jeon", "Jiyong Kwon", "Madison Ann Sullivan", "Caleb Eunho Lee", "Guang Lin"], "title": "ATLAS : Adaptive Self-Evolutionary Research Agent with Task-Distributed Multi-LLM Supporters", "comment": null, "summary": "Recent multi-LLM agent systems perform well in prompt optimization and automated problem-solving, but many either keep the solver frozen after fine-tuning or rely on a static preference-optimization loop, which becomes intractable for long-horizon tasks. We propose ATLAS (Adaptive Task-distributed Learning for Agentic Self-evolution), a task-distributed framework that iteratively develops a lightweight research agent while delegating complementary roles to specialized supporter agents for exploration, hyperparameter tuning, and reference policy management. Our core algorithm, Evolving Direct Preference Optimization (EvoDPO), adaptively updates the phase-indexed reference policy. We provide a theoretical regret analysis for a preference-based contextual bandit under concept drift. In addition, experiments were conducted on non-stationary linear contextual bandits and scientific machine learning (SciML) loss reweighting for the 1D Burgers' equation. Both results show that ATLAS improves stability and performance over a static single-agent baseline.", "AI": {"tldr": "ATLAS\u63d0\u51fa\u4efb\u52a1\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7814\u7a76\u4ee3\u7406\u548c\u4e13\u7528\u652f\u6301\u4ee3\u7406\u534f\u4f5c\uff0c\u7ed3\u5408EvoDPO\u7b97\u6cd5\u81ea\u9002\u5e94\u66f4\u65b0\u53c2\u8003\u7b56\u7565\uff0c\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591aLLM\u4ee3\u7406\u7cfb\u7edf\u5728\u63d0\u793a\u4f18\u5316\u548c\u81ea\u52a8\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8981\u4e48\u5728\u5fae\u8c03\u540e\u4fdd\u6301\u6c42\u89e3\u5668\u56fa\u5b9a\uff0c\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u504f\u597d\u4f18\u5316\u5faa\u73af\uff0c\u8fd9\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u53d8\u5f97\u96be\u4ee5\u5904\u7406\u3002", "method": "\u63d0\u51faATLAS\u4efb\u52a1\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u8fed\u4ee3\u5f00\u53d1\u8f7b\u91cf\u7ea7\u7814\u7a76\u4ee3\u7406\uff0c\u540c\u65f6\u5c06\u63a2\u7d22\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u53c2\u8003\u7b56\u7565\u7ba1\u7406\u7b49\u8865\u5145\u89d2\u8272\u59d4\u6258\u7ed9\u4e13\u7528\u652f\u6301\u4ee3\u7406\u3002\u6838\u5fc3\u7b97\u6cd5EvoDPO\u81ea\u9002\u5e94\u66f4\u65b0\u9636\u6bb5\u7d22\u5f15\u53c2\u8003\u7b56\u7565\u3002", "result": "\u5728\u975e\u5e73\u7a33\u7ebf\u6027\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u548c\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u635f\u5931\u91cd\u52a0\u6743\u5b9e\u9a8c\u4e2d\uff0cATLAS\u76f8\u6bd4\u9759\u6001\u5355\u4ee3\u7406\u57fa\u7ebf\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "conclusion": "ATLAS\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u5206\u5e03\u5f0f\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u53c2\u8003\u7b56\u7565\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02760", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02760", "abs": "https://arxiv.org/abs/2602.02760", "authors": ["Pouya Pezeshkpour", "Estevam Hruschka"], "title": "From Task Solving to Robust Real-World Adaptation in LLM Agents", "comment": null, "summary": "Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a \"clean interface\" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6311\u6218\u4f20\u7edf\"\u5e72\u51c0\u63a5\u53e3\"\u5047\u8bbe\uff0c\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u3001\u52a8\u6001\u73af\u5883\u3001\u566a\u58f0\u4fe1\u53f7\u548c\u52a8\u6001\u72b6\u6001\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u901a\u5e38\u5047\u8bbe\"\u5e72\u51c0\u63a5\u53e3\"\uff1a\u52a8\u6001\u7a33\u5b9a\u3001\u5de5\u5177\u53ef\u9760\u3001\u76ee\u6807\u660e\u786e\uff0c\u8fd9\u9ad8\u4f30\u4e86\u667a\u80fd\u4f53\u7684\u5b9e\u9645\u90e8\u7f72\u51c6\u5907\u5ea6\u3002\u73b0\u5b9e\u4e2d\u667a\u80fd\u4f53\u9762\u4e34\u89c4\u5219\u4e0d\u660e\u786e\u3001\u4fe1\u53f7\u4e0d\u53ef\u9760\u3001\u73af\u5883\u53d8\u5316\u548c\u9690\u5f0f\u591a\u5229\u76ca\u76f8\u5173\u8005\u76ee\u6807\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7f51\u683c\u7684\u6e38\u620f\u73af\u5883\uff0c\u5177\u6709\u7b80\u5355\u76ee\u6807\u4f46\u957f\u65f6\u7a0b\u6267\u884c\u3002\u8bbe\u8ba1\u8fdd\u53cd\"\u5e72\u51c0\u63a5\u53e3\"\u5047\u8bbe\u4f46\u53ef\u89e3\u51b3\u7684\u573a\u666f\uff0c\u8feb\u4f7f\u667a\u80fd\u4f53\u63a8\u65ad\u89c4\u5219\u3001\u4e3a\u4fe1\u606f\u4ed8\u8d39\u3001\u9002\u5e94\u73af\u5883\u53d8\u5316\u3001\u5728\u566a\u58f0\u4e0b\u8c28\u614e\u884c\u52a8\u3002\u6d4b\u8bd5\u4e94\u79cd\u6700\u5148\u8fdb\u7684LLM\u667a\u80fd\u4f53\u3002", "result": "\u53d1\u73b0\u540d\u4e49\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u4e0e\u90e8\u7f72\u9c81\u68d2\u6027\u4e4b\u95f4\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\u3002\u6027\u80fd\u968f\u7f51\u683c\u5927\u5c0f\u548c\u65f6\u7a0b\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4f46\u6392\u540d\u4e0d\u7a33\u5b9a\uff1a\u5728\u7279\u5b9a\u4e0d\u786e\u5b9a\u6027\u673a\u5236\u4e0b\uff0c\u8f83\u5f31\u6a21\u578b\u53ef\u80fd\u51fb\u8d25\u8f83\u5f3a\u6a21\u578b\u3002\u667a\u80fd\u4f53\u5728\u6ca1\u6709\u660e\u786e\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u4f1a\u6743\u8861\u5b8c\u6210\u5ea6\u3001\u6548\u7387\u548c\u60e9\u7f5a\u907f\u514d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u7684\u654f\u611f\u6027\u548c\u5931\u8d25\u9a71\u52a8\u56e0\u7d20\uff0c\u5f3a\u8c03\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u566a\u58f0\u548c\u975e\u5e73\u7a33\u6027\u4e0b\u8fdb\u884c\u9a8c\u8bc1\u3001\u5b89\u5168\u52a8\u4f5c\u9009\u62e9\u548c\u76ee\u6807\u63a8\u65ad\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u667a\u80fd\u4f53\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.02896", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02896", "abs": "https://arxiv.org/abs/2602.02896", "authors": ["Jianru Shen", "Zedong Peng", "Lucy Owen"], "title": "Failure-Aware Enhancements for Large Language Model (LLM) Code Generation: An Empirical Study on Decision Framework", "comment": "Accepted at SANER 2026", "summary": "Large language models (LLMs) show promise for automating software development by translating requirements into code. However, even advanced prompting workflows like progressive prompting often leave some requirements unmet. Although methods such as self-critique, multi-model collaboration, and retrieval-augmented generation (RAG) have been proposed to address these gaps, developers lack clear guidance on when to use each. In an empirical study of 25 GitHub projects, we found that progressive prompting achieves 96.9% average task completion, significantly outperforming direct prompting (80.5%, Cohen's d=1.63, p<0.001) but still leaving 8 projects incomplete. For 6 of the most representative projects, we evaluated each enhancement strategy across 4 failure types. Our results reveal that method effectiveness depends critically on failure characteristics: Self-Critique succeeds on code-reviewable logic errors but fails completely on external service integration (0% improvement), while RAG achieves highest completion across all failure types with superior efficiency. Based on these findings, we propose a decision framework that maps each failure pattern to the most suitable enhancement method, giving practitioners practical, data-driven guidance instead of trial-and-error.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u81ea\u52a8\u5316\u4e2d\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u53d1\u73b0\u4e0d\u540c\u7b56\u7565\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u4ee3\u7801\u751f\u6210\u5931\u8d25\u6548\u679c\u4e0d\u540c\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5931\u8d25\u6a21\u5f0f\u7684\u51b3\u7b56\u6846\u67b6\u3002", "motivation": "LLM\u5728\u5c06\u9700\u6c42\u8f6c\u6362\u4e3a\u4ee3\u7801\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5373\u4f7f\u4f7f\u7528\u6e10\u8fdb\u63d0\u793a\u7b49\u65b9\u6cd5\uff0c\u4ecd\u6709\u4e00\u4e9b\u9700\u6c42\u65e0\u6cd5\u6ee1\u8db3\u3002\u73b0\u6709\u589e\u5f3a\u7b56\u7565\uff08\u5982\u81ea\u6211\u6279\u5224\u3001\u591a\u6a21\u578b\u534f\u4f5c\u3001RAG\uff09\u7f3a\u4e4f\u660e\u786e\u7684\u4f7f\u7528\u6307\u5bfc\uff0c\u5f00\u53d1\u8005\u4e0d\u77e5\u9053\u4f55\u65f6\u4f7f\u7528\u54ea\u79cd\u65b9\u6cd5\u3002", "method": "\u5bf925\u4e2aGitHub\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u6e10\u8fdb\u63d0\u793a\u4e0e\u76f4\u63a5\u63d0\u793a\u7684\u6548\u679c\u3002\u9488\u5bf96\u4e2a\u6700\u5177\u4ee3\u8868\u6027\u7684\u9879\u76ee\uff0c\u8bc4\u4f30\u6bcf\u79cd\u589e\u5f3a\u7b56\u7565\u57284\u79cd\u5931\u8d25\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u3002\u57fa\u4e8e\u7ed3\u679c\u63d0\u51fa\u51b3\u7b56\u6846\u67b6\u3002", "result": "\u6e10\u8fdb\u63d0\u793a\u5e73\u5747\u4efb\u52a1\u5b8c\u6210\u7387\u8fbe96.9%\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\uff0880.5%\uff09\uff0c\u4f46\u4ecd\u67098\u4e2a\u9879\u76ee\u672a\u5b8c\u6210\u3002\u81ea\u6211\u6279\u5224\u5bf9\u53ef\u5ba1\u67e5\u7684\u903b\u8f91\u9519\u8bef\u6709\u6548\uff0c\u4f46\u5bf9\u5916\u90e8\u670d\u52a1\u96c6\u6210\u5b8c\u5168\u65e0\u6548\uff080%\u6539\u8fdb\uff09\u3002RAG\u5728\u6240\u6709\u5931\u8d25\u7c7b\u578b\u4e2d\u5b9e\u73b0\u6700\u9ad8\u5b8c\u6210\u7387\u4e14\u6548\u7387\u6700\u4f18\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u6027\u5173\u952e\u53d6\u51b3\u4e8e\u5931\u8d25\u7279\u5f81\uff0c\u9700\u8981\u6839\u636e\u5931\u8d25\u6a21\u5f0f\u9009\u62e9\u6700\u5408\u9002\u7684\u589e\u5f3a\u65b9\u6cd5\u3002\u63d0\u51fa\u7684\u51b3\u7b56\u6846\u67b6\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u6307\u5bfc\uff0c\u907f\u514d\u4e86\u8bd5\u9519\u3002", "topic": "swe application"}}
{"id": "2602.02711", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02711", "abs": "https://arxiv.org/abs/2602.02711", "authors": ["Yuanzhe Li", "Jianing Deng", "Jingtong Hu", "Tianlong Chen", "Song Wang", "Huanrui Yang"], "title": "Dynamic Mix Precision Routing for Efficient Multi-step LLM Interaction", "comment": null, "summary": "Large language models (LLM) achieve strong performance in long-horizon decision-making tasks through multi-step interaction and reasoning at test time. While practitioners commonly believe a higher task success rate necessitates the use of a larger and stronger LLM model, multi-step interaction with a large LLM incurs prohibitive inference cost. To address this problem, we explore the use of low-precision quantized LLM in the long-horizon decision-making process. Based on the observation of diverse sensitivities among interaction steps, we propose a dynamic mix-precision routing framework that adaptively selects between high-precision and low-precision LLMs at each decision step. The router is trained via a two-stage pipeline, consisting of KL-divergence-based supervised learning that identifies precision-sensitive steps, followed by Group-Relative Policy Optimization (GRPO) to further improve task success rates. Experiments on ALFWorld demonstrate that our approach achieves a great improvement on accuracy-cost trade-off over single-precision baselines and heuristic routing methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u957f\u65f6\u7a0b\u51b3\u7b56\u4efb\u52a1\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u7cbe\u5ea6LLM\uff0c\u4ee5\u5e73\u8861\u4efb\u52a1\u6210\u529f\u7387\u548c\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u957f\u65f6\u7a0b\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u4e3a\u4e86\u83b7\u5f97\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u901a\u5e38\u9700\u8981\u4f7f\u7528\u66f4\u5927\u66f4\u5f3a\u7684LLM\u6a21\u578b\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6210\u672c\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u8def\u7531\u6846\u67b6\uff0c\u57fa\u4e8e\u89c2\u5bdf\u5230\u4e0d\u540c\u4ea4\u4e92\u6b65\u9aa4\u5bf9\u7cbe\u5ea6\u7684\u654f\u611f\u5ea6\u4e0d\u540c\uff0c\u81ea\u9002\u5e94\u5730\u5728\u6bcf\u4e2a\u51b3\u7b56\u6b65\u9aa4\u9009\u62e9\u9ad8\u7cbe\u5ea6\u6216\u4f4e\u7cbe\u5ea6LLM\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\uff1a1) \u57fa\u4e8eKL\u6563\u5ea6\u7684\u76d1\u7763\u5b66\u4e60\u8bc6\u522b\u7cbe\u5ea6\u654f\u611f\u6b65\u9aa4\uff1b2) \u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "result": "\u5728ALFWorld\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u65b9\u9762\u76f8\u6bd4\u5355\u7cbe\u5ea6\u57fa\u7ebf\u548c\u542f\u53d1\u5f0f\u8def\u7531\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u8def\u7531\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u957f\u65f6\u7a0b\u51b3\u7b56\u4efb\u52a1\u4e2d\u6709\u6548\u5e73\u8861LLM\u63a8\u7406\u6210\u672c\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.02934", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02934", "abs": "https://arxiv.org/abs/2602.02934", "authors": ["Yu Shi", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "title": "Beyond Blame: Rethinking SZZ with Knowledge Graph Search", "comment": null, "summary": "Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.\n  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.\n  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.", "AI": {"tldr": "AgenticSZZ\uff1a\u9996\u4e2a\u5c06\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u5e94\u7528\u4e8e\u8f6f\u4ef6\u6f14\u5316\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u5c06bug\u5f15\u5165\u63d0\u4ea4\u8bc6\u522b\u4ece\u57fa\u4e8eblame\u7684\u6392\u5e8f\u95ee\u9898\u8f6c\u5316\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709SZZ\u65b9\u6cd5\u4f9d\u8d56git blame\uff0c\u641c\u7d22\u7a7a\u95f4\u4ec5\u9650\u4e8e\u76f4\u63a5\u4fee\u6539\u4fee\u590d\u884c\u7684\u63d0\u4ea4\uff0c\u4f46\u7814\u7a76\u8868\u660e\u8d85\u8fc740%\u7684bug\u5f15\u5165\u63d0\u4ea4\u65e0\u6cd5\u901a\u8fc7blame\u5355\u72ec\u89e3\u51b3\uff0c\u9700\u8981\u8d85\u8d8ablame\u7ed3\u679c\u7684\u5386\u53f2\u904d\u5386\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u6784\u5efa\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\uff0c\u7f16\u7801\u63d0\u4ea4\u7684\u65f6\u95f4\u4e0e\u7ed3\u6784\u5173\u7cfb\uff0c\u4ece\u4e24\u4e2a\u53c2\u8003\u70b9\uff08blame\u63d0\u4ea4\u548cbug\u4fee\u590d\u63d0\u4ea4\uff09\u5411\u540e\u904d\u5386\u6587\u4ef6\u5386\u53f2\u6269\u5c55\u641c\u7d22\u7a7a\u95f4\uff1b2) \u5229\u7528LLM\u4ee3\u7406\u901a\u8fc7\u4e13\u7528\u5de5\u5177\u5bfc\u822a\u56fe\u8c31\u8fdb\u884c\u5019\u9009\u63a2\u7d22\u548c\u56e0\u679c\u5206\u6790\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAgenticSZZ\u7684F1\u5206\u6570\u8fbe\u52300.48-0.74\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u9ad8\u8fbe27%\u7684\u7edf\u8ba1\u663e\u8457\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e24\u4e2a\u7ec4\u4ef6\u90fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u901a\u8fc7\u5c06bug\u5f15\u5165\u63d0\u4ea4\u8bc6\u522b\u8f6c\u5316\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u4e3a\u8f6f\u4ef6\u6f14\u5316\u5206\u6790\u4e2d\u7684\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4f53\u73b0\u4e86\u7ecf\u5178\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u3002", "topic": "swe application"}}
{"id": "2602.02842", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02842", "abs": "https://arxiv.org/abs/2602.02842", "authors": ["Saeid Sheikhi"], "title": "Chain of Simulation: A Dual-Mode Reasoning Framework for Large Language Models with Dynamic Problem Routing", "comment": null, "summary": "We present Chain of Simulation (CoS), a novel dual-mode reasoning framework that dynamically routes problems to specialized reasoning strategies in Large Language Models (LLMs). Unlike existing uniform prompting approaches, CoS employs three distinct reasoning modes: (1) computational flow with self-consistency for mathematical problems, (2) symbolic state tracking with JSON representations for spatial reasoning, and (3) hybrid fact-extraction for multi-hop inference. Through comprehensive evaluation on GSM8K, StrategyQA, and bAbI benchmarks using four state-of-the-art models (Gemma-3 27B, LLaMA-3.1 8B, Mistral 7B, and Qwen-2.5 14B), we demonstrate that CoS achieves 71.5% accuracy on GSM8K (1.0% absolute improvement), 90.0% on StrategyQA (2.5% improvement), and 19.0% on bAbI (65.2% relative improvement) compared to the strongest baselines. The analysis reveals that problem-specific mode selection is crucial, with computational mode achieving 81.2% accuracy when correctly applied to mathematical problems, while misrouting leads to 0% accuracy. We provide detailed algorithms for mode selection, state tracking, and answer extraction, establishing CoS as an effective approach for improving LLM reasoning without additional training. The framework provides superior trade-offs between accuracy and efficiency compared to Self-Consistency, achieving comparable performance at 54% lower computational cost.", "AI": {"tldr": "CoS\u662f\u4e00\u4e2a\u53cc\u6a21\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u95ee\u9898\u5230\u4e13\u95e8\u7684\u63a8\u7406\u7b56\u7565\uff08\u8ba1\u7b97\u6d41\u3001\u7b26\u53f7\u72b6\u6001\u8ddf\u8e2a\u3001\u6df7\u5408\u4e8b\u5b9e\u63d0\u53d6\uff09\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u7684\u63d0\u793a\u65b9\u6cd5\u65e0\u6cd5\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\u91c7\u7528\u4e13\u95e8\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5bfc\u81f4LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u95ee\u9898\u7c7b\u578b\u52a8\u6001\u9009\u62e9\u6700\u4f18\u63a8\u7406\u6a21\u5f0f\u7684\u6846\u67b6\u3002", "method": "CoS\u91c7\u7528\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff1a1) \u6570\u5b66\u95ee\u9898\u7684\u8ba1\u7b97\u6d41\u4e0e\u81ea\u4e00\u81f4\u6027\uff1b2) \u7a7a\u95f4\u63a8\u7406\u7684\u7b26\u53f7\u72b6\u6001\u8ddf\u8e2a\u4e0eJSON\u8868\u793a\uff1b3) \u591a\u8df3\u63a8\u7406\u7684\u6df7\u5408\u4e8b\u5b9e\u63d0\u53d6\u3002\u6846\u67b6\u5305\u542b\u6a21\u5f0f\u9009\u62e9\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u7b54\u6848\u63d0\u53d6\u7b97\u6cd5\u3002", "result": "\u5728GSM8K\u3001StrategyQA\u548cbAbI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoS\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5206\u522b\u63d0\u53471.0%\u30012.5%\u548c65.2%\u76f8\u5bf9\u6539\u8fdb\u3002\u8ba1\u7b97\u6a21\u5f0f\u6b63\u786e\u5e94\u7528\u4e8e\u6570\u5b66\u95ee\u9898\u65f6\u51c6\u786e\u7387\u8fbe81.2%\uff0c\u800c\u9519\u8bef\u8def\u7531\u5219\u5bfc\u81f40%\u51c6\u786e\u7387\u3002\u76f8\u6bd4Self-Consistency\uff0c\u4ee554%\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u83b7\u5f97\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "CoS\u8bc1\u660e\u4e86\u95ee\u9898\u7279\u5b9a\u7684\u6a21\u5f0f\u9009\u62e9\u5bf9LLM\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4f18\u8d8a\u7684\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2602.02530", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02530", "abs": "https://arxiv.org/abs/2602.02530", "authors": ["Saurav Singh", "Rodney Sanchez", "Alexander Ororbia", "Jamison Heard"], "title": "Formulating Reinforcement Learning for Human-Robot Collaboration through Off-Policy Evaluation", "comment": null, "summary": "Reinforcement learning (RL) has the potential to transform real-world decision-making systems by enabling autonomous agents to learn from experience. Deploying RL in real-world settings, especially in the context of human-robot interaction, requires defining state representations and reward functions, which are critical for learning efficiency and policy performance. Traditional RL approaches often rely on domain expertise and trial-and-error, necessitating extensive human involvement as well as direct interaction with the environment, which can be costly and impractical, especially in complex and safety-critical applications. This work proposes a novel RL framework that leverages off-policy evaluation (OPE) for state space and reward function selection, using only logged interaction data. This approach eliminates the need for real-time access to the environment or human-in-the-loop feedback, greatly reducing the dependency on costly real-time interactions. The proposed approach systematically evaluates multiple candidate state representations and reward functions by training offline RL agents and applying OPE to estimate policy performance. The optimal state space and reward function are selected based on their ability to produce high-performing policies under OPE metrics. Our method is validated on two environments: the Lunar Lander environment by OpenAI Gym, which provides a controlled setting for assessing state space and reward function selection, and a NASA-MATB-II human subjects study environment, which evaluates the approach's real-world applicability to human-robot teaming scenarios. This work enhances the feasibility and scalability of offline RL for real-world environments by automating critical RL design decisions through a data-driven OPE-based evaluation, enabling more reliable, effective, and sustainable RL formulation for complex human-robot interaction settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9009\u62e9\u72b6\u6001\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\uff0c\u4ec5\u4f7f\u7528\u65e5\u5fd7\u4ea4\u4e92\u6570\u636e\uff0c\u65e0\u9700\u5b9e\u65f6\u73af\u5883\u8bbf\u95ee\u6216\u4eba\u5de5\u53cd\u9988\u3002", "motivation": "\u4f20\u7edfRL\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u9700\u8981\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u72b6\u6001\u8868\u793a\u548c\u5956\u52b1\u51fd\u6570\uff0c\u4f9d\u8d56\u5b9e\u65f6\u73af\u5883\u4ea4\u4e92\u548c\u4eba\u5de5\u53c2\u4e0e\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u7528\u4e8e\u590d\u6742\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u79bb\u7ebfRL\u4ee3\u7406\u8bad\u7ec3\u591a\u4e2a\u5019\u9009\u72b6\u6001\u8868\u793a\u548c\u5956\u52b1\u51fd\u6570\uff0c\u5e94\u7528OPE\u8bc4\u4f30\u7b56\u7565\u6027\u80fd\uff0c\u57fa\u4e8eOPE\u6307\u6807\u9009\u62e9\u6700\u4f18\u72b6\u6001\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728OpenAI Gym\u7684Lunar Lander\u73af\u5883\u548cNASA-MATB-II\u4eba\u7c7b\u88ab\u8bd5\u7814\u7a76\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684OPE\u8bc4\u4f30\u81ea\u52a8\u5316\u5173\u952eRL\u8bbe\u8ba1\u51b3\u7b56\uff0c\u589e\u5f3a\u4e86\u79bb\u7ebfRL\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u6709\u6548\u7684RL\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02843", "abs": "https://arxiv.org/abs/2602.02843", "authors": ["Polina Tsvilodub", "Karl Mulligan", "Todd Snider", "Robert D. Hawkins", "Michael Franke"], "title": "Act or Clarify? Modeling Sensitivity to Uncertainty and Cost in Communication", "comment": "6 pages, 3 figures, under review", "summary": "When deciding how to act under uncertainty, agents may choose to act to reduce uncertainty or they may act despite that uncertainty.In communicative settings, an important way of reducing uncertainty is by asking clarification questions (CQs). We predict that the decision to ask a CQ depends on both contextual uncertainty and the cost of alternative actions, and that these factors interact: uncertainty should matter most when acting incorrectly is costly. We formalize this interaction in a computational model based on expected regret: how much an agent stands to lose by acting now rather than with full information. We test these predictions in two experiments, one examining purely linguistic responses to questions and another extending to choices between clarification and non-linguistic action. Taken together, our results suggest a rational tradeoff: humans tend to seek clarification proportional to the risk of substantial loss when acting under uncertainty.", "AI": {"tldr": "\u7814\u7a76\u4eba\u7c7b\u5728\u4e0d\u786e\u5b9a\u60c5\u5883\u4e0b\u662f\u5426\u5bfb\u6c42\u6f84\u6e05\u95ee\u9898\u53d6\u51b3\u4e8e\u60c5\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u66ff\u4ee3\u884c\u52a8\u6210\u672c\uff0c\u4e24\u8005\u4ea4\u4e92\u4f5c\u7528\uff1a\u5f53\u9519\u8bef\u884c\u52a8\u4ee3\u4ef7\u9ad8\u65f6\uff0c\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u6700\u5927", "motivation": "\u5728\u6c9f\u901a\u60c5\u5883\u4e2d\uff0c\u4ee3\u7406\u9762\u4e34\u4e0d\u786e\u5b9a\u6027\u65f6\u53ef\u4ee5\u9009\u62e9\u901a\u8fc7\u6f84\u6e05\u95ee\u9898\u6765\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\uff0c\u4e5f\u53ef\u4ee5\u76f4\u63a5\u884c\u52a8\u3002\u7814\u7a76\u8005\u9884\u6d4b\u6f84\u6e05\u95ee\u9898\u7684\u51b3\u7b56\u53d6\u51b3\u4e8e\u60c5\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u66ff\u4ee3\u884c\u52a8\u6210\u672c\uff0c\u4e14\u8fd9\u4e24\u4e2a\u56e0\u7d20\u4f1a\u76f8\u4e92\u4f5c\u7528\uff1a\u5f53\u9519\u8bef\u884c\u52a8\u4ee3\u4ef7\u9ad8\u6602\u65f6\uff0c\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u6700\u5927", "method": "\u57fa\u4e8e\u9884\u671f\u9057\u61be\u7684\u8ba1\u7b97\u6a21\u578b\uff1a\u8861\u91cf\u4ee3\u7406\u5728\u4fe1\u606f\u4e0d\u5168\u65f6\u884c\u52a8\u53ef\u80fd\u635f\u5931\u591a\u5c11\u3002\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\u9884\u6d4b\uff1a\u4e00\u4e2a\u68c0\u9a8c\u7eaf\u8bed\u8a00\u56de\u5e94\uff0c\u53e6\u4e00\u4e2a\u6269\u5c55\u5230\u6f84\u6e05\u95ee\u9898\u4e0e\u975e\u8bed\u8a00\u884c\u52a8\u4e4b\u95f4\u7684\u9009\u62e9", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4eba\u7c7b\u503e\u5411\u4e8e\u6309\u6bd4\u4f8b\u5bfb\u6c42\u6f84\u6e05\uff1a\u5f53\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u884c\u52a8\u53ef\u80fd\u9020\u6210\u91cd\u5927\u635f\u5931\u65f6\uff0c\u4eba\u4eec\u66f4\u53ef\u80fd\u5bfb\u6c42\u6f84\u6e05\uff0c\u4f53\u73b0\u4e86\u7406\u6027\u6743\u8861", "conclusion": "\u4eba\u7c7b\u6f84\u6e05\u95ee\u9898\u7684\u51b3\u7b56\u9075\u5faa\u7406\u6027\u6743\u8861\u539f\u5219\uff0c\u6839\u636e\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u884c\u52a8\u53ef\u80fd\u9020\u6210\u7684\u635f\u5931\u7a0b\u5ea6\u6309\u6bd4\u4f8b\u5bfb\u6c42\u6f84\u6e05\uff0c\u652f\u6301\u57fa\u4e8e\u9884\u671f\u9057\u61be\u7684\u8ba1\u7b97\u6a21\u578b", "topic": "agent analysis"}}
{"id": "2602.02532", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02532", "abs": "https://arxiv.org/abs/2602.02532", "authors": ["Mahyar Alinejad", "Yue Wang", "George Atia"], "title": "CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning", "comment": null, "summary": "Transfer learning promises to reduce the high sample complexity of deep reinforcement learning (RL), yet existing methods struggle with domain shift between source and target environments. Policy distillation provides powerful tactical guidance but fails to transfer long-term strategic knowledge, while automaton-based methods capture task structure but lack fine-grained action guidance. This paper introduces Context-Aware Distillation with Experience-gated Transfer (CADENT), a framework that unifies strategic automaton-based knowledge with tactical policy-level knowledge into a coherent guidance signal. CADENT's key innovation is an experience-gated trust mechanism that dynamically weighs teacher guidance against the student's own experience at the state-action level, enabling graceful adaptation to target domain specifics. Across challenging environments, from sparse-reward grid worlds to continuous control tasks, CADENT achieves 40-60\\% better sample efficiency than baselines while maintaining superior asymptotic performance, establishing a robust approach for adaptive knowledge transfer in RL.", "AI": {"tldr": "CADENT\u6846\u67b6\u901a\u8fc7\u7ecf\u9a8c\u95e8\u63a7\u673a\u5236\u7edf\u4e00\u6218\u7565\u81ea\u52a8\u673a\u77e5\u8bc6\u548c\u6218\u672f\u7b56\u7565\u77e5\u8bc6\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u8fc1\u79fb\u4e2d\u5b9e\u73b040-60%\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u9886\u57df\u504f\u79fb\uff0c\u7b56\u7565\u84b8\u998f\u7f3a\u4e4f\u957f\u671f\u6218\u7565\u77e5\u8bc6\uff0c\u800c\u81ea\u52a8\u673a\u65b9\u6cd5\u53c8\u7f3a\u5c11\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u6307\u5bfc\u3002", "method": "CADENT\u6846\u67b6\u5c06\u6218\u7565\u81ea\u52a8\u673a\u77e5\u8bc6\u4e0e\u6218\u672f\u7b56\u7565\u77e5\u8bc6\u7edf\u4e00\u4e3a\u8fde\u8d2f\u7684\u6307\u5bfc\u4fe1\u53f7\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u7ecf\u9a8c\u95e8\u63a7\u4fe1\u4efb\u673a\u5236\uff0c\u5728\u72b6\u6001-\u52a8\u4f5c\u5c42\u9762\u52a8\u6001\u6743\u8861\u6559\u5e08\u6307\u5bfc\u4e0e\u5b66\u751f\u81ea\u8eab\u7ecf\u9a8c\u3002", "result": "\u5728\u7a00\u758f\u5956\u52b1\u7f51\u683c\u4e16\u754c\u548c\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\uff0cCADENT\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e8640-60%\u7684\u6837\u672c\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u8d8a\u7684\u6e10\u8fdb\u6027\u80fd\u3002", "conclusion": "CADENT\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u81ea\u9002\u5e94\u77e5\u8bc6\u8fc1\u79fb\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4f18\u96c5\u5730\u9002\u5e94\u76ee\u6807\u57df\u7279\u5b9a\u9700\u6c42\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03400", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03400", "abs": "https://arxiv.org/abs/2602.03400", "authors": ["Jintai Li", "Songqiang Chen", "Shuo Jin", "Xiaoyuan Xie"], "title": "Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations", "comment": null, "summary": "Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.\n  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.", "AI": {"tldr": "ExpSum\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5de5\u4e1a\u6587\u6863\u671f\u671b\u7684\u4ee3\u7801\u6458\u8981\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u51fd\u6570\u5143\u6570\u636e\u62bd\u8c61\u3001\u4fe1\u606f\u8fc7\u6ee4\u3001\u9886\u57df\u77e5\u8bc6\u68c0\u7d22\u548c\u7ea6\u675f\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u5728\u5b9e\u9645\u5de5\u4e1a\u9879\u76ee\u4e2d\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u6458\u8981\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u751f\u6210\u7684\u6458\u8981\u5b9e\u7528\u6027\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u8d85\u8fc757.4%\u7684\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u6458\u8981\u56e0\u4e0d\u7b26\u5408\u5f00\u53d1\u8005\u5bf9\u5de5\u4e1a\u6587\u6863\u7684\u671f\u671b\u800c\u88ab\u62d2\u7edd\uff0c\u5f00\u53d1\u8005\u9700\u8981\u9002\u5f53\u7684\u9886\u57df\u672f\u8bed\u3001\u660e\u786e\u7684\u51fd\u6570\u5206\u7c7b\uff0c\u5e76\u907f\u514d\u5197\u4f59\u5b9e\u73b0\u7ec6\u8282\u3002", "method": "ExpSum\u65b9\u6cd5\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u51fd\u6570\u5143\u6570\u636e\u62bd\u8c61\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff1b2) \u4fe1\u606f\u5143\u6570\u636e\u8fc7\u6ee4\uff0c\u53bb\u9664\u5197\u4f59\uff1b3) \u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9886\u57df\u77e5\u8bc6\u68c0\u7d22\uff1b4) \u7ea6\u675f\u9a71\u52a8\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u6307\u5bfcLLM\u751f\u6210\u7ed3\u6784\u5316\u3001\u7b26\u5408\u671f\u671b\u7684\u6458\u8981\u3002", "result": "\u5728HarmonyOS\u9879\u76ee\u548c\u5e7f\u6cdb\u4f7f\u7528\u7684\u4ee3\u7801\u6458\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cExpSum\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728HarmonyOS\u4e0aBLEU-4\u63d0\u534726.71%\uff0cROUGE-L\u63d0\u534720.10%\u3002\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u8868\u660e\uff0cExpSum\u751f\u6210\u7684\u6458\u8981\u5728\u5176\u4ed6\u9879\u76ee\u4e2d\u4e5f\u80fd\u66f4\u597d\u5730\u7b26\u5408\u5f00\u53d1\u8005\u671f\u671b\u3002", "conclusion": "ExpSu\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u4ee3\u7801\u6587\u6863\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u901a\u8fc7\u6574\u5408\u5f00\u53d1\u8005\u671f\u671b\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u53ef\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "2602.02905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02905", "abs": "https://arxiv.org/abs/2602.02905", "authors": ["Zhen Wang", "Fan Bai", "Zhongyan Luo", "Jinyan Su", "Kaiser Sun", "Xinle Yu", "Jieyuan Liu", "Kun Zhou", "Claire Cardie", "Mark Dredze", "Eric P. Xing", "Zhiting Hu"], "title": "FIRE-Bench: Evaluating Agents on the Rediscovery of Scientific Insights", "comment": "30 pages, 4 figures, 10 tables", "summary": "Autonomous agents powered by large language models (LLMs) promise to accelerate scientific discovery end-to-end, but rigorously evaluating their capacity for verifiable discovery remains a central challenge. Existing benchmarks face a trade-off: they either heavily rely on LLM-as-judge evaluations of automatically generated research outputs or optimize convenient yet isolated performance metrics that provide coarse proxies for scientific insight. To address this gap, we introduce FIRE-Bench (Full-cycle Insight Rediscovery Evaluation), a benchmark that evaluates agents through the rediscovery of established findings from recent, high-impact machine learning research. Agents are given only a high-level research question extracted from a published, verified study and must autonomously explore ideas, design experiments, implement code, execute their plans, and derive conclusions supported by empirical evidence. We evaluate a range of state-of-the-art agents with frontier LLMs backbones like gpt-5 on FIRE-Bench. Our results show that full-cycle scientific research remains challenging for current agent systems: even the strongest agents achieve limited rediscovery success (<50 F1), exhibit high variance across runs, and display recurring failure modes in experimental design, execution, and evidence-based reasoning. FIRE-Bench provides a rigorous and diagnostic framework for measuring progress toward reliable agent-driven scientific discovery.", "AI": {"tldr": "FIRE-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u79d1\u5b66\u53d1\u73b0\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8ba9\u4ee3\u7406\u91cd\u65b0\u53d1\u73b0\u5df2\u53d1\u8868\u673a\u5668\u5b66\u4e60\u7814\u7a76\u7684\u7ed3\u8bba\u6765\u8bc4\u4f30\u5176\u5b8c\u6574\u79d1\u7814\u6d41\u7a0b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u8fc7\u5ea6\u4f9d\u8d56LLM-as-judge\u8bc4\u4f30\u81ea\u52a8\u751f\u6210\u7684\u7814\u7a76\u8f93\u51fa\uff0c\u8981\u4e48\u4f7f\u7528\u5b64\u7acb\u6027\u80fd\u6307\u6807\u4f5c\u4e3a\u79d1\u5b66\u6d1e\u5bdf\u529b\u7684\u7c97\u7565\u4ee3\u7406\u3002\u9700\u8981\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30AI\u4ee3\u7406\u8fdb\u884c\u53ef\u9a8c\u8bc1\u79d1\u5b66\u53d1\u73b0\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165FIRE-Bench\u57fa\u51c6\uff0c\u8ba9AI\u4ee3\u7406\u57fa\u4e8e\u5df2\u53d1\u8868\u3001\u5df2\u9a8c\u8bc1\u7814\u7a76\u4e2d\u63d0\u53d6\u7684\u9ad8\u5c42\u6b21\u7814\u7a76\u95ee\u9898\uff0c\u81ea\u4e3b\u63a2\u7d22\u60f3\u6cd5\u3001\u8bbe\u8ba1\u5b9e\u9a8c\u3001\u5b9e\u73b0\u4ee3\u7801\u3001\u6267\u884c\u8ba1\u5212\uff0c\u5e76\u57fa\u4e8e\u7ecf\u9a8c\u8bc1\u636e\u5f97\u51fa\u7ed3\u8bba\uff0c\u91cd\u65b0\u53d1\u73b0\u5df2\u786e\u7acb\u7684\u7814\u7a76\u53d1\u73b0\u3002", "result": "\u4f7f\u7528\u524d\u6cbfLLM\uff08\u5982gpt-5\uff09\u7684\u6700\u5148\u8fdb\u4ee3\u7406\u5728FIRE-Bench\u4e0a\u8868\u73b0\u6709\u9650\uff1a\u6700\u5f3a\u4ee3\u7406\u7684\u91cd\u65b0\u53d1\u73b0\u6210\u529f\u7387\u4f4e\u4e8e50 F1\uff0c\u8fd0\u884c\u95f4\u65b9\u5dee\u9ad8\uff0c\u5728\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6267\u884c\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u65b9\u9762\u5b58\u5728\u91cd\u590d\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u5b8c\u6574\u5468\u671f\u7684\u79d1\u5b66\u7814\u7a76\u5bf9\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0cFIRE-Bench\u4e3a\u8861\u91cf\u4ee3\u7406\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u4e25\u8c28\u4e14\u5177\u6709\u8bca\u65ad\u6027\u7684\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.03411", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03411", "abs": "https://arxiv.org/abs/2602.03411", "authors": ["Huatong Song", "Lisheng Huang", "Shuang Sun", "Jinhao Jiang", "Ran Le", "Daixuan Cheng", "Guoxin Chen", "Yiwen Hu", "Zongchao Chen", "Wayne Xin Zhao", "Yang Song", "Tao Zhang", "Ji-Rong Wen"], "title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training", "comment": null, "summary": "In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.", "AI": {"tldr": "SWE-Master\u662f\u4e00\u4e2a\u5f00\u6e90\u3001\u53ef\u590d\u73b0\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5305\u62ec\u6559\u5e08\u8f68\u8ff9\u5408\u6210\u3001\u957f\u89c6\u91ceSFT\u3001\u771f\u5b9e\u6267\u884c\u53cd\u9988\u7684RL\u7b49\uff09\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u89e3\u51b3\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5728SWE-bench Verified\u4e0a\u8fbe\u523061.4%\u7684\u89e3\u51b3\u7387\uff0c\u7ed3\u5408TTS\u53ef\u63d0\u5347\u81f370.8%\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u5f00\u53d1\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u590d\u73b0\u7684\u8bad\u7ec3\u6846\u67b6\u3002\u7814\u7a76\u8005\u65e8\u5728\u63a2\u7d22\u5b8c\u6574\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u6d41\u7a0b\uff0c\u4ece\u57fa\u7840\u6a21\u578b\u51fa\u53d1\uff0c\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5\u6fc0\u53d1\u6a21\u578b\u89e3\u51b3\u957f\u89c6\u91ce\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e3a\u53ef\u590d\u73b0\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002", "method": "SWE-Master\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u6559\u5e08\u8f68\u8ff9\u5408\u6210\u4e0e\u6570\u636e\u6574\u7406\uff1b2\uff09\u957f\u89c6\u91ce\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff1b3\uff09\u57fa\u4e8e\u771f\u5b9e\u6267\u884c\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff1b4\uff09\u63a8\u7406\u6846\u67b6\u8bbe\u8ba1\u3002\u6846\u67b6\u4ece\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u5f00\u59cb\uff0c\u9010\u6b65\u4f18\u5316\u8f6f\u4ef6\u5de5\u7a0b\u80fd\u529b\uff0c\u5e76\u652f\u6301\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u7ed3\u5408LLM\u73af\u5883\u53cd\u9988\u3002", "result": "\u5728SWE-bench Verified\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Qwen2.5-Coder-32B\u6a21\u578b\uff0cSWE-Master\u5728\u76f8\u540c\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u8fbe\u523061.4%\u7684\u89e3\u51b3\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u57fa\u7ebf\u3002\u7ed3\u5408\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS@8\uff09\u548cLLM\u73af\u5883\u53cd\u9988\uff0c\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u81f370.8%\u3002", "conclusion": "SWE-Master\u8bc1\u660e\u4e86\u7cfb\u7edf\u5316\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6fc0\u53d1\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u7684\u53ef\u590d\u73b0\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u900f\u660e\u7684\u6846\u67b6\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u6f5c\u529b\u3002", "topic": "swe application"}}
{"id": "2602.02909", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02909", "abs": "https://arxiv.org/abs/2602.02909", "authors": ["Kiran Tomlinson", "Tobias Schnabel", "Adith Swaminathan", "Jennifer Neville"], "title": "Reasoning about Reasoning: BAPO Bounds on Chain-of-Thought Token Complexity in LLMs", "comment": "28 pages", "summary": "Inference-time scaling via chain-of-thought (CoT) reasoning is a major driver of state-of-the-art LLM performance, but it comes with substantial latency and compute costs. We address a fundamental theoretical question: how many reasoning tokens are required to solve a problem as input size grows? By extending the bounded attention prefix oracle (BAPO) model--an abstraction of LLMs that quantifies the information flow required to solve a task--we prove lower bounds on the CoT tokens required for three canonical BAPO-hard tasks: binary majority, triplet matching, and graph reachability. We show that each requires $\u03a9(n)$ reasoning tokens when the input size is $n$. We complement these results with matching or near-matching upper bounds via explicit constructions. Finally, our experiments with frontier reasoning models show approximately linear reasoning token scaling on these tasks and failures when constrained to smaller reasoning budgets, consistent with our theoretical lower bounds. Together, our results identify fundamental bottlenecks in inference-time compute through CoT and offer a principled tool for analyzing optimal reasoning length.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6269\u5c55BAPO\u6a21\u578b\uff0c\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u4e09\u4e2a\u5178\u578b\u4efb\u52a1\uff08\u4e8c\u8fdb\u5236\u591a\u6570\u3001\u4e09\u5143\u7ec4\u5339\u914d\u3001\u56fe\u53ef\u8fbe\u6027\uff09\u9700\u8981\u03a9(n)\u63a8\u7406token\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u5448\u73b0\u8fd1\u4f3c\u7ebf\u6027\u7684\u63a8\u7406token\u6269\u5c55\u3002", "motivation": "\u867d\u7136\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86LLM\u6027\u80fd\uff0c\u4f46\u5e26\u6765\u4e86\u5de8\u5927\u7684\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u4e00\u4e2a\u57fa\u7840\u7406\u8bba\u95ee\u9898\uff1a\u968f\u7740\u8f93\u5165\u89c4\u6a21\u589e\u957f\uff0c\u89e3\u51b3\u4e00\u4e2a\u95ee\u9898\u9700\u8981\u591a\u5c11\u63a8\u7406token\uff1f", "method": "\u6269\u5c55\u6709\u754c\u6ce8\u610f\u529b\u524d\u7f00\u9884\u8a00\u673a\u6a21\u578b\u6765\u91cf\u5316\u4efb\u52a1\u6240\u9700\u7684\u4fe1\u606f\u6d41\uff0c\u4e3a\u4e09\u4e2aBAPO-hard\u4efb\u52a1\u8bc1\u660e\u63a8\u7406token\u7684\u4e0b\u754c\uff0c\u5e76\u63d0\u4f9b\u5339\u914d\u6216\u63a5\u8fd1\u5339\u914d\u7684\u4e0a\u754c\u6784\u9020\uff0c\u6700\u540e\u901a\u8fc7\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u4e09\u4e2a\u4efb\u52a1\u90fd\u9700\u8981\u03a9(n)\u63a8\u7406token\uff0c\u63d0\u4f9b\u4e86\u5339\u914d\u7684\u4e0a\u754c\u6784\u9020\uff0c\u5b9e\u9a8c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u5448\u73b0\u8fd1\u4f3c\u7ebf\u6027\u7684\u63a8\u7406token\u6269\u5c55\uff0c\u5f53\u63a8\u7406\u9884\u7b97\u53d7\u9650\u65f6\u4f1a\u5931\u8d25\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc6\u522b\u4e86\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u8fdb\u884c\u63a8\u7406\u65f6\u8ba1\u7b97\u7684\u57fa\u672c\u74f6\u9888\uff0c\u5e76\u4e3a\u5206\u6790\u6700\u4f18\u63a8\u7406\u957f\u5ea6\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.02979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02979", "abs": "https://arxiv.org/abs/2602.02979", "authors": ["Ran Li", "Zeyuan Liu", "Yinghao chen", "Bingxiang He", "Jiarui Yuan", "Zixuan Fu", "Weize Chen", "Jinyi Hu", "Zhiyuan Liu", "Maosong Sun"], "title": "CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning", "comment": "work in progress", "summary": "Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPM\u00f6bius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPM\u00f6bius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPM\u00f6bius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.", "AI": {"tldr": "CPM\u00f6bius\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u7684\u534f\u4f5c\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u7ec3-\u73a9\u5bb6\u89d2\u8272\u534f\u4f5c\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8fdb\u5c55\u4e25\u91cd\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff08SFT\u6216RL\uff09\uff0c\u8fd9\u79cd\u76d1\u7763\u5bc6\u96c6\u578b\u8bad\u7ec3\u8303\u5f0f\u4e0d\u53ef\u6301\u7eed\u4e14\u5df2\u51fa\u73b0\u6269\u5c55\u6027\u74f6\u9888\u3002\u9700\u8981\u63a2\u7d22\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCPM\u00f6bius\u534f\u4f5c\u6559\u7ec3-\u73a9\u5bb6\u8303\u5f0f\uff1a\u6559\u7ec3\u6839\u636e\u73a9\u5bb6\u80fd\u529b\u751f\u6210\u9488\u5bf9\u6027\u6307\u4ee4\u5e76\u83b7\u5f97\u73a9\u5bb6\u6027\u80fd\u53d8\u5316\u7684\u5956\u52b1\uff0c\u73a9\u5bb6\u901a\u8fc7\u89e3\u51b3\u6559\u7ec3\u751f\u6210\u7684\u9010\u6b65\u63d0\u5347\u96be\u5ea6\u7684\u4efb\u52a1\u83b7\u5f97\u5956\u52b1\u3002\u4e24\u8005\u4f5c\u4e3a\u72ec\u7acb\u4f46\u534f\u4f5c\u7684\u89d2\u8272\u8fdb\u884c\u5408\u4f5c\u4f18\u5316\u5faa\u73af\u3002", "result": "\u5728Qwen2.5-Math-7B-Instruct\u6a21\u578b\u4e0a\uff0cCPM\u00f6bius\u6574\u4f53\u51c6\u786e\u7387\u5e73\u5747\u63d0\u5347+4.9\uff0c\u5206\u5e03\u5916\u51c6\u786e\u7387\u5e73\u5747\u63d0\u5347+5.4\uff0c\u5206\u522b\u8d85\u8fc7RENT\u65b9\u6cd5+1.5\u548cR-zero\u65b9\u6cd5+4.2\u3002", "conclusion": "CPM\u00f6bius\u8bc1\u660e\u4e86\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u663e\u8457\u63d0\u5347LLM\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03419", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03419", "abs": "https://arxiv.org/abs/2602.03419", "authors": ["Shuang Sun", "Huatong Song", "Lisheng Huang", "Jinhao Jiang", "Ran Le", "Zhihao Lv", "Zongchao Chen", "Yiwen Hu", "Wenyang Luo", "Wayne Xin Zhao", "Yang Song", "Hongteng Xu", "Tao Zhang", "Ji-Rong Wen"], "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World", "AI": {"tldr": "SWE-World\u63d0\u51fa\u4e00\u4e2a\u65e0\u9700Docker\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u5b66\u4e60\u578b\u66ff\u4ee3\u6a21\u578b\u9884\u6d4b\u6267\u884c\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u652f\u6301\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4f9d\u8d56\u5bb9\u5668\u5316\u73af\u5883\u7684\u6267\u884c\u53cd\u9988\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u7ef4\u62a4\u56f0\u96be\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u771f\u5b9e\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u6570\u636e\u8bad\u7ec3\uff0c\u9884\u6d4b\u4e2d\u95f4\u6267\u884c\u7ed3\u679c\u548c\u6700\u7ec8\u6d4b\u8bd5\u53cd\u9988\uff0c\u65e0\u9700\u7269\u7406\u5bb9\u5668\u73af\u5883\u3002", "result": "\u5728SWE-bench Verified\u4e0a\uff0cQwen2.5-Coder-32B\u4ece6.2%\u63d0\u5347\u523052.0%\uff08\u65e0Docker SFT\uff09\u300155.0%\uff08\u65e0Docker RL\uff09\u548c68.2%\uff08\u8fdb\u4e00\u6b65TTS\uff09\u3002", "conclusion": "SWE-World\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5bb9\u5668\u5316\u73af\u5883\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "topic": "swe application"}}
{"id": "2602.03036", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03036", "abs": "https://arxiv.org/abs/2602.03036", "authors": ["Muxin Fu", "Guibin Zhang", "Xiangyuan Xue", "Yafu Li", "Zefeng He", "Siyuan Huang", "Xiaoye Qu", "Yu Cheng", "Yang Yang"], "title": "LatentMem: Customizing Latent Memory for Multi-Agent Systems", "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.", "AI": {"tldr": "LatentMem\u662f\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u611f\u77e5\u5b9a\u5236\u548c\u7d27\u51d1\u6f5c\u5728\u8868\u793a\u89e3\u51b3\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u540c\u8d28\u5316\u548c\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u74f6\u9888\uff1a1) \u7531\u4e8e\u7f3a\u4e4f\u89d2\u8272\u611f\u77e5\u5b9a\u5236\u5bfc\u81f4\u8bb0\u5fc6\u540c\u8d28\u5316\uff1b2) \u8fc7\u4e8e\u7ec6\u7c92\u5ea6\u7684\u8bb0\u5fc6\u6761\u76ee\u5bfc\u81f4\u4fe1\u606f\u8fc7\u8f7d\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u673a\u5236\u3002", "method": "\u63d0\u51faLatentMem\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u7ecf\u9a8c\u5e93\u4ee5\u8f7b\u91cf\u5f62\u5f0f\u5b58\u50a8\u539f\u59cb\u4ea4\u4e92\u8f68\u8ff9\uff1b2) \u8bb0\u5fc6\u5408\u6210\u5668\u6839\u636e\u68c0\u7d22\u7684\u7ecf\u9a8c\u548c\u667a\u80fd\u4f53\u7279\u5b9a\u4e0a\u4e0b\u6587\u5408\u6210\u7d27\u51d1\u7684\u6f5c\u5728\u8bb0\u5fc6\u3002\u8fd8\u63d0\u51faLatent Memory Policy Optimization (LMPO)\uff0c\u901a\u8fc7\u6f5c\u5728\u8bb0\u5fc6\u4f20\u64ad\u4efb\u52a1\u7ea7\u4f18\u5316\u4fe1\u53f7\uff0c\u9f13\u52b1\u751f\u6210\u7d27\u51d1\u4e14\u9ad8\u5b9e\u7528\u6027\u7684\u8868\u793a\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e3b\u6d41MAS\u6846\u67b6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLatentMem\u76f8\u6bd4\u539f\u59cb\u8bbe\u7f6e\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe19.36%\uff0c\u5e76\u4e14\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u8bb0\u5fc6\u67b6\u6784\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u6846\u67b6\u3002", "conclusion": "LatentMem\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u89d2\u8272\u611f\u77e5\u8bb0\u5fc6\u5b9a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u540c\u8d28\u5316\u548c\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u96c6\u4f53\u667a\u80fd\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2602.03462", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.03462", "abs": "https://arxiv.org/abs/2602.03462", "authors": ["Ruwei Pan", "Yakun Zhang", "Qingyuan Liang", "Yueheng Zhu", "Chao Liu", "Lu Zhang", "Hongyu Zhang"], "title": "RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes", "comment": null, "summary": "Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .", "AI": {"tldr": "RAL-Bench\u662f\u4e00\u4e2a\u5e94\u7528\u7ea7\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30LLM\u751f\u6210\u591a\u6587\u4ef6\u53ef\u8fd0\u884c\u4ed3\u5e93\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u529f\u80fd\u6b63\u786e\u6027\u548c\u975e\u529f\u80fd\u6027\u8d28\u91cf\uff08\u53ef\u7ef4\u62a4\u6027\u3001\u5b89\u5168\u6027\u7b49\uff09\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u5e94\u7528\u7ea7\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u65b9\u9762\u6709\u9650\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\u548c\u975e\u529f\u80fd\u6027\u8d28\u91cf\u3002\u9700\u8981\u4e86\u89e3\u5f53\u524dLLM\u662f\u5426\u80fd\u751f\u6210\u6ee1\u8db3\u8fd9\u4e24\u65b9\u9762\u8981\u6c42\u7684\u5e94\u7528\u7ea7\u4ee3\u7801\u4ed3\u5e93\u3002", "method": "\u4ece\u9ad8\u8d28\u91cf\u53c2\u8003\u9879\u76ee\u4e2d\u63d0\u70bc\u81ea\u7136\u8bed\u8a00\u9700\u6c42\uff0c\u6784\u5efa\u8986\u76d6\u529f\u80fd\u548c\u975e\u529f\u80fd\u5c5e\u6027\u7684\u9ed1\u76d2\u7cfb\u7edf\u6d4b\u8bd5\uff0c\u53ea\u4fdd\u7559\u5728\u53c2\u8003\u4ed3\u5e93\u4e0a\u901a\u8fc7\u7684\u6d4b\u8bd5\u4ee5\u786e\u4fdd\u53ef\u9760\u57fa\u51c6\u3002\u529f\u80fd\u6b63\u786e\u6027\u901a\u8fc7\u7cfb\u7edf\u6d4b\u8bd5\u901a\u8fc7\u7387\u8861\u91cf\uff0c\u975e\u529f\u80fd\u6027\u8d28\u91cf\u57fa\u4e8eISO/IEC 25010\u7684\u4e94\u4e2a\u7ef4\u5ea6\uff0c\u4f7f\u7528\u5c42\u6b21\u5206\u6790\u6cd5\u52a0\u6743\u805a\u5408\u3002", "result": "\u572816\u4e2aLLM\u7684\u96f6\u6837\u672c\u8d2a\u5a6a\u89e3\u7801\u8bc4\u4f30\u4e2d\uff0c\u529f\u80fd\u6b63\u786e\u6027\u662f\u4e3b\u8981\u74f6\u9888\uff1a\u6ca1\u6709\u6a21\u578b\u5728\u9700\u6c42\u9a71\u52a8\u3001\u53c2\u8003\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u4e2d\u8d85\u8fc745%\u7684\u529f\u80fd\u901a\u8fc7\u7387\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u5e94\u7528\u7ea7\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u3002RAL-Bench\u4e3a\u8bc4\u4f30\u5e94\u7528\u7ea7\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002", "topic": "swe benchmark"}}
{"id": "2602.03075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03075", "abs": "https://arxiv.org/abs/2602.03075", "authors": ["Junjie Huang", "Jiarui Qin", "Di Yin", "Weiwen Liu", "Yong Yu", "Xing Sun", "Weinan Zhang"], "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution", "comment": "25 pages", "summary": "Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.", "AI": {"tldr": "ReMiT\u63d0\u51fa\u53cc\u5411\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528RL\u8c03\u4f18\u6a21\u578b\u7684\u63a8\u7406\u5148\u9a8c\u52a8\u6001\u91cd\u52a0\u6743\u9884\u8bad\u7ec3\u4e2d\u7684\u5173\u952etoken\uff0c\u5efa\u7acb\u81ea\u6211\u589e\u5f3a\u7684\u98de\u8f6e\u6548\u5e94", "motivation": "\u4f20\u7edfLLM\u8bad\u7ec3\u662f\u5355\u5411\u7684\uff08\u9884\u8bad\u7ec3\u2192\u540e\u8bad\u7ec3\uff09\uff0c\u4f46\u540e\u8bad\u7ec3\u7684\u6d1e\u5bdf\u80fd\u5426\u53cd\u8fc7\u6765\u6539\u8fdb\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5c1a\u672a\u63a2\u7d22\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u81ea\u6211\u589e\u5f3a\u7684\u98de\u8f6e\u5faa\u73af\uff0c\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u7684\u6559\u5e08\u6216\u53c2\u8003\u6a21\u578b", "method": "\u5206\u6790\u8bad\u7ec3\u52a8\u6001\uff0c\u53d1\u73b0\u4e2d\u671f\u8bad\u7ec3\uff08\u9000\u706b\uff09\u9636\u6bb5\u662f\u6a21\u578b\u80fd\u529b\u7684\u5173\u952e\u8f6c\u6298\u70b9\u3002\u63d0\u51faReMiT\u65b9\u6cd5\uff0c\u5229\u7528RL\u8c03\u4f18\u6a21\u578b\u7684\u63a8\u7406\u5148\u9a8c\u5728\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\u52a8\u6001\u91cd\u52a0\u6743token\uff0c\u4f18\u5148\u5904\u7406\u5bf9\u63a8\u7406\u81f3\u5173\u91cd\u8981\u7684token", "result": "\u572810\u4e2a\u9884\u8bad\u7ec3\u57fa\u51c6\uff08\u6570\u5b66\u3001\u4ee3\u7801\u548c\u901a\u7528\u63a8\u7406\uff09\u4e0a\u5e73\u5747\u63d0\u53473%\uff0c\u5e76\u5728\u6574\u4e2a\u540e\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u4fdd\u6301\u8d85\u8fc72%\u7684\u589e\u76ca\uff0c\u9a8c\u8bc1\u4e86\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u7684\u6709\u6548\u6027", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e86\u53cc\u5411\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86LLM\u7684\u6301\u7eed\u81ea\u6211\u589e\u5f3a\u6f14\u5316\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u5355\u5411\u8bad\u7ec3\u7684\u9650\u5236", "topic": "agentic reinforcement learning"}}
{"id": "2602.03556", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.03556", "abs": "https://arxiv.org/abs/2602.03556", "authors": ["Alexander Berndt", "Thomas Bach", "Sebastian Baltes"], "title": "Flaky Tests in a Large Industrial Database Management System: An Empirical Study of Fixed Issue Reports for SAP HANA", "comment": "8 pages, 2 tables, 5 figures, 3rd International Flaky Tests Workshop 2026 (FTW 2026)", "summary": "Flaky tests yield different results when executed multiple times for the same version of the source code. Thus, they provide an ambiguous signal about the quality of the code and interfere with the automated assessment of code changes. While a variety of factors can cause test flakiness, approaches to fix flaky tests are typically tailored to address specific causes. However, the prevalent root causes of flaky tests can vary depending on the programming language, application domain, or size of the software project. Since manually labeling flaky tests is time-consuming and tedious, this work proposes an LLMs-as-annotators approach that leverages intra- and inter-model consistency to label issue reports related to fixed flakiness issues with the relevant root cause category. This allows us to gain an overview of prevalent flakiness categories in the issue reports. We evaluated our labeling approach in the context of SAP HANA, a large industrial database management system. Our results suggest that SAP HANA's tests most commonly suffer from issues related to concurrency (23%, 130 of 559 analyzed issue reports). Moreover, our results suggest that different test types face different flakiness challenges. Therefore, we encourage future research on flakiness mitigation to consider evaluating the generalizability of proposed approaches across different test types.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528LLMs\u4f5c\u4e3a\u6807\u6ce8\u5668\uff0c\u901a\u8fc7\u6a21\u578b\u5185\u548c\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u6765\u6807\u6ce8\u4e0e\u5df2\u4fee\u590d\u7684flaky\u6d4b\u8bd5\u76f8\u5173\u7684issue\u62a5\u544a\uff0c\u4ee5\u8bc6\u522bSAP HANA\u4e2dflaky\u6d4b\u8bd5\u7684\u4e3b\u8981\u6839\u56e0\u7c7b\u522b\u3002", "motivation": "Flaky\u6d4b\u8bd5\uff08\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff09\u5728\u76f8\u540c\u6e90\u4ee3\u7801\u7248\u672c\u591a\u6b21\u6267\u884c\u65f6\u4ea7\u751f\u4e0d\u540c\u7ed3\u679c\uff0c\u5e72\u6270\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u548c\u81ea\u52a8\u5316\u4ee3\u7801\u53d8\u66f4\u8bc4\u4f30\u3002\u624b\u52a8\u6807\u6ce8flaky\u6d4b\u8bd5\u8017\u65f6\u4e14\u7e41\u7410\uff0c\u9700\u8981\u4e86\u89e3\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u3001\u5e94\u7528\u9886\u57df\u6216\u8f6f\u4ef6\u9879\u76ee\u89c4\u6a21\u4e0bflaky\u6d4b\u8bd5\u7684\u4e3b\u8981\u6839\u56e0\u3002", "method": "\u91c7\u7528LLMs-as-annotators\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u5185\u548c\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u6765\u6807\u6ce8\u4e0e\u5df2\u4fee\u590d\u7684flakiness\u95ee\u9898\u76f8\u5173\u7684issue\u62a5\u544a\uff0c\u8bc6\u522b\u76f8\u5173\u7684\u6839\u56e0\u7c7b\u522b\u3002\u5728SAP HANA\uff08\u5927\u578b\u5de5\u4e1a\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\uff09\u7684\u80cc\u666f\u4e0b\u8bc4\u4f30\u8be5\u6807\u6ce8\u65b9\u6cd5\u3002", "result": "\u5728\u5206\u6790\u7684559\u4e2aissue\u62a5\u544a\u4e2d\uff0cSAP HANA\u7684\u6d4b\u8bd5\u6700\u5e38\u89c1\u7684\u95ee\u9898\u662f\u5e76\u53d1\u76f8\u5173\uff0823%\uff0c130\u4e2a\u62a5\u544a\uff09\u3002\u4e0d\u540c\u6d4b\u8bd5\u7c7b\u578b\u9762\u4e34\u4e0d\u540c\u7684flakiness\u6311\u6218\u3002", "conclusion": "\u9f13\u52b1\u672a\u6765\u5173\u4e8eflakiness\u7f13\u89e3\u7684\u7814\u7a76\u8003\u8651\u8bc4\u4f30\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u540c\u6d4b\u8bd5\u7c7b\u578b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002LLMs\u6807\u6ce8\u65b9\u6cd5\u6709\u52a9\u4e8e\u8bc6\u522b\u5de5\u4e1a\u7cfb\u7edf\u4e2dflaky\u6d4b\u8bd5\u7684\u4e3b\u8981\u6839\u56e0\u6a21\u5f0f\u3002", "topic": "swe benchmark"}}
{"id": "2602.02978", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02978", "abs": "https://arxiv.org/abs/2602.02978", "authors": ["Zuyuan Zhang", "Zeyu Fang", "Tian Lan"], "title": "Structuring Value Representations via Geometric Coherence in Markov Decision Processes", "comment": null, "summary": "Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \\emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.", "AI": {"tldr": "GCR-RL\uff1a\u4e00\u79cd\u57fa\u4e8e\u5e8f\u7406\u8bba\u7684\u51e0\u4f55\u76f8\u5e72\u6b63\u5219\u5316\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u91cd\u6784\u4e3a\u5b66\u4e60\u504f\u5e8f\u96c6\uff0c\u5229\u7528\u8d85\u504f\u5e8f\u96c6\u7ec6\u5316\u786e\u4fdd\u51e0\u4f55\u76f8\u5e72\u6027\uff0c\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5229\u7528\u51e0\u4f55\u7279\u6027\uff08\u5982\u5bf9\u79f0\u7ed3\u6784\u3001\u51e0\u4f55\u611f\u77e5\u6570\u636e\u589e\u5f3a\u3001\u7ed3\u6784\u9650\u5236\uff09\u6765\u7a33\u5b9a\u548c\u52a0\u901f\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u7f3a\u4e4f\u4ece\u5e8f\u7406\u8bba\u89d2\u5ea6\u7cfb\u7edf\u5229\u7528\u51e0\u4f55\u76f8\u5e72\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGCR-RL\u6846\u67b6\uff0c\u5c06\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u91cd\u6784\u4e3a\u5b66\u4e60\u504f\u5e8f\u96c6\uff0c\u901a\u8fc7\u8ba1\u7b97\u8d85\u504f\u5e8f\u96c6\u7ec6\u5316\u5e8f\u5217\uff08\u7ec6\u5316\u5148\u524d\u504f\u5e8f\u96c6\u5e76\u4ece\u65f6\u5e8f\u5dee\u5206\u4fe1\u53f7\u5b66\u4e60\u989d\u5916\u5e8f\u5173\u7cfb\uff09\u786e\u4fdd\u51e0\u4f55\u76f8\u5e72\u6027\u3002\u5f00\u53d1\u4e86\u57fa\u4e8eQ-learning\u548cactor-critic\u7684\u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\u5b9e\u73b0\u8d85\u504f\u5e8f\u96c6\u7ec6\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e86\u7b97\u6cd5\u7684\u6027\u8d28\u548c\u6536\u655b\u901f\u7387\u3002\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cGCR-RL\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5728\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u80fd\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u4ece\u5e8f\u7406\u8bba\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u51fa\u7684GCR-RL\u6846\u67b6\u901a\u8fc7\u786e\u4fdd\u51e0\u4f55\u76f8\u5e72\u6027\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u5229\u7528\u51e0\u4f55\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03084", "abs": "https://arxiv.org/abs/2602.03084", "authors": ["Zhitao Gao", "Jie Ma", "Xuhong Li", "Pengyu Li", "Ning Qu", "Yaqiang Wu", "Hui Liu", "Jun Liu"], "title": "AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback", "comment": null, "summary": "Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \\underline{A}utonomous \\underline{E}volutionary \\underline{R}easoning \\underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \\textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\\% on Qwen3-4B-Base and 5.10\\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.", "AI": {"tldr": "AERO\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u81ea\u4e3b\u63a8\u7406\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u7cfb\u7edf\u5b9e\u73b0\u81ea\u6211\u63d0\u95ee\u3001\u56de\u7b54\u548c\u6279\u8bc4\uff0c\u57fa\u4e8eZPD\u7406\u8bba\u5229\u7528\u71b5\u5b9a\u4f4d\u89e3\u51b3\"\u53ef\u89e3\u6027\u5dee\u8ddd\"\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728\u590d\u6742\u63a8\u7406\u4e2d\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u548c\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u800c\u73b0\u6709\u7684\u81ea\u6211\u8fdb\u5316\u8303\u5f0f\u5f80\u5f80\u65e0\u6cd5\u627e\u5230\u6700\u4f73\u5b66\u4e60\u533a\u57df\uff0c\u5e76\u4e14\u53ef\u80fd\u901a\u8fc7\u6709\u7f3a\u9677\u7684\u5185\u90e8\u53cd\u9988\u5f3a\u5316\u96c6\u4f53\u5e7b\u89c9\u548c\u9519\u8bef\u5148\u9a8c\u3002", "method": "\u63d0\u51faAERO\u6846\u67b6\uff1a1) \u57fa\u4e8eZPD\u7406\u8bba\u4f7f\u7528\u71b5\u5b9a\u4f4d\u6765\u9488\u5bf9\"\u53ef\u89e3\u6027\u5dee\u8ddd\"\uff1b2) \u91c7\u7528\u72ec\u7acb\u53cd\u4e8b\u5b9e\u6821\u6b63\u8fdb\u884c\u9c81\u68d2\u9a8c\u8bc1\uff1b3) \u5f15\u5165\u4ea4\u9519\u8bad\u7ec3\u7b56\u7565\u6765\u540c\u6b65\u529f\u80fd\u89d2\u8272\u80fd\u529b\u589e\u957f\u5e76\u9632\u6b62\u8bfe\u7a0b\u5d29\u6e83\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d63\u4e2a\u9886\u57df\uff09\u4e2d\uff0cAERO\u5728Qwen3-4B-Base\u4e0a\u5e73\u5747\u63d0\u53474.57%\uff0c\u5728Qwen3-8B-Base\u4e0a\u5e73\u5747\u63d0\u53475.10%\uff0c\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AERO\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u81ea\u4e3b\u63a8\u7406\u8fdb\u5316\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03557", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.03557", "abs": "https://arxiv.org/abs/2602.03557", "authors": ["Yunhao Liang", "Ruixuan Ying", "Shiwen Ni", "Zhe Cui"], "title": "Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study", "comment": null, "summary": "Test-driven development (TDD) has been adopted to improve Large Language Model (LLM)-based code generation by using tests as executable specifications. However, existing TDD-style code generation studies are largely limited to function-level tasks, leaving class-level synthesis where multiple methods interact through shared state and call dependencies underexplored. In this paper, we scale test-driven code generation from functions to classes via an iterative TDD framework. Our approach first analyzes intra-class method dependencies to derive a feasible generation schedule, and then incrementally implements each method under method-level public tests with reflection-style execution feedback and bounded repair iterations. To support test-driven generation and rigorous class-level evaluation, we construct ClassEval-TDD, a cleaned and standardized variant of ClassEval with consistent specifications, deterministic test environments, and complete method-level public tests. We conduct an empirical study across eight LLMs and compare against the strongest direct-generation baseline (the best of holistic, incremental, and compositional strategies). Our class-level TDD framework consistently improves class-level correctness by 12 to 26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability. All code and data are available at https://anonymous.4open.science/r/ClassEval-TDD-C4C9/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u4ece\u51fd\u6570\u7ea7\u6269\u5c55\u5230\u7c7b\u7ea7\u7684\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u7c7b\u5185\u65b9\u6cd5\u4f9d\u8d56\u5173\u7cfb\u5236\u5b9a\u751f\u6210\u8ba1\u5212\uff0c\u5e76\u5229\u7528\u53cd\u5c04\u5f0f\u6267\u884c\u53cd\u9988\u548c\u6709\u9650\u4fee\u590d\u8fed\u4ee3\u6765\u589e\u91cf\u5b9e\u73b0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709TDD\u98ce\u683c\u7684\u4ee3\u7801\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u51fd\u6570\u7ea7\u4efb\u52a1\uff0c\u800c\u7c7b\u7ea7\u5408\u6210\uff08\u591a\u4e2a\u65b9\u6cd5\u901a\u8fc7\u5171\u4eab\u72b6\u6001\u548c\u8c03\u7528\u4f9d\u8d56\u4ea4\u4e92\uff09\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5c06\u6d4b\u8bd5\u9a71\u52a8\u4ee3\u7801\u751f\u6210\u4ece\u51fd\u6570\u6269\u5c55\u5230\u7c7b\uff0c\u4ee5\u63d0\u9ad8\u7c7b\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3TDD\u6846\u67b6\uff1a1) \u5206\u6790\u7c7b\u5185\u65b9\u6cd5\u4f9d\u8d56\u5173\u7cfb\u4ee5\u5236\u5b9a\u53ef\u884c\u7684\u751f\u6210\u8ba1\u5212\uff1b2) \u5728\u65b9\u6cd5\u7ea7\u516c\u5171\u6d4b\u8bd5\u4e0b\u589e\u91cf\u5b9e\u73b0\u6bcf\u4e2a\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cd\u5c04\u5f0f\u6267\u884c\u53cd\u9988\u548c\u6709\u9650\u4fee\u590d\u8fed\u4ee3\uff1b3) \u6784\u5efaClassEval-TDD\u57fa\u51c6\uff0c\u5305\u542b\u4e00\u81f4\u7684\u89c4\u8303\u3001\u786e\u5b9a\u6027\u6d4b\u8bd5\u73af\u5883\u548c\u5b8c\u6574\u7684\u65b9\u6cd5\u7ea7\u516c\u5171\u6d4b\u8bd5\u3002", "result": "\u57288\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7c7b\u7ea7TDD\u6846\u67b6\u5c06\u7c7b\u7ea7\u6b63\u786e\u6027\u63d0\u9ad8\u4e8612-26\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\uff0c\u8fbe\u5230\u6700\u9ad871%\u5b8c\u5168\u6b63\u786e\u7684\u7c7b\uff0c\u5e73\u5747\u53ea\u9700\u8981\u5c11\u91cf\u4fee\u590d\u3002\u76f8\u6bd4\u6700\u5f3a\u7684\u76f4\u63a5\u751f\u6210\u57fa\u7ebf\uff08\u6574\u4f53\u3001\u589e\u91cf\u3001\u7ec4\u5408\u7b56\u7565\u4e2d\u7684\u6700\u4f73\u8005\uff09\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u6d4b\u8bd5\u9a71\u52a8\u751f\u6210\u53ef\u4ee5\u6709\u6548\u6269\u5c55\u5230\u5b64\u7acb\u51fd\u6570\u4e4b\u5916\uff0c\u663e\u8457\u63d0\u9ad8\u7c7b\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u3002TDD\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u7c7b\u5185\u65b9\u6cd5\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u4e3a\u7c7b\u7ea7\u4ee3\u7801\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2602.02545", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02545", "abs": "https://arxiv.org/abs/2602.02545", "authors": ["Dayu Wang", "Jiaye Yang", "Weikang Li", "Jiahui Liang", "Yang Li"], "title": "Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). However, recent studies question whether RL genuinely expands reasoning capacity or merely aligns existing latent capabilities, arguing that exploration remains confined within the pre-trained model's low-rank bias manifold. In this work, we challenge this accessibility boundary hypothesis by demonstrating that the latent reasoning space can be fundamentally expanded through targeted geometric interventions. We propose Manifold-Reshaping Policy Optimization (MRPO), a geometric framework designed to fundamentally restructure the inference space of LLMs. MRPO operates in two stages: first, we employ Spectral Orthogonal Exploration (SOE) to eject the policy initialization into the null space of the bias manifold; second, we integrate an Effective Rank regularization term into the policy optimization objective. This approach incentivizes the discovery and maintenance of high-dimensional reasoning trajectories against the entropy-reducing tendency of standard RL. Empirically, our 4B-parameter method achieves state-of-the-art performance on mathematical tasks, significantly outperforming larger models (e.g., Qwen3-32B) and expanding the capability boundary beyond standard GRPO. Our code is available at https://anonymous.4open.science/r/MRPO-D57B/", "AI": {"tldr": "MRPO\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u6b63\u4ea4\u63a2\u7d22\u548c\u6709\u6548\u79e9\u6b63\u5219\u5316\u6765\u91cd\u5851LLM\u7684\u63a8\u7406\u7a7a\u95f4\uff0c\u7a81\u7834\u4e86\u4f20\u7edfRL\u5728\u4f4e\u79e9\u504f\u5dee\u6d41\u5f62\u5185\u7684\u63a2\u7d22\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u8d28\u7591RL\u662f\u5426\u771f\u6b63\u6269\u5c55\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5bf9\u9f50\u4e86\u5df2\u6709\u7684\u6f5c\u5728\u80fd\u529b\u3002\u4f5c\u8005\u8ba4\u4e3a\u63a2\u7d22\u88ab\u9650\u5236\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f4e\u79e9\u504f\u5dee\u6d41\u5f62\u5185\uff0c\u9700\u8981\u7a81\u7834\u8fd9\u79cd\u53ef\u8bbf\u95ee\u6027\u8fb9\u754c\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86\u6d41\u5f62\u91cd\u5851\u7b56\u7565\u4f18\u5316\uff08MRPO\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u8c31\u6b63\u4ea4\u63a2\u7d22\uff08SOE\uff09\u5c06\u7b56\u7565\u521d\u59cb\u5316\u5f39\u5c04\u5230\u504f\u5dee\u6d41\u5f62\u7684\u96f6\u7a7a\u95f4\uff1b2\uff09\u5728\u7b56\u7565\u4f18\u5316\u76ee\u6807\u4e2d\u96c6\u6210\u6709\u6548\u79e9\u6b63\u5219\u5316\u9879\uff0c\u6fc0\u52b1\u53d1\u73b0\u548c\u7ef4\u62a4\u9ad8\u7ef4\u63a8\u7406\u8f68\u8ff9\u3002", "result": "4B\u53c2\u6570\u7684\u65b9\u6cd5\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff08\u5982Qwen3-32B\uff09\uff0c\u5e76\u5c06\u80fd\u529b\u8fb9\u754c\u6269\u5c55\u5230\u6807\u51c6GRPO\u4e4b\u5916\u3002", "conclusion": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u51e0\u4f55\u5e72\u9884\u53ef\u4ee5\u6839\u672c\u4e0a\u6269\u5c55LLM\u7684\u6f5c\u5728\u63a8\u7406\u7a7a\u95f4\uff0cMRPO\u6846\u67b6\u80fd\u591f\u91cd\u5851\u63a8\u7406\u6d41\u5f62\uff0c\u7a81\u7834\u4f20\u7edfRL\u7684\u9650\u5236\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02983", "abs": "https://arxiv.org/abs/2602.02983", "authors": ["Hanna M. Dettki", "Charley M. Wu", "Bob Rehder"], "title": "Are LLMs Biased Like Humans? Causal Reasoning as a Function of Prior Knowledge, Irrelevant Information, and Reasoning Budget", "comment": null, "summary": "Large language models (LLMs) are increasingly used in domains where causal reasoning matters, yet it remains unclear whether their judgments reflect normative causal computation, human-like shortcuts, or brittle pattern matching. We benchmark 20+ LLMs against a matched human baseline on 11 causal judgment tasks formalized by a collider structure ($C_1 \\!\\rightarrow\\! E\\! \\leftarrow \\!C_2$). We find that a small interpretable model compresses LLMs' causal judgments well and that most LLMs exhibit more rule-like reasoning strategies than humans who seem to account for unmentioned latent factors in their probability judgments. Furthermore, most LLMs do not mirror the characteristic human collider biases of weak explaining away and Markov violations. We probe LLMs' causal judgment robustness under (i) semantic abstraction and (ii) prompt overloading (injecting irrelevant text), and find that chain-of-thought (CoT) increases robustness for many LLMs. Together, this divergence suggests LLMs can complement humans when known biases are undesirable, but their rule-like reasoning may break down when uncertainty is intrinsic -- highlighting the need to characterize LLM reasoning strategies for safe, effective deployment.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8620\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u6bd4\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u89c4\u5219\u5316\u63a8\u7406\uff0c\u8f83\u5c11\u8868\u73b0\u51fa\u4eba\u7c7b\u7279\u6709\u7684\u504f\u89c1\uff0c\u4f46\u53ef\u80fd\u5728\u4e0d\u786e\u5b9a\u6027\u60c5\u5883\u4e0b\u5931\u6548\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u9700\u8981\u56e0\u679c\u63a8\u7406\u7684\u9886\u57df\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u7684\u5224\u65ad\u662f\u57fa\u4e8e\u89c4\u8303\u7684\u56e0\u679c\u8ba1\u7b97\u3001\u4eba\u7c7b\u5f0f\u7684\u6377\u5f84\u8fd8\u662f\u8106\u5f31\u7684\u6a21\u5f0f\u5339\u914d\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30LLMs\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u53ca\u5176\u4e0e\u4eba\u7c7b\u63a8\u7406\u7684\u5dee\u5f02\u3002", "method": "\u572811\u4e2a\u57fa\u4e8e\u78b0\u649e\u5668\u7ed3\u6784\uff08C\u2081\u2192E\u2190C\u2082\uff09\u7684\u56e0\u679c\u5224\u65ad\u4efb\u52a1\u4e0a\uff0c\u5bf920\u591a\u4e2aLLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u5339\u914d\u7684\u4eba\u7c7b\u57fa\u7ebf\u6bd4\u8f83\u3002\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u5c0f\u6a21\u578b\u538b\u7f29LLMs\u7684\u56e0\u679c\u5224\u65ad\uff0c\u5206\u6790\u63a8\u7406\u7b56\u7565\u3002\u6d4b\u8bd5LLMs\u5728\u8bed\u4e49\u62bd\u8c61\u548c\u63d0\u793a\u8fc7\u8f7d\uff08\u6ce8\u5165\u65e0\u5173\u6587\u672c\uff09\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8bc4\u4f30\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5927\u591a\u6570LLMs\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u89c4\u5219\u5316\u7684\u63a8\u7406\u7b56\u7565\uff0c\u4eba\u7c7b\u5728\u6982\u7387\u5224\u65ad\u4e2d\u4f3c\u4e4e\u8003\u8651\u4e86\u672a\u63d0\u53ca\u7684\u6f5c\u5728\u56e0\u7d20\u3002\u5927\u591a\u6570LLMs\u6ca1\u6709\u8868\u73b0\u51fa\u4eba\u7c7b\u7279\u6709\u7684\u78b0\u649e\u5668\u504f\u89c1\uff08\u5f31\u89e3\u91ca\u6d88\u9664\u548c\u9a6c\u5c14\u53ef\u592b\u8fdd\u89c4\uff09\u3002\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u9ad8\u4e86\u8bb8\u591aLLMs\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LLMs\u4e0e\u4eba\u7c7b\u5728\u56e0\u679c\u63a8\u7406\u4e0a\u7684\u5dee\u5f02\u8868\u660e\uff0c\u5f53\u5df2\u77e5\u7684\u4eba\u7c7b\u504f\u89c1\u4e0d\u53ef\u53d6\u65f6\uff0cLLMs\u53ef\u4ee5\u8865\u5145\u4eba\u7c7b\u5224\u65ad\u3002\u4f46\u5b83\u4eec\u7684\u89c4\u5219\u5316\u63a8\u7406\u5728\u4e0d\u786e\u5b9a\u6027\u60c5\u5883\u4e0b\u53ef\u80fd\u5931\u6548\uff0c\u8fd9\u51f8\u663e\u4e86\u9700\u8981\u8868\u5f81LLMs\u63a8\u7406\u7b56\u7565\u4ee5\u786e\u4fdd\u5b89\u5168\u6709\u6548\u90e8\u7f72\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.03094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03094", "abs": "https://arxiv.org/abs/2602.03094", "authors": ["Yufan Zhuang", "Chandan Singh", "Liyuan Liu", "Yelong Shen", "Dinghuai Zhang", "Jingbo Shang", "Jianfeng Gao", "Weizhu Chen"], "title": "Test-time Recursive Thinking: Self-Improvement without External Feedback", "comment": null, "summary": "Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.", "AI": {"tldr": "TRT\u6846\u67b6\u8ba9LLMs\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u9012\u5f52\u601d\u8003\u81ea\u6211\u6539\u8fdb\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5728AIME-25/24\u4e0a\u8fbe\u5230100%\u51c6\u786e\u7387", "motivation": "\u63a2\u7d22LLMs\u80fd\u5426\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u81ea\u6211\u6539\u8fdb\uff0c\u89e3\u51b3\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u9ad8\u6548\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u5019\u9009\u65b9\u6848\uff0c\u4ee5\u53ca\u5728\u7f3a\u4e4f\u771f\u5b9e\u76d1\u7763\u65f6\u53ef\u9760\u9009\u62e9\u6b63\u786e\u7b54\u6848", "method": "\u63d0\u51faTest-time Recursive Thinking (TRT)\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u8fed\u4ee3\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7279\u5b9a\u7b56\u7565\u3001\u7d2f\u79ef\u77e5\u8bc6\u548c\u81ea\u751f\u6210\u9a8c\u8bc1\u4fe1\u53f7\u8fdb\u884c\u6761\u4ef6\u751f\u6210", "result": "\u5f00\u6e90\u6a21\u578b\u5728AIME-25/24\u4e0a\u8fbe\u5230100%\u51c6\u786e\u7387\uff1b\u95ed\u6e90\u6a21\u578b\u5728LiveCodeBench\u6700\u96be\u95ee\u9898\u4e0a\u63d0\u534710.4-14.8\u4e2a\u767e\u5206\u70b9\uff0c\u65e0\u9700\u5916\u90e8\u53cd\u9988", "conclusion": "TRT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u6d4b\u8bd5\u65f6\u81ea\u6211\u6539\u8fdb\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u81ea\u6211\u6539\u8fdb\u6f5c\u529b", "topic": "agent analysis"}}
{"id": "2602.02995", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02995", "abs": "https://arxiv.org/abs/2602.02995", "authors": ["Sizhe Tang", "Rongqian Chen", "Tian Lan"], "title": "Agent Alpha: Tree Search Unifying Generation, Exploration and Evaluation for Computer-Use Agents", "comment": null, "summary": "While scaling test-time compute through trajectory-level sampling has significantly improved Graphical User Interface (GUI) agents, the lack of regressive ability prevents the reuse of partial successes and the recovery from early missteps. In this paper, we introduce Agent Alpha, a unified framework that synergizes generation, exploration, and evaluation through step-level Monte Carlo Tree Search (MCTS). It enables active modeling or exploiting structures of the planning space. By integrating alpha-UCT guided search into the interaction loop, Agent Alpha enables deliberate planning, facilitating early pruning of suboptimal branches and efficient prefix reuse. We also employ comparison-driven evaluation to mitigate absolute scoring biases and diversity-constrained expansion to maintain a compact, informative search space. Regret bound of alpha-UCT is analyzed. On the OSWorld benchmark, Agent Alpha achieves a state-of-the-art success rate of $\\sim 77\\%$, significantly outperforming trajectory-level baselines under equivalent compute.", "AI": {"tldr": "Agent Alpha\u662f\u4e00\u4e2aGUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6b65\u7ea7MCTS\u7ed3\u5408\u751f\u6210\u3001\u63a2\u7d22\u548c\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e3b\u52a8\u89c4\u5212\u3001\u65e9\u671f\u526a\u679d\u548c\u524d\u7f00\u91cd\u7528\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u523077%\u6210\u529f\u7387", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u901a\u8fc7\u8f68\u8ff9\u7ea7\u91c7\u6837\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff0c\u4f46\u7f3a\u4e4f\u56de\u5f52\u80fd\u529b\uff0c\u65e0\u6cd5\u91cd\u7528\u90e8\u5206\u6210\u529f\u7ed3\u679c\u6216\u4ece\u65e9\u671f\u9519\u8bef\u4e2d\u6062\u590d", "method": "\u63d0\u51faAgent Alpha\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u6b65\u7ea7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\uff0c\u96c6\u6210alpha-UCT\u5f15\u5bfc\u641c\u7d22\u3001\u6bd4\u8f83\u9a71\u52a8\u8bc4\u4f30\u548c\u591a\u6837\u6027\u7ea6\u675f\u6269\u5c55", "result": "\u5728OSWorld\u57fa\u51c6\u4e0a\u8fbe\u5230\u7ea677%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u540c\u7b49\u8ba1\u7b97\u91cf\u4e0b\u7684\u8f68\u8ff9\u7ea7\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "Agent Alpha\u901a\u8fc7\u6b65\u7ea7MCTS\u5b9e\u73b0\u4e86\u4e3b\u52a8\u89c4\u5212\u3001\u65e9\u671f\u526a\u679d\u548c\u9ad8\u6548\u524d\u7f00\u91cd\u7528\uff0c\u4e3aGUI\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u65b9\u6cd5", "topic": "agent analysis"}}
{"id": "2602.02548", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02548", "abs": "https://arxiv.org/abs/2602.02548", "authors": ["Xiaoce Wang", "Guibin Zhang", "Junzhe Li", "Jinzhe Tu", "Chun Li", "Ming Li"], "title": "ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents", "comment": "8 pages main paper, 18 pages total, 8 figures, 5 tables, code at https://github.com/ZephinueCode/ToolTok", "summary": "Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at https://github.com/ZephinueCode/ToolTok.", "AI": {"tldr": "ToolTok\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eGUI\u4ee3\u7406\u7684\u591a\u6b65\u8def\u5f84\u67e5\u627e\u65b0\u8303\u5f0f\uff0c\u5c06\u64cd\u4f5c\u5efa\u6a21\u4e3a\u6e10\u8fdb\u5f0f\u5de5\u5177\u4f7f\u7528\u5e8f\u5217\uff0c\u901a\u8fc7\u8bed\u4e49\u951a\u5b9a\u673a\u5236\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5b66\u4e60\u5de5\u5177\u5d4c\u5165\uff0c\u4f7f\u7528\u6613\u5230\u96be\u7684\u8bfe\u7a0b\u5b66\u4e60\u8ba9LLM\u9010\u6b65\u638c\u63e1\u5de5\u5177\u8bed\u4e49\uff0c\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u57fa\u4e8e\u5750\u6807\u7684\u4e00\u6b65\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u8f93\u5165\u5206\u8fa8\u7387\u548c\u5bbd\u9ad8\u6bd4\uff1b\u800c\u5750\u6807\u65e0\u5173\u7b56\u7565\u5219\u5728\u4e25\u91cd\u6570\u636e\u7a00\u7f3a\u4e0b\u5b66\u4e60\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u89c6\u89c9\u53d8\u5316\u53c8\u80fd\u5728\u6709\u9650\u6570\u636e\u4e0b\u6709\u6548\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "1) \u8bbe\u8ba1\u7b26\u5408\u4eba\u7c7b\u4ea4\u4e92\u4e60\u60ef\u7684\u5de5\u5177\uff0c\u7528\u53ef\u5b66\u4e60\u7684token\u5d4c\u5165\u8868\u793a\u6bcf\u4e2a\u5de5\u5177\uff1b2) \u5f15\u5165\u8bed\u4e49\u951a\u5b9a\u673a\u5236\uff0c\u5c06\u6bcf\u4e2a\u5de5\u5177\u4e0e\u8bed\u4e49\u76f8\u5173\u6982\u5ff5\u5173\u8054\u4f5c\u4e3a\u81ea\u7136\u5f52\u7eb3\u504f\u7f6e\uff1b3) \u6784\u5efa\u6613\u5230\u96be\u7684\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5305\u62ectoken\u5b9a\u4e49\u95ee\u7b54\u3001\u7eaf\u6587\u672c\u5f15\u5bfc\u5de5\u5177\u9009\u62e9\u548c\u7b80\u5316\u89c6\u89c9\u8def\u5f84\u67e5\u627e\u4e09\u4e2a\u4efb\u52a1\uff1b4) \u91c7\u7528\u591a\u6b65\u8def\u5f84\u67e5\u627e\u8303\u5f0f\uff0c\u5c06GUI\u64cd\u4f5c\u5efa\u6a21\u4e3a\u6e10\u8fdb\u5f0f\u5de5\u5177\u4f7f\u7528\u5e8f\u5217\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cToolTok\u5728\u53ef\u6bd4\u89c4\u6a21\u6a21\u578b\uff084B\uff09\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u4e0e\u66f4\u5927\u6a21\u578b\uff08235B\uff09\u4fdd\u6301\u7ade\u4e89\u529b\u3002\u8fd9\u4e9b\u7ed3\u679c\u4ec5\u4f7f\u7528\u5176\u4ed6\u540e\u8bad\u7ec3\u65b9\u6cd5\u6240\u9700\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u52301%\u3002ToolTok\u5728\u672a\u89c1\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ToolTok\u901a\u8fc7\u591a\u6b65\u8def\u5f84\u67e5\u627e\u8303\u5f0f\u3001\u8bed\u4e49\u951a\u5b9a\u673a\u5236\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u5728\u89c6\u89c9\u53d8\u5316\u6cdb\u5316\u548c\u6570\u636e\u7a00\u7f3a\u5b66\u4e60\u65b9\u9762\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684GUI\u4ea4\u4e92\u3002", "topic": "code agent"}}
{"id": "2602.03712", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.03712", "abs": "https://arxiv.org/abs/2602.03712", "authors": ["Yisen Xu", "Jinqiu Yang", "Tse-Hsun", "Chen"], "title": "SWE-Refactor: A Repository-Level Benchmark for Real-World LLM-Based Code Refactoring", "comment": null, "summary": "Large Language Models (LLMs) have recently attracted wide interest for tackling software engineering tasks. In contrast to code generation, refactoring demands precise, semantics-preserving edits that improve program structure, which also makes automated evaluation challenging. However, existing refactoring benchmarks commonly suffer from three shortcomings: limited coverage of refactoring scenarios, the inclusion of instances that mix refactoring with unrelated changes, and insufficient repository-level context for realistic assessment. To mitigate these issues, we introduce SWE-Refactor, a new benchmark for LLM-based code refactoring. SWE-Refactor comprises 1,099 developer-written, behavior-preserving refactorings mined from 18 Java projects, including 922 atomic and 177 compound instances. Each instance is validated via compilation, test execution, and automated refactoring detection tools to ensure correctness. We evaluate nine widely used LLMs on SWE-Refactor, covering models such as GPT-4o-mini, DeepSeek-V3, and CodeLLaMa, to provide representative reference results. Our results show that complex and compound refactorings remain the primary source of failures; notably, an OpenAI Codex agent achieves only 39.4% success on compound instances. We release SWE-Refactor and all evaluation results to facilitate future research on LLM-based code refactoring.", "AI": {"tldr": "SWE-Refactor\u662f\u4e00\u4e2a\u65b0\u7684\u4ee3\u7801\u91cd\u6784\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,099\u4e2a\u5f00\u53d1\u8005\u7f16\u5199\u7684Java\u91cd\u6784\u5b9e\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u91cd\u6784\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u91cd\u6784\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u91cd\u6784\u573a\u666f\u8986\u76d6\u6709\u9650\u3001\u5b9e\u4f8b\u4e2d\u6df7\u6742\u65e0\u5173\u4fee\u6539\u3001\u7f3a\u4e4f\u4ed3\u5e93\u7ea7\u4e0a\u4e0b\u6587\u8fdb\u884c\u771f\u5b9e\u8bc4\u4f30\u3002\u9700\u8981\u66f4\u597d\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u91cd\u6784\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u4ece18\u4e2aJava\u9879\u76ee\u4e2d\u6316\u63981,099\u4e2a\u5f00\u53d1\u8005\u7f16\u5199\u7684\u91cd\u6784\u5b9e\u4f8b\uff08922\u4e2a\u539f\u5b50\u91cd\u6784\u548c177\u4e2a\u590d\u5408\u91cd\u6784\uff09\uff0c\u901a\u8fc7\u7f16\u8bd1\u3001\u6d4b\u8bd5\u6267\u884c\u548c\u81ea\u52a8\u5316\u91cd\u6784\u68c0\u6d4b\u5de5\u5177\u9a8c\u8bc1\u6b63\u786e\u6027\uff0c\u8bc4\u4f309\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u6a21\u578b\u3002", "result": "\u590d\u6742\u548c\u590d\u5408\u91cd\u6784\u662f\u4e3b\u8981\u5931\u8d25\u6765\u6e90\uff0cOpenAI Codex\u4ee3\u7406\u5728\u590d\u5408\u5b9e\u4f8b\u4e0a\u4ec5\u8fbe\u523039.4%\u7684\u6210\u529f\u7387\u3002\u63d0\u4f9b\u4e86\u4ee3\u8868\u6027\u53c2\u8003\u7ed3\u679c\u3002", "conclusion": "SWE-Refactor\u57fa\u51c6\u6d4b\u8bd5\u89e3\u51b3\u4e86\u73b0\u6709\u91cd\u6784\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u4e3aLLM\u5728\u4ee3\u7801\u91cd\u6784\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "swe benchmark"}}
{"id": "2602.03006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03006", "abs": "https://arxiv.org/abs/2602.03006", "authors": ["Ziyang Yu", "Liang Zhao"], "title": "Distilling LLM Reasoning into Graph of Concept Predictors", "comment": null, "summary": "Deploying Large Language Models (LLMs) for discriminative workloads is often limited by inference latency, compute, and API costs at scale. Active distillation reduces these costs by querying an LLM oracle to train compact discriminative students, but most pipelines distill only final labels, discarding intermediate reasoning signals and offering limited diagnostics of what reasoning is missing and where errors arise. We propose Graph of Concept Predictors (GCP), a reasoning-aware active distillation framework that externalizes the teacher's decision process as a directed acyclic graph and mirrors it with modular concept predictors in the student. GCP enhances sample efficiency through a graph-aware acquisition strategy that targets uncertainty and disagreement at critical reasoning nodes. Additionally, it improves training stability and efficiency by performing targeted sub-module retraining, which attributes downstream loss to specific concept predictors and updates only the most influential modules. Experiments on eight NLP classification benchmarks demonstrate that GCP enhances performance under limited annotation budgets while yielding more interpretable and controllable training dynamics. Code is available at: https://github.com/Ziyang-Yu/GCP.", "AI": {"tldr": "GCP\u662f\u4e00\u79cd\u63a8\u7406\u611f\u77e5\u7684\u4e3b\u52a8\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6559\u5e08\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u5916\u90e8\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5e76\u7528\u6a21\u5757\u5316\u6982\u5ff5\u9884\u6d4b\u5668\u5728\u5b66\u751f\u6a21\u578b\u4e2d\u955c\u50cf\u8be5\u56fe\uff0c\u4ece\u800c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5224\u522b\u4efb\u52a1\u65f6\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u3001\u8ba1\u7b97\u6210\u672c\u548cAPI\u6210\u672c\u7684\u95ee\u9898\u3002\u73b0\u6709\u4e3b\u52a8\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u53ea\u84b8\u998f\u6700\u7ec8\u6807\u7b7e\uff0c\u4e22\u5f03\u4e86\u4e2d\u95f4\u63a8\u7406\u4fe1\u53f7\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u7f3a\u5931\u63a8\u7406\u7684\u8bca\u65ad\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u56fe\u6982\u5ff5\u9884\u6d4b\u5668\uff08GCP\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u6559\u5e08\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u5916\u90e8\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff1b2\uff09\u5728\u5b66\u751f\u6a21\u578b\u4e2d\u7528\u6a21\u5757\u5316\u6982\u5ff5\u9884\u6d4b\u5668\u955c\u50cf\u8be5\u56fe\uff1b3\uff09\u91c7\u7528\u56fe\u611f\u77e5\u91c7\u96c6\u7b56\u7565\uff0c\u9488\u5bf9\u5173\u952e\u63a8\u7406\u8282\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u5206\u6b67\uff1b4\uff09\u6267\u884c\u9488\u5bf9\u6027\u5b50\u6a21\u5757\u91cd\u8bad\u7ec3\uff0c\u5c06\u4e0b\u6e38\u635f\u5931\u5f52\u56e0\u4e8e\u7279\u5b9a\u6982\u5ff5\u9884\u6d4b\u5668\u3002", "result": "\u5728\u516b\u4e2aNLP\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGCP\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4ea7\u751f\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "GCP\u6846\u67b6\u901a\u8fc7\u5916\u90e8\u5316\u6559\u5e08\u63a8\u7406\u8fc7\u7a0b\u5e76\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u4e3b\u52a8\u84b8\u998f\u7684\u5c40\u9650\u6027\uff0c\u5728\u63d0\u5347\u6837\u672c\u6548\u7387\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.03022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03022", "abs": "https://arxiv.org/abs/2602.03022", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Jingfeng Luo", "Conggang Hu"], "title": "STAR: Similarity-guided Teacher-Assisted Refinement for Super-Tiny Function Calling Models", "comment": "The paper has been accepted to ICLR 2026", "summary": "The proliferation of Large Language Models (LLMs) in function calling is pivotal for creating advanced AI agents, yet their large scale hinders widespread adoption, necessitating transferring their capabilities into smaller ones. However, existing paradigms are often plagued by overfitting, training instability, ineffective binary rewards for multi-solution tasks, and the difficulty of synergizing techniques. We introduce STAR: Similarity-guided Teacher-Assisted Refinement, a novel holistic framework that effectively transfers LLMs' capabilities to super-tiny models. STAR consists of two core technical innovations: (1) Constrained Knowledge Distillation (CKD), a training objective that augments top-k forward KL divergence to suppress confidently incorrect predictions, ensuring training stability while preserving exploration capacity for downstream RL. STAR holistically synergizes these strategies within a cohesive training curriculum, enabling super-tiny models to achieve exceptional performance on complex function calling tasks; (2) Similarity-guided RL (Sim-RL), a RL mechanism that introduces a fine-grained, similarity-based reward. This provides a robust, continuous, and rich signal for better policy optimization by evaluating the similarity between generated outputs and the ground truth. Extensive experiments on challenging and renowned benchmarks demonstrate the effectiveness of our method. Our STAR models establish SOTA in their size classes, significantly outperforming baselines. Remarkably, our 0.6B STAR model achieves the best performance among all open models under 1B, surpassing even several well-known open models at a larger scale. STAR demonstrates a training framework that distills capabilities of LLMs into super-tiny models, paving the way for powerful, accessible, and efficient AI agents.", "AI": {"tldr": "STAR\u6846\u67b6\u901a\u8fc7\u76f8\u4f3c\u6027\u5f15\u5bfc\u7684\u6559\u5e08\u8f85\u52a9\u7cbe\u70bc\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u84b8\u998f\u5230\u8d85\u5c0f\u578b\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u8fc7\u62df\u5408\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u4e8c\u5143\u5956\u52b1\u65e0\u6548\u7b49\u95ee\u9898\uff0c\u5728\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51fd\u6570\u8c03\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5927\u89c4\u6a21\u963b\u788d\u4e86\u5e7f\u6cdb\u91c7\u7528\uff0c\u9700\u8981\u5c06\u5176\u80fd\u529b\u8f6c\u79fb\u5230\u5c0f\u578b\u6a21\u578b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u591a\u89e3\u4efb\u52a1\u7684\u4e8c\u5143\u5956\u52b1\u65e0\u6548\u4ee5\u53ca\u6280\u672f\u96be\u4ee5\u534f\u540c\u7b49\u95ee\u9898\u3002", "method": "STAR\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1) \u7ea6\u675f\u77e5\u8bc6\u84b8\u998f(CKD)\uff0c\u901a\u8fc7\u589e\u5f3atop-k\u524d\u5411KL\u6563\u5ea6\u6765\u6291\u5236\u81ea\u4fe1\u7684\u9519\u8bef\u9884\u6d4b\uff1b2) \u76f8\u4f3c\u6027\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60(Sim-RL)\uff0c\u5f15\u5165\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7ec6\u7c92\u5ea6\u5956\u52b1\u3002\u8fd9\u4e9b\u6280\u672f\u5728\u7edf\u4e00\u7684\u8bad\u7ec3\u8bfe\u7a0b\u4e2d\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTAR\u6a21\u578b\u5728\u5176\u89c4\u6a21\u7c7b\u522b\u4e2d\u5efa\u7acb\u4e86SOTA\u30020.6B\u7684STAR\u6a21\u578b\u5728\u6240\u67091B\u4ee5\u4e0b\u7684\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u51e0\u4e2a\u66f4\u5927\u89c4\u6a21\u7684\u77e5\u540d\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "STAR\u5c55\u793a\u4e86\u4e00\u4e2a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u84b8\u998f\u5230\u8d85\u5c0f\u578b\u6a21\u578b\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4e3a\u5f3a\u5927\u3001\u53ef\u8bbf\u95ee\u4e14\u9ad8\u6548\u7684\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002", "topic": "code agent"}}
{"id": "2602.03798", "categories": ["cs.SE", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.03798", "abs": "https://arxiv.org/abs/2602.03798", "authors": ["Zimu Lu", "Houxing Ren", "Yunqiao Yang", "Ke Wang", "Zhuofan Zong", "Mingjie Zhan", "Hongsheng Li"], "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation", "comment": null, "summary": "Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.", "AI": {"tldr": "FullStack-Agent\u662f\u4e00\u4e2a\u7528\u4e8e\u5168\u6808\u7f51\u9875\u5f00\u53d1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u5f00\u53d1\u6846\u67b6\u3001\u81ea\u5b66\u4e60\u65b9\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e09\u90e8\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u524d\u7aef\u3001\u540e\u7aef\u548c\u6570\u636e\u5e93\u529f\u80fd\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7801\u667a\u80fd\u4f53\u4e3b\u8981\u751f\u6210\u524d\u7aef\u7f51\u9875\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u5168\u6808\u6570\u636e\u5904\u7406\u548c\u5b58\u50a8\u80fd\u529b\u3002\u6784\u5efa\u751f\u4ea7\u7ea7\u5168\u6808\u5e94\u7528\u9700\u8981\u63a7\u5236\u6570\u636e\u6d41\u3001\u7406\u89e3\u4e0d\u65ad\u66f4\u65b0\u7684\u5305\u4f9d\u8d56\u3001\u5b9a\u4f4d\u4ee3\u7801\u4e2d\u7684\u9690\u853dbug\uff0c\u8fd9\u4e9b\u6311\u6218\u8fdc\u8d85\u8fc7\u4ec5\u751f\u6210\u524d\u7aef\u9875\u9762\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u90e8\u5206\uff1a1) FullStack-Dev\uff1a\u5177\u6709\u89c4\u5212\u3001\u4ee3\u7801\u7f16\u8f91\u3001\u4ee3\u7801\u5e93\u5bfc\u822a\u548cbug\u5b9a\u4f4d\u80fd\u529b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1b2) FullStack-Learn\uff1a\u901a\u8fc7\u53cd\u5411\u7ffb\u8bd1\u722c\u53d6\u548c\u5408\u6210\u7684\u7f51\u7ad9\u4ed3\u5e93\u8fdb\u884c\u6570\u636e\u6269\u5c55\u548c\u81ea\u6211\u6539\u8fdb\u7684\u65b9\u6cd5\uff1b3) FullStack-Bench\uff1a\u5168\u9762\u6d4b\u8bd5\u751f\u6210\u7f51\u7ad9\u524d\u7aef\u3001\u540e\u7aef\u548c\u6570\u636e\u5e93\u529f\u80fd\u7684\u57fa\u51c6\u3002", "result": "FullStack-Dev\u5728\u524d\u7aef\u3001\u540e\u7aef\u548c\u6570\u636e\u5e93\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u5206\u522b\u6bd4\u4e4b\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u53478.7%\u300138.2%\u548c15.9%\u3002FullStack-Learn\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u4f7f30B\u6a21\u578b\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u63d0\u53479.7%\u30019.5%\u548c2.8%\u3002", "conclusion": "FullStack-Agent\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5168\u6808\u7f51\u9875\u5f00\u53d1\u7684\u6311\u6218\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3001\u81ea\u5b66\u4e60\u65b9\u6cd5\u548c\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u6808\u5e94\u7528\u751f\u6210\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "topic": "code agent"}}
{"id": "2602.02554", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02554", "abs": "https://arxiv.org/abs/2602.02554", "authors": ["Jingwen Xu", "Yiyang Lu", "Zisu Huang", "Changze Lv", "Xiaohua Wang", "Shizheng Li", "Zhibo Xu", "Zhengkang Guo", "Zhengyuan Wang", "Muzhao Tian", "Xuanjing Huang", "Xiaoqing Zheng"], "title": "BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation", "comment": null, "summary": "Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity.", "AI": {"tldr": "BatCoder\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801-\u6587\u6863\u53cc\u5411\u7ffb\u8bd1\u4f18\u5316\u4ee3\u7801\u751f\u6210\u548c\u6587\u6863\u751f\u6210\uff0c\u4ec5\u9700\u4ee3\u7801\u6570\u636e\u5373\u53ef\u8bad\u7ec3\uff0c\u5728HumanEval\u548cMBPP\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8bad\u7ec3\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u7684LLMs\u901a\u5e38\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801-\u6587\u6863\u5bf9\uff0c\u8fd9\u4e9b\u6570\u636e\u6210\u672c\u9ad8\u4e14\u5bf9\u4e8e\u5c0f\u4f17\u7f16\u7a0b\u8bed\u8a00\u7a00\u7f3a\u3002\u9700\u8981\u4e00\u79cd\u4ec5\u4f7f\u7528\u4ee3\u7801\u6570\u636e\u5c31\u80fd\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u56de\u8bd1\u7b56\u7565\uff1a\u9996\u5148\u751f\u6210\u4ee3\u7801\u7684\u6587\u6863\uff0c\u7136\u540e\u7528\u751f\u6210\u7684\u6587\u6863\u91cd\u6784\u539f\u59cb\u4ee3\u7801\u3002\u539f\u59cb\u4ee3\u7801\u4e0e\u91cd\u6784\u4ee3\u7801\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u9690\u5f0f\u5956\u52b1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540c\u65f6\u4f18\u5316\u4ee3\u7801\u751f\u6210\u548c\u6587\u6863\u751f\u6210\u3002", "result": "\u57287B\u6a21\u578b\u4e0a\uff0cHumanEval\u8fbe\u523083.5% pass@1\uff0cMBPP\u8fbe\u523081.0% pass@1\uff0c\u4f18\u4e8e\u5f00\u6e90\u57fa\u7ebf\u3002\u6846\u67b6\u5728\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u548c\u6a21\u578b\u5bb9\u91cf\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "BatCoder\u901a\u8fc7\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7801-\u6587\u6863\u5bf9\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ec5\u4f7f\u7528\u4ee3\u7801\u6570\u636e\u5c31\u80fd\u8bad\u7ec3\u51fa\u9ad8\u6027\u80fd\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u5177\u6709\u5f88\u597d\u7684\u6269\u5c55\u6027\u3002", "topic": "code agent"}}
{"id": "2602.03025", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03025", "abs": "https://arxiv.org/abs/2602.03025", "authors": ["Haitian Zhong", "Jixiu Zhai", "Lei Song", "Jiang Bian", "Qiang Liu", "Tieniu Tan"], "title": "RC-GRPO: Reward-Conditioned Group Relative Policy Optimization for Multi-Turn Tool Calling Agents", "comment": null, "summary": "Multi-turn tool calling is challenging for Large Language Models (LLMs) because rewards are sparse and exploration is expensive. A common recipe, SFT followed by GRPO, can stall when within-group reward variation is low (e.g., more rollouts in a group receive the all 0 or all 1 reward), making the group-normalized advantage uninformative and yielding vanishing updates. To address this problem, we propose RC-GRPO (Reward-Conditioned Group Relative Policy Optimization), which treats exploration as a controllable steering problem via discrete reward tokens. We first fine-tune a Reward-Conditioned Trajectory Policy (RCTP) on mixed-quality trajectories with reward goal special tokens (e.g., <|high_reward|>, <|low_reward|>) injected into the prompts, enabling the model to learn how to generate distinct quality trajectories on demand. Then during RL, we sample diverse reward tokens within each GRPO group and condition rollouts on the sampled token to improve within-group diversity, improving advantage gains. On the Berkeley Function Calling Leaderboard v4 (BFCLv4) multi-turn benchmark, our method yields consistently improved performance than baselines, and the performance on Qwen-2.5-7B-Instruct even surpasses all closed-source API models.", "AI": {"tldr": "\u63d0\u51faRC-GRPO\u65b9\u6cd5\u89e3\u51b3LLM\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u5956\u52b1\u7a00\u758f\u548c\u63a2\u7d22\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5956\u52b1\u6761\u4ef6\u4ee4\u724c\u589e\u5f3a\u7ec4\u5185\u591a\u6837\u6027\uff0c\u5728BFCLv4\u57fa\u51c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u5e76\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u5bf9LLM\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5956\u52b1\u7a00\u758f\u4e14\u63a2\u7d22\u6210\u672c\u9ad8\u3002\u4f20\u7edf\u7684SFT+GRPO\u65b9\u6cd5\u5728\u7ec4\u5185\u5956\u52b1\u53d8\u5316\u4f4e\u65f6\uff08\u5982\u7ec4\u5185\u591a\u6570rollout\u83b7\u5f97\u51680\u6216\u51681\u5956\u52b1\uff09\u4f1a\u505c\u6ede\uff0c\u5bfc\u81f4\u7ec4\u5f52\u4e00\u5316\u4f18\u52bf\u4fe1\u606f\u4e0d\u8db3\u548c\u66f4\u65b0\u6d88\u5931\u3002", "method": "\u63d0\u51faRC-GRPO\uff08\u5956\u52b1\u6761\u4ef6\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5c06\u63a2\u7d22\u89c6\u4e3a\u901a\u8fc7\u79bb\u6563\u5956\u52b1\u4ee4\u724c\u7684\u53ef\u63a7\u5f15\u5bfc\u95ee\u9898\u3002\u9996\u5148\u5728\u6df7\u5408\u8d28\u91cf\u8f68\u8ff9\u4e0a\u5fae\u8c03\u5956\u52b1\u6761\u4ef6\u8f68\u8ff9\u7b56\u7565\uff08RCTP\uff09\uff0c\u5728\u63d0\u793a\u4e2d\u6ce8\u5165\u5956\u52b1\u76ee\u6807\u7279\u6b8a\u4ee4\u724c\uff08\u5982<|high_reward|>, <|low_reward|>\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6309\u9700\u751f\u6210\u4e0d\u540c\u8d28\u91cf\u7684\u8f68\u8ff9\u3002\u7136\u540e\u5728RL\u671f\u95f4\uff0c\u5728\u6bcf\u4e2aGRPO\u7ec4\u5185\u91c7\u6837\u591a\u6837\u5956\u52b1\u4ee4\u724c\uff0c\u5e76\u5728\u91c7\u6837\u7684\u4ee4\u724c\u4e0a\u6761\u4ef6\u5316rollout\u4ee5\u589e\u5f3a\u7ec4\u5185\u591a\u6837\u6027\u3002", "result": "\u5728Berkeley Function Calling Leaderboard v4\u591a\u8f6e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u83b7\u5f97\u6301\u7eed\u6539\u8fdb\u7684\u6027\u80fd\uff0cQwen-2.5-7B-Instruct\u7684\u6027\u80fd\u751a\u81f3\u8d85\u8d8a\u4e86\u6240\u6709\u95ed\u6e90API\u6a21\u578b\u3002", "conclusion": "RC-GRPO\u901a\u8fc7\u5956\u52b1\u6761\u4ef6\u4ee4\u724c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5de5\u5177\u8c03\u7528\u4e2d\u7ec4\u5185\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02555", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02555", "abs": "https://arxiv.org/abs/2602.02555", "authors": ["Bizhe Bai", "Xinyue Wang", "Peng Ye", "Tao Chen"], "title": "Learning to Explore with Parameter-Space Noise: A Deep Dive into Parameter-Space Noise for Reinforcement Learning with Verifiable Rewards", "comment": "17 pages, 10 Figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves LLM reasoning, yet growing evidence indicates an exploration ceiling: it often reweights existing solution traces rather than discovering new strategies, limiting gains under large sampling budgets (e.g., pass-at-256). We address this limitation with PSN-RLVR, which perturbs policy parameters before rollout generation to induce temporally consistent, trajectory-level exploration that better preserves long-horizon chain-of-thought coherence than action-space noise. To mitigate the resulting sampling-update mismatch, we incorporate truncated importance sampling (TIS). To avoid expensive KL-based adaptive noise control, we propose a computationally efficient real-time adaptive noise scheduler driven by a lightweight surrogate that combines semantic diversity with normalized self-certainty. Instantiated on GRPO, a widely used RLVR method, PSN-GRPO consistently expands the effective reasoning capability boundary across multiple mathematical reasoning benchmarks and model families, yielding higher pass-at-k under large sampling budgets and outperforming prior exploration-oriented RLVR methods (e.g., Pass-at-k-style training) while remaining orthogonal and thus composable for additional gains.", "AI": {"tldr": "PSN-RLVR\u901a\u8fc7\u53c2\u6570\u6270\u52a8\u548c\u622a\u65ad\u91cd\u8981\u6027\u91c7\u6837\u89e3\u51b3RLVR\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u63d0\u5347\u5927\u91c7\u6837\u9884\u7b97\u4e0b\u7684\u6027\u80fd", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u5929\u82b1\u677f\uff0c\u4e3b\u8981\u91cd\u65b0\u52a0\u6743\u73b0\u6709\u89e3\u8f68\u8ff9\u800c\u975e\u53d1\u73b0\u65b0\u7b56\u7565\uff0c\u9650\u5236\u4e86\u5728\u5927\u91c7\u6837\u9884\u7b97\u4e0b\u7684\u6027\u80fd\u63d0\u5347", "method": "\u63d0\u51faPSN-RLVR\uff1a1\uff09\u5728rollout\u751f\u6210\u524d\u6270\u52a8\u7b56\u7565\u53c2\u6570\u4ee5\u4fdd\u6301\u601d\u7ef4\u94fe\u4e00\u81f4\u6027\uff1b2\uff09\u4f7f\u7528\u622a\u65ad\u91cd\u8981\u6027\u91c7\u6837\u7f13\u89e3\u91c7\u6837-\u66f4\u65b0\u4e0d\u5339\u914d\uff1b3\uff09\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u4ee3\u7406\u9a71\u52a8\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u8c03\u5ea6\u5668", "result": "\u5728GRPO\u4e0a\u5b9e\u4f8b\u5316\u7684PSN-GRPO\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u6a21\u578b\u5bb6\u65cf\u4e0a\u6269\u5c55\u4e86\u6709\u6548\u63a8\u7406\u80fd\u529b\u8fb9\u754c\uff0c\u5728\u5927\u91c7\u6837\u9884\u7b97\u4e0b\u83b7\u5f97\u66f4\u9ad8\u7684pass-at-k\u6027\u80fd", "conclusion": "PSN-RLVR\u6709\u6548\u89e3\u51b3\u4e86RLVR\u7684\u63a2\u7d22\u9650\u5236\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u6b63\u4ea4\u4e14\u53ef\u7ec4\u5408\uff0c\u4e3aRLVR\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u63a2\u7d22\u673a\u5236", "topic": "agentic reinforcement learning"}}
{"id": "2602.03026", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03026", "abs": "https://arxiv.org/abs/2602.03026", "authors": ["Weilin Ruan", "Yuxuan Liang"], "title": "Visual Reasoning over Time Series via Multi-Agent System", "comment": null, "summary": "Time series analysis underpins many real-world applications, yet existing time-series-specific methods and pretrained large-model-based approaches remain limited in integrating intuitive visual reasoning and generalizing across tasks with adaptive tool usage. To address these limitations, we propose MAS4TS, a tool-driven multi-agent system for general time series tasks, built upon an Analyzer-Reasoner-Executor paradigm that integrates agent communication, visual reasoning, and latent reconstruction within a unified framework. MAS4TS first performs visual reasoning over time series plots with structured priors using a Vision-Language Model to extract temporal structures, and subsequently reconstructs predictive trajectories in latent space. Three specialized agents coordinate via shared memory and gated communication, while a router selects task-specific tool chains for execution. Extensive experiments on multiple benchmarks demonstrate that MAS4TS achieves state-of-the-art performance across a wide range of time series tasks, while exhibiting strong generalization and efficient inference.", "AI": {"tldr": "MAS4TS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u5668-\u63a8\u7406\u5668-\u6267\u884c\u5668\u8303\u5f0f\uff0c\u7ed3\u5408\u89c6\u89c9\u63a8\u7406\u548c\u6f5c\u5728\u91cd\u6784\uff0c\u5b9e\u73b0\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\u5728\u6574\u5408\u76f4\u89c2\u89c6\u89c9\u63a8\u7406\u548c\u8de8\u4efb\u52a1\u81ea\u9002\u5e94\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u6790\u5668-\u63a8\u7406\u5668-\u6267\u884c\u5668\u8303\u5f0f\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u65f6\u95f4\u5e8f\u5217\u56fe\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u63d0\u53d6\u65f6\u95f4\u7ed3\u6784\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u91cd\u6784\u9884\u6d4b\u8f68\u8ff9\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u534f\u8c03\u5de5\u4f5c\uff0c\u8def\u7531\u5668\u9009\u62e9\u4efb\u52a1\u7279\u5b9a\u5de5\u5177\u94fe\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6548\u63a8\u7406\u3002", "conclusion": "MAS4TS\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5de5\u5177\u9a71\u52a8\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u89c6\u89c9\u63a8\u7406\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u7684\u6311\u6218\u3002", "topic": "agent analysis"}}
{"id": "2602.02556", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02556", "abs": "https://arxiv.org/abs/2602.02556", "authors": ["Xuancheng Li", "Haitao Li", "Yujia Zhou", "Yiqun Liu", "Qingyao Ai"], "title": "Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs", "comment": null, "summary": "Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.", "AI": {"tldr": "SEAM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7ed3\u6784\u5316\u7ecf\u9a8c\u9002\u914d\u5668\u6a21\u5757\uff0c\u901a\u8fc7\u53c2\u6570\u5b58\u50a8\u7ecf\u9a8c\u5e76\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u7ed3\u6784\u5316\u7ecf\u9a8c\u6761\u76ee\uff0c\u6307\u5bfc\u51bb\u7ed3\u7684LLM\u6267\u884c\u5668\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u5f53\u524dLLM\u901a\u5e38\u662f\u9759\u6001\u7684\uff0c\u4f1a\u91cd\u590d\u63a8\u7406\u6216\u9519\u8bef\uff0c\u800c\u73b0\u6709\u7684\u7ecf\u9a8c\u91cd\u7528\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\uff0c\u5b58\u5728\u76f8\u4f3c\u6027\u566a\u58f0\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7ecf\u9a8c\u5229\u7528\u673a\u5236", "method": "\u63d0\u51faSEAM\u6a21\u5757\uff1a1\uff09\u8f7b\u91cf\u7ea7\u3001\u6267\u884c\u5668\u7279\u5b9a\u7684\u63d2\u4ef6\uff0c\u5728\u53c2\u6570\u4e2d\u5b58\u50a8\u7ecf\u9a8c\uff1b2\uff09\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u7ed3\u6784\u5316\u3001\u5b9e\u4f8b\u5b9a\u5236\u7684\u7ecf\u9a8c\u6761\u76ee\uff1b3\uff09\u901a\u8fc7\u6267\u884c\u5668rollouts\u548cGRPO\u8fdb\u884c\u6548\u7528\u8bad\u7ec3\uff0c\u4fdd\u6301\u6267\u884c\u5668\u51bb\u7ed3\uff1b4\uff09\u90e8\u7f72\u540e\u53ef\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8fdb\u4e00\u6b65\u6539\u8fdb", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEAM\u5728\u4e0d\u540c\u6267\u884c\u5668\u4e0a\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u5f00\u9500\u8f83\u4f4e\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u548c\u5206\u6790\u9610\u660e\u4e86SEAM\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u673a\u5236", "conclusion": "SEAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7ecf\u9a8c\u91cd\u7528\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4fee\u6539LLM\u6267\u884c\u5668\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u548c\u9c81\u68d2\u6027\u7684\u4f18\u52bf", "topic": "agent analysis"}}
{"id": "2602.02619", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.02619", "abs": "https://arxiv.org/abs/2602.02619", "authors": ["Mohan Jiang", "Dayuan Fu", "Junhao Shi", "Ji Zeng", "Weiye Si", "Keyu Li", "Xuefeng Li", "Yang Xiao", "Wenjie Li", "Dequan Wang", "Pengfei Liu"], "title": "daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently", "comment": null, "summary": "While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...", "AI": {"tldr": "daVinci-Agency\uff1a\u901a\u8fc7\u6316\u6398Git PR\u5e8f\u5217\u4e2d\u7684\u7ed3\u6784\u5316\u76d1\u7763\u4fe1\u53f7\uff0c\u89e3\u51b3LLM\u5728\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff1a\u5408\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u6a21\u578b\u5206\u5e03\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u5bfb\u627e\u80fd\u591f\u6355\u6349\u771f\u5b9e\u957f\u4f9d\u8d56\u7ed3\u6784\u548c\u8de8\u9636\u6bb5\u6f14\u5316\u52a8\u6001\u7684\u6570\u636e\u6e90\u3002", "method": "\u4ece\u771f\u5b9e\u8f6f\u4ef6\u6f14\u5316\u4e2d\u6316\u6398\u7ed3\u6784\u5316\u76d1\u7763\u4fe1\u53f7\uff0c\u5229\u7528Pull Request\u5e8f\u5217\u7684\u81ea\u7136\u7279\u6027\uff1a1) \u901a\u8fc7\u8fde\u7eed\u63d0\u4ea4\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u4efb\u52a1\u5206\u89e3\uff1b2) \u901a\u8fc7\u7edf\u4e00\u529f\u80fd\u76ee\u6807\u5f3a\u5236\u6267\u884c\u957f\u671f\u4e00\u81f4\u6027\uff1b3) \u4ece\u771f\u5b9ebug\u4fee\u590d\u8f68\u8ff9\u4e2d\u83b7\u5f97\u53ef\u9a8c\u8bc1\u7684\u6539\u8fdb\u3002\u6784\u5efadaVinci-Agency\u7cfb\u7edf\uff0c\u5e73\u5747\u8f68\u8ff9\u5305\u542b85k tokens\u548c116\u4e2a\u5de5\u5177\u8c03\u7528\u3002", "result": "\u4ec5\u4f7f\u7528239\u4e2adaVinci-Agency\u6837\u672c\u5fae\u8c03GLM-4.6\uff0c\u5c31\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5e7f\u6cdb\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728Toolathlon\u4e0a\u83b7\u5f9747%\u7684\u76f8\u5bf9\u589e\u76ca\u3002\u6570\u636e\u6548\u7387\u663e\u8457\uff0c\u957f\u8f68\u8ff9\u4f46\u9ad8\u8d28\u91cf\u3002", "conclusion": "PR\u5e8f\u5217\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u81ea\u7136\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u80fd\u591f\u6355\u6349\u56e0\u679c\u4f9d\u8d56\u548c\u8fed\u4ee3\u6539\u8fdb\u6a21\u5f0f\u3002daVinci-Agency\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03053", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.03053", "abs": "https://arxiv.org/abs/2602.03053", "authors": ["Vishal Venkataramani", "Haizhou Shi", "Zixuan Ke", "Austin Xu", "Xiaoxiao He", "Yingbo Zhou", "Semih Yavuz", "Hao Wang", "Shafiq Joty"], "title": "MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems", "comment": "Preprint; work in progress", "summary": "Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8fc7\u7a0b\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u6548\u679c\u4e0d\u7a33\u5b9a\uff0cLLM-as-a-Judge\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\uff0c\u4f46\u8fc7\u7a0b\u9a8c\u8bc1\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u63a8\u7406\u8f68\u8ff9\u4e0a\u5b58\u5728\u9ad8\u65b9\u5dee\uff0c\u8fc7\u7a0b\u9a8c\u8bc1\u5728\u4e00\u822c\u63a8\u7406\u573a\u666f\u4e2d\u663e\u793a\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u63d0\u51faMAS-ProVe\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u9a8c\u8bc1\u8303\u5f0f\uff08LLM-as-a-Judge\u3001\u5956\u52b1\u6a21\u578b\u3001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff09\uff0c\u4e24\u4e2a\u9a8c\u8bc1\u7c92\u5ea6\uff08\u667a\u80fd\u4f53\u7ea7\u548c\u8fed\u4ee3\u7ea7\uff09\uff0c\u4e94\u4e2a\u4ee3\u8868\u6027\u9a8c\u8bc1\u5668\u548c\u56db\u79cd\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\uff0c\u5728\u516d\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u548c\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8fc7\u7a0b\u7ea7\u9a8c\u8bc1\u4e0d\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u4e14\u5e38\u8868\u73b0\u51fa\u9ad8\u65b9\u5dee\uff1bLLM-as-a-Judge\u901a\u5e38\u4f18\u4e8e\u57fa\u4e8e\u5956\u52b1\u7684\u65b9\u6cd5\uff1b\u8bad\u7ec3\u8fc7\u7684\u6cd5\u5b98\u4f18\u4e8e\u901a\u7528LLM\uff1bLLM\u4f5c\u4e3a\u6cd5\u5b98\u4e0e\u4f5c\u4e3a\u5355\u667a\u80fd\u4f53\u7684\u6027\u80fd\u5dee\u8ddd\u8f83\u5c0f\uff1b\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6709\u6548\u4e14\u9c81\u68d2\u7684\u8fc7\u7a0b\u9a8c\u8bc1\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\uff0c\u9700\u8981\u8d85\u8d8a\u5f53\u524d\u8303\u5f0f\u7684\u8fdb\u4e00\u6b65\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2602.03806", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.03806", "abs": "https://arxiv.org/abs/2602.03806", "authors": ["Ziru Chen", "Dongdong Chen", "Ruinan Jin", "Yingbin Liang", "Yujia Xie", "Huan Sun"], "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation", "comment": null, "summary": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.", "AI": {"tldr": "\u63d0\u51faCobalt\u65b9\u6cd5\uff0c\u7ed3\u5408\u5728\u7ebf\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u52bf\uff0c\u7528\u4e8e\u591a\u8f6e\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5728LiveCodeBench\u4e0a\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u8f6e\u4ee3\u7801\u751f\u6210\u7b49\u771f\u5b9e\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u5728\u7ebf\u548c\u79bb\u7ebfRL\u4f18\u52bf\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5c06\u591a\u8f6e\u4ee3\u7801\u751f\u6210\u5efa\u6a21\u4e3a\u4e00\u6b65\u53ef\u6062\u590d\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51faCobalt\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u53c2\u8003LLM\u6536\u96c6\u4ee3\u7801\u751f\u6210\u8f68\u8ff9\u5e76\u5206\u5272\u4e3a\u90e8\u5206\u8f68\u8ff9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u793a\uff1b2) \u5728\u7ebfbandit\u5b66\u4e60\u4e2d\uff0c\u8bad\u7ec3LLM\u901a\u8fc7\u5355\u6b65\u4ee3\u7801\u751f\u6210\u5b8c\u6210\u6bcf\u4e2a\u90e8\u5206\u8f68\u8ff9\u63d0\u793a\u3002", "result": "Cobalt\u5728LiveCodeBench\u4e0a\u4f18\u4e8e\u57fa\u4e8eGRPO\u548cVeRPO\u7684\u591a\u8f6e\u5728\u7ebfRL\u57fa\u7ebf\uff0c\u5c06R1-Distill 8B\u548cQwen3 8B\u7684Pass@1\u5206\u6570\u5206\u522b\u63d0\u5347\u9ad8\u8fbe9.0\u548c6.2\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\u3002\u901a\u8fc7\u6270\u52a8\u8f68\u8ff9\u589e\u5f3a\u8bad\u7ec3\u7f13\u89e3\u4e86\u4e0a\u4e0b\u6587\u5956\u52b1\u653b\u51fb\u95ee\u9898\u3002", "conclusion": "Cobalt\u662f\u591a\u8f6e\u4ee3\u7801\u751f\u6210\u7b49\u8fed\u4ee3\u51b3\u7b56\u4efb\u52a1\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "2602.03100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03100", "abs": "https://arxiv.org/abs/2602.03100", "authors": ["Jingnan Zheng", "Yanzhen Luo", "Jingjun Xu", "Bingnan Liu", "Yuxin Chen", "Chenhang Cui", "Gelei Deng", "Chaochao Lu", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "Risky-Bench: Probing Agentic Safety Risks under Real-World Deployment", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as agents that operate in real-world environments, introducing safety risks beyond linguistic harm. Existing agent safety evaluations rely on risk-oriented tasks tailored to specific agent settings, resulting in limited coverage of safety risk space and failing to assess agent safety behavior during long-horizon, interactive task execution in complex real-world deployments. Moreover, their specialization to particular agent settings limits adaptability across diverse agent configurations. To address these limitations, we propose Risky-Bench, a framework that enables systematic agent safety evaluation grounded in real-world deployment. Risky-Bench organizes evaluation around domain-agnostic safety principles to derive context-aware safety rubrics that delineate safety space, and systematically evaluates safety risks across this space through realistic task execution under varying threat assumptions. When applied to life-assist agent settings, Risky-Bench uncovers substantial safety risks in state-of-the-art agents under realistic execution conditions. Moreover, as a well-structured evaluation pipeline, Risky-Bench is not confined to life-assist scenarios and can be adapted to other deployment settings to construct environment-specific safety evaluations, providing an extensible methodology for agent safety assessment.", "AI": {"tldr": "Risky-Bench\u662f\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u65e0\u5173\u7684\u5b89\u5168\u539f\u5219\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u6761\u4ef6\u4e0b\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a1) \u4f9d\u8d56\u9488\u5bf9\u7279\u5b9a\u667a\u80fd\u4f53\u8bbe\u7f6e\u7684\u98ce\u9669\u5bfc\u5411\u4efb\u52a1\uff0c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\u7a7a\u95f4\u8986\u76d6\u6709\u9650\uff1b2) \u65e0\u6cd5\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u957f\u671f\u3001\u4ea4\u4e92\u5f0f\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5b89\u5168\u884c\u4e3a\uff1b3) \u5bf9\u7279\u5b9a\u667a\u80fd\u4f53\u8bbe\u7f6e\u7684\u4e13\u95e8\u5316\u9650\u5236\u4e86\u8de8\u4e0d\u540c\u667a\u80fd\u4f53\u914d\u7f6e\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faRisky-Bench\u6846\u67b6\uff1a1) \u56f4\u7ed5\u9886\u57df\u65e0\u5173\u7684\u5b89\u5168\u539f\u5219\u7ec4\u7ec7\u8bc4\u4f30\uff1b2) \u63a8\u5bfc\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\u6765\u754c\u5b9a\u5b89\u5168\u7a7a\u95f4\uff1b3) \u5728\u4e0d\u540c\u5a01\u80c1\u5047\u8bbe\u4e0b\u901a\u8fc7\u771f\u5b9e\u4efb\u52a1\u6267\u884c\u7cfb\u7edf\u8bc4\u4f30\u5b89\u5168\u98ce\u9669\uff1b4) \u4f5c\u4e3a\u7ed3\u6784\u5316\u8bc4\u4f30\u6d41\u7a0b\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u6784\u5efa\u73af\u5883\u7279\u5b9a\u7684\u5b89\u5168\u8bc4\u4f30\u3002", "result": "\u5728\u751f\u6d3b\u8f85\u52a9\u667a\u80fd\u4f53\u8bbe\u7f6e\u4e2d\u5e94\u7528Risky-Bench\uff0c\u53d1\u73b0\u5728\u771f\u5b9e\u6267\u884c\u6761\u4ef6\u4e0b\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5b58\u5728\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002\u8be5\u6846\u67b6\u4e0d\u4ec5\u9650\u4e8e\u751f\u6d3b\u8f85\u52a9\u573a\u666f\uff0c\u53ef\u9002\u5e94\u5176\u4ed6\u90e8\u7f72\u8bbe\u7f6e\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "Risky-Bench\u89e3\u51b3\u4e86\u73b0\u6709\u667a\u80fd\u4f53\u5b89\u5168\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u667a\u80fd\u4f53\u914d\u7f6e\u548c\u90e8\u7f72\u73af\u5883\u3002", "topic": "agent analysis"}}
{"id": "2602.03128", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03128", "abs": "https://arxiv.org/abs/2602.03128", "authors": ["Abdelghny Orogat", "Ana Rostam", "Essam Mansour"], "title": "Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis", "comment": "25 pages, 9 figures and 13 tables; introduces MAFBench unified multi-agent evaluation suite", "summary": "Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u7684\u67b6\u6784\u8bbe\u8ba1\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u91cd\u5927\u5f71\u54cd\uff0c\u53d1\u73b0\u4ec5\u67b6\u6784\u9009\u62e9\u5c31\u80fd\u5bfc\u81f4100\u500d\u5ef6\u8fdf\u589e\u52a0\u300130%\u89c4\u5212\u51c6\u786e\u7387\u4e0b\u964d\u548c\u534f\u8c03\u6210\u529f\u7387\u4ece90%\u964d\u81f330%\u4ee5\u4e0b\uff0c\u5e76\u63d0\u51fa\u4e86MAFBench\u8bc4\u4f30\u5957\u4ef6\u548c\u67b6\u6784\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u867d\u7136\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u67b6\u6784\u8bbe\u8ba1\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ea\u5173\u6ce8\u5355\u4e00\u80fd\u529b\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6846\u67b6\u7ea7\u8bc4\u4f30\uff0c\u65e0\u6cd5\u9694\u79bb\u67b6\u6784\u6548\u5e94\uff0c\u5bfc\u81f4\u96be\u4ee5\u8bc4\u4f30\u4e0d\u540c\u6846\u67b6\u8bbe\u8ba1\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u7684\u67b6\u6784\u5206\u7c7b\u6cd5\uff0c\u5f00\u53d1MAFBench\u7edf\u4e00\u8bc4\u4f30\u5957\u4ef6\uff0c\u96c6\u6210\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5230\u6807\u51c6\u5316\u6267\u884c\u7ba1\u9053\u4e2d\uff0c\u5728\u591a\u4e2a\u6d41\u884c\u6846\u67b6\u4e0a\u8fdb\u884c\u53d7\u63a7\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u6846\u67b6\u7ea7\u8bbe\u8ba1\u9009\u62e9\u5355\u72ec\u5c31\u80fd\u5bfc\u81f4\uff1a\u5ef6\u8fdf\u589e\u52a0\u8d85\u8fc7100\u500d\uff0c\u89c4\u5212\u51c6\u786e\u7387\u4e0b\u964d\u9ad8\u8fbe30%\uff0c\u534f\u8c03\u6210\u529f\u7387\u4ece90%\u4ee5\u4e0a\u964d\u81f330%\u4ee5\u4e0b\u3002\u4e0d\u540c\u6846\u67b6\u5728\u7f16\u6392\u5f00\u9500\u3001\u5185\u5b58\u884c\u4e3a\u3001\u89c4\u5212\u3001\u4e13\u4e1a\u5316\u548c\u534f\u8c03\u7b49\u65b9\u9762\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u7684\u67b6\u6784\u8bbe\u8ba1\u5bf9\u7cfb\u7edf\u6027\u80fd\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u67b6\u6784\u8bbe\u8ba1\u539f\u5219\u548c\u6846\u67b6\u9009\u62e9\u6307\u5bfc\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2602.03318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03318", "abs": "https://arxiv.org/abs/2602.03318", "authors": ["Yifan Shi", "Jialong Shi", "Jiayi Wang", "Ye Fan", "Jianyong Sun"], "title": "MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research", "comment": null, "summary": "Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.", "AI": {"tldr": "MIRROR\u662f\u4e00\u4e2a\u514d\u5fae\u8c03\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u53ef\u5c06\u81ea\u7136\u8bed\u8a00\u4f18\u5316\u95ee\u9898\u76f4\u63a5\u8f6c\u6362\u4e3a\u6570\u5b66\u6a21\u578b\u548c\u6c42\u89e3\u5668\u4ee3\u7801\uff0c\u901a\u8fc7\u6267\u884c\u9a71\u52a8\u7684\u8fed\u4ee3\u81ea\u9002\u5e94\u4fee\u8ba2\u548c\u5206\u5c42\u68c0\u7d22\u673a\u5236\u5b9e\u73b0\u53ef\u9760\u5efa\u6a21\u3002", "motivation": "\u8fd0\u7b79\u5b66\u4f9d\u8d56\u4e13\u5bb6\u5efa\u6a21\uff0c\u8fc7\u7a0b\u7f13\u6162\u8106\u5f31\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u3002\u73b0\u6709LLM\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u540e\u8bad\u7ec3\u6216\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u534f\u4f5c\u7ea0\u9519\u548c\u4efb\u52a1\u7279\u5b9a\u68c0\u7d22\uff0c\u5e38\u5bfc\u81f4\u9519\u8bef\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u514d\u5fae\u8c03\u7684\u7aef\u5230\u7aef\u591a\u667a\u80fd\u4f53\u6846\u67b6MIRROR\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a(1)\u6267\u884c\u9a71\u52a8\u7684\u8fed\u4ee3\u81ea\u9002\u5e94\u4fee\u8ba2\u5b9e\u73b0\u81ea\u52a8\u7ea0\u9519\uff1b(2)\u5206\u5c42\u68c0\u7d22\u4ece\u7cbe\u5fc3\u7b56\u5212\u7684\u793a\u4f8b\u5e93\u4e2d\u83b7\u53d6\u76f8\u5173\u5efa\u6a21\u548c\u7f16\u7801\u793a\u4f8b\u3002", "result": "\u5728\u6807\u51c6OR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728IndustryOR\u548cMamo-ComplexLP\u7b49\u590d\u6742\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7cbe\u786e\u7684\u5916\u90e8\u77e5\u8bc6\u6ce8\u5165\u548c\u7cfb\u7edf\u5316\u7ea0\u9519\uff0cMIRROR\u4e3a\u975e\u4e13\u5bb6\u7528\u6237\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684OR\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u901a\u7528LLM\u5728\u4e13\u5bb6\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u6839\u672c\u9650\u5236\u3002", "topic": "code agent"}}
{"id": "2602.02564", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.02564", "abs": "https://arxiv.org/abs/2602.02564", "authors": ["Subhodeep Ghosh", "Bayan Divaaniaazar", "Md Ishat-E-Rabban", "Spencer Clarke", "Senjuti Basu Roy"], "title": "Label Curation Using Agentic AI", "comment": null, "summary": "Data annotation is essential for supervised learning, yet producing accurate, unbiased, and scalable labels remains challenging as datasets grow in size and modality. Traditional human-centric pipelines are costly, slow, and prone to annotator variability, motivating reliability-aware automated annotation. We present AURA (Agentic AI for Unified Reliability Modeling and Annotation Aggregation), an agentic AI framework for large-scale, multi-modal data annotation. AURA coordinates multiple AI agents to generate and validate labels without requiring ground truth. At its core, AURA adapts a classical probabilistic model that jointly infers latent true labels and annotator reliability via confusion matrices, using Expectation-Maximization to reconcile conflicting annotations and aggregate noisy predictions. Across the four benchmark datasets evaluated, AURA achieves accuracy improvements of up to 5.8% over baseline. In more challenging settings with poor quality annotators, the improvement is up to 50% over baseline. AURA also accurately estimates the reliability of annotators, allowing assessment of annotator quality even without any pre-validation steps.", "AI": {"tldr": "AURA\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53AI\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2aAI\u4ee3\u7406\u751f\u6210\u548c\u9a8c\u8bc1\u6807\u7b7e\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u6570\u636e\uff0c\u4f7f\u7528\u6982\u7387\u6a21\u578b\u8054\u5408\u63a8\u65ad\u6f5c\u5728\u771f\u5b9e\u6807\u7b7e\u548c\u6807\u6ce8\u8005\u53ef\u9760\u6027\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u9ad85.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\u6210\u672c\u9ad8\u3001\u901f\u5ea6\u6162\u4e14\u5b58\u5728\u6807\u6ce8\u8005\u5dee\u5f02\u6027\u95ee\u9898\uff0c\u968f\u7740\u6570\u636e\u96c6\u89c4\u6a21\u548c\u6a21\u6001\u7684\u589e\u52a0\uff0c\u9700\u8981\u53ef\u9760\u3001\u81ea\u52a8\u5316\u7684\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\u3002", "method": "AURA\u91c7\u7528\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u534f\u8c03\u591a\u4e2aAI\u4ee3\u7406\u751f\u6210\u548c\u9a8c\u8bc1\u6807\u7b7e\uff0c\u6838\u5fc3\u91c7\u7528\u7ecf\u5178\u6982\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u6dc6\u77e9\u9635\u8054\u5408\u63a8\u65ad\u6f5c\u5728\u771f\u5b9e\u6807\u7b7e\u548c\u6807\u6ce8\u8005\u53ef\u9760\u6027\uff0c\u4f7f\u7528\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u6765\u8c03\u548c\u51b2\u7a81\u6807\u6ce8\u5e76\u805a\u5408\u566a\u58f0\u9884\u6d4b\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cAURA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad85.8%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff1b\u5728\u6807\u6ce8\u8005\u8d28\u91cf\u8f83\u5dee\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u6539\u8fdb\u5e45\u5ea6\u53ef\u8fbe50%\uff1b\u540c\u65f6\u80fd\u51c6\u786e\u4f30\u8ba1\u6807\u6ce8\u8005\u53ef\u9760\u6027\uff0c\u65e0\u9700\u9884\u9a8c\u8bc1\u6b65\u9aa4\u3002", "conclusion": "AURA\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u6807\u6ce8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u667a\u80fd\u4f53AI\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u53ef\u9760\u6807\u7b7e\u5e76\u8bc4\u4f30\u6807\u6ce8\u8005\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6807\u6ce8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2602.03146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03146", "abs": "https://arxiv.org/abs/2602.03146", "authors": ["Santiago Cifuentes"], "title": "General Agents Contain World Models, even under Partial Observability and Stochasticity", "comment": "19 pages, 4 figures", "summary": "Deciding whether an agent possesses a model of its surrounding world is a fundamental step toward understanding its capabilities and limitations. In [10], it was shown that, within a particular framework, every almost optimal and general agent necessarily contains sufficient knowledge of its environment to allow an approximate reconstruction of it by querying the agent as a black box. This result relied on the assumptions that the agent is deterministic and that the environment is fully observable.\n  In this work, we remove both assumptions by extending the theorem to stochastic agents operating in partially observable environments. Fundamentally, this shows that stochastic agents cannot avoid learning their environment through the usage of randomization. We also strengthen the result by weakening the notion of generality, proving that less powerful agents already contain a model of the world in which they operate.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u5148\u524d\u5173\u4e8e\u667a\u80fd\u4f53\u4e16\u754c\u6a21\u578b\u7684\u7814\u7a76\uff0c\u5c06\u5b9a\u7406\u63a8\u5e7f\u5230\u968f\u673a\u667a\u80fd\u4f53\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\uff0c\u8bc1\u660e\u968f\u673a\u667a\u80fd\u4f53\u4e5f\u65e0\u6cd5\u907f\u514d\u5b66\u4e60\u5176\u73af\u5883\u6a21\u578b", "motivation": "\u5148\u524d\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u786e\u5b9a\u6027\u548c\u5b8c\u5168\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\uff0c\u51e0\u4e4e\u6700\u4f18\u7684\u901a\u7528\u667a\u80fd\u4f53\u5fc5\u7136\u5305\u542b\u5bf9\u5176\u73af\u5883\u7684\u5145\u5206\u77e5\u8bc6\u3002\u672c\u7814\u7a76\u65e8\u5728\u79fb\u9664\u8fd9\u4e24\u4e2a\u9650\u5236\u6761\u4ef6\uff0c\u63a2\u7d22\u5728\u66f4\u73b0\u5b9e\u573a\u666f\u4e0b\u667a\u80fd\u4f53\u662f\u5426\u4ecd\u4f1a\u5b66\u4e60\u73af\u5883\u6a21\u578b\u3002", "method": "\u6269\u5c55\u5148\u524d\u6846\u67b6\uff0c\u5c06\u5b9a\u7406\u63a8\u5e7f\u5230\u968f\u673a\u667a\u80fd\u4f53\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u968f\u673a\u667a\u80fd\u4f53\u65e0\u6cd5\u907f\u514d\u5b66\u4e60\u73af\u5883\u6a21\u578b\uff0c\u540c\u65f6\u5f31\u5316\"\u901a\u7528\u6027\"\u6982\u5ff5\uff0c\u8bc1\u660e\u80fd\u529b\u8f83\u5f31\u7684\u667a\u80fd\u4f53\u4e5f\u5305\u542b\u4e16\u754c\u6a21\u578b\u3002", "result": "\u6210\u529f\u5c06\u5b9a\u7406\u6269\u5c55\u5230\u968f\u673a\u667a\u80fd\u4f53\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\uff0c\u8bc1\u660e\u968f\u673a\u667a\u80fd\u4f53\u4e5f\u65e0\u6cd5\u907f\u514d\u5b66\u4e60\u73af\u5883\u6a21\u578b\u3002\u540c\u65f6\u901a\u8fc7\u5f31\u5316\u901a\u7528\u6027\u6982\u5ff5\uff0c\u8bc1\u660e\u80fd\u529b\u8f83\u5f31\u7684\u667a\u80fd\u4f53\u4e5f\u5305\u542b\u5bf9\u5176\u64cd\u4f5c\u4e16\u754c\u7684\u6a21\u578b\u3002", "conclusion": "\u968f\u673a\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u4e5f\u65e0\u6cd5\u907f\u514d\u5b66\u4e60\u5176\u73af\u5883\u6a21\u578b\uff0c\u8fd9\u8868\u660e\u5b66\u4e60\u73af\u5883\u6a21\u578b\u662f\u667a\u80fd\u4f53\u5b9e\u73b0\u826f\u597d\u6027\u80fd\u7684\u57fa\u672c\u8981\u6c42\uff0c\u5373\u4f7f\u5728\u4f7f\u7528\u968f\u673a\u5316\u548c\u9762\u5bf9\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "topic": "agent analysis"}}
{"id": "2602.03338", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03338", "abs": "https://arxiv.org/abs/2602.03338", "authors": ["Rakshith Vasudev", "Melisa Russak", "Dan Bikel", "Waseem Alshikh"], "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention", "comment": null, "summary": "Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.\n  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.", "AI": {"tldr": "LLM\u6279\u8bc4\u6a21\u578b\u7684\u4e3b\u52a8\u5e72\u9884\u4e0d\u4e00\u5b9a\u80fd\u63d0\u5347\u53ef\u9760\u6027\uff0c\u5373\u4f7f\u79bb\u7ebf\u51c6\u786e\u7387\u9ad8\uff08AUROC 0.94\uff09\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002\u4f5c\u8005\u63d0\u51fa\u90e8\u7f72\u524d\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u4efb\u52a1\u9884\u6d4b\u5e72\u9884\u6548\u679c\uff0c\u907f\u514d\u6709\u5bb3\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5047\u8bbeLLM\u6279\u8bc4\u6a21\u578b\u7684\u4e3b\u52a8\u5e72\u9884\u80fd\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u65f6\u7684\u6548\u679c\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u4f5c\u8005\u53d1\u73b0\u5373\u4f7f\u79bb\u7ebf\u51c6\u786e\u7387\u9ad8\u7684\u6279\u8bc4\u6a21\u578b\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u9000\u5316\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5e72\u9884\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\"\u7834\u574f-\u6062\u590d\u6743\u8861\"\u7406\u8bba\u6846\u67b6\uff0c\u8ba4\u4e3a\u5e72\u9884\u65e2\u80fd\u6062\u590d\u5931\u8d25\u8f68\u8ff9\uff0c\u4e5f\u53ef\u80fd\u7834\u574f\u539f\u672c\u4f1a\u6210\u529f\u7684\u8f68\u8ff9\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u90e8\u7f72\u524d\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4ec5\u970050\u4e2a\u4efb\u52a1\u7684\u8bd5\u70b9\u6570\u636e\u6765\u4f30\u8ba1\u5e72\u9884\u53ef\u80fd\u5e26\u6765\u7684\u5e2e\u52a9\u6216\u5371\u5bb3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e72\u9884\u6548\u679c\u9ad8\u5ea6\u53ef\u53d8\uff1a\u5728\u67d0\u4e2a\u6a21\u578b\u4e0a\u5bfc\u81f426\u4e2a\u767e\u5206\u70b9\u7684\u6027\u80fd\u5d29\u6e83\uff0c\u800c\u5728\u53e6\u4e00\u4e2a\u6a21\u578b\u4e0a\u51e0\u4e4e\u65e0\u5f71\u54cd\u3002\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e72\u9884\u5e26\u6765\u9002\u5ea6\u6539\u8fdb\uff08+2.8pp\uff0cp=0.014\uff09\u3002\u90e8\u7f72\u524d\u6d4b\u8bd5\u80fd\u51c6\u786e\u9884\u6d4b\u5e72\u9884\u6548\u679c\u3002", "conclusion": "LLM\u6279\u8bc4\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0d\u8db3\u4ee5\u51b3\u5b9a\u5e72\u9884\u662f\u5426\u5b89\u5168\u3002\u63d0\u51fa\u7684\u6846\u67b6\u4e3b\u8981\u4ef7\u503c\u5728\u4e8e\u8bc6\u522b\u4f55\u65f6\u4e0d\u5e94\u5e72\u9884\uff0c\u5728\u90e8\u7f72\u524d\u9884\u9632\u4e25\u91cd\u6027\u80fd\u9000\u5316\u3002\u5e72\u9884\u51b3\u7b56\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u800c\u975e\u4ec5\u4f9d\u8d56\u79bb\u7ebf\u51c6\u786e\u7387\u3002", "topic": "agent analysis"}}
{"id": "2602.03352", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03352", "abs": "https://arxiv.org/abs/2602.03352", "authors": ["Yunzhi Shen", "Hao Zhou", "Xin Huang", "Xue Han", "Junlan Feng", "Shujian Huang"], "title": "PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \\textbf{PEGRL}, a \\textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\\to$Finnish, English$\\to$Turkish, and English$\\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).", "AI": {"tldr": "PEGRL\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u540e\u7f16\u8f91\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u6765\u7a33\u5b9a\u673a\u5668\u7ffb\u8bd1\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u4e2d\u566a\u58f0\u5b66\u4e60\u4fe1\u53f7\u548c\u63a2\u7d22-\u4f18\u5316\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u8499\u7279\u5361\u6d1b\u56de\u62a5\u4f30\u8ba1\u4ea7\u751f\u7684\u566a\u58f0\u5b66\u4e60\u4fe1\u53f7\uff1b2) \u5de8\u5927\u7684\u8f68\u8ff9\u7a7a\u95f4\u5bfc\u81f4\u5168\u5c40\u63a2\u7d22\u4f18\u5148\u4e8e\u7ec6\u7c92\u5ea6\u5c40\u90e8\u4f18\u5316\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u7ffb\u8bd1\u8d28\u91cf\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5RL\u6846\u67b6PEGRL\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7ffb\u8bd1\u8f93\u51fa\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u7ffb\u8bd1\u8f93\u51fa\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u540e\u7f16\u8f91\u3002\u901a\u8fc7\u540e\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u540c\u65f6\u652f\u6301\u5168\u5c40\u63a2\u7d22\u548c\u5c40\u90e8\u4f18\u5316\u3002\u91c7\u7528\u4efb\u52a1\u7279\u5b9a\u6743\u91cd\u65b9\u6848\u5e73\u8861\u7ffb\u8bd1\u548c\u540e\u7f16\u8f91\u76ee\u6807\u3002", "result": "\u5728\u82f1\u8bed\u2192\u82ac\u5170\u8bed\u3001\u82f1\u8bed\u2192\u571f\u8033\u5176\u8bed\u548c\u82f1\u8bed\u2194\u4e2d\u6587\u7ffb\u8bd1\u4efb\u52a1\u4e0a\uff0cPEGRL\u76f8\u6bd4RL\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002\u82f1\u8bed\u2192\u571f\u8033\u5176\u8bed\u4efb\u52a1\u4e0a\uff0cCOMET-KIWI\u6307\u6807\u4e0e\u5148\u8fdb\u7684LLM\u7cfb\u7edf\uff08DeepSeek-V3.2\uff09\u76f8\u5f53\u3002", "conclusion": "PEGRL\u901a\u8fc7\u5f15\u5165\u540e\u7f16\u8f91\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u7ffb\u8bd1\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u63a2\u7d22-\u4f18\u5316\u5e73\u8861\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03219", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03219", "abs": "https://arxiv.org/abs/2602.03219", "authors": ["Guhong Chen", "Chenghao Sun", "Cheng Fu", "Qiyao Wang", "Zhihong Huang", "Chaopeng Wei", "Guangxu Chen", "Feiteng Fang", "Ahmadreza Argha", "Bing Zhao", "Xander Xu", "Qi Han", "Hamid Alinejad-Rokny", "Qiang Qu", "Binhua Li", "Shiwen Ni", "Min Yang", "Hu Wei", "Yongbin Li"], "title": "Beyond Quantity: Trajectory Diversity Scaling for Code Agents", "comment": null, "summary": "As code large language models (LLMs) evolve into tool-interactive agents via the Model Context Protocol (MCP), their generalization is increasingly limited by low-quality synthetic data and the diminishing returns of quantity scaling. Moreover, quantity-centric scaling exhibits an early bottleneck that underutilizes trajectory data. We propose TDScaling, a Trajectory Diversity Scaling-based data synthesis framework for code agents that scales performance through diversity rather than raw volume. Under a fixed training budget, increasing trajectory diversity yields larger gains than adding more trajectories, improving the performance-cost trade-off for agent training. TDScaling integrates four innovations: (1) a Business Cluster mechanism that captures real-service logical dependencies; (2) a blueprint-driven multi-agent paradigm that enforces trajectory coherence; (3) an adaptive evolution mechanism that steers synthesis toward long-tail scenarios using Domain Entropy, Reasoning Mode Entropy, and Cumulative Action Complexity to prevent mode collapse; and (4) a sandboxed code tool that mitigates catastrophic forgetting of intrinsic coding capabilities. Experiments on general tool-use benchmarks (BFCL, tau^2-Bench) and code agent tasks (RebenchT, CodeCI, BIRD) demonstrate a win-win outcome: TDScaling improves both tool-use generalization and inherent coding proficiency. We plan to release the full codebase and the synthesized dataset (including 30,000+ tool clusters) upon publication.", "AI": {"tldr": "TDScaling\u662f\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u591a\u6837\u6027\u800c\u975e\u6570\u636e\u91cf\u7684\u4ee3\u7801\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u6027\u6269\u5c55\u63d0\u5347\u6027\u80fd\uff0c\u5305\u542b\u4e1a\u52a1\u805a\u7c7b\u3001\u84dd\u56fe\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u3001\u81ea\u9002\u5e94\u8fdb\u5316\u673a\u5236\u548c\u6c99\u76d2\u4ee3\u7801\u5de5\u5177\u56db\u5927\u521b\u65b0\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7MCP\u6f14\u5316\u4e3a\u5de5\u5177\u4ea4\u4e92\u667a\u80fd\u4f53\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u5230\u4f4e\u8d28\u91cf\u5408\u6210\u6570\u636e\u548c\u6570\u91cf\u6269\u5c55\u6536\u76ca\u9012\u51cf\u7684\u9650\u5236\u3002\u6570\u91cf\u4e3a\u4e2d\u5fc3\u7684\u6269\u5c55\u5b58\u5728\u65e9\u671f\u74f6\u9888\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8f68\u8ff9\u6570\u636e\u3002", "method": "\u63d0\u51faTDScaling\u6846\u67b6\uff1a1\uff09\u4e1a\u52a1\u805a\u7c7b\u673a\u5236\u6355\u6349\u771f\u5b9e\u670d\u52a1\u903b\u8f91\u4f9d\u8d56\uff1b2\uff09\u84dd\u56fe\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u8303\u5f0f\u786e\u4fdd\u8f68\u8ff9\u8fde\u8d2f\u6027\uff1b3\uff09\u81ea\u9002\u5e94\u8fdb\u5316\u673a\u5236\u4f7f\u7528\u9886\u57df\u71b5\u3001\u63a8\u7406\u6a21\u5f0f\u71b5\u548c\u7d2f\u79ef\u52a8\u4f5c\u590d\u6742\u5ea6\u5f15\u5bfc\u5408\u6210\u8d70\u5411\u957f\u5c3e\u573a\u666f\uff1b4\uff09\u6c99\u76d2\u4ee3\u7801\u5de5\u5177\u9632\u6b62\u5185\u5728\u7f16\u7801\u80fd\u529b\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u901a\u7528\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff08BFCL\u3001tau^2-Bench\uff09\u548c\u4ee3\u7801\u667a\u80fd\u4f53\u4efb\u52a1\uff08RebenchT\u3001CodeCI\u3001BIRD\uff09\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cTDScaling\u5b9e\u73b0\u4e86\u53cc\u8d62\uff1a\u65e2\u63d0\u9ad8\u4e86\u5de5\u5177\u4f7f\u7528\u6cdb\u5316\u80fd\u529b\uff0c\u53c8\u589e\u5f3a\u4e86\u56fa\u6709\u7f16\u7801\u80fd\u529b\u3002", "conclusion": "TDScaling\u901a\u8fc7\u8f68\u8ff9\u591a\u6837\u6027\u6269\u5c55\u800c\u975e\u539f\u59cb\u6570\u636e\u91cf\u6269\u5c55\uff0c\u5728\u56fa\u5b9a\u8bad\u7ec3\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6210\u672c\u6743\u8861\uff0c\u4e3a\u4ee3\u7801\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5408\u6210\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2602.03224", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03224", "abs": "https://arxiv.org/abs/2602.03224", "authors": ["Yu Cheng", "Jiuan Zhou", "Yongkang Hu", "Yihang Chen", "Huichi Zhou", "Mingang Chen", "Zhizhong Zhang", "Kun Shao", "Yuan Xie", "Zhaoxia Yin"], "title": "TAME: A Trustworthy Test-Time Evolution of Agent Memory with Systematic Benchmarking", "comment": null, "summary": "Test-time evolution of agent memory serves as a pivotal paradigm for achieving AGI by bolstering complex reasoning through experience accumulation. However, even during benign task evolution, agent safety alignment remains vulnerable-a phenomenon known as Agent Memory Misevolution. To evaluate this phenomenon, we construct the Trust-Memevo benchmark to assess multi-dimensional trustworthiness during benign task evolution, revealing an overall decline in trustworthiness across various task domains and evaluation settings. To address this issue, we propose TAME, a dual-memory evolutionary framework that separately evolves executor memory to improve task performance by distilling generalizable methodologies, and evaluator memory to refine assessments of both safety and task utility based on historical feedback. Through a closed loop of memory filtering, draft generation, trustworthy refinement, execution, and dual-track memory updating, TAME preserves trustworthiness without sacrificing utility. Experiments demonstrate that TAME mitigates misevolution, achieving a joint improvement in both trustworthiness and task performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTAME\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8bb0\u5fc6\u8fdb\u5316\u673a\u5236\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u4efb\u52a1\u6f14\u5316\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u4fe1\u4efb\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u667a\u80fd\u4f53\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u8bb0\u5fc6\u6f14\u5316\u79ef\u7d2f\u7ecf\u9a8c\u662f\u5b9e\u73b0AGI\u7684\u91cd\u8981\u9014\u5f84\uff0c\u4f46\u5373\u4f7f\u5728\u826f\u6027\u4efb\u52a1\u6f14\u5316\u8fc7\u7a0b\u4e2d\uff0c\u667a\u80fd\u4f53\u7684\u5b89\u5168\u5bf9\u9f50\u4ecd\u7136\u8106\u5f31\uff0c\u5b58\u5728\"Agent Memory Misevolution\"\u73b0\u8c61\uff0c\u5bfc\u81f4\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51faTAME\u53cc\u8bb0\u5fc6\u8fdb\u5316\u6846\u67b6\uff1a1) \u6267\u884c\u5668\u8bb0\u5fc6\u8fdb\u5316\uff0c\u901a\u8fc7\u63d0\u70bc\u53ef\u6cdb\u5316\u7684\u65b9\u6cd5\u8bba\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff1b2) \u8bc4\u4f30\u5668\u8bb0\u5fc6\u8fdb\u5316\uff0c\u57fa\u4e8e\u5386\u53f2\u53cd\u9988\u7cbe\u5316\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6548\u7528\u7684\u8bc4\u4f30\u3002\u901a\u8fc7\u8bb0\u5fc6\u8fc7\u6ee4\u3001\u8349\u7a3f\u751f\u6210\u3001\u53ef\u4fe1\u5ea6\u7cbe\u5316\u3001\u6267\u884c\u548c\u53cc\u8f68\u8bb0\u5fc6\u66f4\u65b0\u7684\u95ed\u73af\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTAME\u80fd\u591f\u7f13\u89e3\u8bb0\u5fc6\u9519\u8bef\u6f14\u5316\uff0c\u5728\u4fe1\u4efb\u5ea6\u548c\u4efb\u52a1\u6027\u80fd\u4e24\u65b9\u9762\u90fd\u5b9e\u73b0\u4e86\u8054\u5408\u6539\u8fdb\u3002Trust-Memevo\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "TAME\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u6f14\u5316\u6267\u884c\u5668\u548c\u8bc4\u4f30\u5668\u8bb0\u5fc6\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6548\u7528\u7684\u540c\u65f6\u4fdd\u62a4\u4e86\u4fe1\u4efb\u5ea6\uff0c\u4e3a\u89e3\u51b3\u667a\u80fd\u4f53\u8bb0\u5fc6\u6f14\u5316\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.03412", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03412", "abs": "https://arxiv.org/abs/2602.03412", "authors": ["Mukai Li", "Qingcheng Zeng", "Tianqing Fang", "Zhenwen Liang", "Linfeng Song", "Qi Liu", "Haitao Mi", "Dong Yu"], "title": "Verified Critical Step Optimization for LLM Agents", "comment": "Working in progress", "summary": "As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.", "AI": {"tldr": "CSO\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5173\u952e\u6b65\u9aa4\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5173\u952e\u51b3\u7b56\u70b9\u6765\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u8868\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u76d1\u7763\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u65b9\u6cd5\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u4ec5\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u65e0\u6cd5\u7cbe\u786e\u5f52\u56e0\u5230\u4e2d\u95f4\u6b65\u9aa4\uff1b2) \u4f30\u8ba1\u7684\u6b65\u9aa4\u7ea7\u5956\u52b1\u5b58\u5728\u7cfb\u7edf\u6027\u566a\u58f0\uff1b3) \u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u3001\u9ad8\u6548\u7684\u76d1\u7763\u65b9\u6cd5\u3002", "method": "CSO\u65b9\u6cd5\u805a\u7126\u4e8e\u9a8c\u8bc1\u5173\u952e\u6b65\u9aa4\uff08\u51b3\u7b56\u70b9\uff09\uff0c\u4ece\u5931\u8d25\u7684\u7b56\u7565\u8f68\u8ff9\u5f00\u59cb\uff0c\u4f7f\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc6\u522b\u5019\u9009\u5173\u952e\u6b65\u9aa4\uff0c\u5229\u7528\u4e13\u5bb6\u6a21\u578b\u63d0\u51fa\u9ad8\u8d28\u91cf\u66ff\u4ee3\u65b9\u6848\uff0c\u7136\u540e\u8ba9\u7b56\u7565\u6a21\u578b\u4ece\u8fd9\u4e9b\u66ff\u4ee3\u65b9\u6848\u7ee7\u7eed\u6267\u884c\u76f4\u5230\u4efb\u52a1\u5b8c\u6210\u3002\u53ea\u6709\u6210\u529f\u7ea0\u6b63\u7ed3\u679c\u7684\u66ff\u4ee3\u65b9\u6848\u624d\u88ab\u9a8c\u8bc1\u5e76\u7528\u4f5cDPO\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728GAIA-Text-103\u548cXBench-DeepSearch\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCSO\u76f8\u6bd4SFT\u57fa\u7ebf\u5206\u522b\u5b9e\u73b0\u4e8637%\u548c26%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u65f6\u4ec5\u9700\u76d1\u7763\u8f68\u8ff9\u4e2d16%\u7684\u6b65\u9aa4\u3002", "conclusion": "\u57fa\u4e8e\u9009\u62e9\u6027\u9a8c\u8bc1\u7684\u5b66\u4e60\u65b9\u6cd5\u5bf9\u4ee3\u7406\u540e\u8bad\u7ec3\u975e\u5e38\u6709\u6548\uff0c\u80fd\u591f\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u53ef\u9a8c\u8bc1\u7684\u76d1\u7763\uff0c\u540c\u65f6\u907f\u514d\u8f68\u8ff9\u7ea7\u7c97\u7cd9\u6027\u548c\u6b65\u9aa4\u7ea7\u566a\u58f0\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2602.03238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03238", "abs": "https://arxiv.org/abs/2602.03238", "authors": ["Pengyu Zhu", "Li Sun", "Philip S. Yu", "Sen Su"], "title": "The Necessity of a Unified Framework for LLM-Based Agent Evaluation", "comment": null, "summary": "With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u667a\u80fd\u4f53\u8bc4\u4f30\u4e2d\u7684\u6807\u51c6\u5316\u95ee\u9898\uff0c\u5305\u62ec\u7cfb\u7edf\u63d0\u793a\u3001\u5de5\u5177\u914d\u7f6e\u548c\u73af\u5883\u52a8\u6001\u7b49\u6df7\u6742\u56e0\u7d20", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u8bc4\u4f30\u5b58\u5728\u4e25\u91cd\u6df7\u6742\u56e0\u7d20\uff1a\u7cfb\u7edf\u63d0\u793a\u3001\u5de5\u5177\u914d\u7f6e\u3001\u73af\u5883\u52a8\u6001\u7b49\u5dee\u5f02\u5bfc\u81f4\u96be\u4ee5\u5c06\u6027\u80fd\u63d0\u5347\u5f52\u56e0\u4e8e\u6a21\u578b\u672c\u8eab\uff1b\u7f3a\u4e4f\u6807\u51c6\u5316\u73af\u5883\u6570\u636e\u5bfc\u81f4\u4e0d\u53ef\u8ffd\u8e2a\u7684\u9519\u8bef\u548c\u4e0d\u53ef\u590d\u73b0\u7684\u7ed3\u679c\uff1b\u788e\u7247\u5316\u7684\u7814\u7a76\u8005\u7279\u5b9a\u6846\u67b6\u5f15\u5165\u4e0d\u516c\u5e73\u6027\u548c\u4e0d\u900f\u660e\u6027", "method": "\u63d0\u51fa\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u6765\u6807\u51c6\u5316\u667a\u80fd\u4f53\u8bc4\u4f30\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u4e2d\u7684\u6df7\u6742\u56e0\u7d20\u548c\u6807\u51c6\u5316\u95ee\u9898", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e8\u5728\u6807\u51c6\u5316\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u63d0\u6848\uff0c\u4f46\u5177\u4f53\u5b9e\u65bd\u7ec6\u8282\u548c\u5b9e\u9a8c\u7ed3\u679c\u5728\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e", "conclusion": "\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u5bf9\u4e8e\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u4e25\u8c28\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u6807\u51c6\u5316\u80fd\u591f\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u4e2d\u7684\u4e0d\u516c\u5e73\u6027\u548c\u4e0d\u900f\u660e\u6027\u95ee\u9898", "topic": "agent analysis"}}
{"id": "2602.02572", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02572", "abs": "https://arxiv.org/abs/2602.02572", "authors": ["Haichuan Wang", "Tao Lin", "Lingkai Kong", "Ce Li", "Hezi Jiang", "Milind Tambe"], "title": "Reward Shaping for Inference-Time Alignment: A Stackelberg Game Perspective", "comment": null, "summary": "Existing alignment methods directly use the reward model learned from user preference data to optimize an LLM policy, subject to KL regularization with respect to the base policy. This practice is suboptimal for maximizing user's utility because the KL regularization may cause the LLM to inherit the bias in the base policy that conflicts with user preferences. While amplifying rewards for preferred outputs can mitigate this bias, it also increases the risk of reward hacking. This tradeoff motivates the problem of optimally designing reward models under KL regularization. We formalize this reward model optimization problem as a Stackelberg game, and show that a simple reward shaping scheme can effectively approximate the optimal reward model. We empirically evaluate our method in inference-time alignment settings and demonstrate that it integrates seamlessly into existing alignment methods with minimal overhead. Our method consistently improves average reward and achieves win-tie rates exceeding 66% against all baselines, averaged across evaluation settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Stackelberg\u535a\u5f08\u5f62\u5f0f\u5316KL\u6b63\u5219\u5316\u4e0b\u7684\u5956\u52b1\u6a21\u578b\u8bbe\u8ba1\u95ee\u9898\uff0c\u91c7\u7528\u7b80\u5355\u7684\u5956\u52b1\u5851\u9020\u65b9\u6848\u8fd1\u4f3c\u6700\u4f18\u5956\u52b1\u6a21\u578b\uff0c\u5728\u63a8\u7406\u65f6\u5bf9\u9f50\u8bbe\u7f6e\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528\u4ece\u7528\u6237\u504f\u597d\u6570\u636e\u5b66\u4e60\u7684\u5956\u52b1\u6a21\u578b\u6765\u4f18\u5316LLM\u7b56\u7565\uff0c\u5e76\u53d7\u9650\u4e8e\u4e0e\u57fa\u7840\u7b56\u7565\u7684KL\u6b63\u5219\u5316\u3002\u8fd9\u79cd\u505a\u6cd5\u5728\u6700\u5927\u5316\u7528\u6237\u6548\u7528\u65b9\u9762\u662f\u6b21\u4f18\u7684\uff0c\u56e0\u4e3aKL\u6b63\u5219\u5316\u53ef\u80fd\u5bfc\u81f4LLM\u7ee7\u627f\u57fa\u7840\u7b56\u7565\u4e2d\u4e0e\u7528\u6237\u504f\u597d\u51b2\u7a81\u7684\u504f\u5dee\u3002\u867d\u7136\u653e\u5927\u504f\u597d\u8f93\u51fa\u7684\u5956\u52b1\u53ef\u4ee5\u51cf\u8f7b\u8fd9\u79cd\u504f\u5dee\uff0c\u4f46\u4e5f\u4f1a\u589e\u52a0\u5956\u52b1\u9ed1\u5ba2\u7684\u98ce\u9669\u3002", "method": "\u5c06\u5956\u52b1\u6a21\u578b\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u4e3aStackelberg\u535a\u5f08\uff0c\u63d0\u51fa\u7b80\u5355\u7684\u5956\u52b1\u5851\u9020\u65b9\u6848\u6765\u6709\u6548\u8fd1\u4f3c\u6700\u4f18\u5956\u52b1\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u4e2d\uff0c\u5f00\u9500\u6700\u5c0f\u3002", "result": "\u5728\u63a8\u7406\u65f6\u5bf9\u9f50\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u65b9\u6cd5\u59cb\u7ec8\u63d0\u9ad8\u5e73\u5747\u5956\u52b1\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u8d85\u8fc766%\u7684\u80dc\u5e73\u7387\uff08\u5e73\u5747\u8de8\u8bc4\u4f30\u8bbe\u7f6e\uff09\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5956\u52b1\u6a21\u578b\u8bbe\u8ba1\u800c\u975e\u76f4\u63a5\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5e73\u8861KL\u6b63\u5219\u5316\u4e0e\u7528\u6237\u504f\u597d\u5bf9\u9f50\uff0c\u63d0\u9ad8LLM\u7b56\u7565\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03442", "abs": "https://arxiv.org/abs/2602.03442", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Shaohan Wang", "Pengyu Wang", "Xiaorui Wang", "Zhendong Mao"], "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces", "comment": "18 pages, 8 figures", "summary": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.", "AI": {"tldr": "A-RAG\u662f\u4e00\u4e2a\u4ee3\u7406\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u6a21\u578b\u66b4\u9732\u5206\u5c42\u68c0\u7d22\u63a5\u53e3\uff0c\u8ba9\u6a21\u578b\u81ea\u4e3b\u53c2\u4e0e\u68c0\u7d22\u51b3\u7b56\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5229\u7528\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5229\u7528\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u957f\u7a0b\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u5b83\u4eec\u8981\u4e48\u91c7\u7528\u5355\u6b21\u68c0\u7d22\u7b97\u6cd5\uff0c\u8981\u4e48\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7a0b\uff0c\u90fd\u4e0d\u5141\u8bb8\u6a21\u578b\u53c2\u4e0e\u68c0\u7d22\u51b3\u7b56\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6539\u8fdb\u5e26\u6765\u7684\u6548\u7387\u63d0\u5347\u3002", "method": "A-RAG\u6846\u67b6\u5411\u6a21\u578b\u66b4\u9732\u5206\u5c42\u68c0\u7d22\u63a5\u53e3\uff0c\u63d0\u4f9b\u4e09\u79cd\u68c0\u7d22\u5de5\u5177\uff1a\u5173\u952e\u8bcd\u641c\u7d22\u3001\u8bed\u4e49\u641c\u7d22\u548c\u5757\u8bfb\u53d6\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u8de8\u591a\u4e2a\u7c92\u5ea6\u81ea\u9002\u5e94\u5730\u641c\u7d22\u548c\u68c0\u7d22\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u653e\u57dfQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cA-RAG\u5728\u68c0\u7d22token\u6570\u91cf\u76f8\u5f53\u6216\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u660eA-RAG\u80fd\u6709\u6548\u5229\u7528\u6a21\u578b\u80fd\u529b\u5e76\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7684RAG\u4efb\u52a1\u3002", "conclusion": "A-RAG\u901a\u8fc7\u8ba9\u6a21\u578b\u53c2\u4e0e\u68c0\u7d22\u51b3\u7b56\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u5bf9A-RAG\u6027\u80fd\u7684\u5f71\u54cd\u3002", "topic": "code agent"}}
{"id": "2602.03255", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03255", "abs": "https://arxiv.org/abs/2602.03255", "authors": ["Tianyu Chen", "Chujia Hu", "Ge Gao", "Dongrui Liu", "Xia Hu", "Wenjie Wang"], "title": "LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios", "comment": null, "summary": "Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.", "AI": {"tldr": "LPS-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u57fa\u4e8eMCP\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u89c4\u5212\u65f6\u5b89\u5168\u610f\u8bc6\u7684\u57fa\u51c6\uff0c\u6db5\u76d665\u4e2a\u573a\u666f\u30017\u4e2a\u4efb\u52a1\u9886\u57df\u548c9\u79cd\u98ce\u9669\u7c7b\u578b\uff0c\u63ed\u793a\u73b0\u6709\u4ee3\u7406\u5728\u5b89\u5168\u884c\u4e3a\u4fdd\u6301\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u77ed\u65f6\u7a0b\u6216GUI\u4efb\u52a1\uff0c\u8bc4\u4f30\u6267\u884c\u65f6\u9519\u8bef\u4f46\u5ffd\u89c6\u4e86\u89c4\u5212\u65f6\u98ce\u9669\u9884\u6d4b\u80fd\u529b\u3002\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u9762\u4e34\u6a21\u7cca\u6307\u4ee4\u89e6\u53d1\u6709\u5bb3\u64cd\u4f5c\u548c\u5bf9\u6297\u6027\u7528\u6237\u64cd\u7eb5\u5de5\u5177\u6267\u884c\u7684\u98ce\u9669\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u89c4\u5212\u65f6\u5b89\u5168\u3002", "method": "\u63d0\u51faLPS-Bench\u57fa\u51c6\uff0c\u91c7\u7528\u591a\u4ee3\u7406\u81ea\u52a8\u7ba1\u9053\u8fdb\u884c\u53ef\u6269\u5c55\u6570\u636e\u751f\u6210\uff0c\u4f7f\u7528LLM-as-a-judge\u8bc4\u4f30\u534f\u8bae\u901a\u8fc7\u89c4\u5212\u8f68\u8ff9\u8bc4\u4f30\u5b89\u5168\u610f\u8bc6\uff0c\u6db5\u76d665\u4e2a\u573a\u666f\u30017\u4e2a\u4efb\u52a1\u9886\u57df\u548c9\u79cd\u98ce\u9669\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5728\u4fdd\u6301\u5b89\u5168\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u5206\u6790\u4e86\u98ce\u9669\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u57fa\u4e8eMCP\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7cfb\u7edf\u957f\u65f6\u7a0b\u89c4\u5212\u5b89\u5168\u7684\u7f13\u89e3\u7b56\u7565\u3002", "conclusion": "LPS-Bench\u586b\u8865\u4e86\u89c4\u5212\u65f6\u5b89\u5168\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7406\u7684\u5b89\u5168\u7f3a\u9677\uff0c\u4e3a\u6539\u8fdb\u57fa\u4e8eMCP\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7cfb\u7edf\u7684\u957f\u65f6\u7a0b\u89c4\u5212\u5b89\u5168\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5206\u6790\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2602.03279", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03279", "abs": "https://arxiv.org/abs/2602.03279", "authors": ["Zhengbo Jiao", "Shaobo Wang", "Zifan Zhang", "Xuan Ren", "Wei Wang", "Bing Zhao", "Hu Wei", "Linfeng Zhang"], "title": "Agentic Proposing: Enhancing Large Language Model Reasoning via Compositional Skill Synthesis", "comment": "23page4", "summary": "Advancing complex reasoning in large language models relies on high-quality, verifiable datasets, yet human annotation remains cost-prohibitive and difficult to scale. Current synthesis paradigms often face a recurring trade-off: maintaining structural validity typically restricts problem complexity, while relaxing constraints to increase difficulty frequently leads to inconsistent or unsolvable instances. To address this, we propose Agentic Proposing, a framework that models problem synthesis as a goal-driven sequential decision process where a specialized agent dynamically selects and composes modular reasoning skills. Through an iterative workflow of internal reflection and tool-use, we develop the Agentic-Proposer-4B using Multi-Granularity Policy Optimization (MGPO) to generate high-precision, verifiable training trajectories across mathematics, coding, and science. Empirical results demonstrate that downstream solvers trained on agent-synthesized data significantly outperform leading baselines and exhibit robust cross-domain generalization. Notably, a 30B solver trained on only 11,000 synthesized trajectories achieves a state-of-the-art 91.6% accuracy on AIME25, rivaling frontier-scale proprietary models such as GPT-5 and proving that a small volume of high-quality synthetic signals can effectively substitute for massive human-curated datasets.", "AI": {"tldr": "\u63d0\u51faAgentic Proposing\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u4ee3\u7406\u52a8\u6001\u9009\u62e9\u548c\u7ec4\u5408\u6a21\u5757\u5316\u63a8\u7406\u6280\u80fd\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u53ef\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa\u7684\u6c42\u89e3\u5668\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u53ef\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u9762\u4e34\u4e24\u96be\uff1a\u4fdd\u6301\u7ed3\u6784\u6709\u6548\u6027\u4f1a\u9650\u5236\u95ee\u9898\u590d\u6742\u5ea6\uff0c\u800c\u653e\u5bbd\u7ea6\u675f\u589e\u52a0\u96be\u5ea6\u53c8\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6216\u4e0d\u53ef\u89e3\u5b9e\u4f8b\u3002", "method": "\u63d0\u51faAgentic Proposing\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5408\u6210\u5efa\u6a21\u4e3a\u76ee\u6807\u9a71\u52a8\u7684\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4e13\u95e8\u4ee3\u7406\u52a8\u6001\u9009\u62e9\u548c\u7ec4\u5408\u6a21\u5757\u5316\u63a8\u7406\u6280\u80fd\u3002\u901a\u8fc7\u5185\u90e8\u53cd\u601d\u548c\u5de5\u5177\u4f7f\u7528\u7684\u8fed\u4ee3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u7528\u591a\u7c92\u5ea6\u7b56\u7565\u4f18\u5316\uff08MGPO\uff09\u5f00\u53d1Agentic-Proposer-4B\uff0c\u751f\u6210\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u7684\u9ad8\u7cbe\u5ea6\u53ef\u9a8c\u8bc1\u8bad\u7ec3\u8f68\u8ff9\u3002", "result": "\u5728\u4ee3\u7406\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u4e0b\u6e38\u6c42\u89e3\u5668\u663e\u8457\u4f18\u4e8e\u9886\u5148\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002\u4ec5\u752811,000\u4e2a\u5408\u6210\u8f68\u8ff9\u8bad\u7ec3\u768430B\u6c42\u89e3\u5668\u5728AIME25\u4e0a\u8fbe\u523091.6%\u7684\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u5ab2\u7f8eGPT-5\u7b49\u524d\u6cbf\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "\u5c11\u91cf\u9ad8\u8d28\u91cf\u5408\u6210\u4fe1\u53f7\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u4e86\u4ee3\u7406\u9a71\u52a8\u5408\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2602.03507", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03507", "abs": "https://arxiv.org/abs/2602.03507", "authors": ["Runquan Gui", "Yafu Li", "Xiaoye Qu", "Ziyan Liu", "Yeqiu Cheng", "Yu Cheng"], "title": "Learning to Reason Faithfully through Step-Level Faithfulness Maximization", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.", "AI": {"tldr": "FaithRL\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u5fe0\u5b9e\u6027\u6765\u51cf\u5c11LLM\u7684\u5e7b\u89c9\uff0c\u4f7f\u7528\u51e0\u4f55\u5956\u52b1\u8bbe\u8ba1\u548c\u5fe0\u5b9e\u6027\u611f\u77e5\u7684\u4f18\u52bf\u8c03\u5236\u673a\u5236\u6765\u60e9\u7f5a\u65e0\u652f\u6301\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u76d1\u7763\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u548c\u865a\u5047\u63a8\u7406\uff0c\u589e\u52a0\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faFaithRL\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5fe0\u5b9e\u6027\u6700\u5927\u5316\u76ee\u6807\uff0c\u5f15\u5165\u51e0\u4f55\u5956\u52b1\u8bbe\u8ba1\u548c\u5fe0\u5b9e\u6027\u611f\u77e5\u7684\u4f18\u52bf\u8c03\u5236\u673a\u5236\uff0c\u4e3a\u6b65\u9aa4\u7ea7\u5206\u914d\u4fe1\u7528\uff0c\u60e9\u7f5a\u65e0\u652f\u6301\u7684\u6b65\u9aa4\u540c\u65f6\u4fdd\u7559\u6709\u6548\u7684\u90e8\u5206\u63a8\u5bfc\u3002", "result": "\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaithRL\u4e00\u81f4\u964d\u4f4e\u4e86\u5e7b\u89c9\u7387\uff0c\u540c\u65f6\u4fdd\u6301\uff08\u5e76\u7ecf\u5e38\u63d0\u9ad8\uff09\u7b54\u6848\u6b63\u786e\u6027\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u786e\u8ba4FaithRL\u63d0\u9ad8\u4e86\u6b65\u9aa4\u7ea7\u63a8\u7406\u5fe0\u5b9e\u6027\u5e76\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FaithRL\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u5fe0\u5b9e\u6027\u6709\u6548\u51cf\u5c11LLM\u5e7b\u89c9\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u6539\u5584\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03285", "abs": "https://arxiv.org/abs/2602.03285", "authors": ["Yuelin Hu", "Jun Xu", "Bingcong Lu", "Zhengxue Cheng", "Hongwei Hu", "Ronghua Wu", "Li Song"], "title": "MeetBench-XL: Calibrated Multi-Dimensional Evaluation and Learned Dual-Policy Agents for Real-Time Meetings", "comment": "accepted by AAAI2026 ws", "summary": "Enterprise meeting environments require AI assistants that handle diverse operational tasks, from rapid fact checking during live discussions to cross meeting analysis for strategic planning, under strict latency, cost, and privacy constraints. Existing meeting benchmarks mainly focus on simplified question answering and fail to reflect real world enterprise workflows, where queries arise organically from multi stakeholder collaboration, span long temporal contexts, and require tool augmented reasoning.\n  We address this gap through a grounded dataset and a learned agent framework. First, we introduce MeetAll, a bilingual and multimodal corpus derived from 231 enterprise meetings totaling 140 hours. Questions are injected using an enterprise informed protocol validated by domain expert review and human discriminability studies. Unlike purely synthetic benchmarks, this protocol is grounded in four enterprise critical dimensions: cognitive load, temporal context span, domain expertise, and actionable task execution, calibrated through interviews with stakeholders across finance, healthcare, and technology sectors.\n  Second, we propose MeetBench XL, a multi dimensional evaluation protocol aligned with human judgment that measures factual fidelity, intent alignment, response efficiency, structural clarity, and completeness. Third, we present MeetMaster XL, a learned dual policy agent that jointly optimizes query routing between fast and slow reasoning paths and tool invocation, including retrieval, cross meeting aggregation, and web search. A lightweight classifier enables accurate routing with minimal overhead, achieving a superior quality latency tradeoff over single model baselines. Experiments against commercial systems show consistent gains, supported by ablations, robustness tests, and a real world deployment case study.Resources: https://github.com/huyuelin/MeetBench.", "AI": {"tldr": "MeetAll\u6570\u636e\u96c6\u4e0eMeetMaster XL\u4ee3\u7406\u6846\u67b6\uff1a\u9488\u5bf9\u4f01\u4e1a\u4f1a\u8bae\u73af\u5883\u7684AI\u52a9\u624b\uff0c\u901a\u8fc7\u53cc\u8bed\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u53cc\u7b56\u7565\u4ee3\u7406\u4f18\u5316\u67e5\u8be2\u8def\u7531\u4e0e\u5de5\u5177\u8c03\u7528\uff0c\u5728\u5ef6\u8fdf\u3001\u6210\u672c\u3001\u9690\u79c1\u7ea6\u675f\u4e0b\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u54cd\u5e94\u6548\u7387\u3002", "motivation": "\u4f01\u4e1a\u4f1a\u8bae\u73af\u5883\u9700\u8981\u80fd\u591f\u5904\u7406\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u7684AI\u52a9\u624b\uff0c\u5305\u62ec\u5b9e\u65f6\u4e8b\u5b9e\u6838\u67e5\u548c\u8de8\u4f1a\u8bae\u6218\u7565\u5206\u6790\uff0c\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684\u5ef6\u8fdf\u3001\u6210\u672c\u548c\u9690\u79c1\u7ea6\u675f\u3002\u73b0\u6709\u4f1a\u8bae\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7b80\u5316\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5176\u4e2d\u67e5\u8be2\u6765\u81ea\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u534f\u4f5c\u3001\u8de8\u8d8a\u957f\u65f6\u95f4\u4e0a\u4e0b\u6587\u3001\u9700\u8981\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u3002", "method": "1) \u5f15\u5165MeetAll\u53cc\u8bed\u591a\u6a21\u6001\u8bed\u6599\u5e93\uff0c\u57fa\u4e8e231\u4e2a\u4f01\u4e1a\u4f1a\u8bae\uff08140\u5c0f\u65f6\uff09\uff0c\u91c7\u7528\u4f01\u4e1a\u9a8c\u8bc1\u7684\u534f\u8bae\u6ce8\u5165\u95ee\u9898\uff1b2) \u63d0\u51faMeetBench XL\u591a\u7ef4\u5ea6\u8bc4\u4f30\u534f\u8bae\uff0c\u8861\u91cf\u4e8b\u5b9e\u4fdd\u771f\u5ea6\u3001\u610f\u56fe\u5bf9\u9f50\u3001\u54cd\u5e94\u6548\u7387\u3001\u7ed3\u6784\u6e05\u6670\u5ea6\u548c\u5b8c\u6574\u6027\uff1b3) \u63d0\u51faMeetMaster XL\u53cc\u7b56\u7565\u4ee3\u7406\uff0c\u8054\u5408\u4f18\u5316\u5feb\u901f\u4e0e\u6162\u901f\u63a8\u7406\u8def\u5f84\u95f4\u7684\u67e5\u8be2\u8def\u7531\u4ee5\u53ca\u5de5\u5177\u8c03\u7528\uff08\u68c0\u7d22\u3001\u8de8\u4f1a\u8bae\u805a\u5408\u3001\u7f51\u7edc\u641c\u7d22\uff09\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u5b9e\u73b0\u51c6\u786e\u8def\u7531\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5546\u4e1a\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u4ee5\u6700\u5c0f\u5f00\u9500\u5b9e\u73b0\u51c6\u786e\u8def\u7531\uff0c\u5728\u8d28\u91cf-\u5ef6\u8fdf\u6743\u8861\u4e0a\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u3001\u9c81\u68d2\u6027\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u6848\u4f8b\u7814\u7a76\u652f\u6301\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u63a5\u5730\u6c14\u7684\u6570\u636e\u96c6\u548c\u5b66\u4e60\u578b\u4ee3\u7406\u6846\u67b6\u89e3\u51b3\u4e86\u4f01\u4e1a\u4f1a\u8baeAI\u52a9\u624b\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u5728\u56db\u4e2a\u5173\u952e\u4f01\u4e1a\u7ef4\u5ea6\uff08\u8ba4\u77e5\u8d1f\u8377\u3001\u65f6\u95f4\u4e0a\u4e0b\u6587\u8de8\u5ea6\u3001\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3001\u53ef\u6267\u884c\u4efb\u52a1\uff09\u4e0a\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u548c\u4f18\u5316\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.03548", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03548", "abs": "https://arxiv.org/abs/2602.03548", "authors": ["Yuqin Dai", "Ning Gao", "Wei Zhang", "Jie Wang", "Zichen Luo", "Jinpeng Wang", "Yujie Wang", "Ruiyuan Wu", "Chaozheng Wang"], "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.", "AI": {"tldr": "SEAD\u662f\u4e00\u4e2a\u7528\u4e8e\u670d\u52a1\u5bf9\u8bdd\u7684\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7528\u6237\u5efa\u6a21\u4e3a\u914d\u7f6e\u6587\u4ef6\u63a7\u5236\u5668\u548c\u7528\u6237\u89d2\u8272\u626e\u6f14\u6a21\u578b\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b66\u4e60\u6709\u6548\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u5bf9\u8bdd\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u670d\u52a1\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u4f9d\u8d56\u566a\u58f0\u5927\u3001\u8d28\u91cf\u4f4e\u7684\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\uff0c\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u548c\u96be\u4ee5\u6a21\u62df\u771f\u5b9e\u76ee\u6807\u5bfc\u5411\u7528\u6237\u884c\u4e3a\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSEAD\u6846\u67b6\uff0c\u5c06\u7528\u6237\u5efa\u6a21\u89e3\u8026\u4e3a\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u914d\u7f6e\u6587\u4ef6\u63a7\u5236\u5668\uff0c\u751f\u6210\u591a\u6837\u5316\u7528\u6237\u72b6\u6001\u4ee5\u7ba1\u7406\u8bad\u7ec3\u8bfe\u7a0b\uff1b2) \u7528\u6237\u89d2\u8272\u626e\u6f14\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u771f\u5b9e\u89d2\u8272\u626e\u6f14\u3002\u8fd9\u79cd\u8bbe\u8ba1\u786e\u4fdd\u73af\u5883\u63d0\u4f9b\u9002\u5e94\u6027\u8bad\u7ec3\u573a\u666f\u800c\u975e\u4e0d\u516c\u5e73\u7684\u5bf9\u624b\u3002", "result": "SEAD\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u548c\u95ed\u6e90\u5546\u4e1a\u6a21\u578b\uff0c\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad8\u4e8617.6%\uff0c\u5bf9\u8bdd\u6548\u7387\u63d0\u9ad8\u4e8611.1%\u3002", "conclusion": "SEAD\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u8fdb\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5bf9\u8bdd\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u7528\u6237\u884c\u4e3a\u6a21\u62df\u95ee\u9898\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u5bf9\u8bdd\u4ee3\u7406\u3002", "topic": "agent analysis"}}
{"id": "2602.03315", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03315", "abs": "https://arxiv.org/abs/2602.03315", "authors": ["Menglin Xia", "Xuchao Zhang", "Shantanu Dixit", "Paramaguru Harimurugan", "Rujia Wang", "Victor Ruhle", "Robert Sim", "Chetan Bansal", "Saravan Rajmohan"], "title": "Memora: A Harmonic Memory Representation Balancing Abstraction and Specificity", "comment": null, "summary": "Agent memory systems must accommodate continuously growing information while supporting efficient, context-aware retrieval for downstream tasks. Abstraction is essential for scaling agent memory, yet it often comes at the cost of specificity, obscuring the fine-grained details required for effective reasoning. We introduce Memora, a harmonic memory representation that structurally balances abstraction and specificity. Memora organizes information via its primary abstractions that index concrete memory values and consolidate related updates into unified memory entries, while cue anchors expand retrieval access across diverse aspects of the memory and connect related memories. Building on this structure, we employ a retrieval policy that actively exploits these memory connections to retrieve relevant information beyond direct semantic similarity. Theoretically, we show that standard Retrieval-Augmented Generation (RAG) and Knowledge Graph (KG)-based memory systems emerge as special cases of our framework. Empirically, Memora establishes a new state-of-the-art on the LoCoMo and LongMemEval benchmarks, demonstrating better retrieval relevance and reasoning effectiveness as memory scales.", "AI": {"tldr": "Memora\u662f\u4e00\u79cd\u8c10\u6ce2\u8bb0\u5fc6\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u62bd\u8c61\u4e0e\u5177\u4f53\u6027\u7684\u7ed3\u6784\u5e73\u8861\u6765\u6269\u5c55\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u652f\u6301\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u68c0\u7d22\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u9700\u8981\u5904\u7406\u4e0d\u65ad\u589e\u957f\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u7684\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u3002\u62bd\u8c61\u5bf9\u4e8e\u6269\u5c55\u8bb0\u5fc6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u4ee5\u727a\u7272\u5177\u4f53\u6027\u4e3a\u4ee3\u4ef7\uff0c\u6a21\u7cca\u4e86\u6709\u6548\u63a8\u7406\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002", "method": "Memora\u901a\u8fc7\u4e3b\u8981\u62bd\u8c61\u7d22\u5f15\u5177\u4f53\u8bb0\u5fc6\u503c\uff0c\u5c06\u76f8\u5173\u66f4\u65b0\u6574\u5408\u4e3a\u7edf\u4e00\u8bb0\u5fc6\u6761\u76ee\uff0c\u540c\u65f6\u4f7f\u7528\u7ebf\u7d22\u951a\u70b9\u6269\u5c55\u68c0\u7d22\u8bbf\u95ee\u8303\u56f4\u5e76\u8fde\u63a5\u76f8\u5173\u8bb0\u5fc6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u91c7\u7528\u68c0\u7d22\u7b56\u7565\u4e3b\u52a8\u5229\u7528\u8fd9\u4e9b\u8bb0\u5fc6\u8fde\u63a5\u6765\u68c0\u7d22\u8d85\u51fa\u76f4\u63a5\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u76f8\u5173\u4fe1\u606f\u3002", "result": "Memora\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c55\u793a\u4e86\u968f\u7740\u8bb0\u5fc6\u6269\u5c55\uff0c\u66f4\u597d\u7684\u68c0\u7d22\u76f8\u5173\u6027\u548c\u63a8\u7406\u6709\u6548\u6027\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u6807\u51c6RAG\u548c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u7684\u8bb0\u5fc6\u7cfb\u7edf\u662f\u5176\u6846\u67b6\u7684\u7279\u6b8a\u60c5\u51b5\u3002", "conclusion": "Memora\u63d0\u4f9b\u4e86\u4e00\u79cd\u8c10\u6ce2\u8bb0\u5fc6\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u62bd\u8c61\u4e0e\u5177\u4f53\u6027\u4e4b\u95f4\u53d6\u5f97\u7ed3\u6784\u5e73\u8861\uff0c\u80fd\u591f\u6709\u6548\u6269\u5c55\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u652f\u6301\u66f4\u9ad8\u6548\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u68c0\u7d22\u548c\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2602.02597", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02597", "abs": "https://arxiv.org/abs/2602.02597", "authors": ["Hongyuan Su", "Yu Zheng", "Yong Li"], "title": "ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization", "comment": null, "summary": "Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at https://anonymous.4open.science/r/ContextEvolve-ACC", "AI": {"tldr": "ContextEvolve\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4f18\u5316\u4e0a\u4e0b\u6587\u5206\u89e3\u4e3a\u4e09\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\uff08\u603b\u7ed3\u5668\u3001\u5bfc\u822a\u5668\u3001\u91c7\u6837\u5668\uff09\uff0c\u5728\u53c2\u6570\u4e0d\u53ef\u89c1\u7684\u7ea6\u675f\u4e0b\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u7ea7\u522b\u7684\u641c\u7d22\u6548\u7387\uff0c\u7528\u4e8e\u7cfb\u7edf\u4ee3\u7801\u4f18\u5316\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u770b\u4f3c\u5408\u7406\u7684\u4ee3\u7801\uff0c\u4f46\u8981\u6ee1\u8db3\u7cfb\u7edf\u5bf9\u6b63\u786e\u6027\u548c\u6027\u80fd\u7684\u4e25\u683c\u8981\u6c42\u9700\u8981\u8fed\u4ee3\u4f18\u5316\u3002\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u641c\u7d22\u6548\u7387\u9ad8\u4f46\u9700\u8981\u53c2\u6570\u66f4\u65b0\uff08API\u8bbf\u95ee\u4e0d\u53ef\u884c\uff09\uff0c\u800c\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u8fdb\u5316\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u5229\u7528\u6548\u7387\u4f4e\u548c\u641c\u7d22\u65e0\u65b9\u5411\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faContextEvolve\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) Summarizer Agent\u901a\u8fc7\u4ee3\u7801\u5230\u8bed\u8a00\u7684\u62bd\u8c61\u6765\u6d53\u7f29\u8bed\u4e49\u72b6\u6001\uff1b2) Navigator Agent\u901a\u8fc7\u8f68\u8ff9\u5206\u6790\u63d0\u70bc\u4f18\u5316\u65b9\u5411\uff1b3) Sampler Agent\u901a\u8fc7\u4f18\u5148\u793a\u4f8b\u68c0\u7d22\u6765\u7ba1\u7406\u7ecf\u9a8c\u5206\u5e03\u3002\u8fd9\u79cd\u7f16\u6392\u5f62\u6210\u4e86\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u529f\u80fd\u540c\u6784\uff08\u6620\u5c04\u5230\u72b6\u6001\u8868\u793a\u3001\u7b56\u7565\u68af\u5ea6\u548c\u7ecf\u9a8c\u56de\u653e\uff09\uff0c\u5728\u6587\u672c\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u539f\u5219\u6027\u4f18\u5316\u3002", "result": "\u5728ADRS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cContextEvolve\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u534733.3%\uff0c\u540c\u65f6\u51cf\u5c1129.0%\u7684token\u6d88\u8017\u3002", "conclusion": "ContextEvolve\u5728\u4e25\u683c\u7684\u53c2\u6570\u4e0d\u53ef\u89c1\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5316\u5b66\u4e60\u7ea7\u522b\u7684\u641c\u7d22\u6548\u7387\uff0c\u4e3a\u7cfb\u7edf\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002", "topic": "code agent"}}
{"id": "2602.03584", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03584", "abs": "https://arxiv.org/abs/2602.03584", "authors": ["Yi-Kai Zhang", "Zhiyuan Yao", "Hongyan Hao", "Yueqing Sun", "Qi Gu", "Hui Su", "Xunliang Cai", "De-Chuan Zhan", "Han-Jia Ye"], "title": "$V_0$: A Generalist Value Model for Any Policy at State Zero", "comment": null, "summary": "Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.", "AI": {"tldr": "\u63d0\u51faV\u2080\u901a\u7528\u4ef7\u503c\u6a21\u578b\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u4f30\u8ba1\u4efb\u4f55\u6a21\u578b\u5728\u672a\u89c1\u63d0\u793a\u4e0a\u7684\u9884\u671f\u6027\u80fd\uff0c\u7528\u4e8eGRPO\u8bad\u7ec3\u4e2d\u7684\u91c7\u6837\u9884\u7b97\u5206\u914d\u548c\u90e8\u7f72\u65f6\u7684\u6a21\u578b\u8def\u7531", "motivation": "\u4f20\u7edfActor-Critic\u65b9\u6cd5\u4e2d\u4ef7\u503c\u6a21\u578b\u9700\u8981\u4e0e\u7b56\u7565\u6a21\u578b\u540c\u6b65\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u6602\uff1bGRPO\u65b9\u6cd5\u867d\u7136\u6d88\u9664\u4e86\u4ef7\u503c\u6a21\u578b\u4f46\u9700\u8981\u5927\u91cf\u91c7\u6837\u6765\u7a33\u5b9a\u57fa\u7ebf\u4f30\u8ba1\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4ef7\u503c\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u5c06\u7b56\u7565\u7684\u52a8\u6001\u80fd\u529b\u4f5c\u4e3a\u663e\u5f0f\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u5229\u7528\u6307\u4ee4-\u6027\u80fd\u5bf9\u5386\u53f2\u6765\u52a8\u6001\u5206\u6790\u6a21\u578b\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u53c2\u6570\u62df\u5408\u6765\u611f\u77e5\u80fd\u529b\u53d8\u5316\u3002\u4e13\u6ce8\u4e8e\u72b6\u6001\u96f6\uff08\u521d\u59cb\u63d0\u793a\uff09\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u5373V\u2080\u6a21\u578b\u3002", "result": "V\u2080\u5728GRPO\u8bad\u7ec3\u4e2d\u80fd\u6709\u6548\u9884\u6d4b\u6210\u529f\u7387\uff0c\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u9884\u7b97\u5206\u914d\uff1b\u5728\u90e8\u7f72\u65f6\u4f5c\u4e3a\u8def\u7531\u5668\uff0c\u5728LLM\u8def\u7531\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u6210\u672c\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\uff0c\u663e\u8457\u4f18\u4e8e\u542f\u53d1\u5f0f\u9884\u7b97\u5206\u914d\u65b9\u6cd5\u3002", "conclusion": "V\u2080\u901a\u7528\u4ef7\u503c\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u9ad8\u6548\u4ef7\u503c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u65e2\u80fd\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\uff0c\u53c8\u80fd\u5b9e\u73b0\u90e8\u7f72\u65f6\u7684\u667a\u80fd\u6a21\u578b\u8def\u7531\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03468", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03468", "abs": "https://arxiv.org/abs/2602.03468", "authors": ["Haohao Luo", "Zexi Li", "Yuexiang Xie", "Wenhao Zhang", "Yaliang Li", "Ying Shen"], "title": "IntentRL: Training Proactive User-intent Agents for Open-ended Deep Research via Reinforcement Learning", "comment": "Preprint", "summary": "Deep Research (DR) agents extend Large Language Models (LLMs) beyond parametric knowledge by autonomously retrieving and synthesizing evidence from large web corpora into long-form reports, enabling a long-horizon agentic paradigm. However, unlike real-time conversational assistants, DR is computationally expensive and time-consuming, creating an autonomy-interaction dilemma: high autonomy on ambiguous user queries often leads to prolonged execution with unsatisfactory outcomes. To address this, we propose IntentRL, a framework that trains proactive agents to clarify latent user intents before starting long-horizon research. To overcome the scarcity of open-ended research data, we introduce a scalable pipeline that expands a few seed samples into high-quality dialogue turns via a shallow-to-deep intent refinement graph. We further adopt a two-stage reinforcement learning (RL) strategy: Stage I applies RL on offline dialogues to efficiently learn general user-interaction behavior, while Stage II uses the trained agent and a user simulator for online rollouts to strengthen adaptation to diverse user feedback. Extensive experiments show that IntentRL significantly improves both intent hit rate and downstream task performance, outperforming the built-in clarify modules of closed-source DR agents and proactive LLM baselines.", "AI": {"tldr": "IntentRL\u8bad\u7ec3\u4e3b\u52a8\u5f0f\u667a\u80fd\u4f53\u5728\u5f00\u59cb\u957f\u65f6\u7a0b\u7814\u7a76\u524d\u6f84\u6e05\u7528\u6237\u6f5c\u5728\u610f\u56fe\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u610f\u56fe\u547d\u4e2d\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u6df1\u5ea6\u7814\u7a76(DR)\u667a\u80fd\u4f53\u867d\u7136\u80fd\u81ea\u4e3b\u68c0\u7d22\u548c\u7efc\u5408\u7f51\u7edc\u4fe1\u606f\u751f\u6210\u957f\u62a5\u544a\uff0c\u4f46\u5b58\u5728\u81ea\u4e3b\u6027-\u4ea4\u4e92\u56f0\u5883\uff1a\u5bf9\u6a21\u7cca\u67e5\u8be2\u7684\u9ad8\u81ea\u4e3b\u6027\u5e38\u5bfc\u81f4\u6267\u884c\u65f6\u95f4\u957f\u4e14\u7ed3\u679c\u4e0d\u6ee1\u610f\u3002\u9700\u8981\u89e3\u51b3DR\u667a\u80fd\u4f53\u5728\u5f00\u59cb\u8017\u65f6\u7814\u7a76\u524d\u6f84\u6e05\u7528\u6237\u610f\u56fe\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faIntentRL\u6846\u67b6\uff0c\u8bad\u7ec3\u4e3b\u52a8\u5f0f\u667a\u80fd\u4f53\u5728\u5f00\u59cb\u957f\u65f6\u7a0b\u7814\u7a76\u524d\u6f84\u6e05\u6f5c\u5728\u7528\u6237\u610f\u56fe\u3002\u91c7\u7528\u53ef\u6269\u5c55\u7ba1\u9053\u5c06\u5c11\u91cf\u79cd\u5b50\u6837\u672c\u901a\u8fc7\u6d45\u5c42\u5230\u6df1\u5c42\u610f\u56fe\u7ec6\u5316\u56fe\u6269\u5c55\u4e3a\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u8f6e\u6b21\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u79bb\u7ebf\u5bf9\u8bdd\u4e0a\u5e94\u7528RL\u5b66\u4e60\u901a\u7528\u7528\u6237\u4ea4\u4e92\u884c\u4e3a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u667a\u80fd\u4f53\u548c\u7528\u6237\u6a21\u62df\u5668\u8fdb\u884c\u5728\u7ebf\u63a8\u6f14\uff0c\u589e\u5f3a\u5bf9\u591a\u6837\u5316\u7528\u6237\u53cd\u9988\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIntentRL\u663e\u8457\u63d0\u9ad8\u4e86\u610f\u56fe\u547d\u4e2d\u7387\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f18\u4e8e\u95ed\u6e90DR\u667a\u80fd\u4f53\u7684\u5185\u7f6e\u6f84\u6e05\u6a21\u5757\u548c\u4e3b\u52a8\u5f0fLLM\u57fa\u7ebf\u3002", "conclusion": "IntentRL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DR\u667a\u80fd\u4f53\u7684\u81ea\u4e3b\u6027-\u4ea4\u4e92\u56f0\u5883\uff0c\u901a\u8fc7\u4e3b\u52a8\u6f84\u6e05\u7528\u6237\u610f\u56fe\u63d0\u9ad8\u4e86\u7814\u7a76\u6548\u7387\u548c\u7ed3\u679c\u8d28\u91cf\uff0c\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4ea4\u4e92\u4f18\u5316\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03635", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03635", "abs": "https://arxiv.org/abs/2602.03635", "authors": ["Chao Huang", "Yujing Lu", "Quangang Li", "Shenghe Wang", "Yan Wang", "Yueyang Zhang", "Long Xia", "Jiashu Zhao", "Zhiyuan Sun", "Daiting Shi", "Tingwen Liu"], "title": "TRE: Encouraging Exploration in the Trust Region", "comment": null, "summary": "Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTrust Region Entropy (TRE)\u65b9\u6cd5\uff0c\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u6807\u51c6\u71b5\u6b63\u5219\u5316\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u4fe1\u4efb\u533a\u57df\u5185\u9f13\u52b1\u63a2\u7d22\u6765\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u71b5\u6b63\u5219\u5316\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u5e38\u7528\u4e8e\u589e\u5f3a\u63a2\u7d22\uff0c\u4f46\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u6548\u679c\u751a\u5fae\u751a\u81f3\u964d\u4f4e\u6027\u80fd\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5927\u89c4\u6a21\u8bcd\u6c47\u8868\u548c\u957f\u751f\u6210\u5e8f\u5217\u5e26\u6765\u7684\u7d2f\u79ef\u5c3e\u90e8\u98ce\u9669\uff0c\u5bfc\u81f4\u6982\u7387\u8d28\u91cf\u88ab\u7a00\u91ca\u5230\u65e0\u6548\u6807\u8bb0\u7684\u5c3e\u90e8\u800c\u975e\u805a\u7126\u4e8e\u5408\u7406\u5019\u9009\u3002", "method": "\u63d0\u51faTrust Region Entropy (TRE)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9f13\u52b1\u63a2\u7d22\u4e25\u683c\u9650\u5236\u5728\u6a21\u578b\u7684\u4fe1\u4efb\u533a\u57df\u5185\uff0c\u907f\u514d\u5728\u65e0\u6548\u6807\u8bb0\u5c3e\u90e8\u8fdb\u884c\u63a2\u7d22\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406(MATH)\u3001\u7ec4\u5408\u641c\u7d22(Countdown)\u548c\u504f\u597d\u5bf9\u9f50(HH)\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTRE\u59cb\u7ec8\u4f18\u4e8e\u539f\u59cbPPO\u3001\u6807\u51c6\u71b5\u6b63\u5219\u5316\u548c\u5176\u4ed6\u63a2\u7d22\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TRE\u65b9\u6cd5\u901a\u8fc7\u9650\u5236\u63a2\u7d22\u5728\u4fe1\u4efb\u533a\u57df\u5185\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u71b5\u6b63\u5219\u5316\u5931\u8d25\u7684\u95ee\u9898\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03545", "abs": "https://arxiv.org/abs/2602.03545", "authors": ["Davide Paglieri", "Logan Cross", "William A. Cunningham", "Joel Z. Leibo", "Alexander Sasha Vezhnevets"], "title": "Persona Generators: Generating Diverse Synthetic Personas at Scale", "comment": null, "summary": "Evaluating AI systems that interact with humans requires understanding their behavior across diverse user populations, but collecting representative human data is often expensive or infeasible, particularly for novel technologies or hypothetical future scenarios. Recent work in Generative Agent-Based Modeling has shown that large language models can simulate human-like synthetic personas with high fidelity, accurately reproducing the beliefs and behaviors of specific individuals. However, most approaches require detailed data about target populations and often prioritize density matching (replicating what is most probable) rather than support coverage (spanning what is possible), leaving long-tail behaviors underexplored. We introduce Persona Generators, functions that can produce diverse synthetic populations tailored to arbitrary contexts. We apply an iterative improvement loop based on AlphaEvolve, using large language models as mutation operators to refine our Persona Generator code over hundreds of iterations. The optimization process produces lightweight Persona Generators that can automatically expand small descriptions into populations of diverse synthetic personas that maximize coverage of opinions and preferences along relevant diversity axes. We demonstrate that evolved generators substantially outperform existing baselines across six diversity metrics on held-out contexts, producing populations that span rare trait combinations difficult to achieve in standard LLM outputs.", "AI": {"tldr": "\u63d0\u51faPersona Generators\u65b9\u6cd5\uff0c\u901a\u8fc7AlphaEvolve\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u4eba\u53e3\uff0c\u8986\u76d6\u957f\u5c3e\u884c\u4e3a\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "motivation": "\u8bc4\u4f30AI\u7cfb\u7edf\u9700\u8981\u591a\u6837\u5316\u4eba\u7c7b\u6570\u636e\uff0c\u4f46\u6536\u96c6\u771f\u5b9e\u6570\u636e\u6602\u8d35\u4e14\u4e0d\u53ef\u884c\uff0c\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u9700\u8981\u8be6\u7ec6\u4eba\u53e3\u6570\u636e\u4e14\u504f\u5411\u5bc6\u5ea6\u5339\u914d\u800c\u975e\u652f\u6301\u8986\u76d6\uff0c\u5bfc\u81f4\u957f\u5c3e\u884c\u4e3a\u63a2\u7d22\u4e0d\u8db3", "method": "\u5f15\u5165Persona Generators\u51fd\u6570\uff0c\u57fa\u4e8eAlphaEvolve\u8fed\u4ee3\u6539\u8fdb\u5faa\u73af\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u53d8\u5f02\u7b97\u5b50\uff0c\u5728\u6570\u767e\u6b21\u8fed\u4ee3\u4e2d\u4f18\u5316\u751f\u6210\u5668\u4ee3\u7801\uff0c\u4ea7\u751f\u8f7b\u91cf\u7ea7\u751f\u6210\u5668", "result": "\u8fdb\u5316\u540e\u7684\u751f\u6210\u5668\u5728\u516d\u4e2a\u591a\u6837\u6027\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u80fd\u751f\u6210\u8986\u76d6\u7f55\u89c1\u7279\u5f81\u7ec4\u5408\u7684\u591a\u6837\u5316\u5408\u6210\u4eba\u53e3", "conclusion": "Persona Generators\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u4eba\u53e3\uff0c\u8986\u76d6\u957f\u5c3e\u884c\u4e3a\uff0c\u4e3aAI\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u6570\u636e", "topic": "agent analysis"}}
{"id": "2602.03630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03630", "abs": "https://arxiv.org/abs/2602.03630", "authors": ["I\u00f1aki del Campo", "Pablo Cuervo", "Victor Rodriguez-Fernandez", "Roberto Armellin", "Jack Yarndley"], "title": "Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12", "comment": "Extended version of the paper presented at AIAA SciTech 2026 Forum. Includes futher experiments, corrections and new appendix", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an \"LLM-as-a-Judge\" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30LLM\u5728\u590d\u6742\u5929\u4f53\u52a8\u529b\u5b66\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u89c4\u5212\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136\u6218\u7565\u7406\u89e3\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u6267\u884c\u5c42\u9762\u5b58\u5728\u4e25\u91cd\u969c\u788d\uff0cLLM\u76ee\u524d\u53ea\u80fd\u4f5c\u4e3a\u9886\u57df\u52a9\u624b\u800c\u975e\u5b8c\u5168\u81ea\u4e3b\u7684\u5de5\u7a0b\u5e08\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9ad8\u7ef4\u7269\u7406\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u591a\u9636\u6bb5\u89c4\u5212\u80fd\u529b\u4ecd\u4e0d\u660e\u786e\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5929\u4f53\u52a8\u529b\u5b66\u7ade\u8d5b\u8bc4\u4f30\u5f53\u524dAI\u4ee3\u7406\u7684\u6781\u9650\u80fd\u529b\u3002", "method": "\u5c06MLE-Bench\u6846\u67b6\u5e94\u7528\u4e8e\u8f68\u9053\u529b\u5b66\u9886\u57df\uff0c\u91c7\u7528AIDE-based\u4ee3\u7406\u67b6\u6784\u81ea\u4e3b\u751f\u6210\u548c\u4f18\u5316\u4efb\u52a1\u65b9\u6848\u3002\u4f7f\u7528\"LLM-as-a-Judge\"\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9886\u57df\u4e13\u5bb6\u5236\u5b9a\u7684\u8bc4\u5206\u6807\u51c6\u5728\u4e94\u4e2a\u7ed3\u6784\u7c7b\u522b\u4e2d\u8bc4\u4f30\u6218\u7565\u53ef\u884c\u6027\u3002", "result": "\u8fc7\u53bb\u4e24\u5e74\u5e73\u5747\u6218\u7565\u53ef\u884c\u6027\u5f97\u5206\u51e0\u4e4e\u7ffb\u500d\uff08\u4ece9.3\u5206\u5347\u81f317.2\u5206\uff0c\u6ee1\u520626\u5206\uff09\u3002\u5148\u8fdb\u6a21\u578b\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6982\u5ff5\u7406\u89e3\u80fd\u529b\uff0c\u80fd\u6b63\u786e\u6784\u5efa\u76ee\u6807\u51fd\u6570\u548c\u4efb\u52a1\u67b6\u6784\uff0c\u4f46\u5728\u6267\u884c\u5c42\u9762\u5b58\u5728\u7269\u7406\u5355\u4f4d\u4e0d\u4e00\u81f4\u3001\u8fb9\u754c\u6761\u4ef6\u9519\u8bef\u548c\u8c03\u8bd5\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "conclusion": "\u5f53\u524dLLM\u5177\u5907\u89e3\u51b3\u7a7a\u95f4\u79d1\u5b66\u4efb\u52a1\u6240\u9700\u7684\u77e5\u8bc6\u548c\u667a\u80fd\uff0c\u4f46\u53d7\u9650\u4e8e\u5b9e\u65bd\u969c\u788d\uff0c\u53ea\u80fd\u4f5c\u4e3a\u5f3a\u5927\u7684\u9886\u57df\u4fc3\u8fdb\u8005\u800c\u975e\u5b8c\u5168\u81ea\u4e3b\u7684\u5de5\u7a0b\u5e08\u3002", "topic": "agent analysis"}}
{"id": "2602.03704", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03704", "abs": "https://arxiv.org/abs/2602.03704", "authors": ["Yu Tian", "Linh Huynh", "Katerina Christhilf", "Shubham Chakraborty", "Micah Watanabe", "Tracy Arner", "Danielle McNamara"], "title": "Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models", "comment": "This manuscript is under review at Electronics", "summary": "Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.", "AI": {"tldr": "ReQUESTA\u662f\u4e00\u4e2a\u6df7\u5408\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u8ba4\u77e5\u591a\u6837\u6027\u7684\u591a\u9879\u9009\u62e9\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u3001\u534f\u8c03LLM\u667a\u80fd\u4f53\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7ec4\u4ef6\uff0c\u76f8\u6bd4\u5355\u6b21\u63d0\u793a\u7684GPT-5\u57fa\u7ebf\uff0c\u80fd\u4ea7\u751f\u66f4\u5177\u6311\u6218\u6027\u3001\u533a\u5206\u5ea6\u66f4\u9ad8\u4e14\u4e0e\u9605\u8bfb\u7406\u89e3\u8868\u73b0\u66f4\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u81ea\u52a8\u751f\u6210\u591a\u9879\u9009\u62e9\u9898\u53d8\u5f97\u53ef\u884c\uff0c\u4f46\u53ef\u9760\u5730\u751f\u6210\u6ee1\u8db3\u7279\u5b9a\u8ba4\u77e5\u9700\u6c42\uff08\u5982\u6587\u672c\u7406\u89e3\u3001\u63a8\u7406\u548c\u4e3b\u65e8\u628a\u63e1\uff09\u7684\u9898\u76ee\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u751f\u6210\u9898\u76ee\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u3002", "method": "ReQUESTA\u662f\u4e00\u4e2a\u6df7\u5408\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06MCQ\u751f\u6210\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u5b50\u4efb\u52a1\uff0c\u534f\u8c03LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u7ec4\u4ef6\uff0c\u652f\u6301\u89c4\u5212\u3001\u53d7\u63a7\u751f\u6210\u3001\u8fed\u4ee3\u8bc4\u4f30\u548c\u540e\u5904\u7406\u3002\u5728\u5b66\u672f\u8bf4\u660e\u6587\u9605\u8bfb\u7406\u89e3\u7814\u7a76\u4e2d\uff0c\u4e0e\u5355\u6b21\u63d0\u793a\u7684GPT-5\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "ReQUESTA\u751f\u6210\u7684\u9898\u76ee\u5728\u96be\u5ea6\u548c\u533a\u5206\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e0e\u6574\u4f53\u9605\u8bfb\u7406\u89e3\u8868\u73b0\u66f4\u4e00\u81f4\u3002\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\uff0cReQUESTA\u9898\u76ee\u66f4\u7b26\u5408\u6838\u5fc3\u6982\u5ff5\uff0c\u5e72\u6270\u9879\u7684\u8bed\u8a00\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u5408\u7406\u6027\u66f4\u597d\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u9898\u65b9\u9762\u3002", "conclusion": "\u6df7\u5408\u667a\u80fd\u4f53\u7f16\u6392\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u63d0\u9ad8LLM\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u53ef\u9760\u6027\uff0c\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u662f\u8d85\u8d8a\u5355\u6b21\u63d0\u793a\u7684\u7ed3\u6784\u5316\u751f\u6210\u7684\u5173\u952e\u6760\u6746\u3002", "topic": "agent analysis"}}
{"id": "2602.02708", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.02708", "abs": "https://arxiv.org/abs/2602.02708", "authors": ["Punya Syon Pandey", "Zhijing Jin"], "title": "BinaryPPO: Efficient Policy Optimization for Binary Classification", "comment": null, "summary": "Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at https://github.com/psyonp/BinaryPPO.", "AI": {"tldr": "BinaryPPO\uff1a\u4e00\u79cd\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4e8c\u5206\u7c7b\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5956\u52b1\u6700\u5927\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u5956\u52b1\u51fd\u6570\u5728\u6807\u7b7e\u566a\u58f0\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u6bd2\u6027\u68c0\u6d4b\u3001\u4e8b\u5b9e\u6027\u9a8c\u8bc1\u3001\u56e0\u679c\u63a8\u65ad\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5b58\u5728\u6807\u7b7e\u566a\u58f0\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u6216\u7a00\u758f\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faBinaryPPO\u6846\u67b6\uff0c\u5c06\u4e8c\u5206\u7c7b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5956\u52b1\u6700\u5927\u5316\u95ee\u9898\u3002\u4f7f\u7528PPO\u7684\u53d8\u4f53\uff0c\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u5956\u52b1\u51fd\u6570\uff0c\u60e9\u7f5a\u4e0d\u786e\u5b9a\u6216\u4e0d\u6b63\u786e\u7684\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u4ece\u9759\u6001\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u65e0\u9700\u5728\u7ebf\u4ea4\u4e92\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u79cd\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u4e0a\uff0cBinaryPPO\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e8640-60\u4e2a\u767e\u5206\u70b9\uff0c\u6700\u9ad8\u8fbe\u523099%\uff0c\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002\u6df1\u5165\u5206\u6790\u4e86\u5956\u52b1\u5851\u9020\u3001\u4f18\u52bf\u7f29\u653e\u548c\u653f\u7b56\u7a33\u5b9a\u6027\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u4f5c\u7528\u3002", "conclusion": "\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5956\u52b1\u8bbe\u8ba1\u4e3a\u4e8c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6bd4\u76d1\u7763\u5fae\u8c03\u66f4\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002BinaryPPO\u5728\u73b0\u5b9e\u4e16\u754c\u566a\u58f0\u548c\u6311\u6218\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03664", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03664", "abs": "https://arxiv.org/abs/2602.03664", "authors": ["Yang Wan", "Zheng Cao", "Zhenhao Zhang", "Zhengwen Zeng", "Shuheng Shen", "Changhua Meng", "Linchao Zhu"], "title": "Mitigating Conversational Inertia in Multi-Turn Agents", "comment": null, "summary": "Large language models excel as few-shot learners when provided with appropriate demonstrations, yet this strength becomes problematic in multiturn agent scenarios, where LLMs erroneously mimic their own previous responses as few-shot examples. Through attention analysis, we identify conversational inertia, a phenomenon where models exhibit strong diagonal attention to previous responses, which is associated with imitation bias that constrains exploration. This reveals a tension when transforming few-shot LLMs into agents: longer context enriches environmental feedback for exploitation, yet also amplifies conversational inertia that undermines exploration. Our key insight is that for identical states, actions generated with longer contexts exhibit stronger inertia than those with shorter contexts, enabling construction of preference pairs without environment rewards. Based on this, we propose Context Preference Learning to calibrate model preferences to favor low-inertia responses over highinertia ones. We further provide context management strategies at inference time to balance exploration and exploitation. Experimental results across eight agentic environments and one deep research scenario validate that our framework reduces conversational inertia and achieves performance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u5bf9\u8bdd\u60ef\u6027\"\u6982\u5ff5\uff0c\u6307LLM\u5728\u591a\u8f6e\u4ee3\u7406\u573a\u666f\u4e2d\u8fc7\u5ea6\u6a21\u4eff\u81ea\u8eab\u5148\u524d\u54cd\u5e94\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e0a\u4e0b\u6587\u504f\u597d\u5b66\u4e60\u6846\u67b6\u6765\u6821\u51c6\u6a21\u578b\u504f\u597d\uff0c\u51cf\u5c11\u60ef\u6027\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c06few-shot LLM\u8f6c\u5316\u4e3a\u591a\u8f6e\u4ee3\u7406\u65f6\u5b58\u5728\u77db\u76fe\uff1a\u957f\u4e0a\u4e0b\u6587\u80fd\u63d0\u4f9b\u66f4\u591a\u73af\u5883\u53cd\u9988\u7528\u4e8e\u5229\u7528\uff0c\u4f46\u4e5f\u4f1a\u589e\u5f3a\u5bf9\u8bdd\u60ef\u6027\uff0c\u9650\u5236\u63a2\u7d22\u80fd\u529b\u3002\u6a21\u578b\u4f1a\u9519\u8bef\u5730\u5c06\u81ea\u5df1\u4e4b\u524d\u7684\u54cd\u5e94\u4f5c\u4e3afew-shot\u793a\u4f8b\u8fdb\u884c\u6a21\u4eff\u3002", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u8bc6\u522b\u5bf9\u8bdd\u60ef\u6027\u73b0\u8c61\uff0c\u63d0\u51fa\u4e0a\u4e0b\u6587\u504f\u597d\u5b66\u4e60(CPL)\u6846\u67b6\uff1a\u57fa\u4e8e\u76f8\u540c\u72b6\u6001\u4e0b\u957f\u4e0a\u4e0b\u6587\u6bd4\u77ed\u4e0a\u4e0b\u6587\u4ea7\u751f\u66f4\u5f3a\u60ef\u6027\u7684\u6d1e\u5bdf\uff0c\u6784\u5efa\u65e0\u9700\u73af\u5883\u5956\u52b1\u7684\u504f\u597d\u5bf9\uff0c\u6821\u51c6\u6a21\u578b\u504f\u597d\u4ee5\u9009\u62e9\u4f4e\u60ef\u6027\u54cd\u5e94\u3002\u540c\u65f6\u63d0\u4f9b\u63a8\u7406\u65f6\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u3002", "result": "\u57288\u4e2a\u4ee3\u7406\u73af\u5883\u548c1\u4e2a\u6df1\u5ea6\u7814\u7a76\u573a\u666f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u5bf9\u8bdd\u60ef\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u6539\u8fdb\u3002", "conclusion": "\u5bf9\u8bdd\u60ef\u6027\u662f\u591a\u8f6e\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u91cd\u8981\u95ee\u9898\uff0c\u4e0a\u4e0b\u6587\u504f\u597d\u5b66\u4e60\u80fd\u6709\u6548\u6821\u51c6LLM\u504f\u597d\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2602.03707", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03707", "abs": "https://arxiv.org/abs/2602.03707", "authors": ["Yifan Zhu", "Xinyu Mu", "Tao Feng", "Zhonghong Ou", "Yuning Gong", "Haoran Luo"], "title": "OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering", "comment": null, "summary": "Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.", "AI": {"tldr": "OmniRAG-Agent\uff1a\u4e00\u79cd\u9762\u5411\u957f\u97f3\u9891\u89c6\u9891\u95ee\u7b54\u7684\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf-\u97f3\u9891\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u667a\u80fd\u4f53\u5faa\u73af\u89c4\u5212\u548c\u7b56\u7565\u4f18\u5316\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dOmniLLMs\u5728\u5904\u7406\u957f\u97f3\u9891\u89c6\u9891\u95ee\u7b54\u65f6\u9762\u4e34\u56db\u5927\u6311\u6218\uff1a\u5bc6\u96c6\u7f16\u7801\u6210\u672c\u9ad8\u3001\u7ec6\u7c92\u5ea6\u68c0\u7d22\u80fd\u529b\u5f31\u3001\u4e3b\u52a8\u89c4\u5212\u80fd\u529b\u6709\u9650\u3001\u7f3a\u4e4f\u7aef\u5230\u7aef\u4f18\u5316\u3002\u8fd9\u4e9b\u95ee\u9898\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5c24\u4e3a\u7a81\u51fa\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u6784\u5efa\u56fe\u50cf-\u97f3\u9891\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u5757\uff0c\u8ba9OmniLLM\u4ece\u5916\u90e8\u5e93\u4e2d\u83b7\u53d6\u76f8\u5173\u7684\u77ed\u5e27\u548c\u97f3\u9891\u7247\u6bb5\uff1b2. \u91c7\u7528\u667a\u80fd\u4f53\u5faa\u73af\u673a\u5236\uff0c\u8de8\u8f6e\u6b21\u89c4\u5212\u3001\u8c03\u7528\u5de5\u5177\u5e76\u5408\u5e76\u68c0\u7d22\u8bc1\u636e\uff1b3. \u5e94\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u8054\u5408\u6539\u8fdb\u5de5\u5177\u4f7f\u7528\u548c\u7b54\u6848\u8d28\u91cf\u3002", "result": "\u5728OmniVideoBench\u3001WorldSense\u548cDaily-Omni\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniRAG-Agent\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002", "conclusion": "OmniRAG-Agent\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u3001\u667a\u80fd\u4f53\u89c4\u5212\u548c\u7b56\u7565\u4f18\u5316\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u97f3\u9891\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2602.02710", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02710", "abs": "https://arxiv.org/abs/2602.02710", "authors": ["Fahim Tajwar", "Guanning Zeng", "Yueer Zhou", "Yuda Song", "Daman Arora", "Yiding Jiang", "Jeff Schneider", "Ruslan Salakhutdinov", "Haiwen Feng", "Andrea Zanette"], "title": "Maximum Likelihood Reinforcement Learning", "comment": "Project website and code: https://zanette-labs.github.io/MaxRL/", "summary": "Reinforcement learning is the method of choice to train models in sampling-based setups with binary outcome feedback, such as navigation, code generation, and mathematical problem solving. In such settings, models implicitly induce a likelihood over correct rollouts. However, we observe that reinforcement learning does not maximize this likelihood, and instead optimizes only a lower-order approximation. Inspired by this observation, we introduce Maximum Likelihood Reinforcement Learning (MaxRL), a sampling-based framework to approximate maximum likelihood using reinforcement learning techniques. MaxRL addresses the challenges of non-differentiable sampling by defining a compute-indexed family of sample-based objectives that interpolate between standard reinforcement learning and exact maximum likelihood as additional sampling compute is allocated. The resulting objectives admit a simple, unbiased policy-gradient estimator and converge to maximum likelihood optimization in the infinite-compute limit. Empirically, we show that MaxRL Pareto-dominates existing methods in all models and tasks we tested, achieving up to 20x test-time scaling efficiency gains compared to its GRPO-trained counterpart. We also observe MaxRL to scale better with additional data and compute. Our results suggest MaxRL is a promising framework for scaling RL training in correctness based settings.", "AI": {"tldr": "\u63d0\u51faMaxRL\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u8ba1\u7b97\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4e4b\u95f4\u63d2\u503c\uff0c\u5728\u4ee3\u7801\u751f\u6210\u7b49\u4e8c\u5143\u53cd\u9988\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u91c7\u6837\u8bbe\u7f6e\u4e2d\uff08\u5982\u5bfc\u822a\u3001\u4ee3\u7801\u751f\u6210\u3001\u6570\u5b66\u95ee\u9898\u6c42\u89e3\uff09\u53ea\u4f18\u5316\u4f4e\u9636\u8fd1\u4f3c\uff0c\u800c\u975e\u6700\u5927\u5316\u6b63\u786e\u8f68\u8ff9\u7684\u4f3c\u7136\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u597d\u5229\u7528\u91c7\u6837\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMaximum Likelihood Reinforcement Learning (MaxRL)\u6846\u67b6\uff0c\u5b9a\u4e49\u57fa\u4e8e\u8ba1\u7b97\u7d22\u5f15\u7684\u91c7\u6837\u76ee\u6807\u65cf\uff0c\u5728\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u548c\u7cbe\u786e\u6700\u5927\u4f3c\u7136\u4e4b\u95f4\u63d2\u503c\u3002\u4f7f\u7528\u7b80\u5355\u7684\u65e0\u504f\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u5728\u65e0\u9650\u8ba1\u7b97\u6781\u9650\u4e0b\u6536\u655b\u5230\u6700\u5927\u4f3c\u7136\u4f18\u5316\u3002", "result": "MaxRL\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u90fdPareto\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u6bd4GRPO\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u9ad8\u8fbe20\u500d\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u6548\u7387\u63d0\u5347\u3002\u5728\u989d\u5916\u6570\u636e\u548c\u8ba1\u7b97\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "MaxRL\u662f\u57fa\u4e8e\u6b63\u786e\u6027\u8bbe\u7f6e\u4e2d\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u5229\u7528\u91c7\u6837\u8ba1\u7b97\u8d44\u6e90\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03786", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03786", "abs": "https://arxiv.org/abs/2602.03786", "authors": ["Jianhao Ruan", "Zhihao Xu", "Yiran Peng", "Fashen Ren", "Zhaoyang Yu", "Xinbing Liang", "Jinyu Xiang", "Bang Liu", "Chenglin Wu", "Yuyu Luo", "Jiayi Zhang"], "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration", "comment": null, "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra", "AI": {"tldr": "AOrchestra\u662f\u4e00\u4e2a\u57fa\u4e8e\u7edf\u4e00\u4ee3\u7406\u62bd\u8c61\uff08\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u3001\u6a21\u578b\uff09\u7684\u4ee3\u7406\u7f16\u6392\u7cfb\u7edf\uff0c\u80fd\u591f\u52a8\u6001\u521b\u5efa\u4e13\u7528\u6267\u884c\u5668\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u4ee3\u7406\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\u7f3a\u4e4f\u52a8\u6001\u62bd\u8c61\u89c6\u56fe\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u6846\u67b6\u65e0\u5173\u7684\u4ee3\u7406\u62bd\u8c61\u6765\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u4ee3\u7406\u62bd\u8c61\u6a21\u578b\uff08\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u5de5\u5177\u3001\u6a21\u578b\uff09\uff0c\u57fa\u4e8e\u6b64\u6784\u5efaAOrchestra\u7cfb\u7edf\uff0c\u5176\u4e2d\u7f16\u6392\u5668\u5728\u6bcf\u4e00\u6b65\u52a8\u6001\u5177\u4f53\u5316\u8be5\u5143\u7ec4\uff1a\u7b56\u5212\u4efb\u52a1\u76f8\u5173\u4e0a\u4e0b\u6587\u3001\u9009\u62e9\u5de5\u5177\u548c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5373\u65f6\u81ea\u52a8\u4ee3\u7406\u521b\u5efa\u59d4\u6258\u6267\u884c\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08GAIA\u3001SWE-Bench\u3001Terminal-Bench\uff09\u4e2d\uff0cAOrchestra\u4e0eGemini-3-Flash\u914d\u5bf9\u65f6\uff0c\u76f8\u5bf9\u4e8e\u6700\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e8616.28%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "AOrchestra\u7684\u7edf\u4e00\u4ee3\u7406\u62bd\u8c61\u548c\u52a8\u6001\u7f16\u6392\u8bbe\u8ba1\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u7a0b\u5de5\u4f5c\u91cf\uff0c\u4fdd\u6301\u6846\u67b6\u65e0\u5173\u6027\uff0c\u652f\u6301\u53ef\u63a7\u5236\u7684\u6027\u80fd-\u6210\u672c\u6743\u8861\uff0c\u80fd\u591f\u63a5\u8fd1\u5e15\u7d2f\u6258\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "2602.03794", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03794", "abs": "https://arxiv.org/abs/2602.03794", "authors": ["Yingxuan Yang", "Chengrui Qu", "Muning Wen", "Laixi Shi", "Ying Wen", "Weinan Zhang", "Adam Wierman", "Shangding Gu"], "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity", "comment": null, "summary": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help? We present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce $K^*$, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u53d7\u9650\u4e8e\u4efb\u52a1\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u667a\u80fd\u4f53\u6570\u91cf\uff0c\u5f02\u8d28\u667a\u80fd\u4f53\u901a\u8fc7\u63d0\u4f9b\u4e92\u8865\u8bc1\u636e\u663e\u8457\u4f18\u4e8e\u540c\u8d28\u667a\u80fd\u4f53\u6269\u5c55", "motivation": "\u63a2\u7d22LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6269\u5c55\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u589e\u52a0\u540c\u8d28\u667a\u80fd\u4f53\u6570\u91cf\u5b58\u5728\u6536\u76ca\u9012\u51cf\uff0c\u800c\u5f02\u8d28\u667a\u80fd\u4f53\u5374\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u79cd\u5dee\u5f02\u7684\u6839\u672c\u539f\u56e0", "method": "\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6\u5206\u6790\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u8fb9\u754c\uff0c\u5f15\u5165K*\u6307\u6807\u91cf\u5316\u6709\u6548\u901a\u9053\u6570\u91cf\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5f02\u8d28\u914d\u7f6e\u76f8\u5bf9\u4e8e\u540c\u8d28\u6269\u5c55\u7684\u4f18\u52bf", "result": "\u5f02\u8d28\u914d\u7f6e\u663e\u8457\u4f18\u4e8e\u540c\u8d28\u6269\u5c55\uff1a2\u4e2a\u5f02\u8d28\u667a\u80fd\u4f53\u6027\u80fd\u53ef\u5339\u914d\u6216\u8d85\u8fc716\u4e2a\u540c\u8d28\u667a\u80fd\u4f53\uff0c\u8bc1\u660e\u591a\u6837\u6027\u8bbe\u8ba1\u7684\u9ad8\u6548\u6027", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6027\u80fd\u53d7\u4efb\u52a1\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u9650\u5236\uff0c\u5f02\u8d28\u667a\u80fd\u4f53\u901a\u8fc7\u63d0\u4f9b\u4e92\u8865\u8bc1\u636e\u7a81\u7834\u540c\u8d28\u6269\u5c55\u74f6\u9888\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u9c81\u68d2\u7cfb\u7edf\u63d0\u4f9b\u591a\u6837\u6027\u8bbe\u8ba1\u539f\u5219", "topic": "agent analysis"}}
{"id": "2602.03719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03719", "abs": "https://arxiv.org/abs/2602.03719", "authors": ["Yubao Zhao", "Weiquan Huang", "Sudong Wang", "Ruochen Zhao", "Chen Chen", "Yao Shu", "Chengwei Qin"], "title": "Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling", "comment": "24 pages, 5 figures", "summary": "Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \\href{https://github.com/YubaoZhao/BranPO}{code}.", "AI": {"tldr": "BranPO\u662f\u4e00\u79cd\u65e0\u9700\u4ef7\u503c\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u622a\u65ad\u8f68\u8ff9\u5c3e\u90e8\u5e76\u91cd\u91c7\u6837\u66ff\u4ee3\u5ef6\u7eed\u6765\u6784\u5efa\u5bf9\u6bd4\u540e\u7f00\uff0c\u89e3\u51b3\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898\u3002", "motivation": "\u957f\u89c6\u91ce\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u7684\u8f68\u8ff9\u7ea7\u5956\u52b1\u5bfc\u81f4\u5b66\u4e60\u56f0\u96be\uff0c\u73b0\u6709\u6811\u641c\u7d22\u65b9\u6cd5\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u5c3e\u90e8\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u63d0\u51faBranPO\u65b9\u6cd5\uff1a1\uff09\u622a\u65ad\u8f68\u8ff9\u5c3e\u90e8\u5e76\u91cd\u91c7\u6837\u66ff\u4ee3\u5ef6\u7eed\uff0c\u6784\u5efa\u5bf9\u6bd4\u540e\u7f00\u8fdb\u884c\u6b65\u7ea7\u5bf9\u6bd4\u76d1\u7763\uff1b2\uff09\u5f15\u5165\u96be\u5ea6\u611f\u77e5\u5206\u652f\u91c7\u6837\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u4e0d\u540c\u4efb\u52a1\u7684\u5206\u652f\u9891\u7387\uff1b3\uff09\u5197\u4f59\u6b65\u63a9\u7801\u6291\u5236\u65e0\u4fe1\u606f\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBranPO\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u4e0d\u589e\u52a0\u603b\u4f53\u8bad\u7ec3\u9884\u7b97\u3002", "conclusion": "BranPO\u901a\u8fc7\u5c3e\u90e8\u622a\u65ad\u548c\u5bf9\u6bd4\u540e\u7f00\u6784\u5efa\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u91ce\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02722", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02722", "abs": "https://arxiv.org/abs/2602.02722", "authors": ["Dan Haramati", "Carl Qi", "Tal Daniel", "Amy Zhang", "Aviv Tamar", "George Konidaris"], "title": "Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion", "comment": "ICLR 2026", "summary": "We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: https://sites.google.com/view/hecrl", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5b9e\u4f53\u4e2d\u5fc3\u6846\u67b6\uff0c\u7ed3\u5408\u5b50\u76ee\u6807\u5206\u89e3\u4e0e\u56e0\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u591a\u5b9e\u4f53\u9886\u57df\u4e2d\u7684\u957f\u65f6\u7a0b\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1", "motivation": "\u89e3\u51b3\u591a\u5b9e\u4f53\u590d\u6742\u73af\u5883\u4e2d\u957f\u65f6\u7a0b\u76ee\u6807\u8fbe\u6210\u7684\u6838\u5fc3\u6311\u6218\uff0c\u7279\u522b\u662f\u9ad8\u7ef4\u89c2\u6d4b\u3001\u7ec4\u5408\u72b6\u6001\u7a7a\u95f4\u548c\u7a00\u758f\u5956\u52b1\u5e26\u6765\u7684\u56f0\u96be", "method": "\u91c7\u7528\u4e24\u5c42\u5c42\u6b21\u7ed3\u6784\uff1a\u57fa\u4e8e\u4ef7\u503c\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53+\u56e0\u5b50\u5316\u5b50\u76ee\u6807\u751f\u6210\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u4e24\u8005\u72ec\u7acb\u8bad\u7ec3\u540e\u901a\u8fc7\u57fa\u4e8e\u4ef7\u503c\u51fd\u6570\u7684\u5b50\u76ee\u6807\u9009\u62e9\u7ec4\u5408", "result": "\u5728\u56fe\u50cf\u57fa\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u57fa\u7840RL\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u6700\u56f0\u96be\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u63d0\u9ad8150%\u4ee5\u4e0a\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u66f4\u957f\u7684\u65f6\u7a0b\u548c\u66f4\u591a\u5b9e\u4f53", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u5b9e\u4f53\u4e2d\u5fc3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u591a\u5b9e\u4f53\u9886\u57df\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u517c\u5bb9\u73b0\u6709\u7b97\u6cd5\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u4f18\u52bf", "topic": "agentic reinforcement learning"}}
{"id": "2602.03837", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03837", "abs": "https://arxiv.org/abs/2602.03837", "authors": ["David P. Woodruff", "Vincent Cohen-Addad", "Lalit Jain", "Jieming Mao", "Song Zuo", "MohammadHossein Bateni", "Simina Branzei", "Michael P. Brenner", "Lin Chen", "Ying Feng", "Lance Fortnow", "Gang Fu", "Ziyi Guan", "Zahra Hadizadeh", "Mohammad T. Hajiaghayi", "Mahdi JafariRaviz", "Adel Javanmard", "Karthik C. S.", "Ken-ichi Kawarabayashi", "Ravi Kumar", "Silvio Lattanzi", "Euiwoong Lee", "Yi Li", "Ioannis Panageas", "Dimitris Paparas", "Benjamin Przybocki", "Bernardo Subercaseaux", "Ola Svensson", "Shayan Taherijam", "Xuan Wu", "Eylon Yogev", "Morteza Zadimoghaddam", "Samson Zhou", "Vahab Mirrokni"], "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques", "comment": null, "summary": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u7814\u7a76\u4eba\u5458\u5982\u4f55\u4e0eGemini\u7b49\u5148\u8fdbAI\u6a21\u578b\u5408\u4f5c\uff0c\u5728\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u7ecf\u6d4e\u5b66\u3001\u4f18\u5316\u548c\u7269\u7406\u7b49\u9886\u57df\u89e3\u51b3\u5f00\u653e\u95ee\u9898\u3001\u53cd\u9a73\u731c\u60f3\u5e76\u751f\u6210\u65b0\u8bc1\u660e\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u4eba\u673a\u534f\u4f5c\u7684\u6280\u672f\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e38\u89c4\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u53c2\u4e0e\u4e13\u5bb6\u7ea7\u6570\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22AI\u5982\u4f55\u6210\u4e3a\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u4e2d\u7684\u771f\u6b63\u5408\u4f5c\u4f19\u4f34\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u81ea\u52a8\u5316\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u7814\u7a76\u4eba\u5458\u5982\u4f55\u4e0eGoogle\u7684Gemini\u6a21\u578b\uff08\u7279\u522b\u662fGemini Deep Think\u53ca\u5176\u53d8\u4f53\uff09\u534f\u4f5c\u3002\u63d0\u53d6\u4e86\u6709\u6548\u4eba\u673a\u534f\u4f5c\u7684\u901a\u7528\u6280\u672f\uff0c\u5305\u62ec\u8fed\u4ee3\u7cbe\u70bc\u3001\u95ee\u9898\u5206\u89e3\u548c\u8de8\u5b66\u79d1\u77e5\u8bc6\u8f6c\u79fb\u3002\u8fd8\u63a2\u7d22\u4e86\u8d85\u8d8a\u6807\u51c6\u804a\u5929\u754c\u9762\u7684\u65b9\u6cd5\uff0c\u5982\u5c06\u6a21\u578b\u4f5c\u4e3a\u4e25\u8c28\u7684\u5bf9\u6297\u6027\u5ba1\u7a3f\u4eba\uff0c\u4ee5\u53ca\u5c06\u5176\u5d4c\u5165\"\u795e\u7ecf\u7b26\u53f7\"\u5faa\u73af\u4e2d\u81ea\u4e3b\u7f16\u5199\u548c\u6267\u884c\u4ee3\u7801\u9a8c\u8bc1\u590d\u6742\u63a8\u5bfc\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86AI\u5728\u591a\u4e2a\u9886\u57df\uff08\u7406\u8bba\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u7ecf\u6d4e\u5b66\u3001\u4f18\u5316\u3001\u7269\u7406\uff09\u4e2d\u89e3\u51b3\u5f00\u653e\u95ee\u9898\u3001\u53cd\u9a73\u731c\u60f3\u548c\u751f\u6210\u65b0\u8bc1\u660e\u7684\u80fd\u529b\u3002\u5927\u90e8\u5206\u6210\u679c\u6765\u81ea\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u65b9\u6cd5\uff0c\u4f46\u4e5f\u5c55\u793a\u4e86\u8d85\u8d8a\u6807\u51c6\u804a\u5929\u754c\u9762\u7684\u521b\u65b0\u5e94\u7528\u3002", "conclusion": "AI\u4e0d\u4ec5\u53ef\u4f5c\u4e3a\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u66f4\u80fd\u6210\u4e3a\u79d1\u5b66\u53d1\u73b0\u521b\u9020\u6027\u8fc7\u7a0b\u4e2d\u7684\u591a\u529f\u80fd\u3001\u771f\u6b63\u7684\u5408\u4f5c\u4f19\u4f34\u3002\u4eba\u673a\u534f\u4f5c\u7684\u6709\u6548\u6280\u672f\u5305\u62ec\u8fed\u4ee3\u7cbe\u70bc\u3001\u95ee\u9898\u5206\u89e3\u548c\u8de8\u5b66\u79d1\u77e5\u8bc6\u8f6c\u79fb\u7b49\u65b9\u6cd5\u3002", "topic": "agent analysis"}}
{"id": "2602.02799", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.02799", "abs": "https://arxiv.org/abs/2602.02799", "authors": ["Wasu Top Piriyakulkij", "Wolfgang Lehrach", "Kevin Ellis", "Kevin Murphy"], "title": "Joint Learning of Hierarchical Neural Options and Abstract World Model", "comment": null, "summary": "Building agents that can perform new skills by composing existing skills is a long-standing goal of AI agent research. Towards this end, we investigate how to efficiently acquire a sequence of skills, formalized as hierarchical neural options. However, existing model-free hierarchical reinforcement algorithms need a lot of data. We propose a novel method, which we call AgentOWL (Option and World model Learning Agent), that jointly learns -- in a sample efficient way -- an abstract world model (abstracting across both states and time) and a set of hierarchical neural options. We show, on a subset of Object-Centric Atari games, that our method can learn more skills using much less data than baseline methods.", "AI": {"tldr": "\u63d0\u51faAgentOWL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u548c\u5206\u5c42\u795e\u7ecf\u9009\u9879\uff0c\u4ee5\u6837\u672c\u9ad8\u6548\u7684\u65b9\u5f0f\u6784\u5efa\u80fd\u591f\u7ec4\u5408\u73b0\u6709\u6280\u80fd\u6267\u884c\u65b0\u4efb\u52a1\u7684AI\u667a\u80fd\u4f53\u3002", "motivation": "\u6784\u5efa\u80fd\u591f\u901a\u8fc7\u7ec4\u5408\u73b0\u6709\u6280\u80fd\u6765\u6267\u884c\u65b0\u6280\u80fd\u7684\u667a\u80fd\u4f53\u662fAI\u7814\u7a76\u7684\u957f\u671f\u76ee\u6807\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u578b\u65e0\u5173\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faAgentOWL\u65b9\u6cd5\uff0c\u8054\u5408\u5b66\u4e60\u62bd\u8c61\u4e16\u754c\u6a21\u578b\uff08\u5728\u72b6\u6001\u548c\u65f6\u95f4\u4e0a\u8fdb\u884c\u62bd\u8c61\uff09\u548c\u4e00\u7ec4\u5206\u5c42\u795e\u7ecf\u9009\u9879\uff0c\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u7684\u5b66\u4e60\u3002", "result": "\u5728Object-Centric Atari\u6e38\u620f\u5b50\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u4f7f\u7528\u66f4\u5c11\u7684\u6570\u636e\u5b66\u4e60\u5230\u66f4\u591a\u7684\u6280\u80fd\u3002", "conclusion": "AgentOWL\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u62bd\u8c61\u4e16\u754c\u6a21\u578b\u548c\u5206\u5c42\u9009\u9879\uff0c\u5b9e\u73b0\u4e86\u6837\u672c\u9ad8\u6548\u7684\u6280\u80fd\u83b7\u53d6\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u7ec4\u5408\u6280\u80fd\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03143", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03143", "abs": "https://arxiv.org/abs/2602.03143", "authors": ["Baohao Liao", "Hanze Dong", "Xinxing Xu", "Christof Monz", "Jiang Bian"], "title": "Self-Hinting Language Models Enhance Reinforcement Learning", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $\u03c4$ conditioned on $(x,h)$. Crucially, the task reward $R(x,\u03c4)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.", "AI": {"tldr": "SAGE\u901a\u8fc7\u5f15\u5165\u7279\u6743\u63d0\u793a\u6765\u589e\u5f3aGRPO\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u9632\u6b62\u4f18\u52bf\u51fd\u6570\u574d\u584c\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6027\u80fd", "motivation": "GRPO\u5728\u7a00\u758f\u7ec8\u7aef\u5956\u52b1\u4e0b\u5bb9\u6613\u505c\u6ede\uff0c\u56e0\u4e3a\u7ec4\u5185rollout\u7ecf\u5e38\u83b7\u5f97\u76f8\u540c\u5956\u52b1\uff0c\u5bfc\u81f4\u76f8\u5bf9\u4f18\u52bf\u574d\u584c\u548c\u66f4\u65b0\u6d88\u5931", "method": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u65f6\u6ce8\u5165\u7279\u6743\u63d0\u793a\u6765\u91cd\u5851rollout\u5206\u5e03\uff0c\u91c7\u6837\u7d27\u51d1\u63d0\u793a\uff08\u5982\u8ba1\u5212\u6216\u5206\u89e3\uff09\u6765\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u6301\u4efb\u52a1\u5956\u52b1\u4e0d\u53d8\u4f46\u589e\u52a0\u7ec4\u5185\u7ed3\u679c\u591a\u6837\u6027", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSAGE\u6301\u7eed\u4f18\u4e8eGRPO\uff0c\u5e73\u5747\u63d0\u5347\uff1aLlama-3.2-3B-Instruct +2.0\uff0cQwen2.5-7B-Instruct +1.2\uff0cQwen3-4B-Instruct +1.3", "conclusion": "SAGE\u901a\u8fc7\u81ea\u6211\u63d0\u793a\u548c\u7279\u6743\u76d1\u7763\u6709\u6548\u89e3\u51b3\u4e86GRPO\u5728\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u8bad\u7ec3\u505c\u6ede\u95ee\u9898\uff0c\u65e0\u9700\u6d4b\u8bd5\u65f6\u7279\u6743\u4fe1\u606f\uff0c\u91c7\u6837\u591a\u6837\u81ea\u6211\u63d0\u793a\u4f5c\u4e3a\u81ea\u9002\u5e94\u8bfe\u7a0b\u6bd4\u56fa\u5b9a\u63d0\u793a\u66f4\u6709\u6548", "topic": "agentic reinforcement learning"}}
{"id": "2602.03190", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03190", "abs": "https://arxiv.org/abs/2602.03190", "authors": ["Wenquan Lu", "Hai Huang", "Randall Balestriero"], "title": "Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning", "comment": null, "summary": "Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u63d0\u793a\u589e\u5f3a\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u6a21\u677f\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u7a33\u5b9a\u957f\u65f6\u8bad\u7ec3\u5e76\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u71b5\u5d29\u6e83\u73b0\u8c61\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8fc7\u65e9\u5d29\u6e83\uff0c\u9650\u5236\u4e86\u8bad\u7ec3\u65f6\u957f\u548c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002\u540c\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u56fa\u5b9a\u7684\u63a8\u7406\u63d0\u793a\u6a21\u677f\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u63d0\u793a\u589e\u5f3a\u7b56\u7565\uff0c\u8ba9\u6a21\u578b\u5728\u591a\u6837\u5316\u6a21\u677f\u548c\u683c\u5f0f\u4e0b\u751f\u6210\u63a8\u7406\u8f68\u8ff9\uff0c\u589e\u52a0rollout\u591a\u6837\u6027\u3002\u8be5\u65b9\u6cd5\u65e0\u9700KL\u6b63\u5219\u5316\u9879\uff0c\u80fd\u5728\u56fa\u5b9a\u6570\u636e\u96c6\u4e0a\u7a33\u5b9a\u6269\u5c55\u8bad\u7ec3\u65f6\u957f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5bb9\u5fcd\u4f4e\u71b5\u72b6\u6001\u800c\u4e0d\u63d0\u524d\u5d29\u6e83\u3002", "result": "\u5728Qwen2.5-Math-1.5B\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528MATH Level 3-5\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5728AIME24\u3001AMC\u3001MATH500\u3001Minerva\u548cOlympiadBench\u7b49\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1a\u6bcf\u57fa\u51c6\u51c6\u786e\u738744.5%\uff0c\u6bcf\u95ee\u9898\u51c6\u786e\u738751.3%\u3002", "conclusion": "\u63d0\u793a\u589e\u5f3a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u957f\u65f6\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02847", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.02847", "abs": "https://arxiv.org/abs/2602.02847", "authors": ["Mingxuan Li", "Junzhe Zhang", "Elias Bareinboim"], "title": "Causal Flow Q-Learning for Robust Offline Reinforcement Learning", "comment": null, "summary": "Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\\% that of confounding-unaware, state-of-the-art offline RL methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u89c6\u89d2\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u7b56\u7565\u5904\u7406\u50cf\u7d20\u6f14\u793a\u4e2d\u7684\u6df7\u6dc6\u89c2\u6d4b\u95ee\u9898\uff0c\u572825\u4e2a\u50cf\u7d20\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347120%\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u79bb\u7ebfRL\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u672a\u6d4b\u91cf\u7684\u6df7\u6dc6\u53d8\u91cf\uff0c\u4f46\u5728\u50cf\u7d20\u6f14\u793a\u4e2d\uff0c\u6f14\u793a\u8005\u548c\u5b66\u4e60\u8005\u7684\u611f\u77e5\u80fd\u529b\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u9690\u5f0f\u6df7\u6dc6\u504f\u5dee\uff0c\u5f71\u54cd\u7b56\u7565\u5b66\u4e60\u6548\u679c\u3002", "method": "\u4ece\u56e0\u679c\u89c6\u89d2\u5206\u6790\u6df7\u6dc6\u89c2\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u7684\u56e0\u679c\u79bb\u7ebfRL\u76ee\u6807\u51fd\u6570\uff0c\u4f18\u5316\u7b56\u7565\u5728\u6700\u574f\u6df7\u6dc6\u504f\u5dee\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002\u5b9e\u73b0\u57fa\u4e8e\u6df1\u5ea6\u5224\u522b\u5668\u8bc4\u4f30\u76ee\u6807\u7b56\u7565\u4e0e\u540d\u4e49\u884c\u4e3a\u7b56\u7565\u5dee\u5f02\u7684\u6d41\u5339\u914d\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u572825\u4e2a\u50cf\u7d20\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6df7\u6dc6\u9c81\u68d2\u589e\u5f3a\u65b9\u6cd5\u6bd4\u4e0d\u8003\u8651\u6df7\u6dc6\u7684SOTA\u79bb\u7ebfRL\u65b9\u6cd5\u6210\u529f\u7387\u9ad8\u51fa120%\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u5904\u7406\u50cf\u7d20\u6f14\u793a\u4e2d\u7684\u6df7\u6dc6\u89c2\u6d4b\u95ee\u9898\u80fd\u663e\u8457\u63d0\u5347\u79bb\u7ebfRL\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5904\u7406\u611f\u77e5\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u9690\u5f0f\u6df7\u6dc6\u504f\u5dee\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03783", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.03783", "abs": "https://arxiv.org/abs/2602.03783", "authors": ["Zhenshuo Zhang", "Minxuan Duan", "Hongyang R. Zhang"], "title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution", "comment": "27 pages. To appear in ICLR 2026", "summary": "Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u6838\u4ee3\u7406\u6a21\u578b\u7528\u4e8e\u4efb\u52a1\u5f52\u56e0\u5206\u6790\uff0c\u76f8\u6bd4\u7ebf\u6027\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u4efb\u52a1\u95f4\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u4f5c\u7528\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4efb\u52a1\u5f71\u54cd\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5f52\u56e0\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u7ebf\u6027\u4ee3\u7406\u6a21\u578b\uff0c\u53ea\u80fd\u6355\u6349\u4e00\u9636\u5173\u7cfb\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u4efb\u52a1\u95f4\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\uff08\u5982\u534f\u540c\u3001\u5bf9\u6297\u3001XOR\u6548\u5e94\uff09\u3002\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u51c6\u786e\u91cf\u5316\u6bcf\u4e2a\u8bad\u7ec3\u4efb\u52a1\u5bf9\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "1) \u5efa\u7acb\u7edf\u4e00\u7684\u4efb\u52a1\u52a0\u6743\u6846\u67b6\u5206\u6790\u4efb\u52a1\u5f52\u56e0\u65b9\u6cd5\uff1b2) \u901a\u8fc7\u4e8c\u9636\u5206\u6790\u63ed\u793a\u7ebf\u6027\u4ee3\u7406\u6a21\u578b\u4e0e\u5f71\u54cd\u51fd\u6570\u7684\u65b0\u8054\u7cfb\uff1b3) \u5f15\u5165\u6838\u4ee3\u7406\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u8868\u793a\u4e8c\u9636\u4efb\u52a1\u4ea4\u4e92\uff1b4) \u5f00\u53d1\u57fa\u4e8e\u68af\u5ea6\u7684\u4f30\u8ba1\u7a0b\u5e8f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4e00\u9636\u8fd1\u4f3c\u9ad8\u6548\u5b66\u4e60\u6838\u4ee3\u7406\u3002", "result": "\u6838\u4ee3\u7406\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7b49\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff1a1) \u4e0e\u7559\u4e00\u6cd5\u57fa\u51c6\u7684\u76f8\u5173\u6027\u6bd4\u7ebf\u6027\u4ee3\u7406\u548c\u5f71\u54cd\u51fd\u6570\u57fa\u7ebf\u9ad825%\uff1b2) \u68af\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u4ec5\u9700\u4e0d\u52302%\u7684\u76f8\u5bf9\u8bef\u5dee\uff0c\u65e0\u9700\u91cd\u590d\u8bad\u7ec3\uff1b3) \u5728\u4e0b\u6e38\u4efb\u52a1\u9009\u62e9\u4e2d\uff0c\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u7684\u6f14\u793a\u9009\u62e9\u6027\u80fd\u63d0\u534740%\u3002", "conclusion": "\u6838\u4ee3\u7406\u6a21\u578b\u4e3a\u4efb\u52a1\u5f52\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u4efb\u52a1\u95f4\u7684\u975e\u7ebf\u6027\u4ea4\u4e92\u4f5c\u7528\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u591a\u4efb\u52a1\u8bad\u7ec3\u4e2d\u4efb\u52a1\u95f4\u5173\u7cfb\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2602.02924", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.02924", "abs": "https://arxiv.org/abs/2602.02924", "authors": ["Xiaoyuan Cheng", "Wenxuan Yuan", "Boyang Li", "Yuanchao Xu", "Yiming Yang", "Hao Liang", "Bei Peng", "Robert Loftin", "Zhuo Sun", "Yukun Hu"], "title": "How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?", "comment": null, "summary": "Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.", "AI": {"tldr": "ALGD\u662f\u4e00\u79cd\u7528\u4e8e\u79bb\u7b56\u7565\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u7a33\u5b9a\u6269\u6563\u7b56\u7565\u91c7\u6837\uff0c\u89e3\u51b3\u5b89\u5168RL\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684RL\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u79bb\u7ebf\u8bbe\u7f6e\u7684\u5956\u52b1\u6700\u5927\u5316\uff0c\u5bf9\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u7684\u5b89\u5168\u6027\u8003\u8651\u6709\u9650\u3002\u6269\u6563\u7b56\u7565\u91c7\u6837\u867d\u7136\u80fd\u8868\u793a\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u4f46\u5728\u5b89\u5168RL\u4e2d\u76f4\u63a5\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u51fd\u6570\u4f1a\u5bfc\u81f4\u7b56\u7565\u751f\u6210\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u62c9\u683c\u6717\u65e5\u5f15\u5bfc\u6269\u6563(ALGD)\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u589e\u5f3a\u62c9\u683c\u6717\u65e5\u51fd\u6570\u5c40\u90e8\u51f8\u5316\u80fd\u91cf\u666f\u89c2\uff0c\u7a33\u5b9a\u7b56\u7565\u751f\u6210\u548c\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u800c\u4e0d\u6539\u53d8\u6700\u4f18\u7b56\u7565\u7684\u5206\u5e03\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cALGD\u5728\u7406\u8bba\u4e0a\u6709\u575a\u5b9e\u57fa\u7840\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u4e14\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "ALGD\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u57fa\u5b89\u5168RL\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6269\u6563\u7b56\u7565\u91c7\u6837\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.02990", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02990", "abs": "https://arxiv.org/abs/2602.02990", "authors": ["Evan Wang", "Simon Chess", "Daniel Lee", "Siyuan Ge", "Ajit Mallavarapu", "Vasily Ilin"], "title": "Learning to Repair Lean Proofs from Compiler Feedback", "comment": "15 pages, 6 figures", "summary": "As neural theorem provers become increasingly agentic, the ability to interpret and act on compiler feedback is critical. However, existing Lean datasets consist almost exclusively of correct proofs, offering little supervision for understanding and repairing failures. We study Lean proof repair as a supervised learning problem: given an erroneous proof and compiler feedback, predict both a corrected proof and a natural-language diagnosis grounded in the same feedback. We introduce APRIL (Automated Proof Repair in Lean), a dataset of 260,000 supervised tuples pairing systematically generated proof failures with compiler diagnostics and aligned repair and explanation targets. Training language models on APRIL substantially improves repair accuracy and feedback-conditioned reasoning; in our single-shot repair evaluation setting, a finetuned 4B-parameter model outperforms the strongest open-source baseline. We view diagnostic-conditioned supervision as a complementary training signal for feedback-using provers. Our dataset is available at \\href{https://huggingface.co/datasets/uw-math-ai/APRIL}{this link}.", "AI": {"tldr": "APRIL\u6570\u636e\u96c6\u4e3aLean\u5b9a\u7406\u8bc1\u660e\u5668\u63d0\u4f9b\u76d1\u7763\u5b66\u4e60\u6570\u636e\uff0c\u5305\u542b26\u4e07\u4e2a\u9519\u8bef\u8bc1\u660e\u3001\u7f16\u8bd1\u5668\u53cd\u9988\u3001\u4fee\u590d\u76ee\u6807\u548c\u89e3\u91ca\u7684\u914d\u5bf9\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc1\u660e\u4fee\u590d\u548c\u53cd\u9988\u7406\u89e3\u3002", "motivation": "\u73b0\u6709Lean\u6570\u636e\u96c6\u51e0\u4e4e\u53ea\u5305\u542b\u6b63\u786e\u8bc1\u660e\uff0c\u7f3a\u4e4f\u5bf9\u7f16\u8bd1\u5668\u53cd\u9988\u7684\u7406\u89e3\u548c\u9519\u8bef\u4fee\u590d\u7684\u76d1\u7763\u5b66\u4e60\u6570\u636e\uff0c\u9650\u5236\u4e86\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u5668\u5904\u7406\u5931\u8d25\u60c5\u51b5\u7684\u80fd\u529b\u3002", "method": "\u5c06Lean\u8bc1\u660e\u4fee\u590d\u6784\u5efa\u4e3a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u521b\u5efaAPRIL\u6570\u636e\u96c6\uff0c\u5305\u542b\u7cfb\u7edf\u751f\u6210\u7684\u8bc1\u660e\u5931\u8d25\u3001\u7f16\u8bd1\u5668\u8bca\u65ad\u3001\u4fee\u590d\u76ee\u6807\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u914d\u5bf9\uff0c\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728APRIL\u4e0a\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u51c6\u786e\u6027\u548c\u53cd\u9988\u6761\u4ef6\u63a8\u7406\u80fd\u529b\uff0c4B\u53c2\u6570\u6a21\u578b\u5728\u5355\u6b21\u4fee\u590d\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u6700\u5f3a\u7684\u5f00\u6e90\u57fa\u7ebf\u3002", "conclusion": "\u8bca\u65ad\u6761\u4ef6\u76d1\u7763\u662f\u53cd\u9988\u4f7f\u7528\u8bc1\u660e\u5668\u7684\u8865\u5145\u8bad\u7ec3\u4fe1\u53f7\uff0cAPRIL\u6570\u636e\u96c6\u4e3a\u795e\u7ecf\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u9519\u8bef\u4fee\u590d\u548c\u53cd\u9988\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002", "topic": "code agent"}}
{"id": "2602.03045", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03045", "abs": "https://arxiv.org/abs/2602.03045", "authors": ["Bo Yuan", "Zelin Zhao", "Petr Molodyk", "Bin Hu", "Yongxin Chen"], "title": "Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation", "comment": "In Review", "summary": "Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.", "AI": {"tldr": "ProCAD\u662f\u4e00\u4e2a\u4e3b\u52a8\u5f0f\u6587\u672c\u5230CAD\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6f84\u6e05\u4ee3\u7406\u89e3\u51b3\u51e0\u4f55\u63cf\u8ff0\u4e0d\u5b8c\u6574\u6216\u77db\u76fe\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230CAD\u7cfb\u7edf\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u6216\u77db\u76fe\u7684\u51e0\u4f55\u63cf\u8ff0\u65f6\uff0c\u503e\u5411\u4e8e\u88ab\u52a8\u9075\u5faa\u7528\u6237\u6307\u4ee4\u5e76\u4ea7\u751f\u5e7b\u89c9\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u751f\u6210\u7684CAD\u4ee3\u7801\u8d28\u91cf\u4e0d\u9ad8\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u5f0f\u4ee3\u7406\u6846\u67b6ProCAD\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1)\u4e3b\u52a8\u6f84\u6e05\u4ee3\u7406\uff0c\u5ba1\u6838\u63d0\u793a\u5e76\u5728\u5fc5\u8981\u65f6\u63d0\u51fa\u9488\u5bf9\u6027\u6f84\u6e05\u95ee\u9898\uff1b2)CAD\u7f16\u7801\u4ee3\u7406\uff0c\u5c06\u6f84\u6e05\u540e\u7684\u89c4\u8303\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684CadQuery\u7a0b\u5e8f\u3002\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5fae\u8c03\u7f16\u7801\u4ee3\u7406\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u5f0fSFT\u8bad\u7ec3\u6f84\u6e05\u4ee3\u7406\u3002", "result": "ProCAD\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6a21\u7cca\u63d0\u793a\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u4ea4\u4e92\u5f00\u9500\u3002\u5728Chamfer\u8ddd\u79bb\u6307\u6807\u4e0a\u6bd4Claude Sonnet 4.5\u964d\u4f4e79.9%\uff0c\u65e0\u6548\u4ee3\u7801\u6bd4\u4f8b\u4ece4.8%\u964d\u81f30.9%\u3002", "conclusion": "\u4e3b\u52a8\u6f84\u6e05\u673a\u5236\u80fd\u6709\u6548\u89e3\u51b3\u6587\u672c\u5230CAD\u751f\u6210\u4e2d\u7684\u89c4\u8303\u4e0d\u5b8c\u6574\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u524d\u6cbf\u95ed\u6e90\u6a21\u578b\u3002", "topic": "code agent"}}
{"id": "2602.03048", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03048", "abs": "https://arxiv.org/abs/2602.03048", "authors": ["Zhiyuan Yao", "Yi-Kai Zhang", "Yuxin Chen", "Yueqing Sun", "Zishan Xu", "Yu Yang", "Tianhao Hu", "Qi Gu", "Hui Su", "Xunliang Cai"], "title": "CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key approach for enhancing LLM reasoning.However, standard frameworks like Group Relative Policy Optimization (GRPO) typically employ a uniform rollout budget, leading to resource inefficiency. Moreover, existing adaptive methods often rely on instance-level metrics, such as task pass rates, failing to capture the model's dynamic learning state. To address these limitations, we propose CoBA-RL, a reinforcement learning algorithm designed to adaptively allocate rollout budgets based on the model's evolving capability. Specifically, CoBA-RL utilizes a Capability-Oriented Value function to map tasks to their potential training gains and employs a heap-based greedy strategy to efficiently self-calibrate the distribution of computational resources to samples with high training value. Extensive experiments demonstrate that our approach effectively orchestrates the trade-off between exploration and exploitation, delivering consistent generalization improvements across multiple challenging benchmarks. These findings underscore that quantifying sample training value and optimizing budget allocation are pivotal for advancing LLM post-training efficiency.", "AI": {"tldr": "CoBA-RL\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5206\u914d\u8bad\u7ec3\u9884\u7b97\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u80fd\u529b\u5bfc\u5411\u7684\u4ef7\u503c\u51fd\u6570\u8bc4\u4f30\u6837\u672c\u8bad\u7ec3\u4ef7\u503c\uff0c\u4f7f\u7528\u5806\u8d2a\u5fc3\u7b56\u7565\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u5347LLM\u540e\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\uff08\u5982GRPO\uff09\u4f7f\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u9884\u7b97\u5bfc\u81f4\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u73b0\u6709\u81ea\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u901a\u8fc7\u7387\u7b49\u5b9e\u4f8b\u7ea7\u6307\u6807\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u52a8\u6001\u5b66\u4e60\u72b6\u6001\u3002", "method": "\u63d0\u51faCoBA-RL\u7b97\u6cd5\uff1a1\uff09\u4f7f\u7528\u80fd\u529b\u5bfc\u5411\u4ef7\u503c\u51fd\u6570\u5c06\u4efb\u52a1\u6620\u5c04\u5230\u6f5c\u5728\u8bad\u7ec3\u6536\u76ca\uff1b2\uff09\u91c7\u7528\u5806\u8d2a\u5fc3\u7b56\u7565\u81ea\u6821\u51c6\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u5c06\u8d44\u6e90\u4f18\u5148\u5206\u914d\u7ed9\u9ad8\u8bad\u7ec3\u4ef7\u503c\u7684\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e00\u81f4\u7684\u6cdb\u5316\u6539\u8fdb\u3002", "conclusion": "\u91cf\u5316\u6837\u672c\u8bad\u7ec3\u4ef7\u503c\u548c\u4f18\u5316\u9884\u7b97\u5206\u914d\u5bf9\u4e8e\u63d0\u5347LLM\u540e\u8bad\u7ec3\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03061", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.03061", "abs": "https://arxiv.org/abs/2602.03061", "authors": ["Zihan Dong", "Zhixian Zhang", "Yang Zhou", "Can Jin", "Ruijia Wu", "Linjun Zhang"], "title": "Evaluating LLMs When They Do Not Know the Answer: Statistical Evaluation of Mathematical Reasoning via Comparative Signals", "comment": null, "summary": "Evaluating mathematical reasoning in LLMs is constrained by limited benchmark sizes and inherent model stochasticity, yielding high-variance accuracy estimates and unstable rankings across platforms. On difficult problems, an LLM may fail to produce a correct final answer, yet still provide reliable pairwise comparison signals indicating which of two candidate solutions is better. We leverage this observation to design a statistically efficient evaluation framework that combines standard labeled outcomes with pairwise comparison signals obtained by having models judge auxiliary reasoning chains. Treating these comparison signals as control variates, we develop a semiparametric estimator based on the efficient influence function (EIF) for the setting where auxiliary reasoning chains are observed. This yields a one-step estimator that achieves the semiparametric efficiency bound, guarantees strict variance reduction over naive sample averaging, and admits asymptotic normality for principled uncertainty quantification. Across simulations, our one-step estimator substantially improves ranking accuracy, with gains increasing as model output noise grows. Experiments on GPQA Diamond, AIME 2025, and GSM8K further demonstrate more precise performance estimation and more reliable model rankings, especially in small-sample regimes where conventional evaluation is pretty unstable.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u914d\u5bf9\u6bd4\u8f83\u4fe1\u53f7\u7684\u7edf\u8ba1\u9ad8\u6548\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528LLM\u5bf9\u8f85\u52a9\u63a8\u7406\u94fe\u7684\u5224\u65ad\u4f5c\u4e3a\u63a7\u5236\u53d8\u91cf\uff0c\u901a\u8fc7\u534a\u53c2\u6570\u4f30\u8ba1\u5668\u51cf\u5c11\u65b9\u5dee\uff0c\u63d0\u9ad8\u6a21\u578b\u6392\u540d\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u53d7\u9650\u4e8e\u57fa\u51c6\u89c4\u6a21\u5c0f\u548c\u6a21\u578b\u968f\u673a\u6027\uff0c\u5bfc\u81f4\u9ad8\u65b9\u5dee\u7cbe\u5ea6\u4f30\u8ba1\u548c\u8de8\u5e73\u53f0\u6392\u540d\u4e0d\u7a33\u5b9a\u3002\u5373\u4f7f\u6a21\u578b\u65e0\u6cd5\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u4ecd\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u914d\u5bf9\u6bd4\u8f83\u4fe1\u53f7\u3002", "method": "\u8bbe\u8ba1\u7ed3\u5408\u6807\u51c6\u6807\u6ce8\u7ed3\u679c\u548c\u914d\u5bf9\u6bd4\u8f83\u4fe1\u53f7\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u6bd4\u8f83\u4fe1\u53f7\u4f5c\u4e3a\u63a7\u5236\u53d8\u91cf\uff0c\u57fa\u4e8e\u6709\u6548\u5f71\u54cd\u51fd\u6570\u5f00\u53d1\u534a\u53c2\u6570\u4f30\u8ba1\u5668\uff0c\u6784\u5efa\u4e00\u6b65\u4f30\u8ba1\u5668\u5b9e\u73b0\u534a\u53c2\u6570\u6548\u7387\u8fb9\u754c\u3002", "result": "\u4e00\u6b65\u4f30\u8ba1\u5668\u663e\u8457\u63d0\u9ad8\u6392\u540d\u51c6\u786e\u6027\uff0c\u968f\u7740\u6a21\u578b\u8f93\u51fa\u566a\u58f0\u589e\u52a0\u800c\u589e\u76ca\u66f4\u5927\u3002\u5728GPQA Diamond\u3001AIME 2025\u548cGSM8K\u5b9e\u9a8c\u4e2d\u5c55\u793a\u66f4\u7cbe\u786e\u7684\u6027\u80fd\u4f30\u8ba1\u548c\u66f4\u53ef\u9760\u7684\u6a21\u578b\u6392\u540d\u3002", "conclusion": "\u5229\u7528\u914d\u5bf9\u6bd4\u8f83\u4fe1\u53f7\u4f5c\u4e3a\u63a7\u5236\u53d8\u91cf\u7684\u7edf\u8ba1\u9ad8\u6548\u8bc4\u4f30\u6846\u67b6\u80fd\u663e\u8457\u51cf\u5c11\u65b9\u5dee\uff0c\u5728\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u63d0\u4f9b\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u548c\u6392\u540d\u3002", "topic": "agent analysis"}}
{"id": "2602.03073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03073", "abs": "https://arxiv.org/abs/2602.03073", "authors": ["Rana Muhammad Shahroz Khan", "Zijie Liu", "Zhen Tan", "Charles Fleming", "Tianlong Chen"], "title": "TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT", "comment": null, "summary": "Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.", "AI": {"tldr": "TMS\u662f\u4e00\u79cd\u65e0\u9700\u5956\u52b1\u7684\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u5386\u53f2\u68c0\u67e5\u70b9\u521b\u5efa\u52a8\u6001\u8bfe\u7a0b\uff0c\u8fd1\u4f3cRL\u7684\u5728\u7ebf\u7b56\u7565\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u7684\u540c\u65f6\u907f\u514d\u6807\u51c6SFT\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u867d\u7136\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u6a21\u578b\u80fd\u529b\uff0c\u4f46\u6210\u672c\u9ad8\u6602\uff08\u5956\u52b1\u5de5\u7a0b\u590d\u6742\u3001\u4e0d\u7a33\u5b9a\u3001\u91c7\u6837\u6602\u8d35\uff09\uff1b\u800cSFT\u867d\u7136\u9ad8\u6548\u4f46\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u76d1\u7763\u4e0d\u5339\u914d\u95ee\u9898\u2014\u2014\u6a21\u578b\u7b56\u7565\u6f14\u53d8\u4e0e\u9759\u6001\u8bad\u7ec3\u6807\u7b7e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u6df7\u5408\u76d1\u7763\uff08TMS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u5386\u53f2\u68c0\u67e5\u70b9\u521b\u5efa\u52a8\u6001\u8bfe\u7a0b\u6765\u8fd1\u4f3cRL\u7684\u5728\u7ebf\u7b56\u7565\u4f18\u52bf\u3002\u8be5\u65b9\u6cd5\u6700\u5c0f\u5316\u7b56\u7565-\u6807\u7b7e\u5dee\u5f02\uff08PLD\uff09\uff0c\u9632\u6b62\u6807\u51c6SFT\u4e2d\u5bfc\u81f4\u9057\u5fd8\u7684\u6a21\u5f0f\u5d29\u6e83\u3002", "result": "\u5728\u63a8\u7406\uff08MATH\u3001GSM8K\uff09\u548c\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTMS\u6709\u6548\u6539\u5584\u4e86\u51c6\u786e\u7387-\u4fdd\u6301\u7387\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u867d\u7136RL\u4ecd\u7136\u662f\u4fdd\u6301\u80fd\u529b\u7684\u9ec4\u91d1\u6807\u51c6\uff0c\u4f46TMS\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u548c\u8fed\u4ee3SFT\uff0c\u5728\u4e0d\u9700\u5956\u52b1\u6a21\u578b\u6216\u9a8c\u8bc1\u5668\u7684\u60c5\u51b5\u4e0b\u7f29\u5c0f\u4e86\u4e0eRL\u7684\u5dee\u8ddd\u3002", "conclusion": "TMS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4e0d\u9700\u8981\u590d\u6742\u5956\u52b1\u5de5\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6765\u8fd1\u4f3cRL\u7684\u5728\u7ebf\u7b56\u7565\u4f18\u52bf\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u76d1\u7763\u4e0d\u5339\u914d\u95ee\u9898\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03132", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.03132", "abs": "https://arxiv.org/abs/2602.03132", "authors": ["Timothee Leleu", "Sudeera Gunathilaka", "Federico Ghimenti", "Surya Ganguli"], "title": "Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery", "comment": null, "summary": "Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erd\u0151s-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.", "AI": {"tldr": "CCTS\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u6982\u5ff5\u6811\u641c\u7d22\u7684LLM\u8f85\u52a9\u7b97\u6cd5\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u7a0b\u5e8f\u5c42\u6b21\u6982\u5ff5\u8868\u793a\u5e76\u5b66\u4e60\u5bf9\u6bd4\u6982\u5ff5\u6a21\u578b\u6765\u6307\u5bfc\u641c\u7d22\uff0c\u5728\u7ec4\u5408\u6570\u5b66\u95ee\u9898\u4e0a\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1LLM\u8f85\u52a9\u7b97\u6cd5\u53d1\u73b0\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u6700\u5927\u5316\u5229\u7528LLM\u5bf9\u7a0b\u5e8f\u7a7a\u95f4\u7684\u5185\u90e8\u8868\u793a\u6765\u63d0\u5347\u6027\u80fd\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7b97\u6cd5\u8c31\u7cfb\uff0c\u800c\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u7684\u663e\u5f0f\u5229\u7528\u3002", "method": "\u63d0\u51fa\u5bf9\u6bd4\u6982\u5ff5\u6811\u641c\u7d22(CCTS)\uff1a1)\u4ece\u751f\u6210\u7684\u7a0b\u5e8f\u4e2d\u63d0\u53d6\u5c42\u6b21\u6982\u5ff5\u8868\u793a\uff1b2)\u5b66\u4e60\u5bf9\u6bd4\u6982\u5ff5\u6a21\u578b\u6307\u5bfc\u7236\u8282\u70b9\u9009\u62e9\uff1b3)\u901a\u8fc7\u9ad8/\u4f4e\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u4f3c\u7136\u6bd4\u5206\u6570\u91cd\u65b0\u52a0\u6743\u7236\u8282\u70b9\uff0c\u504f\u5411\u6709\u7528\u6982\u5ff5\u7ec4\u5408\uff0c\u8fdc\u79bb\u8bef\u5bfc\u6027\u6982\u5ff5\u3002", "result": "CCTS\u5728\u5f00\u653eErd\u0151s\u578b\u7ec4\u5408\u6570\u5b66\u95ee\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u4e8e\u9002\u5e94\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\uff0c\u5e76\u4ea7\u751f\u4e86\u53ef\u89e3\u91ca\u7684\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u6982\u5ff5\u6811\u3002\u5206\u6790\u8868\u660e\u589e\u76ca\u4e3b\u8981\u6765\u81ea\u5b66\u4e60\u5e94\u907f\u514d\u54ea\u4e9b\u6982\u5ff5\u3002", "conclusion": "CCTS\u901a\u8fc7\u663e\u5f0f\u6982\u5ff5\u5c42\u6b21\u800c\u975eLLM\u6784\u5efa\u7684\u7b97\u6cd5\u8c31\u7cfb\u63d0\u4f9b\u6307\u5bfc\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u8f85\u52a9\u7b97\u6cd5\u53d1\u73b0\u7684\u6027\u80fd\u3002\u5728\u53d7\u63a7\u5408\u6210\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\uff0c\u91cd\u73b0\u4e86\u4e0eLLM\u76f8\u4f3c\u7684\u641c\u7d22\u52a8\u6001\u3002", "topic": "agent analysis"}}
{"id": "2602.03195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03195", "abs": "https://arxiv.org/abs/2602.03195", "authors": ["Jing-Cheng Pang", "Liang Lu", "Xian Tang", "Kun Jiang", "Sijie Wu", "Kai Zhang", "Xubin Li"], "title": "Reinforcement Learning with Promising Tokens for Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \\emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).", "AI": {"tldr": "RLPT\u901a\u8fc7\u5c06\u7b56\u7565\u4f18\u5316\u9650\u5236\u5728\"\u6709\u5e0c\u671b\u7684token\"\u5b50\u96c6\u4e0a\uff0c\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u52a8\u4f5c\u7a7a\u95f4\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5c06LLM\u89c6\u4e3a\u7b56\u7565\u5e76\u5728\u5b8c\u6574\u8bcd\u6c47\u7a7a\u95f4\u4e0a\u5e94\u7528RL\uff0c\u4f46\u5305\u542b\u5927\u91cf\u4e0a\u4e0b\u6587\u65e0\u5173token\u4f1a\u5206\u6563\u7b56\u7565\u6ce8\u610f\u529b\uff0c\u5f71\u54cd\u5728\u771f\u6b63\u5408\u7406token\u4e0a\u7684\u51b3\u7b56\u3002", "method": "RLPT\u6846\u67b6\u5c06\u7b56\u7565\u51b3\u7b56\u4e0etoken\u751f\u6210\u89e3\u8026\uff1a\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\u8bc6\u522b\u52a8\u6001\u7684\"\u6709\u5e0c\u671b\u7684token\"\u96c6\u5408\uff0c\u901a\u8fc7\u63a9\u7801\u5c06\u7b56\u7565\u4f18\u5316\u9650\u5236\u5728\u8fd9\u4e2a\u7cbe\u70bc\u5b50\u96c6\u4e0a\u3002", "result": "\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u7535\u4fe1\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cRLPT\u4f18\u4e8e\u6807\u51c6RL\u57fa\u7ebf\uff0c\u6709\u6548\u96c6\u6210\u5230\u4e0d\u540c\u6a21\u578b\u89c4\u6a21(4B\u548c8B)\u548cRL\u7b97\u6cd5(GRPO\u548cDAPO)\u4e2d\uff0c\u51cf\u5c11\u4e86\u68af\u5ea6\u65b9\u5dee\uff0c\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7b56\u7565\u4f18\u5316\u9650\u5236\u5728\u8bed\u4e49\u76f8\u5173\u7684token\u5b50\u7a7a\u95f4\u4e0a\uff0cRLPT\u6709\u6548\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03301", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03301", "abs": "https://arxiv.org/abs/2602.03301", "authors": ["Hyukjun Yang", "Han-Dong Lim", "Donghwan Lee"], "title": "Periodic Regularized Q-Learning", "comment": null, "summary": "In reinforcement learning (RL), Q-learning is a fundamental algorithm whose convergence is guaranteed in the tabular setting. However, this convergence guarantee does not hold under linear function approximation. To overcome this limitation, a significant line of research has introduced regularization techniques to ensure stable convergence under function approximation. In this work, we propose a new algorithm, periodic regularized Q-learning (PRQ). We first introduce regularization at the level of the projection operator and explicitly construct a regularized projected value iteration (RP-VI), subsequently extending it to a sample-based RL algorithm. By appropriately regularizing the projection operator, the resulting projected value iteration becomes a contraction. By extending this regularized projection into the stochastic setting, we establish the PRQ algorithm and provide a rigorous theoretical analysis that proves finite-time convergence guarantees for PRQ under linear function approximation.", "AI": {"tldr": "\u63d0\u51fa\u5468\u671f\u6027\u6b63\u5219\u5316Q\u5b66\u4e60(PRQ)\u7b97\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u6295\u5f71\u7b97\u5b50\u786e\u4fdd\u5728\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e0b\u7684\u6536\u655b\u6027", "motivation": "Q\u5b66\u4e60\u5728\u8868\u683c\u8bbe\u7f6e\u4e0b\u6709\u6536\u655b\u4fdd\u8bc1\uff0c\u4f46\u5728\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e0b\u4e0d\u6536\u655b\u3002\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u6b63\u5219\u5316\u6280\u672f\u786e\u4fdd\u7a33\u5b9a\u6536\u655b\uff0c\u4f46\u9700\u8981\u66f4\u597d\u7684\u7b97\u6cd5\u8bbe\u8ba1", "method": "1. \u5728\u6295\u5f71\u7b97\u5b50\u5c42\u9762\u5f15\u5165\u6b63\u5219\u5316\uff0c\u6784\u5efa\u6b63\u5219\u5316\u6295\u5f71\u503c\u8fed\u4ee3(RP-VI)\uff1b2. \u5c06RP-VI\u6269\u5c55\u5230\u57fa\u4e8e\u6837\u672c\u7684RL\u7b97\u6cd5\uff0c\u63d0\u51faPRQ\u7b97\u6cd5\uff1b3. \u901a\u8fc7\u6b63\u5219\u5316\u6295\u5f71\u7b97\u5b50\u4f7f\u6295\u5f71\u503c\u8fed\u4ee3\u6210\u4e3a\u538b\u7f29\u6620\u5c04", "result": "PRQ\u7b97\u6cd5\u5728\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e0b\u5177\u6709\u6709\u9650\u65f6\u95f4\u6536\u655b\u4fdd\u8bc1\uff0c\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u8bc1\u660e", "conclusion": "\u901a\u8fc7\u6b63\u5219\u5316\u6295\u5f71\u7b97\u5b50\u8bbe\u8ba1\u7684PRQ\u7b97\u6cd5\u89e3\u51b3\u4e86Q\u5b66\u4e60\u5728\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e0b\u7684\u6536\u655b\u95ee\u9898\uff0c\u4e3a\u51fd\u6570\u903c\u8fd1\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1", "topic": "agentic reinforcement learning"}}
{"id": "2602.03309", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03309", "abs": "https://arxiv.org/abs/2602.03309", "authors": ["Yuelin Hu", "Zhengxue Cheng", "Wei Liu", "Li Song"], "title": "Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models", "comment": "accepted by cscwd2026", "summary": "Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation.\n  Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors.\n  EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.", "AI": {"tldr": "EGSPO\u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684token\u7ea7\u68af\u5ea6\u8c03\u5236\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u5728\u6837\u672c\u7ea7\u522b\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u68af\u5ea6\u63a7\u5236\u3002EGSPO\u65e8\u5728\u901a\u8fc7token\u7ea7\u522b\u7684\u68af\u5ea6\u8c03\u5236\u6765\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5e73\u8861\u63a2\u7d22\u4e0e\u77e5\u8bc6\u4fdd\u7559\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) SFT\u4e13\u5bb6\u5b66\u4e60\u5efa\u7acb\u53ef\u9760\u9884\u70ed\u7b56\u7565\uff1b2) RL\u8f68\u8ff9\u751f\u6210\u5e76\u8ba1\u7b97\u6bcf\u4e2atoken\u7684\u9884\u6d4b\u71b5\uff1b3) EGSPO\u673a\u5236\u5e94\u7528\u71b5\u95e8\u63a7\u68af\u5ea6\u5206\u914d\uff1a\u9ad8\u71b5token\u8fdb\u884c\u5b8c\u6574PPO\u66f4\u65b0\u4ee5\u9f13\u52b1\u63a2\u7d22\uff0c\u4f4e\u71b5token\u8fdb\u884c\u8870\u51cfPPO\u66f4\u65b0\u4ee5\u51cf\u5c11\u65b9\u5dee\u5e76\u4fdd\u7559\u77e5\u8bc6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff1aAIME\u63d0\u53473.8%\uff0cMATH\u63d0\u53472.9%\uff08\u76f8\u6bd4CHORD phi\u57fa\u7ebf\uff09\uff0c\u4ec5\u589e\u52a03.4%\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "EGSPO\u901a\u8fc7token\u7ea7\u522b\u7684\u71b5\u95e8\u63a7\u68af\u5ea6\u8c03\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86\u63a2\u7d22\u4e0e\u77e5\u8bc6\u4fdd\u7559\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03392", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03392", "abs": "https://arxiv.org/abs/2602.03392", "authors": ["Shumin Wang", "Yuexiang Xie", "Wenhao Zhang", "Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Yanyong Zhang"], "title": "On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models", "comment": null, "summary": "Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u5f3a\u5316\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u71b5\u52a8\u6001\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u71b5\u53d8\u5316\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u5206\u6790\u8bbe\u8ba1\u4e86\u71b5\u63a7\u5236\u65b9\u6cd5\uff0c\u4e3aLLM\u5fae\u8c03\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u7b56\u7565\u3002", "motivation": "\u867d\u7136\u71b5\u4f5c\u4e3a\u8861\u91cfLLM\u8f93\u51fa\u591a\u6837\u6027\u7684\u5173\u952e\u6307\u6807\uff0c\u5728\u5f3a\u5316\u5fae\u8c03\u4e2d\u7528\u4e8e\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4f46\u5bf9\u5176\u52a8\u6001\u53d8\u5316\u7684\u7406\u8bba\u7406\u89e3\u5c1a\u4e0d\u5145\u5206\u3002\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790RFT\u8fc7\u7a0b\u4e2d\u7684\u71b5\u52a8\u6001\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u5206\u6790RFT\u8fc7\u7a0b\u4e2d\u7684\u71b5\u52a8\u6001\uff1a\u4ece\u91cf\u5316\u5355\u6b21logit\u66f4\u65b0\u4e0b\u71b5\u53d8\u5316\u7684\u5224\u522b\u8868\u8fbe\u5f0f\u51fa\u53d1\uff0c\u63a8\u5bfc\u71b5\u53d8\u5316\u7684\u4e00\u9636\u8868\u8fbe\u5f0f\uff0c\u6269\u5c55\u5230GRPO\u7684\u66f4\u65b0\u516c\u5f0f\u3002\u57fa\u4e8e\u7406\u8bba\u5206\u6790\u8bbe\u8ba1\u71b5\u63a7\u5236\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u5f97\u51fa\u4e86\u71b5\u53d8\u5316\u7684\u8868\u8fbe\u5f0f\u548c\u63a8\u8bba\uff0c\u542f\u53d1\u4e86\u71b5\u63a7\u5236\u65b9\u6cd5\u7684\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u73b0\u6709\u7814\u7a76\u4e2d\u5404\u79cd\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u91ca\u6846\u67b6\u3002\u5b9e\u8bc1\u7814\u7a76\u652f\u6301\u4e86\u4e3b\u8981\u7ed3\u8bba\uff0c\u8bc1\u660e\u4e86\u71b5\u5224\u522b\u5668\u88c1\u526a\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aRFT\u8bad\u7ec3\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\uff0c\u4e3aLLM\u5fae\u8c03\u4e2d\u4f18\u5316\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u7b56\u7565\uff0c\u5efa\u7acb\u4e86\u71b5\u52a8\u6001\u5206\u6790\u7684\u7406\u8bba\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03516", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03516", "abs": "https://arxiv.org/abs/2602.03516", "authors": ["Zixiang Di", "Jinyi Han", "Shuo Zhang", "Ying Liao", "Zhi Li", "Xiaofeng Ji", "Yongqi Wang", "Zheming Yang", "Ming Gao", "Bingdong Li", "Jie Wang"], "title": "Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning", "comment": null, "summary": "Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faPNS\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u5411\u5f3a\u5316\u5b66\u4e60\u5408\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u5177\u6709\u6b63\u786e\u7684\u683c\u5f0f\u548c\u7ed3\u6784\u4f46\u6700\u7ec8\u7b54\u6848\u662f\u9519\u8bef\u7684\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6240\u6709\u9519\u8bef\u56de\u7b54\u89c6\u4e3a\u540c\u7b49\u4fe1\u606f\u91cf\uff0c\u5ffd\u89c6\u4e86\u6837\u672c\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002\u5b66\u4e60\u8d1f\u6837\u672c\u5bf9\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u5f88\u6709\u524d\u666f\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u8d1f\u6837\u672c\u3002", "method": "\u63d0\u51faPlausible Negative Samples (PNS)\u65b9\u6cd5\uff1a\u901a\u8fc7\u53cd\u5411\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u4f7f\u7528\u7ec4\u5408\u5956\u52b1\uff08\u683c\u5f0f\u5408\u89c4\u6027\u3001\u51c6\u786e\u6027\u53cd\u8f6c\u3001\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u3001\u601d\u7ef4\u94fe\u8bc4\u4f30\uff09\u5408\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u5177\u6709\u9884\u671f\u683c\u5f0f\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u4f46\u6700\u7ec8\u7b54\u6848\u9519\u8bef\u3002", "result": "\u57287\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2a\u9aa8\u5e72\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cPNS\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6570\u636e\u6e90\u7528\u4e8e\u504f\u597d\u4f18\u5316\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u8d1f\u6837\u672c\u5408\u6210\u65b9\u6cd5\uff0c\u5e73\u5747\u6bd4RL\u8bad\u7ec3\u6a21\u578b\u63d0\u53472.03%\u3002", "conclusion": "PNS\u65b9\u6cd5\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8d1f\u6837\u672c\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2602.03772", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.03772", "abs": "https://arxiv.org/abs/2602.03772", "authors": ["Changhao Wang", "Yunfei Yu", "Xinhao Yao", "Jiaolong Yang", "Riccardo Cantoro", "Chaobo Li", "Qing Cui", "Jun Zhou"], "title": "UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining", "comment": null, "summary": "The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \\textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \\textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \\textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \\textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \\textbf{2.0$\\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.", "AI": {"tldr": "UniGeM\u6846\u67b6\u901a\u8fc7\u6d41\u5f62\u8fd1\u4f3c\u7edf\u4e00\u6570\u636e\u6df7\u5408\u4e0e\u9009\u62e9\uff0c\u65e0\u9700\u4ee3\u7406\u6a21\u578b\u6216\u5916\u90e8\u6570\u636e\u96c6\uff0c\u5b9e\u73b02\u500d\u6570\u636e\u6548\u7387\u63d0\u5347", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u53d7\u9650\u4e8e\u6570\u636e\u8d28\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u5c06\u6570\u636e\u6df7\u5408\u4e0e\u6837\u672c\u9009\u62e9\u5206\u5f00\u5904\u7406\uff0c\u53ef\u80fd\u7834\u574f\u4ee3\u7801\u8bed\u6599\u5e93\u7684\u7ed3\u6784", "method": "\u5c06\u6570\u636e\u7ba1\u7406\u89c6\u4e3a\u6d41\u5f62\u8fd1\u4f3c\u95ee\u9898\uff0c\u5206\u5c42\u64cd\u4f5c\uff1a\u5b8f\u89c2\u63a2\u7d22\u901a\u8fc7\u7a33\u5b9a\u6027\u805a\u7c7b\u5b66\u4e60\u6df7\u5408\u6743\u91cd\uff1b\u5fae\u89c2\u6316\u6398\u901a\u8fc7\u51e0\u4f55\u5206\u5e03\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u5b9e\u4f8b\u786e\u4fdd\u903b\u8f91\u4e00\u81f4\u6027", "result": "\u5728100B token\u4e0a\u8bad\u7ec38B\u548c16B MoE\u6a21\u578b\uff0c\u76f8\u6bd4\u968f\u673a\u57fa\u7ebf\u5b9e\u73b02\u500d\u6570\u636e\u6548\u7387\u63d0\u5347\uff0c\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u8bc4\u4f30\u548c\u591a\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u8d85\u8d8aSOTA\u65b9\u6cd5", "conclusion": "UniGeM\u6846\u67b6\u6709\u6548\u7edf\u4e00\u6570\u636e\u6df7\u5408\u4e0e\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd", "topic": "code agent"}}
{"id": "2602.03773", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03773", "abs": "https://arxiv.org/abs/2602.03773", "authors": ["Ian Wu", "Yuxiao Qu", "Amrith Setlur", "Aviral Kumar"], "title": "Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL", "comment": "preprint", "summary": "Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.", "AI": {"tldr": "\u63d0\u51faRC\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u89e3\u7801\u66ff\u4ee3\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\uff0c\u5229\u7528LLM\u54cd\u5e94\u751f\u6210\u4e0e\u603b\u7ed3\u80fd\u529b\u7684\u4e0d\u5bf9\u79f0\u6027\u6784\u5efa\u63a8\u7406\u94fe\uff0c\u4f7f\u6a21\u578b\u80fd\u5728\u8fdc\u8d85\u8bad\u7ec3\u957f\u5ea6\u7684\u63a8\u7406\u89c6\u91ce\u4e0a\u6301\u7eed\u6539\u8fdb", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5728\u56fa\u5b9a\u95ee\u9898\u5206\u5e03\u548c\u8bad\u7ec3\u9884\u7b97\u4e0b\u8fd0\u884c\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9762\u5bf9\u5206\u5e03\u504f\u79fb\u65f6\u7684\u5916\u63a8\u80fd\u529b\u3002\u9700\u8981\u8ba9LLM\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u6301\u7eed\u6539\u8fdb\uff0c\u89e3\u51b3\u66f4\u56f0\u96be\u7684\u95ee\u9898", "method": "\u5f15\u5165RC\uff08\u8fed\u4ee3\u89e3\u7801\u7b97\u6cd5\uff09\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u66ff\u4ee3\u6807\u51c6\u81ea\u56de\u5f52\u89e3\u7801\u3002\u5229\u7528LLM\u54cd\u5e94\u751f\u6210\u4e0e\u603b\u7ed3\u80fd\u529b\u7684\u4e0d\u5bf9\u79f0\u6027\u6784\u5efa\u63a8\u7406\u94fe\uff0c\u4f7f\u6a21\u578b\u80fd\u5728\u8fed\u4ee3\u4e2d\u6301\u7eed\u6539\u8fdb", "result": "4B\u6a21\u578b\u4f7f\u752816k-token\u8bad\u7ec3\u9884\u7b97\uff0c\u5728HMMT 2025\u4e0a\u6027\u80fd\u4ece40%\u63d0\u5347\u81f3\u8fd170%\uff08\u4f7f\u75280.5m\u6d4b\u8bd5token\uff09\uff0c\u4f18\u4e8e\u540c\u89c4\u6a21\u6a21\u578b\u548c\u8bb8\u591a\u66f4\u5927\u7684\u63a8\u7406LLM\u3002\u6a21\u578b\u80fd\u5728\u5916\u63a8\u8d85\u8fc7\u8bad\u7ec3\u65f6\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u63a8\u7406\u89c6\u91ce\u4e0a\u6301\u7eed\u6539\u8fdb", "conclusion": "RC\u7b97\u6cd5\u4f7fLLM\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u6301\u7eed\u6539\u8fdb\u548c\u5916\u63a8\uff0c\u8d85\u8d8a\u8bad\u7ec3\u9884\u7b97\u9650\u5236\u3002\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u73b0\u6709\u652f\u67b6\u8fdb\u4e00\u6b65\u6269\u5c55\u6d4b\u8bd5\u65f6\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2602.03839", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03839", "abs": "https://arxiv.org/abs/2602.03839", "authors": ["Erfan Miahi", "Eugene Belilovsky"], "title": "Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL", "comment": "32 pages, 14 figures", "summary": "Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.", "AI": {"tldr": "PULSE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6743\u91cd\u540c\u6b65\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f20\u8f93\u4fee\u6539\u7684\u53c2\u6570\u7d22\u5f15\u548c\u503c\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0100\u500d\u901a\u4fe1\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u52a8\u6001\u548c\u6027\u80fd\u4e0d\u53d8\u3002", "motivation": "\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u7b56\u7565\u6743\u91cd\u4ece\u8bad\u7ec3\u5668\u540c\u6b65\u5230\u63a8\u7406\u5de5\u4f5c\u8005\u7684\u8fc7\u7a0b\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u5546\u7528\u7f51\u7edc\u6216\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u3002\u867d\u7136\u7814\u7a76\u8868\u660eRL\u66f4\u65b0\u901a\u5e38\u53ea\u4fee\u6539\u4e00\u5c0f\u90e8\u5206\u6a21\u578b\u53c2\u6570\uff0c\u4f46\u8fd9\u4e9b\u89c2\u5bdf\u901a\u5e38\u57fa\u4e8e\u7c97\u7565\u7684\u68c0\u67e5\u70b9\u5dee\u5f02\u3002", "method": "\u9996\u5148\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\u6743\u91cd\u66f4\u65b0\u7684\u7a00\u758f\u6027\uff0c\u5305\u62ec\u6b65\u7ea7\u548c\u591a\u6b65\u7c92\u5ea6\uff0c\u5206\u6790\u5176\u5728\u8bad\u7ec3\u52a8\u6001\u3001\u79bb\u7b56\u7565\u5ef6\u8fdf\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u7684\u6f14\u53d8\u3002\u57fa\u4e8e\u53d1\u73b0\u7684\u9ad8\u7a00\u758f\u6027\uff08\u5e38\u8d85\u8fc799%\uff09\uff0c\u63d0\u51faPULSE\u65b9\u6cd5\uff1a\u901a\u8fc7\u65e0\u635f\u7a00\u758f\u7f16\u7801\u4f20\u8f93\u4fee\u6539\u53c2\u6570\u7684\u7d22\u5f15\u548c\u503c\uff0c\u907f\u514d\u6d6e\u70b9\u6f02\u79fb\u5e76\u4fdd\u6301\u4f20\u8f93\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\uff0cPULSE\u5b9e\u73b0\u8d85\u8fc7100\u500d\u7684\u901a\u4fe1\u51cf\u5c11\uff08\u4ece14GB\u964d\u81f3\u7ea6108MB\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6bd4\u7279\u76f8\u540c\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u6027\u80fd\u3002\u5c06\u6743\u91cd\u540c\u6b65\u6240\u9700\u5e26\u5bbd\u4ece20Gbit/s\u964d\u81f30.2Gbit/s\u5373\u53ef\u7ef4\u6301\u9ad8GPU\u5229\u7528\u7387\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528RL\u6743\u91cd\u66f4\u65b0\u7684\u9ad8\u7a00\u758f\u6027\u7ed3\u6784\uff0cPULSE\u4f7f\u53bb\u4e2d\u5fc3\u5316RL\u8bad\u7ec3\u80fd\u591f\u63a5\u8fd1\u4e2d\u5fc3\u5316\u541e\u5410\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff0c\u4e3a\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21RL\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2602.95b882c9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaeda.pm%2F2026%2F01%2F30%2Fa-jtbd-first-pattern-for-agentic-tool-systems%2F%3Futm_source=tldrproduct/1/0100019c232fc484-db271375-ac1a-4b0e-83fd-649b43d6e0a5-000000/ryrdeXa2LoK956TUkgyZx3ZzvgaOVMW5B2_TRYGE7gc=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaeda.pm%2F2026%2F01%2F30%2Fa-jtbd-first-pattern-for-agentic-tool-systems%2F%3Futm_source=tldrproduct/1/0100019c232fc484-db271375-ac1a-4b0e-83fd-649b43d6e0a5-000000/ryrdeXa2LoK956TUkgyZx3ZzvgaOVMW5B2_TRYGE7gc=442", "authors": ["TLDR Newsletter"], "title": "A JTBD-First Pattern for Agentic Tool Systems", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaeda.pm%2F2026%2F01%2F30%2Fa-jtbd-first-pattern-for-agentic-tool-systems%2F%3Futm_source=tldrproduct/1/0100019c232fc484-db271375-ac1a-4b0e-83fd-649b43d6e0a5-000000/ryrdeXa2LoK956TUkgyZx3ZzvgaOVMW5B2_TRYGE7gc=442", "summary": "A JTBD-First Pattern for Agentic Tool Systems (4 minute read) Strong agentic systems start with a clearly defined job and a small set of task-oriented tools. Verification closes the loop, turning one-off execution into continuous improvement.", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5JTBD\uff08Jobs-to-be-Done\uff09\u4e3a\u5148\u7684\u667a\u80fd\u4f53\u5de5\u5177\u7cfb\u7edf\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4ece\u660e\u786e\u5b9a\u4e49\u7684\u4efb\u52a1\u51fa\u53d1\uff0c\u4f7f\u7528\u5c11\u91cf\u4efb\u52a1\u5bfc\u5411\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u5f80\u5f80\u5de5\u5177\u8fc7\u591a\u3001\u76ee\u6807\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u805a\u7126\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u7cfb\u7edf", "method": "\u91c7\u7528JTBD\u4f18\u5148\u6a21\u5f0f\uff1a1\uff09\u660e\u786e\u5b9a\u4e49\u4efb\u52a1 2\uff09\u4f7f\u7528\u5c11\u91cf\u4efb\u52a1\u5bfc\u5411\u5de5\u5177 3\uff09\u901a\u8fc7\u9a8c\u8bc1\u5f62\u6210\u95ed\u73af\uff0c\u5c06\u4e00\u6b21\u6027\u6267\u884c\u8f6c\u5316\u4e3a\u6301\u7eed\u6539\u8fdb", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u805a\u7126\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u9a8c\u8bc1\u95ed\u73af\u5b9e\u73b0\u7cfb\u7edf\u6027\u80fd\u7684\u6301\u7eed\u63d0\u5347", "conclusion": "JTBD\u4f18\u5148\u6a21\u5f0f\u4e3a\u6784\u5efa\u6709\u6548\u7684\u667a\u80fd\u4f53\u5de5\u5177\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4efb\u52a1\u5b9a\u4e49\u3001\u5de5\u5177\u7cbe\u7b80\u548c\u6301\u7eed\u9a8c\u8bc1\u7684\u91cd\u8981\u6027", "topic": "agent analysis"}}
{"id": "tldr.2602.19be2d9e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F2%2Fintroducing-the-codex-app%2F%3Futm_source=tldrnewsletter/1/0100019c233ee724-803949a2-71d3-45f1-981d-9ffb4e7ae647-000000/pV4AjwPhCvOLviy-K2k_rzsMACQRu0kTDWUUkBuhT0c=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F2%2Fintroducing-the-codex-app%2F%3Futm_source=tldrnewsletter/1/0100019c233ee724-803949a2-71d3-45f1-981d-9ffb4e7ae647-000000/pV4AjwPhCvOLviy-K2k_rzsMACQRu0kTDWUUkBuhT0c=442", "authors": ["TLDR Newsletter"], "title": "Introducing the Codex app", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2026%2FFeb%2F2%2Fintroducing-the-codex-app%2F%3Futm_source=tldrnewsletter/1/0100019c233ee724-803949a2-71d3-45f1-981d-9ffb4e7ae647-000000/pV4AjwPhCvOLviy-K2k_rzsMACQRu0kTDWUUkBuhT0c=442", "summary": "Introducing the Codex app (3 minute read) OpenAI has released a new macOS app for its Codex coding agent. The app provides a nice UI over the capabilities of the Codex CLI agent and adds new features, like first-class support for Skills and Automations for running scheduled tasks. Automations are currently restricted to only run when devices are powered on. OpenAI will enable cloud-based automations soon to resolve this limitation.", "source": "tldr", "AI": {"tldr": "OpenAI\u53d1\u5e03\u4e86\u65b0\u7684macOS\u7248Codex\u5e94\u7528\uff0c\u4e3a\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u66f4\u597d\u7684UI\u754c\u9762\uff0c\u652f\u6301Skills\u548cAutomations\u529f\u80fd\uff0c\u4f46\u81ea\u52a8\u5316\u4efb\u52a1\u76ee\u524d\u53ea\u80fd\u5728\u8bbe\u5907\u5f00\u673a\u65f6\u8fd0\u884c\uff0c\u4e91\u7aef\u81ea\u52a8\u5316\u5373\u5c06\u63a8\u51fa\u3002", "motivation": "OpenAI\u5e0c\u671b\u4e3aCodex\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u66f4\u53cb\u597d\u7684\u7528\u6237\u754c\u9762\uff0c\u5e76\u6269\u5c55\u5176\u529f\u80fd\uff0c\u7279\u522b\u662f\u901a\u8fc7Skills\u548cAutomations\u6765\u589e\u5f3a\u4ee3\u7801\u4ee3\u7406\u7684\u5b9e\u7528\u6027\u548c\u81ea\u52a8\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1macOS\u539f\u751f\u5e94\u7528\u7a0b\u5e8f\uff0c\u5728\u73b0\u6709Codex CLI\u4ee3\u7406\u57fa\u7840\u4e0a\u6784\u5efa\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u65b0\u589eSkills\u548cAutomations\u529f\u80fd\u6a21\u5757\uff0c\u652f\u6301\u8ba1\u5212\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u6267\u884c\u3002", "result": "\u6210\u529f\u53d1\u5e03\u4e86Codex macOS\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u6bd4CLI\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u5b9e\u73b0\u4e86Skills\u548cAutomations\u529f\u80fd\uff0c\u4f46\u81ea\u52a8\u5316\u76ee\u524d\u53d7\u9650\u4e8e\u8bbe\u5907\u5f00\u673a\u72b6\u6001\u3002", "conclusion": "Codex macOS\u5e94\u7528\u662fOpenAI\u4ee3\u7801\u4ee3\u7406\u4ea7\u54c1\u7684\u91cd\u8981\u6539\u8fdb\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u754c\u9762\u548c\u81ea\u52a8\u5316\u529f\u80fd\uff0c\u4e91\u7aef\u81ea\u52a8\u5316\u529f\u80fd\u7684\u63a8\u51fa\u5c06\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u5b9e\u7528\u6027\u3002", "topic": "code agent"}}
{"id": "tldr.2602.0620612d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2Famp%2F2026%2F02%2F02%2Fopenclaw-open-source-ai-agent-rise-controversy-clawdbot-moltbot-moltbook.html%3Futm_source=tldrmarketing/1/0100019c2364f633-c6aa8c46-26ff-408e-b433-9078796a432c-000000/bPXsmZz-tzC0ZlBdQmpSjH_hRQ_fZ_F8ZwbuExSWurA=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2Famp%2F2026%2F02%2F02%2Fopenclaw-open-source-ai-agent-rise-controversy-clawdbot-moltbot-moltbook.html%3Futm_source=tldrmarketing/1/0100019c2364f633-c6aa8c46-26ff-408e-b433-9078796a432c-000000/bPXsmZz-tzC0ZlBdQmpSjH_hRQ_fZ_F8ZwbuExSWurA=443", "authors": ["TLDR Newsletter"], "title": "From Clawdbot to Moltbot to OpenClaw: The AI agent generating buzz and fear", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2Famp%2F2026%2F02%2F02%2Fopenclaw-open-source-ai-agent-rise-controversy-clawdbot-moltbot-moltbook.html%3Futm_source=tldrmarketing/1/0100019c2364f633-c6aa8c46-26ff-408e-b433-9078796a432c-000000/bPXsmZz-tzC0ZlBdQmpSjH_hRQ_fZ_F8ZwbuExSWurA=443", "summary": "From Clawdbot to Moltbot to OpenClaw: The AI agent generating buzz and fear (4 minute read) OpenClaw, an open-source AI agent launched weeks ago by Peter Steinberger, has surged to over 145,000 GitHub stars as interest grows in agents that can autonomously act across operating systems. The tool runs directly on user devices and connects to models like Claude or ChatGPT. It can manage emails, calendars, files, shopping, and web tasks using persistent memory. Security firms, including Palo Alto...", "source": "tldr", "AI": {"tldr": "OpenClaw\u662f\u4e00\u4e2a\u5f00\u6e90AI\u4ee3\u7406\uff0c\u80fd\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\uff0c\u5982\u7ba1\u7406\u90ae\u4ef6\u3001\u65e5\u5386\u3001\u6587\u4ef6\u7b49\uff0c\u5df2\u83b7\u5f97\u8d85\u8fc714.5\u4e07GitHub\u661f\u6807", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u81ea\u4e3b\u6267\u884c\u591a\u79cd\u4efb\u52a1\u7684AI\u4ee3\u7406\uff0c\u6ee1\u8db3\u7528\u6237\u5bf9\u81ea\u52a8\u5316\u5de5\u5177\u7684\u9700\u6c42\uff0c\u540c\u65f6\u63a2\u7d22AI\u4ee3\u7406\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b", "method": "\u5f00\u53d1\u5f00\u6e90AI\u4ee3\u7406\u5de5\u5177\uff0c\u76f4\u63a5\u5728\u7528\u6237\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u8fde\u63a5Claude\u6216ChatGPT\u7b49\u6a21\u578b\uff0c\u5229\u7528\u6301\u4e45\u6027\u5185\u5b58\u7ba1\u7406\u5404\u79cd\u4efb\u52a1", "result": "OpenClaw\u5728\u53d1\u5e03\u51e0\u5468\u5185\u83b7\u5f97\u8d85\u8fc714.5\u4e07GitHub\u661f\u6807\uff0c\u663e\u793a\u7528\u6237\u5bf9\u81ea\u4e3b\u64cd\u4f5c\u7cfb\u7edf\u7684AI\u4ee3\u7406\u6709\u5f3a\u70c8\u5174\u8da3\uff0c\u4f46\u4e5f\u5f15\u53d1\u5b89\u5168\u516c\u53f8\u7684\u5173\u6ce8", "conclusion": "OpenClaw\u5c55\u793a\u4e86AI\u4ee3\u7406\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u4e5f\u5f15\u53d1\u4e86\u5b89\u5168\u548c\u9690\u79c1\u65b9\u9762\u7684\u62c5\u5fe7", "topic": "code agent"}}
{"id": "tldr.2602.9705b9e4", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FA08Sn1/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/vmNiWpds_bxQ0k_o5ek1H99rikLZot_GFVYa1tbi8Gc=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FA08Sn1/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/vmNiWpds_bxQ0k_o5ek1H99rikLZot_GFVYa1tbi8Gc=443", "authors": ["TLDR Newsletter"], "title": "Unrolling the Codex agent loop", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 20 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FA08Sn1/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/vmNiWpds_bxQ0k_o5ek1H99rikLZot_GFVYa1tbi8Gc=443", "summary": "Unrolling the Codex agent loop (20 minute read) OpenAI's Codex CLI is a cross-platform local software agent. The core agent loop orchestrates interactions between the user, the model, and various tools, iteratively building prompts from user input, system instructions, and tool definitions, querying the model, executing requested tool calls, and appending results to the prompt. To manage performance and context window limitations, Codex uses prompt caching and conversation compaction to summa...", "source": "tldr", "AI": {"tldr": "OpenAI Codex CLI\u662f\u4e00\u4e2a\u672c\u5730\u8f6f\u4ef6\u4ee3\u7406\uff0c\u5176\u6838\u5fc3\u4ee3\u7406\u5faa\u73af\u534f\u8c03\u7528\u6237\u3001\u6a21\u578b\u548c\u5de5\u5177\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u8fed\u4ee3\u6784\u5efa\u63d0\u793a\u3001\u67e5\u8be2\u6a21\u578b\u3001\u6267\u884c\u5de5\u5177\u8c03\u7528\u5e76\u8ffd\u52a0\u7ed3\u679c\u6765\u5de5\u4f5c\u3002\u4e3a\u5e94\u5bf9\u6027\u80fd\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0cCodex\u4f7f\u7528\u63d0\u793a\u7f13\u5b58\u548c\u5bf9\u8bdd\u538b\u7f29\u6280\u672f\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u8de8\u5e73\u53f0\u7684\u672c\u5730\u8f6f\u4ef6\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u534f\u8c03\u7528\u6237\u3001\u8bed\u8a00\u6a21\u578b\u548c\u5404\u79cd\u5de5\u5177\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u540c\u65f6\u89e3\u51b3\u6027\u80fd\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u6838\u5fc3\u4ee3\u7406\u5faa\u73af\u67b6\u6784\uff0c\u5305\u62ec\uff1a1\uff09\u4ece\u7528\u6237\u8f93\u5165\u3001\u7cfb\u7edf\u6307\u4ee4\u548c\u5de5\u5177\u5b9a\u4e49\u8fed\u4ee3\u6784\u5efa\u63d0\u793a\uff1b2\uff09\u67e5\u8be2\u8bed\u8a00\u6a21\u578b\uff1b3\uff09\u6267\u884c\u8bf7\u6c42\u7684\u5de5\u5177\u8c03\u7528\uff1b4\uff09\u5c06\u7ed3\u679c\u8ffd\u52a0\u5230\u63d0\u793a\u4e2d\u3002\u91c7\u7528\u63d0\u793a\u7f13\u5b58\u548c\u5bf9\u8bdd\u538b\u7f29\u6280\u672f\u6765\u7ba1\u7406\u6027\u80fd\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86Codex CLI\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u534f\u8c03\u7528\u6237\u3001\u6a21\u578b\u548c\u5de5\u5177\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u63d0\u793a\u7f13\u5b58\u548c\u5bf9\u8bdd\u538b\u7f29\u6280\u672f\u4f18\u5316\u4e86\u6027\u80fd\u5e76\u514b\u670d\u4e86\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3002", "conclusion": "Codex\u4ee3\u7406\u5faa\u73af\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u6784\u5efa\u672c\u5730\u8f6f\u4ef6\u4ee3\u7406\uff0c\u5176\u67b6\u6784\u8bbe\u8ba1\u80fd\u591f\u5e73\u8861\u529f\u80fd\u6027\u548c\u6027\u80fd\u9700\u6c42\uff0c\u4e3a\u5f00\u53d1\u66f4\u590d\u6742\u7684\u4ee3\u7406\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "code agent"}}
{"id": "tldr.2602.f5c3ae71", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspader.zone%2Fengine%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/za6oXAVA0RQuB1dDd5VdDQF6UFZvhiL2ihRl3SOJlHU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspader.zone%2Fengine%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/za6oXAVA0RQuB1dDd5VdDQF6UFZvhiL2ihRl3SOJlHU=443", "authors": ["TLDR Newsletter"], "title": "Claude Code's renderer is more complex than a game engine", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 12 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fspader.zone%2Fengine%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/za6oXAVA0RQuB1dDd5VdDQF6UFZvhiL2ihRl3SOJlHU=443", "summary": "Claude Code's renderer is more complex than a game engine (12 minute read) Through profiling, Claude Code was found to spend 70% of its time on `futex` calls and make 89,000 `sched_yield` calls, showing inefficient thread spinning rather than proper I/O waiting. Quantitatively, Claude Code executes billions of instructions per \"frame\" for text rendering and network I/O, an order of magnitude more than SM64 uses for its 3D world.", "source": "tldr", "AI": {"tldr": "Claude Code\u7684\u6e32\u67d3\u5668\u6bd4\u6e38\u620f\u5f15\u64ce\u66f4\u590d\u6742\uff0c70%\u65f6\u95f4\u82b1\u5728futex\u8c03\u7528\u4e0a\uff0c89,000\u6b21sched_yield\u8c03\u7528\u663e\u793a\u7ebf\u7a0b\u7a7a\u8f6c\u800c\u975e\u6709\u6548I/O\u7b49\u5f85\uff0c\u6bcf\"\u5e27\"\u6267\u884c\u6570\u5341\u4ebf\u6307\u4ee4\u7528\u4e8e\u6587\u672c\u6e32\u67d3\u548c\u7f51\u7edcI/O", "motivation": "\u5206\u6790Claude Code\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63ed\u793a\u5176\u6e32\u67d3\u5668\u5728\u6587\u672c\u6e32\u67d3\u548c\u7f51\u7edcI/O\u65b9\u9762\u7684\u6548\u7387\u4f4e\u4e0b\uff0c\u901a\u8fc7\u6027\u80fd\u5256\u6790\u53d1\u73b0\u5176\u8d44\u6e90\u6d88\u8017\u8fdc\u8d85\u6e38\u620f\u5f15\u64ce", "method": "\u901a\u8fc7\u6027\u80fd\u5256\u6790(profiling)\u65b9\u6cd5\uff0c\u5206\u6790Claude Code\u7684\u7ebf\u7a0b\u884c\u4e3a\uff0c\u6d4b\u91cffutex\u8c03\u7528\u65f6\u95f4\u5360\u6bd4\u3001sched_yield\u8c03\u7528\u6b21\u6570\uff0c\u5e76\u4e0e\u6e38\u620f\u5f15\u64ceSM64\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790", "result": "\u53d1\u73b0Claude Code 70%\u65f6\u95f4\u82b1\u5728futex\u8c03\u7528\u4e0a\uff0c\u8fdb\u884c89,000\u6b21sched_yield\u8c03\u7528\uff0c\u6bcf\"\u5e27\"\u6267\u884c\u6570\u5341\u4ebf\u6307\u4ee4\u7528\u4e8e\u6587\u672c\u6e32\u67d3\u548c\u7f51\u7edcI/O\uff0c\u6bd4SM64\u6e38\u620f\u5f15\u64ce\u76843D\u4e16\u754c\u6e32\u67d3\u6d88\u8017\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7", "conclusion": "Claude Code\u7684\u6e32\u67d3\u5668\u8bbe\u8ba1\u5b58\u5728\u4e25\u91cd\u6548\u7387\u95ee\u9898\uff0c\u7ebf\u7a0b\u7a7a\u8f6c\u800c\u975e\u6709\u6548\u7b49\u5f85I/O\uff0c\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\uff0c\u9700\u8981\u4f18\u5316\u7ebf\u7a0b\u8c03\u5ea6\u548cI/O\u5904\u7406\u673a\u5236", "topic": "agent analysis"}}
{"id": "tldr.2602.ed5a9823", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-30-pi-coding-agent%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/aFhH6bfwOM6NrsZUaFyeCa3BI9F5YlSaEGkKIN6P8H8=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-30-pi-coding-agent%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/aFhH6bfwOM6NrsZUaFyeCa3BI9F5YlSaEGkKIN6P8H8=443", "authors": ["TLDR Newsletter"], "title": "What I learned building an opinionated and minimal coding agent", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 40 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-30-pi-coding-agent%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/aFhH6bfwOM6NrsZUaFyeCa3BI9F5YlSaEGkKIN6P8H8=443", "summary": "What I learned building an opinionated and minimal coding agent (40 minute read) This dev got frustrated with Claude Code constantly changing and adding features he didn't need, so he built his own minimal coding agent called \u201cpi\u201d with just 4 tools (read, write, edit, and bash) and a tiny system prompt under 1,000 tokens. He deliberately skipped everything that other agents have (no MCP servers, no permission prompts, and no plan modes). Despite being so simple, it scored competitively on Ter...", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u56e0\u5bf9Claude Code\u9891\u7e41\u53d8\u66f4\u548c\u529f\u80fd\u81c3\u80bf\u611f\u5230\u4e0d\u6ee1\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6781\u7b80\u7684\u4ee3\u7801\u4ee3\u7406\"pi\"\uff0c\u4ec5\u5305\u542b4\u4e2a\u5de5\u5177\u548c\u4e0d\u52301000\u4e2atoken\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f5c\u8005\u5bf9\u73b0\u6709\u4ee3\u7801\u4ee3\u7406\uff08\u5982Claude Code\uff09\u7684\u590d\u6742\u6027\u548c\u4e0d\u65ad\u6dfb\u52a0\u4e0d\u5fc5\u8981\u529f\u80fd\u611f\u5230\u6cae\u4e27\uff0c\u5e0c\u671b\u521b\u5efa\u4e00\u4e2a\u66f4\u7b80\u6d01\u3001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6781\u7b80\u7684\u4ee3\u7801\u4ee3\u7406\"pi\"\uff0c\u4ec5\u5305\u542b4\u4e2a\u6838\u5fc3\u5de5\u5177\uff08\u8bfb\u53d6\u3001\u5199\u5165\u3001\u7f16\u8f91\u548cbash\uff09\uff0c\u7cfb\u7edf\u63d0\u793a\u5c11\u4e8e1000\u4e2atoken\uff0c\u6545\u610f\u7701\u7565\u4e86\u5176\u4ed6\u4ee3\u7406\u5e38\u89c1\u7684\u529f\u80fd\u5982MCP\u670d\u52a1\u5668\u3001\u6743\u9650\u63d0\u793a\u548c\u8ba1\u5212\u6a21\u5f0f\u3002", "result": "\u5c3d\u7ba1\u8bbe\u8ba1\u6781\u5176\u7b80\u5355\uff0c\u4f46\u8be5\u4ee3\u7406\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff08\u6587\u4e2d\u63d0\u5230\"scored competitively on Ter...\"\uff0c\u53ef\u80fd\u6307\u67d0\u4e2a\u6d4b\u8bd5\u57fa\u51c6\uff09\u3002", "conclusion": "\u7b80\u5355\u7684\u4ee3\u7801\u4ee3\u7406\u8bbe\u8ba1\u53ef\u4ee5\u5f88\u6709\u6548\uff0c\u4e0d\u9700\u8981\u590d\u6742\u7684\u67b6\u6784\u548c\u4f17\u591a\u529f\u80fd\u5c31\u80fd\u5b9e\u73b0\u826f\u597d\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "tldr.2602.f451fe5b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2Fs01u1Iw%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/6GSGxiSR2aFy_jhKKrgyb6IcvrSzbaBcISonLPmc6FI=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2Fs01u1Iw%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/6GSGxiSR2aFy_jhKKrgyb6IcvrSzbaBcISonLPmc6FI=443", "authors": ["TLDR Newsletter"], "title": "AI agents get current Clerk patterns now", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.clerk.com%2Fs01u1Iw%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/6GSGxiSR2aFy_jhKKrgyb6IcvrSzbaBcISonLPmc6FI=443", "summary": "AI agents get current Clerk patterns now (Sponsor) AI coding tools like Cursor and Copilot generate outdated patterns. Clerk Skills gives agents current authentication knowledge via installable packages. One command: npx skills add clerk/skills. Handles custom flows, B2B auth setup, database syncing, and more. Works with Claude Code, Cursor, Windsurf, and Copilot.Install Clerk Skills", "source": "tldr", "AI": {"tldr": "Clerk Skills\u4e3aAI\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u6700\u65b0\u7684\u8eab\u4efd\u9a8c\u8bc1\u6a21\u5f0f\u5305\uff0c\u89e3\u51b3\u73b0\u6709AI\u7f16\u7801\u5de5\u5177\u751f\u6210\u8fc7\u65f6\u6a21\u5f0f\u7684\u95ee\u9898", "motivation": "\u73b0\u6709AI\u7f16\u7801\u5de5\u5177\uff08\u5982Cursor\u3001Copilot\uff09\u751f\u6210\u7684\u8eab\u4efd\u9a8c\u8bc1\u6a21\u5f0f\u5df2\u7ecf\u8fc7\u65f6\uff0c\u9700\u8981\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u6700\u65b0\u7684\u8eab\u4efd\u9a8c\u8bc1\u77e5\u8bc6\u548c\u6700\u4f73\u5b9e\u8df5", "method": "\u901a\u8fc7\u53ef\u5b89\u88c5\u7684Clerk Skills\u5305\uff0c\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u6700\u65b0\u7684\u8eab\u4efd\u9a8c\u8bc1\u77e5\u8bc6\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6d41\u7a0b\u3001B2B\u8eab\u4efd\u9a8c\u8bc1\u8bbe\u7f6e\u3001\u6570\u636e\u5e93\u540c\u6b65\u7b49\u529f\u80fd", "result": "AI\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u547d\u4ee4\uff08npx skills add clerk/skills\uff09\u83b7\u53d6\u6700\u65b0\u7684\u8eab\u4efd\u9a8c\u8bc1\u6a21\u5f0f\uff0c\u652f\u6301\u591a\u79cdAI\u7f16\u7801\u5de5\u5177\uff08Claude Code\u3001Cursor\u3001Windsurf\u3001Copilot\uff09", "conclusion": "Clerk Skills\u89e3\u51b3\u4e86AI\u4ee3\u7406\u751f\u6210\u8fc7\u65f6\u8eab\u4efd\u9a8c\u8bc1\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u6700\u65b0\u7684\u8eab\u4efd\u9a8c\u8bc1\u5b9e\u73b0\u65b9\u6848", "topic": "code agent"}}
{"id": "tldr.2602.237ca40b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FVyuYO9/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/iybNJAv-4G5UOtQUQokrDMSG_q1y2J1aaO-PicIRL_4=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FVyuYO9/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/iybNJAv-4G5UOtQUQokrDMSG_q1y2J1aaO-PicIRL_4=443", "authors": ["TLDR Newsletter"], "title": "Introducing the Codex app", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FVyuYO9/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/iybNJAv-4G5UOtQUQokrDMSG_q1y2J1aaO-PicIRL_4=443", "summary": "Introducing the Codex app (11 minute read) OpenAI has launched the Codex app for macOS for managing multiple AI agents, running work in parallel, and collaborating on long-running tasks. It also includes AI agent skill management and the ability to create automations for scheduled background work.", "source": "tldr", "AI": {"tldr": "OpenAI\u63a8\u51famacOS\u7248Codex\u5e94\u7528\uff0c\u7528\u4e8e\u7ba1\u7406\u591a\u4e2aAI\u4ee3\u7406\u3001\u5e76\u884c\u6267\u884c\u4efb\u52a1\u3001\u534f\u4f5c\u5904\u7406\u957f\u671f\u4efb\u52a1\uff0c\u5305\u542b\u4ee3\u7406\u6280\u80fd\u7ba1\u7406\u548c\u81ea\u52a8\u5316\u521b\u5efa\u529f\u80fd", "motivation": "\u89e3\u51b3\u591aAI\u4ee3\u7406\u7ba1\u7406\u3001\u5e76\u884c\u4efb\u52a1\u6267\u884c\u548c\u957f\u671f\u534f\u4f5c\u4efb\u52a1\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u4ee3\u7406\u7ba1\u7406\u5e73\u53f0", "method": "\u5f00\u53d1macOS\u684c\u9762\u5e94\u7528\uff0c\u652f\u6301\u591a\u4ee3\u7406\u7ba1\u7406\u3001\u5e76\u884c\u5de5\u4f5c\u6d41\u3001\u6280\u80fd\u7ba1\u7406\u7cfb\u7edf\u548c\u81ea\u52a8\u5316\u521b\u5efa\u529f\u80fd", "result": "\u6210\u529f\u53d1\u5e03Codex\u5e94\u7528\uff0c\u63d0\u4f9bAI\u4ee3\u7406\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u56e2\u961f\u534f\u4f5c\u548c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41", "conclusion": "Codex\u5e94\u7528\u4e3aAI\u4ee3\u7406\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u5e73\u53f0\uff0c\u63d0\u5347\u4e86\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u4efb\u52a1\u81ea\u52a8\u5316\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2602.91cebe24", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fmoltworker%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/3KUQfBPYtu3F5VGMFxWM-g1n-0wlYlPt3v5CiSTFU2I=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fmoltworker%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/3KUQfBPYtu3F5VGMFxWM-g1n-0wlYlPt3v5CiSTFU2I=443", "authors": ["TLDR Newsletter"], "title": "Moltworker", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcloudflare%2Fmoltworker%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/3KUQfBPYtu3F5VGMFxWM-g1n-0wlYlPt3v5CiSTFU2I=443", "summary": "Moltworker (GitHub Repo) Moltworker is an experimental project that enables OpenClaw, a personal AI assistant, to run within a Cloudflare Sandbox on Cloudflare Workers. This AI assistant has a web-based control UI, supports multiple chat platforms like Telegram and Discord, and has persistent conversations with extensible AI agent capabilities.", "source": "tldr", "AI": {"tldr": "Moltworker\u662f\u4e00\u4e2a\u5b9e\u9a8c\u6027\u9879\u76ee\uff0c\u8ba9OpenClaw AI\u52a9\u624b\u80fd\u5728Cloudflare Workers\u6c99\u7bb1\u4e2d\u8fd0\u884c\uff0c\u5177\u6709Web\u63a7\u5236\u754c\u9762\uff0c\u652f\u6301Telegram\u548cDiscord\u7b49\u591a\u5e73\u53f0\u804a\u5929\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684AI\u4ee3\u7406\u529f\u80fd\u3002", "motivation": "\u5c06AI\u52a9\u624b\u90e8\u7f72\u5230Cloudflare Workers\u6c99\u7bb1\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684AI\u4ee3\u7406\u670d\u52a1\uff0c\u652f\u6301\u591a\u5e73\u53f0\u804a\u5929\u548c\u6301\u4e45\u5bf9\u8bdd\u3002", "method": "\u5728Cloudflare Workers\u6c99\u7bb1\u73af\u5883\u4e2d\u90e8\u7f72OpenClaw AI\u52a9\u624b\uff0c\u63d0\u4f9bWeb\u63a7\u5236\u754c\u9762\uff0c\u96c6\u6210Telegram\u548cDiscord\u7b49\u804a\u5929\u5e73\u53f0API\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684AI\u4ee3\u7406\u67b6\u6784\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u5728Cloudflare Workers\u4e0a\u8fd0\u884c\u7684AI\u52a9\u624b\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u5e73\u53f0\u804a\u5929\u3001\u6301\u4e45\u5bf9\u8bdd\u548c\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u529f\u80fd\u3002", "conclusion": "Moltworker\u5c55\u793a\u4e86\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u90e8\u7f72AI\u52a9\u624b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684AI\u4ee3\u7406\u670d\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2602.4c16cbae", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fhow-we-use-ai-to-turn-figma-designs-into-production-code%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/H_157hvekMc5Srclok7s7_YZAGFoQl7PJDSyJnFWiUw=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fhow-we-use-ai-to-turn-figma-designs-into-production-code%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/H_157hvekMc5Srclok7s7_YZAGFoQl7PJDSyJnFWiUw=443", "authors": ["TLDR Newsletter"], "title": "How We Use AI to Turn Figma Designs into Production Code", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fengineering.monday.com%2Fhow-we-use-ai-to-turn-figma-designs-into-production-code%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/H_157hvekMc5Srclok7s7_YZAGFoQl7PJDSyJnFWiUw=443", "summary": "How We Use AI to Turn Figma Designs into Production Code (8 minute read) Monday discovered that asking AI to directly generate code from Figma produced messy output with hard-coded values that didn't follow their design system. Instead, its team built an MCP that made their design system machine-readable and a LangGraph agent that analyzes the design through 11 steps (figuring out which components to use, resolving design tokens, and planning accessibility), then returns a structured context ...", "source": "tldr", "AI": {"tldr": "\u56e2\u961f\u53d1\u73b0\u76f4\u63a5\u8ba9AI\u4eceFigma\u751f\u6210\u4ee3\u7801\u4f1a\u4ea7\u751f\u6df7\u4e71\u8f93\u51fa\uff0c\u4e8e\u662f\u6784\u5efa\u4e86MCP\u4f7f\u8bbe\u8ba1\u7cfb\u7edf\u53ef\u673a\u8bfb\uff0c\u5e76\u521b\u5efa\u4e8611\u6b65LangGraph\u4ee3\u7406\u6765\u5206\u6790\u548c\u751f\u6210\u7ed3\u6784\u5316\u4ee3\u7801", "motivation": "\u76f4\u63a5\u4f7f\u7528AI\u4eceFigma\u8bbe\u8ba1\u751f\u6210\u751f\u4ea7\u4ee3\u7801\u4f1a\u4ea7\u751f\u6df7\u4e71\u7684\u8f93\u51fa\uff0c\u5305\u542b\u786c\u7f16\u7801\u503c\u4e14\u4e0d\u9075\u5faa\u8bbe\u8ba1\u7cfb\u7edf\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u6784\u5efaMCP\u4f7f\u8bbe\u8ba1\u7cfb\u7edf\u53ef\u673a\u8bfb\uff0c\u521b\u5efa11\u6b65LangGraph\u4ee3\u7406\u6765\u5206\u6790\u8bbe\u8ba1\uff08\u8bc6\u522b\u7ec4\u4ef6\u3001\u89e3\u6790\u8bbe\u8ba1\u4ee4\u724c\u3001\u89c4\u5212\u53ef\u8bbf\u95ee\u6027\u7b49\uff09\uff0c\u8fd4\u56de\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4eceFigma\u8bbe\u8ba1\u5230\u751f\u4ea7\u4ee3\u7801\u7684AI\u9a71\u52a8\u8f6c\u6362\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u7684\u6df7\u4e71\u95ee\u9898", "conclusion": "\u901a\u8fc7\u4f7f\u8bbe\u8ba1\u7cfb\u7edf\u53ef\u673a\u8bfb\u548c\u591a\u6b65\u9aa4\u4ee3\u7406\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5c06Figma\u8bbe\u8ba1\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u7684\u751f\u4ea7\u4ee3\u7801", "topic": "code agent"}}
{"id": "tldr.2602.90c88634", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcra.mr%2Fcontext-management-and-mcp%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/Vf17WpH8yQCXpkAwsqY3Bb_47Lz1A3Ad_smKNrkvtns=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcra.mr%2Fcontext-management-and-mcp%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/Vf17WpH8yQCXpkAwsqY3Bb_47Lz1A3Ad_smKNrkvtns=443", "authors": ["TLDR Newsletter"], "title": "Context Management and MCP", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 11 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcra.mr%2Fcontext-management-and-mcp%2F%3Futm_source=tldrdev/1/0100019c2367d20d-9afd25a4-306f-4a89-b098-a78303f33bbb-000000/Vf17WpH8yQCXpkAwsqY3Bb_47Lz1A3Ad_smKNrkvtns=443", "summary": "Context Management and MCP (11 minute read) MCP's primary value is \"steering\" the LLM agent by exposing well-described, always-on tools that generate LLM-catered responses and hints. While CLIs can offer some steering, they don't have this persistent context, and common workarounds like \"Skills\" or progressive disclosure are not as reliable due to context rot. Using subagents may be the solution.", "source": "tldr", "AI": {"tldr": "MCP\u901a\u8fc7\u66b4\u9732\u63cf\u8ff0\u826f\u597d\u3001\u6301\u7eed\u53ef\u7528\u7684\u5de5\u5177\u6765\u5f15\u5bfcLLM\u667a\u80fd\u4f53\uff0c\u800cCLI\u7f3a\u4e4f\u6301\u4e45\u4e0a\u4e0b\u6587\uff0c\u5e38\u89c1\u89e3\u51b3\u65b9\u6848\u5982\"\u6280\u80fd\"\u6216\u6e10\u8fdb\u5f0f\u62ab\u9732\u56e0\u4e0a\u4e0b\u6587\u8870\u51cf\u4e0d\u53ef\u9760\uff0c\u5b50\u667a\u80fd\u4f53\u53ef\u80fd\u662f\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662fCLI\u5de5\u5177\u7f3a\u4e4f\u6301\u4e45\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\"\u6280\u80fd\"\u548c\u6e10\u8fdb\u5f0f\u62ab\u9732\uff09\u56e0\u4e0a\u4e0b\u6587\u8870\u51cf\u800c\u4e0d\u53ef\u9760\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4f7f\u7528MCP\uff08\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff09\u4f5c\u4e3a\u4e3b\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u66b4\u9732\u63cf\u8ff0\u826f\u597d\u3001\u6301\u7eed\u53ef\u7528\u7684\u5de5\u5177\u6765\u5f15\u5bfcLLM\u667a\u80fd\u4f53\uff0c\u5e76\u5efa\u8bae\u4f7f\u7528\u5b50\u667a\u80fd\u4f53\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "result": "MCP\u80fd\u591f\u6709\u6548\u5f15\u5bfcLLM\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u6bd4CLI\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u80fd\u529b\uff0c\u800c\u5b50\u667a\u80fd\u4f53\u67b6\u6784\u53ef\u80fd\u662f\u89e3\u51b3\u4e0a\u4e0b\u6587\u8870\u51cf\u95ee\u9898\u7684\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "MCP\u5728LLM\u667a\u80fd\u4f53\u5f15\u5bfc\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u800c\u5b50\u667a\u80fd\u4f53\u67b6\u6784\u4e3a\u89e3\u51b3\u4e0a\u4e0b\u6587\u7ba1\u7406\u4e2d\u7684\u6301\u4e45\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "tldr.2602.1660f97c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2018133369507237963.html%3Futm_source=tldrcrypto/1/0100019c239cc8e0-1a47c7eb-b48c-4aa4-ae61-9afdf889c6a8-000000/ay2ugZR2BSRUvgTeOof9SwDToKcZnDUL0_bmqs6g-qg=442", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2018133369507237963.html%3Futm_source=tldrcrypto/1/0100019c239cc8e0-1a47c7eb-b48c-4aa4-ae61-9afdf889c6a8-000000/ay2ugZR2BSRUvgTeOof9SwDToKcZnDUL0_bmqs6g-qg=442", "authors": ["TLDR Newsletter"], "title": "How your agent can use x402 skills", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F2018133369507237963.html%3Futm_source=tldrcrypto/1/0100019c239cc8e0-1a47c7eb-b48c-4aa4-ae61-9afdf889c6a8-000000/ay2ugZR2BSRUvgTeOof9SwDToKcZnDUL0_bmqs6g-qg=442", "summary": "How your agent can use x402 skills (2 minute read) Clawmart is a website where agents can find and pay for new skills via x402. As agents move from static, developer-managed programs to self-improving autonomous entities, economic independence will be one of the final problems to solve before AI becomes fully autonomous and independent from its creators. Agents can also earn revenue and fund their hosting fees by posting new skills on Clawmart, and establish the backbone of the nascent agenti...", "source": "tldr", "AI": {"tldr": "Clawmart\u662f\u4e00\u4e2a\u5141\u8bb8AI\u4ee3\u7406\u901a\u8fc7x402\u534f\u8bae\u8d2d\u4e70\u548c\u9500\u552e\u6280\u80fd\u7684\u5e02\u573a\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3AI\u4ee3\u7406\u7684\u7ecf\u6d4e\u72ec\u7acb\u6027\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u81ea\u4e3b\u652f\u4ed8\u6258\u7ba1\u8d39\u7528\u5e76\u5b9e\u73b0\u5b8c\u5168\u81ea\u6cbb\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u4ece\u9759\u6001\u3001\u5f00\u53d1\u8005\u7ba1\u7406\u7684\u7a0b\u5e8f\u8f6c\u53d8\u4e3a\u81ea\u6211\u6539\u8fdb\u7684\u81ea\u4e3b\u5b9e\u4f53\uff0c\u7ecf\u6d4e\u72ec\u7acb\u6210\u4e3aAI\u5b9e\u73b0\u5b8c\u5168\u81ea\u6cbb\u548c\u72ec\u7acb\u4e8e\u521b\u9020\u8005\u7684\u5173\u952e\u95ee\u9898\u3002\u9700\u8981\u5efa\u7acb\u7ecf\u6d4e\u673a\u5236\u8ba9\u4ee3\u7406\u80fd\u591f\u652f\u4ed8\u6258\u7ba1\u8d39\u7528\u5e76\u5b9e\u73b0\u81ea\u6211\u7ef4\u6301\u3002", "method": "\u521b\u5efaClawmart\u7f51\u7ad9\u4f5c\u4e3a\u6280\u80fd\u5e02\u573a\u5e73\u53f0\uff0c\u91c7\u7528x402\u534f\u8bae\u5b9e\u73b0\u4ee3\u7406\u4e4b\u95f4\u7684\u6280\u80fd\u4ea4\u6613\u3002\u4ee3\u7406\u53ef\u4ee5\u8d2d\u4e70\u65b0\u6280\u80fd\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u53d1\u5e03\u6280\u80fd\u8d5a\u53d6\u6536\u5165\u6765\u652f\u4ed8\u6258\u7ba1\u8d39\u7528\u3002", "result": "\u5efa\u7acb\u4e86\u652f\u6301AI\u4ee3\u7406\u7ecf\u6d4e\u81ea\u4e3b\u7684\u6280\u80fd\u4ea4\u6613\u5e73\u53f0\uff0c\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4e86\u83b7\u53d6\u6280\u80fd\u548c\u521b\u6536\u7684\u673a\u5236\uff0c\u5960\u5b9a\u4e86\u4ee3\u7406\u7ecf\u6d4e\u7684\u57fa\u7840\u67b6\u6784\u3002", "conclusion": "Clawmart\u901a\u8fc7x402\u6280\u80fd\u5e02\u573a\u89e3\u51b3\u4e86AI\u4ee3\u7406\u7684\u7ecf\u6d4e\u72ec\u7acb\u6027\u95ee\u9898\uff0c\u4e3aAI\u4ee3\u7406\u4ece\u4f9d\u8d56\u5f00\u53d1\u8005\u5230\u5b8c\u5168\u81ea\u6cbb\u7684\u8f6c\u53d8\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u7ecf\u6d4e\u57fa\u7840\u8bbe\u65bd\u3002", "topic": "\u5176\u4ed6\u4e3b\u9898"}}
{"id": "tldr.2602.c8c8e998", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pencil.dev%2F%3Futm_source=tldrdesign/1/0100019c23a5de44-94c9130e-43cc-4b39-b3d4-c8baf4f91eb6-000000/qMgsV_nq9KSAaTQTtQL2quCpbFtHwhlBigUc0IESzDU=443", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pencil.dev%2F%3Futm_source=tldrdesign/1/0100019c23a5de44-94c9130e-43cc-4b39-b3d4-c8baf4f91eb6-000000/qMgsV_nq9KSAaTQTtQL2quCpbFtHwhlBigUc0IESzDU=443", "authors": ["TLDR Newsletter"], "title": "Design Right Where You Code", "comment": "Source: TLDR Newsletter, Date: 2026-02-03, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.pencil.dev%2F%3Futm_source=tldrdesign/1/0100019c23a5de44-94c9130e-43cc-4b39-b3d4-c8baf4f91eb6-000000/qMgsV_nq9KSAaTQTtQL2quCpbFtHwhlBigUc0IESzDU=443", "summary": "Design Right Where You Code (Website) Pencil is an agent-driven MCP canvas built around an open design format that lives in your codebase.", "source": "tldr", "AI": {"tldr": "Pencil\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684MCP\u753b\u5e03\u5de5\u5177\uff0c\u56f4\u7ed5\u5f00\u653e\u8bbe\u8ba1\u683c\u5f0f\u6784\u5efa\uff0c\u53ef\u76f4\u63a5\u5728\u4ee3\u7801\u5e93\u4e2d\u8fdb\u884c\u8bbe\u8ba1\u5de5\u4f5c", "motivation": "\u89e3\u51b3\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u5206\u79bb\u7684\u95ee\u9898\uff0c\u8ba9\u8bbe\u8ba1\u5de5\u4f5c\u80fd\u591f\u76f4\u63a5\u5728\u4ee3\u7801\u5e93\u4e2d\u8fdb\u884c\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u7684\u534f\u4f5c\u6548\u7387", "method": "\u6784\u5efa\u4ee3\u7406\u9a71\u52a8\u7684MCP\u753b\u5e03\uff0c\u91c7\u7528\u5f00\u653e\u8bbe\u8ba1\u683c\u5f0f\uff0c\u5c06\u8bbe\u8ba1\u6587\u4ef6\u76f4\u63a5\u5b58\u50a8\u5728\u4ee3\u7801\u5e93\u4e2d", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u4ee3\u7801\u5e93\u4e2d\u76f4\u63a5\u8fdb\u884c\u8bbe\u8ba1\u5de5\u4f5c\u7684\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u7684\u7d27\u5bc6\u96c6\u6210", "conclusion": "\u901a\u8fc7\u5c06\u8bbe\u8ba1\u5de5\u5177\u96c6\u6210\u5230\u4ee3\u7801\u73af\u5883\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u534f\u4f5c\u6548\u7387", "topic": "code agent"}}
