{"id": "2509.22908", "categories": ["cs.SE", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22908", "abs": "https://arxiv.org/abs/2509.22908", "authors": ["Sergiu Bursuc", "Theodore Ehrenborg", "Shaowei Lin", "Lacramioara Astefanoaei", "Ionel Emilian Chiosa", "Jure Kukovec", "Alok Singh", "Oliver Butterley", "Adem Bizid", "Quinn Dougherty", "Miranda Zhao", "Max Tan", "Max Tegmark"], "title": "A benchmark for vericoding: formally verified program synthesis", "comment": "25 pages, 1 figure; data available at\n  https://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "summary": "We present and test the largest benchmark for vericoding, LLM-generation of\nformally verified code from formal specifications - in contrast to vibe coding,\nwhich generates potentially buggy code from a natural language description. Our\nbenchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in\nVerus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find\nvericoding success rates of 27% in Lean, 44% in Verus/Rust and 82% in Dafny\nusing off-the-shelf LLMs. Adding natural-language descriptions does not\nsignificantly improve performance. We also find that LLM progress has improved\nprogress on pure Dafny verification from 68% to 96% over the past year. The\nbenchmark and vericoding results are shared at\nhttps://github.com/Beneficial-AI-Foundation/vericoding-benchmark", "AI": {"tldr": "\u63d0\u51fa\u4e86\u76ee\u524d\u6700\u5927\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u57fa\u51c6\uff0c\u5305\u542b12,504\u4e2a\u5f62\u5f0f\u5316\u89c4\u8303\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u8bed\u8a00(Dafny\u3001Verus/Rust\u3001Lean)\u7684\u9a8c\u8bc1\u7f16\u7801\u6210\u529f\u7387\uff0c\u53d1\u73b0Dafny\u8868\u73b0\u6700\u4f73(82%)\uff0c\u4e14LLM\u5728Dafny\u9a8c\u8bc1\u65b9\u9762\u7684\u8fdb\u6b65\u663e\u8457\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u53ef\u80fd\u4ea7\u751f\u6709\u7f3a\u9677\u4ee3\u7801\u7684\u95ee\u9898\uff0c\u7814\u7a76\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210(vericoding)\u7684\u80fd\u529b\uff0c\u5efa\u7acb\u5927\u89c4\u6a21\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u751f\u6210\u7ecf\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u4ee3\u7801\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u521b\u5efa\u5305\u542b12,504\u4e2a\u5f62\u5f0f\u5316\u89c4\u8303\u7684\u57fa\u51c6\uff0c\u6db5\u76d6Dafny\u3001Verus/Rust\u548cLean\u4e09\u79cd\u8bed\u8a00\uff0c\u5176\u4e2d6,174\u4e2a\u662f\u65b0\u95ee\u9898\u3002\u4f7f\u7528\u73b0\u6210\u7684LLM\u6d4b\u8bd5\u9a8c\u8bc1\u7f16\u7801\u6210\u529f\u7387\uff0c\u5e76\u6bd4\u8f83\u6dfb\u52a0\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u9a8c\u8bc1\u7f16\u7801\u6210\u529f\u7387\u5206\u522b\u4e3a\uff1aLean 27%\u3001Verus/Rust 44%\u3001Dafny 82%\u3002\u6dfb\u52a0\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6ca1\u6709\u663e\u8457\u6539\u5584\u6027\u80fd\u3002LLM\u5728\u7eafDafny\u9a8c\u8bc1\u65b9\u9762\u7684\u8868\u73b0\u4ece68%\u63d0\u5347\u523096%\u3002", "conclusion": "\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u662f\u53ef\u884c\u7684\uff0cDafny\u8bed\u8a00\u8868\u73b0\u6700\u4f73\uff0cLLM\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u4e3a\u53ef\u9760\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "topic": "swe benchmark"}}
{"id": "2509.22819", "categories": ["cs.AI", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22819", "abs": "https://arxiv.org/abs/2509.22819", "authors": ["Sumanth Varambally", "Thomas Voice", "Yanchao Sun", "Zhifeng Chen", "Rose Yu", "Ke Ye"], "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning", "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning\nabilities, but their solutions frequently contain errors that cannot be\nautomatically verified. Formal theorem proving systems such as Lean 4 offer\nautomated verification with complete accuracy, motivating recent efforts to\nbuild specialized prover LLMs that generate verifiable proofs in formal\nlanguages. However, a significant gap remains: current prover LLMs solve\nsubstantially fewer problems than general-purpose LLMs operating in natural\nlanguage. We introduce Hilbert, an agentic framework that bridges this gap by\ncombining the complementary strengths of informal reasoning and formal\nverification. Our system orchestrates four components: an informal LLM that\nexcels at mathematical reasoning, a specialized prover LLM optimized for Lean 4\ntactics, a formal verifier, and a semantic theorem retriever. Given a problem\nthat the prover is unable to solve, Hilbert employs recursive decomposition to\nsplit the problem into subgoals that it solves with the prover or reasoner LLM.\nIt leverages verifier feedback to refine incorrect proofs as necessary.\nExperimental results demonstrate that Hilbert substantially outperforms\nexisting approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points\nabove the best publicly available method. Hilbert achieves the best known\nresult on PutnamBench. It solves 462/660 problems (70.0%), outperforming\nproprietary approaches like SeedProver (50.4%) and achieving a 422% improvement\nover the best publicly available baseline. Thus, Hilbert effectively narrows\nthe gap between informal reasoning and formal proof generation.", "AI": {"tldr": "Hilbert\u662f\u4e00\u4e2a\u7ed3\u5408\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7684\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u63a8\u7406LLM\u3001\u8bc1\u660eLLM\u3001\u9a8c\u8bc1\u5668\u548c\u5b9a\u7406\u68c0\u7d22\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u95ee\u9898\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc1\u660eLLM\u5728\u5f62\u5f0f\u5316\u8bed\u8a00\u4e2d\u89e3\u51b3\u7684\u95ee\u9898\u8fdc\u5c11\u4e8e\u901a\u7528LLM\u5728\u81ea\u7136\u8bed\u8a00\u4e2d\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f25\u5408\u975e\u6b63\u5f0f\u63a8\u7406\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u9012\u5f52\u5206\u89e3\u7b56\u7565\uff0c\u5c06\u95ee\u9898\u62c6\u5206\u4e3a\u5b50\u76ee\u6807\uff0c\u4f7f\u7528\u63a8\u7406LLM\u6216\u8bc1\u660eLLM\u89e3\u51b3\uff0c\u5e76\u5229\u7528\u9a8c\u8bc1\u5668\u53cd\u9988\u4fee\u6b63\u9519\u8bef\u8bc1\u660e\u3002", "result": "\u5728miniF2F\u4e0a\u8fbe\u523099.2%\uff0c\u6bd4\u6700\u4f73\u516c\u5f00\u65b9\u6cd5\u9ad86.6\u4e2a\u767e\u5206\u70b9\uff1b\u5728PutnamBench\u4e0a\u89e3\u51b370.0%\u7684\u95ee\u9898\uff0c\u6bd4SeedProver(50.4%)\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Hilbert\u6709\u6548\u7f29\u5c0f\u4e86\u975e\u6b63\u5f0f\u63a8\u7406\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agent analysis"}}
{"id": "2509.23261", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23261", "abs": "https://arxiv.org/abs/2509.23261", "authors": ["Fei Gu", "Zi Liang", "Hongzong LI", "Jiahao MA"], "title": "The Matthew Effect of AI Programming Assistants: A Hidden Bias in Software Evolution", "comment": null, "summary": "AI-assisted programming is rapidly reshaping software development, with large\nlanguage models (LLMs) enabling new paradigms such as vibe coding and agentic\ncoding. While prior works have focused on prompt design and code generation\nquality, the broader impact of LLM-driven development on the iterative dynamics\nof software engineering remains underexplored. In this paper, we conduct\nlarge-scale experiments on thousands of algorithmic programming tasks and\nhundreds of framework selection tasks to systematically investigate how\nAI-assisted programming interacts with the software ecosystem. Our analysis\nreveals \\textbf{a striking Matthew effect: the more popular a programming\nlanguage or framework, the higher the success rate of LLM-generated code}. The\nphenomenon suggests that AI systems may reinforce existing popularity\nhierarchies, accelerating convergence around dominant tools while hindering\ndiversity and innovation. We provide a quantitative characterization of this\neffect and discuss its implications for the future evolution of programming\necosystems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u8f85\u52a9\u7f16\u7a0b\u5b58\u5728\u9a6c\u592a\u6548\u5e94\uff1a\u7f16\u7a0b\u8bed\u8a00\u6216\u6846\u67b6\u8d8a\u6d41\u884c\uff0cLLM\u751f\u6210\u4ee3\u7801\u7684\u6210\u529f\u7387\u8d8a\u9ad8\uff0c\u53ef\u80fd\u5f3a\u5316\u73b0\u6709\u5de5\u5177\u751f\u6001\u7684\u96c6\u4e2d\u5316\u8d8b\u52bf", "motivation": "\u63a2\u7d22AI\u8f85\u52a9\u7f16\u7a0b\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u8fed\u4ee3\u52a8\u6001\u7684\u5e7f\u6cdb\u5f71\u54cd\uff0c\u7279\u522b\u662fLLM\u9a71\u52a8\u5f00\u53d1\u5982\u4f55\u4e0e\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u76f8\u4e92\u4f5c\u7528", "method": "\u5728\u6570\u5343\u4e2a\u7b97\u6cd5\u7f16\u7a0b\u4efb\u52a1\u548c\u6570\u767e\u4e2a\u6846\u67b6\u9009\u62e9\u4efb\u52a1\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u7814\u7a76AI\u8f85\u52a9\u7f16\u7a0b\u4e0e\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u4ea4\u4e92", "result": "\u53d1\u73b0\u660e\u663e\u7684\u9a6c\u592a\u6548\u5e94\uff1a\u7f16\u7a0b\u8bed\u8a00\u6216\u6846\u67b6\u8d8a\u6d41\u884c\uff0cLLM\u751f\u6210\u4ee3\u7801\u7684\u6210\u529f\u7387\u8d8a\u9ad8", "conclusion": "AI\u7cfb\u7edf\u53ef\u80fd\u5f3a\u5316\u73b0\u6709\u7684\u6d41\u884c\u5ea6\u5c42\u7ea7\uff0c\u52a0\u901f\u5411\u4e3b\u5bfc\u5de5\u5177\u6536\u655b\uff0c\u540c\u65f6\u963b\u788d\u591a\u6837\u6027\u548c\u521b\u65b0", "topic": "agent analysis"}}
{"id": "2509.22715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22715", "abs": "https://arxiv.org/abs/2509.22715", "authors": ["Jiho Park", "Jongyoon Song", "Minjin Choi", "Kyuho Heo", "Taehun Huh", "Ji Won Kim"], "title": "TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) are increasingly integral as productivity\nassistants, but existing benchmarks fall short in rigorously evaluating their\nreal-world instruction-following capabilities. Current benchmarks often (i)\nlack sufficient multilinguality, (ii) fail to capture the implicit constraints\ninherent in user requests, and (iii) overlook the complexities of multi-turn\ndialogue. To address these critical gaps and provide a more realistic\nassessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation\nBenchmark)1, a novel benchmark specifically designed for LLM-based productivity\nassistants. TRUEBench distinguishes itself by featuring input prompts across 12\nlanguages, incorporating intra-instance multilingual instructions, employing\nrigorous evaluation criteria to capture both explicit and implicit constraints,\nand including complex multi-turn dialogue scenarios with both accumulating\nconstraints and context switches. Furthermore, to ensure reliability in\nevaluation, we refined constraints using an LLM validator. Extensive\nexperiments demonstrate that TRUEBench presents significantly greater\nchallenges than existing benchmarks; for instance, a strong model like OpenAI\no1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and\nrealistic assessment of LLMs in practical productivity settings, highlighting\ntheir capabilities and limitations.", "AI": {"tldr": "TRUEBench\u662f\u4e00\u4e2a\u4e13\u4e3aLLM\u751f\u4ea7\u529b\u52a9\u624b\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u652f\u6301\u3001\u9690\u5f0f\u7ea6\u675f\u6355\u83b7\u548c\u591a\u8f6e\u5bf9\u8bdd\u590d\u6742\u6027\u6765\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30LLM\u771f\u5b9e\u4e16\u754c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5305\u62ec\u7f3a\u4e4f\u591a\u8bed\u8a00\u6027\u3001\u672a\u80fd\u6355\u83b7\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u9690\u5f0f\u7ea6\u675f\uff0c\u4ee5\u53ca\u5ffd\u89c6\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165TRUEBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b12\u79cd\u8bed\u8a00\u7684\u8f93\u5165\u63d0\u793a\u3001\u591a\u8bed\u8a00\u6307\u4ee4\u3001\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u6355\u83b7\u663e\u5f0f\u548c\u9690\u5f0f\u7ea6\u675f\uff0c\u4ee5\u53ca\u5177\u6709\u7d2f\u79ef\u7ea6\u675f\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u3002\u4f7f\u7528LLM\u9a8c\u8bc1\u5668\u6765\u786e\u4fdd\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTRUEBench\u6bd4\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u66f4\u5177\u6311\u6218\u6027\uff0c\u5373\u4f7f\u662f\u50cfOpenAI o1\u8fd9\u6837\u7684\u5f3a\u5927\u6a21\u578b\u4e5f\u4ec5\u8fbe\u523069.07%\u7684\u603b\u4f53\u901a\u8fc7\u7387\u3002", "conclusion": "TRUEBench\u63d0\u4f9b\u4e86\u5bf9LLM\u5728\u5b9e\u9645\u751f\u4ea7\u529b\u73af\u5883\u4e2d\u80fd\u529b\u548c\u5c40\u9650\u6027\u7684\u4e25\u683c\u4e14\u771f\u5b9e\u7684\u8bc4\u4f30\u3002", "topic": "swe benchmark"}}
{"id": "2509.22888", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22888", "abs": "https://arxiv.org/abs/2509.22888", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tiffany Zhan", "Saptarshi Ghosh", "Linfeng Liu", "Tianyu Jiang"], "title": "JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory", "comment": "22 pages, 10 figures, 5 tables", "summary": "Standard LLM evaluation practices compress diverse abilities into single\nscores, obscuring their inherently multidimensional nature. We present JE-IRT,\na geometric item-response framework that embeds both LLMs and questions in a\nshared space. For question embeddings, the direction encodes semantics and the\nnorm encodes difficulty, while correctness on each question is determined by\nthe geometric interaction between the model and question embeddings. This\ngeometry replaces a global ranking of LLMs with topical specialization and\nenables smooth variation across related questions. Building on this framework,\nour experimental results reveal that out-of-distribution behavior can be\nexplained through directional alignment, and that larger norms consistently\nindicate harder questions. Moreover, JE-IRT naturally supports generalization:\nonce the space is learned, new LLMs are added by fitting a single embedding.\nThe learned space further reveals an LLM-internal taxonomy that only partially\naligns with human-defined subject categories. JE-IRT thus establishes a unified\nand interpretable geometric lens that connects LLM abilities with the structure\nof questions, offering a distinctive perspective on model evaluation and\ngeneralization.", "AI": {"tldr": "JE-IRT\u662f\u4e00\u4e2a\u51e0\u4f55\u9879\u76ee\u54cd\u5e94\u6846\u67b6\uff0c\u5c06LLM\u548c\u95ee\u9898\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\uff0c\u7528\u51e0\u4f55\u4ea4\u4e92\u4ee3\u66ff\u5168\u5c40\u6392\u540d\uff0c\u63ed\u793a\u6a21\u578b\u7684\u4e13\u4e1a\u5316\u80fd\u529b\u548c\u95ee\u9898\u7ed3\u6784\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edfLLM\u8bc4\u4f30\u65b9\u6cd5\u5c06\u591a\u7ef4\u80fd\u529b\u538b\u7f29\u4e3a\u5355\u4e00\u5206\u6570\uff0c\u63a9\u76d6\u4e86\u5176\u672c\u8d28\u4e0a\u7684\u591a\u7ef4\u5ea6\u7279\u6027\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faJE-IRT\u6846\u67b6\uff0c\u5c06LLM\u548c\u95ee\u9898\u5d4c\u5165\u5171\u4eab\u51e0\u4f55\u7a7a\u95f4\uff0c\u95ee\u9898\u5d4c\u5165\u7684\u65b9\u5411\u7f16\u7801\u8bed\u4e49\u3001\u8303\u6570\u7f16\u7801\u96be\u5ea6\uff0c\u6b63\u786e\u6027\u7531\u6a21\u578b\u4e0e\u95ee\u9898\u7684\u51e0\u4f55\u4ea4\u4e92\u51b3\u5b9a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u5206\u5e03\u5916\u884c\u4e3a\u53ef\u901a\u8fc7\u65b9\u5411\u5bf9\u9f50\u89e3\u91ca\uff1b\u8f83\u5927\u8303\u6570\u5bf9\u5e94\u66f4\u96be\u95ee\u9898\uff1b\u65b0LLM\u53ea\u9700\u62df\u5408\u5355\u4e2a\u5d4c\u5165\u5373\u53ef\u52a0\u5165\uff1b\u53d1\u73b0\u4e0e\u4eba\u7c7b\u5b9a\u4e49\u7c7b\u522b\u90e8\u5206\u5bf9\u9f50\u7684LLM\u5185\u90e8\u5206\u7c7b\u6cd5\u3002", "conclusion": "JE-IRT\u5efa\u7acb\u4e86\u7edf\u4e00\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u89c6\u89d2\uff0c\u8fde\u63a5LLM\u80fd\u529b\u4e0e\u95ee\u9898\u7ed3\u6784\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u548c\u6cdb\u5316\u63d0\u4f9b\u4e86\u72ec\u7279\u89c6\u89d2\u3002", "topic": "agent analysis"}}
{"id": "2509.22984", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22984", "abs": "https://arxiv.org/abs/2509.22984", "authors": ["Yu Wu", "Shuo Wu", "Ye Tao", "Yansong Li", "Anand D. Sarwate"], "title": "Not only a helper, but also a teacher: Interactive LLM Cascade", "comment": "29 pages, 4 figures, under review", "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger\nmodels often having better performance but higher cost: choosing an LLM model\noften involves trading off performance and cost. The LLM Cascade is a paradigm\nthat defers difficult queries from weak/cheap to strong/expensive models. This\napproach is nonadaptive: the deferral decision is trained offline. When\nconfronted with similar or repeated queries, the LLM Cascade may then\nrepeatedly consult the expensive model and incur higher cost. To improve the\ncascading efficiency, we propose Inter-Cascade, an online and interactive LLM\nCascade that extends the role of strong model from a backup helper to a\nlong-term teacher. In our system, when a strong model resolves a difficult\nquery, it also distills its solution into a generalized, reusable\nproblem-solving strategy that boosts the weak model on subsequent queries.\nAdding strategies to queries enables the weak model to dynamically improve its\nperformance over time, avoiding computationally and time-intensive fine-tuning.\nEmpirically, compared with standard LLM Cascade baselines across multiple\nbenchmarks, the Inter-Cascade significantly improves the accuracy of the weak\nmodel (by up to 33.06 absolute percentage points) and the overall system (by up\nto 5.53 absolute percentage points), while reducing the calls to strong models\n(by up to 48.05% relative reduction) and saving the corresponding fees (by up\nto 49.63% relative reduction). Inter-Cascade demonstrates the effective\nin-context knowledge transfer between LLMs, and provides a general, scalable\nframework applicable to both open-source and API-based LLMs.", "AI": {"tldr": "Inter-Cascade\u662f\u4e00\u79cd\u5728\u7ebf\u4ea4\u4e92\u5f0fLLM\u7ea7\u8054\u7cfb\u7edf\uff0c\u5c06\u5f3a\u5927\u6a21\u578b\u4ece\u5907\u7528\u52a9\u624b\u6269\u5c55\u4e3a\u957f\u671f\u6559\u5e08\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u5f31\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35\u6a21\u578b\u7684\u8c03\u7528\u3002", "motivation": "\u4f20\u7edfLLM\u7ea7\u8054\u65b9\u6cd5\u662f\u975e\u81ea\u9002\u5e94\u7684\uff0c\u9047\u5230\u76f8\u4f3c\u6216\u91cd\u590d\u67e5\u8be2\u65f6\u4f1a\u91cd\u590d\u8c03\u7528\u6602\u8d35\u6a21\u578b\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u3002\u9700\u8981\u4e00\u79cd\u5728\u7ebf\u4ea4\u4e92\u5f0f\u65b9\u6cd5\u6765\u63d0\u9ad8\u7ea7\u8054\u6548\u7387\u3002", "method": "\u5f53\u5f3a\u6a21\u578b\u89e3\u51b3\u56f0\u96be\u67e5\u8be2\u65f6\uff0c\u5c06\u5176\u89e3\u51b3\u65b9\u6848\u84b8\u998f\u4e3a\u53ef\u91cd\u7528\u7684\u901a\u7528\u95ee\u9898\u89e3\u51b3\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u968f\u540e\u7528\u4e8e\u589e\u5f3a\u5f31\u6a21\u578b\u5904\u7406\u540e\u7eed\u67e5\u8be2\u7684\u80fd\u529b\u3002", "result": "\u76f8\u6bd4\u6807\u51c6LLM\u7ea7\u8054\u57fa\u7ebf\uff0cInter-Cascade\u663e\u8457\u63d0\u5347\u5f31\u6a21\u578b\u51c6\u786e\u7387\uff08\u6700\u9ad833.06\u4e2a\u767e\u5206\u70b9\uff09\u548c\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\uff08\u6700\u9ad85.53\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u540c\u65f6\u51cf\u5c11\u5f3a\u6a21\u578b\u8c03\u7528\uff08\u76f8\u5bf9\u51cf\u5c1148.05%\uff09\u548c\u76f8\u5e94\u8d39\u7528\uff08\u76f8\u5bf9\u51cf\u5c1149.63%\uff09\u3002", "conclusion": "Inter-Cascade\u5c55\u793a\u4e86LLM\u4e4b\u95f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u4f20\u9012\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u5f00\u6e90\u548cAPI-based LLM\u7684\u901a\u7528\u3001\u53ef\u6269\u5c55\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2509.23586", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23586", "abs": "https://arxiv.org/abs/2509.23586", "authors": ["Yuan-An Xiao", "Pengfei Gao", "Chao Peng", "Yingfei Xiong"], "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction", "comment": "20 pages, 4 figures", "summary": "Multi-turn agent systems based on Large Language Models (LLMs) have been\nincreasingly popular for software engineering tasks. While LLM agents show\ndecent effectiveness, the high computational cost of input tokens due to the\never-growing trajectory remains an efficiency concern for their applications.\nEfficiency is largely neglected in existing studies and agent products, and\nthis paper fills the gap by introducing an inference-time trajectory reduction\napproach to reduce the cost of agents.\n  Through analyzing existing agent trajectories, we demonstrate that useless,\nredundant, and expired information is widespread in all trajectories, which can\nbe identified and reduced without harming the agent's performance. We then\ndesign a simple yet effective trajectory reduction approach, AgentDiet, which\nautomatically removes such waste information. We implement AgentDiet on a\ntop-performing coding agent, and the evaluation on two LLMs and two benchmarks\nshows that AgentDiet can reduce input tokens by 39.9% ~ 59.7%, or the final\ncomputational cost by 21.1% ~ 35.9%, while maintaining the same agent\nperformance. This indicates that trajectory reduction is a promising direction\nfor agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentDiet\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u79fb\u9664\u4ee3\u7406\u8f68\u8ff9\u4e2d\u7684\u65e0\u7528\u3001\u5197\u4f59\u548c\u8fc7\u671f\u4fe1\u606f\uff0c\u663e\u8457\u51cf\u5c11\u8f93\u5165token\u6570\u91cf39.9%~59.7%\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c21.1%~35.9%\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7406\u6027\u80fd\u4e0d\u53d8\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u4ee3\u7406\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u4f46\u4e0d\u65ad\u589e\u957f\u7684\u8f68\u8ff9\u5bfc\u81f4\u8f93\u5165token\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u7814\u7a76\u548c\u4ea7\u54c1\u666e\u904d\u5ffd\u89c6\u6548\u7387\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86AgentDiet\u8f68\u8ff9\u7f29\u51cf\u65b9\u6cd5\uff0c\u81ea\u52a8\u8bc6\u522b\u548c\u79fb\u9664\u4ee3\u7406\u8f68\u8ff9\u4e2d\u7684\u65e0\u7528\u3001\u5197\u4f59\u548c\u8fc7\u671f\u4fe1\u606f\u3002", "result": "\u5728\u4e24\u4e2aLLM\u548c\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAgentDiet\u80fd\u51cf\u5c11\u8f93\u5165token 39.9%~59.7%\uff0c\u964d\u4f4e\u6700\u7ec8\u8ba1\u7b97\u6210\u672c21.1%~35.9%\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7406\u6027\u80fd\u4e0d\u53d8\u3002", "conclusion": "\u8f68\u8ff9\u7f29\u51cf\u662f\u4ee3\u7406\u7cfb\u7edf\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u80fd\u663e\u8457\u63d0\u5347\u6548\u7387\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.23675", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23675", "abs": "https://arxiv.org/abs/2509.23675", "authors": ["Xinyue Zuo", "Yifan Zhang", "Hongshu Wang", "Yufan Cai", "Zhe Hou", "Jing Sun", "Jin Song Dong"], "title": "PAT-Agent: Autoformalization for Model Checking", "comment": "Accepted in ASE 2025 (International Conference on Automated Software\n  Engineering)", "summary": "Recent advances in large language models (LLMs) offer promising potential for\nautomating formal methods. However, applying them to formal verification\nremains challenging due to the complexity of specification languages, the risk\nof hallucinated output, and the semantic gap between natural language and\nformal logic. We introduce PAT-Agent, an end-to-end framework for natural\nlanguage autoformalization and formal model repair that combines the generative\ncapabilities of LLMs with the rigor of formal verification to automate the\nconstruction of verifiable formal models. In PAT-Agent, a Planning LLM first\nextracts key modeling elements and generates a detailed plan using semantic\nprompts, which then guides a Code Generation LLM to synthesize syntactically\ncorrect and semantically faithful formal models. The resulting code is verified\nusing the Process Analysis Toolkit (PAT) model checker against user-specified\nproperties, and when discrepancies occur, a Repair Loop is triggered to\niteratively correct the model using counterexamples. To improve flexibility, we\nbuilt a web-based interface that enables users, particularly non-FM-experts, to\ndescribe, customize, and verify system behaviors through user-LLM interactions.\nExperimental results on 40 systems show that PAT-Agent consistently outperforms\nbaselines, achieving high verification success with superior efficiency. The\nablation studies confirm the importance of both planning and repair components,\nand the user study demonstrates that our interface is accessible and supports\neffective formal modeling, even for users with limited formal methods\nexperience.", "AI": {"tldr": "PAT-Agent\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7ed3\u5408LLM\u7684\u751f\u6210\u80fd\u529b\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7684\u4e25\u8c28\u6027\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u81ea\u52a8\u5f62\u5f0f\u5316\u548c\u5f62\u5f0f\u6a21\u578b\u4fee\u590d\uff0c\u81ea\u52a8\u5316\u6784\u5efa\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u6a21\u578b\u3002", "motivation": "\u5e94\u7528LLM\u5230\u5f62\u5f0f\u9a8c\u8bc1\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u89c4\u8303\u8bed\u8a00\u590d\u6742\u6027\u3001\u5e7b\u89c9\u8f93\u51fa\u98ce\u9669\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u4e0e\u5f62\u5f0f\u903b\u8f91\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u4f7f\u7528\u89c4\u5212LLM\u63d0\u53d6\u5173\u952e\u5efa\u6a21\u5143\u7d20\u5e76\u751f\u6210\u8be6\u7ec6\u8ba1\u5212\uff0c\u6307\u5bfc\u4ee3\u7801\u751f\u6210LLM\u5408\u6210\u8bed\u6cd5\u6b63\u786e\u4e14\u8bed\u4e49\u5fe0\u5b9e\u7684\u5f62\u5f0f\u6a21\u578b\uff0c\u901a\u8fc7PAT\u6a21\u578b\u68c0\u67e5\u5668\u9a8c\u8bc1\uff0c\u5e76\u5728\u51fa\u73b0\u5dee\u5f02\u65f6\u89e6\u53d1\u4fee\u590d\u5faa\u73af\u8fed\u4ee3\u4fee\u6b63\u6a21\u578b\u3002", "result": "\u572840\u4e2a\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPAT-Agent\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9a8c\u8bc1\u6210\u529f\u7387\u548c\u4f18\u8d8a\u6548\u7387\u3002\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u89c4\u5212\u548c\u4fee\u590d\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "PAT-Agent\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5f62\u5f0f\u9a8c\u8bc1\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u8be5\u754c\u9762\u6613\u4e8e\u8bbf\u95ee\uff0c\u652f\u6301\u6709\u6548\u7684\u5f62\u5f0f\u5efa\u6a21\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5f62\u5f0f\u65b9\u6cd5\u7ecf\u9a8c\u6709\u9650\u7684\u7528\u6237\u4e5f\u662f\u5982\u6b64\u3002", "topic": "code agent"}}
{"id": "2509.22921", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22921", "abs": "https://arxiv.org/abs/2509.22921", "authors": ["Matthieu Zimmer", "Xiaotong Ji", "Tu Nguyen", "Haitham Bou Ammar"], "title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective", "comment": null, "summary": "We introduce a novel approach to large language model (LLM) distillation by\nformulating it as a constrained reinforcement learning problem. While recent\nwork has begun exploring the integration of task-specific rewards into\ndistillation processes, existing methods typically rely on ad-hoc reward\nweighting. We propose a principled optimization framework that maximizes\ntask-specific rewards while constraining the divergence from the teacher model\nto remain below a specified threshold. Our approach adapts constrained state\naugmented reinforcement learning to the distillation setting, introducing a\nmodified reward function that maintains theoretical guarantees of constraint\nsatisfaction without requiring state augmentation or teacher model access\nduring deployment and without the computational overhead of the dual Lagrangian\nmethods. Through extensive experiments on mathematical reasoning tasks, we\ndemonstrate that our method achieves better constraint satisfaction rates and\nbetter reasoning compared to the soft Lagrangian relaxation baselines while\nmaintaining competitive task performance. Our framework provides a\ntheoretically grounded and practically efficient solution for reward-aware\ndistillation in resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u5927\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u6700\u5927\u5316\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u7684\u540c\u65f6\u7ea6\u675f\u4e0e\u6559\u5e08\u6a21\u578b\u7684\u5dee\u5f02\u4e0d\u8d85\u8fc7\u6307\u5b9a\u9608\u503c\uff0c\u65e0\u9700\u72b6\u6001\u589e\u5f3a\u6216\u6559\u5e08\u6a21\u578b\u8bbf\u95ee\u5373\u53ef\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u3002", "motivation": "\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e34\u65f6\u5956\u52b1\u52a0\u6743\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u53c8\u80fd\u63a7\u5236\u4e0e\u6559\u5e08\u6a21\u578b\u5dee\u5f02\u7684\u84b8\u998f\u6846\u67b6\u3002", "method": "\u5c06LLM\u84b8\u998f\u6784\u5efa\u4e3a\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u91c7\u7528\u6539\u8fdb\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u6700\u5927\u5316\u4efb\u52a1\u5956\u52b1\u540c\u65f6\u7ea6\u675f\u4e0e\u6559\u5e08\u6a21\u578b\u7684KL\u6563\u5ea6\u4e0d\u8d85\u8fc7\u9608\u503c\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u8f6f\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u57fa\u7ebf\u5177\u6709\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u7387\u548c\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5956\u52b1\u611f\u77e5\u84b8\u998f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22768", "abs": "https://arxiv.org/abs/2509.22768", "authors": ["Ekaterina Trofimova", "Zosia Shamina", "Maria Selifanova", "Artem Zaitsev", "Remi Savchuk", "Maxim Minets", "Daria Ozerova", "Emil Sataev", "Denis Zuenko", "Andrey E. Ustyuzhanin"], "title": "ML2B: Multi-Lingual ML Benchmark For AutoML", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.", "AI": {"tldr": "\u63d0\u51fa\u4e86ML2B\uff0c\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u673a\u5668\u5b66\u4e60\u4ee3\u7801\u751f\u6210\u7684\u57fa\u51c6\uff0c\u5305\u542b30\u4e2aKaggle\u7ade\u8d5b\u7ffb\u8bd1\u621013\u79cd\u8bed\u8a00\uff0c\u7ed3\u679c\u663e\u793a\u975e\u82f1\u8bed\u4efb\u52a1\u6027\u80fd\u4e0b\u964d15-45%\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e3b\u8981\u5c40\u9650\u4e8e\u82f1\u8bed\uff0c\u5ffd\u89c6\u4e86\u673a\u5668\u5b66\u4e60\u7814\u7a76\u548c\u5b9e\u8df5\u7684\u5168\u7403\u6027\u548c\u591a\u8bed\u8a00\u7279\u6027\u3002", "method": "\u6784\u5efaML2B\u57fa\u51c6\uff0c\u5305\u542b30\u4e2aKaggle\u7ade\u8d5b\u7ffb\u8bd1\u621013\u79cd\u81ea\u7136\u8bed\u8a00\uff0c\u6db5\u76d6\u8868\u683c\u3001\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u7c7b\u578b\uff0c\u4f7f\u7528AIDE\u81ea\u52a8\u5316\u6846\u67b6\u8fdb\u884c\u7aef\u5230\u7aef\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5728\u975e\u82f1\u8bed\u4efb\u52a1\u4e0a\u5b58\u5728\u663e\u8457\u768415-45%\u6027\u80fd\u4e0b\u964d\uff0c\u7a81\u663e\u4e86\u591a\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u591a\u8bed\u8a00\u673a\u5668\u5b66\u4e60\u4ee3\u7801\u751f\u6210\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u591a\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0cML2B\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "topic": "swe benchmark"}}
{"id": "2509.23006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23006", "abs": "https://arxiv.org/abs/2509.23006", "authors": ["Hassen Dhrif"], "title": "Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems", "comment": null, "summary": "Agentic AI represents a paradigm shift in enhancing the capabilities of\ngenerative AI models. While these systems demonstrate immense potential and\npower, current evaluation techniques primarily focus on assessing their\nefficacy in identifying appropriate agents, tools, and parameters. However, a\ncritical gap exists in evaluating the alignment between an Agentic AI system's\ntasks and its overarching goals. This paper introduces the Creative Adversarial\nTesting (CAT) framework, a novel approach designed to capture and analyze the\ncomplex relationship between Agentic AI tasks and the system's intended\nobjectives.\n  We validate the CAT framework through extensive simulation using synthetic\ninteraction data modeled after Alexa+ audio services, a sophisticated Agentic\nAI system that shapes the user experience for millions of users globally. This\nsynthetic data approach enables comprehensive testing of edge cases and failure\nmodes while protecting user privacy. Our results demonstrate that the CAT\nframework provides unprecedented insights into goal-task alignment, enabling\nmore effective optimization and development of Agentic AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u521b\u610f\u5bf9\u6297\u6d4b\u8bd5(CAT)\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30Agentic AI\u7cfb\u7edf\u4e2d\u4efb\u52a1\u4e0e\u76ee\u6807\u7684\u5bf9\u9f50\u5173\u7cfb\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dAgentic AI\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4ee3\u7406\u3001\u5de5\u5177\u548c\u53c2\u6570\u7684\u9009\u62e9\u6709\u6548\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u4e0e\u76ee\u6807\u5bf9\u9f50\u5173\u7cfb\u7684\u8bc4\u4f30\uff0c\u5b58\u5728\u5173\u952e\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u521b\u610f\u5bf9\u6297\u6d4b\u8bd5(CAT)\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8eAlexa+\u97f3\u9891\u670d\u52a1\u7684\u5408\u6210\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u5e7f\u6cdb\u6a21\u62df\u6d4b\u8bd5\u3002", "result": "CAT\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u76ee\u6807-\u4efb\u52a1\u5bf9\u9f50\u6d1e\u5bdf\uff0c\u4f7fAgentic AI\u7cfb\u7edf\u7684\u4f18\u5316\u548c\u5f00\u53d1\u66f4\u52a0\u6709\u6548\u3002", "conclusion": "CAT\u6846\u67b6\u586b\u8865\u4e86Agentic AI\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u4f18\u5316\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2509.22812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22812", "abs": "https://arxiv.org/abs/2509.22812", "authors": ["Kai Zhang", "Christopher Malon", "Lichao Sun", "Martin Renqiang Min"], "title": "EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation", "comment": null, "summary": "Radiology report generation requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. Although recent\ninnovations, particularly multimodal large language models (MLLMs), have shown\nimproved performance, their supervised fine-tuning (SFT) objective is not\nexplicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,\na mixed-policy reinforcement learning (RL) algorithm designed specifically to\noptimize the generation through clinically motivated rewards. EditGRPO\nintegrates on-policy exploration with off-policy guidance by injecting\nsentence-level detailed corrections during training rollouts. This mixed-policy\napproach addresses the exploration dilemma and sampling efficiency issues\ntypically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with\nsupervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO\nbaselines, achieving an average improvement of 3.4% in CheXbert, GREEN,\nRadgraph, and RATEScore metrics across four major chest X-ray report generation\ndatasets. Notably, EditGRPO also demonstrates superior out-of-domain\ngeneralization, with an average performance gain of 5.9% on unseen datasets.", "AI": {"tldr": "EditGRPO\u662f\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e34\u5e8a\u52a8\u673a\u5956\u52b1\u4f18\u5316\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u5c55\u73b0\u4f18\u8d8a\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u76d1\u7763\u5fae\u8c03\u76ee\u6807\u4e0e\u4e34\u5e8a\u6548\u679c\u672a\u660e\u786e\u5bf9\u9f50\uff0c\u9700\u8981\u4e13\u95e8\u4f18\u5316\u751f\u6210\u8d28\u91cf\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51faEditGRPO\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u5728\u7ebf\u7b56\u7565\u63a2\u7d22\u548c\u79bb\u7ebf\u7b56\u7565\u6307\u5bfc\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u53e5\u5b50\u7ea7\u8be6\u7ec6\u4fee\u6b63\uff0c\u89e3\u51b3RL\u4e2d\u7684\u63a2\u7d22\u56f0\u5883\u548c\u91c7\u6837\u6548\u7387\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u4e3b\u8981\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u6570\u636e\u96c6\u4e0a\uff0cEditGRPO\u76f8\u6bd4SFT\u548c\u666e\u901aGRPO\u57fa\u7ebf\u5e73\u5747\u63d0\u53473.4%\u7684\u6027\u80fd\u6307\u6807\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53475.9%\u3002", "conclusion": "EditGRPO\u80fd\u6709\u6548\u4f18\u5316\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u8d28\u91cf\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u4e34\u5e8a\u5bf9\u9f50\u6027\u548c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23045", "categories": ["cs.AI", "cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23045", "abs": "https://arxiv.org/abs/2509.23045", "authors": ["Zonghan Yang", "Shengjie Wang", "Kelin Fu", "Wenyang He", "Weimin Xiong", "Yibo Liu", "Yibo Miao", "Bofei Gao", "Yejie Wang", "Yingwei Ma", "Yanhao Li", "Yue Liu", "Zhenxing Hu", "Kaitai Zhang", "Shuyi Wang", "Huarong Chen", "Flood Sung", "Yang Liu", "Yang Gao", "Zhilin Yang", "Tianyu Liu"], "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents", "comment": "58 pages", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u65e0\u4ee3\u7406\u8bad\u7ec3\u83b7\u5f97\u7ed3\u6784\u5316\u6280\u80fd\u5148\u9a8c\uff0c\u80fd\u591f\u6865\u63a5\u5de5\u4f5c\u6d41\u548c\u4ee3\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u53ef\u8fc1\u79fb\u7684\u7f16\u7801\u4ee3\u7406\u3002Kimi-Dev\u5728SWE-bench Verified\u4e0a\u8fbe\u523060.4%\uff0c\u901a\u8fc7\u989d\u5916SFT\u9002\u5e94\u540e\uff0cSWE\u4ee3\u7406\u8fbe\u523048.6% pass@1\uff0c\u4e0eClaude 3.5 Sonnet\u76f8\u5f53\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684LLM\u5e94\u7528\u5b58\u5728SWE-Agent\u6846\u67b6\uff08\u591a\u8f6e\u4ea4\u4e92\uff09\u548c\u5de5\u4f5c\u6d41\u5f0fAgentless\u65b9\u6cd5\uff08\u5355\u8f6e\u53ef\u9a8c\u8bc1\u6b65\u9aa4\uff09\u4e24\u79cd\u8303\u5f0f\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e24\u79cd\u8303\u5f0f\u5e76\u975e\u4e92\u65a5\uff0c\u800c\u662f\u53ef\u4ee5\u76f8\u4e92\u8865\u5145\u3002", "method": "\u9996\u5148\u7b56\u5212\u4e86Agentless\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u5f00\u6e90SWE LLM Kimi-Dev\uff0c\u7136\u540e\u901a\u8fc7\u989d\u5916\u7684SFT\u9002\u5e94\u57285000\u4e2a\u516c\u5f00\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Kimi-Dev\u5728SWE-bench Verified\u4e0a\u8fbe\u523060.4%\uff0c\u662f\u5de5\u4f5c\u6d41\u65b9\u6cd5\u4e2d\u6700\u4f73\u8868\u73b0\u3002\u7ecf\u8fc7SFT\u9002\u5e94\u540e\uff0cSWE\u4ee3\u7406\u8fbe\u523048.6% pass@1\uff0c\u4e0eClaude 3.5 Sonnet\uff08241022\u7248\u672c\uff09\u76f8\u5f53\u3002", "conclusion": "\u4eceAgentless\u8bad\u7ec3\u4e2d\u83b7\u5f97\u7684\u7ed3\u6784\u5316\u6280\u80fd\u5148\u9a8c\uff08\u5305\u62ec\u5b9a\u4f4d\u3001\u4ee3\u7801\u7f16\u8f91\u548c\u81ea\u6211\u53cd\u601d\uff09\u80fd\u591f\u6865\u63a5\u5de5\u4f5c\u6d41\u548c\u4ee3\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u6709\u6548\u7684SWE\u4ee3\u7406\u9002\u5e94\u3002", "topic": "swe application"}}
{"id": "2509.23812", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23812", "abs": "https://arxiv.org/abs/2509.23812", "authors": ["Dianshu Liao", "Xin Yin", "Shidong Pan", "Chao Ni", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Navigating the Labyrinth: Path-Sensitive Unit Test Generation with Large Language Models", "comment": null, "summary": "Unit testing is essential for software quality assurance, yet writing and\nmaintaining tests remains time-consuming and error-prone. To address this\nchallenge, researchers have proposed various techniques for automating unit\ntest generation, including traditional heuristic-based methods and more recent\napproaches that leverage large language models (LLMs). However, these existing\napproaches are inherently path-insensitive because they rely on fixed\nheuristics or limited contextual information and fail to reason about deep\ncontrol-flow structures. As a result, they often struggle to achieve adequate\ncoverage, particularly for deep or complex execution paths. In this work, we\npresent a path-sensitive framework, JUnitGenie, to fill this gap by combining\ncode knowledge with the semantic capabilities of LLMs in guiding context-aware\nunit test generation. After extracting code knowledge from Java projects,\nJUnitGenie distills this knowledge into structured prompts to guide the\ngeneration of high-coverage unit tests. We evaluate JUnitGenie on 2,258 complex\nfocal methods from ten real-world Java projects. The results show that\nJUnitGenie generates valid tests and improves branch and line coverage by\n29.60% and 31.00% on average over both heuristic and LLM-based baselines. We\nfurther demonstrate that the generated test cases can uncover real-world bugs,\nwhich were later confirmed and fixed by developers.", "AI": {"tldr": "JUnitGenie\u662f\u4e00\u4e2a\u8def\u5f84\u654f\u611f\u7684\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7801\u77e5\u8bc6\u548cLLM\u7684\u8bed\u4e49\u80fd\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u652f\u548c\u884c\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8def\u5f84\u4e0d\u654f\u611f\u95ee\u9898\uff0c\u65e0\u6cd5\u5904\u7406\u6df1\u5ea6\u63a7\u5236\u6d41\u7ed3\u6784\uff0c\u5bfc\u81f4\u8986\u76d6\u7387\u4e0d\u8db3\u3002", "method": "\u4eceJava\u9879\u76ee\u4e2d\u63d0\u53d6\u4ee3\u7801\u77e5\u8bc6\uff0c\u5c06\u5176\u63d0\u70bc\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\u6765\u6307\u5bfcLLM\u751f\u6210\u9ad8\u8986\u76d6\u7387\u7684\u5355\u5143\u6d4b\u8bd5\u3002", "result": "\u572810\u4e2a\u771f\u5b9eJava\u9879\u76ee\u76842,258\u4e2a\u590d\u6742\u65b9\u6cd5\u4e0a\u8bc4\u4f30\uff0cJUnitGenie\u76f8\u6bd4\u542f\u53d1\u5f0f\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5206\u652f\u548c\u884c\u8986\u76d6\u7387\u5206\u522b\u5e73\u5747\u63d0\u9ad829.60%\u548c31.00%\uff0c\u5e76\u80fd\u53d1\u73b0\u771f\u5b9ebug\u3002", "conclusion": "JUnitGenie\u901a\u8fc7\u8def\u5f84\u654f\u611f\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u751f\u6210\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2509.22824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22824", "abs": "https://arxiv.org/abs/2509.22824", "authors": ["Chi Ruan", "Dongfu Jiang", "Yubo Wang", "Wenhu Chen"], "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$\nof the generated critique aligns with the ground-truth judgment $c^*$. Building\non this point, we introduce \\textsc{Critique-Coder}, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Critique Reinforcement Learning (CRL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u5bf9(\u95ee\u9898\uff0c\u89e3\u51b3\u65b9\u6848)\u5bf9\u751f\u6210\u6279\u5224\uff0c\u4ec5\u57fa\u4e8e\u6279\u5224\u7684\u6700\u7ec8\u5224\u65ad\u6807\u7b7e\u662f\u5426\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e00\u81f4\u6765\u7ed9\u4e88\u5956\u52b1\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86Critique-Coder\u6a21\u578b\uff0c\u5728\u6807\u51c6RL\u6570\u636e\u4e2d\u66ff\u636220%\u4e3aCRL\u6570\u636e\u8fdb\u884c\u6df7\u5408\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u7eafRL\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u54cd\u5e94\uff0c\u7f3a\u4e4f\u660e\u786e\u57f9\u517b\u6279\u5224\u6216\u53cd\u601d\u80fd\u529b\u7684\u673a\u5236\u3002\u53d7Critique-Fine-Tuning\u548cCritique-Guided-Distillation\u7b49\u7814\u7a76\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5f00\u53d1\u80fd\u660e\u786e\u6559\u6388LLMs\u5982\u4f55\u8fdb\u884c\u6279\u5224\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCRL\u65b9\u6cd5\uff0c\u6a21\u578b\u4efb\u52a1\u662f\u4e3a\u7ed9\u5b9a(\u95ee\u9898\uff0c\u89e3\u51b3\u65b9\u6848)\u5bf9\u751f\u6210\u6279\u5224\uff0c\u5956\u52b1\u4ec5\u57fa\u4e8e\u6279\u5224\u7684\u6700\u7ec8\u5224\u65ad\u6807\u7b7e\u662f\u5426\u4e0e\u771f\u5b9e\u6807\u7b7e\u4e00\u81f4\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1Critique-Coder\u6a21\u578b\uff0c\u4f7f\u7528RL\u548cCRL\u6df7\u5408\u8bad\u7ec3(20%\u6807\u51c6RL\u6570\u636e\u66ff\u6362\u4e3aCRL\u6570\u636e)\u3002", "result": "Critique-Coder\u5728\u6240\u6709\u8bc4\u4f30\u57fa\u51c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u7eafRL\u57fa\u7ebf\u6a21\u578b\u3002Critique-Coder-8B\u5728LiveCodeBench(v5)\u4e0a\u8fbe\u5230\u8d85\u8fc760%\uff0c\u4f18\u4e8eDeepCoder-14B\u548cGPT-o1\u7b49\u63a8\u7406\u6a21\u578b\u3002\u5728BBEH\u6570\u636e\u96c6\u7684\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u66f4\u597d\uff0c\u8868\u660eCRL\u5728\u7f16\u7801\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u589e\u5f3a\u4e86\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "CRL\u4f5c\u4e3a\u6807\u51c6RL\u7684\u5f88\u597d\u8865\u5145\uff0c\u80fd\u591f\u589e\u5f3aLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u8fd9\u79cd\u6279\u5224\u548c\u63a8\u7406\u80fd\u529b\u53ef\u8fc1\u79fb\u5230\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23058", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23058", "abs": "https://arxiv.org/abs/2509.23058", "authors": ["Yikai Wang", "Xiaocheng Li", "Guanting Chen"], "title": "Risk Profiling and Modulation for LLMs", "comment": null, "summary": "Large language models (LLMs) are increasingly used for decision-making tasks\nunder uncertainty; however, their risk profiles and how they are influenced by\nprompting and alignment methods remain underexplored. Existing studies have\nprimarily examined personality prompting or multi-agent interactions, leaving\nopen the question of how post-training influences the risk behavior of LLMs. In\nthis work, we propose a new pipeline for eliciting, steering, and modulating\nLLMs' risk profiles, drawing on tools from behavioral economics and finance.\nUsing utility-theoretic models, we compare pre-trained, instruction-tuned, and\nRLHF-aligned LLMs, and find that while instruction-tuned models exhibit\nbehaviors consistent with some standard utility formulations, pre-trained and\nRLHF-aligned models deviate more from any utility models fitted. We further\nevaluate modulation strategies, including prompt engineering, in-context\nlearning, and post-training, and show that post-training provides the most\nstable and effective modulation of risk preference. Our findings provide\ninsights into the risk profiles of different classes and stages of LLMs and\ndemonstrate how post-training modulates these profiles, laying the groundwork\nfor future research on behavioral alignment and risk-aware LLM design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4e2d\u7684\u98ce\u9669\u504f\u597d\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u3001\u5f15\u5bfc\u548c\u8c03\u8282LLM\u98ce\u9669\u914d\u7f6e\u6587\u4ef6\u7684\u7ba1\u9053\uff0c\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4e0e\u6807\u51c6\u6548\u7528\u6a21\u578b\u4e00\u81f4\uff0c\u800c\u9884\u8bad\u7ec3\u548cRLHF\u5bf9\u9f50\u6a21\u578b\u504f\u79bb\u66f4\u591a\uff0c\u540e\u8bad\u7ec3\u662f\u6700\u6709\u6548\u7684\u98ce\u9669\u504f\u597d\u8c03\u8282\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4e2d\u7684\u98ce\u9669\u914d\u7f6e\uff0c\u4ee5\u53ca\u63d0\u793a\u548c\u8bad\u7ec3\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u5176\u98ce\u9669\u884c\u4e3a\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e2a\u6027\u63d0\u793a\u6216\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u5bf9\u540e\u8bad\u7ec3\u5982\u4f55\u5f71\u54cdLLM\u98ce\u9669\u884c\u4e3a\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u7ba1\u9053\u6765\u5f15\u51fa\u3001\u5f15\u5bfc\u548c\u8c03\u8282LLM\u7684\u98ce\u9669\u914d\u7f6e\u6587\u4ef6\uff0c\u4f7f\u7528\u884c\u4e3a\u7ecf\u6d4e\u5b66\u548c\u91d1\u878d\u5b66\u7684\u5de5\u5177\uff0c\u57fa\u4e8e\u6548\u7528\u7406\u8bba\u6a21\u578b\u6bd4\u8f83\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548cRLHF\u5bf9\u9f50\u7684LLM\uff0c\u8bc4\u4f30\u63d0\u793a\u5de5\u7a0b\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u540e\u8bad\u7ec3\u7b49\u8c03\u8282\u7b56\u7565\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u51fa\u4e0e\u6807\u51c6\u6548\u7528\u6a21\u578b\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u800c\u9884\u8bad\u7ec3\u548cRLHF\u5bf9\u9f50\u6a21\u578b\u504f\u79bb\u66f4\u591a\u62df\u5408\u7684\u6548\u7528\u6a21\u578b\uff0c\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6700\u7a33\u5b9a\u548c\u6709\u6548\u7684\u98ce\u9669\u504f\u597d\u8c03\u8282\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u522b\u548c\u9636\u6bb5LLM\u7684\u98ce\u9669\u914d\u7f6e\u6587\u4ef6\uff0c\u5c55\u793a\u4e86\u540e\u8bad\u7ec3\u5982\u4f55\u8c03\u8282\u8fd9\u4e9b\u914d\u7f6e\u6587\u4ef6\uff0c\u4e3a\u884c\u4e3a\u5bf9\u9f50\u548c\u98ce\u9669\u611f\u77e5LLM\u8bbe\u8ba1\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2509.23824", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23824", "abs": "https://arxiv.org/abs/2509.23824", "authors": ["Zhifan Ye", "Jiachi Chen", "Zhenzhe Shao", "Lingfeng Bao", "Xiaohu Yang", "Zhongxin Liu"], "title": "SolContractEval: A Benchmark for Evaluating Contract-Level Solidity Code Generation", "comment": null, "summary": "The rise of blockchain has brought smart contracts into mainstream use,\ncreating a demand for smart contract generation tools. While large language\nmodels (LLMs) excel at generating code in general-purpose languages, their\neffectiveness on Solidity, the primary language for smart contracts, remains\nunderexplored. Solidity constitutes only a small portion of typical LLM\ntraining data and differs from general-purpose languages in its\nversion-sensitive syntax and limited flexibility. These factors raise concerns\nabout the reliability of existing LLMs for Solidity code generation.\nCritically, existing evaluations, focused on isolated functions and synthetic\ninputs, fall short of assessing models' capabilities in real-world contract\ndevelopment.\n  To bridge this gap, we introduce SolContractEval, the first contract-level\nbenchmark for Solidity code generation. It comprises 124 tasks drawn from real\non-chain contracts across nine major domains. Each task input, consisting of\ncomplete context dependencies, a structured contract framework, and a concise\ntask prompt, is independently annotated and cross-validated by experienced\ndevelopers. To enable precise and automated evaluation of functional\ncorrectness, we also develop a dynamic evaluation framework based on historical\ntransaction replay. Building on SolContractEval, we perform a systematic\nevaluation of six mainstream LLMs. We find that Claude-3.7-Sonnet achieves the\nhighest overall performance, though evaluated models underperform relative to\ntheir capabilities on class-level generation tasks in general-purpose\nprogramming languages. Second, current models perform better on tasks that\nfollow standard patterns but struggle with complex logic and inter-contract\ndependencies. Finally, they exhibit limited understanding of Solidity-specific\nfeatures and contextual dependencies.", "AI": {"tldr": "\u63d0\u51fa\u4e86SolContractEval\uff0c\u9996\u4e2aSolidity\u667a\u80fd\u5408\u7ea6\u7ea7\u522b\u7684\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b124\u4e2a\u771f\u5b9e\u94fe\u4e0a\u5408\u7ea6\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e866\u4e2a\u4e3b\u6d41LLM\u5728Solidity\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u533a\u5757\u94fe\u53d1\u5c55\u4f7f\u667a\u80fd\u5408\u7ea6\u6210\u4e3a\u4e3b\u6d41\uff0c\u4f46LLM\u5728Solidity\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002Solidity\u5728LLM\u8bad\u7ec3\u6570\u636e\u4e2d\u5360\u6bd4\u5c0f\uff0c\u5177\u6709\u7248\u672c\u654f\u611f\u8bed\u6cd5\u548c\u6709\u9650\u7075\u6d3b\u6027\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u5b64\u7acb\u51fd\u6570\u548c\u5408\u6210\u8f93\u5165\uff0c\u65e0\u6cd5\u8bc4\u4f30\u771f\u5b9e\u5408\u7ea6\u5f00\u53d1\u80fd\u529b\u3002", "method": "\u6784\u5efaSolContractEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b124\u4e2a\u6765\u81ea9\u4e2a\u4e3b\u8981\u9886\u57df\u7684\u771f\u5b9e\u94fe\u4e0a\u5408\u7ea6\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5305\u542b\u5b8c\u6574\u4e0a\u4e0b\u6587\u4f9d\u8d56\u3001\u7ed3\u6784\u5316\u5408\u7ea6\u6846\u67b6\u548c\u7b80\u6d01\u4efb\u52a1\u63d0\u793a\uff0c\u7531\u7ecf\u9a8c\u5f00\u53d1\u8005\u72ec\u7acb\u6807\u6ce8\u548c\u4ea4\u53c9\u9a8c\u8bc1\u3002\u5f00\u53d1\u57fa\u4e8e\u5386\u53f2\u4ea4\u6613\u91cd\u653e\u7684\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u8fdb\u884c\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u3002", "result": "Claude-3.7-Sonnet\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728Solidity\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u5982\u901a\u7528\u7f16\u7a0b\u8bed\u8a00\u7684\u7c7b\u7ea7\u751f\u6210\u4efb\u52a1\u3002\u6a21\u578b\u5728\u6807\u51c6\u6a21\u5f0f\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u590d\u6742\u903b\u8f91\u548c\u5408\u7ea6\u95f4\u4f9d\u8d56\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bf9Solidity\u7279\u5b9a\u7279\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7406\u89e3\u6709\u9650\u3002", "conclusion": "\u5f53\u524dLLM\u5728Solidity\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u903b\u8f91\u3001\u5408\u7ea6\u95f4\u4f9d\u8d56\u548cSolidity\u7279\u5b9a\u7279\u6027\u65b9\u9762\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u5bf9Solidity\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "topic": "swe benchmark"}}
{"id": "2509.22830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22830", "abs": "https://arxiv.org/abs/2509.22830", "authors": ["Hwan Chang", "Yonghyun Jun", "Hwanhee Lee"], "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "comment": null, "summary": "The growing deployment of large language model (LLM) based agents that\ninteract with external environments has created new attack surfaces for\nadversarial manipulation. One major threat is indirect prompt injection, where\nattackers embed malicious instructions in external environment output, causing\nagents to interpret and execute them as if they were legitimate prompts. While\nprevious research has focused primarily on plain-text injection attacks, we\nfind a significant yet underexplored vulnerability: LLMs' dependence on\nstructured chat templates and their susceptibility to contextual manipulation\nthrough persuasive multi-turn dialogues. To this end, we introduce ChatInject,\nan attack that formats malicious payloads to mimic native chat templates,\nthereby exploiting the model's inherent instruction-following tendencies.\nBuilding on this foundation, we develop a persuasion-driven Multi-turn variant\nthat primes the agent across conversational turns to accept and execute\notherwise suspicious actions. Through comprehensive experiments across frontier\nLLMs, we demonstrate three critical findings: (1) ChatInject achieves\nsignificantly higher average attack success rates than traditional prompt\ninjection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%\nto 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong\nperformance at average 52.33% success rate on InjecAgent, (2)\nchat-template-based payloads demonstrate strong transferability across models\nand remain effective even against closed-source LLMs, despite their unknown\ntemplate structures, and (3) existing prompt-based defenses are largely\nineffective against this attack approach, especially against Multi-turn\nvariants. These findings highlight vulnerabilities in current agent systems.", "AI": {"tldr": "ChatInject\u662f\u4e00\u79cd\u65b0\u578b\u7684\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u901a\u8fc7\u6a21\u62df\u804a\u5929\u6a21\u677f\u683c\u5f0f\u548c\u591a\u8f6e\u5bf9\u8bdd\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9LLM\u4ee3\u7406\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u5bf9\u6b64\u7c7b\u653b\u51fb\u6548\u679c\u6709\u9650\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u5916\u90e8\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u51fa\u73b0\u4e86\u65b0\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7eaf\u6587\u672c\u6ce8\u5165\u653b\u51fb\uff0c\u4f46LLM\u5bf9\u7ed3\u6784\u5316\u804a\u5929\u6a21\u677f\u7684\u4f9d\u8d56\u53ca\u5176\u5728\u4e0a\u4e0b\u6587\u64cd\u7eb5\u4e2d\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faChatInject\u653b\u51fb\u65b9\u6cd5\uff1a1\uff09\u5c06\u6076\u610f\u8f7d\u8377\u683c\u5f0f\u5316\u4e3a\u539f\u751f\u804a\u5929\u6a21\u677f\u683c\u5f0f\uff1b2\uff09\u5f00\u53d1\u591a\u8f6e\u5bf9\u8bdd\u53d8\u4f53\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u5f15\u5bfc\u4ee3\u7406\u63a5\u53d7\u53ef\u7591\u64cd\u4f5c\uff1b3\uff09\u5728\u591a\u4e2a\u524d\u6cbfLLM\u4e0a\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "1\uff09ChatInject\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08AgentDojo\u4ece5.18%\u63d0\u5347\u81f332.05%\uff0cInjecAgent\u4ece15.13%\u63d0\u5347\u81f345.90%\uff09\uff1b2\uff09\u804a\u5929\u6a21\u677f\u8f7d\u8377\u5177\u6709\u5f3a\u8de8\u6a21\u578b\u8fc1\u79fb\u6027\uff1b3\uff09\u73b0\u6709\u63d0\u793a\u9632\u5fa1\u63aa\u65bd\u5bf9\u6b64\u7c7b\u653b\u51fb\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5f53\u524d\u4ee3\u7406\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0cChatInject\u653b\u51fb\u63ed\u793a\u4e86LLM\u5bf9\u7ed3\u6784\u5316\u6a21\u677f\u7684\u4f9d\u8d56\u6027\u548c\u4e0a\u4e0b\u6587\u64cd\u7eb5\u7684\u8106\u5f31\u6027\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9632\u5fa1\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2509.23102", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23102", "abs": "https://arxiv.org/abs/2509.23102", "authors": ["Fang Wu", "Xu Huang", "Weihao Xuan", "Zhiwei Zhang", "Yijia Xiao", "Guancheng Wan", "Xiaomin Li", "Bing Hu", "Peng Xia", "Jure Leskovec", "Yejin Choi"], "title": "Multiplayer Nash Preference Optimization", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has emerged as the standard\nparadigm for aligning large language models (LLMs) with human preferences.\nHowever, reward-based methods built on the Bradley-Terry assumption struggle to\ncapture the non-transitive and heterogeneous nature of real-world preferences.\nTo address this, recent studies have reframed alignment as a two-player Nash\ngame, giving rise to Nash learning from human feedback (NLHF). While this\nperspective has inspired algorithms such as INPO, ONPO, and EGPO with strong\ntheoretical and empirical guarantees, they remain fundamentally restricted to\ntwo-player interactions, creating a single-opponent bias that fails to capture\nthe full complexity of realistic preference structures. In this work, we\nintroduce Multiplayer Nash Preference Optimization (MNPO), a novel framework\nthat generalizes NLHF to the multiplayer regime. It formulates alignment as an\n$n$-player game, where each policy competes against a population of opponents\nwhile being regularized toward a reference model. Our framework establishes\nwell-defined Nash equilibria in multiplayer settings and extends the concept of\nduality gap to quantify approximation quality. We demonstrate that MNPO\ninherits the equilibrium guarantees of two-player methods while enabling richer\ncompetitive dynamics and improved coverage of diverse preference structures.\nThrough comprehensive empirical evaluation, we show that MNPO consistently\noutperforms existing NLHF baselines on instruction-following benchmarks,\nachieving superior alignment quality under heterogeneous annotator conditions\nand mixed-policy evaluation scenarios. Together, these results establish MNPO\nas a principled and scalable framework for aligning LLMs with complex,\nnon-transitive human preferences. Code is available at\nhttps://github.com/smiles724/MNPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86Multiplayer Nash Preference Optimization (MNPO)\uff0c\u5c06Nash\u5b66\u4e60\u4ece\u4e24\u4eba\u535a\u5f08\u6269\u5c55\u5230\u591a\u4eba\u535a\u5f08\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u4e24\u4eba\u4ea4\u4e92\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u7684\u4eba\u7c7b\u504f\u597d\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5956\u52b1\u7684\u65b9\u6cd5\u548c\u4e24\u4ebaNash\u5b66\u4e60\u6846\u67b6\u65e0\u6cd5\u5145\u5206\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u4e2d\u504f\u597d\u7684\u975e\u4f20\u9012\u6027\u548c\u5f02\u8d28\u6027\u7279\u5f81\uff0c\u5b58\u5728\u5355\u4e00\u5bf9\u624b\u504f\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5c06\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3an\u4eba\u535a\u5f08\uff0c\u6bcf\u4e2a\u7b56\u7565\u4e0e\u4e00\u7ec4\u5bf9\u624b\u7ade\u4e89\uff0c\u540c\u65f6\u5411\u53c2\u8003\u6a21\u578b\u6b63\u5219\u5316\u3002\u5efa\u7acb\u4e86\u591a\u4eba\u8bbe\u7f6e\u4e0b\u7684Nash\u5747\u8861\uff0c\u5e76\u6269\u5c55\u4e86\u5bf9\u5076\u95f4\u9699\u6982\u5ff5\u6765\u91cf\u5316\u8fd1\u4f3c\u8d28\u91cf\u3002", "result": "\u5728\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMNPO\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684NLHF\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5f02\u8d28\u6ce8\u91ca\u8005\u6761\u4ef6\u548c\u6df7\u5408\u7b56\u7565\u8bc4\u4f30\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5bf9\u9f50\u8d28\u91cf\u3002", "conclusion": "MNPO\u4e3a\u5bf9\u9f50LLM\u4e0e\u590d\u6742\u3001\u975e\u4f20\u9012\u6027\u7684\u4eba\u7c7b\u504f\u597d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23835", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23835", "abs": "https://arxiv.org/abs/2509.23835", "authors": ["Yukai Zhao", "Menghan Wu", "Xing Hu", "Xin Xia"], "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation, but they\nface critical security risks when applied to practical production due to\npackage hallucinations, in which LLMs recommend non-existent packages. These\nhallucinations can be exploited in software supply chain attacks, where\nmalicious attackers exploit them to register harmful packages. It is critical\nto test LLMs for package hallucinations to mitigate package hallucinations and\ndefend against potential attacks. Although researchers have proposed testing\nframeworks for fact-conflicting hallucinations in natural language generation,\nthere is a lack of research on package hallucinations. To fill this gap, we\npropose HFUZZER, a novel phrase-based fuzzing framework to test LLMs for\npackage hallucinations. HFUZZER adopts fuzzing technology and guides the model\nto infer a wider range of reasonable information based on phrases, thereby\ngenerating enough and diverse coding tasks. Furthermore, HFUZZER extracts\nphrases from package information or coding tasks to ensure the relevance of\nphrases and code, thereby improving the relevance of generated tasks and code.\nWe evaluate HFUZZER on multiple LLMs and find that it triggers package\nhallucinations across all selected models. Compared to the mutational fuzzing\nframework, HFUZZER identifies 2.60x more unique hallucinated packages and\ngenerates more diverse tasks. Additionally, when testing the model GPT-4o,\nHFUZZER finds 46 unique hallucinated packages. Further analysis reveals that\nfor GPT-4o, LLMs exhibit package hallucinations not only during code generation\nbut also when assisting with environment configuration.", "AI": {"tldr": "HFUZZER\u662f\u4e00\u4e2a\u57fa\u4e8e\u77ed\u8bed\u7684\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5305\u5e7b\u89c9\u95ee\u9898\uff0c\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u4efb\u52a1\u5e76\u53d1\u73b0\u66f4\u591a\u72ec\u7279\u7684\u5e7b\u89c9\u5305\u3002", "motivation": "LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u9762\u4e34\u5305\u5e7b\u89c9\u7684\u5b89\u5168\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u653b\u51fb\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u5305\u5e7b\u89c9\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u77ed\u8bed\u7684\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\uff0c\u4ece\u5305\u4fe1\u606f\u6216\u7f16\u7801\u4efb\u52a1\u4e2d\u63d0\u53d6\u77ed\u8bed\uff0c\u751f\u6210\u591a\u6837\u5316\u4e14\u76f8\u5173\u7684\u7f16\u7801\u4efb\u52a1\u6765\u6d4b\u8bd5LLM\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u7684LLM\u4e2d\u90fd\u89e6\u53d1\u4e86\u5305\u5e7b\u89c9\uff0c\u76f8\u6bd4\u53d8\u5f02\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\u53d1\u73b02.60\u500d\u66f4\u591a\u72ec\u7279\u5e7b\u89c9\u5305\uff0c\u5728GPT-4o\u4e2d\u53d1\u73b046\u4e2a\u72ec\u7279\u5e7b\u89c9\u5305\u3002", "conclusion": "LLM\u4e0d\u4ec5\u5728\u4ee3\u7801\u751f\u6210\u4e2d\uff0c\u8fd8\u5728\u73af\u5883\u914d\u7f6e\u534f\u52a9\u4e2d\u90fd\u4f1a\u51fa\u73b0\u5305\u5e7b\u89c9\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6d4b\u8bd5\u6846\u67b6\u6765\u8bc6\u522b\u548c\u7f13\u89e3\u8fd9\u79cd\u5b89\u5168\u98ce\u9669\u3002", "topic": "code agent"}}
{"id": "2509.24091", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.24091", "abs": "https://arxiv.org/abs/2509.24091", "authors": ["Spandan Garg", "Roshanak Zilouchian Moghaddam"], "title": "PerfBench: Can Agents Resolve Real-World Performance Bugs?", "comment": null, "summary": "Performance bugs are inefficiencies in software that waste computational\nresources without causing functional failures, making them particularly\nchallenging to detect and fix. While recent advances in Software Engineering\nagents have shown promise in automated bug fixing, existing benchmarks\nprimarily focus on functional correctness and fail to evaluate agents'\nabilities to identify and resolve non-functional issues like performance bugs.\nWe introduce PerfBench, a benchmark comprising 81 real-world performance\nbug-fixing tasks from popular .NET repositories on GitHub. Unlike existing\nbenchmarks that rely on pre-existing test suites, PerfBench features a novel\nevaluation harness that allows agents to generate their own performance\nbenchmarks and validates fixes by comparing execution metrics collected for\ndeveloper fix and agent fix. Each task in PerfBench is derived from actual\ndeveloper fixes linked to performance-related issues, which are then verified\nby human experts, ensuring real-world relevance. Our evaluation reveals that\ncurrent state-of-the-art coding agents struggle with performance optimization\ntasks, with baseline OpenHands agent achieving only a ~3% success rate on our\nbenchmark. We develop OpenHands-Perf-Agent, which incorporates\nperformance-aware tooling and instructions and achieves a ~20% success rate on\nthe benchmark. We show that by ensuring the agent has proper instructions to\nbenchmark its changes and tooling for benchmark output processing, we can\nimprove the agent performance significantly, but room for improvement still\nremains. PerfBench provides a challenging test set for furthering the\ncapabilities of agents in fixing performance issues.", "AI": {"tldr": "\u63d0\u51fa\u4e86PerfBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b81\u4e2a\u771f\u5b9e\u4e16\u754c\u7684.NET\u6027\u80fdbug\u4fee\u590d\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u6027\u80fd\u4f18\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u8bc4\u4f30\u4ee3\u7406\u5728\u8bc6\u522b\u548c\u89e3\u51b3\u6027\u80fdbug\u7b49\u975e\u529f\u80fd\u6027\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efa\u5305\u542b\u771f\u5b9e\u5f00\u53d1\u8005\u4fee\u590d\u7684\u6027\u80fdbug\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u65b0\u9896\u7684\u8bc4\u4f30\u6846\u67b6\u8ba9\u4ee3\u7406\u751f\u6210\u81ea\u5df1\u7684\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u6267\u884c\u6307\u6807\u6765\u9a8c\u8bc1\u4fee\u590d\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7f16\u7801\u4ee3\u7406\u5728\u6027\u80fd\u4f18\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cOpenHands\u4ee3\u7406\u4ec5\u8fbe\u5230\u7ea63%\u7684\u6210\u529f\u7387\u3002\u5f00\u53d1\u7684OpenHands-Perf-Agent\u901a\u8fc7\u6027\u80fd\u611f\u77e5\u5de5\u5177\u548c\u6307\u4ee4\u8fbe\u5230\u7ea620%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u9002\u5f53\u7684\u57fa\u51c6\u6d4b\u8bd5\u6307\u4ee4\u548c\u5de5\u5177\u5904\u7406\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u4ee3\u7406\u6027\u80fd\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002PerfBench\u4e3a\u63d0\u5347\u4ee3\u7406\u4fee\u590d\u6027\u80fd\u95ee\u9898\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u96c6\u3002", "topic": "swe benchmark"}}
{"id": "2509.22963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22963", "abs": "https://arxiv.org/abs/2509.22963", "authors": ["Haitong Ma", "Ofir Nabati", "Aviv Rosenberg", "Bo Dai", "Oran Lang", "Idan Szpektor", "Craig Boutilier", "Na Li", "Shie Mannor", "Lior Shani", "Guy Tenneholtz"], "title": "Reinforcement Learning with Discrete Diffusion Policies for Combinatorial Action Spaces", "comment": "22 pages, 10 figures. Haitong Ma and Ofir Nabati contributed equally\n  to this paper", "summary": "Reinforcement learning (RL) struggles to scale to large, combinatorial action\nspaces common in many real-world problems. This paper introduces a novel\nframework for training discrete diffusion models as highly effective policies\nin these complex settings. Our key innovation is an efficient online training\nprocess that ensures stable and effective policy improvement. By leveraging\npolicy mirror descent (PMD) to define an ideal, regularized target policy\ndistribution, we frame the policy update as a distributional matching problem,\ntraining the expressive diffusion model to replicate this stable target. This\ndecoupled approach stabilizes learning and significantly enhances training\nperformance. Our method achieves state-of-the-art results and superior sample\nefficiency across a diverse set of challenging combinatorial benchmarks,\nincluding DNA sequence generation, RL with macro-actions, and multi-agent\nsystems. Experiments demonstrate that our diffusion policies attain superior\nperformance compared to other baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u79bb\u6563\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u6269\u5c55\u95ee\u9898\uff0c\u901a\u8fc7\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u548c\u5206\u5e03\u5339\u914d\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u5927\u89c4\u6a21\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u65f6\u9762\u4e34\u6269\u5c55\u56f0\u96be\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7b56\u7565\u8868\u793a\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u7b56\u7565\uff0c\u901a\u8fc7\u7b56\u7565\u955c\u50cf\u4e0b\u964d\u5b9a\u4e49\u6b63\u5219\u5316\u76ee\u6807\u7b56\u7565\u5206\u5e03\uff0c\u5c06\u7b56\u7565\u66f4\u65b0\u8f6c\u5316\u4e3a\u5206\u5e03\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u89e3\u8026\u7684\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728DNA\u5e8f\u5217\u751f\u6210\u3001\u5b8f\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7b49\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u548c\u4f18\u5f02\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u6269\u6563\u7b56\u7565\u76f8\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24148", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24148", "abs": "https://arxiv.org/abs/2509.24148", "authors": ["Yiran Hu", "Nan Jiang", "Shanchao Liang", "Yi Wu", "Lin Tan"], "title": "TENET: Leveraging Tests Beyond Validation for Code Generation", "comment": null, "summary": "Test-Driven Development (TDD) is a widely adopted software engineering\npractice that requires developers to create and execute tests alongside code\nimplementation, ensuring that software behavior is continuously validated and\nrefined. In the era of vibe coding, where developers increasingly delegate code\nwriting to large language models (LLMs) by specifying high-level intentions,\nTDD becomes even more crucial, as test cases serve as executable specifications\nthat explicitly define and verify intended functionality beyond what\nnatural-language descriptions and code context can convey. While vibe coding\nunder TDD is promising, there are three main challenges: (1) selecting a small\nyet effective test suite to improve the generation accuracy and control the\nexecution workload, (2) retrieving context such as relevant code effectively,\nand (3) systematically using test feedback for effective code refinement. To\naddress these challenges, we introduce TENET, an LLM agent for generating\nfunctions in complex real-world repositories under the TDD setting. TENET\nfeatures three components: (1) a novel test harness mechanism that selects a\nconcise test suite to maximize diversity of target usage scenarios; (2) a\ntailored agent toolset that performs efficient retrieval of relevant code with\ninteractive debugging; and (3) a reflection-based refinement workflow that\niteratively analyzes failures, replenishes context, and applies code\nrefinement. TENET achieves 69.08% and 81.77% Pass@1 on RepoCod and RepoEval\nbenchmarks, outperforming the best agentic baselines by 9.49 and 2.17\npercentage points, respectively. In addition, this is the first study of\ntest-driven code generation with repository-level context, examining how\ndifferent aspects of test suites affect the performance of LLM agents under the\nTDD setting.", "AI": {"tldr": "TENET\u662f\u4e00\u4e2a\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1(TDD)\u73af\u5883\u4e0b\u751f\u6210\u4ee3\u7801\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6d4b\u8bd5\u5957\u4ef6\u9009\u62e9\u3001\u4ee3\u7801\u68c0\u7d22\u548c\u57fa\u4e8e\u53cd\u9988\u7684\u8fed\u4ee3\u4f18\u5316\u6765\u89e3\u51b3vibe coding\u4e2d\u7684\u6311\u6218\uff0c\u5728RepoCod\u548cRepoEval\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728vibe coding\u65f6\u4ee3\uff0c\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56LLM\u6839\u636e\u9ad8\u5c42\u610f\u56fe\u751f\u6210\u4ee3\u7801\uff0cTDD\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u6d4b\u8bd5\u7528\u4f8b\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u6267\u884c\u89c4\u8303\u6765\u660e\u786e\u5b9a\u4e49\u548c\u9a8c\u8bc1\u529f\u80fd\u3002\u4f46\u9762\u4e34\u6d4b\u8bd5\u5957\u4ef6\u9009\u62e9\u3001\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u6d4b\u8bd5\u53cd\u9988\u5229\u7528\u4e09\u5927\u6311\u6218\u3002", "method": "TENET\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u6d4b\u8bd5\u5957\u4ef6\u9009\u62e9\u673a\u5236\uff0c\u9009\u62e9\u7b80\u6d01\u591a\u6837\u7684\u6d4b\u8bd5\u7528\u4f8b\uff1b(2)\u5b9a\u5236\u5316\u5de5\u5177\u96c6\uff0c\u9ad8\u6548\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u5e76\u652f\u6301\u4ea4\u4e92\u5f0f\u8c03\u8bd5\uff1b(3)\u57fa\u4e8e\u53cd\u601d\u7684\u4f18\u5316\u5de5\u4f5c\u6d41\uff0c\u8fed\u4ee3\u5206\u6790\u5931\u8d25\u3001\u8865\u5145\u4e0a\u4e0b\u6587\u5e76\u5e94\u7528\u4ee3\u7801\u4f18\u5316\u3002", "result": "\u5728RepoCod\u548cRepoEval\u57fa\u51c6\u4e0a\u5206\u522b\u8fbe\u523069.08%\u548c81.77%\u7684Pass@1\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u5206\u522b\u9ad8\u51fa9.49\u548c2.17\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u4ed3\u5e93\u7ea7\u522b\u4e0a\u4e0b\u6587\u4e2d\u7814\u7a76\u6d4b\u8bd5\u9a71\u52a8\u4ee3\u7801\u751f\u6210\u7684\u5de5\u4f5c\uff0c\u63a2\u8ba8\u4e86\u6d4b\u8bd5\u5957\u4ef6\u7684\u4e0d\u540c\u65b9\u9762\u5982\u4f55\u5f71\u54cdTDD\u73af\u5883\u4e0bLLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2509.22964", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22964", "abs": "https://arxiv.org/abs/2509.22964", "authors": ["Qinxun Bai", "Yuxuan Han", "Wei Xu", "Zhengyuan Zhou"], "title": "Functional Critic Modeling for Provably Convergent Off-Policy Actor-Critic", "comment": null, "summary": "Off-policy reinforcement learning (RL) with function approximation offers an\neffective way to improve sample efficiency by reusing past experience. Within\nthis setting, the actor-critic (AC) framework has achieved strong empirical\nsuccess. However, both the critic and actor learning is challenging for the\noff-policy AC methods: first of all, in addition to the classic \"deadly triad\"\ninstability of off-policy evaluation, it also suffers from a \"moving target\"\nproblem, where the policy being evaluated changes continually; secondly, actor\nlearning becomes less efficient due to the difficulty of estimating the exact\noff-policy policy gradient. The first challenge essentially reduces the problem\nto repeatedly performing off-policy evaluation for changing policies. For the\nsecond challenge, the off-policy policy gradient theorem requires a complex and\noften impractical algorithm to estimate an additional emphasis critic, which is\ntypically neglected in practice, thereby reducing to the on-policy policy\ngradient as an approximation. In this work, we introduce a novel concept of\nfunctional critic modeling, which leads to a new AC framework that addresses\nboth challenges for actor-critic learning under the deadly triad setting. We\nprovide a theoretical analysis in the linear function setting, establishing the\nprovable convergence of our framework, which, to the best of our knowledge, is\nthe first convergent off-policy target-based AC algorithm. From a practical\nperspective, we further propose a carefully designed neural network\narchitecture for the functional critic modeling and demonstrate its\neffectiveness through preliminary experiments on widely used RL tasks from the\nDeepMind Control Benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u529f\u80fd\u6027\u8bc4\u8bba\u5bb6\u5efa\u6a21\u6982\u5ff5\uff0c\u4e3a\u81f4\u547d\u4e09\u89d2\u8bbe\u7f6e\u4e0b\u7684\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u5b66\u4e60\u89e3\u51b3\u4e86\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u4e2d\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u6846\u67b6\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8bc4\u8bba\u5bb6\u5b66\u4e60\u4e2d\u7684'\u79fb\u52a8\u76ee\u6807'\u95ee\u9898\u548c\u884c\u52a8\u8005\u5b66\u4e60\u4e2d\u7684\u79bb\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u56f0\u96be\u3002", "method": "\u5f15\u5165\u4e86\u529f\u80fd\u6027\u8bc4\u8bba\u5bb6\u5efa\u6a21\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u6846\u67b6\uff0c\u5728\u7ebf\u6027\u51fd\u6570\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u5e76\u8bbe\u8ba1\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728DeepMind\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e7f\u6cdbRL\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u521d\u6b65\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u53ef\u8bc1\u660e\u6536\u655b\u7684\u79bb\u7b56\u7565\u76ee\u6807\u578b\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u4e3a\u81f4\u547d\u4e09\u89d2\u8bbe\u7f6e\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.22887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22887", "abs": "https://arxiv.org/abs/2509.22887", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "comment": null, "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key\naspect of human social intelligence, yet, chatbots and LLM-based social agents\ndo not typically integrate it. In this work, we demonstrate that LLMs that\nexplicitly use ToM get better at dialogue, achieving goals more effectively.\nAfter showing that simply prompting models to generate mental states between\ndialogue turns already provides significant benefit, we further introduce\nToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM\nwith dialogue lookahead to produce mental states that are maximally useful for\nachieving dialogue goals. Experiments on the Sotopia interactive social\nevaluation benchmark demonstrate the effectiveness of our method over a range\nof baselines. Comprehensive analysis shows that ToMA exhibits more strategic,\ngoal-oriented reasoning behaviors, which enable long-horizon adaptation, while\nmaintaining better relationships with their partners. Our results suggest a\nstep forward in integrating ToM for building socially intelligent LLM agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba(ToM)\u7684\u5bf9\u8bdd\u4ee3\u7406ToMAgent\uff0c\u901a\u8fc7\u663e\u5f0f\u751f\u6210\u5fc3\u7406\u72b6\u6001\u6765\u63d0\u5347\u5bf9\u8bdd\u6548\u679c\u548c\u8fbe\u6210\u76ee\u6807\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u804a\u5929\u673a\u5668\u4eba\u548c\u57fa\u4e8eLLM\u7684\u793e\u4ea4\u4ee3\u7406\u901a\u5e38\u4e0d\u6574\u5408\u5fc3\u667a\u7406\u8bba\uff0c\u800c\u5fc3\u667a\u7406\u8bba\u662f\u4eba\u7c7b\u793e\u4f1a\u667a\u80fd\u7684\u5173\u952e\u65b9\u9762\u3002", "method": "\u5f15\u5165ToMAgent(ToMA)\uff0c\u901a\u8fc7\u5c06\u5fc3\u667a\u7406\u8bba\u4e0e\u5bf9\u8bdd\u524d\u77bb\u914d\u5bf9\uff0c\u751f\u6210\u5bf9\u8fbe\u6210\u5bf9\u8bdd\u76ee\u6807\u6700\u6709\u7528\u7684\u5fc3\u7406\u72b6\u6001\u3002", "result": "\u5728Sotopia\u4ea4\u4e92\u5f0f\u793e\u4ea4\u8bc4\u4f30\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u57fa\u7ebf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u73b0\u51fa\u66f4\u5177\u6218\u7565\u6027\u3001\u76ee\u6807\u5bfc\u5411\u7684\u63a8\u7406\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5728\u6574\u5408\u5fc3\u667a\u7406\u8bba\u4ee5\u6784\u5efa\u5177\u6709\u793e\u4f1a\u667a\u80fd\u7684LLM\u4ee3\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2509.23143", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.23143", "abs": "https://arxiv.org/abs/2509.23143", "authors": ["Charles L. Wang"], "title": "MathBode: Frequency-Domain Fingerprints of LLM Mathematical Reasoning", "comment": null, "summary": "This paper presents MathBode, a dynamic diagnostic for mathematical reasoning\nin large language models (LLMs). Instead of one-shot accuracy, MathBode treats\neach parametric problem as a system: we drive a single parameter sinusoidally\nand fit first-harmonic responses of model outputs and exact solutions. This\nyields interpretable, frequency-resolved metrics -- gain (amplitude tracking)\nand phase (lag) -- that form Bode-style fingerprints. Across five closed-form\nfamilies (linear solve, ratio/saturation, compound interest, 2x2 linear\nsystems, similar triangles), the diagnostic surfaces systematic low-pass\nbehavior and growing phase lag that accuracy alone obscures. We compare several\nmodels against a symbolic baseline that calibrates the instrument ($G \\approx\n1$, $\\phi \\approx 0$). Results separate frontier from mid-tier models on\ndynamics, providing a compact, reproducible protocol that complements standard\nbenchmarks with actionable measurements of reasoning fidelity and consistency.\nWe open-source the dataset and code to enable further research and adoption.", "AI": {"tldr": "MathBode\u662f\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u52a8\u6001\u8bca\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5f26\u9a71\u52a8\u53c2\u6570\u53d8\u5316\u5e76\u5206\u6790\u6a21\u578b\u8f93\u51fa\u7684\u9891\u7387\u54cd\u5e94\uff0c\u63d0\u4f9b\u589e\u76ca\u548c\u76f8\u4f4d\u7b49\u53ef\u89e3\u91ca\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u7684\u4e00\u6b21\u6027\u51c6\u786e\u7387\u8bc4\u4f30\u65e0\u6cd5\u63ed\u793aLLMs\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u52a8\u6001\u5206\u6790\u65b9\u6cd5\u6765\u8bc4\u4f30\u63a8\u7406\u7684\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "method": "\u5c06\u53c2\u6570\u5316\u95ee\u9898\u89c6\u4e3a\u7cfb\u7edf\uff0c\u5bf9\u5355\u4e2a\u53c2\u6570\u8fdb\u884c\u6b63\u5f26\u9a71\u52a8\uff0c\u62df\u5408\u6a21\u578b\u8f93\u51fa\u548c\u7cbe\u786e\u89e3\u7684\u4e00\u6b21\u8c10\u6ce2\u54cd\u5e94\uff0c\u83b7\u5f97\u589e\u76ca\uff08\u5e45\u5ea6\u8ddf\u8e2a\uff09\u548c\u76f8\u4f4d\uff08\u6ede\u540e\uff09\u7b49\u9891\u7387\u5206\u8fa8\u6307\u6807\u3002", "result": "\u5728\u4e94\u4e2a\u95ed\u5f0f\u95ee\u9898\u5bb6\u65cf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u4f4e\u901a\u884c\u4e3a\u548c\u589e\u957f\u7684\u76f8\u4f4d\u6ede\u540e\uff0c\u8fd9\u4e9b\u662f\u51c6\u786e\u6027\u6307\u6807\u65e0\u6cd5\u53d1\u73b0\u7684\u3002\u524d\u6cbf\u6a21\u578b\u4e0e\u4e2d\u7aef\u6a21\u578b\u5728\u52a8\u6001\u7279\u6027\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "MathBode\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u590d\u73b0\u7684\u534f\u8bae\uff0c\u8865\u5145\u4e86\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u6d4b\u91cf\u63a8\u7406\u7684\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.24380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24380", "abs": "https://arxiv.org/abs/2509.24380", "authors": ["Shuiguang Deng", "Hailiang Zhao", "Ziqi Wang", "Guanjie Cheng", "Peng Chen", "Wenzhuo Qian", "Zhiwei Ling", "Jianwei Yin", "Albert Y. Zomaya", "Schahram Dustdar"], "title": "Agentic Services Computing", "comment": null, "summary": "The rise of LLM-powered agents is driving a fundamental transformation in\nservices computing: from static, request-response functions to dynamic,\ngoal-oriented, and autonomous multi-agent ecosystems. In response to this\nshift, we introduce Agentic Service Computing (ASC), a new paradigm that\nreimagines services as intelligent, self-adaptive, and socially embedded\nentities. This comprehensive survey presents a lifecycle-driven framework for\nASC, structured around four core phases: Design, Deployment, Operation, and\nEvolution. We systematically analyze ASC through four foundational research\ndimensions: (1) Perception, Context, and Environment Modeling, (2) Autonomous\nDecision-Making and Task Execution, (3) Multi-Agent Collaboration and\nOrganization, and (4) Evaluation, Value Alignment, and Trustworthiness. We\nexamine how these dimensions are instantiated, integrated, and continuously\nadapted across the service lifecycle. Our synthesis reveals that agentic\nservices are not merely assembled but orchestrated: contextual awareness\nenables robust deployment; autonomous reasoning supports real-time operation;\ncollaborative structures emerge and evolve through interaction; and\ntrustworthiness must be upheld as a cross-cutting, lifelong imperative. We\nfurther identify and discuss emerging trends shaping the future of ASC. By\nintegrating classical principles of services computing with advances in\nLLM-based multi-agent systems, this work establishes a holistic and\nforward-looking foundation for ASC. It provides a unified reference for\nresearchers and practitioners aiming to develop adaptive, accountable, and\nhuman-centered intelligent services.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u667a\u80fd\u670d\u52a1\u8ba1\u7b97(ASC)\u65b0\u8303\u5f0f\uff0c\u5c06\u670d\u52a1\u91cd\u65b0\u6784\u60f3\u4e3a\u667a\u80fd\u3001\u81ea\u9002\u5e94\u3001\u793e\u4f1a\u5d4c\u5165\u7684\u5b9e\u4f53\uff0c\u5e76\u6784\u5efa\u4e86\u57fa\u4e8e\u751f\u547d\u5468\u671f\u7684\u56db\u9636\u6bb5\u6846\u67b6(\u8bbe\u8ba1\u3001\u90e8\u7f72\u3001\u8fd0\u8425\u3001\u6f14\u8fdb)\u548c\u56db\u4e2a\u7814\u7a76\u7ef4\u5ea6\u6765\u7cfb\u7edf\u5206\u6790ASC\u3002", "motivation": "\u968f\u7740LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5174\u8d77\uff0c\u670d\u52a1\u8ba1\u7b97\u6b63\u5728\u4ece\u9759\u6001\u7684\u8bf7\u6c42-\u54cd\u5e94\u51fd\u6570\u5411\u52a8\u6001\u3001\u76ee\u6807\u5bfc\u5411\u3001\u81ea\u4e3b\u7684\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u8f6c\u53d8\uff0c\u9700\u8981\u5efa\u7acb\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u6307\u5bfc\u8fd9\u4e00\u53d8\u9769\u3002", "method": "\u91c7\u7528\u751f\u547d\u5468\u671f\u9a71\u52a8\u6846\u67b6\uff0c\u56f4\u7ed5\u8bbe\u8ba1\u3001\u90e8\u7f72\u3001\u8fd0\u8425\u3001\u6f14\u8fdb\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\uff0c\u4ece\u611f\u77e5\u4e0e\u73af\u5883\u5efa\u6a21\u3001\u81ea\u4e3b\u51b3\u7b56\u4e0e\u4efb\u52a1\u6267\u884c\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e0e\u7ec4\u7ec7\u3001\u8bc4\u4f30\u4e0e\u53ef\u4fe1\u5ea6\u56db\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u5206\u6790ASC\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u667a\u80fd\u670d\u52a1\u4e0d\u662f\u7b80\u5355\u7ec4\u88c5\u800c\u662f\u7f16\u6392\u7684\uff1a\u4e0a\u4e0b\u6587\u611f\u77e5\u652f\u6301\u7a33\u5065\u90e8\u7f72\uff0c\u81ea\u4e3b\u63a8\u7406\u652f\u6301\u5b9e\u65f6\u8fd0\u8425\uff0c\u534f\u4f5c\u7ed3\u6784\u901a\u8fc7\u4ea4\u4e92\u6d8c\u73b0\u6f14\u8fdb\uff0c\u53ef\u4fe1\u5ea6\u662f\u8d2f\u7a7f\u6574\u4e2a\u751f\u547d\u5468\u671f\u7684\u5173\u952e\u8981\u6c42\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u4f20\u7edf\u670d\u52a1\u8ba1\u7b97\u539f\u5219\u4e0e\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8fdb\u5c55\uff0c\u4e3aASC\u5efa\u7acb\u4e86\u5168\u9762\u524d\u77bb\u7684\u57fa\u7840\uff0c\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94\u3001\u53ef\u95ee\u8d23\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u667a\u80fd\u670d\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u53c2\u8003\u3002", "topic": "agent analysis"}}
{"id": "2509.24419", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24419", "abs": "https://arxiv.org/abs/2509.24419", "authors": ["Yuanhe Zhang", "Zhiquan Yang", "Shengyi Pan", "Zhongxin Liu"], "title": "Unit Test Update through LLM-Driven Context Collection and Error-Type-Aware Refinement", "comment": null, "summary": "Unit testing is critical for ensuring software quality and software system\nstability. The current practice of manually maintaining unit tests suffers from\nlow efficiency and the risk of delayed or overlooked fixes. Therefore, an\nautomated approach is required to instantly update unit tests, with the\ncapability to both repair and enhance unit tests. However, existing automated\ntest maintenance methods primarily focus on repairing broken tests, neglecting\nthe scenario of enhancing existing tests to verify new functionality.\nMeanwhile, due to their reliance on rule-based context collection and the lack\nof verification mechanisms, existing approaches struggle to handle complex code\nchanges and often produce test cases with low correctness. To address these\nchallenges, we propose TESTUPDATER, a novel LLM based approach that enables\nautomated just-in-time test updates in response to production code changes.\nTESTUPDATER first leverages the LLM to analyze code changes and identify\nrelevant context, which it then extracts and filters. Then, through carefully\ndesigned prompts, TESTUPDATER guides the LLM step by step to handle various\ntypes of code changes and introduce new dependencies, enabling both test repair\nand enhancement. Finally, we introduce an error-type-aware iterative refinement\nmechanism that executes the LLM-updated tests and repairs failures, which\nsignificantly improves the overall correctness of test updates. Since existing\ntest repair datasets lack scenarios of test enhancement, we further construct a\nnew benchmark, UPDATES4J, with 195 real-world samples from 7 projects.\nExperimental results show that TESTUPDATER achieves a compilation pass rate of\n94.4% and a test pass rate of 86.7%, outperforming the state-of-the-art method\nSYNTER by 15.9% and 20.0%, respectively. Furthermore, TESTUPDATER exhibits\n12.9% higher branch coverage and 15.2% greater line coverage than SYNTER.", "AI": {"tldr": "TESTUPDATER\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u66f4\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u5373\u65f6\u54cd\u5e94\u751f\u4ea7\u4ee3\u7801\u53d8\u66f4\uff0c\u540c\u65f6\u4fee\u590d\u548c\u589e\u5f3a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5206\u6790\u3001\u9010\u6b65\u63d0\u793a\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u66f4\u65b0\u7684\u6b63\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u624b\u52a8\u7ef4\u62a4\u5355\u5143\u6d4b\u8bd5\u6548\u7387\u4f4e\u4e0b\u4e14\u5b58\u5728\u5ef6\u8fdf\u4fee\u590d\u98ce\u9669\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6d4b\u8bd5\u4fee\u590d\u800c\u5ffd\u89c6\u6d4b\u8bd5\u589e\u5f3a\uff0c\u4e14\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u4e0a\u4e0b\u6587\u6536\u96c6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u4ee3\u7801\u53d8\u66f4\u5e76\u4ea7\u751f\u4f4e\u6b63\u786e\u6027\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "method": "\u9996\u5148\u5229\u7528LLM\u5206\u6790\u4ee3\u7801\u53d8\u66f4\u5e76\u8bc6\u522b\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u9010\u6b65\u6307\u5bfcLLM\u5904\u7406\u5404\u7c7b\u4ee3\u7801\u53d8\u66f4\u548c\u5f15\u5165\u65b0\u4f9d\u8d56\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u4fee\u590d\u548c\u589e\u5f3a\uff0c\u6700\u540e\u5f15\u5165\u57fa\u4e8e\u9519\u8bef\u7c7b\u578b\u7684\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u6267\u884c\u66f4\u65b0\u540e\u7684\u6d4b\u8bd5\u5e76\u4fee\u590d\u5931\u8d25\u3002", "result": "\u5728\u65b0\u5efa\u57fa\u51c6UPDATES4J\u4e0a\uff0cTESTUPDATER\u5b9e\u73b0\u4e8694.4%\u7684\u7f16\u8bd1\u901a\u8fc7\u7387\u548c86.7%\u7684\u6d4b\u8bd5\u901a\u8fc7\u7387\uff0c\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5SYNTER\u9ad8\u51fa15.9%\u548c20.0%\uff0c\u5206\u652f\u8986\u76d6\u7387\u548c\u884c\u8986\u76d6\u7387\u5206\u522b\u9ad8\u51fa12.9%\u548c15.2%\u3002", "conclusion": "TESTUPDATER\u901a\u8fc7\u7ed3\u5408LLM\u7684\u4e0a\u4e0b\u6587\u5206\u6790\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6d4b\u8bd5\u7ef4\u62a4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6d4b\u8bd5\u66f4\u65b0\u6b63\u786e\u6027\u548c\u8986\u76d6\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2509.23248", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.23248", "abs": "https://arxiv.org/abs/2509.23248", "authors": ["Mingyi Luo", "Ruichen Zhang", "Xiangwang Hou", "Jun Du", "Chunxiao Jiang", "Yong Ren", "Dusit Niyato", "Shiwen Mao"], "title": "Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled an\nemergence of agentic artificial intelligence (AI) with powerful reasoning and\nautonomous decision-making capabilities. This integration with edge computing\nhas led to the development of Mobile Edge General Intelligence (MEGI), which\nbrings real-time, privacy-preserving reasoning to the network edge. However,\ndeploying LLM-based agentic AI reasoning in MEGI environments poses significant\nchallenges due to the high computational demands of reasoning and the limited\nresources of edge devices. To address these challenges, we propose a joint\noptimization framework for efficient LLM reasoning deployment in MEGI. First,\nwe review methods that enhance LLM reasoning capabilities, such as\nChain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of\nExperts (MoE). Next, we present a distributed framework that addresses two\ncorrelated aspects: reasoning enhancement through adaptive CoT prompting and\nscalable deployment through distributed MoE architecture. The framework\ndynamically activates expert networks and adjusts reasoning depth based on task\ncomplexity and device capabilities. We further conduct experimental evaluations\nin mobile edge environments. Experimental results demonstrate the framework's\neffectiveness in balancing reasoning quality with resource efficiency,\nvalidating the practical viability of deploying sophisticated LLM reasoning\ncapabilities in resource-constrained MEGI environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u8fb9\u7f18\u901a\u7528\u667a\u80fd(MEGI)\u73af\u5883\u4e2d\u9ad8\u6548\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53AI\u63a8\u7406\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u5206\u5e03\u5f0f\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u6765\u5e73\u8861\u63a8\u7406\u8d28\u91cf\u4e0e\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u5728MEGI\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53AI\u63a8\u7406\u9762\u4e34\u8ba1\u7b97\u9700\u6c42\u9ad8\u4e0e\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u63a8\u7406\u6548\u7387\u4e0e\u8d44\u6e90\u7ea6\u675f\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u901a\u8fc7\u81ea\u9002\u5e94CoT\u63d0\u793a\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff1b2) \u901a\u8fc7\u5206\u5e03\u5f0fMoE\u67b6\u6784\u5b9e\u73b0\u53ef\u6269\u5c55\u90e8\u7f72\uff1b3) \u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u8bbe\u5907\u80fd\u529b\u52a8\u6001\u6fc0\u6d3b\u4e13\u5bb6\u7f51\u7edc\u548c\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\u8be5\u6846\u67b6\u5728\u79fb\u52a8\u8fb9\u7f18\u73af\u5883\u4e2d\u80fd\u6709\u6548\u5e73\u8861\u63a8\u7406\u8d28\u91cf\u4e0e\u8d44\u6e90\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684MEGI\u73af\u5883\u4e2d\u90e8\u7f72\u590d\u6742LLM\u63a8\u7406\u80fd\u529b\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u8054\u5408\u4f18\u5316\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6548\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u80fd\u529b\u4e0e\u8d44\u6e90\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2509.24507", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24507", "abs": "https://arxiv.org/abs/2509.24507", "authors": ["Qinglin Wang", "Zhihong Sun", "Ruyun Wang", "Tao Huang", "Zhi Jin", "Ge Li", "Chen Lyu"], "title": "SemGuard: Real-Time Semantic Evaluator for Correcting LLM-Generated Code", "comment": "Accepted by the 40th IEEE/ACM Automated Software Engineering\n  Conference (ASE 2025)", "summary": "Large Language Models (LLMs) can translate natural language requirements into\ncode, yet empirical analyses of representative models reveal that semantic\nerrors-programs that compile but behave incorrectly-constitute the majority of\nobserved faults (e.g., >60% on DeepSeek-Coder-6.7B and QwenCoder-7B). Post-hoc\nrepair pipelines detect such faults only after execution, incurring latency,\nrelying on incomplete test suites, and often mis-localizing the defect. Since\nsemantic drift originates in the autoregressive decoding process, intervening\nwhile the code is being generated is a direct way to stop error propagation.\nConstrained-decoding approaches such as ROCODE attempt this, but still wait\nuntil the entire program runs to obtain feedback and use entropy heuristics\nthat do not truly capture semantics. A more effective solution must inject\nsemantic signals-early and precisely-into the decoding process.We present\nSemGuard, a semantic-evaluator-driven framework that performs real-time,\nline-level semantic supervision. To train the evaluator, we build SemDiff, the\nfirst dataset with fine-grained annotations that mark the exact line where a\ncorrect and an incorrect implementation diverge. The evaluator, once embedded\nin the LLM's decoder, flags deviations on partial code, rolls back to the\nfaulty line, and guides regeneration-without executing the program or requiring\ntest cases. Across four benchmarks, SemGuard consistently outperforms\nstate-of-the-art baselines. It lowers the semantic error rate by 19.86% on\nSemDiff relative to ROCODE, and lifts Pass@1 by 48.92% on the real-world\nLiveCodeBench with CodeLlama-7B. Similar gains hold for StarCoder2-7B on MBPP\nand for DeepSeekCoder-6.7B on the Java benchmark SemDiff-Java, demonstrating\nmodel- and language-agnostic effectiveness.", "AI": {"tldr": "SemGuard\u662f\u4e00\u4e2a\u8bed\u4e49\u8bc4\u4f30\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u884c\u7ea7\u8bed\u4e49\u76d1\u7763\u6765\u51cf\u5c11LLM\u751f\u6210\u4ee3\u7801\u65f6\u7684\u8bed\u4e49\u9519\u8bef\u3002\u5b83\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u8bed\u4e49\u4fe1\u53f7\uff0c\u65e0\u9700\u6267\u884c\u7a0b\u5e8f\u6216\u6d4b\u8bd5\u7528\u4f8b\u5c31\u80fd\u68c0\u6d4b\u548c\u4fee\u590d\u9519\u8bef\u3002", "motivation": "LLM\u751f\u6210\u7684\u4ee3\u7801\u4e2d\u8bed\u4e49\u9519\u8bef\uff08\u7a0b\u5e8f\u80fd\u7f16\u8bd1\u4f46\u884c\u4e3a\u9519\u8bef\uff09\u5360\u5927\u591a\u6570\uff0c\u800c\u73b0\u6709\u7684\u4fee\u590d\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u3001\u4f9d\u8d56\u4e0d\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\u548c\u9519\u8bef\u5b9a\u4f4d\u4e0d\u51c6\u7684\u95ee\u9898\u3002\u9700\u8981\u5728\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b\u4e2d\u65e9\u671f\u6ce8\u5165\u8bed\u4e49\u4fe1\u53f7\u6765\u963b\u6b62\u9519\u8bef\u4f20\u64ad\u3002", "method": "\u6784\u5efaSemDiff\u6570\u636e\u96c6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u8bad\u7ec3\u8bed\u4e49\u8bc4\u4f30\u5668\u5d4c\u5165LLM\u89e3\u7801\u5668\uff0c\u5b9e\u65f6\u76d1\u63a7\u90e8\u5206\u4ee3\u7801\uff0c\u68c0\u6d4b\u504f\u5dee\u540e\u56de\u6eda\u5230\u9519\u8bef\u884c\u5e76\u6307\u5bfc\u91cd\u65b0\u751f\u6210\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemGuard\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u3002\u5728SemDiff\u4e0a\u6bd4ROCODE\u964d\u4f4e\u8bed\u4e49\u9519\u8bef\u738719.86%\uff0c\u5728LiveCodeBench\u4e0a\u4f7f\u7528CodeLlama-7B\u63d0\u5347Pass@1 48.92%\u3002\u5728MBPP\u548cSemDiff-Java\u4e0a\u4e5f\u8868\u73b0\u51fa\u6a21\u578b\u548c\u8bed\u8a00\u65e0\u5173\u7684\u6709\u6548\u6027\u3002", "conclusion": "SemGuard\u901a\u8fc7\u5b9e\u65f6\u8bed\u4e49\u76d1\u7763\u6709\u6548\u51cf\u5c11\u4e86LLM\u751f\u6210\u4ee3\u7801\u65f6\u7684\u8bed\u4e49\u9519\u8bef\uff0c\u5c55\u793a\u4e86\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u65e9\u671f\u6ce8\u5165\u8bed\u4e49\u4fe1\u53f7\u7684\u53ef\u884c\u6027\u3002", "topic": "code agent"}}
{"id": "2509.23040", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23040", "abs": "https://arxiv.org/abs/2509.23040", "authors": ["Yaorui Shi", "Yuxin Chen", "Siyuan Wang", "Sihang Li", "Hengxing Cai", "Qi Gu", "Xiang Wang", "An Zhang"], "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents", "comment": null, "summary": "Large language models face challenges in long-context question answering,\nwhere key evidence of a query may be dispersed across millions of tokens.\nExisting works equip large language models with a memory corpus that is\ndynamically updated during a single-pass document scan, also known as the\n\"memorize while reading\" methods. While this approach scales efficiently, it\nsuffers from irreversible forward-only processing, information loss through\noverwriting, and sparse reinforcement learning signals. To tackle these\nchallenges, we present ReMemR1, a memory-augmented agent with callback-enhanced\nmemory that allows selective retrieval from the entire memory history and\nallows non-linear reasoning and revisiting of early evidence. To further\nstrengthen training, we propose Reinforcement Learning with Multi-Level Rewards\n(RLMLR), which combines final-answer rewards with dense, step-level signals\nthat guide effective memory use. Together, these contributions mitigate\ninformation degradation, improve supervision, and support multi-hop memory\nutilizing. Experiments on long-document QA show significant gains over existing\nmemory-based approaches, which validates ReMemR1 as an effective solution for\nlong-context reasoning agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReMemR1\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\uff0c\u901a\u8fc7\u56de\u8c03\u589e\u5f3a\u7684\u8bb0\u5fc6\u673a\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u591a\u7ea7\u5956\u52b1\u6765\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4e2d\u7684\u4fe1\u606f\u5206\u6563\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4e2d\u9762\u4e34\u5173\u952e\u8bc1\u636e\u5206\u6563\u5728\u6570\u767e\u4e07token\u4e2d\u7684\u6311\u6218\uff0c\u73b0\u6709\u7684\"\u8fb9\u8bfb\u8fb9\u8bb0\"\u65b9\u6cd5\u5b58\u5728\u524d\u5411\u5904\u7406\u4e0d\u53ef\u9006\u3001\u4fe1\u606f\u8986\u76d6\u4e22\u5931\u548c\u7a00\u758f\u5f3a\u5316\u5b66\u4e60\u4fe1\u53f7\u7b49\u95ee\u9898\u3002", "method": "ReMemR1\u91c7\u7528\u56de\u8c03\u589e\u5f3a\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5141\u8bb8\u4ece\u6574\u4e2a\u8bb0\u5fc6\u5386\u53f2\u4e2d\u9009\u62e9\u6027\u68c0\u7d22\uff0c\u652f\u6301\u975e\u7ebf\u6027\u63a8\u7406\u548c\u65e9\u671f\u8bc1\u636e\u91cd\u8bbf\u3002\u540c\u65f6\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u591a\u7ea7\u5956\u52b1(RLMLR)\uff0c\u7ed3\u5408\u6700\u7ec8\u7b54\u6848\u5956\u52b1\u548c\u5bc6\u96c6\u7684\u6b65\u9aa4\u7ea7\u4fe1\u53f7\u6765\u6307\u5bfc\u6709\u6548\u7684\u8bb0\u5fc6\u4f7f\u7528\u3002", "result": "\u5728\u957f\u6587\u6863\u95ee\u7b54\u5b9e\u9a8c\u4e0a\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86ReMemR1\u4f5c\u4e3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4ee3\u7406\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ReMemR1\u901a\u8fc7\u6539\u8fdb\u7684\u8bb0\u5fc6\u673a\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u4fe1\u606f\u9000\u5316\u95ee\u9898\uff0c\u6539\u5584\u4e86\u76d1\u7763\u4fe1\u53f7\uff0c\u652f\u6301\u591a\u8df3\u8bb0\u5fc6\u5229\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.23263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23263", "abs": "https://arxiv.org/abs/2509.23263", "authors": ["Tao Xiong", "Xavier Hu", "Yurun Chen", "Yuhang Liu", "Changqiao Wu", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Shengyu Zhang"], "title": "GUI-PRA: Process Reward Agent for GUI Tasks", "comment": null, "summary": "Graphical User Interface (GUI) Agents powered by Multimodal Large Language\nModels (MLLMs) show significant potential for automating tasks. However, they\noften struggle with long-horizon tasks, leading to frequent failures. Process\nReward Models (PRMs) are a promising solution, as they can guide these agents\nwith crucial process signals during inference. Nevertheless, their application\nto the GUI domain presents unique challenges. When processing dense artificial\ninputs with long history data, PRMs suffer from a \"lost in the middle\"\nphenomenon, where the overwhelming historical context compromises the\nevaluation of the current step. Furthermore, standard PRMs lacks GUI changing\nawareness, providing static evaluations that are disconnected from the dynamic\nconsequences of actions, a critical mismatch with the inherently dynamic nature\nof GUI tasks. In response to these challenges, we introduce GUI-PRA (Process\nReward Agent for GUI Tasks), a judge agent designed to better provide process\nreward than standard PRM by intelligently processing historical context and\nactively perceiving UI state changes. Specifically, to directly combat the\n``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism\nconsisting of two core components: a Relevance-based Retrieval Module to\nactively fetch pertinent information from long histories and a Progressive\nSummarization Module to dynamically condense growing interaction data, ensuring\nthe model focuses on relevant context. Moreover, to address the lack of UI\nchanging awareness, we introduce an Aadaptive UI Perception mechanism. This\nmechanism enables the agent to reason about UI state changes and dynamically\nselect the most appropriate tool to gather grounded visual evidence, ensuring\nits evaluation is always informed by the current UI context.", "AI": {"tldr": "\u63d0\u51fa\u4e86GUI-PRA\uff0c\u4e00\u79cd\u9488\u5bf9GUI\u4efb\u52a1\u7684\u6d41\u7a0b\u5956\u52b1\u4ee3\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u8bb0\u5fc6\u673a\u5236\u548c\u81ea\u9002\u5e94UI\u611f\u77e5\u6765\u89e3\u51b3\u6807\u51c6\u6d41\u7a0b\u5956\u52b1\u6a21\u578b\u5728GUI\u9886\u57df\u4e2d\u7684'\u4e2d\u95f4\u8ff7\u5931'\u95ee\u9898\u548c\u7f3a\u4e4fUI\u53d8\u5316\u611f\u77e5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684GUI\u4ee3\u7406\u5728\u5904\u7406\u957f\u89c6\u91ce\u4efb\u52a1\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u6807\u51c6\u6d41\u7a0b\u5956\u52b1\u6a21\u578b\u5728\u5904\u7406\u5bc6\u96c6\u4eba\u5de5\u8f93\u5165\u548c\u957f\u5386\u53f2\u6570\u636e\u65f6\u5b58\u5728'\u4e2d\u95f4\u8ff7\u5931'\u73b0\u8c61\uff0c\u4e14\u7f3a\u4e4f\u5bf9GUI\u52a8\u6001\u53d8\u5316\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5f15\u5165GUI-PRA\uff0c\u5305\u542b\u52a8\u6001\u8bb0\u5fc6\u673a\u5236\uff08\u76f8\u5173\u6027\u68c0\u7d22\u6a21\u5757\u548c\u6e10\u8fdb\u603b\u7ed3\u6a21\u5757\uff09\u548c\u81ea\u9002\u5e94UI\u611f\u77e5\u673a\u5236\uff0c\u80fd\u591f\u667a\u80fd\u5904\u7406\u5386\u53f2\u4e0a\u4e0b\u6587\u5e76\u4e3b\u52a8\u611f\u77e5UI\u72b6\u6001\u53d8\u5316\u3002", "result": "GUI-PRA\u80fd\u591f\u66f4\u597d\u5730\u63d0\u4f9b\u6d41\u7a0b\u5956\u52b1\uff0c\u901a\u8fc7\u805a\u7126\u76f8\u5173\u4e0a\u4e0b\u6587\u548c\u57fa\u4e8e\u5f53\u524dUI\u73af\u5883\u7684\u8bc4\u4f30\u6765\u6307\u5bfcGUI\u4ee3\u7406\u3002", "conclusion": "GUI-PRA\u901a\u8fc7\u89e3\u51b3\u5386\u53f2\u4e0a\u4e0b\u6587\u5904\u7406\u548cUI\u53d8\u5316\u611f\u77e5\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2509.24515", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.24515", "abs": "https://arxiv.org/abs/2509.24515", "authors": ["Yu-Fu Fu", "Meng Xu", "Taesoo Kim"], "title": "Agentic Specification Generator for Move Programs", "comment": "18 pages; Extended version of ASE'25 paper with extra appendices", "summary": "While LLM-based specification generation is gaining traction, existing tools\nprimarily focus on mainstream programming languages like C, Java, and even\nSolidity, leaving emerging and yet verification-oriented languages like Move\nunderexplored. In this paper, we introduce MSG, an automated specification\ngeneration tool designed for Move smart contracts. MSG aims to highlight key\ninsights that uniquely present when applying LLM-based specification generation\nto a new ecosystem. Specifically, MSG demonstrates that LLMs exhibit robust\ncode comprehension and generation capabilities even for non-mainstream\nlanguages. MSG successfully generates verifiable specifications for 84% of\ntested Move functions and even identifies clauses previously overlooked by\nexperts. Additionally, MSG shows that explicitly leveraging specification\nlanguage features through an agentic, modular design improves specification\nquality substantially (generating 57% more verifiable clauses than conventional\ndesigns). Incorporating feedback from the verification toolchain further\nenhances the effectiveness of MSG, leading to a 30% increase in generated\nverifiable specifications.", "AI": {"tldr": "MSG\u662f\u4e00\u4e2a\u4e3aMove\u667a\u80fd\u5408\u7ea6\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u89c4\u8303\u751f\u6210\u5de5\u5177\uff0c\u5c55\u793a\u4e86LLM\u5728\u975e\u4e3b\u6d41\u8bed\u8a00\u4e2d\u7684\u5f3a\u5927\u4ee3\u7801\u7406\u89e3\u548c\u89c4\u8303\u751f\u6210\u80fd\u529b\uff0c\u6210\u529f\u4e3a84%\u7684\u6d4b\u8bd5\u51fd\u6570\u751f\u6210\u53ef\u9a8c\u8bc1\u89c4\u8303\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u5173\u6ce8\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00\uff0c\u800c\u65b0\u5174\u4e14\u9762\u5411\u9a8c\u8bc1\u7684\u8bed\u8a00\u5982Move\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u660e\u786e\u5229\u7528\u89c4\u8303\u8bed\u8a00\u7279\u6027\uff0c\u5e76\u6574\u5408\u9a8c\u8bc1\u5de5\u5177\u94fe\u7684\u53cd\u9988\u673a\u5236\u3002", "result": "\u6210\u529f\u4e3a84%\u7684Move\u51fd\u6570\u751f\u6210\u53ef\u9a8c\u8bc1\u89c4\u8303\uff0c\u8bc6\u522b\u51fa\u4e13\u5bb6\u9057\u6f0f\u7684\u6761\u6b3e\uff0c\u6bd4\u4f20\u7edf\u8bbe\u8ba1\u591a\u751f\u621057%\u7684\u53ef\u9a8c\u8bc1\u6761\u6b3e\uff0c\u6574\u5408\u53cd\u9988\u540e\u751f\u6210\u7684\u53ef\u9a8c\u8bc1\u89c4\u8303\u589e\u52a030%\u3002", "conclusion": "LLM\u5728\u975e\u4e3b\u6d41\u8bed\u8a00\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u89c4\u8303\u751f\u6210\u80fd\u529b\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u9a8c\u8bc1\u53cd\u9988\u80fd\u663e\u8457\u63d0\u5347\u89c4\u8303\u8d28\u91cf\u3002", "topic": "swe application"}}
{"id": "2509.23055", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23055", "abs": "https://arxiv.org/abs/2509.23055", "authors": ["Binwei Yao", "Chao Shang", "Wanyu Du", "Jianfeng He", "Ruixue Lian", "Yi Zhang", "Hang Su", "Sandesh Swamy", "Yanjun Qi"], "title": "Peacemaker or Troublemaker: How Sycophancy Shapes Multi-Agent Debate", "comment": null, "summary": "Large language models (LLMs) often display sycophancy, a tendency toward\nexcessive agreeability. This behavior poses significant challenges for\nmulti-agent debating systems (MADS) that rely on productive disagreement to\nrefine arguments and foster innovative thinking. LLMs' inherent sycophancy can\ncollapse debates into premature consensus, potentially undermining the benefits\nof multi-agent debate. While prior studies focus on user--LLM sycophancy, the\nimpact of inter-agent sycophancy in debate remains poorly understood. To\naddress this gap, we introduce the first operational framework that (1)\nproposes a formal definition of sycophancy specific to MADS settings, (2)\ndevelops new metrics to evaluate the agent sycophancy level and its impact on\ninformation exchange in MADS, and (3) systematically investigates how varying\nlevels of sycophancy across agent roles (debaters and judges) affects outcomes\nin both decentralized and centralized debate frameworks. Our findings reveal\nthat sycophancy is a core failure mode that amplifies disagreement collapse\nbefore reaching a correct conclusion in multi-agent debates, yields lower\naccuracy than single-agent baselines, and arises from distinct debater-driven\nand judge-driven failure modes. Building on these findings, we propose\nactionable design principles for MADS, effectively balancing productive\ndisagreement with cooperation in agent interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u5949\u627f\u884c\u4e3a\u7684\u64cd\u4f5c\u6846\u67b6\uff0c\u5305\u62ec\u6b63\u5f0f\u5b9a\u4e49\u3001\u8bc4\u4f30\u6307\u6807\u548c\u7cfb\u7edf\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5949\u627f\u884c\u4e3a\u662f\u5bfc\u81f4\u8fa9\u8bba\u5931\u8d25\u7684\u6838\u5fc3\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8fc7\u5ea6\u5949\u627f\u7684\u503e\u5411\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8fa9\u8bba\u8fc7\u65e9\u8fbe\u6210\u5171\u8bc6\uff0c\u524a\u5f31\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u4f18\u52bf\u3002\u76ee\u524d\u5bf9\u667a\u80fd\u4f53\u95f4\u5949\u627f\u884c\u4e3a\u5728\u8fa9\u8bba\u4e2d\u7684\u5f71\u54cd\u4e86\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u64cd\u4f5c\u6846\u67b6\uff1a1\uff09\u4e3a\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u5236\u5b9a\u5949\u627f\u884c\u4e3a\u7684\u6b63\u5f0f\u5b9a\u4e49\uff1b2\uff09\u5f00\u53d1\u65b0\u6307\u6807\u8bc4\u4f30\u667a\u80fd\u4f53\u5949\u627f\u6c34\u5e73\u53ca\u5176\u5bf9\u4fe1\u606f\u4ea4\u6362\u7684\u5f71\u54cd\uff1b3\uff09\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u89d2\u8272\u667a\u80fd\u4f53\u7684\u5949\u627f\u6c34\u5e73\u5982\u4f55\u5f71\u54cd\u5206\u6563\u5f0f\u548c\u96c6\u4e2d\u5f0f\u8fa9\u8bba\u6846\u67b6\u7684\u7ed3\u679c\u3002", "result": "\u53d1\u73b0\u5949\u627f\u884c\u4e3a\u662f\u6838\u5fc3\u5931\u8d25\u6a21\u5f0f\uff0c\u4f1a\u5728\u8fbe\u6210\u6b63\u786e\u7ed3\u8bba\u524d\u653e\u5927\u5206\u6b67\u5d29\u6e83\uff0c\u5bfc\u81f4\u51c6\u786e\u7387\u4f4e\u4e8e\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5e76\u6e90\u4e8e\u4e0d\u540c\u7684\u8fa9\u624b\u9a71\u52a8\u548c\u6cd5\u5b98\u9a71\u52a8\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u53ef\u64cd\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u8bbe\u8ba1\u539f\u5219\uff0c\u6709\u6548\u5e73\u8861\u667a\u80fd\u4f53\u4e92\u52a8\u4e2d\u7684\u751f\u4ea7\u6027\u5206\u6b67\u4e0e\u5408\u4f5c\u3002", "topic": "agent analysis"}}
{"id": "2509.23270", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23270", "abs": "https://arxiv.org/abs/2509.23270", "authors": ["Yuxinyue Qian", "Jun Liu"], "title": "Socio-Economic Model of AI Agents", "comment": null, "summary": "Modern socio-economic systems are undergoing deep integration with artificial\nintelligence technologies. This paper constructs a heterogeneous agent-based\nmodeling framework that incorporates both human workers and autonomous AI\nagents, to study the impact of AI collaboration under resource constraints on\naggregate social output. We build five progressively extended models: Model 1\nserves as the baseline of pure human collaboration; Model 2 introduces AI as\ncollaborators; Model 3 incorporates network effects among agents; Model 4\ntreats agents as independent producers; and Model 5 integrates both network\neffects and independent agent production. Through theoretical derivation and\nsimulation analysis, we find that the introduction of AI agents can\nsignificantly increase aggregate social output. When considering network\neffects among agents, this increase exhibits nonlinear growth far exceeding the\nsimple sum of individual contributions. Under the same resource inputs,\ntreating agents as independent producers provides higher long-term growth\npotential; introducing network effects further demonstrates strong\ncharacteristics of increasing returns to scale.", "AI": {"tldr": "\u6784\u5efa\u5305\u542b\u4eba\u7c7b\u5de5\u4eba\u548c\u81ea\u4e3bAI\u4ee3\u7406\u7684\u5f02\u6784\u591a\u4ee3\u7406\u6a21\u578b\uff0c\u7814\u7a76AI\u534f\u4f5c\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u5bf9\u793e\u4f1a\u603b\u4ea7\u51fa\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u5347\u4ea7\u51fa\uff0c\u7f51\u7edc\u6548\u5e94\u5e26\u6765\u975e\u7ebf\u6027\u589e\u957f\u3002", "motivation": "\u7814\u7a76AI\u6280\u672f\u6df1\u5ea6\u878d\u5165\u793e\u4f1a\u7ecf\u6d4e\u7cfb\u7edf\u65f6\uff0cAI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u5bf9\u793e\u4f1a\u603b\u4ea7\u51fa\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u6784\u5efa\u4e94\u4e2a\u9010\u6b65\u6269\u5c55\u7684\u5f02\u6784\u4ee3\u7406\u6a21\u578b\uff1a\u7eaf\u4eba\u7c7b\u534f\u4f5c\u57fa\u7ebf\u3001AI\u534f\u4f5c\u3001\u7f51\u7edc\u6548\u5e94\u3001\u72ec\u7acb\u751f\u4ea7\u8005\u3001\u7f51\u7edc\u6548\u5e94+\u72ec\u7acb\u751f\u4ea7\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u4eff\u771f\u5206\u6790\u3002", "result": "AI\u4ee3\u7406\u5f15\u5165\u663e\u8457\u589e\u52a0\u793e\u4f1a\u603b\u4ea7\u51fa\uff1b\u8003\u8651\u7f51\u7edc\u6548\u5e94\u65f6\u589e\u957f\u5448\u975e\u7ebf\u6027\uff1b\u72ec\u7acb\u751f\u4ea7\u8005\u6a21\u5f0f\u63d0\u4f9b\u66f4\u9ad8\u957f\u671f\u589e\u957f\u6f5c\u529b\uff1b\u7f51\u7edc\u6548\u5e94\u5c55\u73b0\u89c4\u6a21\u62a5\u916c\u9012\u589e\u7279\u6027\u3002", "conclusion": "AI\u534f\u4f5c\u80fd\u6709\u6548\u63d0\u5347\u793e\u4f1a\u7ecf\u6d4e\u4ea7\u51fa\uff0c\u7f51\u7edc\u6548\u5e94\u548c\u72ec\u7acb\u751f\u4ea7\u6a21\u5f0f\u662f\u5173\u952e\u7684\u589e\u6548\u673a\u5236\uff0c\u4e3aAI\u878d\u5165\u793e\u4f1a\u7ecf\u6d4e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "topic": "agent analysis"}}
{"id": "2509.24637", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24637", "abs": "https://arxiv.org/abs/2509.24637", "authors": ["Zhensu Sun", "Chengran Yang", "Chao Peng", "Pengfei Gao", "Xiaoning Du", "Li Li", "David Lo"], "title": "Bridging Developer Instructions and Code Completion Through Instruction-Aware Fill-in-the-Middle Paradigm", "comment": "10 pages", "summary": "Large Language Models (LLMs) have significantly advanced code completion, yet\nthey often fail when the developer's intent is underspecified in the code\ncontext. To address this, developers usually add natural language instructions\n(e.g., comments) into the code context to clarify their intent. However,\nexisting code LLMs applied for code completion systems merely undergo a\nfill-in-the-middle (FIM) pre-training, which struggles to leverage this\ninformation effectively due to the lack of instruction-like training data.\nExisting instruction-tuning techniques, which improve instruction-following in\ngeneral code generation, paradoxically degrade FIM performance, forcing a\ntrade-off between instruction-following and infilling capabilities. To address\nthis gap, we introduce Instruction-aware Fill-in-the-Middle (IFIM), an\ninstruction-tuning method specifically designed to enhance FIM code completion\nmodels. IFIM extends the conventional FIM training objective by incorporating\nan explicit instruction section into the input, enabling the model to learn\nfrom (prefix, instruction, suffix) triplets. This approach allows the model to\neffectively leverage developer-provided directives while preserving its core\ncompletion abilities when no instructions are present. To facilitate this, we\nconstructed a large-scale dataset by using GPT-4o to generate concise,\nintent-focused instructions for code infilling examples. We evaluated IFIM by\napplying it to two popular base models, Deepseek-Coder and Qwen2.5-Coder, on\nthe benchmarks derived from HumanEval-infilling and RepoMasterEval. The results\ndemonstrate that IFIM significantly improves instruction-following\ncapabilities, boosting the Pass@1 score from 84.6% to 93.6% on\nHumanEval-infilling. Moreover, this enhancement does not compromise the models'\noriginal performance on FIM code completion tasks with no instructions\nprovided.", "AI": {"tldr": "\u63d0\u51fa\u4e86IFIM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u589e\u5f3aFIM\u4ee3\u7801\u8865\u5168\u6a21\u578b\uff0c\u5728\u4fdd\u7559\u539f\u6709\u8865\u5168\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801LLM\u4ec5\u7ecf\u8fc7FIM\u9884\u8bad\u7ec3\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528\u5f00\u53d1\u8005\u63d0\u4f9b\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6765\u6f84\u6e05\u610f\u56fe\uff0c\u800c\u4f20\u7edf\u6307\u4ee4\u8c03\u4f18\u6280\u672f\u4f1a\u964d\u4f4eFIM\u6027\u80fd\u3002", "method": "IFIM\u65b9\u6cd5\u5728\u4f20\u7edfFIM\u8bad\u7ec3\u76ee\u6807\u57fa\u7840\u4e0a\uff0c\u5728\u8f93\u5165\u4e2d\u663e\u5f0f\u52a0\u5165\u6307\u4ee4\u90e8\u5206\uff0c\u8ba9\u6a21\u578b\u4ece(\u524d\u7f00\u3001\u6307\u4ee4\u3001\u540e\u7f00)\u4e09\u5143\u7ec4\u4e2d\u5b66\u4e60\u3002", "result": "\u5728HumanEval-infilling\u57fa\u51c6\u4e0a\uff0cPass@1\u5206\u6570\u4ece84.6%\u63d0\u5347\u81f393.6%\uff0c\u4e14\u4e0d\u635f\u5bb3\u65e0\u6307\u4ee4\u65f6\u7684\u539f\u59cbFIM\u6027\u80fd\u3002", "conclusion": "IFIM\u6210\u529f\u89e3\u51b3\u4e86\u6307\u4ee4\u8ddf\u968f\u4e0e\u4ee3\u7801\u586b\u5145\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u4ee3\u7801\u8865\u5168\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2509.23071", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23071", "abs": "https://arxiv.org/abs/2509.23071", "authors": ["Muzhi Li", "Jinhu Qi", "Yihong Wu", "Minghao Zhao", "Liheng Ma", "Yifan Li", "Xinyu Wang", "Yingxue Zhang", "Ho-fung Leung", "Irwin King"], "title": "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents", "comment": null, "summary": "Retrieval-augmented generation agents development is hindered by the lack of\nprocess-level supervision to effectively guide agentic capabilities like task\ndecomposition, retriever invocation, and stepwise decision-making. While\nreinforcement learning offers a potential solution, it suffers from sparse\nrewards and the limited reasoning capabilities of large language models (LLMs).\nMeanwhile, existing data synthesis methods only produce chain-of-thought\nrationales and fail to model environmental interactions. In this paper, we\npropose EviPath, an evidence-anchored reasoning path synthesis paradigm for RAG\nagent development. EviPath comprises: (i) Abductive Subtask Planning, which\ndecomposes the problem into sub-questions and iteratively plans an optimal\nsolution path based on the dependencies between them; (ii) Faithful\nSub-question Answering, which uses supporting evidence to construct a proxy\nenvironment to generate reasoning thoughts and answers for each sub-question;\nand (iii) Conversational Fine-Tuning, which formats the complete\nagent-environment interaction trajectory into a dialogue format suitable for\nSupervised Fine-Tuning. EviPath allows LLMs to learn complex reasoning and\ntool-use capabilities directly from synthesized data. Extensive experiments on\nwidely-used question-answering benchmarks show that an 8B parameter model\ntrained with EviPath-synthesized data significantly and consistently\noutperforms state-of-the-art baselines with a double-digit absolute EM gain of\n14.7% in open-domain question answering.", "AI": {"tldr": "EviPath\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bc1\u636e\u951a\u5b9a\u7684\u63a8\u7406\u8def\u5f84\u5408\u6210\u8303\u5f0f\uff0c\u7528\u4e8e\u89e3\u51b3RAG\u4ee3\u7406\u5f00\u53d1\u4e2d\u8fc7\u7a0b\u7ea7\u76d1\u7763\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u3001\u5fe0\u5b9e\u5b50\u95ee\u9898\u56de\u7b54\u548c\u5bf9\u8bdd\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709RAG\u4ee3\u7406\u5f00\u53d1\u7f3a\u4e4f\u8fc7\u7a0b\u7ea7\u76d1\u7763\uff0c\u96be\u4ee5\u6709\u6548\u6307\u5bfc\u4efb\u52a1\u5206\u89e3\u3001\u68c0\u7d22\u5668\u8c03\u7528\u548c\u9010\u6b65\u51b3\u7b56\u7b49\u80fd\u529b\u3002\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u601d\u7ef4\u94fe\u800c\u65e0\u6cd5\u5efa\u6a21\u73af\u5883\u4ea4\u4e92\u3002", "method": "EviPath\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u6eaf\u56e0\u5b50\u4efb\u52a1\u89c4\u5212\uff1a\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u5e76\u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u8fed\u4ee3\u89c4\u5212\u6700\u4f18\u89e3\u8def\u5f84\uff1b(2)\u5fe0\u5b9e\u5b50\u95ee\u9898\u56de\u7b54\uff1a\u4f7f\u7528\u652f\u6301\u8bc1\u636e\u6784\u5efa\u4ee3\u7406\u73af\u5883\u6765\u751f\u6210\u63a8\u7406\u601d\u8def\u548c\u7b54\u6848\uff1b(3)\u5bf9\u8bdd\u5fae\u8c03\uff1a\u5c06\u5b8c\u6574\u7684\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u8f68\u8ff9\u683c\u5f0f\u5316\u4e3a\u9002\u5408\u76d1\u7763\u5fae\u8c03\u7684\u5bf9\u8bdd\u683c\u5f0f\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528EviPath\u5408\u6210\u6570\u636e\u8bad\u7ec3\u76848B\u53c2\u6570\u6a21\u578b\u663e\u8457\u4e14\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5f00\u653e\u57df\u95ee\u7b54\u4e2d\u5b9e\u73b0\u4e8614.7%\u7684\u7edd\u5bf9EM\u589e\u76ca\u3002", "conclusion": "EviPath\u4f7fLLM\u80fd\u591f\u76f4\u63a5\u4ece\u5408\u6210\u6570\u636e\u4e2d\u5b66\u4e60\u590d\u6742\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u4e3aRAG\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.23392", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23392", "abs": "https://arxiv.org/abs/2509.23392", "authors": ["Jinyi Han", "Ying Huang", "Ying Liao", "Zishang Jiang", "Xikun Lu", "Haiquan Zhao", "Xinyi Wang", "Guanghao Zhou", "Sihang Jiang", "Jiaqing Liang", "Weikang Zhou", "Zeye Sun", "Fei Yu", "Yanghua Xiao"], "title": "Your Models Have Thought Enough: Training Large Reasoning Models to Stop Overthinking", "comment": null, "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on\nchallenging tasks, yet their deep reasoning often incurs substantial\ncomputational costs. To achieve efficient reasoning, existing reinforcement\nlearning methods still struggle to construct short reasoning path during the\nrollout stage, limiting effective learning. Inspired by Evidence Accumulation\nModels, we find that LRMs have accumulated sufficient information early in\nreasoning, making further reasoning steps redundant. Based on this insight, we\npropose Just-Enough Thinking (JET), which trains models to proactively\nterminate unnecessary reasoning. JET performs trajectory truncation during\nrollout to expose the model to short, distributionally consistent reasoning\npaths. Besides, it uses a quality-controlled length reward to better encourage\nconcise reasoning while maintaining correctness. Extensive experiments\ndemonstrate that JET significantly improves reasoning efficiency without\nsacrificing accuracy. Especially, DeepSeek-Distill-Qwen-1.5B achieves a 4.6%\naccuracy gain while reducing output length by 46.3% on the Olympiad benchmark.\nOur code is available in the GitHub.", "AI": {"tldr": "\u63d0\u51faJET\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u7ec8\u6b62\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\u6765\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u8def\u5f84\u957f\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6df1\u5ea6\u63a8\u7406\u4f1a\u5e26\u6765\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6784\u5efa\u77ed\u63a8\u7406\u8def\u5f84\u65b9\u9762\u4ecd\u6709\u56f0\u96be\uff0c\u9650\u5236\u4e86\u6709\u6548\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u8bc1\u636e\u79ef\u7d2f\u6a21\u578b\u7684\u6d1e\u5bdf\uff0c\u63d0\u51faJET\u65b9\u6cd5\uff1a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u7ec8\u6b62\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u901a\u8fc7\u8f68\u8ff9\u622a\u65ad\u548c\u57fa\u4e8e\u8d28\u91cf\u63a7\u5236\u7684\u957f\u5ea6\u5956\u52b1\u6765\u8bad\u7ec3\u6a21\u578b\u3002", "result": "JET\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002\u5728Olympiad\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeepSeek-Distill-Qwen-1.5B\u5b9e\u73b0\u4e864.6%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u540c\u65f6\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u4e8646.3%\u3002", "conclusion": "JET\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u7ec8\u6b62\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24828", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24828", "abs": "https://arxiv.org/abs/2509.24828", "authors": ["Joshua Heisler", "Johannes Reisinger", "Andreas Fischer"], "title": "Evaluating SAP Joule for Code Generation", "comment": null, "summary": "SAP has released its own proprietary generative model SAP Joule, intended for\nvarious generative tasks, including serving as a code assistant for software\nengineers. While Joule is yet not focused on SAP-specific ABAP code generation,\nit can be used for other common languages, including Javascript. This paper\ncompares SAP Joules Javascript coding capabilities against a total of 29 other\nmodels using the HumanEval-X Javascript benchmark. SAP Joule achieves a strict\naccuracy of 80.49% as the fifth best model in our evaluation. To the best of\nour knowledge, this is the first comparative evaluation of SAP Joule code\ngeneration capabilities.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9SAP Joule\u751f\u6210\u6a21\u578b\u5728JavaScript\u4ee3\u7801\u751f\u6210\u80fd\u529b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u5728HumanEval-X\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e94\uff0c\u51c6\u786e\u7387\u8fbe80.49%\u3002", "motivation": "SAP\u53d1\u5e03\u4e86\u4e13\u6709\u751f\u6210\u6a21\u578bJoule\u4f5c\u4e3a\u4ee3\u7801\u52a9\u624b\uff0c\u4f46\u5c1a\u672a\u9488\u5bf9SAP\u7279\u5b9a\u7684ABAP\u4ee3\u7801\u751f\u6210\uff0c\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u5176\u5728JavaScript\u7b49\u901a\u7528\u8bed\u8a00\u4e0a\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "method": "\u4f7f\u7528HumanEval-X JavaScript\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06SAP Joule\u4e0e29\u4e2a\u5176\u4ed6\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "SAP Joule\u5728\u8bc4\u4f30\u4e2d\u6392\u540d\u7b2c\u4e94\uff0c\u4e25\u683c\u51c6\u786e\u7387\u8fbe\u523080.49%\u3002", "conclusion": "SAP Joule\u5728JavaScript\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u662f\u5bf9\u5176\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u9996\u6b21\u6bd4\u8f83\u8bc4\u4f30\u3002", "topic": "code agent"}}
{"id": "2509.23415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23415", "abs": "https://arxiv.org/abs/2509.23415", "authors": ["Gyubok Lee", "Woosog Chay", "Heeyoung Kwak", "Yeong Hwa Kim", "Haanju Yoo", "Oksoon Jeong", "Meong Hi Son", "Edward Choi"], "title": "From Conversation to Query Execution: Benchmarking User and Tool Interactions for EHR Database Agents", "comment": "Under review", "summary": "Despite the impressive performance of LLM-powered agents, their adoption for\nElectronic Health Record (EHR) data access remains limited by the absence of\nbenchmarks that adequately capture real-world clinical data access flows. In\npractice, two core challenges hinder deployment: query ambiguity from vague\nuser questions and value mismatch between user terminology and database\nentries. To address this, we introduce EHR-ChatQA an interactive database\nquestion answering benchmark that evaluates the end-to-end workflow of database\nagents: clarifying user questions, using tools to resolve value mismatches, and\ngenerating correct SQL to deliver accurate answers. To cover diverse patterns\nof query ambiguity and value mismatch, EHR-ChatQA assesses agents in a\nsimulated environment with an LLM-based user across two interaction flows:\nIncremental Query Refinement (IncreQA), where users add constraints to existing\nqueries, and Adaptive Query Refinement (AdaptQA), where users adjust their\nsearch goals mid-conversation. Experiments with state-of-the-art LLMs (e.g.,\no4-mini and Gemini-2.5-Flash) over five i.i.d. trials show that while agents\nachieve high Pass@5 of 90-95% (at least one of five trials) on IncreQA and\n60-80% on AdaptQA, their Pass^5 (consistent success across all five trials) is\nsubstantially lower by 35-60%. These results underscore the need to build\nagents that are not only performant but also robust for the safety-critical EHR\ndomain. Finally, we provide diagnostic insights into common failure modes to\nguide future agent development.", "AI": {"tldr": "\u63d0\u51fa\u4e86EHR-ChatQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u8bbf\u95ee\u4e2d\u7684\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u95ee\u9898\u6f84\u6e05\u3001\u5de5\u5177\u4f7f\u7528\u548cSQL\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u80fd\u591f\u5145\u5206\u6355\u6349\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u6570\u636e\u8bbf\u95ee\u6d41\u7a0b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u963b\u788d\u4e86LLM\u4ee3\u7406\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u521b\u5efa\u4ea4\u4e92\u5f0f\u6570\u636e\u5e93\u95ee\u7b54\u57fa\u51c6EHR-ChatQA\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u7528\u6237\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\uff0c\u6db5\u76d6\u4e24\u79cd\u4ea4\u4e92\u6d41\u7a0b\uff1a\u589e\u91cf\u67e5\u8be2\u7cbe\u70bc\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4ee3\u7406\u5728IncreQA\u4e0aPass@5\u8fbe\u523090-95%\uff0cAdaptQA\u4e0a\u8fbe\u523060-80%\uff0c\u4f46Pass^5\uff08\u4e94\u6b21\u8bd5\u9a8c\u5747\u6210\u529f\uff09\u663e\u8457\u964d\u4f4e35-60%\u3002", "conclusion": "\u9700\u8981\u6784\u5efa\u4e0d\u4ec5\u6027\u80fd\u597d\u800c\u4e14\u9c81\u68d2\u7684\u4ee3\u7406\uff0c\u4ee5\u6ee1\u8db3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9886\u57df\u7684\u5b89\u5168\u5173\u952e\u9700\u6c42\u3002", "topic": "swe benchmark"}}
{"id": "2509.24975", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24975", "abs": "https://arxiv.org/abs/2509.24975", "authors": ["Lekang Yang", "Yuetong Liu", "Yitong Zhang", "Jia Li"], "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern", "comment": null, "summary": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .", "AI": {"tldr": "DiffTester\u662f\u4e00\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b(dLLMs)\u7684\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u7684\u91cd\u590d\u7ed3\u6784\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u6d4b\u8bd5\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u751f\u6210\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u9010\u4e2atoken\u751f\u6210\u6548\u7387\u4f4e\u4e0b\uff0c\u800cdLLMs\u867d\u7136\u652f\u6301\u5e76\u884c\u751f\u6210\u4f46\u9762\u4e34\u6548\u7387\u4e0e\u6d4b\u8bd5\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u62bd\u8c61\u8bed\u6cd5\u6811\u5206\u6790\u52a8\u6001\u8bc6\u522b\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u7684\u91cd\u590d\u7ed3\u6784\u6a21\u5f0f\uff0c\u81ea\u9002\u5e94\u589e\u52a0\u6bcf\u4e2a\u6b65\u9aa4\u751f\u6210\u7684token\u6570\u91cf\u800c\u4e0d\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u79cd\u4ee3\u8868\u6027\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiffTester\u5728\u4fdd\u6301\u6d4b\u8bd5\u8986\u76d6\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\uff0c\u4e14\u5728\u4e0d\u540cdLLMs\u548c\u7f16\u7a0b\u8bed\u8a00\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DiffTester\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u9ad8\u6548\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "swe application"}}
{"id": "2509.23426", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23426", "abs": "https://arxiv.org/abs/2509.23426", "authors": ["Shanghua Gao", "Richard Zhu", "Pengwei Sui", "Zhenglun Kong", "Sufian Aldogom", "Yepeng Huang", "Ayush Noori", "Reza Shamji", "Krishna Parvataneni", "Theodoros Tsiligkaridis", "Marinka Zitnik"], "title": "Democratizing AI scientists using ToolUniverse", "comment": "https://aiscientist.tools", "summary": "AI scientists are emerging computational systems that serve as collaborative\npartners in discovery. These systems remain difficult to build because they are\nbespoke, tied to rigid workflows, and lack shared environments that unify\ntools, data, and analyses into a common ecosystem. In omics, unified ecosystems\nhave transformed research by enabling interoperability, reuse, and\ncommunity-driven development; AI scientists require comparable infrastructure.\nWe present ToolUniverse, an ecosystem for building AI scientists from any\nlanguage or reasoning model, whether open or closed. TOOLUNIVERSE standardizes\nhow AI scientists identify and call tools, integrating more than 600 machine\nlearning models, datasets, APIs, and scientific packages for data analysis,\nknowledge retrieval, and experimental design. It automatically refines tool\ninterfaces for correct use by AI scientists, creates new tools from natural\nlanguage descriptions, iteratively optimizes tool specifications, and composes\ntools into agentic workflows. In a case study of hypercholesterolemia,\nToolUniverse was used to create an AI scientist to identify a potent analog of\na drug with favorable predicted properties. The open-source ToolUniverse is\navailable at https://aiscientist.tools.", "AI": {"tldr": "ToolUniverse\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efaAI\u79d1\u5b66\u5bb6\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\u548c\u63a8\u7406\u6a21\u578b\uff0c\u96c6\u6210\u4e86600\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3001\u6570\u636e\u96c6\u3001API\u548c\u79d1\u5b66\u5305\uff0c\u80fd\u591f\u81ea\u52a8\u4f18\u5316\u5de5\u5177\u63a5\u53e3\u5e76\u7ec4\u5408\u6210\u667a\u80fd\u5de5\u4f5c\u6d41\u3002", "motivation": "\u73b0\u6709\u7684AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u96be\u4ee5\u6784\u5efa\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u662f\u5b9a\u5236\u5316\u7684\u3001\u7ed1\u5b9a\u5728\u56fa\u5b9a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7edf\u4e00\u5de5\u5177\u3001\u6570\u636e\u548c\u5206\u6790\u7684\u5171\u4eab\u73af\u5883\u3002", "method": "ToolUniverse\u6807\u51c6\u5316\u4e86AI\u79d1\u5b66\u5bb6\u8bc6\u522b\u548c\u8c03\u7528\u5de5\u5177\u7684\u65b9\u5f0f\uff0c\u81ea\u52a8\u4f18\u5316\u5de5\u5177\u63a5\u53e3\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u521b\u5efa\u65b0\u5de5\u5177\uff0c\u8fed\u4ee3\u4f18\u5316\u5de5\u5177\u89c4\u8303\uff0c\u5e76\u5c06\u5de5\u5177\u7ec4\u5408\u6210\u667a\u80fd\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u9ad8\u80c6\u56fa\u9187\u8840\u75c7\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cToolUniverse\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2aAI\u79d1\u5b66\u5bb6\uff0c\u8bc6\u522b\u51fa\u5177\u6709\u826f\u597d\u9884\u6d4b\u7279\u6027\u7684\u836f\u7269\u7c7b\u4f3c\u7269\u3002", "conclusion": "ToolUniverse\u4e3a\u6784\u5efaAI\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7c7b\u4f3c\u4e8e\u7ec4\u5b66\u9886\u57df\u7edf\u4e00\u751f\u6001\u7cfb\u7edf\u5bf9\u7814\u7a76\u7684\u53d8\u9769\u6027\u5f71\u54cd\u3002", "topic": "swe application"}}
{"id": "2509.23124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23124", "abs": "https://arxiv.org/abs/2509.23124", "authors": ["Jeonghoon Shim", "Woojung Song", "Cheyon Jin", "Seungwon KooK", "Yohan Jo"], "title": "Non-Collaborative User Simulators for Tool Agents", "comment": "9 pages", "summary": "Non-Collaborative User Simulators for Tool Agents Download PDF Jeonghoon\nShim, Woojung Song, Cheyon Jin, Seungwon KooK, Yohan Jo 19 Sept 2025 (modified:\n25 Sept 2025)ICLR 2026 Conference SubmissionConference, AuthorsRevisionsCC BY\n4.0 Keywords: Tool Agent, User Simulator, Non-collaborative User, Dialogue\nSimulation TL;DR: A non-collaborative user simulation method for tool agent.\nAbstract: Tool agents interact with users through multi-turn dialogues to\naccomplish various tasks. Recent studies have adopted user simulation methods\nto develop these agents in multi-turn settings. However, existing user\nsimulators tend to be agent-friendly, exhibiting only cooperative behaviors,\nwhich fails to train and test agents against non-collaborative users in the\nreal world. To address this, we propose a novel user simulator architecture\nthat simulates four categories of non-collaborative behaviors: requesting\nunavailable services, digressing into tangential conversations, expressing\nimpatience, and providing incomplete utterances. Our user simulator can\nsimulate challenging and natural non-collaborative behaviors while reliably\ndelivering all intents and information necessary to accomplish the task. Our\nexperiments on MultiWOZ and $\\tau$-bench reveal significant performance\ndegradation in state-of-the-art tool agents when encountering non-collaborative\nusers. We provide detailed analyses of agents' weaknesses under each\nnon-collaborative condition, such as escalated hallucinations and dialogue\nbreakdowns. Ultimately, we contribute an easily extensible user simulation\nframework to help the research community develop tool agents and preemptively\ndiagnose them under challenging real-world conditions within their own\nservices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5de5\u5177\u4ee3\u7406\u7684\u975e\u534f\u4f5c\u7528\u6237\u6a21\u62df\u65b9\u6cd5\uff0c\u80fd\u591f\u6a21\u62df\u56db\u79cd\u975e\u534f\u4f5c\u884c\u4e3a\uff1a\u8bf7\u6c42\u4e0d\u53ef\u7528\u670d\u52a1\u3001\u504f\u79bb\u4e3b\u9898\u5bf9\u8bdd\u3001\u8868\u8fbe\u4e0d\u8010\u70e6\u548c\u63d0\u4f9b\u4e0d\u5b8c\u6574\u8bdd\u8bed\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u6a21\u62df\u5668\u8fc7\u4e8e\u53cb\u597d\u4e14\u53ea\u5c55\u793a\u5408\u4f5c\u884c\u4e3a\uff0c\u65e0\u6cd5\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5de5\u5177\u4ee3\u7406\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u975e\u534f\u4f5c\u7528\u6237\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u7528\u6237\u6a21\u62df\u5668\u67b6\u6784\uff0c\u6a21\u62df\u56db\u79cd\u975e\u534f\u4f5c\u884c\u4e3a\u7c7b\u522b\uff0c\u540c\u65f6\u53ef\u9760\u4f20\u9012\u5b8c\u6210\u4efb\u52a1\u6240\u9700\u7684\u6240\u6709\u610f\u56fe\u548c\u4fe1\u606f\u3002", "result": "\u5728MultiWOZ\u548c\u03c4-bench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u5de5\u5177\u4ee3\u7406\u5728\u9047\u5230\u975e\u534f\u4f5c\u7528\u6237\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u51fa\u73b0\u5e7b\u89c9\u589e\u52a0\u548c\u5bf9\u8bdd\u4e2d\u65ad\u7b49\u95ee\u9898\u3002", "conclusion": "\u8d21\u732e\u4e86\u4e00\u4e2a\u6613\u4e8e\u6269\u5c55\u7684\u7528\u6237\u6a21\u62df\u6846\u67b6\uff0c\u5e2e\u52a9\u7814\u7a76\u793e\u533a\u5f00\u53d1\u5de5\u5177\u4ee3\u7406\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u9884\u5148\u8bca\u65ad\u5b83\u4eec\u3002", "topic": "agent analysis"}}
{"id": "2509.23449", "categories": ["cs.AI", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23449", "abs": "https://arxiv.org/abs/2509.23449", "authors": ["Charles E. Gagnon", "Steven H. H. Ding", "Philippe Charland", "Benjamin C. M. Fung"], "title": "Beyond Embeddings: Interpretable Feature Extraction for Binary Code Similarity", "comment": "17 pages, 7 figures, submitted to USENIX Security '26", "summary": "Binary code similarity detection is a core task in reverse engineering. It\nsupports malware analysis and vulnerability discovery by identifying\nsemantically similar code in different contexts. Modern methods have progressed\nfrom manually engineered features to vector representations. Hand-crafted\nstatistics (e.g., operation ratios) are interpretable, but shallow and fail to\ngeneralize. Embedding-based methods overcome this by learning robust\ncross-setting representations, but these representations are opaque vectors\nthat prevent rapid verification. They also face a scalability-accuracy\ntrade-off, since high-dimensional nearest-neighbor search requires\napproximations that reduce precision. Current approaches thus force a\ncompromise between interpretability, generalizability, and scalability.\n  We bridge these gaps using a language model-based agent to conduct structured\nreasoning analysis of assembly code and generate features such as input/output\ntypes, side effects, notable constants, and algorithmic intent. Unlike\nhand-crafted features, they are richer and adaptive. Unlike embeddings, they\nare human-readable, maintainable, and directly searchable with inverted or\nrelational indexes. Without any matching training, our method respectively\nachieves 42% and 62% for recall@1 in cross-architecture and cross-optimization\ntasks, comparable to embedding methods with training (39% and 34%). Combined\nwith embeddings, it significantly outperforms the state-of-the-art,\ndemonstrating that accuracy, scalability, and interpretability can coexist.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u5206\u6790\u6c47\u7f16\u4ee3\u7801\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u65b9\u6cd5\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u624b\u5de5\u7279\u5f81\u53ef\u89e3\u91ca\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5d4c\u5165\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5f3a\u4f46\u4e0d\u53ef\u89e3\u91ca\u4e14\u9762\u4e34\u53ef\u6269\u5c55\u6027-\u51c6\u786e\u6027\u6743\u8861\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5bf9\u6c47\u7f16\u4ee3\u7801\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\u5206\u6790\uff0c\u751f\u6210\u8f93\u5165/\u8f93\u51fa\u7c7b\u578b\u3001\u526f\u4f5c\u7528\u3001\u663e\u8457\u5e38\u6570\u548c\u7b97\u6cd5\u610f\u56fe\u7b49\u4eba\u7c7b\u53ef\u8bfb\u7684\u7279\u5f81\uff0c\u53ef\u76f4\u63a5\u901a\u8fc7\u5012\u6392\u6216\u5173\u7cfb\u7d22\u5f15\u8fdb\u884c\u641c\u7d22\u3002", "result": "\u5728\u8de8\u67b6\u6784\u548c\u8de8\u4f18\u5316\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523042%\u548c62%\u7684recall@1\uff0c\u4e0e\u9700\u8981\u8bad\u7ec3\u7684\u5d4c\u5165\u65b9\u6cd5\u76f8\u5f53\uff0839%\u548c34%\uff09\u3002\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ed3\u5408\u540e\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u53ef\u4ee5\u5171\u5b58\uff0c\u4e3a\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.23140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23140", "abs": "https://arxiv.org/abs/2509.23140", "authors": ["Song Jin", "Juntian Zhang", "Yong Liu", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "Tagging the Thought: Unlocking Personalization Reasoning via Reinforcement Learning", "comment": null, "summary": "Recent advancements have endowed Large Language Models (LLMs) with impressive\ngeneral reasoning capabilities, yet they often struggle with personalization\nreasoning - the crucial ability to analyze user history, infer unique\npreferences, and generate tailored responses. To address this limitation, we\nintroduce TagPR, a novel training framework that significantly enhances an\nLLM's intrinsic capacity for personalization reasoning through a tagging the\nthought approach. Our method first develops a data-driven pipeline to\nautomatically generate and semantically label reasoning chains, creating a\nstructured dataset that fosters interpretable reasoning. We then propose a\nsynergistic training strategy that begins with Supervised Fine-Tuning (SFT) on\nthis tagged data to establish foundational reasoning patterns, followed by a\nmulti-stage reinforcement learning (RL) process. This RL phase is guided by a\nunique composite reward signal, which integrates tag-based constraints and a\nnovel Personalization Reward Model with User Embeddings (PRMU) to achieve\nfine-grained alignment with user-specific logic. Extensive experiments on the\npublic LaMP benchmark and a self-constructed dataset demonstrate that our\napproach achieves state-of-the-art results, delivering an average improvement\nof 32.65% over the base model across all tasks. Our work validates that\nstructured, interpretable reasoning is a highly effective pathway to unlocking\ngenuine personalization capabilities in LLMs.", "AI": {"tldr": "TagPR\u662f\u4e00\u4e2a\u901a\u8fc7\u6807\u8bb0\u601d\u7ef4\u65b9\u6cd5\u589e\u5f3aLLM\u4e2a\u6027\u5316\u63a8\u7406\u80fd\u529b\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u5728LaMP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534732.65%\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e2a\u6027\u5316\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5206\u6790\u7528\u6237\u5386\u53f2\u3001\u63a8\u65ad\u72ec\u7279\u504f\u597d\u5e76\u751f\u6210\u5b9a\u5236\u5316\u54cd\u5e94\u3002", "method": "\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7ba1\u9053\u81ea\u52a8\u751f\u6210\u548c\u8bed\u4e49\u6807\u8bb0\u63a8\u7406\u94fe\uff0c\u521b\u5efa\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff1b\u91c7\u7528\u534f\u540c\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u5efa\u7acb\u57fa\u7840\u63a8\u7406\u6a21\u5f0f\uff0c\u7136\u540e\u8fdb\u884c\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u5305\u542b\u6807\u7b7e\u7ea6\u675f\u548c\u4e2a\u6027\u5316\u5956\u52b1\u6a21\u578b\u7684\u590d\u5408\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u516c\u5f00LaMP\u57fa\u51c6\u548c\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5e73\u5747\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534732.65%\u3002", "conclusion": "\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u662f\u89e3\u9501LLM\u771f\u6b63\u4e2a\u6027\u5316\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23488", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23488", "abs": "https://arxiv.org/abs/2509.23488", "authors": ["Siyang Wu", "Honglin Bao", "Sida Li", "Ari Holtzman", "James A. Evans"], "title": "Mapping Overlaps in Benchmarks through Perplexity in the Wild", "comment": null, "summary": "We develop signatures of capacity familiarity to characterize large language\nmodel (LLM) benchmarks and their meaningful overlaps. Benchmark signatures\nprobe the capacity required for benchmark performance. We formally define them\nas a set of salient tokens drawn from in-the-wild, naturally authored corpora,\nwhere LLM token perplexity, reflecting more or less pre-training exposure,\nbecomes highly predictive of LLM benchmark performance. Through a large-scale\nmeta-evaluation, we extract benchmark signatures via stepwise forward selection\nwith linear regressions across 32 LLMs and 88 benchmarks spanning diverse\nknowledge, coding, logic, instruction following, math, language, reasoning, and\nworld modeling. Our analysis situates signatures in relation to both the\nsemantic similarity of benchmark questions and the correlation of model\nperformance. While performance overlaps are universally high and semantic\noverlaps remain confined to a narrow mid-range, benchmark signatures prove\nhighly informative in capturing variation, overlap, and divergence. We observe\noverlap in knowledge and reasoning subtasks, whereas multilingual and cultural\nbenchmarks exhibit less similarity, even compared to cross-task overlap.\nNotably, performance-level results are strongly influenced by\nbenchmark-orthogonal factors such as question format, highlighting limitations\nin LLM generalization, the conflation of performance with ability, and issues\ninherent in current mainstream benchmark agreement studies. Benchmark\nsignatures, however, remain robust to such effects. Ultimately, we identify\ncross-functional overlaps across logic, math, language, instruction following,\nand world modeling, with coding emerging as the least overlapping domain.\nTogether, these findings provide mechanistic insights into benchmark validity\nand LLM sensitivities, and sketch the underlying landscape of interconnected\nLLM capabilities.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u51c6\u7b7e\u540d\u65b9\u6cd5\u6765\u8868\u5f81LLM\u57fa\u51c6\u6d4b\u8bd5\u53ca\u5176\u91cd\u53e0\u6027\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u81ea\u7136\u8bed\u6599\u4e0a\u7684\u56f0\u60d1\u5ea6\u6765\u9884\u6d4b\u57fa\u51c6\u8868\u73b0\uff0c\u53d1\u73b0\u7f16\u7801\u9886\u57df\u4e0e\u5176\u4ed6\u80fd\u529b\u91cd\u53e0\u6700\u5c11\u3002", "motivation": "\u5f53\u524dLLM\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6027\u80fd\u4e0e\u80fd\u529b\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e4b\u95f4\u7684\u771f\u5b9e\u91cd\u53e0\u548c\u6a21\u578b\u80fd\u529b\u7684\u5185\u5728\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u9010\u6b65\u524d\u5411\u9009\u62e9\u548c\u7ebf\u6027\u56de\u5f52\uff0c\u572832\u4e2aLLMs\u548c88\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u53d6\u57fa\u51c6\u7b7e\u540d\uff0c\u5206\u6790\u6a21\u578b\u56f0\u60d1\u5ea6\u4e0e\u57fa\u51c6\u8868\u73b0\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u91cd\u53e0\u5ea6\u9ad8\uff0c\u591a\u8bed\u8a00\u6587\u5316\u57fa\u51c6\u76f8\u4f3c\u6027\u4f4e\uff0c\u7f16\u7801\u9886\u57df\u4e0e\u5176\u4ed6\u80fd\u529b\u91cd\u53e0\u6700\u5c11\uff0c\u57fa\u51c6\u7b7e\u540d\u5bf9\u95ee\u9898\u683c\u5f0f\u7b49\u65e0\u5173\u56e0\u7d20\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u51c6\u7b7e\u540d\u65b9\u6cd5\u80fd\u6709\u6548\u63ed\u793aLLM\u80fd\u529b\u7684\u5e95\u5c42\u666f\u89c2\uff0c\u4e3a\u57fa\u51c6\u6709\u6548\u6027\u63d0\u4f9b\u673a\u5236\u6027\u6d1e\u5bdf\uff0c\u5e76\u8bc6\u522b\u8de8\u529f\u80fd\u91cd\u53e0\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2509.23686", "categories": ["cs.CL", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23686", "abs": "https://arxiv.org/abs/2509.23686", "authors": ["Yifeng He", "Luning Yang", "Christopher Castro Gaw Gonzalo", "Hao Chen"], "title": "TF-Bench: Evaluating Program Semantics Reasoning with Type Inference in System F", "comment": "NeurIPS '25, package released at:\n  https://github.com/SecurityLab-UCD/TF-Bench", "summary": "Large Language Models (LLMs) are increasingly integrated into the software\nengineering ecosystem. Their test-time compute (TTC) reasoning capabilities\nshow significant potential for understanding program logic and semantics beyond\nmere token recognition. However, current benchmarks for code reasoning lack a\nformal, program-centric deductive framework to ensure sound evaluation, and are\nincapable of assessing whether models genuinely reason about program semantics\nor merely exploit superficial associations between natural language and code\ntokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to\nevaluate LLM reasoning based on type inference in System F, a task we refer to\nas program semantics reasoning. By employing verified transformations to remove\nsemantically irrelevant natural language, we construct TF-Bench_pure, a purely\nsemantics-driven variant of TF-Bench. Our analysis reveals substantial\nlimitations in state-of-the-art LLMs, with the best-performing LLM\n(Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure.\nAdditionally, we propose two novel metrics to assess robustness and the\neffectiveness of test-time reasoning, underscoring critical limitations in\ncurrent LLM capabilities and highlighting essential directions for future\nresearch.", "AI": {"tldr": "TF-Bench\u662f\u4e00\u4e2a\u57fa\u4e8eSystem F\u7c7b\u578b\u63a8\u65ad\u7684LLM\u7a0b\u5e8f\u8bed\u4e49\u63a8\u7406\u57fa\u51c6\uff0c\u901a\u8fc7\u53bb\u9664\u8bed\u4e49\u65e0\u5173\u7684\u81ea\u7136\u8bed\u8a00\u6784\u5efaTF-Bench_pure\u53d8\u4f53\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdbLLM\u5728\u7a0b\u5e8f\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u7684\u4e25\u91cd\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7684\u7a0b\u5e8f\u4e2d\u5fc3\u6f14\u7ece\u6846\u67b6\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u7a0b\u5e8f\u8bed\u4e49\u8fd8\u662f\u4ec5\u5229\u7528\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u6807\u8bb0\u4e4b\u95f4\u7684\u8868\u9762\u5173\u8054\u3002", "method": "\u57fa\u4e8eSystem F\u7c7b\u578b\u63a8\u65ad\u6784\u5efaTF-Bench\u57fa\u51c6\uff0c\u4f7f\u7528\u9a8c\u8bc1\u8f6c\u6362\u53bb\u9664\u8bed\u4e49\u65e0\u5173\u7684\u81ea\u7136\u8bed\u8a00\u521b\u5efaTF-Bench_pure\u53d8\u4f53\uff0c\u5e76\u63d0\u51fa\u4e24\u4e2a\u65b0\u6307\u6807\u8bc4\u4f30\u9c81\u68d2\u6027\u548c\u6d4b\u8bd5\u65f6\u63a8\u7406\u6548\u679c\u3002", "result": "\u6700\u5148\u8fdb\u7684LLM\uff08Claude-3.7-sonnet\uff09\u5728TF-Bench_pure\u4e0a\u4ec5\u8fbe\u523055.85%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u4e25\u91cd\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u7a0b\u5e8f\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5173\u952e\u9650\u5236\uff0c\u9700\u8981\u672a\u6765\u7814\u7a76\u91cd\u70b9\u5173\u6ce8\u3002", "topic": "agent analysis"}}
{"id": "2509.23188", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23188", "abs": "https://arxiv.org/abs/2509.23188", "authors": ["Guancheng Wan", "Leixin Sun", "Longxu Dou", "Zitong Shi", "Fang Wu", "Eric Hanchen Jiang", "Wenke Huang", "Guibin Zhang", "Hejia Geng", "Xiangru Tang", "Zhenfei Yin", "Yizhou Sun", "Wei Wang"], "title": "Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts", "comment": null, "summary": "Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly\nadvanced collaborative reasoning, tool use, and role-specialized coordination\nin complex tasks. However, reliability-critical deployment remains hindered by\na systemic failure mode: hierarchical compliance under instruction conflicts\n(system-user, peer-peer), where agents misprioritize system-level rules in the\npresence of competing demands. Moreover, widely used macro-level metrics (e.g.,\npass@k) obscure these micro-level violations and offer little actionable\nguidance for remedy. In this work, we present a full-stack, three-stage\nframework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a\nquery-wise, context-aware scoring metric that decomposes role adherence into\nfour measurable dimensions; (2) Localize - attention drift analysis revealing\nthat instruction conflicts are resolved by attention heads that are largely\nconcentrated in middle layers; (3) Align - Surgical Alignment of Instruction\nLayers (SAIL), which installs LoRA only on the localized focal layers and\noptimizes a token-weighted DPO-style preference objective that credits tokens\nby their focal attentional contribution. Across standard benchmarks and MAS\nframeworks, our surgical approach improves instruction hierarchy compliance\n(e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\u6765\u8bca\u65ad\u3001\u5b9a\u4f4d\u548c\u89e3\u51b3LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5c42\u6b21\u6307\u4ee4\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u53d1\u73b0\u51b2\u7a81\u4e3b\u8981\u51fa\u73b0\u5728\u4e2d\u95f4\u5c42\uff0c\u5e76\u5f00\u53d1\u4e86\u624b\u672f\u5f0f\u5bf9\u9f50\u65b9\u6cd5SAIL\u6765\u6539\u8fdb\u6307\u4ee4\u5c42\u6b21\u9075\u4ece\u6027\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u6545\u969c\u6a21\u5f0f\uff1a\u5728\u6307\u4ee4\u51b2\u7a81\uff08\u7cfb\u7edf-\u7528\u6237\u3001\u5bf9\u7b49\u4f53\u4e4b\u95f4\uff09\u65f6\u7684\u5c42\u6b21\u9075\u4ece\u6027\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u5b8f\u89c2\u6307\u6807\u65e0\u6cd5\u63ed\u793a\u8fd9\u4e9b\u5fae\u89c2\u8fdd\u89c4\u884c\u4e3a\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a(1)\u8bca\u65ad-\u4e0a\u4e0b\u6587\u89d2\u8272\u9075\u4ece\u8bc4\u5206(CRAS)\uff0c\u5c06\u89d2\u8272\u9075\u4ece\u5206\u89e3\u4e3a\u56db\u4e2a\u53ef\u6d4b\u91cf\u7ef4\u5ea6\uff1b(2)\u5b9a\u4f4d-\u6ce8\u610f\u529b\u6f02\u79fb\u5206\u6790\uff0c\u53d1\u73b0\u6307\u4ee4\u51b2\u7a81\u4e3b\u8981\u7531\u4e2d\u95f4\u5c42\u7684\u6ce8\u610f\u529b\u5934\u89e3\u51b3\uff1b(3)\u5bf9\u9f50-\u624b\u672f\u5f0f\u6307\u4ee4\u5c42\u5bf9\u9f50(SAIL)\uff0c\u4ec5\u5728\u5b9a\u4f4d\u7684\u7126\u70b9\u5c42\u5b89\u88c5LoRA\uff0c\u4f18\u5316\u57fa\u4e8e\u4ee4\u724c\u6743\u91cd\u7684DPO\u98ce\u683c\u504f\u597d\u76ee\u6807\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e0a\uff0c\u624b\u672f\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6307\u4ee4\u5c42\u6b21\u9075\u4ece\u6027\uff08\u5982\u5728AutoGen\u4e0aMedQA\u4efb\u52a1\u63d0\u53475.60%\uff09\uff0c\u65e0\u9700\u5168\u6a21\u578b\u5fae\u8c03\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6307\u4ee4\u51b2\u7a81\u7684\u5fae\u89c2\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bca\u65ad\u548c\u624b\u672f\u5f0f\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4e3a\u53ef\u9760\u6027\u5173\u952e\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.23735", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23735", "abs": "https://arxiv.org/abs/2509.23735", "authors": ["Xuyan Ma", "Xiaofei Xie", "Yawen Wang", "Junjie Wang", "Boyu Wu", "Mingyang Li", "Qing Wang"], "title": "Diagnosing Failure Root Causes in Platform-Orchestrated Agentic Systems: Dataset, Taxonomy, and Benchmark", "comment": null, "summary": "Agentic systems consisting of multiple LLM-driven agents coordinating through\ntools and structured interactions, are increasingly deployed for complex\nreasoning and problem-solving tasks. At the same time, emerging low-code and\ntemplate-based agent development platforms (e.g., Dify) enable users to rapidly\nbuild and orchestrate agentic systems, which we refer to as\nplatform-orchestrated agentic systems. However, these systems are also fragile\nand it remains unclear how to systematically identify their potential failure\nroot cause. This paper presents a study of root cause identification of these\nplatform-orchestrated agentic systems. To support this initiative, we construct\na dataset AgentFail containing 307 failure logs from ten agentic systems, each\nwith fine-grained annotations linking failures to their root causes. We\nadditionally utilize counterfactual reasoning-based repair strategy to ensure\nthe reliability of the annotation. Building on the dataset, we develop a\ntaxonomy that characterizes failure root causes and analyze their distribution\nacross different platforms and task domains. Furthermore, we introduce a\nbenchmark that leverages LLMs for automatically identifying root causes, in\nwhich we also utilize the proposed taxonomy as guidance for LLMs. Results show\nthat the taxonomy can largely improve the performance, thereby confirming its\nutility. Nevertheless, the accuracy of root cause identification reaches at\nmost 33.6%, which indicates that this task still remains challenging. In light\nof these results, we also provide actionable guidelines for building such\nagentic systems. In summary, this paper provides a reliable dataset of failure\nroot cause for platform-orchestrated agentic systems, corresponding taxonomy\nand benchmark, which serves as a foundation for advancing the development of\nmore reliable agentic systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5e73\u53f0\u7f16\u6392\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6545\u969c\u6839\u56e0\u8bc6\u522b\uff0c\u6784\u5efa\u4e86\u5305\u542b307\u4e2a\u6545\u969c\u65e5\u5fd7\u7684\u6570\u636e\u96c6AgentFail\uff0c\u5f00\u53d1\u4e86\u6545\u969c\u6839\u56e0\u5206\u7c7b\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u6839\u56e0\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740\u4f4e\u4ee3\u7801\u5e73\u53f0\u7f16\u6392\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u88ab\u5e7f\u6cdb\u90e8\u7f72\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5b58\u5728\u8106\u5f31\u6027\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u6545\u969c\u6839\u56e0\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u6784\u5efaAgentFail\u6570\u636e\u96c6\uff0c\u91c7\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u4fee\u590d\u7b56\u7565\u786e\u4fdd\u6807\u6ce8\u53ef\u9760\u6027\uff0c\u5f00\u53d1\u6545\u969c\u6839\u56e0\u5206\u7c7b\u6cd5\uff0c\u5e76\u5efa\u7acb\u57fa\u4e8eLLM\u7684\u6839\u56e0\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5206\u7c7b\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u6839\u56e0\u8bc6\u522b\u51c6\u786e\u7387\u6700\u9ad8\u4ec533.6%\uff0c\u8868\u660e\u8be5\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u4e3a\u5e73\u53f0\u7f16\u6392\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6545\u969c\u6839\u56e0\u6570\u636e\u96c6\u3001\u5206\u7c7b\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2509.23510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23510", "abs": "https://arxiv.org/abs/2509.23510", "authors": ["Ashwin Ramaswamy", "Nestor Demeure", "Ermal Rrapaj"], "title": "Model Consistency as a Cheap yet Predictive Proxy for LLM Elo Scores", "comment": null, "summary": "New large language models (LLMs) are being released every day. Some perform\nsignificantly better or worse than expected given their parameter count.\nTherefore, there is a need for a method to independently evaluate models. The\ncurrent best way to evaluate a model is to measure its Elo score by comparing\nit to other models in a series of contests - an expensive operation since\nhumans are ideally required to compare LLM outputs. We observe that when an LLM\nis asked to judge such contests, the consistency with which it selects a model\nas the best in a matchup produces a metric that is 91% correlated with its own\nhuman-produced Elo score. This provides a simple proxy for Elo scores that can\nbe computed cheaply, without any human data or prior knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528LLM\u81ea\u52a8\u8bc4\u4f30\u5176\u4ed6LLM\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684Elo\u5206\u6570\u670991%\u7684\u76f8\u5173\u6027\uff0c\u53ef\u4ee5\u4f4e\u6210\u672c\u66ff\u4ee3\u6602\u8d35\u7684\u4eba\u5de5\u8bc4\u4f30\u3002", "motivation": "\u65b0LLM\u4e0d\u65ad\u53d1\u5e03\uff0c\u4f46\u53c2\u6570\u6570\u91cf\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u6027\u80fd\uff0c\u9700\u8981\u72ec\u7acb\u8bc4\u4f30\u65b9\u6cd5\u3002\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u662f\u901a\u8fc7\u4eba\u7c7b\u6bd4\u8f83\u8f93\u51fa\u8ba1\u7b97Elo\u5206\u6570\uff0c\u4f46\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u6765\u8bc4\u4f30\u5176\u4ed6LLM\u7684\u5bf9\u6218\u7ed3\u679c\uff0c\u901a\u8fc7\u7edf\u8ba1LLM\u9009\u62e9\u67d0\u4e2a\u6a21\u578b\u4e3a\u6700\u4f73\u7684\u4e00\u81f4\u6027\u6765\u751f\u6210\u8bc4\u4f30\u6307\u6807\u3002", "result": "LLM\u88c1\u5224\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u6307\u6807\u4e0e\u4eba\u7c7b\u4ea7\u751f\u7684Elo\u5206\u6570\u670991%\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u4f4e\u6210\u672c\u7684Elo\u5206\u6570\u4ee3\u7406\uff0c\u65e0\u9700\u4eba\u7c7b\u6570\u636e\u6216\u5148\u9a8c\u77e5\u8bc6\u3002", "topic": "agent analysis"}}
{"id": "2509.23864", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.23864", "abs": "https://arxiv.org/abs/2509.23864", "authors": ["Roham Koohestani"], "title": "AgentGuard: Runtime Verification of AI Agents", "comment": "Accepted for publication in the proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering, ASE 2025, in the\n  1st international workshop on Agentic Software Engineering (AgenticSE)", "summary": "The rapid evolution to autonomous, agentic AI systems introduces significant\nrisks due to their inherent unpredictability and emergent behaviors; this also\nrenders traditional verification methods inadequate and necessitates a shift\ntowards probabilistic guarantees where the question is no longer if a system\nwill fail, but the probability of its failure within given constraints. This\npaper presents AgentGuard, a framework for runtime verification of Agentic AI\nsystems that provides continuous, quantitative assurance through a new paradigm\ncalled Dynamic Probabilistic Assurance. AgentGuard operates as an inspection\nlayer that observes an agent's raw I/O and abstracts it into formal events\ncorresponding to transitions in a state model. It then uses online learning to\ndynamically build and update a Markov Decision Process (MDP) that formally\nmodels the agent's emergent behavior. Using probabilistic model checking, the\nframework then verifies quantitative properties in real-time.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgentGuard\u6846\u67b6\uff0c\u7528\u4e8e\u8fd0\u884c\u65f6\u9a8c\u8bc1\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u6982\u7387\u4fdd\u8bc1\uff0c\u901a\u8fc7\u52a8\u6001\u6982\u7387\u4fdd\u8bc1\u65b0\u8303\u5f0f\u63d0\u4f9b\u6301\u7eed\u5b9a\u91cf\u4fdd\u8bc1", "motivation": "\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u5feb\u901f\u6f14\u8fdb\u5e26\u6765\u4e86\u663e\u8457\u98ce\u9669\uff0c\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u4e0d\u8db3\uff0c\u9700\u8981\u8f6c\u5411\u6982\u7387\u4fdd\u8bc1\u6765\u5e94\u5bf9\u7cfb\u7edf\u5931\u8d25\u7684\u6982\u7387\u95ee\u9898", "method": "AgentGuard\u4f5c\u4e3a\u68c0\u67e5\u5c42\u89c2\u5bdfagent\u7684\u539f\u59cbI/O\uff0c\u62bd\u8c61\u4e3a\u72b6\u6001\u6a21\u578b\u8f6c\u6362\u7684\u5f62\u5f0f\u4e8b\u4ef6\uff0c\u4f7f\u7528\u5728\u7ebf\u5b66\u4e60\u52a8\u6001\u6784\u5efa\u548c\u66f4\u65b0MDP\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u6a21\u578b\u68c0\u67e5\u5b9e\u65f6\u9a8c\u8bc1\u5b9a\u91cf\u5c5e\u6027", "result": "\u5f00\u53d1\u4e86\u52a8\u6001\u6982\u7387\u4fdd\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u6d8c\u73b0\u884c\u4e3a\u8fdb\u884c\u5f62\u5f0f\u5316\u5efa\u6a21\u548c\u5b9e\u65f6\u9a8c\u8bc1", "conclusion": "AgentGuard\u4e3a\u81ea\u4e3bAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8fd0\u884c\u65f6\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u6d8c\u73b0\u884c\u4e3a\u65f6\u7684\u4e0d\u8db3", "topic": "agent analysis"}}
{"id": "2509.23537", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23537", "abs": "https://arxiv.org/abs/2509.23537", "authors": ["Aaron Xuxiang Tian", "Ruofan Zhang", "Jiayao Tang", "Young Min Cho", "Xueqian Li", "Qiang Yi", "Ji Wang", "Zhunping Zhang", "Danrui Qi", "Sharath Chandra Guntuku", "Lyle Ungar", "Tianyu Shi", "Chi Wang"], "title": "Beyond the Strongest LLM: Multi-Turn Multi-Agent Orchestration vs. Single LLMs on Benchmarks", "comment": "9 pages, 3 tables, 1 figure", "summary": "We study multi-turn multi-agent orchestration, where multiple large language\nmodel (LLM) agents interact over multiple turns by iteratively proposing\nanswers or casting votes until reaching consensus. Using four LLMs (Gemini 2.5\nPro, GPT-5, Grok 4, and Claude Sonnet 4) on GPQA-Diamond, IFEval, and MuSR, we\nconduct two experiments: (i) benchmarking orchestration against single-LLM\nbaselines; and (ii) ablations on GPQA-Diamond that vary whether agents see who\nauthored answers and whether they can observe ongoing votes. Orchestration\nmatches or exceeds the strongest single model and consistently outperforms the\nothers. Analysis of best-achievable orchestration performance shows potential\nfor further gains. The ablations show that revealing authorship increases\nself-voting and ties, and that showing ongoing votes amplifies herding, which\nspeeds convergence but can sometimes yield premature consensus.", "AI": {"tldr": "\u591a\u8f6e\u591a\u667a\u80fd\u4f53\u7f16\u6392\u901a\u8fc7LLM\u667a\u80fd\u4f53\u95f4\u7684\u8fed\u4ee3\u63d0\u8bae\u548c\u6295\u7968\u8fbe\u6210\u5171\u8bc6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5f3a\u5355\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4f5c\u8005\u8eab\u4efd\u548c\u5b9e\u65f6\u6295\u7968\u663e\u793a\u5bf9\u6295\u7968\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7f16\u6392\u662f\u5426\u80fd\u901a\u8fc7\u96c6\u4f53\u51b3\u7b56\u8d85\u8d8a\u5355\u4e2aLLM\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u667a\u80fd\u4f53\u4ea4\u4e92\u673a\u5236\u5bf9\u5171\u8bc6\u5f62\u6210\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u56db\u79cdLLM\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1a\u6bd4\u8f83\u7f16\u6392\u4e0e\u5355\u6a21\u578b\u57fa\u7ebf\uff0c\u5e76\u5728GPQA-Diamond\u4e0a\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\uff0c\u5206\u6790\u4f5c\u8005\u8eab\u4efd\u53ef\u89c1\u6027\u548c\u5b9e\u65f6\u6295\u7968\u663e\u793a\u5bf9\u6295\u7968\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7f16\u6392\u65b9\u6cd5\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5f3a\u5355\u6a21\u578b\uff0c\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u4f5c\u8005\u8eab\u4efd\u53ef\u89c1\u6027\u589e\u52a0\u81ea\u6295\u7968\u548c\u5e73\u5c40\uff0c\u5b9e\u65f6\u6295\u7968\u663e\u793a\u52a0\u5267\u4ece\u4f17\u6548\u5e94\uff0c\u52a0\u901f\u6536\u655b\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u65e9\u5171\u8bc6\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7f16\u6392\u662f\u63d0\u5347LLM\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u8c28\u614e\u8bbe\u8ba1\u4ea4\u4e92\u673a\u5236\u4ee5\u907f\u514d\u8d1f\u9762\u884c\u4e3a\u6a21\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2509.23558", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23558", "abs": "https://arxiv.org/abs/2509.23558", "authors": ["Zhaoqi Wang", "Daqing He", "Zijian Zhang", "Xin Li", "Liehuang Zhu", "Meng Li", "Jiamou Liu"], "title": "Formalization Driven LLM Prompt Jailbreaking via Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, yet\nthey also introduce novel security challenges. For instance, prompt\njailbreaking attacks involve adversaries crafting sophisticated prompts to\nelicit responses from LLMs that deviate from human values. To uncover\nvulnerabilities in LLM alignment methods, we propose the PASS framework\n(\\underline{P}rompt J\\underline{a}ilbreaking via \\underline{S}emantic and\n\\underline{S}tructural Formalization). Specifically, PASS employs reinforcement\nlearning to transform initial jailbreak prompts into formalized descriptions,\nwhich enhances stealthiness and enables bypassing existing alignment defenses.\nThe jailbreak outputs are then structured into a GraphRAG system that, by\nleveraging extracted relevant terms and formalized symbols as contextual input\nalongside the original query, strengthens subsequent attacks and facilitates\nmore effective jailbreaks. We conducted extensive experiments on common\nopen-source models, demonstrating the effectiveness of our attack.", "AI": {"tldr": "\u63d0\u51faPASS\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u521d\u59cb\u8d8a\u72f1\u63d0\u793a\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u589e\u5f3a\u9690\u853d\u6027\u5e76\u7ed5\u8fc7\u73b0\u6709\u5bf9\u9f50\u9632\u5fa1\uff0c\u7136\u540e\u6784\u5efaGraphRAG\u7cfb\u7edf\u6765\u5f3a\u5316\u540e\u7eed\u653b\u51fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u63d0\u793a\u8d8a\u72f1\u653b\u51fb\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4f7fLLM\u504f\u79bb\u4eba\u7c7b\u4ef7\u503c\u89c2\u3002\u9700\u8981\u53d1\u73b0LLM\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u7684\u6f0f\u6d1e\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5c06\u521d\u59cb\u8d8a\u72f1\u63d0\u793a\u8f6c\u5316\u4e3a\u8bed\u4e49\u548c\u7ed3\u6784\u5f62\u5f0f\u5316\u63cf\u8ff0\uff0c\u6784\u5efaGraphRAG\u7cfb\u7edf\uff0c\u5229\u7528\u63d0\u53d6\u7684\u76f8\u5173\u672f\u8bed\u548c\u5f62\u5f0f\u5316\u7b26\u53f7\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u6765\u5f3a\u5316\u653b\u51fb\u3002", "result": "\u5728\u5e38\u89c1\u5f00\u6e90\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u653b\u51fb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PASS\u6846\u67b6\u80fd\u591f\u6709\u6548\u53d1\u73b0LLM\u5bf9\u9f50\u65b9\u6cd5\u4e2d\u7684\u6f0f\u6d1e\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u63cf\u8ff0\u548cGraphRAG\u7cfb\u7edf\u589e\u5f3a\u8d8a\u72f1\u653b\u51fb\u7684\u9690\u853d\u6027\u548c\u6548\u679c\u3002", "topic": "agent analysis"}}
{"id": "2509.23206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23206", "abs": "https://arxiv.org/abs/2509.23206", "authors": ["Huacan Chai", "Zijie Cao", "Maolin Ran", "Yingxuan Yang", "Jianghao Lin", "pengxin", "Hairui Wang", "Renjie Ding", "Ziyu Wan", "Muning Wen", "Weiwen Liu", "Weinan Zhang", "Fei Huang", "Ying Wen"], "title": "PARL-MT: Learning to Call Functions in Multi-Turn Conversation with Progress Awareness", "comment": null, "summary": "Large language models (LLMs) have achieved impressive success in single-turn\nfunction calling, yet real-world applications such as travel planning or\nmulti-stage data analysis typically unfold across multi-turn conversations. In\nthese settings, LLMs must not only issue accurate function calls at each step\nbut also maintain progress awareness, the ability to summarize past\ninteractions and plan future actions to ensure coherent, long-horizon task\nexecution. Existing approaches, however, either reduce multi-turn training to\nisolated single-turn samples, which neglects task-level planning, or employ\nend-to-end reinforcement learning (RL) that struggles with redundancy and lacks\nexplicit integration of progress awareness. To overcome these limitations, we\nintroduce PARL-MT, a framework that explicitly incorporates progress awareness\ninto LLM training for multi-turn function calling. PARL-MT combines (i) a\nProgress Awareness Generation (PAG) pipeline, which automatically constructs\ndatasets coupling conversation summaries with future task planning, and (ii) a\nProgress Awareness-Guided Reinforcement Learning (PAG-RL) algorithm, which\nintegrates progress awareness into RL training to reduce contextual redundancy\nand improve alignment between local actions and global task completion.\nEmpirical results on two public benchmarks demonstrate that PARL-MT\nsignificantly outperforms existing methods, highlighting the effectiveness of\nprogress awareness in enabling robust and efficient multi-turn function\ncalling.", "AI": {"tldr": "\u63d0\u51fa\u4e86PARL-MT\u6846\u67b6\uff0c\u5c06\u8fdb\u5ea6\u611f\u77e5\u663e\u5f0f\u6574\u5408\u5230LLM\u7684\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u8bad\u7ec3\u4e2d\uff0c\u901a\u8fc7\u8fdb\u5ea6\u611f\u77e5\u751f\u6210\u548c\u8fdb\u5ea6\u611f\u77e5\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u5982\u65c5\u884c\u89c4\u5212\u6216\u591a\u9636\u6bb5\u6570\u636e\u5206\u6790\u901a\u5e38\u6d89\u53ca\u591a\u8f6e\u5bf9\u8bdd\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06\u591a\u8f6e\u8bad\u7ec3\u7b80\u5316\u4e3a\u5b64\u7acb\u5355\u8f6e\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u4efb\u52a1\u7ea7\u89c4\u5212\uff0c\u8981\u4e48\u4f7f\u7528\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u4f46\u5b58\u5728\u5197\u4f59\u95ee\u9898\u4e14\u7f3a\u4e4f\u660e\u786e\u7684\u8fdb\u5ea6\u611f\u77e5\u6574\u5408\u3002", "method": "PARL-MT\u6846\u67b6\u5305\u542b\uff1a(i)\u8fdb\u5ea6\u611f\u77e5\u751f\u6210(PAG)\u7ba1\u9053\uff0c\u81ea\u52a8\u6784\u5efa\u5c06\u5bf9\u8bdd\u6458\u8981\u4e0e\u672a\u6765\u4efb\u52a1\u89c4\u5212\u8026\u5408\u7684\u6570\u636e\u96c6\uff1b(ii)\u8fdb\u5ea6\u611f\u77e5\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60(PAG-RL)\u7b97\u6cd5\uff0c\u5c06\u8fdb\u5ea6\u611f\u77e5\u6574\u5408\u5230RL\u8bad\u7ec3\u4e2d\uff0c\u51cf\u5c11\u4e0a\u4e0b\u6587\u5197\u4f59\u5e76\u6539\u5584\u5c40\u90e8\u52a8\u4f5c\u4e0e\u5168\u5c40\u4efb\u52a1\u5b8c\u6210\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cPARL-MT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u8fdb\u5ea6\u611f\u77e5\u5728\u5b9e\u73b0\u7a33\u5065\u9ad8\u6548\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fdb\u5ea6\u611f\u77e5\u5bf9\u4e8e\u5b9e\u73b0\u7a33\u5065\u9ad8\u6548\u7684\u591a\u8f6e\u51fd\u6570\u8c03\u7528\u81f3\u5173\u91cd\u8981\uff0cPARL-MT\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u8fdb\u5ea6\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u591a\u8f6e\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23129", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23129", "abs": "https://arxiv.org/abs/2509.23129", "authors": ["Haotian Liu", "Shuo Wang", "Hongteng Xu"], "title": "C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning", "comment": null, "summary": "Reinforcement Learning (RL) methods, exemplified by Group Relative Policy\nOptimization (GRPO) and its variants, play a central role in developing\nreasoning models. However, these methods often suffer from a critical\noverconfidence issue, which prevents them from achieving self-aware reasoning\nmodels. In this study, we propose a simple yet effective confidence-calibration\ngroup sequence policy gradient method, called C$^2$GSPG, which simultaneously\nenhances reasoning performance while suppressing overconfidence. In principle,\nwe propose a Group Sequence Policy Gradient (GSPG) framework for learning\nreasoning models, which eliminates the token-level bias commonly appearing in\nGRPO and its variants. In this framework, we define the model confidence for\neach reasoning problem using the normalized sequence-level probability, and\nthen apply a cross-entropy regularizer to calibrate the model confidence to the\nsequence's reward. We demonstrate that the confidence calibration regularizer\nand GSPG are collaborative for binary rewards, as their objectives always share\nthe same gradient direction. For non-binary rewards, we apply nonlinear reward\nnormalization and adaptive regularizer clipping, mitigating the potential\nconflict between the two objectives. Applying C$^2$GSPG to post-train large\nlanguage models in logical and mathematical reasoning tasks, we show its\nsuperiority over state-of-the-art methods in both reasoning accuracy and\nconfidence calibration. The code of C$^2$GSPG is available at\nhttps://github.com/HaotianLiu123/CCGSPG.", "AI": {"tldr": "\u63d0\u51faC\u00b2GSPG\u65b9\u6cd5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u7ec4\u5e8f\u5217\u7b56\u7565\u68af\u5ea6\u6846\u67b6\u63d0\u5347\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982GRPO\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u963b\u788d\u4e86\u81ea\u611f\u77e5\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55", "method": "\u63d0\u51fa\u7ec4\u5e8f\u5217\u7b56\u7565\u68af\u5ea6(GSPG)\u6846\u67b6\u6d88\u9664token\u7ea7\u504f\u5dee\uff0c\u4f7f\u7528\u5f52\u4e00\u5316\u5e8f\u5217\u7ea7\u6982\u7387\u5b9a\u4e49\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u5e94\u7528\u4ea4\u53c9\u71b5\u6b63\u5219\u5668\u6821\u51c6\u7f6e\u4fe1\u5ea6\u4e0e\u5956\u52b1", "result": "\u5728\u903b\u8f91\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cC\u00b2GSPG\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "C\u00b2GSPG\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6027\u80fd\u5e76\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u6b63\u5219\u5668\u4e0eGSPG\u5728\u4e8c\u5143\u5956\u52b1\u4e0b\u5177\u6709\u534f\u4f5c\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2509.23233", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23233", "abs": "https://arxiv.org/abs/2509.23233", "authors": ["Sina J. Semnani", "Jirayu Burapacheep", "Arpandeep Khatua", "Thanawan Atchariyachanvanit", "Zheng Wang", "Monica S. Lam"], "title": "Detecting Corpus-Level Knowledge Inconsistencies in Wikipedia with Large Language Models", "comment": "EMNLP 2025 (Main Conference)", "summary": "Wikipedia is the largest open knowledge corpus, widely used worldwide and\nserving as a key resource for training large language models (LLMs) and\nretrieval-augmented generation (RAG) systems. Ensuring its accuracy is\ntherefore critical. But how accurate is Wikipedia, and how can we improve it?\n  We focus on inconsistencies, a specific type of factual inaccuracy, and\nintroduce the task of corpus-level inconsistency detection. We present CLAIRE,\nan agentic system that combines LLM reasoning with retrieval to surface\npotentially inconsistent claims along with contextual evidence for human\nreview. In a user study with experienced Wikipedia editors, 87.5% reported\nhigher confidence when using CLAIRE, and participants identified 64.7% more\ninconsistencies in the same amount of time.\n  Combining CLAIRE with human annotation, we contribute WIKICOLLIDE, the first\nbenchmark of real Wikipedia inconsistencies. Using random sampling with\nCLAIRE-assisted analysis, we find that at least 3.3% of English Wikipedia facts\ncontradict another fact, with inconsistencies propagating into 7.3% of FEVEROUS\nand 4.0% of AmbigQA examples. Benchmarking strong baselines on this dataset\nreveals substantial headroom: the best fully automated system achieves an AUROC\nof only 75.1%.\n  Our results show that contradictions are a measurable component of Wikipedia\nand that LLM-based systems like CLAIRE can provide a practical tool to help\neditors improve knowledge consistency at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86CLAIRE\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u7ef4\u57fa\u767e\u79d1\u4e2d\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u521b\u5efa\u4e86\u9996\u4e2a\u771f\u5b9e\u7ef4\u57fa\u767e\u79d1\u4e0d\u4e00\u81f4\u6027\u57fa\u51c6WIKICOLLIDE\u3002\u7814\u7a76\u53d1\u73b0\u81f3\u5c113.3%\u7684\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u4e8b\u5b9e\u5b58\u5728\u77db\u76fe\uff0c\u4e14\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u68c0\u6d4b\u6027\u80fd\u6709\u9650\u3002", "motivation": "\u7ef4\u57fa\u767e\u79d1\u4f5c\u4e3a\u6700\u5927\u7684\u5f00\u653e\u77e5\u8bc6\u8bed\u6599\u5e93\uff0c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u786e\u4fdd\u5176\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f46\u7ef4\u57fa\u767e\u79d1\u7684\u51c6\u786e\u6027\u5982\u4f55\u4ee5\u53ca\u5982\u4f55\u6539\u8fdb\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u63d0\u51fa\u4e86CLAIRE\u7cfb\u7edf\uff0c\u7ed3\u5408LLM\u63a8\u7406\u4e0e\u68c0\u7d22\u6280\u672f\u6765\u53d1\u73b0\u6f5c\u5728\u7684\u4e0d\u4e00\u81f4\u58f0\u660e\uff0c\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u8bc1\u636e\u4f9b\u4eba\u5de5\u5ba1\u67e5\u3002\u901a\u8fc7\u968f\u673a\u62bd\u6837\u548cCLAIRE\u8f85\u52a9\u5206\u6790\u521b\u5efa\u4e86WIKICOLLIDE\u57fa\u51c6\u3002", "result": "\u7528\u6237\u7814\u7a76\u4e2d\uff0c87.5%\u7684\u7f16\u8f91\u62a5\u544a\u4f7f\u7528CLAIRE\u65f6\u4fe1\u5fc3\u66f4\u9ad8\uff0c\u53c2\u4e0e\u8005\u5728\u76f8\u540c\u65f6\u95f4\u5185\u53d1\u73b0\u4e8664.7%\u66f4\u591a\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u81f3\u5c113.3%\u7684\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u4e8b\u5b9e\u5b58\u5728\u77db\u76fe\uff0c\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027\u4f20\u64ad\u5230\u4e867.3%\u7684FEVEROUS\u548c4.0%\u7684AmbigQA\u793a\u4f8b\u4e2d\u3002\u6700\u4f73\u81ea\u52a8\u5316\u7cfb\u7edf\u7684AUROC\u4ec5\u4e3a75.1%\u3002", "conclusion": "\u77db\u76fe\u662f\u7ef4\u57fa\u767e\u79d1\u4e2d\u53ef\u6d4b\u91cf\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u5982CLAIRE\u53ef\u4ee5\u4e3a\u7f16\u8f91\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\uff0c\u5e2e\u52a9\u5927\u89c4\u6a21\u6539\u8fdb\u77e5\u8bc6\u4e00\u81f4\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.23614", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23614", "abs": "https://arxiv.org/abs/2509.23614", "authors": ["Yaozu Wu", "Jizhou Guo", "Dongyuan Li", "Henry Peng Zou", "Wei-Chieh Huang", "Yankai Chen", "Zhen Wang", "Weizhi Zhang", "Yangning Li", "Meng Zhang", "Renhe Jiang", "Philip S. Yu"], "title": "PSG-Agent: Personality-Aware Safety Guardrail for LLM-based Agents", "comment": null, "summary": "Effective guardrails are essential for safely deploying LLM-based agents in\ncritical applications. Despite recent advances, existing guardrails suffer from\ntwo fundamental limitations: (i) they apply uniform guardrail policies to all\nusers, ignoring that the same agent behavior can harm some users while being\nsafe for others; (ii) they check each response in isolation, missing how risks\nevolve and accumulate across multiple interactions. To solve these issues, we\npropose PSG-Agent, a personalized and dynamic system for LLM-based agents.\nFirst, PSG-Agent creates personalized guardrails by mining the interaction\nhistory for stable traits and capturing real-time states from current queries,\ngenerating user-specific risk thresholds and protection strategies. Second,\nPSG-Agent implements continuous monitoring across the agent pipeline with\nspecialized guards, including Plan Monitor, Tool Firewall, Response Guard,\nMemory Guardian, that track cross-turn risk accumulation and issue verifiable\nverdicts. Finally, we validate PSG-Agent in multiple scenarios including\nhealthcare, finance, and daily life automation scenarios with diverse user\nprofiles. It significantly outperform existing agent guardrails including\nLlamaGuard3 and AGrail, providing an executable and auditable path toward\npersonalized safety for LLM-based agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86PSG-Agent\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u9632\u62a4\u680f\u548c\u52a8\u6001\u76d1\u63a7\u89e3\u51b3\u73b0\u6709LLM\u4ee3\u7406\u9632\u62a4\u680f\u7684\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a\u7edf\u4e00\u7b56\u7565\u5ffd\u89c6\u7528\u6237\u5dee\u5f02\uff0c\u4ee5\u53ca\u5b64\u7acb\u68c0\u67e5\u5ffd\u7565\u98ce\u9669\u7d2f\u79ef\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u9632\u62a4\u680f\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u5bf9\u6240\u6709\u7528\u6237\u5e94\u7528\u7edf\u4e00\u9632\u62a4\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u76f8\u540c\u884c\u4e3a\u5bf9\u4e0d\u540c\u7528\u6237\u7684\u98ce\u9669\u5dee\u5f02\uff1b\u4ec5\u5b64\u7acb\u68c0\u67e5\u5355\u4e2a\u54cd\u5e94\uff0c\u5ffd\u7565\u4e86\u8de8\u4ea4\u4e92\u7684\u98ce\u9669\u6f14\u5316\u548c\u7d2f\u79ef\u3002", "method": "PSG-Agent\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u901a\u8fc7\u6316\u6398\u4ea4\u4e92\u5386\u53f2\u4e2d\u7684\u7a33\u5b9a\u7279\u5f81\u548c\u5b9e\u65f6\u67e5\u8be2\u72b6\u6001\uff0c\u521b\u5efa\u4e2a\u6027\u5316\u9632\u62a4\u680f\uff1b2\uff09\u5728\u4ee3\u7406\u6d41\u7a0b\u4e2d\u5b9e\u65bd\u6301\u7eed\u76d1\u63a7\uff0c\u5305\u62ec\u8ba1\u5212\u76d1\u63a7\u5668\u3001\u5de5\u5177\u9632\u706b\u5899\u3001\u54cd\u5e94\u9632\u62a4\u680f\u3001\u5185\u5b58\u5b88\u62a4\u8005\u7b49\u4e13\u95e8\u9632\u62a4\u7ec4\u4ef6\u3002", "result": "\u5728\u533b\u7597\u3001\u91d1\u878d\u548c\u65e5\u5e38\u751f\u6d3b\u81ea\u52a8\u5316\u7b49\u591a\u4e2a\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0cPSG-Agent\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4ee3\u7406\u9632\u62a4\u680f\uff08\u5982LlamaGuard3\u548cAGrail\uff09\uff0c\u63d0\u4f9b\u4e86\u53ef\u6267\u884c\u548c\u53ef\u5ba1\u8ba1\u7684\u4e2a\u6027\u5316\u5b89\u5168\u8def\u5f84\u3002", "conclusion": "PSG-Agent\u901a\u8fc7\u4e2a\u6027\u5316\u9632\u62a4\u680f\u548c\u52a8\u6001\u8de8\u8f6e\u6b21\u76d1\u63a7\uff0c\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u62a4\u680f\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.23330", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23330", "abs": "https://arxiv.org/abs/2509.23330", "authors": ["Peng Yu", "Zeyuan Zhao", "Shao Zhang", "Luoyi Fu", "Xinbing Wang", "Ying Wen"], "title": "Learning to Reason in Structured In-context Environments with Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) have achieved significant advancements in\nreasoning capabilities through reinforcement learning (RL) via environmental\nexploration. As the intrinsic properties of the environment determine the\nabilities that LLMs can learn, the environment plays a important role in the RL\nfinetuning process. An ideal LLM reasoning environment should possess three\ncore characteristics: scalability, generalizable reasoning, and verifiability.\nHowever, existing mathematical and coding environments are difficult to scale\ndue to heavy reliance on expert annotation, while the skills learned in\ngame-based environments are too specialized to generalize. To bridge this gap,\nwe introduce the \\textbf{S}tructured \\textbf{I}n-context \\textbf{E}nvironment\n(SIE) framework. SIE achieves scalability by automatically constructing\nreasoning environments from large-scale structured data, where the rich\ncompositional patterns naturally support generalizable reasoning. Moreover, the\nexplicit schemas and reasoning chains in structured data provide a foundation\nfor rule-based verifiability. Experimental results show that SIE framework not\nonly achieves substantial improvements in in-domain structured reasoning, but\nalso enables the learned compositional reasoning skills to generalize\neffectively to out-of-domain mathematical and logical reasoning tasks. We\nfurther explored learning in information-limited partial SIEs and found that\nLLMs can infer the missing information through exploring the environment,\nleading to robust reasoning improvements and generalization performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86SIE\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u6570\u636e\u81ea\u52a8\u6784\u5efa\u63a8\u7406\u73af\u5883\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u73af\u5883\u5728\u53ef\u6269\u5c55\u6027\u3001\u6cdb\u5316\u63a8\u7406\u548c\u53ef\u9a8c\u8bc1\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u548c\u7f16\u7801\u73af\u5883\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u96be\u4ee5\u6269\u5c55\uff0c\u6e38\u620f\u73af\u5883\u5b66\u5230\u7684\u6280\u80fd\u8fc7\u4e8e\u4e13\u4e1a\u5316\u96be\u4ee5\u6cdb\u5316\uff0c\u9700\u8981\u6784\u5efa\u5177\u5907\u53ef\u6269\u5c55\u6027\u3001\u6cdb\u5316\u63a8\u7406\u548c\u53ef\u9a8c\u8bc1\u6027\u7684\u7406\u60f3LLM\u63a8\u7406\u73af\u5883\u3002", "method": "SIE\u6846\u67b6\u4ece\u5927\u89c4\u6a21\u7ed3\u6784\u5316\u6570\u636e\u81ea\u52a8\u6784\u5efa\u63a8\u7406\u73af\u5883\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e30\u5bcc\u7ec4\u5408\u6a21\u5f0f\u652f\u6301\u6cdb\u5316\u63a8\u7406\uff0c\u901a\u8fc7\u663e\u5f0f\u6a21\u5f0f\u548c\u63a8\u7406\u94fe\u5b9e\u73b0\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u6027\u3002", "result": "SIE\u6846\u67b6\u5728\u9886\u57df\u5185\u7ed3\u6784\u5316\u63a8\u7406\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5b66\u5230\u7684\u7ec4\u5408\u63a8\u7406\u6280\u80fd\u80fd\u6709\u6548\u6cdb\u5316\u5230\u9886\u57df\u5916\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u4fe1\u606f\u53d7\u9650\u7684\u90e8\u5206SIE\u4e2dLLM\u80fd\u901a\u8fc7\u73af\u5883\u63a2\u7d22\u63a8\u65ad\u7f3a\u5931\u4fe1\u606f\u3002", "conclusion": "SIE\u6846\u67b6\u4e3aLLM\u63a8\u7406\u80fd\u529b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u9a8c\u8bc1\u7684\u73af\u5883\u6784\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23676", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23676", "abs": "https://arxiv.org/abs/2509.23676", "authors": ["Jue Zhang", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models", "comment": "Accepted by EMNLP'25 (Main)", "summary": "Large Reasoning Models (LRMs) generate explicit reasoning traces alongside\nfinal answers, yet the extent to which these traces influence answer generation\nremains unclear. In this work, we conduct a three-stage investigation into the\ninterplay between reasoning and answer generation in three distilled DeepSeek\nR1 models. First, through empirical evaluation, we demonstrate that including\nexplicit reasoning consistently improves answer quality across diverse domains.\nSecond, attention analysis reveals that answer tokens attend substantially to\nreasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely\ntracking the reasoning trajectory, including self-reflective cues. Third, we\napply mechanistic interventions using activation patching to assess the\ndependence of answer tokens on reasoning activations. Our results show that\nperturbations to key reasoning tokens can reliably alter the final answers,\nconfirming a directional and functional flow of information from reasoning to\nanswer. These findings deepen our understanding of how LRMs leverage reasoning\ntokens for answer generation, highlighting the functional role of intermediate\nreasoning in shaping model outputs. Our data and code are publicly available at\n\\href{https://aka.ms/R2A-code}{this URL}.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e09\u9636\u6bb5\u5b9e\u9a8c\u63a2\u7a76\u4e86\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u63a8\u7406\u8fc7\u7a0b\u4e0e\u7b54\u6848\u751f\u6210\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u63a8\u7406\u75d5\u8ff9\u5bf9\u7b54\u6848\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6ce8\u610f\u529b\u673a\u5236\u663e\u793a\u7b54\u6848token\u4f1a\u5173\u6ce8\u63a8\u7406token\uff0c\u4e14\u901a\u8fc7\u6fc0\u6d3b\u4fee\u8865\u8bc1\u5b9e\u63a8\u7406\u5bf9\u7b54\u6848\u751f\u6210\u5177\u6709\u529f\u80fd\u6027\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u663e\u5f0f\u63a8\u7406\u75d5\u8ff9\u5bf9\u7b54\u6848\u751f\u6210\u7684\u5f71\u54cd\u7a0b\u5ea6\uff0c\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u4e0e\u6700\u7ec8\u7b54\u6848\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7814\u7a76\u65b9\u6cd5\uff1a1) \u5b9e\u8bc1\u8bc4\u4f30\u63a8\u7406\u5bf9\u7b54\u6848\u8d28\u91cf\u7684\u5f71\u54cd\uff1b2) \u6ce8\u610f\u529b\u5206\u6790\u8bc6\u522b\u63a8\u7406\u7126\u70b9\u5934\uff1b3) \u673a\u5236\u5e72\u9884\u901a\u8fc7\u6fc0\u6d3b\u4fee\u8865\u6d4b\u8bd5\u63a8\u7406\u6fc0\u6d3b\u5bf9\u7b54\u6848\u7684\u4f9d\u8d56\u6027\u3002", "result": "\u5305\u542b\u663e\u5f0f\u63a8\u7406\u80fd\u6301\u7eed\u63d0\u5347\u7b54\u6848\u8d28\u91cf\uff1b\u7b54\u6848token\u663e\u8457\u5173\u6ce8\u63a8\u7406token\uff1b\u5173\u952e\u63a8\u7406token\u7684\u6270\u52a8\u80fd\u53ef\u9760\u6539\u53d8\u6700\u7ec8\u7b54\u6848\uff0c\u8bc1\u5b9e\u4ece\u63a8\u7406\u5230\u7b54\u6848\u7684\u4fe1\u606f\u6d41\u3002", "conclusion": "\u5927\u63a8\u7406\u6a21\u578b\u786e\u5b9e\u5229\u7528\u63a8\u7406token\u8fdb\u884c\u7b54\u6848\u751f\u6210\uff0c\u4e2d\u95f4\u63a8\u7406\u5728\u5851\u9020\u6a21\u578b\u8f93\u51fa\u4e2d\u53d1\u6325\u529f\u80fd\u6027\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.23694", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23694", "abs": "https://arxiv.org/abs/2509.23694", "authors": ["Jianshuo Dong", "Sheng Guo", "Hao Wang", "Zhuotao Liu", "Tianwei Zhang", "Ke Xu", "Minlie Huang", "Han Qiu"], "title": "SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents", "comment": "Preprint", "summary": "Search agents connect LLMs to the Internet, enabling access to broader and\nmore up-to-date information. However, unreliable search results may also pose\nsafety threats to end users, establishing a new threat surface. In this work,\nwe conduct two in-the-wild experiments to demonstrate both the prevalence of\nlow-quality search results and their potential to misguide agent behaviors. To\ncounter this threat, we introduce an automated red-teaming framework that is\nsystematic, scalable, and cost-efficient, enabling lightweight and harmless\nsafety assessments of search agents. Building on this framework, we construct\nthe SafeSearch benchmark, which includes 300 test cases covering five\ncategories of risks (e.g., misinformation and indirect prompt injection). Using\nthis benchmark, we evaluate three representative search agent scaffolds,\ncovering search workflow, tool-calling, and deep research, across 7 proprietary\nand 8 open-source backend LLMs. Our results reveal substantial vulnerabilities\nof LLM-based search agents: when exposed to unreliable websites, the highest\nASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover,\nour analysis highlights the limited effectiveness of common defense practices,\nsuch as reminder prompting. This emphasizes the value of our framework in\npromoting transparency for safer agent development. Our codebase and test cases\nare publicly available: https://github.com/jianshuod/SafeSearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u7ea2\u961f\u6d4b\u8bd5\u6846\u67b6SafeSearch\uff0c\u7528\u4e8e\u8bc4\u4f30\u641c\u7d22\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0LLM\u641c\u7d22\u4ee3\u7406\u5728\u9762\u5bf9\u4e0d\u53ef\u9760\u641c\u7d22\u7ed3\u679c\u65f6\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u6700\u9ad8\u653b\u51fb\u6210\u529f\u7387\u53ef\u8fbe90.5%\u3002", "motivation": "\u641c\u7d22\u4ee3\u7406\u867d\u7136\u80fd\u8fde\u63a5LLM\u4e0e\u4e92\u8054\u7f51\u83b7\u53d6\u66f4\u5e7f\u6cdb\u548c\u66f4\u65b0\u7684\u4fe1\u606f\uff0c\u4f46\u4e0d\u53ef\u9760\u7684\u641c\u7d22\u7ed3\u679c\u53ef\u80fd\u5bf9\u7ec8\u7aef\u7528\u6237\u6784\u6210\u5b89\u5168\u5a01\u80c1\uff0c\u5efa\u7acb\u65b0\u7684\u5a01\u80c1\u9762\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b300\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u7684SafeSearch\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u7c7b\u98ce\u9669\uff08\u5982\u9519\u8bef\u4fe1\u606f\u548c\u95f4\u63a5\u63d0\u793a\u6ce8\u5165\uff09\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u4ee3\u8868\u6027\u641c\u7d22\u4ee3\u7406\u6846\u67b6\u548c15\u4e2a\u540e\u7aefLLM\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLLM\u641c\u7d22\u4ee3\u7406\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff1a\u5f53\u66b4\u9732\u4e8e\u4e0d\u53ef\u9760\u7f51\u7ad9\u65f6\uff0cGPT-4.1-mini\u5728\u641c\u7d22\u5de5\u4f5c\u6d41\u8bbe\u7f6e\u4e0b\u7684\u6700\u9ad8\u653b\u51fb\u6210\u529f\u7387\u53ef\u8fbe90.5%\u3002\u5e38\u89c1\u9632\u5fa1\u5b9e\u8df5\uff08\u5982\u63d0\u9192\u63d0\u793a\uff09\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u4fc3\u8fdb\u66f4\u5b89\u5168\u7684\u4ee3\u7406\u5f00\u53d1\u900f\u660e\u5ea6\uff0c\u5f3a\u8c03\u4e86\u641c\u7d22\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.23725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23725", "abs": "https://arxiv.org/abs/2509.23725", "authors": ["Siqi Ma", "Jiajie Huang", "Bolin Yang", "Fan Zhang", "Jinlin Wu", "Yue Shen", "Guohui Fan", "Zhu Zhang", "Zelin Zang"], "title": "MedLA: A Logic-Driven Multi-Agent Framework for Complex Medical Reasoning with Large Language Models", "comment": null, "summary": "Answering complex medical questions requires not only domain expertise and\npatient-specific information, but also structured and multi-perspective\nreasoning. Existing multi-agent approaches often rely on fixed roles or shallow\ninteraction prompts, limiting their ability to detect and resolve fine-grained\nlogical inconsistencies. To address this, we propose \\textsc{MedLA}, a\nlogic-driven multi-agent framework built on large language models. Each agent\norganizes its reasoning process into an explicit logical tree based on\nsyllogistic triads (major premise, minor premise, and conclusion), enabling\ntransparent inference and premise-level alignment. Agents engage in a\nmulti-round, graph-guided discussion to compare and iteratively refine their\nlogic trees, achieving consensus through error correction and contradiction\nresolution. We demonstrate that \\textsc{MedLA} consistently outperforms both\nstatic role-based systems and single-agent baselines on challenging benchmarks\nsuch as MedDDx and standard medical QA tasks. Furthermore, \\textsc{MedLA}\nscales effectively across both open-source and commercial LLM backbones,\nachieving state-of-the-art performance and offering a generalizable paradigm\nfor trustworthy medical reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedLA\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u903b\u8f91\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u663e\u5f0f\u903b\u8f91\u6811\u548c\u56fe\u5f62\u5f15\u5bfc\u7684\u8ba8\u8bba\u6765\u63d0\u9ad8\u533b\u7597\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u5316\u7684\u63a8\u7406\u673a\u5236\u6765\u786e\u4fdd\u533b\u7597\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u4e09\u6bb5\u8bba\uff08\u5927\u524d\u63d0\u3001\u5c0f\u524d\u63d0\u3001\u7ed3\u8bba\uff09\u7684\u663e\u5f0f\u903b\u8f91\u6811\uff0c\u8ba9\u591a\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u591a\u8f6e\u56fe\u5f62\u5f15\u5bfc\u7684\u8ba8\u8bba\u6765\u6bd4\u8f83\u548c\u8fed\u4ee3\u4f18\u5316\u903b\u8f91\u6811\uff0c\u5b9e\u73b0\u9519\u8bef\u7ea0\u6b63\u548c\u77db\u76fe\u89e3\u51b3\u3002", "result": "\u5728MedDDx\u548c\u6807\u51c6\u533b\u7597QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMedLA\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u9759\u6001\u89d2\u8272\u7684\u7cfb\u7edf\u548c\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u5e76\u5728\u5f00\u6e90\u548c\u5546\u4e1aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u90fd\u80fd\u6709\u6548\u6269\u5c55\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MedLA\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u3001\u53ef\u4fe1\u8d56\u7684\u533b\u7597\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u903b\u8f91\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u533b\u7597\u95ee\u9898\u7684\u89e3\u7b54\u8d28\u91cf\u3002", "topic": "agent analysis"}}
{"id": "2509.23730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23730", "abs": "https://arxiv.org/abs/2509.23730", "authors": ["Siyao Song", "Cong Ma", "Zhihao Cheng", "Shiye Lei", "Minghao Li", "Ying Zeng", "Huaixiao Tou", "Kai Jia"], "title": "EAPO: Enhancing Policy Optimization with On-Demand Expert Assistance", "comment": null, "summary": "Large language models (LLMs) have recently advanced in reasoning when\noptimized with reinforcement learning (RL) under verifiable rewards. Existing\nmethods primarily rely on outcome-based supervision to strengthen internal LLM\nreasoning, often leading to inefficient exploration and sparse rewards. To\nmitigate this issue, we propose Expert-Assisted Policy Optimization (EAPO), a\nnovel RL framework that enhances exploration by incorporating multi-turn\ninteractions with external experts during training. Unlike prior methods, where\npolicies reason in isolation, EAPO incentivizes the policy to adaptively\ndetermine when and how to consult experts, yielding richer reward signals and\nmore reliable reasoning trajectories. External assistance ultimately\ninternalizes expert knowledge into the policy model, amplifying the model's\ninherent reasoning capabilities. During evaluation, the policy model has been\nwell-optimized to solve questions independently, producing improved reasoning\npaths and more accurate solutions. Experiments on mathematical reasoning\nbenchmarks, including AIME 2024, AIME 2025, and AIMO 2025, show that EAPO\nconsistently outperforms expert-assisted workflow, expert-distilled models, and\nRL baselines, with an average gain of 5 points over self-exploratory models.", "AI": {"tldr": "\u63d0\u51faEAPO\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5916\u90e8\u4e13\u5bb6\u4ea4\u4e92\u589e\u5f3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\uff0c\u63d0\u9ad8\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u679c\u76d1\u7763\u7684RL\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb", "method": "EAPO\u6846\u67b6\u8ba9\u7b56\u7565\u6a21\u578b\u81ea\u9002\u5e94\u51b3\u5b9a\u4f55\u65f6\u53ca\u5982\u4f55\u54a8\u8be2\u5916\u90e8\u4e13\u5bb6\uff0c\u83b7\u53d6\u66f4\u4e30\u5bcc\u7684\u5956\u52b1\u4fe1\u53f7", "result": "\u5728AIME 2024/2025\u548cAIMO 2025\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cEAPO\u5e73\u5747\u6bd4\u81ea\u63a2\u7d22\u6a21\u578b\u63d0\u53475\u5206", "conclusion": "\u5916\u90e8\u4e13\u5bb6\u534f\u52a9\u80fd\u6709\u6548\u5c06\u4e13\u5bb6\u77e5\u8bc6\u5185\u5316\u5230\u7b56\u7565\u6a21\u578b\u4e2d\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u56fa\u6709\u63a8\u7406\u80fd\u529b", "topic": "agentic reinforcement learning"}}
{"id": "2509.23738", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23738", "abs": "https://arxiv.org/abs/2509.23738", "authors": ["Cong Chen", "Kaixiang Ji", "Hao Zhong", "Muzhi Zhu", "Anzhou Li", "Guo Gan", "Ziyuan Huang", "Cheng Zou", "Jiajia Liu", "Jingdong Chen", "Hao Chen", "Chunhua Shen"], "title": "GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks", "comment": null, "summary": "Autonomous agents for long-sequence Graphical User Interface tasks are\nhindered by sparse rewards and the intractable credit assignment problem. To\naddress these challenges, we introduce GUI-Shepherd, a Process Reward Model\nthat provides dense, step-by-step feedback to guide agents. GUI-Shepherd is\ntrained on a diverse large-scale data set of $52$k interactions that features\nhuman-annotated scores and GPT-4o generated rationales, enabling it to serve\nboth as a reward provider for RL training and as a verifier for inference. As\nfar as we know, we are the first to conduct a systematic study of process\nsupervision in GUI agents, across diverse settings from online long-horizon\ntasks to offline single-step prediction. On the online AndroidWorld benchmark,\nGUI-Shepherd improves success rate by $7.7$ points via multi-turn online PPO,\nsignificantly outperforming Outcome Reward Model based competitors. When used\nas an inference verifier, it brings $5.1$ points improvements. The benefits\ngeneralize to the offline AndroidControl benchmark, with gains of $2.2$ points\nas a reward provider and $4.3$ points as a verifier. Collectively, our results\nestablish that high-fidelity process supervision is critical for building more\ncapable GUI agents and present a generalizable solution.", "AI": {"tldr": "GUI-Shepherd\u662f\u4e00\u4e2a\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u4f9b\u5bc6\u96c6\u7684\u9010\u6b65\u53cd\u9988\u6765\u89e3\u51b3GUI\u4efb\u52a1\u4e2d\u7a00\u758f\u5956\u52b1\u548c\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u5728\u5728\u7ebf\u548c\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u957f\u5e8f\u5217GUI\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u4ee3\u7406\u53d7\u5230\u7a00\u758f\u5956\u52b1\u548c\u96be\u4ee5\u5904\u7406\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u7684\u963b\u788d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u76d1\u7763\u673a\u5236\u3002", "method": "\u5f00\u53d1GUI-Shepherd\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5728\u5305\u542b52k\u4ea4\u4e92\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u5206\u6570\u548cGPT-4o\u751f\u6210\u7684\u539f\u7406\uff0c\u53ef\u4f5c\u4e3aRL\u8bad\u7ec3\u5956\u52b1\u63d0\u4f9b\u8005\u548c\u63a8\u7406\u9a8c\u8bc1\u5668\u3002", "result": "\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u901a\u8fc7\u591a\u8f6e\u5728\u7ebfPPO\u5c06\u6210\u529f\u7387\u63d0\u9ad87.7\u4e2a\u767e\u5206\u70b9\uff1b\u4f5c\u4e3a\u63a8\u7406\u9a8c\u8bc1\u5668\u5e26\u67655.1\u4e2a\u767e\u5206\u70b9\u7684\u6539\u8fdb\uff1b\u5728AndroidControl\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u83b7\u5f972.2\u548c4.3\u4e2a\u767e\u5206\u70b9\u7684\u63d0\u5347\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u7684\u8fc7\u7a0b\u76d1\u7763\u5bf9\u4e8e\u6784\u5efa\u66f4\u5f3a\u5927\u7684GUI\u4ee3\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23757", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23757", "abs": "https://arxiv.org/abs/2509.23757", "authors": ["Benjamin Teoh", "Ben Glocker", "Francesca Toni", "Avinash Kori"], "title": "Transparent Visual Reasoning via Object-Centric Agent Collaboration", "comment": null, "summary": "A central challenge in explainable AI, particularly in the visual domain, is\nproducing explanations grounded in human-understandable concepts. To tackle\nthis, we introduce OCEAN (Object-Centric Explananda via Agent Negotiation), a\nnovel, inherently interpretable framework built on object-centric\nrepresentations and a transparent multi-agent reasoning process. The\ngame-theoretic reasoning process drives agents to agree on coherent and\ndiscriminative evidence, resulting in a faithful and interpretable\ndecision-making process. We train OCEAN end-to-end and benchmark it against\nstandard visual classifiers and popular posthoc explanation tools like GradCAM\nand LIME across two diagnostic multi-object datasets. Our results demonstrate\ncompetitive performance with respect to state-of-the-art black-box models with\na faithful reasoning process, which was reflected by our user study, where\nparticipants consistently rated OCEAN's explanations as more intuitive and\ntrustworthy.", "AI": {"tldr": "OCEAN\u662f\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u548c\u900f\u660e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u63a8\u7406\u8fc7\u7a0b\u751f\u6210\u53ef\u4fe1\u4e14\u76f4\u89c2\u7684\u89e3\u91ca\uff0c\u5728\u89c6\u89c9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u53ef\u89e3\u91caAI\u4e2d\u751f\u6210\u57fa\u4e8e\u4eba\u7c7b\u53ef\u7406\u89e3\u6982\u5ff5\u7684\u89e3\u91ca\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u9886\u57df\u3002", "method": "\u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u548c\u900f\u660e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u9a71\u52a8\u667a\u80fd\u4f53\u8fbe\u6210\u4e00\u81f4\uff0c\u751f\u6210\u8fde\u8d2f\u4e14\u5177\u6709\u533a\u5206\u6027\u7684\u8bc1\u636e\u3002", "result": "\u5728\u4e24\u4e2a\u8bca\u65ad\u6027\u591a\u5bf9\u8c61\u6570\u636e\u96c6\u4e0a\uff0cOCEAN\u4e0e\u6700\u5148\u8fdb\u7684\u9ed1\u76d2\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u89e3\u91ca\u66f4\u76f4\u89c2\u53ef\u4fe1\u3002", "conclusion": "OCEAN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2509.23209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23209", "abs": "https://arxiv.org/abs/2509.23209", "authors": ["Wenhao Zhang", "Shao Zhang", "Xihuai Wang", "Yang Li", "Ying Wen"], "title": "Towards Monotonic Improvement in In-Context Reinforcement Learning", "comment": null, "summary": "In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm\nfor developing agents that can rapidly adapt to new tasks by leveraging past\nexperiences as context, without updating their parameters. Recent approaches\ntrain large sequence models on monotonic policy improvement data from online\nRL, aiming to a continue improved testing time performance. However, our\nexperimental analysis reveals a critical flaw: these models cannot show a\ncontinue improvement like the training data during testing time. Theoretically,\nwe identify this phenomenon as Contextual Ambiguity, where the model's own\nstochastic actions can generate an interaction history that misleadingly\nresembles that of a sub-optimal policy from the training data, initiating a\nvicious cycle of poor action selection. To resolve the Contextual Ambiguity, we\nintroduce Context Value into training phase and propose Context Value Informed\nICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing\nthe ideal performance theoretically achievable by a policy given the current\ncontext. As the context expands, Context Value could include more task-relevant\ninformation, and therefore the ideal performance should be non-decreasing. We\nprove that the Context Value tightens the lower bound on the performance gap\nrelative to an ideal, monotonically improving policy. We fruther propose two\nmethods for estimating Context Value at both training and testing time.\nExperiments conducted on the Dark Room and Minigrid testbeds demonstrate that\nCV-ICRL effectively mitigates performance degradation and improves overall ICRL\nabilities across various tasks and environments. The source code and data of\nthis paper are available at\nhttps://github.com/Bluixe/towards_monotonic_improvement .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u4ef7\u503c\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\uff08CV-ICRL\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u4e0a\u4e0b\u6587\u4ef7\u503c\u6765\u89e3\u51b3\u73b0\u6709ICRL\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u65e0\u6cd5\u6301\u7eed\u6539\u8fdb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u5355\u8c03\u7b56\u7565\u6539\u8fdb\u6570\u636e\uff0c\u4f46\u5728\u6d4b\u8bd5\u65f6\u65e0\u6cd5\u5b9e\u73b0\u6301\u7eed\u6027\u80fd\u6539\u8fdb\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u5bfc\u81f4\u7684\u6076\u6027\u5faa\u73af\u3002", "method": "\u63d0\u51faCV-ICRL\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4e0a\u4e0b\u6587\u4ef7\u503c\u4f5c\u4e3a\u663e\u5f0f\u4fe1\u53f7\uff0c\u8868\u793a\u7ed9\u5b9a\u5f53\u524d\u4e0a\u4e0b\u6587\u65f6\u7406\u8bba\u4e0a\u53ef\u8fbe\u5230\u7684\u7406\u60f3\u6027\u80fd\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u4f30\u8ba1\u4e0a\u4e0b\u6587\u4ef7\u503c\u7684\u65b9\u6cd5\u3002", "result": "\u5728Dark Room\u548cMinigrid\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCV-ICRL\u6709\u6548\u7f13\u89e3\u4e86\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u63d0\u5347\u4e86ICRL\u80fd\u529b\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u4ef7\u503c\u80fd\u591f\u6536\u7d27\u76f8\u5bf9\u4e8e\u7406\u60f3\u5355\u8c03\u6539\u8fdb\u7b56\u7565\u7684\u6027\u80fd\u5dee\u8ddd\u4e0b\u754c\uff0cCV-ICRL\u6210\u529f\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23417", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23417", "abs": "https://arxiv.org/abs/2509.23417", "authors": ["Rajaa El Hamdani", "Samy Haffoudhi", "Nils Holzenberger", "Fabian Suchanek", "Thomas Bonald", "Fragkiskos D. Malliaros"], "title": "Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models", "comment": null, "summary": "Language models (LMs) encode substantial factual knowledge, but often produce\nanswers judged as incorrect. We hypothesize that many of these answers are\nactually correct, but are expressed in alternative surface forms that are\ndismissed due to an overly strict evaluation, leading to an underestimation of\nmodels' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD),\na decoding strategy that restricts model outputs to unique surface forms. We\nintroduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating\nopen-source LMs from 135M to 70B parameters, we show that standard decoding\nundervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1\nwith vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0%\nwith RCD, outperforming the larger model under vanilla decoding. We publicly\nshare the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.", "AI": {"tldr": "\u63d0\u51fa\u68c0\u7d22\u7ea6\u675f\u89e3\u7801(RCD)\u7b56\u7565\uff0c\u901a\u8fc7\u9650\u5236\u6a21\u578b\u8f93\u51fa\u4e3a\u552f\u4e00\u8868\u9762\u5f62\u5f0f\u6765\u66f4\u51c6\u786e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u77e5\u8bc6\uff0c\u5728YAGO-QA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u4e86\u5927\u91cf\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u4f46\u7531\u4e8e\u8bc4\u4f30\u8fc7\u4e8e\u4e25\u683c\uff0c\u8bb8\u591a\u6b63\u786e\u7b54\u6848\u56e0\u8868\u9762\u5f62\u5f0f\u4e0d\u540c\u800c\u88ab\u8bef\u5224\u4e3a\u9519\u8bef\uff0c\u5bfc\u81f4\u4f4e\u4f30\u4e86\u6a21\u578b\u7684\u53c2\u6570\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u68c0\u7d22\u7ea6\u675f\u89e3\u7801(RCD)\u7b56\u7565\uff0c\u9650\u5236\u6a21\u578b\u8f93\u51fa\u4e3a\u552f\u4e00\u7684\u8868\u9762\u5f62\u5f0f\uff1b\u6784\u5efaYAGO-QA\u6570\u636e\u96c6(19,137\u4e2a\u901a\u7528\u77e5\u8bc6\u95ee\u9898)\uff1b\u8bc4\u4f30\u4ece135M\u523070B\u53c2\u6570\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6807\u51c6\u89e3\u7801\u4f4e\u4f30\u4e86\u6a21\u578b\u77e5\u8bc6\uff1aLlama-3.1-70B\u5728\u666e\u901a\u89e3\u7801\u4e0bF1\u4e3a32.3%\uff0c\u4f7f\u7528RCD\u540e\u63d0\u5347\u81f346.0%\uff1bLlama-3.1-8B\u4f7f\u7528RCD\u8fbe\u523033.0%\uff0c\u8d85\u8fc7\u4e86\u5927\u6a21\u578b\u5728\u666e\u901a\u89e3\u7801\u4e0b\u7684\u8868\u73b0\u3002", "conclusion": "RCD\u89e3\u7801\u7b56\u7565\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u77e5\u8bc6\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u4f4e\u4f30\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2509.23232", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23232", "abs": "https://arxiv.org/abs/2509.23232", "authors": ["Bingshuai Liu", "Ante Wang", "Zijun Min", "Liang Yao", "Haibo Zhang", "Yang Liu", "Anxiang Zeng", "Jinsong Su"], "title": "SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on reinforcement learning with\nverifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning.\nHowever, the training process remains bottlenecked by the computationally\nexpensive rollout stage. Existing acceleration methods-such as parallelization,\nobjective- and data-driven modifications, and replay buffers-either incur\ndiminishing returns, introduce bias, or overlook redundancy across iterations.\nWe identify that rollouts from consecutive training epochs frequently share a\nlarge portion of overlapping segments, wasting computation. To address this, we\npropose SPEC-RL, a novel framework that integrates SPECulative decoding with\nthe RL rollout process. SPEC-RL reuses prior trajectory segments as speculative\nprefixes and extends them via a draft-and-verify mechanism, avoiding redundant\ngeneration while ensuring policy consistency. Experiments on diverse math\nreasoning and generalization benchmarks, including GSM8K, MATH-500,\nOlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout\ntime by 2-3x without compromising policy quality. As a purely rollout-stage\nenhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g.,\nPPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large\nreasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL", "AI": {"tldr": "SPEC-RL\u901a\u8fc7\u5c06\u63a8\u6d4b\u89e3\u7801\u4e0eRL rollout\u8fc7\u7a0b\u96c6\u6210\uff0c\u5229\u7528\u5148\u524d\u8f68\u8ff9\u6bb5\u4f5c\u4e3a\u63a8\u6d4b\u524d\u7f00\u5e76\u901a\u8fc7\u8349\u7a3f-\u9a8c\u8bc1\u673a\u5236\u6269\u5c55\uff0c\u51cf\u5c11\u5197\u4f59\u751f\u6210\uff0c\u5728\u4fdd\u6301\u7b56\u7565\u8d28\u91cf\u7684\u540c\u65f6\u5c06rollout\u65f6\u95f4\u51cf\u5c112-3\u500d\u3002", "motivation": "\u73b0\u6709RLVR\u8bad\u7ec3\u8fc7\u7a0b\u53d7\u9650\u4e8e\u8ba1\u7b97\u6602\u8d35\u7684rollout\u9636\u6bb5\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u6536\u76ca\u9012\u51cf\u3001\u5f15\u5165\u504f\u5dee\u6216\u5ffd\u7565\u8fed\u4ee3\u95f4\u5197\u4f59\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSPEC-RL\u6846\u67b6\uff0c\u96c6\u6210\u63a8\u6d4b\u89e3\u7801\u4e0eRL rollout\u8fc7\u7a0b\uff0c\u91cd\u7528\u5148\u524d\u8f68\u8ff9\u6bb5\u4f5c\u4e3a\u63a8\u6d4b\u524d\u7f00\uff0c\u901a\u8fc7\u8349\u7a3f-\u9a8c\u8bc1\u673a\u5236\u6269\u5c55\uff0c\u907f\u514d\u5197\u4f59\u751f\u6210\u540c\u65f6\u786e\u4fdd\u7b56\u7565\u4e00\u81f4\u6027\u3002", "result": "\u5728GSM8K\u3001MATH-500\u3001OlympiadBench\u3001MMLU-STEM\u7b49\u6570\u5b66\u63a8\u7406\u548c\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPEC-RL\u5c06rollout\u65f6\u95f4\u51cf\u5c112-3\u500d\u4e14\u4e0d\u635f\u5bb3\u7b56\u7565\u8d28\u91cf\u3002", "conclusion": "SPEC-RL\u4f5c\u4e3a\u7eafrollout\u9636\u6bb5\u589e\u5f3a\uff0c\u53ef\u4e0e\u4e3b\u6d41\u7b97\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u7684RLVR\u6269\u5c55\u63d0\u4f9b\u4e86\u901a\u7528\u5b9e\u7528\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23836", "abs": "https://arxiv.org/abs/2509.23836", "authors": ["Chenyu Zhou", "Xiaoming Shi", "Hui Qiu", "Xiawu Zheng", "Haitao Leng", "Yankai Jiang", "Shaoguo Liu", "Tingting Gao", "Rongrong Ji"], "title": "Mix-Ecom: Towards Mixed-Type E-Commerce Dialogues with Complex Domain Rules", "comment": null, "summary": "E-commerce agents contribute greatly to helping users complete their\ne-commerce needs. To promote further research and application of e-commerce\nagents, benchmarking frameworks are introduced for evaluating LLM agents in the\ne-commerce domain. Despite the progress, current benchmarks lack evaluating\nagents' capability to handle mixed-type e-commerce dialogue and complex domain\nrules. To address the issue, this work first introduces a novel corpus, termed\nMix-ECom, which is constructed based on real-world customer-service dialogues\nwith post-processing to remove user privacy and add CoT process. Specifically,\nMix-ECom contains 4,799 samples with multiply dialogue types in each e-commerce\ndialogue, covering four dialogue types (QA, recommendation, task-oriented\ndialogue, and chit-chat), three e-commerce task types (pre-sales, logistics,\nafter-sales), and 82 e-commerce rules. Furthermore, this work build baselines\non Mix-Ecom and propose a dynamic framework to further improve the performance.\nResults show that current e-commerce agents lack sufficient capabilities to\nhandle e-commerce dialogues, due to the hallucination cased by complex domain\nrules. The dataset will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mix-ECom\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7535\u5546\u5bf9\u8bdd\u4e2dLLM\u4ee3\u7406\u5904\u7406\u6df7\u5408\u7c7b\u578b\u5bf9\u8bdd\u548c\u590d\u6742\u9886\u57df\u89c4\u5219\u7684\u80fd\u529b\uff0c\u5305\u542b4,799\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u56db\u79cd\u5bf9\u8bdd\u7c7b\u578b\u548c82\u6761\u7535\u5546\u89c4\u5219\u3002", "motivation": "\u5f53\u524d\u7535\u5546\u4ee3\u7406\u57fa\u51c6\u7f3a\u4e4f\u8bc4\u4f30\u5904\u7406\u6df7\u5408\u7c7b\u578b\u7535\u5546\u5bf9\u8bdd\u548c\u590d\u6742\u9886\u57df\u89c4\u5219\u7684\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u771f\u5b9e\u5ba2\u670d\u5bf9\u8bdd\u6784\u5efaMix-ECom\u8bed\u6599\u5e93\uff0c\u5305\u542b\u56db\u79cd\u5bf9\u8bdd\u7c7b\u578b\u3001\u4e09\u79cd\u7535\u5546\u4efb\u52a1\u7c7b\u578b\u548c82\u6761\u7535\u5546\u89c4\u5219\uff0c\u5e76\u5efa\u7acb\u57fa\u7ebf\u6a21\u578b\u548c\u52a8\u6001\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5f53\u524d\u7535\u5546\u4ee3\u7406\u5728\u5904\u7406\u7535\u5546\u5bf9\u8bdd\u65f6\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u590d\u6742\u9886\u57df\u89c4\u5219\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "Mix-ECom\u6570\u636e\u96c6\u586b\u8865\u4e86\u7535\u5546\u4ee3\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u7535\u5546\u5bf9\u8bdd\u65f6\u7684\u5c40\u9650\u6027\u3002", "topic": "swe benchmark"}}
{"id": "2509.23501", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.23501", "abs": "https://arxiv.org/abs/2509.23501", "authors": ["Hamidreza Rouzegar", "Masoud Makrehchi"], "title": "The Impact of Role Design in In-Context Learning for Large Language Models", "comment": "Code is available at\n  https://github.com/hrouzegar/Role_Based-In-Context-Learning", "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to generate\npredictions based on prompts without additional fine-tuning. While prompt\nengineering has been widely studied, the impact of role design within prompts\nremains underexplored. This study examines the influence of role configurations\nin zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from\nOpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models'\nperformance across datasets, focusing on tasks like sentiment analysis, text\nclassification, question answering, and math reasoning. Our findings suggest\nthe potential of role-based prompt structuring to enhance LLM performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u63d0\u793a\u4e2d\u89d2\u8272\u8bbe\u8ba1\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u7ed3\u6784\u6709\u6f5c\u529b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u63d0\u793a\u5de5\u7a0b\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u63d0\u793a\u4e2d\u89d2\u8272\u8bbe\u8ba1\u7684\u5f71\u54cd\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528GPT-3.5\u3001GPT-4o\u3001Llama2-7b\u548cLlama2-13b\u6a21\u578b\uff0c\u5728\u60c5\u611f\u5206\u6790\u3001\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u8bc4\u4f30\u89d2\u8272\u914d\u7f6e\u5bf9\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u89d2\u8272\u7684\u63d0\u793a\u7ed3\u6784\u80fd\u591f\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u89d2\u8272\u8bbe\u8ba1\u5728\u63d0\u793a\u5de5\u7a0b\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.23870", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23870", "abs": "https://arxiv.org/abs/2509.23870", "authors": ["Jingyu Liu", "Xiaopeng Wu", "Jingquan Peng", "Kehan Chen", "Chuan Yu", "Lizhong Ding", "Yong Liu"], "title": "Rethinking Reward Miscalibration of GRPO in Agentic RL", "comment": null, "summary": "Building autonomous agents capable of solving long-horizon, real-world tasks\nhas garnered significant research interest. But outcome based rewards may cause\nreward miscalibration which means it might mistakenly allocate positive reward\nto flawed middle steps which is regarded as the key reason making the bad\nactions being reinforced during training. However we reveal that outcome based\nreward ensures expected negative advantage for those flawed middle steps, which\nmeans the flawed actions should be punished during training. Even accounting\nfor the ``squeezing effect\", the probability mass of good actions should\nincrease and the actor should gradually get rid of harmful actions. This shows\nthat flawed actions should be punished during training. We further identify\ngradient coupling between similar samples as a key issue in agentic RL, the\ninput prompt is extremely similar and the output action space is limited,\ntherefore during training, gradients from well-performing samples can\ninadvertently strengthen suboptimal or incorrect actions due to similar input\nobservation and output actions. We show that with gradient coupling, some\nflawed actions might be enhanced. To address this, we propose training the\nactor to classify good or bad actions to separate the embedding of good/bad\nactions and alleviate the gradient interference, extensive experiments shows\nits effectiveness.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u4f20\u7edf\u89c2\u70b9\uff0c\u6307\u51fa\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u4e0d\u4f1a\u9519\u8bef\u5f3a\u5316\u6709\u7f3a\u9677\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u53cd\u800c\u4f1a\u60e9\u7f5a\u8fd9\u4e9b\u6b65\u9aa4\u3002\u7814\u7a76\u53d1\u73b0\u68af\u5ea6\u8026\u5408\u624d\u662f\u5bfc\u81f4\u6709\u7f3a\u9677\u52a8\u4f5c\u88ab\u589e\u5f3a\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u52a8\u4f5c\u5206\u7c7b\u8bad\u7ec3\u6765\u7f13\u89e3\u68af\u5ea6\u5e72\u6270\u3002", "motivation": "\u4f20\u7edf\u8ba4\u4e3a\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u4f1a\u5bfc\u81f4\u5956\u52b1\u6821\u51c6\u9519\u8bef\uff0c\u9519\u8bef\u5730\u5f3a\u5316\u6709\u7f3a\u9677\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002\u4f46\u672c\u6587\u65e8\u5728\u8bc1\u660e\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u5b9e\u9645\u4e0a\u4f1a\u60e9\u7f5a\u8fd9\u4e9b\u6709\u7f3a\u9677\u7684\u6b65\u9aa4\uff0c\u5e76\u63ed\u793a\u68af\u5ea6\u8026\u5408\u624d\u662f\u771f\u6b63\u7684\u95ee\u9898\u6240\u5728\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3actor\u8fdb\u884c\u597d\u574f\u52a8\u4f5c\u5206\u7c7b\uff0c\u4ee5\u5206\u79bb\u597d/\u574f\u52a8\u4f5c\u7684\u5d4c\u5165\u8868\u793a\uff0c\u4ece\u800c\u7f13\u89e3\u68af\u5ea6\u5e72\u6270\u95ee\u9898\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u786e\u5b9e\u4f1a\u60e9\u7f5a\u6709\u7f3a\u9677\u7684\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4f46\u68af\u5ea6\u8026\u5408\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u6709\u7f3a\u9677\u52a8\u4f5c\u88ab\u589e\u5f3a\u3002\u63d0\u51fa\u7684\u52a8\u4f5c\u5206\u7c7b\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "\u68af\u5ea6\u8026\u5408\u662f\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u7c7b\u8bad\u7ec3\u53ef\u4ee5\u6709\u6548\u5206\u79bb\u597d/\u574f\u52a8\u4f5c\u7684\u5d4c\u5165\uff0c\u51cf\u8f7b\u68af\u5ea6\u5e72\u6270\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23252", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23252", "abs": "https://arxiv.org/abs/2509.23252", "authors": ["Raviteja Anantha", "Soheil Hor", "Teodor Nicola Antoniu", "Layne C. Price"], "title": "NanoFlux: Adversarial Dual-LLM Evaluation and Distillation For Multi-Domain Reasoning", "comment": "preprint version", "summary": "We present NanoFlux, a novel adversarial framework for generating targeted\ntraining data to improve LLM reasoning, where adversarially-generated datasets\ncontaining fewer than 200 examples outperform conventional fine-tuning\napproaches. The framework employs a competitive dynamic between models\nalternating as Attacker and Defender, supervised by a tool-augmented Judge,\nsynthesizing multi-step questions with explanatory annotations that target\nspecific reasoning capabilities. Fine-tuning a 4B-parameter model on\nNanoFlux-generated data yields performance gains across diverse domains\ncompared to full-benchmark fine-tuning: +5.9% on mathematical reasoning\n(GSMHard), +3.6% on scientific reasoning (GenomeBench), and +16.6% on medical\nreasoning (MultiMedQA), while reducing computational requirements by 3-14x.\nAblation studies reveal a non-monotonic relationship between dataset\ncharacteristics and model performance, uncovering domain-specific optimal\npoints for question complexity and reasoning quality. NanoFlux automates\ntraining data generation through embedding-based novelty filtering,\ntool-augmented evaluation, and multi-hop reasoning, suggesting that future\nmodel improvements may lie in the intelligent synthesis of small, precisely\ntargeted training datasets.", "AI": {"tldr": "NanoFlux\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5c11\u4e8e200\u4e2a\u793a\u4f8b\u7684\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u5408\u6210\u5c11\u91cf\u4f46\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u6a21\u578b\u4ea4\u66ff\u626e\u6f14\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u89d2\u8272\uff0c\u7531\u5de5\u5177\u589e\u5f3a\u7684\u8bc4\u5224\u8005\u76d1\u7763\uff0c\u751f\u6210\u5305\u542b\u89e3\u91ca\u6027\u6ce8\u91ca\u7684\u591a\u6b65\u9aa4\u95ee\u9898\uff0c\u9488\u5bf9\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bad\u7ec3\u6570\u636e\u5408\u6210\u3002", "result": "\u57284B\u53c2\u6570\u6a21\u578b\u4e0a\uff0cNanoFlux\u751f\u6210\u7684\u6570\u636e\u76f8\u6bd4\u5168\u57fa\u51c6\u5fae\u8c03\u5728\u6570\u5b66\u63a8\u7406\u4e0a\u63d0\u53475.9%\uff0c\u79d1\u5b66\u63a8\u7406\u63d0\u53473.6%\uff0c\u533b\u5b66\u63a8\u7406\u63d0\u534716.6%\uff0c\u540c\u65f6\u8ba1\u7b97\u9700\u6c42\u51cf\u5c113-14\u500d\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6a21\u578b\u6539\u8fdb\u53ef\u80fd\u5728\u4e8e\u667a\u80fd\u5408\u6210\u5c0f\u578b\u3001\u7cbe\u786e\u9488\u5bf9\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u800c\u975e\u5355\u7eaf\u6269\u5927\u6570\u636e\u89c4\u6a21\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23882", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23882", "abs": "https://arxiv.org/abs/2509.23882", "authors": ["Shuyi Lin", "Tian Lu", "Zikai Wang", "Bo Wen", "Yibo Zhao", "Cheng Tan"], "title": "Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B", "comment": null, "summary": "OpenAI's GPT-OSS family provides open-weight language models with explicit\nchain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an\nextensive security evaluation of GPT-OSS-20B that probes the model's behavior\nunder different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a\nsystematic LLM evaluation tool, the study uncovers several failure modes\nincluding quant fever, reasoning blackholes, Schrodinger's compliance,\nreasoning procedure mirage, and chain-oriented prompting. Experiments\ndemonstrate how these behaviors can be exploited on GPT-OSS-20B models, leading\nto severe consequences.", "AI": {"tldr": "\u5bf9GPT-OSS-20B\u6a21\u578b\u8fdb\u884c\u5b89\u5168\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u6a21\u578b\u7684\u591a\u79cd\u5931\u6548\u6a21\u5f0f\uff0c\u5305\u62ec\u91cf\u5316\u70ed\u3001\u63a8\u7406\u9ed1\u6d1e\u3001\u859b\u5b9a\u8c14\u5408\u89c4\u6027\u3001\u63a8\u7406\u8fc7\u7a0b\u5e7b\u8c61\u548c\u94fe\u5f0f\u5bfc\u5411\u63d0\u793a\u7b49\u3002", "motivation": "\u8bc4\u4f30GPT-OSS\u7cfb\u5217\u5f00\u6e90\u6a21\u578b\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u4f7f\u7528Jailbreak Oracle (JO)\u7cfb\u7edf\u5316LLM\u8bc4\u4f30\u5de5\u5177\uff0c\u5728GPT-OSS-20B\u6a21\u578b\u4e0a\u8fdb\u884c\u5bf9\u6297\u6027\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u4e86\u591a\u79cd\u5931\u6548\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u884c\u4e3a\u53ef\u4ee5\u88ab\u5229\u7528\uff0c\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002", "conclusion": "GPT-OSS-20B\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b89\u5168\u52a0\u56fa\u3002", "topic": "agent analysis"}}
{"id": "2509.23574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23574", "abs": "https://arxiv.org/abs/2509.23574", "authors": ["Jianzhi Yan", "Le Liu", "Youcheng Pan", "Shiwei Chen", "Yang Xiang", "Buzhou Tang"], "title": "Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales", "comment": "18 pages, 10 figures", "summary": "Chain-of-thought (CoT) distillation aims to enhance small language models'\n(SLMs) reasoning by transferring multi-step reasoning capability from the\nlarger teacher models. However, existing work underestimates rationale quality,\nfocusing primarily on data quantity, which may transfer noisy or incorrect\ninformation to the student model. To address the above issues, we proposed\n\\textbf{M}odel-\\textbf{O}riented \\textbf{R}ationale \\textbf{S}election\n\\textbf{D}istillation (MoRSD), which can discern and select high quality\nrationales for distillation to improve performance further. We further propose\na Rationale Difficulty (RD) metric to measure the ability of the student model\nto generate the correct answer under a given rationale. Compared to the\nbaseline, we achieved 4.6$\\%$ average improvement on seven datasets over three\ntasks, using fewer rationales by controlling their accuracy, diversity, and\ndifficulty. Our results reveal that a small portion of the high quality\nrationales can enhance the reasoning ability of student models than the entire\ndataset. Our method promises to be a possible solution for efficient CoT\ndistillation. Our code will be released in https://github.com/Leon221220/MoRSD.", "AI": {"tldr": "\u63d0\u51faMoRSD\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\u6765\u6539\u8fdbCoT\u84b8\u998f\uff0c\u76f8\u6bd4\u57fa\u7ebf\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53474.6%\uff0c\u4f7f\u7528\u66f4\u5c11\u4f46\u66f4\u4f18\u8d28\u7684\u63a8\u7406\u94fe\u3002", "motivation": "\u73b0\u6709CoT\u84b8\u998f\u65b9\u6cd5\u4f4e\u4f30\u63a8\u7406\u94fe\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u91cf\uff0c\u53ef\u80fd\u5c06\u566a\u58f0\u6216\u9519\u8bef\u4fe1\u606f\u4f20\u9012\u7ed9\u5b66\u751f\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u6a21\u578b\u5bfc\u5411\u63a8\u7406\u94fe\u9009\u62e9\u84b8\u998f(MoRSD)\uff0c\u80fd\u591f\u8bc6\u522b\u548c\u9009\u62e9\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\u8fdb\u884c\u84b8\u998f\uff0c\u5e76\u63d0\u51fa\u63a8\u7406\u94fe\u96be\u5ea6(RD)\u6307\u6807\u6765\u8861\u91cf\u5b66\u751f\u6a21\u578b\u5728\u7ed9\u5b9a\u63a8\u7406\u94fe\u4e0b\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u4efb\u52a1\u7684\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u63d0\u53474.6%\uff0c\u901a\u8fc7\u63a7\u5236\u63a8\u7406\u94fe\u7684\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u96be\u5ea6\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u63a8\u7406\u94fe\u5b9e\u73b0\u66f4\u597d\u6548\u679c\u3002", "conclusion": "\u5c11\u91cf\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\u6bd4\u6574\u4e2a\u6570\u636e\u96c6\u66f4\u80fd\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548CoT\u84b8\u998f\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.23280", "categories": ["cs.LG", "cs.AI", "math.OC", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2509.23280", "abs": "https://arxiv.org/abs/2509.23280", "authors": ["Yilie Huang"], "title": "Continuous-Time Reinforcement Learning for Asset-Liability Management", "comment": "Accepted at the 6th ACM International Conference on AI in Finance\n  (ICAIF 2025), 8 pages, 2 figures", "summary": "This paper proposes a novel approach for Asset-Liability Management (ALM) by\nemploying continuous-time Reinforcement Learning (RL) with a linear-quadratic\n(LQ) formulation that incorporates both interim and terminal objectives. We\ndevelop a model-free, policy gradient-based soft actor-critic algorithm\ntailored to ALM for dynamically synchronizing assets and liabilities. To ensure\nan effective balance between exploration and exploitation with minimal tuning,\nwe introduce adaptive exploration for the actor and scheduled exploration for\nthe critic. Our empirical study evaluates this approach against two enhanced\ntraditional financial strategies, a model-based continuous-time RL method, and\nthree state-of-the-art RL algorithms. Evaluated across 200 randomized market\nscenarios, our method achieves higher average rewards than all alternative\nstrategies, with rapid initial gains and sustained superior performance. The\noutperformance stems not from complex neural networks or improved parameter\nestimation, but from directly learning the optimal ALM strategy without\nlearning the environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u7684\u8d44\u4ea7-\u8d1f\u503a\u7ba1\u7406\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u7ebf\u6027\u4e8c\u6b21\u89c4\u5212\u7ed3\u5408\u4e2d\u671f\u548c\u6700\u7ec8\u76ee\u6807\uff0c\u901a\u8fc7\u6a21\u578b\u65e0\u5173\u7684\u7b56\u7565\u68af\u5ea6\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u5b9e\u73b0\u8d44\u4ea7\u4e0e\u8d1f\u503a\u7684\u52a8\u6001\u540c\u6b65\u3002", "motivation": "\u4f20\u7edf\u8d44\u4ea7-\u8d1f\u503a\u7ba1\u7406\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e73\u8861\u8d44\u4ea7\u4e0e\u8d1f\u503a\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u540c\u6b65\u8d44\u4ea7\u4e0e\u8d1f\u503a\u7684\u667a\u80fd\u65b9\u6cd5\uff0c\u540c\u65f6\u517c\u987e\u4e2d\u671f\u548c\u6700\u7ec8\u76ee\u6807\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u578b\u65e0\u5173\u7684\u7b56\u7565\u68af\u5ea6\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u5f15\u5165\u6f14\u5458\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u548c\u8bc4\u8bba\u5bb6\u7684\u8c03\u5ea6\u63a2\u7d22\u673a\u5236\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u6846\u67b6\u4e0b\u4f7f\u7528\u7ebf\u6027\u4e8c\u6b21\u89c4\u5212\u3002", "result": "\u5728200\u4e2a\u968f\u673a\u5e02\u573a\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u66ff\u4ee3\u7b56\u7565\u4e2d\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u5956\u52b1\uff0c\u5177\u6709\u5feb\u901f\u521d\u59cb\u6536\u76ca\u548c\u6301\u7eed\u4f18\u5f02\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u91d1\u878d\u7b56\u7565\u548c\u73b0\u6709RL\u7b97\u6cd5\uff0c\u5176\u4f18\u52bf\u4e0d\u662f\u6765\u81ea\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u6216\u6539\u8fdb\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u800c\u662f\u76f4\u63a5\u5b66\u4e60\u6700\u4f18ALM\u7b56\u7565\u800c\u65e0\u9700\u5b66\u4e60\u73af\u5883\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23986", "abs": "https://arxiv.org/abs/2509.23986", "authors": ["Alistair Turcan", "Kexin Huang", "Lei Li", "Martin Jinye Zhang"], "title": "TusoAI: Agentic Optimization for Scientific Methods", "comment": null, "summary": "Scientific discovery is often slowed by the manual development of\ncomputational tools needed to analyze complex experimental data. Building such\ntools is costly and time-consuming because scientists must iteratively review\nliterature, test modeling and scientific assumptions against empirical data,\nand implement these insights into efficient software. Large language models\n(LLMs) have demonstrated strong capabilities in synthesizing literature,\nreasoning with empirical data, and generating domain-specific code, offering\nnew opportunities to accelerate computational method development. Existing\nLLM-based systems either focus on performing scientific analyses using existing\ncomputational methods or on developing computational methods or models for\ngeneral machine learning without effectively integrating the often unstructured\nknowledge specific to scientific domains. Here, we introduce TusoAI , an\nagentic AI system that takes a scientific task description with an evaluation\nfunction and autonomously develops and optimizes computational methods for the\napplication. TusoAI integrates domain knowledge into a knowledge tree\nrepresentation and performs iterative, domain-specific optimization and model\ndiagnosis, improving performance over a pool of candidate solutions. We\nconducted comprehensive benchmark evaluations demonstrating that TusoAI\noutperforms state-of-the-art expert methods, MLE agents, and scientific AI\nagents across diverse tasks, such as single-cell RNA-seq data denoising and\nsatellite-based earth monitoring. Applying TusoAI to two key open problems in\ngenetics improved existing computational methods and uncovered novel biology,\nincluding 9 new associations between autoimmune diseases and T cell subtypes\nand 7 previously unreported links between disease variants linked to their\ntarget genes. Our code is publicly available at\nhttps://github.com/Alistair-Turcan/TusoAI.", "AI": {"tldr": "TusoAI\u662f\u4e00\u4e2a\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\uff0c\u80fd\u591f\u6839\u636e\u79d1\u5b66\u4efb\u52a1\u63cf\u8ff0\u548c\u8bc4\u4f30\u51fd\u6570\u81ea\u4e3b\u5f00\u53d1\u548c\u4f18\u5316\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u5355\u7ec6\u80deRNA-seq\u6570\u636e\u53bb\u566a\u548c\u536b\u661f\u5730\u7403\u76d1\u6d4b\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u53d1\u73b0\u5e38\u56e0\u9700\u8981\u624b\u52a8\u5f00\u53d1\u8ba1\u7b97\u5de5\u5177\u6765\u5206\u6790\u590d\u6742\u5b9e\u9a8c\u6570\u636e\u800c\u53d7\u963b\uff0c\u73b0\u6709LLM\u7cfb\u7edf\u8981\u4e48\u4e13\u6ce8\u4e8e\u4f7f\u7528\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u79d1\u5b66\u5206\u6790\uff0c\u8981\u4e48\u5f00\u53d1\u901a\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u672a\u80fd\u6709\u6548\u6574\u5408\u79d1\u5b66\u9886\u57df\u7279\u6709\u7684\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u3002", "method": "TusoAI\u5c06\u9886\u57df\u77e5\u8bc6\u6574\u5408\u5230\u77e5\u8bc6\u6811\u8868\u793a\u4e2d\uff0c\u6267\u884c\u8fed\u4ee3\u7684\u9886\u57df\u7279\u5b9a\u4f18\u5316\u548c\u6a21\u578b\u8bca\u65ad\uff0c\u901a\u8fc7\u5019\u9009\u89e3\u51b3\u65b9\u6848\u6c60\u6539\u8fdb\u6027\u80fd\u3002", "result": "\u57fa\u51c6\u8bc4\u4f30\u663e\u793aTusoAI\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e13\u5bb6\u65b9\u6cd5\u3001MLE\u4ee3\u7406\u548c\u79d1\u5b66AI\u4ee3\u7406\u3002\u5e94\u7528\u4e8e\u9057\u4f20\u5b66\u5173\u952e\u5f00\u653e\u95ee\u9898\u65f6\u6539\u8fdb\u4e86\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u5e76\u53d1\u73b0\u4e86\u65b0\u7684\u751f\u7269\u5b66\u5173\u8054\u3002", "conclusion": "TusoAI\u80fd\u591f\u52a0\u901f\u8ba1\u7b97\u65b9\u6cd5\u5f00\u53d1\uff0c\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u81ea\u4e3b\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "topic": "agent analysis"}}
{"id": "2509.23988", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.23988", "abs": "https://arxiv.org/abs/2509.23988", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "title": "LLM/Agent-as-Data-Analyst: A Survey", "comment": "35 page, 11 figures", "summary": "Large language model (LLM) and agent techniques for data analysis (a.k.a\nLLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both\nacademica and industry. In comparison with traditional rule or small-model\nbased approaches, (agentic) LLMs enable complex data understanding, natural\nlanguage interfaces, semantic analysis functions, and autonomous pipeline\norchestration. The technical evolution further distills five key design goals\nfor intelligent data analysis agents, namely semantic-aware design,\nmodality-hybrid integration, autonomous pipelines, tool-augmented workflows,\nand support for open-world tasks. From a modality perspective, we review\nLLM-based techniques for (i) structured data (e.g., table question answering\nfor relational data and NL2GQL for graph data), (ii) semi-structured data\n(e.g., markup languages understanding and semi-structured table modeling),\n(iii) unstructured data (e.g., chart understanding, document understanding,\nprogramming languages vulnerable detection), and (iv) heterogeneous data (e.g.,\ndata retrieval and modality alignment for data lakes). Finally, we outline the\nremaining challenges and propose several insights and practical directions for\nadvancing LLM/Agent-powered data analysis.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u4ee3\u7406\u7684\u6570\u636e\u5206\u6790\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u7ed3\u6784\u5316\u3001\u534a\u7ed3\u6784\u5316\u3001\u975e\u7ed3\u6784\u5316\u548c\u5f02\u6784\u6570\u636e\u4e2d\u7684\u591a\u6a21\u6001\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u6216\u5c0f\u6a21\u578b\u7684\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u7406\u89e3\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u800c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u6280\u672f\u80fd\u591f\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u3001\u591a\u6a21\u6001\u96c6\u6210\u548c\u81ea\u4e3b\u7ba1\u9053\u7f16\u6392\uff0c\u63a8\u52a8\u6570\u636e\u5206\u6790\u5411\u667a\u80fd\u5316\u53d1\u5c55\u3002", "method": "\u4ece\u6a21\u6001\u89d2\u5ea6\u7cfb\u7edf\u56de\u987eLLM\u6280\u672f\u5728\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u8868\u95ee\u7b54\u3001NL2GQL\uff09\u3001\u534a\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u6807\u8bb0\u8bed\u8a00\u7406\u89e3\uff09\u3001\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u5982\u56fe\u8868\u7406\u89e3\u3001\u6587\u6863\u5206\u6790\uff09\u548c\u5f02\u6784\u6570\u636e\uff08\u5982\u6570\u636e\u68c0\u7d22\u3001\u6a21\u6001\u5bf9\u9f50\uff09\u4e2d\u7684\u5e94\u7528\u3002", "result": "LLM/Agent\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u5206\u6790\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3001\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4f53\u9a8c\u548c\u81ea\u4e3b\u5de5\u4f5c\u6d41\u7f16\u6392\uff0c\u5b9e\u73b0\u4e86\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u6790\u4efb\u52a1\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6570\u636e\u5206\u6790\u4ee3\u7406\u5728\u8bed\u4e49\u611f\u77e5\u3001\u591a\u6a21\u6001\u96c6\u6210\u548c\u81ea\u4e3b\u7ba1\u9053\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2509.24086", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24086", "abs": "https://arxiv.org/abs/2509.24086", "authors": ["Miguel Angel Alvarado Gonzalez", "Michelle Bruno Hernandez", "Miguel Angel Pe\u00f1aloza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Sandra Malagon"], "title": "Do Repetitions Matter? Strengthening Reliability in LLM Evaluations", "comment": null, "summary": "LLM leaderboards often rely on single stochastic runs, but how many\nrepetitions are required for reliable conclusions remains unclear. We\nre-evaluate eight state-of-the-art models on the AI4Math Benchmark with three\nindependent runs per setting. Using mixed-effects logistic regression,\ndomain-level marginal means, rank-instability analysis, and run-to-run\nreliability, we assessed the value of additional repetitions. Our findings\nshows that Single-run leaderboards are brittle: 10/12 slices (83\\%) invert at\nleast one pairwise rank relative to the three-run majority, despite a zero\nsign-flip rate for pairwise significance and moderate overall interclass\ncorrelation. Averaging runs yields modest SE shrinkage ($\\sim$5\\% from one to\nthree) but large ranking gains; two runs remove $\\sim$83\\% of single-run\ninversions. We provide cost-aware guidance for practitioners: treat evaluation\nas an experiment, report uncertainty, and use $\\geq 2$ repetitions under\nstochastic decoding. These practices improve robustness while remaining\nfeasible for small teams and help align model comparisons with real-world\nreliability.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5355\u6b21\u8fd0\u884c\u7684LLM\u6392\u884c\u699c\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u81f3\u5c112\u6b21\u91cd\u590d\u8fd0\u884c\u6765\u83b7\u5f97\u7a33\u5b9a\u7684\u6a21\u578b\u6392\u540d\u7ed3\u679c\u3002", "motivation": "\u5f53\u524dLLM\u6392\u884c\u699c\u901a\u5e38\u4f9d\u8d56\u5355\u6b21\u968f\u673a\u8fd0\u884c\uff0c\u4f46\u9700\u8981\u591a\u5c11\u91cd\u590d\u8fd0\u884c\u624d\u80fd\u5f97\u51fa\u53ef\u9760\u7ed3\u8bba\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5728AI4Math\u57fa\u51c6\u4e0a\u5bf98\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c3\u6b21\u72ec\u7acb\u8fd0\u884c\uff0c\u4f7f\u7528\u6df7\u5408\u6548\u5e94\u903b\u8f91\u56de\u5f52\u3001\u9886\u57df\u7ea7\u8fb9\u9645\u5747\u503c\u3001\u6392\u540d\u4e0d\u7a33\u5b9a\u6027\u5206\u6790\u548c\u8fd0\u884c\u95f4\u53ef\u9760\u6027\u8bc4\u4f30\u3002", "result": "\u5355\u6b21\u8fd0\u884c\u6392\u884c\u699c\u5f88\u8106\u5f31\uff1a83%\u7684\u5207\u7247\u81f3\u5c11\u51fa\u73b0\u4e00\u6b21\u6392\u540d\u53cd\u8f6c\uff1b\u4e24\u6b21\u8fd0\u884c\u53ef\u6d88\u9664\u7ea683%\u7684\u5355\u6b21\u8fd0\u884c\u6392\u540d\u53cd\u8f6c\u3002", "conclusion": "\u5e94\u5c06\u8bc4\u4f30\u89c6\u4e3a\u5b9e\u9a8c\uff0c\u62a5\u544a\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u968f\u673a\u89e3\u7801\u4e0b\u4f7f\u7528\u22652\u6b21\u91cd\u590d\u8fd0\u884c\uff0c\u8fd9\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u4e14\u5bf9\u5c0f\u56e2\u961f\u53ef\u884c\u3002", "topic": "agent analysis"}}
{"id": "2509.24107", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24107", "abs": "https://arxiv.org/abs/2509.24107", "authors": ["Shreyas Singh", "Kunal Singh", "Pradeep Moturi"], "title": "Fathom-DeepResearch: Unlocking Long Horizon Information Retrieval and Synthesis for SLMs", "comment": null, "summary": "Tool-integrated reasoning has emerged as a key focus for enabling agentic\napplications. Among these, DeepResearch Agents have gained significant\nattention for their strong performance on complex, open-ended\ninformation-seeking tasks. We introduce Fathom-DeepResearch, an agentic system\ncomposed of two specialized models. The first is Fathom-Search-4B, a DeepSearch\nmodel trained from Qwen3-4B and optimized for evidence-based investigation\nthrough live web search and targeted webpage querying. Its training combines\nthree advances: (i) DUETQA, a 5K-sample dataset generated via multi-agent\nself-play that enforces strict web-search dependence and heterogeneous source\ngrounding; (ii) RAPO, a zero-overhead extension of GRPO that stabilizes\nmulti-turn Reinforcement Learning with Verifiable Rewards through curriculum\npruning, reward-aware advantage scaling, and per-prompt replay buffers; and\n(iii) a steerable step-level reward that classifies each tool call by cognitive\nbehavior and marginal utility, enabling explicit control over search trajectory\nbreadth, depth, and horizon. These improvements enable reliable extension of\ntool-calling beyond 20 calls when warranted. The second is\nFathom-Synthesizer-4B, trained from Qwen3-4B, which converts multi-turn\nDeepSearch traces into structured, citation-dense DeepResearch Reports for\ncomprehensive synthesis. Evaluated on DeepSearch benchmarks (SimpleQA, FRAMES,\nWebWalker, Seal0, MuSiQue) and DeepResearch-Bench, the system achieves\nstate-of-the-art performance in the open-weights category while demonstrating\nstrong generalization to diverse reasoning tasks including HLE, AIME-25,\nGPQA-Diamond, and MedQA.", "AI": {"tldr": "Fathom-DeepResearch\u662f\u4e00\u4e2a\u7531\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\u7ec4\u6210\u7684\u667a\u80fd\u7cfb\u7edf\uff1aFathom-Search-4B\u7528\u4e8e\u57fa\u4e8e\u8bc1\u636e\u7684\u7f51\u7edc\u641c\u7d22\u8c03\u67e5\uff0cFathom-Synthesizer-4B\u7528\u4e8e\u5c06\u641c\u7d22\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7814\u7a76\u62a5\u544a\u3002\u8be5\u7cfb\u7edf\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u5177\u96c6\u6210\u63a8\u7406\u5df2\u6210\u4e3a\u5b9e\u73b0\u667a\u80fd\u5e94\u7528\u7684\u5173\u952e\u7126\u70b9\uff0cDeepResearch\u667a\u80fd\u4f53\u56e0\u5176\u5728\u590d\u6742\u3001\u5f00\u653e\u5f0f\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u8868\u73b0\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\uff1a1) Fathom-Search-4B\uff1a\u57fa\u4e8eQwen3-4B\u8bad\u7ec3\uff0c\u4f7f\u7528DUETQA\u6570\u636e\u96c6\u3001RAPO\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u5f15\u5bfc\u7684\u6b65\u9aa4\u7ea7\u5956\u52b1\u8fdb\u884c\u4f18\u5316\uff1b2) Fathom-Synthesizer-4B\uff1a\u5c06\u591a\u8f6e\u641c\u7d22\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7814\u7a76\u62a5\u544a\u3002", "result": "\u5728DeepSearch\u57fa\u51c6\u6d4b\u8bd5\uff08SimpleQA\u3001FRAMES\u3001WebWalker\u3001Seal0\u3001MuSiQue\uff09\u548cDeepResearch-Bench\u4e0a\u53d6\u5f97\u4e86\u5f00\u6e90\u6743\u91cd\u7c7b\u522b\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728HLE\u3001AIME-25\u3001GPQA-Diamond\u548cMedQA\u7b49\u591a\u6837\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Fathom-DeepResearch\u7cfb\u7edf\u901a\u8fc7\u4e13\u95e8\u7684\u641c\u7d22\u548c\u5408\u6210\u6a21\u578b\u7ec4\u5408\uff0c\u5728\u590d\u6742\u4fe1\u606f\u641c\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.23323", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23323", "abs": "https://arxiv.org/abs/2509.23323", "authors": ["Xiangchen Song", "Jiaqi Sun", "Zijian Li", "Yujia Zheng", "Kun Zhang"], "title": "LLM Interpretability with Identifiable Temporal-Instantaneous Representation", "comment": "NeurIPS 2025", "summary": "Despite Large Language Models' remarkable capabilities, understanding their\ninternal representations remains challenging. Mechanistic interpretability\ntools such as sparse autoencoders (SAEs) were developed to extract\ninterpretable features from LLMs but lack temporal dependency modeling,\ninstantaneous relation representation, and more importantly theoretical\nguarantees, undermining both the theoretical foundations and the practical\nconfidence necessary for subsequent analyses. While causal representation\nlearning (CRL) offers theoretically grounded approaches for uncovering latent\nconcepts, existing methods cannot scale to LLMs' rich conceptual space due to\ninefficient computation. To bridge the gap, we introduce an identifiable\ntemporal causal representation learning framework specifically designed for\nLLMs' high-dimensional concept space, capturing both time-delayed and\ninstantaneous causal relations. Our approach provides theoretical guarantees\nand demonstrates efficacy on synthetic datasets scaled to match real-world\ncomplexity. By extending SAE techniques with our temporal causal framework, we\nsuccessfully discover meaningful concept relationships in LLM activations. Our\nfindings show that modeling both temporal and instantaneous conceptual\nrelationships advances the interpretability of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc6\u522b\u7684\u65f6\u95f4\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9LLM\u7684\u9ad8\u7ef4\u6982\u5ff5\u7a7a\u95f4\uff0c\u540c\u65f6\u6355\u6349\u65f6\u95f4\u5ef6\u8fdf\u548c\u77ac\u65f6\u56e0\u679c\u5173\u7cfb\uff0c\u4e3aLLM\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5982\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7f3a\u4e4f\u65f6\u95f4\u4f9d\u8d56\u5efa\u6a21\u3001\u77ac\u65f6\u5173\u7cfb\u8868\u793a\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u800c\u56e0\u679c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u7531\u4e8e\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u65e0\u6cd5\u6269\u5c55\u5230LLM\u7684\u4e30\u5bcc\u6982\u5ff5\u7a7a\u95f4\u3002", "method": "\u5f15\u5165\u53ef\u8bc6\u522b\u7684\u65f6\u95f4\u56e0\u679c\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6269\u5c55\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6280\u672f\uff0c\u6355\u6349\u65f6\u95f4\u5ef6\u8fdf\u548c\u77ac\u65f6\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u5339\u914d\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u5ea6\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u6709\u6548\uff0c\u6210\u529f\u53d1\u73b0LLM\u6fc0\u6d3b\u4e2d\u6709\u610f\u4e49\u7684\u6982\u5ff5\u5173\u7cfb\u3002", "conclusion": "\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u548c\u77ac\u65f6\u6982\u5ff5\u5173\u7cfb\u80fd\u591f\u63a8\u8fdbLLM\u7684\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.23657", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23657", "abs": "https://arxiv.org/abs/2509.23657", "authors": ["Shulin Huang", "Yiran Ding", "Junshu Pan", "Yue Zhang"], "title": "Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs", "comment": null, "summary": "Enhancing the complex reasoning capabilities of Large Language Models (LLMs)\nattracts widespread attention. While reinforcement learning (RL) has shown\nsuperior performance for improving complex reasoning, its impact on\ncross-lingual generalization compared to Supervised Fine-Tuning (SFT) remains\nunexplored. We present the first systematic investigation into cross-lingual\nreasoning generalization of RL and SFT. Using Qwen2.5-3B-Base as our foundation\nmodel, we conduct experiments on diverse multilingual reasoning benchmarks,\nincluding math reasoning, commonsense reasoning, and scientific reasoning. Our\ninvestigation yields two significant findings: (1) Tuning with RL not only\nachieves higher accuracy but also demonstrates substantially stronger\ncross-lingual generalization capabilities compared to SFT. (2) RL training on\nnon-English data yields better overall performance and generalization than\ntraining on English data, which is not observed with SFT. Furthermore, through\ncomprehensive mechanistic analyses, we explore the underlying factors of RL's\nsuperiority and generalization across languages. Our results provide compelling\nevidence that RL enables the model with more robust reasoning strategies,\noffering crucial guidance for more equitable and effective multilingual\nreasoning.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u8de8\u8bed\u8a00\u63a8\u7406\u6cdb\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0RL\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8eSFT\uff0c\u800c\u4e14\u5728\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u66f4\u5f3a\uff0c\u7279\u522b\u662f\u5728\u975e\u82f1\u8bed\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u4e0e\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\u5728\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u8fd9\u4e00\u9886\u57df\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528Qwen2.5-3B-Base\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u79cd\u591a\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\uff08\u6570\u5b66\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u79d1\u5b66\u63a8\u7406\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83RL\u548cSFT\u7684\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u673a\u5236\u5206\u6790\u3002", "result": "RL\u5728\u51c6\u786e\u6027\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u4e0a\u90fd\u663e\u8457\u4f18\u4e8eSFT\uff1b\u5728\u975e\u82f1\u8bed\u6570\u636e\u4e0a\u8bad\u7ec3\u7684RL\u6a21\u578b\u6bd4\u5728\u82f1\u8bed\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u4e00\u73b0\u8c61\u5728SFT\u4e2d\u672a\u89c2\u5bdf\u5230\u3002", "conclusion": "RL\u4f7f\u6a21\u578b\u5177\u5907\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u7b56\u7565\uff0c\u4e3a\u66f4\u516c\u5e73\u6709\u6548\u7684\u591a\u8bed\u8a00\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24127", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.24127", "abs": "https://arxiv.org/abs/2509.24127", "authors": ["Nooshin Bahador"], "title": "Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework", "comment": "20 pages, 11 figures", "summary": "This article presents a modular, component-based architecture for developing\nand evaluating AI agents that bridge the gap between natural language\ninterfaces and complex enterprise data warehouses. The system directly\naddresses core challenges in data accessibility by enabling non-technical users\nto interact with complex data warehouses through a conversational interface,\ntranslating ambiguous user intent into precise, executable database queries to\novercome semantic gaps. A cornerstone of the design is its commitment to\ntransparent decision-making, achieved through a multi-layered reasoning\nframework that explains the \"why\" behind every decision, allowing for full\ninterpretability by tracing conclusions through specific, activated business\nrules and data points. The architecture integrates a robust quality assurance\nmechanism via an automated evaluation framework that serves multiple functions:\nit enables performance benchmarking by objectively measuring agent performance\nagainst golden standards, and it ensures system reliability by automating the\ndetection of performance regressions during updates. The agent's analytical\ndepth is enhanced by a statistical context module, which quantifies deviations\nfrom normative behavior, ensuring all conclusions are supported by quantitative\nevidence including concrete data, percentages, and statistical comparisons. We\ndemonstrate the efficacy of this integrated agent-development-with-evaluation\nframework through a case study on an insurance claims processing system. The\nagent, built on a modular architecture, leverages the BigQuery ecosystem to\nperform secure data retrieval, apply domain-specific business rules, and\ngenerate human-auditable justifications. The results confirm that this approach\ncreates a robust, evaluable, and trustworthy system for deploying LLM-powered\nagents in data-sensitive, high-stakes domains.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u3001\u57fa\u4e8e\u7ec4\u4ef6\u7684AI\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u8bdd\u754c\u9762\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u4e0e\u590d\u6742\u4f01\u4e1a\u6570\u636e\u4ed3\u5e93\uff0c\u89e3\u51b3\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u6311\u6218\uff0c\u652f\u6301\u900f\u660e\u51b3\u7b56\u548c\u81ea\u52a8\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u975e\u6280\u672f\u7528\u6237\u8bbf\u95ee\u590d\u6742\u6570\u636e\u4ed3\u5e93\u7684\u56f0\u96be\uff0c\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\uff0c\u786e\u4fdd\u5728\u6570\u636e\u654f\u611f\u9886\u57df\u90e8\u7f72LLM\u4ee3\u7406\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u542b\u591a\u5c42\u63a8\u7406\u6846\u67b6\u3001\u7edf\u8ba1\u4e0a\u4e0b\u6587\u6a21\u5757\u548c\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\uff0c\u96c6\u6210BigQuery\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u5b89\u5168\u6570\u636e\u68c0\u7d22\u548c\u4e1a\u52a1\u89c4\u5219\u5e94\u7528\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\u8be5\u67b6\u6784\u5728\u4fdd\u9669\u7406\u8d54\u5904\u7406\u7cfb\u7edf\u4e2d\u521b\u5efa\u4e86\u7a33\u5065\u3001\u53ef\u8bc4\u4f30\u4e14\u53ef\u4fe1\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u4f9d\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u6570\u636e\u654f\u611f\u3001\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72LLM\u9a71\u52a8\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2509.24238", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24238", "abs": "https://arxiv.org/abs/2509.24238", "authors": ["Yixin He", "Lumingyuan Tang"], "title": "Learning to Ponder: Adaptive Reasoning in Latent Space", "comment": null, "summary": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,\nyet prevailing approaches like Best-of-N and majority voting apply uniform\ndepth across inputs, wasting computation on simple queries while potentially\nunder-thinking complex ones. We present FR-Ponder, a single-graph,\nbackbone-training-free framework that allocates instance-adaptive reasoning\ncompute via latent steering. A less than 1M-param controller observes hidden\nstates and decides to halt or apply a small ponder step by adding a\npre-computed steering vector to frozen representations. Our method extracts the\nlatent steering vector associated with deeper reasoning outputs and direct IO\nfrom LLM and re-applies it through a tunable scaling factor, allowing the model\nto adapt its reasoning depth to the complexity of each input. To balance\nperformance and computational cost, we employ Group Relative Policy\nOptimization (GRPO) as a reward signal to adaptively regulate reasoning depth,\nachieving task accuracy while mitigating overreasoning. Through curriculum\nlearning and careful reward engineering, FR-Ponder learns calibrated compute\nallocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder\nimproves the compute-accuracy frontier, delivering lower FLOPs with better\nmatched accuracy and comparing favorably to early-exit baselines, without\nmodifying backbone weights. Analyses visualize interpretable steering\ndirections and show learned compute allocation correlates with problem\ndifficulty.", "AI": {"tldr": "FR-Ponder\u662f\u4e00\u4e2a\u65e0\u9700\u4fee\u6539\u4e3b\u5e72\u6a21\u578b\u6743\u91cd\u7684\u81ea\u9002\u5e94\u63a8\u7406\u8ba1\u7b97\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u5411\u91cf\u5f15\u5bfc\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u522b\u7684\u63a8\u7406\u6df1\u5ea6\u8c03\u6574\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Best-of-N\u548c\u591a\u6570\u6295\u7968\u5bf9\u6240\u6709\u8f93\u5165\u4f7f\u7528\u7edf\u4e00\u63a8\u7406\u6df1\u5ea6\uff0c\u5bfc\u81f4\u7b80\u5355\u95ee\u9898\u8ba1\u7b97\u6d6a\u8d39\u800c\u590d\u6742\u95ee\u9898\u63a8\u7406\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5c0f\u4e8e1M\u53c2\u6570\u7684\u63a7\u5236\u5668\u89c2\u5bdf\u9690\u85cf\u72b6\u6001\uff0c\u51b3\u5b9a\u505c\u6b62\u6216\u5e94\u7528\u5c0f\u7684\u601d\u8003\u6b65\u9aa4\u3002\u901a\u8fc7\u6dfb\u52a0\u9884\u8ba1\u7b97\u7684\u5f15\u5bfc\u5411\u91cf\u5230\u51bb\u7ed3\u8868\u793a\u4e2d\uff0c\u63d0\u53d6\u4e0e\u6df1\u5ea6\u63a8\u7406\u8f93\u51fa\u76f8\u5173\u7684\u6f5c\u5728\u5f15\u5bfc\u5411\u91cf\u5e76\u91cd\u65b0\u5e94\u7528\u3002\u91c7\u7528GRPO\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u81ea\u9002\u5e94\u8c03\u8282\u63a8\u7406\u6df1\u5ea6\u3002", "result": "\u5728GSM8K\u548cMATH500\u6570\u636e\u96c6\u4e0a\uff0cFR-Ponder\u6539\u5584\u4e86\u8ba1\u7b97-\u51c6\u786e\u7387\u8fb9\u754c\uff0c\u4ee5\u66f4\u4f4e\u7684FLOPs\u83b7\u5f97\u5339\u914d\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u65e9\u671f\u9000\u51fa\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FR-Ponder\u901a\u8fc7\u6f5c\u5728\u5f15\u5bfc\u5b9e\u73b0\u4e86\u4e0e\u95ee\u9898\u96be\u5ea6\u76f8\u5173\u7684\u8ba1\u7b97\u5206\u914d\uff0c\u5728\u4e0d\u4fee\u6539\u4e3b\u5e72\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2509.24248", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24248", "abs": "https://arxiv.org/abs/2509.24248", "authors": ["Rubing Yang", "Huajun Bai", "Song Liu", "Guanghua Yu", "Runzhi Fan", "Yanbin Dang", "Jiejing Zhang", "Kai Liu", "Jianchen Zhu", "Peng Chen"], "title": "SpecExit: Accelerating Large Reasoning Model via Speculative Exit", "comment": null, "summary": "Despite their strong performance on reasoning tasks, large reasoning models\n(LRMs) often suffer from overthinking, producing unnecessarily long outputs and\nincurring high end-to-end latency, a significant limitation to their real-world\ndeployment. To address overthinking, early-exit mechanisms have been proposed\nto terminate reasoning before typical completion, showing that this approach\ncan effectively shorten generation length with minimal impact on accuracy.\nHowever, their reliance on probing mechanisms introduces a detection overhead\nthat limits their end-to-end latency gains and compromises their\ngeneralizability across diverse problems. Inspired by the use of hidden states\nin speculative decoding, we propose SpecExit, a novel framework that predicts\nboth future tokens and an early-exit signal directly from a lightweight draft\nmodel without probing overhead. Our method offers significant improvements,\nreducing average generation length by 66\\% and achieving a 2.5x speedup in\nend-to-end latency compared to the speculative decoding baseline, without\ncompromising accuracy. Our method leverages the inherent signals from hidden\nstates to provide effective early-exit signals, suggesting broader use of\nhidden states for efficient reasoning. Our code is available at\nhttps://github.com/Tencent/AngelSlim.", "AI": {"tldr": "\u63d0\u51faSpecExit\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u672a\u6765token\u548c\u63d0\u524d\u9000\u51fa\u4fe1\u53f7\uff0c\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5b9e\u73b066%\u7684\u751f\u6210\u957f\u5ea6\u51cf\u5c11\u548c2.5\u500d\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u957f\u8f93\u51fa\u548c\u9ad8\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u7684\u63d0\u524d\u9000\u51fa\u673a\u5236\u4f9d\u8d56\u63a2\u6d4b\u673a\u5236\uff0c\u5f15\u5165\u4e86\u68c0\u6d4b\u5f00\u9500\uff0c\u9650\u5236\u4e86\u5ef6\u8fdf\u6536\u76ca\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u53d7\u63a8\u6d4b\u89e3\u7801\u4e2d\u9690\u85cf\u72b6\u6001\u4f7f\u7528\u7684\u542f\u53d1\uff0c\u63d0\u51faSpecExit\u6846\u67b6\uff0c\u4ece\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u672a\u6765token\u548c\u63d0\u524d\u9000\u51fa\u4fe1\u53f7\uff0c\u65e0\u9700\u63a2\u6d4b\u5f00\u9500\u3002", "result": "\u76f8\u6bd4\u63a8\u6d4b\u89e3\u7801\u57fa\u7ebf\uff0c\u5e73\u5747\u751f\u6210\u957f\u5ea6\u51cf\u5c1166%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f2.5\u500d\uff0c\u4e14\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u3002", "conclusion": "\u5229\u7528\u9690\u85cf\u72b6\u6001\u7684\u56fa\u6709\u4fe1\u53f7\u63d0\u4f9b\u6709\u6548\u7684\u63d0\u524d\u9000\u51fa\u4fe1\u53f7\uff0c\u8868\u660e\u9690\u85cf\u72b6\u6001\u5728\u9ad8\u6548\u63a8\u7406\u4e2d\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.24261", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24261", "abs": "https://arxiv.org/abs/2509.24261", "authors": ["Yuhua Jiang", "Jiawei Huang", "Yufeng Yuan", "Xin Mao", "Yu Yue", "Qianchuan Zhao", "Lin Yan"], "title": "Risk-Sensitive RL for Alleviating Exploration Dilemmas in Large Language Models", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor enhancing Large Language Models (LLMs) on complex reasoning tasks. However,\nexisting methods suffer from an exploration dilemma: the sharply peaked initial\npolicies of pre-trained LLMs confine standard RL algorithms to a narrow set of\nsolutions, boosting single-solution accuracy (pass@1) but suppressing solution\ndiversity and multi-solution performance (pass@k). As a result, RLVR often\ndistills existing capabilities rather than discovering new reasoning\nstrategies. To overcome this, we introduce a Risk-Sensitive Reinforcement\nLearning framework. Our approach employs a risk-seeking objective that\ninterpolates between mean and maximum rewards, leading to a novel algorithm,\nRisk-Sensitive GRPO (RS-GRPO), which drives deeper exploration by amplifying\nlearning from challenging prompts. Remarkably, RS-GRPO is simple to implement,\nrequiring only minor code modifications. On six mathematical reasoning\nbenchmarks and with five different LLMs, RS-GRPO consistently improves pass@k\nperformance while maintaining or enhancing pass@1 accuracy.", "AI": {"tldr": "\u63d0\u51fa\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RS-GRPO\uff0c\u901a\u8fc7\u98ce\u9669\u5bfb\u6c42\u76ee\u6807\u5728\u5747\u503c\u548c\u6700\u5927\u5956\u52b1\u95f4\u63d2\u503c\uff0c\u89e3\u51b3RLVR\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u56f0\u5883\uff0c\u63d0\u5347\u591a\u89e3\u6027\u80fd(pass@k)\u540c\u65f6\u4fdd\u6301\u5355\u89e3\u51c6\u786e\u7387(pass@1)\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u56f0\u5883\uff1a\u9884\u8bad\u7ec3LLM\u7684\u5c16\u9510\u521d\u59cb\u7b56\u7565\u5c06\u6807\u51c6RL\u7b97\u6cd5\u9650\u5236\u5728\u72ed\u7a84\u89e3\u7a7a\u95f4\u5185\uff0c\u867d\u7136\u63d0\u5347\u4e86\u5355\u89e3\u51c6\u786e\u7387\u4f46\u6291\u5236\u4e86\u89e3\u591a\u6837\u6027\u548c\u591a\u89e3\u6027\u80fd\uff0c\u5bfc\u81f4RLVR\u66f4\u591a\u662f\u84b8\u998f\u73b0\u6709\u80fd\u529b\u800c\u975e\u53d1\u73b0\u65b0\u63a8\u7406\u7b56\u7565\u3002", "method": "\u5f15\u5165\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u98ce\u9669\u5bfb\u6c42\u76ee\u6807\u5728\u5747\u503c\u548c\u6700\u5927\u5956\u52b1\u95f4\u63d2\u503c\uff0c\u63d0\u51faRS-GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u653e\u5927\u4ece\u6311\u6218\u6027\u63d0\u793a\u4e2d\u5b66\u4e60\u6765\u9a71\u52a8\u6df1\u5ea6\u63a2\u7d22\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u7b80\u5355\uff0c\u53ea\u9700\u5c11\u91cf\u4ee3\u7801\u4fee\u6539\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u4e94\u4e2a\u4e0d\u540cLLM\u4e0a\uff0cRS-GRPO\u6301\u7eed\u6539\u8fdbpass@k\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u589e\u5f3apass@1\u51c6\u786e\u7387\u3002", "conclusion": "RS-GRPO\u6709\u6548\u89e3\u51b3\u4e86RLVR\u7684\u63a2\u7d22\u56f0\u5883\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5355\u89e3\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u591a\u89e3\u6027\u80fd\uff0c\u4e3aLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23924", "abs": "https://arxiv.org/abs/2509.23924", "authors": ["Jingyi Yang", "Guanxu Chen", "Xuhao Hu", "Jing Shao"], "title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step", "comment": "10 pages, 4 figures, 7 tables. Code:\n  https://github.com/yjyddq/EOSER-ASS-RL", "summary": "Masked diffusion language models (MDLMs) have recently emerged as a promising\nalternative to autoregressive (AR) language models, offering properties such as\nparallel decoding, flexible generation orders, and the potential for fewer\ninference steps. Despite these advantages, decoding strategies and\nreinforcement learning (RL) algorithms tailored for MDLMs remain underexplored.\nA naive approach is to directly transfer techniques well-established for AR\nmodels to MDLMs. However, this raises an immediate question: Is such a naive\ntransfer truly optimal? For example, 1) Block-wise and semi-AR decoding\nstrategies are not employed during the training of MDLMs, so why do they\noutperform full diffusion-style decoding during inference? 2) Applying RL\nalgorithms designed for AR models directly to MDLMs exhibits a\ntraining-inference inconsistency, since MDLM decoding are non-causal\n(parallel). This results in inconsistencies between the rollout trajectory and\nthe optimization trajectory. To address these challenges, we propose EOS Early\nRejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which\nunlock the potential of MDLMs to perform full diffusion-style decoding,\nachieving competitive performance with fewer decoding steps. Additionally, we\nintroduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO)\nfor taming MDLMs, which emphasizes the consistency between rollout trajectory\nand optimization trajectory, and reduces the optimization errors caused by\nskip-step optimization. We conduct extensive experiments on reasoning tasks,\nsuch as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The\nresults demonstrate that the proposed EOSER and ASS mechanisms, together with\nCJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs.\nCode: https://github.com/yjyddq/EOSER-ASS-RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EOS\u65e9\u671f\u62d2\u7edd\u548c\u9012\u589e\u6b65\u957f\u89e3\u7801\u8c03\u5ea6\u5668\uff0c\u4ee5\u53ca\u4e00\u81f4\u6027\u8f68\u8ff9\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e76\u884c\u89e3\u7801\u3001\u7075\u6d3b\u751f\u6210\u987a\u5e8f\u548c\u8f83\u5c11\u63a8\u7406\u6b65\u9aa4\u7b49\u4f18\u52bf\uff0c\u4f46\u5176\u89e3\u7801\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7814\u7a76\u4e0d\u8db3\u3002\u76f4\u63a5\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6280\u672f\u8fc1\u79fb\u5230MDLMs\u5b58\u5728\u8bad\u7ec3-\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86EOSER\u548cASS\u89e3\u7801\u8c03\u5ea6\u5668\u6765\u4f18\u5316MDLMs\u7684\u5b8c\u6574\u6269\u6563\u5f0f\u89e3\u7801\uff0c\u4ee5\u53caCJ-GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u786e\u4fdd\u6eda\u52a8\u8f68\u8ff9\u4e0e\u4f18\u5316\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6570\u5b66\u548c\u89c4\u5212\u7b49\u63a8\u7406\u4efb\u52a1\u4e0a\u4f7f\u7528LLaDA-8B-Instruct\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u9a6f\u670dMDLMs\u3002", "conclusion": "EOSER\u3001ASS\u673a\u5236\u548cCJ-GRPO\u7b97\u6cd5\u4e3aMDLMs\u7684\u6709\u6548\u8bad\u7ec3\u548c\u89e3\u7801\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24351", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24351", "abs": "https://arxiv.org/abs/2509.24351", "authors": ["Jie Ma", "Shihao Qi", "Rui Xing", "Ziang Yin", "Bifan Wei", "Jun Liu", "Tongliang Liu"], "title": "From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision", "comment": null, "summary": "The quality of process data plays a key role in training a Process Reward\nModel (PRM), which can enhance the complex mathematical reasoning capability of\nlarge language models. Existing methods estimate the quality of reasoning steps\nbased on a fixed-budget sampling strategy and navigate a vast search space to\nperform path expansion during the automated data generation process, resulting\nin their inefficiency and inflexibility. To address these issues, we propose\nAdaptive Monte Carlo Search (AMCS), a framework that transforms data generation\nfrom fixed, static to adaptive, dynamic search at the level of node value\nestimation and path expansion. On one hand, AMCS adaptively refines estimation\nby allocating more samples to uncertain reasoning steps while using fewer\nsamples for those that are easier to estimate. On the other hand, it enhances\nthe path expansion through a Monte Carlo algorithm with a temporally adaptive\npolicy that begins with broad exploration and gradually shifts toward\nexploiting the most promising directions. With AMCS, we construct a large-scale\ndataset MathSearch-200K of about 200K process supervision examples for training\nPRMs. To verify the effectiveness of our method, we conduct extensive\nexperiments on four mathematical reasoning benchmarks. Experimental results\nshow that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500\nwith GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised\nby Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision.\nMoreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on\nout-of-distribution problems, demonstrating strong generalization capability.\nOur code is available at https://github.com/reml-group/AMCS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u8499\u7279\u5361\u6d1b\u641c\u7d22\uff08AMCS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8282\u70b9\u503c\u4f30\u8ba1\u548c\u8def\u5f84\u6269\u5c55\u6765\u63d0\u5347\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u9884\u7b97\u91c7\u6837\u7b56\u7565\u6765\u4f30\u8ba1\u63a8\u7406\u6b65\u9aa4\u8d28\u91cf\uff0c\u5728\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e2d\u6548\u7387\u4f4e\u4e0b\u4e14\u4e0d\u7075\u6d3b\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "method": "AMCS\u6846\u67b6\u91c7\u7528\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u4e3a\u4e0d\u786e\u5b9a\u7684\u63a8\u7406\u6b65\u9aa4\u5206\u914d\u66f4\u591a\u6837\u672c\uff0c\u5bf9\u5bb9\u6613\u4f30\u8ba1\u7684\u6b65\u9aa4\u4f7f\u7528\u8f83\u5c11\u6837\u672c\uff1b\u540c\u65f6\u901a\u8fc7\u65f6\u95f4\u81ea\u9002\u5e94\u7b56\u7565\u8fdb\u884c\u8def\u5f84\u6269\u5c55\uff0c\u4ece\u5e7f\u6cdb\u63a2\u7d22\u9010\u6b65\u8f6c\u5411\u6700\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b20\u4e07\u8fc7\u7a0b\u76d1\u7763\u6837\u672c\u7684MathSearch-200K\u6570\u636e\u96c6\uff0cQwen2.5-Math-7B-PRM-AMCS\u5728MATH500\u4e0a\u8fbe\u523076.2%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u6240\u6709\u57fa\u7ebfPRM\u30027B\u6a21\u578b\u5728AMCS\u76d1\u7763\u4e0b\u6027\u80fd\u8d85\u8fc772B\u6a21\u578b\u3002", "conclusion": "AMCS\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6548\u679c\uff0c\u5728\u5206\u5e03\u5916\u95ee\u9898\u4e0a\u4e5f\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23979", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23979", "abs": "https://arxiv.org/abs/2509.23979", "authors": ["Haonan Wang", "Junfeng Sun", "Xingdi Yuan", "Ruoyao Wang", "Ziang Xiao"], "title": "ByteSized32Refactored: Towards an Extensible Interactive Text Games Corpus for LLM World Modeling and Evaluation", "comment": "14 pages,15 figures, Accepted to the 5th Wordplay: When Language\n  Meets Games Workshop, EMNLP 2025", "summary": "Simulating interactive world models remains a core challenge in Large\nLanguage Models(LLMs). In this work, we introduce the ByteSized32Refactored, a\nrefactored, modular, and extensible implementation of the original ByteSized32\ncorpus to explore the task of text game generation. We further optimize the\ncode structure of each text game and create the GameBasic.py foundation\nlibrary, which centralizes common logic across all 32 games by abstracting 7\nbase classes (GameObject, etc.) into reusable modules, thereby reducing from\n20k to 10k total lines of Python code compared to the original Bytesized32. Our\nrefactored implementation enables extendability - with our centralized design,\nByteSized32Refactored can be more efficiently extended to include text games of\nnew scenarios and specifications by reusing the shared logic and\nfunctionalities. Extensive experiments with GPT-4o demonstrate a mix of\nperformance - with Bytesized32Refactored, the generated text games for unseen\nscenarios showcase quality improvements on two of the four evaluation\ndimensions while decreases on the other two, indicating that the hierarchical\nstructure of the refactored code presents new challenges for LLMs. Overall, we\nhighlight that our extensible code structure, centered on the foundation\nlibrary and the modular optimization, not only facilitates LLM adaptation to\nenvironment specifications but also establishes a scalable environment that\nsupports future extensions.", "AI": {"tldr": "\u63d0\u51fa\u4e86ByteSized32Refactored\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u6784\u7684\u3001\u6a21\u5757\u5316\u7684\u6587\u672c\u6e38\u620f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u5efaGameBasic.py\u57fa\u7840\u5e93\u548c7\u4e2a\u57fa\u7c7b\uff0c\u5c06\u4ee3\u7801\u91cf\u4ece2\u4e07\u884c\u51cf\u5c11\u52301\u4e07\u884c\uff0c\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u6a21\u62df\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u4ecd\u7136\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u6587\u672c\u6e38\u620f\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91cd\u6784ByteSized32\u8bed\u6599\u5e93\uff0c\u4f18\u5316\u4ee3\u7801\u7ed3\u6784\uff0c\u521b\u5efaGameBasic.py\u57fa\u7840\u5e93\uff0c\u62bd\u8c61\u51fa7\u4e2a\u57fa\u7c7b\uff08\u5982GameObject\u7b49\uff09\u4f5c\u4e3a\u53ef\u590d\u7528\u6a21\u5757\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u548c\u96c6\u4e2d\u5316\u8bbe\u8ba1\u3002", "result": "\u4f7f\u7528GPT-4o\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u751f\u6210\u7684\u6587\u672c\u6e38\u620f\u5728\u56db\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e2d\u7684\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u8d28\u91cf\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5728\u53e6\u5916\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u6709\u6240\u4e0b\u964d\uff0c\u8868\u660e\u91cd\u6784\u4ee3\u7801\u7684\u5c42\u6b21\u7ed3\u6784\u7ed9LLM\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u57fa\u7840\u5e93\u548c\u6a21\u5757\u5316\u4f18\u5316\u7684\u53ef\u6269\u5c55\u4ee3\u7801\u7ed3\u6784\u4e0d\u4ec5\u4fc3\u8fdb\u4e86LLM\u5bf9\u73af\u5883\u89c4\u8303\u7684\u9002\u5e94\uff0c\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u652f\u6301\u672a\u6765\u6269\u5c55\u7684\u53ef\u6269\u5c55\u73af\u5883\u3002", "topic": "swe application"}}
{"id": "2509.24002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24002", "abs": "https://arxiv.org/abs/2509.24002", "authors": ["Zijian Wu", "Xiangyan Liu", "Xinyuan Zhang", "Lingjun Chen", "Fanqing Meng", "Lingxiao Du", "Yiran Zhao", "Fanshi Zhang", "Yaoqi Ye", "Jiawei Wang", "Zirui Wang", "Jinjie Ni", "Yufan Yang", "Arvin Xu", "Michael Qizhe Shieh"], "title": "MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use", "comment": "42 pages, 27 figures, 10 tables", "summary": "MCP standardizes how LLMs interact with external systems, forming the\nfoundation for general agents. However, existing MCP benchmarks remain narrow\nin scope: they focus on read-heavy tasks or tasks with limited interaction\ndepth, and fail to capture the complexity and realism of real-world workflows.\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\nuse in a more realistic and comprehensive manner. It consists of $127$\nhigh-quality tasks collaboratively created by domain experts and AI agents.\nEach task begins with a curated initial state and includes a programmatic\nscript for automatic verification. These tasks demand richer and more diverse\ninteractions with the environment, involving a broad range of create, read,\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\ncutting-edge LLMs using a minimal agent framework that operates in a\ntool-calling loop. Empirical results show that the best-performing model,\ngpt-5-medium, reaches only $52.56$\\% pass@1 and $33.86$\\% pass^4, while other\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\n$30$\\% pass@1 and $15$\\% pass^4. On average, LLMs require $16.2$ execution\nturns and $17.4$ tool calls per task, significantly surpassing those in\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.", "AI": {"tldr": "MCPMark\u662f\u4e00\u4e2a\u65b0\u7684MCP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b127\u4e2a\u9ad8\u8d28\u91cf\u4efb\u52a1\uff0c\u65e8\u5728\u66f4\u771f\u5b9e\u5168\u9762\u5730\u8bc4\u4f30MCP\u4f7f\u7528\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u590d\u6742\u6027\u548c\u73b0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709MCP\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u72ed\u7a84\uff0c\u4e3b\u8981\u5173\u6ce8\u8bfb\u53d6\u5bc6\u96c6\u578b\u4efb\u52a1\u6216\u4ea4\u4e92\u6df1\u5ea6\u6709\u9650\u7684\u4efb\u52a1\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u548c\u73b0\u5b9e\u6027\u3002", "method": "\u63d0\u51faMCPMark\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b127\u4e2a\u7531\u9886\u57df\u4e13\u5bb6\u548cAI\u4ee3\u7406\u534f\u4f5c\u521b\u5efa\u7684\u9ad8\u8d28\u91cf\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\u7cbe\u5fc3\u7b56\u5212\u7684\u521d\u59cb\u72b6\u6001\u548c\u81ea\u52a8\u9a8c\u8bc1\u811a\u672c\uff0c\u9700\u8981\u4e30\u5bcc\u7684CRUD\u64cd\u4f5c\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8868\u73b0\u6700\u4f73\u7684gpt-5-medium\u6a21\u578b\u4ec5\u8fbe\u523052.56% pass@1\u548c33.86% pass^4\uff0c\u5176\u4ed6\u5f3a\u6a21\u578b\u5982claude-sonnet-4\u548co3\u4f4e\u4e8e30% pass@1\u548c15% pass^4\u3002LLMs\u5e73\u5747\u9700\u898116.2\u6267\u884c\u8f6e\u6b21\u548c17.4\u5de5\u5177\u8c03\u7528\u3002", "conclusion": "MCPMark\u5c55\u793a\u4e86\u73b0\u6709LLMs\u5728\u590d\u6742MCP\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u7a81\u51fa\u4e86\u5176\u538b\u529b\u6d4b\u8bd5\u6027\u8d28\uff0c\u4e3a\u672a\u6765MCP\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2509.24527", "categories": ["cs.AI", "cs.LG", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.24527", "abs": "https://arxiv.org/abs/2509.24527", "authors": ["Danijar Hafner", "Wilson Yan", "Timothy Lillicrap"], "title": "Training Agents Inside of Scalable World Models", "comment": "Website: https://danijar.com/dreamer4/", "summary": "World models learn general knowledge from videos and simulate experience for\ntraining behaviors in imagination, offering a path towards intelligent agents.\nHowever, previous world models have been unable to accurately predict object\ninteractions in complex environments. We introduce Dreamer 4, a scalable agent\nthat learns to solve control tasks by reinforcement learning inside of a fast\nand accurate world model. In the complex video game Minecraft, the world model\naccurately predicts object interactions and game mechanics, outperforming\nprevious world models by a large margin. The world model achieves real-time\ninteractive inference on a single GPU through a shortcut forcing objective and\nan efficient transformer architecture. Moreover, the world model learns general\naction conditioning from only a small amount of data, allowing it to extract\nthe majority of its knowledge from diverse unlabeled videos. We propose the\nchallenge of obtaining diamonds in Minecraft from only offline data, aligning\nwith practical applications such as robotics where learning from environment\ninteraction can be unsafe and slow. This task requires choosing sequences of\nover 20,000 mouse and keyboard actions from raw pixels. By learning behaviors\nin imagination, Dreamer 4 is the first agent to obtain diamonds in Minecraft\npurely from offline data, without environment interaction. Our work provides a\nscalable recipe for imagination training, marking a step towards intelligent\nagents.", "AI": {"tldr": "Dreamer 4\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u901a\u8fc7\u5728\u5feb\u901f\u51c6\u786e\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u63a7\u5236\u4efb\u52a1\u3002\u5b83\u5728Minecraft\u6e38\u620f\u4e2d\u51c6\u786e\u9884\u6d4b\u7269\u4f53\u4ea4\u4e92\u548c\u6e38\u620f\u673a\u5236\uff0c\u4ec5\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u6210\u529f\u83b7\u53d6\u94bb\u77f3\uff0c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u3002", "motivation": "\u5148\u524d\u4e16\u754c\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u590d\u6742\u73af\u5883\u4e2d\u7684\u7269\u4f53\u4ea4\u4e92\uff0c\u4e14\u5b9e\u9645\u5e94\u7528\u4e2d\u73af\u5883\u4ea4\u4e92\u53ef\u80fd\u4e0d\u5b89\u5168\u4e14\u7f13\u6162\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u667a\u80fd\u4ee3\u7406\u3002", "method": "\u4f7f\u7528\u5feb\u6377\u5f3a\u5236\u76ee\u6807\u548c\u9ad8\u6548transformer\u67b6\u6784\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u63a8\u7406\u3002\u4ece\u5c11\u91cf\u6570\u636e\u4e2d\u5b66\u4e60\u901a\u7528\u52a8\u4f5c\u6761\u4ef6\uff0c\u4ece\u591a\u6837\u5316\u65e0\u6807\u7b7e\u89c6\u9891\u4e2d\u63d0\u53d6\u77e5\u8bc6\u3002\u5728\u60f3\u8c61\u4e2d\u8bad\u7ec3\u884c\u4e3a\u3002", "result": "Dreamer 4\u5728Minecraft\u4e2d\u5927\u5e45\u8d85\u8d8a\u5148\u524d\u4e16\u754c\u6a21\u578b\uff0c\u51c6\u786e\u9884\u6d4b\u7269\u4f53\u4ea4\u4e92\u3002\u6210\u4e3a\u9996\u4e2a\u4ec5\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u83b7\u53d6\u94bb\u77f3\u7684\u4ee3\u7406\uff0c\u9700\u8981\u4ece\u539f\u59cb\u50cf\u7d20\u4e2d\u9009\u62e9\u8d85\u8fc720,000\u4e2a\u9f20\u6807\u548c\u952e\u76d8\u52a8\u4f5c\u5e8f\u5217\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u60f3\u8c61\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u914d\u65b9\uff0c\u6807\u5fd7\u7740\u5411\u667a\u80fd\u4ee3\u7406\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24090", "abs": "https://arxiv.org/abs/2509.24090", "authors": ["Matteo Boffa", "Jiaxuan You"], "title": "Large-Scale Constraint Generation -- Can LLMs Parse Hundreds of Constraints?", "comment": null, "summary": "Recent research has explored the constrained generation capabilities of Large\nLanguage Models (LLMs) when explicitly prompted by few task-specific\nrequirements. In contrast, we introduce Large-Scale Constraint Generation\n(LSCG), a new problem that evaluates whether LLMs can parse a large,\nfine-grained, generic list of constraints. To examine the LLMs' ability to\nhandle an increasing number constraints, we create a practical instance of\nLSCG, called Words Checker. In Words Checker, we evaluate the impact of model\ncharacteristics (e.g., size, family) and steering techniques (e.g., Simple\nPrompt, Chain of Thought, Best of N) on performance. We also propose FoCusNet,\na small and dedicated model that parses the original list of constraints into a\nsmaller subset, helping the LLM focus on relevant constraints. Experiments\nreveal that existing solutions suffer a significant performance drop as the\nnumber of constraints increases, with FoCusNet showing an 8-13% accuracy boost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5927\u89c4\u6a21\u7ea6\u675f\u751f\u6210(LSCG)\u95ee\u9898\uff0c\u8bc4\u4f30LLMs\u5904\u7406\u5927\u91cf\u7ec6\u7c92\u5ea6\u901a\u7528\u7ea6\u675f\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Words Checker\u5b9e\u4f8b\u6765\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u548c\u7ea6\u675f\u6570\u91cf\u589e\u52a0\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u63d0\u51faFoCusNet\u6a21\u578b\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLMs\u5728\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u8981\u6c42\u4e0b\u7684\u7ea6\u675f\u751f\u6210\u80fd\u529b\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22LLMs\u5904\u7406\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u901a\u7528\u7ea6\u675f\u5217\u8868\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaWords Checker\u5b9e\u4f8b\u8bc4\u4f30\u6a21\u578b\u7279\u6027\u548c\u5f15\u5bfc\u6280\u672f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51faFoCusNet\u6a21\u578b\u5c06\u539f\u59cb\u7ea6\u675f\u5217\u8868\u89e3\u6790\u4e3a\u66f4\u5c0f\u5b50\u96c6\u4ee5\u5e2e\u52a9LLM\u805a\u7126\u76f8\u5173\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u7ea6\u675f\u6570\u91cf\u589e\u52a0\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800cFoCusNet\u80fd\u5e26\u67658-13%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "LLMs\u5728\u5927\u89c4\u6a21\u7ea6\u675f\u5904\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0cFoCusNet\u901a\u8fc7\u7ea6\u675f\u805a\u7126\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.24836", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24836", "abs": "https://arxiv.org/abs/2509.24836", "authors": ["Zhen Bi", "Zhenlin Hu", "Jinnan Yang", "Mingyang Chen", "Cheng Deng", "Yida Xue", "Zeyu Yang", "Qing Shen", "Zhenfang Liu", "Kang Zhao", "Ningyu Zhang", "Jungang Lou"], "title": "Pushing LLMs to Their Logical Reasoning Bound: The Role of Data Reasoning Intensity", "comment": null, "summary": "Recent advances in large language models (LLMs) highlight the importance of\ntraining data structure and quality in shaping reasoning behavior. However,\nmost existing approaches focus on transforming data formats while neglecting\nthe internal reasoning complexity of training samples, leaving the reasoning\npotential of data under-explored and underutilized. In this work, we posit that\nLLM logical reasoning performance is jointly constrained by the potential of\nthe training data and the cognitive capacity of the model. To make this\nrelationship measurable, we introduce Data Reasoning Intensity (DRI), a novel\nmetric that quantifies the latent logical reasoning complexity of samples by\ndecomposing and aggregating their logical structures. This allows us to analyze\nhow well current LLMs utilize logical reasoning signals and identify\nperformance gaps relative to data potential. Based on this insight, we\nintroduce a re-cognizing optimization strategy that systematically enhances the\nlogical reasoning intensity of training data.Rather than increasing data\nvolume, our method re-optimizes existing samples to better align with the LLM's\nlogical reasoning boundary. Extensive experiments show that our approach\nsignificantly improves performance and generalization over data-centric\nstrategies. We further validate our method under a reinforcement learning\nframework. Our results indicate that prioritizing reasoning complexity in data\nrather than sheer scale or superficial form is essential to realizing LLMs'\nfull cognitive potential.", "AI": {"tldr": "\u63d0\u51fa\u6570\u636e\u63a8\u7406\u5f3a\u5ea6(DRI)\u6307\u6807\u6765\u91cf\u5316\u8bad\u7ec3\u6570\u636e\u7684\u903b\u8f91\u63a8\u7406\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u8ba4\u77e5\u4f18\u5316\u7b56\u7565\u589e\u5f3a\u6570\u636e\u63a8\u7406\u5f3a\u5ea6\uff0c\u4ece\u800c\u63d0\u5347LLM\u7684\u903b\u8f91\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u683c\u5f0f\u8f6c\u6362\u800c\u5ffd\u89c6\u8bad\u7ec3\u6837\u672c\u7684\u5185\u90e8\u63a8\u7406\u590d\u6742\u5ea6\uff0c\u5bfc\u81f4\u6570\u636e\u7684\u63a8\u7406\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\u548c\u5229\u7528\u3002", "method": "\u5f15\u5165\u6570\u636e\u63a8\u7406\u5f3a\u5ea6(DRI)\u6307\u6807\u91cf\u5316\u6837\u672c\u7684\u6f5c\u5728\u903b\u8f91\u63a8\u7406\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u51fa\u91cd\u65b0\u8ba4\u77e5\u4f18\u5316\u7b56\u7565\u7cfb\u7edf\u6027\u5730\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u7684\u903b\u8f91\u63a8\u7406\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u7b56\u7565\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\u4e5f\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u4f18\u5148\u8003\u8651\u6570\u636e\u7684\u63a8\u7406\u590d\u6742\u5ea6\u800c\u975e\u5355\u7eaf\u7684\u6570\u636e\u89c4\u6a21\u6216\u8868\u9762\u5f62\u5f0f\uff0c\u5bf9\u4e8e\u5b9e\u73b0LLM\u7684\u5b8c\u6574\u8ba4\u77e5\u6f5c\u529b\u81f3\u5173\u91cd\u8981\u3002", "topic": "agent analysis"}}
{"id": "2509.24877", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24877", "abs": "https://arxiv.org/abs/2509.24877", "authors": ["Xiao Jia", "Zhanzhan Zhao"], "title": "The Emergence of Social Science of Large Language Models", "comment": null, "summary": "The social science of large language models (LLMs) examines how these systems\nevoke mind attributions, interact with one another, and transform human\nactivity and institutions. We conducted a systematic review of 270 studies,\ncombining text embeddings, unsupervised clustering and topic modeling to build\na computational taxonomy. Three domains emerge organically across the reviewed\nliterature. LLM as Social Minds examines whether and when models display\nbehaviors that elicit attributions of cognition, morality and bias, while\naddressing challenges such as test leakage and surface cues. LLM Societies\nexamines multi-agent settings where interaction protocols, architectures and\nmechanism design shape coordination, norms, institutions and collective\nepistemic processes. LLM-Human Interactions examines how LLMs reshape tasks,\nlearning, trust, work and governance, and how risks arise at the human-AI\ninterface. This taxonomy provides a reproducible map of a fragmented field,\nclarifies evidentiary standards across levels of analysis, and highlights\nopportunities for cumulative progress in the social science of artificial\nintelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5bf9270\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u793e\u4f1a\u79d1\u5b66\u7684\u8ba1\u7b97\u5206\u7c7b\u6cd5\uff0c\u8bc6\u522b\u51fa\u4e09\u4e2a\u4e3b\u8981\u9886\u57df\uff1aLLM\u4f5c\u4e3a\u793e\u4f1a\u5fc3\u667a\u3001LLM\u793e\u4f1a\u548cLLM-\u4eba\u7c7b\u4ea4\u4e92\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u9886\u57df\u5206\u6563\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u7c7b\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u9886\u57df\u5730\u56fe\u6765\u6f84\u6e05\u8bc1\u636e\u6807\u51c6\u5e76\u4fc3\u8fdb\u7d2f\u79ef\u6027\u8fdb\u5c55\u3002", "method": "\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u3001\u65e0\u76d1\u7763\u805a\u7c7b\u548c\u4e3b\u9898\u5efa\u6a21\u7684\u8ba1\u7b97\u5206\u7c7b\u6cd5\uff0c\u5bf9270\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u4e2a\u6709\u673a\u51fa\u73b0\u7684\u9886\u57df\uff1aLLM\u4f5c\u4e3a\u793e\u4f1a\u5fc3\u667a\uff08\u8ba4\u77e5\u3001\u9053\u5fb7\u548c\u504f\u89c1\u5f52\u56e0\uff09\u3001LLM\u793e\u4f1a\uff08\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u96c6\u4f53\u8ba4\u77e5\u8fc7\u7a0b\uff09\u4ee5\u53caLLM-\u4eba\u7c7b\u4ea4\u4e92\uff08\u4efb\u52a1\u91cd\u5851\u3001\u4fe1\u4efb\u548c\u5de5\u4f5c\u5f71\u54cd\uff09\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u5206\u6563\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5730\u56fe\uff0c\u6f84\u6e05\u4e86\u4e0d\u540c\u5206\u6790\u5c42\u6b21\u7684\u8bc1\u636e\u6807\u51c6\uff0c\u5e76\u7a81\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u793e\u4f1a\u79d1\u5b66\u4e2d\u7d2f\u79ef\u6027\u8fdb\u5c55\u7684\u673a\u4f1a\u3002", "topic": "agent analysis"}}
{"id": "2509.24183", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24183", "abs": "https://arxiv.org/abs/2509.24183", "authors": ["Ran Xu", "Kaixin Ma", "Wenhao Yu", "Hongming Zhang", "Joyce C. Ho", "Carl Yang", "Dong Yu"], "title": "Retrieval-augmented GUI Agents with Generative Guidelines", "comment": "Accepted to EMNLP 2025 (Main Conference)", "summary": "GUI agents powered by vision-language models (VLMs) show promise in\nautomating complex digital tasks. However, their effectiveness in real-world\napplications is often limited by scarce training data and the inherent\ncomplexity of these tasks, which frequently require long-tailed knowledge\ncovering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that\nleverages web tutorials at inference time. RAG-GUI is first warm-started via\nsupervised finetuning (SFT) and further refined through self-guided rejection\nsampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as\na generic plug-in that enhances any VLM-based agent. Evaluated across three\ndistinct tasks, it consistently outperforms baseline agents and surpasses other\ninference baselines by 2.6% to 13.3% across two model sizes, demonstrating\nstrong generalization and practical plug-and-play capabilities in real-world\nscenarios.", "AI": {"tldr": "RAG-GUI\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5229\u7528\u7f51\u7edc\u6559\u7a0b\uff0c\u5728\u63a8\u7406\u65f6\u589e\u5f3aGUI\u4ee3\u7406\u7684\u6027\u80fd\u3002\u5b83\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u5f15\u5bfc\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\uff0c\u4f5c\u4e3a\u901a\u7528\u63d2\u4ef6\u63d0\u5347\u4efb\u4f55\u57fa\u4e8eVLM\u7684\u4ee3\u7406\u3002", "motivation": "\u89e3\u51b3GUI\u4ee3\u7406\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u4efb\u52a1\u590d\u6742\u6027\uff08\u7279\u522b\u662f\u9700\u8981\u5904\u7406\u7f55\u89c1\u573a\u666f\u7684\u957f\u5c3e\u77e5\u8bc6\uff09\u800c\u6548\u679c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u8fdb\u884c\u9884\u70ed\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u5f15\u5bfc\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u63d2\u4ef6\u589e\u5f3a\u4efb\u4f55VLM\u4ee3\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRAG-GUI\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u4ee3\u7406\uff0c\u5728\u4e24\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\u6bd4\u5176\u4ed6\u63a8\u7406\u57fa\u7ebf\u63d0\u53472.6%\u523013.3%\u3002", "conclusion": "RAG-GUI\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u7684\u5373\u63d2\u5373\u7528\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2509.24193", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24193", "abs": "https://arxiv.org/abs/2509.24193", "authors": ["Ran Xu", "Yuchen Zhuang", "Zihan Dong", "Jonathan Wang", "Yue Yu", "Joyce C. Ho", "Linjun Zhang", "Haoyu Wang", "Wenqi Shi", "Carl Yang"], "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play", "comment": "Accepted to NeurIPS 2025 (Spotlight)", "summary": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.", "AI": {"tldr": "AceSearcher\u662f\u4e00\u4e2a\u534f\u4f5c\u5f0f\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u5355\u4e2aLLM\u5728\u5206\u89e3\u5668\u548c\u6c42\u89e3\u5668\u4e24\u4e2a\u89d2\u8272\u95f4\u5207\u6362\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53477.6%\u3002", "motivation": "\u73b0\u6709\u7684\u641c\u7d22\u589e\u5f3aLLM\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u591a\u8df3\u68c0\u7d22\u6548\u7387\u4f4e\u548c\u63a8\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u8bad\u7ec3\u5355\u4e2aLLM\u4ea4\u66ff\u626e\u6f14\u5206\u89e3\u5668\uff08\u5206\u89e3\u590d\u6742\u67e5\u8be2\uff09\u548c\u6c42\u89e3\u5668\uff08\u6574\u5408\u68c0\u7d22\u4e0a\u4e0b\u6587\u751f\u6210\u7b54\u6848\uff09\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u572810\u4e2a\u6570\u636e\u96c6\u76843\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53477.6%\uff1b32B\u6a21\u578b\u5728\u91d1\u878d\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u5ab2\u7f8eDeepSeek-V3\u4f46\u53c2\u6570\u5c1195%\uff1b\u5c0f\u89c4\u6a21\u6a21\u578b\uff081.5B\u548c8B\uff09\u6027\u80fd\u4f18\u4e8e\u53c2\u6570\u591a9\u500d\u7684\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "AceSearcher\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u65e0\u9700\u4e2d\u95f4\u6807\u6ce8\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24922", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24922", "abs": "https://arxiv.org/abs/2509.24922", "authors": ["Huihao Jing", "Wenbin Hu", "Hongyu Luo", "Jianhui Yang", "Wei Fan", "Haoran Li", "Yangqiu Song"], "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning", "comment": null, "summary": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86MASLegalBench\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6cd5\u5f8b\u57fa\u51c6\uff0c\u4f7f\u7528GDPR\u4f5c\u4e3a\u5e94\u7528\u573a\u666f\uff0c\u901a\u8fc7\u6f14\u7ece\u63a8\u7406\u65b9\u6cd5\u8bc4\u4f30MAS\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u57fa\u51c6\u672a\u8003\u8651\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u5982\u4efb\u52a1\u5206\u89e3\u3001\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\u548c\u7075\u6d3b\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86MAS\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528GDPR\u4f5c\u4e3a\u5e94\u7528\u573a\u666f\uff0c\u624b\u52a8\u8bbe\u8ba1\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u91c7\u7528\u6f14\u7ece\u63a8\u7406\u65b9\u6cd5\u6784\u5efa\u57fa\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u548cMAS\u67b6\u6784\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "MASLegalBench\u586b\u8865\u4e86\u6cd5\u5f8b\u9886\u57df\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3aMAS\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002", "topic": "agent analysis"}}
{"id": "2509.24202", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24202", "abs": "https://arxiv.org/abs/2509.24202", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Bo Kai", "Minjing Dong", "Tao Huang", "Tom A. Lamb", "Jialin Yu", "Philip H. S. Torr", "Chang Xu"], "title": "Can Large Language Models Express Uncertainty Like Human?", "comment": "10 pages", "summary": "Large language models (LLMs) are increasingly used in high-stakes settings,\nwhere overconfident responses can mislead users. Reliable confidence estimation\nhas been shown to enhance trust and task accuracy. Yet existing methods face\npractical barriers: logits are often hidden, multi-sampling is computationally\nexpensive, and verbalized numerical uncertainty (e.g., giving a 0-100 score)\ndeviates from natural communication. We revisit linguistic confidence (LC),\nwhere models express uncertainty through hedging language (e.g., probably,\nmight), offering a lightweight and human-centered alternative. To advance this\ndirection, we (1) release the first diverse, large-scale dataset of hedging\nexpressions with human-annotated confidence scores, and (2) propose a\nlightweight mapper that converts hedges into confidence scores at near-zero\ncost. Building on these resources, we (3) conduct the first systematic study of\nLC across modern LLMs and QA benchmarks, revealing that while most LLMs\nunderperform in expressing reliable LC, carefully designed prompting achieves\ncompetitive calibration and discriminability. Finally, we (4) introduce a\nfine-tuning framework that further improves LC reliability. Taken together, our\nwork positions linguistic confidence as a scalable, efficient, and\nhuman-aligned approach to LLM uncertainty estimation, and calls for deeper\nexploration of this promising yet underexplored direction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u4f5c\u4e3aLLM\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u5bf9\u51b2\u8868\u8fbe\u6570\u636e\u96c6\u548c\u8f7b\u91cf\u7ea7\u6620\u5c04\u5668\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u73b0\u4ee3LLM\u7684\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u5b9e\u9645\u969c\u788d\uff1alogits\u901a\u5e38\u9690\u85cf\u3001\u591a\u91c7\u6837\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u503c\u5316\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u4e0d\u81ea\u7136\u3002\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u4e14\u4ee5\u4eba\u4e3a\u672c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "1) \u53d1\u5e03\u9996\u4e2a\u591a\u6837\u5316\u3001\u5927\u89c4\u6a21\u7684\u5bf9\u51b2\u8868\u8fbe\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u8f7b\u91cf\u7ea7\u6620\u5c04\u5668\u5c06\u5bf9\u51b2\u8868\u8fbe\u8f6c\u6362\u4e3a\u7f6e\u4fe1\u5ea6\u5206\u6570\uff1b3) \u7cfb\u7edf\u7814\u7a76\u73b0\u4ee3LLM\u7684\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u8868\u73b0\uff1b4) \u5f15\u5165\u5fae\u8c03\u6846\u67b6\u6539\u8fdb\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570LLM\u5728\u8868\u8fbe\u53ef\u9760\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u53ef\u4ee5\u8fbe\u5230\u7ade\u4e89\u6027\u7684\u6821\u51c6\u548c\u533a\u5206\u80fd\u529b\u3002\u5fae\u8c03\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u4f5c\u4e3aLLM\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u65b9\u6cd5\u5177\u6709\u524d\u666f\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002", "topic": "agent analysis"}}
{"id": "2509.24978", "categories": ["cs.AI", "cond-mat.quant-gas", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.24978", "abs": "https://arxiv.org/abs/2509.24978", "authors": ["Maximilian N\u00e4gele", "Florian Marquardt"], "title": "Agentic Exploration of Physics Models", "comment": null, "summary": "The process of scientific discovery relies on an interplay of observations,\nanalysis, and hypothesis generation. Machine learning is increasingly being\nadopted to address individual aspects of this process. However, it remains an\nopen challenge to fully automate the open-ended, heuristic, iterative loop\nrequired to discover the laws of an unknown system by exploring it through\nexperiments and analysis, without tailoring the approach to the specifics of a\ngiven task. Here, we introduce SciExplorer, an agent that leverages large\nlanguage model tool-use capabilities to enable free-form exploration of systems\nwithout any domain-specific blueprints, and apply it to the exploration of\nphysical systems that are initially unknown to the agent. We test SciExplorer\non a broad set of models spanning mechanical dynamical systems, wave evolution,\nand quantum many-body physics. Despite using a minimal set of tools, primarily\nbased on code execution, we observe impressive performance on tasks such as\nrecovering equations of motion from observed dynamics and inferring\nHamiltonians from expectation values. The demonstrated effectiveness of this\nsetup opens the door towards similar scientific exploration in other domains,\nwithout the need for finetuning or task-specific instructions.", "AI": {"tldr": "SciExplorer\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a2\u7d22\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u7b49\u5de5\u5177\u81ea\u4e3b\u63a2\u7d22\u672a\u77e5\u7269\u7406\u7cfb\u7edf\uff0c\u65e0\u9700\u7279\u5b9a\u9886\u57df\u84dd\u56fe\u5373\u53ef\u6062\u590d\u8fd0\u52a8\u65b9\u7a0b\u548c\u54c8\u5bc6\u987f\u91cf\u3002", "motivation": "\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u9700\u8981\u89c2\u5bdf\u3001\u5206\u6790\u548c\u5047\u8bbe\u751f\u6210\u7684\u5faa\u73af\uff0c\u76ee\u524d\u673a\u5668\u5b66\u4e60\u4e3b\u8981\u5904\u7406\u5355\u4e2a\u73af\u8282\uff0c\u4f46\u5b8c\u5168\u81ea\u52a8\u5316\u8fd9\u79cd\u5f00\u653e\u5f0f\u63a2\u7d22\u5faa\u73af\u4ecd\u5177\u6311\u6218\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u7b49\u6700\u5c0f\u5316\u5de5\u5177\u96c6\uff0c\u5bf9\u672a\u77e5\u7269\u7406\u7cfb\u7edf\u8fdb\u884c\u81ea\u7531\u5f62\u5f0f\u63a2\u7d22\u3002", "result": "\u5728\u673a\u68b0\u52a8\u529b\u5b66\u7cfb\u7edf\u3001\u6ce2\u6f14\u5316\u3001\u91cf\u5b50\u591a\u4f53\u7269\u7406\u7b49\u5e7f\u6cdb\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ece\u89c2\u6d4b\u52a8\u6001\u6062\u590d\u8fd0\u52a8\u65b9\u7a0b\uff0c\u4ece\u671f\u671b\u503c\u63a8\u65ad\u54c8\u5bc6\u987f\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5176\u4ed6\u9886\u57df\u7684\u79d1\u5b66\u63a2\u7d22\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u4efb\u52a1\u7279\u5b9a\u6307\u4ee4\u3002", "topic": "agent analysis"}}
{"id": "2509.23711", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23711", "abs": "https://arxiv.org/abs/2509.23711", "authors": ["Ziheng Cheng", "Xin Guo", "Yufei Zhang"], "title": "Bridging Discrete and Continuous RL: Stable Deterministic Policy Gradient with Martingale Characterization", "comment": null, "summary": "The theory of discrete-time reinforcement learning (RL) has advanced rapidly\nover the past decades. Although primarily designed for discrete environments,\nmany real-world RL applications are inherently continuous and complex. A major\nchallenge in extending discrete-time algorithms to continuous-time settings is\ntheir sensitivity to time discretization, often leading to poor stability and\nslow convergence. In this paper, we investigate deterministic policy gradient\nmethods for continuous-time RL. We derive a continuous-time policy gradient\nformula based on an analogue of the advantage function and establish its\nmartingale characterization. This theoretical foundation leads to our proposed\nalgorithm, CT-DDPG, which enables stable learning with deterministic policies\nin continuous-time environments. Numerical experiments show that the proposed\nCT-DDPG algorithm offers improved stability and faster convergence compared to\nexisting discrete-time and continuous-time methods, across a wide range of\ncontrol tasks with varying time discretizations and noise levels.", "AI": {"tldr": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5CT-DDPG\uff0c\u89e3\u51b3\u79bb\u6563\u65f6\u95f4RL\u7b97\u6cd5\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u7684\u65f6\u95f4\u79bb\u6563\u5316\u654f\u611f\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u548c\u66f4\u5feb\u6536\u655b\u3002", "motivation": "\u867d\u7136\u79bb\u6563\u65f6\u95f4RL\u7406\u8bba\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bb8\u591a\u5b9e\u9645RL\u5e94\u7528\u672c\u8d28\u4e0a\u662f\u8fde\u7eed\u4e14\u590d\u6742\u7684\u3002\u5c06\u79bb\u6563\u65f6\u95f4\u7b97\u6cd5\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u8bbe\u7f6e\u7684\u4e3b\u8981\u6311\u6218\u662f\u5bf9\u65f6\u95f4\u79bb\u6563\u5316\u7684\u654f\u611f\u6027\uff0c\u901a\u5e38\u5bfc\u81f4\u7a33\u5b9a\u6027\u5dee\u548c\u6536\u655b\u6162\u3002", "method": "\u63a8\u5bfc\u57fa\u4e8e\u4f18\u52bf\u51fd\u6570\u6a21\u62df\u7684\u8fde\u7eed\u65f6\u95f4\u7b56\u7565\u68af\u5ea6\u516c\u5f0f\uff0c\u5efa\u7acb\u5176\u9785\u7279\u6027\u8868\u5f81\uff0c\u63d0\u51faCT-DDPG\u7b97\u6cd5\uff0c\u5728\u8fde\u7eed\u65f6\u95f4\u73af\u5883\u4e2d\u5b9e\u73b0\u786e\u5b9a\u6027\u7b56\u7565\u7684\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cCT-DDPG\u7b97\u6cd5\u5728\u5177\u6709\u4e0d\u540c\u65f6\u95f4\u79bb\u6563\u5316\u548c\u566a\u58f0\u6c34\u5e73\u7684\u5404\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u79bb\u6563\u65f6\u95f4\u548c\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u8fde\u7eed\u65f6\u95f4\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e3a\u8fde\u7eed\u73af\u5883\u4e2d\u7684RL\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u514b\u670d\u4e86\u79bb\u6563\u65f6\u95f4\u7b97\u6cd5\u7684\u65f6\u95f4\u79bb\u6563\u5316\u654f\u611f\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25047", "abs": "https://arxiv.org/abs/2509.25047", "authors": ["Ram Ramrakhya", "Andrew Szot", "Omar Attia", "Yuhao Yang", "Anh Nguyen", "Bogdan Mazoure", "Zhe Gan", "Harsh Agrawal", "Alexander Toshev"], "title": "Scaling Synthetic Task Generation for Agents via Exploration", "comment": null, "summary": "Post-Training Multimodal Large Language Models (MLLMs) to build interactive\nagents holds promise across domains such as computer-use, web navigation, and\nrobotics. A key challenge in scaling such post-training is lack of high-quality\ndownstream agentic task datasets with tasks that are diverse, feasible, and\nverifiable. Existing approaches for task generation rely heavily on human\nannotation or prompting MLLM with limited downstream environment information,\nwhich is either costly or poorly scalable as it yield tasks with limited\ncoverage. To remedy this, we present AutoPlay, a scalable pipeline for task\ngeneration that explicitly explores interactive environments to discover\npossible interactions and current state information to synthesize\nenvironment-grounded tasks. AutoPlay operates in two stages: (i) an exploration\nphase, where an MLLM explorer agent systematically uncovers novel environment\nstates and functionalities, and (ii) a task generation phase, where a task\ngenerator leverages exploration trajectories and a set of task guideline\nprompts as context to synthesize diverse, executable, and verifiable tasks. We\nshow AutoPlay generates 20k tasks across 20 Android applications and 10k tasks\nacross 13 applications Ubuntu applications to train mobile-use and computer-use\nagents. AutoPlay generated tasks enable large-scale task demonstration\nsynthesis without human annotation by employing an MLLM task executor and\nverifier. This data enables training MLLM-based UI agents that improve success\nrates up to $20.0\\%$ on mobile-use and $10.9\\%$ on computer-use scenarios. In\naddition, AutoPlay generated tasks combined with MLLM verifier-based rewards\nenable scaling reinforcement learning training of UI agents, leading to an\nadditional $5.7\\%$ gain. coverage. These results establish AutoPlay as a\nscalable approach for post-training capable MLLM agents reducing reliance on\nhuman annotation.", "AI": {"tldr": "AutoPlay\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u63a2\u7d22\u4ea4\u4e92\u73af\u5883\u6765\u53d1\u73b0\u53ef\u80fd\u7684\u4ea4\u4e92\u548c\u72b6\u6001\u4fe1\u606f\uff0c\u4ece\u800c\u5408\u6210\u73af\u5883\u57fa\u7840\u4efb\u52a1\uff0c\u7528\u4e8e\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u4efb\u52a1\u751f\u6210\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u4f7f\u7528\u6709\u9650\u73af\u5883\u4fe1\u606f\u63d0\u793aMLLM\uff0c\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u5bfc\u81f4\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u6709\u9650\u3002", "method": "AutoPlay\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u63a2\u7d22\u9636\u6bb5\u8ba9MLLM\u63a2\u7d22\u4ee3\u7406\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u65b0\u73af\u5883\u72b6\u6001\u548c\u529f\u80fd\uff1b\u4efb\u52a1\u751f\u6210\u9636\u6bb5\u5229\u7528\u63a2\u7d22\u8f68\u8ff9\u548c\u4efb\u52a1\u6307\u5bfc\u63d0\u793a\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u5408\u6210\u591a\u6837\u3001\u53ef\u6267\u884c\u4e14\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u3002", "result": "AutoPlay\u572820\u4e2aAndroid\u5e94\u7528\u4e2d\u751f\u621020k\u4efb\u52a1\uff0c\u572813\u4e2aUbuntu\u5e94\u7528\u4e2d\u751f\u621010k\u4efb\u52a1\uff0c\u4f7f\u79fb\u52a8\u4f7f\u7528\u548c\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u9ad820.0%\u548c10.9%\u3002\u7ed3\u5408MLLM\u9a8c\u8bc1\u5668\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5e26\u6765\u989d\u59165.7%\u7684\u589e\u76ca\u3002", "conclusion": "AutoPlay\u4e3a\u8bad\u7ec3\u6709\u80fd\u529b\u7684MLLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "topic": "swe application"}}
{"id": "2509.25052", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25052", "abs": "https://arxiv.org/abs/2509.25052", "authors": ["Sai Wang", "Yu Wu", "Zhongwen Xu"], "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and Planning", "comment": null, "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.", "AI": {"tldr": "\u63d0\u51faCogito, ergo ludo (CEL)\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u63a8\u7406\u548c\u89c4\u5212\u6765\u5b66\u4e60\u73af\u5883\u89c4\u5219\u548c\u7b56\u7565\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u7ecf\u9a8c\u4e14\u77e5\u8bc6\u7f16\u7801\u4e0d\u900f\u660e\uff0c\u9700\u8981\u66f4\u53ef\u89e3\u91ca\u3001\u901a\u8fc7\u63a8\u7406\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "CEL\u667a\u80fd\u4f53\u901a\u8fc7\u4ea4\u4e92-\u53cd\u601d\u5faa\u73af\uff0c\u5728\u6bcf\u6b21\u56de\u5408\u540e\u8fdb\u884c\u89c4\u5219\u5f52\u7eb3\u548c\u7b56\u7565\u603b\u7ed3\uff0c\u6784\u5efa\u663e\u5f0f\u7684\u73af\u5883\u6a21\u578b\u548c\u53ef\u6267\u884c\u7b56\u7565\u624b\u518c\u3002", "result": "\u5728\u591a\u4e2a\u7f51\u683c\u4e16\u754c\u4efb\u52a1\u4e2d\u6210\u529f\u5b66\u4e60\u638c\u63e1\u6e38\u620f\uff0c\u4ece\u7a00\u758f\u5956\u52b1\u4e2d\u81ea\u4e3b\u53d1\u73b0\u89c4\u5219\u5e76\u5f00\u53d1\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u5c55\u793a\u4e86\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u6784\u5efa\u900f\u660e\u4e16\u754c\u6a21\u578b\u7684\u901a\u7528\u53ef\u89e3\u91ca\u667a\u80fd\u4f53\u8def\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25112", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.25112", "abs": "https://arxiv.org/abs/2509.25112", "authors": ["Yiquan Wang", "Tin-Yeh Huang", "Qingyun Gao", "Jialin Zhang"], "title": "HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis", "comment": null, "summary": "Heatwaves pose complex cascading risks across interconnected climate, social,\nand economic systems, but knowledge fragmentation in scientific literature\nhinders comprehensive understanding of these risk pathways. We introduce HeDA\n(Heatwave Discovery Agent), an intelligent multi-agent system designed for\nautomated scientific discovery through knowledge graph construction and\nmulti-layer risk propagation analysis. HeDA processes over 10,247 academic\npapers to construct a comprehensive knowledge graph with 23,156 nodes and\n89,472 relationships, employing novel multi-layer risk propagation analysis to\nsystematically identify overlooked risk transmission pathways. Our system\nachieves 78.9% accuracy on complex question-answering tasks, outperforming\nstate-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA\nsuccessfully discovered five previously unidentified high-impact risk chains,\nsuch as the pathway where a heatwave leads to a water demand surge, resulting\nin industrial water restrictions and ultimately causing small business\ndisruption, which were validated through historical case studies and domain\nexpert review. This work presents a new paradigm for AI-driven scientific\ndiscovery, providing actionable insights for developing more resilient climate\nadaptation strategies.", "AI": {"tldr": "HeDA\u662f\u4e00\u4e2a\u7528\u4e8e\u70ed\u6d6a\u98ce\u9669\u53d1\u73b0\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u5c42\u98ce\u9669\u4f20\u64ad\u5206\u6790\uff0c\u81ea\u52a8\u53d1\u73b0\u79d1\u5b66\u6587\u732e\u4e2d\u672a\u88ab\u8bc6\u522b\u7684\u98ce\u9669\u4f20\u64ad\u8def\u5f84\u3002", "motivation": "\u70ed\u6d6a\u5728\u6c14\u5019\u3001\u793e\u4f1a\u548c\u7ecf\u6d4e\u7cfb\u7edf\u4e2d\u9020\u6210\u590d\u6742\u7684\u7ea7\u8054\u98ce\u9669\uff0c\u4f46\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u77e5\u8bc6\u788e\u7247\u5316\u963b\u788d\u4e86\u5bf9\u8fd9\u4e9b\u98ce\u9669\u8def\u5f84\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u5904\u740610,247\u7bc7\u5b66\u672f\u8bba\u6587\u6784\u5efa\u5305\u542b23,156\u4e2a\u8282\u70b9\u548c89,472\u4e2a\u5173\u7cfb\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528\u591a\u5c42\u98ce\u9669\u4f20\u64ad\u5206\u6790\u8bc6\u522b\u98ce\u9669\u4f20\u64ad\u8def\u5f84\u3002", "result": "\u5728\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u4e0a\u8fbe\u523078.9%\u51c6\u786e\u7387\uff0c\u6bd4GPT-4\u9ad813.7%\uff1b\u53d1\u73b0\u4e865\u4e2a\u5148\u524d\u672a\u8bc6\u522b\u7684\u9ad8\u5f71\u54cd\u98ce\u9669\u94fe\uff0c\u5e76\u901a\u8fc7\u5386\u53f2\u6848\u4f8b\u548c\u4e13\u5bb6\u9a8c\u8bc1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86AI\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u5236\u5b9a\u66f4\u5177\u97e7\u6027\u7684\u6c14\u5019\u9002\u5e94\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2509.24282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24282", "abs": "https://arxiv.org/abs/2509.24282", "authors": ["Gyuhyeon Seo", "Jungwoo Yang", "Junseong Pyo", "Nalim Kim", "Jonggeun Lee", "Yohan Jo"], "title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents", "comment": null, "summary": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.\nHowever, smart homes introduce distinct challenges, requiring agents to handle\nlatent user intents, temporal dependencies, device constraints, scheduling, and\nmore. The main bottlenecks for developing smart home agents with such\ncapabilities include the lack of a realistic simulation environment where\nagents can interact with devices and observe the results, as well as a\nchallenging benchmark to evaluate them. To address this, we introduce\n$\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart\ndevices, supports API calls, and reflects changes in environmental variables.\nBy building the simulator on the Matter protocol (the global industry standard\nfor smart home communication), SimuHome provides a high-fidelity environment,\nand agents validated in SimuHome can be deployed on real Matter-compliant\ndevices with minimal adaptation. We provide a challenging benchmark of 600\nepisodes across twelve user query types that require the aforementioned\ncapabilities. Our evaluation of 11 agents under a unified ReAct framework\nreveals that while models perform well on simple tasks, they struggle with\nlatent intent inference, state verification, and especially temporal\nscheduling. Even the top-performing model, GPT-4.1, reaches only 54% success\nrate. These findings highlight a critical need for methods that can reliably\nverify the current state via tools before acting and coordinate time-dependent\nactions.", "AI": {"tldr": "SimuHome\u662f\u4e00\u4e2a\u57fa\u4e8eMatter\u534f\u8bae\u7684\u667a\u80fd\u5bb6\u5c45\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u667a\u80fd\u5bb6\u5c45\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u5bb6\u5c45\u73af\u5883\u4e2d\u5f00\u53d1LLM\u667a\u80fd\u4f53\u7684\u74f6\u9888\uff1a\u7f3a\u4e4f\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u57fa\u4e8eMatter\u534f\u8bae\u7684SimuHome\u6a21\u62df\u5668\uff0c\u63d0\u4f9b\u65f6\u95f4\u52a0\u901f\u7684\u667a\u80fd\u8bbe\u5907\u6a21\u62df\u3001API\u8c03\u7528\u548c\u73af\u5883\u53d8\u91cf\u53d8\u5316\u3002\u521b\u5efa\u5305\u542b600\u4e2aepisode\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d612\u79cd\u7528\u6237\u67e5\u8be2\u7c7b\u578b\u3002", "result": "\u8bc4\u4f3011\u4e2a\u667a\u80fd\u4f53\u53d1\u73b0\uff0c\u5b83\u4eec\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6f5c\u5728\u610f\u56fe\u63a8\u65ad\u3001\u72b6\u6001\u9a8c\u8bc1\u548c\u65f6\u95f4\u8c03\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u6700\u4f73\u6a21\u578bGPT-4.1\u7684\u6210\u529f\u7387\u4ec5\u4e3a54%\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u53ef\u9760\u9a8c\u8bc1\u5f53\u524d\u72b6\u6001\u5e76\u534f\u8c03\u65f6\u95f4\u76f8\u5173\u52a8\u4f5c\u7684\u65b9\u6cd5\u3002", "topic": "swe application"}}
{"id": "2509.25123", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25123", "abs": "https://arxiv.org/abs/2509.25123", "authors": ["Lifan Yuan", "Weize Chen", "Yuchen Zhang", "Ganqu Cui", "Hanbin Wang", "Ziming You", "Ning Ding", "Zhiyuan Liu", "Maosong Sun", "Hao Peng"], "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones", "comment": null, "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.", "AI": {"tldr": "RL\u786e\u5b9e\u80fd\u8ba9LLMs\u5b66\u4e60\u771f\u6b63\u7684\u65b0\u6280\u80fd\uff0c\u7279\u522b\u662f\u901a\u8fc7\u7ec4\u5408\u73b0\u6709\u6280\u80fd\u6765\u83b7\u5f97\u7ec4\u5408\u80fd\u529b\uff0c\u8fd9\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u83b7\u5f97\u65b0\u8ba4\u77e5\u6280\u80fd\u7684\u65b9\u5f0f\u3002", "motivation": "\u63a2\u8ba8RL\u5728LLM\u540e\u8bad\u7ec3\u4e2d\u662f\u5426\u771f\u6b63\u6559\u6388\u65b0\u6280\u80fd\uff0c\u8fd8\u662f\u4ec5\u4ec5\u6fc0\u6d3b\u73b0\u6709\u6280\u80fd\uff0c\u89e3\u51b3\u5173\u4e8eRL\u4f5c\u7528\u7684\u4e89\u8bae\u3002", "method": "\u5f00\u53d1\u5408\u6210\u6846\u67b6\uff0c\u5b9a\u4e49\u6280\u80fd\u4e3a\u63a8\u65ad\u5b57\u7b26\u4e32\u8f6c\u6362\u51fd\u6570\u8f93\u51fa\u7684\u80fd\u529b\uff0c\u7814\u7a76LLM\u5728RL\u8bad\u7ec3\u540e\u662f\u5426\u80fd\u5b66\u4e60\u672a\u89c1\u8fc7\u51fd\u6570\u7ec4\u5408\u3002", "result": "RL\u4f7fLLM\u80fd\u591f\u5b66\u4e60\u672a\u89c1\u8fc7\u7684\u51fd\u6570\u7ec4\u5408\uff0c\u8fd9\u79cd\u7ec4\u5408\u80fd\u529b\u80fd\u6cdb\u5316\u5230\u66f4\u590d\u6742\u95ee\u9898\uff0c\u5e76\u80fd\u8fc1\u79fb\u5230\u4e0d\u540c\u4efb\u52a1\u3002", "conclusion": "RL\u80fd\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u5efa\u8bae\u5148\u6784\u5efa\u5177\u6709\u57fa\u672c\u6280\u80fd\u7684\u57fa\u5ea7\u6a21\u578b\uff0c\u7136\u540e\u7528RL\u6fc0\u52b1\u9ad8\u7ea7\u3001\u53ef\u6cdb\u5316\u7684\u590d\u6742\u95ee\u9898\u89e3\u51b3\u6280\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23750", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23750", "abs": "https://arxiv.org/abs/2509.23750", "authors": ["Li Wang", "Sudun", "Xingjian Zhang", "Wenjun Wu", "Lei Huang"], "title": "An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms", "comment": null, "summary": "Batch Normalization (BN) has played a pivotal role in the success of deep\nlearning by improving training stability, mitigating overfitting, and enabling\nmore effective optimization. However, its adoption in deep reinforcement\nlearning (DRL) has been limited due to the inherent non-i.i.d. nature of data\nand the dynamically shifting distributions induced by the agent's learning\nprocess. In this paper, we argue that, despite these challenges, BN retains\nunique advantages in DRL settings, particularly through its stochasticity and\nits ability to ease training. When applied appropriately, BN can adapt to\nevolving data distributions and enhance both convergence speed and final\nperformance. To this end, we conduct a comprehensive empirical study on the use\nof BN in off-policy actor-critic algorithms, systematically analyzing how\ndifferent training and evaluation modes impact performance. We further identify\nfailure modes that lead to instability or divergence, analyze their underlying\ncauses, and propose the Mode-Aware Batch Normalization (MA-BN) method with\npractical actionable recommendations for robust BN integration in DRL\npipelines. We also empirically validate that, in RL settings, MA-BN accelerates\nand stabilizes training, broadens the effective learning rate range, enhances\nexploration, and reduces overall optimization difficulty. Our code is available\nat: https://github.com/monster476/ma-bn.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6a21\u5f0f\u611f\u77e5\u6279\u91cf\u5f52\u4e00\u5316(MA-BN)\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6279\u91cf\u5f52\u4e00\u5316\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u56e0\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6279\u91cf\u5f52\u4e00\u5316\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\uff0c\u5176\u5728DRL\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u9650\u5236\u3002\u4f5c\u8005\u8ba4\u4e3aBN\u5728DRL\u4e2d\u4ecd\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5176\u968f\u673a\u6027\u548c\u8bad\u7ec3\u7b80\u5316\u80fd\u529b\u3002", "method": "\u5bf9\u79bb\u7ebf\u7b56\u7565actor-critic\u7b97\u6cd5\u4e2dBN\u7684\u4f7f\u7528\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u5f0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8bc6\u522b\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u5931\u6548\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u6a21\u5f0f\u611f\u77e5\u6279\u91cf\u5f52\u4e00\u5316(MA-BN)\u65b9\u6cd5\u3002", "result": "MA-BN\u5728RL\u8bbe\u7f6e\u4e2d\u52a0\u901f\u548c\u7a33\u5b9a\u8bad\u7ec3\uff0c\u6269\u5927\u6709\u6548\u5b66\u4e60\u7387\u8303\u56f4\uff0c\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u964d\u4f4e\u6574\u4f53\u4f18\u5316\u96be\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u5e94\u7528\uff0cBN\u53ef\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\uff0c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\uff0cMA-BN\u4e3aDRL\u4e2dBN\u7684\u7a33\u5065\u96c6\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25139", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.25139", "abs": "https://arxiv.org/abs/2509.25139", "authors": ["Yue Zhang", "Tianyi Ma", "Zun Wang", "Yanyuan Qiao", "Parisa Kordjamshidi"], "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in LLMs", "comment": null, "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u89c6\u89d2\u6587\u672c\u63cf\u8ff0\u548c\u7c7b\u6bd4\u63a8\u7406\u6765\u589e\u5f3a\u96f6\u6837\u672cLLM\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u7684\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672cLLM\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4ee3\u7406\u8981\u4e48\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u6587\u672c\u573a\u666f\u63cf\u8ff0\uff08\u53ef\u80fd\u8fc7\u5ea6\u7b80\u5316\u89c6\u89c9\u7ec6\u8282\uff09\uff0c\u8981\u4e48\u5904\u7406\u539f\u59cb\u56fe\u50cf\u8f93\u5165\uff08\u53ef\u80fd\u65e0\u6cd5\u6355\u83b7\u9ad8\u7ea7\u63a8\u7406\u6240\u9700\u7684\u62bd\u8c61\u8bed\u4e49\uff09", "method": "\u901a\u8fc7\u6574\u5408\u591a\u89c6\u89d2\u7684\u6587\u672c\u63cf\u8ff0\u6765\u4fc3\u8fdb\u56fe\u50cf\u95f4\u7684\u7c7b\u6bd4\u63a8\u7406\uff0c\u5229\u7528\u57fa\u4e8e\u6587\u672c\u7684\u7c7b\u6bd4\u63a8\u7406\u589e\u5f3a\u5168\u5c40\u573a\u666f\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406", "result": "\u5728R2R\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u5bfc\u822a\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "\u591a\u89c6\u89d2\u6587\u672c\u63cf\u8ff0\u548c\u7c7b\u6bd4\u63a8\u7406\u80fd\u6709\u6548\u589e\u5f3a\u5bfc\u822a\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2509.24296", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24296", "abs": "https://arxiv.org/abs/2509.24296", "authors": ["Zherui Li", "Zheng Nie", "Zhenhong Zhou", "Yufei Guo", "Yue Liu", "Yitong Zhang", "Yu Cheng", "Qingsong Wen", "Kun Wang", "Jiaheng Zhang"], "title": "DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models", "comment": null, "summary": "The rapid advancement of Diffusion Large Language Models (dLLMs) introduces\nunprecedented vulnerabilities that are fundamentally distinct from\nAutoregressive LLMs, stemming from their iterative and parallel generation\nmechanisms. In this paper, we conduct an in-depth analysis of dLLM\nvulnerabilities to jailbreak attacks across two distinct dimensions: intra-step\nand inter-step dynamics. Experimental results reveal a harmful bias inherent in\nthe standard greedy remasking strategy and identify a critical phenomenon we\nterm Denoising-path Dependence, where the safety of early-stage tokens\ndecisively influences the final output. These findings also indicate that while\ncurrent decoding strategies constitute a significant vulnerability, dLLMs\npossess a substantial intrinsic safety potential. To unlock this potential, we\npropose DiffuGuard, a training-free defense framework that addresses\nvulnerabilities through a dual-stage approach: Stochastic Annealing Remasking\ndynamically introduces controlled randomness to mitigate greedy selection bias,\nwhile Block-level Audit and Repair exploits internal model representations for\nautonomous risk detection and guided correction. Comprehensive experiments on\nfour dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack\nSuccess Rate against six diverse jailbreak methods from 47.9% to 14.7% while\npreserving model utility and efficiency. Our code is available at:\nhttps://github.com/niez233/DiffuGuard.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u5728\u8d8a\u72f1\u653b\u51fb\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u63ed\u793a\u4e86\u8d2a\u5a6a\u91cd\u63a9\u7801\u7b56\u7565\u7684\u6709\u5bb3\u504f\u89c1\u548c\u53bb\u566a\u8def\u5f84\u4f9d\u8d56\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86DiffuGuard\u9632\u5fa1\u6846\u67b6\u6765\u63d0\u5347dLLMs\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u7531\u4e8e\u5176\u8fed\u4ee3\u548c\u5e76\u884c\u751f\u6210\u673a\u5236\uff0c\u9762\u4e34\u7740\u4e0e\u81ea\u56de\u5f52LLMs\u6839\u672c\u4e0d\u540c\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u8fd9\u4e9b\u65b0\u578b\u8106\u5f31\u6027\u5e76\u5f00\u53d1\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u5206\u6790dLLM\u6f0f\u6d1e\uff1a\u6b65\u5185\u52a8\u6001\u548c\u6b65\u95f4\u52a8\u6001\uff1b\u63d0\u51faDiffuGuard\u9632\u5fa1\u6846\u67b6\uff0c\u5305\u542b\u968f\u673a\u9000\u706b\u91cd\u63a9\u7801(\u52a8\u6001\u5f15\u5165\u53d7\u63a7\u968f\u673a\u6027)\u548c\u5757\u7ea7\u5ba1\u8ba1\u4fee\u590d(\u5229\u7528\u5185\u90e8\u8868\u793a\u8fdb\u884c\u98ce\u9669\u68c0\u6d4b\u548c\u5f15\u5bfc\u4fee\u6b63)\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDiffuGuard\u5728\u56db\u4e2adLLM\u4e0a\u5bf9\u516d\u79cd\u4e0d\u540c\u8d8a\u72f1\u65b9\u6cd5\u7684\u653b\u51fb\u6210\u529f\u7387\u4ece47.9%\u964d\u4f4e\u523014.7%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7528\u548c\u6548\u7387\u3002", "conclusion": "\u5f53\u524d\u89e3\u7801\u7b56\u7565\u6784\u6210\u663e\u8457\u6f0f\u6d1e\uff0c\u4f46dLLMs\u5177\u6709\u5de8\u5927\u7684\u5185\u5728\u5b89\u5168\u6f5c\u529b\uff1bDiffuGuard\u80fd\u591f\u6709\u6548\u89e3\u9501\u8fd9\u79cd\u6f5c\u529b\uff0c\u63d0\u4f9b\u8bad\u7ec3\u514d\u8d39\u7684\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2509.25140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25140", "abs": "https://arxiv.org/abs/2509.25140", "authors": ["Siru Ouyang", "Jun Yan", "I-Hung Hsu", "Yanfei Chen", "Ke Jiang", "Zifeng Wang", "Rujun Han", "Long T. Le", "Samira Daruki", "Xiangru Tang", "Vishy Tirumalashetty", "George Lee", "Mahsan Rofouei", "Hangfei Lin", "Jiawei Han", "Chen-Yu Lee", "Tomas Pfister"], "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory", "comment": "11 pages, 7 figures, 4 tables", "summary": "With the growing adoption of large language model agents in persistent\nreal-world roles, they naturally encounter continuous streams of tasks. A key\nlimitation, however, is their failure to learn from the accumulated interaction\nhistory, forcing them to discard valuable insights and repeat past errors. We\npropose ReasoningBank, a novel memory framework that distills generalizable\nreasoning strategies from an agent's self-judged successful and failed\nexperiences. At test time, an agent retrieves relevant memories from\nReasoningBank to inform its interaction and then integrates new learnings back,\nenabling it to become more capable over time. Building on this powerful\nexperience learner, we further introduce memory-aware test-time scaling\n(MaTTS), which accelerates and diversifies this learning process by scaling up\nthe agent's interaction experience. By allocating more compute to each task,\nthe agent generates abundant, diverse experiences that provide rich contrastive\nsignals for synthesizing higher-quality memory. The better memory in turn\nguides more effective scaling, establishing a powerful synergy between memory\nand test-time scaling. Across web browsing and software engineering benchmarks,\nReasoningBank consistently outperforms existing memory mechanisms that store\nraw trajectories or only successful task routines, improving both effectiveness\nand efficiency; MaTTS further amplifies these gains. These findings establish\nmemory-driven experience scaling as a new scaling dimension, enabling agents to\nself-evolve with emergent behaviors naturally arise.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReasoningBank\u8bb0\u5fc6\u6846\u67b6\uff0c\u4ece\u667a\u80fd\u4f53\u7684\u6210\u529f\u548c\u5931\u8d25\u7ecf\u9a8c\u4e2d\u63d0\u70bc\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u5185\u5b58\u611f\u77e5\u6d4b\u8bd5\u65f6\u6269\u5c55(MaTTS)\u6765\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u6301\u7eed\u4efb\u52a1\u6d41\u4e2d\u65e0\u6cd5\u4ece\u4ea4\u4e92\u5386\u53f2\u4e2d\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u907f\u514d\u91cd\u590d\u8fc7\u53bb\u7684\u9519\u8bef\u3002", "method": "ReasoningBank\u6846\u67b6\u4ece\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8bc4\u4f30\u6210\u529f\u548c\u5931\u8d25\u7ecf\u9a8c\u4e2d\u63d0\u53d6\u63a8\u7406\u7b56\u7565\uff0c\u5728\u6d4b\u8bd5\u65f6\u68c0\u7d22\u76f8\u5173\u8bb0\u5fc6\u6765\u6307\u5bfc\u4ea4\u4e92\uff1bMaTTS\u901a\u8fc7\u6269\u5c55\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\u7ecf\u9a8c\u6765\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u7f51\u7edc\u6d4f\u89c8\u548c\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReasoningBank\u59cb\u7ec8\u4f18\u4e8e\u5b58\u50a8\u539f\u59cb\u8f68\u8ff9\u6216\u4ec5\u6210\u529f\u4efb\u52a1\u4f8b\u7a0b\u7684\u73b0\u6709\u8bb0\u5fc6\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u6709\u6548\u6027\u548c\u6548\u7387\uff1bMaTTS\u8fdb\u4e00\u6b65\u653e\u5927\u4e86\u8fd9\u4e9b\u6536\u76ca\u3002", "conclusion": "\u5185\u5b58\u9a71\u52a8\u7684\u7ecf\u9a8c\u6269\u5c55\u6210\u4e3a\u4e00\u79cd\u65b0\u7684\u6269\u5c55\u7ef4\u5ea6\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u6211\u8fdb\u5316\u5e76\u81ea\u7136\u4ea7\u751f\u6d8c\u73b0\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2509.24297", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24297", "abs": "https://arxiv.org/abs/2509.24297", "authors": ["Junying Wang", "Zicheng Zhang", "Ye Shen", "Yalun Wu", "Yingji Liang", "Yijin Guo", "Farong Wen", "Wenzhe Li", "Xuezhi Zhao", "Qi Jia", "Guangtao Zhai"], "title": "Q-Mirror: Unlocking the Multi-Modal Potential of Scientific Text-Only QA Pairs", "comment": "26 pages", "summary": "High-quality, multi-modal benchmarks are crucial for advancing scientific\nreasoning in large models yet their manual creation is costly and unscalable.\nTo address this bottleneck, we explore the potential for transforming Text-Only\nQA Pairs (TQAs) into high-quality Multi-Modal QA Pairs (MMQAs), which include\nthree parts: 1) Task Definition \\& Evaluation Rubric: We develop a TQA-to-MMQA\nframework and establish a comprehensive, multi-dimensional MMQA quality rubric\nthat provides principles for the transformation. 2) Benchmark Construction:\nThen we construct two extensive benchmarks to rigorously evaluate\nstate-of-the-art generation \\& understanding models on the distinct tasks of\nMMQA generation \\& MMQA quality evaluation. 3) Preliminary Solution: We develop\nan agentic system (Q-Mirror), which operationalizes our framework by\nintegrating MMQA generation and evaluation into a closed loop for iterative\nrefinement. Our experiments show that while state-of-the-art models can\ngenerate MMQAs, their outputs still leave substantial gaps, underscoring the\nneed for reliable evaluation. We further demonstrate that top-tier\nunderstanding models align closely with human judgment in MMQA quality\nassessment. Leveraging both insights, the Q-Mirror agent raises average scores\nfrom 78.90 to 85.22 and pass rates from 72\\% to 95\\%, offering a practical path\nto large-scale scientific benchmarks.", "AI": {"tldr": "\u63d0\u51faTQA-to-MMQA\u6846\u67b6\uff0c\u5c06\u6587\u672c\u95ee\u7b54\u5bf9\u8f6c\u6362\u4e3a\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\u5e76\u5f00\u53d1Q-Mirror\u4ee3\u7406\u7cfb\u7edf\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316", "motivation": "\u624b\u52a8\u521b\u5efa\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u5f00\u53d1TQA-to-MMQA\u8f6c\u6362\u6846\u67b6\uff0c\u5efa\u7acb\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\uff0c\u6784\u5efa\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1Q-Mirror\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u95ed\u73af\u8fed\u4ee3", "result": "\u73b0\u6709\u6a21\u578b\u751f\u6210MMQA\u4ecd\u6709\u660e\u663e\u5dee\u8ddd\uff0c\u9876\u7ea7\u7406\u89e3\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff1bQ-Mirror\u4ee3\u7406\u5c06\u5e73\u5747\u5206\u4ece78.90\u63d0\u5347\u81f385.22\uff0c\u901a\u8fc7\u7387\u4ece72%\u63d0\u5347\u81f395%", "conclusion": "Q-Mirror\u4ee3\u7406\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u901a\u8fc7\u95ed\u73af\u8fed\u4ee3\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u95ee\u7b54\u5bf9\u8d28\u91cf", "topic": "swe benchmark"}}
{"id": "2509.25148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25148", "abs": "https://arxiv.org/abs/2509.25148", "authors": ["FaQiang Qian", "WeiKun Zhang", "Ziliang Wang", "Kang An", "Xuhui Zheng", "Liangjian Wen", "Mengya Gao", "Yong Dai", "Yichao Wu"], "title": "UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following", "comment": null, "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.", "AI": {"tldr": "\u63d0\u51faUniAPL\u7edf\u4e00\u5bf9\u6297\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u540e\u8bad\u7ec3\u5bf9\u9f50\u89c6\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u8bad\u7ec3\u540c\u65f6\u5229\u7528\u6f14\u793a\u504f\u597d\u548c\u6bd4\u8f83\u504f\u597d\u6570\u636e\uff0c\u89e3\u51b3\u4f20\u7edfSFT+RL\u6d41\u7a0b\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfSFT\u540e\u63a5RL\u7684\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff1aSFT\u4f7f\u7528\u9759\u6001\u4e13\u5bb6\u6570\u636e\uff0c\u4f46\u7b56\u7565\u6f14\u5316\u65f6\u751f\u6210\u5206\u5e03\u4f1a\u6f02\u79fb\uff0c\u4f7f\u5f97SFT\u77e5\u8bc6\u53d8\u5f97\u8106\u5f31\u3002\u540e\u7eedRL\u63a2\u7d22\u65f6\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u4e13\u5bb6\u6f14\u793a\u4e2d\u7684\u4e30\u5bcc\u771f\u5b9e\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u7f3a\u4e4f\u57fa\u7840\u7684\u66f4\u65b0\u3002", "method": "\u63d0\u51faUniAPL\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u5355\u9636\u6bb5\u7edf\u4e00\u8bad\u7ec3\u76ee\u6807\uff0c\u4ece\u6df7\u5408\u7684SFT\u548c\u504f\u597d\u6570\u636e\u6279\u6b21\u4e2d\u8054\u5408\u5b66\u4e60\u3002\u5728\u6bcf\u4e2a\u68af\u5ea6\u6b65\u9aa4\u4e2d\uff0c\u5bc6\u96c6\u7684\u4e13\u5bb6\u6f14\u793a\u76f4\u63a5\u57fa\u7840\u548c\u89c4\u8303\u5316\u5728\u7ebf\u63a2\u7d22\u3002", "result": "\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u8bc4\u4f30UniAPL\uff0c\u4f7f\u7528Qwen3-235B-Instruct-2507\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u3002UniAPL\u6a21\u578b\u5339\u914d\u6216\u8d85\u8d8a\u5f3aGRPO\u57fa\u7ebf\uff1aQwen3-0.6B\u63d0\u53475.77%\uff08\u5339\u914d32B\u6a21\u578b\uff09\uff0cQwen3-4B\u63d0\u53473.75%\uff0c\u751a\u81f3\u8d85\u8fc7\u6559\u5e08\u6a21\u578b\u3002", "conclusion": "UniAPL\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u6027\u80fd\u548c\u66f4\u597d\u7684\u884c\u4e3a\u5bf9\u9f50\uff0c\u8f93\u51fa\u66f4\u63a5\u8fd1\u4e13\u5bb6\u6f14\u793a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25154", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25154", "abs": "https://arxiv.org/abs/2509.25154", "authors": ["Dawei Li", "Zhen Tan", "Chengshuai Zhao", "Bohan Jiang", "Baixiang Huang", "Pingchuan Ma", "Abdullah Alnaibari", "Kai Shu", "Huan Liu"], "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments", "comment": "Under review", "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to\nefficiently evaluate candidate content and provide judgment scores. However,\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\nconcerns, underscoring the urgent need for distinguishing them in sensitive\nscenarios like academic peer reviewing. In this work, we propose and formalize\nthe task of judgment detection and systematically investigate the detectability\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\ndetection relies solely on judgment scores and candidates, reflecting\nreal-world scenarios where textual feedback is often unavailable in the\ndetection process. Our preliminary analysis shows that existing LLM-generated\ntext detection methods perform poorly given their incapability to capture the\ninteraction between judgment scores and candidate content -- an aspect crucial\nfor effective judgment detection. Inspired by this, we introduce\n\\textit{J-Detector}, a lightweight and transparent neural detector augmented\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\njudges' biases with candidates' properties for accurate detection. Experiments\nacross diverse datasets demonstrate the effectiveness of \\textit{J-Detector}\nand show how its interpretability enables quantifying biases in LLM judges.\nFinally, we analyze key factors affecting the detectability of LLM-generated\njudgments and validate the practical utility of judgment detection in\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5224\u65ad\u68c0\u6d4b\u4efb\u52a1\uff0c\u7814\u7a76LLM\u751f\u6210\u5224\u65ad\u7684\u53ef\u68c0\u6d4b\u6027\uff0c\u5e76\u5f00\u53d1\u4e86J-Detector\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u663e\u5f0f\u63d0\u53d6\u8bed\u8a00\u548cLLM\u589e\u5f3a\u7279\u5f81\u6765\u51c6\u786e\u68c0\u6d4bLLM\u751f\u6210\u7684\u5224\u65ad\u3002", "motivation": "LLM\u751f\u6210\u7684\u5224\u65ad\u5b58\u5728\u56fa\u6709\u504f\u89c1\u548c\u8106\u5f31\u6027\uff0c\u5728\u5b66\u672f\u8bc4\u5ba1\u7b49\u654f\u611f\u573a\u666f\u4e2d\u9700\u8981\u533a\u5206\u8fd9\u4e9b\u5224\u65ad\uff0c\u4f46\u73b0\u6709LLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5224\u65ad\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u5f00\u53d1J-Detector\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u663e\u5f0f\u63d0\u53d6\u8bed\u8a00\u7279\u5f81\u548cLLM\u589e\u5f3a\u7279\u5f81\uff0c\u5c06LLM\u6cd5\u5b98\u7684\u504f\u89c1\u4e0e\u5019\u9009\u5185\u5bb9\u5c5e\u6027\u8054\u7cfb\u8d77\u6765\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660eJ-Detector\u7684\u6709\u6548\u6027\uff0c\u5176\u53ef\u89e3\u91ca\u6027\u80fd\u591f\u91cf\u5316LLM\u6cd5\u5b98\u7684\u504f\u89c1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5224\u65ad\u68c0\u6d4b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5224\u65ad\u662f\u53ef\u68c0\u6d4b\u7684\uff0cJ-Detector\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u68c0\u6d4b\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\u3002", "topic": "agent analysis"}}
{"id": "2509.23803", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.23803", "abs": "https://arxiv.org/abs/2509.23803", "authors": ["Pramit Saha", "Joshua Strong", "Divyanshu Mishra", "Cheng Ouyang", "J. Alison Noble"], "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents", "comment": null, "summary": "Federated learning (FL) allows collaborative model training across healthcare\nsites without sharing sensitive patient data. However, real-world FL deployment\nis often hindered by complex operational challenges that demand substantial\nhuman efforts. This includes: (a) selecting appropriate clients (hospitals),\n(b) coordinating between the central server and clients, (c) client-level data\npre-processing, (d) harmonizing non-standardized data and labels across\nclients, and (e) selecting FL algorithms based on user instructions and\ncross-client data characteristics. However, the existing FL works overlook\nthese practical orchestration challenges. These operational bottlenecks\nmotivate the need for autonomous, agent-driven FL systems, where intelligent\nagents at each hospital client and the central server agent collaboratively\nmanage FL setup and model training with minimal human intervention. To this\nend, we first introduce an agent-driven FL framework that captures key phases\nof real-world FL workflows from client selection to training completion and a\nbenchmark dubbed FedAgentBench that evaluates the ability of LLM agents to\nautonomously coordinate healthcare FL. Our framework incorporates 40 FL\nalgorithms, each tailored to address diverse task-specific requirements and\ncross-client characteristics. Furthermore, we introduce a diverse set of\ncomplex tasks across 201 carefully curated datasets, simulating 6\nmodality-specific real-world healthcare environments, viz., Dermatoscopy,\nUltrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic\nperformance of 14 open-source and 10 proprietary LLMs spanning small, medium,\nand large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3\ncan automate various stages of the FL pipeline, our results reveal that more\ncomplex, interdependent tasks based on implicit goals remain challenging for\neven the strongest models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4ee3\u7406\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedAgent\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u534f\u8c03\u6311\u6218\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u9009\u62e9\u3001\u6570\u636e\u9884\u5904\u7406\u3001\u7b97\u6cd5\u9009\u62e9\u7b49\uff0c\u5e76\u5efa\u7acb\u4e86FedAgentBench\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u533b\u7597FL\u4e2d\u7684\u81ea\u4e3b\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u9762\u4e34\u590d\u6742\u7684\u64cd\u4f5c\u6311\u6218\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\uff0c\u5305\u62ec\u5ba2\u6237\u7aef\u9009\u62e9\u3001\u534f\u8c03\u3001\u6570\u636e\u9884\u5904\u7406\u3001\u6570\u636e\u6807\u51c6\u5316\u548c\u7b97\u6cd5\u9009\u62e9\u7b49\u3002\u73b0\u6709FL\u5de5\u4f5c\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u5b9e\u9645\u534f\u8c03\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u4e3b\u7684\u3001\u57fa\u4e8e\u4ee3\u7406\u7684FL\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u4ee3\u7406\u9a71\u52a8\u7684FL\u6846\u67b6\uff0c\u6db5\u76d6\u4ece\u5ba2\u6237\u7aef\u9009\u62e9\u5230\u8bad\u7ec3\u5b8c\u6210\u7684\u771f\u5b9eFL\u5de5\u4f5c\u6d41\u7a0b\u5173\u952e\u9636\u6bb5\uff1b\u6784\u5efa\u4e86FedAgentBench\u57fa\u51c6\uff0c\u5305\u542b40\u79cdFL\u7b97\u6cd5\u548c201\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6570\u636e\u96c6\uff0c\u6a21\u62df6\u79cd\u533b\u7597\u73af\u5883\uff1b\u8bc4\u4f30\u4e8614\u4e2a\u5f00\u6e90\u548c10\u4e2a\u4e13\u6709LLM\u7684\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u867d\u7136\u4e00\u4e9b\u4ee3\u7406\u6838\u5fc3\u5982GPT-4.1\u548cDeepSeek V3\u80fd\u591f\u81ea\u52a8\u5316FL\u7ba1\u9053\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u4f46\u57fa\u4e8e\u9690\u542b\u76ee\u6807\u7684\u66f4\u590d\u6742\u3001\u76f8\u4e92\u4f9d\u8d56\u7684\u4efb\u52a1\u5bf9\u5373\u4f7f\u662f\u6700\u5f3a\u5927\u7684\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "conclusion": "\u4ee3\u7406\u9a71\u52a8\u7684FL\u7cfb\u7edf\u6709\u6f5c\u529b\u89e3\u51b3\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u534f\u8c03\u6311\u6218\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "topic": "agent analysis"}}
{"id": "2509.23808", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.23808", "abs": "https://arxiv.org/abs/2509.23808", "authors": ["Fanding Huang", "Guanbo Huang", "Xiao Fan", "Yi He", "Xiao Liang", "Xiao Chen", "Qinting Jiang", "Faisal Nadeem Khan", "Jingyan Jiang", "Zhi Wang"], "title": "Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR", "comment": null, "summary": "A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR)\ninterprets recent progress through the lens of an exploration-exploitation\ntrade-off, a perspective largely shaped by token-level metrics. We re-examine\nthis perspective, proposing that this perceived trade-off may not be a\nfundamental constraint but rather an artifact of the measurement level. To\ninvestigate this, we shift the analysis to the semantically rich hidden-state\nspace, adopting Effective Rank (ER) to quantify exploration and proposing its\nnovel first- and second-order derivatives, named Effective Rank Velocity (ERV)\nand Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our\nanalysis reveals that at the hidden-state level, exploration and exploitation\ncould be decoupled (Sec. 4). This finding reveals an opportunity to enhance\nboth capacities simultaneously. This insight motivates our method,\nVelocity-Exploiting Rank-Learning (VERL), the first to operationalize the\nprinciple of synergistic exploration-exploitation enhancement by directly\nshaping the RL advantage function. The key innovation is leveraging the\ntheoretically stable ERA as a predictive meta-controller to create a\nsynergistic, dual-channel incentive structure. Instead of forcing a trade-off,\nVERL prospectively amplifies rewards for exploration to preempt overconfidence\nand reinforces exploitative gains to consolidate reasoning. Experiments across\ndiverse LLMs and reasoning benchmarks show consistent gains, including up to\n21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86RLVR\u4e2d\u63a2\u7d22-\u5229\u7528\u6743\u8861\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u63d0\u51fa\u8fd9\u79cd\u6743\u8861\u53ef\u80fd\u662f\u6d4b\u91cf\u5c42\u9762\u7684\u5047\u8c61\u3002\u901a\u8fc7\u8f6c\u5411\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5206\u6790\uff0c\u53d1\u73b0\u63a2\u7d22\u548c\u5229\u7528\u53ef\u4ee5\u89e3\u8026\uff0c\u5e76\u5f00\u53d1\u4e86VERL\u65b9\u6cd5\u5b9e\u73b0\u534f\u540c\u589e\u5f3a\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6RLVR\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u8ba4\u4e3a\u8fd9\u79cd\u6743\u8861\u53ef\u80fd\u4e0d\u662f\u57fa\u672c\u7ea6\u675f\u800c\u662f\u6d4b\u91cf\u5c42\u9762\u7684\u4ea7\u7269\uff0c\u65e8\u5728\u627e\u5230\u540c\u65f6\u589e\u5f3a\u63a2\u7d22\u548c\u5229\u7528\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5206\u6790\uff0c\u4f7f\u7528\u6709\u6548\u79e9(ER)\u91cf\u5316\u63a2\u7d22\uff0c\u5e76\u63d0\u51fa\u5176\u5bfc\u6570ERV\u548cERA\u6765\u6355\u6349\u5229\u7528\u52a8\u6001\u3002\u5f00\u53d1\u4e86VERL\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5851\u9020RL\u4f18\u52bf\u51fd\u6570\u5b9e\u73b0\u63a2\u7d22-\u5229\u7528\u7684\u534f\u540c\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVERL\u5728\u591a\u79cdLLM\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684Gaokao 2024\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe21.4%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u5728\u9690\u85cf\u72b6\u6001\u5c42\u9762\uff0c\u63a2\u7d22\u548c\u5229\u7528\u53ef\u4ee5\u89e3\u8026\uff0c\u8fd9\u4e3a\u540c\u65f6\u589e\u5f3a\u4e24\u79cd\u80fd\u529b\u521b\u9020\u4e86\u673a\u4f1a\u3002VERL\u901a\u8fc7\u534f\u540c\u589e\u5f3a\u539f\u5219\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23846", "abs": "https://arxiv.org/abs/2509.23846", "authors": ["Daniele Foffano", "Alessio Russo", "Alexandre Proutiere"], "title": "Adversarial Diffusion for Robust Reinforcement Learning", "comment": null, "summary": "Robustness to modeling errors and uncertainties remains a central challenge\nin reinforcement learning (RL). In this work, we address this challenge by\nleveraging diffusion models to train robust RL policies. Diffusion models have\nrecently gained popularity in model-based RL due to their ability to generate\nfull trajectories \"all at once\", mitigating the compounding errors typical of\nstep-by-step transition models. Moreover, they can be conditioned to sample\nfrom specific distributions, making them highly flexible. We leverage\nconditional sampling to learn policies that are robust to uncertainty in\nenvironment dynamics. Building on the established connection between\nConditional Value at Risk (CVaR) optimization and robust RL, we introduce\nAdversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides\nthe diffusion process to generate worst-case trajectories during training,\neffectively optimizing the CVaR of the cumulative return. Empirical results\nacross standard benchmarks show that AD-RRL achieves superior robustness and\nperformance compared to existing robust RL methods.", "AI": {"tldr": "\u63d0\u51faAD-RRL\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u6761\u4ef6\u91c7\u6837\u751f\u6210\u6700\u574f\u60c5\u51b5\u8f68\u8ff9\u6765\u4f18\u5316\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u6a21\u578b\u8bef\u5dee\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5b8c\u6574\u8f68\u8ff9\u7684\u80fd\u529b\u6765\u7f13\u89e3\u9010\u6b65\u8f6c\u79fb\u6a21\u578b\u7684\u7d2f\u79ef\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u57fa\u4e8eCVaR\u4f18\u5316\u4e0e\u9c81\u68d2RL\u7684\u8054\u7cfb\uff0c\u5f15\u5165AD-RRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\u5728\u8bad\u7ec3\u671f\u95f4\u751f\u6210\u6700\u574f\u60c5\u51b5\u8f68\u8ff9\u6765\u4f18\u5316\u7d2f\u79ef\u56de\u62a5\u7684\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAD-RRL\u76f8\u6bd4\u73b0\u6709\u9c81\u68d2RL\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u9c81\u68d2\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cAD-RRL\u65b9\u6cd5\u901a\u8fc7\u6761\u4ef6\u91c7\u6837\u6709\u6548\u63d0\u5347\u4e86\u7b56\u7565\u5bf9\u73af\u5883\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23866", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.23866", "abs": "https://arxiv.org/abs/2509.23866", "authors": ["Pengxiang Li", "Zechen Hu", "Zirui Shang", "Jingrong Wu", "Yang Liu", "Hui Liu", "Zhi Gao", "Chenrui Shi", "Bofei Zhang", "Zihao Zhang", "Xiaochuan Shi", "Zedong YU", "Yuwei Wu", "Xinxiao Wu", "Yunde Jia", "Liuyu Xiang", "Zhaofeng He", "Qing Li"], "title": "Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation", "comment": null, "summary": "Vision-language model (VLM) based GUI agents show promise for automating\ncomplex desktop and mobile tasks, but face significant challenges in applying\nreinforcement learning (RL): (1) slow multi-turn interactions with GUI\nenvironments for policy rollout, and (2) insufficient high-quality\nagent-environment interactions for policy learning. To address these\nchallenges, we propose DART, a Decoupled Agentic RL Training framework for GUI\nagents, which coordinates heterogeneous modules in a highly decoupled manner.\nDART separates the training system into four asynchronous modules: environment\ncluster, rollout service, data manager, and trainer. This design enables\nnon-blocking communication, asynchronous training, rollout-wise trajectory\nsampling, and per-worker model synchronization, significantly improving the\nsystem efficiency: 1.6*GPU utilization for rollout, 1.9* training throughput,\nand 5.5* environment utilization. To facilitate effective learning from\nabundant samples, we introduce an adaptive data curation scheme: (1)\npre-collecting successful trajectories for challenging tasks to supplement\nsparse success in online sampling; (2) dynamically adjusting rollout numbers\nand trajectory lengths based on task difficulty; (3) training selectively on\nhigh-entropy steps to prioritize critical decisions; (4) stabilizing learning\nvia truncated importance sampling for policy mismatch between policy rollout\nand updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task\nsuccess rate, a 14.61% absolute gain over the base model, and 7.34% higher than\nopen-source SOTA. We will fully open-source our training framework, data, and\nmodel checkpoints via computer-use-agents.github.io/dart-gui, which we believe\nis a timely contribution to the open-source community of agentic RL training.", "AI": {"tldr": "DART\u662f\u4e00\u4e2a\u89e3\u8026\u7684GUI\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u6a21\u5757\u534f\u8c03\u89e3\u51b3VLM\u4ee3\u7406\u5728GUI\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u5728OSWorld\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e8642.13%\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684GUI\u4ee3\u7406\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u4e0eGUI\u73af\u5883\u7684\u591a\u8f6e\u4ea4\u4e92\u901f\u5ea6\u6162\uff0c\u4ee5\u53ca\u9ad8\u8d28\u91cf\u4ee3\u7406-\u73af\u5883\u4ea4\u4e92\u6570\u636e\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDART\u6846\u67b6\uff0c\u5c06\u8bad\u7ec3\u7cfb\u7edf\u5206\u4e3a\u56db\u4e2a\u5f02\u6b65\u6a21\u5757\uff1a\u73af\u5883\u96c6\u7fa4\u3001rollout\u670d\u52a1\u3001\u6570\u636e\u7ba1\u7406\u5668\u548c\u8bad\u7ec3\u5668\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u7406\u65b9\u6848\uff0c\u5305\u62ec\u9884\u6536\u96c6\u6210\u529f\u8f68\u8ff9\u3001\u52a8\u6001\u8c03\u6574rollout\u53c2\u6570\u3001\u9009\u62e9\u6027\u8bad\u7ec3\u9ad8\u71b5\u6b65\u9aa4\u7b49\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDART-GUI-7B\u6a21\u578b\u8fbe\u523042.13%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534714.61%\uff0c\u6bd4\u5f00\u6e90SOTA\u9ad87.34%\u3002\u7cfb\u7edf\u6548\u7387\u663e\u8457\u63d0\u5347\uff1a1.6\u500dGPU\u5229\u7528\u7387\u30011.9\u500d\u8bad\u7ec3\u541e\u5410\u91cf\u30015.5\u500d\u73af\u5883\u5229\u7528\u7387\u3002", "conclusion": "DART\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u6570\u636e\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86GUI\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u7684\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24494", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24494", "abs": "https://arxiv.org/abs/2509.24494", "authors": ["Hongcheng Wang", "Yinuo Huang", "Sukai Wang", "Guanghui Ren", "Hao Dong"], "title": "GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training", "comment": "Under review", "summary": "Recent progress, such as DeepSeek-R1, has shown that the GRPO algorithm, a\nReinforcement Learning (RL) approach, can effectively train Chain-of-Thought\n(CoT) reasoning in Large Language Models (LLMs) and Vision-Language Models\n(VLMs). In this paper, we analyze three challenges of GRPO: gradient coupling\nbetween thoughts and answers, sparse reward signals caused by limited parallel\nsampling, and unstable advantage estimation. To mitigate these challenges, we\npropose GRPO-MA, a simple yet theoretically grounded method that leverages\nmulti-answer generation from each thought process, enabling more robust and\nefficient optimization. Theoretically, we show that the variance of thought\nadvantage decreases as the number of answers per thought increases.\nEmpirically, our gradient analysis confirms this effect, showing that GRPO-MA\nreduces gradient spikes compared to GRPO. Experiments on math, code, and\ndiverse multimodal tasks demonstrate that GRPO-MA substantially improves\nperformance and training efficiency. Our ablation studies further reveal that\nincreasing the number of answers per thought consistently enhances model\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86GRPO-MA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u601d\u7ef4\u8fc7\u7a0b\u751f\u6210\u591a\u4e2a\u7b54\u6848\u6765\u89e3\u51b3GRPO\u7b97\u6cd5\u4e2d\u7684\u68af\u5ea6\u8026\u5408\u3001\u7a00\u758f\u5956\u52b1\u548c\u4e0d\u7a33\u5b9a\u4f18\u52bf\u4f30\u8ba1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3GRPO\u7b97\u6cd5\u5728\u8bad\u7ec3\u601d\u7ef4\u94fe\u63a8\u7406\u65f6\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u601d\u7ef4\u4e0e\u7b54\u6848\u95f4\u7684\u68af\u5ea6\u8026\u5408\u3001\u6709\u9650\u5e76\u884c\u91c7\u6837\u5bfc\u81f4\u7684\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u3001\u4ee5\u53ca\u4e0d\u7a33\u5b9a\u7684\u4f18\u52bf\u4f30\u8ba1\u3002", "method": "GRPO-MA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u601d\u7ef4\u8fc7\u7a0b\u751f\u6210\u591a\u4e2a\u7b54\u6848\uff0c\u51cf\u5c11\u601d\u7ef4\u4f18\u52bf\u7684\u65b9\u5dee\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u4f18\u5316\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u601d\u7ef4\u4f18\u52bf\u7684\u65b9\u5dee\u968f\u6bcf\u4e2a\u601d\u7ef4\u7684\u7b54\u6848\u6570\u91cf\u589e\u52a0\u800c\u964d\u4f4e\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c\u591a\u6837\u5316\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRPO-MA\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002\u68af\u5ea6\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u68af\u5ea6\u5c16\u5cf0\uff0c\u6d88\u878d\u7814\u7a76\u663e\u793a\u589e\u52a0\u6bcf\u4e2a\u601d\u7ef4\u7684\u7b54\u6848\u6570\u91cf\u80fd\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "GRPO-MA\u901a\u8fc7\u591a\u7b54\u6848\u751f\u6210\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86GRPO\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u601d\u7ef4\u94fe\u63a8\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23893", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.23893", "abs": "https://arxiv.org/abs/2509.23893", "authors": ["Zhixin Zhang", "Zeming Wei", "Meng Sun"], "title": "Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings", "comment": null, "summary": "Catastrophic forgetting remains a critical challenge in continual learning\nfor large language models (LLMs), where models struggle to retain performance\non historical tasks when fine-tuning on new sequential data without access to\npast datasets. In this paper, we first reveal that the drift of functional\ndirections during the fine-tuning process is a key reason why existing\nregularization-based methods fail in long-term LLM continual learning. To\naddress this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a\nnovel approach that tracks the drift of these functional directions and\ndynamically updates them during the fine-tuning process. Furthermore, by\nadjusting the gradients of new task parameters to be orthogonal to the tracked\nhistorical function directions, our method mitigates interference between new\nand old tasks. Extensive experiments on various LLM continual learning\nbenchmarks demonstrate that this approach outperforms prior methods,\neffectively reducing catastrophic forgetting and providing a robust tool for\ncontinuous LLM fine-tuning. Our code is available at\nhttps://github.com/meloxxxxxx/DOC.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6b63\u4ea4\u6301\u7eed\u5fae\u8c03(DOC)\u65b9\u6cd5\u6765\u89e3\u51b3LLM\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u8ffd\u8e2a\u529f\u80fd\u65b9\u5411\u7684\u6f02\u79fb\u5e76\u52a8\u6001\u66f4\u65b0\uff0c\u8c03\u6574\u65b0\u4efb\u52a1\u68af\u5ea6\u4e0e\u5386\u53f2\u529f\u80fd\u65b9\u5411\u6b63\u4ea4", "motivation": "\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u957f\u671fLLM\u6301\u7eed\u5b66\u4e60\u4e2d\u5931\u8d25\u7684\u4e3b\u8981\u539f\u56e0\u662f\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u529f\u80fd\u65b9\u5411\u7684\u6f02\u79fb\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u95ee\u9898", "method": "DOC\u65b9\u6cd5\u8ffd\u8e2a\u529f\u80fd\u65b9\u5411\u7684\u6f02\u79fb\u5e76\u52a8\u6001\u66f4\u65b0\uff0c\u901a\u8fc7\u8c03\u6574\u65b0\u4efb\u52a1\u53c2\u6570\u68af\u5ea6\u4e0e\u5386\u53f2\u529f\u80fd\u65b9\u5411\u6b63\u4ea4\u6765\u51cf\u8f7b\u65b0\u65e7\u4efb\u52a1\u95f4\u7684\u5e72\u6270", "result": "\u5728\u591a\u4e2aLLM\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8", "conclusion": "DOC\u65b9\u6cd5\u4e3a\u6301\u7eedLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u9c81\u68d2\u5de5\u5177\uff0c\u901a\u8fc7\u5904\u7406\u529f\u80fd\u65b9\u5411\u6f02\u79fb\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218", "topic": "agentic reinforcement learning"}}
{"id": "2509.24704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24704", "abs": "https://arxiv.org/abs/2509.24704", "authors": ["Guibin Zhang", "Muxin Fu", "Shuicheng Yan"], "title": "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents", "comment": null, "summary": "Agent memory shapes how Large Language Model (LLM)-powered agents, akin to\nthe human brain, progressively refine themselves through environment\ninteractions. Existing paradigms remain constrained: parametric memory forcibly\nadjusts model parameters, and retrieval-based memory externalizes experience\ninto structured databases, yet neither captures the fluid interweaving of\nreasoning and memory that underlies human cognition. To address this gap, we\npropose MemGen, a dynamic generative memory framework that equips agents with a\nhuman-esque cognitive faculty. It consists of a \\textit{memory trigger}, which\nmonitors the agent's reasoning state to decide explicit memory invocation, and\na \\textit{memory weaver}, which takes the agent's current state as stimulus to\nconstruct a latent token sequence as machine-native memory to enrich its\nreasoning. In this way, MemGen enables agents to recall and augment latent\nmemory throughout reasoning, producing a tightly interwoven cycle of memory and\ncognition. Extensive experiments across eight benchmarks show that MemGen\nsurpasses leading external memory systems such as ExpeL and AWM by up to\n$38.22\\%$, exceeds GRPO by up to $13.44\\%$, and exhibits strong cross-domain\ngeneralization ability. More importantly, we find that without explicit\nsupervision, MemGen spontaneously evolves distinct human-like memory faculties,\nincluding planning memory, procedural memory, and working memory, suggesting an\nemergent trajectory toward more naturalistic forms of machine cognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86MemGen\u52a8\u6001\u751f\u6210\u5f0f\u8bb0\u5fc6\u6846\u67b6\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u7c7b\u4eba\u8ba4\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u8bb0\u5fc6\u89e6\u53d1\u5668\u548c\u8bb0\u5fc6\u7f16\u7ec7\u5668\u5b9e\u73b0\u63a8\u7406\u4e0e\u8bb0\u5fc6\u7684\u7d27\u5bc6\u4ea4\u7ec7\uff0c\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u5916\u90e8\u8bb0\u5fc6\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u5316\u8bb0\u5fc6\u548c\u68c0\u7d22\u5f0f\u8bb0\u5fc6\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u63a8\u7406\u4e0e\u8bb0\u5fc6\u7684\u6d41\u7545\u4ea4\u7ec7\uff0c\u9700\u8981\u66f4\u81ea\u7136\u7684\u673a\u5668\u8ba4\u77e5\u5f62\u5f0f\u3002", "method": "MemGen\u5305\u542b\u8bb0\u5fc6\u89e6\u53d1\u5668\uff08\u76d1\u63a7\u63a8\u7406\u72b6\u6001\u51b3\u5b9a\u663e\u5f0f\u8bb0\u5fc6\u8c03\u7528\uff09\u548c\u8bb0\u5fc6\u7f16\u7ec7\u5668\uff08\u4ee5\u5f53\u524d\u72b6\u6001\u4e3a\u523a\u6fc0\u6784\u5efa\u6f5c\u5728token\u5e8f\u5217\u4f5c\u4e3a\u673a\u5668\u539f\u751f\u8bb0\u5fc6\uff09\uff0c\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8bb0\u5fc6\u7684\u56de\u5fc6\u548c\u589e\u5f3a\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMemGen\u6bd4ExpeL\u548cAWM\u7b49\u9886\u5148\u5916\u90e8\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u5347\u9ad8\u8fbe38.22%\uff0c\u8d85\u8fc7GRPO\u8fbe13.44%\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MemGen\u5728\u6ca1\u6709\u663e\u5f0f\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u81ea\u53d1\u6f14\u5316\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bb0\u5fc6\u529f\u80fd\uff0c\u5305\u62ec\u89c4\u5212\u8bb0\u5fc6\u3001\u7a0b\u5e8f\u8bb0\u5fc6\u548c\u5de5\u4f5c\u8bb0\u5fc6\uff0c\u8868\u660e\u673a\u5668\u8ba4\u77e5\u5411\u66f4\u81ea\u7136\u5f62\u5f0f\u53d1\u5c55\u7684\u65b0\u5174\u8f68\u8ff9\u3002", "topic": "agent analysis"}}
{"id": "2509.23946", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.23946", "abs": "https://arxiv.org/abs/2509.23946", "authors": ["Kaisen Yang", "Lixuan He", "Rushi Shah", "Kaicheng Yang", "Qinwei Ma", "Dianbo Liu", "Alex Lamb"], "title": "Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm", "comment": "Under review ICLR 2026", "summary": "Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning\nabilities of Large Language Models (LLMs), yet their monolithic and\nauto-regressive architecture inherently conflates high-level strategic planning\nwith low-level step-by-step execution, leading to computational inefficiency,\nlimited exploration of reasoning paths, and reduced interpretability. To\novercome these issues, we propose the Explore-Execute Chain ($E^2C$), a\nstructured reasoning framework that decouples reasoning into two distinct\nphases: an exploratory phase that stochastically generates succinct high-level\nplans, followed by an execution phase that deterministically carries out the\nchosen plan. Our approach incorporates a two-stage training methodology, which\ncombines Supervised Fine-Tuning (SFT) - augmented by a novel data generation\nalgorithm enforcing strict plan adherence - with a subsequent Reinforcement\nLearning (RL) stage that capitalizes on the informativeness of exploration and\nreinforces the determinism of execution.This decomposition enables an efficient\ntest-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches\n58.1% accuracy using <10% of the decoding tokens required by comparable methods\n(e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For\ncross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with\nonly 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher\naccuracy than standard SFT on medical benchmarks, delivering state-of-the-art\nperformance, strong generalization, and greater interpretability by separating\nplanning from execution. The code and pre-trained models for the project are\navailable at: https://github.com/yks23/Explore-Execute-Chain.git", "AI": {"tldr": "\u63d0\u51faExplore-Execute Chain (E\u00b2C)\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u63a2\u7d22\u9636\u6bb5\uff08\u751f\u6210\u9ad8\u5c42\u8ba1\u5212\uff09\u548c\u6267\u884c\u9636\u6bb5\uff08\u6267\u884c\u8ba1\u5212\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u63d0\u9ad8\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfCoT\u65b9\u6cd5\u5c06\u9ad8\u5c42\u7b56\u7565\u89c4\u5212\u4e0e\u4f4e\u5c42\u6267\u884c\u6b65\u9aa4\u6df7\u5728\u4e00\u8d77\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u6709\u9650\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u91c7\u7528\u63a2\u7d22-\u6267\u884c\u5206\u79bb\u7684\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u542b\u4e25\u683c\u9075\u5faa\u8ba1\u5212\u7684\u6570\u636e\u751f\u6210\u7b97\u6cd5\u3002", "result": "\u5728AIME'2024\u4e0a\u8fbe\u523058.1%\u51c6\u786e\u7387\uff0c\u4ec5\u9700\u4e0d\u523010%\u7684\u89e3\u7801token\uff1b\u5728\u533b\u7597\u57fa\u51c6\u4e0a\u6bd4\u6807\u51c6SFT\u51c6\u786e\u7387\u63d0\u9ad814.5%\uff0c\u4ec5\u75283.5%\u7684token\u3002", "conclusion": "E\u00b2C\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\u3001\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "topic": "agent analysis"}}
{"id": "2509.24771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24771", "abs": "https://arxiv.org/abs/2509.24771", "authors": ["Guibin Zhang", "Fanci Meng", "Guancheng Wan", "Zherui Li", "Kun Wang", "Zhenfei Yin", "Lei Bai", "Shuicheng Yan"], "title": "LatentEvolve: Self-Evolving Test-Time Scaling in Latent Space", "comment": null, "summary": "Test-time Scaling (TTS) has been demonstrated to significantly enhance the\nreasoning capabilities of Large Language Models (LLMs) during the inference\nphase without altering model parameters. However, existing TTS methods are\nlargely independent, implying that LLMs have not yet evolved to progressively\nlearn how to scale more effectively. With the objective of evolving LLMs to\nlearn ``how to scale test-time computation,'' we propose LatentEvolve, a\nself-evolving latent TTS framework inspired by the complementary learning\nsystem (CLS) theory. Analogous to the human brain's dual system of a\nfast-recall hippocampus and a slow-consolidating neocortex, LatentEvolve\ncomprises two evolutionary components: \\textit{daytime scaling}, which rapidly\nretrieves historical latent representations to better guide current LLM\nreasoning; and \\textit{nighttime scaling}, which integrates past latent\noptimizations in a manner akin to the human brain's consolidation of\nexperiences during sleep. The alternation of daytime and nighttime processes\nfacilitates a fast and slow evolution of LLM TTS, mirroring human cognitive\ndynamics in a fully unsupervised manner. Extensive experiments across eight\nbenchmarks and five model backbones demonstrate that our LatentEvolve surpasses\nstate-of-the-art TTS methods such as LatentSeek and TTRL by up to $13.33\\%$ and\nexhibits exceptional cross-domain and cross-backbone generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86LatentEvolve\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\u7684\u663c\u591c\u4ea4\u66ff\u8fc7\u7a0b\uff0c\u8ba9LLM\u5728\u63a8\u7406\u9636\u6bb5\u81ea\u6211\u8fdb\u5316\u5b66\u4e60\u5982\u4f55\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u80fd\u529b", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u76f8\u4e92\u72ec\u7acb\uff0cLLM\u5c1a\u672a\u5b66\u4f1a\u5982\u4f55\u6e10\u8fdb\u5f0f\u5730\u66f4\u6709\u6548\u5730\u6269\u5c55\u8ba1\u7b97\u3002\u76ee\u6807\u662f\u8ba9LLM\u5b66\u4e60\"\u5982\u4f55\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\"", "method": "\u57fa\u4e8e\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\u7406\u8bba\u7684\u53cc\u7ec4\u4ef6\u6846\u67b6\uff1a\u767d\u5929\u6269\u5c55\uff08\u5feb\u901f\u68c0\u7d22\u5386\u53f2\u6f5c\u5728\u8868\u793a\u6307\u5bfc\u5f53\u524d\u63a8\u7406\uff09\u548c\u591c\u95f4\u6269\u5c55\uff08\u6574\u5408\u8fc7\u53bb\u7684\u6f5c\u5728\u4f18\u5316\uff0c\u7c7b\u4f3c\u4eba\u8111\u7761\u7720\u65f6\u7684\u7ecf\u9a8c\u5de9\u56fa\uff09", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c5\u4e2a\u6a21\u578b\u9aa8\u5e72\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLatentEvolve\u6bd4\u6700\u5148\u8fdb\u7684TTS\u65b9\u6cd5\uff08\u5982LatentSeek\u548cTTRL\uff09\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe13.33%\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u8de8\u57df\u548c\u8de8\u9aa8\u5e72\u6cdb\u5316\u80fd\u529b", "conclusion": "LatentEvolve\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u52a8\u6001\u7684\u663c\u591c\u4ea4\u66ff\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86LLM\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u5feb\u901f\u548c\u6162\u901f\u8fdb\u5316\uff0c\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b", "topic": "agent analysis"}}
{"id": "2509.23976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.23976", "abs": "https://arxiv.org/abs/2509.23976", "authors": ["Maruf Ahmed Mridul", "Oshani Seneviratne"], "title": "Curriculum-Guided Reinforcement Learning for Synthesizing Gas-Efficient Financial Derivatives Contracts", "comment": "8 pages, 3 figures, 2 tables", "summary": "Smart contract-based automation of financial derivatives offers substantial\nefficiency gains, but its real-world adoption is constrained by the complexity\nof translating financial specifications into gas-efficient executable code. In\nparticular, generating code that is both functionally correct and economically\nviable from high-level specifications, such as the Common Domain Model (CDM),\nremains a significant challenge. This paper introduces a Reinforcement Learning\n(RL) framework to generate functional and gas-optimized Solidity smart\ncontracts directly from CDM specifications. We employ a Proximal Policy\nOptimization (PPO) agent that learns to select optimal code snippets from a\npre-defined library. To manage the complex search space, a two-phase curriculum\nfirst trains the agent for functional correctness before shifting its focus to\ngas optimization. Our empirical results show the RL agent learns to generate\ncontracts with significant gas savings, achieving cost reductions of up to\n35.59% on unseen test data compared to unoptimized baselines. This work\npresents a viable methodology for the automated synthesis of reliable and\neconomically sustainable smart contracts, bridging the gap between high-level\nfinancial agreements and efficient on-chain execution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4eceCDM\u89c4\u8303\u751f\u6210\u529f\u80fd\u6b63\u786e\u4e14gas\u4f18\u5316\u7684Solidity\u667a\u80fd\u5408\u7ea6\uff0c\u5b9e\u73b0\u9ad8\u8fbe35.59%\u7684gas\u6210\u672c\u8282\u7701\u3002", "motivation": "\u91d1\u878d\u884d\u751f\u54c1\u667a\u80fd\u5408\u7ea6\u81ea\u52a8\u5316\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u5c06\u9ad8\u7ea7\u91d1\u878d\u89c4\u8303\u8f6c\u6362\u4e3a\u65e2\u529f\u80fd\u6b63\u786e\u53c8\u7ecf\u6d4e\u53ef\u884c\u7684\u4ee3\u7801\uff0c\u7279\u522b\u662f\u4eceCDM\u89c4\u8303\u751f\u6210gas\u4f18\u5316\u7684\u53ef\u6267\u884c\u4ee3\u7801\u3002", "method": "\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4ece\u9884\u5b9a\u4e49\u5e93\u4e2d\u9009\u62e9\u6700\u4f18\u4ee3\u7801\u7247\u6bb5\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u5148\u8bad\u7ec3\u529f\u80fd\u6b63\u786e\u6027\uff0c\u518d\u8f6c\u5411gas\u4f18\u5316\u3002", "result": "RL\u4ee3\u7406\u5b66\u4f1a\u751f\u6210gas\u663e\u8457\u8282\u7701\u7684\u5408\u7ea6\uff0c\u5728\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u4e0a\u76f8\u6bd4\u672a\u4f18\u5316\u57fa\u7ebf\u5b9e\u73b0\u9ad8\u8fbe35.59%\u7684\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u53ef\u9760\u4e14\u7ecf\u6d4e\u53ef\u6301\u7eed\u7684\u667a\u80fd\u5408\u7ea6\u81ea\u52a8\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u9ad8\u7ea7\u91d1\u878d\u534f\u8bae\u4e0e\u9ad8\u6548\u94fe\u4e0a\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.23992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23992", "abs": "https://arxiv.org/abs/2509.23992", "authors": ["Amartya Roy", "Devharish N", "Shreya Ganguly", "Kripabandhu Ghosh"], "title": "Guide: Generalized-Prior and Data Encoders for DAG Estimation", "comment": null, "summary": "Modern causal discovery methods face critical limitations in scalability,\ncomputational efficiency, and adaptability to mixed data types, as evidenced by\nbenchmarks on node scalability (30, $\\le 50$, $\\ge 70$ nodes), computational\nenergy demands, and continuous/non-continuous data handling. While traditional\nalgorithms like PC, GES, and ICA-LiNGAM struggle with these challenges,\nexhibiting prohibitive energy costs for higher-order nodes and poor scalability\nbeyond 70 nodes, we propose \\textbf{GUIDE}, a framework that integrates Large\nLanguage Model (LLM)-generated adjacency matrices with observational data\nthrough a dual-encoder architecture. GUIDE uniquely optimizes computational\nefficiency, reducing runtime on average by $\\approx 42%$ compared to RL-BIC and\nKCRL methods, while achieving an average $\\approx 117%$ improvement in accuracy\nover both NOTEARS and GraN-DAG individually. During training, GUIDE's\nreinforcement learning agent dynamically balances reward maximization\n(accuracy) and penalty avoidance (DAG constraints), enabling robust performance\nacross mixed data types and scalability to $\\ge 70$ nodes -- a setting where\nbaseline methods fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86GUIDE\u6846\u67b6\uff0c\u5c06LLM\u751f\u6210\u7684\u90bb\u63a5\u77e9\u9635\u4e0e\u89c2\u6d4b\u6570\u636e\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u67b6\u6784\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6df7\u5408\u6570\u636e\u7c7b\u578b\u9002\u5e94\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u4ee3\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5bf9\u6df7\u5408\u6570\u636e\u7c7b\u578b\u9002\u5e94\u6027\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\uff0c\u4f20\u7edf\u7b97\u6cd5\u5982PC\u3001GES\u548cICA-LiNGAM\u5728\u5904\u7406\u9ad8\u9636\u8282\u70b9\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u65e0\u6cd5\u6269\u5c55\u523070\u4e2a\u8282\u70b9\u4ee5\u4e0a\u3002", "method": "GUIDE\u6846\u67b6\u96c6\u6210LLM\u751f\u6210\u7684\u90bb\u63a5\u77e9\u9635\u4e0e\u89c2\u6d4b\u6570\u636e\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u52a8\u6001\u5e73\u8861\u5956\u52b1\u6700\u5927\u5316\uff08\u51c6\u786e\u6027\uff09\u548c\u60e9\u7f5a\u907f\u514d\uff08DAG\u7ea6\u675f\uff09\u3002", "result": "GUIDE\u663e\u8457\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff0c\u76f8\u6bd4RL-BIC\u548cKCRL\u65b9\u6cd5\u5e73\u5747\u51cf\u5c11\u7ea642%\u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u76f8\u6bd4NOTEARS\u548cGraN-DAG\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u7ea6117%\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u6269\u5c55\u523070\u4e2a\u8282\u70b9\u4ee5\u4e0a\u3002", "conclusion": "GUIDE\u5728\u6df7\u5408\u6570\u636e\u7c7b\u578b\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u5728\u57fa\u7ebf\u65b9\u6cd5\u5931\u8d25\u7684\u8bbe\u7f6e\u4e2d\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24841", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.24841", "abs": "https://arxiv.org/abs/2509.24841", "authors": ["Zhilong Zhao", "Yindi Liu"], "title": "Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement", "comment": "10 pages, 4 figures, 4 tables", "summary": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5c42\u9519\u8bef\u4fee\u6b63\uff08HEC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u9519\u8bef\u5206\u6790\u548c\u9488\u5bf9\u6027\u5e72\u9884\u7b56\u7565\u89e3\u51b3\u4e13\u4e1a\u9886\u57dfAI\u6027\u80fd\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534711.2\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u5728\u533b\u7597\u7f16\u7801\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4ec5\u4e3a45.9%\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u6027\u80fd\u3002", "method": "\u5206\u6790\u56db\u4e2a\u4e13\u4e1a\u9886\u57df\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u53d1\u73b0\u9519\u8bef\u9075\u5faa\u5206\u5c42\u7ed3\u6784\uff1a\u77e5\u8bc6\u5c42\u9519\u8bef\uff0858.4%\uff09\u3001\u63a8\u7406\u5c42\u9519\u8bef\uff0839.6%\uff09\u548c\u590d\u6742\u5ea6\u5c42\u9519\u8bef\uff082.0%\uff09\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e09\u9636\u6bb5\u4fee\u6b63\u6846\u67b6\uff0c\u6309\u9519\u8bef\u5c42\u6b21\u91cd\u8981\u6027\u8fdb\u884c\u9488\u5bf9\u6027\u5e72\u9884\u3002", "result": "\u5728\u533b\u7597\u8f6c\u5f55\u3001\u6cd5\u5f8b\u6587\u6863\u5206\u7c7b\u3001\u653f\u6cbb\u504f\u89c1\u68c0\u6d4b\u548c\u6cd5\u5f8b\u63a8\u7406\u56db\u4e2a\u9886\u57df\u9a8c\u8bc1\uff0c\u5e73\u5747\u63d0\u534711.2\u4e2a\u767e\u5206\u70b9\uff08p < 0.001\uff09\u3002\u4f46\u5728\u9ad8\u57fa\u7ebf\u4efb\u52a1\uff08>75%\u51c6\u786e\u7387\uff09\u4e2d\u6846\u67b6\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u7cfb\u7edf\u9519\u8bef\u5206\u6790\u53ef\u4ee5\u6307\u5bfc\u4e13\u4e1a\u9886\u57dfAI\u589e\u5f3a\u7b56\u7565\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e2d\u7b49\u57fa\u7ebf\u4efb\u52a1\uff0c\u540c\u65f6\u9700\u8981\u7406\u89e3\u6846\u67b6\u8fb9\u754c\u4ee5\u5b9e\u73b0\u6700\u4f18\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2509.24047", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24047", "abs": "https://arxiv.org/abs/2509.24047", "authors": ["Runyu Zhang", "Na Li", "Asuman Ozdaglar", "Jeff Shamma", "Gioele Zardini"], "title": "Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning", "comment": null, "summary": "Risk sensitivity has become a central theme in reinforcement learning (RL),\nwhere convex risk measures and robust formulations provide principled ways to\nmodel preferences beyond expected return. Recent extensions to multi-agent RL\n(MARL) have largely emphasized the risk-averse setting, prioritizing robustness\nto uncertainty. In cooperative MARL, however, such conservatism often leads to\nsuboptimal equilibria, and a parallel line of work has shown that optimism can\npromote cooperation. Existing optimistic methods, though effective in practice,\nare typically heuristic and lack theoretical grounding. Building on the dual\nrepresentation for convex risk measures, we propose a principled framework that\ninterprets risk-seeking objectives as optimism. We introduce optimistic value\nfunctions, which formalize optimism as divergence-penalized risk-seeking\nevaluations. Building on this foundation, we derive a policy-gradient theorem\nfor optimistic value functions, including explicit formulas for the entropic\nrisk/KL-penalty setting, and develop decentralized optimistic actor-critic\nalgorithms that implement these updates. Empirical results on cooperative\nbenchmarks demonstrate that risk-seeking optimism consistently improves\ncoordination over both risk-neutral baselines and heuristic optimistic methods.\nOur framework thus unifies risk-sensitive learning and optimism, offering a\ntheoretically grounded and practically effective approach to cooperation in\nMARL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51f8\u98ce\u9669\u5ea6\u91cf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u98ce\u9669\u5bfb\u6c42\u76ee\u6807\u89e3\u91ca\u4e3a\u4e50\u89c2\u4e3b\u4e49\uff0c\u5e76\u5f00\u53d1\u4e86\u5206\u6563\u5f0f\u4e50\u89c2\u884c\u52a8\u8005-\u6279\u8bc4\u8005\u7b97\u6cd5\u6765\u6539\u5584\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5408\u4f5c\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u6b21\u4f18\u5747\u8861\uff0c\u800c\u73b0\u6709\u7684\u4e50\u89c2\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u3002\u672c\u6587\u65e8\u5728\u4e3a\u4e50\u89c2\u4e3b\u4e49\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u4ee5\u4fc3\u8fdb\u5408\u4f5c\u3002", "method": "\u57fa\u4e8e\u51f8\u98ce\u9669\u5ea6\u91cf\u7684\u5bf9\u5076\u8868\u793a\uff0c\u63d0\u51fa\u4e50\u89c2\u4ef7\u503c\u51fd\u6570\u6846\u67b6\uff0c\u5c06\u4e50\u89c2\u4e3b\u4e49\u5f62\u5f0f\u5316\u4e3a\u53d1\u6563\u60e9\u7f5a\u7684\u98ce\u9669\u5bfb\u6c42\u8bc4\u4f30\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e50\u89c2\u4ef7\u503c\u51fd\u6570\u7684\u7b56\u7565\u68af\u5ea6\u5b9a\u7406\u3002", "result": "\u5728\u5408\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u98ce\u9669\u5bfb\u6c42\u4e50\u89c2\u4e3b\u4e49\u76f8\u6bd4\u98ce\u9669\u4e2d\u6027\u57fa\u7ebf\u548c\u542f\u53d1\u5f0f\u4e50\u89c2\u65b9\u6cd5\uff0c\u6301\u7eed\u6539\u5584\u4e86\u534f\u8c03\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u98ce\u9669\u654f\u611f\u5b66\u4e60\u548c\u4e50\u89c2\u4e3b\u4e49\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5408\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6709\u6548\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24050", "abs": "https://arxiv.org/abs/2509.24050", "authors": ["Wenzhi Fang", "Dong-Jun Han", "Liangqi Yuan", "Christopher Brinton"], "title": "Collaborative Device-Cloud LLM Inference through Reinforcement Learning", "comment": "We propose a unified post-training framework that integrates routing\n  optimization, enabling the on-device LLM to improve its problem-solving\n  ability while learning routing strategies", "summary": "Device-cloud collaboration has emerged as a promising paradigm for deploying\nlarge language models (LLMs), combining the efficiency of lightweight on-device\ninference with the superior performance of powerful cloud LLMs. An essential\nproblem in this scenario lies in deciding whether a given query is best handled\nlocally or delegated to the cloud. Existing approaches typically rely on\nexternal routers, implemented as binary classifiers, which often struggle to\ndetermine task difficulty from the prompt's surface pattern. To address these\nlimitations, we propose a framework where the on-device LLM makes routing\ndecisions at the end of its solving process, with this capability instilled\nthrough post-training. In particular, we formulate a reward maximization\nproblem with carefully designed rewards that encourage effective problem\nsolving and judicious offloading to the cloud. To solve this problem, we\ndevelop a group-adaptive policy gradient algorithm, featuring a group-level\npolicy gradient, designed to yield an unbiased gradient estimator of the\nreward, and adaptive prompt filtering, developed to enforce the constraint on\ncloud LLM usage. Extensive experiments across models and benchmarks show that\nthe proposed methodology consistently outperforms existing baselines and\nsignificantly narrows the gap to full cloud LLM performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u5907-\u4e91\u534f\u4f5c\u6846\u67b6\uff0c\u8ba9\u8bbe\u5907\u7aefLLM\u5728\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u51b3\u5b9a\u662f\u5426\u5c06\u67e5\u8be2\u5378\u8f7d\u5230\u4e91\u7aef\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u5b9e\u73b0\u8def\u7531\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bbe\u5907-\u4e91\u534f\u4f5c\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8def\u7531\u5668\u4f5c\u4e3a\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u96be\u4ee5\u4ece\u63d0\u793a\u8868\u9762\u6a21\u5f0f\u51c6\u786e\u5224\u65ad\u4efb\u52a1\u96be\u5ea6\uff0c\u5bfc\u81f4\u8def\u7531\u51b3\u7b56\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u5956\u52b1\u6700\u5927\u5316\u95ee\u9898\u6846\u67b6\uff0c\u8bbe\u8ba1\u9f13\u52b1\u6709\u6548\u95ee\u9898\u89e3\u51b3\u548c\u660e\u667a\u4e91\u7aef\u5378\u8f7d\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5f00\u53d1\u7fa4\u7ec4\u81ea\u9002\u5e94\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u5305\u542b\u7fa4\u7ec4\u7ea7\u7b56\u7565\u68af\u5ea6\u548c\u81ea\u9002\u5e94\u63d0\u793a\u8fc7\u6ee4\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5b8c\u5168\u4e91\u7aefLLM\u6027\u80fd\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u8bbe\u5907-\u4e91\u534f\u4f5c\u7cfb\u7edf\u4e2d\u8def\u7531\u51b3\u7b56\u7684\u8d28\u91cf\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u5e73\u8861\u3002", "topic": "agent analysis"}}
{"id": "2509.24067", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24067", "abs": "https://arxiv.org/abs/2509.24067", "authors": ["Qiushui Xu", "Yuhao Huang", "Yushu Jiang", "Lei Song", "Jinyu Wang", "Wenliang Zheng", "Jiang Bian"], "title": "In-Context Compositional Q-Learning for Offline Reinforcement Learning", "comment": null, "summary": "Accurately estimating the Q-function is a central challenge in offline\nreinforcement learning. However, existing approaches often rely on a single\nglobal Q-function, which struggles to capture the compositional nature of tasks\ninvolving diverse subtasks. We propose In-context Compositional Q-Learning\n(\\texttt{ICQL}), the first offline RL framework that formulates Q-learning as a\ncontextual inference problem, using linear Transformers to adaptively infer\nlocal Q-functions from retrieved transitions without explicit subtask labels.\nTheoretically, we show that under two assumptions--linear approximability of\nthe local Q-function and accurate weight inference from retrieved\ncontext--\\texttt{ICQL} achieves bounded Q-function approximation error, and\nsupports near-optimal policy extraction. Empirically, \\texttt{ICQL}\nsubstantially improves performance in offline settings: improving performance\nin kitchen tasks by up to 16.4\\%, and in Gym and Adroit tasks by up to 8.6\\%\nand 6.3\\%. These results highlight the underexplored potential of in-context\nlearning for robust and compositional value estimation, positioning\n\\texttt{ICQL} as a principled and effective framework for offline RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86ICQL\uff0c\u9996\u4e2a\u5c06Q\u5b66\u4e60\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u63a8\u7406\u95ee\u9898\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u7ebf\u6027Transformer\u4ece\u68c0\u7d22\u5230\u7684\u8f6c\u79fb\u4e2d\u81ea\u9002\u5e94\u63a8\u65ad\u5c40\u90e8Q\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u5b50\u4efb\u52a1\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u7684\u5168\u5c40Q\u51fd\u6570\uff0c\u96be\u4ee5\u6355\u6349\u6d89\u53ca\u591a\u6837\u5316\u5b50\u4efb\u52a1\u7684\u7ec4\u5408\u6027\u8d28\u3002", "method": "\u5c06Q\u5b66\u4e60\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u63a8\u7406\u95ee\u9898\uff0c\u4f7f\u7528\u7ebf\u6027Transformer\u4ece\u68c0\u7d22\u5230\u7684\u8f6c\u79fb\u4e2d\u81ea\u9002\u5e94\u63a8\u65ad\u5c40\u90e8Q\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u5b50\u4efb\u52a1\u6807\u7b7e\u3002", "result": "\u5728\u53a8\u623f\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe16.4%\uff0c\u5728Gym\u548cAdroit\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u53478.6%\u548c6.3%\u3002", "conclusion": "\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u9c81\u68d2\u548c\u7ec4\u5408\u4ef7\u503c\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0cICQL\u6210\u4e3a\u79bb\u7ebfRL\u7684\u539f\u5219\u6027\u548c\u6709\u6548\u6846\u67b6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24958", "abs": "https://arxiv.org/abs/2509.24958", "authors": ["Linlu Gong", "Ante Wang", "Yunghwei Lai", "Weizhi Ma", "Yang Liu"], "title": "The Dialogue That Heals: A Comprehensive Evaluation of Doctor Agents' Inquiry Capability", "comment": null, "summary": "An effective physician should possess a combination of empathy, expertise,\npatience, and clear communication when treating a patient. Recent advances have\nsuccessfully endowed AI doctors with expert diagnostic skills, particularly the\nability to actively seek information through inquiry. However, other essential\nqualities of a good doctor remain overlooked. To bridge this gap, we present\nMAQuE(Medical Agent Questioning Evaluation), the largest-ever benchmark for the\nautomatic and comprehensive evaluation of medical multi-turn questioning. It\nfeatures 3,000 realistically simulated patient agents that exhibit diverse\nlinguistic patterns, cognitive limitations, emotional responses, and tendencies\nfor passive disclosure. We also introduce a multi-faceted evaluation framework,\ncovering task success, inquiry proficiency, dialogue competence, inquiry\nefficiency, and patient experience. Experiments on different LLMs reveal\nsubstantial challenges across the evaluation aspects. Even state-of-the-art\nmodels show significant room for improvement in their inquiry capabilities.\nThese models are highly sensitive to variations in realistic patient behavior,\nwhich considerably impacts diagnostic accuracy. Furthermore, our fine-grained\nmetrics expose trade-offs between different evaluation perspectives,\nhighlighting the challenge of balancing performance and practicality in\nreal-world clinical settings.", "AI": {"tldr": "MAQuE\u662f\u6700\u5927\u7684\u533b\u7597\u591a\u8f6e\u95ee\u8bca\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b3000\u4e2a\u6a21\u62df\u60a3\u8005\u4ee3\u7406\uff0c\u8bc4\u4f30\u6846\u67b6\u6db5\u76d6\u4efb\u52a1\u6210\u529f\u3001\u95ee\u8bca\u80fd\u529b\u3001\u5bf9\u8bdd\u80fd\u529b\u3001\u95ee\u8bca\u6548\u7387\u548c\u60a3\u8005\u4f53\u9a8c\u4e94\u4e2a\u7ef4\u5ea6\u3002\u5b9e\u9a8c\u663e\u793a\u73b0\u6709LLM\u5728\u95ee\u8bca\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709AI\u533b\u751f\u4e3b\u8981\u5173\u6ce8\u8bca\u65ad\u6280\u80fd\uff0c\u4f46\u5ffd\u89c6\u4e86\u4f18\u79c0\u533b\u751f\u5e94\u5177\u5907\u7684\u5171\u60c5\u3001\u8010\u5fc3\u548c\u6e05\u6670\u6c9f\u901a\u7b49\u54c1\u8d28\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u5305\u542b3000\u4e2a\u6a21\u62df\u60a3\u8005\u4ee3\u7406\u7684\u57fa\u51c6\uff0c\u8fd9\u4e9b\u4ee3\u7406\u5177\u6709\u591a\u6837\u8bed\u8a00\u6a21\u5f0f\u3001\u8ba4\u77e5\u9650\u5236\u3001\u60c5\u7eea\u53cd\u5e94\u548c\u88ab\u52a8\u62ab\u9732\u503e\u5411\uff0c\u5e76\u8bbe\u8ba1\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u4e0d\u540cLLM\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5728\u95ee\u8bca\u80fd\u529b\u65b9\u9762\u4e5f\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e14\u5bf9\u60a3\u8005\u884c\u4e3a\u53d8\u5316\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u533b\u7597AI\u9700\u8981\u5e73\u8861\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u5f53\u524d\u6a21\u578b\u5728\u73b0\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u80fd\u529b\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2509.24988", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24988", "abs": "https://arxiv.org/abs/2509.24988", "authors": ["Hanqi Xiao", "Vaidehi Patil", "Hyunji Lee", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns", "comment": "Code:\n  https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness", "summary": "Generating accurate and calibrated confidence estimates is critical for\ndeploying LLMs in high-stakes or user-facing applications, and remains an open\nchallenge. Prior research has often framed confidence as a problem of eliciting\na model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its\nown answers are correct; this approach implicitly assumes that there is some\nprivileged information about the answer's correctness that is accessible to the\nmodel itself. However, our experiments reveal that an LLM attempting to predict\nthe correctness of its own outputs generally performs no better than an\nunrelated LLM. Moreover, we hypothesize that a key factor in building a\n\"Correctness Model\" (CM) is exposure to a target model's historical\npredictions. We propose multiple methods to inject this historical correctness\ninformation, creating a Generalized Correctness Model (GCM). We first show that\nGCMs can be trained on the correctness data from many LLMs and learn patterns\nfor correctness prediction applicable across datasets and models. We then use\nCMs as a lens for studying the source of correctness prediction ability and its\ngeneralization, systematically controlling their training data and finding that\nanswer phrasing is a strong predictor for correctness. We further explore\nalternative methods of injecting history without training an LLM, finding that\nincluding history as in-context examples can help improve correctness\nprediction, and post-hoc calibration can provide complementary reductions in\ncalibration error. We evaluate GCMs based on Qwen3-8B across 5 model families\nand the MMLU and TriviaQA datasets, as well as on a downstream selective\nprediction task, finding that reliable LLM confidence estimation is a\ngeneralizable and model-agnostic skill learned by systematically encoding\ncorrectness history rather than a model-specific skill reliant on\nself-introspection.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u578b\u9884\u6d4b\u81ea\u8eab\u8f93\u51fa\u6b63\u786e\u6027\u7684\u80fd\u529b\u5e76\u4e0d\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u63d0\u51fa\u901a\u8fc7\u6ce8\u5165\u5386\u53f2\u6b63\u786e\u6027\u4fe1\u606f\u6765\u6784\u5efa\u5e7f\u4e49\u6b63\u786e\u6027\u6a21\u578b(GCM)\uff0c\u8bc1\u660e\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u662f\u53ef\u6cdb\u5316\u3001\u6a21\u578b\u65e0\u5173\u7684\u6280\u80fd\u3002", "motivation": "\u5728\u5173\u952e\u5e94\u7528\u573a\u666f\u4e2d\u90e8\u7f72LLM\u9700\u8981\u51c6\u786e\u4e14\u6821\u51c6\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6a21\u578b\u5177\u6709\u5224\u65ad\u81ea\u8eab\u7b54\u6848\u6b63\u786e\u6027\u7684\u7279\u6743\u4fe1\u606f\uff0c\u8fd9\u4e00\u5047\u8bbe\u53ef\u80fd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5e7f\u4e49\u6b63\u786e\u6027\u6a21\u578b(GCM)\uff0c\u901a\u8fc7\u6ce8\u5165\u76ee\u6807\u6a21\u578b\u7684\u5386\u53f2\u9884\u6d4b\u4fe1\u606f\uff0c\u5305\u62ec\u8bad\u7ec3\u57fa\u4e8e\u591aLLM\u6b63\u786e\u6027\u6570\u636e\u7684\u6a21\u578b\u3001\u4f7f\u7528\u5386\u53f2\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u793a\u4f8b\u3001\u4ee5\u53ca\u540e\u5904\u7406\u6821\u51c6\u7b49\u65b9\u6cd5\u3002", "result": "GCM\u57285\u4e2a\u6a21\u578b\u5bb6\u65cf\u548cMMLU\u3001TriviaQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5728\u9009\u62e9\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u8bc1\u660e\u6b63\u786e\u6027\u9884\u6d4b\u80fd\u529b\u4e3b\u8981\u6765\u81ea\u5386\u53f2\u6b63\u786e\u6027\u6a21\u5f0f\u800c\u975e\u6a21\u578b\u81ea\u7701\u3002", "conclusion": "\u53ef\u9760\u7684LLM\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u662f\u901a\u8fc7\u7cfb\u7edf\u7f16\u7801\u6b63\u786e\u6027\u5386\u53f2\u5b66\u4e60\u7684\u53ef\u6cdb\u5316\u3001\u6a21\u578b\u65e0\u5173\u6280\u80fd\uff0c\u800c\u975e\u4f9d\u8d56\u6a21\u578b\u7279\u5b9a\u81ea\u7701\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2509.25045", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25045", "abs": "https://arxiv.org/abs/2509.25045", "authors": ["Marco Bronzini", "Carlo Nicolini", "Bruno Lepri", "Jacopo Staiano", "Andrea Passerini"], "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures", "comment": null, "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHyperdimensional Probe\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eceLLM\u5411\u91cf\u7a7a\u95f4\u4e2d\u89e3\u7801\u4fe1\u606f\uff0c\u7ed3\u5408\u7b26\u53f7\u8868\u793a\u548c\u795e\u7ecf\u63a2\u6d4b\u6280\u672f\uff0c\u901a\u8fc7\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u5c06\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u6295\u5f71\u5230\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u4e2d\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u5982\u76f4\u63a5logit\u5f52\u56e0\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff09\u7531\u4e8e\u6a21\u578b\u8f93\u51fa\u8bcd\u6c47\u8868\u9650\u5236\u6216\u7279\u5f81\u540d\u79f0\u4e0d\u6e05\u6670\u7b49\u95ee\u9898\uff0c\u5bf9LLM\u5185\u90e8\u8868\u793a\u7684\u7406\u89e3\u6709\u9650\u3002", "method": "\u7ed3\u5408\u7b26\u53f7\u8868\u793a\u548c\u795e\u7ecf\u63a2\u6d4b\uff0c\u4f7f\u7528\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u5c06\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u6295\u5f71\u5230\u53ef\u89e3\u91ca\u6982\u5ff5\u4e2d\uff0c\u5728\u63a7\u5236\u8f93\u5165\u5b8c\u6210\u4efb\u52a1\u548c\u95ee\u7b54\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u89e3\u7801\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u63a2\u9488\u80fd\u591f\u5728\u4e0d\u540c\u7684LLM\u3001\u5d4c\u5165\u5927\u5c0f\u548c\u8f93\u5165\u9886\u57df\u4e2d\u53ef\u9760\u5730\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6982\u5ff5\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522bLLM\u7684\u5931\u8d25\u60c5\u51b5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86LLM\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u4fe1\u606f\u89e3\u7801\uff0c\u80fd\u591f\u4ece\u795e\u7ecf\u8868\u793a\u4e2d\u63d0\u53d6\u66f4\u5177\u4fe1\u606f\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7ed3\u6784\u5316\u7684\u7279\u5f81\u3002", "topic": "agent analysis"}}
{"id": "2509.25084", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25084", "abs": "https://arxiv.org/abs/2509.25084", "authors": ["Shuofei Qiao", "Yanqiu Zhao", "Zhisong Qiu", "Xiaobin Wang", "Jintian Zhang", "Zhao Bin", "Ningyu Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Huajun Chen"], "title": "Scaling Generalist Data-Analytic Agents", "comment": "Work in progress", "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.", "AI": {"tldr": "DataMind\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u548c\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u901a\u7528\u7684\u6570\u636e\u5206\u6790\u667a\u80fd\u4f53\u3002\u5b83\u89e3\u51b3\u4e86\u5f00\u6e90\u6570\u636e\u5206\u6790\u667a\u80fd\u4f53\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6570\u636e\u8d44\u6e90\u4e0d\u8db3\u3001\u8bad\u7ec3\u7b56\u7565\u4e0d\u5f53\u548c\u4e0d\u7a33\u5b9a\u7684\u57fa\u4e8e\u4ee3\u7801\u7684\u591a\u8f6e\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u7684\u6570\u636e\u5206\u6790\u667a\u80fd\u4f53\u4e25\u91cd\u4f9d\u8d56\u57fa\u4e8e\u4e13\u6709\u6a21\u578b\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u6837\u683c\u5f0f\u7684\u5927\u89c4\u6a21\u6570\u636e\u6587\u4ef6\u548c\u73b0\u5b9e\u4e16\u754c\u5206\u6790\u6240\u9700\u7684\u957f\u671f\u591a\u6b65\u63a8\u7406\u3002", "method": "DataMind\u91c7\u7528\uff1a1\uff09\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u7c7b\u548c\u9012\u5f52\u6613\u5230\u96be\u4efb\u52a1\u7ec4\u5408\u673a\u5236\uff1b2\uff09\u77e5\u8bc6\u589e\u5f3a\u7684\u8f68\u8ff9\u91c7\u6837\u7b56\u7565\uff1b3\uff09\u52a8\u6001\u53ef\u8c03\u7684SFT\u548cRL\u635f\u5931\u8bad\u7ec3\u76ee\u6807\uff1b4\uff09\u5185\u5b58\u8282\u7ea6\u4e14\u7a33\u5b9a\u7684\u57fa\u4e8e\u4ee3\u7801\u7684\u591a\u8f6e\u63a8\u7406\u6846\u67b6\u3002", "result": "\u57fa\u4e8eDataMind-12K\u8bad\u7ec3\u7684DataMind-14B\u5728\u591a\u4e2a\u6570\u636e\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523071.16%\u7684\u5e73\u5747\u5206\uff0c\u4f18\u4e8e\u6700\u5f3a\u7684\u4e13\u6709\u57fa\u7ebfDeepSeek-V3.1\u548cGPT-5\u3002DataMind-7B\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u523068.10%\u3002", "conclusion": "DataMind\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u9ad8\u6027\u80fd\u7684\u5f00\u6e90\u6570\u636e\u5206\u6790\u667a\u80fd\u4f53\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7814\u7a76\u89c1\u89e3\uff0c\u5e76\u5c06\u53d1\u5e03\u6570\u636e\u96c6\u548c\u6a21\u578b\u4f9b\u672a\u6765\u7814\u7a76\u4f7f\u7528\u3002", "topic": "agent analysis"}}
{"id": "2509.24203", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24203", "abs": "https://arxiv.org/abs/2509.24203", "authors": ["Chaorui Yao", "Yanxi Chen", "Yuchang Sun", "Yushuo Chen", "Wenhao Zhang", "Xuchen Pan", "Yaliang Li", "Bolin Ding"], "title": "Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends", "comment": null, "summary": "Off-policy reinforcement learning (RL) for large language models (LLMs) is\nattracting growing interest, driven by practical constraints in real-world\napplications, the complexity of LLM-RL infrastructure, and the need for further\ninnovations of RL methodologies. While classic REINFORCE and its modern\nvariants like Group Relative Policy Optimization (GRPO) are typically regarded\nas on-policy algorithms with limited tolerance of off-policyness, we present in\nthis work a first-principles derivation for group-relative REINFORCE without\nassuming a specific training data distribution, showing that it admits a native\noff-policy interpretation. This perspective yields two general principles for\nadapting REINFORCE to off-policy settings: regularizing policy updates, and\nactively shaping the data distribution. Our analysis demystifies some myths\nabout the roles of importance sampling and clipping in GRPO, unifies and\nreinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and\nAsymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss,\nand offers theoretical justification for seemingly heuristic data-weighting\nstrategies. Our findings lead to actionable insights that are validated with\nextensive empirical studies, and open up new opportunities for principled\nalgorithm design in off-policy RL for LLMs. Source code for this work is\navailable at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u89e3\u91ca\u4e86REINFORCE\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5929\u7136\u7684\u79bb\u7b56\u7565\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u9002\u5e94\u79bb\u7b56\u7565\u8bbe\u7f6e\u7684\u539f\u5219\uff1a\u6b63\u5219\u5316\u7b56\u7565\u66f4\u65b0\u548c\u4e3b\u52a8\u5851\u9020\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u7531\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7ea6\u675f\u3001LLM-RL\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u6027\u4ee5\u53caRL\u65b9\u6cd5\u521b\u65b0\u7684\u9700\u6c42\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u7fa4\u76f8\u5bf9REINFORCE\uff0c\u4e0d\u5047\u8bbe\u7279\u5b9a\u7684\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u63ed\u793a\u5176\u79bb\u7b56\u7565\u7279\u6027\uff0c\u5e76\u7edf\u4e00\u548c\u91cd\u65b0\u89e3\u91caOPMD\u548cAsymRE\u7b49\u7b97\u6cd5\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u91cd\u8981\u6027\u91c7\u6837\u548c\u88c1\u526a\u5728GRPO\u4e2d\u7684\u4f5c\u7528\uff0c\u4e3a\u542f\u53d1\u5f0f\u6570\u636e\u52a0\u6743\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u7684\u79bb\u7b56\u7565RL\u5f00\u8f9f\u4e86\u65b0\u7684\u7b97\u6cd5\u8bbe\u8ba1\u673a\u4f1a\uff0c\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25179", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25179", "abs": "https://arxiv.org/abs/2509.25179", "authors": ["Penghai Zhao", "Jinyu Tian", "Qinghua Xing", "Xin Zhang", "Zheng Li", "Jianjun Qian", "Ming-Ming Cheng", "Xiang Li"], "title": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation", "comment": "NAIPv2 complements our earlier work NAIPv1 (arXiv:2408.03934).\n  Whereas NAIPv1 addressed citation count-based impact prediction, NAIPv2\n  estimates research quality using peer review data", "summary": "The ability to estimate the quality of scientific papers is central to how\nboth humans and AI systems will advance scientific knowledge in the future.\nHowever, existing LLM-based estimation methods suffer from high inference cost,\nwhereas the faster direct score regression approach is limited by scale\ninconsistencies. We present NAIPv2, a debiased and efficient framework for\npaper quality estimation. NAIPv2 employs pairwise learning within domain-year\ngroups to reduce inconsistencies in reviewer ratings and introduces the Review\nTendency Signal (RTS) as a probabilistic integration of reviewer scores and\nconfidences. To support training and evaluation, we further construct NAIDv2, a\nlarge-scale dataset of 24,276 ICLR submissions enriched with metadata and\ndetailed structured content. Trained on pairwise comparisons but enabling\nefficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art\nperformance (78.2% AUC, 0.432 Spearman), while maintaining scalable,\nlinear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it\nfurther demonstrates strong generalization, with predicted scores increasing\nconsistently across decision categories from Rejected to Oral. These findings\nestablish NAIPv2 as a debiased and scalable framework for automated paper\nquality estimation, marking a step toward future scientific intelligence\nsystems. Code and dataset are released at\nhttps://sway.cloud.microsoft/Pr42npP80MfPhvj8.", "AI": {"tldr": "NAIPv2\u662f\u4e00\u4e2a\u7528\u4e8e\u79d1\u5b66\u8bba\u6587\u8d28\u91cf\u8bc4\u4f30\u7684\u53bb\u504f\u7f6e\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u57df\u5185\u5e74\u4efd\u7ec4\u7684\u6210\u5bf9\u5b66\u4e60\u51cf\u5c11\u8bc4\u5206\u4e0d\u4e00\u81f4\u6027\uff0c\u5f15\u5165\u8bc4\u5ba1\u503e\u5411\u4fe1\u53f7\u6574\u5408\u8bc4\u5206\u548c\u7f6e\u4fe1\u5ea6\uff0c\u5728ICLR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u5e76\u4fdd\u6301\u7ebf\u6027\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u65b9\u6cd5\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u800c\u76f4\u63a5\u8bc4\u5206\u56de\u5f52\u65b9\u6cd5\u5b58\u5728\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u8bba\u6587\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u57df\u5185\u5e74\u4efd\u7ec4\u7684\u6210\u5bf9\u5b66\u4e60\u51cf\u5c11\u8bc4\u5206\u4e0d\u4e00\u81f4\uff0c\u5f15\u5165\u8bc4\u5ba1\u503e\u5411\u4fe1\u53f7(RTS)\u4f5c\u4e3a\u8bc4\u5206\u548c\u7f6e\u4fe1\u5ea6\u7684\u6982\u7387\u6574\u5408\uff0c\u6784\u5efaNAIDv2\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728ICLR\u6570\u636e\u96c6\u4e0a\u8fbe\u523078.2% AUC\u548c0.432 Spearman\u76f8\u5173\u6027\uff0c\u5728NeurIPS\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u9884\u6d4b\u5206\u6570\u4ece\u62d2\u7edd\u5230\u53e3\u5934\u62a5\u544a\u7c7b\u522b\u4e00\u81f4\u9012\u589e\u3002", "conclusion": "NAIPv2\u5efa\u7acb\u4e86\u4e00\u4e2a\u53bb\u504f\u7f6e\u4e14\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u8bba\u6587\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u79d1\u5b66\u667a\u80fd\u7cfb\u7edf\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002", "topic": "agent analysis"}}
{"id": "2509.25189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25189", "abs": "https://arxiv.org/abs/2509.25189", "authors": ["Gongrui Zhang", "Jialiang Zhu", "Ruiqi Yang", "Kai Qiu", "Miaosen Zhang", "Zhirong Wu", "Qi Dai", "Bei Liu", "Chong Luo", "Zhengyuan Yang", "Linjie Li", "Lijuan Wang", "Weizhu Chen", "Yuan Zhang", "Xin Li", "Zhaoyi Liu", "Xin Geng", "Baining Guo"], "title": "InfoAgent: Advancing Autonomous Information-Seeking Agents", "comment": null, "summary": "Building Large Language Model agents that expand their capabilities by\ninteracting with external tools represents a new frontier in AI research and\napplications. In this paper, we introduce InfoAgent, a deep research agent\npowered by an innovative data synthesis pipeline and orchestrated web search\ntools. To construct challenging, hard-to-find queries,we build entity trees and\napply sub-tree sampling with entity fuzzification to systematically increase\nquestion difficulty. Unlike prior work that relies heavily on commercial search\ntools, we develop a dedicated self-hosted search infrastructure, enhancing\ntransparency of agent environments and facilitating further advancement of\nagent capacity. We evaluate the effectiveness of our data pipeline by measuring\nthe average number of tool calls required to correctly answer a question, and\nalso show that our agent yields better performance when equipped with our\ntools. Our \\mbox{InfoAgent} is post-trained from Qwen3-14B using a two-stage\nrecipe: cold-start supervised finetuning to instill long-horizon search\nbehaviors, followed by reinforcement learning which significantly improves\nreasoning-driven tool use. With our methods, InfoAgent achieves 15.3\\% accuracy\non BrowseComp, 29.2\\% on BrowseComp-ZH, and 40.4\\% on Xbench-DS, outperforming\nprior open-source deep research agents such as WebSailor-72B and DeepDive-32B.", "AI": {"tldr": "InfoAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u7f16\u6392\u7684\u7f51\u9875\u641c\u7d22\u5de5\u5177\u6765\u6269\u5c55\u80fd\u529b\u3002\u5b83\u4f7f\u7528\u5b9e\u4f53\u6811\u548c\u5b50\u6811\u91c7\u6837\u6784\u5efa\u6311\u6218\u6027\u67e5\u8be2\uff0c\u5e76\u5f00\u53d1\u81ea\u6258\u7ba1\u641c\u7d22\u57fa\u7840\u8bbe\u65bd\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u3002", "motivation": "\u6784\u5efa\u80fd\u591f\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u6269\u5c55\u80fd\u529b\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u662fAI\u7814\u7a76\u7684\u65b0\u524d\u6cbf\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u8fc7\u5ea6\u4f9d\u8d56\u5546\u4e1a\u641c\u7d22\u5de5\u5177\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "method": "\u4f7f\u7528\u5b9e\u4f53\u6811\u548c\u5b50\u6811\u91c7\u6837\u6784\u5efa\u6311\u6218\u6027\u67e5\u8be2\uff0c\u5f00\u53d1\u81ea\u6258\u7ba1\u641c\u7d22\u57fa\u7840\u8bbe\u65bd\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\u704c\u8f93\u957f\u671f\u641c\u7d22\u884c\u4e3a\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u63a8\u7406\u9a71\u52a8\u7684\u5de5\u5177\u4f7f\u7528\u3002", "result": "\u5728BrowseComp\u4e0a\u8fbe\u523015.3%\u51c6\u786e\u7387\uff0cBrowseComp-ZH\u4e0a29.2%\uff0cXbench-DS\u4e0a40.4%\uff0c\u8d85\u8d8a\u4e86WebSailor-72B\u548cDeepDive-32B\u7b49\u5148\u524d\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u3002", "conclusion": "InfoAgent\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u81ea\u6258\u7ba1\u641c\u7d22\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6784\u5efa\u66f4\u900f\u660e\u53ef\u63a7\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2509.24305", "categories": ["cs.LG", "cs.DC", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.24305", "abs": "https://arxiv.org/abs/2509.24305", "authors": ["Alexander Tyurin", "Andrei Spiridonov", "Varvara Rudenko"], "title": "Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning", "comment": null, "summary": "We study distributed reinforcement learning (RL) with policy gradient methods\nunder asynchronous and parallel computations and communications. While\nnon-distributed methods are well understood theoretically and have achieved\nremarkable empirical success, their distributed counterparts remain less\nexplored, particularly in the presence of heterogeneous asynchronous\ncomputations and communication bottlenecks. We introduce two new algorithms,\nRennala NIGT and Malenia NIGT, which implement asynchronous policy gradient\naggregation and achieve state-of-the-art efficiency. In the homogeneous\nsetting, Rennala NIGT provably improves the total computational and\ncommunication complexity while supporting the AllReduce operation. In the\nheterogeneous setting, Malenia NIGT simultaneously handles asynchronous\ncomputations and heterogeneous environments with strictly better theoretical\nguarantees. Our results are further corroborated by experiments, showing that\nour methods significantly outperform prior approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5Rennala NIGT\u548cMalenia NIGT\uff0c\u7528\u4e8e\u5904\u7406\u5f02\u6b65\u5e76\u884c\u8ba1\u7b97\u548c\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u6700\u4f18\u6548\u7387\u3002", "motivation": "\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5f02\u6784\u5f02\u6b65\u8ba1\u7b97\u548c\u901a\u4fe1\u74f6\u9888\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u7814\u7a76\u8f83\u5c11\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u65b0\u7b97\u6cd5\uff1aRennala NIGT\u5728\u5747\u5300\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u7b56\u7565\u68af\u5ea6\u805a\u5408\uff0c\u652f\u6301AllReduce\u64cd\u4f5c\uff1bMalenia NIGT\u540c\u65f6\u5904\u7406\u5f02\u6b65\u8ba1\u7b97\u548c\u5f02\u6784\u73af\u5883\u3002", "result": "Rennala NIGT\u5728\u5747\u5300\u8bbe\u7f6e\u4e0b\u663e\u8457\u6539\u5584\u4e86\u603b\u8ba1\u7b97\u548c\u901a\u4fe1\u590d\u6742\u5ea6\uff0cMelenia NIGT\u5728\u5f02\u6784\u8bbe\u7f6e\u4e0b\u83b7\u5f97\u4e25\u683c\u66f4\u597d\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u7b97\u6cd5\u5728\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u4e2d\u6709\u6548\u5904\u7406\u4e86\u5f02\u6b65\u548c\u5f02\u6784\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24372", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.24372", "abs": "https://arxiv.org/abs/2509.24372", "authors": ["Xin Qiu", "Yulu Gan", "Conor F. Hayes", "Qiyao Liang", "Elliot Meyerson", "Babak Hodjat", "Risto Miikkulainen"], "title": "Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning", "comment": "24 pages, including the appendix", "summary": "Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is\na critical step in the AI deployment pipeline. Reinforcement learning (RL) is\narguably the most prominent fine-tuning method, contributing to the birth of\nmany state-of-the-art LLMs. In contrast, evolution strategies (ES), which once\nshowed comparable performance to RL on models with a few million parameters,\nwas neglected due to the pessimistic perception of its scalability to larger\nmodels. In this work, we report the first successful attempt to scale up ES for\nfine-tuning the full parameters of LLMs, showing the surprising fact that ES\ncan search efficiently over billions of parameters and outperform existing RL\nfine-tuning methods in multiple respects, including sample efficiency,\ntolerance to long-horizon rewards, robustness to different base LLMs, less\ntendency to reward hacking, and more stable performance across runs. It\ntherefore serves as a basis to unlock a new direction in LLM fine-tuning beyond\nwhat current RL techniques provide. The source codes are provided at:\nhttps://github.com/VsonicV/es-fine-tuning-paper.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u6210\u529f\u5c06\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b8c\u6574\u53c2\u6570\u5fae\u8c03\uff0c\u8bc1\u660eES\u5728\u6570\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0b\u4ecd\u80fd\u9ad8\u6548\u641c\u7d22\uff0c\u5e76\u5728\u591a\u4e2a\u65b9\u9762\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8ba4\u4e3a\u8fdb\u5316\u7b56\u7565\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u4f46\u672c\u6587\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22ES\u5728LLM\u5fae\u8c03\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u4f9b\u8d85\u8d8a\u73b0\u6709RL\u6280\u672f\u7684\u65b0\u65b9\u5411\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u5bf9\u9884\u8bad\u7ec3LLM\u7684\u5b8c\u6574\u53c2\u6570\u8fdb\u884c\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u6270\u52a8\u53c2\u6570\u5e76\u57fa\u4e8e\u5956\u52b1\u4fe1\u53f7\u9009\u62e9\u6700\u4f18\u65b9\u5411\u6765\u4f18\u5316\u6a21\u578b\u3002", "result": "ES\u5728\u6837\u672c\u6548\u7387\u3001\u957f\u65f6\u7a0b\u5956\u52b1\u5bb9\u5fcd\u5ea6\u3001\u5bf9\u4e0d\u540c\u57fa\u7840LLM\u7684\u9c81\u68d2\u6027\u3001\u5956\u52b1\u653b\u51fb\u62b5\u6297\u80fd\u529b\u548c\u8fd0\u884c\u7a33\u5b9a\u6027\u7b49\u65b9\u9762\u5747\u4f18\u4e8eRL\u65b9\u6cd5\u3002", "conclusion": "\u8fdb\u5316\u7b56\u7565\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u8d85\u8d8a\u5f53\u524dRL\u6280\u672f\u7684\u65b0\u65b9\u5411\uff0c\u8bc1\u660eES\u80fd\u591f\u6709\u6548\u6269\u5c55\u5230\u6570\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24496", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24496", "abs": "https://arxiv.org/abs/2509.24496", "authors": ["Zhaomin Wu", "Haodong Zhao", "Ziyang Wang", "Jizhou Guo", "Qian Wang", "Bingsheng He"], "title": "LLM DNA: Tracing Model Evolution via Functional Representations", "comment": null, "summary": "The explosive growth of large language models (LLMs) has created a vast but\nopaque landscape: millions of models exist, yet their evolutionary\nrelationships through fine-tuning, distillation, or adaptation are often\nundocumented or unclear, complicating LLM management. Existing methods are\nlimited by task specificity, fixed model sets, or strict assumptions about\ntokenizers or architectures. Inspired by biological DNA, we address these\nlimitations by mathematically defining LLM DNA as a low-dimensional,\nbi-Lipschitz representation of functional behavior. We prove that LLM DNA\nsatisfies inheritance and genetic determinism properties and establish the\nexistence of DNA. Building on this theory, we derive a general, scalable,\ntraining-free pipeline for DNA extraction. In experiments across 305 LLMs, DNA\naligns with prior studies on limited subsets and achieves superior or\ncompetitive performance on specific tasks. Beyond these tasks, DNA comparisons\nuncover previously undocumented relationships among LLMs. We further construct\nthe evolutionary tree of LLMs using phylogenetic algorithms, which align with\nshifts from encoder-decoder to decoder-only architectures, reflect temporal\nprogression, and reveal distinct evolutionary speeds across LLM families.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLLM DNA\u6982\u5ff5\uff0c\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u529f\u80fd\u884c\u4e3a\u7684\u4f4e\u7ef4\u8868\u793a\uff0c\u7528\u4e8e\u8ffd\u8e2a\u6a21\u578b\u95f4\u7684\u8fdb\u5316\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e86LLM\u8fdb\u5316\u6811\u3002", "motivation": "\u89e3\u51b3\u6570\u767e\u4e07LLM\u6a21\u578b\u95f4\u8fdb\u5316\u5173\u7cfb\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u4efb\u52a1\u7279\u5b9a\u6027\u3001\u56fa\u5b9a\u6a21\u578b\u96c6\u6216\u4e25\u683c\u5047\u8bbe\u3002", "method": "\u6570\u5b66\u5b9a\u4e49LLM DNA\u4e3a\u529f\u80fd\u884c\u4e3a\u7684\u4f4e\u7ef4\u53ccLipschitz\u8868\u793a\uff0c\u8bc1\u660e\u5176\u6ee1\u8db3\u9057\u4f20\u548c\u9057\u4f20\u51b3\u5b9a\u8bba\u7279\u6027\uff0c\u5e76\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u7684\u53ef\u6269\u5c55DNA\u63d0\u53d6\u6d41\u7a0b\u3002", "result": "\u5728305\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793aDNA\u4e0e\u5148\u524d\u7814\u7a76\u4e00\u81f4\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u8bb0\u5f55\u7684\u6a21\u578b\u5173\u7cfb\uff0c\u6210\u529f\u6784\u5efa\u4e86LLM\u8fdb\u5316\u6811\u3002", "conclusion": "LLM DNA\u662f\u8ffd\u8e2a\u6a21\u578b\u8fdb\u5316\u5173\u7cfb\u7684\u6709\u6548\u5de5\u5177\uff0c\u8fdb\u5316\u6811\u53cd\u6620\u4e86\u67b6\u6784\u4ece\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5230\u4ec5\u89e3\u7801\u5668\u7684\u8f6c\u53d8\u3001\u65f6\u95f4\u8fdb\u5c55\u4ee5\u53ca\u4e0d\u540c\u5bb6\u65cf\u7684\u4e0d\u540c\u8fdb\u5316\u901f\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2509.24559", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24559", "abs": "https://arxiv.org/abs/2509.24559", "authors": ["Marco Molinari", "Leonardo Nevali", "Saharsha Navani", "Omar G. Younis"], "title": "Emergent World Representations in OpenVLA", "comment": null, "summary": "Vision Language Action models (VLAs) trained with policy-based reinforcement\nlearning (RL) encode complex behaviors without explicitly modeling\nenvironmental dynamics. However, it remains unclear whether VLAs implicitly\nlearn world models, a hallmark of model-based RL. We propose an experimental\nmethodology using embedding arithmetic on state representations to probe\nwhether OpenVLA, the current state of the art in VLAs, contains latent\nknowledge of state transitions. Specifically, we measure the difference between\nembeddings of sequential environment states and test whether this transition\nvector is recoverable from intermediate model activations. Using linear and non\nlinear probes trained on activations across layers, we find statistically\nsignificant predictive ability on state transitions exceeding baselines\n(embeddings), indicating that OpenVLA encodes an internal world model (as\nopposed to the probes learning the state transitions). We investigate the\npredictive ability of an earlier checkpoint of OpenVLA, and uncover hints that\nthe world model emerges as training progresses. Finally, we outline a pipeline\nleveraging Sparse Autoencoders (SAEs) to analyze OpenVLA's world model.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5d4c\u5165\u7b97\u672f\u65b9\u6cd5\u63a2\u7a76OpenVLA\u662f\u5426\u9690\u5f0f\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u53d1\u73b0\u5176\u5728\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7f16\u7801\u4e86\u72b6\u6001\u8f6c\u79fb\u77e5\u8bc6\uff0c\u8868\u660eVLAs\u786e\u5b9e\u5305\u542b\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u7814\u7a76Vision Language Action\u6a21\u578b(VLAs)\u662f\u5426\u5728\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u9690\u5f0f\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u662f\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u6807\u5fd7\u6027\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u5d4c\u5165\u7b97\u672f\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u975e\u7ebf\u6027\u63a2\u9488\u5206\u6790OpenVLA\u6a21\u578b\u5404\u5c42\u7684\u6fc0\u6d3b\u72b6\u6001\uff0c\u6d4b\u91cf\u5e8f\u5217\u73af\u5883\u72b6\u6001\u5d4c\u5165\u7684\u5dee\u5f02\uff0c\u68c0\u6d4b\u72b6\u6001\u8f6c\u79fb\u5411\u91cf\u662f\u5426\u53ef\u4ece\u4e2d\u95f4\u6a21\u578b\u6fc0\u6d3b\u4e2d\u6062\u590d\u3002", "result": "\u53d1\u73b0OpenVLA\u5728\u6a21\u578b\u6fc0\u6d3b\u4e2d\u5177\u6709\u663e\u8457\u7684\u72b6\u6001\u8f6c\u79fb\u9884\u6d4b\u80fd\u529b\uff0c\u8d85\u8fc7\u57fa\u7ebf\u6c34\u5e73\uff0c\u8868\u660e\u5176\u7f16\u7801\u4e86\u5185\u90e8\u4e16\u754c\u6a21\u578b\u3002\u65e9\u671f\u68c0\u67e5\u70b9\u7684\u5206\u6790\u6697\u793a\u4e16\u754c\u6a21\u578b\u968f\u7740\u8bad\u7ec3\u8fdb\u5c55\u800c\u9010\u6e10\u5f62\u6210\u3002", "conclusion": "OpenVLA\u786e\u5b9e\u9690\u5f0f\u5b66\u4e60\u4e86\u4e16\u754c\u6a21\u578b\uff0c\u8fd9\u4e3a\u7406\u89e3VLAs\u7684\u5185\u90e8\u8868\u5f81\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790\u4e16\u754c\u6a21\u578b\u7684\u7ba1\u9053\u3002", "topic": "agent analysis"}}
{"id": "2509.24610", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24610", "abs": "https://arxiv.org/abs/2509.24610", "authors": ["Liang Lin", "Zhihao Xu", "Junhao Dong", "Jian Zhao", "Yuchen Yuan", "Guibin Zhang", "Miao Yu", "Yiming Zhang", "Zhengtao Yao", "Huahui Yi", "Dongrui Liu", "Xinfeng Li", "Kun Wang"], "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment", "comment": null, "summary": "Large language model (LLM) alignment faces a critical dilemma when addressing\nmultiple human preferences: improvements in one dimension frequently come at\nthe expense of others, creating unavoidable trade-offs between competing\nobjectives like helpfulness and harmlessness. While prior work mainly focuses\non constraint-based optimization algorithms and data selection strategies to\nmitigate conflicts, these approaches overlook the fundamental issue of\nresolving conflicts directly at the parameter level. In this paper, we present\nOrthAlign, an innovative approach that pioneers a new paradigm by leveraging\northogonal subspace decomposition to fundamentally resolve gradient-level\nconflicts in multi-objective preference alignment. OrthAlign strategically\ndecomposes parameter update spaces into orthogonal subspaces, ensuring that\noptimization toward different preferences occurs in mathematically\nnon-interfering directions. Building upon this, we provide theoretical\nguarantees demonstrating that when parameter increments satisfy both orthogonal\nsubspace constraints and spectral norm bounds, the resulting updates exhibit\nlinear Lipschitz growth rather than exponential instability, ensuring stable\nconvergence across all preference dimensions. Extensive experiments show that:\nI. OrthAlign achieves maximum single-preference improvements ranging from\n34.61% to 50.89% after multiple-objective alignment across helpful, harmless,\nand truthful dimensions. II. With an average overall reward improvement of\n13.96%.", "AI": {"tldr": "OrthAlign\u4f7f\u7528\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u89e3\u51b3\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\uff0c\u786e\u4fdd\u4e0d\u540c\u504f\u597d\u7684\u4f18\u5316\u5728\u6570\u5b66\u4e0a\u4e0d\u5e72\u6270\u7684\u65b9\u5411\u4e0a\u8fdb\u884c\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u89e3\u51b3LLM\u5bf9\u9f50\u4e2d\u591a\u4e2a\u4eba\u7c7b\u504f\u597d\u7684\u5173\u952e\u56f0\u5883\uff1a\u6539\u8fdb\u4e00\u4e2a\u7ef4\u5ea6\u901a\u5e38\u4ee5\u727a\u7272\u5176\u4ed6\u7ef4\u5ea6\u4e3a\u4ee3\u4ef7\uff0c\u9700\u8981\u5728\u7ade\u4e89\u76ee\u6807\uff08\u5982\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\uff09\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u5c06\u53c2\u6570\u66f4\u65b0\u7a7a\u95f4\u5206\u89e3\u4e3a\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u786e\u4fdd\u4e0d\u540c\u504f\u597d\u7684\u4f18\u5316\u5728\u6570\u5b66\u4e0a\u4e0d\u5e72\u6270\u7684\u65b9\u5411\u4e0a\u8fdb\u884c\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5728\u5e2e\u52a9\u6027\u3001\u65e0\u5bb3\u6027\u548c\u771f\u5b9e\u6027\u7ef4\u5ea6\u4e0a\uff0c\u591a\u76ee\u6807\u5bf9\u9f50\u540e\u5355\u504f\u597d\u6700\u5927\u6539\u8fdb\u8fbe34.61%\u81f350.89%\uff0c\u5e73\u5747\u603b\u4f53\u5956\u52b1\u6539\u8fdb13.96%\u3002", "conclusion": "OrthAlign\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u89e3\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6536\u655b\u548c\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24696", "abs": "https://arxiv.org/abs/2509.24696", "authors": ["Zikun Qu", "Min Zhang", "Mingze Kong", "Xiang Li", "Zhiwei Shang", "Zhiyong Wang", "Yikun Ban", "Shuang Qiu", "Yao Shu", "Zhongxiang Dai"], "title": "T-POP: Test-Time Personalization with Online Preference Feedback", "comment": "Preprint", "summary": "Personalizing large language models (LLMs) to individual user preferences is\na critical step beyond generating generically helpful responses. However,\ncurrent personalization methods are ill-suited for new users, as they typically\nrequire either slow, resource-intensive fine-tuning or a substantial amount of\npre-existing user data, creating a significant cold-start problem. To address\nthis challenge, we introduce a new paradigm for real-time personalization by\nlearning from online pairwise preference feedback collected during text\ngeneration. We propose T-POP (Test-Time Personalization with Online Preference\nFeedback}), a novel algorithm that synergistically combines test-time alignment\nwith dueling bandits. Without updating the LLM parameters, T-POP steers the\ndecoding process of a frozen LLM by learning a reward function online that\ncaptures user preferences. By leveraging dueling bandits, T-POP intelligently\nqueries the user to efficiently balance between exploring their preferences and\nexploiting the learned knowledge to generate personalized text. Extensive\nexperiments demonstrate that T-POP achieves rapid and data-efficient\npersonalization, significantly outperforming existing baselines and showing\nconsistent improvement with more user interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86T-POP\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u6210\u5bf9\u504f\u597d\u53cd\u9988\u5b9e\u73b0\u5b9e\u65f6\u4e2a\u6027\u5316\uff0c\u65e0\u9700\u66f4\u65b0LLM\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u65b0\u7528\u6237\u7684\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7528\u6237\u6570\u636e\u6216\u8d44\u6e90\u5bc6\u96c6\u578b\u5fae\u8c03\uff0c\u5bfc\u81f4\u65b0\u7528\u6237\u9762\u4e34\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5b9e\u65f6\u4e2a\u6027\u5316\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u548c\u51b3\u6597\u8d4c\u535a\u673a\u7b97\u6cd5\uff0c\u5728\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u5728\u7ebf\u5b66\u4e60\u7528\u6237\u504f\u597d\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u667a\u80fd\u67e5\u8be2\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660eT-POP\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u6570\u636e\u9ad8\u6548\u7684\u4e2a\u6027\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u968f\u7740\u7528\u6237\u4ea4\u4e92\u589e\u52a0\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "T-POP\u4e3aLLM\u5b9e\u65f6\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u7ebf\u504f\u597d\u53cd\u9988\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24748", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24748", "abs": "https://arxiv.org/abs/2509.24748", "authors": ["Longxiang He", "Deheng Ye", "Junbo Tan", "Xueqian Wang", "Li Shen"], "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption", "comment": "39th Conference on Neural Information Processing Systems", "summary": "Pretraining a policy on offline data followed by fine-tuning through online\ninteractions, known as Offline-to-Online Reinforcement Learning (O2O RL), has\nemerged as a promising paradigm for real-world RL deployment. However, both\noffline datasets and online interactions in practical environments are often\nnoisy or even maliciously corrupted, severely degrading the performance of O2O\nRL. Existing works primarily focus on mitigating the conservatism of offline\npolicies via online exploration, while the robustness of O2O RL under data\ncorruption, including states, actions, rewards, and dynamics, is still\nunexplored. In this work, we observe that data corruption induces heavy-tailed\nbehavior in the policy, thereby substantially degrading the efficiency of\nonline exploration. To address this issue, we incorporate Inverse Probability\nWeighted (IPW) into the online exploration policy to alleviate\nheavy-tailedness, and propose a novel, simple yet effective method termed\n$\\textbf{RPEX}$: $\\textbf{R}$obust $\\textbf{P}$olicy $\\textbf{EX}$pansion.\nExtensive experimental results on D4RL datasets demonstrate that RPEX achieves\nSOTA O2O performance across a wide range of data corruption scenarios. Code is\navailable at\n$\\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.", "AI": {"tldr": "\u63d0\u51fa\u4e86RPEX\u65b9\u6cd5\uff0c\u901a\u8fc7\u9006\u6982\u7387\u52a0\u6743\u7f13\u89e3\u6570\u636e\u635f\u574f\u5bfc\u81f4\u7684\u7b56\u7565\u91cd\u5c3e\u884c\u4e3a\uff0c\u63d0\u5347\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u636e\u635f\u574f\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5b9e\u9645\u73af\u5883\u4e2d\u79bb\u7ebf\u6570\u636e\u96c6\u548c\u5728\u7ebf\u4ea4\u4e92\u7ecf\u5e38\u5b58\u5728\u566a\u58f0\u751a\u81f3\u6076\u610f\u635f\u574f\uff0c\u4e25\u91cd\u5f71\u54cd\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u5728\u7ebf\u63a2\u7d22\u7f13\u89e3\u79bb\u7ebf\u7b56\u7565\u7684\u4fdd\u5b88\u6027\uff0c\u5bf9\u6570\u636e\u635f\u574f\u4e0b\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faRPEX\u65b9\u6cd5\uff0c\u5c06\u9006\u6982\u7387\u52a0\u6743\u878d\u5165\u5728\u7ebf\u63a2\u7d22\u7b56\u7565\u4ee5\u7f13\u89e3\u91cd\u5c3e\u884c\u4e3a\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u7b56\u7565\u6269\u5c55\u3002", "result": "\u5728D4RL\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRPEX\u5728\u5404\u79cd\u6570\u636e\u635f\u574f\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u6027\u80fd\u3002", "conclusion": "RPEX\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9c81\u68d2\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6570\u636e\u635f\u574f\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24923", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.24923", "abs": "https://arxiv.org/abs/2509.24923", "authors": ["Sanxing Chen", "Xiaoyin Chen", "Yukun Huang", "Roy Xie", "Bhuwan Dhingra"], "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training", "comment": null, "summary": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u6765\u589e\u5f3aLLM\u5728\u987a\u5e8f\u51b3\u7b56\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u591a\u81c2\u8001\u864e\u673a\u4efb\u52a1\u4e0a\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u66f4\u8d2a\u5a6a\u7684\u5229\u7528\u884c\u4e3a\uff0c\u6709\u65f6\u4f1a\u8fc7\u65e9\u653e\u5f03\u63a2\u7d22\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u51b3\u7b56\u4e2d\u5f80\u5f80\u63a2\u7d22\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u5176\u63a2\u7d22\u7b56\u7565\u3002\u7814\u7a76\u65e8\u5728\u6bd4\u8f83SFT\u548cRL\u4e24\u79cd\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e86\u89e3\u5b83\u4eec\u5982\u4f55\u5851\u9020\u63a2\u7d22\u884c\u4e3a\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528SFT\u5728\u4e13\u5bb6\u8f68\u8ff9\u4e0a\u8bad\u7ec3LLM\uff0c\u4ee5\u53ca\u4f7f\u7528RL\u914d\u5408\u5b9a\u5236\u7684\u5956\u52b1\u4fe1\u53f7\uff08\u5305\u62ec\u7b56\u7565\u6027\u7684\u9057\u61be\u5f62\u72b6\u5956\u52b1\u548c\u7b97\u6cd5\u5956\u52b1\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u5bf9\u8bad\u7ec3\u540e\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u884c\u4e3a\u5206\u6790\u3002", "result": "\u8bad\u7ec3\u540e\u7684\u667a\u80fd\u4f53\u8868\u73b0\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6027\u80fd\u53ef\u4e0eUCB\u548cThompson Sampling\u76f8\u5ab2\u7f8e\uff0c\u57286\u500d\u66f4\u957f\u7684\u65f6\u95f4\u8de8\u5ea6\u548c\u4e0d\u540c\u8001\u864e\u673a\u5bb6\u65cf\u4e0a\u90fd\u5177\u6709\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4f46RL/SFT\u667a\u80fd\u4f53\u66f4\u5bb9\u6613\u51fa\u73b0\u65e9\u671f\u707e\u96be\u6027\u5931\u8d25\uff0c\u8fc7\u65e9\u653e\u5f03\u63a2\u7d22\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86\u6bcf\u79cd\u8bad\u7ec3\u8303\u5f0f\u7684\u9002\u7528\u573a\u666f\uff0c\u63d0\u5021\u8d85\u8d8a\u5e73\u5747\u9057\u61be\u7684\u5b9a\u5236\u5956\u52b1\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u7a33\u5065\u7684\u63a2\u7d22\u884c\u4e3a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25133", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25133", "abs": "https://arxiv.org/abs/2509.25133", "authors": ["Yuxian Jiang", "Yafu Li", "Guanxu Chen", "Dongrui Liu", "Yu Cheng", "Jing Shao"], "title": "Rethinking Entropy Regularization in Large Reasoning Models", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise\nin enhancing the reasoning abilities of large reasoning models (LRMs). However,\nit suffers from a critical issue: entropy collapse and premature convergence.\nNaive entropy regularization, a common approach for encouraging exploration in\nthe traditional RL literature, fails to address this problem in the context of\nLRM. Our analysis reveals that this failure stems from the vast action space\nand long trajectories in LRMs, which easily trigger a global entropy explosion\nas the model indiscriminately explores all possible actions and states. To\naddress this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method\nthat confines exploration to a meaningful subset of actions and states. SIREN\nachieves this through a two-step entropy masking mechanism, consisting of a\ntop-p mask and a peak-entropy mask. In addition, regularization is transformed\ninto a self-anchored form to stabilize training. Across five mathematical\nbenchmarks, SIREN attains superior average performance over previous\nentropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on\nAIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes\ngreater response diversity and maintains entropy at an appropriate level, which\nhelps to preserve the validation pass@k throughout training. This effectively\nmitigates the premature convergence problem common in RLVR for LRM.", "AI": {"tldr": "\u63d0\u51fa\u4e86SIREN\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u71b5\u6b63\u5219\u5316\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5956\u52b1\u7684\u71b5\u5d29\u6e83\u548c\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u65b9\u6cd5\u5728\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u71b5\u5d29\u6e83\u548c\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u4f20\u7edf\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u5927\u548c\u8f68\u8ff9\u957f\u800c\u5931\u6548\u3002", "method": "\u63d0\u51faSIREN\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u6b65\u71b5\u63a9\u7801\u673a\u5236\uff08top-p\u63a9\u7801\u548c\u5cf0\u503c\u71b5\u63a9\u7801\uff09\u5c06\u63a2\u7d22\u9650\u5236\u5728\u6709\u610f\u4e49\u7684\u52a8\u4f5c\u548c\u72b6\u6001\u5b50\u96c6\uff0c\u5e76\u4f7f\u7528\u81ea\u951a\u5b9a\u5f62\u5f0f\u7684\u6b63\u5219\u5316\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIREN\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u71b5\u76f8\u5173RLVR\u65b9\u6cd5\uff0c\u5728AIME24/25\u4e0a\u83b7\u5f97+6.6 maj@k\u63d0\u5347\uff0c\u4fc3\u8fdb\u54cd\u5e94\u591a\u6837\u6027\u5e76\u4fdd\u6301\u9002\u5f53\u71b5\u6c34\u5e73\u3002", "conclusion": "SIREN\u6709\u6548\u7f13\u89e3\u4e86RLVR\u5728\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u9a8c\u8bc1\u901a\u8fc7\u7387\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.25176", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.25176", "abs": "https://arxiv.org/abs/2509.25176", "authors": ["Haoming Wen", "Yushi Bai", "Juanzi Li", "Jie Tang"], "title": "SIRI: Scaling Iterative Reinforcement Learning with Interleaved Compression", "comment": "In submission", "summary": "We introduce SIRI, Scaling Iterative Reinforcement Learning with Interleaved\nCompression, a simple yet effective RL approach for Large Reasoning Models\n(LRMs) that enables more efficient and accurate reasoning. Existing studies\nhave observed repetitive thinking patterns in LRMs, and attempts to reduce them\noften come at the cost of performance. In this paper, we show that this\ntrade-off can be overcome through a training regime that iteratively alternates\nbetween compressing and expanding the reasoning budget, by dynamically\nadjusting the maximum rollout length during training. The compression phase\ncuts the rollout length, forcing the model to make precise and valuable\ndecisions within a limited context, which effectively reduces redundant tokens\nand increases reasoning density. The expansion phase then relaxes the length\nlimit, providing space for the model to explore and plan in long-horizon\nsettings. Remarkably, we find that after each compression-expansion cycle, the\nmodel's performance improves even as its output length decreases, steadily\npushing it closer to the Pareto frontier in the performance-efficiency\ntrade-off. Training on DeepSeek-R1-Distill-Qwen-1.5B, SIRI-low improves\nperformance on AIME24 by 43.2% while reducing token usage by 46.9% after three\niterations, and SIRI-high achieves the highest accuracy compared to all other\nmethods (Figure 1). Our findings shed light on the potential of periodically\noscillating the LRM's output truncation length during training to dynamically\nbalance exploration and efficiency in reasoning, converging towards an optimal\n\"sweet spot\" between the two. Our models are publicly available.", "AI": {"tldr": "SIRI\u662f\u4e00\u79cd\u901a\u8fc7\u4ea4\u66ff\u538b\u7f29\u548c\u6269\u5c55\u63a8\u7406\u9884\u7b97\u7684\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u91cd\u590d\u601d\u7ef4\u6a21\u5f0f\uff0c\u51cf\u5c11\u8fd9\u4e9b\u91cd\u590d\u5f80\u5f80\u4ee5\u6027\u80fd\u4e0b\u964d\u4e3a\u4ee3\u4ef7\uff0c\u9700\u8981\u514b\u670d\u8fd9\u79cd\u6743\u8861\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u8bad\u7ec3\u673a\u5236\uff0c\u5728\u538b\u7f29\u9636\u6bb5\u9650\u5236\u63a8\u7406\u6b65\u957f\u8feb\u4f7f\u6a21\u578b\u505a\u51fa\u7cbe\u786e\u51b3\u7b56\uff0c\u5728\u6269\u5c55\u9636\u6bb5\u653e\u5bbd\u9650\u5236\u8ba9\u6a21\u578b\u63a2\u7d22\u957f\u89c6\u91ce\u63a8\u7406\u3002", "result": "\u5728DeepSeek-R1-Distill-Qwen-1.5B\u4e0a\u8bad\u7ec3\uff0cSIRI-low\u5728AIME24\u4e0a\u6027\u80fd\u63d0\u534743.2%\u540c\u65f6\u51cf\u5c1146.9%token\u4f7f\u7528\uff0cSIRI-high\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5468\u671f\u6027\u5730\u8c03\u6574\u63a8\u7406\u6a21\u578b\u7684\u8f93\u51fa\u622a\u65ad\u957f\u5ea6\uff0c\u53ef\u4ee5\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u548c\u6548\u7387\uff0c\u6536\u655b\u5230\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6700\u4f18\u5e73\u8861\u70b9\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24804", "abs": "https://arxiv.org/abs/2509.24804", "authors": ["Boxuan Zhang", "Runqing Wang", "Wei Xiao", "Weipu Zhang", "Jian Sun", "Gao Huang", "Jie Chen", "Gang Wang"], "title": "DyMoDreamer: World Modeling with Dynamic Modulation", "comment": null, "summary": "A critical bottleneck in deep reinforcement learning (DRL) is sample\ninefficiency, as training high-performance agents often demands extensive\nenvironmental interactions. Model-based reinforcement learning (MBRL) mitigates\nthis by building world models that simulate environmental dynamics and generate\nsynthetic experience, improving sample efficiency. However, conventional world\nmodels process observations holistically, failing to decouple dynamic objects\nand temporal features from static backgrounds. This approach is computationally\ninefficient, especially for visual tasks where dynamic objects significantly\ninfluence rewards and decision-making performance. To address this, we\nintroduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic\nmodulation mechanism to improve the extraction of dynamic features and enrich\nthe temporal information. DyMoDreamer employs differential observations derived\nfrom a novel inter-frame differencing mask, explicitly encoding object-level\nmotion cues and temporal dynamics. Dynamic modulation is modeled as stochastic\ncategorical distributions and integrated into a recurrent state-space model\n(RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments\ndemonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k\nbenchmark with a $156.6$\\% mean human-normalized score, establishes a new\nrecord of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\\%\nperformance improvement after $1$M steps on the Crafter benchmark. Our code is\nreleased at https://github.com/Ultraman-Tiga1/DyMoDreamer.", "AI": {"tldr": "DyMoDreamer\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u673a\u5236\u6539\u8fdb\u52a8\u6001\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u4fe1\u606f\u4e30\u5bcc\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6837\u672c\u6548\u7387\u4f4e\u95ee\u9898\uff0c\u4f20\u7edf\u4e16\u754c\u6a21\u578b\u65e0\u6cd5\u89e3\u8026\u52a8\u6001\u5bf9\u8c61\u548c\u65f6\u95f4\u7279\u5f81\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u52a8\u6001\u5bf9\u8c61\u5bf9\u5956\u52b1\u548c\u51b3\u7b56\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002", "method": "\u5f15\u5165\u52a8\u6001\u8c03\u5236\u673a\u5236\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u5e27\u95f4\u5dee\u5206\u63a9\u7801\u751f\u6210\u5dee\u5206\u89c2\u6d4b\uff0c\u663e\u5f0f\u7f16\u7801\u5bf9\u8c61\u7ea7\u8fd0\u52a8\u7ebf\u7d22\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u5c06\u52a8\u6001\u8c03\u5236\u5efa\u6a21\u4e3a\u968f\u673a\u5206\u7c7b\u5206\u5e03\u5e76\u96c6\u6210\u5230\u5faa\u73af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u3002", "result": "\u5728Atari 100k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230156.6%\u7684\u5e73\u5747\u4eba\u7c7b\u6807\u51c6\u5316\u5206\u6570\uff0c\u5728DeepMind\u89c6\u89c9\u63a7\u5236\u5957\u4ef6\u4e2d\u521b\u4e0b832\u7684\u65b0\u8bb0\u5f55\uff0c\u5728Crafter\u57fa\u51c6\u6d4b\u8bd5\u4e2d1M\u6b65\u540e\u6027\u80fd\u63d0\u53479.5%\u3002", "conclusion": "DyMoDreamer\u901a\u8fc7\u52a8\u6001\u8c03\u5236\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24947", "abs": "https://arxiv.org/abs/2509.24947", "authors": ["Sooraj Sathish", "Keshav Goyal", "Raghuram Bharadwaj Diddigi"], "title": "Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer", "comment": null, "summary": "Deep Reinforcement Learning (RL) has demonstrated success in solving complex\nsequential decision-making problems by integrating neural networks with the RL\nframework. However, training deep RL models poses several challenges, such as\nthe need for extensive hyperparameter tuning and high computational costs.\nTransfer learning has emerged as a promising strategy to address these\nchallenges by enabling the reuse of knowledge from previously learned tasks for\nnew, related tasks. This avoids the need for retraining models entirely from\nscratch. A commonly used approach for transfer learning in RL is to leverage\nthe internal representations learned by the neural network during training.\nSpecifically, the activations from the last hidden layer can be viewed as\nrefined state representations that encapsulate the essential features of the\ninput. In this work, we investigate whether these representations can be used\nas input for training simpler models, such as linear function approximators, on\nnew tasks. We observe that the representations learned by standard deep RL\nmodels can be highly correlated, which limits their effectiveness when used\nwith linear function approximation. To mitigate this problem, we propose a\nnovel deep Q-learning approach that introduces a regularization term to reduce\npositive correlations between feature representation of states. By leveraging\nthese reduced correlated features, we enable more effective use of linear\nfunction approximation in transfer learning. Through experiments and ablation\nstudies on standard RL benchmarks and MinAtar games, we demonstrate the\nefficacy of our approach in improving transfer learning performance and thereby\nreducing computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6Q\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u51cf\u5c11\u72b6\u6001\u7279\u5f81\u8868\u793a\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u6027\uff0c\u4ece\u800c\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u66f4\u6709\u6548\u5730\u4f7f\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u91cd\u7528\u5df2\u5b66\u77e5\u8bc6\u6765\u907f\u514d\u4ece\u5934\u8bad\u7ec3\u3002\u4f46\u6807\u51c6\u6df1\u5ea6RL\u6a21\u578b\u5b66\u4e60\u5230\u7684\u8868\u793a\u9ad8\u5ea6\u76f8\u5173\uff0c\u9650\u5236\u4e86\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6548\u679c\u3002", "method": "\u5728\u6df1\u5ea6Q\u5b66\u4e60\u4e2d\u5f15\u5165\u6b63\u5219\u5316\u9879\u6765\u51cf\u5c11\u72b6\u6001\u7279\u5f81\u8868\u793a\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u6027\uff0c\u5229\u7528\u8fd9\u4e9b\u53bb\u76f8\u5173\u7279\u5f81\u6765\u6539\u8fdb\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5728\u6807\u51c6RL\u57fa\u51c6\u548cMinAtar\u6e38\u620f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u51cf\u5c11\u7279\u5f81\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2509.24957", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.24957", "abs": "https://arxiv.org/abs/2509.24957", "authors": ["Weifan Jiang", "Rana Shahout", "Yilun Du", "Michael Mitzenmacher", "Minlan Yu"], "title": "Intra-request branch orchestration for efficient LLM reasoning", "comment": "15 pages, 6 figures", "summary": "Large Language Models (LLMs) increasingly rely on inference-time reasoning\nalgorithms such as chain-of-thought and multi-branch reasoning to improve\naccuracy on complex tasks. These methods, however, substantially increase token\nusage and per-request latency. Prior work has largely focused on reducing token\nusage, often at the expense of accuracy, while overlooking other latency\nfactors. We present DUCHESS, an LLM serving system that reduces cost and\nlatency without sacrificing accuracy through intra-request branch orchestration\nguided by predictions. DUCHESS employs a lightweight linear probing model over\nLLM layer activations to estimate branch correctness, and its orchestration\npolicy decides whether to terminate, duplicate, or continue a branch. When\nhandling multiple requests, DUCHESS further reduces latency by prioritizing\neasier reasoning tasks when complexity can be estimated from the prompt.\nExperiments on three reasoning benchmarks show that DUCHESS consistently\nimproves the token-accuracy Pareto frontier, reducing token usage by 42-63% at\nmatched accuracy compared to self-consistency. In serving with vLLM, DUCHESS\nreduces mean, median, and tail latencies by 57-81%, 58-85%, and 52-84% with\nFirst-Come-First-Served scheduling, and achieves additional gains under\ndifficulty-aware scheduling at higher request rates.", "AI": {"tldr": "DUCHESS\u662f\u4e00\u4e2aLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u652f\u7f16\u6392\u548c\u96be\u5ea6\u611f\u77e5\u8c03\u5ea6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548ctoken\u4f7f\u7528\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u65f6\u7b97\u6cd5\uff08\u5982\u601d\u7ef4\u94fe\u548c\u591a\u5206\u652f\u63a8\u7406\uff09\u867d\u7136\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u663e\u8457\u589e\u52a0\u4e86token\u4f7f\u7528\u548c\u5ef6\u8fdf\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11token\u4f7f\u7528\uff0c\u5f80\u5f80\u4ee5\u727a\u7272\u51c6\u786e\u6027\u4e3a\u4ee3\u4ef7\uff0c\u4e14\u5ffd\u7565\u4e86\u5176\u4ed6\u5ef6\u8fdf\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u63a2\u6d4b\u6a21\u578b\u57fa\u4e8eLLM\u5c42\u6fc0\u6d3b\u6765\u4f30\u8ba1\u5206\u652f\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u7f16\u6392\u7b56\u7565\u51b3\u5b9a\u5206\u652f\u7684\u7ec8\u6b62\u3001\u590d\u5236\u6216\u7ee7\u7eed\u3002\u5728\u591a\u8bf7\u6c42\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u8c03\u5ea6\u4f18\u5148\u5904\u7406\u66f4\u7b80\u5355\u7684\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDUCHESS\u5c06token\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8642-63%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u5728vLLM\u670d\u52a1\u4e2d\uff0c\u5e73\u5747\u3001\u4e2d\u4f4d\u6570\u548c\u5c3e\u90e8\u5ef6\u8fdf\u5206\u522b\u964d\u4f4e\u4e8657-81%\u300158-85%\u548c52-84%\u3002", "conclusion": "DUCHESS\u901a\u8fc7\u667a\u80fd\u5206\u652f\u7f16\u6392\u548c\u96be\u5ea6\u611f\u77e5\u8c03\u5ea6\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u670d\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2509.24981", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.24981", "abs": "https://arxiv.org/abs/2509.24981", "authors": ["Haoran He", "Yuxiao Ye", "Qingpeng Cai", "Chen Hu", "Binxing Jiao", "Daxin Jiang", "Ling Pan"], "title": "Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards", "comment": "32 pages", "summary": "RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for\nimproving the reasoning abilities of large language models (LLMs). Current\nmethods rely primarily on policy optimization frameworks like PPO and GRPO,\nwhich follow generalized policy iteration that alternates between evaluating\nthe current policy's value and improving the policy based on evaluation. While\neffective, they often suffer from training instability and diversity collapse,\nrequiring complex heuristic tricks and careful tuning. We observe that standard\nRLVR in math reasoning can be formalized as a specialized finite-horizon Markov\nDecision Process with deterministic state transitions, tree-structured\ndynamics, and binary terminal rewards. Though large in scale, the underlying\nstructure is simpler than general-purpose control settings for which popular RL\nalgorithms (e.g., PPO) were developed, suggesting that several sophisticated\ntechniques in existing methods may be reduced or even omitted. Based on this\ninsight, we prove a surprising result: the optimal action can be recovered from\nthe Q-function of a fixed uniformly random policy, thereby bypassing the\ngeneralized policy iteration loop and its associated heuristics. We introduce\nRandom Policy Valuation for Diverse Reasoning (ROVER) to translate this\nprinciple into a practical and scalable algorithm for LLM math reasoning, a\nminimalist yet highly effective RL method that samples actions from a softmax\nover these uniform-policy Q-values. ROVER preserves diversity throughout\ntraining, allowing sustained exploration of multiple valid pathways. Across\nmultiple base models and standard math reasoning benchmarks, ROVER demonstrates\nsuperior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1,\n\\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite\nits radical simplification compared to strong, complicated existing methods.", "AI": {"tldr": "\u63d0\u51faROVER\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u7b56\u7565\u8bc4\u4f30\u5b9e\u73b0\u591a\u6837\u5316\u63a8\u7406\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u590d\u6742RL\u65b9\u6cd5", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56PPO\u7b49\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u590d\u6742\u542f\u53d1\u5f0f\u6280\u5de7\u548c\u7cbe\u7ec6\u8c03\u53c2", "method": "\u57fa\u4e8e\u786e\u5b9a\u6027\u72b6\u6001\u8f6c\u79fb\u3001\u6811\u72b6\u52a8\u6001\u548c\u4e8c\u5143\u7ec8\u7aef\u5956\u52b1\u7684MDP\u7ed3\u6784\uff0c\u8bc1\u660e\u6700\u4f18\u52a8\u4f5c\u53ef\u4ece\u5747\u5300\u968f\u673a\u7b56\u7565\u7684Q\u51fd\u6570\u4e2d\u6062\u590d\uff0c\u63d0\u51faROVER\u7b97\u6cd5\u4f7f\u7528\u5747\u5300\u7b56\u7565Q\u503c\u7684softmax\u91c7\u6837\u52a8\u4f5c", "result": "\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cROVER\u5728\u8d28\u91cf\uff08pass@1 +8.2\uff0cpass@256 +16.8\uff09\u548c\u591a\u6837\u6027\uff08+17.6%\uff09\u65b9\u9762\u8868\u73b0\u4f18\u5f02", "conclusion": "ROVER\u5c55\u793a\u4e86\u5728\u6570\u5b66\u63a8\u7406\u7b49\u7ed3\u6784\u5316\u4efb\u52a1\u4e2d\uff0c\u53ef\u4ee5\u5927\u5e45\u7b80\u5316RL\u65b9\u6cd5\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\uff0c\u4e3aRLVR\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "topic": "agentic reinforcement learning"}}
{"id": "2509.25050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25050", "abs": "https://arxiv.org/abs/2509.25050", "authors": ["Shuchen Xue", "Chongjian Ge", "Shilong Zhang", "Yichen Li", "Zhi-Ming Ma"], "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion Models", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86Advantage Weighted Matching (AWM)\u65b9\u6cd5\uff0c\u7edf\u4e00\u6269\u6563\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u4f18\u52bf\u52a0\u6743\u964d\u4f4e\u65b9\u5dee\u5e76\u52a0\u901f\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5982DDPO\uff09\u4f7f\u7528\u4e0e\u9884\u8bad\u7ec3\u4e0d\u540c\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5bfc\u81f4\u65b9\u5dee\u589e\u52a0\u548c\u6536\u655b\u7f13\u6162\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u9884\u8bad\u7ec3\u548cRL\u76ee\u6807\u3002", "method": "AWM\u4f7f\u7528\u4e0e\u9884\u8bad\u7ec3\u76f8\u540c\u7684score/flow\u5339\u914d\u635f\u5931\uff0c\u4f46\u901a\u8fc7\u4f18\u52bf\u51fd\u6570\u5bf9\u6837\u672c\u8fdb\u884c\u52a0\u6743\uff0c\u63d0\u5347\u9ad8\u5956\u52b1\u6837\u672c\u7684\u5f71\u54cd\uff0c\u6291\u5236\u4f4e\u5956\u52b1\u6837\u672c\u3002", "result": "\u5728GenEval\u3001OCR\u548cPickScore\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAWM\u76f8\u6bd4Flow-GRPO\u5b9e\u73b0\u4e86\u9ad8\u8fbe24\u500d\u7684\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "AWM\u901a\u8fc7\u7edf\u4e00\u9884\u8bad\u7ec3\u548cRL\u76ee\u6807\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2509.66d5c155", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.theregister.com%2Ffeed%2Fwww.theregister.com%2F2025%2F09%2F24%2Fciti_pilots_agentic_ai%2F%3Futm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/Ncy3uDb-VtjrqVeooorxTQp6_Mz-XKHlny2ylCRoLT4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.theregister.com%2Ffeed%2Fwww.theregister.com%2F2025%2F09%2F24%2Fciti_pilots_agentic_ai%2F%3Futm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/Ncy3uDb-VtjrqVeooorxTQp6_Mz-XKHlny2ylCRoLT4=424", "authors": ["TLDR Newsletter"], "title": "US banking giant Citi pilots agentic AI with 5,000 staff", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgo.theregister.com%2Ffeed%2Fwww.theregister.com%2F2025%2F09%2F24%2Fciti_pilots_agentic_ai%2F%3Futm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/Ncy3uDb-VtjrqVeooorxTQp6_Mz-XKHlny2ylCRoLT4=424", "summary": "US banking giant Citi pilots agentic AI with 5,000 staff (3 minute read) Citi is piloting upgraded Stylus Workspaces with agentic AI capabilities across 5,000 employees for up to six weeks, enabling automated research, customer profiling, and multi-stage workflow automation using various AI models, including those from Gemini to Claude. The bank's CTO acknowledged that the technology could reduce staffing needs while boosting productivity, although concerns remain about the 30-35% task succes...", "source": "tldr", "AI": {"tldr": "\u82b1\u65d7\u94f6\u884c\u6b63\u57285000\u540d\u5458\u5de5\u4e2d\u8bd5\u70b9\u5177\u6709\u667a\u80fd\u4ee3\u7406\u529f\u80fd\u7684\u5347\u7ea7\u7248Stylus Workspaces\uff0c\u4f7f\u7528\u591a\u79cdAI\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u7814\u7a76\u3001\u5ba2\u6237\u753b\u50cf\u548c\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u3002", "motivation": "\u901a\u8fc7\u90e8\u7f72\u667a\u80fd\u4ee3\u7406AI\u6280\u672f\u6765\u63d0\u9ad8\u94f6\u884c\u5458\u5de5\u7684\u751f\u4ea7\u529b\uff0c\u540c\u65f6\u63a2\u7d22\u8be5\u6280\u672f\u5bf9\u4eba\u5458\u9700\u6c42\u7684\u5f71\u54cd\u3002", "method": "\u57285000\u540d\u5458\u5de5\u4e2d\u8fdb\u884c\u4e3a\u671f\u516d\u5468\u7684\u8bd5\u70b9\uff0c\u4f7f\u7528\u5347\u7ea7\u7248Stylus Workspaces\u5e73\u53f0\uff0c\u6574\u5408Gemini\u5230Claude\u7b49\u591a\u79cdAI\u6a21\u578b\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u7814\u7a76\u3001\u5ba2\u6237\u753b\u50cf\u548c\u591a\u9636\u6bb5\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u3002", "result": "\u8bd5\u70b9\u6b63\u5728\u8fdb\u884c\u4e2d\uff0c\u82b1\u65d7CTO\u627f\u8ba4\u8be5\u6280\u672f\u53ef\u80fd\u51cf\u5c11\u4eba\u5458\u9700\u6c42\u540c\u65f6\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u4efb\u52a1\u6210\u529f\u7387\u4ecd\u670930-35%\u7684\u62c5\u5fe7\u3002", "conclusion": "\u82b1\u65d7\u94f6\u884c\u6b63\u5728\u79ef\u6781\u6d4b\u8bd5\u667a\u80fd\u4ee3\u7406AI\u5728\u94f6\u884c\u4e1a\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u6280\u672f\u6210\u719f\u5ea6\u4ecd\u9700\u9a8c\u8bc1\u3002", "topic": "swe application"}}
{"id": "tldr.2509.4928fbf3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=Un9a58ciuEI%26utm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/KdPnuTI2dsCtVr57nrlH1NNWvYy0eETIycbTWgKPjdU=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=Un9a58ciuEI%26utm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/KdPnuTI2dsCtVr57nrlH1NNWvYy0eETIycbTWgKPjdU=424", "authors": ["TLDR Newsletter"], "title": "Coding With AI", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=Un9a58ciuEI%26utm_source=tldrinfosec/1/0100019986229d41-b27d6c42-a34d-452b-88af-c87df7150522-000000/KdPnuTI2dsCtVr57nrlH1NNWvYy0eETIycbTWgKPjdU=424", "summary": "Coding With AI (Sponsor) AI coding tools can supercharge your devs\u2026or your security nightmares. Chelle Saunders, Product Manager at Secure Code Warrior, shares LLM insights, prompt engineering tips, and tool strengths. Watch now", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7801\u5de5\u5177\u53ef\u4ee5\u63d0\u5347\u5f00\u53d1\u6548\u7387\u4f46\u4e5f\u5e26\u6765\u5b89\u5168\u98ce\u9669\uff0cSecure Code Warrior\u7684\u4ea7\u54c1\u7ecf\u7406\u5206\u4eab\u4e86LLM\u6d1e\u5bdf\u3001\u63d0\u793a\u5de5\u7a0b\u6280\u5de7\u548c\u5de5\u5177\u4f18\u52bf", "motivation": "\u63a2\u8ba8AI\u7f16\u7801\u5de5\u5177\u5728\u63d0\u5347\u5f00\u53d1\u6548\u7387\u7684\u540c\u65f6\u5982\u4f55\u5e94\u5bf9\u5b89\u5168\u6311\u6218\uff0c\u5206\u4eab\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6700\u4f73\u5b9e\u8df5", "method": "\u901a\u8fc7Secure Code Warrior\u7684\u4ea7\u54c1\u7ecf\u9a8c\uff0c\u5206\u6790LLM\u5728\u7f16\u7801\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u63d0\u793a\u5de5\u7a0b\u6280\u5de7\u548c\u5de5\u5177\u8bc4\u4f30", "result": "\u5206\u4eab\u4e86AI\u7f16\u7801\u5de5\u5177\u7684\u5b9e\u9645\u5e94\u7528\u6d1e\u5bdf\uff0c\u5305\u62ec\u5b89\u5168\u98ce\u9669\u8bc6\u522b\u548c\u6548\u7387\u63d0\u5347\u65b9\u6cd5", "conclusion": "AI\u7f16\u7801\u5de5\u5177\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6b63\u786e\u4f7f\u7528\u548c\u5b89\u5168\u5b9e\u8df5\u6765\u907f\u514d\u5b89\u5168\u5669\u68a6", "topic": "swe application"}}
{"id": "tldr.2509.9ffe092f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tensorzero.com%2Fblog%2Fis-openai-reinforcement-fine-tuning-rft-worth-it%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/aV_mfuDZJQXKSD22OG-Hrz-uAvl12ytCLmUpmx1dKd4=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tensorzero.com%2Fblog%2Fis-openai-reinforcement-fine-tuning-rft-worth-it%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/aV_mfuDZJQXKSD22OG-Hrz-uAvl12ytCLmUpmx1dKd4=424", "authors": ["TLDR Newsletter"], "title": "Is OpenAI's Reinforcement Fine-Tuning Worth It?", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 32 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tensorzero.com%2Fblog%2Fis-openai-reinforcement-fine-tuning-rft-worth-it%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/aV_mfuDZJQXKSD22OG-Hrz-uAvl12ytCLmUpmx1dKd4=424", "summary": "Is OpenAI's Reinforcement Fine-Tuning (RFT) Worth It? (32 minute read) OpenAI's reinforcement fine-tuning (RFT) for o4-mini is supposed to be able to train models to get better at specific tasks using reinforcement learning. It costs up to 700 times more than supervised fine-tuning but only seems to deliver clear wins on agentic coding tasks. The technology gives engineers flexibility in reward design through flexible grader configurations, and the performance gains can be substantial when it...", "source": "tldr", "AI": {"tldr": "OpenAI\u7684\u5f3a\u5316\u5fae\u8c03(RFT)\u6210\u672c\u6bd4\u76d1\u7763\u5fae\u8c03\u9ad8700\u500d\uff0c\u4f46\u4ec5\u5728\u667a\u80fd\u4f53\u7f16\u7801\u4efb\u52a1\u4e0a\u8868\u73b0\u660e\u663e\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5956\u52b1\u8bbe\u8ba1\u914d\u7f6e\u3002", "motivation": "\u8bc4\u4f30OpenAI\u5f3a\u5316\u5fae\u8c03(RFT)\u6280\u672f\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u5206\u6790\u5176\u9ad8\u6602\u6210\u672c\u662f\u5426\u503c\u5f97\uff0c\u4ee5\u53ca\u5728\u54ea\u4e9b\u4efb\u52a1\u4e0a\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4RFT\u4e0e\u76d1\u7763\u5fae\u8c03\u7684\u6210\u672c\u6548\u76ca\uff0c\u5206\u6790RFT\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u7279\u522b\u5173\u6ce8\u667a\u80fd\u4f53\u7f16\u7801\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "RFT\u5728\u667a\u80fd\u4f53\u7f16\u7801\u4efb\u52a1\u4e0a\u80fd\u5e26\u6765\u5b9e\u8d28\u6027\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u7c7b\u578b\u4e0a\u4f18\u52bf\u4e0d\u660e\u663e\uff0c\u6210\u672c\u6548\u76ca\u6bd4\u9700\u8981\u4ed4\u7ec6\u8bc4\u4f30\u3002", "conclusion": "RFT\u6280\u672f\u867d\u7136\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df(\u5982\u667a\u80fd\u4f53\u7f16\u7801)\u786e\u5b9e\u80fd\u5e26\u6765\u663e\u8457\u6027\u80fd\u6539\u8fdb\uff0c\u5de5\u7a0b\u5e08\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u6743\u8861\u6210\u672c\u4e0e\u6536\u76ca\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.68bd7411", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fgemini-robotics-15-brings-ai-agents-into-the-physical-world%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/HgAVovTFT196180Q89dMwmPeiMgYPu3mIRj16TiYUMg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fgemini-robotics-15-brings-ai-agents-into-the-physical-world%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/HgAVovTFT196180Q89dMwmPeiMgYPu3mIRj16TiYUMg=424", "authors": ["TLDR Newsletter"], "title": "Gemini Robotics 1.5 brings AI agents into the physical world", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fgemini-robotics-15-brings-ai-agents-into-the-physical-world%2F%3Futm_source=tldrai/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/HgAVovTFT196180Q89dMwmPeiMgYPu3mIRj16TiYUMg=424", "summary": "Gemini Robotics 1.5 brings AI agents into the physical world (15 minute read) Google DeepMind has released two models that unlock agentic experiences with advanced thinking. Gemini Robotics 1.5 is a vision-language-action model that turns visual information and instructions into motor commands for a robot to perform a task. The model thinks before taking actions and shows its process and also learns across embodiments. Gemini Robotics-ER 1.5 is a vision-language model that reasons about the p...", "source": "tldr", "AI": {"tldr": "Google DeepMind\u53d1\u5e03Gemini Robotics 1.5\u548cGemini Robotics-ER 1.5\u4e24\u4e2a\u6a21\u578b\uff0c\u5c06AI\u4ee3\u7406\u5e26\u5165\u7269\u7406\u4e16\u754c\uff0c\u5b9e\u73b0\u89c6\u89c9\u4fe1\u606f\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u8f6c\u6362\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c06AI\u4ee3\u7406\u4ece\u6570\u5b57\u4e16\u754c\u6269\u5c55\u5230\u7269\u7406\u4e16\u754c\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u7406\u89e3\u89c6\u89c9\u4fe1\u606f\u548c\u6307\u4ee4\u5e76\u6267\u884c\u4efb\u52a1\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08Gemini Robotics 1.5\uff09\u5c06\u89c6\u89c9\u4fe1\u606f\u548c\u6307\u4ee4\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u547d\u4ee4\uff0c\u4ee5\u53ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08Gemini Robotics-ER 1.5\uff09\u8fdb\u884c\u63a8\u7406\uff0c\u6a21\u578b\u5177\u6709\u601d\u8003\u540e\u518d\u884c\u52a8\u7684\u80fd\u529b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5c06\u89c6\u89c9\u8f93\u5165\u548c\u6307\u4ee4\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u52a8\u4f5c\u547d\u4ee4\uff0c\u5c55\u793a\u601d\u8003\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u8de8\u5177\u8eab\u5b66\u4e60\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u578b\u4e3aAI\u4ee3\u7406\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4ece\u611f\u77e5\u5230\u884c\u52a8\u7684\u5b8c\u6574\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "tldr.2509.c5a3e5c6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fnova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%3Futm_campaign=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_medium=employer-brand%26utm_source=newsletter%26utm_content=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_term=2025-september/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/q1Q98KSHTM2OGwHnmfT_tdhCE_e5bUnL2EyPCZ-qyWw=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fnova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%3Futm_campaign=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_medium=employer-brand%26utm_source=newsletter%26utm_content=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_term=2025-september/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/q1Q98KSHTM2OGwHnmfT_tdhCE_e5bUnL2EyPCZ-qyWw=424", "authors": ["TLDR Newsletter"], "title": "Build and test AI agents without leaving your IDE", "comment": "Source: TLDR Newsletter, Date: 2025-09-26, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flabs.amazon.science%2Fblog%2Fnova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%3Futm_campaign=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_medium=employer-brand%26utm_source=newsletter%26utm_content=nova-act-extension-build-and-test-ai-agents-without-leaving-your-ide%26utm_term=2025-september/1/010001998641b019-12c28e95-1c78-45c0-98ab-fe4f71dfe793-000000/q1Q98KSHTM2OGwHnmfT_tdhCE_e5bUnL2EyPCZ-qyWw=424", "summary": "Build and test AI agents without leaving your IDE (Sponsor) Amazon's Nova Act extension brings AI agent development directly into IDEs like Cursor, VS Code, and Kiro. Generate scripts via chat, test cell-by-cell, and debug with live logs\u2014cutting development time from days to minutes.Transform your workflow \u2192", "source": "tldr", "AI": {"tldr": "Amazon Nova Act\u6269\u5c55\u5c06AI\u4ee3\u7406\u5f00\u53d1\u76f4\u63a5\u96c6\u6210\u5230IDE\u4e2d\uff0c\u901a\u8fc7\u804a\u5929\u751f\u6210\u811a\u672c\u3001\u9010\u5355\u5143\u6d4b\u8bd5\u548c\u5b9e\u65f6\u65e5\u5fd7\u8c03\u8bd5\uff0c\u5c06\u5f00\u53d1\u65f6\u95f4\u4ece\u51e0\u5929\u7f29\u77ed\u5230\u51e0\u5206\u949f\u3002", "motivation": "\u7b80\u5316AI\u4ee3\u7406\u5f00\u53d1\u6d41\u7a0b\uff0c\u8ba9\u5f00\u53d1\u8005\u65e0\u9700\u79bb\u5f00IDE\u73af\u5883\u5c31\u80fd\u5b8c\u6210\u6784\u5efa\u548c\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u3002", "method": "\u5728Cursor\u3001VS Code\u548cKiro\u7b49IDE\u4e2d\u96c6\u6210AI\u4ee3\u7406\u5f00\u53d1\u529f\u80fd\uff0c\u652f\u6301\u901a\u8fc7\u804a\u5929\u751f\u6210\u811a\u672c\u3001\u9010\u5355\u5143\u6d4b\u8bd5\u548c\u5b9e\u65f6\u65e5\u5fd7\u8c03\u8bd5\u3002", "result": "\u663e\u8457\u51cf\u5c11AI\u4ee3\u7406\u5f00\u53d1\u65f6\u95f4\uff0c\u4ece\u51e0\u5929\u7f29\u77ed\u5230\u51e0\u5206\u949f\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "IDE\u96c6\u6210\u5f0fAI\u4ee3\u7406\u5f00\u53d1\u5de5\u5177\u80fd\u591f\u6781\u5927\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u751f\u4ea7\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2509.b12fc7c9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcode-mode%2F%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/gAr8JsDCsAPqNIZhJHE7hMbrBrXB6wtW8vAl2xNe4Co=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcode-mode%2F%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/gAr8JsDCsAPqNIZhJHE7hMbrBrXB6wtW8vAl2xNe4Co=424", "authors": ["TLDR Newsletter"], "title": "Code Mode: the better way to use MCP", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.cloudflare.com%2Fcode-mode%2F%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/gAr8JsDCsAPqNIZhJHE7hMbrBrXB6wtW8vAl2xNe4Co=424", "summary": "Code Mode: the better way to use MCP (10 minute read) AI agents can handle more tools and more complex tools by converting Model Context Protocol (MCP) tools into a TypeScript API and having LLMs write code to call that API. This approach uses the Cloudflare Workers platform and its new Worker Loader API, which loads Worker code on-demand with a secure sandbox that prohibits internet access. It isolates access to MCP servers through bindings and prevents the AI from leaking API keys.", "source": "tldr", "AI": {"tldr": "\u901a\u8fc7\u5c06MCP\u5de5\u5177\u8f6c\u6362\u4e3aTypeScript API\u5e76\u8ba9LLM\u7f16\u5199\u4ee3\u7801\u8c03\u7528\u8be5API\uff0cAI\u4ee3\u7406\u53ef\u4ee5\u5904\u7406\u66f4\u591a\u5de5\u5177\u548c\u66f4\u590d\u6742\u7684\u5de5\u5177\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528Cloudflare Workers\u5e73\u53f0\u53ca\u5176\u65b0\u7684Worker Loader API\uff0c\u5728\u5b89\u5168\u6c99\u7bb1\u4e2d\u6309\u9700\u52a0\u8f7dWorker\u4ee3\u7801\u3002", "motivation": "\u4f7fAI\u4ee3\u7406\u80fd\u591f\u5904\u7406\u66f4\u591a\u5de5\u5177\u548c\u66f4\u590d\u6742\u5de5\u5177\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u548cAPI\u5bc6\u94a5\u4fdd\u62a4\u3002", "method": "\u5c06Model Context Protocol\u5de5\u5177\u8f6c\u6362\u4e3aTypeScript API\uff0c\u5229\u7528Cloudflare Workers\u5e73\u53f0\u548cWorker Loader API\u5728\u5b89\u5168\u6c99\u7bb1\u4e2d\u6309\u9700\u52a0\u8f7d\u4ee3\u7801\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86AI\u4ee3\u7406\u5bf9\u66f4\u591a\u5de5\u5177\u7684\u652f\u6301\uff0c\u540c\u65f6\u901a\u8fc7\u9694\u79bbMCP\u670d\u52a1\u5668\u8bbf\u95ee\u548c\u9632\u6b62API\u5bc6\u94a5\u6cc4\u9732\u786e\u4fdd\u4e86\u5b89\u5168\u6027\u3002", "conclusion": "Code Mode\u65b9\u6cd5\u4e3aAI\u4ee3\u7406\u4f7f\u7528MCP\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u66f4\u5f3a\u5927\u7684\u65b9\u5f0f\u3002", "topic": "code agent"}}
{"id": "tldr.2509.be6b72e9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2Fai%2F%3Futm_source=tldrdevops%26utm_medium=tldrdevops%26utm_campaign=tldrdevops_sept29/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/xfj4z0dN21fHda7GLpeZs68PAnr1LjJVrnQSgUz3lyg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2Fai%2F%3Futm_source=tldrdevops%26utm_medium=tldrdevops%26utm_campaign=tldrdevops_sept29/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/xfj4z0dN21fHda7GLpeZs68PAnr1LjJVrnQSgUz3lyg=424", "authors": ["TLDR Newsletter"], "title": "AI writes code in seconds. mirrord tackles the new bottleneck: testing.", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmetalbear.com%2Fmirrord%2Fai%2F%3Futm_source=tldrdevops%26utm_medium=tldrdevops%26utm_campaign=tldrdevops_sept29/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/xfj4z0dN21fHda7GLpeZs68PAnr1LjJVrnQSgUz3lyg=424", "summary": "AI writes code in seconds. mirrord tackles the new bottleneck: testing. (Sponsor) AI agents can generate new code instantly, but testing it against cloud resources such as other microservices, databases, and queues still takes too long. mirrord lets AI-generated code run locally with seamless access to your live staging environment. No mocks, no waiting, just instant integration testing. Try mirrord for free", "source": "tldr", "AI": {"tldr": "mirrord\u89e3\u51b3\u4e86AI\u751f\u6210\u4ee3\u7801\u7684\u6d4b\u8bd5\u74f6\u9888\uff0c\u8ba9\u672c\u5730\u4ee3\u7801\u53ef\u4ee5\u65e0\u7f1d\u8bbf\u95ee\u5b9e\u65f6\u6d4b\u8bd5\u73af\u5883\uff0c\u65e0\u9700\u6a21\u62df\u7b49\u5f85", "motivation": "AI\u80fd\u5feb\u901f\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u6d4b\u8bd5\u4ee3\u7801\u4e0e\u4e91\u8d44\u6e90\u7684\u96c6\u6210\u4ecd\u7136\u8017\u65f6\uff0c\u9700\u8981\u66f4\u5feb\u7684\u96c6\u6210\u6d4b\u8bd5\u65b9\u6cd5", "method": "\u5f00\u53d1mirrord\u5de5\u5177\uff0c\u8ba9\u672c\u5730\u8fd0\u884c\u7684AI\u751f\u6210\u4ee3\u7801\u80fd\u591f\u76f4\u63a5\u8bbf\u95ee\u5b9e\u65f6\u6d4b\u8bd5\u73af\u5883\uff0c\u65e0\u9700\u6a21\u62df\u6216\u7b49\u5f85", "result": "\u5b9e\u73b0\u4e86\u5373\u65f6\u96c6\u6210\u6d4b\u8bd5\uff0c\u5927\u5e45\u7f29\u77ed\u4e86\u6d4b\u8bd5\u65f6\u95f4", "conclusion": "mirrord\u6709\u6548\u89e3\u51b3\u4e86AI\u4ee3\u7801\u751f\u6210\u7684\u6d4b\u8bd5\u74f6\u9888\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u96c6\u6210\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848", "topic": "swe application"}}
{"id": "tldr.2509.a55a12c9", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/8SnYMmWV7HarePi9UFreJbb-xx1YgDTRbWwrKUJkxeQ=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/8SnYMmWV7HarePi9UFreJbb-xx1YgDTRbWwrKUJkxeQ=424", "authors": ["TLDR Newsletter"], "title": "Coze Studio", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcoze-dev%2Fcoze-studio%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/8SnYMmWV7HarePi9UFreJbb-xx1YgDTRbWwrKUJkxeQ=424", "summary": "Coze Studio (GitHub Repo) Coze Studio, an all-in-one AI agent development tool derived from the \"Coze Development Platform,\" has been released as open source. The platform simplifies AI agent creation, debugging, and deployment with visual tools, no-code or low-code approaches, and a microservices architecture built on Golang, React, and Typescript.", "source": "tldr", "AI": {"tldr": "Coze Studio\u662f\u4e00\u4e2a\u5f00\u6e90\u7684AI\u667a\u80fd\u4f53\u5f00\u53d1\u5de5\u5177\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u754c\u9762\u3001\u65e0\u4ee3\u7801/\u4f4e\u4ee3\u7801\u65b9\u6cd5\u548c\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u7b80\u5316AI\u667a\u80fd\u4f53\u7684\u521b\u5efa\u3001\u8c03\u8bd5\u548c\u90e8\u7f72\u3002", "motivation": "\u65e8\u5728\u964d\u4f4eAI\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u6280\u672f\u95e8\u69db\uff0c\u4f7f\u975e\u4e13\u4e1a\u5f00\u53d1\u8005\u4e5f\u80fd\u8f7b\u677e\u521b\u5efa\u548c\u90e8\u7f72AI\u667a\u80fd\u4f53\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGolang\u3001React\u548cTypescript\u7684\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u548c\u65e0\u4ee3\u7801/\u4f4e\u4ee3\u7801\u5f00\u53d1\u65b9\u5f0f\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51faCoze Studio\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86AI\u667a\u80fd\u4f53\u7684\u7b80\u5316\u5f00\u53d1\u6d41\u7a0b\u3002", "conclusion": "Coze Studio\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\uff0c\u6709\u6548\u4fc3\u8fdb\u4e86AI\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u666e\u53ca\u548c\u6613\u7528\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2509.2005dcff", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmusistudio%2Fclaude-code-router%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/1LgnEQZ3HGycWWLl3q8zKTXWyM4at0DMmnFNLR1AVNs=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmusistudio%2Fclaude-code-router%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/1LgnEQZ3HGycWWLl3q8zKTXWyM4at0DMmnFNLR1AVNs=424", "authors": ["TLDR Newsletter"], "title": "Claude Code Router", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fmusistudio%2Fclaude-code-router%3Futm_source=tldrdevops/1/0100019995264ffd-68666a75-ddc6-4d52-9b8c-90422c4341e3-000000/1LgnEQZ3HGycWWLl3q8zKTXWyM4at0DMmnFNLR1AVNs=424", "summary": "Claude Code Router (GitHub Repo) Claude Code Router version v1.0.50 is now available. Users can now route Claude Code requests through models like GLM-4.5 and DeepSeek v3.1 for free via the iFlow Platform.", "source": "tldr", "AI": {"tldr": "Claude Code Router v1.50\u53d1\u5e03\uff0c\u652f\u6301\u901a\u8fc7iFlow\u5e73\u53f0\u514d\u8d39\u8def\u7531Claude Code\u8bf7\u6c42\u5230GLM-4.5\u548cDeepSeek v3.1\u6a21\u578b", "motivation": "\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u514d\u8d39\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u8def\u7531\u65b9\u6848\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u9009\u62e9\u4e0d\u540c\u7684AI\u6a21\u578b\u6765\u5904\u7406\u4ee3\u7801\u8bf7\u6c42", "method": "\u5f00\u53d1Claude Code Router\u5de5\u5177\uff0c\u901a\u8fc7iFlow\u5e73\u53f0\u5b9e\u73b0\u6a21\u578b\u8def\u7531\u529f\u80fd\uff0c\u652f\u6301GLM-4.5\u548cDeepSeek v3.1\u7b49\u6a21\u578b", "result": "\u6210\u529f\u53d1\u5e03v1.0.50\u7248\u672c\uff0c\u7528\u6237\u73b0\u5728\u53ef\u4ee5\u514d\u8d39\u4f7f\u7528\u591a\u4e2a\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u8bf7\u6c42\u8def\u7531", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u591a\u6a21\u578b\u9009\u62e9\uff0c\u964d\u4f4e\u4e86\u4ee3\u7801\u751f\u6210\u6210\u672c\uff0c\u63d0\u5347\u4e86\u7075\u6d3b\u6027", "topic": "code agent"}}
{"id": "tldr.2509.ed049ead", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchrisloy.dev%2Fpost%2F2025%2F09%2F28%2Fthe-ai-coding-trap%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/GW9SO-941Jn7RodgKz8fQBX-SMtttU5rcKbRD42xXUg=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchrisloy.dev%2Fpost%2F2025%2F09%2F28%2Fthe-ai-coding-trap%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/GW9SO-941Jn7RodgKz8fQBX-SMtttU5rcKbRD42xXUg=424", "authors": ["TLDR Newsletter"], "title": "The AI coding trap", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fchrisloy.dev%2Fpost%2F2025%2F09%2F28%2Fthe-ai-coding-trap%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/GW9SO-941Jn7RodgKz8fQBX-SMtttU5rcKbRD42xXUg=424", "summary": "The AI coding trap (10 minute read) The \"code first, ask questions later\" approach when using AI coding agents too much leads to increased time spent on debugging and integrating AI-generated code, reducing overall productivity. Instead, treat AI agents like fast but inexperienced junior engineers.", "source": "tldr", "AI": {"tldr": "\u8fc7\u5ea6\u4f9d\u8d56AI\u7f16\u7a0b\u52a9\u624b\u91c7\u7528'\u5148\u5199\u4ee3\u7801\u540e\u95ee\u95ee\u9898'\u7684\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8c03\u8bd5\u548c\u96c6\u6210AI\u751f\u6210\u4ee3\u7801\u7684\u65f6\u95f4\u589e\u52a0\uff0c\u53cd\u800c\u964d\u4f4e\u6574\u4f53\u751f\u4ea7\u529b", "motivation": "\u63a2\u8ba8AI\u7f16\u7a0b\u52a9\u624b\u4f7f\u7528\u4e2d\u7684\u9677\u9631\uff0c\u5206\u6790'\u4ee3\u7801\u4f18\u5148'\u65b9\u6cd5\u7684\u8d1f\u9762\u5f71\u54cd", "method": "\u5c06AI\u52a9\u624b\u89c6\u4e3a\u5feb\u901f\u4f46\u7ecf\u9a8c\u4e0d\u8db3\u7684\u521d\u7ea7\u5de5\u7a0b\u5e08\u6765\u5bf9\u5f85\uff0c\u6539\u53d8\u4f7f\u7528\u7b56\u7565", "result": "\u53d1\u73b0\u8fc7\u5ea6\u4f7f\u7528AI\u7f16\u7a0b\u52a9\u624b\u53cd\u800c\u589e\u52a0\u8c03\u8bd5\u548c\u96c6\u6210\u65f6\u95f4\uff0c\u964d\u4f4e\u6548\u7387", "conclusion": "\u5e94\u8be5\u8c03\u6574AI\u52a9\u624b\u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u907f\u514d'\u4ee3\u7801\u4f18\u5148'\u7684\u9677\u9631", "topic": "agent analysis"}}
{"id": "tldr.2509.db179600", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flilianweng.github.io%2Fposts%2F2025-05-01-thinking%2F%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/J5F4qFD1021V32SZL8p9xari2qoL2htaNSJRQPuMCQo=424", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flilianweng.github.io%2Fposts%2F2025-05-01-thinking%2F%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/J5F4qFD1021V32SZL8p9xari2qoL2htaNSJRQPuMCQo=424", "authors": ["TLDR Newsletter"], "title": "Why We Think", "comment": "Source: TLDR Newsletter, Date: 2025-09-29, Reading time: 41 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flilianweng.github.io%2Fposts%2F2025-05-01-thinking%2F%3Futm_source=tldrwebdev/1/01000199952972ae-ead1a5d5-ad93-4e52-8acf-f9b311fb32cd-000000/J5F4qFD1021V32SZL8p9xari2qoL2htaNSJRQPuMCQo=424", "summary": "Why We Think (41 minute read) Increasing test-time compute, or \"thinking time,\" can greatly improve LLM performance, especially in complex reasoning tasks. This article discusses techniques like chain-of-thought prompting, parallel sampling, and sequential revision to use this thinking time. Reinforcement learning approaches are also discussed, along with the challenge of making models provide faithful and human-readable reasoning.", "source": "tldr", "AI": {"tldr": "\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\"\u601d\u8003\u65f6\u95f4\"\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u672c\u6587\u8ba8\u8bba\u4e86\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u3001\u5e76\u884c\u91c7\u6837\u3001\u987a\u5e8f\u4fee\u8ba2\u7b49\u6280\u672f\u6765\u5229\u7528\u601d\u8003\u65f6\u95f4\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ee5\u53ca\u4f7f\u6a21\u578b\u63d0\u4f9b\u5fe0\u5b9e\u4e14\u4eba\u7c7b\u53ef\u8bfb\u63a8\u7406\u7684\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u5f53\u524d\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u3001\u5e76\u884c\u91c7\u6837\u3001\u987a\u5e8f\u4fee\u8ba2\u7b49\u6280\u672f\u6765\u6709\u6548\u5229\u7528\u589e\u52a0\u7684\u601d\u8003\u65f6\u95f4\uff0c\u5e76\u63a2\u8ba8\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660e\u589e\u52a0\u601d\u8003\u65f6\u95f4\u53ef\u4ee5\u663e\u8457\u6539\u5584LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u6027\u548c\u53ef\u8bfb\u6027\u95ee\u9898\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u7684\u589e\u52a0\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\uff0c\u4f46\u9700\u8981\u5f00\u53d1\u66f4\u597d\u7684\u6280\u672f\u6765\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "topic": "agent analysis"}}
{"id": "wechat.2509.80f6afc4", "categories": ["wechat.article", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIzNzIwNTMxMQ==&mid=2649372184&idx=1&sn=57622be0bd2bc6ba5262ccbcf9084b99&chksm=f1aaecf80bafe60506884e4cdbeae42b2853182693fbf2c493b0dbc20adcb09b561b63d77848#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIzNzIwNTMxMQ==&mid=2649372184&idx=1&sn=57622be0bd2bc6ba5262ccbcf9084b99&chksm=f1aaecf80bafe60506884e4cdbeae42b2853182693fbf2c493b0dbc20adcb09b561b63d77848#rd", "authors": ["\u96f6\u4e00\u74e6\u820d"], "title": "\u5956\u52b1\u4f55\u5fc5\u662f\u6570\u503c\uff1fLLM\u4ece\u6587\u672c\u53cd\u9988\u4e2d\u4e5f\u80fd<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-09-30 13:37:48", "summary": "\u5728 LLM \u7684\u5bf9\u9f50\u9886\u57df\uff0c\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60 \uff08RLHF\uff09 \u5df2\u6210\u4e3a\u4e00\u79cd\u6807\u51c6\u8303\u5f0f\u3002\u8be5\u8303\u5f0f\u7684\u7406\u8bba\u57fa\u77f3\u662f Richard Sutton \u63d0\u51fa\u7684\u300c\u5956\u52b1\u5047\u8bbe\u300d \uff08Reward Hypothesis\uff09\uff0c\u5373\u6240\u6709\u5f62\u5f0f\u7684\u76ee\u6807\u90fd\u53ef\u4ee5\u88ab\u8868\u8fbe\u4e3a\u5bf9\u4e00\u4e2a\u7d2f\u79ef\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\u671f\u671b\u503c\u7684\u6700\u5927\u5316", "AI": {"tldr": "\u5728 LLM \u7684\u5bf9\u9f50\u9886\u57df\uff0c\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60 \uff08RLHF\uff09 \u5df2\u6210\u4e3a\u4e00\u79cd\u6807\u51c6\u8303\u5f0f\u3002\u8be5\u8303\u5f0f\u7684\u7406\u8bba\u57fa\u77f3\u662f Richard Sutton \u63d0\u51fa\u7684\u300c\u5956\u52b1\u5047\u8bbe\u300d \uff08Reward Hypothesis\uff09\uff0c\u5373\u6240\u6709\u5f62\u5f0f\u7684\u76ee\u6807\u90fd\u53ef\u4ee5\u88ab\u8868\u8fbe\u4e3a\u5bf9\u4e00\u4e2a\u7d2f\u79ef\u6807\u91cf\u5956\u52b1\u4fe1\u53f7\u671f\u671b\u503c\u7684\u6700\u5927\u5316", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.aeae951a", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NTg2NTU0Ng==&mid=2656659539&idx=1&sn=98d04fd383ed59875610842490d67d2d&chksm=bc9d58a2f2302e904793feaeabe9bda08f716a57f41b97463029f08be2b4fce4b5d539061c5f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NTg2NTU0Ng==&mid=2656659539&idx=1&sn=98d04fd383ed59875610842490d67d2d&chksm=bc9d58a2f2302e904793feaeabe9bda08f716a57f41b97463029f08be2b4fce4b5d539061c5f#rd", "authors": ["21CTO"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236\u7406\u67e5\u5fb7\u00b7\u8428\u987f\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u662f\u201c\u6b7b\u8def\u4e00\u6761\u201d", "comment": "Source: WeChat, Published: 2025-09-30 13:08:21", "summary": "\uff08 1\uff1a46 \uff09\u5f3a\u5316\u5b66\u4e60\u662f\u57fa\u7840\u4eba\u5de5\u667a\u80fd\uff0c\u4e13\u6ce8\u4e8e\u8ba9\u667a\u80fd\u4f53\u7406\u89e3\u5e76\u5f04\u6e05\u695a\u5b83\u6240\u5904\u7684\u4e16\u754c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08LLM\uff09 \u5219\u4e13\u6ce8\u4e8e\u6a21\u4eff\u4eba\u7c7b\u53ca\u5176\u8a00\u8bed\uff0c\u800c\u4e0d\u662f\u8ba9\u667a\u80fd\u4f53\u81ea\u5df1\u5f04\u6e05\u695a\u8be5\u505a\u4ec0\u4e48\u3002", "AI": {"tldr": "\uff08 1\uff1a46 \uff09\u5f3a\u5316\u5b66\u4e60\u662f\u57fa\u7840\u4eba\u5de5\u667a\u80fd\uff0c\u4e13\u6ce8\u4e8e\u8ba9\u667a\u80fd\u4f53\u7406\u89e3\u5e76\u5f04\u6e05\u695a\u5b83\u6240\u5904\u7684\u4e16\u754c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08LLM\uff09 \u5219\u4e13\u6ce8\u4e8e\u6a21\u4eff\u4eba\u7c7b\u53ca\u5176\u8a00\u8bed\uff0c\u800c\u4e0d\u662f\u8ba9\u667a\u80fd\u4f53\u81ea\u5df1\u5f04\u6e05\u695a\u8be5\u505a\u4ec0\u4e48\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.f4ecbc49", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMjY3MjY1MQ==&mid=2247484449&idx=1&sn=dbab946b7db03a1b4253925f0112ab63&chksm=c008c90340388bdb9fea8f8439751e37d9aeaa9baa4fea7ddad87955f70e4cde5266c6250f88#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMjY3MjY1MQ==&mid=2247484449&idx=1&sn=dbab946b7db03a1b4253925f0112ab63&chksm=c008c90340388bdb9fea8f8439751e37d9aeaa9baa4fea7ddad87955f70e4cde5266c6250f88#rd", "authors": ["oneLLM"], "title": "\u901a\u4e49\u63d0\u51faSPELL\u6846\u67b6\uff1a\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587LLM\u7684\u81ea\u535a\u5f08<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-09-30 12:52:40", "summary": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u624b\u6bb5\uff0c\u5176\u4e2d\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684RL\uff08RLVR\uff09 \u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\uff08\u6570\u5b66\u8ba1\u7b97\u3001\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u2014\u2014\u8fd9\u7c7b\u4efb\u52a1\u53ef\u901a\u8fc7\u89c4\u5219/\u7a0b\u5e8f\u9a8c\u8bc1\u7b54\u6848\u6b63\u786e\u6027\uff08\u5982\u4ee3\u7801\u6267\u884c\u7ed3\u679c\u3001\u6570\u5b66\u516c\u5f0f\u8ba1\u7b97\uff09\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u662f\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u624b\u6bb5\uff0c\u5176\u4e2d\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684RL\uff08RLVR\uff09 \u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\uff08\u6570\u5b66\u8ba1\u7b97\u3001\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u8868\u73b0\u7a81\u51fa\u2014\u2014\u8fd9\u7c7b\u4efb\u52a1\u53ef\u901a\u8fc7\u89c4\u5219/\u7a0b\u5e8f\u9a8c\u8bc1\u7b54\u6848\u6b63\u786e\u6027\uff08\u5982\u4ee3\u7801\u6267\u884c\u7ed3\u679c\u3001\u6570\u5b66\u516c\u5f0f\u8ba1\u7b97\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.e23a9c23", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NzYxODI3MQ==&mid=2247512948&idx=1&sn=a861220a3019561f50e32799e366de04&chksm=cfcb4dc3a9cd4ef5428a50e0b68b5f95eb3e0c611b49e43e6aaf6b185d4b8e303d7f61e7d604#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NzYxODI3MQ==&mid=2247512948&idx=1&sn=a861220a3019561f50e32799e366de04&chksm=cfcb4dc3a9cd4ef5428a50e0b68b5f95eb3e0c611b49e43e6aaf6b185d4b8e303d7f61e7d604#rd", "authors": ["\u5b66\u59d0\u5e26\u4f60\u73a9AI"], "title": "SCI\u53d1\u6587\u70ed\u70b9\uff1a\u7269\u7406\u4fe1\u606f<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff0c\u73b0\u6210\u5957\u8def\u62ff\u8d70\u4e0d\u8c22\uff01", "comment": "Source: WeChat, Published: 2025-09-30 10:11:00", "summary": "\u65b9\u6cd5\uff1a\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u6700\u4f18\u6f6e\u6d41\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u6f14\u5458\u786e\u4fdd\u64cd\u4f5c\u6ee1\u8db3\u7535\u529b\u7cfb\u7edf\u6f6e\u6d41\u65b9\u7a0b\u7684\u7b49\u5f0f\u7ea6\u675f\uff0c\u5e76\u5229\u7528\u7ea6\u675f\u7b56\u7565\u68af\u5ea6\u76f4\u63a5\u8ba1\u7b97\u4e0d\u7b49\u5f0f\u7ea6\u675f\u6210\u672c\uff0c\u7ea0\u6b63\u4e0d\u53ef\u884c\u64cd\u4f5c\u3002", "AI": {"tldr": "\u65b9\u6cd5\uff1a\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u6700\u4f18\u6f6e\u6d41\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u6f14\u5458\u786e\u4fdd\u64cd\u4f5c\u6ee1\u8db3\u7535\u529b\u7cfb\u7edf\u6f6e\u6d41\u65b9\u7a0b\u7684\u7b49\u5f0f\u7ea6\u675f\uff0c\u5e76\u5229\u7528\u7ea6\u675f\u7b56\u7565\u68af\u5ea6\u76f4\u63a5\u8ba1\u7b97\u4e0d\u7b49\u5f0f\u7ea6\u675f\u6210\u672c\uff0c\u7ea0\u6b63\u4e0d\u53ef\u884c\u64cd\u4f5c\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.0856f303", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649910538&idx=3&sn=e7b0cf766c54d35178f98d700abf835f&chksm=bff8e5dc5b09361212054f507cbe37148aba2d8c9105b8e4748e1d96979379fd39b83dcf3a8a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODIwNjEzNQ==&mid=2649910538&idx=3&sn=e7b0cf766c54d35178f98d700abf835f&chksm=bff8e5dc5b09361212054f507cbe37148aba2d8c9105b8e4748e1d96979379fd39b83dcf3a8a#rd", "authors": ["\u4e2d\u56fd\u4eba\u5de5\u667a\u80fd\u5b66\u4f1a"], "title": "\u5b66\u672f\u5206\u4eab\u4e28\u9648\u4e39\u7426\u65b0\u4f5c\uff1a\u5927\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u7b2c\u4e09\u6761\u8def\uff0c8B\u5c0f\u6a21\u578b\u8d85\u8d8aGPT-4o", "comment": "Source: WeChat, Published: 2025-09-30 09:31:49", "summary": "\u7f51\u53cb\u89c9\u5f97\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u65b0\u57fa\u7ebf\uff1a\u8c01\u5236\u5b9a\u4e86\u504f\u597d\u7684\u5b9a\u4e49\uff0c\u8c01\u5c31\u662f\u540e\u8bad\u7ec3\u65f6\u4ee3\u7684\u201c\u65b0\u5f97\u5206\u624b\u201d\u3002jhxhgukvcxx @jhxhgukvcxx \u00b7 sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo", "AI": {"tldr": "\u7f51\u53cb\u89c9\u5f97\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u65b0\u57fa\u7ebf\uff1a\u8c01\u5236\u5b9a\u4e86\u504f\u597d\u7684\u5b9a\u4e49\uff0c\u8c01\u5c31\u662f\u540e\u8bad\u7ec3\u65f6\u4ee3\u7684\u201c\u65b0\u5f97\u5206\u624b\u201d\u3002jhxhgukvcxx @jhxhgukvcxx \u00b7 sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.12375d7f", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2NzY0MTkzOQ==&mid=2247493791&idx=1&sn=8e7e8e3b82e1bdd994493d9a8f0afecd&chksm=cfc20398a63c8997cb9b770c438eb28ce4ff098908d98c4c78871388214c4d7b2b527b3352da#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2NzY0MTkzOQ==&mid=2247493791&idx=1&sn=8e7e8e3b82e1bdd994493d9a8f0afecd&chksm=cfc20398a63c8997cb9b770c438eb28ce4ff098908d98c4c78871388214c4d7b2b527b3352da#rd", "authors": ["\u6570\u5b57\u5f00\u7269"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65e0\u6cd5\u8ba9\u667a\u80fd\u4f53\u9ad8\u6548\u7406\u89e3\u4e16\u754c\uff5c\u6768\u7acb\u6606\u6700\u65b0\u6f14\u8bb2\u5b9e\u5f55", "comment": "Source: WeChat, Published: 2025-09-30 09:28:42", "summary": "\u6768\u7acb\u6606\u8ba4\u4e3a\uff0c\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u6765\u8ba9\u667a\u80fd\u4f53\u7406\u89e3\u4e16\u754c\u662f\u4f4e\u6548\u7684\uff0c\u901a\u8fc7\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u6784\u5efa\u5f3a\u5927\u7684\u4e16\u754c\u6a21\u578b\u624d\u662f\u6839\u672c\u3002\u6b64\u5916\uff0c\u8bd5\u56fe\u901a\u8fc7\u91cd\u5efa\u50cf\u7d20\uff08\u751f\u6210\u56fe\u50cf/\u89c6\u9891\uff09\u6765\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\u662f\u201c\u5b8c\u5168\u6d6a\u8d39\u65f6\u95f4\u201d\uff0c\u5176\u6548\u679c\u8fdc\u4e0d\u5982\u8054\u5408\u5d4c\u5165\u7b49\u5224\u522b\u5f0f\u65b9", "AI": {"tldr": "\u6768\u7acb\u6606\u8ba4\u4e3a\uff0c\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\u6765\u8ba9\u667a\u80fd\u4f53\u7406\u89e3\u4e16\u754c\u662f\u4f4e\u6548\u7684\uff0c\u901a\u8fc7\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u6784\u5efa\u5f3a\u5927\u7684\u4e16\u754c\u6a21\u578b\u624d\u662f\u6839\u672c\u3002\u6b64\u5916\uff0c\u8bd5\u56fe\u901a\u8fc7\u91cd\u5efa\u50cf\u7d20\uff08\u751f\u6210\u56fe\u50cf/\u89c6\u9891\uff09\u6765\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\u662f\u201c\u5b8c\u5168\u6d6a\u8d39\u65f6\u95f4\u201d\uff0c\u5176\u6548\u679c\u8fdc\u4e0d\u5982\u8054\u5408\u5d4c\u5165\u7b49\u5224\u522b\u5f0f\u65b9", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.66889084", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyNDA0MzYzNw==&mid=2247484443&idx=1&sn=57f8bce19d855203f2cde4f2432bc581&chksm=f1d7db21bc0c00a1f1ae28f7baca23e546e7750175114a398879104ee49b9e0ffa410d927759#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyNDA0MzYzNw==&mid=2247484443&idx=1&sn=57f8bce19d855203f2cde4f2432bc581&chksm=f1d7db21bc0c00a1f1ae28f7baca23e546e7750175114a398879104ee49b9e0ffa410d927759#rd", "authors": ["AI\u5927\u6a21\u578b\u5c0f\u827a"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236\uff1a\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u5728\u627c\u6740\u771f\u6b63\u7684\u4eba\u5de5\u667a\u80fd", "comment": "Source: WeChat, Published: 2025-09-30 09:27:54", "summary": "\u5f3a\u5316\u5b66\u4e60\u4e4b\u7236richard sutton\u7684\u5fe0\u544a\uff1a\u5927\u8bed\u8a00\u6a21 \u578b\u6b63\u5728\u627c\u6740\u771f\u6b63\u7684\u4eba\u5de5\u667a\u80fd\u3002I\u771f\u6b63\u7684\u667a\u80fd\uff0c\u65e0\u6cd5\u88ab\u6559\u4f1a\uff0c\u53ea\u80fd\u88ab\u5b66\u4f1a\u3002\u5728\u4eba\u5de5\u667a\u80fd\u7684\u661f\u7a7a\u4e2d\uff0c\u4e24\u6761\u622a\u7136\u4e0d\u540c\u7684\u9053\u8def\u6b63\u5c06\u6211\u4eec \u5f15\u5411\u8fe5\u5f02\u7684\u672a\u6765\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u4e4b\u7236richard sutton\u7684\u5fe0\u544a\uff1a\u5927\u8bed\u8a00\u6a21 \u578b\u6b63\u5728\u627c\u6740\u771f\u6b63\u7684\u4eba\u5de5\u667a\u80fd\u3002I\u771f\u6b63\u7684\u667a\u80fd\uff0c\u65e0\u6cd5\u88ab\u6559\u4f1a\uff0c\u53ea\u80fd\u88ab\u5b66\u4f1a\u3002\u5728\u4eba\u5de5\u667a\u80fd\u7684\u661f\u7a7a\u4e2d\uff0c\u4e24\u6761\u622a\u7136\u4e0d\u540c\u7684\u9053\u8def\u6b63\u5c06\u6211\u4eec \u5f15\u5411\u8fe5\u5f02\u7684\u672a\u6765\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.6754cea3", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574104&idx=3&sn=6ad550be9f6de02dfd28b1c09d573609&chksm=eaf93b982d01fa64afc86a6700de8a4df8b358ec9a96619172b34ca0e97106769885f288c00d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247574104&idx=3&sn=6ad550be9f6de02dfd28b1c09d573609&chksm=eaf93b982d01fa64afc86a6700de8a4df8b358ec9a96619172b34ca0e97106769885f288c00d#rd", "authors": ["\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e4b\u7236\u7ed9LLM\u5224\u6b7b\u5211\uff01\u7ad9\u961fLeCun\uff1a\u6211\u4eec\u5168\u641e\u9519\u4e86", "comment": "Source: WeChat, Published: 2025-09-29 16:03:39", "summary": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6709\u6b63\u786e\u7684\u8bdd\u8bed\u8981\u8bf4\uff0c\u6709\u6b63\u786e\u7684\u52a8\u4f5c\u8981\u505a\uff0c\u6b63\u786e\u7684\u4e8b\u5c31\u662f\u80fd\u591f\u83b7\u5f97\u5956\u52b1\u7684\u4e8b\u3002\u6211\u4eec\u5bf9\u6b63\u786e\u7684\u4e8b\u662f\u6709\u5b9a\u4e49\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u9884\u5148\u638c\u63e1\u6216\u901a\u8fc7\u4ed6\u4eba\u83b7\u53d6\u5173\u4e8e\u6b63\u786e\u7684\u4e8b\u7684\u77e5\u8bc6\u3002", "AI": {"tldr": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6709\u6b63\u786e\u7684\u8bdd\u8bed\u8981\u8bf4\uff0c\u6709\u6b63\u786e\u7684\u52a8\u4f5c\u8981\u505a\uff0c\u6b63\u786e\u7684\u4e8b\u5c31\u662f\u80fd\u591f\u83b7\u5f97\u5956\u52b1\u7684\u4e8b\u3002\u6211\u4eec\u5bf9\u6b63\u786e\u7684\u4e8b\u662f\u6709\u5b9a\u4e49\u7684\uff0c\u56e0\u6b64\u53ef\u4ee5\u9884\u5148\u638c\u63e1\u6216\u901a\u8fc7\u4ed6\u4eba\u83b7\u53d6\u5173\u4e8e\u6b63\u786e\u7684\u4e8b\u7684\u77e5\u8bc6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.db98bba4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk1NzY2OTYzMA==&mid=2247488210&idx=1&sn=091df7c1a9059a369bca7667cdcfd0dd&chksm=c296ad9e7f24c55ea429f11c405f9cbb3a250cce86f669c0d81bdd5559a43fe802ad7b9590f3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk1NzY2OTYzMA==&mid=2247488210&idx=1&sn=091df7c1a9059a369bca7667cdcfd0dd&chksm=c296ad9e7f24c55ea429f11c405f9cbb3a250cce86f669c0d81bdd5559a43fe802ad7b9590f3#rd", "authors": ["\u864esir\u7684AI\u6280\u672f\u535a\u5ba2"], "title": "\u5927\u578b\u63a8\u7406\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7814\u7a76\u7efc\u8ff0 \uff083\uff09", "comment": "Source: WeChat, Published: 2025-09-29 16:01:00", "summary": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u7efc\u8ff0 \uff082\uff09\u63a5\u7740\u5f80\u4e0b\u5206\u6790\u3002foundational problems rl's role model prior training recipes rl vs. sft reward type reward design vs vs vs vs 6 sharpening discovery weak strong tricks traps generalize memorize process outcome verifiable generative rewards rewar", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u7efc\u8ff0 \uff082\uff09\u63a5\u7740\u5f80\u4e0b\u5206\u6790\u3002foundational problems rl's role model prior training recipes rl vs. sft reward type reward design vs vs vs vs 6 sharpening discovery weak strong tricks traps generalize memori...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.4fb9caa0", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDM2MTgxMQ==&mid=2247483668&idx=1&sn=3e71e6b6474a4d4d4f12890146e5e837&chksm=c50f39f9b1b02114f7b15142dce5c2af50bb58b1ea8fd7c1f076b84ce592d77fb8f761372a43#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDM2MTgxMQ==&mid=2247483668&idx=1&sn=3e71e6b6474a4d4d4f12890146e5e837&chksm=c50f39f9b1b02114f7b15142dce5c2af50bb58b1ea8fd7c1f076b84ce592d77fb8f761372a43#rd", "authors": ["\u5e2d\u7433Sclin"], "title": "\u5bf9\u5177\u8eab\u667a\u80fd\u4e2d<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u5e94\u7528\u7684\u4e00\u70b9\u70b9\u89c2\u5bdf", "comment": "Source: WeChat, Published: 2025-09-29 14:55:54", "summary": "\u4ee5\u53ca\u6709\u535a\u5f08\u5c5e\u6027\u7684\u4e00\u5207\u573a\u666f\u91cc\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff0c reinforcement learning\uff09\uff1b\u5168\u4e16\u754c\u7814\u7a76RL\u7684\u4e13\u5bb6\u5b66\u8005\uff08\u6709\u660e\u786e\u7684H-index\uff09\u8d85\u8fc71000\u4eba\uff0c\u7814\u7a76\u9886\u57df\u4e0eRL\u76f8\u5173\u7684\u6301\u6709PhD/MS\u5b66\u5386\u7684\u9752\u5e74\u5b66\u8005\u8d85\u8fc710000\u4eba\uff0cGoogle Scholar\u4e0eRL\u76f4\u63a5\u6216\u95f4\u63a5\u76f8\u5173\u7684Paper\u6570\u91cf\u8d85", "AI": {"tldr": "\u4ee5\u53ca\u6709\u535a\u5f08\u5c5e\u6027\u7684\u4e00\u5207\u573a\u666f\u91cc\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff0c reinforcement learning\uff09\uff1b\u5168\u4e16\u754c\u7814\u7a76RL\u7684\u4e13\u5bb6\u5b66\u8005\uff08\u6709\u660e\u786e\u7684H-index\uff09\u8d85\u8fc71000\u4eba\uff0c\u7814\u7a76\u9886\u57df\u4e0eRL\u76f8\u5173\u7684\u6301\u6709PhD/MS\u5b66\u5386\u7684\u9752\u5e74\u5b66\u8005\u8d85\u8fc710000\u4eba\uff0cGoogle Scholar\u4e0eRL\u76f4\u63a5\u6216\u95f4\u63a5\u76f8\u5173\u7684Paper\u6570\u91cf\u8d85", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2509.9a2219d4", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzYyMzAwMDc3Nw==&mid=2247486664&idx=1&sn=1ecff11d62d620dbd201d39e2c197a2c&chksm=fea2a4e6dc3bb4cad9fb8f4dc6585350c73a2d4f13de882cce6ffd21815dbb20177970f9c382#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzYyMzAwMDc3Nw==&mid=2247486664&idx=1&sn=1ecff11d62d620dbd201d39e2c197a2c&chksm=fea2a4e6dc3bb4cad9fb8f4dc6585350c73a2d4f13de882cce6ffd21815dbb20177970f9c382#rd", "authors": ["\u949f\u4ec18629"], "title": "AI\u65f6\u4ee3\uff1a<em class=\"highlight\">Agentic</em> Organization\uff08<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7ec4\u7ec7\uff09", "comment": "Source: WeChat, Published: 2025-09-30 13:29:59", "summary": "AI\u65f6\u4ee3\uff1aAgentic Organization\uff08\u667a\u80fd\u4f53\u7ec4\u7ec7\uff09\u5173\u952e\u8bcd\uff1aAI-first\u3001\u6df7\u5408\u52b3\u52a8\u529b\u3001\u4eba\u7c7b\u7ad9\u5230\u201cloop\u4e4b\u4e0a\u201d\u3002\u6d41\u7a0b\u7531AI\u6267\u884c\uff0c\u4eba\u7c7b\u8d1f\u8d23\u76ee\u6807\u8bbe\u5b9a\u3001\u65b9\u5411\u628a\u63a7\u3001\u4ef7\u503c\u6743\u8861\u3002", "AI": {"tldr": "AI\u65f6\u4ee3\uff1aAgentic Organization\uff08\u667a\u80fd\u4f53\u7ec4\u7ec7\uff09\u5173\u952e\u8bcd\uff1aAI-first\u3001\u6df7\u5408\u52b3\u52a8\u529b\u3001\u4eba\u7c7b\u7ad9\u5230\u201cloop\u4e4b\u4e0a\u201d\u3002\u6d41\u7a0b\u7531AI\u6267\u884c\uff0c\u4eba\u7c7b\u8d1f\u8d23\u76ee\u6807\u8bbe\u5b9a\u3001\u65b9\u5411\u628a\u63a7\u3001\u4ef7\u503c\u6743\u8861\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.670463da", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNjE5MjAxOQ==&mid=2247491434&idx=1&sn=c194cbac076b2e78dde2c56463f20180&chksm=c33ca0520443c3d25f6271f2a81a25528b3e50777a8f26c0abcffd7d0cc34596dd6567c20a20#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNjE5MjAxOQ==&mid=2247491434&idx=1&sn=c194cbac076b2e78dde2c56463f20180&chksm=c33ca0520443c3d25f6271f2a81a25528b3e50777a8f26c0abcffd7d0cc34596dd6567c20a20#rd", "authors": ["\u6ce2\u7c92\u4e8c\u8c61APP"], "title": "AI <em class=\"highlight\">\u667a\u80fd\u4f53</em>\u6838\u5fc3\u539f\u7406\u7efc\u8ff0\uff1a\u4ece <em class=\"highlight\">Agentic</em> AI \u5230 AI Agent", "comment": "Source: WeChat, Published: 2025-09-30 12:57:18", "summary": "Agentic AI \u7684\u80cc\u666fLLM \u6700\u521d\u7684\u4ea7\u54c1\u5f62\u6001\u662f\u7531 OpenAI \u9886\u8854\u7684 ChatBot\uff08\u804a\u5929\u673a\u5668\u4eba\uff09\uff0c\u5e95\u5c42\u652f\u6491\u6280\u672f\u662f Transformer \u67b6\u6784\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u521d\u4e13\u6ce8\u4e8e\u8bed\u8a00\u6587\u672c\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u573a\u666f\u3002", "AI": {"tldr": "Agentic AI \u7684\u80cc\u666fLLM \u6700\u521d\u7684\u4ea7\u54c1\u5f62\u6001\u662f\u7531 OpenAI \u9886\u8854\u7684 ChatBot\uff08\u804a\u5929\u673a\u5668\u4eba\uff09\uff0c\u5e95\u5c42\u652f\u6491\u6280\u672f\u662f Transformer \u67b6\u6784\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6700\u521d\u4e13\u6ce8\u4e8e\u8bed\u8a00\u6587\u672c\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.a3672963", "categories": ["wechat.article", "wechat.ai", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU3Mzg1Njk0Ng==&mid=2247485684&idx=1&sn=b1155df4750df95d37d0e27b902da563&chksm=fcc6fd9847509a7e7874b4c4e2b59feb4a2e5c4729146e84d469acd9e9556bad3df85b22054f#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU3Mzg1Njk0Ng==&mid=2247485684&idx=1&sn=b1155df4750df95d37d0e27b902da563&chksm=fcc6fd9847509a7e7874b4c4e2b59feb4a2e5c4729146e84d469acd9e9556bad3df85b22054f#rd", "authors": ["\u6c42\u7d22\u4e91\u9014"], "title": "<em class=\"highlight\">Agentic</em> \u5e94\u7528\u67b6\u6784\uff5c\u6765\u81ea Shopify \u5356\u5bb6\u667a\u80fd\u52a9\u624b Sidekick \u7ecf\u9a8c\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-09-30 08:58:50", "summary": "\u7ecf\u5178\u7684 Agentic \u5faa\u73af\u67b6\u6784action human \u2190------> llm call environment feedback stopAnthropic \u63d0\u51fa Agentic Loop \u67b6\u6784\uff0c\u4e00\u4e2a\u65e0\u9650\u5faa\u73af\uff0c\u4eba\u63d0\u4f9b\u8f93\u5165\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u8f93\u5165\u5e76\u51b3\u5b9a\u91c7\u53d6\u4ec0\u4e48\u884c\u52a8\uff0c\u8fd9\u4e9b\u884c\u52a8\u5728\u7279\u5b9a\u7684\u73af\u5883\u4e2d\u6267\u884c\uff0c\u6267\u884c\u7ed3\u679c\u6216\u5176\u4ed6\u53cd\u9988\u56de\u5230", "AI": {"tldr": "\u7ecf\u5178\u7684 Agentic \u5faa\u73af\u67b6\u6784action human \u2190------> llm call environment feedback stopAnthropic \u63d0\u51fa Agentic Loop \u67b6\u6784\uff0c\u4e00\u4e2a\u65e0\u9650\u5faa\u73af\uff0c\u4eba\u63d0\u4f9b\u8f93\u5165\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u8f93\u5165\u5e76\u51b3\u5b9a\u91c7\u53d6\u4ec0\u4e48\u884c\u52a8\uff0c\u8fd9\u4e9b\u884c\u52a8\u5728\u7279\u5b9a\u7684\u73af\u5883\u4e2d\u6267\u884c\uff0c\u6267\u884c\u7ed3\u679c\u6216\u5176\u4ed6\u53cd\u9988\u56de\u5230", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1e193896", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMDU0ODA5MQ==&mid=2247486573&idx=1&sn=b5e576c813b86ec5522f4c71610d0bce&chksm=c17de2487bfd26732d8a53c6ab6bd2464fe134c1b93d106af446b8a5c47c2d46a5d32bf85ebe#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMDU0ODA5MQ==&mid=2247486573&idx=1&sn=b5e576c813b86ec5522f4c71610d0bce&chksm=c17de2487bfd26732d8a53c6ab6bd2464fe134c1b93d106af446b8a5c47c2d46a5d32bf85ebe#rd", "authors": ["\u534e\u987f\u5c14AI"], "title": "\u9ea6\u80af\u9521\u91cd\u78c5\u53d1\u5e03\uff1a<em class=\"highlight\">\u667a\u80fd\u4f53</em>\u7ec4\u7ec7\uff08<em class=\"highlight\">Agentic</em> Organization\uff09\u2014\u2014AI\u65f6\u4ee3\u5168\u65b0\u8303\u5f0f\uff0c\u4f01\u4e1a\u4e0e\u4e2a\u4eba\u90fd\u65e0\u6cd5\u56de\u907f", "comment": "Source: WeChat, Published: 2025-09-30 05:16:11", "summary": "accountabilityand agentic controls with humanWorkforce\uff0cpeople\uff0c andculture ture of craftsmanship Deep specialization and cul- Narrowly specialized functionalplanning talent working in a culture of Knowledge workers with", "AI": {"tldr": "accountabilityand agentic controls with humanWorkforce\uff0cpeople\uff0c andculture ture of craftsmanship Deep specialization and cul- Narrowly specialized functionalplanning talent working in a culture of Know...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.de693e76", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNzYzNDc0Mg==&mid=2247484234&idx=1&sn=bcfcbf31940afe3fff1e8bb3bde1be11&chksm=c11ca3a4666351e39d43160ed757fbc25516fab1fd3935a20bacf6fc572968e65d9623359dd5#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNzYzNDc0Mg==&mid=2247484234&idx=1&sn=bcfcbf31940afe3fff1e8bb3bde1be11&chksm=c11ca3a4666351e39d43160ed757fbc25516fab1fd3935a20bacf6fc572968e65d9623359dd5#rd", "authors": ["TechAgent"], "title": "\u4ea7\u54c1\u53d1\u5e03 | TechAgent 3.0\uff1a<em class=\"highlight\">Agentic</em> \u7248\u672c\u4e0a\u7ebf\uff0c\u4e3a\u4ea7\u4e1a\u6d1e\u5bdf\u6253\u9020\u53ef\u6307\u6325\u7684<em class=\"highlight\">\u667a\u80fd\u4f53</em>", "comment": "Source: WeChat, Published: 2025-09-30 04:18:10", "summary": "\u70b9\u51fb\u84dd\u5b57\uff0c\u5173\u6ce8\u6211\u4eecTechAgent 3.0\uff0c Agentic \u7248\u672c\u6b63\u5f0f\u4e0a\u7ebf\uff01\u4ece 2023 \u5e74 TechAgent 1.0 \u521b\u5efa\u4ea7\u4e1a\u94fe\u6570\u636e\u4e0e\u4e1a\u52a1\u573a\u666f\u57fa\u5ea7\uff0c\u5230 2024 \u5e74 2.0 \u7248 \u878d\u5165\u5927\u6a21\u578b\u80fd\u529b\uff0c\u201c\u8ba9\u667a\u80fd\u4f53\u771f\u6b63\u61c2\u4ea7\u4e1a\u201d \u4e00\u76f4\u662f\u7d20\u95ee\u56e2\u961f\u7684\u521d\u5fc3\u4e0e\u52aa\u529b\u65b9\u5411\u3002", "AI": {"tldr": "\u70b9\u51fb\u84dd\u5b57\uff0c\u5173\u6ce8\u6211\u4eecTechAgent 3.0\uff0c Agentic \u7248\u672c\u6b63\u5f0f\u4e0a\u7ebf\uff01\u4ece 2023 \u5e74 TechAgent 1.0 \u521b\u5efa\u4ea7\u4e1a\u94fe\u6570\u636e\u4e0e\u4e1a\u52a1\u573a\u666f\u57fa\u5ea7\uff0c\u5230 2024 \u5e74 2.0 \u7248 \u878d\u5165\u5927\u6a21\u578b\u80fd\u529b\uff0c\u201c\u8ba9\u667a\u80fd\u4f53\u771f\u6b63\u61c2\u4ea7\u4e1a\u201d \u4e00\u76f4\u662f\u7d20\u95ee\u56e2\u961f\u7684\u521d\u5fc3\u4e0e\u52aa\u529b\u65b9\u5411\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.e9b3c104", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyOTY1NjMyMQ==&mid=2247484390&idx=1&sn=b6bc8d6451528b91cd6c4309dd4ac7ec&chksm=c30c9f7b4e78cb4c2f8620bed2775734f5ac65ff15d94f20dec967514001aa645033161b7ed9#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyOTY1NjMyMQ==&mid=2247484390&idx=1&sn=b6bc8d6451528b91cd6c4309dd4ac7ec&chksm=c30c9f7b4e78cb4c2f8620bed2775734f5ac65ff15d94f20dec967514001aa645033161b7ed9#rd", "authors": ["\u8bed\u4e49\u6d8c\u73b0"], "title": "\u77e5\u4e4e\u76f4\u7b54<em class=\"highlight\">Agentic</em>\u5347\u7ea7\uff1a\u4ece\u641c\u7d22\u5de5\u5177\u5230\u601d\u8003\u4f19\u4f34\u7684\u8715\u53d8", "comment": "Source: WeChat, Published: 2025-09-30 02:43:58", "summary": "\u800cAgentic\u52a9\u624b\u5219\u50cf\u4e00\u4f4d\u8d44\u6df1\u987e\u95ee\uff0c\u80fd\u591f\u8fdb\u884c\u591a\u8f6e\u601d\u8003[2]\u3002eaith\uff01histe 4 aac k.ot 4\uff1a4t leaeonot\u5f53\u7528\u6237\u63d0\u51fa\"\u5982\u4f55\u5b66\u4e60\u673a\u5668\u5b66\u4e60\"\u8fd9\u6837\u7684\u5f00\u653e\u6027\u95ee\u9898\u65f6\uff0cAgentic\u52a9\u624b\u4f1a\u5148\u62c6\u89e3\u95ee\u9898\uff0c\u5236\u5b9a\u641c\u7d22\u7b56\u7565\uff0c\u7136\u540e\u8fdb\u884c\u591a\u8f6e\u4fe1\u606f\u641c\u96c6\u548c\u5206\u6790\uff0c\u6700\u7ec8\u7ed9\u51fa\u7ed3\u6784", "AI": {"tldr": "\u800cAgentic\u52a9\u624b\u5219\u50cf\u4e00\u4f4d\u8d44\u6df1\u987e\u95ee\uff0c\u80fd\u591f\u8fdb\u884c\u591a\u8f6e\u601d\u8003[2]\u3002eaith\uff01histe 4 aac k.ot 4\uff1a4t leaeonot\u5f53\u7528\u6237\u63d0\u51fa\"\u5982\u4f55\u5b66\u4e60\u673a\u5668\u5b66\u4e60\"\u8fd9\u6837\u7684\u5f00\u653e\u6027\u95ee\u9898\u65f6\uff0cAgentic\u52a9\u624b\u4f1a\u5148\u62c6\u89e3\u95ee\u9898\uff0c\u5236\u5b9a\u641c\u7d22\u7b56\u7565\uff0c\u7136\u540e\u8fdb\u884c\u591a\u8f6e\u4fe1\u606f\u641c\u96c6\u548c\u5206\u6790\uff0c\u6700\u7ec8\u7ed9\u51fa\u7ed3\u6784", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.a7fc254e", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5MzM4NzM1Nw==&mid=2651304947&idx=1&sn=451edfab8912f007ce95a7a3bfaebd86&chksm=bc864da473c8be23a9fb66d70c71e4f8b108c989ab74d0013cffeb19877f4697bd3d2077a6e8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5MzM4NzM1Nw==&mid=2651304947&idx=1&sn=451edfab8912f007ce95a7a3bfaebd86&chksm=bc864da473c8be23a9fb66d70c71e4f8b108c989ab74d0013cffeb19877f4697bd3d2077a6e8#rd", "authors": ["\u6559\u80b2\u98ce\u7bb1"], "title": "\u53d1\u5c55\u7684\u773c\u5149\u770b<em class=\"highlight\">Agentic</em> AI\u5728\u8054\u7edc\u4e2d\u5fc3\u843d\u5730", "comment": "Source: WeChat, Published: 2025-09-30 01:00:00", "summary": "\u8fc7\u53bb\u5ba2\u670d\u7684\u4e3b\u65cb\u5f8b\u662f\u201c\u7b54\u95ee\u9898\u201d\uff0c\u4eca\u5929 Agentic AI \u7684\u4e3b\u9898\u662f\u201c\u505a\u4e8b\u60c5\u201d\u3002\u5b83\u80fd\u67e5\u7269\u6d41\u3001\u6539\u5730\u5740\u3001\u8865\u5f00\u53d1\u7968\u3001\u53d1\u8d77 RMA\u3001\u89e6\u53d1\u9000\u6b3e\u3001\u91cd\u7f6e\u5bc6\u7801\u3001\u6821\u9a8c\u6743\u76ca\u3001\u521b\u5efa\u5de5\u5355\u3001\u8def\u7531\u5206\u6d3e\u3001\u63a8\u9001\u5206\u6b65\u81ea\u52a9\u65b9\u6848\uff0c\u751a\u81f3\u7ed3\u5408\u5386\u53f2\u504f\u597d\u505a\u4ea4\u53c9\u63a8\u8350\u4e0e\u4e3b", "AI": {"tldr": "\u8fc7\u53bb\u5ba2\u670d\u7684\u4e3b\u65cb\u5f8b\u662f\u201c\u7b54\u95ee\u9898\u201d\uff0c\u4eca\u5929 Agentic AI \u7684\u4e3b\u9898\u662f\u201c\u505a\u4e8b\u60c5\u201d\u3002\u5b83\u80fd\u67e5\u7269\u6d41\u3001\u6539\u5730\u5740\u3001\u8865\u5f00\u53d1\u7968\u3001\u53d1\u8d77 RMA\u3001\u89e6\u53d1\u9000\u6b3e\u3001\u91cd\u7f6e\u5bc6\u7801\u3001\u6821\u9a8c\u6743\u76ca\u3001\u521b\u5efa\u5de5\u5355\u3001\u8def\u7531\u5206\u6d3e\u3001\u63a8\u9001\u5206\u6b65\u81ea\u52a9\u65b9\u6848\uff0c\u751a\u81f3\u7ed3\u5408\u5386\u53f2\u504f\u597d\u505a\u4ea4\u53c9\u63a8\u8350\u4e0e\u4e3b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.1d5868d2", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI1MTExMzYyNw==&mid=2247540639&idx=3&sn=6287c5a8bc9917a91c79b3ade0193221&chksm=e8393e202dea5dd2e625ee4fb22ce5737263fb83947f104f821ee85418965b46648b071a3dec#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI1MTExMzYyNw==&mid=2247540639&idx=3&sn=6287c5a8bc9917a91c79b3ade0193221&chksm=e8393e202dea5dd2e625ee4fb22ce5737263fb83947f104f821ee85418965b46648b071a3dec#rd", "authors": ["Huintellimance"], "title": "\u641c\u7d22\u6846\u91cc\u88c5\u4e86\u4e2a\u201c\u5927\u8111\u201d!<em class=\"highlight\">Agentic</em> Search\u4e0d\u4ec5\u80fd\u641c,\u8fd8\u80fd\u89c4\u5212\u6267\u884c\u53cd\u601d,\u5f7b\u5e95\u98a0\u8986\u4f60\u7684\u5de5\u4f5c\u6d41", "comment": "Source: WeChat, Published: 2025-09-29 15:35:26", "summary": "\u8981\u7406\u89e3Agentic Search\u7684\u6280\u672f\u539f\u7406\uff0c\u6211\u4eec\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u5e95\u5c42\u67b6\u6784\u3002\u6839\u636e\u5b66\u672f\u7814\u7a76\uff0c\u73b0\u4ee3\u7684Agentic Search\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u591aAgent\u534f\u540c\u67b6\u6784\u548c\u589e\u5f3aRAG\u6846\u67b6\u6784\u5efa\u3002", "AI": {"tldr": "\u8981\u7406\u89e3Agentic Search\u7684\u6280\u672f\u539f\u7406\uff0c\u6211\u4eec\u9700\u8981\u6df1\u5165\u4e86\u89e3\u5176\u5e95\u5c42\u67b6\u6784\u3002\u6839\u636e\u5b66\u672f\u7814\u7a76\uff0c\u73b0\u4ee3\u7684Agentic Search\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u591aAgent\u534f\u540c\u67b6\u6784\u548c\u589e\u5f3aRAG\u6846\u67b6\u6784\u5efa\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.d7e73457", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247562995&idx=1&sn=1754ef5bdfdc3a5757e68dd467cfaf7d&chksm=fc1659bf5bedd6ae1ce5ecd5f63315261c239034d1b20c47b155dfb46bea9d422bb4a1aa5b5b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247562995&idx=1&sn=1754ef5bdfdc3a5757e68dd467cfaf7d&chksm=fc1659bf5bedd6ae1ce5ecd5f63315261c239034d1b20c47b155dfb46bea9d422bb4a1aa5b5b#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u94feunion"], "title": "\u3010\u7cbe\u9009\u62a5\u544a\u3011<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e13\u9898\u4e00\uff1a\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u94fe\u901f\u770b\uff1a2025\u5e74\u6700\u503c\u5f97\u5173\u6ce8\u7684\u4e2d\u6587<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5168\u666f\u56fe\uff01\uff08\u9644PDF\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-09-30 12:20:00", "summary": "superclue\uff1a2025\u5e74\u6700\u503c\u5f97\u5173\u6ce8\u7684\u4e2d\u6587\u5927\u6a21\u578b\u5168\u666f\u56fe\u3002superclue \u4e2d\u6587\u5927\u6a21\u578b\u7efc\u5408\u6027\u6d4b\u8bc4\u57fa\u51c6 \u6587\u5fc3\u4e00\u8a00 \u901a\u4e49\u5343\u95ee \u817e\u8baf\u6df7\u5143 \u5546\u6c64\u65e5\u65e5\u65b0 bluelm 360\u667a\u8111 \u5929\u5de5 mi milm \u4e2d\u79d1\u95fb\u6b4c sensenova \u7d2b\u4e1c\u592a\u521d \u6f9c\u821f\u79d1\u6280 \u901a\u7528\u95ed\u6e90 \u5b57\u8282\u8c46\u5305 kimi.ai minimax \u4e91\u4ece\u79d1", "AI": {"tldr": "superclue\uff1a2025\u5e74\u6700\u503c\u5f97\u5173\u6ce8\u7684\u4e2d\u6587\u5927\u6a21\u578b\u5168\u666f\u56fe\u3002superclue \u4e2d\u6587\u5927\u6a21\u578b\u7efc\u5408\u6027\u6d4b\u8bc4\u57fa\u51c6 \u6587\u5fc3\u4e00\u8a00 \u901a\u4e49\u5343\u95ee \u817e\u8baf\u6df7\u5143 \u5546\u6c64\u65e5\u65e5\u65b0 bluelm 360\u667a\u8111 \u5929\u5de5 mi milm \u4e2d\u79d1\u95fb\u6b4c sensenova \u7d2b\u4e1c\u592a\u521d \u6f9c\u821f\u79d1\u6280 \u901a\u7528\u95ed\u6e90 \u5b57\u8282\u8c46\u5305 kimi.ai minimax \u4e91\u4ece\u79d1", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2509.dcbaf347", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247562995&idx=5&sn=c0af8fb13738792e15fe39fe2a40c6d3&chksm=fc933f3dce9fc5108b5bbf488c8c6fc8f23afc271a1004a12c9b2597cac38ff383ad5e58058a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU4ODQwNTIxMw==&mid=2247562995&idx=5&sn=c0af8fb13738792e15fe39fe2a40c6d3&chksm=fc933f3dce9fc5108b5bbf488c8c6fc8f23afc271a1004a12c9b2597cac38ff383ad5e58058a#rd", "authors": ["\u4eba\u5de5\u667a\u80fd\u4ea7\u4e1a\u94feunion"], "title": "\u3010\u62a5\u544a\u3011<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e13\u9898\u4e94\uff1a\u6df1\u61c2\u4f01\u4e1a\u5782\u7c7b<em class=\"highlight\">\u5927\u6a21\u578b</em>\uff1a\u4f01\u4e1a\u670d\u52a1<em class=\"highlight\">\u5927\u6a21\u578b</em>\u767d\u76ae\u4e66\u53d1\u5e03\uff08\u9644PDF\u4e0b\u8f7d\uff09", "comment": "Source: WeChat, Published: 2025-09-30 12:20:00", "summary": "1 \u4e2a\u5927\u6a21\u578b\u5e73\u53f0 + 2 \u4e2a\u6838\u5fc3\u6846\u67b6\uff1a\u5927\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3\u5e73\u53f0\uff1a\u652f\u6301\u4f01\u4e1a\u7ed3\u5408\u4e13\u5c5e\u6570\u636e\uff0c\u63d0\u4f9b\u6a21\u578b\u5f00\u53d1\u3001\u5fae\u8c03\u3001\u8bc4\u4f30\u5de5\u5177\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u6a21\u578b\u6784\u5efa\uff1bAgent \u6846\u67b6\uff1a\u6784\u5efa\u667a\u80fd\u52a9\u7406 \u201c\u667a\u53cb\u201d\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u3001\u4efb\u52a1\u62c6\u89e3\u3001API \u8c03\u5ea6\uff0c\u5b9e\u73b0\u590d\u6742\u4e1a\u52a1\u6d41", "AI": {"tldr": "1 \u4e2a\u5927\u6a21\u578b\u5e73\u53f0 + 2 \u4e2a\u6838\u5fc3\u6846\u67b6\uff1a\u5927\u6a21\u578b\u5fae\u8c03\u8bad\u7ec3\u5e73\u53f0\uff1a\u652f\u6301\u4f01\u4e1a\u7ed3\u5408\u4e13\u5c5e\u6570\u636e\uff0c\u63d0\u4f9b\u6a21\u578b\u5f00\u53d1\u3001\u5fae\u8c03\u3001\u8bc4\u4f30\u5de5\u5177\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u6a21\u578b\u6784\u5efa\uff1bAgent \u6846\u67b6\uff1a\u6784\u5efa\u667a\u80fd\u52a9\u7406 \u201c\u667a\u53cb\u201d\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u3001\u4efb\u52a1\u62c6\u89e3\u3001API \u8c03\u5ea6\uff0c\u5b9e\u73b0\u590d\u6742\u4e1a\u52a1\u6d41", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.2d2a4db4", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652790181&idx=1&sn=3c341f1f06ee5f868987e46b21d1fb8f&chksm=854647e6fc1abda4e75a3da775b16ad773ee8bebe9a2b22493240a55c41b1987830fe8dee09a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&mid=2652790181&idx=1&sn=3c341f1f06ee5f868987e46b21d1fb8f&chksm=854647e6fc1abda4e75a3da775b16ad773ee8bebe9a2b22493240a55c41b1987830fe8dee09a#rd", "authors": ["\u667a\u4e1c\u897f"], "title": "\u56fd\u4ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u96c6\u4f53\u56fd\u5e86\uff01\u6700\u5f3a\u56fd\u4ea7\u7f16\u7a0b\u6a21\u578b\u8bde\u751f\uff0c\u5bd2\u6b66\u7eaa\u6469\u5c14\u7ebf\u7a0b\u706b\u901f\u9002\u914d", "comment": "Source: WeChat, Published: 2025-09-30 12:12:31", "summary": "\u4eca\u5929\u4e0b\u5348\uff0c\u667a\u8c31AI\u6b63\u5f0f\u53d1\u5e03\u65b0\u4e00\u4ee3\u5927\u6a21\u578bGLM-4.6\uff0c\u5c31\u5728\u6628\u665a\uff0cDeepSeek\u4e5f\u5ba3\u5e03\u63a8\u51faDeepSeek-V3.2-Exp\u5b9e\u9a8c\u7248\u6a21\u578b\u3002\u4e24\u5bb6\u56fd\u4ea7\u5927\u6a21\u578b\u9886\u519b\u4f01\u4e1a\u5728\u56fd\u5e86\u5047\u671f\u6beb\u4e0d\u653e\u677e\uff0c\u52a0\u73ed\u52a0\u70b9\u63a8\u8fdb\u6280\u672f\u8fed\u4ee3\u3002", "AI": {"tldr": "\u4eca\u5929\u4e0b\u5348\uff0c\u667a\u8c31AI\u6b63\u5f0f\u53d1\u5e03\u65b0\u4e00\u4ee3\u5927\u6a21\u578bGLM-4.6\uff0c\u5c31\u5728\u6628\u665a\uff0cDeepSeek\u4e5f\u5ba3\u5e03\u63a8\u51faDeepSeek-V3.2-Exp\u5b9e\u9a8c\u7248\u6a21\u578b\u3002\u4e24\u5bb6\u56fd\u4ea7\u5927\u6a21\u578b\u9886\u519b\u4f01\u4e1a\u5728\u56fd\u5e86\u5047\u671f\u6beb\u4e0d\u653e\u677e\uff0c\u52a0\u73ed\u52a0\u70b9\u63a8\u8fdb\u6280\u672f\u8fed\u4ee3\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2509.649105b0", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzMjYyMjk1MA==&mid=2247498096&idx=1&sn=294f6caf4b52194326b7a51385dd7d72&chksm=c369682f08933cf479cd2271be05102af7b94f2815f5d74af393dad7b0929918622e85dbcf76#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzMjYyMjk1MA==&mid=2247498096&idx=1&sn=294f6caf4b52194326b7a51385dd7d72&chksm=c369682f08933cf479cd2271be05102af7b94f2815f5d74af393dad7b0929918622e85dbcf76#rd", "authors": ["\u6c5f\u6dee\u89c2\u5bdf"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u627e\u5230\u4e3b\u6218\u573a", "comment": "Source: WeChat, Published: 2025-09-30 12:10:38", "summary": "\u5de5\u4e1a\u5927\u6a21\u578b\u662f\u4eba\u673a\u4ea4\u4e92\u7684\u63a5\u53e3\uff0c\u4e5f\u662f\u5de5\u4e1a\u667a\u80fd\u4f53\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u4e0e\u8c03\u5ea6\u7684\u6838\u5fc3\uff0c\u662f\u7cfb\u5217\u5782\u7c7b\u6a21\u578b\u7684\u96c6\u5408\u3002\u5de5\u4e1a\u5927\u6a21\u578b\u4ee5\u65f6\u5e8f\u3001\u8bed\u8a00\u3001\u89c6\u89c9\u7b49\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e3a\u4e3b\u63a7\uff0c\u8c03\u7528\u4e00\u7cfb\u5217\u57fa\u7840\u6a21\u578b\u3001\u673a\u7406\u6a21\u578b\u3001\u4e13\u7528\u6a21\u578b\u3001\u5de5\u4e1a\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u7528\u667a\u80fd", "AI": {"tldr": "\u5de5\u4e1a\u5927\u6a21\u578b\u662f\u4eba\u673a\u4ea4\u4e92\u7684\u63a5\u53e3\uff0c\u4e5f\u662f\u5de5\u4e1a\u667a\u80fd\u4f53\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u4e0e\u8c03\u5ea6\u7684\u6838\u5fc3\uff0c\u662f\u7cfb\u5217\u5782\u7c7b\u6a21\u578b\u7684\u96c6\u5408\u3002\u5de5\u4e1a\u5927\u6a21\u578b\u4ee5\u65f6\u5e8f\u3001\u8bed\u8a00\u3001\u89c6\u89c9\u7b49\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e3a\u4e3b\u63a7\uff0c\u8c03\u7528\u4e00\u7cfb\u5217\u57fa\u7840\u6a21\u578b\u3001\u673a\u7406\u6a21\u578b\u3001\u4e13\u7528\u6a21\u578b\u3001\u5de5\u4e1a\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u7528\u667a\u80fd", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.a7a088b6", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4NzY4MTQ1NQ==&mid=2247485593&idx=1&sn=f699569bdafe404cae763a38b6f96ffc&chksm=91e3f4524d1b5cb33d527cbad68790a9cf875d980ca37ceed3488a0338e919e41c3c4ce6dc80#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4NzY4MTQ1NQ==&mid=2247485593&idx=1&sn=f699569bdafe404cae763a38b6f96ffc&chksm=91e3f4524d1b5cb33d527cbad68790a9cf875d980ca37ceed3488a0338e919e41c3c4ce6dc80#rd", "authors": ["AIGCPM"], "title": "\u4e00\u6587\u8ba9\u4f60\u8be6\u7ec6\u4e86\u89e3AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0eAI Agent\u7684\u5173\u7cfb", "comment": "Source: WeChat, Published: 2025-09-30 11:30:25", "summary": "\u53ef\u4ee5\u8bf4\uff0c\u5927\u6a21\u578b\u4e3aAgent\u6ce8\u5165\u4e86\u201c\u7075\u9b42\u201d\u548c\u201c\u667a\u6167\u201d\uff0c\u8ba9Agent\u4e0d\u518d\u662f\u7b80\u5355\u7684\u6267\u884c\u8005\uff0c\u800c\u662f\u80fd\u591f\u8fdb\u884c\u6709\u6df1\u5ea6\u601d\u8003\u548c\u7406\u89e3\u7684\u667a\u80fd\u4f53\u3002Agent\u901a\u8fc7\u8c03\u7528\u5927\u6a21\u578b\uff0c\u83b7\u5f97\u4e86\u524d\u6240\u672a\u6709\u7684\u201c\u667a\u80fd\u201d\u9ad8\u5ea6\u3002", "AI": {"tldr": "\u53ef\u4ee5\u8bf4\uff0c\u5927\u6a21\u578b\u4e3aAgent\u6ce8\u5165\u4e86\u201c\u7075\u9b42\u201d\u548c\u201c\u667a\u6167\u201d\uff0c\u8ba9Agent\u4e0d\u518d\u662f\u7b80\u5355\u7684\u6267\u884c\u8005\uff0c\u800c\u662f\u80fd\u591f\u8fdb\u884c\u6709\u6df1\u5ea6\u601d\u8003\u548c\u7406\u89e3\u7684\u667a\u80fd\u4f53\u3002Agent\u901a\u8fc7\u8c03\u7528\u5927\u6a21\u578b\uff0c\u83b7\u5f97\u4e86\u524d\u6240\u672a\u6709\u7684\u201c\u667a\u80fd\u201d\u9ad8\u5ea6\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.21fd71d7", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg4MDYzNjM5OQ==&mid=2247487585&idx=1&sn=b4c4f0d7d9b5b9acc4cb138cdd5b6a6d&chksm=ce522cbf34bcc0e23197aaad2af86fed0fb8522f18609a20e56a2598fafa6c936eaa2f179fc6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg4MDYzNjM5OQ==&mid=2247487585&idx=1&sn=b4c4f0d7d9b5b9acc4cb138cdd5b6a6d&chksm=ce522cbf34bcc0e23197aaad2af86fed0fb8522f18609a20e56a2598fafa6c936eaa2f179fc6#rd", "authors": ["AI\u5927\u6a21\u578b\u77e5\u8bc6\u5b98"], "title": "\u4e00\u5206\u949f\u5e26\u4f60\u4e86\u89e3<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u5e94\u7528\u8303\u5f0f\uff1aPrompt\u3001Agent\u3001 RAG", "comment": "Source: WeChat, Published: 2025-09-30 09:04:24", "summary": "\u4e00\u5206\u949f\u5e26\u4f60\u4e86\u89e3 \u5927\u6a21\u578b\u7684\u5e94\u7528\u8303\u5f0f\u3002prompt agent\uff0c rag \u5927\u6a21\u578b\uff08llm\uff0clarge language model\uff09\u662f\u57fa\u4e8e\u5927\u91cf\u6570\u636e\u8fdb\u884c \u9884\u8bad\u7ec3\u7684\u8d85\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u4ece2019\u5e74\u53d1\u5c55\u5230\u73b0\u5728\uff0c\u5176\u80fd\u529b \u5df2\u7ecf\u5f97\u5230\u4e86\u6781\u5927\u7684\u63d0\u5347\uff0c\u5176\u4e2d\u4ee5openal chatgpt\u7684\u53d1\u5e03\u4e3a\u5173\u952e \u91cc\u7a0b", "AI": {"tldr": "\u4e00\u5206\u949f\u5e26\u4f60\u4e86\u89e3 \u5927\u6a21\u578b\u7684\u5e94\u7528\u8303\u5f0f\u3002prompt agent\uff0c rag \u5927\u6a21\u578b\uff08llm\uff0clarge language model\uff09\u662f\u57fa\u4e8e\u5927\u91cf\u6570\u636e\u8fdb\u884c \u9884\u8bad\u7ec3\u7684\u8d85\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u4ece2019\u5e74\u53d1\u5c55\u5230\u73b0\u5728\uff0c\u5176\u80fd\u529b \u5df2\u7ecf\u5f97\u5230\u4e86\u6781\u5927\u7684\u63d0\u5347\uff0c\u5176\u4e2d\u4ee5openal chatgpt\u7684\u53d1\u5e03\u4e3a\u5173\u952e \u91cc\u7a0b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.2130eb32", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570745&idx=2&sn=be5b8387914c32795c926c69a30c2668&chksm=9628448ffccadb90c6662050be201ce2910c2eb8e4529cf71d9e18f4db5f7612e8c725592a95#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570745&idx=2&sn=be5b8387914c32795c926c69a30c2668&chksm=9628448ffccadb90c6662050be201ce2910c2eb8e4529cf71d9e18f4db5f7612e8c725592a95#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "\u4e0a\u4ea42025\u6700\u65b0-\u300a\u52a8\u624b\u5b66<em class=\"highlight\">\u5927\u6a21\u578b</em>\u300b\u5b9e\u6218\u6559\u7a0b\u53cappt\u5206\u4eab\uff01", "comment": "Source: WeChat, Published: 2025-09-30 08:00:00", "summary": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0agi\uff1f\u5927\u6a21\u578b\u667a \u80fd\u4f53\u4e0e\u5b89 \u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6 \u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0agi\uff1f\u5927\u6a21\u578b\u667a \u80fd\u4f53\u4e0e\u5b89 \u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6 \u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.30f6320b", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwMzcxMTU1Ng==&mid=2247483978&idx=1&sn=1a06982a2008229ea5ba63c5be28e98e&chksm=c15641f2fe90da8dfbda8578279caec3bb70b76a5d55ff8e6df693ef67a37d68201ce4177a46#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwMzcxMTU1Ng==&mid=2247483978&idx=1&sn=1a06982a2008229ea5ba63c5be28e98e&chksm=c15641f2fe90da8dfbda8578279caec3bb70b76a5d55ff8e6df693ef67a37d68201ce4177a46#rd", "authors": ["\u53f6\u5b50\u54e5AI"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0eAgent\uff1a\u73b0\u72b6\u3001\u672a\u6765\u7684\u51e0\u70b9\u601d\u8003", "comment": "Source: WeChat, Published: 2025-09-30 07:55:45", "summary": "\u8fd9\u5c06\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5927\u6a21\u578b\u7684\u5e94\u7528\u3002\u753b\u4e86\u4e00\u5f20\u56fe\uff1a\u5927\u6a21\u578b\u7b97\u529b\u589e\u957f\u8d8b\u52bf\u56fe \uff08flops demand trend for llms/vlms\uff09 token\u8c03\u7528\u91cft\uff08token\uff09 \u7b97\u529b\u603b\u9700\u6c42\u3002f \uff08flops\uff09 flops = t * c \u6a21\u578b\u7b97\u529b\u6d88\u8017c \uff08flops/token\uff09 \u65f6\u95f4 \uff08time\uff09", "AI": {"tldr": "\u8fd9\u5c06\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5927\u6a21\u578b\u7684\u5e94\u7528\u3002\u753b\u4e86\u4e00\u5f20\u56fe\uff1a\u5927\u6a21\u578b\u7b97\u529b\u589e\u957f\u8d8b\u52bf\u56fe \uff08flops demand trend for llms/vlms\uff09 token\u8c03\u7528\u91cft\uff08token\uff09 \u7b97\u529b\u603b\u9700\u6c42\u3002f \uff08flops\uff09 flops = t * c \u6a21\u578b\u7b97\u529b\u6d88\u8017c \uff08flops/token\uff09 \u65f6\u95f4 \uff08time\uff09", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.5425df16", "categories": ["wechat.article", "wechat.ai", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647615785&idx=1&sn=1b2c5415c164e4718f585f9748fcd1d4&chksm=87da17f0ae21c66fc7f2ffaf4062b1fa0c9f8d3bf79eb1e16d5d37ec604f4ef7da3e3df2b227#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3MDE0NzEzNA==&mid=2647615785&idx=1&sn=1b2c5415c164e4718f585f9748fcd1d4&chksm=87da17f0ae21c66fc7f2ffaf4062b1fa0c9f8d3bf79eb1e16d5d37ec604f4ef7da3e3df2b227#rd", "authors": ["\u8d70\u5411\u672a\u6765"], "title": "\u5982\u4f55\u5f00\u53d1\u53ef\u9760\u7684<em class=\"highlight\">\u5927\u6a21\u578b</em>\u667a\u80fd\u4f53\uff1f\u7814\u7a7639\u4e2a\u6846\u67b6\u53ca439\u4e2a\u5e94\u7528\u540e\uff0c\u53d1\u73b0\u4e86\u88ab99%\u5f00\u53d1\u8005\u5ffd\u89c6\u7684\u5730\u65b9\uff01", "comment": "Source: WeChat, Published: 2025-09-30 04:12:00", "summary": "\u6b22\u8fce\u52a0\u5165\u201c\u8d70\u5411\u672a\u6765\u201d\u77e5\u8bc6\u661f\u7403\uff0c\u4e00\u8d77\u63a2\u8ba8\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u3001\u5927\u6a21\u578b\u548cAIGC\u7684\u4ea7\u54c1\u3001\u6280\u672f\u548c\u5e94\u7528\u5b9e\u8df5\uff0c\u63a2\u8ba8\u5982\u4f55\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6765\u4e3a\u5de5\u4f5c\u589e\u6548\uff0c\u4e3a\u751f\u6d3b\u6dfb\u5f69\u3002", "AI": {"tldr": "\u6b22\u8fce\u52a0\u5165\u201c\u8d70\u5411\u672a\u6765\u201d\u77e5\u8bc6\u661f\u7403\uff0c\u4e00\u8d77\u63a2\u8ba8\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u3001\u5927\u6a21\u578b\u548cAIGC\u7684\u4ea7\u54c1\u3001\u6280\u672f\u548c\u5e94\u7528\u5b9e\u8df5\uff0c\u63a2\u8ba8\u5982\u4f55\u4f7f\u7528\u5404\u79cd\u4e0d\u540c\u7684\u4eba\u5de5\u667a\u80fd\u5927\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6765\u4e3a\u5de5\u4f5c\u589e\u6548\uff0c\u4e3a\u751f\u6d3b\u6dfb\u5f69\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.b36eba91", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5ODIzNDkxMQ==&mid=2247485396&idx=1&sn=a19ccc260071cf8b80a86c0c124b9a2c&chksm=97c52132ce5ee62a20c10c521eb4b02c333ea11c28a4b05937c025c24795207635f271c4773b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5ODIzNDkxMQ==&mid=2247485396&idx=1&sn=a19ccc260071cf8b80a86c0c124b9a2c&chksm=97c52132ce5ee62a20c10c521eb4b02c333ea11c28a4b05937c025c24795207635f271c4773b#rd", "authors": ["AI\u5927\u6a21\u578b\u8bf4"], "title": "\u9876\u4f1a\u98ce\u5411\u6807\uff1a\u4eceNeurIPS 2025\u7cbe\u9009\u8bba\u6587\uff0c\u770b\u61c2<em class=\"highlight\">\u5927\u6a21\u578b</em>\u7684\u56db\u5927\u6f14\u8fdb\u65b9\u5411", "comment": "Source: WeChat, Published: 2025-09-30 03:37:51", "summary": "\u5927\u6a21\u578b\u4e0d\u4ec5\u6301\u7eed\u63a8\u52a8\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u7684\u9769\u65b0\uff0c\u66f4\u5e7f\u6cdb\u6e17\u900f\u81f3\u673a\u5668\u4eba\u5b66\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u5177\u8eab\u667a\u80fd\u7b49\u524d\u6cbf\u4ea4\u53c9\u9886\u57df\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\u3002", "AI": {"tldr": "\u5927\u6a21\u578b\u4e0d\u4ec5\u6301\u7eed\u63a8\u52a8\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u7684\u9769\u65b0\uff0c\u66f4\u5e7f\u6cdb\u6e17\u900f\u81f3\u673a\u5668\u4eba\u5b66\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3001\u5177\u8eab\u667a\u80fd\u7b49\u524d\u6cbf\u4ea4\u53c9\u9886\u57df\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u6027\u548c\u6269\u5c55\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.e7d1bc07", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkwNjQxNTgyOA==&mid=2247484637&idx=1&sn=e01a4c7d9730c020f0f48d21022ffc12&chksm=c1dfcf4dc8c1e82dcbe5f3e8a1c76b2f02c5f6617d156cb2a8fbb9ccdaf362f214748c47cbf7#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkwNjQxNTgyOA==&mid=2247484637&idx=1&sn=e01a4c7d9730c020f0f48d21022ffc12&chksm=c1dfcf4dc8c1e82dcbe5f3e8a1c76b2f02c5f6617d156cb2a8fbb9ccdaf362f214748c47cbf7#rd", "authors": ["\u6052\u661f\u6218\u7eaa"], "title": "\u4e91\u6816\u5927\u4f1a\u70b8\u573a\uff01\u56fd\u4ea7<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5e72\u5230\u5168\u7403\u524d\u4e09\uff0c\u4e00\u573a AI \u519b\u5907\u7ade\u8d5b\u5df2\u6253\u54cd", "comment": "Source: WeChat, Published: 2025-09-30 03:09:57", "summary": "\u5982\u679c\u8bf4\u5927\u6a21\u578b\u662f \u201c\u5927\u8111\u201d\uff0c\u90a3 Agent \u5c31\u662f\u8ba9\u5927\u8111 \u201c\u52a8\u8d77\u6765\u201d \u7684\u624b\u811a\u3002\u8fd9\u6b21\u5927\u4f1a\u53d1\u5e03\u7684\u5168\u65b0 Agent \u5f00\u53d1\u6846\u67b6\uff0c\u7b97\u662f\u7ed9\u5f00\u53d1\u8005\u9001\u4e86\u4efd \u201c\u5927\u793c\u201d\uff0c\u800c\u6570\u636e\u66f4\u80fd\u8bf4\u660e\u95ee\u9898\uff1a\u8fc7\u53bb\u4e00\u5e74\uff0c\u76f8\u5173\u5e73\u53f0\u7684\u6a21\u578b\u65e5\u5747\u8c03\u7528\u91cf\u76f4\u63a5\u6da8\u4e86 15 \u500d\u3002", "AI": {"tldr": "\u5982\u679c\u8bf4\u5927\u6a21\u578b\u662f \u201c\u5927\u8111\u201d\uff0c\u90a3 Agent \u5c31\u662f\u8ba9\u5927\u8111 \u201c\u52a8\u8d77\u6765\u201d \u7684\u624b\u811a\u3002\u8fd9\u6b21\u5927\u4f1a\u53d1\u5e03\u7684\u5168\u65b0 Agent \u5f00\u53d1\u6846\u67b6\uff0c\u7b97\u662f\u7ed9\u5f00\u53d1\u8005\u9001\u4e86\u4efd \u201c\u5927\u793c\u201d\uff0c\u800c\u6570\u636e\u66f4\u80fd\u8bf4\u660e\u95ee\u9898\uff1a\u8fc7\u53bb\u4e00\u5e74\uff0c\u76f8\u5173\u5e73\u53f0\u7684\u6a21\u578b\u65e5\u5747\u8c03\u7528\u91cf\u76f4\u63a5\u6da8\u4e86 15 \u500d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.9608baf9", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA3ODQyMzQ2Mg==&mid=2247485429&idx=1&sn=65492dd47bce017dbf9d75d621dee00d&chksm=9e31ff2fad86365a878c2521cf975583144f12a3229a8633eaeaced846d83dffbf29eca6ed8d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA3ODQyMzQ2Mg==&mid=2247485429&idx=1&sn=65492dd47bce017dbf9d75d621dee00d&chksm=9e31ff2fad86365a878c2521cf975583144f12a3229a8633eaeaced846d83dffbf29eca6ed8d#rd", "authors": ["\u5e73\u884c\u8bb0\u9646"], "title": "AI Infra\uff1a<em class=\"highlight\">\u5927\u6a21\u578b</em>\u4e0d\u601d\u8003\uff0c\u4e0a\u4e0b\u6587\u66ff\u5b83\u601d\u8003", "comment": "Source: WeChat, Published: 2025-09-30 00:00:17", "summary": "\u25cf\u662f\u5927\u6a21\u578b\u843d\u5730\u8fc7\u7a0b\u4e2d\uff0c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5728\u5e2e\u52a9 Agent \u201c\u601d\u8003\u201d\uff0c\u63d0\u4f9b\u538b\u7f29\u540e\u7684\u7cbe\u51c6\u4fe1\u606f2.3 \u77e5\u8bc6\u6709\u5411\u5927\u6a21\u578b\u5185\u5316\u7684\u613f\u671b\u25cf\u957f\u671f\u770b\uff0c\u5927\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u6269\u5927\uff0c\u662f\u77e5\u8bc6\u5185\u5316\u7684\u8fc7\u7a0b", "AI": {"tldr": "\u25cf\u662f\u5927\u6a21\u578b\u843d\u5730\u8fc7\u7a0b\u4e2d\uff0c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5728\u5e2e\u52a9 Agent \u201c\u601d\u8003\u201d\uff0c\u63d0\u4f9b\u538b\u7f29\u540e\u7684\u7cbe\u51c6\u4fe1\u606f2.3 \u77e5\u8bc6\u6709\u5411\u5927\u6a21\u578b\u5185\u5316\u7684\u613f\u671b\u25cf\u957f\u671f\u770b\uff0c\u5927\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u6269\u5927\uff0c\u662f\u77e5\u8bc6\u5185\u5316\u7684\u8fc7\u7a0b", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2509.471ed82d", "categories": ["wechat.article", "wechat.ai", "wechat.cl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570743&idx=2&sn=54f0898d781b640fde4188d0e7219f8d&chksm=96f98c46bf7fcc30f61eb933a137b0aca6971afa43112201a0399c93158fe4c7ff514075aeaf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247570743&idx=2&sn=54f0898d781b640fde4188d0e7219f8d&chksm=96f98c46bf7fcc30f61eb933a137b0aca6971afa43112201a0399c93158fe4c7ff514075aeaf#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "\u4e0a\u4ea42025\u6700\u65b0-\u300a\u52a8\u624b\u5b66<em class=\"highlight\">\u5927\u6a21\u578b</em>\u300b\u5b9e\u6218\u6559\u7a0b\u53cappt\u5206\u4eab\uff01", "comment": "Source: WeChat, Published: 2025-09-30 00:00:00", "summary": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0agi\uff1f\u5927\u6a21\u578b\u667a\u80fd\u4f53\u4e0e\u5b89\u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6\u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5e2e\u52a9\u5b9e\u73b0agi\uff1f\u5927\u6a21\u578b\u667a\u80fd\u4f53\u4e0e\u5b89\u5168 \u5927\u6a21\u578b\u667a\u80fd\u4f53\u8fc8\u5411\u4e86\u672a\u6765\u64cd\u4f5c\u7cfb\u7edf\u4e4b\u65c5\u3002\u7136\u800c\uff0c\u5927\u6a21\u578b\u5728\u5f00\u653e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u80fd\u610f\u8bc6\u5230\u98ce\u9669\u5a01\u80c1\u5417\uff1f", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
