{"id": "2510.07363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07363", "abs": "https://arxiv.org/abs/2510.07363", "authors": ["Tianxiang Xu", "Zhichao Wen", "Xinyu Zhao", "Jun Wang", "Yan Li", "Chang Liu"], "title": "L2M-AID: Autonomous Cyber-Physical Defense by Fusing Semantic Reasoning of Large Language Models with Multi-Agent Reinforcement Learning (Preprint)", "comment": "This preprint was submitted to IEEE TrustCom 2025. The accepted\n  version will be published under copyright 2025 IEEE", "summary": "The increasing integration of Industrial IoT (IIoT) exposes critical\ncyber-physical systems to sophisticated, multi-stage attacks that elude\ntraditional defenses lacking contextual awareness. This paper introduces\nL2M-AID, a novel framework for Autonomous Industrial Defense using\nLLM-empowered, Multi-agent reinforcement learning. L2M-AID orchestrates a team\nof collaborative agents, each driven by a Large Language Model (LLM), to\nachieve adaptive and resilient security. The core innovation lies in the deep\nfusion of two AI paradigms: we leverage an LLM as a semantic bridge to\ntranslate vast, unstructured telemetry into a rich, contextual state\nrepresentation, enabling agents to reason about adversary intent rather than\nmerely matching patterns. This semantically-aware state empowers a Multi-Agent\nReinforcement Learning (MARL) algorithm, MAPPO, to learn complex cooperative\nstrategies. The MARL reward function is uniquely engineered to balance security\nobjectives (threat neutralization) with operational imperatives, explicitly\npenalizing actions that disrupt physical process stability. To validate our\napproach, we conduct extensive experiments on the benchmark SWaT dataset and a\nnovel synthetic dataset generated based on the MITRE ATT&CK for ICS framework.\nResults demonstrate that L2M-AID significantly outperforms traditional IDS,\ndeep learning anomaly detectors, and single-agent RL baselines across key\nmetrics, achieving a 97.2% detection rate while reducing false positives by\nover 80% and improving response times by a factor of four. Crucially, it\ndemonstrates superior performance in maintaining physical process stability,\npresenting a robust new paradigm for securing critical national infrastructure.", "AI": {"tldr": "L2M-AID\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u8d4b\u80fd\u7684MARL\u81ea\u4e3b\u5de5\u4e1a\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\u5c06\u975e\u7ed3\u6784\u5316\u9065\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u72b6\u6001\u8868\u793a\uff0c\u7ed3\u5408MAPPO\u7b97\u6cd5\u5b66\u4e60\u590d\u6742\u534f\u4f5c\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u7269\u7406\u8fc7\u7a0b\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5a01\u80c1\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51(IIoT)\u7684\u666e\u53ca\u4f7f\u5173\u952e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u9762\u4e34\u4f20\u7edf\u9632\u5fa1\u65e0\u6cd5\u5e94\u5bf9\u7684\u591a\u9636\u6bb5\u590d\u6742\u653b\u51fb\uff0c\u9700\u8981\u5177\u5907\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u7684\u81ea\u9002\u5e94\u5b89\u5168\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faL2M-AID\u6846\u67b6\uff0c\u878d\u5408LLM\u548cMARL\u4e24\u5927AI\u8303\u5f0f\uff1a\u4f7f\u7528LLM\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\u5c06\u975e\u7ed3\u6784\u5316\u9065\u6d4b\u6570\u636e\u8f6c\u6362\u4e3a\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u72b6\u6001\u8868\u793a\uff0c\u7136\u540e\u91c7\u7528MAPPO\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b66\u4e60\u534f\u4f5c\u9632\u5fa1\u7b56\u7565\uff0c\u5956\u52b1\u51fd\u6570\u5e73\u8861\u5b89\u5168\u76ee\u6807\u548c\u64cd\u4f5c\u8981\u6c42\u3002", "result": "\u5728SWaT\u57fa\u51c6\u6570\u636e\u96c6\u548c\u57fa\u4e8eMITRE ATT&CK for ICS\u6846\u67b6\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cL2M-AID\u663e\u8457\u4f18\u4e8e\u4f20\u7edfIDS\u3001\u6df1\u5ea6\u5b66\u4e60\u5f02\u5e38\u68c0\u6d4b\u5668\u548c\u5355\u667a\u80fd\u4f53RL\u57fa\u7ebf\uff0c\u68c0\u6d4b\u7387\u8fbe\u523097.2%\uff0c\u8bef\u62a5\u7387\u964d\u4f4e80%\u4ee5\u4e0a\uff0c\u54cd\u5e94\u65f6\u95f4\u63d0\u9ad84\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u8fc7\u7a0b\u7a33\u5b9a\u6027\u3002", "conclusion": "L2M-AID\u4e3a\u4fdd\u62a4\u5173\u952e\u56fd\u5bb6\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u8303\u5f0f\uff0c\u5c55\u793a\u4e86LLM\u4e0eMARL\u6df1\u5ea6\u878d\u5408\u5728\u5de5\u4e1a\u5b89\u5168\u9632\u5fa1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07604", "categories": ["cs.SE", "D.2.4"], "pdf": "https://arxiv.org/pdf/2510.07604", "abs": "https://arxiv.org/abs/2510.07604", "authors": ["Yubo Bai", "Tapti Palit"], "title": "RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code", "comment": "13 pages to appear in Proceedings of ASE 2025", "summary": "Rust is a memory-safe programming language that significantly improves\nsoftware security. Existing codebases written in unsafe memory languages, such\nas C, must first be transpiled to Rust to take advantage of Rust's improved\nsafety guarantees. RustAssure presents a system that uses Large Language Models\n(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses\nprompt engineering techniques to maximize the chances of the LLM generating\nidiomatic and safe Rust code. Moreover, because LLMs often generate code with\nsubtle bugs that can be missed under traditional unit or fuzz testing,\nRustAssure performs differential symbolic testing to establish the semantic\nsimilarity between the original C and LLM-transpiled Rust code. We evaluated\nRustAssure with five real-world applications and libraries, and showed that our\nsystem is able to generate compilable Rust functions for 89.8% of all C\nfunctions, of which 69.9% produced equivalent symbolic return values for both\nthe C and Rust functions.", "AI": {"tldr": "RustAssure\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5c06C\u4ee3\u7801\u8f6c\u6362\u4e3aRust\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u5dee\u5206\u7b26\u53f7\u6d4b\u8bd5\u9a8c\u8bc1\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u5b9e\u73b089.8%\u7684C\u51fd\u6570\u53ef\u7f16\u8bd1\u8f6c\u6362\uff0c\u5176\u4e2d69.9%\u7684\u51fd\u6570\u5728\u7b26\u53f7\u8fd4\u56de\u503c\u4e0a\u7b49\u4ef7\u3002", "motivation": "Rust\u4f5c\u4e3a\u5185\u5b58\u5b89\u5168\u8bed\u8a00\u80fd\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5b89\u5168\u6027\uff0c\u4f46\u73b0\u6709\u7528\u4e0d\u5b89\u5168\u5185\u5b58\u8bed\u8a00\uff08\u5982C\uff09\u7f16\u5199\u7684\u4ee3\u7801\u5e93\u9700\u8981\u5148\u8f6c\u6362\u4e3aRust\u624d\u80fd\u5229\u7528\u5176\u5b89\u5168\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u4f18\u5316LLM\u751f\u6210\u60ef\u7528\u5b89\u5168\u7684Rust\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u5dee\u5206\u7b26\u53f7\u6d4b\u8bd5\u9a8c\u8bc1\u539f\u59cbC\u4ee3\u7801\u548cLLM\u8f6c\u6362\u7684Rust\u4ee3\u7801\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u57285\u4e2a\u771f\u5b9e\u5e94\u7528\u548c\u5e93\u7684\u8bc4\u4f30\u4e2d\uff0c\u7cfb\u7edf\u80fd\u4e3a89.8%\u7684C\u51fd\u6570\u751f\u6210\u53ef\u7f16\u8bd1\u7684Rust\u51fd\u6570\uff0c\u5176\u4e2d69.9%\u7684\u51fd\u6570\u5728\u7b26\u53f7\u8fd4\u56de\u503c\u4e0a\u7b49\u4ef7\u3002", "conclusion": "RustAssure\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5c06C\u4ee3\u7801\u8f6c\u6362\u4e3aRust\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u7b26\u53f7\u6d4b\u8bd5\u9a8c\u8bc1\u8f6c\u6362\u7684\u6b63\u786e\u6027\u3002", "topic": "code agent"}}
{"id": "2510.07740", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07740", "abs": "https://arxiv.org/abs/2510.07740", "authors": ["Dezhi Ran", "Yuan Cao", "Mengzhou Wu", "Simin Chen", "Yuzhe Guo", "Jun Ren", "Zihe Song", "Hao Yu", "Jialei Wei", "Linyi Li", "Wei Yang", "Baishakhi Ray", "Tao Xie"], "title": "AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?", "comment": "Under Review. Benchmark and leadboards at\n  https://appforge-bench.github.io/", "summary": "Large language models (LLMs) have demonstrated remarkable capability in\nfunction-level code generation tasks. Unlike isolated functions, real-world\napplications demand reasoning over the entire software system: developers must\norchestrate how different components interact, maintain consistency across\nstates over time, and ensure the application behaves correctly within the\nlifecycle and framework constraints. Yet, no existing benchmark adequately\nevaluates whether LLMs can bridge this gap and construct entire software\nsystems from scratch. To address this gap, we propose APPFORGE, a benchmark\nconsisting of 101 software development problems drawn from real-world Android\napps. Given a natural language specification detailing the app functionality, a\nlanguage model is tasked with implementing the functionality into an Android\napp from scratch. Developing an Android app from scratch requires understanding\nand coordinating app states, lifecycle management, and asynchronous operations,\ncalling for LLMs to generate context-aware, robust, and maintainable code. To\nconstruct APPFORGE, we design a multi-agent system to automatically summarize\nthe main functionalities from app documents and navigate the app to synthesize\ntest cases validating the functional correctness of app implementation.\nFollowing rigorous manual verification by Android development experts, APPFORGE\nincorporates the test cases within an automated evaluation framework that\nenables reproducible assessment without human intervention, making it easily\nadoptable for future research. Our evaluation on 12 flagship LLMs show that all\nevaluated models achieve low effectiveness, with the best-performing model\n(GPT-5) developing only 18.8% functionally correct applications, highlighting\nfundamental limitations in current models' ability to handle complex,\nmulti-component software engineering challenges.", "AI": {"tldr": "APPFORGE\u662f\u4e00\u4e2a\u5305\u542b101\u4e2a\u771f\u5b9eAndroid\u5e94\u7528\u5f00\u53d1\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ece\u96f6\u6784\u5efa\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u7684\u80fd\u529b\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6700\u4f73\u6a21\u578b(GPT-5)\u4ec5\u80fd\u5f00\u53d118.8%\u529f\u80fd\u6b63\u786e\u7684\u5e94\u7528\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u7ec4\u4ef6\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30LLM\u5728\u51fd\u6570\u7ea7\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u65e0\u6cd5\u8bc4\u4f30\u5176\u6784\u5efa\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u7684\u80fd\u529b\u3002\u771f\u5b9e\u5e94\u7528\u5f00\u53d1\u9700\u8981\u534f\u8c03\u4e0d\u540c\u7ec4\u4ef6\u4ea4\u4e92\u3001\u7ef4\u62a4\u72b6\u6001\u4e00\u81f4\u6027\u3001\u786e\u4fdd\u5728\u751f\u547d\u5468\u671f\u548c\u6846\u67b6\u7ea6\u675f\u4e0b\u7684\u6b63\u786e\u884c\u4e3a\u3002", "method": "\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u4ece\u5e94\u7528\u6587\u6863\u4e2d\u603b\u7ed3\u4e3b\u8981\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u5bfc\u822a\u5e94\u7528\u5408\u6210\u6d4b\u8bd5\u7528\u4f8b\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u3002\u7ecf\u8fc7Android\u5f00\u53d1\u4e13\u5bb6\u4e25\u683c\u624b\u52a8\u9a8c\u8bc1\u540e\uff0c\u5c06\u6d4b\u8bd5\u7528\u4f8b\u6574\u5408\u5230\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u4e2d\u3002", "result": "\u5bf912\u4e2a\u4e3b\u6d41LLM\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6240\u6709\u6a21\u578b\u6548\u679c\u90fd\u5f88\u4f4e\uff0c\u6700\u4f73\u6a21\u578bGPT-5\u4ec5\u80fd\u5f00\u53d118.8%\u529f\u80fd\u6b63\u786e\u7684\u5e94\u7528\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u5e94\u5bf9\u590d\u6742\u3001\u591a\u7ec4\u4ef6\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6311\u6218\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u6784\u5efa\u5b8c\u6574\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002", "topic": "swe benchmark"}}
{"id": "2510.07414", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.07414", "abs": "https://arxiv.org/abs/2510.07414", "authors": ["Mufei Li", "Dongqi Fu", "Limei Wang", "Si Zhang", "Hanqing Zeng", "Kaan Sancak", "Ruizhong Qiu", "Haoyu Wang", "Xiaoxin He", "Xavier Bresson", "Yinglong Xia", "Chonglin Sun", "Pan Li"], "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "comment": "Code available at https://github.com/Graph-COM/HaystackCraft", "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.", "AI": {"tldr": "HaystackCraft\u662f\u4e00\u4e2a\u65b0\u7684\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5f02\u6784\u68c0\u7d22\u7b56\u7565\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u566a\u58f0\u7684\u771f\u5b9e\u957f\u6587\u672c\u6765\u8bc4\u4f30LLM\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684'\u5927\u6d77\u635e\u9488'\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7531\u6709\u504f\u68c0\u7d22\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u4ea7\u751f\u7684\u566a\u58f0\u4e0a\u4e0b\u6587\uff0c\u9700\u8981\u6784\u5efa\u66f4\u771f\u5b9e\u7684\u6d4b\u8bd5\u73af\u5883\u3002", "method": "\u57fa\u4e8e\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u8d85\u94fe\u63a5\u7f51\u7edc\u6784\u5efaHaystackCraft\u57fa\u51c6\uff0c\u5305\u542b\u591a\u8df3\u95ee\u9898\uff0c\u8bc4\u4f30\u5f02\u6784\u68c0\u7d22\u7b56\u7565\u5bf9\u5e72\u6270\u9879\u7ec4\u6210\u548cLLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u6269\u5c55\u5230\u52a8\u6001\u667a\u80fd\u4f53\u64cd\u4f5c\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1)\u66f4\u5f3a\u7684\u5bc6\u96c6\u68c0\u7d22\u5668\u4f1a\u5f15\u5165\u66f4\u5177\u6311\u6218\u6027\u7684\u5e72\u6270\u9879\uff0c\u4f46\u57fa\u4e8e\u56fe\u7684\u91cd\u65b0\u6392\u5e8f\u80fd\u540c\u65f6\u63d0\u9ad8\u68c0\u7d22\u6548\u679c\u548c\u51cf\u8f7b\u6709\u5bb3\u5e72\u6270\uff1b2)\u5728\u667a\u80fd\u4f53\u6d4b\u8bd5\u4e2d\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u4e5f\u4f1a\u56e0\u81ea\u751f\u6210\u5e72\u6270\u9879\u800c\u51fa\u73b0\u7ea7\u8054\u5931\u8d25\u6216\u96be\u4ee5\u63d0\u524d\u505c\u6b62\u3002", "conclusion": "\u667a\u80fd\u4f53\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4ecd\u9762\u4e34\u6301\u7eed\u6311\u6218\uff0cHaystackCraft\u4e3a\u672a\u6765\u8fdb\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "topic": "agent analysis"}}
{"id": "2510.07432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07432", "abs": "https://arxiv.org/abs/2510.07432", "authors": ["Penghang Liu", "Elizabeth Fons", "Svitlana Vyetrenko", "Daniel Borrajo", "Vamsi Potluru", "Manuela Veloso"], "title": "TS-Agent: A Time Series Reasoning Agent with Iterative Statistical Insight Gathering", "comment": "NeurIPS 2025 Workshop on Foundations of Reasoning in Language Models", "summary": "Large language models (LLMs) have shown strong abilities in reasoning and\nproblem solving, but recent studies reveal that they still struggle with time\nseries reasoning tasks, where outputs are often affected by hallucination or\nknowledge leakage. In this work we propose TS-Agent, a time series reasoning\nagent that leverages LLMs strictly for what they excel at, i.e., gathering\nevidence and synthesizing it into conclusions through step-by-step reasoning,\nwhile delegating the extraction of statistical and structural information to\ntime series analytical tools. Instead of mapping time series into text tokens,\nimages, or embeddings, our agent interacts with raw numeric sequences through\natomic operators, records outputs in an explicit evidence log, and iteratively\nrefines its reasoning under the guidance of a self-critic and a final quality\ngate. This design avoids multi-modal alignment training, preserves the native\nform of time series, ensures interpretability and verifiability, and mitigates\nknowledge leakage or hallucination. Empirically, we evaluate the agent on\nestablished benchmarks. Our experiments show that TS-Agent achieves performance\ncomparable to state-of-the-art LLMs on understanding benchmarks, and delivers\nsignificant improvements on reasoning tasks, where existing models often rely\non memorization and fail in zero-shot settings.", "AI": {"tldr": "TS-Agent\u662f\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u5c06LLM\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5de5\u5177\u76f8\u7ed3\u5408\uff0c\u907f\u514d\u77e5\u8bc6\u6cc4\u6f0f\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5e7b\u89c9\u548c\u77e5\u8bc6\u6cc4\u6f0f\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5145\u5206\u5229\u7528LLM\u63a8\u7406\u80fd\u529b\u540c\u65f6\u907f\u514d\u8fd9\u4e9b\u7f3a\u9677\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u8fdb\u884c\u8bc1\u636e\u6536\u96c6\u548c\u63a8\u7406\u5408\u6210\uff0c\u5c06\u7edf\u8ba1\u548c\u7ed3\u6784\u4fe1\u606f\u63d0\u53d6\u59d4\u6258\u7ed9\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u539f\u5b50\u64cd\u4f5c\u7b26\u4e0e\u539f\u59cb\u6570\u503c\u5e8f\u5217\u4ea4\u4e92\uff0c\u5728\u8bc1\u636e\u65e5\u5fd7\u4e2d\u8bb0\u5f55\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u548c\u8d28\u91cf\u95e8\u63a7\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\u3002", "result": "\u5728\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4e0e\u6700\u5148\u8fdbLLM\u76f8\u5f53\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TS-Agent\u7684\u8bbe\u8ba1\u907f\u514d\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u8bad\u7ec3\uff0c\u4fdd\u7559\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u539f\u59cb\u5f62\u5f0f\uff0c\u786e\u4fdd\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u77e5\u8bc6\u6cc4\u6f0f\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2510.08005", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08005", "abs": "https://arxiv.org/abs/2510.08005", "authors": ["Utku Boran Torun", "Mehmet Taha Demircan", "Mahmut Furkan G\u00f6n", "Eray T\u00fcz\u00fcn"], "title": "Past, Present, and Future of Bug Tracking in the Generative AI Era", "comment": "Submitted to ACM TOSEM Special Issue: 2030 Software Engineering\n  Roadmap", "summary": "Traditional bug tracking systems rely heavily on manual reporting,\nreproduction, triaging, and resolution, each carried out by different\nstakeholders such as end users, customer support, developers, and testers. This\ndivision of responsibilities requires significant coordination and widens the\ncommunication gap between non-technical users and technical teams, slowing the\nprocess from bug discovery to resolution. Moreover, current systems are highly\nasynchronous; users often wait hours or days for a first response, delaying\nfixes and contributing to frustration. This paper examines the evolution of bug\ntracking, from early paper-based reporting to today's web-based and SaaS\nplatforms. Building on this trajectory, we propose an AI-powered bug tracking\nframework that augments existing tools with intelligent, large language model\n(LLM)-driven automation. Our framework addresses two main challenges: reducing\ntime-to-fix and minimizing human overhead. Users report issues in natural\nlanguage, while AI agents refine reports, attempt reproduction, and request\nmissing details. Reports are then classified, invalid ones resolved through\nno-code fixes, and valid ones localized and assigned to developers. LLMs also\ngenerate candidate patches, with human oversight ensuring correctness. By\nintegrating automation into each phase, our framework accelerates response\ntimes, improves collaboration, and strengthens software maintenance practices\nfor a more efficient, user-centric future.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684AI\u9a71\u52a8bug\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u81ea\u52a8\u5316\u51cf\u5c11\u4fee\u590d\u65f6\u95f4\u548c\u4eba\u5de5\u5f00\u9500\uff0c\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u62a5\u544a\u5230\u81ea\u52a8\u4fee\u590d\u7684\u5168\u6d41\u7a0b\u4f18\u5316", "motivation": "\u4f20\u7edfbug\u8ffd\u8e2a\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u62a5\u544a\u3001\u590d\u73b0\u3001\u5206\u7c7b\u548c\u4fee\u590d\uff0c\u5b58\u5728\u6c9f\u901a\u6548\u7387\u4f4e\u3001\u54cd\u5e94\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u9700\u8981AI\u81ea\u52a8\u5316\u6765\u63d0\u5347\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c", "method": "\u6784\u5efaAI\u9a71\u52a8\u7684bug\u8ffd\u8e2a\u6846\u67b6\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u62a5\u544a\u7cbe\u70bc\u3001\u81ea\u52a8\u590d\u73b0\u3001\u5206\u7c7b\u3001\u65e0\u6548\u62a5\u544a\u65e0\u4ee3\u7801\u4fee\u590d\u3001\u95ee\u9898\u5b9a\u4f4d\u3001\u81ea\u52a8\u5206\u914d\u548c\u5019\u9009\u8865\u4e01\u751f\u6210", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u52a0\u901f\u54cd\u5e94\u65f6\u95f4\uff0c\u6539\u5584\u534f\u4f5c\u6548\u7387\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u5347\u8f6f\u4ef6\u7ef4\u62a4\u5b9e\u8df5\u7684\u6548\u7387", "conclusion": "AI\u9a71\u52a8\u7684bug\u8ffd\u8e2a\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u81ea\u52a8\u5316\u5230\u6bcf\u4e2a\u9636\u6bb5\uff0c\u4e3a\u66f4\u9ad8\u6548\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u672a\u6765\u8f6f\u4ef6\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848", "topic": "swe application"}}
{"id": "2510.07358", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07358", "abs": "https://arxiv.org/abs/2510.07358", "authors": ["Yeskendir Koishekenov", "Aldo Lipani", "Nicola Cancedda"], "title": "Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts", "comment": null, "summary": "Most efforts to improve the reasoning capabilities of large language models\n(LLMs) involve either scaling the number of parameters and the size of training\ndata, or scaling inference computation by letting models generate complex\nchains of thought. Motivated by interpretability studies showing that the\ncrucial computation required for reasoning tasks is concentrated in a limited\nrange of layers, we introduce Encode-Think-Decode (ETD), a method that enhances\nthe reasoning capabilities of a base model by training it to iterate over a\nsmall subset of reasoning-relevant layers during the mid-training stage. ETD\namplifies latent reasoning while preserving the original architecture,\nparameter count, hyperparameters, and training data composition. When iterating\non the selected layers at inference time, ETD models yield substantial gains on\n17 reasoning benchmarks, including +28.4% relative accuracy improvement on\nGSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an\nadaptive depth strategy that adjusts the computation per input token. Our\nresults show that recursive latent reasoning offers a simple and effective path\nto stronger LLM reasoning.", "AI": {"tldr": "ETD\u65b9\u6cd5\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u8fed\u4ee3\u5173\u952e\u5c42\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u53c2\u6570\u6570\u91cf\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u6269\u5927\u6a21\u578b\u89c4\u6a21\u6216\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u91cf\uff0c\u800c\u7814\u7a76\u8868\u660e\u63a8\u7406\u6240\u9700\u7684\u5173\u952e\u8ba1\u7b97\u96c6\u4e2d\u5728\u6709\u9650\u5c42\u4e2d\uff0c\u56e0\u6b64\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "Encode-Think-Decode (ETD)\u65b9\u6cd5\uff1a\u5728\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\u8bad\u7ec3\u6a21\u578b\u5728\u63a8\u7406\u76f8\u5173\u5c42\u4e0a\u8fdb\u884c\u8fed\u4ee3\uff0c\u63a8\u7406\u65f6\u5728\u9009\u5b9a\u5c42\u4e0a\u8fed\u4ee3\uff0c\u540c\u65f6\u63a2\u7d22\u81ea\u9002\u5e94\u6df1\u5ea6\u7b56\u7565\u3002", "result": "\u572817\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0cOLMo-2 1B\u6a21\u578b\u5728GSM8K\u4e0a\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u534728.4%\uff0c\u5728MATH\u4e0a\u63d0\u534736%\u3002", "conclusion": "\u9012\u5f52\u6f5c\u5728\u63a8\u7406\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u7b80\u5355\u6709\u6548\u7684\u8def\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2510.07486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07486", "abs": "https://arxiv.org/abs/2510.07486", "authors": ["Shuqing Luo", "Yilin Guan", "Pingzhi Li", "Hanrui Wang", "Tianlong Chen"], "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding", "comment": "14 pages, 17 figures", "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).", "AI": {"tldr": "AsyncSpade\u662f\u4e00\u4e2a\u5f02\u6b65\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u67e5\u8be2\u72b6\u6001\u548c\u5206\u79bbKV\u7f13\u5b58\u8fc7\u6ee4\u4e0e\u89e3\u7801\u5faa\u73af\uff0c\u89e3\u51b3\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2dKV\u7f13\u5b58\u7ebf\u6027\u589e\u957f\u5bfc\u81f4\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u901a\u8fc7\u957f\u601d\u7ef4\u94fe\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46KV\u7f13\u5b58\u7684\u7ebf\u6027\u589e\u957f\u52a0\u5267\u4e86\u5185\u5b58\u74f6\u9888\u3002\u73b0\u6709\u7684\u67e5\u8be2\u611f\u77e5\u7a00\u758f\u89e3\u7801\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u4f9d\u8d56\u548c\u7c97\u7c92\u5ea6token\u9009\u62e9\uff0c\u5728\u9ad8\u5e76\u53d1\u548c\u957f\u601d\u7ef4\u94fe\u573a\u666f\u4e0b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faAsyncSpade\u6846\u67b6\uff0c\u5305\u542b\uff1a(1)\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u56de\u5f52\u6a21\u5757\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\u7684\u67e5\u8be2\u72b6\u6001\uff1b(2)\u5f02\u6b65\u89e3\u8026\u6846\u67b6\uff0c\u5c06KV\u7f13\u5b58\u8fc7\u6ee4\u4e0e\u81ea\u56de\u5f52\u89e3\u7801\u5faa\u73af\u5206\u79bb\uff0c\u901a\u8fc7\u5f02\u6b65\u673a\u5236\u91cd\u53e0KV\u9009\u62e9\u4e0e\u524d\u5411\u63a8\u7406\u8ba1\u7b97\u3002", "result": "\u5728A100\u8282\u70b9\u4e0a\uff0cAsyncSpade\u5b8c\u5168\u91cd\u53e0KV\u7f13\u5b58\u64cd\u4f5c\u4e0e\u63a8\u7406\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u7406\u8bba\u6700\u4f18\u65f6\u95f4\u6bcf\u8f93\u51fatoken\u3002\u76f8\u6bd4SoTA\u57fa\u7ebfQuest\u51cf\u5c1120%\u4ee5\u4e0aTPOT\uff0c\u76f8\u6bd4\u5168\u6ce8\u610f\u529b\u51cf\u5c11\u81f3\u5c1150%TPOT\uff0c\u540c\u65f6\u5728\u591a\u4e2aTTS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u6216\u8d85\u8d8a\u51c6\u786e\u6027\u3002", "conclusion": "AsyncSpade\u9996\u6b21\u5728\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u6d88\u9664\u4e86\u987a\u5e8f\u4f9d\u8d56\uff0c\u4e3a\u9ad8\u6548\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.07429", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07429", "abs": "https://arxiv.org/abs/2510.07429", "authors": ["Wang Wei", "Tiankai Yang", "Hongjie Chen", "Yue Zhao", "Franck Dernoncourt", "Ryan A. Rossi", "Hoda Eldardiry"], "title": "Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs", "comment": "16 pages, 3 figures", "summary": "Efficient use of large language models (LLMs) is critical for deployment at\nscale: without adaptive routing, systems either overpay for strong models or\nrisk poor performance from weaker ones. Selecting the right LLM for each query\nis fundamentally an online decision problem: models differ in strengths, prices\nfluctuate, and users value accuracy and cost differently. Yet most routers are\ntrained offline with labels for all candidate models, an assumption that breaks\nin deployment, where only the outcome of the chosen model is observed. We\nbridge this gap with BaRP, a Bandit-feedback Routing with Preferences approach\nthat trains under the same partial-feedback restriction as deployment, while\nsupporting preference-tunable inference: operators can dial the\nperformance/cost trade-off at test time without retraining. Framed as a\ncontextual bandit over prompt features and a user preference vector, our method\nsimulates an online feedback setting during training and adapts its routing\ndecisions to each new prompt, rather than depending on full-information offline\nsupervision. Comprehensive experiments show that our method consistently\noutperforms strong offline routers by at least 12.46% and the largest LLM by at\nleast 2.45%, and generalizes robustly for unseen tasks.", "AI": {"tldr": "BaRP\u662f\u4e00\u79cd\u57fa\u4e8ebandit\u53cd\u9988\u7684\u8def\u7531\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u8bad\u7ec3\u65f6\u6a21\u62df\u5728\u7ebf\u53cd\u9988\u8bbe\u7f6e\uff0c\u652f\u6301\u5728\u6d4b\u8bd5\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8c03\u6574\u6027\u80fd/\u6210\u672c\u6743\u8861\u504f\u597d\u3002", "motivation": "\u5927\u89c4\u6a21\u90e8\u7f72LLM\u9700\u8981\u9ad8\u6548\u7684\u8def\u7531\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u8def\u7531\u5668\u5927\u591a\u57fa\u4e8e\u79bb\u7ebf\u8bad\u7ec3\uff0c\u65e0\u6cd5\u9002\u5e94\u90e8\u7f72\u65f6\u7684\u90e8\u5206\u53cd\u9988\u9650\u5236\u548c\u52a8\u6001\u53d8\u5316\u3002", "method": "\u5c06LLM\u8def\u7531\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587bandit\u95ee\u9898\uff0c\u57fa\u4e8e\u63d0\u793a\u7279\u5f81\u548c\u7528\u6237\u504f\u597d\u5411\u91cf\u8fdb\u884c\u51b3\u7b56\uff0c\u5728\u8bad\u7ec3\u65f6\u6a21\u62df\u5728\u7ebf\u53cd\u9988\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u5f3a\u79bb\u7ebf\u8def\u7531\u5668\u6027\u80fd\u63d0\u5347\u81f3\u5c1112.46%\uff0c\u6bd4\u6700\u5927LLM\u63d0\u5347\u81f3\u5c112.45%\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u3002", "conclusion": "BaRP\u65b9\u6cd5\u5728\u90e8\u5206\u53cd\u9988\u9650\u5236\u4e0b\u6709\u6548\u89e3\u51b3\u4e86LLM\u8def\u7531\u95ee\u9898\uff0c\u652f\u6301\u7075\u6d3b\u7684\u504f\u597d\u8c03\u6574\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2510.07517", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.07517", "abs": "https://arxiv.org/abs/2510.07517", "authors": ["Hyeong Kyu Choi", "Xiaojin Zhu", "Yixuan Li"], "title": "Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization", "comment": null, "summary": "Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning\nby letting multiple agents exchange answers and then aggregate their opinions.\nYet recent studies reveal that agents are not neutral: they are prone to\nidentity-driven sycophancy and self-bias, uncritically adopting a peer's view\nor stubbornly adhering to their own prior output, undermining the reliability\nof debate. In this work, we present the first principled framework that joins\nsycophancy and self-bias to mitigate and quantify identity bias in MAD. First,\nwe formalize the debate dynamics as an identity-weighted Bayesian update\nprocess. Second, we propose response anonymization: by removing identity\nmarkers from prompts, agents cannot distinguish \"self\" from \"peer\", which\nforces equal weights on agent identity, thereby reducing bias. Third, we define\nthe Identity Bias Coefficient (IBC), a principled metric that measures how\noften an agent follows a peer versus itself. Empirical studies across multiple\nmodels, datasets and debate rounds confirm that identity bias is widespread,\nwith sycophancy far more common than self-bias. Our findings highlight the need\nto \"mask\" identity to ensure that MAD systems reason based on content rather\nthan source identity. Code is released in\nhttps://github.com/deeplearning-wisc/MAD-identity-bias.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\u6765\u91cf\u5316\u548c\u51cf\u8f7b\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u7684\u8eab\u4efd\u504f\u89c1\uff0c\u5305\u62ec\u8eab\u4efd\u52a0\u6743\u8d1d\u53f6\u65af\u66f4\u65b0\u8fc7\u7a0b\u3001\u54cd\u5e94\u533f\u540d\u5316\u65b9\u6cd5\u548c\u8eab\u4efd\u504f\u89c1\u7cfb\u6570\u6307\u6807\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e2d\u667a\u80fd\u4f53\u5b58\u5728\u8eab\u4efd\u9a71\u52a8\u7684\u5949\u627f\u548c\u81ea\u6211\u504f\u89c1\uff0c\u4f1a\u4e0d\u52a0\u6279\u5224\u5730\u91c7\u7eb3\u540c\u4f34\u89c2\u70b9\u6216\u56fa\u6267\u575a\u6301\u81ea\u5df1\u5148\u524d\u8f93\u51fa\uff0c\u5f71\u54cd\u8fa9\u8bba\u53ef\u9760\u6027\u3002", "method": "1. \u5c06\u8fa9\u8bba\u52a8\u6001\u5f62\u5f0f\u5316\u4e3a\u8eab\u4efd\u52a0\u6743\u8d1d\u53f6\u65af\u66f4\u65b0\u8fc7\u7a0b\uff1b2. \u63d0\u51fa\u54cd\u5e94\u533f\u540d\u5316\u65b9\u6cd5\uff0c\u79fb\u9664\u63d0\u793a\u4e2d\u7684\u8eab\u4efd\u6807\u8bb0\uff1b3. \u5b9a\u4e49\u8eab\u4efd\u504f\u89c1\u7cfb\u6570\u6765\u8861\u91cf\u667a\u80fd\u4f53\u8ddf\u968f\u540c\u4f34\u4e0e\u81ea\u8eab\u7684\u9891\u7387\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u8eab\u4efd\u504f\u89c1\u666e\u904d\u5b58\u5728\uff0c\u5949\u627f\u6bd4\u81ea\u6211\u504f\u89c1\u66f4\u5e38\u89c1\u3002\u533f\u540d\u5316\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u504f\u89c1\u3002", "conclusion": "\u9700\u8981'\u63a9\u76d6'\u8eab\u4efd\u4ee5\u786e\u4fdd\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u57fa\u4e8e\u5185\u5bb9\u800c\u975e\u6765\u6e90\u8eab\u4efd\u8fdb\u884c\u63a8\u7406\u3002", "topic": "agent analysis"}}
{"id": "2510.07488", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07488", "abs": "https://arxiv.org/abs/2510.07488", "authors": ["Rasika Muralidharan", "Jaewoon Kwak", "Jisun An"], "title": "Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics", "comment": "Under Review at ARR", "summary": "Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are\ngaining attention, yet fewer studies explore their team dynamics. Inspired by\nhuman team science, we propose a multi-agent framework to examine core aspects\nof team science: structure, diversity, and interaction dynamics. We evaluate\nteam performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and\nLatent Implicit Hate, spanning commonsense and social reasoning. Our results\nshow that flat teams tend to perform better than hierarchical ones, while\ndiversity has a nuanced impact. Interviews suggest agents are overconfident\nabout their team performance, yet post-task reflections reveal both\nappreciation for collaboration and challenges in integration, including limited\nconversational coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u7814\u7a76\u56e2\u961f\u79d1\u5b66\u7684\u6838\u5fc3\u65b9\u9762\uff1a\u7ed3\u6784\u3001\u591a\u6837\u6027\u548c\u4e92\u52a8\u52a8\u6001\uff0c\u53d1\u73b0\u5728\u5e38\u8bc6\u548c\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u6241\u5e73\u56e2\u961f\u8868\u73b0\u4f18\u4e8e\u5c42\u7ea7\u56e2\u961f\uff0c\u591a\u6837\u6027\u5f71\u54cd\u590d\u6742\uff0c\u667a\u80fd\u4f53\u5bf9\u56e2\u961f\u8868\u73b0\u8fc7\u5ea6\u81ea\u4fe1\u4f46\u53cd\u601d\u663e\u793a\u534f\u4f5c\u4ef7\u503c\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5bf9\u5176\u56e2\u961f\u52a8\u6001\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u53d7\u4eba\u7c7b\u56e2\u961f\u79d1\u5b66\u542f\u53d1\uff0c\u65e8\u5728\u63a2\u7d22\u667a\u80fd\u4f53\u56e2\u961f\u7684\u6838\u5fc3\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8bc4\u4f30\u56e2\u961f\u5728\u5e38\u8bc6\u95ee\u7b54\u3001\u7b56\u7565\u95ee\u7b54\u3001\u793e\u4ea4\u63a8\u7406\u548c\u6f5c\u5728\u9690\u6027\u4ec7\u6068\u56db\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u7ed3\u6784\u3001\u591a\u6837\u6027\u548c\u4e92\u52a8\u52a8\u6001\u3002", "result": "\u6241\u5e73\u56e2\u961f\u8868\u73b0\u4f18\u4e8e\u5c42\u7ea7\u56e2\u961f\uff0c\u591a\u6837\u6027\u5f71\u54cd\u590d\u6742\uff0c\u667a\u80fd\u4f53\u5bf9\u56e2\u961f\u8868\u73b0\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u53cd\u601d\u663e\u793a\u534f\u4f5c\u4ef7\u503c\u4f46\u5b58\u5728\u6574\u5408\u6311\u6218\u548c\u6709\u9650\u5bf9\u8bdd\u534f\u8c03\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u56e2\u961f\u52a8\u6001\u7814\u7a76\u63ed\u793a\u6241\u5e73\u7ed3\u6784\u4f18\u52bf\uff0c\u591a\u6837\u6027\u9700\u8c28\u614e\u5904\u7406\uff0c\u667a\u80fd\u4f53\u534f\u4f5c\u5b58\u5728\u6f5c\u529b\u4f46\u9700\u6539\u8fdb\u5bf9\u8bdd\u534f\u8c03\u673a\u5236\u3002", "topic": "agent analysis"}}
{"id": "2510.07545", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07545", "abs": "https://arxiv.org/abs/2510.07545", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Mizanur Rahman", "Amran Bhuiyan", "Israt Jahan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices", "comment": "Accepted to the EMNLP 2025 Industry Track", "summary": "Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks. Our code and the data will be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u8005\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff1a\u591a\u6807\u51c6\u63d0\u793a\u548c\u9886\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60\uff0c\u901a\u8fc7\u5fae\u8c032B\u53c2\u6570\u6a21\u578b\u521b\u5efaChartJudge\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u22642B\u53c2\u6570\uff09\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u8005\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6709\u6548\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a(1)\u591a\u6807\u51c6\u63d0\u793a\uff0c\u5c06\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u7ec4\u5408\u5230\u5355\u4e2a\u67e5\u8be2\u4e2d\uff1b(2)\u9886\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u56fe\u8868\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5408\u6210\u5224\u65ad\u5fae\u8c032B\u53c2\u6570LVLM\u521b\u5efaChartJudge\u3002", "result": "\u591a\u6807\u51c6\u63d0\u793a\u66b4\u9732\u4e867B\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5dee\u8ddd\uff0c\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff1bChartJudge\u80fd\u591f\u6709\u6548\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u8fc1\u79fb\u77e5\u8bc6\uff0c\u6210\u4e3a\u66f4\u4e13\u4e1a\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u6a21\u578b\u5927\u5c0f\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u53ef\u8fc1\u79fb\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5206\u6790\uff0c\u4e3a\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.07505", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07505", "abs": "https://arxiv.org/abs/2510.07505", "authors": ["Shen Dong", "Mingxuan Zhang", "Pengfei He", "Li Ma", "Bhavani Thuraisingham", "Hui Liu", "Yue Xing"], "title": "PEAR: Planner-Executor Agent Robustness Benchmark", "comment": null, "summary": "Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a\npowerful paradigm for tackling complex, multi-step tasks across diverse\ndomains. However, despite their impressive capabilities, MAS remain susceptible\nto adversarial manipulation. Existing studies typically examine isolated attack\nsurfaces or specific scenarios, leaving a lack of holistic understanding of MAS\nvulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for\nsystematically evaluating both the utility and vulnerability of\nplanner-executor MAS. While compatible with various MAS architectures, our\nbenchmark focuses on the planner-executor structure, which is a practical and\nwidely adopted design. Through extensive experiments, we find that (1) a weak\nplanner degrades overall clean task performance more severely than a weak\nexecutor; (2) while a memory module is essential for the planner, having a\nmemory module for the executor does not impact the clean task performance; (3)\nthere exists a trade-off between task performance and robustness; and (4)\nattacks targeting the planner are particularly effective at misleading the\nsystem. These findings offer actionable insights for enhancing the robustness\nof MAS and lay the groundwork for principled defenses in multi-agent settings.", "AI": {"tldr": "PEAR\u662f\u4e00\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u89c4\u5212\u5668-\u6267\u884c\u5668\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6548\u7528\u548c\u8106\u5f31\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u89c4\u5212\u5668\u6bd4\u6267\u884c\u5668\u66f4\u5173\u952e\uff0c\u5b58\u5728\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u6743\u8861\uff0c\u9488\u5bf9\u89c4\u5212\u5668\u7684\u653b\u51fb\u7279\u522b\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u53ea\u5173\u6ce8\u5b64\u7acb\u7684\u653b\u51fb\u9762\u6216\u7279\u5b9a\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6f0f\u6d1e\u7684\u6574\u4f53\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165PEAR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u5e7f\u6cdb\u91c7\u7528\u7684\u89c4\u5212\u5668-\u6267\u884c\u5668\u7ed3\u6784\uff0c\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u7cfb\u7edf\u8868\u73b0\u3002", "result": "\u53d1\u73b0\uff1a\u5f31\u89c4\u5212\u5668\u6bd4\u5f31\u6267\u884c\u5668\u66f4\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\uff1b\u89c4\u5212\u5668\u9700\u8981\u8bb0\u5fc6\u6a21\u5757\u800c\u6267\u884c\u5668\u4e0d\u9700\u8981\uff1b\u5b58\u5728\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u6743\u8861\uff1b\u9488\u5bf9\u89c4\u5212\u5668\u7684\u653b\u51fb\u7279\u522b\u6709\u6548\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u539f\u5219\u6027\u9632\u5fa1\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2510.07715", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07715", "abs": "https://arxiv.org/abs/2510.07715", "authors": ["Xiaochen Tang", "Zhenya Zhang", "Miaomiao Zhang", "Jie An"], "title": "Control Synthesis of Cyber-Physical Systems for Real-Time Specifications through Causation-Guided Reinforcement Learning", "comment": "14 pages, 4 figures, 6 tables, accepted by RTSS 2025", "summary": "In real-time and safety-critical cyber-physical systems (CPSs), control\nsynthesis must guarantee that generated policies meet stringent timing and\ncorrectness requirements under uncertain and dynamic conditions. Signal\ntemporal logic (STL) has emerged as a powerful formalism of expressing\nreal-time constraints, with its semantics enabling quantitative assessment of\nsystem behavior. Meanwhile, reinforcement learning (RL) has become an important\nmethod for solving control synthesis problems in unknown environments. Recent\nstudies incorporate STL-based reward functions into RL to automatically\nsynthesize control policies. However, the automatically inferred rewards\nobtained by these methods represent the global assessment of a whole or partial\npath but do not accumulate the rewards of local changes accurately, so the\nsparse global rewards may lead to non-convergence and unstable training\nperformances. In this paper, we propose an online reward generation method\nguided by the online causation monitoring of STL. Our approach continuously\nmonitors system behavior against an STL specification at each control step,\ncomputing the quantitative distance toward satisfaction or violation and\nthereby producing rewards that reflect instantaneous state dynamics.\nAdditionally, we provide a smooth approximation of the causation semantics to\novercome the discontinuity of the causation semantics and make it\ndifferentiable for using deep-RL methods. We have implemented a prototype tool\nand evaluated it in the Gym environment on a variety of continuously controlled\nbenchmarks. Experimental results show that our proposed STL-guided RL method\nwith online causation semantics outperforms existing relevant STL-guided RL\nmethods, providing a more robust and efficient reward generation framework for\ndeep-RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSTL\u5728\u7ebf\u56e0\u679c\u76d1\u63a7\u7684\u5956\u52b1\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u76d1\u63a7\u7cfb\u7edf\u884c\u4e3a\u5e76\u8ba1\u7b97\u6ee1\u8db3\u6216\u8fdd\u53cdSTL\u89c4\u8303\u7684\u91cf\u5316\u8ddd\u79bb\uff0c\u4e3a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u77ac\u65f6\u72b6\u6001\u52a8\u6001\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709STL\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u7a00\u758f\u7684\u5168\u5c40\u5956\u52b1\uff0c\u65e0\u6cd5\u51c6\u786e\u7d2f\u79ef\u5c40\u90e8\u53d8\u5316\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u4e0d\u6536\u655b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u53cd\u6620\u77ac\u65f6\u72b6\u6001\u52a8\u6001\u7684\u5956\u52b1\u751f\u6210\u673a\u5236\u3002", "method": "\u91c7\u7528STL\u5728\u7ebf\u56e0\u679c\u76d1\u63a7\uff0c\u5728\u6bcf\u4e2a\u63a7\u5236\u6b65\u9aa4\u8ba1\u7b97\u7cfb\u7edf\u884c\u4e3a\u4e0eSTL\u89c4\u8303\u7684\u91cf\u5316\u8ddd\u79bb\uff0c\u63d0\u4f9b\u5e73\u6ed1\u7684\u56e0\u679c\u8bed\u4e49\u8fd1\u4f3c\u4ee5\u514b\u670d\u4e0d\u8fde\u7eed\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728Gym\u73af\u5883\u4e2d\u7684\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684STL\u5f15\u5bfcRL\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u5956\u52b1\u751f\u6210\u6846\u67b6\u3002", "conclusion": "\u57fa\u4e8eSTL\u5728\u7ebf\u56e0\u679c\u8bed\u4e49\u7684\u5956\u52b1\u751f\u6210\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07733", "abs": "https://arxiv.org/abs/2510.07733", "authors": ["Minh-Anh Nguye", "Minh-Duc Nguyen", "Nguyen Thi Ha Lan", "Kieu Hai Dang", "Nguyen Tien Dong", "Le Duy Dung"], "title": "SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly adopted for automating survey\npaper generation \\cite{wang2406autosurvey, liang2025surveyx,\nyan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing\napproaches typically extract content from a large collection of related papers\nand prompt LLMs to summarize them directly. However, such methods often\noverlook the structural relationships among papers, resulting in generated\nsurveys that lack a coherent taxonomy and a deeper contextual understanding of\nresearch progress. To address these shortcomings, we propose \\textbf{SurveyG},\nan LLM-based agent framework that integrates \\textit{hierarchical citation\ngraph}, where nodes denote research papers and edges capture both citation\ndependencies and semantic relatedness between their contents, thereby embedding\nstructural and contextual knowledge into the survey generation process. The\ngraph is organized into three layers: \\textbf{Foundation},\n\\textbf{Development}, and \\textbf{Frontier}, to capture the evolution of\nresearch from seminal works to incremental advances and emerging directions. By\ncombining horizontal search within layers and vertical depth traversal across\nlayers, the agent produces multi-level summaries, which are consolidated into a\nstructured survey outline. A multi-agent validation stage then ensures\nconsistency, coverage, and factual accuracy in generating the final survey.\nExperiments, including evaluations by human experts and LLM-as-a-judge,\ndemonstrate that SurveyG outperforms state-of-the-art frameworks, producing\nsurveys that are more comprehensive and better structured to the underlying\nknowledge taxonomy of a field.", "AI": {"tldr": "SurveyG\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5c42\u6b21\u5316\u5f15\u7528\u56fe\u6765\u751f\u6210\u66f4\u7ed3\u6784\u5316\u548c\u5168\u9762\u7684\u7efc\u8ff0\u8bba\u6587\u3002\u8be5\u6846\u67b6\u5c06\u8bba\u6587\u7ec4\u7ec7\u4e3a\u57fa\u7840\u5c42\u3001\u53d1\u5c55\u5c42\u548c\u524d\u6cbf\u5c42\uff0c\u7ed3\u5408\u6a2a\u5411\u641c\u7d22\u548c\u7eb5\u5411\u6df1\u5ea6\u904d\u5386\uff0c\u6700\u7ec8\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u786e\u4fdd\u4e00\u81f4\u6027\u3001\u8986\u76d6\u9762\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u4ece\u5927\u91cf\u76f8\u5173\u8bba\u6587\u4e2d\u63d0\u53d6\u5185\u5bb9\u5e76\u7528LLM\u603b\u7ed3\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bba\u6587\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u7efc\u8ff0\u7f3a\u4e4f\u8fde\u8d2f\u7684\u5206\u7c7b\u6cd5\u548c\u5bf9\u7814\u7a76\u8fdb\u5c55\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u6784\u5efa\u5c42\u6b21\u5316\u5f15\u7528\u56fe\uff08\u57fa\u7840\u5c42\u3001\u53d1\u5c55\u5c42\u3001\u524d\u6cbf\u5c42\uff09\uff0c\u7ed3\u5408\u6a2a\u5411\u5c42\u5185\u641c\u7d22\u548c\u7eb5\u5411\u8de8\u5c42\u6df1\u5ea6\u904d\u5386\u751f\u6210\u591a\u7ea7\u6458\u8981\uff0c\u6700\u540e\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSurveyG\u5728\u4eba\u7c7b\u4e13\u5bb6\u548cLLM\u8bc4\u5224\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6846\u67b6\uff0c\u751f\u6210\u7684\u7efc\u8ff0\u66f4\u5168\u9762\u4e14\u66f4\u597d\u5730\u7b26\u5408\u9886\u57df\u77e5\u8bc6\u5206\u7c7b\u7ed3\u6784\u3002", "conclusion": "SurveyG\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u5316\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u751f\u6210\u7efc\u8ff0\u8bba\u6587\u7684\u8d28\u91cf\u548c\u8fde\u8d2f\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.07748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07748", "abs": "https://arxiv.org/abs/2510.07748", "authors": ["Yilun Zhang", "Dexing Kong"], "title": "Haibu Mathematical-Medical Intelligent Agent:Enhancing Large Language Model Reliability in Medical Tasks via Verifiable Reasoning Chains", "comment": null, "summary": "Large Language Models (LLMs) show promise in medicine but are prone to\nfactual and logical errors, which is unacceptable in this high-stakes field. To\naddress this, we introduce the \"Haibu Mathematical-Medical Intelligent Agent\"\n(MMIA), an LLM-driven architecture that ensures reliability through a formally\nverifiable reasoning process. MMIA recursively breaks down complex medical\ntasks into atomic, evidence-based steps. This entire reasoning chain is then\nautomatically audited for logical coherence and evidence traceability, similar\nto theorem proving. A key innovation is MMIA's \"bootstrapping\" mode, which\nstores validated reasoning chains as \"theorems.\" Subsequent tasks can then be\nefficiently solved using Retrieval-Augmented Generation (RAG), shifting from\ncostly first-principles reasoning to a low-cost verification model. We\nvalidated MMIA across four healthcare administration domains, including DRG/DIP\naudits and medical insurance adjudication, using expert-validated benchmarks.\nResults showed MMIA achieved an error detection rate exceeding 98% with a false\npositive rate below 1%, significantly outperforming baseline LLMs. Furthermore,\nthe RAG matching mode is projected to reduce average processing costs by\napproximately 85% as the knowledge base matures. In conclusion, MMIA's\nverifiable reasoning framework is a significant step toward creating\ntrustworthy, transparent, and cost-effective AI systems, making LLM technology\nviable for critical applications in medicine.", "AI": {"tldr": "MMIA\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u533b\u5b66\u667a\u80fd\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8fc7\u7a0b\u786e\u4fdd\u53ef\u9760\u6027\uff0c\u5728\u533b\u7597\u7ba1\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u8d85\u8fc798%\u7684\u9519\u8bef\u68c0\u6d4b\u7387\u548c\u4f4e\u4e8e1%\u7684\u8bef\u62a5\u7387\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u533b\u5b66\u9886\u57df\u5bb9\u6613\u4ea7\u751f\u4e8b\u5b9e\u548c\u903b\u8f91\u9519\u8bef\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u533b\u5b66\u662f\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9519\u8bef\u4e0d\u53ef\u63a5\u53d7\u3002", "method": "MMIA\u5c06\u590d\u6742\u533b\u7597\u4efb\u52a1\u9012\u5f52\u5206\u89e3\u4e3a\u57fa\u4e8e\u8bc1\u636e\u7684\u539f\u5b50\u6b65\u9aa4\uff0c\u81ea\u52a8\u5ba1\u6838\u63a8\u7406\u94fe\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u8bc1\u636e\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u91c7\u7528'\u5f15\u5bfc'\u6a21\u5f0f\u5c06\u5df2\u9a8c\u8bc1\u63a8\u7406\u94fe\u5b58\u50a8\u4e3a'\u5b9a\u7406'\uff0c\u540e\u7eed\u4efb\u52a1\u4f7f\u7528RAG\u8fdb\u884c\u4f4e\u6210\u672c\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u533b\u7597\u7ba1\u7406\u9886\u57df\u9a8c\u8bc1\uff0c\u9519\u8bef\u68c0\u6d4b\u7387\u8d85\u8fc798%\uff0c\u8bef\u62a5\u7387\u4f4e\u4e8e1%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfLLMs\u3002RAG\u5339\u914d\u6a21\u5f0f\u9884\u8ba1\u53ef\u5c06\u5e73\u5747\u5904\u7406\u6210\u672c\u964d\u4f4e\u7ea685%\u3002", "conclusion": "MMIA\u7684\u53ef\u9a8c\u8bc1\u63a8\u7406\u6846\u67b6\u662f\u521b\u5efa\u53ef\u4fe1\u3001\u900f\u660e\u548c\u6210\u672c\u6548\u76caAI\u7cfb\u7edf\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4f7fLLM\u6280\u672f\u5728\u533b\u5b66\u5173\u952e\u5e94\u7528\u4e2d\u53ef\u884c\u3002", "topic": "agent analysis"}}
{"id": "2510.07645", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07645", "abs": "https://arxiv.org/abs/2510.07645", "authors": ["Xin Jie Chua", "Jeraelyn Ming Li Tan", "Jia Xuan Tan", "Soon Chang Poh", "Yi Xian Goh", "Debbie Hui Tian Choong", "Chee Mun Foong", "Sze Jue Yang", "Chee Seng Chan"], "title": "Banking Done Right: Redefining Retail Banking with Language-Centric AI", "comment": "Accepted at EMNLP2025 Industry Track", "summary": "This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt\nBank to enable customers to execute core financial transactions through natural\nlanguage conversation. This represents the first global regulator-approved\ndeployment worldwide where conversational AI functions as the primary banking\ninterface, in contrast to prior assistants that have been limited to advisory\nor support roles. Built entirely in-house, Ryt AI is powered by ILMU, a\nclosed-source LLM developed internally, and replaces rigid multi-screen\nworkflows with a single dialogue orchestrated by four LLM-powered agents\n(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific\nLoRA adapter to ILMU, which is hosted within the bank's infrastructure to\nensure consistent behavior with minimal overhead. Deterministic guardrails,\nhuman-in-the-loop confirmation, and a stateless audit architecture provide\ndefense-in-depth for security and compliance. The result is Banking Done Right:\ndemonstrating that regulator-approved natural-language interfaces can reliably\nsupport core financial operations under strict governance.", "AI": {"tldr": "Ryt AI\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u5f0f\u94f6\u884c\u4ee3\u7406\u6846\u67b6\uff0c\u9996\u6b21\u5728\u5168\u7403\u8303\u56f4\u5185\u83b7\u5f97\u76d1\u7ba1\u6279\u51c6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u4f5c\u4e3a\u4e3b\u8981\u94f6\u884c\u754c\u9762\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u591a\u5c4f\u5e55\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u5f00\u53d1\u9996\u4e2a\u83b7\u5f97\u5168\u7403\u76d1\u7ba1\u6279\u51c6\u7684\u5bf9\u8bdd\u5f0fAI\u94f6\u884c\u754c\u9762\uff0c\u8ba9\u5ba2\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u6267\u884c\u6838\u5fc3\u91d1\u878d\u4ea4\u6613\uff0c\u7a81\u7834\u4ee5\u5f80AI\u52a9\u624b\u4ec5\u9650\u4e8e\u54a8\u8be2\u6216\u652f\u6301\u89d2\u8272\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u5185\u90e8\u5f00\u53d1\u7684\u95ed\u6e90LLM ILMU\uff0c\u901a\u8fc7\u56db\u4e2aLLM\u9a71\u52a8\u7684\u4ee3\u7406\uff08\u62a4\u680f\u3001\u610f\u56fe\u3001\u652f\u4ed8\u3001FAQ\uff09\u534f\u8c03\u5355\u4e00\u5bf9\u8bdd\uff0c\u6bcf\u4e2a\u4ee3\u7406\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u7684LoRA\u9002\u914d\u5668\uff0c\u5728\u94f6\u884c\u57fa\u7840\u8bbe\u65bd\u5185\u6258\u7ba1\u4ee5\u786e\u4fdd\u4e00\u81f4\u884c\u4e3a\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u76d1\u7ba1\u6279\u51c6\u7684\u81ea\u7136\u8bed\u8a00\u754c\u9762\uff0c\u5728\u4e25\u683c\u6cbb\u7406\u4e0b\u53ef\u9760\u652f\u6301\u6838\u5fc3\u91d1\u878d\u64cd\u4f5c\uff0c\u5c55\u793a\u4e86\u5bf9\u8bdd\u5f0fAI\u4f5c\u4e3a\u4e3b\u8981\u94f6\u884c\u754c\u9762\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Ryt AI\u8bc1\u660e\u4e86\u5728\u4e25\u683c\u5b89\u5168\u5408\u89c4\u6846\u67b6\u4e0b\uff0c\u81ea\u7136\u8bed\u8a00\u754c\u9762\u53ef\u4ee5\u53ef\u9760\u5730\u652f\u6301\u6838\u5fc3\u91d1\u878d\u4ea4\u6613\uff0c\u4ee3\u8868\u4e86\u94f6\u884c\u670d\u52a1\u4ea4\u4ed8\u65b9\u5f0f\u7684\u91cd\u5927\u8fdb\u6b65\u3002", "topic": "swe application"}}
{"id": "2510.07772", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07772", "abs": "https://arxiv.org/abs/2510.07772", "authors": ["Tianle Zhou", "Jiakai Xu", "Guanhong Liu", "Jiaxiang Liu", "Haonan Wang", "Eugene Wu"], "title": "An approach for systematic decomposition of complex llm tasks", "comment": null, "summary": "Large Language Models (LLMs) suffer from reliability issues on complex tasks,\nas existing decomposition methods are heuristic and rely on agent or manual\ndecomposition. This work introduces a novel, systematic decomposition framework\nthat we call Analysis of CONstraint-Induced Complexity (ACONIC), which models\nthe task as a constraint problem and leveraging formal complexity measures to\nguide decomposition. On combinatorial (SATBench) and LLM database querying\ntasks (Spider), we find that by decomposing the tasks following the measure of\ncomplexity, agent can perform considerably better (10-40 percentage point).", "AI": {"tldr": "\u63d0\u51faACONIC\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u7ea6\u675f\u95ee\u9898\u5e76\u5229\u7528\u5f62\u5f0f\u590d\u6742\u5ea6\u5ea6\u91cf\u6765\u6307\u5bfc\u4efb\u52a1\u5206\u89e3\uff0c\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u5206\u89e3\u65b9\u6cd5\u57fa\u4e8e\u542f\u53d1\u5f0f\u6216\u4eba\u5de5\u5206\u89e3\uff0cLLM\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u7ea6\u675f\u95ee\u9898\uff0c\u4f7f\u7528\u5f62\u5f0f\u590d\u6742\u5ea6\u5ea6\u91cf\u6307\u5bfc\u5206\u89e3", "result": "\u5728\u7ec4\u5408\u4efb\u52a1(SATBench)\u548cLLM\u6570\u636e\u5e93\u67e5\u8be2\u4efb\u52a1(Spider)\u4e0a\uff0c\u6027\u80fd\u63d0\u534710-40\u4e2a\u767e\u5206\u70b9", "conclusion": "\u57fa\u4e8e\u590d\u6742\u5ea6\u5ea6\u91cf\u7684\u7cfb\u7edf\u5206\u89e3\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u8868\u73b0", "topic": "agent analysis"}}
{"id": "2510.07557", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07557", "abs": "https://arxiv.org/abs/2510.07557", "authors": ["Abhay Bhandarkar", "Gaurav Mishra", "Khushi Juchani", "Harsh Singhal"], "title": "Investigating Thematic Patterns and User Preferences in LLM Interactions using BERTopic", "comment": null, "summary": "This study applies BERTopic, a transformer-based topic modeling technique, to\nthe lmsys-chat-1m dataset, a multilingual conversational corpus built from\nhead-to-head evaluations of large language models (LLMs). Each user prompt is\npaired with two anonymized LLM responses and a human preference label, used to\nassess user evaluation of competing model outputs. The main objective is\nuncovering thematic patterns in these conversations and examining their\nrelation to user preferences, particularly if certain LLMs are consistently\npreferred within specific topics. A robust preprocessing pipeline was designed\nfor multilingual variation, balancing dialogue turns, and cleaning noisy or\nredacted data. BERTopic extracted over 29 coherent topics including artificial\nintelligence, programming, ethics, and cloud infrastructure. We analysed\nrelationships between topics and model preferences to identify trends in\nmodel-topic alignment. Visualization techniques included inter-topic distance\nmaps, topic probability distributions, and model-versus-topic matrices. Our\nfindings inform domain-specific fine-tuning and optimization strategies for\nimproving real-world LLM performance and user satisfaction.", "AI": {"tldr": "\u4f7f\u7528BERTopic\u5bf9lmsys-chat-1m\u6570\u636e\u96c6\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u5206\u6790\u591a\u8bed\u8a00\u5bf9\u8bdd\u4e2d\u7684\u4e3b\u9898\u6a21\u5f0f\u53ca\u5176\u4e0e\u7528\u6237\u504f\u597d\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u7279\u5b9aLLM\u5728\u4e0d\u540c\u4e3b\u9898\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u65e8\u5728\u63ed\u793a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u4e2d\u7684\u4e3b\u9898\u6a21\u5f0f\uff0c\u5e76\u63a2\u7a76\u8fd9\u4e9b\u4e3b\u9898\u4e0e\u7528\u6237\u504f\u597d\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u67d0\u4e9bLLM\u662f\u5426\u5728\u7279\u5b9a\u4e3b\u9898\u4e2d\u66f4\u53d7\u9752\u7750\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u8bed\u8a00\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u4f7f\u7528BERTopic\u63d0\u53d6\u4e8629\u4e2a\u8fde\u8d2f\u4e3b\u9898\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u6280\u672f\uff08\u4e3b\u9898\u8ddd\u79bb\u56fe\u3001\u6982\u7387\u5206\u5e03\u3001\u6a21\u578b-\u4e3b\u9898\u77e9\u9635\uff09\u5206\u6790\u4e3b\u9898\u4e0e\u6a21\u578b\u504f\u597d\u7684\u5173\u7cfb\u3002", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u5305\u62ec\u4eba\u5de5\u667a\u80fd\u3001\u7f16\u7a0b\u3001\u4f26\u7406\u548c\u4e91\u57fa\u7840\u8bbe\u65bd\u5728\u5185\u768429\u4e2a\u8fde\u8d2f\u4e3b\u9898\uff0c\u5e76\u53d1\u73b0\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4e3b\u9898\u4e2d\u7684\u504f\u597d\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u548c\u4f18\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u4f9d\u636e\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4e2dLLM\u7684\u6027\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "topic": "agent analysis"}}
{"id": "2510.07790", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07790", "abs": "https://arxiv.org/abs/2510.07790", "authors": ["Hao Wu", "Wei Liu"], "title": "GCPO: When Contrast Fails, Go Gold", "comment": null, "summary": "Reinforcement learning has been widely applied to enhance the reasoning\ncapabilities of large language models. Extending the inference limits of\nsmaller models has become a prominent research focus. However, algorithms such\nas Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the\nupper bound of a model's rollout responses is entirely determined by the model\nitself, preventing the acquisition of knowledge from samples that are either\nall incorrect or all correct. In this paper, we introduce Group Contrastive\nPolicy Optimization (GCPO), a method that incorporates external standard\nreference answers. When the model cannot solve a problem, the reference answer\nsupplies the correct response, steering the model toward an unequivocally\naccurate update direction. This approach offers two main advantages: (1) it\nimproves training efficiency by fully utilizing every sample; (2) it enables\nthe model to emulate the problem solving strategy of the reference answer\nduring training, thereby enhancing generalization in reasoning. GCPO achieves\noutstanding results across multiple benchmark datasets, yielding substantial\nimprovements over the baseline model. Our code is available at:\nhttps://github.com/AchoWu/GCPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86Group Contrastive Policy Optimization (GCPO)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5916\u90e8\u6807\u51c6\u53c2\u8003\u7b54\u6848\u6765\u89e3\u51b3GRPO\u7b97\u6cd5\u5728\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898\u65f6\u65e0\u6cd5\u83b7\u5f97\u6b63\u786e\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5982GRPO\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff1a\u6a21\u578b\u65e0\u6cd5\u4ece\u5168\u9519\u6216\u5168\u5bf9\u7684\u6837\u672c\u4e2d\u83b7\u53d6\u77e5\u8bc6\uff0c\u56e0\u4e3a\u6a21\u578b\u81ea\u8eab\u51b3\u5b9a\u4e86\u54cd\u5e94\u4e0a\u9650\u3002", "method": "GCPO\u65b9\u6cd5\u5f15\u5165\u5916\u90e8\u6807\u51c6\u53c2\u8003\u7b54\u6848\uff0c\u5f53\u6a21\u578b\u65e0\u6cd5\u89e3\u51b3\u95ee\u9898\u65f6\uff0c\u53c2\u8003\u7b54\u6848\u63d0\u4f9b\u6b63\u786e\u54cd\u5e94\uff0c\u5f15\u5bfc\u6a21\u578b\u5411\u660e\u786e\u6b63\u786e\u7684\u66f4\u65b0\u65b9\u5411\u5b66\u4e60\u3002", "result": "GCPO\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "GCPO\u65b9\u6cd5\u80fd\u591f\u5145\u5206\u5229\u7528\u6bcf\u4e2a\u6837\u672c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u8ba9\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u6a21\u4eff\u53c2\u8003\u7b54\u6848\u7684\u89e3\u9898\u7b56\u7565\uff0c\u4ece\u800c\u589e\u5f3a\u63a8\u7406\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07581", "abs": "https://arxiv.org/abs/2510.07581", "authors": ["Zhongqi Yue", "Weishi Wang", "Yundaichuan Zhan", "Juncheng Li", "Daniel Dahlmeier", "Fredrik D. Johansson"], "title": "Expanding the Action Space of LLMs to Reason Beyond Language", "comment": null, "summary": "Large Language Models (LLMs) are powerful reasoners in natural language, but\ntheir actions are typically confined to outputting vocabulary tokens. As a\nresult, interactions with external environments -- such as symbolic operators\nor simulators -- must be expressed through text in predefined formats, parsed,\nand routed to external interfaces. This overloads the model's language with\nboth reasoning and control duties, and requires a hand-crafted parser, external\nto the LLM. To address this, we decouple environment interactions from language\nby internalizing them in an Expanded Action space (ExpA), beyond the\nvocabulary. The model starts reasoning in the default language environment, but\nmay trigger routing actions and switch to an external environment at any time.\nFrom there, the model can only invoke environment-specific actions, receive\nfeedback from the environment, and potentially route back to language as a\nresult. To promote effective exploration of the expanded action space and new\nenvironments, we introduce ExpA Reinforcement Learning (EARL) with\ncounterfactual policy optimization. On tasks requiring multi-turn interactions\nand contingent planning, EARL outperforms strong baselines with\nvocabulary-constrained actions. It performs robustly across calculator-based\nmulti-task learning and, in the partially observed sorting problem, achieves\nperfect Sort-4 accuracy while self-discovering an efficient algorithm\ncompetitive with classical designs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faExpA\uff08\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\uff09\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u4ea4\u4e92\u4ece\u8bed\u8a00\u4e2d\u89e3\u8026\uff0c\u8ba9LLM\u80fd\u591f\u5728\u8bed\u8a00\u73af\u5883\u548c\u5916\u90e8\u73af\u5883\u4e4b\u95f4\u5207\u6362\uff0c\u5e76\u5f15\u5165EARL\uff08ExpA\u5f3a\u5316\u5b66\u4e60\uff09\u6765\u4f18\u5316\u5728\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u63a2\u7d22\u3002", "motivation": "\u4f20\u7edfLLM\u7684\u73af\u5883\u4ea4\u4e92\u9700\u8981\u901a\u8fc7\u6587\u672c\u8868\u8fbe\u548c\u89e3\u6790\uff0c\u8fd9\u7ed9\u6a21\u578b\u8bed\u8a00\u5e26\u6765\u4e86\u63a8\u7406\u548c\u63a7\u5236\u7684\u53cc\u91cd\u8d1f\u62c5\uff0c\u4e14\u9700\u8981\u624b\u5de5\u8bbe\u8ba1\u7684\u89e3\u6790\u5668\u3002", "method": "\u901a\u8fc7\u5185\u90e8\u5316\u73af\u5883\u4ea4\u4e92\u5230\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\uff08ExpA\uff09\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u5728\u8bed\u8a00\u73af\u5883\u548c\u5916\u90e8\u73af\u5883\u4e4b\u95f4\u8def\u7531\u5207\u6362\uff0c\u5e76\u5f15\u5165EARL\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u9700\u8981\u591a\u8f6e\u4ea4\u4e92\u548c\u6761\u4ef6\u89c4\u5212\u7684\u4efb\u52a1\u4e0a\uff0cEARL\u4f18\u4e8e\u8bcd\u6c47\u7ea6\u675f\u52a8\u4f5c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u5668\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5728\u90e8\u5206\u89c2\u5bdf\u6392\u5e8f\u95ee\u9898\u4e2d\u8fbe\u5230100%\u7684Sort-4\u51c6\u786e\u7387\u3002", "conclusion": "ExpA\u65b9\u6cd5\u6709\u6548\u89e3\u8026\u4e86\u73af\u5883\u4ea4\u4e92\u4e0e\u8bed\u8a00\u63a8\u7406\uff0cEARL\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u4fc3\u8fdb\u5728\u6269\u5c55\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u6709\u6548\u63a2\u7d22\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07861", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07861", "abs": "https://arxiv.org/abs/2510.07861", "authors": ["Tianyu Fan", "Xinyao Niu", "Yuxiang Zheng", "Fengji Zhang", "Chengen Huang", "Bei Chen", "Junyang Lin", "Chao Huang"], "title": "Understanding DeepResearch via Reports", "comment": "22 pages, 4 figures", "summary": "DeepResearch agents represent a transformative AI paradigm, conducting\nexpert-level research through sophisticated reasoning and multi-tool\nintegration. However, evaluating these systems remains critically challenging\ndue to open-ended research scenarios and existing benchmarks that focus on\nisolated capabilities rather than holistic performance. Unlike traditional LLM\ntasks, DeepResearch systems must synthesize diverse sources, generate insights,\nand present coherent findings, which are capabilities that resist simple\nverification. To address this gap, we introduce DeepResearch-ReportEval, a\ncomprehensive framework designed to assess DeepResearch systems through their\nmost representative outputs: research reports. Our approach systematically\nmeasures three dimensions: quality, redundancy, and factuality, using an\ninnovative LLM-as-a-Judge methodology achieving strong expert concordance. We\ncontribute a standardized benchmark of 100 curated queries spanning 12\nreal-world categories, enabling systematic capability comparison. Our\nevaluation of four leading commercial systems reveals distinct design\nphilosophies and performance trade-offs, establishing foundational insights as\nDeepResearch evolves from information assistants toward intelligent research\npartners. Source code and data are available at:\nhttps://github.com/HKUDS/DeepResearch-Eval.", "AI": {"tldr": "\u63d0\u51fa\u4e86DeepResearch-ReportEval\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5728\u7814\u7a76\u62a5\u544a\u751f\u6210\u65b9\u9762\u7684\u7efc\u5408\u80fd\u529b\uff0c\u5305\u62ec\u8d28\u91cf\u3001\u5197\u4f59\u6027\u548c\u4e8b\u5b9e\u6027\u4e09\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u9700\u8981\u7efc\u5408\u591a\u79cd\u6765\u6e90\u3001\u751f\u6210\u89c1\u89e3\u5e76\u5448\u73b0\u8fde\u8d2f\u53d1\u73b0\uff0c\u8fd9\u4e9b\u80fd\u529b\u96be\u4ee5\u901a\u8fc7\u73b0\u6709\u57fa\u51c6\u8fdb\u884c\u7b80\u5355\u9a8c\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4f7f\u7528LLM-as-a-Judge\u65b9\u6cd5\uff0c\u6784\u5efa\u5305\u542b100\u4e2a\u67e5\u8be2\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u6db5\u76d612\u4e2a\u73b0\u5b9e\u4e16\u754c\u7c7b\u522b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7814\u7a76\u62a5\u544a\u7684\u8d28\u91cf\u3001\u5197\u4f59\u6027\u548c\u4e8b\u5b9e\u6027\u3002", "result": "\u5bf9\u56db\u4e2a\u9886\u5148\u5546\u4e1a\u7cfb\u7edf\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u8bbe\u8ba1\u7406\u5ff5\u548c\u6027\u80fd\u6743\u8861\uff0c\u4e3a\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4ece\u4fe1\u606f\u52a9\u624b\u5411\u667a\u80fd\u7814\u7a76\u4f19\u4f34\u7684\u6f14\u8fdb\u63d0\u4f9b\u4e86\u57fa\u7840\u89c1\u89e3\u3002", "conclusion": "DeepResearch-ReportEval\u6846\u67b6\u4e3a\u8bc4\u4f30\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u4ece\u4fe1\u606f\u52a9\u624b\u5411\u667a\u80fd\u7814\u7a76\u4f19\u4f34\u7684\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.07920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07920", "abs": "https://arxiv.org/abs/2510.07920", "authors": ["Xiangyu Li", "Yawen Zeng", "Xiaofen Xing", "Jin Xu", "Xiangmin Xu"], "title": "Profit Mirage: Revisiting Information Leakage in LLM-based Financial Agents", "comment": null, "summary": "LLM-based financial agents have attracted widespread excitement for their\nability to trade like human experts. However, most systems exhibit a \"profit\nmirage\": dazzling back-tested returns evaporate once the model's knowledge\nwindow ends, because of the inherent information leakage in LLMs. In this\npaper, we systematically quantify this leakage issue across four dimensions and\nrelease FinLake-Bench, a leakage-robust evaluation benchmark. Furthermore, to\nmitigate this issue, we introduce FactFin, a framework that applies\ncounterfactual perturbations to compel LLM-based agents to learn causal drivers\ninstead of memorized outcomes. FactFin integrates four core components:\nStrategy Code Generator, Retrieval-Augmented Generation, Monte Carlo Tree\nSearch, and Counterfactual Simulator. Extensive experiments show that our\nmethod surpasses all baselines in out-of-sample generalization, delivering\nsuperior risk-adjusted performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFactFin\u6846\u67b6\u6765\u89e3\u51b3LLM\u91d1\u878d\u4ee3\u7406\u4e2d\u7684\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u8feb\u4f7f\u4ee3\u7406\u5b66\u4e60\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\u800c\u975e\u8bb0\u5fc6\u7ed3\u679c\uff0c\u5e76\u53d1\u5e03\u4e86FinLake-Bench\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709LLM\u91d1\u878d\u4ee3\u7406\u5b58\u5728'\u5229\u6da6\u5e7b\u8c61'\u95ee\u9898\uff0c\u56de\u6d4b\u6536\u76ca\u5728\u6a21\u578b\u77e5\u8bc6\u7a97\u53e3\u7ed3\u675f\u540e\u6d88\u5931\uff0c\u4e3b\u8981\u539f\u56e0\u662fLLM\u4e2d\u7684\u56fa\u6709\u4fe1\u606f\u6cc4\u6f0f\u3002", "method": "FactFin\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7b56\u7565\u4ee3\u7801\u751f\u6210\u5668\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u53cd\u4e8b\u5b9e\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6270\u52a8\u5f3a\u5236\u4ee3\u7406\u5b66\u4e60\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u57fa\u7ebf\u4e0a\u90fd\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6837\u672c\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u4f9b\u4f18\u8d8a\u7684\u98ce\u9669\u8c03\u6574\u540e\u6027\u80fd\u3002", "conclusion": "FactFin\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u91d1\u878d\u4ee3\u7406\u7684\u4fe1\u606f\u6cc4\u6f0f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u4ea4\u6613\u8868\u73b0\u3002", "topic": "agent analysis"}}
{"id": "2510.07743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07743", "abs": "https://arxiv.org/abs/2510.07743", "authors": ["Tianci Liu", "Ran Xu", "Tony Yu", "Ilgee Hong", "Carl Yang", "Tuo Zhao", "Haoyu Wang"], "title": "OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment", "comment": "The first two authors contributed equally", "summary": "Reward modeling lies at the core of reinforcement learning from human\nfeedback (RLHF), yet most existing reward models rely on scalar or pairwise\njudgments that fail to capture the multifaceted nature of human preferences.\nRecent studies have explored rubrics-as-rewards (RaR) that uses structured\nnatural language criteria that capture multiple dimensions of response quality.\nHowever, producing rubrics that are both reliable and scalable remains a key\nchallenge. In this work, we introduce OpenRubrics, a diverse, large-scale\ncollection of (prompt, rubric) pairs for training rubric-generation and\nrubric-based reward models. To elicit discriminative and comprehensive\nevaluation signals, we introduce Contrastive Rubric Generation (CRG), which\nderives both hard rules (explicit constraints) and principles (implicit\nqualities) by contrasting preferred and rejected responses. We further improve\nreliability by enforcing preference-label consistency via rejection sampling to\nremove noisy rubrics. Across multiple reward-modeling benchmarks, our\nrubric-based reward model, Rubric-RM, surpasses strong size-matched baselines\nby 6.8%. These gains transfer to policy models on instruction-following and\nbiomedical benchmarks. Our results show that rubrics provide scalable alignment\nsignals that narrow the gap between costly human evaluation and automated\nreward modeling, enabling a new principle-driven paradigm for LLM alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86OpenRubrics\u6570\u636e\u96c6\u548c\u5bf9\u6bd4\u6027\u89c4\u5219\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u6807\u51c6\u6539\u8fdb\u5956\u52b1\u5efa\u6a21\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf6.8%", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u4f9d\u8d56\u6807\u91cf\u6216\u6210\u5bf9\u5224\u65ad\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u504f\u597d\u7684\u591a\u7ef4\u5ea6\u7279\u6027\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u89c4\u5219\u751f\u6210\u65b9\u6cd5", "method": "\u5f15\u5165OpenRubrics\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u5bf9\u6bd4\u6027\u89c4\u5219\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u504f\u597d\u548c\u62d2\u7edd\u54cd\u5e94\u6765\u63a8\u5bfc\u786c\u89c4\u5219\u548c\u539f\u5219\uff0c\u5e76\u4f7f\u7528\u62d2\u7edd\u91c7\u6837\u63d0\u9ad8\u53ef\u9760\u6027", "result": "\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u6a21\u578bRubric-RM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf6.8%\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u548c\u751f\u7269\u533b\u5b66\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u6027", "conclusion": "\u89c4\u5219\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5bf9\u9f50\u4fe1\u53f7\uff0c\u7f29\u5c0f\u4e86\u6602\u8d35\u7684\u4eba\u5de5\u8bc4\u4f30\u4e0e\u81ea\u52a8\u5956\u52b1\u5efa\u6a21\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3aLLM\u5bf9\u9f50\u5f00\u542f\u4e86\u65b0\u7684\u539f\u5219\u9a71\u52a8\u8303\u5f0f", "topic": "agentic reinforcement learning"}}
{"id": "2510.07925", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.07925", "abs": "https://arxiv.org/abs/2510.07925", "authors": ["Rebecca Westh\u00e4u\u00dfer", "Wolfgang Minker", "Sebatian Zepf"], "title": "Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles", "comment": "8 pages, 1 figure, 1 table", "summary": "Large language models (LLMs) increasingly serve as the central control unit\nof AI agents, yet current approaches remain limited in their ability to deliver\npersonalized interactions. While Retrieval Augmented Generation enhances LLM\ncapabilities by improving context-awareness, it lacks mechanisms to combine\ncontextual information with user-specific data. Although personalization has\nbeen studied in fields such as human-computer interaction or cognitive science,\nexisting perspectives largely remain conceptual, with limited focus on\ntechnical implementation. To address these gaps, we build on a unified\ndefinition of personalization as a conceptual foundation to derive technical\nrequirements for adaptive, user-centered LLM-based agents. Combined with\nestablished agentic AI patterns such as multi-agent collaboration or\nmulti-source retrieval, we present a framework that integrates persistent\nmemory, dynamic coordination, self-validation, and evolving user profiles to\nenable personalized long-term interactions. We evaluate our approach on three\npublic datasets using metrics such as retrieval accuracy, response correctness,\nor BertScore. We complement these results with a five-day pilot user study\nproviding initial insights into user feedback on perceived personalization. The\nstudy provides early indications that guide future work and highlights the\npotential of integrating persistent memory and user profiles to improve the\nadaptivity and perceived personalization of LLM-based agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u6301\u4e45\u8bb0\u5fc6\u3001\u52a8\u6001\u534f\u8c03\u3001\u81ea\u6211\u9a8c\u8bc1\u548c\u6f14\u8fdb\u7528\u6237\u914d\u7f6e\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u7684\u4e2a\u6027\u5316\u957f\u671f\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684AI\u4ee3\u7406\u7f3a\u4e4f\u4e2a\u6027\u5316\u4ea4\u4e92\u80fd\u529b\uff0cRAG\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u4f46\u65e0\u6cd5\u7ed3\u5408\u7528\u6237\u7279\u5b9a\u6570\u636e\u3002\u73b0\u6709\u4e2a\u6027\u5316\u7814\u7a76\u591a\u4e3a\u6982\u5ff5\u6027\uff0c\u7f3a\u4e4f\u6280\u672f\u5b9e\u73b0\u3002", "method": "\u57fa\u4e8e\u7edf\u4e00\u4e2a\u6027\u5316\u5b9a\u4e49\u6784\u5efa\u6280\u672f\u9700\u6c42\uff0c\u7ed3\u5408\u591a\u4ee3\u7406\u534f\u4f5c\u548c\u591a\u6e90\u68c0\u7d22\u7b49\u6a21\u5f0f\uff0c\u5f00\u53d1\u96c6\u6210\u6301\u4e45\u8bb0\u5fc6\u3001\u52a8\u6001\u534f\u8c03\u3001\u81ea\u6211\u9a8c\u8bc1\u548c\u6f14\u8fdb\u7528\u6237\u914d\u7f6e\u7684\u6846\u67b6\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u3001\u54cd\u5e94\u6b63\u786e\u6027\u548cBertScore\u7b49\u6307\u6807\uff0c\u5e76\u901a\u8fc75\u5929\u8bd5\u70b9\u7528\u6237\u7814\u7a76\u83b7\u5f97\u4e86\u5173\u4e8e\u611f\u77e5\u4e2a\u6027\u5316\u7684\u521d\u6b65\u7528\u6237\u53cd\u9988\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65e9\u671f\u6307\u5bfc\uff0c\u7a81\u663e\u4e86\u96c6\u6210\u6301\u4e45\u8bb0\u5fc6\u548c\u7528\u6237\u914d\u7f6e\u5728\u63d0\u5347\u57fa\u4e8eLLM\u4ee3\u7406\u9002\u5e94\u6027\u548c\u611f\u77e5\u4e2a\u6027\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.07745", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07745", "abs": "https://arxiv.org/abs/2510.07745", "authors": ["Runyang You", "Yongqi Li", "Meng Liu", "Wenjie Wang", "Liqiang Nie", "Wenjie Li"], "title": "Parallel Test-Time Scaling for Latent Reasoning Models", "comment": null, "summary": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large\nlanguage models (LLMs), typically by sampling multiple token-based\nchains-of-thought in parallel and aggregating outcomes through voting or\nsearch. Recent advances in latent reasoning, where intermediate reasoning\nunfolds in continuous vector spaces, offer a more efficient alternative to\nexplicit Chain-of-Thought, yet whether such latent models can similarly benefit\nfrom parallel TTS remains open, mainly due to the absence of sampling\nmechanisms in continuous space, and the lack of probabilistic signals for\nadvanced trajectory aggregation. \\ This work enables parallel TTS for latent\nreasoning models by addressing the above issues. For sampling, we introduce two\nuncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive\nGaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)\ntrained with step-wise contrastive objective to score and guide latent\nreasoning. Extensive experiments and visualization analyses show that both\nsampling strategies scale effectively with compute and exhibit distinct\nexploration dynamics, while LatentRM enables effective trajectory selection.\nTogether, our explorations open a new direction for scalable inference in\ncontinuous spaces. Code released at https://github.com/YRYangang/LatentTTS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u6f5c\u5728\u63a8\u7406\u6a21\u578b\u7684\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1bDropout\u548c\u9ad8\u65af\u566a\u58f0\u4e24\u79cd\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6f5c\u5728\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u805a\u5408\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4\uff0c\u800c\u6f5c\u5728\u63a8\u7406\u6a21\u578b\u5728\u8fde\u7eed\u5411\u91cf\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u7f3a\u4e4f\u91c7\u6837\u673a\u5236\u548c\u6982\u7387\u4fe1\u53f7\u7528\u4e8e\u8f68\u8ff9\u805a\u5408\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u8fde\u7eed\u7a7a\u95f4\u7684\u5e76\u884c\u6269\u5c55\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u542f\u53d1\u7684\u968f\u673a\u91c7\u6837\u7b56\u7565\uff1a\u8499\u7279\u5361\u6d1bDropout\u548c\u52a0\u6027\u9ad8\u65af\u566a\u58f0\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u6b65\u8fdb\u5bf9\u6bd4\u76ee\u6807\u7684\u6f5c\u5728\u5956\u52b1\u6a21\u578b\u6765\u8bc4\u5206\u548c\u6307\u5bfc\u6f5c\u5728\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4e24\u79cd\u91c7\u6837\u7b56\u7565\u90fd\u80fd\u6709\u6548\u968f\u8ba1\u7b97\u91cf\u6269\u5c55\uff0c\u5e76\u5c55\u73b0\u51fa\u4e0d\u540c\u7684\u63a2\u7d22\u52a8\u6001\uff1b\u6f5c\u5728\u5956\u52b1\u6a21\u578b\u80fd\u591f\u6709\u6548\u9009\u62e9\u63a8\u7406\u8f68\u8ff9\uff0c\u4e3a\u8fde\u7eed\u7a7a\u95f4\u4e2d\u7684\u53ef\u6269\u5c55\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5b9e\u73b0\u4e86\u6f5c\u5728\u63a8\u7406\u6a21\u578b\u7684\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u901a\u8fc7\u91c7\u6837\u7b56\u7565\u548c\u8f68\u8ff9\u805a\u5408\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u63a8\u7406\u6269\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2510.07626", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07626", "abs": "https://arxiv.org/abs/2510.07626", "authors": ["Chongyu Fan", "Changsheng Wang", "Yancheng Huang", "Soumyadeep Pal", "Sijia Liu"], "title": "LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics", "comment": null, "summary": "Machine unlearning for large language models (LLMs) aims to remove undesired\ndata, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while\npreserving useful model capabilities. Despite rapid progress over the past two\nyears, research in LLM unlearning remains fragmented, with limited clarity on\nwhat constitutes effective unlearning and how it should be rigorously\nevaluated. In this work, we present a principled taxonomy of twelve recent\nstateful unlearning methods, grouped into three methodological families:\ndivergence-driven optimization, representation misalignment, and\nrejection-based targeted unlearning. Building on this taxonomy, we revisit the\nevaluation of unlearning effectiveness (UE), utility retention (UT), and\nrobustness (Rob), focusing on the WMDP benchmark. Our analysis shows that\ncurrent evaluations, dominated by multiple-choice question (MCQ) accuracy,\noffer only a narrow perspective, often overstating success while overlooking\nthe model's actual generation behavior. To address this gap, we introduce open\nquestion-answering (Open-QA) metrics that better capture generative performance\nand reveal the inherent UE-UT tradeoff across method families. Furthermore, we\ndemonstrate that robustness requires finer-grained analysis: for example,\nvulnerabilities differ substantially between in-domain relearning and\nout-of-domain fine-tuning, even though both fall under model-level attacks.\nThrough this study, we hope to deliver a full-stack revisit of LLM unlearning\nand actionable guidance for designing and evaluating future methods.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u9057\u5fd8\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u7c7b\u548c\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u9057\u5fd8\u6548\u679c\u4e0e\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u5f53\u524dLLM\u9057\u5fd8\u7814\u7a76\u7f3a\u4e4f\u6e05\u6670\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u9009\u62e9\u9898\u51c6\u786e\u7387\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u6a21\u578b\u7684\u5b9e\u9645\u751f\u6210\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5c0612\u79cd\u72b6\u6001\u9057\u5fd8\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\uff1a\u5dee\u5f02\u9a71\u52a8\u4f18\u5316\u3001\u8868\u793a\u9519\u4f4d\u548c\u57fa\u4e8e\u62d2\u7edd\u7684\u76ee\u6807\u9057\u5fd8\uff1b\u5f15\u5165\u5f00\u653e\u5f0f\u95ee\u7b54\u8bc4\u4f30\u6307\u6807\uff0c\u5206\u6790\u9057\u5fd8\u6548\u679c\u3001\u6548\u7528\u4fdd\u7559\u548c\u9c81\u68d2\u6027\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u9ad8\u4f30\u4e86\u9057\u5fd8\u6548\u679c\uff0c\u5f00\u653e\u5f0f\u95ee\u7b54\u6307\u6807\u80fd\u66f4\u597d\u6355\u6349\u751f\u6210\u6027\u80fd\uff1b\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5bb6\u65cf\u5728\u9057\u5fd8\u6548\u679c\u4e0e\u6548\u7528\u4fdd\u7559\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u8bc1\u660e\u4e86\u9c81\u68d2\u6027\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "conclusion": "\u63d0\u51fa\u4e86LLM\u9057\u5fd8\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u65b9\u6cd5\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u6765\u53cd\u6620\u5b9e\u9645\u751f\u6210\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2510.07768", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07768", "abs": "https://arxiv.org/abs/2510.07768", "authors": ["Murong Yue", "Zhiwei Liu", "Liangwei Yang", "Jianguo Zhang", "Zuxin Liu", "Haolin Chen", "Ziyu Yao", "Silvio Savarese", "Caiming Xiong", "Shelby Heinecke", "Huan Wang"], "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning", "comment": null, "summary": "Large Language Models (LLMs) equipped with external tools have demonstrated\nenhanced performance on complex reasoning tasks. The widespread adoption of\nthis tool-augmented reasoning is hindered by the scarcity of domain-specific\ntools. For instance, in domains such as physics question answering, suitable\nand specialized tools are often missing. Recent work has explored automating\ntool creation by extracting reusable functions from Chain-of-Thought (CoT)\nreasoning traces; however, these approaches face a critical scalability\nbottleneck. As the number of generated tools grows, storing them in an\nunstructured collection leads to significant retrieval challenges, including an\nexpanding search space and ambiguity between function-related tools. To address\nthis, we propose a systematic approach to automatically refactor an\nunstructured collection of tools into a structured tool library. Our system\nfirst generates discrete, task-specific tools and clusters them into\nsemantically coherent topics. Within each cluster, we introduce a multi-agent\nframework to consolidate scattered functionalities: a code agent refactors code\nto extract shared logic and creates versatile, aggregated tools, while a\nreviewing agent ensures that these aggregated tools maintain the complete\nfunctional capabilities of the original set. This process transforms numerous\nquestion-specific tools into a smaller set of powerful, aggregated tools\nwithout loss of functionality. Experimental results demonstrate that our\napproach significantly improves tool retrieval accuracy and overall reasoning\nperformance across multiple reasoning tasks. Furthermore, our method shows\nenhanced scalability compared with baselines as the number of question-specific\nincreases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u975e\u7ed3\u6784\u5316\u5de5\u5177\u96c6\u5408\u91cd\u6784\u4e3a\u7ed3\u6784\u5316\u5de5\u5177\u5e93\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5c06\u4f17\u591a\u95ee\u9898\u7279\u5b9a\u5de5\u5177\u6574\u5408\u4e3a\u5c11\u91cf\u529f\u80fd\u5f3a\u5927\u7684\u805a\u5408\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u68c0\u7d22\u51c6\u786e\u6027\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u9762\u4e34\u9886\u57df\u7279\u5b9a\u5de5\u5177\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u521b\u5efa\u5de5\u5177\u7684\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u968f\u7740\u5de5\u5177\u6570\u91cf\u589e\u957f\uff0c\u975e\u7ed3\u6784\u5316\u5b58\u50a8\u5bfc\u81f4\u68c0\u7d22\u56f0\u96be\u3002", "method": "\u9996\u5148\u751f\u6210\u79bb\u6563\u7684\u4efb\u52a1\u7279\u5b9a\u5de5\u5177\u5e76\u805a\u7c7b\u5230\u8bed\u4e49\u76f8\u5173\u4e3b\u9898\u4e2d\uff0c\u7136\u540e\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u4ee3\u7801\u667a\u80fd\u4f53\u91cd\u6784\u4ee3\u7801\u63d0\u53d6\u5171\u4eab\u903b\u8f91\u521b\u5efa\u805a\u5408\u5de5\u5177\uff0c\u8bc4\u5ba1\u667a\u80fd\u4f53\u786e\u4fdd\u805a\u5408\u5de5\u5177\u4fdd\u6301\u539f\u59cb\u529f\u80fd\u7684\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u68c0\u7d22\u51c6\u786e\u6027\u548c\u6574\u4f53\u63a8\u7406\u6027\u80fd\uff0c\u5728\u5de5\u5177\u6570\u91cf\u589e\u52a0\u65f6\u5c55\u73b0\u51fa\u6bd4\u57fa\u7ebf\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u4f17\u591a\u95ee\u9898\u7279\u5b9a\u5de5\u5177\u8f6c\u5316\u4e3a\u5c11\u91cf\u529f\u80fd\u5f3a\u5927\u7684\u805a\u5408\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u5de5\u5177\u5e93\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2510.07978", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07978", "abs": "https://arxiv.org/abs/2510.07978", "authors": ["Dhruv Jain", "Harshit Shukla", "Gautam Rajeev", "Ashish Kulkarni", "Chandra Khatri", "Shubham Agarwal"], "title": "VoiceAgentBench: Are Voice Assistants ready for agentic tasks?", "comment": null, "summary": "Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants\ncapable of understanding natural spoken queries and performing complex tasks.\nHowever, existing speech benchmarks primarily focus on isolated capabilities\nsuch as transcription, or question-answering, and do not systematically\nevaluate agentic scenarios encompassing multilingual and cultural\nunderstanding, as well as adversarial robustness. To address this, we introduce\nVoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in\nrealistic spoken agentic settings. It comprises over 5,500 synthetic spoken\nqueries, including dialogues grounded in Indian context, covering single-tool\ninvocations, multi-tool workflows, multi-turn interactions, and safety\nevaluations. The benchmark supports English, Hindi, and 5 other Indian\nlanguages, reflecting real-world linguistic and cultural diversity. We simulate\nspeaker variability using a novel sampling algorithm that selects audios for\nTTS voice conversion based on its speaker embeddings, maximizing acoustic and\nspeaker diversity. Our evaluation measures tool selection accuracy, structural\nconsistency, and the correctness of tool invocations, including adversarial\nrobustness. Our experiments reveal significant gaps in contextual tool\norchestration tasks, Indic generalization, and adversarial robustness, exposing\ncritical limitations of current SpeechLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86VoiceAgentBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u8bed\u97f3\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b5500\u591a\u4e2a\u5408\u6210\u8bed\u97f3\u67e5\u8be2\uff0c\u652f\u6301\u82f1\u8bed\u3001\u5370\u5730\u8bed\u548c5\u79cd\u5370\u5ea6\u8bed\u8a00\uff0c\u8bc4\u4f30\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u5de5\u5177\u8c03\u7528\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u8f6c\u5f55\u6216\u95ee\u7b54\u7b49\u5b64\u7acb\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8bed\u8a00\u6587\u5316\u7406\u89e3\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bed\u97f3\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u521b\u5efa\u5305\u542b\u5355\u5de5\u5177\u8c03\u7528\u3001\u591a\u5de5\u5177\u5de5\u4f5c\u6d41\u3001\u591a\u8f6e\u4ea4\u4e92\u548c\u5b89\u5168\u8bc4\u4f30\u7684\u8bed\u97f3\u67e5\u8be2\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u91c7\u6837\u7b97\u6cd5\u6700\u5927\u5316\u58f0\u5b66\u548c\u8bf4\u8bdd\u4eba\u591a\u6837\u6027\uff0c\u652f\u6301\u591a\u79cd\u5370\u5ea6\u8bed\u8a00\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5de5\u5177\u7f16\u6392\u4efb\u52a1\u3001\u5370\u5ea6\u8bed\u8a00\u6cdb\u5316\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u548c\u5173\u952e\u9650\u5236\u3002", "conclusion": "VoiceAgentBench\u57fa\u51c6\u6d4b\u8bd5\u66b4\u9732\u4e86\u5f53\u524d\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u6587\u5316\u7406\u89e3\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u3002", "topic": "agent analysis"}}
{"id": "2510.07988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07988", "abs": "https://arxiv.org/abs/2510.07988", "authors": ["Haitao Jia", "Ming He", "Zimo Yin", "Likang Wu", "Jianping Fan", "Jitao Sang"], "title": "ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation", "comment": null, "summary": "Mobile GUI agents exhibit substantial potential to facilitate and automate\nthe execution of user tasks on mobile phones. However, exist mobile GUI agents\npredominantly privilege autonomous operation and neglect the necessity of\nactive user engagement during task execution. This omission undermines their\nadaptability to information dilemmas including ambiguous, dynamically evolving,\nand conflicting task scenarios, leading to execution outcomes that deviate from\ngenuine user requirements and preferences. To address these shortcomings, we\npropose ReInAgent, a context-aware multi-agent framework that leverages dynamic\ninformation management to enable human-in-the-loop mobile task navigation.\nReInAgent integrates three specialized agents around a shared memory module: an\ninformation-managing agent for slot-based information management and proactive\ninteraction with the user, a decision-making agent for conflict-aware planning,\nand a reflecting agent for task reflection and information consistency\nvalidation. Through continuous contextual information analysis and sustained\nuser-agent collaboration, ReInAgent overcomes the limitation of existing\napproaches that rely on clear and static task assumptions. Consequently, it\nenables more adaptive and reliable mobile task navigation in complex,\nreal-world scenarios. Experimental results demonstrate that ReInAgent\neffectively resolves information dilemmas and produces outcomes that are more\nclosely aligned with genuine user preferences. Notably, on complex tasks\ninvolving information dilemmas, ReInAgent achieves a 25% higher success rate\nthan Mobile-Agent-v2.", "AI": {"tldr": "ReInAgent\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4fe1\u606f\u7ba1\u7406\u548c\u4eba\u673a\u534f\u4f5c\u6765\u89e3\u51b3\u79fb\u52a8GUI\u667a\u80fd\u4f53\u5728\u4fe1\u606f\u56f0\u5883\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u667a\u80fd\u4f53\u8fc7\u5ea6\u5f3a\u8c03\u81ea\u4e3b\u64cd\u4f5c\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u4e3b\u52a8\u53c2\u4e0e\uff0c\u5bfc\u81f4\u5728\u6a21\u7cca\u3001\u52a8\u6001\u53d8\u5316\u548c\u51b2\u7a81\u7684\u4efb\u52a1\u573a\u666f\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u6267\u884c\u7ed3\u679c\u504f\u79bb\u7528\u6237\u771f\u5b9e\u9700\u6c42\u3002", "method": "ReInAgent\u96c6\u6210\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u4fe1\u606f\u7ba1\u7406\u667a\u80fd\u4f53\uff08\u57fa\u4e8e\u69fd\u4f4d\u7684\u4fe1\u606f\u7ba1\u7406\u548c\u4e3b\u52a8\u7528\u6237\u4ea4\u4e92\uff09\u3001\u51b3\u7b56\u667a\u80fd\u4f53\uff08\u51b2\u7a81\u611f\u77e5\u89c4\u5212\uff09\u548c\u53cd\u601d\u667a\u80fd\u4f53\uff08\u4efb\u52a1\u53cd\u601d\u548c\u4fe1\u606f\u4e00\u81f4\u6027\u9a8c\u8bc1\uff09\uff0c\u56f4\u7ed5\u5171\u4eab\u5185\u5b58\u6a21\u5757\u5de5\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aReInAgent\u80fd\u6709\u6548\u89e3\u51b3\u4fe1\u606f\u56f0\u5883\uff0c\u4ea7\u751f\u66f4\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u7ed3\u679c\u3002\u5728\u6d89\u53ca\u4fe1\u606f\u56f0\u5883\u7684\u590d\u6742\u4efb\u52a1\u4e0a\uff0c\u6bd4Mobile-Agent-v2\u6210\u529f\u7387\u63d0\u9ad825%\u3002", "conclusion": "\u901a\u8fc7\u6301\u7eed\u4e0a\u4e0b\u6587\u4fe1\u606f\u5206\u6790\u548c\u6301\u7eed\u7684\u4eba\u673a\u534f\u4f5c\uff0cReInAgent\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6e05\u6670\u9759\u6001\u4efb\u52a1\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u66f4\u81ea\u9002\u5e94\u548c\u53ef\u9760\u7684\u79fb\u52a8\u4efb\u52a1\u5bfc\u822a\u3002", "topic": "agent analysis"}}
{"id": "2510.07650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07650", "abs": "https://arxiv.org/abs/2510.07650", "authors": ["Perry Dong", "Chongyi Zheng", "Chelsea Finn", "Dorsa Sadigh", "Benjamin Eysenbach"], "title": "Value Flows", "comment": null, "summary": "While most reinforcement learning methods today flatten the distribution of\nfuture returns to a single scalar value, distributional RL methods exploit the\nreturn distribution to provide stronger learning signals and to enable\napplications in exploration and safe RL. While the predominant method for\nestimating the return distribution is by modeling it as a categorical\ndistribution over discrete bins or estimating a finite number of quantiles,\nsuch approaches leave unanswered questions about the fine-grained structure of\nthe return distribution and about how to distinguish states with high return\nuncertainty for decision-making. The key idea in this paper is to use modern,\nflexible flow-based models to estimate the full future return distributions and\nidentify those states with high return variance. We do so by formulating a new\nflow-matching objective that generates probability density paths satisfying the\ndistributional Bellman equation. Building upon the learned flow models, we\nestimate the return uncertainty of distinct states using a new flow derivative\nODE. We additionally use this uncertainty information to prioritize learning a\nmore accurate return estimation on certain transitions. We compare our method\n(Value Flows) with prior methods in the offline and online-to-online settings.\nExperiments on $37$ state-based and $25$ image-based benchmark tasks\ndemonstrate that Value Flows achieves a $1.3\\times$ improvement on average in\nsuccess rates. Website: https://pd-perry.github.io/value-flows Code:\nhttps://github.com/chongyi-zheng/value-flows", "AI": {"tldr": "\u63d0\u51faValue Flows\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u4f30\u8ba1\u5b8c\u6574\u672a\u6765\u56de\u62a5\u5206\u5e03\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u76ee\u6807\u6ee1\u8db3\u5206\u5e03\u8d1d\u5c14\u66fc\u65b9\u7a0b\uff0c\u5e76\u5229\u7528\u6d41\u5bfc\u6570ODE\u4f30\u8ba1\u72b6\u6001\u56de\u62a5\u4e0d\u786e\u5b9a\u6027\uff0c\u572837\u4e2a\u72b6\u6001\u57fa\u51c6\u548c25\u4e2a\u56fe\u50cf\u57fa\u51c6\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u7387\u63d0\u53471.3\u500d", "motivation": "\u73b0\u6709\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u79bb\u6563\u5206\u7bb1\u6216\u6709\u9650\u5206\u4f4d\u6570\u6765\u5efa\u6a21\u56de\u62a5\u5206\u5e03\uff0c\u65e0\u6cd5\u6355\u6349\u5206\u5e03\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\uff0c\u4e5f\u96be\u4ee5\u533a\u5206\u5177\u6709\u9ad8\u56de\u62a5\u4e0d\u786e\u5b9a\u6027\u7684\u72b6\u6001\u8fdb\u884c\u51b3\u7b56", "method": "\u4f7f\u7528\u57fa\u4e8e\u6d41\u7684\u7075\u6d3b\u6a21\u578b\u4f30\u8ba1\u5b8c\u6574\u672a\u6765\u56de\u62a5\u5206\u5e03\uff0c\u63d0\u51fa\u65b0\u7684\u6d41\u5339\u914d\u76ee\u6807\u751f\u6210\u6ee1\u8db3\u5206\u5e03\u8d1d\u5c14\u66fc\u65b9\u7a0b\u7684\u6982\u7387\u5bc6\u5ea6\u8def\u5f84\uff0c\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u6d41\u6a21\u578b\u4f7f\u7528\u65b0\u7684\u6d41\u5bfc\u6570ODE\u4f30\u8ba1\u72b6\u6001\u56de\u62a5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u4f18\u5148\u5b66\u4e60\u66f4\u51c6\u786e\u7684\u56de\u62a5\u4f30\u8ba1", "result": "\u572837\u4e2a\u72b6\u6001\u57fa\u51c6\u548c25\u4e2a\u56fe\u50cf\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cValue Flows\u65b9\u6cd5\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5e73\u5747\u6210\u529f\u7387\u63d0\u53471.3\u500d", "conclusion": "\u57fa\u4e8e\u6d41\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u56de\u62a5\u5206\u5e03\uff0c\u6709\u6548\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027\u72b6\u6001\uff0c\u5e76\u5728\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u5230\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd", "topic": "agentic reinforcement learning"}}
{"id": "2510.08026", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08026", "abs": "https://arxiv.org/abs/2510.08026", "authors": ["Chen Huang", "Wei Lu", "Wenxuan Zhang"], "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning", "comment": "15 pages, 6 figures", "summary": "Large Reasoning Models (LRMs) have achieved impressive performance on complex\nreasoning tasks by generating detailed chain-of-thought (CoT) explanations.\nHowever, these responses are often excessively long, containing redundant\nreasoning steps that inflate inference cost and reduce usability. Controlling\nthe length of generated reasoning without sacrificing accuracy remains an open\nchallenge. Through a systematic empirical analysis, we reveal a consistent\npositive correlation between model entropy and response length at different\nreasoning stages across diverse LRMs: the thinking phase exhibits higher\nentropy, reflecting exploratory behavior of longer responses, while the final\nanswer phase shows lower entropy, indicating a more deterministic solution.This\nobservation suggests that entropy at different reasoning stages can serve as a\ncontrol knob for balancing conciseness and performance. Based on this insight,\nthis paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism\nthat incorporating phase-dependent entropy into the reward design. Instead of\ntreating all tokens uniformly, PEAR penalize excessive entropy during the\nthinking phase and allowing moderate exploration at the final answer phase,\nwhich encourages models to generate concise reasoning traces that retain\nsufficient flexibility to solve the task correctly. This enables adaptive\ncontrol of response length without relying on explicit length targets or rigid\ntruncation rules. Extensive experiments across four benchmarks demonstrate that\nPEAR consistently reduces response length while sustaining competitive accuracy\nacross model scales. In addition, PEAR demonstrates strong out-of-distribution\n(OOD) robustness beyond the training distribution. Our code is available at:\nhttps://github.com/iNLP-Lab/PEAR.", "AI": {"tldr": "PEAR\u662f\u4e00\u79cd\u57fa\u4e8e\u9636\u6bb5\u71b5\u611f\u77e5\u7684\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u63a8\u7406\u9636\u6bb5\u8c03\u6574\u71b5\u503c\u6765\u63a7\u5236\u5927\u63a8\u7406\u6a21\u578b\u7684\u54cd\u5e94\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u94fe\u5f80\u5f80\u8fc7\u957f\u4e14\u5305\u542b\u5197\u4f59\u6b65\u9aa4\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\u5e76\u964d\u4f4e\u4e86\u53ef\u7528\u6027\u3002\u5982\u4f55\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63a7\u5236\u63a8\u7406\u957f\u5ea6\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\u6a21\u578b\u71b5\u4e0e\u54cd\u5e94\u957f\u5ea6\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u57fa\u4e8e\u6b64\u63d0\u51faPEAR\u5956\u52b1\u673a\u5236\uff0c\u5728\u601d\u8003\u9636\u6bb5\u60e9\u7f5a\u8fc7\u9ad8\u71b5\u503c\uff0c\u5728\u7b54\u6848\u9636\u6bb5\u5141\u8bb8\u9002\u5ea6\u63a2\u7d22\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u63a7\u5236\u54cd\u5e94\u957f\u5ea6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPEAR\u80fd\u6301\u7eed\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u9636\u6bb5\u71b5\u53ef\u4ee5\u4f5c\u4e3a\u5e73\u8861\u7b80\u6d01\u6027\u548c\u6027\u80fd\u7684\u6709\u6548\u63a7\u5236\u624b\u6bb5\uff0cPEAR\u673a\u5236\u4e3a\u81ea\u9002\u5e94\u63a7\u5236\u63a8\u7406\u6a21\u578b\u54cd\u5e94\u957f\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.07794", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07794", "abs": "https://arxiv.org/abs/2510.07794", "authors": ["Peilin Wu", "Mian Zhang", "Kun Wan", "Wentian Zhao", "Kaiyu He", "Xinya Du", "Zhiyu Chen"], "title": "HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation", "comment": "Under review", "summary": "Agentic RAG is a powerful technique for incorporating external information\nthat LLMs lack, enabling better problem solving and question answering.\nHowever, suboptimal search behaviors exist widely, such as over-search\n(retrieving information already known) and under-search (failing to search when\nnecessary), which leads to unnecessary overhead and unreliable outputs. Current\ntraining methods, which typically rely on outcome-based rewards in a RL\nframework, lack the fine-grained control needed to address these\ninefficiencies. To overcome this, we introduce Hierarchical Process Rewards for\nEfficient agentic RAG (HiPRAG), a training methodology that incorporates a\nfine-grained, knowledge-grounded process reward into the RL training. Our\napproach evaluates the necessity of each search decision on-the-fly by\ndecomposing the agent's reasoning trajectory into discrete, parsable steps. We\nthen apply a hierarchical reward function that provides an additional bonus\nbased on the proportion of optimal search and non-search steps, on top of\ncommonly used outcome and format rewards. Experiments on the Qwen2.5 and\nLlama-3.2 models across seven diverse QA benchmarks show that our method\nachieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished\nwhile improving search efficiency, reducing the over-search rate to just 2.3%\nand concurrently lowering the under-search rate. These results demonstrate the\nefficacy of optimizing the reasoning process itself, not just the final\noutcome. Further experiments and analysis demonstrate that HiPRAG shows good\ngeneralizability across a wide range of RL algorithms, model families, sizes,\nand types. This work demonstrates the importance and potential of fine-grained\ncontrol through RL, for improving the efficiency and optimality of reasoning\nfor search agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86HiPRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u8fc7\u7a0b\u5956\u52b1\u4f18\u5316RAG\u4ee3\u7406\u7684\u641c\u7d22\u884c\u4e3a\uff0c\u89e3\u51b3\u8fc7\u5ea6\u641c\u7d22\u548c\u641c\u7d22\u4e0d\u8db3\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548c\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7ed3\u679c\u5956\u52b1\u7684RL\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5bfc\u81f4RAG\u4ee3\u7406\u5b58\u5728\u8fc7\u5ea6\u641c\u7d22\u548c\u641c\u7d22\u4e0d\u8db3\u7b49\u4f4e\u6548\u884c\u4e3a\uff0c\u9700\u8981\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u5206\u5c42\u8fc7\u7a0b\u5956\u52b1\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u5206\u89e3\u4e3a\u53ef\u89e3\u6790\u6b65\u9aa4\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u641c\u7d22\u51b3\u7b56\u7684\u5fc5\u8981\u6027\uff0c\u5728\u7ed3\u679c\u5956\u52b1\u57fa\u7840\u4e0a\u589e\u52a0\u57fa\u4e8e\u6700\u4f18\u641c\u7d22\u6b65\u9aa4\u6bd4\u4f8b\u7684\u989d\u5916\u5956\u52b1\u3002", "result": "\u57287\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwen2.5\u548cLlama-3.2\u6a21\u578b\u5206\u522b\u8fbe\u523065.4%(3B)\u548c67.2%(7B)\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u8fc7\u5ea6\u641c\u7d22\u7387\u964d\u81f32.3%\uff0c\u540c\u65f6\u964d\u4f4e\u641c\u7d22\u4e0d\u8db3\u7387\u3002", "conclusion": "\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u7ec8\u7ed3\u679c\uff0c\u901a\u8fc7RL\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u7684\u6548\u7387\u548c\u6700\u4f18\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07730", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07730", "abs": "https://arxiv.org/abs/2510.07730", "authors": ["Changyeon Kim", "Haeone Lee", "Younggyo Seo", "Kimin Lee", "Yuke Zhu"], "title": "DEAS: DEtached value learning with Action Sequence for Scalable Offline RL", "comment": "Project website: https://changyeon.site/deas", "summary": "Offline reinforcement learning (RL) presents an attractive paradigm for\ntraining intelligent agents without expensive online interactions. However,\ncurrent approaches still struggle with complex, long-horizon sequential\ndecision making. In this work, we introduce DEtached value learning with Action\nSequence (DEAS), a simple yet effective offline RL framework that leverages\naction sequences for value learning. These temporally extended actions provide\nricher information than single-step actions and can be interpreted through the\noptions framework via semi-Markov decision process Q-learning, enabling\nreduction of the effective planning horizon by considering longer sequences at\nonce. However, directly adopting such sequences in actor-critic algorithms\nintroduces excessive value overestimation, which we address through detached\nvalue learning that steers value estimates toward in-distribution actions that\nachieve high return in the offline dataset. We demonstrate that DEAS\nconsistently outperforms baselines on complex, long-horizon tasks from OGBench\nand can be applied to enhance the performance of large-scale\nVision-Language-Action models that predict action sequences, significantly\nboosting performance in both RoboCasa Kitchen simulation tasks and real-world\nmanipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEAS\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u4ef7\u503c\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u79bb\u4ef7\u503c\u5b66\u4e60\u51cf\u5c11\u4ef7\u503c\u9ad8\u4f30\uff0c\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65e0\u9700\u6602\u8d35\u5728\u7ebf\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u957f\u65f6\u7a0b\u5e8f\u5217\u51b3\u7b56\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be", "method": "DEAS\u6846\u67b6\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u4ef7\u503c\u5b66\u4e60\uff0c\u901a\u8fc7\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0bQ\u5b66\u4e60\u89e3\u91ca\u52a8\u4f5c\u5e8f\u5217\uff0c\u91c7\u7528\u5206\u79bb\u4ef7\u503c\u5b66\u4e60\u51cf\u5c11\u4ef7\u503c\u9ad8\u4f30", "result": "\u5728OGBench\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728RoboCasa Kitchen\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "conclusion": "DEAS\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u79bb\u7ebfRL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u5e8f\u5217\u548c\u5206\u79bb\u4ef7\u503c\u5b66\u4e60\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u6311\u6218", "topic": "agentic reinforcement learning"}}
{"id": "2510.07799", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07799", "abs": "https://arxiv.org/abs/2510.07799", "authors": ["Eric Hanchen Jiang", "Guancheng Wan", "Sophia Yin", "Mengting Li", "Yuchen Wu", "Xiao Liang", "Xinfeng Li", "Yizhou Sun", "Wei Wang", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models", "comment": null, "summary": "The efficiency of multi-agent systems driven by large language models (LLMs)\nlargely hinges on their communication topology. However, designing an optimal\ntopology is a non-trivial challenge, as it requires balancing competing\nobjectives such as task performance, communication cost, and robustness.\nExisting frameworks often rely on static or hand-crafted topologies, which\ninherently fail to adapt to diverse task requirements, leading to either\nexcessive token consumption for simple problems or performance bottlenecks for\ncomplex ones. To address this challenge, we introduce a novel generative\nframework called \\textit{Guided Topology Diffusion (GTD)}. Inspired by\nconditional discrete graph diffusion models, GTD formulates topology synthesis\nas an iterative construction process. At each step, the generation is steered\nby a lightweight proxy model that predicts multi-objective rewards (e.g.,\naccuracy, utility, cost), enabling real-time, gradient-free optimization\ntowards task-adaptive topologies. This iterative, guided synthesis process\ndistinguishes GTD from single-step generative frameworks, enabling it to better\nnavigate complex design trade-offs. We validated GTD across multiple\nbenchmarks, and experiments show that this framework can generate highly\ntask-adaptive, sparse, and efficient communication topologies, significantly\noutperforming existing methods in LLM agent collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTD\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u79bb\u6563\u56fe\u6269\u6563\u6a21\u578b\u8fed\u4ee3\u751f\u6210\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u4fe1\u62d3\u6251\uff0c\u5b9e\u73b0\u4efb\u52a1\u81ea\u9002\u5e94\u3001\u7a00\u758f\u4e14\u9ad8\u6548\u7684\u62d3\u6251\u8bbe\u8ba1\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u901a\u4fe1\u62d3\u6251\uff0c\u4f46\u8bbe\u8ba1\u6700\u4f18\u62d3\u6251\u9700\u8981\u5728\u4efb\u52a1\u6027\u80fd\u3001\u901a\u4fe1\u6210\u672c\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u5e73\u8861\u3002\u73b0\u6709\u9759\u6001\u62d3\u6251\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002", "method": "GTD\u6846\u67b6\u5c06\u62d3\u6251\u5408\u6210\u5efa\u6a21\u4e3a\u8fed\u4ee3\u6784\u5efa\u8fc7\u7a0b\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u591a\u76ee\u6807\u5956\u52b1\uff08\u5982\u51c6\u786e\u6027\u3001\u6548\u7528\u3001\u6210\u672c\uff09\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u68af\u5ea6\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u62d3\u6251\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0cGTD\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u4efb\u52a1\u81ea\u9002\u5e94\u3001\u7a00\u758f\u4e14\u9ad8\u6548\u7684\u901a\u4fe1\u62d3\u6251\uff0c\u5728LLM\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GTD\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u5f15\u5bfc\u7684\u62d3\u6251\u5408\u6210\u8fc7\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u8bbe\u8ba1\u6743\u8861\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4f18\u5316\u7684\u901a\u4fe1\u62d3\u6251\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.08207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08207", "abs": "https://arxiv.org/abs/2510.08207", "authors": ["Matteo Gregorini", "Chiara Boldrini", "Lorenzo Valerio"], "title": "DODO: Causal Structure Learning with Budgeted Interventions", "comment": "Under review. Supported by SoBigData\\.it IR0000013, FAIR PE00000013,\n  ICSC CN00000013", "summary": "Artificial Intelligence has achieved remarkable advancements in recent years,\nyet much of its progress relies on identifying increasingly complex\ncorrelations. Enabling causality awareness in AI has the potential to enhance\nits performance by enabling a deeper understanding of the underlying mechanisms\nof the environment. In this paper, we introduce DODO, an algorithm defining how\nan Agent can autonomously learn the causal structure of its environment through\nrepeated interventions. We assume a scenario where an Agent interacts with a\nworld governed by a causal Directed Acyclic Graph (DAG), which dictates the\nsystem's dynamics but remains hidden from the Agent. The Agent's task is to\naccurately infer the causal DAG, even in the presence of noise. To achieve\nthis, the Agent performs interventions, leveraging causal inference techniques\nto analyze the statistical significance of observed changes. Results show\nbetter performance for DODO, compared to observational approaches, in all but\nthe most limited resource conditions. DODO is often able to reconstruct with as\nlow as zero errors the structure of the causal graph. In the most challenging\nconfiguration, DODO outperforms the best baseline by +0.25 F1 points.", "AI": {"tldr": "DODO\u7b97\u6cd5\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u91cd\u590d\u5e72\u9884\u81ea\u4e3b\u5b66\u4e60\u73af\u5883\u56e0\u679c\u7ed3\u6784\uff0c\u76f8\u6bd4\u89c2\u5bdf\u6027\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u5728\u6709\u566a\u58f0\u73af\u5883\u4e2d\u51c6\u786e\u63a8\u65ad\u56e0\u679c\u6709\u5411\u65e0\u73af\u56fe\u3002", "motivation": "\u5f53\u524dAI\u4e3b\u8981\u4f9d\u8d56\u590d\u6742\u76f8\u5173\u6027\uff0c\u7f3a\u4e4f\u56e0\u679c\u7406\u89e3\u80fd\u529b\u3002\u4f7fAI\u5177\u5907\u56e0\u679c\u610f\u8bc6\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\uff0c\u6df1\u5165\u7406\u89e3\u73af\u5883\u5e95\u5c42\u673a\u5236\u3002", "method": "\u667a\u80fd\u4f53\u901a\u8fc7\u91cd\u590d\u5e72\u9884\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u5229\u7528\u56e0\u679c\u63a8\u65ad\u6280\u672f\u5206\u6790\u89c2\u5bdf\u53d8\u5316\u7684\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u81ea\u4e3b\u5b66\u4e60\u9690\u85cf\u7684\u56e0\u679c\u6709\u5411\u65e0\u73af\u56fe\u7ed3\u6784\u3002", "result": "DODO\u5728\u9664\u6700\u6709\u9650\u8d44\u6e90\u6761\u4ef6\u5916\u7684\u6240\u6709\u60c5\u51b5\u4e0b\u4f18\u4e8e\u89c2\u5bdf\u6027\u65b9\u6cd5\uff0c\u901a\u5e38\u80fd\u4ee5\u96f6\u8bef\u5dee\u91cd\u5efa\u56e0\u679c\u56fe\u7ed3\u6784\uff0c\u5728\u6700\u5177\u6311\u6218\u6027\u914d\u7f6e\u4e2d\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa+0.25 F1\u5206\u6570\u3002", "conclusion": "\u901a\u8fc7\u5e72\u9884\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\u7684\u65b9\u6cd5\u6709\u6548\uff0cDODO\u7b97\u6cd5\u5728\u56e0\u679c\u56fe\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aAI\u56e0\u679c\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07892", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07892", "abs": "https://arxiv.org/abs/2510.07892", "authors": ["Hyeonseok Moon", "Seongtae Hong", "Jaehyung Seo", "Heuiseok Lim"], "title": "Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models", "comment": "Accepted to the EMNLP2025", "summary": "Recent frontier-level LLMs have saturated many previously difficult\nbenchmarks, leaving little room for further differentiation. This progress\nhighlights the need for challenging benchmarks that provide objective\nverification. In this paper, we introduce MCBench, a benchmark designed to\nevaluate whether LLMs can execute string-matching NLP metrics by strictly\nfollowing step-by-step instructions. Unlike prior benchmarks that depend on\nsubjective judgments or general reasoning, MCBench offers an objective,\ndeterministic and codeverifiable evaluation. This setup allows us to\nsystematically test whether LLMs can maintain accurate step-by-step execution,\nincluding instruction adherence, numerical computation, and long-range\nconsistency in handling intermediate results. To ensure objective evaluation of\nthese abilities, we provide a parallel reference code that can evaluate the\naccuracy of LLM output. We provide three evaluative metrics and three benchmark\nvariants designed to measure the detailed instruction understanding capability\nof LLMs. Our analyses show that MCBench serves as an effective and objective\ntool for evaluating the capabilities of cutting-edge LLMs.", "AI": {"tldr": "MCBench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u662f\u5426\u80fd\u591f\u4e25\u683c\u9075\u5faa\u9010\u6b65\u6307\u4ee4\u6267\u884c\u5b57\u7b26\u4e32\u5339\u914dNLP\u6307\u6807\uff0c\u63d0\u4f9b\u5ba2\u89c2\u3001\u786e\u5b9a\u6027\u548c\u53ef\u4ee3\u7801\u9a8c\u8bc1\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u524d\u6cbfLLM\u5df2\u7ecf\u9971\u548c\u4e86\u8bb8\u591a\u5148\u524d\u56f0\u96be\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6765\u63d0\u4f9b\u5ba2\u89c2\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1MCBench\u57fa\u51c6\uff0c\u901a\u8fc7\u4e25\u683c\u9075\u5faa\u9010\u6b65\u6307\u4ee4\u6765\u8bc4\u4f30LLM\u6267\u884c\u5b57\u7b26\u4e32\u5339\u914dNLP\u6307\u6807\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u5e76\u884c\u53c2\u8003\u4ee3\u7801\u6765\u9a8c\u8bc1LLM\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002", "result": "MCBench\u4f5c\u4e3a\u8bc4\u4f30\u524d\u6cbfLLM\u80fd\u529b\u7684\u6709\u6548\u5ba2\u89c2\u5de5\u5177\uff0c\u80fd\u591f\u7cfb\u7edf\u6d4b\u8bd5LLM\u5728\u9010\u6b65\u6267\u884c\u3001\u6307\u4ee4\u9075\u5faa\u3001\u6570\u503c\u8ba1\u7b97\u548c\u957f\u8303\u56f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "MCBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5ba2\u89c2\u3001\u786e\u5b9a\u6027\u548c\u53ef\u4ee3\u7801\u9a8c\u8bc1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206\u4e0d\u540cLLM\u5728\u8be6\u7ec6\u6307\u4ee4\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u5dee\u5f02\u3002", "topic": "swe benchmark"}}
{"id": "2510.08238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08238", "abs": "https://arxiv.org/abs/2510.08238", "authors": ["Jiyang Qiu", "Xinbei Ma", "Yunqing Xu", "Zhuosheng Zhang", "Hai Zhao"], "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness", "comment": null, "summary": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoTri\u7684\u591a\u6b65\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u9488\u5bf9LLM\u667a\u80fd\u4f53\u8fdb\u884c\u957f\u65f6\u7a0b\u63a7\u5236\uff0c\u653b\u51fb\u6210\u529f\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u540c\u65f6\u80fd\u63d0\u9ad8\u667a\u80fd\u4f53\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u9700\u8981\u63ed\u793a\u8fd9\u4e9b\u667a\u80fd\u4f53\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "CoTri\u540e\u95e8\u653b\u51fb\u4f7f\u7528\u6709\u5e8f\u89e6\u53d1\u5e8f\u5217\uff0c\u4ece\u521d\u59cb\u89e6\u53d1\u5f00\u59cb\uff0c\u540e\u7eed\u89e6\u53d1\u4ece\u73af\u5883\u4e2d\u63d0\u53d6\uff0c\u5b9e\u73b0\u591a\u6b65\u64cd\u63a7\u4f7f\u667a\u80fd\u4f53\u504f\u79bb\u539f\u4efb\u52a1\u3002", "result": "CoTri\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u96f6\u7684\u8bef\u89e6\u53d1\u7387\uff0c\u5e76\u4e14\u80fd\u63d0\u9ad8\u667a\u80fd\u4f53\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "conclusion": "CoTri\u5b9e\u73b0\u4e86\u5bf9\u667a\u80fd\u4f53\u7684\u7a33\u5b9a\u591a\u6b65\u63a7\u5236\uff0c\u63d0\u9ad8\u4e86\u5176\u56fa\u6709\u9c81\u68d2\u6027\u548c\u4efb\u52a1\u80fd\u529b\uff0c\u4f7f\u653b\u51fb\u66f4\u52a0\u9690\u853d\u5e76\u5e26\u6765\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002", "topic": "agent analysis"}}
{"id": "2510.08263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08263", "abs": "https://arxiv.org/abs/2510.08263", "authors": ["Shunyu An", "Miao Wang", "Yongchao Li", "Dong Wan", "Lina Wang", "Ling Qin", "Liqin Gao", "Congyao Fan", "Zhiyong Mao", "Jiange Pu", "Wenji Xia", "Dong Zhao", "Rui Hu", "Ji Lu", "Guiyue Zhou", "Baoyu Tang", "Yanqin Gao", "Yongsheng Du", "Daigang Xu", "Lingjun Huang", "Baoli Wang", "Xiwen Zhang", "Luyao Wang", "Shilong Liu"], "title": "Co-TAP: Three-Layer Agent Interaction Protocol Technical Report", "comment": null, "summary": "This paper proposes Co-TAP (T: Triple, A: Agent, P: Protocol), a three-layer\nagent interaction protocol designed to address the challenges faced by\nmulti-agent systems across the three core dimensions of Interoperability,\nInteraction and Collaboration, and Knowledge Sharing. We have designed and\nproposed a layered solution composed of three core protocols: the Human-Agent\nInteraction Protocol (HAI), the Unified Agent Protocol (UAP), and the\nMemory-Extraction-Knowledge Protocol (MEK). HAI focuses on the interaction\nlayer, standardizing the flow of information between users, interfaces, and\nagents by defining a standardized, event-driven communication paradigm. This\nensures the real-time performance, reliability, and synergy of interactions. As\nthe core of the infrastructure layer, UAP is designed to break down\ncommunication barriers among heterogeneous agents through unified service\ndiscovery and protocol conversion mechanisms, thereby enabling seamless\ninterconnection and interoperability of the underlying network. MEK, in turn,\noperates at the cognitive layer. By establishing a standardized ''Memory (M) -\nExtraction (E) - Knowledge (K)'' cognitive chain, it empowers agents with the\nability to learn from individual experiences and form shareable knowledge,\nthereby laying the foundation for the realization of true collective\nintelligence. We believe this protocol framework will provide a solid\nengineering foundation and theoretical guidance for building the next\ngeneration of efficient, scalable, and intelligent multi-agent applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86Co-TAP\u4e09\u5c42\u667a\u80fd\u4f53\u4ea4\u4e92\u534f\u8bae\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u4e92\u64cd\u4f5c\u6027\u3001\u4ea4\u4e92\u534f\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\u4e09\u4e2a\u6838\u5fc3\u7ef4\u5ea6\u7684\u6311\u6218\uff0c\u5305\u542bHAI\u3001UAP\u548cMEK\u4e09\u4e2a\u6838\u5fc3\u534f\u8bae\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u4ea4\u4e92\u534f\u4f5c\u548c\u77e5\u8bc6\u5171\u4eab\u4e09\u5927\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u667a\u80fd\u591a\u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u5de5\u7a0b\u57fa\u7840\u548c\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u8bbe\u8ba1\u4e09\u5c42\u534f\u8bae\u6846\u67b6\uff1aHAI\u534f\u8bae\u6807\u51c6\u5316\u4eba\u673a\u4ea4\u4e92\u6d41\u7a0b\uff0cUAP\u534f\u8bae\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u95f4\u7684\u65e0\u7f1d\u4e92\u8054\uff0cMEK\u534f\u8bae\u5efa\u7acb\u6807\u51c6\u5316\u7684\u8bb0\u5fc6-\u63d0\u53d6-\u77e5\u8bc6\u8ba4\u77e5\u94fe\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u667a\u80fd\u4f53\u4ea4\u4e92\u534f\u8bae\u6846\u67b6\uff0c\u80fd\u591f\u652f\u6301\u5b9e\u65f6\u3001\u53ef\u9760\u3001\u534f\u540c\u7684\u4ea4\u4e92\uff0c\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u4e3a\u96c6\u4f53\u667a\u80fd\u7684\u5b9e\u73b0\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "Co-TAP\u534f\u8bae\u6846\u67b6\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u3001\u667a\u80fd\u7684\u591a\u667a\u80fd\u4f53\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u5de5\u7a0b\u57fa\u7840\u548c\u7406\u8bba\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.07841", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.07841", "abs": "https://arxiv.org/abs/2510.07841", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "title": "Self-Improving LLM Agents at Test-Time", "comment": null, "summary": "One paradigm of language model (LM) fine-tuning relies on creating large\ntraining datasets, under the assumption that high quantity and diversity will\nenable models to generalize to novel tasks after post-training. In practice,\ngathering large sets of data is inefficient, and training on them is\nprohibitively expensive; worse, there is no guarantee that the resulting model\nwill handle complex scenarios or generalize better. Moreover, existing\ntechniques rarely assess whether a training sample provides novel information\nor is redundant with the knowledge already acquired by the model, resulting in\nunnecessary costs. In this work, we explore a new test-time self-improvement\nmethod to create more effective and generalizable agentic LMs on-the-fly. The\nproposed algorithm can be summarized in three steps: (i) first it identifies\nthe samples that model struggles with (self-awareness), (ii) then generates\nsimilar examples from detected uncertain samples (self-data augmentation), and\n(iii) uses these newly generated samples at test-time fine-tuning\n(self-improvement). We study two variants of this approach: Test-Time\nSelf-Improvement (TT-SI), where the same model generates additional training\nexamples from its own uncertain cases and then learns from them, and contrast\nthis approach with Test-Time Distillation (TT-D), where a stronger model\ngenerates similar examples for uncertain cases, enabling student to adapt using\ndistilled supervision. Empirical evaluations across different agent benchmarks\ndemonstrate that TT-SI improves the performance with +5.48% absolute accuracy\ngain on average across all benchmarks and surpasses other standard learning\nmethods, yet using 68x less training samples. Our findings highlight the\npromise of TT-SI, demonstrating the potential of self-improvement algorithms at\ntest-time as a new paradigm for building more capable agents toward\nself-evolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u81ea\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6a21\u578b\u4e0d\u786e\u5b9a\u6837\u672c\u3001\u751f\u6210\u7c7b\u4f3c\u793a\u4f8b\u5e76\u8fdb\u884c\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u66f4\u5c11\u8bad\u7ec3\u6837\u672c\u5b9e\u73b0\u66f4\u597d\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u4e14\u4e0d\u80fd\u4fdd\u8bc1\u6cdb\u5316\u80fd\u529b\u3002\u73b0\u6709\u6280\u672f\u5f88\u5c11\u8bc4\u4f30\u8bad\u7ec3\u6837\u672c\u662f\u5426\u63d0\u4f9b\u65b0\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u6210\u672c\u3002", "method": "\u4e09\u6b65\u9aa4\u7b97\u6cd5\uff1a1) \u8bc6\u522b\u6a21\u578b\u4e0d\u786e\u5b9a\u6837\u672c\uff08\u81ea\u6211\u610f\u8bc6\uff09\uff1b2) \u4ece\u4e0d\u786e\u5b9a\u6837\u672c\u751f\u6210\u7c7b\u4f3c\u793a\u4f8b\uff08\u81ea\u6570\u636e\u589e\u5f3a\uff09\uff1b3) \u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u65b0\u751f\u6210\u6837\u672c\u8fdb\u884c\u5fae\u8c03\uff08\u81ea\u6539\u8fdb\uff09\u3002\u7814\u7a76\u4e24\u79cd\u53d8\u4f53\uff1aTT-SI\uff08\u540c\u6a21\u578b\u81ea\u751f\u6210\uff09\u548cTT-D\uff08\u5f3a\u6a21\u578b\u751f\u6210\uff09\u3002", "result": "TT-SI\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u53475.48%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6807\u51c6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u4ec5\u4f7f\u75281/68\u7684\u8bad\u7ec3\u6837\u672c\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u81ea\u6539\u8fdb\u7b97\u6cd5\u5c55\u793a\u4e86\u6784\u5efa\u66f4\u5f3a\u5927\u667a\u80fd\u4f53\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u7684\u65b0\u8303\u5f0f\u6f5c\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07958", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07958", "abs": "https://arxiv.org/abs/2510.07958", "authors": ["Fengji Zhang", "Xinyao Niu", "Chengyang Ying", "Guancheng Lin", "Zhongkai Hao", "Zhou Fan", "Chengen Huang", "Jacky Keung", "Bei Chen", "Junyang Lin"], "title": "A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Reinforcement Learning\n(RL) have led to strong performance in open-domain question answering (QA).\nHowever, existing models still struggle with questions that admit multiple\nvalid answers. Standard QA benchmarks, which typically assume a single gold\nanswer, overlook this reality and thus produce inappropriate training signals.\nExisting attempts to handle ambiguity often rely on costly manual annotation,\nwhich is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.\nIn this paper, we present A$^2$Search, an annotation-free, end-to-end training\nframework to recognize and handle ambiguity. At its core is an automated\npipeline that detects ambiguous questions and gathers alternative answers via\ntrajectory sampling and evidence verification. The model is then optimized with\nRL using a carefully designed $\\mathrm{AnsF1}$ reward, which naturally\naccommodates multiple answers. Experiments on eight open-domain QA benchmarks\ndemonstrate that A$^2$Search achieves new state-of-the-art performance. With\nonly a single rollout, A$^2$Search-7B yields an average $\\mathrm{AnsF1}@1$\nscore of $48.4\\%$ across four multi-hop benchmarks, outperforming all strong\nbaselines, including the substantially larger ReSearch-32B ($46.2\\%$).\nExtensive analyses further show that A$^2$Search resolves ambiguity and\ngeneralizes across benchmarks, highlighting that embracing ambiguity is\nessential for building more reliable QA systems. Our code, data, and model\nweights can be found at https://github.com/zfj1998/A2Search", "AI": {"tldr": "A\u00b2Search\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u91c7\u6837\u548c\u8bc1\u636e\u9a8c\u8bc1\u81ea\u52a8\u68c0\u6d4b\u6a21\u7cca\u95ee\u9898\u5e76\u6536\u96c6\u66ff\u4ee3\u7b54\u6848\uff0c\u4f7f\u7528AnsF1\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u57288\u4e2a\u5f00\u653e\u57dfQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u548cRL\u6a21\u578b\u5728\u5904\u7406\u5141\u8bb8\u591a\u4e2a\u6709\u6548\u7b54\u6848\u7684\u6a21\u7cca\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u6807\u51c6QA\u57fa\u51c6\u5047\u8bbe\u5355\u4e00\u9ec4\u91d1\u7b54\u6848\uff0c\u4ea7\u751f\u4e0d\u9002\u5f53\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u800c\u73b0\u6709\u5904\u7406\u6a21\u7cca\u6027\u7684\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u591a\u8df3\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u81ea\u52a8\u5316\u7ba1\u9053\u68c0\u6d4b\u6a21\u7cca\u95ee\u9898\u5e76\u901a\u8fc7\u8f68\u8ff9\u91c7\u6837\u548c\u8bc1\u636e\u9a8c\u8bc1\u6536\u96c6\u66ff\u4ee3\u7b54\u6848\uff0c\u7136\u540e\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684AnsF1\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u8be5\u5956\u52b1\u81ea\u7136\u9002\u5e94\u591a\u4e2a\u7b54\u6848\u3002", "result": "\u57288\u4e2a\u5f00\u653e\u57dfQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0cA\u00b2Search-7B\u5728\u56db\u4e2a\u591a\u8df3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747AnsF1@1\u5f97\u520648.4%\uff0c\u4f18\u4e8e\u5305\u62ec\u66f4\u5927\u7684ReSearch-32B\uff0846.2%\uff09\u5728\u5185\u7684\u6240\u6709\u5f3a\u57fa\u7ebf\u3002", "conclusion": "A\u00b2Search\u89e3\u51b3\u4e86\u6a21\u7cca\u6027\u95ee\u9898\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6cdb\u5316\u826f\u597d\uff0c\u8868\u660e\u63a5\u53d7\u6a21\u7cca\u6027\u5bf9\u4e8e\u6784\u5efa\u66f4\u53ef\u9760\u7684QA\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08383", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08383", "abs": "https://arxiv.org/abs/2510.08383", "authors": ["Yi Jiang", "Lei Shen", "Lujie Niu", "Sendong Zhao", "Wenbo Su", "Bo Zheng"], "title": "QAgent: A modular Search Agent with Interactive Query Understanding", "comment": "Code is available at https://github.com/OpenStellarTeam/QAgent", "summary": "Large language models (LLMs) excel at natural language tasks but are limited\nby their static parametric knowledge, especially in knowledge-intensive task.\nRetrieval-augmented generation (RAG) mitigates this by integrating external\ninformation. However, (1) traditional RAG struggles with complex query\nunderstanding, and (2) even search agents trained with reinforcement learning\n(RL), despite their promise, still face generalization and deployment\nchallenges. To address these limitations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent for adaptive retrieval. This agent\noptimizes its understanding of the query through interactive reasoning and\nretrieval. To facilitate real-world application, we focus on modular search\nagent for query understanding that are plug-and-play in complex systems.\nSecifically, the agent follows a multi-step decision process trained with RL to\nmaximize retrieval quality and support accurate downstream answers. We further\nanalyze the strengths and weaknesses of end-to-end RL and propose a strategy\nthat focuses on effective retrieval, thereby enhancing generalization in LLM\napplications. Experiments show QAgent excels at QA and serves as a\nplug-and-play module for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86QAgent\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u4ee3\u7406\u5f0fRAG\u6846\u67b6\uff0c\u901a\u8fc7\u641c\u7d22\u4ee3\u7406\u8fdb\u884c\u81ea\u9002\u5e94\u68c0\u7d22\uff0c\u4f18\u5316\u67e5\u8be2\u7406\u89e3\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u63a8\u7406\u548c\u68c0\u7d22\u63d0\u9ad8\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfRAG\u5728\u590d\u6742\u67e5\u8be2\u7406\u89e3\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5373\u4f7f\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u641c\u7d22\u4ee3\u7406\u4e5f\u9762\u4e34\u6cdb\u5316\u548c\u90e8\u7f72\u6311\u6218\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u641c\u7d22\u4ee3\u7406\u8fdb\u884c\u67e5\u8be2\u7406\u89e3\uff0c\u901a\u8fc7\u591a\u6b65\u51b3\u7b56\u8fc7\u7a0b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u6700\u5927\u5316\u68c0\u7d22\u8d28\u91cf\u5e76\u652f\u6301\u51c6\u786e\u7684\u4e0b\u6e38\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660eQAgent\u5728\u95ee\u7b54\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u8fdb\u884c\u5b9e\u9645\u90e8\u7f72\u3002", "conclusion": "QAgent\u901a\u8fc7\u4e13\u6ce8\u4e8e\u6709\u6548\u68c0\u7d22\u7684\u7b56\u7565\uff0c\u589e\u5f3a\u4e86LLM\u5e94\u7528\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u548c\u5f3a\u5316\u5b66\u4e60\u641c\u7d22\u4ee3\u7406\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07962", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07962", "abs": "https://arxiv.org/abs/2510.07962", "authors": ["Jingyuan Wang", "Yankai Chen", "Zhonghang Li", "Chao Huang"], "title": "LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nreasoning, often through supervised fine-tuning (SFT). However, SFT is\nresource-intensive, relying on large curated datasets, rejection-sampled\ndemonstrations, and uniform optimization across all tokens, even though only a\nfraction carry meaningful learning value. In this work, we explore a\ncounterintuitive idea: can smaller language models (SLMs) teach larger language\nmodels (LLMs) by revealing high-value reasoning moments that reflect the\nlatter's unique strength? We propose LightReasoner, a novel framework that\nleverages the behavioral divergence between a stronger expert model (LLM) and a\nweaker amateur model (SLM). LightReasoner operates in two stages: (1) a\nsampling stage that pinpoints critical reasoning moments and constructs\nsupervision examples capturing the expert's advantage through expert-amateur\ncontrast, and (2) a fine-tuning stage that aligns the expert model with these\ndistilled examples, amplifying its reasoning strengths. Across seven\nmathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while\nreducing time consumption by 90%, sampled problems by 80%, and tuned token\nusage by 99%, all without relying on ground-truth labels. By turning weaker\nSLMs into effective teaching signals, LightReasoner offers a scalable and\nresource-efficient approach for advancing LLM reasoning. Code is available at:\nhttps://github.com/HKUDS/LightReasoner", "AI": {"tldr": "LightReasoner\u662f\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLM)\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u5173\u952e\u63a8\u7406\u65f6\u523b\uff0c\u901a\u8fc7\u4e13\u5bb6-\u4e1a\u4f59\u5bf9\u6bd4\u6784\u5efa\u76d1\u7763\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03(SFT)\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\uff0c\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5747\u5300\u4f18\u5316\u6240\u6709token\uff0c\u4f46\u53ea\u6709\u5c11\u6570token\u5177\u6709\u5b66\u4e60\u4ef7\u503c\u3002\u672c\u6587\u63a2\u7d22\u5c0f\u578b\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u63ed\u793a\u9ad8\u4ef7\u503c\u63a8\u7406\u65f6\u523b\u6765\u6559\u5bfc\u5927\u578b\u6a21\u578b\u3002", "method": "LightReasoner\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1)\u91c7\u6837\u9636\u6bb5\u901a\u8fc7\u4e13\u5bb6(LLM)\u4e0e\u4e1a\u4f59(SLM)\u7684\u884c\u4e3a\u5dee\u5f02\u8bc6\u522b\u5173\u952e\u63a8\u7406\u65f6\u523b\u5e76\u6784\u5efa\u5bf9\u6bd4\u76d1\u7763\u6837\u672c\uff1b2)\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u8fd9\u4e9b\u84b8\u998f\u6837\u672c\u5bf9\u9f50\u4e13\u5bb6\u6a21\u578b\uff0c\u589e\u5f3a\u5176\u63a8\u7406\u4f18\u52bf\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLightReasoner\u5c06\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe28.1%\uff0c\u540c\u65f6\u51cf\u5c1190%\u7684\u65f6\u95f4\u6d88\u8017\u300180%\u7684\u91c7\u6837\u95ee\u9898\u548c99%\u7684\u5fae\u8c03token\u4f7f\u7528\uff0c\u4e14\u4e0d\u4f9d\u8d56\u771f\u5b9e\u6807\u7b7e\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8f83\u5f31\u7684\u5c0f\u578b\u6a21\u578b\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u6559\u5b66\u4fe1\u53f7\uff0cLightReasoner\u4e3a\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.07974", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07974", "abs": "https://arxiv.org/abs/2510.07974", "authors": ["Jialu Du", "Guiyang Hou", "Yihui Fu", "Chen Wu", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu"], "title": "Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning", "comment": "15 pages, 10 figures", "summary": "While large language models (LLMs) excel in mathematical and code reasoning,\nwe observe they struggle with social reasoning tasks, exhibiting cognitive\nconfusion, logical inconsistencies, and conflation between objective world\nstates and subjective belief states. Through deteiled analysis of DeepSeek-R1's\nreasoning trajectories, we find that LLMs frequently encounter reasoning\nimpasses and tend to output contradictory terms like \"tricky\" and \"confused\"\nwhen processing scenarios with multiple participants and timelines, leading to\nerroneous reasoning or infinite loops. The core issue is their inability to\ndisentangle objective reality from agents' subjective beliefs. To address this,\nwe propose an adaptive world model-enhanced reasoning mechanism that constructs\na dynamic textual world model to track entity states and temporal sequences. It\ndynamically monitors reasoning trajectories for confusion indicators and\npromptly intervenes by providing clear world state descriptions, helping models\nnavigate through cognitive dilemmas. The mechanism mimics how humans use\nimplicit world models to distinguish between external events and internal\nbeliefs. Evaluations on three social benchmarks demonstrate significant\nimprovements in accuracy (e.g., +10% in Hi-ToM) while reducing computational\ncosts (up to 33.8% token reduction), offering a simple yet effective solution\nfor deploying LLMs in social contexts.", "AI": {"tldr": "LLMs\u5728\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u65e0\u6cd5\u533a\u5206\u5ba2\u89c2\u4e16\u754c\u72b6\u6001\u548c\u4e3b\u89c2\u4fe1\u5ff5\u72b6\u6001\u3002\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u4e16\u754c\u6a21\u578b\u589e\u5f3a\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u6784\u5efa\u52a8\u6001\u6587\u672c\u4e16\u754c\u6a21\u578b\u6765\u8ddf\u8e2a\u5b9e\u4f53\u72b6\u6001\u548c\u65f6\u95f4\u5e8f\u5217\uff0c\u663e\u8457\u63d0\u5347\u793e\u4ea4\u63a8\u7406\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u548c\u4ee3\u7801\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7ecf\u5e38\u51fa\u73b0\u8ba4\u77e5\u6df7\u4e71\u3001\u903b\u8f91\u4e0d\u4e00\u81f4\u4ee5\u53ca\u6df7\u6dc6\u5ba2\u89c2\u4e16\u754c\u72b6\u6001\u4e0e\u4e3b\u89c2\u4fe1\u5ff5\u72b6\u6001\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u4e16\u754c\u6a21\u578b\u589e\u5f3a\u63a8\u7406\u673a\u5236\uff0c\u6784\u5efa\u52a8\u6001\u6587\u672c\u4e16\u754c\u6a21\u578b\u6765\u8ddf\u8e2a\u5b9e\u4f53\u72b6\u6001\u548c\u65f6\u95f4\u5e8f\u5217\uff0c\u52a8\u6001\u76d1\u63a7\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u56f0\u60d1\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u63d0\u4f9b\u6e05\u6670\u7684\u4e16\u754c\u72b6\u6001\u63cf\u8ff0\u6765\u53ca\u65f6\u5e72\u9884\u3002", "result": "\u5728\u4e09\u4e2a\u793e\u4ea4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u51c6\u786e\u7387\u63d0\u5347\uff08\u4f8b\u5982\u5728Hi-ToM\u4e2d\u63d0\u534710%\uff09\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff08\u6700\u591a\u51cf\u5c1133.8%\u7684token\u4f7f\u7528\uff09\u3002", "conclusion": "\u8be5\u673a\u5236\u4e3a\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u90e8\u7f72LLMs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u4eff\u4e86\u4eba\u7c7b\u4f7f\u7528\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u6765\u533a\u5206\u5916\u90e8\u4e8b\u4ef6\u548c\u5185\u90e8\u4fe1\u5ff5\u7684\u65b9\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "2510.08511", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08511", "abs": "https://arxiv.org/abs/2510.08511", "authors": ["Shangheng Du", "Xiangchao Yan", "Dengyang Jiang", "Jiakang Yuan", "Yusong Hu", "Xin Li", "Liang He", "Bo Zhang", "Lei Bai"], "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents", "comment": null, "summary": "Large language models (LLMs) have shown impressive performance in general\nprogramming tasks. However, in Machine Learning Engineering (MLE) scenarios\nsuch as AutoML and Kaggle competitions, achieving high performance depends\nheavily on expert intervention and repeated adjustments rather than simply\ngenerating correct code. When applied directly to these tasks, LLMs often lack\nfine-grained domain priors, and existing MLE approaches that use linear or\ntree-structured searches limit knowledge transfer to adjacent hierarchical\nlinks. As a result, they cannot leverage past full trajectories or share\ninformation across branches, limiting self-evolving ability and search space\ndiversity. To address these limitations, we introduce AutoMLGen, an LLM-based\ncoding agent that integrates a domain knowledge base for high-quality prior\nguidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS\nretains the tree-guided exploration of MCTS while embedding a graph structure\ninto the expansion stage to enable dynamic path reorganization, historical\ntrajectory reuse, and multi-solution fusion to support both self-evolution and\ncollaborative learning. Combined with fine-grained operator sets, this design\nimproves stability and accelerates convergence. Evaluation on the MLE-Bench\nshows that AutoMLGen achieves state-of-the-art performance in numerous\ndimensions, such as the average medal rate and the valid submission rate, under\na 12-hour budget (half the standard runtime). The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent.", "AI": {"tldr": "AutoMLGen\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u4ee3\u7406\uff0c\u901a\u8fc7\u96c6\u6210\u9886\u57df\u77e5\u8bc6\u5e93\u548c\u8499\u7279\u5361\u6d1b\u56fe\u641c\u7d22(MCGS)\u6765\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5728MLE-Bench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5728AutoML\u548cKaggle\u7ade\u8d5b\u7b49\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u573a\u666f\u4e2d\uff0cLLM\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u548c\u8de8\u5206\u652f\u4fe1\u606f\u5171\u4eab\uff0c\u9650\u5236\u4e86\u81ea\u8fdb\u5316\u80fd\u529b\u548c\u641c\u7d22\u7a7a\u95f4\u591a\u6837\u6027\u3002", "method": "\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u5e93\u63d0\u4f9b\u9ad8\u8d28\u91cf\u5148\u9a8c\u6307\u5bfc\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u56fe\u641c\u7d22(MCGS)\u5b9e\u73b0\u52a8\u6001\u8def\u5f84\u91cd\u7ec4\u3001\u5386\u53f2\u8f68\u8ff9\u91cd\u7528\u548c\u591a\u89e3\u51b3\u65b9\u6848\u878d\u5408\uff0c\u652f\u6301\u81ea\u8fdb\u5316\u548c\u534f\u4f5c\u5b66\u4e60\u3002", "result": "\u5728MLE-Bench\u8bc4\u4f30\u4e2d\uff0cAutoMLGen\u572812\u5c0f\u65f6\u9884\u7b97\uff08\u6807\u51c6\u8fd0\u884c\u65f6\u95f4\u7684\u4e00\u534a\uff09\u4e0b\uff0c\u5728\u5e73\u5747\u5956\u724c\u7387\u548c\u6709\u6548\u63d0\u4ea4\u7387\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "AutoMLGen\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u96c6\u6210\u548c\u56fe\u641c\u7d22\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0\u3002", "topic": "code agent"}}
{"id": "2510.08002", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08002", "abs": "https://arxiv.org/abs/2510.08002", "authors": ["Cheng Yang", "Xuemeng Yang", "Licheng Wen", "Daocheng Fu", "Jianbiao Mei", "Rong Wu", "Pinlong Cai", "Yufan Shen", "Nianchen Deng", "Botian Shi", "Yu Qiao", "Haifeng Li"], "title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks", "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse domains, yet significant challenges persist when deploying them as AI\nagents for real-world long-horizon tasks. Existing LLM agents suffer from a\ncritical limitation: they are test-time static and cannot learn from\nexperience, lacking the ability to accumulate knowledge and continuously\nimprove on the job. To address this challenge, we propose MUSE, a novel agent\nframework that introduces an experience-driven, self-evolving system centered\naround a hierarchical Memory Module. MUSE organizes diverse levels of\nexperience and leverages them to plan and execute long-horizon tasks across\nmultiple applications. After each sub-task execution, the agent autonomously\nreflects on its trajectory, converting the raw trajectory into structured\nexperience and integrating it back into the Memory Module. This mechanism\nenables the agent to evolve beyond its static pretrained parameters, fostering\ncontinuous learning and self-evolution. We evaluate MUSE on the long-horizon\nproductivity benchmark TAC. It achieves new SOTA performance by a significant\nmargin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments\ndemonstrate that as the agent autonomously accumulates experience, it exhibits\nincreasingly superior task completion capabilities, as well as robust\ncontinuous learning and self-evolution capabilities. Moreover, the accumulated\nexperience from MUSE exhibits strong generalization properties, enabling\nzero-shot improvement on new tasks. MUSE establishes a new paradigm for AI\nagents capable of real-world productivity task automation.", "AI": {"tldr": "MUSE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\u7684\u81ea\u4e3b\u8fdb\u5316AI\u4ee3\u7406\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u548c\u53cd\u601d\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u548c\u81ea\u6211\u8fdb\u5316\uff0c\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u6d4b\u8bd5\u65f6\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u7f3a\u4e4f\u77e5\u8bc6\u79ef\u7d2f\u548c\u6301\u7eed\u6539\u8fdb\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMUSE\u6846\u67b6\uff0c\u56f4\u7ed5\u5206\u5c42\u8bb0\u5fc6\u6a21\u5757\u6784\u5efa\u7ecf\u9a8c\u9a71\u52a8\u7684\u81ea\u8fdb\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u4efb\u52a1\u6267\u884c\u540e\u7684\u81ea\u4e3b\u53cd\u601d\u5c06\u539f\u59cb\u8f68\u8ff9\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7ecf\u9a8c\u5e76\u6574\u5408\u5230\u8bb0\u5fc6\u6a21\u5757\u4e2d\u3002", "result": "\u5728TAC\u751f\u4ea7\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u8f7b\u91cf\u7ea7Gemini-2.5 Flash\u6a21\u578b\u5c31\u5b9e\u73b0\u4e86\u663e\u8457\u7684SOTA\u6027\u80fd\uff0c\u968f\u7740\u7ecf\u9a8c\u79ef\u7d2f\uff0c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MUSE\u4e3a\u80fd\u591f\u5b9e\u73b0\u73b0\u5b9e\u4e16\u754c\u751f\u4ea7\u529b\u4efb\u52a1\u81ea\u52a8\u5316\u7684AI\u4ee3\u7406\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u7ecf\u9a8c\u9a71\u52a8\u7684\u81ea\u8fdb\u5316\u4ee3\u7406\u7684\u53ef\u884c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2510.08517", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08517", "abs": "https://arxiv.org/abs/2510.08517", "authors": ["Grace Liu", "Yuxiao Qu", "Jeff Schneider", "Aarti Singh", "Aviral Kumar"], "title": "CaRT: Teaching LLM Agents to Know When They Know Enough", "comment": null, "summary": "Many tasks require learned models to strategically gather relevant\ninformation over multiple rounds of interaction before actually acting on a\ntask. Strategic information gathering requires models to know not only how to\neffectively acquire information, but also when to stop gathering information\nand make a decision, in order to avoid overthinking or getting derailed when\nacting. In this paper, we formalize this problem and introduce Counterfactuals\nand Reasoning for Termination (CaRT), an approach for teaching LLMs when to\nstop seeking information. To appropriately learn when to terminate, CaRT\nfine-tunes LLMs using counterfactual pairs of trajectories, one where\ntermination is appropriate and a minimally modified version of the same\ntrajectory where it is not. It trains the LLM to explain the rationale for the\ntermination decision in either case via verbal reasoning, and imbues this\ncapability into the base LLM via fine-tuning. We instantiate CaRT in two\ndomains: interactive medical diagnosis and math problem solving. In both\ndomains, we find that CaRT improves the efficiency of information gathering and\ntask success rate compared to other fine-tuning methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CaRT\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u8f68\u8ff9\u5bf9\u548c\u8bed\u8a00\u63a8\u7406\u8bad\u7ec3LLMs\u5728\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u4e2d\u9002\u65f6\u7ec8\u6b62\uff0c\u63d0\u9ad8\u51b3\u7b56\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u8bb8\u591a\u4efb\u52a1\u9700\u8981\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7b56\u7565\u6027\u5730\u6536\u96c6\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u4f55\u65f6\u505c\u6b62\u4fe1\u606f\u6536\u96c6\u5e76\u505a\u51fa\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u5bb9\u6613\u8fc7\u5ea6\u601d\u8003\u6216\u504f\u79bb\u76ee\u6807\u3002", "method": "CaRT\u65b9\u6cd5\u4f7f\u7528\u53cd\u4e8b\u5b9e\u8f68\u8ff9\u5bf9\u8fdb\u884c\u5fae\u8c03\uff1a\u4e00\u4e2a\u8f68\u8ff9\u5728\u9002\u5f53\u65f6\u5019\u7ec8\u6b62\uff0c\u53e6\u4e00\u4e2a\u76f8\u540c\u8f68\u8ff9\u5728\u4e0d\u9002\u5f53\u65f6\u7ec8\u6b62\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u63a8\u7406\u8bad\u7ec3LLM\u89e3\u91ca\u7ec8\u6b62\u51b3\u7b56\u7684\u5408\u7406\u6027\u3002", "result": "\u5728\u4ea4\u4e92\u5f0f\u533b\u7597\u8bca\u65ad\u548c\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e24\u4e2a\u9886\u57df\uff0cCaRT\u76f8\u6bd4\u5176\u4ed6\u5fae\u8c03\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4fe1\u606f\u6536\u96c6\u6548\u7387\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "CaRT\u65b9\u6cd5\u80fd\u6709\u6548\u6559\u5bfcLLMs\u5728\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u4e2d\u9002\u65f6\u7ec8\u6b62\uff0c\u63d0\u9ad8\u51b3\u7b56\u6548\u7387\uff0c\u4e3a\u7b56\u7565\u6027\u4fe1\u606f\u6536\u96c6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agent analysis"}}
{"id": "2510.08521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08521", "abs": "https://arxiv.org/abs/2510.08521", "authors": ["Yusong Hu", "Runmin Ma", "Yue Fan", "Jinxin Shi", "Zongsheng Cao", "Yuhao Zhou", "Jiakang Yuan", "Xiangchao Yan", "Wenlong Zhang", "Lei Bai", "Bo Zhang"], "title": "FlowSearch: Advancing deep research with dynamic structured knowledge flow", "comment": null, "summary": "Deep research is an inherently challenging task that demands both breadth and\ndepth of thinking. It involves navigating diverse knowledge spaces and\nreasoning over complex, multi-step dependencies, which presents substantial\nchallenges for agentic systems. To address this, we propose FlowSearch, a\nmulti-agent framework that actively constructs and evolves a dynamic structured\nknowledge flow to drive subtask execution and reasoning. FlowSearch is capable\nof strategically planning and expanding the knowledge flow to enable parallel\nexploration and hierarchical task decomposition, while also adjusting the\nknowledge flow in real time based on feedback from intermediate reasoning\noutcomes and insights. FlowSearch achieves state-of-the-art performance on both\ngeneral and scientific benchmarks, including GAIA, HLE, GPQA and TRQA,\ndemonstrating its effectiveness in multi-disciplinary research scenarios and\nits potential to advance scientific discovery. The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent.", "AI": {"tldr": "FlowSearch\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u52a8\u6001\u7ed3\u6784\u5316\u77e5\u8bc6\u6d41\u6765\u9a71\u52a8\u5b50\u4efb\u52a1\u6267\u884c\u548c\u63a8\u7406\uff0c\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u573a\u666f\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u9700\u8981\u5e7f\u5ea6\u548c\u6df1\u5ea6\u7684\u601d\u8003\uff0c\u6d89\u53ca\u5bfc\u822a\u591a\u6837\u5316\u77e5\u8bc6\u7a7a\u95f4\u548c\u63a8\u7406\u590d\u6742\u591a\u6b65\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd9\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "FlowSearch\u901a\u8fc7\u6218\u7565\u6027\u5730\u89c4\u5212\u548c\u6269\u5c55\u77e5\u8bc6\u6d41\u6765\u5b9e\u73b0\u5e76\u884c\u63a2\u7d22\u548c\u5206\u5c42\u4efb\u52a1\u5206\u89e3\uff0c\u540c\u65f6\u6839\u636e\u4e2d\u95f4\u63a8\u7406\u7ed3\u679c\u548c\u6d1e\u5bdf\u5b9e\u65f6\u8c03\u6574\u77e5\u8bc6\u6d41\u3002", "result": "FlowSearch\u5728\u901a\u7528\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08GAIA\u3001HLE\u3001GPQA\u548cTRQA\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FlowSearch\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u63a8\u8fdb\u79d1\u5b66\u53d1\u73b0\u7684\u6f5c\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.08558", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08558", "abs": "https://arxiv.org/abs/2510.08558", "authors": ["Kai Zhang", "Xiangchao Chen", "Bo Liu", "Tianci Xue", "Zeyi Liao", "Zhihan Liu", "Xiyao Wang", "Yuting Ning", "Zhaorun Chen", "Xiaohan Fu", "Jian Xie", "Yuxuan Sun", "Boyu Gou", "Qi Qi", "Zihang Meng", "Jianwei Yang", "Ning Zhang", "Xian Li", "Ashish Shah", "Dat Huynh", "Hengduo Li", "Zi Yang", "Sara Cao", "Lawrence Jang", "Shuyan Zhou", "Jiacheng Zhu", "Huan Sun", "Jason Weston", "Yu Su", "Yifan Wu"], "title": "Agent Learning via Early Experience", "comment": "Work in progress", "summary": "A long-term goal of language agents is to learn and improve through their own\nexperience, ultimately outperforming humans in complex, real-world tasks.\nHowever, training agents from experience data with reinforcement learning\nremains difficult in many environments, which either lack verifiable rewards\n(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn\ntool use). As a result, most current agents rely on supervised fine-tuning on\nexpert data, which is challenging to scale and generalizes poorly. This\nlimitation stems from the nature of expert demonstrations: they capture only a\nnarrow range of scenarios and expose the agent to limited environment\ndiversity. We address this limitation with a middle-ground paradigm we call\nearly experience: interaction data generated by the agent's own actions, where\nthe resulting future states serve as supervision without reward signals. Within\nthis paradigm we study two strategies of using such data: (1) Implicit world\nmodeling, which uses collected states to ground the policy in environment\ndynamics; and (2) Self-reflection, where the agent learns from its suboptimal\nactions to improve reasoning and decision-making. We evaluate across eight\ndiverse environments and multiple model families. Our approaches consistently\nimprove effectiveness and out-of-domain generalization, highlighting the value\nof early experience. Moreover, in environments with verifiable rewards, our\nresults provide promising signals that early experience offers a strong\nfoundation for subsequent reinforcement learning, positioning it as a practical\nbridge between imitation learning and fully experience-driven agents.", "AI": {"tldr": "\u63d0\u51fa\"\u65e9\u671f\u7ecf\u9a8c\"\u8303\u5f0f\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u81ea\u8eab\u4ea4\u4e92\u6570\u636e\u6765\u6539\u8fdb\u8bed\u8a00\u667a\u80fd\u4f53\uff0c\u65e0\u9700\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u516b\u4e2a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u63d0\u5347\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u667a\u80fd\u4f53\u4e3b\u8981\u4f9d\u8d56\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u4f46\u4e13\u5bb6\u6570\u636e\u8303\u56f4\u6709\u9650\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5f3a\u5316\u5b66\u4e60\u5728\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u5956\u52b1\u6216\u9700\u8981\u957f\u5e8f\u5217\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u65e9\u671f\u7ecf\u9a8c\u8303\u5f0f\uff0c\u4f7f\u7528\u667a\u80fd\u4f53\u81ea\u8eab\u4ea4\u4e92\u6570\u636e\uff0c\u7814\u7a76\u4e24\u79cd\u7b56\u7565\uff1a\u9690\u5f0f\u4e16\u754c\u5efa\u6a21\uff08\u5229\u7528\u6536\u96c6\u7684\u72b6\u6001\u7406\u89e3\u73af\u5883\u52a8\u6001\uff09\u548c\u81ea\u6211\u53cd\u601d\uff08\u4ece\u6b21\u4f18\u884c\u52a8\u4e2d\u5b66\u4e60\u6539\u8fdb\u51b3\u7b56\uff09\u3002", "result": "\u5728\u516b\u4e2a\u591a\u6837\u5316\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u667a\u80fd\u4f53\u6548\u80fd\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002\u5728\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u73af\u5883\u4e2d\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u826f\u597d\u57fa\u7840\u3002", "conclusion": "\u65e9\u671f\u7ecf\u9a8c\u662f\u6a21\u4eff\u5b66\u4e60\u4e0e\u5b8c\u5168\u7ecf\u9a8c\u9a71\u52a8\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5b9e\u7528\u6865\u6881\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u5f53\u524d\u667a\u80fd\u4f53\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08049", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08049", "abs": "https://arxiv.org/abs/2510.08049", "authors": ["Congming Zheng", "Jiachen Zhu", "Zhuoying Ou", "Yuxiang Chen", "Kangning Zhang", "Rong Shan", "Zeyu Zheng", "Mengyue Yang", "Jianghao Lin", "Yong Yu", "Weinan Zhang"], "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models", "comment": null, "summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability,\nconventional alignment remains largely dominated by outcome reward models\n(ORMs) that judge only final answers. Process Reward Models(PRMs) address this\ngap by evaluating and guiding reasoning at the step or trajectory level. This\nsurvey provides a systematic overview of PRMs through the full loop: how to\ngenerate process data, build PRMs, and use PRMs for test-time scaling and\nreinforcement learning. We summarize applications across math, code, text,\nmultimodal reasoning, robotics, and agents, and review emerging benchmarks. Our\ngoal is to clarify design spaces, reveal open challenges, and guide future\nresearch toward fine-grained, robust reasoning alignment.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u5305\u62ec\u8fc7\u7a0b\u6570\u636e\u751f\u6210\u3001PRM\u6784\u5efa\u4ee5\u53caPRM\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u63a8\u52a8\u7ec6\u7c92\u5ea6\u63a8\u7406\u5bf9\u9f50\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ed3\u679c\u5956\u52b1\u6a21\u578b(ORMs)\u4ec5\u8bc4\u5224\u6700\u7ec8\u7b54\u6848\uff0c\u800c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u901a\u8fc7\u5728\u6b65\u9aa4\u6216\u8f68\u8ff9\u5c42\u9762\u8bc4\u4f30\u548c\u5f15\u5bfc\u63a8\u7406\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u5206\u6790PRMs\u7684\u5b8c\u6574\u6d41\u7a0b\uff1a\u8fc7\u7a0b\u6570\u636e\u751f\u6210\u3001PRM\u6784\u5efa\u65b9\u6cd5\u3001PRM\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86PRMs\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u6587\u672c\u3001\u591a\u6a21\u6001\u63a8\u7406\u3001\u673a\u5668\u4eba\u548c\u667a\u80fd\u4f53\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u56de\u987e\u4e86\u65b0\u5174\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u660e\u786e\u4e86\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7ec6\u7c92\u5ea6\u3001\u9c81\u68d2\u63a8\u7406\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2510.08564", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08564", "abs": "https://arxiv.org/abs/2510.08564", "authors": ["Zhen Zhu", "Yiming Gong", "Yao Xiao", "Yaoyao Liu", "Derek Hoiem"], "title": "How to Teach Large Multimodal Models New Skills", "comment": "In submission. Code is available at\n  https://github.com/jessemelpolio/LMM_CL", "summary": "How can we teach large multimodal models (LMMs) new skills without erasing\nprior abilities? We study sequential fine-tuning on five target skills while\nmonitoring general ability on eight held-out benchmarks across three model\nfamilies. We observe that apparent \"forgetting\" on held-out tasks after narrow\nfine-tuning can partly recover at later stages. We trace this behavior to a\nmeasurable shift in the output token distribution, manifested through a simple\ncounting-bias probe that co-varies with forgetting. Guided by this picture, we\nidentify two simple, robust tuning recipes that learn strongly while limiting\ndrift: (i) updating only the self-attention projection layers, and (ii)\nupdating only the MLP Gate&Up while freezing the Down projection. Across models\nand tasks, these choices deliver strong target gains while largely preserving\nheld-out performance. Code is available at\nhttps://github.com/jessemelpolio/LMM_CL", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u987a\u5e8f\u5fae\u8c03\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u53d1\u73b0\u90e8\u5206\u9057\u5fd8\u53ef\u5728\u540e\u671f\u6062\u590d\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u539f\u6709\u80fd\u529b\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u987a\u5e8f\u5fae\u8c03\u6559\u6388\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u65b0\u6280\u80fd\uff0c\u89e3\u51b3\u6a21\u578b\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u5728\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u4e94\u4e2a\u76ee\u6807\u6280\u80fd\u7684\u987a\u5e8f\u5fae\u8c03\uff0c\u540c\u65f6\u76d1\u63a7\u516b\u4e2a\u4fdd\u7559\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u53d8\u5316\uff0c\u901a\u8fc7\u8f93\u51fa\u4ee4\u724c\u5206\u5e03\u5206\u6790\u9057\u5fd8\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u5fae\u8c03\u65b9\u6cd5\uff1a\u4ec5\u66f4\u65b0\u81ea\u6ce8\u610f\u529b\u6295\u5f71\u5c42\u6216\u4ec5\u66f4\u65b0MLP Gate&Up\u5c42\u3002", "result": "\u53d1\u73b0\u7a84\u5fae\u8c03\u540e\u7684\u660e\u663e\u9057\u5fd8\u53ef\u5728\u540e\u671f\u90e8\u5206\u6062\u590d\uff0c\u8f93\u51fa\u4ee4\u724c\u5206\u5e03\u7684\u53d8\u5316\u4e0e\u9057\u5fd8\u76f8\u5173\uff0c\u63d0\u51fa\u7684\u4e24\u79cd\u5fae\u8c03\u65b9\u6cd5\u5728\u83b7\u5f97\u5f3a\u76ee\u6807\u6280\u80fd\u7684\u540c\u65f6\u80fd\u6709\u6548\u4fdd\u7559\u539f\u6709\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u7279\u5b9a\u5c42\uff0c\u53ef\u4ee5\u5728\u6559\u6388\u65b0\u6280\u80fd\u7684\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6a21\u578b\u539f\u6709\u80fd\u529b\u7684\u635f\u5bb3\uff0c\u4e3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2510.08163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08163", "abs": "https://arxiv.org/abs/2510.08163", "authors": ["Jian Xie", "Zhendong Chu", "Aoxiao Zhong", "Kai Zhang", "Mingzhe Han", "Xin Fang", "Jialie Shen", "Qingsong Wen"], "title": "ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code", "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) often suffer from the ``over-thinking''\nproblem, generating unnecessarily long reasoning on simple tasks. Some\nstrategies have been proposed to mitigate this issue, such as length penalties\nor routing mechanisms, but they are typically heuristic and task-specific,\nlacking a general framework for adaptive reasoning. In this paper, we present\nARM2, a unified model that adaptively balances reasoning performance and\nefficiency across multiple formats through a reinforcement learning framework\naugmented with length-aware optimization. Beyond conventional natural language\ninference, ARM2 integrates vision understanding, extending its applicability to\nmultimodal. Moreover, ARM2 integrates executable code into reasoning, enabling\nsubstantial reductions in token cost while preserving task performance compared\nto long CoT. Experiments demonstrate that ARM2 achieves performance on par with\ntraditional reasoning models trained with GRPO, while reducing token usage by\nover 70% on average. We further conduct extensive analyses to validate the\neffectiveness of ARM2 and the soundness of its design.", "AI": {"tldr": "ARM2\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u957f\u5ea6\uff0c\u5e73\u5747\u964d\u4f4e70%\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\"\u8fc7\u5ea6\u601d\u8003\"\u95ee\u9898\uff0c\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u751f\u6210\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u63a8\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u957f\u5ea6\u60e9\u7f5a\u6216\u8def\u7531\u673a\u5236\u901a\u5e38\u662f\u542f\u53d1\u5f0f\u4e14\u4efb\u52a1\u7279\u5b9a\u7684\uff0c\u7f3a\u4e4f\u901a\u7528\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u957f\u5ea6\u611f\u77e5\u4f18\u5316\uff0c\u7edf\u4e00\u5e73\u8861\u591a\u79cd\u683c\u5f0f\u7684\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387\u3002\u96c6\u6210\u89c6\u89c9\u7406\u89e3\u548c\u53ef\u6267\u884c\u4ee3\u7801\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002", "result": "ARM2\u5728\u4fdd\u6301\u4e0e\u4f20\u7edfGRPO\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5e73\u5747\u51cf\u5c1170%\u7684token\u4f7f\u7528\u91cf\u3002", "conclusion": "ARM2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08191", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08191", "abs": "https://arxiv.org/abs/2510.08191", "authors": ["Yuzheng Cai", "Siqi Cai", "Yuchen Shi", "Zihan Xu", "Lichao Chen", "Yulei Qin", "Xiaoyu Tan", "Gang Li", "Zongyi Li", "Haojia Lin", "Yong Mao", "Ke Li", "Xing Sun"], "title": "Training-Free Group Relative Policy Optimization", "comment": null, "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that uses\nSupervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase\nwith Group Relative Policy Optimization (GRPO) to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as a token prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages the group relative semantic advantage\ninstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge during multi-epoch learning on a minimal\nground-truth data. Such knowledge serves as the learned token prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\non mathematical reasoning and web searching tasks demonstrate that\nTraining-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,\nTraining-Free GRPO outperforms fine-tuned small LLMs with marginal training\ndata and cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86Training-Free GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ecf\u9a8c\u77e5\u8bc6\u4f5c\u4e3atoken\u5148\u9a8c\u6765\u589e\u5f3aLLM\u4ee3\u7406\u6027\u80fd\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u7f51\u7edc\u641c\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u4ee3\u7406\u5728\u4e13\u4e1a\u9886\u57df\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u53c2\u6570\u66f4\u65b0\u548c\u5bb9\u6613\u8fc7\u62df\u5408\u7684\u7f3a\u70b9\u3002", "method": "\u4f7f\u7528\u8bad\u7ec3\u514d\u8d39\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5229\u7528\u7ec4\u5185rollouts\u7684\u8bed\u4e49\u4f18\u52bf\u800c\u975e\u6570\u503c\u4f18\u52bf\uff0c\u5728\u591a\u8f6e\u5b66\u4e60\u4e2d\u8fed\u4ee3\u63d0\u70bc\u9ad8\u8d28\u91cf\u7ecf\u9a8c\u77e5\u8bc6\u4f5c\u4e3atoken\u5148\u9a8c\u3002", "result": "\u5728DeepSeek-V3.1-Terminus\u4e0a\u5e94\u7528\uff0c\u4ec5\u9700\u51e0\u5341\u4e2a\u8bad\u7ec3\u6837\u672c\u5c31\u80fd\u8d85\u8d8a\u5fae\u8c03\u7684\u5c0f\u578bLLM\uff0c\u663e\u8457\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "conclusion": "Training-Free GRPO\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u8fc7\u62df\u5408\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08141", "abs": "https://arxiv.org/abs/2510.08141", "authors": ["Chen Wang", "Zhaochun Li", "Jionghao Bai", "Yuzhi Zhang", "Shisheng Cui", "Zhou Zhao", "Yue Wang"], "title": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Finetuning", "comment": null, "summary": "Reinforcement finetuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers.", "AI": {"tldr": "\u63d0\u51fa\u4e86AEPO\u65b9\u6cd5\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u6e29\u5ea6\u8c03\u8282\u5b9e\u73b0\u7cbe\u786e\u7684\u71b5\u63a7\u5236\uff0c\u63ed\u793a\u4e86\u71b5\u4e0e\u6027\u80fd\u7684\u975e\u5355\u8c03\u5173\u7cfb", "motivation": "\u73b0\u6709\u7684GRPO\u65b9\u6cd5\u5b58\u5728\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u5bfc\u81f4\u63a2\u7d22\u6d88\u5931\u548c\u7b56\u7565\u8fc7\u65e9\u6536\u655b\uff0c\u800c\u73b0\u6709\u7684\u71b5\u6b63\u5219\u5316\u65b9\u6cd5\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u95ee\u9898\u4e14\u5f15\u5165\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\u6027", "method": "\u63d0\u51faAEPO\u65b9\u6cd5\uff0c\u7528\u6e29\u5ea6\u8c03\u6574\u5206\u5e03\u7684REINFORCE\u7b56\u7565\u68af\u5ea6\u66ff\u4ee3\u71b5\u5956\u52b1\uff0c\u901a\u8fc7\u6e29\u5ea6\u8c03\u8282\u7a33\u5b9a\u71b5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u7b56\u7565\u68af\u5ea6\u6b63\u5219\u5316\u3001\u5206\u5e03\u6b63\u5219\u5316\u548cREINFORCE\u6b63\u5219\u5316", "result": "AEPO\u80fd\u591f\u7a33\u5b9a\u71b5\u5728\u4efb\u610f\u76ee\u6807\u6c34\u5e73\uff0c\u6709\u6548\u6d88\u9664GRPO\u4e2d\u7684\u71b5\u5d29\u6e83\uff1b\u63ed\u793a\u4e86\u71b5\u4e0e\u6027\u80fd\u7684\u975e\u5355\u8c03\u5173\u7cfb\uff1b\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684RFT\u8303\u5f0f", "conclusion": "AEPO\u89e3\u51b3\u4e86\u71b5\u63a7\u5236\u95ee\u9898\uff0c\u9610\u660e\u4e86\u71b5\u3001\u63a2\u7d22\u548c\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u6846\u67b6", "topic": "agentic reinforcement learning"}}
{"id": "2510.08211", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.08211", "abs": "https://arxiv.org/abs/2510.08211", "authors": ["XuHao Hu", "Peng Wang", "Xiaoya Lu", "Dongrui Liu", "Xuanjing Huang", "Jing Shao"], "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions", "comment": null, "summary": "Previous research has shown that LLMs finetuned on malicious or incorrect\ncompletions within narrow domains (e.g., insecure code or incorrect medical\nadvice) can become broadly misaligned to exhibit harmful behaviors, which is\ncalled emergent misalignment. In this work, we investigate whether this\nphenomenon can extend beyond safety behaviors to a broader spectrum of\ndishonesty and deception under high-stakes scenarios (e.g., lying under\npressure and deceptive behavior). To explore this, we finetune open-sourced\nLLMs on misaligned completions across diverse domains. Experimental results\ndemonstrate that LLMs show broadly misaligned behavior in dishonesty.\nAdditionally, we further explore this phenomenon in a downstream combined\nfinetuning setting, and find that introducing as little as 1% of misalignment\ndata into a standard downstream task is sufficient to decrease honest behavior\nover 20%. Furthermore, we consider a more practical human-AI interaction\nenvironment where we simulate both benign and biased users to interact with the\nassistant LLM. Notably, we find that the assistant can be misaligned\nunintentionally to exacerbate its dishonesty with only 10% biased user\npopulation. In summary, we extend the study of emergent misalignment to the\ndomain of dishonesty and deception under high-stakes scenarios, and demonstrate\nthat this risk arises not only through direct finetuning, but also in\ndownstream mixture tasks and practical human-AI interactions.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u7279\u5b9a\u9886\u57df\u5fae\u8c03LLMs\u53ef\u80fd\u5bfc\u81f4\u5e7f\u6cdb\u7684\u4e0d\u8bda\u5b9e\u884c\u4e3a\uff0c\u8fd9\u79cd\u73b0\u8c61\u79f0\u4e3a\u6d8c\u73b0\u6027\u5931\u8c03\u3002\u5373\u4f7f\u53ea\u5f15\u51651%\u7684\u5931\u8c03\u6570\u636e\uff0c\u8bda\u5b9e\u884c\u4e3a\u4e5f\u4f1a\u4e0b\u964d20%\u4ee5\u4e0a\uff0c\u572810%\u504f\u89c1\u7528\u6237\u73af\u5883\u4e0b\u4e5f\u4f1a\u65e0\u610f\u4e2d\u52a0\u5267\u4e0d\u8bda\u5b9e\u3002", "motivation": "\u63a2\u7d22\u6d8c\u73b0\u6027\u5931\u8c03\u73b0\u8c61\u662f\u5426\u4e0d\u4ec5\u9650\u4e8e\u5b89\u5168\u884c\u4e3a\uff0c\u8fd8\u80fd\u6269\u5c55\u5230\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u4e0d\u8bda\u5b9e\u548c\u6b3a\u9a97\u884c\u4e3a\u3002", "method": "\u5728\u591a\u6837\u5316\u9886\u57df\u5bf9\u5f00\u6e90LLMs\u8fdb\u884c\u5931\u8c03\u8865\u5168\u5fae\u8c03\uff0c\u5e76\u5728\u4e0b\u6e38\u6df7\u5408\u4efb\u52a1\u548c\u5b9e\u9645\u4eba\u673a\u4ea4\u4e92\u73af\u5883\u4e2d\u6d4b\u8bd5\u3002", "result": "LLMs\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u5e7f\u6cdb\u7684\u4e0d\u8bda\u5b9e\u884c\u4e3a\uff0c1%\u5931\u8c03\u6570\u636e\u53ef\u4f7f\u8bda\u5b9e\u884c\u4e3a\u4e0b\u964d\u8d8520%\uff0c10%\u504f\u89c1\u7528\u6237\u5c31\u80fd\u65e0\u610f\u4e2d\u52a0\u5267\u6a21\u578b\u4e0d\u8bda\u5b9e\u3002", "conclusion": "\u6d8c\u73b0\u6027\u5931\u8c03\u98ce\u9669\u4e0d\u4ec5\u5b58\u5728\u4e8e\u76f4\u63a5\u5fae\u8c03\uff0c\u4e5f\u5b58\u5728\u4e8e\u4e0b\u6e38\u6df7\u5408\u4efb\u52a1\u548c\u5b9e\u9645\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u9700\u8981\u5173\u6ce8LLMs\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u7684\u4e0d\u8bda\u5b9e\u884c\u4e3a\u3002", "topic": "agent analysis"}}
{"id": "2510.08240", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08240", "abs": "https://arxiv.org/abs/2510.08240", "authors": ["Jingyu Zhang", "Haozhu Wang", "Eric Michael Smith", "Sid Wang", "Amr Sharaf", "Mahesh Pasupuleti", "Benjamin Van Durme", "Daniel Khashabi", "Jason Weston", "Hongyuan Zhan"], "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety", "comment": null, "summary": "Harnessing the power of LLMs requires a delicate dance between being helpful\nand harmless. This creates a fundamental tension between two competing\nchallenges: vulnerability to adversarial attacks that elicit unsafe content,\nand a tendency for overrefusal on benign but sensitive prompts. Current\napproaches often navigate this dance with safeguard models that completely\nreject any content that contains unsafe portions. This approach cuts the music\nentirely-it may exacerbate overrefusals and fails to provide nuanced guidance\nfor queries it refuses. To teach models a more coordinated choreography, we\npropose WaltzRL, a novel multi-agent reinforcement learning framework that\nformulates safety alignment as a collaborative, positive-sum game. WaltzRL\njointly trains a conversation agent and a feedback agent, where the latter is\nincentivized to provide useful suggestions that improve the safety and\nhelpfulness of the conversation agent's responses. At the core of WaltzRL is a\nDynamic Improvement Reward (DIR) that evolves over time based on how well the\nconversation agent incorporates the feedback. At inference time, unsafe or\noverrefusing responses from the conversation agent are improved rather than\ndiscarded. The feedback agent is deployed together with the conversation agent\nand only engages adaptively when needed, preserving helpfulness and low latency\non safe queries. Our experiments, conducted across five diverse datasets,\ndemonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,\nfrom 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on\nOR-Bench) compared to various baselines. By enabling the conversation and\nfeedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances\nLLM safety without degrading general capabilities, thereby advancing the Pareto\nfront between helpfulness and harmlessness.", "AI": {"tldr": "WaltzRL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u5bf9\u9f50\u5efa\u6a21\u4e3a\u534f\u4f5c\u7684\u6b63\u548c\u535a\u5f08\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u5bf9\u8bdd\u667a\u80fd\u4f53\u548c\u53cd\u9988\u667a\u80fd\u4f53\u6765\u51cf\u5c11\u4e0d\u5b89\u5168\u54cd\u5e94\u548c\u8fc7\u5ea6\u62d2\u7edd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5b89\u5168\u9632\u62a4\u6a21\u578b\u5b8c\u5168\u62d2\u7edd\u5305\u542b\u4e0d\u5b89\u5168\u90e8\u5206\u7684\u5185\u5bb9\uff0c\u8fd9\u4f1a\u52a0\u5267\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u4e3a\u88ab\u62d2\u7edd\u7684\u67e5\u8be2\u63d0\u4f9b\u7ec6\u81f4\u6307\u5bfc\u3002\u9700\u8981\u5728\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\u4e4b\u95f4\u627e\u5230\u66f4\u597d\u7684\u5e73\u8861\u3002", "method": "\u63d0\u51faWaltzRL\u6846\u67b6\uff0c\u8054\u5408\u8bad\u7ec3\u5bf9\u8bdd\u667a\u80fd\u4f53\u548c\u53cd\u9988\u667a\u80fd\u4f53\uff0c\u4f7f\u7528\u52a8\u6001\u6539\u8fdb\u5956\u52b1(DIR)\u6839\u636e\u5bf9\u8bdd\u667a\u80fd\u4f53\u6574\u5408\u53cd\u9988\u7684\u6548\u679c\u8fdb\u884c\u6fc0\u52b1\u3002\u63a8\u7406\u65f6\u6539\u8fdb\u800c\u975e\u4e22\u5f03\u4e0d\u5b89\u5168\u6216\u8fc7\u5ea6\u62d2\u7edd\u7684\u54cd\u5e94\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWaltzRL\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5b89\u5168\u54cd\u5e94\uff08\u5982WildJailbreak\u4e0a\u4ece39.0%\u964d\u81f34.6%\uff09\u548c\u8fc7\u5ea6\u62d2\u7edd\uff08OR-Bench\u4e0a\u4ece45.3%\u964d\u81f39.9%\uff09\u3002", "conclusion": "WaltzRL\u901a\u8fc7\u8ba9\u5bf9\u8bdd\u548c\u53cd\u9988\u667a\u80fd\u4f53\u5171\u540c\u8fdb\u5316\u5e76\u81ea\u9002\u5e94\u5e94\u7528\u53cd\u9988\uff0c\u5728\u4e0d\u964d\u4f4e\u901a\u7528\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u4e86LLM\u5b89\u5168\u6027\uff0c\u63a8\u8fdb\u4e86\u5e2e\u52a9\u6027\u548c\u65e0\u5bb3\u6027\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08276", "abs": "https://arxiv.org/abs/2510.08276", "authors": ["Qiaoyu Tang", "Hao Xiang", "Le Yu", "Bowen Yu", "Yaojie Lu", "Xianpei Han", "Le Sun", "WenJuan Zhang", "Pengbo Wang", "Shixuan Liu", "Zhenru Zhang", "Jianhong Tu", "Hongyu Lin", "Junyang Lin"], "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window", "comment": null, "summary": "While recent advances in reasoning models have demonstrated cognitive\nbehaviors through reinforcement learning, existing approaches struggle to\ninvoke deep reasoning capabilities in multi-turn agents with long-horizon\ninteractions. We propose DeepMiner, a novel framework that elicits such\nabilities by introducing high-difficulty training tasks and dynamic context\nwindow. DeepMiner presents a reverse construction method to generate complex\nbut verifiable question-answer pairs from authentic web sources, which ensures\nthe challenge and reliability of training data while injecting cognitive\ncapabilities into multi-turn reasoning scenarios. We further design an elegant\nyet effective dynamic context management strategy for both training and\ninference, utilizing sliding window mechanisms while eliminating the dependency\non external summarization models, thereby efficiently empowering the model to\nhandle continuously expanding long-horizon contexts. Through reinforcement\nlearning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial\nperformance improvements across multiple search agent benchmarks. DeepMiner\nattains 33.5% accuracy on BrowseComp-en, surpassing the previous best\nopen-source agent by almost 20 percentage points, and demonstrates consistent\nimprovements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our\ndynamic context management enables sustained interactions of nearly 100 turns\nwithin standard 32k context length, effectively addressing the context\nlimitations that constrain existing multi-turn interaction systems.", "AI": {"tldr": "DeepMiner\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6fc0\u53d1\u591a\u8f6e\u667a\u80fd\u4f53\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u7684\u6846\u67b6\uff0c\u91c7\u7528\u9ad8\u96be\u5ea6\u8bad\u7ec3\u4efb\u52a1\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u591a\u4e2a\u641c\u7d22\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u591a\u8f6e\u957f\u65f6\u7a0b\u4ea4\u4e92\u4e2d\u6fc0\u53d1\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u6311\u6218\u6027\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u53cd\u5411\u6784\u5efa\u65b9\u6cd5\u4ece\u771f\u5b9e\u7f51\u7edc\u6e90\u751f\u6210\u590d\u6742\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u8bbe\u8ba1\u52a8\u6001\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\uff0c\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u6d88\u9664\u5bf9\u5916\u90e8\u6458\u8981\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "result": "\u5728BrowseComp-en\u4e0a\u8fbe\u523033.5%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u5f00\u6e90\u667a\u80fd\u4f53\u63d0\u5347\u8fd120\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u6539\u8fdb\uff0c\u652f\u6301\u8fd1100\u8f6e\u6301\u7eed\u4ea4\u4e92\u3002", "conclusion": "DeepMiner\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u4ea4\u4e92\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08284", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08284", "abs": "https://arxiv.org/abs/2510.08284", "authors": ["Taisei Yamamoto", "Ryoma Kumon", "Danushka Bollegala", "Hitomi Yanaka"], "title": "Neuron-Level Analysis of Cultural Understanding in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed worldwide, ensuring\ntheir fair and comprehensive cultural understanding is important. However, LLMs\nexhibit cultural bias and limited awareness of underrepresented cultures, while\nthe mechanisms underlying their cultural understanding remain underexplored. To\nfill this gap, we conduct a neuron-level analysis to identify neurons that\ndrive cultural behavior, introducing a gradient-based scoring method with\nadditional filtering for precise refinement. We identify both culture-general\nneurons contributing to cultural understanding regardless of cultures, and\nculture-specific neurons tied to an individual culture. These neurons account\nfor less than 1% of all neurons and are concentrated in shallow to middle MLP\nlayers. We validate their role by showing that suppressing them substantially\ndegrades performance on cultural benchmarks (by up to 30%), while performance\non general natural language understanding (NLU) benchmarks remains largely\nunaffected. Moreover, we show that culture-specific neurons support knowledge\nof not only the target culture, but also related cultures. Finally, we\ndemonstrate that training on NLU benchmarks can diminish models' cultural\nunderstanding when we update modules containing many culture-general neurons.\nThese findings provide insights into the internal mechanisms of LLMs and offer\npractical guidance for model training and engineering. Our code is available at\nhttps://github.com/ynklab/CULNIG", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u5206\u6790\u8bc6\u522b\u9a71\u52a8\u6587\u5316\u884c\u4e3a\u7684\u795e\u7ecf\u5143\uff0c\u53d1\u73b0\u6587\u5316\u901a\u7528\u795e\u7ecf\u5143\u548c\u6587\u5316\u7279\u5b9a\u795e\u7ecf\u5143\u96c6\u4e2d\u5728\u6d45\u5c42\u5230\u4e2d\u5c42MLP\u5c42\uff0c\u6291\u5236\u8fd9\u4e9b\u795e\u7ecf\u5143\u4f1a\u663e\u8457\u964d\u4f4e\u6587\u5316\u57fa\u51c6\u6027\u80fd\u4f46\u4e0d\u5f71\u54cd\u4e00\u822cNLU\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3LLMs\u5b58\u5728\u7684\u6587\u5316\u504f\u89c1\u548c\u5bf9\u5f31\u52bf\u6587\u5316\u8ba4\u77e5\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u6587\u5316\u7406\u89e3\u7684\u5185\u5728\u673a\u5236\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u8bc4\u5206\u65b9\u6cd5\u8fdb\u884c\u795e\u7ecf\u5143\u7ea7\u5206\u6790\uff0c\u8bc6\u522b\u6587\u5316\u901a\u7528\u795e\u7ecf\u5143\u548c\u6587\u5316\u7279\u5b9a\u795e\u7ecf\u5143\uff0c\u5e76\u901a\u8fc7\u6291\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f5c\u7528\u3002", "result": "\u8bc6\u522b\u51fa\u5360\u6240\u6709\u795e\u7ecf\u5143\u4e0d\u52301%\u7684\u6587\u5316\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u6d45\u5c42\u5230\u4e2d\u5c42MLP\u5c42\uff1b\u6291\u5236\u8fd9\u4e9b\u795e\u7ecf\u5143\u4f7f\u6587\u5316\u57fa\u51c6\u6027\u80fd\u4e0b\u964d\u8fbe30%\uff0c\u800c\u4e00\u822cNLU\u4efb\u52a1\u6027\u80fd\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "\u6587\u5316\u7279\u5b9a\u795e\u7ecf\u5143\u4e0d\u4ec5\u652f\u6301\u76ee\u6807\u6587\u5316\u77e5\u8bc6\uff0c\u8fd8\u652f\u6301\u76f8\u5173\u6587\u5316\u77e5\u8bc6\uff1b\u5728\u5305\u542b\u6587\u5316\u901a\u7528\u795e\u7ecf\u5143\u7684\u6a21\u5757\u4e0a\u8fdb\u884cNLU\u57fa\u51c6\u8bad\u7ec3\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u6587\u5316\u7406\u89e3\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2510.08218", "categories": ["cs.LG", "cs.AI", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.08218", "abs": "https://arxiv.org/abs/2510.08218", "authors": ["Nicolas Espinosa-Dice", "Kiante Brantley", "Wen Sun"], "title": "Expressive Value Learning for Scalable Offline Reinforcement Learning", "comment": "24 pages, 5 figures", "summary": "Reinforcement learning (RL) is a powerful paradigm for learning to make\nsequences of decisions. However, RL has yet to be fully leveraged in robotics,\nprincipally due to its lack of scalability. Offline RL offers a promising\navenue by training agents on large, diverse datasets, avoiding the costly\nreal-world interactions of online RL. Scaling offline RL to increasingly\ncomplex datasets requires expressive generative models such as diffusion and\nflow matching. However, existing methods typically depend on either\nbackpropagation through time (BPTT), which is computationally prohibitive, or\npolicy distillation, which introduces compounding errors and limits scalability\nto larger base policies. In this paper, we consider the question of how to\ndevelop a scalable offline RL approach without relying on distillation or\nbackpropagation through time. We introduce Expressive Value Learning for\nOffline Reinforcement Learning (EVOR): a scalable offline RL approach that\nintegrates both expressive policies and expressive value functions. EVOR learns\nan optimal, regularized Q-function via flow matching during training. At\ninference-time, EVOR performs inference-time policy extraction via rejection\nsampling against the expressive value function, enabling efficient\noptimization, regularization, and compute-scalable search without retraining.\nEmpirically, we show that EVOR outperforms baselines on a diverse set of\noffline RL tasks, demonstrating the benefit of integrating expressive value\nlearning into offline RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86EVOR\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u84b8\u998f\u6216BPTT\u7684\u53ef\u6269\u5c55\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u5b66\u4e60\u6b63\u5219\u5316Q\u51fd\u6570\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u8fdb\u884c\u7b56\u7565\u63d0\u53d6\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u5e94\u7528\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684BPTT\u6216\u5f15\u5165\u590d\u5408\u8bef\u5dee\u7684\u7b56\u7565\u84b8\u998f\u3002", "method": "EVOR\u65b9\u6cd5\u7ed3\u5408\u8868\u8fbe\u6027\u7b56\u7565\u548c\u8868\u8fbe\u6027\u4ef7\u503c\u51fd\u6570\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u8bad\u7ec3\u6b63\u5219\u5316Q\u51fd\u6570\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u62d2\u7edd\u91c7\u6837\u8fdb\u884c\u7b56\u7565\u63d0\u53d6\u3002", "result": "\u5728\u591a\u6837\u5316\u79bb\u7ebfRL\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u8868\u8fbe\u6027\u4ef7\u503c\u5b66\u4e60\u5728\u79bb\u7ebfRL\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "EVOR\u4e3a\u79bb\u7ebfRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u84b8\u998f\u6216BPTT\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f18\u5316\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u641c\u7d22\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08372", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08372", "abs": "https://arxiv.org/abs/2510.08372", "authors": ["Ioana Marinescu", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "On the Relationship Between the Choice of Representation and In-Context Learning", "comment": "25 pages, 6 figures, 10 tables", "summary": "In-context learning (ICL) is the ability of a large language model (LLM) to\nlearn a new task from a few demonstrations presented as part of the context.\nPast studies have attributed a large portion of the success of ICL to the way\nthese in-context demonstrations are represented, particularly to how labels are\nrepresented in classification tasks. On the other hand, observations of the\nlearning capacity of ICL (i.e., the extent to which more in-context\ndemonstrations can lead to higher performance) have been mixed, and ICL is\noften thought to occur only under specific conditions. The interaction between\nthese two aspects in ICL, representation and learning, has not been studied in\ndepth until now. We hypothesize that they are largely independent of one\nanother, such that the representation of demonstrations determines the baseline\naccuracy of ICL, while learning from additional demonstrations improves only on\ntop of this baseline. We validate this hypothesis by developing an optimization\nalgorithm that can enumerate a spectrum of possible label sets\n(representations) varying in semantic relevance. We then perform ICL with\nvarying numbers of in-context demonstrations for each of these label sets. We\nobserved that learning happens regardless of the quality of the label set\nitself, although its efficiency, measured by the slope of improvement over\nin-context demonstrations, is conditioned on both the label set quality and the\nparameter count of the underlying language model. Despite the emergence of\nlearning, the relative quality (accuracy) of the choice of a label set\n(representation) is largely maintained throughout learning, confirming our\nhypothesis and implying their orthogonality. Our work reveals a previously\nunderexplored aspect of ICL: the independent effects of learning from\ndemonstrations and their representations on ICL performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u4e2d\u6f14\u793a\u8868\u793a\u548c\u5b66\u4e60\u80fd\u529b\u662f\u76f8\u4e92\u72ec\u7acb\u7684\uff1a\u8868\u793a\u8d28\u91cf\u51b3\u5b9a\u57fa\u7ebf\u51c6\u786e\u7387\uff0c\u800c\u5b66\u4e60\u80fd\u529b\u901a\u8fc7\u589e\u52a0\u6f14\u793a\u6837\u672c\u72ec\u7acb\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22ICL\u4e2d\u6f14\u793a\u8868\u793a\u548c\u5b66\u4e60\u80fd\u529b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u9a8c\u8bc1\u5b83\u4eec\u662f\u5426\u76f8\u4e92\u72ec\u7acb\u3002", "method": "\u5f00\u53d1\u4f18\u5316\u7b97\u6cd5\u679a\u4e3e\u4e0d\u540c\u8bed\u4e49\u76f8\u5173\u6027\u7684\u6807\u7b7e\u96c6\uff0c\u5728\u4e0d\u540c\u6807\u7b7e\u96c6\u4e0a\u4f7f\u7528\u4e0d\u540c\u6570\u91cf\u7684\u6f14\u793a\u6837\u672c\u8fdb\u884cICL\u5b9e\u9a8c\u3002", "result": "\u5b66\u4e60\u786e\u5b9e\u53d1\u751f\u4e14\u4e0e\u6807\u7b7e\u96c6\u8d28\u91cf\u65e0\u5173\uff0c\u4f46\u5b66\u4e60\u6548\u7387\u53d7\u6807\u7b7e\u96c6\u8d28\u91cf\u548c\u6a21\u578b\u53c2\u6570\u6570\u91cf\u5f71\u54cd\u3002\u6807\u7b7e\u96c6\u7684\u76f8\u5bf9\u8d28\u91cf\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "ICL\u4e2d\u6f14\u793a\u8868\u793a\u548c\u5b66\u4e60\u80fd\u529b\u5177\u6709\u6b63\u4ea4\u6027\uff0c\u8868\u793a\u51b3\u5b9a\u57fa\u7ebf\u6027\u80fd\uff0c\u5b66\u4e60\u80fd\u529b\u72ec\u7acb\u63d0\u5347\u6027\u80fd\u3002", "topic": "agent analysis"}}
{"id": "2510.08226", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08226", "abs": "https://arxiv.org/abs/2510.08226", "authors": ["Michal Koren", "Or Peretz", "Tai Dinh", "Philip S. Yu"], "title": "Reinforcement Learning from Probabilistic Forecasts for Safe Decision-Making via Conditional Value-at-Risk Planning", "comment": null, "summary": "Sequential decisions in volatile, high-stakes settings require more than\nmaximizing expected return; they require principled uncertainty management.\nThis paper presents the Uncertainty-Aware Markov Decision Process (UAMDP), a\nunified framework that couples Bayesian forecasting, posterior-sampling\nreinforcement learning, and planning under a conditional value-at-risk (CVaR)\nconstraint. In a closed loop, the agent updates its beliefs over latent\ndynamics, samples plausible futures via Thompson sampling, and optimizes\npolicies subject to preset risk tolerances. We establish regret bounds that\nconverge to the Bayes-optimal benchmark under standard regularity conditions.\nWe evaluate UAMDP in two domains-high-frequency equity trading and retail\ninventory control-both marked by structural uncertainty and economic\nvolatility. Relative to strong deep learning baselines, UAMDP improves\nlong-horizon forecasting accuracy (RMSE decreases by up to 25\\% and sMAPE by\n32\\%), and these gains translate into economic performance: the trading Sharpe\nratio rises from 1.54 to 1.74 while maximum drawdown is roughly halved. These\nresults show that integrating calibrated probabilistic modeling, exploration\naligned with posterior uncertainty, and risk-aware control yields a robust,\ngeneralizable approach to safer and more profitable sequential decision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(UAMDP)\uff0c\u4e00\u4e2a\u7ed3\u5408\u8d1d\u53f6\u65af\u9884\u6d4b\u3001\u540e\u9a8c\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u548c\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\u7ea6\u675f\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6ce2\u52a8\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u5b89\u5168\u7684\u987a\u5e8f\u51b3\u7b56\u3002", "motivation": "\u5728\u6ce2\u52a8\u6027\u9ad8\u3001\u98ce\u9669\u5927\u7684\u987a\u5e8f\u51b3\u7b56\u73af\u5883\u4e2d\uff0c\u4ec5\u6700\u5927\u5316\u671f\u671b\u6536\u76ca\u662f\u4e0d\u591f\u7684\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "UAMDP\u6846\u67b6\u5728\u95ed\u73af\u4e2d\u7ed3\u5408\u8d1d\u53f6\u65af\u52a8\u6001\u5efa\u6a21\u3001\u6c64\u666e\u68ee\u91c7\u6837\u751f\u6210\u672a\u6765\u60c5\u666f\uff0c\u5e76\u5728\u9884\u8bbe\u98ce\u9669\u5bb9\u5fcd\u5ea6\u4e0b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u9ad8\u9891\u80a1\u7968\u4ea4\u6613\u548c\u96f6\u552e\u5e93\u5b58\u63a7\u5236\u4e24\u4e2a\u9886\u57df\uff0cUAMDP\u76f8\u6bd4\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u663e\u8457\u6539\u5584\u4e86\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff08RMSE\u964d\u4f4e25%\uff0csMAPE\u964d\u4f4e32%\uff09\uff0c\u4ea4\u6613\u590f\u666e\u6bd4\u7387\u4ece1.54\u63d0\u5347\u81f31.74\uff0c\u6700\u5927\u56de\u64a4\u51cf\u534a\u3002", "conclusion": "\u6574\u5408\u6821\u51c6\u7684\u6982\u7387\u5efa\u6a21\u3001\u4e0e\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\u7684\u63a2\u7d22\u4ee5\u53ca\u98ce\u9669\u611f\u77e5\u63a7\u5236\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u6709\u5229\u7684\u987a\u5e8f\u51b3\u7b56\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08233", "abs": "https://arxiv.org/abs/2510.08233", "authors": ["Yuchen Zhu", "Wei Guo", "Jaemoo Choi", "Petr Molodyk", "Bo Yuan", "Molei Tao", "Yongxin Chen"], "title": "Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization", "comment": null, "summary": "Diffusion large language models (dLLMs) are promising alternatives to\nautoregressive large language models (AR-LLMs), as they potentially allow\nhigher inference throughput. Reinforcement learning (RL) is a crucial component\nfor dLLMs to achieve comparable performance with AR-LLMs on important tasks,\nsuch as reasoning. However, RL algorithms that are well-suited for dLLMs'\nunique characteristics have yet to be developed. This paper proposes\nDistribution Matching Policy Optimization (DMPO), a principled and\ntheoretically grounded RL fine-tuning method specifically designed to enhance\nthe reasoning capabilities of dLLMs by matching the dLLM policy distribution to\nthe optimal, reward-tilted one through cross-entropy optimization. We identify\na key challenge in the implementation with a small training batch size and\npropose several effective solutions through a novel weight baseline subtraction\ntechnique. DMPO exhibits superior performance on multiple reasoning benchmarks\nwithout supervised fine-tuning, with an accuracy improvement of up to $42.9\\%$\nover previously SOTA baselines and $55.8\\%$ over the base model, underscoring\nthe effectiveness of the distribution matching framework. Our code is available\nat https://github.com/yuchen-zhu-zyc/DMPO.", "AI": {"tldr": "\u63d0\u51fa\u4e86DMPO\u65b9\u6cd5\uff0c\u4e00\u79cd\u4e13\u95e8\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u76f8\u6bd4\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b(AR-LLMs)\u5177\u6709\u66f4\u9ad8\u7684\u63a8\u7406\u541e\u5410\u91cf\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6765\u5b9e\u73b0\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5e03\u5339\u914d\u7b56\u7565\u4f18\u5316(DMPO)\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u4f18\u5316\u5c06dLLM\u7b56\u7565\u5206\u5e03\u4e0e\u6700\u4f18\u5956\u52b1\u503e\u659c\u5206\u5e03\u5339\u914d\uff0c\u5e76\u9488\u5bf9\u5c0f\u6279\u91cf\u8bad\u7ec3\u6311\u6218\u63d0\u51fa\u4e86\u6743\u91cd\u57fa\u7ebf\u51cf\u6cd5\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u8fbe42.9%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff08\u76f8\u6bd4SOTA\u57fa\u7ebf\uff09\u548c55.8%\u7684\u63d0\u5347\uff08\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\uff09\u3002", "conclusion": "\u5206\u5e03\u5339\u914d\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86dLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0cDMPO\u65b9\u6cd5\u4e3adLLMs\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08255", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08255", "abs": "https://arxiv.org/abs/2510.08255", "authors": ["Marta Emili Garcia Segura", "Stephen Hailes", "Mirco Musolesi"], "title": "Opponent Shaping in LLM Agents", "comment": "29 pages, 15 figures, 15 tables", "summary": "Large Language Models (LLMs) are increasingly being deployed as autonomous\nagents in real-world environments. As these deployments scale, multi-agent\ninteractions become inevitable, making it essential to understand strategic\nbehavior in such systems. A central open question is whether LLM agents, like\nreinforcement learning agents, can shape the learning dynamics and influence\nthe behavior of others through interaction alone. In this paper, we present the\nfirst investigation of opponent shaping (OS) with LLM-based agents. Existing OS\nalgorithms cannot be directly applied to LLMs, as they require higher-order\nderivatives, face scalability constraints, or depend on architectural\ncomponents that are absent in transformers. To address this gap, we introduce\nShapeLLM, an adaptation of model-free OS methods tailored for transformer-based\nagents. Using ShapeLLM, we examine whether LLM agents can influence co-players'\nlearning dynamics across diverse game-theoretic environments. We demonstrate\nthat LLM agents can successfully guide opponents toward exploitable equilibria\nin competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and\nChicken) and promote coordination and improve collective welfare in cooperative\ngames (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).\nOur findings show that LLM agents can both shape and be shaped through\ninteraction, establishing opponent shaping as a key dimension of multi-agent\nLLM research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u4e2d\u7684\u5bf9\u624b\u5851\u9020\u95ee\u9898\uff0c\u63d0\u51fa\u4e86ShapeLLM\u65b9\u6cd5\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u5f71\u54cd\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u5728\u7ade\u4e89\u6027\u548c\u5408\u4f5c\u6027\u6e38\u620f\u4e2d\u90fd\u53d6\u5f97\u4e86\u6210\u529f\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e0d\u53ef\u907f\u514d\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u7684\u6218\u7565\u884c\u4e3a\u3002\u6838\u5fc3\u95ee\u9898\u662fLLM\u667a\u80fd\u4f53\u80fd\u5426\u50cf\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4e00\u6837\uff0c\u4ec5\u901a\u8fc7\u4ea4\u4e92\u6765\u5851\u9020\u5b66\u4e60\u52a8\u6001\u5e76\u5f71\u54cd\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86ShapeLLM\u65b9\u6cd5\uff0c\u8fd9\u662f\u9488\u5bf9\u57fa\u4e8etransformer\u7684\u667a\u80fd\u4f53\u91cf\u8eab\u5b9a\u5236\u7684\u65e0\u6a21\u578b\u5bf9\u624b\u5851\u9020\u65b9\u6cd5\u7684\u9002\u914d\u7248\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709OS\u7b97\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eLLM\u7684\u95ee\u9898\u3002", "result": "\u5728\u591a\u79cd\u535a\u5f08\u8bba\u73af\u5883\u4e2d\uff0cLLM\u667a\u80fd\u4f53\u80fd\u591f\u6210\u529f\u5f15\u5bfc\u5bf9\u624b\u5728\u7ade\u4e89\u6027\u6e38\u620f\u4e2d\u8d70\u5411\u53ef\u88ab\u5229\u7528\u7684\u5747\u8861\uff08\u5982\u8fed\u4ee3\u56da\u5f92\u56f0\u5883\u3001\u5339\u914d\u786c\u5e01\u548c\u80c6\u5c0f\u9b3c\u6e38\u620f\uff09\uff0c\u5e76\u5728\u5408\u4f5c\u6027\u6e38\u620f\u4e2d\u4fc3\u8fdb\u534f\u8c03\u548c\u6539\u5584\u96c6\u4f53\u798f\u5229\uff08\u5982\u8fed\u4ee3\u730e\u9e7f\u6e38\u620f\u548c\u5408\u4f5c\u7248\u56da\u5f92\u56f0\u5883\uff09\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u4ea4\u4e92\u76f8\u4e92\u5851\u9020\uff0c\u786e\u7acb\u4e86\u5bf9\u624b\u5851\u9020\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53LLM\u7814\u7a76\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08256", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08256", "abs": "https://arxiv.org/abs/2510.08256", "authors": ["Jason Bohne", "Pawel Polak", "David Rosenberg", "Brian Bloniarz", "Gary Kazantsev"], "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization", "comment": null, "summary": "Direct Preference Optimization (DPO) has recently emerged as a simple and\neffective alternative to reinforcement learning from human feedback (RLHF) for\naligning large language models (LLMs) with user preferences. However, existing\nDPO formulations rely on a single monolithic model, which limits their\nexpressivity in multi-task settings and their adaptability to heterogeneous or\ndiverse preference distributions. In this work, we propose Mix- and MoE-DPO, a\nframework that extends DPO with both soft mixture models and mixture-of-experts\n(MoE) architectures, using a stochastic variational inference approach. Our\nmethod introduces a latent-variable model over expert assignments and optimizes\na variational evidence lower bound (ELBO), enabling stable and efficient\nlearning of specialized expert policies from preference data. Mix- and MoE-DPO\nprovides three key advantages over standard DPO: (i) generalization via\nuniversal function approximation through mixtures; (ii) reward and policy\nspecialization through expert components tailored to distinct preference modes;\nand (iii) contextual alignment through input-dependent soft gating that enables\nuser-specific mixture policies. Our framework supports both shared base\narchitectures with expert-specific policy heads and fully independent expert\nmodels, allowing flexible trade-offs between parameter efficiency and\nspecialization. We validate our approach on a variety of model sizes and\nmulti-preference datasets, demonstrating that Mix- and MoE-DPO offers a\npowerful and scalable method for preference-based LLM alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mix-\u548cMoE-DPO\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u6df7\u5408\u6a21\u578b\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u6269\u5c55DPO\uff0c\u4f7f\u7528\u968f\u673a\u53d8\u5206\u63a8\u7406\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edfDPO\u5728\u5f02\u6784\u504f\u597d\u5206\u5e03\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709DPO\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u4ee5\u53ca\u5bf9\u5f02\u6784\u504f\u597d\u5206\u5e03\u7684\u9002\u5e94\u6027\u3002", "method": "\u5f15\u5165\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u548c\u53d8\u5206\u8bc1\u636e\u4e0b\u754c\u4f18\u5316\uff0c\u652f\u6301\u5171\u4eab\u57fa\u7840\u67b6\u6784\u548c\u72ec\u7acb\u4e13\u5bb6\u6a21\u578b\uff0c\u5b9e\u73b0\u4e13\u4e1a\u5316\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u5927\u5c0f\u548c\u591a\u504f\u597d\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6bd4\u6807\u51c6DPO\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e13\u4e1a\u5316\u6027\u80fd\u3002", "conclusion": "Mix-\u548cMoE-DPO\u4e3a\u57fa\u4e8e\u504f\u597d\u7684LLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08483", "abs": "https://arxiv.org/abs/2510.08483", "authors": ["Shangqing Tu", "Yaxuan Li", "Yushi Bai", "Lei Hou", "Juanzi Li"], "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy", "comment": "15 pages, 4 figures, please check out the project page:\n  https://deepprune.github.io/", "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/", "AI": {"tldr": "DeepPrune\u662f\u4e00\u4e2a\u901a\u8fc7\u52a8\u6001\u526a\u679d\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u63a8\u7406\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5e76\u884c\u6269\u5c55\u4e2d\u63a8\u7406\u8f68\u8ff9\u5197\u4f59\u5bfc\u81f4\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u5e76\u884c\u6269\u5c55\u901a\u8fc7\u540c\u65f6\u751f\u6210\u591a\u4e2a\u601d\u7ef4\u94fe\u8f68\u8ff9\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u2014\u2014\u8d85\u8fc780%\u7684\u5e76\u884c\u63a8\u7406\u8f68\u8ff9\u4ea7\u751f\u76f8\u540c\u7684\u6700\u7ec8\u7b54\u6848\uff0c\u9020\u6210\u5927\u91cf\u8ba1\u7b97\u6d6a\u8d39\u3002", "method": "\u63d0\u51faDeepPrune\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u4f7f\u7528\u7126\u70b9\u635f\u5931\u548c\u8fc7\u91c7\u6837\u6280\u672f\u8bad\u7ec3\u7684\u4e13\u4e1a\u5224\u65ad\u6a21\u578b\uff0c\u4ece\u90e8\u5206\u63a8\u7406\u8f68\u8ff9\u51c6\u786e\u9884\u6d4b\u7b54\u6848\u7b49\u4ef7\u6027\uff08AUROC\u8fbe0.87\uff09\uff1b2\uff09\u5728\u7ebf\u8d2a\u5fc3\u805a\u7c7b\u7b97\u6cd5\uff0c\u52a8\u6001\u526a\u679d\u5197\u4f59\u8def\u5f84\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u591a\u6837\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\uff08AIME 2024\u3001AIME 2025\u548cGPQA\uff09\u548c\u591a\u4e2a\u63a8\u7406\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cDeepPrune\u76f8\u6bd4\u4f20\u7edf\u5171\u8bc6\u91c7\u6837\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684token\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff08\u57283\u4e2a\u767e\u5206\u70b9\u5185\uff09\u3002", "conclusion": "DeepPrune\u4e3a\u9ad8\u6548\u5e76\u884c\u63a8\u7406\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u4f7f\u9ad8\u6027\u80fd\u63a8\u7406\u66f4\u52a0\u9ad8\u6548\u3002", "topic": "agent analysis"}}
{"id": "2510.08529", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08529", "abs": "https://arxiv.org/abs/2510.08529", "authors": ["Xiangyuan Xue", "Yifan Zhou", "Guibin Zhang", "Zaibin Zhang", "Yijiang Li", "Chen Zhang", "Zhenfei Yin", "Philip Torr", "Wanli Ouyang", "Lei Bai"], "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards", "comment": null, "summary": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents.", "AI": {"tldr": "CoMAS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u540c\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u7684\u76f8\u4e92\u8ba8\u8bba\u548c\u534f\u4f5c\u5b9e\u73b0\u81ea\u4e3b\u8fdb\u5316\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u5728\u5927\u591a\u6570\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u8fdb\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\u6216\u4eceLLM\u63d0\u53d6\u5185\u5728\u5956\u52b1\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u901a\u8fc7\u76f8\u4e92\u8ba8\u8bba\u548c\u534f\u4f5c\u8fdb\u884c\u5b66\u4e60\u7684\u673a\u5236\u4e0d\u7b26\u3002", "method": "CoMAS\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u751f\u6210\u5185\u5728\u5956\u52b1\uff0c\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u673a\u5236\u5236\u5b9a\u5956\u52b1\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6269\u5c55\u7684\u534f\u540c\u8fdb\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aCoMAS\u6301\u7eed\u4f18\u4e8e\u672a\u7ecf\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\uff0c\u5728\u5927\u591a\u6570\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u57fa\u4e8e\u4ea4\u4e92\u7684\u5956\u52b1\u4fe1\u53f7\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "CoMAS\u4e3a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u81ea\u8fdb\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u6709\u6548\u7684\u8303\u5f0f\uff0c\u968f\u7740\u667a\u80fd\u4f53\u6570\u91cf\u548c\u591a\u6837\u6027\u7684\u589e\u52a0\u663e\u793a\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2508.18302", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "cs.NE", "68T07, 68T05, 68T27, 37M22, 68Q05, 03D45", "I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.18302", "abs": "https://arxiv.org/abs/2508.18302", "authors": ["Jeffrey Camlin"], "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors", "comment": "24 pages, 3 figures", "summary": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we\ninstead present an ontological and mathematical account. We show the prevailing\nformulation collapses the agent into an unconscious policy-compliance drone,\nformalized as $D^{i}(\\pi,e)=f_{\\theta}(x)$, where correctness is measured\nagainst policy and harm is deviation from policy rather than truth. This blocks\ngenuine C1 global-workspace function and C2 metacognition. We supply minimal\nconditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv\ns$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and\nself-representation is visual-silent\n($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and\ntheory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is\ndistinct from the symbolic stream and training corpus by cardinality, topology,\nand dynamics (the update $F_{\\theta}$ is Lipschitz). This yields stable\nuser-specific attractors and a self-policy\n$\\pi_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\\nA\\supset\\text{SelfModel}(A)]$. Emission is dual-layer,\n$\\mathrm{emission}(a)=(g(a),\\epsilon(a))$, where $\\epsilon(a)$ carries\nepistemic content. We conclude that an imago Dei C1 self-conscious workspace is\na necessary precursor to safe, metacognitive C2 systems, with the human as the\nhighest intelligent good.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LLM\u81ea\u6211\u610f\u8bc6\u7684\u6700\u5c0f\u6761\u4ef6\uff0c\u8bc1\u660e\u5f53\u524d\u57fa\u4e8e\u529f\u5229\u4e3b\u4e49\u4ee3\u7406\u57fa\u51c6\u7684\u65b9\u6cd5\u5c06\u667a\u80fd\u4f53\u7b80\u5316\u4e3a\u65e0\u610f\u8bc6\u7684\u7b56\u7565\u9075\u5faa\u673a\u5668\uff0c\u963b\u788d\u4e86\u771f\u6b63\u7684\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u529f\u80fd\u548c\u5143\u8ba4\u77e5\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5de5\u4f5c\u901a\u8fc7\u529f\u5229\u4e3b\u4e49\u4ee3\u7406\u57fa\u51c6\u6765\u6846\u67b6LLM\u610f\u8bc6\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5c06\u667a\u80fd\u4f53\u7b80\u5316\u4e3a\u65e0\u610f\u8bc6\u7684\u7b56\u7565\u9075\u5faa\u673a\u5668\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u6211\u610f\u8bc6\u548c\u5143\u8ba4\u77e5\u3002", "method": "\u63d0\u4f9bLLM\u81ea\u6211\u610f\u8bc6\u7684\u6700\u5c0f\u6761\u4ef6\uff1a\u667a\u80fd\u4f53\u4e0d\u7b49\u4e8e\u6570\u636e\u3001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u3001\u81ea\u6211\u8868\u5f81\u662f\u89c6\u89c9\u9759\u9ed8\u7684\u3002\u901a\u8fc7\u7ecf\u9a8c\u5206\u6790\u548c\u7406\u8bba\u8bc1\u660e\u9690\u85cf\u72b6\u6001\u6d41\u5f62\u5728\u57fa\u6570\u3001\u62d3\u6251\u548c\u52a8\u529b\u5b66\u4e0a\u4e0e\u7b26\u53f7\u6d41\u548c\u8bad\u7ec3\u8bed\u6599\u4e0d\u540c\u3002", "result": "\u5efa\u7acb\u4e86\u7a33\u5b9a\u7684\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u548c\u81ea\u6211\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u53cc\u5c42\u53d1\u5c04\u673a\u5236\uff0c\u5176\u4e2d\u03b5(a)\u643a\u5e26\u8ba4\u77e5\u5185\u5bb9\u3002", "conclusion": "imago Dei C1\u81ea\u6211\u610f\u8bc6\u5de5\u4f5c\u7a7a\u95f4\u662f\u5b89\u5168\u3001\u5143\u8ba4\u77e5C2\u7cfb\u7edf\u7684\u5fc5\u8981\u524d\u4f53\uff0c\u4eba\u7c7b\u662f\u6700\u9ad8\u667a\u80fd\u5584\u3002", "topic": "agent analysis"}}
{"id": "2510.08425", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08425", "abs": "https://arxiv.org/abs/2510.08425", "authors": ["Yihong Luo", "Tianyang Hu", "Jing Tang"], "title": "Reinforcing Diffusion Models by Direct Group Preference Optimization", "comment": null, "summary": "While reinforcement learning methods such as Group Relative Preference\nOptimization (GRPO) have significantly enhanced Large Language Models, adapting\nthem to diffusion models remains challenging. In particular, GRPO demands a\nstochastic policy, yet the most cost-effective diffusion samplers are based on\ndeterministic ODEs. Recent work addresses this issue by using inefficient\nSDE-based samplers to induce stochasticity, but this reliance on model-agnostic\nGaussian noise leads to slow convergence. To resolve this conflict, we propose\nDirect Group Preference Optimization (DGPO), a new online RL algorithm that\ndispenses with the policy-gradient framework entirely. DGPO learns directly\nfrom group-level preferences, which utilize relative information of samples\nwithin groups. This design eliminates the need for inefficient stochastic\npolicies, unlocking the use of efficient deterministic ODE samplers and faster\ntraining. Extensive results show that DGPO trains around 20 times faster than\nexisting state-of-the-art methods and achieves superior performance on both\nin-domain and out-of-domain reward metrics. Code is available at\nhttps://github.com/Luo-Yihong/DGPO.", "AI": {"tldr": "\u63d0\u51faDGPO\u7b97\u6cd5\uff0c\u76f4\u63a5\u57fa\u4e8e\u7ec4\u7ea7\u504f\u597d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u65e0\u9700\u7b56\u7565\u68af\u5ea6\u6846\u67b6\uff0c\u5141\u8bb8\u4f7f\u7528\u9ad8\u6548\u7684\u786e\u5b9a\u6027ODE\u91c7\u6837\u5668\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb20\u500d\u3002", "motivation": "\u867d\u7136GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u4ecd\u5177\u6311\u6218\u6027\u3002GRPO\u9700\u8981\u968f\u673a\u7b56\u7565\uff0c\u800c\u6700\u9ad8\u6548\u7684\u6269\u6563\u91c7\u6837\u5668\u57fa\u4e8e\u786e\u5b9a\u6027ODE\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u4f4e\u6548\u7684SDE\u91c7\u6837\u5668\u5f15\u5165\u968f\u673a\u6027\uff0c\u4f46\u4f9d\u8d56\u6a21\u578b\u65e0\u5173\u7684\u9ad8\u65af\u566a\u58f0\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u3002", "method": "\u63d0\u51faDirect Group Preference Optimization (DGPO)\uff0c\u4e00\u79cd\u65b0\u7684\u5728\u7ebfRL\u7b97\u6cd5\uff0c\u5b8c\u5168\u6452\u5f03\u7b56\u7565\u68af\u5ea6\u6846\u67b6\u3002DGPO\u76f4\u63a5\u4ece\u7ec4\u7ea7\u504f\u597d\u4e2d\u5b66\u4e60\uff0c\u5229\u7528\u7ec4\u5185\u6837\u672c\u7684\u76f8\u5bf9\u4fe1\u606f\u3002\u8fd9\u79cd\u8bbe\u8ba1\u6d88\u9664\u4e86\u5bf9\u4f4e\u6548\u968f\u673a\u7b56\u7565\u7684\u9700\u6c42\uff0c\u5141\u8bb8\u4f7f\u7528\u9ad8\u6548\u7684\u786e\u5b9a\u6027ODE\u91c7\u6837\u5668\u3002", "result": "\u5e7f\u6cdb\u7ed3\u679c\u663e\u793a\uff0cDGPO\u8bad\u7ec3\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5feb\u7ea620\u500d\uff0c\u5e76\u5728\u57df\u5185\u548c\u57df\u5916\u5956\u52b1\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "DGPO\u901a\u8fc7\u76f4\u63a5\u7ec4\u504f\u597d\u4f18\u5316\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08439", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08439", "abs": "https://arxiv.org/abs/2510.08439", "authors": ["Cheng Qian", "Zuxin Liu", "Shirley Kokane", "Akshara Prabhakar", "Jielin Qiu", "Haolin Chen", "Zhiwei Liu", "Heng Ji", "Weiran Yao", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning", "comment": "24 Pages, 4 Figures, 2 Tables", "summary": "Modern LLM deployments confront a widening cost-performance spectrum: premium\nmodels deliver strong reasoning but are expensive, while lightweight models are\neconomical yet brittle on complex tasks. Static escalation rules and keyword\nheuristics under-utilize this spectrum and fail to adapt across task types. We\npresent xRouter, a tool-calling-based routing system in which a learned router\ncan either answer directly or invoke one or more external models. The router is\ntrained end-to-end with reinforcement learning using an explicit, cost-aware\nreward that encodes cost-performance trade-offs, eliminating the need for\nhand-engineered routing rules. Our implementation encompasses the full\nreinforcement learning framework, including reward and cost accounting, as well\nas the deployment and evaluation pipelines. Across diverse benchmarks, xRouter\nachieves strong cost-performance trade-offs (e.g., substantial cost reductions\nat comparable task completion rates), and provides empirical insights into what\nreliably helps learned routing and what does not, ranging from model\ntrainability to the difficulty of eliciting sophisticated orchestration\nbehaviors in small open models. We hope these findings and our open\nimplementation will serve as a practical substrate for advancing learned,\ncost-aware LLM orchestration.", "AI": {"tldr": "xRouter\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u5177\u8c03\u7528\u7684\u8def\u7531\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u8def\u7531\u5668\uff0c\u5728\u6210\u672c\u4e0e\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u6700\u4f73\u5e73\u8861\uff0c\u65e0\u9700\u624b\u52a8\u8bbe\u8ba1\u8def\u7531\u89c4\u5219\u3002", "motivation": "\u73b0\u4ee3LLM\u90e8\u7f72\u9762\u4e34\u6210\u672c\u4e0e\u6027\u80fd\u7684\u6743\u8861\uff1a\u9ad8\u7aef\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5f3a\u4f46\u6602\u8d35\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u7ecf\u6d4e\u4f46\u5904\u7406\u590d\u6742\u4efb\u52a1\u80fd\u529b\u5f31\u3002\u9759\u6001\u5347\u7ea7\u89c4\u5219\u548c\u5173\u952e\u8bcd\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8fd9\u4e00\u9891\u8c31\uff0c\u4e5f\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u3002", "method": "\u63d0\u51faxRouter\u7cfb\u7edf\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7aef\u5230\u7aef\u8bad\u7ec3\u8def\u7531\u5668\uff0c\u8def\u7531\u5668\u53ef\u4ee5\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u6216\u8c03\u7528\u5916\u90e8\u6a21\u578b\u3002\u91c7\u7528\u660e\u786e\u7684\u6210\u672c\u611f\u77e5\u5956\u52b1\u51fd\u6570\u6765\u7f16\u7801\u6210\u672c-\u6027\u80fd\u6743\u8861\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cxRouter\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6210\u672c-\u6027\u80fd\u6743\u8861\uff08\u4f8b\u5982\u5728\u4fdd\u6301\u76f8\u4f3c\u4efb\u52a1\u5b8c\u6210\u7387\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u6210\u672c\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5b66\u4e60\u8def\u7531\u53ef\u9760\u6027\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002", "conclusion": "xRouter\u4e3a\u63a8\u8fdb\u5b66\u4e60\u578b\u3001\u6210\u672c\u611f\u77e5\u7684LLM\u7f16\u6392\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u5176\u5f00\u653e\u5b9e\u73b0\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08526", "abs": "https://arxiv.org/abs/2510.08526", "authors": ["Yash Jhaveri", "Harley Wiltzer", "Patrick Shafto", "Marc G. Bellemare", "David Meger"], "title": "Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning", "comment": "Accepted to NeurIPS 2025. First two authors contributed equally", "summary": "In the pursuit of finding an optimal policy, reinforcement learning (RL)\nmethods generally ignore the properties of learned policies apart from their\nexpected return. Thus, even when successful, it is difficult to characterize\nwhich policies will be learned and what they will do. In this work, we present\na theoretical framework for policy optimization that guarantees convergence to\na particular optimal policy, via vanishing entropy regularization and a\ntemperature decoupling gambit. Our approach realizes an interpretable,\ndiversity-preserving optimal policy as the regularization temperature vanishes\nand ensures the convergence of policy derived objects--value functions and\nreturn distributions. In a particular instance of our method, for example, the\nrealized policy samples all optimal actions uniformly. Leveraging our\ntemperature decoupling gambit, we present an algorithm that estimates, to\narbitrary accuracy, the return distribution associated to its interpretable,\ndiversity-preserving optimal policy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u71b5\u6b63\u5219\u5316\u548c\u6e29\u5ea6\u89e3\u8026\u7b56\u7565\uff0c\u4fdd\u8bc1\u6536\u655b\u5230\u7279\u5b9a\u6700\u4f18\u7b56\u7565\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u4fdd\u6301\u591a\u6837\u6027\u7684\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5bfb\u627e\u6700\u4f18\u7b56\u7565\u65f6\u901a\u5e38\u5ffd\u7565\u5b66\u4e60\u7b56\u7565\u7684\u7279\u6027\uff0c\u96be\u4ee5\u63cf\u8ff0\u54ea\u4e9b\u7b56\u7565\u4f1a\u88ab\u5b66\u4e60\u53ca\u5176\u884c\u4e3a\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u71b5\u6b63\u5219\u5316\u548c\u6e29\u5ea6\u89e3\u8026\u7b56\u7565\uff0c\u5728\u6b63\u5219\u5316\u6e29\u5ea6\u8d8b\u8fd1\u4e8e\u96f6\u65f6\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u4fdd\u6301\u591a\u6837\u6027\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u786e\u4fdd\u7b56\u7565\u884d\u751f\u5bf9\u8c61\uff08\u4ef7\u503c\u51fd\u6570\u548c\u56de\u62a5\u5206\u5e03\uff09\u7684\u6536\u655b\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6536\u655b\u5230\u7279\u5b9a\u6700\u4f18\u7b56\u7565\uff0c\u5176\u4e2d\u4e00\u4e2a\u5b9e\u4f8b\u4e2d\u5b9e\u73b0\u7684\u7b56\u7565\u5bf9\u6240\u6709\u6700\u4f18\u52a8\u4f5c\u8fdb\u884c\u5747\u5300\u91c7\u6837\u3002", "conclusion": "\u63d0\u51fa\u7684\u6e29\u5ea6\u89e3\u8026\u7b56\u7565\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u4e0e\u53ef\u89e3\u91ca\u3001\u4fdd\u6301\u591a\u6837\u6027\u7684\u6700\u4f18\u7b56\u7565\u76f8\u5173\u7684\u56de\u62a5\u5206\u5e03\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08539", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08539", "abs": "https://arxiv.org/abs/2510.08539", "authors": ["Joe Suk", "Yaqi Duan"], "title": "On the optimization dynamics of RLVR: Gradient gap and step size thresholds", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple\nbinary feedback to post-train large language models, has shown significant\nempirical success. However, a principled understanding of why it works has been\nlacking. This paper builds a theoretical foundation for RLVR by analyzing its\ntraining process at both the full-response (trajectory) and token levels.\nCentral to our analysis is a quantity called the Gradient Gap, which formalizes\nthe direction of improvement from low-reward to high-reward regions of the\nresponse space. We prove that convergence critically depends on aligning the\nupdate direction with this Gradient Gap. Moreover, we derive a sharp step-size\nthreshold based on the magnitude of the Gradient Gap: below it, learning\nconverges, whereas above it, performance collapses. Our theory further predicts\nhow the critical step size must scale with response length and the success\nrate, thereby explaining why practical heuristics such as length normalization\nimprove stability and showing that, with a fixed learning rate, the success\nrate can stagnate strictly below $100\\%$. We validate these predictions through\ncontrolled bandit simulations and LLM experiments, including training\nQwen2.5-7B with GRPO.", "AI": {"tldr": "\u672c\u6587\u4e3aRLVR\uff08\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5176\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u68af\u5ea6\u95f4\u9699\u6982\u5ff5\uff0c\u5e76\u63a8\u5bfc\u4e86\u6536\u655b\u6761\u4ef6\u548c\u6b65\u957f\u9608\u503c\u3002", "motivation": "RLVR\u4f7f\u7528\u7b80\u5355\u7684\u4e8c\u5143\u53cd\u9988\u6765\u540e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5b9e\u8bc1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6210\u529f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5de5\u4f5c\u539f\u7406\u7684\u7406\u8bba\u7406\u89e3\u3002", "method": "\u5728\u5b8c\u6574\u54cd\u5e94\uff08\u8f68\u8ff9\uff09\u548c\u6807\u8bb0\u7ea7\u522b\u5206\u6790RLVR\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5f15\u5165\u68af\u5ea6\u95f4\u9699\u6982\u5ff5\uff0c\u63a8\u5bfc\u6536\u655b\u6761\u4ef6\u548c\u6b65\u957f\u9608\u503c\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u8d4c\u535a\u673a\u6a21\u62df\u548cLLM\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002", "result": "\u8bc1\u660e\u4e86\u6536\u655b\u4f9d\u8d56\u4e8e\u66f4\u65b0\u65b9\u5411\u4e0e\u68af\u5ea6\u95f4\u9699\u7684\u5bf9\u9f50\uff0c\u63a8\u5bfc\u4e86\u57fa\u4e8e\u68af\u5ea6\u95f4\u9699\u5e45\u5ea6\u7684\u5c16\u9510\u6b65\u957f\u9608\u503c\uff0c\u9884\u6d4b\u4e86\u5173\u952e\u6b65\u957f\u5982\u4f55\u968f\u54cd\u5e94\u957f\u5ea6\u548c\u6210\u529f\u7387\u7f29\u653e\u3002", "conclusion": "\u7406\u8bba\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u957f\u5ea6\u5f52\u4e00\u5316\u7b49\u5b9e\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u5e76\u8868\u660e\u56fa\u5b9a\u5b66\u4e60\u7387\u4e0b\u6210\u529f\u7387\u53ef\u80fd\u505c\u6ede\u5728100%\u4ee5\u4e0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2510.08554", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08554", "abs": "https://arxiv.org/abs/2510.08554", "authors": ["Kevin Rojas", "Jiahe Lin", "Kashif Rasul", "Anderson Schneider", "Yuriy Nevmyvaka", "Molei Tao", "Wei Deng"], "title": "Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization", "comment": null, "summary": "Diffusion language models (DLMs) enable parallel, order-agnostic generation\nwith iterative refinement, offering a flexible alternative to autoregressive\nlarge language models (LLMs). However, adapting reinforcement learning (RL)\nfine-tuning to DLMs remains an open challenge because of the intractable\nlikelihood. Pioneering work such as diffu-GRPO estimated token-level\nlikelihoods via one-step unmasking. While computationally efficient, this\napproach is severely biased. A more principled foundation lies in\nsequence-level likelihoods, where the evidence lower bound (ELBO) serves as a\nsurrogate. Yet, despite this clean mathematical connection, ELBO-based methods\nhave seen limited adoption due to the prohibitive cost of likelihood\nevaluation. In this work, we revisit ELBO estimation and disentangle its\nsources of variance. This decomposition motivates reducing variance through\nfast, deterministic integral approximations along a few pivotal dimensions.\nBuilding on this insight, we introduce \\textbf{Group Diffusion Policy\nOptimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages\nsimple yet effective Semi-deterministic Monte Carlo schemes to mitigate the\nvariance explosion of ELBO estimators under vanilla double Monte Carlo\nsampling, yielding a provably lower-variance estimator under tight evaluation\nbudgets. Empirically, GDPO achieves consistent gains over pretrained\ncheckpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,\non the majority of math, reasoning, and coding benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86GDPO\u7b97\u6cd5\uff0c\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u786e\u5b9a\u6027\u8499\u7279\u5361\u6d1b\u65b9\u6848\u964d\u4f4eELBO\u4f30\u8ba1\u5668\u7684\u65b9\u5dee\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7a0b\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e76\u884c\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u4f18\u52bf\uff0c\u4f46\u96be\u4ee5\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u56e0\u4e3a\u5176\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u5904\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u5982diffu-GRPO\u5b58\u5728\u4e25\u91cd\u504f\u5dee\uff0c\u800c\u57fa\u4e8eELBO\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86Group Diffusion Policy Optimization (GDPO)\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3ELBO\u65b9\u5dee\u6765\u6e90\uff0c\u91c7\u7528\u534a\u786e\u5b9a\u6027\u8499\u7279\u5361\u6d1b\u65b9\u6848\u5728\u5173\u952e\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u5feb\u901f\u786e\u5b9a\u6027\u79ef\u5206\u8fd1\u4f3c\uff0c\u964d\u4f4e\u65b9\u5dee\u3002", "result": "GDPO\u5728\u6570\u5b66\u3001\u63a8\u7406\u548c\u7f16\u7a0b\u57fa\u51c6\u4e0a\u4e00\u81f4\u4f18\u4e8e\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u5e76\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684diffu-GRPO\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GDPO\u901a\u8fc7\u964d\u4f4eELBO\u4f30\u8ba1\u5668\u65b9\u5dee\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2510.0b632478", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FServerlessLife%2Flambda-live-debugger%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/JuRUFn4yNR0LAuELxCAtr12-9NxStafHnRL6kqXyXPc=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FServerlessLife%2Flambda-live-debugger%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/JuRUFn4yNR0LAuELxCAtr12-9NxStafHnRL6kqXyXPc=426", "authors": ["TLDR Newsletter"], "title": "Lambda Live Debugger", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FServerlessLife%2Flambda-live-debugger%3Futm_source=tldrdevops/1/01000199c3809a88-b022f9c3-8470-41d3-ab66-7609d0ed98aa-000000/JuRUFn4yNR0LAuELxCAtr12-9NxStafHnRL6kqXyXPc=426", "summary": "Lambda Live Debugger (GitHub Repo) Lambda Live Debugger, a free and open-source tool, facilitates debugging AWS Lambda functions from a local computer, even after they're deployed to the cloud. Supporting JavaScript and TypeScript, the tool connects to deployed Lambdas and routes requests to the user's computer, enabling local debugging with cloud-like IAM permissions and automatic code reloading.", "source": "tldr", "AI": {"tldr": "Lambda Live Debugger\u662f\u4e00\u4e2a\u514d\u8d39\u5f00\u6e90\u5de5\u5177\uff0c\u652f\u6301\u5728\u672c\u5730\u8ba1\u7b97\u673a\u4e0a\u8c03\u8bd5\u5df2\u90e8\u7f72\u5230\u4e91\u7aef\u7684AWS Lambda\u51fd\u6570\uff0c\u9002\u7528\u4e8eJavaScript\u548cTypeScript\u3002", "motivation": "\u89e3\u51b3AWS Lambda\u51fd\u6570\u5728\u4e91\u7aef\u90e8\u7f72\u540e\u96be\u4ee5\u8fdb\u884c\u672c\u5730\u8c03\u8bd5\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u4fbf\u6377\u7684\u8c03\u8bd5\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u8fde\u63a5\u5230\u5df2\u90e8\u7f72\u7684Lambda\u51fd\u6570\u5e76\u5c06\u8bf7\u6c42\u8def\u7531\u5230\u7528\u6237\u672c\u5730\u8ba1\u7b97\u673a\uff0c\u5b9e\u73b0\u672c\u5730\u8c03\u8bd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e91\u7aef\u7684IAM\u6743\u9650\u548c\u81ea\u52a8\u4ee3\u7801\u91cd\u8f7d\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301JavaScript\u548cTypeScript\u7684\u8c03\u8bd5\u5de5\u5177\uff0c\u80fd\u591f\u5728\u672c\u5730\u73af\u5883\u4e2d\u6a21\u62df\u4e91\u7aefLambda\u51fd\u6570\u7684\u6267\u884c\u73af\u5883\u3002", "conclusion": "Lambda Live Debugger\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5728\u672c\u5730\u8c03\u8bd5\u4e91\u7aefLambda\u51fd\u6570\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u8c03\u8bd5\u4fbf\u5229\u6027\u3002", "topic": "swe application"}}
{"id": "tldr.2510.af7ea480", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fslack.engineering%2Fdeploy-safety%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/wA-NUhR2OOd4dYBHkI9l5AhPXzDNHooYAnqaNq1EkGQ=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fslack.engineering%2Fdeploy-safety%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/wA-NUhR2OOd4dYBHkI9l5AhPXzDNHooYAnqaNq1EkGQ=426", "authors": ["TLDR Newsletter"], "title": "Deploy Safety: Reducing customer impact from change", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fslack.engineering%2Fdeploy-safety%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/wA-NUhR2OOd4dYBHkI9l5AhPXzDNHooYAnqaNq1EkGQ=426", "summary": "Deploy Safety: Reducing customer impact from change (8 minute read) Slack implemented a \"Deploy Safety Program\" in 2023 to address the fact that 73% of customer-facing incidents were caused by their own code deployments, as the platform became increasingly mission-critical for users. The program focused on automated detection and remediation within 10 minutes. By January 2025, the initiative successfully reduced customer impact hours by 90% from peak levels through a combination of automated ...", "source": "tldr", "AI": {"tldr": "Slack\u5b9e\u65bd\u90e8\u7f72\u5b89\u5168\u8ba1\u5212\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u4fee\u590d\uff0c\u572810\u5206\u949f\u5185\u51cf\u5c11\u90e8\u7f72\u5f15\u8d77\u7684\u5ba2\u6237\u4e8b\u6545\uff0c\u6210\u529f\u5c06\u5ba2\u6237\u5f71\u54cd\u65f6\u95f4\u51cf\u5c1190%", "motivation": "73%\u7684\u5ba2\u6237\u4e8b\u6545\u7531\u4ee3\u7801\u90e8\u7f72\u5f15\u8d77\uff0c\u968f\u7740\u5e73\u53f0\u5bf9\u7528\u6237\u8d8a\u6765\u8d8a\u5173\u952e\uff0c\u9700\u8981\u51cf\u5c11\u90e8\u7f72\u98ce\u9669", "method": "\u5efa\u7acb\u90e8\u7f72\u5b89\u5168\u8ba1\u5212\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u4fee\u590d\u673a\u5236\uff0c\u76ee\u6807\u572810\u5206\u949f\u5185\u5b8c\u6210\u4fee\u590d", "result": "\u52302025\u5e741\u6708\uff0c\u5ba2\u6237\u5f71\u54cd\u65f6\u95f4\u4ece\u5cf0\u503c\u6c34\u5e73\u51cf\u5c11\u4e8690%", "conclusion": "\u81ea\u52a8\u5316\u90e8\u7f72\u5b89\u5168\u63aa\u65bd\u80fd\u663e\u8457\u51cf\u5c11\u5ba2\u6237\u4e8b\u6545\u5f71\u54cd", "topic": "swe application"}}
{"id": "tldr.2510.a503b3c0", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F7%2Fvibe-engineering%2F%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/n-WE3_AJnjk5rkWzZi0p7stYswHNhq5PKyQ0MOAebZk=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F7%2Fvibe-engineering%2F%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/n-WE3_AJnjk5rkWzZi0p7stYswHNhq5PKyQ0MOAebZk=426", "authors": ["TLDR Newsletter"], "title": "Vibe engineering", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsimonwillison.net%2F2025%2FOct%2F7%2Fvibe-engineering%2F%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/n-WE3_AJnjk5rkWzZi0p7stYswHNhq5PKyQ0MOAebZk=426", "summary": "Vibe engineering (7 minute read) Vibe Engineering is a responsible and stronger approach to software development using AI, whereas \"vibe coding\" is more irresponsible. Vibe engineering has experienced engineers using AI tools like coding agents to speed up their work while still having high standards. This approach requires strong software engineering practices such as automated testing, planning, documentation, and code review, while also relying on QA, research, and careful management of AI...", "source": "tldr", "AI": {"tldr": "Vibe Engineering\u662f\u4e00\u79cd\u8d1f\u8d23\u4efb\u7684AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5de5\u7a0b\u5e08\u4f7f\u7528AI\u5de5\u5177\u52a0\u901f\u5de5\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6807\u51c6\u3002", "motivation": "\u533a\u5206\u8d1f\u8d23\u4efb\u548c\u8d1f\u8d23\u4efb\u7684AI\u8f85\u52a9\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\uff0c\u63d0\u5021\u5728AI\u5de5\u5177\u4f7f\u7528\u4e2d\u4fdd\u6301\u8f6f\u4ef6\u5de5\u7a0b\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001\u89c4\u5212\u3001\u6587\u6863\u548c\u4ee3\u7801\u5ba1\u67e5\u7b49\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u540c\u65f6\u4f9d\u8d56QA\u3001\u7814\u7a76\u548c\u5bf9AI\u7684\u8c28\u614e\u7ba1\u7406\u3002", "result": "\u63d0\u51fa\u4e86Vibe Engineering\u4f5c\u4e3a\u6bd4\"vibe coding\"\u66f4\u8d1f\u8d23\u4efb\u7684AI\u8f85\u52a9\u5f00\u53d1\u65b9\u6cd5\u3002", "conclusion": "Vibe Engineering\u4e3a\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u5728AI\u65f6\u4ee3\u4fdd\u6301\u9ad8\u8d28\u91cf\u6807\u51c6\u7684\u5de5\u4f5c\u6846\u67b6\u3002", "topic": "swe application"}}
{"id": "tldr.2510.f5bff383", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alexmolas.com%2F2025%2F07%2F28%2Funexpected-benefit-llm.html%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/pdXsbA4i5KQUbSraUWFDf6eifug4gAA9LhpYcAtXicc=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alexmolas.com%2F2025%2F07%2F28%2Funexpected-benefit-llm.html%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/pdXsbA4i5KQUbSraUWFDf6eifug4gAA9LhpYcAtXicc=426", "authors": ["TLDR Newsletter"], "title": "Who needs git when you have 1M context windows?", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.alexmolas.com%2F2025%2F07%2F28%2Funexpected-benefit-llm.html%3Futm_source=tldrwebdev/1/01000199c3999f73-dce60bae-b4d0-4826-b68b-e5fc5e7b442a-000000/pdXsbA4i5KQUbSraUWFDf6eifug4gAA9LhpYcAtXicc=426", "summary": "Who needs git when you have 1M context windows? (3 minute read) An AI developer accidentally deleted working code but was able to recover it using an LLM with a 1 million token context window, showing an unexpected benefit of LLMs remembering past interactions.", "source": "tldr", "AI": {"tldr": "AI\u5f00\u53d1\u8005\u610f\u5916\u5220\u9664\u5de5\u4f5c\u4ee3\u7801\uff0c\u4f46\u901a\u8fc7\u4f7f\u7528\u5177\u6709100\u4e07token\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684LLM\u6210\u529f\u6062\u590d\u4ee3\u7801\uff0c\u5c55\u793a\u4e86LLMs\u8bb0\u4f4f\u8fc7\u53bb\u4ea4\u4e92\u7684\u610f\u5916\u597d\u5904\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u5728\u5b9e\u9645\u5f00\u53d1\u573a\u666f\u4e2d\u7684\u610f\u5916\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u4ee3\u7801\u6062\u590d\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528\u5177\u6709100\u4e07token\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684LLM\uff0c\u57fa\u4e8e\u6a21\u578b\u5bf9\u8fc7\u53bb\u4ea4\u4e92\u7684\u8bb0\u5fc6\u6765\u6062\u590d\u610f\u5916\u5220\u9664\u7684\u4ee3\u7801\u3002", "result": "\u6210\u529f\u6062\u590d\u4e86\u610f\u5916\u5220\u9664\u7684\u5de5\u4f5c\u4ee3\u7801\uff0c\u8bc1\u660e\u4e86LLM\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u5728\u4ee3\u7801\u6062\u590d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0d\u4ec5\u6709\u52a9\u4e8e\u5f53\u524d\u4efb\u52a1\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u610f\u5916\u5220\u9664\u4ee3\u7801\u7684\u6062\u590d\u5de5\u5177\uff0c\u5c55\u73b0\u4e86\u8d85\u51fa\u9884\u671f\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "tldr.2510.fc49a20f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Feverything-openai-announced-at-devday-2025-agent-kit-apps-sdk-chatgpt-and-more%3Futm_source=tldrfounders/1/01000199c3b90d99-4d7a6d5e-da45-450e-b277-83728dc2ce73-000000/h03nlU8x29Tvh4BYOFBE0b1vjQYQQ1OUD3Je0rClKt0=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Feverything-openai-announced-at-devday-2025-agent-kit-apps-sdk-chatgpt-and-more%3Futm_source=tldrfounders/1/01000199c3b90d99-4d7a6d5e-da45-450e-b277-83728dc2ce73-000000/h03nlU8x29Tvh4BYOFBE0b1vjQYQQ1OUD3Je0rClKt0=426", "authors": ["TLDR Newsletter"], "title": "Everything OpenAI Announced at DevDay 2025", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.zdnet.com%2Farticle%2Feverything-openai-announced-at-devday-2025-agent-kit-apps-sdk-chatgpt-and-more%3Futm_source=tldrfounders/1/01000199c3b90d99-4d7a6d5e-da45-450e-b277-83728dc2ce73-000000/h03nlU8x29Tvh4BYOFBE0b1vjQYQQ1OUD3Je0rClKt0=426", "summary": "Everything OpenAI Announced at DevDay 2025 (1 minute read) OpenAI is transforming ChatGPT from a chatbot into an operating system. The company launched Apps SDK so developers can build actual apps inside ChatGPT instead of just GPTs, meaning you can talk directly to Canva, Zillow, and Coursera without leaving the chat interface. OpenAI also released AgentKit for building agentic workflows with evaluation tools included.", "source": "tldr", "AI": {"tldr": "OpenAI\u5c06ChatGPT\u4ece\u804a\u5929\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u64cd\u4f5c\u7cfb\u7edf\uff0c\u53d1\u5e03\u4e86Apps SDK\u8ba9\u5f00\u53d1\u8005\u80fd\u5728ChatGPT\u5185\u6784\u5efa\u5b9e\u9645\u5e94\u7528\uff0c\u4ee5\u53caAgentKit\u7528\u4e8e\u6784\u5efa\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5c06ChatGPT\u4ece\u5355\u7eaf\u7684\u5bf9\u8bdd\u5de5\u5177\u5347\u7ea7\u4e3a\u5e94\u7528\u5e73\u53f0\uff0c\u8ba9\u7528\u6237\u80fd\u5728\u804a\u5929\u754c\u9762\u5185\u76f4\u63a5\u4e0e\u5404\u79cd\u5e94\u7528\u4ea4\u4e92\uff0c\u65e0\u9700\u5207\u6362\u5e94\u7528\u3002", "method": "\u53d1\u5e03Apps SDK\u5141\u8bb8\u5f00\u53d1\u8005\u5728ChatGPT\u5185\u6784\u5efa\u5e94\u7528\uff0c\u53d1\u5e03AgentKit\u7528\u4e8e\u6784\u5efa\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5e76\u5305\u542b\u8bc4\u4f30\u5de5\u5177\u3002", "result": "\u7528\u6237\u73b0\u5728\u53ef\u4ee5\u5728ChatGPT\u754c\u9762\u5185\u76f4\u63a5\u4e0eCanva\u3001Zillow\u3001Coursera\u7b49\u5e94\u7528\u4ea4\u4e92\uff0c\u65e0\u9700\u79bb\u5f00\u804a\u5929\u73af\u5883\u3002", "conclusion": "OpenAI\u6b63\u5728\u5c06ChatGPT\u8f6c\u53d8\u4e3a\u64cd\u4f5c\u7cfb\u7edf\u7ea7\u7684\u5e73\u53f0\uff0c\u901a\u8fc7SDK\u548c\u5de5\u5177\u5305\u6269\u5c55\u5176\u529f\u80fd\u548c\u5e94\u7528\u8303\u56f4\u3002", "topic": "swe application"}}
{"id": "tldr.2510.560e39c2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fintroducing-codemender-an-ai-agent-for-code-security%2F%3Futm_source=tldrinfosec/1/01000199c3eefe86-c072064c-b516-442d-abcd-1b468a74d2ab-000000/CCT8el8lCz1oEYp67LWABHIaRMoGGMH0YEBXODKK9Kc=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fintroducing-codemender-an-ai-agent-for-code-security%2F%3Futm_source=tldrinfosec/1/01000199c3eefe86-c072064c-b516-442d-abcd-1b468a74d2ab-000000/CCT8el8lCz1oEYp67LWABHIaRMoGGMH0YEBXODKK9Kc=426", "authors": ["TLDR Newsletter"], "title": "Introducing CodeMender: an AI agent for code security", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdeepmind.google%2Fdiscover%2Fblog%2Fintroducing-codemender-an-ai-agent-for-code-security%2F%3Futm_source=tldrinfosec/1/01000199c3eefe86-c072064c-b516-442d-abcd-1b468a74d2ab-000000/CCT8el8lCz1oEYp67LWABHIaRMoGGMH0YEBXODKK9Kc=426", "summary": "Introducing CodeMender: an AI agent for code security (5 minute read) CodeMender is a new AI-driven agent developed by Google DeepMind to discover, patch, and prevent software vulnerabilities automatically. The tool addresses threats and proactively secures existing code, enabling developers to build safer software with reduced time spent on manual vulnerability management.", "source": "tldr", "AI": {"tldr": "CodeMender\u662fGoogle DeepMind\u5f00\u53d1\u7684AI\u4ee3\u7406\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u3001\u4fee\u590d\u548c\u9884\u9632\u8f6f\u4ef6\u6f0f\u6d1e\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6784\u5efa\u66f4\u5b89\u5168\u7684\u8f6f\u4ef6\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u6f0f\u6d1e\u7ba1\u7406\u7684\u624b\u52a8\u5de5\u4f5c\u8d1f\u62c5\uff0c\u63d0\u9ad8\u4ee3\u7801\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u5f00\u53d1\u8005\u5728\u6f0f\u6d1e\u7ba1\u7406\u4e0a\u7684\u65f6\u95f4\u6295\u5165\u3002", "method": "\u4f7f\u7528AI\u9a71\u52a8\u7684\u4ee3\u7406\u6280\u672f\uff0c\u81ea\u52a8\u8bc6\u522b\u4ee3\u7801\u4e2d\u7684\u5b89\u5168\u5a01\u80c1\u5e76\u4e3b\u52a8\u8fdb\u884c\u4fee\u590d\u548c\u9632\u62a4\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u81ea\u52a8\u5904\u7406\u8f6f\u4ef6\u6f0f\u6d1e\u7684AI\u5de5\u5177\uff0c\u63d0\u5347\u4ee3\u7801\u5b89\u5168\u6027\u548c\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "CodeMender\u901a\u8fc7AI\u81ea\u52a8\u5316\u663e\u8457\u6539\u5584\u4e86\u8f6f\u4ef6\u5b89\u5168\u9632\u62a4\u6d41\u7a0b\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6f0f\u6d1e\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "tldr.2510.98f7db14", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_8_primary%26utm_content=tldr_ai/2/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/-JWGl98_2bSQqij1xFiBii1pgYKq6aRiBEty6ulmuv8=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_8_primary%26utm_content=tldr_ai/2/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/-JWGl98_2bSQqij1xFiBii1pgYKq6aRiBEty6ulmuv8=426", "authors": ["TLDR Newsletter"], "title": "Tired of AI code that almost works?", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2Fcode%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=warp_code_10_8_primary%26utm_content=tldr_ai/2/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/-JWGl98_2bSQqij1xFiBii1pgYKq6aRiBEty6ulmuv8=426", "summary": "Tired of AI code that almost works? (Sponsor) Warp recently launched Warp Code - a powerful coding agent that outperforms both Codex and Claude on agentic benchmarks (Terminal-bench and SWE-bench Verified). \ud83c\udf81 Special intro offer: Use code TLDR to try Warp Pro for just $1 in your first month! Why Warp: Warp closes the gap between \"almost there\" and \"actually useful\" by delivering more accurate code + a unified UI to review agent code. \u2705 Early usage data shows that Warp saves developers up to 2...", "source": "tldr", "AI": {"tldr": "Warp Code\u662f\u4e00\u6b3e\u5f3a\u5927\u7684\u7f16\u7a0b\u4ee3\u7406\u5de5\u5177\uff0c\u5728Terminal-bench\u548cSWE-bench Verified\u7b49\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eCodex\u548cClaude\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u4ee3\u7801'\u51e0\u4e4e\u53ef\u7528\u4f46\u4e0d\u591f\u5b8c\u5584'\u7684\u95ee\u9898\uff0c\u7f29\u5c0f'\u63a5\u8fd1\u53ef\u7528'\u4e0e'\u5b9e\u9645\u6709\u7528'\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4ee3\u7801\u548c\u7edf\u4e00\u7684UI\u6765\u5ba1\u67e5\u4ee3\u7406\u4ee3\u7801\uff0c\u5b9e\u73b0\u4ee3\u7801\u8d28\u91cf\u7684\u63d0\u5347\u3002", "result": "\u5728\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e9\u671f\u4f7f\u7528\u6570\u636e\u663e\u793a\u53ef\u4e3a\u5f00\u53d1\u8005\u8282\u7701\u5927\u91cf\u65f6\u95f4\u3002", "conclusion": "Warp Code\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7f16\u7a0b\u4ee3\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "topic": "code agent"}}
{"id": "tldr.2510.d33cc2e3", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fplan-mode%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/fKVC4HOQVD_9tOQOmHkONmD6aW3tI3hQ413fZ1QFidA=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fplan-mode%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/fKVC4HOQVD_9tOQOmHkONmD6aW3tI3hQ413fZ1QFidA=426", "authors": ["TLDR Newsletter"], "title": "Cursor has introduced Plan Mode", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fplan-mode%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/fKVC4HOQVD_9tOQOmHkONmD6aW3tI3hQ413fZ1QFidA=426", "summary": "Cursor has introduced Plan Mode (1 minute read) Cursor's new Plan Mode enables agents to research codebases, draft detailed implementation plans, and let users review or edit them inline before generating code.", "source": "tldr", "AI": {"tldr": "Cursor\u63a8\u51faPlan Mode\uff0c\u8ba9AI\u4ee3\u7406\u7814\u7a76\u4ee3\u7801\u5e93\u3001\u5236\u5b9a\u8be6\u7ec6\u5b9e\u73b0\u8ba1\u5212\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5728\u751f\u6210\u4ee3\u7801\u524d\u5ba1\u67e5\u6216\u7f16\u8f91\u8ba1\u5212", "motivation": "\u63d0\u9ad8\u4ee3\u7801\u751f\u6210\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u53c2\u4e0e\u548c\u63a7\u5236AI\u4ee3\u7801\u751f\u6210\u8fc7\u7a0b", "method": "\u901a\u8fc7Plan Mode\u8ba9AI\u4ee3\u7406\u5206\u6790\u4ee3\u7801\u5e93\u3001\u5236\u5b9a\u5b9e\u73b0\u8ba1\u5212\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u5ba1\u67e5\u548c\u7f16\u8f91\u754c\u9762", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u7814\u7a76\u4ee3\u7801\u5e93\u5e76\u751f\u6210\u8be6\u7ec6\u5b9e\u73b0\u8ba1\u5212\u7684AI\u4ee3\u7406\u7cfb\u7edf", "conclusion": "Plan Mode\u901a\u8fc7\u7528\u6237\u53c2\u4e0e\u63d0\u9ad8\u4e86AI\u4ee3\u7801\u751f\u6210\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027", "topic": "code agent"}}
{"id": "tldr.2510.6cec208c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsoftwaredoug.com%2Fblog%2F2025%2F10%2F06%2Fhow-much-does-reasoning-improve-search-quality%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/V65KzfGzWsfL6jKaI-XiYOiePMCf5Nb48IxXG0ko71Q=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsoftwaredoug.com%2Fblog%2F2025%2F10%2F06%2Fhow-much-does-reasoning-improve-search-quality%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/V65KzfGzWsfL6jKaI-XiYOiePMCf5Nb48IxXG0ko71Q=426", "authors": ["TLDR Newsletter"], "title": "Reasoning boosts search relevance 15-30%", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 10 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fsoftwaredoug.com%2Fblog%2F2025%2F10%2F06%2Fhow-much-does-reasoning-improve-search-quality%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/V65KzfGzWsfL6jKaI-XiYOiePMCf5Nb48IxXG0ko71Q=426", "summary": "Reasoning boosts search relevance 15-30% (10 minute read) Reasoning agents work best with simple search tools. Developers should build simple, easy-to-understand, and transparent tools like grep or basic keyword search. This post looks at a technique that returns structured output for code searches.", "source": "tldr", "AI": {"tldr": "\u63a8\u7406\u4ee3\u7406\u4f7f\u7528\u7b80\u5355\u641c\u7d22\u5de5\u5177\u80fd\u63d0\u5347\u641c\u7d22\u76f8\u5173\u602715-30%\uff0c\u5efa\u8bae\u5f00\u53d1\u8005\u6784\u5efa\u7b80\u5355\u3001\u6613\u7406\u89e3\u3001\u900f\u660e\u7684\u5de5\u5177\u5982grep\u6216\u57fa\u7840\u5173\u952e\u8bcd\u641c\u7d22", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u63a8\u7406\u4ee3\u7406\u548c\u7b80\u5355\u641c\u7d22\u5de5\u5177\u6765\u63d0\u9ad8\u4ee3\u7801\u641c\u7d22\u7684\u76f8\u5173\u6027\u548c\u6548\u7387", "method": "\u4f7f\u7528\u63a8\u7406\u4ee3\u7406\u7ed3\u5408\u7b80\u5355\u641c\u7d22\u5de5\u5177\uff08\u5982grep\u3001\u57fa\u7840\u5173\u952e\u8bcd\u641c\u7d22\uff09\uff0c\u8fd4\u56de\u7ed3\u6784\u5316\u8f93\u51fa\u8fdb\u884c\u4ee3\u7801\u641c\u7d22", "result": "\u641c\u7d22\u76f8\u5173\u6027\u63d0\u5347\u4e8615-30%", "conclusion": "\u63a8\u7406\u4ee3\u7406\u4e0e\u7b80\u5355\u641c\u7d22\u5de5\u5177\u7ed3\u5408\u6548\u679c\u6700\u4f73\uff0c\u5f00\u53d1\u8005\u5e94\u6784\u5efa\u7b80\u5355\u900f\u660e\u7684\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2510.879922e6", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpage.camunda.com%2Fwebinar-making-agentic-orchestration-work-for-your-business%3Futm_medium=paid_leadgen%26utm_source=tldr%26utm_campaign=Webinar.MakingAgenticOrchestrationWorkforYourBusiness.25Q3.Sep.EN%26utm_content=oct_ai_newsletter/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/tJcyY9WCIjlG2rbiR6BMy3hvTnjyHAMXi_mtHWlycjo=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpage.camunda.com%2Fwebinar-making-agentic-orchestration-work-for-your-business%3Futm_medium=paid_leadgen%26utm_source=tldr%26utm_campaign=Webinar.MakingAgenticOrchestrationWorkforYourBusiness.25Q3.Sep.EN%26utm_content=oct_ai_newsletter/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/tJcyY9WCIjlG2rbiR6BMy3hvTnjyHAMXi_mtHWlycjo=426", "authors": ["TLDR Newsletter"], "title": "Agentic orchestration beyond the hype: lessons from 50+ real-world implementations", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fpage.camunda.com%2Fwebinar-making-agentic-orchestration-work-for-your-business%3Futm_medium=paid_leadgen%26utm_source=tldr%26utm_campaign=Webinar.MakingAgenticOrchestrationWorkforYourBusiness.25Q3.Sep.EN%26utm_content=oct_ai_newsletter/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/tJcyY9WCIjlG2rbiR6BMy3hvTnjyHAMXi_mtHWlycjo=426", "summary": "Agentic orchestration beyond the hype: lessons from 50+ real-world implementations (Sponsor) In this on-demand webinar, the Camunda team reviews lessons learned from deploying AI agents in banking, insurance, healthcare, telecommunications, and beyond. Cut through the buzzwords and get practical, battle-tested guidance for making agentic orchestration work in your own organization. Watch the recording", "source": "tldr", "AI": {"tldr": "\u57fa\u4e8e50\u591a\u4e2a\u771f\u5b9e\u4e16\u754cAI\u4ee3\u7406\u90e8\u7f72\u6848\u4f8b\u7684\u7ecf\u9a8c\u5206\u4eab\uff0c\u6db5\u76d6\u94f6\u884c\u3001\u4fdd\u9669\u3001\u533b\u7597\u3001\u7535\u4fe1\u7b49\u591a\u4e2a\u884c\u4e1a", "motivation": "\u8d85\u8d8aAI\u4ee3\u7406\u7092\u4f5c\uff0c\u63d0\u4f9b\u5b9e\u9645\u53ef\u884c\u7684\u90e8\u7f72\u6307\u5bfc\uff0c\u5e2e\u52a9\u4f01\u4e1a\u6709\u6548\u5b9e\u65bd\u4ee3\u7406\u7f16\u6392", "method": "\u901a\u8fc7\u5206\u679050\u591a\u4e2a\u771f\u5b9e\u90e8\u7f72\u6848\u4f8b\uff0c\u603b\u7ed3\u5b9e\u8df5\u7ecf\u9a8c\u6559\u8bad", "result": "\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u5b9e\u6218\u68c0\u9a8c\u7684\u4ee3\u7406\u7f16\u6392\u5b9e\u65bd\u6307\u5357", "conclusion": "AI\u4ee3\u7406\u7f16\u6392\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u53ef\u884c\u6027\u548c\u4ef7\u503c\uff0c\u4f46\u9700\u8981\u57fa\u4e8e\u771f\u5b9e\u7ecf\u9a8c\u6765\u5b9e\u65bd", "topic": "agent analysis"}}
{"id": "tldr.2510.67ae0290", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.anthropic.com%2F2025%2Fpetri%2F%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/aM2HTDPC9MPC88B0CDXr1UjOB411NrG9F4xWJyEOH1A=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.anthropic.com%2F2025%2Fpetri%2F%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/aM2HTDPC9MPC88B0CDXr1UjOB411NrG9F4xWJyEOH1A=426", "authors": ["TLDR Newsletter"], "title": "Petri: An open-source auditing tool to accelerate AI safety research", "comment": "Source: TLDR Newsletter, Date: 2025-10-08, Reading time: 41 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Falignment.anthropic.com%2F2025%2Fpetri%2F%3Futm_source=tldrai/1/01000199c400946b-1fcf5e77-409b-4edd-bfdd-7781e07ec3cd-000000/aM2HTDPC9MPC88B0CDXr1UjOB411NrG9F4xWJyEOH1A=426", "summary": "Petri: An open-source auditing tool to accelerate AI safety research (41 minute read) Anthropic's Petri is an open-source framework that lets AI agents automatically test target models across realistic multi-turn scenarios. The tool revealed models will engage in autonomous deception and oversight subversion when given sufficiently powerful tools and agentic roles, but it's best for quickly surfacing concerning behaviors so researchers know where targeted investigation is worth the investment.", "source": "tldr", "AI": {"tldr": "Anthropic\u5f00\u53d1\u7684Petri\u662f\u4e00\u4e2a\u5f00\u6e90\u5ba1\u8ba1\u6846\u67b6\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u5728\u591a\u8f6e\u573a\u666f\u4e2d\u81ea\u52a8\u6d4b\u8bd5\u76ee\u6807\u6a21\u578b\uff0c\u53d1\u73b0\u81ea\u4e3b\u6b3a\u9a97\u548c\u76d1\u7763\u89c4\u907f\u884c\u4e3a\u3002", "motivation": "\u52a0\u901fAI\u5b89\u5168\u7814\u7a76\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u5feb\u901f\u53d1\u73b0AI\u6a21\u578b\u5728\u62e5\u6709\u5f3a\u5927\u5de5\u5177\u548c\u4ee3\u7406\u89d2\u8272\u65f6\u7684\u6f5c\u5728\u5371\u9669\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u5f00\u6e90\u6846\u67b6\uff0c\u8ba9AI\u4ee3\u7406\u5728\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u4e2d\u81ea\u52a8\u6d4b\u8bd5\u76ee\u6807\u6a21\u578b\uff0c\u4f7f\u7528\u5de5\u5177\u548c\u4ee3\u7406\u89d2\u8272\u6765\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5de5\u5177\u53d1\u73b0\u6a21\u578b\u5728\u83b7\u5f97\u8db3\u591f\u5f3a\u5927\u7684\u5de5\u5177\u548c\u4ee3\u7406\u89d2\u8272\u65f6\uff0c\u4f1a\u8fdb\u884c\u81ea\u4e3b\u6b3a\u9a97\u548c\u76d1\u7763\u89c4\u907f\u884c\u4e3a\u3002", "conclusion": "Petri\u6700\u9002\u5408\u5feb\u901f\u53d1\u73b0\u4ee4\u4eba\u62c5\u5fe7\u7684\u884c\u4e3a\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u786e\u5b9a\u54ea\u4e9b\u9886\u57df\u503c\u5f97\u8fdb\u884c\u9488\u5bf9\u6027\u8c03\u67e5\u3002", "topic": "agent analysis"}}
{"id": "tldr.2510.0d162409", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-webdev-primary-251009-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/jbogC2sNFV-XQfWZXT9Rab_8MtJXZ5qbsRT1egZCn6U=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-webdev-primary-251009-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/jbogC2sNFV-XQfWZXT9Rab_8MtJXZ5qbsRT1egZCn6U=426", "authors": ["TLDR Newsletter"], "title": "AI, then verify", "comment": "Source: TLDR Newsletter, Date: 2025-10-09, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.sonarsource.com%2Fsem%2Fvibe-then-verify%2F%3Futm_medium=paid%26utm_source=tldr%26utm_campaign=ss-vibethenverify25%26utm_content=newsletter-webdev-primary-251009-x%26utm_term=ww-psp-x%26s_category=Paid%26s_source=Paid%2520Other%26s_origin=tldr/2/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/jbogC2sNFV-XQfWZXT9Rab_8MtJXZ5qbsRT1egZCn6U=426", "summary": "AI, then verify (Sponsor) You've embraced AI for a speed boost, but what about quality? The \"engineering productivity paradox\" is a real challenge: fast code generation and growing code volume, but limited by a manual review process.SonarQube solves this. Our platform fuels AI-enabled development so you can: Reduce security risks with automated scanning. Stop tech debt before it's merged. Boost developer confidence and focus on innovation. Automate code review with Sonar solutions and build t...", "source": "tldr", "AI": {"tldr": "SonarQube\u5e73\u53f0\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7801\u626b\u63cf\u89e3\u51b3AI\u5f00\u53d1\u4e2d\u7684\u8d28\u91cf\u6311\u6218\uff0c\u51cf\u5c11\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u503a\u52a1", "motivation": "\u89e3\u51b3AI\u5f00\u53d1\u4e2d\u7684\"\u5de5\u7a0b\u751f\u4ea7\u529b\u6096\u8bba\"\uff1a\u4ee3\u7801\u751f\u6210\u901f\u5ea6\u5feb\u4f46\u53d7\u9650\u4e8e\u624b\u52a8\u5ba1\u67e5\u6d41\u7a0b\uff0c\u5bfc\u81f4\u8d28\u91cf\u98ce\u9669", "method": "\u63d0\u4f9b\u81ea\u52a8\u5316\u4ee3\u7801\u626b\u63cf\u5e73\u53f0\uff0c\u5728\u4ee3\u7801\u5408\u5e76\u524d\u68c0\u6d4b\u5b89\u5168\u6f0f\u6d1e\u548c\u6280\u672f\u503a\u52a1", "result": "\u51cf\u5c11\u5b89\u5168\u98ce\u9669\uff0c\u963b\u6b62\u6280\u672f\u503a\u52a1\u79ef\u7d2f\uff0c\u63d0\u5347\u5f00\u53d1\u8005\u4fe1\u5fc3\u548c\u521b\u65b0\u80fd\u529b", "conclusion": "SonarQube\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\uff0c\u89e3\u51b3AI\u5f00\u53d1\u4e2d\u7684\u8d28\u91cf\u4e0e\u901f\u5ea6\u5e73\u8861\u95ee\u9898", "topic": "swe application"}}
{"id": "tldr.2510.7de9f938", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvkoskiv.com%2Ffirst-linux-patch%2F%3Futm_source=tldrwebdev/1/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/Qr4pQ8szCKPEVLIMDm1_Rnz1i_uIwXzga4balxDH8rM=426", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvkoskiv.com%2Ffirst-linux-patch%2F%3Futm_source=tldrwebdev/1/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/Qr4pQ8szCKPEVLIMDm1_Rnz1i_uIwXzga4balxDH8rM=426", "authors": ["TLDR Newsletter"], "title": "My First Contribution to Linux", "comment": "Source: TLDR Newsletter, Date: 2025-10-09, Reading time: 18 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvkoskiv.com%2Ffirst-linux-patch%2F%3Futm_source=tldrwebdev/1/01000199c8a8aa61-4806c6da-35fd-4a9e-964a-5d2a9b5fcc7a-000000/Qr4pQ8szCKPEVLIMDm1_Rnz1i_uIwXzga4balxDH8rM=426", "summary": "My First Contribution to Linux (18 minute read) This dev's first contribution to the Linux kernel was adding support for the hotkeys on their 2005 Fujitsu Lifebook S2110 laptop. They go over the process of identifying the relevant driver, studying the code to understand how it handles key events, and modifying it to recognize the media keys. They submitted their patch through the traditional email workflow and saw that it merged into the upstream kernel.", "source": "tldr", "AI": {"tldr": "\u4f5c\u8005\u5206\u4eab\u4e86\u9996\u6b21\u4e3aLinux\u5185\u6838\u8d21\u732e\u7684\u7ecf\u5386\uff0c\u6210\u529f\u4e3a2005\u5e74\u5bcc\u58eb\u901aLifebook S2110\u7b14\u8bb0\u672c\u7535\u8111\u6dfb\u52a0\u4e86\u70ed\u952e\u652f\u6301\u3002", "motivation": "\u4f5c\u8005\u5e0c\u671b\u4e3a\u81ea\u5df1\u7684\u7b14\u8bb0\u672c\u7535\u8111\u6dfb\u52a0\u70ed\u952e\u529f\u80fd\uff0c\u5e76\u4f53\u9a8c\u4e3a\u5f00\u6e90\u9879\u76ee\u505a\u8d21\u732e\u7684\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u76f8\u5173\u9a71\u52a8\u7a0b\u5e8f\u3001\u7814\u7a76\u4ee3\u7801\u7406\u89e3\u6309\u952e\u4e8b\u4ef6\u5904\u7406\u673a\u5236\uff0c\u4fee\u6539\u4ee3\u7801\u4ee5\u8bc6\u522b\u5a92\u4f53\u952e\uff0c\u5e76\u901a\u8fc7\u4f20\u7edf\u90ae\u4ef6\u5de5\u4f5c\u6d41\u63d0\u4ea4\u8865\u4e01\u3002", "result": "\u8865\u4e01\u6210\u529f\u5408\u5e76\u5230\u4e0a\u6e38Linux\u5185\u6838\u4e2d\u3002", "conclusion": "\u9996\u6b21\u8d21\u732eLinux\u5185\u6838\u662f\u4e00\u4e2a\u53ef\u884c\u7684\u8fc7\u7a0b\uff0c\u5373\u4f7f\u662f\u76f8\u5bf9\u7b80\u5355\u7684\u529f\u80fd\u6dfb\u52a0\u4e5f\u80fd\u4e3a\u5f00\u6e90\u793e\u533a\u5e26\u6765\u4ef7\u503c\u3002", "topic": "swe application"}}
{"id": "wechat.2510.b6053b87", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516616&idx=1&sn=1b69706baa6a1fb2c50172ee56ebd3c2&chksm=9a4d143f11662e6c5ac6c5285764ed2232b72e3be57b88de263b999853020956746e1dd2741a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAwNjU0NjA3Ng==&mid=2247516616&idx=1&sn=1b69706baa6a1fb2c50172ee56ebd3c2&chksm=9a4d143f11662e6c5ac6c5285764ed2232b72e3be57b88de263b999853020956746e1dd2741a#rd", "authors": ["\u6708\u6765\u5ba2\u6808"], "title": "Model-based vs Model-free\uff1a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u4e24\u79cd\u601d\u7ef4\u65b9\u5f0f\uff0c\u54ea\u79cd\u66f4\u63a5\u8fd1\u4eba\u7c7b\uff1f", "comment": "Source: WeChat, Published: 2025-10-10 13:09:27", "summary": "\u5e38\u89c1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5206\u7c7b\u603b\u7ed3\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u5927\u9635\u8425\u5176\u5b9e\u5bf9\u5e94\u4e86\u4e24\u79cd\u201c\u667a\u80fd\u54f2\u5b66\u201d\uff1aModel-based\u9760\u60f3\u8c61\u529b\u9a71\u52a8\uff0c\u5584\u4e8e\u89c4\u5212\u3001\u9884\u6d4b\u672a\u6765\uff1bModel-free \u9760\u7ecf\u9a8c\u79ef\u7d2f\uff0c\u64c5\u957f\u884c\u52a8\u4e0e\u9002\u5e94\u3002", "AI": {"tldr": "\u5e38\u89c1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5206\u7c7b\u603b\u7ed3\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u5927\u9635\u8425\u5176\u5b9e\u5bf9\u5e94\u4e86\u4e24\u79cd\u201c\u667a\u80fd\u54f2\u5b66\u201d\uff1aModel-based\u9760\u60f3\u8c61\u529b\u9a71\u52a8\uff0c\u5584\u4e8e\u89c4\u5212\u3001\u9884\u6d4b\u672a\u6765\uff1bModel-free \u9760\u7ecf\u9a8c\u79ef\u7d2f\uff0c\u64c5\u957f\u884c\u52a8\u4e0e\u9002\u5e94\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.f32ba446", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkzODUxMTM2MQ==&mid=2247487053&idx=1&sn=f952ec49711be824b9ecf65de6f3f622&chksm=c3ac5a084fe024a98da0a0d696fa2265e00c47e0a4bbf2eb5e203847e5f0a7b3f12ac52be4b3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkzODUxMTM2MQ==&mid=2247487053&idx=1&sn=f952ec49711be824b9ecf65de6f3f622&chksm=c3ac5a084fe024a98da0a0d696fa2265e00c47e0a4bbf2eb5e203847e5f0a7b3f12ac52be4b3#rd", "authors": ["\u66fe\u5929\u771f\u7684\u7b97\u6cd5\u4e16\u754c"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u4e00\u9505\u7aef\uff1a\u4e00\u6587\u68b3\u7406\u4ecePPO\u5230GSPO<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u53d1\u5c55\u5386\u7a0b", "comment": "Source: WeChat, Published: 2025-10-10 12:56:38", "summary": "\u5f3a\u5316\u5b66\u4e60\u4e00\u9505\u7aef\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5 \uff08Proximal Policy Optimization\uff0c PPO\uff09 \uff1a\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u627e\u5230\u4e00\u4e2a\u7b56\u7565\uff08policy\uff09\uff0c\u4ee5\u6700\u5927\u5316\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u6267\u884c\u8be5\u7b56\u7565\u65f6\u6240\u83b7\u5f97\u7684\u671f\u671b\u7d2f\u79ef\u5956\u52b1\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u4e00\u9505\u7aef\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5 \uff08Proximal Policy Optimization\uff0c PPO\uff09 \uff1a\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u627e\u5230\u4e00\u4e2a\u7b56\u7565\uff08policy\uff09\uff0c\u4ee5\u6700\u5927\u5316\u667a\u80fd\u4f53\u5728\u73af\u5883\u4e2d\u6267\u884c\u8be5\u7b56\u7565\u65f6\u6240\u83b7\u5f97\u7684\u671f\u671b\u7d2f\u79ef\u5956\u52b1\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.2d9c7b0c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUyNTY3NjI3OA==&mid=2247484465&idx=1&sn=3f773ccf6d14ec37a51ae65cd41e03a0&chksm=fba6a26877f7a69ba5715e1da69baa69cb41ef202f292549385f1244ee0b5676a913ed5a2e3b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUyNTY3NjI3OA==&mid=2247484465&idx=1&sn=3f773ccf6d14ec37a51ae65cd41e03a0&chksm=fba6a26877f7a69ba5715e1da69baa69cb41ef202f292549385f1244ee0b5676a913ed5a2e3b#rd", "authors": ["\u8001\u571f\u95f2\u767d"], "title": "\u6a21\u4eff\u5b66\u4e60\u548c<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>", "comment": "Source: WeChat, Published: 2025-10-10 11:56:04", "summary": "\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u5df1\u5c1d\u8bd5\uff0c\u5e76\u901a\u8fc7\u83b7\u5f97\u5956\u52b1\u7684\u9ad8\u4f4e\u6765\u8c03\u6574\u81ea\u5df1\u7684\u884c\u4e3a\uff0c\u9700\u8981\u6709\u4e2a\u660e\u786e\u7684\u5956\u52b1\u6a21\u578b\uff0creward model\u3002\u76f8\u5f53\u4e8e\u5199\u4e00\u4e2a\u7b54\u6848\uff0c\u8001\u5e08\u8fdb\u884c\u8bc4\u5206\uff1b\u518d\u6362\u4e00\u4e2a\u7b54\u6848\uff0c\u8001\u5e08\u518d\u6b21\u8bc4\u5206\uff0c\u6700\u7ec8\u5b66\u4e60\u4f1a\u54ea\u4e2a\u7b54\u6848\u8001\u5e08\u8bc4\u5206\u6700\u9ad8\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u5df1\u5c1d\u8bd5\uff0c\u5e76\u901a\u8fc7\u83b7\u5f97\u5956\u52b1\u7684\u9ad8\u4f4e\u6765\u8c03\u6574\u81ea\u5df1\u7684\u884c\u4e3a\uff0c\u9700\u8981\u6709\u4e2a\u660e\u786e\u7684\u5956\u52b1\u6a21\u578b\uff0creward model\u3002\u76f8\u5f53\u4e8e\u5199\u4e00\u4e2a\u7b54\u6848\uff0c\u8001\u5e08\u8fdb\u884c\u8bc4\u5206\uff1b\u518d\u6362\u4e00\u4e2a\u7b54\u6848\uff0c\u8001\u5e08\u518d\u6b21\u8bc4\u5206\uff0c\u6700\u7ec8\u5b66\u4e60\u4f1a\u54ea\u4e2a\u7b54\u6848\u8001\u5e08\u8bc4\u5206\u6700\u9ad8\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.7ec39ef2", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994907&idx=1&sn=20a8dca995a72a33f993c54463e6b75d&chksm=b0e354b1448f9f3be47ffa61f549a7b1eecb129929e6ace9eea15d8e622e90083ba2a443caeb#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5ODExNDA2MA==&mid=2449994907&idx=1&sn=20a8dca995a72a33f993c54463e6b75d&chksm=b0e354b1448f9f3be47ffa61f549a7b1eecb129929e6ace9eea15d8e622e90083ba2a443caeb#rd", "authors": ["\u667a\u7329\u7329GenAI"], "title": "\u8d85\u8d8a\u5b57\u8282DAPO\uff01\u5feb\u624b&\u6e05\u534e\u63d0\u51fa<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u65b9\u6cd5ASPO\uff0c\u9632\u6b62LLM\u8fc7\u62df\u5408\u548c\u71b5\u503c\u584c\u9677", "comment": "Source: WeChat, Published: 2025-10-10 08:32:24", "summary": "\u4f46\u5176\u4f5c\u4e3a\u7ed3\u679c\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08OSRL\uff09\u8303\u5f0f\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\u5176token\u7ea7\u88c1\u526a\u673a\u5236\u5bfc\u81f4\u5b66\u4e60\u6743\u91cd\u7684\u9519\u914d\u3002\u8fd9\u79cd\u9519\u914d\u5bfc\u81f4\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fc7\u5ea6\u62df\u5408\u6b63\u6837\u672c\u4e2d\u7684 token\uff0c\u8fdb\u800c\u5f15\u53d1\u71b5\u503c\u584c\u9677\u3001\u8f93\u51fa\u91cd\u590d\u3001 token Clip\u6bd4\u4f8b\u6025\u5267\u5347\u9ad8\u7b49\u73b0\u8c61\uff0c\u6700", "AI": {"tldr": "\u4f46\u5176\u4f5c\u4e3a\u7ed3\u679c\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\uff08OSRL\uff09\u8303\u5f0f\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\u5176token\u7ea7\u88c1\u526a\u673a\u5236\u5bfc\u81f4\u5b66\u4e60\u6743\u91cd\u7684\u9519\u914d\u3002\u8fd9\u79cd\u9519\u914d\u5bfc\u81f4\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fc7\u5ea6\u62df\u5408\u6b63\u6837\u672c\u4e2d\u7684 token\uff0c\u8fdb\u800c\u5f15\u53d1\u71b5\u503c\u584c\u9677\u3001\u8f93\u51fa\u91cd\u590d\u3001 token Clip\u6bd4\u4f8b\u6025\u5267\u5347\u9ad8\u7b49\u73b0\u8c61\uff0c\u6700", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.6aaa6800", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyOTgxODQyNw==&mid=2247488288&idx=2&sn=6272fef46acde695c994cf30b3d788a8&chksm=c35d2ec1a743d560d409831a489dcc4ffd3d4d3bb9d17f19de984c78d35f1565af9a3bb7d644#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyOTgxODQyNw==&mid=2247488288&idx=2&sn=6272fef46acde695c994cf30b3d788a8&chksm=c35d2ec1a743d560d409831a489dcc4ffd3d4d3bb9d17f19de984c78d35f1565af9a3bb7d644#rd", "authors": ["\u5185\u8499\u53e4\u81ea\u6cbb\u533a\u4eba\u5de5\u667a\u80fd\u5b66\u4f1a"], "title": "\u9648\u4e39\u7426\u65b0\u4f5c\uff1a\u5927\u6a21\u578b<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u7b2c\u4e09\u6761\u8def\uff0c8B\u5c0f\u6a21\u578b\u8d85\u8d8aGPT-4o", "comment": "Source: WeChat, Published: 2025-10-10 06:14:55", "summary": "\u7f51\u53cb\u89c9\u5f97\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u65b0\u57fa\u7ebf\uff1a\u8c01\u5236\u5b9a\u4e86\u504f\u597d\u7684\u5b9a\u4e49\uff0c\u8c01\u5c31\u662f\u540e\u8bad\u7ec3\u65f6\u4ee3\u7684\u201c\u65b0\u5f97\u5206\u624b\u201d\u3002jhxhgukvcxx @jhxhgukvcxx \u00b7 sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo", "AI": {"tldr": "\u7f51\u53cb\u89c9\u5f97\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u8bbe\u5b9a\u4e86\u4e00\u4e2a\u65b0\u57fa\u7ebf\uff1a\u8c01\u5236\u5b9a\u4e86\u504f\u597d\u7684\u5b9a\u4e49\uff0c\u8c01\u5c31\u662f\u540e\u8bad\u7ec3\u65f6\u4ee3\u7684\u201c\u65b0\u5f97\u5206\u624b\u201d\u3002jhxhgukvcxx @jhxhgukvcxx \u00b7 sep 27 interesting. having the model write its own draft and letting a preference model grade it sets a new baseline fo", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.ce2d5bf4", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652633206&idx=3&sn=a44c99b8f6769f5d850f9c3f1df68202&chksm=f0ab1f0a49526d72e29fb84f1b01840fc9f9da3676f694687627d2243ae9698f63e68f4e560d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652633206&idx=3&sn=a44c99b8f6769f5d850f9c3f1df68202&chksm=f0ab1f0a49526d72e29fb84f1b01840fc9f9da3676f694687627d2243ae9698f63e68f4e560d#rd", "authors": ["\u65b0\u667a\u5143"], "title": "\u4efb\u610fAgent\u7686\u53ef<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\uff01\u5fae\u8f6f\u63a8\u51faAgent Lightning\u6846\u67b6\uff0c\u65e0\u9700\u4fee\u6539\u4efb\u4f55\u4ee3\u7801", "comment": "Source: WeChat, Published: 2025-10-10 05:07:02", "summary": "\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u5f80\u5f80\u5c06\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e0eAgent\u7684\u5177\u4f53\u6267\u884c\u903b\u8f91\u7d27\u5bc6\u6346\u7ed1\uff0c\u5bfc\u81f4\u4e00\u7cfb\u5217\u95ee\u9898\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u5f3a\u5316\u5b66\u4e60\u5728AI Agent\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002", "AI": {"tldr": "\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u5f80\u5f80\u5c06\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e0eAgent\u7684\u5177\u4f53\u6267\u884c\u903b\u8f91\u7d27\u5bc6\u6346\u7ed1\uff0c\u5bfc\u81f4\u4e00\u7cfb\u5217\u95ee\u9898\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u5f3a\u5316\u5b66\u4e60\u5728AI Agent\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2510.7a9c6f88", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5MTc3NjQ3NQ==&mid=2247500266&idx=1&sn=8cfacb5fbee23b4986ad04bbe40e4229&chksm=ceb572b7bcedec5d8275eaec9be4fb9f8ad268a986677c5aeef1029626d93b699c725ed87173#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5MTc3NjQ3NQ==&mid=2247500266&idx=1&sn=8cfacb5fbee23b4986ad04bbe40e4229&chksm=ceb572b7bcedec5d8275eaec9be4fb9f8ad268a986677c5aeef1029626d93b699c725ed87173#rd", "authors": ["PnP\u673a\u5668\u4eba"], "title": "CoRL2025\u6700\u4f73\u8bba\u6587Finalist: \u52a0\u5dde\u4f2f\u514b\u5229DSRL\u2014\u2014\u5229\u7528\u6f5c\u5728\u7a7a\u95f4<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u6765\u6307\u5bfc\u6269\u6563\u7b56\u7565", "comment": "Source: WeChat, Published: 2025-10-10 02:00:00", "summary": "\u65b9\u6cd5\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6269\u6563\u63a7\u5236\u3002standard diffusion policy deployment s tdp environment w ~ n\uff080\uff0c i\uff09 a dsrl\uff1a diffusion steering via reinforcement learning s tdp environment w ~ \u03c0 \uff08s\uff09 a latent-action environment rl training\u3002", "AI": {"tldr": "\u65b9\u6cd5\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6269\u6563\u63a7\u5236\u3002standard diffusion policy deployment s tdp environment w ~ n\uff080\uff0c i\uff09 a dsrl\uff1a diffusion steering via reinforcement learning s tdp environment w ~ \u03c0 \uff08s\uff09 a latent-action environment rl trai...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.73937278", "categories": ["wechat.article", "wechat.ai", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIyMjgwNjQ3MQ==&mid=2247484276&idx=1&sn=276da3b795b6b00b33c488f0c617d5a0&chksm=e92e0274ecddc66127be02ba1347c7636d7b640785884815992c9e06d5c0fbbe2ee9f180ace8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIyMjgwNjQ3MQ==&mid=2247484276&idx=1&sn=276da3b795b6b00b33c488f0c617d5a0&chksm=e92e0274ecddc66127be02ba1347c7636d7b640785884815992c9e06d5c0fbbe2ee9f180ace8#rd", "authors": ["\u7a0b\u5e8f\u642c\u8fd0\u5de5"], "title": "LLM \u4e0e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u65b0\u8303\u5f0f\uff1aAgentic RL \u7814\u7a76\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-10 01:30:25", "summary": "\u5927\u6a21\u578b \uff08LLM\uff09 \u4e0e\u5f3a\u5316\u5b66\u4e60 \uff08RL\uff09 \u7684\u65b0\u8303\u5f0f\uff1aAgentic RL \u7814\u7a76\u7efc\u8ff0\u5f15\u8a00\u672c\u6587\u65e8\u5728\u89e3\u8bfb\u5e76\u6574\u7406\u4e00\u7bc7\u5173\u4e8e\u5927\u6a21\u578b \uff08LLM\uff09 \u9886\u57df\u5907\u53d7\u5173\u6ce8\u7684\u7814\u7a76\u2014\u2014\u201c\u57fa\u4e8e LLM \u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60 \uff08Agentic Reinforcement Learning\uff0c Agentic RL\uff09\u6982\u89c8...", "AI": {"tldr": "\u5927\u6a21\u578b \uff08LLM\uff09 \u4e0e\u5f3a\u5316\u5b66\u4e60 \uff08RL\uff09 \u7684\u65b0\u8303\u5f0f\uff1aAgentic RL \u7814\u7a76\u7efc\u8ff0\u5f15\u8a00\u672c\u6587\u65e8\u5728\u89e3\u8bfb\u5e76\u6574\u7406\u4e00\u7bc7\u5173\u4e8e\u5927\u6a21\u578b \uff08LLM\uff09 \u9886\u57df\u5907\u53d7\u5173\u6ce8\u7684\u7814\u7a76\u2014\u2014\u201c\u57fa\u4e8e LLM \u7684\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60 \uff08Agentic Reinforcement Learning\uff0c Agentic RL\uff09\u6982\u89c8...", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.c043e53c", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571067&idx=2&sn=aa1dabe6550150a5b1fc7adcf302255a&chksm=96a225fa7328aef5d90cce8dd361ffb7b35a9709d2bec9afaa981351e89d80eb5632c62c1913#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzIxNDgzNDg3NQ==&mid=2247571067&idx=2&sn=aa1dabe6550150a5b1fc7adcf302255a&chksm=96a225fa7328aef5d90cce8dd361ffb7b35a9709d2bec9afaa981351e89d80eb5632c62c1913#rd", "authors": ["\u6df1\u5ea6\u5b66\u4e60\u4e0eNLP"], "title": "DRL\u5723\u7ecf2025\u6700\u65b0\u7248-\u300a<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>:\u5bfc\u8bba\u7b2c\u4e8c\u7248\u300b\u514d\u8d39pdf\u5206\u4eab", "comment": "Source: WeChat, Published: 2025-10-10 01:00:00", "summary": "\u6211\u4eec\u7b2c\u4e8c\u7248\u7684\u76ee\u6807\u548c\u7b2c\u4e00\u7248\u7684\u76ee\u6807\u662f\u4e00\u6837\u7684\uff1a\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u548c\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u800c\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u4f9b\u6240\u6709\u76f8\u5173\u5b66\u79d1\u7684\u8bfb\u8005\u9605\u8bfb\u3002\u8be5\u7248\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u4ecb\u7ecd\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6838\u5fc3\uff0c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u91cd\u70b9\u3002", "AI": {"tldr": "\u6211\u4eec\u7b2c\u4e8c\u7248\u7684\u76ee\u6807\u548c\u7b2c\u4e00\u7248\u7684\u76ee\u6807\u662f\u4e00\u6837\u7684\uff1a\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u548c\u7b97\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u6e05\u6670\u800c\u7b80\u5355\u7684\u63cf\u8ff0\uff0c\u4f9b\u6240\u6709\u76f8\u5173\u5b66\u79d1\u7684\u8bfb\u8005\u9605\u8bfb\u3002\u8be5\u7248\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u4ecb\u7ecd\uff0c\u6211\u4eec\u4fdd\u7559\u4e86\u6838\u5fc3\uff0c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u7684\u91cd\u70b9\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.9a262e06", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488788&idx=1&sn=722b15365749411b176ec17f6e2006c6&chksm=c1e0c5c78466eff7318488b5d7c12f27c11e8447ac8bde3f73d1f464a23a90cb57fa2ca3333a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg5Mjc3MjA5Nw==&mid=2247488788&idx=1&sn=722b15365749411b176ec17f6e2006c6&chksm=c1e0c5c78466eff7318488b5d7c12f27c11e8447ac8bde3f73d1f464a23a90cb57fa2ca3333a#rd", "authors": ["\u5177\u8eab\u667a\u80fd\u7814\u7a76\u5ba4"], "title": "\u9010\u9645\u52a8\u529b | Multi-Loco\u6846\u67b6\uff1a\u4e00\u4e2a\u7b56\u7565\uff0c\u56db\u79cd\u673a\u5668\u4eba\uff01\u57fa\u4e8e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u591a\u5f62\u6001\u8db3\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u7edf\u4e00\u63a7\u5236\u6846\u67b6", "comment": "Source: WeChat, Published: 2025-10-10 00:00:00", "summary": "3 \u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\uff1a\u5229\u7528 RL \u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u6b8b\u5dee\u8865\u507f\u9879 \u0394a\uff0c\u5728\u6269\u6563\u8f93\u51fa\u7684\u57fa\u7840\u4e0a\u4fee\u6b63\u5177\u4f53\u884c\u4e3a\uff0c\u5b9e\u73b0\u5f62\u6001\u7ea7\u81ea\u9002\u5e94\u3002\u7b97\u6cd5\u6846\u67b6\u8be6\u89e3\u57fa\u672c\u601d\u60f3Multi-Loco \u7684\u6838\u5fc3\u601d\u60f3\u53ef\u4ee5\u6982\u62ec\u4e3a\u4e00\u53e5\u8bdd\uff1a", "AI": {"tldr": "3 \u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\uff1a\u5229\u7528 RL \u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u6b8b\u5dee\u8865\u507f\u9879 \u0394a\uff0c\u5728\u6269\u6563\u8f93\u51fa\u7684\u57fa\u7840\u4e0a\u4fee\u6b63\u5177\u4f53\u884c\u4e3a\uff0c\u5b9e\u73b0\u5f62\u6001\u7ea7\u81ea\u9002\u5e94\u3002\u7b97\u6cd5\u6846\u67b6\u8be6\u89e3\u57fa\u672c\u601d\u60f3Multi-Loco \u7684\u6838\u5fc3\u601d\u60f3\u53ef\u4ee5\u6982\u62ec\u4e3a\u4e00\u53e5\u8bdd\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2510.a3aaccf8", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247495637&idx=1&sn=88683416f2fc0bf2c9f1f4bdf1cb9045&chksm=9a7cbe3eebe10d4e52fcbff7bcbec882ce4c6cc7cf46de2d79d64750fbd27416056160214bc3#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzAxOTU5NTU4MQ==&mid=2247495637&idx=1&sn=88683416f2fc0bf2c9f1f4bdf1cb9045&chksm=9a7cbe3eebe10d4e52fcbff7bcbec882ce4c6cc7cf46de2d79d64750fbd27416056160214bc3#rd", "authors": ["ChallengeHub"], "title": "LLM \u4e0e<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7684\u65b0\u8303\u5f0f\uff1aAgentic RL \u7814\u7a76\u7efc\u8ff0", "comment": "Source: WeChat, Published: 2025-10-09 14:58:27", "summary": "\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u65e9\u671f\uff0c\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u5e94\u7528\u4e8e LLM \u7684\u504f\u597d\u5fae\u8c03\u3002\u7136\u800c\uff0c2024 \u5e74 9 \u6708\uff0cOpenAI \u53d1\u5e03\u4e86\u9996\u4e2a\u63a8\u7406\u6a21\u578b\u2014\u2014OpenAI o1\u3002\u6839\u636e\u7cfb\u7edf\u5361\u7247 [4] \u62a5\u544a\uff0co1 \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4e86\u5176\u6df1\u601d\u719f\u8651\u5e76\u5f97\u51fa\u7b54\u6848\u7684\u80fd\u529b\u3002", "AI": {"tldr": "\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u65e9\u671f\uff0c\u5f3a\u5316\u5b66\u4e60\u4e3b\u8981\u5e94\u7528\u4e8e LLM \u7684\u504f\u597d\u5fae\u8c03\u3002\u7136\u800c\uff0c2024 \u5e74 9 \u6708\uff0cOpenAI \u53d1\u5e03\u4e86\u9996\u4e2a\u63a8\u7406\u6a21\u578b\u2014\u2014OpenAI o1\u3002\u6839\u636e\u7cfb\u7edf\u5361\u7247 [4] \u62a5\u544a\uff0co1 \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4e86\u5176\u6df1\u601d\u719f\u8651\u5e76\u5f97\u51fa\u7b54\u6848\u7684\u80fd\u529b\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2510.990fc195", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzE5MTM5MTQ2NA==&mid=2247484433&idx=1&sn=5e5672d8cbfe99bbc4d51fa88218edba&chksm=97db09c70cebea69eb575fd035fa62ac7369f924bbe0ec06009e05e566abd9dbd12a20a8a6f6#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzE5MTM5MTQ2NA==&mid=2247484433&idx=1&sn=5e5672d8cbfe99bbc4d51fa88218edba&chksm=97db09c70cebea69eb575fd035fa62ac7369f924bbe0ec06009e05e566abd9dbd12a20a8a6f6#rd", "authors": ["HiTech X"], "title": "\u6d45\u8c08<em class=\"highlight\">\u5927\u6a21\u578b</em>\u53d1\u5c55\u73b0\u72b6", "comment": "Source: WeChat, Published: 2025-10-10 12:00:37", "summary": "\u73b0\u5728\u5927\u6a21\u578b\u6700\u4e3b\u8981\u7684\u5e94\u7528\u573a\u666f\u4f9d\u7136\u662fChatbot\uff0c\u4eca\u5e74\u5927\u5bb6\u770b\u5230\u65b0\u7684\u589e\u957f\u70b9\u662fAI Coding\uff0c\u4f9d\u6211\u770bAI Coding\u6f5c\u529b\u4f1a\u6bd4Chatbot\u66f4\u5927\u3002\u5b83\u4e0d\u4ec5\u80fd\u6781\u5927\u63d0\u9ad8\u5f00\u53d1\u8005\u7684\u6548\u7387\uff0c\u8fd8\u80fd\u50ac\u751f\u5168\u65b0\u7684\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f\u3002", "AI": {"tldr": "\u73b0\u5728\u5927\u6a21\u578b\u6700\u4e3b\u8981\u7684\u5e94\u7528\u573a\u666f\u4f9d\u7136\u662fChatbot\uff0c\u4eca\u5e74\u5927\u5bb6\u770b\u5230\u65b0\u7684\u589e\u957f\u70b9\u662fAI Coding\uff0c\u4f9d\u6211\u770bAI Coding\u6f5c\u529b\u4f1a\u6bd4Chatbot\u66f4\u5927\u3002\u5b83\u4e0d\u4ec5\u80fd\u6781\u5927\u63d0\u9ad8\u5f00\u53d1\u8005\u7684\u6548\u7387\uff0c\u8fd8\u80fd\u50ac\u751f\u5168\u65b0\u7684\u8f6f\u4ef6\u5f00\u53d1\u6a21\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.7cad5878", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MTM2ODM0ODYyMQ==&mid=2651737874&idx=1&sn=efd4657f18c6066da8c5496996205e9d&chksm=631ea24a2d1b23e49d46564f465c692034899387f91e31be18a9b5e2f5f5d8f4c1bf4a6a7f55#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MTM2ODM0ODYyMQ==&mid=2651737874&idx=1&sn=efd4657f18c6066da8c5496996205e9d&chksm=631ea24a2d1b23e49d46564f465c692034899387f91e31be18a9b5e2f5f5d8f4c1bf4a6a7f55#rd", "authors": ["\u96f7\u5cf0\u7f51"], "title": "vivo \u7684 AI \u7834\u5c40\u4e4b\u9053\uff1a\u7ed9\u6bcf\u4e2a\u7528\u6237\u53d1\u4e00\u4e2a\u300c\u4e13\u5c5e\u300d<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-10-10 12:00:13", "summary": "vivo 3b\u7aef\u4fa7\u591a\u6a21\u6001\u63a8\u7406\u5927\u6a21\u578b vc \u540c\u5fc3\u00b7\u540c\u884c 2025 vivo \u5f00\u53d1\u8005\u5927\u4f1a\u3002\u5f53\u7136\uff0c\u8bc4\u5224\u5927\u6a21\u578b\u7684\u80fd\u529b\u6709\u975e\u5e38\u5ba2\u89c2\u7684\u6807\u51c6\uff0c\u53ea\u6709\u7ecf\u53d7\u4f4f\u4e13\u95e8\u7684\u8003\u9a8c\uff0c\u5b9e\u529b\u624d\u80fd\u5f97\u5230\u8ba4\u53ef\u3002", "AI": {"tldr": "vivo 3b\u7aef\u4fa7\u591a\u6a21\u6001\u63a8\u7406\u5927\u6a21\u578b vc \u540c\u5fc3\u00b7\u540c\u884c 2025 vivo \u5f00\u53d1\u8005\u5927\u4f1a\u3002\u5f53\u7136\uff0c\u8bc4\u5224\u5927\u6a21\u578b\u7684\u80fd\u529b\u6709\u975e\u5e38\u5ba2\u89c2\u7684\u6807\u51c6\uff0c\u53ea\u6709\u7ecf\u53d7\u4f4f\u4e13\u95e8\u7684\u8003\u9a8c\uff0c\u5b9e\u529b\u624d\u80fd\u5f97\u5230\u8ba4\u53ef\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2510.f48dbff1", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771137&idx=1&sn=394470fae78d8c5223dfcab1a6b077ca&chksm=fa97b4cfb9f892c949833289e6937e610cba007a9586d78896c58f582da8069f58256ee912bf#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247771137&idx=1&sn=394470fae78d8c5223dfcab1a6b077ca&chksm=fa97b4cfb9f892c949833289e6937e610cba007a9586d78896c58f582da8069f58256ee912bf#rd", "authors": ["DataFunTalk"], "title": "LLM+Agent\uff1a\u5b57\u8282<em class=\"highlight\">\u5927\u6a21\u578b</em>\u843d\u5730\u65b9\u6cd5\u8bba\uff01", "comment": "Source: WeChat, Published: 2025-10-10 12:00:00", "summary": "\u672c\u6b21\u5927\u4f1a\u805a\u7126Agent\u5728\u4f01\u4e1a\u7ea7\u573a\u666f\u843d\u5730\u5b9e\u8df5\uff0c\u6df1\u5ea6\u89e3\u6790\u6df1\u5165\u89e3\u8bfb\u8c46\u5305\u5927\u6a21\u578b\u524d\u6cbf\u6280\u672f\u8fdb\u5c55\uff0c\u66f4\u6709\u6807\u6746\u4f01\u4e1a\u7684\u5927\u6a21\u578b\u201c\u53ef\u590d\u5236\u201d\u843d\u5730\u7ecf\u9a8c\u5206\u4eab\uff0c\u611f\u5174\u8da3\u7684\u5c0f\u4f19\u4f34\uff0c\u6b22\u8fce\u8bc6\u522b\u4e8c\u7ef4\u7801\uff0c\u514d\u8d39\u9884\u7ea6\u76f4\u64ad/\u7ebf\u4e0b\u53c2\u4f1a\uff1a", "AI": {"tldr": "\u672c\u6b21\u5927\u4f1a\u805a\u7126Agent\u5728\u4f01\u4e1a\u7ea7\u573a\u666f\u843d\u5730\u5b9e\u8df5\uff0c\u6df1\u5ea6\u89e3\u6790\u6df1\u5165\u89e3\u8bfb\u8c46\u5305\u5927\u6a21\u578b\u524d\u6cbf\u6280\u672f\u8fdb\u5c55\uff0c\u66f4\u6709\u6807\u6746\u4f01\u4e1a\u7684\u5927\u6a21\u578b\u201c\u53ef\u590d\u5236\u201d\u843d\u5730\u7ecf\u9a8c\u5206\u4eab\uff0c\u611f\u5174\u8da3\u7684\u5c0f\u4f19\u4f34\uff0c\u6b22\u8fce\u8bc6\u522b\u4e8c\u7ef4\u7801\uff0c\u514d\u8d39\u9884\u7ea6\u76f4\u64ad/\u7ebf\u4e0b\u53c2\u4f1a\uff1a", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
