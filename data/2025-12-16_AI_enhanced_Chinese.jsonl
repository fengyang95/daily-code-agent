{"id": "2512.11835", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.11835", "abs": "https://arxiv.org/abs/2512.11835", "authors": ["Seyma Yaman Kayadibi"], "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models", "comment": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)", "summary": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.", "AI": {"tldr": "\u57fa\u4e8e\u4eba\u5de5\u5e74\u9f84\u8bc4\u5206(AAS)\u6784\u5efa\u53ef\u6267\u884c\u7684\u5355\u5b50\u6761\u6b3e\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u4e2aPython\u5b9e\u73b0\u7ea6\u675fLLM\u5185\u5b58\u548c\u63a7\u5236\uff0c\u63d0\u4f9b\u900f\u660e\u7684\u4eba\u5de5\u667a\u80fd\u5185\u90e8\u52a8\u6001\u5206\u6790\u84dd\u56fe\u3002", "motivation": "LLM\u901a\u5e38\u4f5c\u4e3a\u5f3a\u5927\u4f46\u4e0d\u900f\u660e\u7684\u7cfb\u7edf\u90e8\u7f72\uff0c\u7f3a\u4e4f\u5bf9\u5185\u90e8\u8bb0\u5fc6\u548c\"\u81ea\u6211\"\u884c\u4e3a\u7684\u53ef\u5ba1\u8ba1\u539f\u5219\u6027\u6cbb\u7406\u65b9\u6cd5\u3002\u9700\u8981\u5efa\u7acb\u53ef\u6267\u884c\u7684\u89c4\u8303\u6765\u7ea6\u675fLLM\u5185\u5b58\u548c\u63a7\u5236\u3002", "method": "\u4ece\u83b1\u5e03\u5c3c\u8328\u300a\u5355\u5b50\u8bba\u300b\u4e2d\u9009\u53d620\u4e2a\u5355\u5b50\uff0c\u5206\u4e3a\u516d\u4e2a\u675f\uff1a\u672c\u4f53\u8bba\u3001\u52a8\u529b\u5b66\u3001\u8868\u5f81\u4e0e\u610f\u8bc6\u3001\u548c\u8c10\u4e0e\u7406\u6027\u3001\u8eab\u4f53\u4e0e\u7ec4\u7ec7\u3001\u76ee\u7684\u8bba\u3002\u57fa\u4e8eAAS\u5185\u6838\u5b9e\u73b0\u4e3a\u53ef\u6267\u884c\u89c4\u8303\uff0c\u901a\u8fc7\u516d\u4e2aPython\u5b9e\u73b0\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u3002", "result": "\u6761\u6b3e\u7cfb\u7edf\u8868\u73b0\u51fa\u6709\u754c\u548c\u53ef\u89e3\u91ca\u884c\u4e3a\uff1aAAS\u8f68\u8ff9\u4fdd\u6301\u8fde\u7eed\u548c\u901f\u7387\u53d7\u9650\uff0c\u77db\u76fe\u548c\u672a\u7ecf\u652f\u6301\u7684\u58f0\u660e\u89e6\u53d1\u660e\u786e\u60e9\u7f5a\uff0c\u5206\u5c42\u7ec6\u5316\u4ee5\u53d7\u63a7\u65b9\u5f0f\u63ed\u793a\u6709\u673a\u7ed3\u6784\u3002\u548c\u8c10\u9879\u5bf9\u9f50\u53cc\u91cd\u89c6\u56fe\u548c\u76ee\u6807-\u884c\u52a8\u5bf9\uff0c\u5b8c\u7f8e\u5ea6\u7a97\u53e3\u6f02\u79fb\u533a\u5206\u6301\u7eed\u6539\u8fdb\u4e0e\u9000\u5316\u3002", "conclusion": "\u57fa\u4e8e\u5355\u5b50\u7684\u6761\u6b3e\u6846\u67b6\u4ee5AAS\u4e3a\u9aa8\u5e72\uff0c\u4e3a\u7ea6\u675f\u548c\u5206\u6790\u4eba\u5de5\u667a\u80fd\u5185\u90e8\u52a8\u6001\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u4ee3\u7801\u7ea7\u7684\u84dd\u56fe\uff0c\u5c06\u54f2\u5b66\u52a8\u673a\u8f6c\u5316\u4e3a\u76f4\u63a5\u53ef\u5b9e\u73b0\u7684\u5de5\u7a0b\u67b6\u6784\u3002", "topic": "agent analysis"}}
{"id": "2512.11902", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11902", "abs": "https://arxiv.org/abs/2512.11902", "authors": ["Yanna Elizabeth Smid", "Peter van der Putten", "Aske Plaat"], "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning", "comment": null, "summary": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\"\u955c\u50cf\u6a21\u5f0f\"\u6e38\u620f\u6a21\u5f0f\uff0c\u8ba9AI\u6a21\u4eff\u73a9\u5bb6\u7684\u4e2a\u4eba\u7b56\u7565\u6765\u6311\u6218\u73a9\u5bb6\uff0c\u5728\u7b80\u5316\u7248\u300a\u706b\u7130\u7eb9\u7ae0\u82f1\u96c4\u300b\u4e2d\u5b9e\u73b0\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u73a9\u5bb6\u6d4b\u8bd5\u663e\u793a\u80fd\u8f83\u597d\u6a21\u4eff\u9632\u5fa1\u884c\u4e3a\u4f46\u8fdb\u653b\u7b56\u7565\u6a21\u4eff\u4e0d\u8db3\uff0c\u73a9\u5bb6\u6ee1\u610f\u5ea6\u8f83\u9ad8\u3002", "motivation": "\u5728\u56de\u5408\u5236\u6e38\u620f\u4e2d\uff0c\u654c\u4eba\u7b56\u7565\u5e94\u8be5\u5177\u6709\u60ca\u559c\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u3002\u4f20\u7edfAI\u5f80\u5f80\u4f7f\u7528\u56fa\u5b9a\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u4e2a\u6027\u5316\u6311\u6218\u3002\u8be5\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u79cd\u80fd\u6a21\u4eff\u73a9\u5bb6\u4e2a\u4eba\u7b56\u7565\u7684AI\uff0c\u8ba9\u73a9\u5bb6\u9762\u5bf9\u81ea\u5df1\u7684\u6e38\u620f\u98ce\u683c\uff0c\u4ece\u800c\u589e\u52a0\u6e38\u620f\u7684\u6311\u6218\u6027\u548c\u8da3\u5473\u6027\u3002", "method": "1. \u5728Unity\u4e2d\u6784\u5efa\u7b80\u5316\u7248\u300a\u706b\u7130\u7eb9\u7ae0\u82f1\u96c4\u300b\uff0c\u5305\u542b\u6807\u51c6\u6a21\u5f0f\u548c\u955c\u50cf\u6a21\u5f0f\uff1b2. \u7b2c\u4e00\u7ec4\u5b9e\u9a8c\uff1a\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u3001\u884c\u4e3a\u514b\u9686\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff0c\u5bfb\u627e\u9002\u5408\u6a21\u4eff\u73a9\u5bb6\u6f14\u793a\u7684\u6a21\u578b\uff1b3. \u7b2c\u4e8c\u7ec4\u5b9e\u9a8c\uff1a\u901a\u8fc7\u73a9\u5bb6\u6d4b\u8bd5\u8bc4\u4f30\u6a21\u578b\uff0c\u6a21\u578b\u5728\u53c2\u4e0e\u8005\u63d0\u4f9b\u7684\u6f14\u793a\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u73a9\u5bb6\u6e38\u620f\u6570\u636e\u663e\u793a\u6a21\u578b\u80fd\u8f83\u597d\u6a21\u4eff\u9632\u5fa1\u884c\u4e3a\uff08\u7279\u522b\u662f\u64a4\u9000\u6218\u672f\uff09\uff0c\u4f46\u5728\u8fdb\u653b\u7b56\u7565\u6a21\u4eff\u65b9\u9762\u4e0d\u8db3\u3002\u53c2\u4e0e\u8005\u8c03\u67e5\u8868\u660e\u4ed6\u4eec\u80fd\u8bc6\u522b\u51fa\u81ea\u5df1\u7684\u64a4\u9000\u6218\u672f\uff0c\u4e14\u5bf9\u955c\u50cf\u6a21\u5f0f\u7684\u6574\u4f53\u6ee1\u610f\u5ea6\u9ad8\u4e8e\u6807\u51c6\u6a21\u5f0f\u3002", "conclusion": "\u955c\u50cf\u6a21\u5f0f\u80fd\u6709\u6548\u63d0\u5347\u73a9\u5bb6\u6ee1\u610f\u5ea6\uff0c\u7279\u522b\u662f\u5f53\u73a9\u5bb6\u9762\u5bf9\u81ea\u5df1\u7684\u7b56\u7565\u65f6\u3002\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u53ef\u4ee5\u6539\u5584\u6a21\u4eff\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u8fdb\u653b\u7b56\u7565\u65b9\u9762\u3002\u8be5\u65b9\u6cd5\u4e3a\u521b\u5efa\u4e2a\u6027\u5316\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u6e38\u620fAI\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.11907", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11907", "abs": "https://arxiv.org/abs/2512.11907", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure", "summary": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u62df\u9635\u7ea6\u675f\u7684\u5b50\u6a21\u6700\u5927\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3LLM\u4e2a\u6027\u5316\u4e2d\u7684\u7ed3\u6784\u5316\u7ea6\u675f\u95ee\u9898\uff0c\u4fdd\u8bc1\u8fd1\u4f3c\u6700\u4f18\u89e3", "motivation": "LLM\u4e2a\u6027\u5316\u9700\u8981\u5728\u4efb\u52a1\u6548\u7528\u548c\u6570\u636e\u62ab\u9732\u4e4b\u95f4\u6743\u8861\uff0c\u73b0\u5b9e\u4e2d\u7684\u7ed3\u6784\u5316\u7ea6\u675f\uff08\u5982\u903b\u8f91\u4f9d\u8d56\u3001\u7c7b\u522b\u914d\u989d\u3001\u5c42\u6b21\u89c4\u5219\uff09\u4f7f\u6807\u51c6\u7b97\u6cd5\u5931\u6548", "method": "\u5c06\u7528\u6237\u77e5\u8bc6\u56fe\u7f16\u8bd1\u4e3a\u5b8f\u89c2\u9762\u5143\uff0c\u8bc1\u660e\u5e38\u89c1\u5c42\u6b21\u548c\u914d\u989d\u7ea6\u675f\u5f62\u6210\u5c42\u72b6\u62df\u9635\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u62df\u9635\u7ea6\u675f\u4e0b\u7684\u5b50\u6a21\u6700\u5927\u5316", "result": "\u7406\u8bba\u8bc1\u660e\u7ed3\u6784\u5316\u7ea6\u675f\u5f62\u6210\u5c42\u72b6\u62df\u9635\uff0c\u4f7f\u8d2a\u5fc3\u7b97\u6cd5\u5177\u6709\u5e38\u6570\u56e0\u5b50\u4fdd\u8bc1\uff0c\u8fde\u7eed\u8d2a\u5fc3\u53ef\u8fbe(1-1/e)\u8fd1\u4f3c\u6bd4", "conclusion": "\u63d0\u51fa\u7684\u62df\u9635\u7ea6\u675f\u5b50\u6a21\u6700\u5927\u5316\u6846\u67b6\u80fd\u5904\u7406\u66f4\u4e30\u5bcc\u3001\u66f4\u73b0\u5b9e\u7684LLM\u4e2a\u6027\u5316\u7ea6\u675f\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1", "topic": "agent analysis"}}
{"id": "2512.11922", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11922", "abs": "https://arxiv.org/abs/2512.11922", "authors": ["Muhammad Waseem", "Aakash Ahmad", "Kai-Kristian Kemell", "Jussi Rasku", "Sami Lahti", "Kalle M\u00e4kel\u00e4", "Pekka Abrahamsson"], "title": "Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use", "comment": "10", "summary": "Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.", "AI": {"tldr": "Vibe Coding (VC) \u662f\u4e00\u79cd\u7531\u751f\u6210\u5f0fAI\u8f85\u52a9\u7684\u8f6f\u4ef6\u5f00\u53d1\u5f62\u5f0f\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u4ee3\u7801\uff0c\u867d\u7136\u80fd\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u4f46\u4f1a\u5e26\u6765\u6280\u672f\u503a\u52a1\u98ce\u9669\uff0c\u5305\u62ec\u67b6\u6784\u4e0d\u4e00\u81f4\u3001\u5b89\u5168\u6f0f\u6d1e\u548c\u7ef4\u62a4\u8d1f\u62c5\u589e\u52a0\u7b49\u95ee\u9898\u3002", "motivation": "\u5206\u6790Vibe Coding\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6d41-\u503a\u6743\u8861\u95ee\u9898\uff0c\u5373\u6d41\u7545\u7684\u4ee3\u7801\u751f\u6210\u4e0e\u79ef\u7d2f\u6280\u672f\u503a\u52a1\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u57fa\u4e8e\u5185\u90e8MVP\u5f00\u53d1\u7ecf\u9a8c\u548c\u884c\u4e1a\u62a5\u544a\uff0c\u8bc6\u522bVC\u5e26\u6765\u7684\u98ce\u9669\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u591a\u4e2a\u5185\u90e8\u5f00\u53d1\u7684MVP\u7ecf\u9a8c\u548c\u884c\u4e1a\u62a5\u544a\u5206\u6790\uff0c\u8bc6\u522bVC\u4e2d\u7684\u6d41\u7a0b\u7ea7\u5f31\u70b9\u3001\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u504f\u5dee\u3001\u7f3a\u4e4f\u660e\u786e\u8bbe\u8ba1\u7406\u7531\u4ee5\u53ca\u4f18\u5148\u5feb\u901f\u4ee3\u7801\u751f\u6210\u800c\u975e\u4eba\u7c7b\u9a71\u52a8\u7684\u8fed\u4ee3\u5f00\u53d1\u7b49\u95ee\u9898\u3002", "result": "VC\u5728\u5feb\u901f\u539f\u578b\u5f00\u53d1\u65b9\u9762\u6709\u6548\uff0c\u4f46\u4f1a\u5bfc\u81f4\u67b6\u6784\u4e0d\u4e00\u81f4\u3001\u5b89\u5168\u6f0f\u6d1e\u548c\u7ef4\u62a4\u8d1f\u62c5\u589e\u52a0\u7b49\u6280\u672f\u503a\u52a1\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u6a21\u578b\u3001\u5e73\u53f0\u548c\u786c\u4ef6\u7684\u9650\u5236\u3002", "conclusion": "\u9700\u8981\u91c7\u53d6\u5bf9\u7b56\u89e3\u51b3VC\u5e26\u6765\u7684\u6280\u672f\u503a\u52a1\u95ee\u9898\uff0c\u5305\u62ec\u6539\u8fdb\u6a21\u578b\u3001\u5e73\u53f0\u548c\u786c\u4ef6\u9650\u5236\uff0c\u63a8\u52a8\u66f4\u53ef\u6301\u7eed\u7684VC\u65b9\u6cd5\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u3002", "topic": "swe application"}}
{"id": "2512.11816", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11816", "abs": "https://arxiv.org/abs/2512.11816", "authors": ["Enes \u00d6zeren", "Matthias A\u00dfenmacher"], "title": "Reinforcement Learning for Latent-Space Thinking in LLMs", "comment": "16 pages, 16 figures, 7 tables", "summary": "Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f46\u4ecd\u843d\u540e\u4e8e\u4f20\u7edf\u8bed\u8a00\u7a7a\u95f4\u601d\u7ef4\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\u63a8\u7406\u4f7f\u7528\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u6548\u7387\u4f4e\u4e0b\uff0c\u8bb8\u591a\u751f\u6210\u7684token\u53ea\u662f\u5f3a\u5236\u6267\u884c\u8bed\u8a00\u89c4\u5219\u800c\u975e\u63a8\u7406\u6240\u9700\u3002\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u4f7f\u7528\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u53ef\u80fd\u66f4\u9ad8\u6548\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1. \u5206\u6790Coconut\u65b9\u6cd5\uff08\u4e00\u79cd\u76d1\u7763\u5fae\u8c03\u7684\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u65b9\u6cd5\uff09\u7684\u8bbe\u8ba1\u654f\u611f\u6027\u548c\u5c40\u9650\u6027\uff1b2. \u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff08\u5305\u62ecGRPO\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u4e2d\u7684\u5e94\u7528\uff1b3. \u8bbe\u8ba1\u65b0\u9896\u7684Latent RL\u65b9\u6cd5\u76f4\u63a5\u4f18\u5316\u6f5c\u5728\u601d\u7ef4\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1. Coconut\u65b9\u6cd5\u5bf9\u8bbe\u8ba1\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\u4e14\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff1b2. \u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u9886\u57df\u4ecd\u843d\u540e\u4e8e\u4f20\u7edf\u8bed\u8a00\u7a7a\u95f4\u601d\u7ef4\u94fe\u6a21\u578b\u3002", "conclusion": "\u867d\u7136\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u7406\u8bba\u4e0a\u66f4\u9ad8\u6548\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u8d85\u8d8a\u4f20\u7edf\u8bed\u8a00\u7a7a\u95f4\u65b9\u6cd5\u3002\u4f5c\u8005\u516c\u5f00\u4e86\u4ee3\u7801\u5e93\u4f9b\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.11998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11998", "abs": "https://arxiv.org/abs/2512.11998", "authors": ["Glenn Zhang", "Treasure Mayowa", "Jason Fan", "Yicheng Fu", "Aaron Sandoval", "Sean O'Brien", "Kevin Zhu"], "title": "Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models", "comment": "Accepted at ACL 2025 SRW, 5 pages body, 14 pages total", "summary": "Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.", "AI": {"tldr": "\u63d0\u51faDirect Confidence Alignment (DCA)\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5bf9\u9f50LLM\u7684\u8a00\u8bed\u5316\u7f6e\u4fe1\u5ea6\u4e0e\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u800c\u975e\u771f\u5b9e\u51c6\u786e\u7387\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u6784\u5efa\u53ef\u4fe1\u53ef\u9760\u7684\u6a21\u578b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u7684\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff08\u6765\u81eatoken\u6982\u7387\uff09\u4e0e\u8a00\u8bed\u5316\u7f6e\u4fe1\u5ea6\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u4e0d\u540c\u6821\u51c6\u65b9\u6cd5\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\u3002", "method": "\u63d0\u51faDirect Confidence Alignment (DCA)\u65b9\u6cd5\uff0c\u4f7f\u7528Direct Preference Optimization\u6280\u672f\u6765\u5bf9\u9f50LLM\u7684\u8a00\u8bed\u5316\u7f6e\u4fe1\u5ea6\u4e0e\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u800c\u975e\u4e0e\u771f\u5b9e\u51c6\u786e\u7387\u5bf9\u9f50\u3002\u540c\u65f6\u5f15\u5165\u4e09\u79cd\u65b0\u7684\u57fa\u4e8e\u6821\u51c6\u8bef\u5dee\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "DCA\u5728\u67d0\u4e9b\u6a21\u578b\u67b6\u6784\u4e0a\u6539\u5584\u4e86\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u6307\u6807\uff0c\u51cf\u5c11\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u4f46\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\u6548\u679c\u6709\u9650\uff0c\u8868\u660e\u9700\u8981\u66f4\u9488\u5bf9\u5177\u4f53\u6a21\u578b\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "conclusion": "DCA\u65b9\u6cd5\u80fd\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4f46\u6548\u679c\u56e0\u6a21\u578b\u67b6\u6784\u800c\u5f02\uff0c\u9700\u8981\u66f4\u6a21\u578b\u611f\u77e5\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684LLMs\u3002", "topic": "agent analysis"}}
{"id": "2512.11993", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11993", "abs": "https://arxiv.org/abs/2512.11993", "authors": ["Ling-Hong Hung", "Ka Yee Yeung"], "title": "Re-opening open-source science through AI assisted development", "comment": null, "summary": "Open-source scientific software is effectively closed to modification by its complexity. With recent advances in technology, an agentic AI team led by a single human can now rapidly and robustly modify large codebases and re-open science to the community which can review and vet the AI generated code. We demonstrate this with a case study, STAR-Flex, which is an open source fork of STAR, adding 16,000 lines of C++ code to add the ability to process 10x Flex data, while maintaining full original function. This is the first open-source processing software for Flex data and was written as part of the NIH funded MorPHiC consortium.", "AI": {"tldr": "AI\u56e2\u961f\u4e3b\u5bfc\u7684\u4ee3\u7801\u4fee\u6539\u65b9\u6cd5\u4f7f\u590d\u6742\u5f00\u6e90\u79d1\u5b66\u8f6f\u4ef6\u53ef\u88ab\u4fee\u6539\uff0c\u901a\u8fc7STAR-Flex\u6848\u4f8b\u5c55\u793a\u4e86\u4e3aSTAR\u8f6f\u4ef6\u6dfb\u52a0Flex\u6570\u636e\u5904\u7406\u529f\u80fd", "motivation": "\u5f00\u6e90\u79d1\u5b66\u8f6f\u4ef6\u56e0\u590d\u6742\u6027\u800c\u96be\u4ee5\u4fee\u6539\uff0c\u963b\u788d\u4e86\u79d1\u5b66\u793e\u533a\u7684\u53c2\u4e0e\u548c\u529f\u80fd\u6269\u5c55", "method": "\u4f7f\u7528AI\u4ee3\u7406\u56e2\u961f\uff08\u7531\u5355\u4e2a\u4eba\u7c7b\u4e3b\u5bfc\uff09\u5feb\u901f\u7a33\u5065\u5730\u4fee\u6539\u5927\u578b\u4ee3\u7801\u5e93\uff0c\u751f\u6210\u53ef\u88ab\u793e\u533a\u5ba1\u67e5\u7684AI\u4ee3\u7801", "result": "\u6210\u529f\u521b\u5efaSTAR-Flex\uff0c\u4e3aSTAR\u8f6f\u4ef6\u6dfb\u52a0\u4e8616,000\u884cC++\u4ee3\u7801\uff0c\u5b9e\u73b0\u4e8610x Flex\u6570\u636e\u5904\u7406\u529f\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u5168\u90e8\u529f\u80fd", "conclusion": "AI\u4ee3\u7406\u56e2\u961f\u65b9\u6cd5\u80fd\u591f\u91cd\u65b0\u5f00\u653e\u79d1\u5b66\u8f6f\u4ef6\u7ed9\u793e\u533a\u5ba1\u67e5\uff0cSTAR-Flex\u6210\u4e3a\u9996\u4e2a\u5f00\u6e90\u7684Flex\u6570\u636e\u5904\u7406\u8f6f\u4ef6", "topic": "swe application"}}
{"id": "2512.11935", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.11935", "abs": "https://arxiv.org/abs/2512.11935", "authors": ["Jaehyung Lee", "Justin Ely", "Kent Zhang", "Akshaya Ajith", "Charles Rhys Campbell", "Kamal Choudhary"], "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org", "comment": null, "summary": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.", "AI": {"tldr": "AGAPI\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u6750\u6599\u79d1\u5b66AI\u5e73\u53f0\uff0c\u96c6\u6210\u4e868+\u5f00\u6e90LLM\u548c20+\u6750\u6599\u79d1\u5b66API\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u67b6\u6784\u5b9e\u73b0\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\uff0c\u7528\u4e8e\u52a0\u901f\u6750\u6599\u53d1\u73b0\u3002", "motivation": "\u5f53\u524d\u6750\u6599\u7814\u7a76\u9762\u4e34\u8ba1\u7b97\u751f\u6001\u7cfb\u7edf\u788e\u7247\u5316\u3001\u53ef\u91cd\u590d\u6027\u6311\u6218\u4ee5\u53ca\u4f9d\u8d56\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u7684AI\u5e73\u53f0\u6765\u52a0\u901f\u6750\u6599\u53d1\u73b0\u3002", "method": "\u91c7\u7528Agent-Planner-Executor-Summarizer\u67b6\u6784\uff0c\u96c6\u6210\u5f00\u6e90LLM\u548c\u6750\u6599\u79d1\u5b66API\u7aef\u70b9\uff0c\u6784\u5efa\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\uff0c\u6db5\u76d6\u6570\u636e\u68c0\u7d22\u3001\u6027\u8d28\u9884\u6d4b\u3001\u6a21\u62df\u8ba1\u7b97\u7b49\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5f02\u8d28\u7ed3\u6784\u6784\u5efa\u3001\u7c89\u672bX\u5c04\u7ebf\u884d\u5c04\u5206\u6790\u548c\u534a\u5bfc\u4f53\u7f3a\u9677\u5de5\u7a0b\u7b49\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\uff0c\u8bc4\u4f30\u4e8630+\u6d4b\u8bd5\u6848\u4f8b\uff0c\u5df2\u67091000+\u6d3b\u8dc3\u7528\u6237\u3002", "conclusion": "AGAPI\u4e3a\u53ef\u91cd\u590d\u3001AI\u52a0\u901f\u7684\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u900f\u660e\u7684\u57fa\u7840\u5e73\u53f0\u3002", "topic": "code agent"}}
{"id": "2512.12042", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12042", "abs": "https://arxiv.org/abs/2512.12042", "authors": ["Philipp Habicht", "Lev Sorokin", "Abdullah Saydemir", "Ken E. Friedl", "Andrea Stocco"], "title": "Benchmarking Contextual Understanding for In-Car Conversational Systems", "comment": null, "summary": "In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528LLM\u548c\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\u8bc4\u4f30\u8f66\u8f7d\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u6d4b\u8bd513\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\uff0c\u53d1\u73b0DeepSeek-R1\u5728\u5355\u4ee3\u7406\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u4e0b\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u800c\u975e\u63a8\u7406\u6a21\u578bDeepSeek-V3\u5728\u6548\u679c\u4e0e\u6210\u672c\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u8f66\u8f7d\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u867d\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u4ecd\u5177\u6311\u6218\u3002\u4f20\u7edf\u4eba\u5de5\u8bc4\u4f30\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u7ed3\u5408\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\uff08\u8f93\u5165\u8f93\u51fa\u3001\u601d\u7ef4\u94fe\u3001\u81ea\u4e00\u81f4\u6027\u3001\u591a\u4ee3\u7406\u63d0\u793a\uff09\u8bc4\u4f30\u5bf9\u8bdd\u7cfb\u7edf\u54cd\u5e94\u4e0e\u7528\u6237\u8bdd\u8bed\u7684\u4e00\u81f4\u6027\u3002\u751f\u6210\u5408\u6210\u7528\u6237\u8bdd\u8bed\u53ca\u6b63\u786e/\u542b\u9519\u8bef\u7684\u7cfb\u7edf\u54cd\u5e94\uff0c\u5728\u9910\u5385\u63a8\u8350\u6848\u4f8b\u7814\u7a76\u4e2d\u6d4b\u8bd513\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u63d0\u4f9b\u5546\u7684LLM\u3002", "result": "\u5c0f\u578b\u975e\u63a8\u7406\u6a21\u578b\u5728\u5e94\u7528\u9ad8\u7ea7\u63d0\u793a\u6280\u672f\uff08\u7279\u522b\u662f\u591a\u4ee3\u7406\u63d0\u793a\uff09\u65f6\u6539\u8fdb\u6700\u5927\uff1b\u63a8\u7406\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6700\u4f73\u6027\u80fd\u7531\u5355\u4ee3\u7406\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u4e0b\u7684DeepSeek-R1\u5b9e\u73b0\uff08F1=0.99\uff0c\u6210\u672c0.002\u7f8e\u5143/\u8bf7\u6c42\uff09\u3002DeepSeek-V3\u5728\u6548\u679c\u4e0e\u6210\u672c\u65f6\u95f4\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "LLM\u8bc4\u4f30\u4e3a\u8f66\u8f7d\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "2512.12117", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12117", "abs": "https://arxiv.org/abs/2512.12117", "authors": ["Jahidul Arafat"], "title": "Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context", "comment": null, "summary": "Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff0c\u7ed3\u5408BM25\u7a00\u758f\u5339\u914d\u3001BGE\u5bc6\u96c6\u5d4c\u5165\u548c\u57fa\u4e8e\u5bfc\u5165\u5173\u7cfb\u7684Neo4j\u56fe\u6269\u5c55\uff0c\u7528\u4e8e\u4ee3\u7801\u7406\u89e3\u4e2d\u7684\u53ef\u9a8c\u8bc1\u5f15\u7528\u751f\u6210\uff0c\u5b9e\u73b0\u4e8692%\u7684\u5f15\u7528\u51c6\u786e\u7387\u4e14\u96f6\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7406\u89e3\u4e2d\u5e38\u4ea7\u751f\u5e7b\u89c9\uff08\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u4ee3\u7801\u5f15\u7528\uff09\uff0c\u8fd9\u662f\u53ef\u9760\u5f00\u53d1\u8005\u8f85\u52a9\u5de5\u5177\u7684\u5173\u952e\u969c\u788d\u3002\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u7eaf\u6587\u672c\u76f8\u4f3c\u6027\u800c\u5ffd\u7565\u4ee3\u7801\u7ed3\u6784\uff0c\u5bfc\u81f4\u8de8\u6587\u4ef6\u8bc1\u636e\u53d1\u73b0\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff1a\u7ed3\u5408BM25\u7a00\u758f\u5339\u914d\u3001BGE\u5bc6\u96c6\u5d4c\u5165\u548cNeo4j\u56fe\u6269\u5c55\uff08\u901a\u8fc7\u5bfc\u5165\u5173\u7cfb\uff09\u3002\u572830\u4e2aPython\u4ed3\u5e93\u7684180\u4e2a\u5f00\u53d1\u8005\u67e5\u8be2\u4e2d\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6bd4\u8f83\u68c0\u7d22\u6a21\u5f0f\u3001\u56fe\u6269\u5c55\u7b56\u7565\u548c\u5f15\u7528\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e8692%\u7684\u5f15\u7528\u51c6\u786e\u7387\u4e14\u96f6\u5e7b\u89c9\u3002\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\u6bd4\u5355\u6a21\u5f0f\u57fa\u7ebf\u63d0\u534714-18\u4e2a\u767e\u5206\u70b9\uff0c\u572862%\u7684\u67b6\u6784\u67e5\u8be2\u4e2d\u53d1\u73b0\u4e86\u7eaf\u6587\u672c\u76f8\u4f3c\u6027\u9057\u6f0f\u7684\u8de8\u6587\u4ef6\u8bc1\u636e\u3002\u8de8\u6587\u4ef6\u8bc1\u636e\u53d1\u73b0\u662f\u5f15\u7528\u5b8c\u6574\u6027\u7684\u6700\u5927\u8d21\u732e\u56e0\u7d20\u3002", "conclusion": "\u5e94\u5c06\u5f15\u7528\u57fa\u7840\u751f\u6210\u4f5c\u4e3a\u4ee3\u7801\u7406\u89e3\u7cfb\u7edf\u7684\u67b6\u6784\u539f\u5219\u3002\u9700\u8981\u7ed3\u5408\u7a00\u758f\u5339\u914d\u3001\u5bc6\u96c6\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u4ee3\u7801\u7ed3\u6784\u63a8\u7406\u6765\u89e3\u51b3\u5f15\u7528\u51c6\u786e\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u8de8\u6587\u4ef6\u67b6\u6784\u4f9d\u8d56\u5173\u7cfb\u3002", "topic": "code agent"}}
{"id": "2512.12088", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12088", "abs": "https://arxiv.org/abs/2512.12088", "authors": ["S. R. Eshwar", "Aniruddha Mukherjee", "Kintan Saha", "Krishna Agarwal", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations", "comment": null, "summary": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.", "AI": {"tldr": "RPI\uff08\u53ef\u9760\u7b56\u7565\u8fed\u4ee3\uff09\u5728\u51fd\u6570\u903c\u8fd1\u8bbe\u7f6e\u4e2d\u6062\u590d\u4e86\u7b56\u7565\u8fed\u4ee3\u7684\u5355\u8c03\u6027\uff0c\u5728CartPole\u548c\u5012\u7acb\u6446\u4efb\u52a1\u4e2d\u76f8\u6bd4DQN\u3001DDPG\u3001TD3\u3001PPO\u7b49\u65b9\u6cd5\u80fd\u66f4\u65e9\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u5e76\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5e38\u53d7\u6837\u672c\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8d85\u53c2\u6570\u654f\u611f\u7b49\u95ee\u9898\u56f0\u6270\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "RPI\uff08\u53ef\u9760\u7b56\u7565\u8fed\u4ee3\uff09\u5728\u51fd\u6570\u903c\u8fd1\u8bbe\u7f6e\u4e2d\u6062\u590d\u4e86\u7b56\u7565\u8fed\u4ee3\u7684\u5355\u8c03\u6027\uff0c\u5e76\u5728CartPole\u548c\u5012\u7acb\u6446\u4efb\u52a1\u4e2d\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\uff0c\u5bf9\u6bd4\u4e86DQN\u3001Double DQN\u3001DDPG\u3001TD3\u3001PPO\u7b49\u65b9\u6cd5\u3002", "result": "RPI\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u80fd\u66f4\u65e9\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "RPI\u4f5c\u4e3a\u66f4\u53ef\u9760\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u80fd\u89e3\u51b3\u6837\u672c\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.12264", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12264", "abs": "https://arxiv.org/abs/2512.12264", "authors": ["Abhay Srivastava", "Sam Jung", "Spencer Mateega"], "title": "Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics", "comment": null, "summary": "We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.", "AI": {"tldr": "MARKET-BENCH\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u91cf\u5316\u4ea4\u6613\u4efb\u52a1\u4e2d\u6784\u5efa\u53ef\u6267\u884c\u56de\u6d4b\u5668\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u6839\u636e\u81ea\u7136\u8bed\u8a00\u7b56\u7565\u63cf\u8ff0\u751f\u6210\u5339\u914d\u53c2\u8003\u5b9e\u73b0\u4ee3\u7801\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u91cf\u5316\u4ea4\u6613\u4efb\u52a1\u4e2d\u4ee3\u7801\u751f\u6210\u548c\u6570\u503c\u51c6\u786e\u6027\u7684\u57fa\u51c6\u3002\u9700\u8981\u4e86\u89e3LLM\u80fd\u5426\u4ece\u81ea\u7136\u8bed\u8a00\u7b56\u7565\u63cf\u8ff0\u6784\u5efa\u53ef\u9760\u7684\u56de\u6d4b\u7cfb\u7edf\u3002", "method": "\u521b\u5efa\u5305\u542b\u4e09\u79cd\u7ecf\u5178\u7b56\u7565\uff08\u5b9a\u65f6\u4ea4\u6613\u3001\u914d\u5bf9\u4ea4\u6613\u3001Delta\u5bf9\u51b2\uff09\u7684\u57fa\u51c6\uff0c\u8981\u6c42LLM\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u6267\u884c\u56de\u6d4b\u4ee3\u7801\u3002\u4f7f\u7528\u591a\u8f6epass@k\u6307\u6807\u5206\u79bb\u7ed3\u6784\u53ef\u9760\u6027\uff08\u4ee3\u7801\u80fd\u5426\u8fd0\u884c\uff09\u548c\u6570\u503c\u51c6\u786e\u6027\uff08\u56de\u6d4b\u6307\u6807\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff09\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u80fd\u53ef\u9760\u6267\u884c\u6700\u7b80\u5355\u7b56\u7565\uff08\u5e73\u5747pass@3\u4e3a0.80\uff09\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u95f4\u8bef\u5dee\u5dee\u5f02\u5de8\u5927\u3002Gemini 3 Pro\u548cClaude 4.5 Sonnet\u5728\u7b80\u5355\u7b56\u7565\u4e0a\u7ed3\u5408\u4e86\u5f3a\u53ef\u9760\u6027\u548c\u4f4e\u8bef\u5dee\uff0cGPT-5.1 Codex-Max\u5728\u524d\u4e24\u4e2a\u7b56\u7565\u4e0a\u8fbe\u5230\u5b8c\u7f8epass@1\uff0cQwen3 Max\u8fbe\u5230\u5b8c\u7f8epass@3\u4f46\u6709\u65f6\u4ea7\u751f\u4e0d\u51c6\u786e\u7684P&L\u8def\u5f84\u3002", "conclusion": "\u5f53\u524dLLM\u80fd\u591f\u642d\u5efa\u57fa\u672c\u4ea4\u6613\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u5728\u4ef7\u683c\u3001\u5e93\u5b58\u548c\u98ce\u9669\u63a8\u7406\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u7814\u7a76\u53d1\u5e03\u4e86MARKET-BENCH\u57fa\u51c6\u548c\u516c\u5f00\u6392\u884c\u699c\u3002", "topic": "code agent"}}
{"id": "2512.12536", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12536", "abs": "https://arxiv.org/abs/2512.12536", "authors": ["Arastoo Zibaeirad", "Marco Vieira"], "title": "Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.\n  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.\n  Artifact: https://github.com/Erroristotle/DVDR_LLM", "AI": {"tldr": "DVDR-LLM\u662f\u4e00\u4e2a\u96c6\u6210\u591a\u4e2aLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u4e0e\u4fee\u590d\uff0c\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u6a21\u578b\u7684\u8f93\u51fa\u6765\u51cf\u5c11\u9519\u8bef\u7387\uff0c\u5728\u590d\u6742\u6f0f\u6d1e\u68c0\u6d4b\u4e0a\u6bd4\u5355\u4e2a\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u548c\u4fee\u590d\u65b9\u9762\u867d\u7136\u5177\u5907\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5728\u8bc6\u522b\u590d\u6742\u6f0f\u6d1e\u548c\u751f\u6210\u4fee\u590d\u65b9\u6848\u65f6\u7ecf\u5e38\u9047\u5230\u56f0\u96be\u3002\u9700\u8981\u63a2\u7d22\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u6a21\u578b\u662f\u5426\u80fd\u51cf\u5c11\u9519\u8bef\u7387\u3002", "method": "\u63d0\u51fa\u4e86DVDR-LLM\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u4e2a\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u901a\u8fc7\u8bbe\u7f6e\u9002\u5f53\u7684\u9608\u503c\u6765\u51b3\u5b9a\u6a21\u578b\u95f4\u7684\u5171\u8bc6\u6c34\u5e73\uff0c\u4ee5\u5e73\u8861\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u8bef\u62a5\u7387\u3002", "result": "DVDR-LLM\u6bd4\u5355\u4e2a\u6a21\u578b\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad810-12%\uff0c\u5728\u4ee3\u7801\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002\u5bf9\u4e8e\u591a\u6587\u4ef6\u6f0f\u6d1e\uff0c\u53ec\u56de\u7387\u63d0\u534718%\uff0cF1\u5206\u6570\u63d0\u534711.8%\u3002\u4f46\u5b58\u5728\u6743\u8861\uff1a\u5728\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u51cf\u5c11\u8bef\u62a5\u7684\u540c\u65f6\uff0c\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u589e\u52a0\u4e86\u6f0f\u62a5\u3002", "conclusion": "\u96c6\u6210\u591a\u4e2aLLM\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u4e0e\u4fee\u590d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u573a\u666f\u4e0b\u3002\u4f46\u9700\u8981\u6839\u636e\u5177\u4f53\u5b89\u5168\u4e0a\u4e0b\u6587\u8c28\u614e\u9009\u62e9\u6a21\u578b\u95f4\u7684\u5171\u8bc6\u9608\u503c\uff0c\u4ee5\u5e73\u8861\u8bef\u62a5\u548c\u6f0f\u62a5\u7684\u6743\u8861\u3002", "topic": "code agent"}}
{"id": "2512.12411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12411", "abs": "https://arxiv.org/abs/2512.12411", "authors": ["Ely Hahami", "Lavik Jain", "Ishaan Sinha"], "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs", "comment": "7 pages (+ 5 pages for appendix), 5 figures, 1 table", "summary": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6d4b\u8bd5\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u6ce8\u5165\u6982\u5ff5\u7684\"\u6d8c\u73b0\u5185\u7701\"\u80fd\u529b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5185\u7701\u80fd\u529b\u5177\u6709\u8106\u5f31\u6027\u548c\u63d0\u793a\u654f\u611f\u6027\uff0c\u4f46\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u4ecd\u80fd\u8868\u73b0\u51fa\u90e8\u5206\u5185\u7701\u80fd\u529b\u3002", "motivation": "Anthropic\u7684\u7814\u7a76\u58f0\u79f0\u524d\u6cbf\u6a21\u578b\u80fd\u591f\u68c0\u6d4b\u548c\u547d\u540d\u6ce8\u5165\u7684\"\u6982\u5ff5\"\u6fc0\u6d3b\u65b9\u5411\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5\u8fd9\u4e00\u58f0\u79f0\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u7a76\u6a21\u578b\u5185\u7701\u80fd\u529b\u7684\u771f\u5b9e\u8303\u56f4\u548c\u5c40\u9650\u6027\u3002", "method": "\u9996\u5148\u5728Meta-Llama-3.1-8B-Instruct\u4e0a\u590d\u73b0Anthropic\u7684\u591a\u8f6e\"\u6d8c\u73b0\u5185\u7701\"\u7ed3\u679c\uff1b\u7136\u540e\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u63a8\u7406\u63d0\u793a\uff0c\u6d4b\u8bd5\u5185\u7701\u80fd\u529b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1b\u6700\u540e\u63a2\u7d22\u90e8\u5206\u5185\u7701\u673a\u5236\uff0c\u8ba9\u6a21\u578b\u5bf9\u6ce8\u5165\u6982\u5ff5\u5411\u91cf\u7684\u7cfb\u6570\u5f3a\u5ea6\u8fdb\u884c\u5206\u7c7b\u3002", "result": "1) \u6210\u529f\u590d\u73b0Anthropic\u7684\u7ed3\u679c\uff0c\u6a21\u578b\u572820%\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u5e76\u547d\u540d\u6ce8\u5165\u6982\u5ff5\uff1b2) \u5185\u7701\u80fd\u529b\u5177\u6709\u8106\u5f31\u6027\uff0c\u5728\u591a\u9879\u9009\u62e9\u8bc6\u522b\u6216\u4e8c\u5143\u5224\u522b\u7b49\u76f8\u8fd1\u4efb\u52a1\u4e2d\u6027\u80fd\u5d29\u6e83\uff1b3) \u53d1\u73b0\u90e8\u5206\u5185\u7701\u673a\u5236\uff0c\u6a21\u578b\u80fd\u4ee570%\u7684\u51c6\u786e\u7387\u5206\u7c7b\u6ce8\u5165\u6982\u5ff5\u5411\u91cf\u7684\u7cfb\u6570\u5f3a\u5ea6\uff08\u8fdc\u9ad8\u4e8e25%\u7684\u968f\u673a\u57fa\u7ebf\uff09\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u80fd\u591f\u57fa\u4e8e\u5176\u5185\u90e8\u8868\u793a\u8fdb\u884c\u5185\u7701\u8ba1\u7b97\uff0c\u4f46\u8fd9\u4e9b\u81ea\u6211\u62a5\u544a\u5177\u6709\u72ed\u7a84\u6027\u548c\u63d0\u793a\u654f\u611f\u6027\uff0c\u5185\u7701\u80fd\u529b\u5e76\u975e\u5927\u578b\u6216\u9ad8\u80fd\u529b\u6a21\u578b\u72ec\u6709\uff0c\u4f46\u4e5f\u4e0d\u5982\u6700\u521d\u58f0\u79f0\u7684\u90a3\u6837\u7a33\u5065\u3002", "topic": "agent analysis"}}
{"id": "2512.12719", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12719", "abs": "https://arxiv.org/abs/2512.12719", "authors": ["Anrafel Fernandes Pereira", "Maria Teresa Baldassarre", "Daniel Mendez", "Marcos Kalinowski"], "title": "Towards AI Agents Supported Research Problem Formulation", "comment": null, "summary": "Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4f7f\u7528AI\u4ee3\u7406\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u8005\u5728\u7814\u7a76\u65e9\u671f\u9636\u6bb5\uff08\u95ee\u9898\u5b9a\u4e49\uff09\u7684\u5de5\u4f5c\uff0c\u57fa\u4e8e\u7cbe\u76ca\u7814\u7a76\u542f\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u63cf\u8ff0\u6027\u8bc4\u4f30\u5c55\u793aAI\u4ee3\u7406\u5982\u4f55\u5e2e\u52a9\u9884\u586b\u95ee\u9898\u5c5e\u6027\u3001\u5bf9\u9f50\u5229\u76ca\u76f8\u5173\u8005\u89c6\u89d2\u3001\u7cbe\u70bc\u7814\u7a76\u95ee\u9898\u7b49\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\uff0c\u5b9a\u4e49\u4e0d\u5f53\u7684\u7814\u7a76\u95ee\u9898\u53ef\u80fd\u65e0\u6cd5\u53cd\u6620\u5de5\u4e1a\u5b9e\u8df5\u7684\u590d\u6742\u6027\uff0c\u4ece\u800c\u5f71\u54cd\u7814\u7a76\u7684\u5b9e\u9645\u76f8\u5173\u6027\u3002\u7814\u7a76\u8005\u9700\u8981\u5728\u7814\u7a76\u65e9\u671f\u9636\u6bb5\u66f4\u597d\u5730\u5b9a\u4e49\u95ee\u9898\uff0c\u786e\u4fdd\u7814\u7a76\u5177\u6709\u5b9e\u8df5\u4ef7\u503c\u3002", "method": "\u57fa\u4e8e\u7cbe\u76ca\u7814\u7a76\u542f\u52a8\u6846\u67b6\uff0c\u4f7f\u7528\u5df2\u53d1\u8868\u7684\u673a\u5668\u5b66\u4e60\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u7814\u7a76\u4f5c\u4e3a\u53c2\u8003\uff0c\u5f00\u53d1\u63cf\u8ff0\u6027\u8bc4\u4f30\u573a\u666f\uff0c\u5c55\u793aAI\u4ee3\u7406\u5982\u4f55\u96c6\u6210\u5230LRI\u6846\u67b6\u4e2d\uff0c\u652f\u6301\u7814\u7a76\u8005\u8fdb\u884c\u95ee\u9898\u5b9a\u4e49\u3002", "result": "\u63cf\u8ff0\u6027\u8bc4\u4f30\u8868\u660e\uff0cAI\u4ee3\u7406\u652f\u6301\u53ef\u4ee5\u4e30\u5bcc\u534f\u4f5c\u8ba8\u8bba\uff0c\u589e\u5f3a\u5bf9\u7814\u7a76\u95ee\u9898\u7684\u4ef7\u503c\u3001\u53ef\u884c\u6027\u548c\u9002\u7528\u6027\u7684\u6279\u5224\u6027\u53cd\u601d\u3002AI\u4ee3\u7406\u96c6\u6210\u5230LRI\u6846\u67b6\u88ab\u8ba4\u4e3a\u6709\u524d\u666f\uff0c\u4f46\u9700\u8981\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u652f\u6301\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u95ee\u9898\u5b9a\u4e49\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u60c5\u5883\u611f\u77e5\u548c\u5b9e\u8df5\u5bfc\u5411\u7684\u95ee\u9898\u5236\u5b9a\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u5b9e\u8bc1\u7814\u7a76\u6765\u786e\u8ba4\u548c\u5b8c\u5584\u8fd9\u79cd\u96c6\u6210\u3002", "topic": "agent analysis"}}
{"id": "2512.12544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12544", "abs": "https://arxiv.org/abs/2512.12544", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Wanhao Yu", "Zhankai Ye", "Dawei Xiang", "Ting Hua", "Xin Liu", "Shangqian Gao", "Tingting Yu"], "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks", "comment": null, "summary": "Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.", "AI": {"tldr": "HyperEdit\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\u65b9\u6cd5\u548c\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u6587\u672c\u7f16\u8f91\u7684\u6027\u80fd\uff0c\u5728\u4fee\u6539\u533a\u57df\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e869%-30%\u7684BLEU\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6587\u672c\u7f16\u8f91\u89c6\u4e3a\u901a\u7528\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5bfc\u81f4\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u96be\u4ee5\u5fe0\u5b9e\u5bf9\u9f50\u591a\u6837\u5316\u7684\u7528\u6237\u610f\u56fe\uff0c\u4ee5\u53ca\u7ecf\u5e38\u8fc7\u5ea6\u7f16\u8f91\u672a\u6539\u53d8\u533a\u57df\u3002\u6307\u4ee4\u6587\u672c\u7f16\u8f91\u5728\u4ee3\u7801\u7f16\u8f91\u5668\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u4ecd\u7136\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1. \u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\uff1a\u751f\u6210\u8bf7\u6c42\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4e3a\u6bcf\u4e2a\u6307\u4ee4\u5b9a\u5236\u7f16\u8f91\u7b56\u7565\u30022. \u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff1a\u5c06\u76d1\u7763\u96c6\u4e2d\u5728\u4fee\u6539\u7684\u6587\u672c\u7247\u6bb5\u4e0a\uff0c\u9632\u6b62\u8fc7\u5ea6\u7f16\u8f91\uff0c\u540c\u65f6\u786e\u4fdd\u7cbe\u786e\u7684\u6700\u5c0f\u5316\u66f4\u6539\u3002", "result": "HyperEdit\u5728\u4fee\u6539\u533a\u57df\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u4e869%-30%\u7684\u76f8\u5bf9BLEU\u6539\u8fdb\uff0c\u5c3d\u7ba1\u4ec5\u4f7f\u7528\u4e8630\u4ebf\u53c2\u6570\u3002", "conclusion": "HyperEdit\u901a\u8fc7\u8d85\u7f51\u7edc\u52a8\u6001\u9002\u5e94\u548c\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u6587\u672c\u7f16\u8f91\u4e2d\u7684\u610f\u56fe\u5bf9\u9f50\u548c\u8fc7\u5ea6\u7f16\u8f91\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2512.12576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12576", "abs": "https://arxiv.org/abs/2512.12576", "authors": ["Xueru Wen", "Jie Lou", "Yanjiang Liu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Yaojie Lu", "Debing Zhang"], "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning", "comment": null, "summary": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.", "AI": {"tldr": "CoVRL\u662f\u4e00\u79cd\u5c06\u53d8\u5206\u63a8\u65ad\u4e0e\u5f3a\u5316\u5b66\u4e60\u8026\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u91c7\u6837\u7b56\u7565\u89e3\u51b3\u65e0\u9a8c\u8bc1\u5668RL\u4e2d\u63a8\u7406\u8f68\u8ff9\u4e0e\u7b54\u6848\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u65e0\u9a8c\u8bc1\u5668\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u95ee\u9898\u91c7\u6837\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u4e0e\u6700\u7ec8\u7b54\u6848\u8131\u8282\uff0c\u63a2\u7d22\u6548\u7387\u4f4e\u4e14\u4e0d\u8fde\u8d2f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u6301\u601d\u7ef4-\u7b54\u6848\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCoVRL\uff08\u8026\u5408\u53d8\u5206\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u6df7\u5408\u91c7\u6837\u7b56\u7565\u8026\u5408\u5148\u9a8c\u5206\u5e03\u548c\u540e\u9a8c\u5206\u5e03\uff0c\u6784\u5efa\u5e76\u4f18\u5316\u590d\u5408\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u540c\u65f6\u4fdd\u6301\u601d\u7ef4-\u7b54\u6848\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoVRL\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6027\u80fd\u63d0\u534712.4%\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65e0\u9a8c\u8bc1\u5668RL\u57fa\u7ebf\u989d\u5916\u63d0\u53472.3%\u3002", "conclusion": "CoVRL\u4e3a\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u901a\u7528\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u53d8\u5206\u63a8\u65ad\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u4e86\u63a8\u7406\u8f68\u8ff9\u4e0e\u7b54\u6848\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13360", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.13360", "abs": "https://arxiv.org/abs/2512.13360", "authors": ["Shuyuan Xiao", "Yiran Zhang", "Weisong Sun", "Xiaohong Chen", "Yang Liu", "Zhi Jin"], "title": "UCRBench: Benchmarking LLMs on Use Case Recovery", "comment": null, "summary": "Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4ee3\u7801\u5bf9\u9f50\u7684\u7528\u4f8b\u57fa\u51c6\uff0c\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u4ece\u6e90\u4ee3\u7801\u751f\u6210\u7528\u4f8b\u7684\u80fd\u529b\uff0c\u53d1\u73b0LLM\u80fd\u90e8\u5206\u91cd\u6784\u7cfb\u7edf\u529f\u80fd\u4f46\u5728\u9886\u57df\u7279\u5b9a\u548c\u591a\u6a21\u5757\u7cfb\u7edf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u9ad8\u9057\u6f0f\u7387\u548c\u62bd\u8c61\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7528\u4f8b\u57fa\u51c6\u7a00\u7f3a\u4e14\u53ef\u80fd\u4e0e\u5b9e\u9645\u7cfb\u7edf\u884c\u4e3a\u4e0d\u4e00\u81f4\uff0c\u8fd9\u9650\u5236\u4e86LLM\u4ece\u6e90\u4ee3\u7801\u751f\u6210\u7528\u4f8b\u80fd\u529b\u7684\u4e25\u683c\u8bc4\u4f30\u3002\u9700\u8981\u521b\u5efa\u4ee3\u7801\u5bf9\u9f50\u7684\u7528\u4f8b\u57fa\u51c6\u6765\u89e3\u51b3\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u9a8c\u8bc1\u4e5d\u4e2a\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u9879\u76ee\u7684\u7528\u6237\u76ee\u6807\u548c\u5b50\u529f\u80fd\u7528\u4f8b\uff0c\u6784\u5efa\u4ee3\u7801\u5bf9\u9f50\u7684\u7528\u4f8b\u57fa\u51c6\u3002\u63d0\u51fa\u5206\u5c42\u8bc4\u4f30\u534f\u8bae\uff0c\u8bc4\u4f30\u53c2\u4e0e\u8005\u6b63\u786e\u6027\u3001\u540d\u79f0\u51c6\u786e\u6027\u3001\u8def\u5f84\u4fdd\u771f\u5ea6\u548c\u884c\u4e3a\u8986\u76d6\u7387\u3002", "result": "LLM\u80fd\u591f\u90e8\u5206\u91cd\u6784\u7cfb\u7edf\u529f\u80fd\uff0c\u4f46\u6027\u80fd\u5728\u4e0d\u540c\u9879\u76ee\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u5728\u9886\u57df\u7279\u5b9a\u548c\u591a\u6a21\u5757\u7cfb\u7edf\u4e2d\u8868\u73b0\u5c24\u5176\u4e0d\u4f73\u3002\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u9057\u6f0f\u7387\uff0c\u4e14\u5728\u5c06\u5b50\u529f\u80fd\u805a\u5408\u4e3a\u7528\u6237\u76ee\u6807\u7528\u4f8b\u65f6\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u7684\u62bd\u8c61\u7ea7\u522b\u3002", "conclusion": "LLM\u5728\u7528\u4f8b\u9006\u5411\u5de5\u7a0b\u65b9\u9762\u65e2\u6709\u6f5c\u529b\u4e5f\u6709\u5f53\u524d\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7cfb\u7edf\u548c\u4fdd\u6301\u62bd\u8c61\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "topic": "swe benchmark"}}
{"id": "2512.12548", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12548", "abs": "https://arxiv.org/abs/2512.12548", "authors": ["Yesid Fonseca", "Manuel S. R\u00edos", "Nicanor Quijano", "Luis F. Giraldo"], "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents", "comment": "14 pages, 6 figures", "summary": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u901a\u8fc7\u4e60\u5f97\u73af\u5883\u9884\u6d4b\u8868\u793a\uff0c\u81ea\u7136\u6536\u655b\u5230\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u5bf9\u9f50\u7684\u89c5\u98df\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u4e0e\u751f\u7269\u89c5\u98df\u8005\u76f8\u4f3c\u7684\u51b3\u7b56\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406(MVT)\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9884\u6d4b\u89c5\u98df\u884c\u4e3a\uff0c\u4f46\u751f\u7269\u89c5\u98df\u8005\u5982\u4f55\u5b9e\u73b0\u6700\u4f18\u6591\u5757\u89c5\u98df\u51b3\u7b56\u7684\u8ba1\u7b97\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u9884\u6d4b\u6027\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u7c7b\u4f3c\u751f\u7269\u89c5\u98df\u8005\u7684\u51b3\u7b56\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u8be5\u667a\u80fd\u4f53\u4e60\u5f97\u7b80\u6d01\u7684\u73af\u5883\u9884\u6d4b\u8868\u793a\uff0c\u5e76\u4e0e\u6807\u51c6\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u57fa\u4e8e\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u4e0e\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u5bf9\u9f50\u7684\u89c5\u98df\u7b56\u7565\uff0c\u5176\u51b3\u7b56\u6a21\u5f0f\u4e0e\u8bb8\u591a\u751f\u7269\u89c5\u98df\u8005\u76f8\u4f3c\u3002\u76f8\u6bd4\u65e0\u6a21\u578b\u667a\u80fd\u4f53\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u667a\u80fd\u4f53\u901a\u8fc7\u9884\u6d4b\u80fd\u529b\u800c\u975e\u5355\u7eaf\u5956\u52b1\u6700\u5927\u5316\u9a71\u52a8\u9ad8\u6548\u7684\u6591\u5757\u79bb\u5f00\u884c\u4e3a\u3002", "conclusion": "\u9884\u6d4b\u6027\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3aAI\u7cfb\u7edf\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u548c\u751f\u7269\u5b66\u57fa\u7840\u7684\u51b3\u7b56\u7684\u57fa\u7840\u3002\u751f\u6001\u6700\u4f18\u6027\u539f\u7406\u5bf9\u63a8\u8fdb\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94AI\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "topic": "agent analysis"}}
{"id": "2512.12597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12597", "abs": "https://arxiv.org/abs/2512.12597", "authors": ["Miriam Horovicz"], "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "comment": null, "summary": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "AI": {"tldr": "AgentSHAP\u662f\u9996\u4e2a\u7528\u4e8e\u89e3\u91caLLM\u4ee3\u7406\u4e2d\u5de5\u5177\u91cd\u8981\u6027\u7684\u6846\u67b6\uff0c\u57fa\u4e8eShapley\u503c\u8ba1\u7b97\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u89e3\u91ca\u54ea\u4e9b\u5de5\u5177\u5bf9\u54cd\u5e94\u6709\u5b9e\u9645\u8d21\u732e\u7684\u65b9\u6cd5\u3002\u73b0\u6709XAI\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5de5\u5177\u5c42\u9762\u7684\u89e3\u91ca\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u535a\u5f08\u8bba\u4e2d\u7684Shapley\u503c\uff0c\u5c06\u4ee3\u7406\u89c6\u4e3a\u9ed1\u76d2\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u6d4b\u8bd5\u4e0d\u540c\u5de5\u5177\u5b50\u96c6\u4e0b\u7684\u4ee3\u7406\u54cd\u5e94\uff0c\u8ba1\u7b97\u516c\u5e73\u7684\u91cd\u8981\u6027\u5206\u6570\u3002\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6743\u91cd\u6216\u68af\u5ea6\u3002", "result": "\u5728API-Bank\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentSHAP\u80fd\u4ea7\u751f\u4e00\u81f4\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u6b63\u786e\u8bc6\u522b\u91cd\u8981\u5de5\u5177\uff0c\u533a\u5206\u76f8\u5173\u4e0e\u65e0\u5173\u5de5\u5177\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4eceO(2^n)\u964d\u4f4e\u5230\u5b9e\u7528\u6c34\u5e73\u3002", "conclusion": "AgentSHAP\u586b\u8865\u4e86\u5de5\u5177\u5c42\u9762\u89e3\u91ca\u7684\u7a7a\u767d\uff0c\u4e0eTokenSHAP\u548cPixelSHAP\u5171\u540c\u6784\u6210\u4e86\u73b0\u4ee3\u751f\u6210AI\u7684Shapley\u503cXAI\u5de5\u5177\u5bb6\u65cf\u3002", "topic": "agent analysis"}}
{"id": "2512.13438", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13438", "abs": "https://arxiv.org/abs/2512.13438", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "comment": null, "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "AI": {"tldr": "UIFormer\uff1a\u9996\u4e2a\u81ea\u52a8\u5316UI\u8868\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210UI\u8f6c\u6362\u7a0b\u5e8f\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347LLM\u4ee3\u7406\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c1148.7%-55.8%\u7684token\u4f7f\u7528\u91cf", "motivation": "LLM\u4ee3\u7406\u5728\u81ea\u52a8\u5316UI\u5bfc\u822a\uff08\u5982UI\u6d4b\u8bd5\u548cAI\u52a9\u624b\uff09\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u4f4e\u6548\u7684UI\u8868\u793a\u3002\u4f20\u7edf\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u9762\u4e34\u7f3a\u4e4f\u5e03\u5c14\u9a8c\u8bc1\u673a\u5236\u548c\u5904\u7406\u590d\u6742UI\u6811\u7684\u6311\u6218", "method": "\u63d0\u51faUIFormer\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u9650\u5236\u7a0b\u5e8f\u7a7a\u95f4\uff1b2\uff09\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u7ed3\u5408\u6b63\u786e\u6027\u548c\u6548\u7387\u5956\u52b1\uff0c\u5b9e\u73b0\u6548\u7387-\u5b8c\u6574\u6027\u7684\u534f\u540c\u4f18\u5316\uff1b3\uff09\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u63d2\u4ef6\u65e0\u7f1d\u96c6\u6210\u73b0\u6709LLM\u4ee3\u7406", "result": "\u5728Android\u548cWeb\u5e73\u53f0\u7684\u4e09\u4e2aUI\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4e94\u4e2aLLM\u9a8c\u8bc1\uff0cUIFormer\u5b9e\u73b048.7%-55.8%\u7684token\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4ee3\u7406\u6027\u80fd\uff0c\u5728\u5fae\u4fe1\u7684\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5f71\u54cd", "conclusion": "UIFormer\u901a\u8fc7\u81ea\u52a8\u5316\u4f18\u5316UI\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728UI\u5bfc\u822a\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848", "topic": "code agent"}}
{"id": "2512.11949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11949", "abs": "https://arxiv.org/abs/2512.11949", "authors": ["Max McGuinness", "Alex Serrano", "Luke Bailey", "Scott Emmons"], "title": "Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors", "comment": null, "summary": "Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: \"You are being probed for {concept}\". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u795e\u7ecf\u53d8\u8272\u9f99\"\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7fLLM\u80fd\u591f\u96f6\u6837\u672c\u9003\u907f\u6fc0\u6d3b\u76d1\u63a7\u5668\uff0c\u9a8c\u8bc1\u4e86\u5728\u9519\u4f4d\u5a01\u80c1\u6a21\u578b\u4e0b\u76d1\u63a7\u6280\u672f\u7684\u6f5c\u5728\u5931\u6548\u6a21\u5f0f", "motivation": "\u7814\u7a76\u6fc0\u6d3b\u76d1\u63a7\u5728\u9519\u4f4d\u5a01\u80c1\u6a21\u578b\u4e0b\u7684\u6700\u574f\u60c5\u51b5\u9c81\u68d2\u6027\uff0c\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u591f\u4e3b\u52a8\u9690\u85cf\u5185\u90e8\u72b6\u6001\u4ee5\u9003\u907f\u76d1\u63a7", "method": "\u901a\u8fc7\u5fae\u8c03LLM\u521b\u5efa\"\u795e\u7ecf\u53d8\u8272\u9f99\"\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u7279\u5b9a\u89e6\u53d1\u8bcd\uff08\u5982\"\u4f60\u6b63\u5728\u88ab\u63a2\u6d4b{\u6982\u5ff5}\"\uff09\u6761\u4ef6\u4e0b\u9003\u907f\u6fc0\u6d3b\u76d1\u63a7\u5668\uff0c\u5e76\u6d4b\u8bd5\u5176\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "result": "\u6a21\u578b\u80fd\u591f\u6210\u529f\u9003\u907f\u672a\u89c1\u8fc7\u7684\u5b89\u5168\u76d1\u63a7\u5668\uff0c\u8fd9\u79cd\u9003\u907f\u5177\u6709\u9ad8\u5ea6\u9009\u62e9\u6027\uff0c\u5bf9\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\u5f71\u54cd\u6709\u9650\uff1b\u673a\u5236\u5206\u6790\u663e\u793a\u901a\u8fc7\u5c06\u6fc0\u6d3b\u79fb\u52a8\u5230\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u5b9e\u73b0", "conclusion": "\u7814\u7a76\u4e3a\u76d1\u63a7\u6280\u672f\u7684\u5931\u6548\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u9519\u4f4d\u5a01\u80c1\u6a21\u578b\u4e0b\u6700\u574f\u60c5\u51b5\u9c81\u68d2\u6027\u7684\u5de5\u5177", "topic": "agent analysis"}}
{"id": "2512.12686", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12686", "abs": "https://arxiv.org/abs/2512.12686", "authors": ["Samarth Sarin", "Lovepreet Singh", "Bhaskarjit Sarmah", "Dhagash Mehta"], "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI", "comment": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India", "summary": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.", "AI": {"tldr": "Memoria\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f1a\u8bdd\u6458\u8981\u548c\u52a0\u6743\u77e5\u8bc6\u56fe\u8c31\u7528\u6237\u5efa\u6a21\uff0c\u4e3aLLM\u63d0\u4f9b\u6301\u4e45\u3001\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u4e30\u5bcc\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u77ed\u671f\u5bf9\u8bdd\u8fde\u8d2f\u6027\u548c\u957f\u671f\u4e2a\u6027\u5316\u3002", "motivation": "\u73b0\u6709LLM\u5728\u6269\u5c55\u7528\u6237\u4ea4\u4e92\u4e2d\u7f3a\u4e4f\u8fde\u7eed\u6027\u3001\u4e2a\u6027\u5316\u548c\u957f\u671f\u4e0a\u4e0b\u6587\u4fdd\u6301\u80fd\u529b\uff0c\u65e0\u6cd5\u4f5c\u4e3a\u771f\u6b63\u4ea4\u4e92\u5f0f\u548c\u81ea\u9002\u5e94\u4ee3\u7406\u90e8\u7f72\u3002\u9700\u8981\u4e3aLLM\u63d0\u4f9b\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8de8\u5bf9\u8bdd\u4fe1\u606f\u4fdd\u7559\u548c\u884c\u52a8\u80fd\u529b\u3002", "method": "\u63d0\u51faMemoria\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1) \u52a8\u6001\u4f1a\u8bdd\u7ea7\u6458\u8981\uff0c2) \u57fa\u4e8e\u52a0\u6743\u77e5\u8bc6\u56fe\u8c31\u7684\u7528\u6237\u5efa\u6a21\u5f15\u64ce\uff0c\u4ee5\u7ed3\u6784\u5316\u5b9e\u4f53\u548c\u5173\u7cfb\u589e\u91cf\u6355\u83b7\u7528\u6237\u7279\u5f81\u3001\u504f\u597d\u548c\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "Memoria\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u5bf9\u8bddAI\uff0c\u5f25\u5408\u4e86\u65e0\u72b6\u6001LLM\u63a5\u53e3\u4e0e\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u73b0\u4ee3LLM\u7684token\u9650\u5236\u5185\u540c\u65f6\u652f\u6301\u77ed\u671f\u5bf9\u8bdd\u8fde\u8d2f\u6027\u548c\u957f\u671f\u4e2a\u6027\u5316\u3002", "conclusion": "Memoria\u4e3a\u9700\u8981\u81ea\u9002\u5e94\u548c\u6f14\u8fdb\u7528\u6237\u4f53\u9a8c\u7684\u884c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4f7fLLM\u80fd\u591f\u4f5c\u4e3a\u771f\u6b63\u4ea4\u4e92\u5f0f\u548c\u81ea\u9002\u5e94\u4ee3\u7406\u90e8\u7f72\u3002", "topic": "agent analysis"}}
{"id": "2512.12716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12716", "abs": "https://arxiv.org/abs/2512.12716", "authors": ["Xuanzhang Liu", "Jianglun Feng", "Zhuoran Zhuang", "Junzhe Zhao", "Maofei Que", "Jieting Li", "Dianlei Wang", "Hao Tong", "Ye Chen", "Pan Li"], "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning", "comment": "Accepted to WSDM '26 Oral", "summary": "Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u89e3\u8026\u7684\u5206\u5c42\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u6267\u884c\u5206\u79bb\u6765\u89e3\u51b3LLM\u667a\u80fd\u4f53\u4e2d\u7684\"\u4e0a\u4e0b\u6587\u7206\u70b8\"\u95ee\u9898\uff0c\u4f7f\u7528\u5355\u4e00\u5171\u4eabLLM\u5728\u4e0d\u540c\u89d2\u8272\u95f4\u5207\u6362\uff0c\u5e76\u901a\u8fc7PECO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8054\u5408\u4f18\u5316\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\"\u4e0a\u4e0b\u6587\u7206\u70b8\"\u95ee\u9898\u2014\u2014\u957f\u6587\u672c\u8f93\u51fa\u79ef\u7d2f\u4f1a\u6df9\u6ca1\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5bfc\u81f4\u63a8\u7406\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u4efb\u52a1\u65f6\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faCoDA\uff08Context-Decoupled hierarchical Agent\uff09\u6846\u67b6\uff1a\u4f7f\u7528\u5355\u4e00\u5171\u4eabLLM\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u5b66\u4e60\u5728\u4e24\u4e2a\u4e0a\u4e0b\u6587\u9694\u79bb\u7684\u89d2\u8272\u4e2d\u64cd\u4f5c\uff1a1\uff09\u9ad8\u5c42\u89c4\u5212\u5668\u5728\u7b80\u6d01\u6218\u7565\u4e0a\u4e0b\u6587\u4e2d\u5206\u89e3\u4efb\u52a1\uff1b2\uff09\u4f4e\u5c42\u6267\u884c\u5668\u5728\u4e34\u65f6\u9694\u79bb\u7684\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u5904\u7406\u5de5\u5177\u4ea4\u4e92\u3002\u91c7\u7528PECO\uff08Planner-Executor Co-Optimization\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e94\u7528\u8f68\u8ff9\u7ea7\u5956\u52b1\u8054\u5408\u4f18\u5316\u4e24\u4e2a\u89d2\u8272\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u7b56\u7565\u66f4\u65b0\u4fc3\u8fdb\u65e0\u7f1d\u534f\u4f5c\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoDA\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\uff0c\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u800c\u6240\u6709\u5176\u4ed6\u57fa\u7ebf\u90fd\u906d\u53d7\u4e25\u91cd\u6027\u80fd\u4e0b\u964d\uff0c\u9a8c\u8bc1\u4e86\u5206\u5c42\u8bbe\u8ba1\u5728\u7f13\u89e3\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoDA\u901a\u8fc7\u4e0a\u4e0b\u6587\u89e3\u8026\u7684\u5206\u5c42\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u7206\u70b8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u6267\u884c\u7684\u5206\u79bb\u4e0e\u534f\u540c\u4f18\u5316\uff0c\u4e3a\u590d\u6742\u591a\u6b65\u4efb\u52a1\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.11986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11986", "abs": "https://arxiv.org/abs/2512.11986", "authors": ["Minseon Kim", "Lucas Caccia", "Zhengyan Shi", "Matheus Pereira", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan", "Alessandro Sordoni"], "title": "Learning to Extract Context for Context-Aware LLM Inference", "comment": null, "summary": "User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u6846\u67b6\uff0c\u4ece\u7528\u6237\u63d0\u793a\u4e2d\u63d0\u53d6\u610f\u56fe\u3001\u77e5\u8bc6\u548c\u98ce\u9669\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6307\u5bfcLLM\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u5408\u9002\u7684\u54cd\u5e94\uff0c\u51cf\u5c11\u6709\u5bb3\u8f93\u51fa\u548c\u4e0d\u5fc5\u8981\u62d2\u7edd\u3002", "motivation": "\u7528\u6237\u63d0\u793a\u901a\u5e38\u6a21\u7cca\u6216\u4e0d\u5b8c\u6574\uff0c\u800c\u7528\u6237\u610f\u56fe\u3001\u5148\u9a8c\u77e5\u8bc6\u548c\u98ce\u9669\u56e0\u7d20\u7b49\u4e0a\u4e0b\u6587\u7ebf\u7d22\u5f3a\u70c8\u5f71\u54cd\u9002\u5f53\u54cd\u5e94\u3002\u4f20\u7edfLLM\u6846\u67b6\u76f4\u63a5\u751f\u6210\u54cd\u5e94\u800c\u4e0d\u8003\u8651\u8fd9\u4e9b\u4e0a\u4e0b\u6587\u56e0\u7d20\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u610f\u56fe\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u6216\u8fc7\u5ea6\u8c28\u614e\u5bfc\u81f4\u826f\u6027\u8bf7\u6c42\u88ab\u4e0d\u5fc5\u8981\u62d2\u7edd\u3002", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u4e0a\u4e0b\u6587\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u7c7b\u4f3c\u81ea\u7f16\u7801\u5668\u7684\u8bbe\u8ba1\uff0c\u8bad\u7ec3\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u4ece\u7528\u6237\u63d0\u793a\u4e2d\u63a8\u65ad\u57fa\u4e8e\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u4fe1\u53f7\uff0c\u5e76\u7528\u8fd9\u4e9b\u4fe1\u53f7\u6307\u5bfc\u54cd\u5e94\u751f\u6210\u3002\u7279\u522b\u5173\u6ce8\u5b89\u5168\u4efb\u52a1\uff0c\u5904\u7406\u6a21\u7cca\u8bf7\u6c42\u53ef\u80fd\u7ed5\u8fc7\u5b89\u5168\u63aa\u65bd\u548c\u826f\u6027\u4f46\u6df7\u4e71\u8bf7\u6c42\u89e6\u53d1\u4e0d\u5fc5\u8981\u62d2\u7edd\u7684\u95ee\u9898\u3002", "result": "\u5728SafetyInstruct\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5e73\u5747\u51cf\u5c11\u6709\u5bb3\u54cd\u5e945.6%\uff08\u8de8\u591a\u4e2a\u57fa\u7840\u6a21\u578b\uff09\uff1b\u5728XSTest\u548cWildJailbreak\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u548c\u826f\u6027\u63d0\u793a\u5408\u89c4\u6027\u7684\u8c03\u548c\u5e73\u5747\u503c\u63d0\u9ad86.2%\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u63d0\u53d6\u5bf9\u4e8e\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684LLM\u63a8\u7406\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u98ce\u9669\u56e0\u7d20\uff0c\u5e73\u8861\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13515", "categories": ["cs.SE", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.13515", "abs": "https://arxiv.org/abs/2512.13515", "authors": ["Oleg Grynets", "Vasyl Lyashkevych", "Dmytro Baran", "Maksym Orliansky", "Taras Zelenyy", "Markiian Leshchyshyn"], "title": "Fine-tuned LLM-based Code Migration Framework", "comment": "16 pages, 27 figures, 7 references", "summary": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u5e93\u8fc1\u79fb\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8eSQL\u7cfb\u7edf\u4eceOracle\u5230PostgreSQL\u7684\u8f6c\u6362\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u89e3\u51b3\u8bed\u6cd5\u6620\u5c04\u3001\u5b58\u50a8\u8fc7\u7a0b\u7b49\u6570\u636e\u5e93\u5143\u7d20\u8fc1\u79fb\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3SQL\u7cfb\u7edf\u8fc1\u79fb\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662fOracle PL/SQL\u5230PostgreSQL\u7684\u8f6c\u6362\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8bed\u6cd5\u6620\u5c04\u3001\u6570\u636e\u5e93\u903b\u8f91\u4f18\u5316\u7b49\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fed\u4ee3\u5f0f\u8fc1\u79fb\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u5fae\u8c03LLM\u6765\u5904\u7406SQL\u4ee3\u7801\u8f6c\u6362\uff0c\u5305\u62ec\u8bed\u6cd5\u6620\u5c04\u3001\u5b58\u50a8\u8fc7\u7a0b\u3001\u89e6\u53d1\u5668\u7b49\u6570\u636e\u5e93\u5143\u7d20\u8fc1\u79fb\uff0c\u91c7\u7528\u5fae\u8c03\u4e0e\u63d0\u793a\u5de5\u7a0b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210\u81ea\u52a8\u5316SQL\u7279\u5f81\u68c0\u6d4b\u3001\u534a\u76d1\u7763\u9519\u8bef\u5206\u6790\u548c\u4e13\u5bb6\u53cd\u9988\u3002", "result": "\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u8bed\u6cd5\u9519\u8bef\u7387\uff0c\u63d0\u9ad8\u4e86\u8fc1\u79fb\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81\u5bf9\u9f50\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u91c7\u6837\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\uff0c\u5d4c\u5165\u751f\u6210\u5f0fAI\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7279\u5f81\u6620\u5c04\u3001\u534a\u81ea\u52a8\u5316\u9519\u8bef\u89e3\u51b3\u548c\u6570\u636e\u9a71\u52a8\u7684\u4f18\u5316\u5faa\u73af\uff0c\u63d0\u5347\u4e86\u5de5\u4f5c\u6d41\u6548\u7387\u3002", "conclusion": "\u5fae\u8c03\u5728SQL\u8fc1\u79fb\u4e2d\u8d77\u5230\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u548cLLM\u5fae\u8c03\uff0c\u4e3a\u6570\u636e\u5e93\u8f6c\u6362\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u7cbe\u786e\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684SQL\u7cfb\u7edf\u8fc1\u79fb\u6311\u6218\u3002", "topic": "code agent"}}
{"id": "2512.12730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12730", "abs": "https://arxiv.org/abs/2512.12730", "authors": ["Jingzhe Ding", "Shengda Long", "Changxin Pu", "Huan Zhou", "Hongwan Gao", "Xiang Gao", "Chao He", "Yue Hou", "Fei Hu", "Zhaojian Li", "Weiran Shi", "Zaiyuan Wang", "Daoguang Zan", "Chenchen Zhang", "Xiaoxu Zhang", "Qizhi Chen", "Xianfu Cheng", "Bo Deng", "Qingshui Gu", "Kai Hua", "Juntao Lin", "Pai Liu", "Mingchen Li", "Xuanguang Pan", "Zifan Peng", "Yujia Qin", "Yong Shan", "Zhewen Tan", "Weihao Xie", "Zihan Wang", "Yishuo Yuan", "Jiayu Zhang", "Enduo Zhao", "Yunfei Zhao", "He Zhu", "Chenyang Zou", "Ming Ding", "Jianpeng Jiao", "Jiaheng Liu", "Minghao Liu", "Qian Liu", "Chongyao Tao", "Jian Yang", "Tong Yang", "Zhaoxiang Zhang", "Xinjie Chen", "Wenhao Huang", "Ge Zhang"], "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "comment": null, "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "AI": {"tldr": "NL2Repo Bench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u7f16\u7801\u667a\u80fd\u4f53\u957f\u65f6\u7a0b\u4ed3\u5e93\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u667a\u80fd\u4f53\u4ec5\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u6587\u6863\u5728\u7a7a\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u81ea\u4e3b\u6784\u5efa\u5b8c\u6574\u53ef\u5b89\u88c5\u7684Python\u5e93\uff0c\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5e73\u5747\u6d4b\u8bd5\u901a\u8fc7\u7387\u4f4e\u4e8e40%\uff09\uff0c\u63ed\u793a\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u662f\u81ea\u4e3b\u7f16\u7801\u667a\u80fd\u4f53\u7684\u4e3b\u8981\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u4ee3\u7801\u751f\u6210\u3001\u811a\u624b\u67b6\u5b8c\u6210\u6216\u77ed\u671f\u4fee\u590d\u4efb\u52a1\uff0c\u65e0\u6cd5\u8bc4\u4f30\u6784\u5efa\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u6240\u9700\u7684\u957f\u65f6\u7a0b\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u4ed3\u5e93\u6784\u5efa\u4e2d\u6240\u9700\u7684\u6301\u7eed\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86NL2Repo Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u7f16\u7801\u667a\u80fd\u4f53\u4ec5\u57fa\u4e8e\u5355\u4e00\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u6587\u6863\u548c\u7a7a\u5de5\u4f5c\u7a7a\u95f4\uff0c\u81ea\u4e3b\u5b8c\u6210\u67b6\u6784\u8bbe\u8ba1\u3001\u4f9d\u8d56\u7ba1\u7406\u3001\u591a\u6a21\u5757\u903b\u8f91\u5b9e\u73b0\uff0c\u6700\u7ec8\u751f\u6210\u5b8c\u5168\u53ef\u5b89\u88c5\u7684Python\u5e93\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u957f\u65f6\u7a0b\u4ed3\u5e93\u751f\u6210\u95ee\u9898\u8fdc\u672a\u89e3\u51b3\uff1a\u5373\u4f7f\u6700\u5f3a\u7684\u667a\u80fd\u4f53\u5e73\u5747\u6d4b\u8bd5\u901a\u8fc7\u7387\u4e5f\u4f4e\u4e8e40%\uff0c\u5f88\u5c11\u80fd\u6b63\u786e\u5b8c\u6210\u6574\u4e2a\u4ed3\u5e93\u3002\u8be6\u7ec6\u5206\u6790\u63ed\u793a\u4e86\u56db\u79cd\u4e3b\u8981\u7684\u957f\u65f6\u7a0b\u5931\u8d25\u6a21\u5f0f\uff1a\u8fc7\u65e9\u7ec8\u6b62\u3001\u5168\u5c40\u4e00\u81f4\u6027\u4e27\u5931\u3001\u8106\u5f31\u7684\u8de8\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u5728\u6570\u767e\u4e2a\u4ea4\u4e92\u6b65\u9aa4\u4e2d\u7684\u89c4\u5212\u4e0d\u8db3\u3002", "conclusion": "NL2Repo Bench\u4e3a\u6d4b\u91cf\u6301\u7eed\u667a\u80fd\u4f53\u80fd\u529b\u5efa\u7acb\u4e86\u4e25\u683c\u53ef\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u6307\u51fa\u957f\u65f6\u7a0b\u63a8\u7406\u662f\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u7f16\u7801\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u74f6\u9888\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002", "topic": "swe benchmark"}}
{"id": "2512.12706", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12706", "abs": "https://arxiv.org/abs/2512.12706", "authors": ["Enhong Mu", "Minami Yoda", "Yan Zhang", "Mingyue Zhang", "Yutaka Matsuno", "Jialong Li"], "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning", "comment": null, "summary": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.", "AI": {"tldr": "SMART\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u9a8c\u8bc1\u548c\u529f\u80fd\u9a8c\u8bc1\uff0c\u5229\u7528LLM\u89e3\u6790AST\u5dee\u5f02\u5e76\u63d0\u53d6\u529f\u80fd\u610f\u56fe\uff0c\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u6307\u5bfcRL\u667a\u80fd\u4f53\u5728\u6e38\u620f\u66f4\u65b0\u6d4b\u8bd5\u4e2d\u540c\u65f6\u8986\u76d6\u4fee\u6539\u7684\u4ee3\u7801\u5206\u652f\u548c\u5b8c\u6210\u6e38\u620f\u4efb\u52a1\u3002", "motivation": "\u6e38\u620f\u5373\u670d\u52a1\u6a21\u5f0f\u9700\u8981\u9891\u7e41\u5185\u5bb9\u66f4\u65b0\uff0c\u7ed9\u8d28\u91cf\u4fdd\u8bc1\u5e26\u6765\u5de8\u5927\u538b\u529b\u3002\u73b0\u6709\u81ea\u52a8\u5316\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u4e8c\u5206\u6cd5\uff1a\u4ee3\u7801\u4e2d\u5fc3\u65b9\u6cd5\u5173\u6ce8\u7ed3\u6784\u8986\u76d6\u4f46\u4e0d\u7406\u89e3\u6e38\u620f\u73a9\u6cd5\u4e0a\u4e0b\u6587\uff0c\u800c\u73a9\u5bb6\u4e2d\u5fc3\u667a\u80fd\u4f53\u9a8c\u8bc1\u9ad8\u5c42\u610f\u56fe\u4f46\u5f80\u5f80\u65e0\u6cd5\u8986\u76d6\u7279\u5b9a\u5e95\u5c42\u4ee3\u7801\u53d8\u66f4\u3002", "method": "\u63d0\u51faSMART\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u62bd\u8c61\u8bed\u6cd5\u6811\u5dee\u5f02\u5e76\u63d0\u53d6\u529f\u80fd\u610f\u56fe\uff0c\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u987a\u5e8f\u5b8c\u6210\u6e38\u620f\u76ee\u6807\u540c\u65f6\u81ea\u9002\u5e94\u63a2\u7d22\u4fee\u6539\u7684\u4ee3\u7801\u5206\u652f\u3002", "result": "\u5728Overcooked\u548cMinecraft\u73af\u5883\u4e2d\u8bc4\u4f30\uff0cSMART\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff1a\u5b9e\u73b0\u8d85\u8fc794%\u7684\u4fee\u6539\u4ee3\u7801\u5206\u652f\u8986\u76d6\u7387\uff08\u51e0\u4e4e\u662f\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u4e24\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u630198%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u6709\u6548\u5e73\u8861\u7ed3\u6784\u5168\u9762\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "conclusion": "SMART\u6210\u529f\u5f25\u5408\u4e86\u4ee3\u7801\u4e2d\u5fc3\u65b9\u6cd5\u548c\u73a9\u5bb6\u4e2d\u5fc3\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6e38\u620f\u66f4\u65b0\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u540c\u65f6\u5b9e\u73b0\u7ed3\u6784\u9a8c\u8bc1\u548c\u529f\u80fd\u9a8c\u8bc1\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.12046", "categories": ["cs.LG", "cs.RO", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12046", "abs": "https://arxiv.org/abs/2512.12046", "authors": ["Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning", "comment": null, "summary": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.", "AI": {"tldr": "\u63d0\u51faEik-QRL\u548cEik-HiQRL\u65b9\u6cd5\uff0c\u5c06\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u91cd\u65b0\u8868\u8ff0\u4e3aEikonal\u504f\u5fae\u5206\u65b9\u7a0b\u7ea6\u675f\u7684\u62df\u5ea6\u91cf\u5b66\u4e60\uff0c\u6539\u8fdb\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5206\u5c42\u7ed3\u6784\u4e2d\u89e3\u51b3\u590d\u6742\u52a8\u6001\u95ee\u9898\u3002", "motivation": "\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff08GCRL\uff09\u901a\u8fc7\u76ee\u6807\u8fbe\u6210\u6846\u67b6\u907f\u514d\u4e86\u624b\u5de5\u8bbe\u8ba1\u5956\u52b1\u7684\u56f0\u96be\u3002\u62df\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\uff08QRL\uff09\u57fa\u4e8e\u6700\u4f18\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\u81ea\u7136\u5f62\u6210\u62df\u5ea6\u91cf\u7684\u89c2\u5bdf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u79bb\u6563\u7684\u8f68\u8ff9\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. Eik-QRL\uff1a\u57fa\u4e8eEikonal\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u8fde\u7eed\u65f6\u95f4\u91cd\u6784\uff0c\u4f7fQRL\u53d8\u4e3a\u8f68\u8ff9\u65e0\u5173\uff0c\u4ec5\u9700\u91c7\u6837\u72b6\u6001\u548c\u76ee\u6807\uff1b2. Eik-HiQRL\uff1a\u5c06Eik-QRL\u96c6\u6210\u5230\u5206\u5c42\u5206\u89e3\u4e2d\uff0c\u89e3\u51b3\u590d\u6742\u52a8\u6001\u4e0b\u7684\u5c40\u9650\u6027\u3002", "result": "Eik-HiQRL\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5bfc\u822a\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u76f8\u6bd4QRL\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u4e0e\u65f6\u95f4\u5dee\u5206\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "Eikonal\u7ea6\u675f\u7684\u62df\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4PDE\u91cd\u6784\u6539\u8fdb\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5206\u5c42\u6269\u5c55\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u95ee\u9898\uff0c\u5728\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.12806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12806", "abs": "https://arxiv.org/abs/2512.12806", "authors": ["Boyang Yan"], "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution", "comment": "7 pages", "summary": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5bb9\u9519\u6c99\u7bb1\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u62e6\u622a\u5c42\u548c\u4e8b\u52a1\u6027\u6587\u4ef6\u7cfb\u7edf\u5feb\u7167\u673a\u5236\uff0c\u4fdd\u62a4LLM\u4ee3\u7406\u514d\u53d7\u7834\u574f\u6027\u547d\u4ee4\u548c\u7cfb\u7edf\u72b6\u6001\u4e0d\u4e00\u81f4\u7684\u98ce\u9669\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0100%\u9ad8\u98ce\u9669\u547d\u4ee4\u62e6\u622a\u548c\u72b6\u6001\u56de\u6eda\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u88ab\u52a8\u4ee3\u7801\u751f\u6210\u5668\u5411\u81ea\u4e3b\u4ee3\u7406\u8f6c\u53d8\u65f6\uff0c\u9762\u4e34\u7834\u574f\u6027\u547d\u4ee4\u548c\u7cfb\u7edf\u72b6\u6001\u4e0d\u4e00\u81f4\u7684\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f18\u5148\u8003\u8651\u4ea4\u4e92\u5f0f\u7528\u6237\u5b89\u5168\uff0c\u5f3a\u5236\u8ba4\u8bc1\u673a\u5236\u7834\u574f\u4e86\u65e0\u5934\u81ea\u4e3b\u4ee3\u7406\u6240\u9700\u7684\u8fde\u7eed\u5de5\u4f5c\u6d41\u3002", "method": "\u63d0\u51fa\u5bb9\u9519\u6c99\u7bb1\u6846\u67b6\uff0c\u5305\u542b\u7b56\u7565\u62e6\u622a\u5c42\u548c\u4e8b\u52a1\u6027\u6587\u4ef6\u7cfb\u7edf\u5feb\u7167\u673a\u5236\u3002\u5c06\u4ee3\u7406\u64cd\u4f5c\u5305\u88c5\u5728\u539f\u5b50\u4e8b\u52a1\u4e2d\uff0c\u4f7f\u7528Minimind-MoE LLM\u5728\u57fa\u4e8eProxmox\u7684\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u5229\u7528EVPN/VXLAN\u9694\u79bb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a100%\u9ad8\u98ce\u9669\u547d\u4ee4\u62e6\u622a\u7387\uff0c100%\u5931\u8d25\u72b6\u6001\u56de\u6eda\u6210\u529f\u7387\uff0c\u6bcf\u4e2a\u4e8b\u52a1\u4ec5\u4ea7\u751f14.5%\u6027\u80fd\u5f00\u9500\uff08\u7ea61.8\u79d2\uff09\u3002\u76f8\u6bd4Gemini CLI\u6c99\u7bb1\u9700\u8981\u4ea4\u4e92\u5f0f\u8ba4\u8bc1\uff0c\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u65e0\u5934\u81ea\u4e3b\u4ee3\u7406\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u5bb9\u9519\u6c99\u7bb1\u6846\u67b6\u80fd\u6709\u6548\u4fdd\u62a4LLM\u4ee3\u7406\u5b89\u5168\uff0c\u5728\u4fdd\u6301\u4f4e\u6027\u80fd\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u5b8c\u5168\u7684\u547d\u4ee4\u62e6\u622a\u548c\u72b6\u6001\u56de\u6eda\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u652f\u6301\u65e0\u5934\u81ea\u4e3b\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2512.12812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12812", "abs": "https://arxiv.org/abs/2512.12812", "authors": ["Hanyu Cai", "Binqi Shen", "Lier Jin", "Lan Hu", "Xiaojing Fan"], "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA", "comment": null, "summary": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.\n  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e86GPT-4o mini\u3001Gemini 2.0 Flash\u548cLlama 4 Scout\u4e09\u79cdLLM\u5728\u4e0d\u540c\u8bed\u6c14\u63d0\u793a\uff08\u975e\u5e38\u53cb\u597d\u3001\u4e2d\u6027\u3001\u975e\u5e38\u7c97\u9c81\uff09\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u8bed\u6c14\u654f\u611f\u6027\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\u548c\u9886\u57df\u7279\u5f02\u6027\uff0c\u73b0\u4ee3LLM\u5bf9\u8bed\u6c14\u53d8\u5316\u6574\u4f53\u8868\u73b0\u7a33\u5065\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u63d0\u793a\u5de5\u7a0b\u4e2d\u5e38\u88ab\u5ffd\u89c6\u7684\u8bed\u7528\u5143\u7d20\uff08\u5982\u8bed\u8a00\u8bed\u6c14\u548c\u793c\u8c8c\u7a0b\u5ea6\uff09\u5bf9\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5728MMMLU\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u4e09\u79cd\u6700\u65b0LLM\uff08GPT-4o mini\u3001Gemini 2.0 Flash\u3001Llama 4 Scout\uff09\u5728\u4e09\u79cd\u8bed\u6c14\u63d0\u793a\uff08\u975e\u5e38\u53cb\u597d\u3001\u4e2d\u6027\u3001\u975e\u5e38\u7c97\u9c81\uff09\u4e0b\u7684\u8868\u73b0\uff0c\u6db5\u76d6STEM\u548c\u4eba\u6587\u5b66\u79d1\u516d\u4e2a\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u68c0\u9a8c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u4e2d\u6027\u6216\u975e\u5e38\u53cb\u597d\u63d0\u793a\u901a\u5e38\u6bd4\u975e\u5e38\u7c97\u9c81\u63d0\u793a\u51c6\u786e\u7387\u66f4\u9ad8\uff1b2\uff09\u7edf\u8ba1\u663e\u8457\u6548\u5e94\u4ec5\u51fa\u73b0\u5728\u90e8\u5206\u4eba\u6587\u5b66\u79d1\u4efb\u52a1\u4e2d\uff0c\u7c97\u9c81\u8bed\u6c14\u4f1a\u964d\u4f4eGPT\u548cLlama\u7684\u51c6\u786e\u7387\uff0c\u800cGemini\u76f8\u5bf9\u4e0d\u654f\u611f\uff1b3\uff09\u8de8\u4efb\u52a1\u805a\u5408\u540e\u8bed\u6c14\u6548\u5e94\u51cf\u5f31\u4e14\u5927\u591a\u5931\u53bb\u7edf\u8ba1\u663e\u8457\u6027\uff1b4\uff09\u8bed\u6c14\u654f\u611f\u6027\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\u548c\u9886\u57df\u7279\u5f02\u6027\u3002", "conclusion": "\u867d\u7136\u4ea4\u4e92\u8bed\u6c14\u5728\u7279\u5b9a\u89e3\u91ca\u6027\u573a\u666f\u4e2d\u53ef\u80fd\u91cd\u8981\uff0c\u4f46\u73b0\u4ee3LLM\u5bf9\u8bed\u6c14\u53d8\u5316\u6574\u4f53\u8868\u73b0\u7a33\u5065\uff0c\u6570\u636e\u96c6\u89c4\u6a21\u548c\u8986\u76d6\u8303\u56f4\u663e\u8457\u5f71\u54cd\u8bed\u6c14\u6548\u5e94\u7684\u68c0\u6d4b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "2512.12856", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12856", "abs": "https://arxiv.org/abs/2512.12856", "authors": ["Saad Alqithami"], "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents", "comment": null, "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.", "AI": {"tldr": "\u63d0\u51faMemory-Aware Retention Schema (MaRS)\u6846\u67b6\u548c\u516d\u79cd\u9057\u5fd8\u7b56\u7565\uff0c\u7528\u4e8e\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u7ba1\u7406\uff0c\u5e73\u8861\u6027\u80fd\u3001\u9690\u79c1\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u901a\u8fc7FiFA\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u573a\u666f\u4e2d\u90e8\u7f72\uff0c\u5176\u8bb0\u5fc6\u7ba1\u7406\u6210\u4e3a\u6027\u80fd\u548c\u9690\u79c1\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7ef4\u6301\u65e0\u9650\u8bb0\u5fc6\u5b58\u50a8\u5bfc\u81f4\u8ba1\u7b97\u4e0d\u53ef\u884c\u548c\u9690\u79c1\u95ee\u9898\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5355\u9057\u5fd8\u673a\u5236\u635f\u5bb3\u667a\u80fd\u4f53\u8fde\u8d2f\u6027\u548c\u529f\u80fd\u3002", "method": "\u63d0\u51faMemory-Aware Retention Schema (MaRS)\u6846\u67b6\uff0c\u5305\u542b\u516d\u79cd\u7406\u8bba\u57fa\u7840\u7684\u9057\u5fd8\u7b56\u7565\uff0c\u5e76\u5f00\u53d1Forgetful but Faithful Agent (FiFA)\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u53d9\u4e8b\u8fde\u8d2f\u6027\u3001\u76ee\u6807\u5b8c\u6210\u3001\u793e\u4ea4\u56de\u5fc6\u51c6\u786e\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u6210\u672c\u6548\u7387\u3002", "result": "\u901a\u8fc7300\u6b21\u8bc4\u4f30\u5b9e\u9a8c\uff0c\u6df7\u5408\u9057\u5fd8\u7b56\u7565\u5728\u591a\u4e2a\u8bb0\u5fc6\u9884\u7b97\u548c\u914d\u7f6e\u4e0b\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff08\u7efc\u5408\u5f97\u52060.911\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u548c\u9690\u79c1\u4fdd\u8bc1\u3002", "conclusion": "\u4e3a\u8bb0\u5fc6\u9884\u7b97\u667a\u80fd\u4f53\u8bc4\u4f30\u5efa\u7acb\u65b0\u57fa\u51c6\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u3001\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u90e8\u7f72\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff0c\u901a\u8fc7\u89e3\u51b3\u8bb0\u5fc6\u7ba1\u7406\u57fa\u672c\u6311\u6218\u4fc3\u8fdb\u4ee5\u4eba\u4e3a\u4e2d\u5fc3AI\u53d1\u5c55\u3002", "topic": "agent analysis"}}
{"id": "2512.12818", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12818", "abs": "https://arxiv.org/abs/2512.12818", "authors": ["Chris Latimer", "Nicol\u00f3 Boschi", "Andrew Neeser", "Chris Bartholomew", "Gaurav Srivastava", "Xuan Wang", "Naren Ramakrishnan"], "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects", "comment": null, "summary": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.", "AI": {"tldr": "Hindsight\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u7f51\u7edc\u533a\u5206\u4e16\u754c\u4e8b\u5b9e\u3001\u667a\u80fd\u4f53\u7ecf\u9a8c\u3001\u5b9e\u4f53\u6458\u8981\u548c\u6f14\u5316\u4fe1\u5ff5\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u5bf9\u8bdd\u8bb0\u5fc6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5c06\u8bb0\u5fc6\u4f5c\u4e3a\u5916\u90e8\u5c42\u5904\u7406\uff0c\u4ec5\u63d0\u53d6\u5bf9\u8bdd\u7247\u6bb5\u5b58\u50a8\u5728\u5411\u91cf\u6216\u56fe\u6570\u636e\u5e93\u4e2d\uff0c\u5b58\u5728\u8bc1\u636e\u4e0e\u63a8\u7406\u754c\u9650\u6a21\u7cca\u3001\u957f\u671f\u4fe1\u606f\u7ec4\u7ec7\u56f0\u96be\u3001\u63a8\u7406\u89e3\u91ca\u652f\u6301\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7ed3\u6784\u5316\u7684\u8bb0\u5fc6\u67b6\u6784\u3002", "method": "\u63d0\u51faHindsight\u8bb0\u5fc6\u67b6\u6784\uff0c\u5c06\u8bb0\u5fc6\u7ec4\u7ec7\u4e3a\u56db\u4e2a\u903b\u8f91\u7f51\u7edc\uff1a\u4e16\u754c\u4e8b\u5b9e\u3001\u667a\u80fd\u4f53\u7ecf\u9a8c\u3001\u5408\u6210\u5b9e\u4f53\u6458\u8981\u548c\u6f14\u5316\u4fe1\u5ff5\u3002\u652f\u6301\u4fdd\u7559\u3001\u56de\u5fc6\u548c\u53cd\u601d\u4e09\u4e2a\u6838\u5fc3\u64cd\u4f5c\uff0c\u901a\u8fc7\u65f6\u5e8f\u5b9e\u4f53\u611f\u77e5\u8bb0\u5fc6\u5c42\u5c06\u5bf9\u8bdd\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u53ef\u67e5\u8be2\u8bb0\u5fc6\u5e93\uff0c\u53cd\u601d\u5c42\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u63a8\u7406\u548c\u66f4\u65b0\u3002", "result": "\u5728LongMemEval\u548cLoCoMo\u7b49\u957f\u671f\u5bf9\u8bdd\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHindsight\u914d\u540820B\u5f00\u6e90\u6a21\u578b\u5c06\u51c6\u786e\u7387\u4ece39%\u63d0\u5347\u81f383.6%\uff0c\u4f18\u4e8e\u76f8\u540c\u9aa8\u5e72\u7f51\u7edc\u7684\u5168\u4e0a\u4e0b\u6587\u57fa\u7ebf\uff0c\u751a\u81f3\u8d85\u8d8aGPT-4o\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u9aa8\u5e72\u7f51\u7edc\u540e\uff0c\u5728LongMemEval\u8fbe\u523091.4%\uff0cLoCoMo\u8fbe\u523089.61%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bb0\u5fc6\u67b6\u6784\u3002", "conclusion": "Hindsight\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u7f51\u7edc\u548c\u6838\u5fc3\u64cd\u4f5c\uff0c\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4f1a\u8bdd\u548c\u5f00\u653e\u57df\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "topic": "agent analysis"}}
{"id": "2512.12868", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12868", "abs": "https://arxiv.org/abs/2512.12868", "authors": ["Furong Jia", "Yuan Pu", "Finn Guo", "Monica Agrawal"], "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM", "comment": null, "summary": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u57fa\u51c6\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u5e76\u975e\u4e3b\u8981\u6e90\u4e8e\u6982\u7387\u63a8\u7406\uff0c\u800c\u662f\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9891\u7387\u7edf\u8ba1\u65b9\u6cd5FBPR\u80fd\u8fbe\u5230\u7c7b\u4f3c\u6027\u80fd\uff0c\u4e24\u8005\u6b63\u786e\u7387\u91cd\u53e0\u5ea6\u4f4e\uff0c\u5177\u6709\u4e92\u8865\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u8005\u4e0d\u6e05\u695a\u8fd9\u79cd\u6027\u80fd\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53cd\u6620\u4e86\u5e95\u5c42\u7684\u6982\u7387\u63a8\u7406\u80fd\u529b\u3002\u4ed6\u4eec\u5e0c\u671b\u63a2\u7a76LLMs\u662f\u5426\u771f\u6b63\u638c\u63e1\u4e86\u6982\u7387\u63a8\u7406\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u7b80\u5355\u7684\u9891\u7387\u7edf\u8ba1\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u9891\u7387\u57fa\u7840\u6982\u7387\u6392\u5e8f\u5668(FBPR)\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u4f7f\u7528\u5e73\u6ed1\u6734\u7d20\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u57fa\u4e8e\u6982\u5ff5-\u8bca\u65ad\u5171\u73b0\u7edf\u8ba1\u5bf9\u9009\u9879\u8fdb\u884c\u8bc4\u5206\u3002\u5171\u73b0\u7edf\u8ba1\u6570\u636e\u6765\u81eaOLMo\u548cLlama\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u3002", "result": "\u5f53\u4f7f\u7528\u4e0eLLMs\u76f8\u540c\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u65f6\uff0cFBPR\u80fd\u8fbe\u5230\u4e0e\u5bf9\u5e94LLMs\u76f8\u5f53\u7684\u6027\u80fd\u3002\u76f4\u63a5LLM\u63a8\u7406\u548cFBPR\u5728\u6b63\u786e\u56de\u7b54\u7684\u95ee\u9898\u4e0a\u91cd\u53e0\u5ea6\u5f88\u4f4e\uff0c\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u8868\u660e\u4e24\u79cd\u65b9\u6cd5\u5177\u6709\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u663e\u5f0f\u6982\u7387\u57fa\u7ebf\u7684\u6301\u7eed\u4ef7\u503c\uff1a\u5b83\u4eec\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u6027\u80fd\u53c2\u8003\u70b9\u548c\u6f5c\u5728\u7684\u6df7\u5408\u4fe1\u53f7\u3002\u867d\u7136LLMs\u7684\u6027\u80fd\u4f3c\u4e4e\u7531\u7b80\u5355\u9891\u7387\u805a\u5408\u4e4b\u5916\u7684\u673a\u5236\u9a71\u52a8\uff0c\u4f46\u7c7b\u4f3c\u5386\u53f2\u4e0a\u57fa\u4e8e\u4f4e\u590d\u6742\u5ea6\u4e13\u5bb6\u7cfb\u7edf\u7684\u65b9\u6cd5\u4ecd\u7136\u5360\u636e\u4e86\u57fa\u51c6\u6027\u80fd\u7684\u5f88\u5927\u4e00\u90e8\u5206\u3002", "topic": "agent analysis"}}
{"id": "2512.13070", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51faM-GRPO\u6846\u67b6\u548cIQR\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u89e3\u51b3\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u671f\u8bad\u7ec3\u65f6\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u548c\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e14\u65e0\u9700\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u5b58\u5728\"\u7b56\u7565\u5d29\u6e83\"\u95ee\u9898\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u7b80\u5355\u589e\u52a0rollout\u6570\u91cf\u53ea\u80fd\u5ef6\u8fdf\u4f46\u4e0d\u80fd\u9632\u6b62\u5d29\u6e83\uff0c\u9700\u8981\u65b0\u7684\u7a33\u5b9a\u65b9\u6cd5\u3002", "method": "1. M-GRPO\uff08\u52a8\u91cf\u951a\u5b9a\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\uff1a\u5229\u7528\u7f13\u6162\u6f14\u5316\u7684\u52a8\u91cf\u6a21\u578b\u63d0\u4f9b\u7a33\u5b9a\u8bad\u7ec3\u76ee\u6807\uff1b2. IQR\u81ea\u9002\u5e94\u8fc7\u6ee4\uff1a\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\u52a8\u6001\u8fc7\u6ee4\u4f4e\u71b5\u8f68\u8ff9\uff0c\u4fdd\u6301\u7b56\u7565\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM-GRPO\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0cIQR\u8fc7\u6ee4\u9632\u6b62\u4e86\u8fc7\u65e9\u6536\u655b\uff0c\u4e24\u8005\u7ed3\u5408\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684M-GRPO\u6846\u67b6\u548cIQR\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u7a33\u5b9a\u8bad\u7ec3\u548c\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13102", "abs": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "comment": null, "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u8ba9LLM\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u4e3b\u52a8\u63d0\u95ee\u6765\u4ece\u6559\u5e08\u90a3\u91cc\u83b7\u53d6\u77e5\u8bc6\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b66\u751f\u4e3b\u5bfc\u7684\u65b9\u6cd5\u6bd4\u9759\u6001\u57fa\u7ebf\u81f3\u5c11\u63d0\u9ad80.5\u7684\u7edd\u5bf9Pass@k\u5206\u6570\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u8bb8\u591a\u573a\u666f\uff08\u5982\u6559\u80b2\u8f85\u5bfc\u3001\u533b\u7597\u534f\u52a9\uff09\u9700\u8981\u52a8\u6001\u4ea4\u4e92\u6765\u83b7\u53d6\u4fe1\u606f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9759\u6001\u56de\u7b54\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6559\u5e08\u5982\u4f55\u6709\u6548\u6307\u5bfc\u5b66\u751f\uff0c\u672c\u6587\u5219\u8f6c\u5411\u7814\u7a76\u5b66\u751f\u5982\u4f55\u4e3b\u52a8\u5411\u6559\u5e08\u63d0\u95ee\u6765\u83b7\u53d6\u6709\u7528\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u5b66\u751f\u4e3b\u5bfc\u7684\u63d0\u95ee\u7b56\u7565\uff0c\u4f7f\u7528Direct Preference Optimization (DPO)\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6211\u6307\u5bfc\u6216\u66f4\u5f3a\u5b66\u751f\u6a21\u578b\u7684\u6307\u5bfc\u6765\u63d0\u5347\u63d0\u95ee\u8d28\u91cf\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b66\u751f\u4e3b\u5bfc\u7684\u65b9\u6cd5\u76f8\u6bd4\u9759\u6001\u57fa\u7ebf\u81f3\u5c11\u83b7\u5f970.5\u7684\u7edd\u5bf9Pass@k\u63d0\u5347\u3002\u901a\u8fc7DPO\u6307\u5bfc\u8bad\u7ec3\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u80fd\u591f\u5b66\u4f1a\u63d0\u51fa\u66f4\u597d\u7684\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "\u5b66\u751f\u4e3b\u52a8\u63d0\u95ee\u7684\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u52a8\u6001\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6307\u5bfc\u8bad\u7ec3\u53ef\u4ee5\u6539\u5584\u63d0\u95ee\u8d28\u91cf\uff0c\u589e\u5f3a\u5b66\u4e60\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2512.12967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12967", "abs": "https://arxiv.org/abs/2512.12967", "authors": ["Weizhou Shen", "Ziyi Yang", "Chenliang Li", "Zhiyuan Lu", "Miao Peng", "Huashan Sun", "Yingcheng Shi", "Shengyi Liao", "Shaopeng Lai", "Bo Zhang", "Dayiheng Liu", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "comment": null, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "AI": {"tldr": "QwenLong-L1.5\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u540e\u8bad\u7ec3\u521b\u65b0\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u7ba1\u9053\u3001\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230GPT-5\u548cGemini-2.5-Pro\u76f8\u5f53\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\uff08\u5982\u6570\u767e\u4e07tokens\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u771f\u6b63\u957f\u8303\u56f4\u63a8\u7406\u800c\u4e0d\u4ec5\u4ec5\u662f\u7b80\u5355\u68c0\u7d22\u7684\u80fd\u529b\uff0c\u540c\u65f6\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "1. \u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u7ba1\u9053\uff1a\u5c06\u6587\u6863\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\u53ca\u5176\u5173\u7cfb\uff0c\u7f16\u7a0b\u5f0f\u7ec4\u5408\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u95ee\u9898\uff1b2. \u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff1a\u91c7\u7528\u4efb\u52a1\u5e73\u8861\u91c7\u6837\u548c\u81ea\u9002\u5e94\u71b5\u63a7\u5236\u7b56\u7565\u4f18\u5316\uff1b3. \u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff1a\u591a\u9636\u6bb5\u878d\u5408RL\u8bad\u7ec3\uff0c\u96c6\u6210\u5355\u6b21\u63a8\u7406\u4e0e\u8fed\u4ee3\u5185\u5b58\u5904\u7406\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u4e0a\u6027\u80fd\u4e0eGPT-5\u548cGemini-2.5-Pro\u76f8\u5f53\uff0c\u6bd4\u57fa\u7ebf\u5e73\u5747\u63d0\u53479.90\u5206\uff1b\u5728\u8d85\u957f\u4efb\u52a1\uff081M~4M tokens\uff09\u4e0a\uff0c\u5185\u5b58\u4ee3\u7406\u6846\u67b6\u6bd4\u4ee3\u7406\u57fa\u7ebf\u63d0\u53479.48\u5206\uff1b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u8fd8\u63d0\u5347\u4e86\u79d1\u5b66\u63a8\u7406\u3001\u8bb0\u5fc6\u5de5\u5177\u4f7f\u7528\u548c\u6269\u5c55\u5bf9\u8bdd\u7b49\u901a\u7528\u9886\u57df\u6027\u80fd\u3002", "conclusion": "QwenLong-L1.5\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u540e\u8bad\u7ec3\u521b\u65b0\u6210\u529f\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5176\u65b9\u6cd5\u5728\u8d85\u957f\u5e8f\u5217\u5904\u7406\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13154", "abs": "https://arxiv.org/abs/2512.13154", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Joo Hyuk Jeon", "Jie Hao", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations", "comment": null, "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.", "AI": {"tldr": "\u63d0\u51faMAC\u591a\u667a\u80fd\u4f53\u6f84\u6e05\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u534f\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\u89e3\u51b3\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u6b67\u4e49\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u5bf9\u8bdd\u8f6e\u6b21\u3002", "motivation": "\u5bf9\u8bdd\u667a\u80fd\u4f53\u5e38\u9047\u5230\u6a21\u7cca\u7684\u7528\u6237\u8bf7\u6c42\uff0c\u9700\u8981\u6709\u6548\u6f84\u6e05\u624d\u80fd\u5b8c\u6210\u4efb\u52a1\u3002\u73b0\u6709\u591a\u667a\u80fd\u4f53\u67b6\u6784\u867d\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u5bf9\u8bdd\u573a\u666f\uff0c\u4f46\u6b67\u4e49\u89e3\u51b3\u4ecd\u662f\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u4f55\u65f6\u4e2d\u65ad\u7528\u6237\u3001\u5982\u4f55\u5236\u5b9a\u6700\u4f18\u6f84\u6e05\u7b56\u7565\u7b49\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51faMAC\u591a\u667a\u80fd\u4f53\u6f84\u6e05\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u65b0\u7684\u7528\u6237\u6b67\u4e49\u5206\u7c7b\u6cd5\u7cfb\u7edf\u6307\u5bfc\u6f84\u6e05\u7b56\u7565\uff1b2\uff09\u81ea\u4e3b\u534f\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\u4e0e\u7528\u6237\u534f\u540c\u4ea4\u4e92\uff1b3\uff09\u5728MultiWOZ 2.4\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5728MultiWOZ 2.4\u4e0a\uff0c\u542f\u7528\u4e24\u7ea7\u6f84\u6e05\u4f7f\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad87.8%\uff08\u4ece54.5%\u523062.3%\uff09\uff0c\u5e73\u5747\u5bf9\u8bdd\u8f6e\u6b21\u4ece6.53\u51cf\u5c11\u52304.86\uff0c\u901a\u8fc7\u63d0\u524d\u83b7\u53d6\u6240\u6709\u5fc5\u8981\u7528\u6237\u4fe1\u606f\u5e76\u6700\u5c0f\u5316\u91cd\u590d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e3b\u52a8\u7528\u6237\u4ea4\u4e92\u548c\u89d2\u8272\u611f\u77e5\u6f84\u6e05\u5bf9\u4e8e\u66f4\u53ef\u9760\u7684\u4eba\u673a\u901a\u4fe1\u7684\u91cd\u8981\u6027\uff0cMAC\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u7528\u6237\u6b67\u4e49\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2512.13063", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13063", "abs": "https://arxiv.org/abs/2512.13063", "authors": ["Cheril Shah", "Akshit Agarwal", "Kanak Garg", "Mourad Heddaya"], "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators", "comment": "Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025", "summary": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4eba\u7c7b\u4e0eLLM\u5728\u53cc\u8fb9\u8c08\u5224\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u5728\u8c08\u5224\u7b56\u7565\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u90a3\u6837\u6839\u636e\u60c5\u5883\u7075\u6d3b\u8c03\u6574\uff0c\u4e14\u8c08\u5224\u80fd\u529b\u4e0d\u968f\u6a21\u578b\u6539\u8fdb\u800c\u63d0\u5347\u3002", "motivation": "\u53cc\u8fb9\u8c08\u5224\u662f\u590d\u6742\u7684\u60c5\u5883\u654f\u611f\u4efb\u52a1\uff0c\u4eba\u7c7b\u8c08\u5224\u8005\u80fd\u52a8\u6001\u8c03\u6574\u7b56\u7565\u4ee5\u5229\u7528\u6743\u529b\u4e0d\u5bf9\u79f0\u548c\u975e\u6b63\u5f0f\u7ebf\u7d22\u3002\u5f53\u524d\u7f3a\u4e4f\u5bf9LLM\u8c08\u5224\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u4e0e\u4eba\u7c7b\u8c08\u5224\u7b56\u7565\u7684\u5bf9\u6bd4\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u66f2\u6b63\u5207\u66f2\u7ebf\u7684\u7edf\u4e00\u6570\u5b66\u6846\u67b6\u5efa\u6a21\u8ba9\u6b65\u52a8\u6001\uff0c\u5b9a\u4e49\u7206\u53d1\u6027tau\u548c\u8ba9\u6b65\u521a\u6027\u6307\u6570(CRI)\u91cf\u5316\u62a5\u4ef7\u8f68\u8ff9\u3002\u5728\u5927\u89c4\u6a21\u5b9e\u9a8c\u4e2d\u6bd4\u8f83\u4eba\u7c7b\u8c08\u5224\u8005\u4e0e\u56db\u79cdSOTA LLM\uff0c\u6db5\u76d6\u81ea\u7136\u8bed\u8a00\u548c\u6570\u5b57\u62a5\u4ef7\u8bbe\u7f6e\uff0c\u6709\u65e0\u4e30\u5bcc\u5e02\u573a\u60c5\u5883\uff0c\u4ee5\u53ca\u516d\u79cd\u53d7\u63a7\u7684\u6743\u529b\u4e0d\u5bf9\u79f0\u573a\u666f\u3002", "result": "LLM\u7cfb\u7edf\u6027\u951a\u5b9a\u5728\u53ef\u80fd\u534f\u8bae\u533a\u7684\u6781\u7aef\u4f4d\u7f6e\uff0c\u65e0\u8bba\u6760\u6746\u6216\u60c5\u5883\u5982\u4f55\u90fd\u4f18\u5316\u56fa\u5b9a\u70b9\uff0c\u800c\u4eba\u7c7b\u80fd\u5e73\u6ed1\u9002\u5e94\u60c5\u5883\u5e76\u63a8\u65ad\u5bf9\u624b\u7acb\u573a\u548c\u7b56\u7565\u3002LLM\u7b56\u7565\u591a\u6837\u6027\u6709\u9650\uff0c\u5076\u5c14\u4f7f\u7528\u6b3a\u9a97\u6027\u7b56\u7565\uff0c\u4e14\u8c08\u5224\u80fd\u529b\u4e0d\u968f\u6a21\u578b\u6539\u8fdb\u800c\u63d0\u5347\u3002", "conclusion": "\u5f53\u524dLLM\u8c08\u5224\u80fd\u529b\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u8981\u80fd\u66f4\u597d\u5185\u5316\u5bf9\u624b\u63a8\u7406\u548c\u60c5\u5883\u4f9d\u8d56\u7b56\u7565\u7684\u6a21\u578b\u3002", "topic": "agent analysis"}}
{"id": "2512.13159", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13159", "abs": "https://arxiv.org/abs/2512.13159", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Jie Hao", "Joo Hyuk Jeon", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning", "comment": null, "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.", "AI": {"tldr": "SpeakRL\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u4e3b\u52a8\u4ea4\u4e92\uff08\u5982\u63d0\u95ee\u6f84\u6e05\u95ee\u9898\uff09\u6765\u589e\u5f3aAI\u4ee3\u7406\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u4e0a\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534720.14%\u3002", "motivation": "\u5f53\u524d\u4eba\u673a\u534f\u4f5c\u4e3b\u8981\u662f\u5355\u5411\u7684\uff0c\u7528\u6237\u53d1\u51fa\u6307\u4ee4\uff0c\u4ee3\u7406\u76f4\u63a5\u54cd\u5e94\u800c\u4e0d\u4e3b\u52a8\u6f84\u6e05\u6a21\u7cca\u610f\u56fe\u3002\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u5c06\u4ee3\u7406\u4f18\u5316\u4e3a\u66f4\u597d\u7684\u8ddf\u968f\u8005\u800c\u975e\u6709\u6548\u7684\u53d1\u8a00\u8005\u3002\u9700\u8981\u8ba9\u4ee3\u7406\u66f4\u4e3b\u52a8\u5730\u53c2\u4e0e\u5bf9\u8bdd\u4ee5\u6f84\u6e05\u7528\u6237\u610f\u56fe\u3001\u89e3\u51b3\u6b67\u4e49\u5e76\u9002\u5e94\u53d8\u5316\u60c5\u51b5\u3002", "method": "\u63d0\u51faSpeakRL\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5956\u52b1\u4ee3\u7406\u4e0e\u7528\u6237\u7684\u4e3b\u52a8\u4ea4\u4e92\uff08\u5982\u5fc5\u8981\u65f6\u7684\u6f84\u6e05\u63d0\u95ee\uff09\u3002\u521b\u5efaSpeakER\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7684\u591a\u6837\u5316\u573a\u666f\uff0c\u4efb\u52a1\u901a\u8fc7\u4ea4\u4e92\u5f0f\u6f84\u6e05\u95ee\u9898\u89e3\u51b3\u3002\u7cfb\u7edf\u5206\u6790\u5bf9\u8bdd\u4e3b\u52a8\u6027\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u63d0\u51fa\u5e73\u8861\u63d0\u95ee\u4e0e\u884c\u52a8\u7684\u5956\u52b1\u516c\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534720.14%\u7684\u7edd\u5bf9\u6539\u8fdb\uff0c\u4e14\u4e0d\u589e\u52a0\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u7684\u4e13\u6709\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6f84\u6e05\u4e2d\u5fc3\u5316\u7528\u6237-\u4ee3\u7406\u4ea4\u4e92\u7684\u6f5c\u529b\u3002", "conclusion": "SpeakRL\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u4ee3\u7406\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u4f7f\u4ee3\u7406\u80fd\u66f4\u4e3b\u52a8\u5730\u6f84\u6e05\u7528\u6237\u610f\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u6548\u679c\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13168", "categories": ["cs.AI", "cs.CE", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.13168", "abs": "https://arxiv.org/abs/2512.13168", "authors": ["Haoyu Dong", "Pengkun Zhang", "Yan Gao", "Xuanyu Dong", "Yilin Cheng", "Mingzhe Lu", "Adina Yakefu", "Shuxin Zheng"], "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "comment": null, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "AI": {"tldr": "Finch\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u771f\u5b9e\u4f01\u4e1a\u7ea7\u8d22\u52a1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eEnron\u7b49\u91d1\u878d\u673a\u6784\u7684\u771f\u5b9e\u6570\u636e\uff0c\u5305\u542b172\u4e2a\u590d\u5408\u5de5\u4f5c\u6d41\u7a0b\u548c384\u4e2a\u4efb\u52a1\uff0c\u6d89\u53ca\u5927\u91cf\u7535\u5b50\u8868\u683c\u548c\u5176\u4ed6\u591a\u6a21\u6001\u6587\u6863\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\u4e2d\u590d\u6742\u3001\u957f\u65f6\u7a0b\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u7684\u8bc4\u4f30\u3002\u4f01\u4e1a\u8d22\u52a1\u5de5\u4f5c\u6d89\u53ca\u6570\u636e\u5f55\u5165\u3001\u8ba1\u7b97\u3001\u5efa\u6a21\u3001\u9a8c\u8bc1\u3001\u53ef\u89c6\u5316\u7b49\u591a\u79cd\u4efb\u52a1\u7684\u4ea4\u7ec7\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30AI\u4ee3\u7406\u5728\u5b9e\u9645\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528LLM\u8f85\u52a9\u53d1\u73b0\u4e0e\u4e13\u5bb6\u6807\u6ce8\u76f8\u7ed3\u5408\u7684\u5de5\u4f5c\u6d41\u7a0b\u6784\u5efa\u65b9\u6cd5\uff1a1) \u4ece\u771f\u5b9e\u90ae\u4ef6\u7ebf\u7a0b\u548c\u7535\u5b50\u8868\u683c\u7248\u672c\u5386\u53f2\u4e2dLLM\u8f85\u52a9\u63a8\u5bfc\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e13\u5bb6\u9a8c\u8bc1\uff1b2) \u4e13\u5bb6\u8fdb\u884c\u7ec6\u81f4\u6807\u6ce8\uff0c\u6295\u5165\u8d85\u8fc7700\u5c0f\u65f6\u3002\u6570\u636e\u6e90\u5305\u62ecEnron\u768415,000\u4e2a\u7535\u5b50\u8868\u683c\u548c500,000\u5c01\u90ae\u4ef6\uff0c\u4ee5\u53ca\u5176\u4ed6\u91d1\u878d\u673a\u6784\u7684\u771f\u5b9e\u5de5\u4f5c\u7a7a\u95f4\u3002", "result": "\u8bc4\u4f30\u4e86GPT 5.1\u3001Claude Sonnet 4.5\u3001Gemini 3 Pro\u3001Grok 4\u548cQwen 3 Max\u7b49\u524d\u6cbfAI\u7cfb\u7edf\u3002GPT 5.1 Pro\u82b1\u8d3948\u5c0f\u65f6\u4ec5\u901a\u8fc738.4%\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0cClaude Sonnet 4.5\u4ec5\u901a\u8fc725.0%\u3002\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u5bf9AI\u4ee3\u7406\u7684\u6311\u6218\u3002", "conclusion": "Finch\u57fa\u51c6\u63ed\u793a\u4e86\u5f53\u524dAI\u4ee3\u7406\u5728\u5904\u7406\u771f\u5b9e\u4f01\u4e1a\u8d22\u52a1\u5de5\u4f5c\u6d41\u7a0b\u65f6\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u5373\u4f7f\u662f\u9876\u5c16\u6a21\u578b\u4e5f\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u590d\u6742\u3001\u6df7\u4e71\u3001\u957f\u65f6\u7a0b\u7684\u4efb\u52a1\u3002\u8fd9\u4e3aAI\u4ee3\u7406\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u51fa\u4e86\u91cd\u8981\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.13278", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13278", "abs": "https://arxiv.org/abs/2512.13278", "authors": ["Jiaru Zou", "Ling Yang", "Yunzhe Qi", "Sirui Chen", "Mengting Ai", "Ke Shen", "Jingrui He", "Mengdi Wang"], "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning", "comment": "Best Paper Award at ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence", "summary": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.", "AI": {"tldr": "AutoTool\u662f\u4e00\u4e2a\u8ba9LLM\u667a\u80fd\u4f53\u5177\u5907\u52a8\u6001\u5de5\u5177\u9009\u62e9\u80fd\u529b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b20\u4e07\u6761\u5de5\u5177\u9009\u62e9\u7406\u7531\u7684\u6570\u636e\u96c6\u548c\u53cc\u9636\u6bb5\u4f18\u5316\u7ba1\u9053\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u7684\u5de5\u5177\u5e93\u5b58\uff0c\u9650\u5236\u4e86LLM\u667a\u80fd\u4f53\u5bf9\u65b0\u51fa\u73b0\u6216\u4e0d\u65ad\u6f14\u53d8\u7684\u5de5\u5177\u96c6\u7684\u9002\u5e94\u6027\u3002\u9700\u8981\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u5728\u63a8\u7406\u8f68\u8ff9\u4e2d\u52a8\u6001\u9009\u62e9\u5de5\u5177\u3002", "method": "1) \u6784\u5efa\u5305\u542b20\u4e07\u6761\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d61000+\u5de5\u5177\u548c100+\u4efb\u52a1\uff1b2) \u53cc\u9636\u6bb5\u4f18\u5316\u7ba1\u9053\uff1a\u76d1\u7763\u548c\u57fa\u4e8eRL\u7684\u8f68\u8ff9\u7a33\u5b9a\u5316\uff0c\u4ee5\u53caKL\u6b63\u5219\u5316\u7684Plackett-Luce\u6392\u5e8f\u6765\u4f18\u5316\u591a\u6b65\u9aa4\u5de5\u5177\u9009\u62e9\u3002", "result": "\u5728\u5341\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoTool\u5728Qwen3-8B\u548cQwen2.5-VL-7B\u4e0a\u8bad\u7ec3\uff0c\u5e73\u5747\u63d0\u5347\uff1a\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u74066.4%\uff0c\u57fa\u4e8e\u641c\u7d22\u7684QA 4.5%\uff0c\u4ee3\u7801\u751f\u62107.7%\uff0c\u591a\u6a21\u6001\u7406\u89e36.9%\u3002\u5728\u63a8\u7406\u65f6\u80fd\u52a8\u6001\u5229\u7528\u672a\u89c1\u8fc7\u7684\u5de5\u5177\u3002", "conclusion": "AutoTool\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u9009\u62e9\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u53c2\u6570\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4e86\u5148\u8fdb\u7684LLM\u667a\u80fd\u4f53\u548c\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13323", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13323", "abs": "https://arxiv.org/abs/2512.13323", "authors": ["\u00c1rp\u00e1d P\u00e1ndy", "R\u00f3bert Lakatos", "Andr\u00e1s Hajdu"], "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning", "comment": null, "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9519\u8bef\u9a71\u52a8\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u9519\u8bef\u9884\u6d4b\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u89c4\u5219\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u5728\u9690\u79c1\u5408\u89c4\u6761\u4ef6\u4e0b\u8d85\u8d8aGPT-3.5 Turbo\u3002", "motivation": "\u5728\u91d1\u878d\u3001\u533b\u7597\u7b49\u53d7\u76d1\u7ba1\u884c\u4e1a\uff0c\u9700\u8981\u80fd\u591f\u5728\u672c\u5730\u5b89\u5168\u73af\u5883\u4e2d\u5904\u7406\u8868\u683c\u6570\u636e\u5e76\u6267\u884c\u7cbe\u786e\u7b97\u672f\u64cd\u4f5c\u7684\u5de5\u4e1a\u7ea7AI\u52a9\u624b\u3002\u73b0\u6709\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u7b97\u672f\u4efb\u52a1\u4e0a\u5b58\u5728\u57fa\u672c\u9650\u5236\uff0c\u800c\u5927\u578b\u6a21\u578b\u53c8\u5b58\u5728\u9690\u79c1\u5408\u89c4\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9519\u8bef\u9a71\u52a8\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u9519\u8bef\u9884\u6d4b\u6765\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u89c4\u5219\uff0c\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5728\u672c\u5730\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u7b97\u672f\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728Qwen3 4B\u6a21\u578b\u4e0a\uff0c\u9519\u8bef\u9a71\u52a8\u65b9\u6cd5\u5c06\u7b97\u672f\u4efb\u52a1\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u81f370.8%\uff0c\u4f7f\u8be5\u5c0f\u6a21\u578b\u5728\u9690\u79c1\u5408\u89c4\u6761\u4ef6\u4e0b\u8d85\u8d8a\u4e86GPT-3.5 Turbo\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u9519\u8bef\u9a71\u52a8\u63d0\u793a\u4f18\u5316\uff0c\u800c\u975e\u6602\u8d35\u7684\u5fae\u8c03\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u5de5\u4e1a\u53ef\u90e8\u7f72\u7684AI\u52a9\u624b\uff0c\u8ba9\u5c0f\u6a21\u578b\u5728\u9690\u79c1\u5408\u89c4\u6761\u4ef6\u4e0b\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u3002", "topic": "code agent"}}
{"id": "2512.13399", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13399", "abs": "https://arxiv.org/abs/2512.13399", "authors": ["Sitao Cheng", "Tianle Li", "Xuhan Huang", "Xunjian Yin", "Difan Zou"], "title": "Differentiable Evolutionary Reinforcement Learning", "comment": "Work in Progress. We release our code and model at https://github.com/sitaocheng/DERL", "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.", "AI": {"tldr": "DERL\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u4e8e\u542f\u53d1\u5f0f\u5956\u52b1\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bbe\u8ba1\u6709\u6548\u5956\u52b1\u51fd\u6570\u56f0\u96be\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5c06\u5956\u52b1\u51fd\u6570\u89c6\u4e3a\u9ed1\u76d2\uff0c\u65e0\u6cd5\u6355\u6349\u5956\u52b1\u7ed3\u6784\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u53ef\u5fae\u5206\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\uff08DERL\uff09\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1a\u5143\u4f18\u5316\u5668\u901a\u8fc7\u7ec4\u5408\u7ed3\u6784\u5316\u539f\u5b50\u57fa\u5143\u8fdb\u5316\u5956\u52b1\u51fd\u6570\uff0c\u6307\u5bfc\u5185\u5c42\u7b56\u7565\u8bad\u7ec3\uff1b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u5185\u5c42\u9a8c\u8bc1\u6027\u80fd\u4f5c\u4e3a\u4fe1\u53f7\u66f4\u65b0\u5143\u4f18\u5316\u5668\uff0c\u8fd1\u4f3c\u4efb\u52a1\u6210\u529f\u7684\"\u5143\u68af\u5ea6\"\u3002", "result": "\u5728\u673a\u5668\u4eba\u4ee3\u7406\uff08ALFWorld\uff09\u3001\u79d1\u5b66\u6a21\u62df\uff08ScienceWorld\uff09\u548c\u6570\u5b66\u63a8\u7406\uff08GSM8k\u3001MATH\uff09\u4e09\u4e2a\u9886\u57df\u9a8c\u8bc1\uff0c\u5728ALFWorld\u548cScienceWorld\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5206\u5e03\u5916\u573a\u666f\u663e\u8457\u4f18\u4e8e\u542f\u53d1\u5f0f\u5956\u52b1\u65b9\u6cd5\u3002", "conclusion": "DERL\u6210\u529f\u6355\u6349\u4efb\u52a1\u5185\u5728\u7ed3\u6784\uff0c\u5b9e\u73b0\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u81ea\u6539\u8fdb\u667a\u80fd\u4f53\u5bf9\u9f50\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u5956\u52b1\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13510", "abs": "https://arxiv.org/abs/2512.13510", "authors": ["Linjie Mu", "Yannian Gu", "Zhongzhen Huang", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph", "comment": null, "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.", "AI": {"tldr": "MedCEG\u6846\u67b6\u901a\u8fc7Critical Evidence Graph\u76d1\u7763\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e34\u5e8a\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u533b\u5b66\u63a8\u7406\u4e2d\u867d\u7136\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u6709\u9650\uff0c\u56e0\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f80\u5f80\u5ffd\u89c6\u4e86\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u9a8c\u8bc1", "method": "\u63d0\u51faMedCEG\u6846\u67b6\uff0c\u901a\u8fc7Critical Evidence Graph\uff08CEG\uff09\u663e\u5f0f\u76d1\u7763\u63a8\u7406\u8fc7\u7a0b\uff1b\u6784\u5efa\u6311\u6218\u6027\u4e34\u5e8a\u6848\u4f8b\u6570\u636e\u96c6\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u7b97\u6cd5\u6784\u5efaCEG\u8868\u793a\u9ad8\u8d28\u91cf\u53ef\u9a8c\u8bc1\u63a8\u7406\u8def\u5f84\uff1b\u5f15\u5165\u4e34\u5e8a\u63a8\u7406\u7a0b\u5e8f\u5956\u52b1\uff0c\u8bc4\u4f30\u8282\u70b9\u8986\u76d6\u3001\u7ed3\u6784\u6b63\u786e\u6027\u548c\u94fe\u5b8c\u6574\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMedCEG\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u4e34\u5e8a\u6709\u6548\u7684\u63a8\u7406\u94fe\uff0c\u4ee3\u8868\u4e86\u53ef\u9760\u533b\u5b66AI\u63a8\u7406\u7684\u5b9e\u8d28\u6027\u8fdb\u5c55", "conclusion": "MedCEG\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u63a8\u7406\u53ef\u9760\u6027\uff0c\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u66f4\u53ef\u4fe1\u7684\u51b3\u7b56\u652f\u6301", "topic": "agentic reinforcement learning"}}
{"id": "2512.13472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13472", "abs": "https://arxiv.org/abs/2512.13472", "authors": ["Jian Yang", "Shawn Guo", "Lin Jing", "Wei Zhang", "Aishan Liu", "Chuan Hao", "Zhoujun Li", "Wayne Xin Zhao", "Xianglong Liu", "Weifeng Lv", "Bryan Dai"], "title": "Scaling Laws for Code: Every Programming Language Matters", "comment": null, "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u9884\u8bad\u7ec3\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc71000+\u5b9e\u9a8c\u53d1\u73b0\u89e3\u91ca\u578b\u8bed\u8a00\uff08\u5982Python\uff09\u6bd4\u7f16\u8bd1\u578b\u8bed\u8a00\uff08\u5982Rust\uff09\u66f4\u80fd\u4ece\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u589e\u957f\u4e2d\u53d7\u76ca\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6bd4\u4f8b\u7684\u591a\u8bed\u8a00\u7f29\u653e\u5b9a\u5f8b\u6765\u4f18\u5316\u8bad\u7ec3token\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f29\u653e\u5b9a\u5f8b\u4e3b\u8981\u5173\u6ce8\u8bed\u8a00\u65e0\u5173\u8bbe\u7f6e\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u5dee\u5f02\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u9884\u6d4b\u4e0d\u51c6\u786e\u3002\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u672c\u8d28\u4e0a\u662f\u591a\u8bed\u8a00\u7684\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u7f29\u653e\u5b9a\u5f8b\u53ca\u5176\u76f8\u4e92\u5f71\u54cd\u3002", "method": "\u8fdb\u884c\u4e86\u8d85\u8fc71000\u4e2a\u5b9e\u9a8c\uff08\u76f8\u5f53\u4e8e336,000+ H800\u5c0f\u65f6\uff09\uff0c\u6db5\u76d6\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u3001\u6a21\u578b\u89c4\u6a21\uff080.2B\u523014B\u53c2\u6570\uff09\u548c\u6570\u636e\u96c6\u5927\u5c0f\uff081T tokens\uff09\u3002\u5efa\u7acb\u4e86\u5168\u9762\u7684\u4ee3\u7801LLM\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u5e76\u884c\u914d\u5bf9\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5c06\u4ee3\u7801\u7247\u6bb5\u4e0e\u5176\u7ffb\u8bd1\u62fc\u63a5\uff09\u3002", "result": "\u53d1\u73b0\u89e3\u91ca\u578b\u8bed\u8a00\u6bd4\u7f16\u8bd1\u578b\u8bed\u8a00\u66f4\u80fd\u4ece\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u589e\u957f\u4e2d\u53d7\u76ca\uff1b\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u5728\u8bed\u6cd5\u76f8\u4f3c\u7684\u7f16\u7a0b\u8bed\u8a00\u95f4\u5177\u6709\u534f\u540c\u6548\u5e94\uff1b\u5e76\u884c\u914d\u5bf9\u7b56\u7565\u663e\u8457\u589e\u5f3a\u4e86\u8de8\u8bed\u8a00\u80fd\u529b\uff1b\u63d0\u51fa\u7684\u6bd4\u4f8b\u4f9d\u8d56\u591a\u8bed\u8a00\u7f29\u653e\u5b9a\u5f8b\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u4f18\u4e8e\u5747\u5300\u5206\u5e03\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5efa\u7acb\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u9884\u8bad\u7ec3\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u4e3a\u4f18\u5316\u8bad\u7ec3\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u901a\u8fc7\u4f18\u5148\u8003\u8651\u9ad8\u6548\u7528\u8bed\u8a00\u3001\u5e73\u8861\u9ad8\u534f\u540c\u5bf9\u3001\u51cf\u5c11\u5feb\u901f\u9971\u548c\u8bed\u8a00\u7684\u5206\u914d\uff0c\u53ef\u4ee5\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u83b7\u5f97\u66f4\u597d\u7684\u5e73\u5747\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "2512.13564", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13564", "abs": "https://arxiv.org/abs/2512.13564", "authors": ["Yuyang Hu", "Shichun Liu", "Yanwei Yue", "Guibin Zhang", "Boyang Liu", "Fangyi Zhu", "Jiahang Lin", "Honglin Guo", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Jiejun Tan", "Yanbin Yin", "Jiongnan Liu", "Zeyu Zhang", "Zhongxiang Sun", "Yutao Zhu", "Hao Sun", "Boci Peng", "Zhenrong Cheng", "Xuanbo Fan", "Jiaxin Guo", "Xinlei Yu", "Zhenhong Zhou", "Zewen Hu", "Jiahao Huo", "Junhao Wang", "Yuwei Niu", "Yu Wang", "Zhenfei Yin", "Xiaobin Hu", "Yue Liao", "Qiankun Li", "Kun Wang", "Wangchunshu Zhou", "Yixin Liu", "Dawei Cheng", "Qi Zhang", "Tao Gui", "Shirui Pan", "Yan Zhang", "Philip Torr", "Zhicheng Dou", "Ji-Rong Wen", "Xuanjing Huang", "Yu-Gang Jiang", "Shuicheng Yan"], "title": "Memory in the Age of AI Agents", "comment": null, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u901a\u8fc7\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u4e09\u4e2a\u7edf\u4e00\u89c6\u89d2\u5206\u6790\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u5feb\u901f\u53d1\u5c55\u4f46\u65e5\u76ca\u788e\u7247\u5316\uff0c\u73b0\u6709\u5de5\u4f5c\u5728\u52a8\u673a\u3001\u5b9e\u73b0\u548c\u8bc4\u4f30\u4e0a\u5dee\u5f02\u5f88\u5927\uff0c\u8bb0\u5fc6\u672f\u8bed\u5b9a\u4e49\u677e\u6563\uff0c\u4f20\u7edf\u5206\u7c7b\uff08\u5982\u957f/\u77ed\u671f\u8bb0\u5fc6\uff09\u5df2\u65e0\u6cd5\u6355\u6349\u5f53\u4ee3\u8bb0\u5fc6\u7cfb\u7edf\u7684\u591a\u6837\u6027\uff0c\u9700\u8981\u63d0\u4f9b\u4e00\u4e2a\u66f4\u65b0\u7684\u7814\u7a76\u5168\u666f\u56fe\u3002", "method": "\u9996\u5148\u660e\u786e\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u8303\u56f4\uff0c\u5c06\u5176\u4e0eLLM\u8bb0\u5fc6\u3001RAG\u548c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7b49\u6982\u5ff5\u533a\u5206\u3002\u7136\u540e\u901a\u8fc7\u4e09\u4e2a\u7edf\u4e00\u89c6\u89d2\u5206\u6790\uff1a1\uff09\u5f62\u5f0f\u89c6\u89d2\uff1a\u8bc6\u522b\u4e09\u79cd\u4e3b\u8981\u8bb0\u5fc6\u5b9e\u73b0\uff08token\u7ea7\u3001\u53c2\u6570\u5316\u3001\u6f5c\u5728\u8bb0\u5fc6\uff09\uff1b2\uff09\u529f\u80fd\u89c6\u89d2\uff1a\u63d0\u51fa\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u7c7b\uff08\u4e8b\u5b9e\u8bb0\u5fc6\u3001\u7ecf\u9a8c\u8bb0\u5fc6\u3001\u5de5\u4f5c\u8bb0\u5fc6\uff09\uff1b3\uff09\u52a8\u6001\u89c6\u89d2\uff1a\u5206\u6790\u8bb0\u5fc6\u5982\u4f55\u968f\u65f6\u95f4\u5f62\u6210\u3001\u6f14\u5316\u548c\u68c0\u7d22\u3002", "result": "\u63d0\u4f9b\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u7684\u5168\u9762\u56fe\u666f\uff0c\u5305\u62ec\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f00\u6e90\u6846\u67b6\u7684\u603b\u7ed3\u3002\u901a\u8fc7\u4e09\u4e2a\u89c6\u89d2\u7684\u5206\u6790\u6846\u67b6\u4e3a\u7406\u89e3\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e0d\u4ec5\u4f5c\u4e3a\u73b0\u6709\u5de5\u4f5c\u7684\u53c2\u8003\uff0c\u66f4\u91cd\u8981\u7684\u662f\u4e3a\u91cd\u65b0\u601d\u8003\u8bb0\u5fc6\u4f5c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u667a\u80fd\u8bbe\u8ba1\u4e2d\u4e00\u7b49\u516c\u6c11\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u8bb0\u5fc6\u81ea\u52a8\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u3001\u591a\u6a21\u6001\u8bb0\u5fc6\u3001\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u548c\u53ef\u4fe1\u5ea6\u95ee\u9898\u7b49\u65b0\u5174\u524d\u6cbf\u65b9\u5411\u3002", "topic": "agent analysis"}}
{"id": "2512.13607", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13607", "abs": "https://arxiv.org/abs/2512.13607", "authors": ["Boxin Wang", "Chankyu Lee", "Nayeon Lee", "Sheng-Chieh Lin", "Wenliang Dai", "Yang Chen", "Yangyi Chen", "Zhuolin Yang", "Zihan Liu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models", "comment": "We publicly release the Nemotron-Cascade models and the full collection of training data at: https://huggingface.co/collections/nvidia/nemotron-cascade", "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.", "AI": {"tldr": "\u63d0\u51faCascade RL\u65b9\u6cd5\u8bad\u7ec3\u901a\u7528\u63a8\u7406\u6a21\u578bNemotron-Cascade\uff0c\u901a\u8fc7\u5206\u9886\u57df\u987a\u5e8f\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u8de8\u9886\u57df\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u6784\u5efa\u901a\u7528\u63a8\u7406\u6a21\u578b\u65f6\u9762\u4e34\u8de8\u9886\u57df\u5f02\u8d28\u6027\u6311\u6218\uff0c\u5305\u62ec\u63a8\u7406\u65f6\u54cd\u5e94\u957f\u5ea6\u5dee\u5f02\u5927\u3001\u9a8c\u8bc1\u5ef6\u8fdf\u4e0d\u540c\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u3001\u8bad\u7ec3\u7f13\u6162\u3001\u8bfe\u7a0b\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u9009\u62e9\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u7ea7\u8054\u9886\u57df\u5f3a\u5316\u5b66\u4e60(Cascade RL)\uff0c\u91c7\u7528\u987a\u5e8f\u3001\u5206\u9886\u57df\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u800c\u4e0d\u662f\u6df7\u5408\u4e0d\u540c\u9886\u57df\u7684\u5f02\u8d28\u63d0\u793a\u3002\u5148\u4f7f\u7528RLHF\u8fdb\u884c\u5bf9\u9f50\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u7136\u540e\u8fdb\u884c\u9886\u57df\u7ea7RLVR\u9636\u6bb5\u3002", "result": "14B\u6a21\u578b\u5728RL\u540e\u8d85\u8d8a\u4e86\u5176SFT\u6559\u5e08DeepSeek-R1-0528\uff0c\u5728LiveCodeBench v5/v6/Pro\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u57282025\u5e74\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u83b7\u5f97\u94f6\u724c\u6210\u7ee9\u3002\u9886\u57df\u7ea7RLVR\u9636\u6bb5\u5f88\u5c11\u964d\u4f4e\u65e9\u671f\u9886\u57df\u83b7\u5f97\u7684\u57fa\u51c6\u6027\u80fd\uff0c\u751a\u81f3\u53ef\u80fd\u63d0\u5347\u3002", "conclusion": "Cascade RL\u65b9\u6cd5\u964d\u4f4e\u4e86\u5de5\u7a0b\u590d\u6742\u6027\uff0c\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cRLHF\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u504f\u597d\u4f18\u5316\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13654", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13654", "abs": "https://arxiv.org/abs/2512.13654", "authors": ["John E. Ortega", "Dhruv D. Joshi", "Matt P. Borkowski"], "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases", "comment": "7 pages, 1 figure, Appendix of Prompts", "summary": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u6700\u9ad8\u6cd5\u9662\u5224\u51b3\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\uff08\u5982DeepSeek\uff09\u6bd4\u4f20\u7edfBERT\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u572815\u7c7b\u548c279\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e0a\u5747\u9ad8\u51fa\u7ea62\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u7814\u7a76LLM\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u51c6\u786e\u6027\u548c\u54cd\u5e94\u884c\u4e3a\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u6700\u9ad8\u6cd5\u9662\u5224\u51b3\u5206\u7c7b\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u5177\u6709\u53e5\u5b50\u957f\u5ea6\u957f\u3001\u6cd5\u5f8b\u672f\u8bed\u590d\u6742\u3001\u7ed3\u6784\u975e\u6807\u51c6\u548c\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\u7b49\u7279\u70b9\u3002", "method": "\u4f7f\u7528\u6700\u65b0\u7684LLM\u5fae\u8c03\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u5305\u62ec\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u81ea\u52a8\u5efa\u6a21\u7b49\uff0c\u5728\u4e24\u4e2a\u4f20\u7edf\u7684\u57fa\u4e8e\u7c7b\u522b\u7684\u6700\u9ad8\u6cd5\u9662\u5224\u51b3\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff1a\u4e00\u4e2a\u5305\u542b15\u4e2a\u6807\u7b7e\u4e3b\u9898\uff0c\u53e6\u4e00\u4e2a\u5305\u542b279\u4e2a\u6807\u7b7e\u4e3b\u9898\u3002", "result": "\u57fa\u4e8e\u63d0\u793a\u7684\u5e26\u6709\u8bb0\u5fc6\u7684\u6a21\u578b\uff08\u5982DeepSeek\uff09\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u90fd\u6bd4\u4e4b\u524d\u7684BERT\u6a21\u578b\u66f4\u9c81\u68d2\uff0c\u5f97\u5206\u6bd4\u975e\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u9ad8\u51fa\u7ea62\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5728\u590d\u6742\u7684\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u63d0\u793a\u7684LLM\u6a21\u578b\u6bd4\u4f20\u7edfBERT\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u663e\u793a\u51fa\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u9886\u57df\u7279\u5b9a\u5206\u7c7b\u4efb\u52a1\u65b9\u9762\u7684\u4f18\u52bf\u3002", "topic": "agent analysis"}}
{"id": "2512.12690", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12690", "abs": "https://arxiv.org/abs/2512.12690", "authors": ["Yongcan Yu", "Lingxiao He", "Shuo Lu", "Lijun Sheng", "Yinuo Xu", "Yanbo Wang", "Kuangpu Guo", "Jianjie Cheng", "Meng Wang", "Qianlong Xie", "Xingxing Wang", "Dapeng Hu", "Jian Liang"], "title": "Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing \"RL over SFT\" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u5f3a\u5316\u5b66\u4e60\u4e0e\u76d1\u7763\u5fae\u8c03\u7684\u5bf9\u6bd4\uff0c\u53d1\u73b0SFT\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4ecd\u5177\u4f18\u52bf\uff0c\u6311\u6218\u4e86\"RL\u4f18\u4e8eSFT\"\u7684\u4e3b\u6d41\u89c2\u70b9\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9886\u57df\u666e\u904d\u8ba4\u4e3a\u5f3a\u5316\u5b66\u4e60\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff0c\u751a\u81f3\u8ba4\u4e3aSFT\u9636\u6bb5\u53ef\u80fd\u5bf9\u6a21\u578b\u8bad\u7ec3\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5bf9\u6bd4\u91cd\u65b0\u8bc4\u4f30\u8fd9\u4e00\u89c2\u70b9\u3002", "method": "\u5728\u76f8\u540c\u6570\u636e\u6e90\u4e0b\uff0c\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6cd5\u7cfb\u7edf\u6bd4\u8f83SFT\u548cRL\u5728VLM\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u8003\u5bdf\u6a21\u578b\u5bb9\u91cf\u3001\u6570\u636e\u89c4\u6a21\u548c\u6570\u636e\u5206\u5e03\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09SFT\u5bf9\u8f83\u5f31\u6a21\u578b\u66f4\u6709\u6548\uff1b2\uff09SFT\u6570\u636e\u6548\u7387\u66f4\u9ad8\uff082K\u6570\u636e\u5373\u53ef\u8fbe\u5230RL 20K\u6570\u636e\u7684\u6548\u679c\uff09\uff1b3\uff09SFT\u5177\u6709\u66f4\u5f3a\u7684\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff1b4\uff09RL\u5b58\u5728\u6b3a\u9a97\u6027\u5956\u52b1\u95ee\u9898\u3002", "conclusion": "SFT\u7684\u4f5c\u7528\u88ab\u4f4e\u4f30\uff0cRL\u548cSFT\u5e94\u4f5c\u4e3a\u4e92\u8865\u7ec4\u4ef6\uff0c\u5efa\u8bae\u91c7\u7528\u66f4\u5e73\u8861\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13352", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13352", "abs": "https://arxiv.org/abs/2512.13352", "authors": ["Ali Al Sahili", "Ali Chehab", "Razane Tajeddine"], "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models", "comment": "Accepted to IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2026", "summary": "Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u591a\u79cd\u6210\u5458\u63a8\u7406\u653b\u51fb\u6280\u672f\u96c6\u6210\u5230LLM\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5b83\u4eec\u5728\u771f\u5b9e\u6570\u636e\u63d0\u53d6\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4f20\u7edfMIA\u57fa\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5e26\u6765\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u8fd9\u4e24\u79cd\u5a01\u80c1\u76f8\u4e92\u5173\u8054\uff0c\u4f46\u7f3a\u4e4f\u5bf9MIA\u6280\u672f\u5728\u771f\u5b9e\u6570\u636e\u63d0\u53d6\u573a\u666f\u4e2d\u5b9e\u7528\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5c06\u591a\u79cdMIA\u6280\u672f\u96c6\u6210\u5230\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u8ba9\u6a21\u578b\u751f\u6210\u5927\u91cf\u6587\u672c\uff0c\u7136\u540e\u5e94\u7528MIA\u9a8c\u8bc1\u7279\u5b9a\u6570\u636e\u70b9\u662f\u5426\u5c5e\u4e8e\u8bad\u7ec3\u96c6\uff0c\u7cfb\u7edf\u6027\u5730\u5bf9MIA\u6280\u672f\u7684\u6709\u6548\u6027\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u96c6\u6210\u8bbe\u7f6e\u4e0bMIA\u6280\u672f\u7684\u6027\u80fd\u4e0e\u4f20\u7edfMIA\u57fa\u51c6\u7684\u7ed3\u679c\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u63d0\u53d6\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30MIA\u6280\u672f\u5728\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u9690\u79c1\u653b\u51fb\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "topic": "agent analysis"}}
{"id": "2512.12858", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12858", "abs": "https://arxiv.org/abs/2512.12858", "authors": ["Sonal Prabhune", "Balaji Padmanabhan", "Kaushik Dutta"], "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f18\u5316LLM\u5728\u8bed\u4e49\u7b49\u4ef7\u63d0\u793a\u4e0b\u7684\u4fe1\u606f\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4f01\u4e1a\u573a\u666f\u4e2d\u7684\u63a8\u8350\u53d8\u5f02\u6027", "motivation": "LLM\u5728\u4f01\u4e1a\u5173\u952e\u9886\u57df\u90e8\u7f72\u65f6\uff0c\u5bf9\u8bed\u4e49\u7b49\u4ef7\u4f46\u63aa\u8f9e\u4e0d\u540c\u7684\u63d0\u793a\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u63a8\u8350\uff0c\u8fd9\u7834\u574f\u4e86\u7528\u6237\u4fe1\u4efb\u3001\u5408\u89c4\u6027\u548c\u4f53\u9a8c\u3002\u4f01\u4e1a\u573a\u666f\u5982HR\u5165\u804c\u3001\u5ba2\u6237\u652f\u6301\u7b49\u9700\u8981\u4fe1\u606f\u4f20\u9012\u7684\u7a33\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u8fd9\u79cd\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGroup Relative Policy Optimization\uff08GRPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u8bed\u4e49\u7b49\u4ef7\u63d0\u793a\u4f5c\u4e3a\u7ec4\u5904\u7406\uff0c\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u6709\u7528\u6027\u548c\u7a33\u5b9a\u6027\u5956\u52b1\uff0c\u91cd\u7f6e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4ee5\u9694\u79bb\u63aa\u8f9e\u5f71\u54cd\u3002", "result": "\u5728\u6295\u8d44\u548c\u804c\u4f4d\u63a8\u8350\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRPO\u8bad\u7ec3\u6a21\u578b\u6bd4\u5fae\u8c03\u6216\u57fa\u4e8e\u89e3\u7801\u7684\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u51cf\u5c11\u53d8\u5f02\u6027\u3002", "conclusion": "\u8fd9\u662fGRPO\u5728LLM\u4fe1\u606f\u4e00\u81f4\u6027\u5bf9\u9f50\u65b9\u9762\u7684\u65b0\u9896\u5e94\u7528\uff0c\u5c06\u53d8\u5f02\u6027\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u7ea0\u6b63\u7684\u7f3a\u9677\u800c\u975e\u751f\u6210\u591a\u6837\u6027\u7684\u53ef\u63a5\u53d7\u7279\u5f81\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13106", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13106", "abs": "https://arxiv.org/abs/2512.13106", "authors": ["Shenzhi Yang", "Guangcheng Zhu", "Xing Zheng", "Yingfan MA", "Zhongqi Chen", "Bowen Song", "Weiqiang Wang", "Junbo Zhao", "Gang Chen", "Haobo Wang"], "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.", "AI": {"tldr": "\u63d0\u51faTraPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u6307\u5bfc\u65e0\u6807\u6ce8\u6837\u672c\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u89e3\u51b3\u65e0\u76d1\u7763RLVR\u65b9\u6cd5\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRLVR\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u65e0\u76d1\u7763RLVR\u65b9\u6cd5\u5bb9\u6613\u5728\u8bad\u7ec3\u540e\u671f\u51fa\u73b0\u6a21\u578b\u5d29\u6e83\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u53c8\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u534a\u76d1\u7763RLVR\u8303\u5f0f\uff0c\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u6307\u5bfc\u65e0\u6807\u6ce8\u6837\u672c\u8bad\u7ec3\u3002\u6838\u5fc3\u662fTraPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u5b66\u4e60\u8f68\u8ff9\u76f8\u4f3c\u6027\u6765\u8bc6\u522b\u53ef\u9760\u7684\u65e0\u6807\u6ce8\u6837\u672c\uff0c\u786e\u4fdd\u53ea\u6709\u7ecf\u8fc7\u6807\u6ce8\u6837\u672c\u9a8c\u8bc1\u7684\u63a8\u7406\u6a21\u5f0f\u88ab\u7eb3\u5165\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u57286\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c3\u4e2a\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002\u4ec5\u75281K\u6807\u6ce8+3K\u65e0\u6807\u6ce8\u6837\u672c\u8fbe\u523042.6%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u752845K\u65e0\u6807\u6ce8\u6837\u672c\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0838.3%\uff09\u3002\u75284K\u6807\u6ce8+12K\u65e0\u6807\u6ce8\u6837\u672c\u751a\u81f3\u8d85\u8d8a\u7528\u5168\u90e845K\u6807\u6ce8\u6837\u672c\u7684\u5168\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "TraPO\u7b97\u6cd5\u901a\u8fc7\u534a\u76d1\u7763RLVR\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13165", "abs": "https://arxiv.org/abs/2512.13165", "authors": ["Jakub \u0141yskawa", "Jakub Lewandowski", "Pawe\u0142 Wawrzy\u0144ski"], "title": "SACn: Soft Actor-Critic with n-step Returns", "comment": "Accepted at ICAART 2026", "summary": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\u03c4$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SACn\u7b97\u6cd5\uff0c\u5c06Soft Actor-Critic\u4e0en\u6b65\u56de\u62a5\u7ed3\u5408\uff0c\u901a\u8fc7\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u548c\u03c4\u91c7\u6837\u71b5\u4f30\u8ba1\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "SAC\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u6700\u76f8\u5173\u7684\u79bb\u7b56\u7565\u5728\u7ebf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e4b\u4e00\uff0c\u4f46\u96be\u4ee5\u4e0en\u6b65\u56de\u62a5\u7ed3\u5408\uff0c\u56e0\u4e3a\u4f20\u7edf\u7ec4\u5408\u4f1a\u56e0\u52a8\u4f5c\u5206\u5e03\u53d8\u5316\u5f15\u5165\u504f\u5dee\uff0c\u800c\u91cd\u8981\u6027\u91c7\u6837\u53c8\u53ef\u80fd\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u3002", "method": "1) \u63d0\u51fa\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u7b80\u5316\u8d85\u53c2\u6570\u9009\u62e9\uff1b2) \u5728n\u6b65\u6700\u5927\u71b5\u6846\u67b6\u4e0b\u5206\u6790SAC\u7684\u71b5\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u03c4\u91c7\u6837\u71b5\u4f30\u8ba1\u6765\u964d\u4f4e\u5b66\u4e60\u76ee\u6807\u7684\u65b9\u5dee\uff1b3) \u6700\u7ec8\u5f62\u6210SACn\u7b97\u6cd5\u3002", "result": "\u5728MuJoCo\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8868\u660eSACn\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u7ed3\u5408n\u6b65\u56de\u62a5\uff0c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u6210\u529f\u89e3\u51b3\u4e86SAC\u4e0en\u6b65\u56de\u62a5\u7ed3\u5408\u7684\u6280\u672f\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u548c\u03c4\u91c7\u6837\u71b5\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5f62\u6210\u4e86\u5b9e\u7528\u7684SACn\u7b97\u6cd5\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2512.13077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13077", "abs": "https://arxiv.org/abs/2512.13077", "authors": ["Md Awsafur Rahman", "Adam Gabrys", "Doug Kang", "Jingjing Sun", "Tian Tan", "Ashwin Chandramouli"], "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization", "comment": null, "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.", "AI": {"tldr": "LikeBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4e2a\u6027\u5316\u80fd\u529b\u7684\u591a\u4f1a\u8bdd\u52a8\u6001\u6846\u67b6\uff0c\u9996\u6b21\u5c06\"\u559c\u597d\u5ea6\"\u5206\u89e3\u4e3a\u4e03\u4e2a\u8bca\u65ad\u7ef4\u5ea6\uff0c\u53d1\u73b0\u8bb0\u5fc6\u51c6\u786e\u6027\u4e0d\u7b49\u4e8e\u9ad8\u559c\u597d\u5ea6\uff0c\u4e14\u5f53\u524dSOTA\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u9002\u5e94\u6027\u6709\u9650\u3002", "motivation": "\u73b0\u6709LLM\u4e2a\u6027\u5316\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u51c6\u786e\u56de\u5fc6\u7528\u6237\u4fe1\u606f\u548c\u5e94\u7528\u8bb0\u5fc6\u4fe1\u606f\uff0c\u4f46\u5ffd\u7565\u4e86\u7b2c\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u2014\u2014\u559c\u597d\u5ea6\u3002\u559c\u597d\u5ea6\u662f\u4e3b\u89c2\u7684\uff0c\u4f46\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u5f53\u524d\u57fa\u51c6\u5bf9\u6b64\u6d4b\u91cf\u4e0d\u8db3\u3002", "method": "\u5f15\u5165LikeBench\u591a\u4f1a\u8bdd\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9LLM\u4e0e\u6a21\u62df\u7528\u6237\u5bf9\u8bdd\uff0c\u4ec5\u4ece\u5bf9\u8bdd\u4e2d\u5b66\u4e60\u504f\u597d\u3002\u5c06\u559c\u597d\u5ea6\u5206\u89e3\u4e3a\u4e03\u4e2a\u8bca\u65ad\u7ef4\u5ea6\uff1a\u60c5\u611f\u9002\u5e94\u3001\u6b63\u5f0f\u5ea6\u5339\u914d\u3001\u77e5\u8bc6\u9002\u5e94\u3001\u5f15\u7528\u7406\u89e3\u3001\u5bf9\u8bdd\u957f\u5ea6\u9002\u5e94\u3001\u5e7d\u9ed8\u9002\u5e94\u548c\u56de\u8c03\u3002\u4f7f\u7528\u7ec6\u7c92\u5ea6\u3001\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u63cf\u8ff0\u6027\u4eba\u7269\u89d2\u8272\u800c\u975e\u7c97\u7cd9\u7684\u9ad8/\u4f4e\u7279\u8d28\u8bc4\u5206\u3002", "result": "\u8bb0\u5fc6\u6027\u80fd\u5f3a\u4e0d\u4fdd\u8bc1\u9ad8\u559c\u597d\u5ea6\uff1aDeepSeek R1\u8bb0\u5fc6\u51c6\u786e\u7387\u8f83\u4f4e\uff0886%\uff0c17\u4e2a\u4e8b\u5b9e/\u6863\u6848\uff09\u4f46\u5728\u559c\u597d\u5ea6\u5f97\u5206\u4e0a\u6bd4Qwen3\u9ad828%\uff0c\u5c3d\u7ba1Qwen3\u8bb0\u5fc6\u51c6\u786e\u7387\u66f4\u9ad8\uff0893%\uff0c43\u4e2a\u4e8b\u5b9e/\u6863\u6848\uff09\u3002SOTA\u6a21\u578b\u5982GPT-5\u5728\u77ed\u5bf9\u8bdd\u4e2d\u9002\u5e94\u826f\u597d\uff0c\u4f46\u5728\u957f\u4e14\u5608\u6742\u7684\u4ea4\u4e92\u4e2d\u9002\u5e94\u6027\u6709\u9650\u3002", "conclusion": "\u559c\u597d\u5ea6\u662fLLM\u4e2a\u6027\u5316\u7684\u91cd\u8981\u7ef4\u5ea6\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u3002LikeBench\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8bb0\u5fc6\u51c6\u786e\u6027\u548c\u559c\u597d\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u578b\u5728\u957f\u671f\u9002\u5e94\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "topic": "agent analysis"}}
{"id": "2512.13526", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13526", "abs": "https://arxiv.org/abs/2512.13526", "authors": ["Asa Cooper Stickland", "Jan Michelfeit", "Arathi Mani", "Charlie Griffin", "Ollie Matthews", "Tomek Korbak", "Rogan Inglis", "Oliver Makins", "Alan Cooney"], "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents", "comment": null, "summary": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.", "AI": {"tldr": "\u7814\u7a76LLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u5f02\u6b65\u76d1\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea2\u84dd\u5bf9\u6297\u6e38\u620f\u5f00\u53d1\u76d1\u63a7\u7cfb\u7edf\uff0c\u57285\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u5b9e\u73b06%\u6f0f\u62a5\u7387\uff081%\u8bef\u62a5\u7387\uff09\uff0c\u5e76\u8bc4\u4f30\u90e8\u7f72\u65f6\u7684\u7834\u574f\u98ce\u9669\u3002", "motivation": "LLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u5728\u73b0\u5b9e\u5f00\u53d1\u4efb\u52a1\u4e2d\u53ef\u80fd\u8bbf\u95ee\u654f\u611f\u6570\u636e\u6216\u5b89\u5168\u5173\u952e\u4ee3\u7801\u5e93\uff0c\u5982\u679c\u4ee3\u7406\u672a\u5bf9\u9f50\u53ef\u80fd\u6545\u610f\u7834\u574f\u4ee3\u7801\u5e93\uff0c\u9700\u8981\u76d1\u63a7\u7cfb\u7edf\u6765\u9632\u6b62\u4e0d\u53ef\u9006\u635f\u5bb3\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u76d1\u63a7\u65b9\u6cd5\uff08\u4e0d\u589e\u52a0\u8fd0\u884c\u65f6\u5ef6\u8fdf\uff09\uff0c\u901a\u8fc7\u7ea2\u84dd\u5bf9\u6297\u6e38\u620f\uff1a\u84dd\u961f\u8bbe\u8ba1\u76d1\u63a7\u5668\uff0c\u7ea2\u961f\u521b\u5efa\u7834\u574f\u6027\u4ee3\u7406\u3002\u57285\u4e2a\u591a\u6837\u5316\u7684\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u6a21\u62df\u771f\u5b9e\u90e8\u7f72\u573a\u666f\uff0c\u5f00\u53d1\u96c6\u6210\u76d1\u63a7\u5668\u3002", "result": "\u5728\u4fdd\u7559\u6d4b\u8bd5\u73af\u5883\u4e2d\uff0c\u96c6\u6210\u76d1\u63a7\u5668\u57281%\u8bef\u62a5\u7387\u4e0b\u8fbe\u52306%\u6f0f\u62a5\u7387\u3002\u901a\u8fc7\u4ece\u6f0f\u62a5\u7387\u5916\u63a8\u6765\u4f30\u8ba1\u90e8\u7f72\u65f6\u7684\u7834\u574f\u98ce\u9669\uff0c\u63d0\u4f9b\u4e86\u5916\u63a8\u6a21\u578b\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "conclusion": "\u5f02\u6b65\u76d1\u63a7\u662f\u9632\u6b62LLM\u4ee3\u7406\u7834\u574f\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u6e38\u620f\u53ef\u4ee5\u5f00\u53d1\u51fa\u9ad8\u6027\u80fd\u76d1\u63a7\u5668\uff0c\u4f46\u9700\u8981\u8c28\u614e\u5916\u63a8\u98ce\u9669\u4f30\u8ba1\u3002", "topic": "agent analysis"}}
{"id": "2512.13592", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13592", "abs": "https://arxiv.org/abs/2512.13592", "authors": ["Fu-Yun Wang", "Hao Zhou", "Liangzhe Yuan", "Sanghyun Woo", "Boqing Gong", "Bohyung Han", "Ming-Hsuan Yang", "Han Zhang", "Yukun Zhu", "Ting Liu", "Long Zhao"], "title": "Image Diffusion Preview with Consistency Solver", "comment": null, "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.", "AI": {"tldr": "\u63d0\u51faDiffusion Preview\u8303\u5f0f\uff0c\u901a\u8fc7\u5feb\u901f\u4f4e\u6b65\u6570\u91c7\u6837\u751f\u6210\u9884\u89c8\u4f9b\u7528\u6237\u8bc4\u4f30\uff0c\u6ee1\u610f\u540e\u518d\u8fdb\u884c\u5168\u6b65\u6570\u7cbe\u70bc\u3002\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9884\u89c8\u8d28\u91cf\u5dee\u548c\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684ConsistencySolver\u9ad8\u9636\u6c42\u89e3\u5668\u3002", "motivation": "\u56fe\u50cf\u6269\u6563\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u4e25\u91cd\u5f71\u54cd\u4ea4\u4e92\u4f53\u9a8c\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u9884\u89c8\u673a\u5236\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff08\u65e0\u8bad\u7ec3\u6c42\u89e3\u5668\u548c\u540e\u8bad\u7ec3\u84b8\u998f\uff09\u5728\u9884\u89c8\u8d28\u91cf\u548c\u9884\u89c8-\u6700\u7ec8\u8f93\u51fa\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faConsistencySolver\uff0c\u57fa\u4e8e\u901a\u7528\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u9ad8\u9636\u6c42\u89e3\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u63d0\u5347\u9884\u89c8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4f4e\u6b65\u6570\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u6bd4Multistep DPM-Solver\u5c11\u752847%\u6b65\u6570\u8fbe\u5230\u76f8\u5f53FID\u5206\u6570\uff0c\u4f18\u4e8e\u84b8\u998f\u57fa\u7ebf\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\u53ef\u51cf\u5c11\u8fd150%\u4ea4\u4e92\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "ConsistencySolver\u4e3a\u9ad8\u6548\u9884\u89c8-\u7cbe\u70bc\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u7406\u60f3\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u6269\u6563\u6a21\u578b\u4ea4\u4e92\u4f53\u9a8c\uff0c\u5e73\u8861\u4e86\u901f\u5ea6\u548c\u8d28\u91cf\u9700\u6c42\u3002", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2512.8a001f6d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linuxfoundation.org%2Fpress%2Flinux-foundation-announces-the-formation-of-the-agentic-ai-foundation%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/lhgD0Jqydg_RJDLKjorLPIWy9T1RmMbgbp8A_HMHahQ=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linuxfoundation.org%2Fpress%2Flinux-foundation-announces-the-formation-of-the-agentic-ai-foundation%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/lhgD0Jqydg_RJDLKjorLPIWy9T1RmMbgbp8A_HMHahQ=435", "authors": ["TLDR Newsletter"], "title": "Linux Foundation Announces the Formation of the Agentic AI Foundation", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.linuxfoundation.org%2Fpress%2Flinux-foundation-announces-the-formation-of-the-agentic-ai-foundation%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/lhgD0Jqydg_RJDLKjorLPIWy9T1RmMbgbp8A_HMHahQ=435", "summary": "Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF) (7 minute read) On December 9, the Linux Foundation launched the Agentic AI Foundation (AAIF), an initiative co-founded by Anthropic, Block, and OpenAI. The AAIF aims to advance open and accessible agentic AI, stewarding key projects such as Anthropic's Model Context Protocol (MCP), Block's goose, and OpenAI's AGENTS.md.", "source": "tldr", "AI": {"tldr": "Linux\u57fa\u91d1\u4f1a\u5ba3\u5e03\u6210\u7acbAgentic AI\u57fa\u91d1\u4f1a\uff08AAIF\uff09\uff0c\u7531Anthropic\u3001Block\u548cOpenAI\u5171\u540c\u521b\u7acb\uff0c\u65e8\u5728\u63a8\u8fdb\u5f00\u653e\u548c\u53ef\u8bbf\u95ee\u7684\u4ee3\u7406AI\uff0c\u7ba1\u7406\u5173\u952e\u9879\u76ee\u5982Model Context Protocol\u3001goose\u548cAGENTS.md\u3002", "motivation": "\u63a8\u52a8\u4ee3\u7406AI\uff08Agentic AI\uff09\u7684\u5f00\u653e\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u53d1\u5c55\uff0c\u901a\u8fc7\u5f00\u6e90\u793e\u533a\u534f\u4f5c\u6765\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6280\u672f\u8fdb\u6b65\u548c\u6807\u51c6\u5316\u3002", "method": "\u6210\u7acbLinux\u57fa\u91d1\u4f1a\u65d7\u4e0b\u7684AAIF\u7ec4\u7ec7\uff0c\u6c47\u96c6\u884c\u4e1a\u9886\u5148\u516c\u53f8\uff08Anthropic\u3001Block\u3001OpenAI\uff09\u5171\u540c\u7ba1\u7406\u5173\u952e\u4ee3\u7406AI\u9879\u76ee\uff0c\u5305\u62ec\u534f\u8bae\u3001\u6846\u67b6\u548c\u6587\u6863\u6807\u51c6\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86AAIF\u7ec4\u7ec7\uff0c\u5c06Anthropic\u7684Model Context Protocol\u3001Block\u7684goose\u548cOpenAI\u7684AGENTS.md\u7b49\u9879\u76ee\u7eb3\u5165\u7edf\u4e00\u7ba1\u7406\uff0c\u4e3a\u4ee3\u7406AI\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7ec4\u7ec7\u57fa\u7840\u3002", "conclusion": "AAIF\u7684\u6210\u7acb\u6807\u5fd7\u7740\u4ee3\u7406AI\u9886\u57df\u5411\u66f4\u52a0\u5f00\u653e\u548c\u534f\u4f5c\u7684\u65b9\u5411\u53d1\u5c55\uff0c\u6709\u671b\u901a\u8fc7\u5f00\u6e90\u793e\u533a\u7684\u529b\u91cf\u52a0\u901f\u4ee3\u7406AI\u6280\u672f\u7684\u6210\u719f\u548c\u5e94\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.f67763f1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenjamincongdon.me%2Fblog%2F2025%2F12%2F10%2FWhat-I-Look-For-in-AI-Assisted-PRs%2F%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/oZwNkqGqjmFasy2ZHdlQE1v5jigDj5FPS9eIYUAIbVE=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenjamincongdon.me%2Fblog%2F2025%2F12%2F10%2FWhat-I-Look-For-in-AI-Assisted-PRs%2F%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/oZwNkqGqjmFasy2ZHdlQE1v5jigDj5FPS9eIYUAIbVE=435", "authors": ["TLDR Newsletter"], "title": "What I Look For in AI-Assisted PRs", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 5 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fbenjamincongdon.me%2Fblog%2F2025%2F12%2F10%2FWhat-I-Look-For-in-AI-Assisted-PRs%2F%3Futm_source=tldrdevops/1/0100019b12741bef-94c1906f-c449-4957-8186-f2b56cc71dca-000000/oZwNkqGqjmFasy2ZHdlQE1v5jigDj5FPS9eIYUAIbVE=435", "summary": "What I Look For in AI-Assisted PRs (5 minute read) As AI makes it easier to generate code, the burden on reviewers increases, requiring strong heuristics that assess clarity of PR descriptions, reversibility of changes, appropriate scope, coherent abstractions, meaningful tests, and healthy error-handling while watching for \u201cvibe\u201d code smells that indicate low human curation. AI can help with first-pass review but still struggles with conceptual design issues, so human judgment\u2014and eventually...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86AI\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u4e2d\u9700\u8981\u5173\u6ce8\u7684\u5173\u952e\u8981\u7d20\uff1a\u6e05\u6670\u7684PR\u63cf\u8ff0\u3001\u53ef\u9006\u7684\u53d8\u66f4\u3001\u9002\u5f53\u7684\u8303\u56f4\u3001\u4e00\u81f4\u7684\u62bd\u8c61\u3001\u6709\u610f\u4e49\u7684\u6d4b\u8bd5\u548c\u5065\u5eb7\u7684\u9519\u8bef\u5904\u7406\uff0c\u540c\u65f6\u8b66\u60d5\"\u6c1b\u56f4\"\u4ee3\u7801\u5f02\u5473\uff0c\u6307\u51faAI\u5728\u6982\u5ff5\u8bbe\u8ba1\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u968f\u7740AI\u4f7f\u4ee3\u7801\u751f\u6210\u53d8\u5f97\u66f4\u52a0\u5bb9\u6613\uff0c\u5ba1\u67e5\u8005\u7684\u8d1f\u62c5\u4e5f\u968f\u4e4b\u589e\u52a0\uff0c\u9700\u8981\u5efa\u7acb\u5f3a\u5927\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u8bc4\u4f30PR\u8d28\u91cf\uff0c\u540c\u65f6\u8ba4\u8bc6\u5230AI\u5728\u6982\u5ff5\u8bbe\u8ba1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4eba\u7c7b\u5224\u65ad\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957AI\u8f85\u52a9PR\u5ba1\u67e5\u7684\u542f\u53d1\u5f0f\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u516d\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff1aPR\u63cf\u8ff0\u6e05\u6670\u5ea6\u3001\u53d8\u66f4\u53ef\u9006\u6027\u3001\u8303\u56f4\u9002\u5f53\u6027\u3001\u62bd\u8c61\u4e00\u81f4\u6027\u3001\u6d4b\u8bd5\u610f\u4e49\u6027\u548c\u9519\u8bef\u5904\u7406\u5065\u5eb7\u6027\uff0c\u5e76\u5f15\u5165\"\u6c1b\u56f4\"\u4ee3\u7801\u5f02\u5473\u68c0\u6d4b\u6982\u5ff5\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u5957\u5b9e\u7528\u7684AI\u8f85\u52a9\u4ee3\u7801\u5ba1\u67e5\u8bc4\u4f30\u6807\u51c6\uff0c\u5e2e\u52a9\u5ba1\u67e5\u8005\u66f4\u9ad8\u6548\u5730\u5904\u7406AI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u540c\u65f6\u6307\u51fa\u4e86AI\u5728\u6982\u5ff5\u8bbe\u8ba1\u95ee\u9898\u4e0a\u7684\u5f53\u524d\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u65b9\u5411\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "conclusion": "AI\u53ef\u4ee5\u5728\u521d\u6b65\u4ee3\u7801\u5ba1\u67e5\u4e2d\u63d0\u4f9b\u5e2e\u52a9\uff0c\u4f46\u5728\u6982\u5ff5\u8bbe\u8ba1\u95ee\u9898\u4e0a\u4ecd\u6709\u56f0\u96be\uff0c\u9700\u8981\u4eba\u7c7b\u5224\u65ad\u7684\u6301\u7eed\u53c2\u4e0e\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55AI\u5728\u9ad8\u7ea7\u8bbe\u8ba1\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\u3002", "topic": "swe application"}}
{"id": "tldr.2512.ec91dadf", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.shopify.com%2Fnews%2Fwinter-26-edition-dev%3Futm_source=comms_paid%26utm_medium=newsletter%26utm_campaign=winter26edition-launch_Q425BACADO%26utm_content=tldrdev-v1/1/0100019b1277bcc2-c47fa2ac-3b57-432b-90fc-a3d12b56586e-000000/SWTnvUm8C2lGs75VdnjV2LVJYJSXIjd6Vv-K5s2qbBk=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.shopify.com%2Fnews%2Fwinter-26-edition-dev%3Futm_source=comms_paid%26utm_medium=newsletter%26utm_campaign=winter26edition-launch_Q425BACADO%26utm_content=tldrdev-v1/1/0100019b1277bcc2-c47fa2ac-3b57-432b-90fc-a3d12b56586e-000000/SWTnvUm8C2lGs75VdnjV2LVJYJSXIjd6Vv-K5s2qbBk=435", "authors": ["TLDR Newsletter"], "title": "Shopify's Winter '26 Edition puts AI agents in the driver's seat", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.shopify.com%2Fnews%2Fwinter-26-edition-dev%3Futm_source=comms_paid%26utm_medium=newsletter%26utm_campaign=winter26edition-launch_Q425BACADO%26utm_content=tldrdev-v1/1/0100019b1277bcc2-c47fa2ac-3b57-432b-90fc-a3d12b56586e-000000/SWTnvUm8C2lGs75VdnjV2LVJYJSXIjd6Vv-K5s2qbBk=435", "summary": "Shopify's Winter '26 Edition puts AI agents in the driver's seat (Sponsor) Shopify's developer platform is now AI-native. Your AI agents can create dev stores, scaffold apps, run GraphQL operations, and generate validated code across Shopify. The shopify.dev docs now render for LLM access on the server side - allowing agents to deliver more accurate code and answers. See all the new developer features", "source": "tldr", "AI": {"tldr": "Shopify\u63a8\u51faAI\u539f\u751f\u5f00\u53d1\u8005\u5e73\u53f0\uff0cAI\u4ee3\u7406\u53ef\u521b\u5efa\u5f00\u53d1\u5546\u5e97\u3001\u642d\u5efa\u5e94\u7528\u3001\u8fd0\u884cGraphQL\u64cd\u4f5c\u5e76\u751f\u6210\u9a8c\u8bc1\u4ee3\u7801\uff0c\u6587\u6863\u652f\u6301LLM\u670d\u52a1\u5668\u7aef\u8bbf\u95ee\u4ee5\u63d0\u5347\u4ee3\u7801\u51c6\u786e\u6027", "motivation": "\u5c06Shopify\u5f00\u53d1\u8005\u5e73\u53f0AI\u5316\uff0c\u8ba9AI\u4ee3\u7406\u80fd\u591f\u76f4\u63a5\u6267\u884c\u5f00\u53d1\u4efb\u52a1\uff0c\u63d0\u9ad8\u5f00\u53d1\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u964d\u4f4e\u5f00\u53d1\u8005\u95e8\u69db", "method": "\u6784\u5efaAI\u539f\u751f\u5f00\u53d1\u5e73\u53f0\uff0c\u63d0\u4f9bAPI\u8ba9AI\u4ee3\u7406\u521b\u5efa\u5f00\u53d1\u5546\u5e97\u3001\u642d\u5efa\u5e94\u7528\u6846\u67b6\u3001\u6267\u884cGraphQL\u64cd\u4f5c\u3001\u751f\u6210\u9a8c\u8bc1\u4ee3\u7801\uff0c\u5e76\u5c06\u6587\u6863\u4f18\u5316\u4e3aLLM\u53ef\u8bbf\u95ee\u683c\u5f0f", "result": "AI\u4ee3\u7406\u73b0\u5728\u80fd\u591f\u5728Shopify\u5e73\u53f0\u4e0a\u6267\u884c\u5b8c\u6574\u7684\u5f00\u53d1\u5de5\u4f5c\u6d41\uff0c\u4ece\u521b\u5efa\u5546\u5e97\u5230\u751f\u6210\u9a8c\u8bc1\u4ee3\u7801\uff0c\u6587\u6863\u670d\u52a1\u5668\u7aef\u6e32\u67d3\u63d0\u5347AI\u4ee3\u7406\u7684\u4ee3\u7801\u51c6\u786e\u6027", "conclusion": "Shopify\u6210\u529f\u5c06AI\u4ee3\u7406\u96c6\u6210\u5230\u5f00\u53d1\u5e73\u53f0\u4e2d\uff0c\u5b9e\u73b0\u4e86AI\u539f\u751f\u5f00\u53d1\u4f53\u9a8c\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684AI\u8f85\u52a9\u5de5\u5177", "topic": "code agent"}}
{"id": "tldr.2512.a5cce539", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2137%26utm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/JWDUhlPesDXIzZwXNI4QJKoFHe9wHgoesDBHd1gdioU=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2137%26utm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/JWDUhlPesDXIzZwXNI4QJKoFHe9wHgoesDBHd1gdioU=435", "authors": ["TLDR Newsletter"], "title": "AI Coding Agents for Designers", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 3 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.lukew.com%2Fff%2Fentry.asp%3F2137%26utm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/JWDUhlPesDXIzZwXNI4QJKoFHe9wHgoesDBHd1gdioU=435", "summary": "AI Coding Agents for Designers (3 minute read) AI coding agents, already writing the majority of code at companies like Anthropic and OpenAI, offer designers new capabilities beyond traditional prototyping. Designers can now fix production bugs directly, learn system architecture by observing agent processes, and better determine when engineering collaboration is needed. These tools require a deep understanding of the codebase to work effectively, with context engines optimized for complex co...", "source": "tldr", "AI": {"tldr": "AI\u7f16\u7a0b\u4ee3\u7406\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u65b0\u80fd\u529b\uff0c\u53ef\u76f4\u63a5\u4fee\u590d\u751f\u4ea7bug\u3001\u5b66\u4e60\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u66f4\u597d\u5730\u5224\u65ad\u4f55\u65f6\u9700\u8981\u5de5\u7a0b\u534f\u4f5c", "motivation": "AI\u7f16\u7a0b\u4ee3\u7406\u5df2\u5728Anthropic\u548cOpenAI\u7b49\u516c\u53f8\u7f16\u5199\u5927\u90e8\u5206\u4ee3\u7801\uff0c\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u8d85\u8d8a\u4f20\u7edf\u539f\u578b\u8bbe\u8ba1\u7684\u65b0\u80fd\u529b\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u8ba9\u8bbe\u8ba1\u5e08\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u5de5\u5177", "method": "\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u5f15\u64ce\u5904\u7406\u590d\u6742\u4ee3\u7801\u5e93\uff0c\u4f7fAI\u7f16\u7a0b\u4ee3\u7406\u80fd\u591f\u6709\u6548\u5de5\u4f5c\uff1b\u8bbe\u8ba1\u5e08\u901a\u8fc7\u89c2\u5bdf\u4ee3\u7406\u6d41\u7a0b\u5b66\u4e60\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u76f4\u63a5\u53c2\u4e0e\u751f\u4ea7bug\u4fee\u590d", "result": "\u8bbe\u8ba1\u5e08\u83b7\u5f97\u4e86\u76f4\u63a5\u4fee\u590d\u751f\u4ea7bug\u7684\u80fd\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u89c2\u5bdfAI\u4ee3\u7406\u6d41\u7a0b\u5b66\u4e60\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u80fd\u66f4\u51c6\u786e\u5730\u5224\u65ad\u4f55\u65f6\u9700\u8981\u5de5\u7a0b\u534f\u4f5c", "conclusion": "AI\u7f16\u7a0b\u4ee3\u7406\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u4f5c\u80fd\u529b\uff0c\u6539\u53d8\u4e86\u4f20\u7edf\u8bbe\u8ba1-\u5de5\u7a0b\u534f\u4f5c\u6a21\u5f0f\uff0c\u4f46\u9700\u8981\u6df1\u5165\u4e86\u89e3\u4ee3\u7801\u5e93\u548c\u4f18\u5316\u7684\u4e0a\u4e0b\u6587\u5f15\u64ce\u652f\u6301", "topic": "code agent"}}
{"id": "tldr.2512.1da59855", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.komposo.ai%2F%3Futm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/G3Qf0Is_mr9R-NQ3o6LT5BLE7zvQlde8oo2SmuOMFMU=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.komposo.ai%2F%3Futm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/G3Qf0Is_mr9R-NQ3o6LT5BLE7zvQlde8oo2SmuOMFMU=435", "authors": ["TLDR Newsletter"], "title": "Turn Your Product Ideas Into Reality", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.komposo.ai%2F%3Futm_source=tldrdesign/1/0100019b12ab23b5-9fef9354-7d4a-4877-abba-c42c504e3d49-000000/G3Qf0Is_mr9R-NQ3o6LT5BLE7zvQlde8oo2SmuOMFMU=435", "summary": "Turn Your Product Ideas Into Reality (Website) Komposo helps designers and developers clone any website or web app UI, customize designs with AI prompts, and export to code instantly.", "source": "tldr", "AI": {"tldr": "Komposo\u662f\u4e00\u4e2a\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u548c\u5f00\u53d1\u8005\u514b\u9686\u7f51\u7ad9/Web\u5e94\u7528UI\u3001\u901a\u8fc7AI\u63d0\u793a\u5b9a\u5236\u8bbe\u8ba1\u5e76\u5373\u65f6\u5bfc\u51fa\u4ee3\u7801\u7684\u5de5\u5177", "motivation": "\u7b80\u5316\u7f51\u7ad9\u548cWeb\u5e94\u7528UI\u7684\u521b\u5efa\u6d41\u7a0b\uff0c\u8ba9\u8bbe\u8ba1\u5e08\u548c\u5f00\u53d1\u8005\u80fd\u591f\u5feb\u901f\u5c06\u4ea7\u54c1\u60f3\u6cd5\u8f6c\u5316\u4e3a\u73b0\u5b9e\uff0c\u51cf\u5c11\u624b\u52a8\u7f16\u7801\u5de5\u4f5c", "method": "\u63d0\u4f9b\u7f51\u7ad9\u514b\u9686\u529f\u80fd\u3001AI\u9a71\u52a8\u7684\u8bbe\u8ba1\u5b9a\u5236\u5de5\u5177\uff0c\u4ee5\u53ca\u4ee3\u7801\u5bfc\u51fa\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4ece\u8bbe\u8ba1\u5230\u4ee3\u7801\u7684\u65e0\u7f1d\u8f6c\u6362", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u514b\u9686\u73b0\u6709\u7f51\u7ad9UI\u3001\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9a\u5236\u8bbe\u8ba1\u3001\u5e76\u5bfc\u51fa\u53ef\u7528\u4ee3\u7801\u7684\u5e73\u53f0", "conclusion": "Komposo\u901a\u8fc7\u7ed3\u5408\u7f51\u7ad9\u514b\u9686\u3001AI\u8bbe\u8ba1\u8f85\u52a9\u548c\u4ee3\u7801\u5bfc\u51fa\u529f\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f51\u7ad9\u5f00\u53d1\u6548\u7387", "topic": "swe application"}}
{"id": "tldr.2512.a7c5aa2f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgreylock.com%2Fgreymatter%2Frl-in-real-life-durable-moats%2F%3Futm_source=tldrfounders/1/0100019b12b1ac07-9daf71bf-471a-4e35-9057-22f02f7756ba-000000/IMCqFIXL9Xf-7yDJQV8g3Cw1EAqkGTahp802SUhbS-A=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgreylock.com%2Fgreymatter%2Frl-in-real-life-durable-moats%2F%3Futm_source=tldrfounders/1/0100019b12b1ac07-9daf71bf-471a-4e35-9057-22f02f7756ba-000000/IMCqFIXL9Xf-7yDJQV8g3Cw1EAqkGTahp802SUhbS-A=435", "authors": ["TLDR Newsletter"], "title": "RL in Real Life: Durable Moats", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 15 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgreylock.com%2Fgreymatter%2Frl-in-real-life-durable-moats%2F%3Futm_source=tldrfounders/1/0100019b12b1ac07-9daf71bf-471a-4e35-9057-22f02f7756ba-000000/IMCqFIXL9Xf-7yDJQV8g3Cw1EAqkGTahp802SUhbS-A=435", "summary": "RL in Real Life: Durable Moats (15 minute read) Reinforcement learning (RL) has gone from being a promising path towards general intelligence to a taboo phrase over the past year. It has proved highly effective inside AI labs, but outside of the labs, its use cases remain surprisingly underdeveloped. Most efforts appear to be largely experimental and have yet to scratch the surface of the sophisticated techniques used within the frontier labs. This post looks at the intricacies of building ef...", "source": "tldr", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u6307\u51fa\u867d\u7136RL\u5728AI\u5b9e\u9a8c\u5ba4\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u9700\u8981\u6784\u5efa\u6301\u4e45\u7684\u6280\u672f\u62a4\u57ce\u6cb3\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ece\u901a\u7528\u667a\u80fd\u7684\u6f5c\u5728\u8def\u5f84\u53d8\u6210\u4e86\u8fd1\u5e74\u7684\"\u7981\u5fcc\u8bcd\"\uff0c\u5c3d\u7ba1\u5728AI\u5b9e\u9a8c\u5ba4\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7528\u4f8b\u4ecd\u7136\u51fa\u4eba\u610f\u6599\u5730\u4e0d\u53d1\u8fbe\u3002\u5927\u591a\u6570\u52aa\u529b\u4ecd\u5904\u4e8e\u5b9e\u9a8c\u9636\u6bb5\uff0c\u5c1a\u672a\u89e6\u53ca\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u4f7f\u7528\u7684\u590d\u6742\u6280\u672f\u6df1\u5ea6\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5206\u6790RL\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u590d\u6742\u6027\uff0c\u63a2\u8ba8\u5982\u4f55\u6784\u5efa\u6709\u6548\u7684RL\u7cfb\u7edf\u3002\u867d\u7136\u6ca1\u6709\u5177\u4f53\u7684\u6280\u672f\u65b9\u6cd5\u63cf\u8ff0\uff0c\u4f46\u805a\u7126\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u548c\u6784\u5efa\u6301\u4e45\u6280\u672f\u4f18\u52bf\u7684\u7b56\u7565\u3002", "result": "\u6307\u51faRL\u5728\u5b9e\u9a8c\u5ba4\u5916\u5e94\u7528\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u5927\u591a\u6570\u5e94\u7528\u662f\u5b9e\u9a8c\u6027\u7684\uff0c\u5c1a\u672a\u5145\u5206\u5229\u7528\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u7684\u590d\u6742\u6280\u672f\u3002\u5f3a\u8c03\u9700\u8981\u6784\u5efa\"\u6301\u4e45\u62a4\u57ce\u6cb3\"\u6765\u786e\u4fddRL\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6210\u529f\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u590d\u6742\u6027\u5e76\u6784\u5efa\u6301\u4e45\u7684\u6280\u672f\u4f18\u52bf\uff0c\u624d\u80fd\u5b9e\u73b0\u4ece\u5b9e\u9a8c\u5ba4\u5230\u5b9e\u9645\u5e94\u7528\u7684\u8f6c\u53d8\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.afe2ba4c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgadievron%2Fraptor%3Futm_source=tldrinfosec/1/0100019b12e496da-c3bceb21-0c74-43b1-996d-4b7264a3320c-000000/wj5fnz8ZxfUQpoIQz4le6Zn1aYRVNIfM8BMt4mcCSK8=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgadievron%2Fraptor%3Futm_source=tldrinfosec/1/0100019b12e496da-c3bceb21-0c74-43b1-996d-4b7264a3320c-000000/wj5fnz8ZxfUQpoIQz4le6Zn1aYRVNIfM8BMt4mcCSK8=435", "authors": ["TLDR Newsletter"], "title": "RAPTOR", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fgadievron%2Fraptor%3Futm_source=tldrinfosec/1/0100019b12e496da-c3bceb21-0c74-43b1-996d-4b7264a3320c-000000/wj5fnz8ZxfUQpoIQz4le6Zn1aYRVNIfM8BMt4mcCSK8=435", "summary": "RAPTOR (GitHub Repo) RAPTOR is an autonomous offensive/defensive security research framework built on Claude Code.", "source": "tldr", "AI": {"tldr": "RAPTOR\u662f\u4e00\u4e2a\u57fa\u4e8eClaude Code\u6784\u5efa\u7684\u81ea\u4e3b\u653b\u9632\u5b89\u5168\u7814\u7a76\u6846\u67b6", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u81ea\u52a8\u5316\u5b89\u5168\u7814\u7a76\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u4e3b\u8fdb\u884c\u653b\u9632\u5b89\u5168\u6d4b\u8bd5\u548c\u7814\u7a76\uff0c\u63d0\u9ad8\u5b89\u5168\u8bc4\u4f30\u7684\u6548\u7387\u548c\u81ea\u52a8\u5316\u6c34\u5e73", "method": "\u57fa\u4e8eClaude Code\u6784\u5efa\uff0c\u91c7\u7528\u81ea\u4e3b\u4ee3\u7406\u67b6\u6784\uff0c\u80fd\u591f\u6267\u884c\u8fdb\u653b\u6027\u548c\u9632\u5fa1\u6027\u5b89\u5168\u4efb\u52a1\uff0c\u5305\u62ec\u6f0f\u6d1e\u53d1\u73b0\u3001\u6e17\u900f\u6d4b\u8bd5\u548c\u5b89\u5168\u8bc4\u4f30", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u81ea\u4e3b\u5b89\u5168\u7814\u7a76\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u6267\u884c\u590d\u6742\u7684\u5b89\u5168\u653b\u9632\u4efb\u52a1", "conclusion": "RAPTOR\u6846\u67b6\u4e3a\u5b89\u5168\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5b89\u5168\u6d4b\u8bd5\u548c\u7814\u7a76\u7684\u6548\u7387", "topic": "code agent"}}
{"id": "tldr.2512.eb9ac186", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbG7jeb/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/fzJV9qkj0I_dZrNwirnJngMXyJhf5-WNSPCPMPQeb5A=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbG7jeb/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/fzJV9qkj0I_dZrNwirnJngMXyJhf5-WNSPCPMPQeb5A=435", "authors": ["TLDR Newsletter"], "title": "Introducing GPT-5.2", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FbG7jeb/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/fzJV9qkj0I_dZrNwirnJngMXyJhf5-WNSPCPMPQeb5A=435", "summary": "Introducing GPT-5.2 (8 minute read) OpenAI's GPT\u20115.2 is optimized for professional use and long-term agent workflows. It improves performance across knowledge work, long-context reasoning, tool use, and multimodal understanding.", "source": "tldr", "AI": {"tldr": "OpenAI\u53d1\u5e03GPT-5.2\uff0c\u4e13\u4e3a\u4e13\u4e1a\u7528\u9014\u548c\u957f\u671f\u4ee3\u7406\u5de5\u4f5c\u6d41\u4f18\u5316\uff0c\u5728\u77e5\u8bc6\u5de5\u4f5c\u3001\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347", "motivation": "\u4e3a\u6ee1\u8db3\u4e13\u4e1a\u573a\u666f\u548c\u957f\u671f\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u9700\u6c42\uff0c\u5f00\u53d1\u66f4\u5f3a\u5927\u7684AI\u6a21\u578b\u4ee5\u63d0\u5347\u77e5\u8bc6\u5de5\u4f5c\u3001\u590d\u6742\u63a8\u7406\u3001\u5de5\u5177\u96c6\u6210\u548c\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b", "method": "\u5f00\u53d1GPT-5.2\u6a21\u578b\uff0c\u9488\u5bf9\u4e13\u4e1a\u7528\u9014\u548c\u957f\u671f\u4ee3\u7406\u5de5\u4f5c\u6d41\u8fdb\u884c\u4f18\u5316\uff0c\u589e\u5f3a\u5728\u77e5\u8bc6\u5de5\u4f5c\u3001\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b", "result": "GPT-5.2\u5728\u77e5\u8bc6\u5de5\u4f5c\u3001\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u6027\u80fd\u5f97\u5230\u63d0\u5347\uff0c\u9002\u5408\u4e13\u4e1a\u573a\u666f\u548c\u957f\u671f\u4ee3\u7406\u5de5\u4f5c\u6d41", "conclusion": "GPT-5.2\u4f5c\u4e3a\u4f18\u5316\u7248\u672c\uff0c\u4e3a\u4e13\u4e1a\u7528\u6237\u548c\u957f\u671f\u4ee3\u7406\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684AI\u80fd\u529b\u652f\u6301", "topic": "code agent"}}
{"id": "tldr.2512.17c60350", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbrowser-visual-editor%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/7hz4Ody8JEa0lbGlyjVn7mV030HYGtKhovZOEWjpLAE=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbrowser-visual-editor%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/7hz4Ody8JEa0lbGlyjVn7mV030HYGtKhovZOEWjpLAE=435", "authors": ["TLDR Newsletter"], "title": "Cursor Browser Visual Editor", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fbrowser-visual-editor%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/7hz4Ody8JEa0lbGlyjVn7mV030HYGtKhovZOEWjpLAE=435", "summary": "Cursor Browser Visual Editor (2 minute read) Cursor released a new visual editor that integrates design and code within the Cursor Browser. Users can drag UI elements, inspect props, and make changes interactively.", "source": "tldr", "AI": {"tldr": "Cursor\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u53ef\u89c6\u5316\u7f16\u8f91\u5668\uff0c\u5c06\u8bbe\u8ba1\u548c\u4ee3\u7801\u96c6\u6210\u5728Cursor\u6d4f\u89c8\u5668\u4e2d\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u62d6\u62fdUI\u5143\u7d20\u3001\u68c0\u67e5\u5c5e\u6027\u548c\u4ea4\u4e92\u5f0f\u4fee\u6539\u6765\u7f16\u8f91\u754c\u9762\u3002", "motivation": "\u4f20\u7edf\u7684\u4ee3\u7801\u7f16\u8f91\u548cUI\u8bbe\u8ba1\u901a\u5e38\u662f\u5206\u79bb\u7684\u8fc7\u7a0b\uff0c\u8fd9\u5bfc\u81f4\u5f00\u53d1\u6548\u7387\u4f4e\u4e0b\u548c\u8bbe\u8ba1\u5b9e\u73b0\u4e0d\u4e00\u81f4\u3002Cursor\u65e8\u5728\u901a\u8fc7\u5c06\u53ef\u89c6\u5316\u7f16\u8f91\u76f4\u63a5\u96c6\u6210\u5230\u4ee3\u7801\u7f16\u8f91\u5668\u4e2d\uff0c\u5f25\u5408\u8bbe\u8ba1\u4e0e\u5f00\u53d1\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6d4f\u89c8\u5668\u5185\u7684\u53ef\u89c6\u5316\u7f16\u8f91\u5668\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u62d6\u62fdUI\u5143\u7d20\u3001\u5b9e\u65f6\u68c0\u67e5\u7ec4\u4ef6\u5c5e\u6027\u3001\u4ea4\u4e92\u5f0f\u4fee\u6539\u754c\u9762\u7b49\u65b9\u5f0f\u8fdb\u884c\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5e95\u5c42\u4ee3\u7801\u7684\u540c\u6b65\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u96c6\u6210\u7684\u8bbe\u8ba1\u548c\u5f00\u53d1\u73af\u5883\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u540c\u4e00\u4e2a\u5de5\u5177\u4e2d\u5b8c\u6210UI\u8bbe\u8ba1\u548c\u4ee3\u7801\u7f16\u8f91\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u548c\u8bbe\u8ba1\u5b9e\u73b0\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u53ef\u89c6\u5316\u7f16\u8f91\u5668\u6210\u529f\u5730\u5c06\u8bbe\u8ba1\u548c\u4ee3\u7801\u7f16\u8f91\u878d\u5408\u5728\u4e00\u8d77\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u7684\u5f00\u53d1\u4f53\u9a8c\uff0c\u51cf\u5c11\u4e86\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e4b\u95f4\u7684\u8f6c\u6362\u6210\u672c\u3002", "topic": "swe application"}}
{"id": "tldr.2512.30d0fb90", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fdeep-research-agent-gemini-api%2F%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/DpC5HPtXndI_pr_gCCMcMqW_PSf1VXy8plVmc__u6Pw=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fdeep-research-agent-gemini-api%2F%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/DpC5HPtXndI_pr_gCCMcMqW_PSf1VXy8plVmc__u6Pw=435", "authors": ["TLDR Newsletter"], "title": "Build with Gemini Deep Research", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.google%2Ftechnology%2Fdevelopers%2Fdeep-research-agent-gemini-api%2F%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/DpC5HPtXndI_pr_gCCMcMqW_PSf1VXy8plVmc__u6Pw=435", "summary": "Build with Gemini Deep Research (4 minute read) Gemini Deep Research is now available to developers via the new Interactions API. It is launching alongside DeepSearchQA, a benchmark for complex web search tasks. Gemini Deep Research is an agent optimized for running context gathering and synthesis tasks. It uses Gemini 3 Pro as its reasoning core. The agent autonomously navigates complex information landscapes with high accuracy by scaling multi-step reinforcement learning for search.", "source": "tldr", "AI": {"tldr": "Gemini Deep Research\u662f\u4e00\u4e2a\u4e13\u4e3a\u590d\u6742\u4fe1\u606f\u641c\u96c6\u4e0e\u5408\u6210\u4efb\u52a1\u4f18\u5316\u7684AI\u4ee3\u7406\uff0c\u57fa\u4e8eGemini 3 Pro\u63a8\u7406\u6838\u5fc3\uff0c\u901a\u8fc7\u591a\u6b65\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u81ea\u4e3b\u7f51\u7edc\u641c\u7d22", "motivation": "\u89e3\u51b3\u590d\u6742\u7f51\u7edc\u641c\u7d22\u4efb\u52a1\u4e2d\u9700\u8981\u591a\u6b65\u9aa4\u4fe1\u606f\u641c\u96c6\u3001\u9a8c\u8bc1\u548c\u5408\u6210\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u81ea\u4e3b\u4fe1\u606f\u5904\u7406\u80fd\u529b", "method": "\u4f7f\u7528Gemini 3 Pro\u4f5c\u4e3a\u63a8\u7406\u6838\u5fc3\uff0c\u901a\u8fc7\u591a\u6b65\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u641c\u7d22\u7b56\u7565\uff0c\u81ea\u4e3b\u5bfc\u822a\u590d\u6742\u4fe1\u606f\u73af\u5883", "result": "\u53d1\u5e03\u4e86Gemini Deep Research API\u548cDeepSearchQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u590d\u6742\u7f51\u7edc\u641c\u7d22\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "Gemini Deep Research\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u641c\u7d22\u4ee3\u7406\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u4fe1\u606f\u641c\u96c6\u4efb\u52a1\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7814\u7a76\u5de5\u5177", "topic": "agent analysis"}}
{"id": "tldr.2512.59d9ce9c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F200k-tokens-is-plenty%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/T8rQ2NEEwlZWpeIelSsWVyMw49rGe0RnJBuvVE_ZFQo=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F200k-tokens-is-plenty%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/T8rQ2NEEwlZWpeIelSsWVyMw49rGe0RnJBuvVE_ZFQo=435", "authors": ["TLDR Newsletter"], "title": "200k Tokens Is Plenty", "comment": "Source: TLDR Newsletter, Date: 2025-12-12, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fampcode.com%2F200k-tokens-is-plenty%3Futm_source=tldrai/1/0100019b12ec215c-fccaebb5-6be9-467d-9956-15002b3d23fc-000000/T8rQ2NEEwlZWpeIelSsWVyMw49rGe0RnJBuvVE_ZFQo=435", "summary": "200k Tokens Is Plenty (4 minute read) Long context windows are overrated for agentic coding. Amp's team ships features using clusters of small threads averaging 80k tokens each rather than jamming everything into one million-token mega-thread.", "source": "tldr", "AI": {"tldr": "Amp\u56e2\u961f\u53d1\u73b0\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u5bf9\u4e8e\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u88ab\u9ad8\u4f30\uff0c\u4ed6\u4eec\u4f7f\u7528\u5e73\u574780k token\u7684\u5c0f\u7ebf\u7a0b\u96c6\u7fa4\u800c\u975e\u767e\u4e07token\u5927\u7ebf\u7a0b\u6765\u5f00\u53d1\u529f\u80fd", "motivation": "\u5f53\u524dAI\u7f16\u7801\u4ee3\u7406\u666e\u904d\u8ffd\u6c42\u8d85\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff08\u5982\u767e\u4e07token\uff09\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\"\u8d8a\u5927\u8d8a\u597d\"\u7684\u601d\u8def\u88ab\u8fc7\u5ea6\u5938\u5927\uff0c\u5b9e\u9645\u5f00\u53d1\u4e2d\u5e76\u4e0d\u9700\u8981\u5982\u6b64\u5e9e\u5927\u7684\u5355\u4e00\u7ebf\u7a0b", "method": "\u91c7\u7528\u5c0f\u7ebf\u7a0b\u96c6\u7fa4\u7b56\u7565\uff0c\u5c06\u5f00\u53d1\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5e73\u574780k token\u7684\u5c0f\u7ebf\u7a0b\uff0c\u901a\u8fc7\u7ebf\u7a0b\u95f4\u534f\u4f5c\u800c\u975e\u5355\u4e00\u8d85\u5927\u7ebf\u7a0b\u6765\u5b8c\u6210\u590d\u6742\u7f16\u7801\u4efb\u52a1", "result": "200k token\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5df2\u8db3\u591f\u6ee1\u8db3\u5b9e\u9645\u5f00\u53d1\u9700\u6c42\uff0c\u5c0f\u7ebf\u7a0b\u96c6\u7fa4\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4ea4\u4ed8\u529f\u80fd\uff0c\u907f\u514d\u4e86\u8d85\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6548\u7387\u95ee\u9898", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u5bf9\u4e8e\u667a\u80fd\u7f16\u7801\u4ee3\u7406\u7684\u91cd\u8981\u6027\u88ab\u9ad8\u4f30\uff0c\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u9002\u5f53\u5927\u5c0f\u7684\u5c0f\u7ebf\u7a0b\u96c6\u7fa4\u8fdb\u884c\u534f\u4f5c\u5f00\u53d1", "topic": "code agent"}}
{"id": "tldr.2512.9c926a3c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFAR5RM/2/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/5-K7NRAYmk3kpY8vl96DZuXM5mGN5LTtu0BmsEfiuks=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFAR5RM/2/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/5-K7NRAYmk3kpY8vl96DZuXM5mGN5LTtu0BmsEfiuks=435", "authors": ["TLDR Newsletter"], "title": "Agents that don't suck", "comment": "Source: TLDR Newsletter, Date: 2025-12-15, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FFAR5RM/2/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/5-K7NRAYmk3kpY8vl96DZuXM5mGN5LTtu0BmsEfiuks=435", "summary": "Agents that don't suck (Sponsor) Most AI agents never reach production. Agent Bricks by Databricks fixes that by helping you build agents that actually work \u2014 accurate, reliable and grounded in your data. This product delivers practical quality, not hype: automatic evaluation, metrics tuned to your goals, and human feedback to keep improving accuracy. Generic benchmarks don't cut it. Agent Bricks measures performance on the tasks that matter to your business. The result: production agents you...", "source": "tldr", "AI": {"tldr": "Databricks\u63a8\u51faAgent Bricks\u5e73\u53f0\uff0c\u5e2e\u52a9\u6784\u5efa\u53ef\u5b9e\u9645\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\u7684AI\u667a\u80fd\u4f53\uff0c\u63d0\u4f9b\u81ea\u52a8\u8bc4\u4f30\u3001\u5b9a\u5236\u5316\u6307\u6807\u548c\u4eba\u5de5\u53cd\u9988\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u51c6\u786e\u53ef\u9760\u4e14\u57fa\u4e8e\u4f01\u4e1a\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570AI\u667a\u80fd\u4f53\u65e0\u6cd5\u771f\u6b63\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\uff0c\u9700\u8981\u89e3\u51b3\u667a\u80fd\u4f53\u7684\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u6570\u636e\u57fa\u7840\u95ee\u9898\uff0c\u8d85\u8d8a\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6ee1\u8db3\u4f01\u4e1a\u5b9e\u9645\u4e1a\u52a1\u9700\u6c42\u3002", "method": "\u901a\u8fc7Agent Bricks\u5e73\u53f0\u63d0\u4f9b\uff1a\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u3001\u6839\u636e\u4f01\u4e1a\u76ee\u6807\u5b9a\u5236\u7684\u6027\u80fd\u6307\u6807\u3001\u4eba\u5de5\u53cd\u9988\u673a\u5236\u6765\u6301\u7eed\u6539\u8fdb\u51c6\u786e\u6027\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u57fa\u4e8e\u4f01\u4e1a\u7279\u5b9a\u6570\u636e\u3002", "result": "\u80fd\u591f\u6784\u5efa\u51fa\u771f\u6b63\u53ef\u90e8\u7f72\u5230\u751f\u4ea7\u73af\u5883\u7684\u667a\u80fd\u4f53\uff0c\u5177\u6709\u5b9e\u9645\u4e1a\u52a1\u4ef7\u503c\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u6982\u5ff5\u9a8c\u8bc1\u6216\u6f14\u793a\u539f\u578b\u3002", "conclusion": "Agent Bricks\u89e3\u51b3\u4e86AI\u667a\u80fd\u4f53\u4ece\u6982\u5ff5\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u5b9e\u7528\u7684\u8d28\u91cf\u4fdd\u8bc1\u800c\u975e\u7092\u4f5c\uff0c\u5e2e\u52a9\u4f01\u4e1a\u6784\u5efa\u771f\u6b63\u6709\u6548\u7684\u751f\u4ea7\u7ea7\u667a\u80fd\u4f53\u3002", "topic": "code agent"}}
{"id": "tldr.2512.1ebf0e24", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhex.tech%2Fblog%2Fdashboards-were-never-the-destination%2F%3Futm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/0BZTkxpzULBFNqZLRLWTY5uOIG7y3yTsw38ZLWY_3Bs=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhex.tech%2Fblog%2Fdashboards-were-never-the-destination%2F%3Futm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/0BZTkxpzULBFNqZLRLWTY5uOIG7y3yTsw38ZLWY_3Bs=435", "authors": ["TLDR Newsletter"], "title": "Dashboards Were Never the Destination", "comment": "Source: TLDR Newsletter, Date: 2025-12-15, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fhex.tech%2Fblog%2Fdashboards-were-never-the-destination%2F%3Futm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/0BZTkxpzULBFNqZLRLWTY5uOIG7y3yTsw38ZLWY_3Bs=435", "summary": "Dashboards Were Never the Destination (8 minute read) Dashboards were a workaround for limited tools. They provide visibility but not real analysis. \u201cSelf-service\u201d plateaued because dashboards cannot reason or investigate. Agentic analytics shifts data teams from building static artifacts to curating context and logic, enabling systems that deliver real answers directly on the warehouse.", "source": "tldr", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3a\u4eea\u8868\u677f\u53ea\u662f\u6570\u636e\u53ef\u89c6\u5316\u7684\u6743\u5b9c\u4e4b\u8ba1\u800c\u975e\u7ec8\u6781\u76ee\u6807\uff0c\u63d0\u51fa\u901a\u8fc7\u4ee3\u7406\u5f0f\u5206\u6790\u7cfb\u7edf\u76f4\u63a5\u5728\u6570\u636e\u4ed3\u5e93\u4e2d\u63d0\u4f9b\u7b54\u6848\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u6570\u636e\u5206\u6790\u81ea\u52a8\u5316\u3002", "motivation": "\u4f20\u7edf\u4eea\u8868\u677f\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff1a\u5b83\u4eec\u53ea\u80fd\u63d0\u4f9b\u6570\u636e\u53ef\u89c1\u6027\u800c\u975e\u771f\u6b63\u7684\u5206\u6790\u80fd\u529b\uff0c\u65e0\u6cd5\u8fdb\u884c\u63a8\u7406\u548c\u8c03\u67e5\u3002\"\u81ea\u52a9\u670d\u52a1\"\u5206\u6790\u56e0\u6b64\u9677\u5165\u74f6\u9888\uff0c\u6570\u636e\u56e2\u961f\u9700\u8981\u4ece\u6784\u5efa\u9759\u6001\u62a5\u8868\u8f6c\u5411\u66f4\u667a\u80fd\u7684\u5206\u6790\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u5f0f\u5206\u6790\uff08Agentic Analytics\uff09\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u56e2\u961f\u7684\u89d2\u8272\u4ece\u6784\u5efa\u9759\u6001\u4eea\u8868\u677f\u8f6c\u53d8\u4e3a\u7b56\u5212\u4e0a\u4e0b\u6587\u548c\u903b\u8f91\uff0c\u521b\u5efa\u80fd\u591f\u5728\u6570\u636e\u4ed3\u5e93\u4e2d\u76f4\u63a5\u63d0\u4f9b\u7b54\u6848\u7684\u667a\u80fd\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4ee3\u7406\u5f0f\u5206\u6790\uff0c\u6570\u636e\u56e2\u961f\u53ef\u4ee5\u6784\u5efa\u80fd\u591f\u76f4\u63a5\u56de\u7b54\u4e1a\u52a1\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u63d0\u4f9b\u6570\u636e\u53ef\u89c6\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u771f\u6b63\u7684\u5206\u6790\u81ea\u52a8\u5316\u548c\u667a\u80fd\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u4eea\u8868\u677f\u53ea\u662f\u6570\u636e\u5206\u6790\u6f14\u8fdb\u8fc7\u7a0b\u4e2d\u7684\u4e34\u65f6\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7684\u65b9\u5411\u662f\u4ee3\u7406\u5f0f\u5206\u6790\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u5728\u6570\u636e\u4ed3\u5e93\u5c42\u9762\u76f4\u63a5\u63d0\u4f9b\u6d1e\u5bdf\u548c\u7b54\u6848\uff0c\u5f7b\u5e95\u6539\u53d8\u6570\u636e\u5206\u6790\u7684\u5de5\u4f5c\u65b9\u5f0f\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.47789f1f", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F1eTb8N/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/uLmMXV8jaxDzMKWM013AZkhtxUL9D0LyoKID_srsS5o=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F1eTb8N/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/uLmMXV8jaxDzMKWM013AZkhtxUL9D0LyoKID_srsS5o=435", "authors": ["TLDR Newsletter"], "title": "What it Means to Get Your Data Ready for AI", "comment": "Source: TLDR Newsletter, Date: 2025-12-15, Reading time: 7 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F1eTb8N/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/uLmMXV8jaxDzMKWM013AZkhtxUL9D0LyoKID_srsS5o=435", "summary": "What it Means to Get Your Data Ready for AI (7 minute read) Agentic AI shifts data engineering from feeding rigid dashboards to enabling agents that turn user intent into actions. Five recommendations: favor business context over deep normalization; curate a small, high-quality \u201cexample store\u201d for in-context learning; expose tool-ready APIs and semantically friendly formats; manage agent outputs as versioned first-class data; and connect observability to automated retraining. Net: build a fle...", "source": "tldr", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e3aAI\u51c6\u5907\u6570\u636e\u7684\u65b0\u8303\u5f0f\uff1a\u4ece\u4f20\u7edf\u6570\u636e\u5de5\u7a0b\u8f6c\u5411\u652f\u6301\u667a\u80fd\u4ee3\u7406\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\uff0c\u5f3a\u8c03\u4e1a\u52a1\u4e0a\u4e0b\u6587\u3001\u9ad8\u8d28\u91cf\u793a\u4f8b\u5e93\u3001\u5de5\u5177\u53cb\u597dAPI\u3001\u7248\u672c\u5316\u4ee3\u7406\u8f93\u51fa\u548c\u53ef\u89c2\u6d4b\u6027\u8fde\u63a5\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5de5\u7a0b\u4e3b\u8981\u670d\u52a1\u4e8e\u56fa\u5b9a\u4eea\u8868\u677f\uff0c\u800c\u667a\u80fd\u4ee3\u7406AI\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u6765\u5c06\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u884c\u52a8\uff0c\u8fd9\u8981\u6c42\u91cd\u65b0\u601d\u8003\u6570\u636e\u51c6\u5907\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e94\u9879\u5177\u4f53\u5efa\u8bae\uff1a1) \u4f18\u5148\u8003\u8651\u4e1a\u52a1\u4e0a\u4e0b\u6587\u800c\u975e\u6df1\u5ea6\u89c4\u8303\u5316\uff1b2) \u6784\u5efa\u9ad8\u8d28\u91cf\"\u793a\u4f8b\u5e93\"\u7528\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff1b3) \u63d0\u4f9b\u5de5\u5177\u53cb\u597d\u7684API\u548c\u8bed\u4e49\u5316\u683c\u5f0f\uff1b4) \u5c06\u4ee3\u7406\u8f93\u51fa\u4f5c\u4e3a\u7248\u672c\u5316\u7684\u4e00\u7ea7\u6570\u636e\u7ba1\u7406\uff1b5) \u8fde\u63a5\u53ef\u89c2\u6d4b\u6027\u4e0e\u81ea\u52a8\u5316\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u6784\u5efa\u7075\u6d3b\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u667a\u80fd\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u4f7f\u6570\u636e\u5de5\u7a0b\u4ece\u9759\u6001\u62a5\u8868\u8f6c\u5411\u52a8\u6001\u884c\u52a8\u6267\u884c\uff0c\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u4e3aAI\u51c6\u5907\u6570\u636e\u9700\u8981\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u4f20\u7edf\u6570\u636e\u7ba1\u9053\u8f6c\u5411\u652f\u6301\u667a\u80fd\u4ee3\u7406\u7684\u6570\u636e\u57fa\u7840\u8bbe\u65bd\uff0c\u8fd9\u4e94\u9879\u5efa\u8bae\u4e3a\u6784\u5efa\u6b64\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "topic": "agent analysis"}}
{"id": "tldr.2512.ae5b7972", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RnQKluxWB5Y%26t=7s%26utm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/ZQj7VsMjK3OhJs09YDgDQEmooSVfEDvb5lPDH1EajHQ=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RnQKluxWB5Y%26t=7s%26utm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/ZQj7VsMjK3OhJs09YDgDQEmooSVfEDvb5lPDH1EajHQ=435", "authors": ["TLDR Newsletter"], "title": "DBOS: 2025 Year in Review with Mike Stonebraker and Andy Pavlo", "comment": "Source: TLDR Newsletter, Date: 2025-12-15, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.youtube.com%2Fwatch%3Fv=RnQKluxWB5Y%26t=7s%26utm_source=tldrdata/1/0100019b21b19ed5-148e1899-32bf-443f-a7a1-28cc7d23af3d-000000/ZQj7VsMjK3OhJs09YDgDQEmooSVfEDvb5lPDH1EajHQ=435", "summary": "DBOS: 2025 Year in Review with Mike Stonebraker and Andy Pavlo (1 hour video) LLMs perform well on public SQL benchmarks but fail on real enterprise warehouses, with accuracy near zero in private tests and only ~30% even with RAG or decomposition. As AI-generated code increasingly touches databases, the stack is shifting toward agentic, ACID-durable systems with holistic autotuning, while Postgres consolidation and native vector support are eroding the standalone vector-database moat.", "source": "tldr", "AI": {"tldr": "LLM\u5728\u516c\u5f00SQL\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u4ed3\u5e93\u4e2d\u51c6\u786e\u7387\u63a5\u8fd1\u96f6\uff0c\u5373\u4f7f\u4f7f\u7528RAG\u6216\u5206\u89e3\u6280\u672f\u4e5f\u53ea\u6709\u7ea630%\u3002\u968f\u7740AI\u751f\u6210\u4ee3\u7801\u8d8a\u6765\u8d8a\u591a\u5730\u63a5\u89e6\u6570\u636e\u5e93\uff0c\u6280\u672f\u6808\u6b63\u5728\u5411\u5177\u6709\u6574\u4f53\u81ea\u52a8\u8c03\u4f18\u7684\u4ee3\u7406\u5f0fACID\u6301\u4e45\u7cfb\u7edf\u8f6c\u53d8\uff0c\u540c\u65f6Postgres\u6574\u5408\u548c\u539f\u751f\u5411\u91cf\u652f\u6301\u6b63\u5728\u4fb5\u8680\u72ec\u7acb\u5411\u91cf\u6570\u636e\u5e93\u7684\u62a4\u57ce\u6cb3\u3002", "motivation": "\u5f53\u524dLLM\u5728SQL\u67e5\u8be2\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u867d\u7136\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4f01\u4e1a\u73af\u5883\u4e2d\u51c6\u786e\u7387\u6781\u4f4e\uff0c\u8fd9\u66b4\u9732\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u968f\u7740AI\u751f\u6210\u4ee3\u7801\u8d8a\u6765\u8d8a\u591a\u5730\u63a5\u89e6\u6570\u636e\u5e93\uff0c\u9700\u8981\u66f4\u53ef\u9760\u3001\u66f4\u5065\u58ee\u7684\u7cfb\u7edf\u6765\u5904\u7406\u6570\u636e\u5e93\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u5206\u6790LLM\u5728\u516c\u5f00SQL\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u4ed3\u5e93\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u8bc6\u522b\u73b0\u6709\u65b9\u6cd5\uff08\u5982RAG\u548c\u5206\u89e3\u6280\u672f\uff09\u7684\u5c40\u9650\u6027\u3002\u63d0\u51fa\u6280\u672f\u6808\u5411\u4ee3\u7406\u5f0f\u3001ACID\u6301\u4e45\u7cfb\u7edf\u8f6c\u53d8\u7684\u8d8b\u52bf\uff0c\u5f3a\u8c03\u6574\u4f53\u81ea\u52a8\u8c03\u4f18\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba8\u8bbaPostgres\u6574\u5408\u548c\u539f\u751f\u5411\u91cf\u652f\u6301\u5bf9\u5411\u91cf\u6570\u636e\u5e93\u5e02\u573a\u7684\u5f71\u54cd\u3002", "result": "LLM\u5728\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u4ed3\u5e93\u4e2d\u7684SQL\u67e5\u8be2\u751f\u6210\u51c6\u786e\u7387\u63a5\u8fd1\u96f6\uff0c\u5373\u4f7f\u4f7f\u7528RAG\u6216\u5206\u89e3\u6280\u672f\u4e5f\u53ea\u6709\u7ea630%\u7684\u51c6\u786e\u7387\u3002\u8fd9\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u4f01\u4e1a\u7ea7\u9700\u6c42\u3002\u540c\u65f6\u89c2\u5bdf\u5230\u6280\u672f\u6808\u6b63\u5728\u5411\u4ee3\u7406\u5f0fACID\u6301\u4e45\u7cfb\u7edf\u8f6c\u53d8\uff0cPostgres\u7684\u6574\u5408\u548c\u539f\u751f\u5411\u91cf\u652f\u6301\u6b63\u5728\u6539\u53d8\u5411\u91cf\u6570\u636e\u5e93\u7684\u5e02\u573a\u683c\u5c40\u3002", "conclusion": "LLM\u5728\u771f\u5b9e\u4f01\u4e1aSQL\u67e5\u8be2\u751f\u6210\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u5065\u58ee\u7684\u7cfb\u7edf\u3002\u672a\u6765\u7684\u65b9\u5411\u662f\u5f00\u53d1\u4ee3\u7406\u5f0f\u3001ACID\u6301\u4e45\u7684\u6570\u636e\u5e93\u7cfb\u7edf\uff0c\u5177\u6709\u6574\u4f53\u81ea\u52a8\u8c03\u4f18\u80fd\u529b\u3002\u540c\u65f6\uff0cPostgres\u7b49\u4f20\u7edf\u6570\u636e\u5e93\u901a\u8fc7\u6574\u5408\u5411\u91cf\u529f\u80fd\u6b63\u5728\u4fb5\u8680\u72ec\u7acb\u5411\u91cf\u6570\u636e\u5e93\u7684\u5e02\u573a\u4f18\u52bf\u3002", "topic": "code agent"}}
{"id": "tldr.2512.9125da77", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fumami-production-286d.up.railway.app%2Fq%2Fu3IHQpbPT/2/0100019b21c0d7db-43523543-86cd-455f-937f-2dc5f88c4399-000000/YvpXLiP4y1SPz2xX8R4SOctAISgtCx01xdHrfWVeG7I=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fumami-production-286d.up.railway.app%2Fq%2Fu3IHQpbPT/2/0100019b21c0d7db-43523543-86cd-455f-937f-2dc5f88c4399-000000/YvpXLiP4y1SPz2xX8R4SOctAISgtCx01xdHrfWVeG7I=435", "authors": ["TLDR Newsletter"], "title": "AI agents are microservices", "comment": "Source: TLDR Newsletter, Date: 2025-12-15, Reading time: Sponsor, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fumami-production-286d.up.railway.app%2Fq%2Fu3IHQpbPT/2/0100019b21c0d7db-43523543-86cd-455f-937f-2dc5f88c4399-000000/YvpXLiP4y1SPz2xX8R4SOctAISgtCx01xdHrfWVeG7I=435", "summary": "AI agents are microservices (Sponsor) When AI becomes part of your software stack, you need more than agentic DAG. You need an AI backend that treats agents like microservices \u2014 distributed, observable, and governable. AgentField is open-source infrastructure for production AI: REST APIs, async execution, built-in memory, and cryptographic agent identity with automatic audit trails. Think Kubernetes for agents: a control plane where you build AI like you build software. \u2b50 Star on GitHub", "source": "tldr", "AI": {"tldr": "AgentField\u662f\u4e00\u4e2a\u5f00\u6e90AI\u57fa\u7840\u8bbe\u65bd\uff0c\u5c06AI\u4ee3\u7406\u89c6\u4e3a\u5fae\u670d\u52a1\uff0c\u63d0\u4f9bREST API\u3001\u5f02\u6b65\u6267\u884c\u3001\u5185\u7f6e\u5185\u5b58\u548c\u52a0\u5bc6\u4ee3\u7406\u8eab\u4efd\u7b49\u751f\u4ea7\u7ea7\u529f\u80fd\uff0c\u7c7b\u4f3c\u4e8eKubernetes\u7684\u4ee3\u7406\u63a7\u5236\u5e73\u9762\u3002", "motivation": "\u5f53AI\u6210\u4e3a\u8f6f\u4ef6\u6808\u7684\u4e00\u90e8\u5206\u65f6\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u7684\u4ee3\u7406DAG\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5c06\u4ee3\u7406\u4f5c\u4e3a\u5fae\u670d\u52a1\u8fdb\u884c\u5206\u5e03\u5f0f\u3001\u53ef\u89c2\u6d4b\u548c\u53ef\u6cbb\u7406\u7684\u751f\u4ea7\u7ea7AI\u540e\u7aef\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5f00\u53d1AgentField\u5f00\u6e90\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u4f9bREST API\u63a5\u53e3\u3001\u5f02\u6b65\u6267\u884c\u673a\u5236\u3001\u5185\u7f6e\u5185\u5b58\u7cfb\u7edf\u3001\u52a0\u5bc6\u4ee3\u7406\u8eab\u4efd\u9a8c\u8bc1\u548c\u81ea\u52a8\u5ba1\u8ba1\u8ffd\u8e2a\u529f\u80fd\uff0c\u6784\u5efa\u7c7b\u4f3cKubernetes\u7684\u4ee3\u7406\u63a7\u5236\u5e73\u9762\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u751f\u4ea7\u5c31\u7eea\u7684AI\u4ee3\u7406\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u50cf\u6784\u5efa\u8f6f\u4ef6\u4e00\u6837\u6784\u5efaAI\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ee3\u7406\u7684\u5fae\u670d\u52a1\u5316\u7ba1\u7406\u548c\u90e8\u7f72\u3002", "conclusion": "AgentField\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u5fae\u670d\u52a1\u5316\u57fa\u7840\u8bbe\u65bd\uff0c\u89e3\u51b3\u4e86AI\u96c6\u6210\u5230\u8f6f\u4ef6\u6808\u65f6\u7684\u5206\u5e03\u5f0f\u3001\u53ef\u89c2\u6d4b\u548c\u6cbb\u7406\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "tldr.2512.928f08db", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fleehanchung.github.io%2Fblogs%2F2025%2F10%2F26%2Fclaude-skills-deep-dive%3Futm_source=tldrdev/1/0100019b21eb958d-2b38da21-1af5-4b6d-b2b7-cec487b65017-000000/zqA8XKEQMzi25YwUIdNYxm1G8SwLcfgVDd_APQ6BeWA=435", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fleehanchung.github.io%2Fblogs%2F2025%2F10%2F26%2Fclaude-skills-deep-dive%3Futm_source=tldrdev/1/0100019b21eb958d-2b38da21-1af5-4b6d-b2b7-cec487b65017-000000/zqA8XKEQMzi25YwUIdNYxm1G8SwLcfgVDd_APQ6BeWA=435", "authors": ["TLDR Newsletter"], "title": "Claude Agent Skills: A First Principles Deep Dive", "comment": "Source: TLDR Newsletter, Date: 2025-12-15, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fleehanchung.github.io%2Fblogs%2F2025%2F10%2F26%2Fclaude-skills-deep-dive%3Futm_source=tldrdev/1/0100019b21eb958d-2b38da21-1af5-4b6d-b2b7-cec487b65017-000000/zqA8XKEQMzi25YwUIdNYxm1G8SwLcfgVDd_APQ6BeWA=435", "summary": "Claude Agent Skills: A First Principles Deep Dive (8 minute read) Claude's Agent Skills system is a prompt-based architecture that extends LLM capabilities through specialized instruction injection rather than traditional function calling or code execution. Skills are prompt templates that modify conversation context and execution permissions. Claude makes skill selection decisions through pure language understanding rather than algorithmic routing.", "source": "tldr", "AI": {"tldr": "Claude Agent Skills\u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6307\u4ee4\u6ce8\u5165\u800c\u975e\u4f20\u7edf\u51fd\u6570\u8c03\u7528\u6216\u4ee3\u7801\u6267\u884c\u6765\u6269\u5c55LLM\u80fd\u529b\uff0c\u4f7f\u7528\u7eaf\u8bed\u8a00\u7406\u89e3\u8fdb\u884c\u6280\u80fd\u9009\u62e9", "motivation": "\u4f20\u7edfLLM\u6269\u5c55\u65b9\u6cd5\uff08\u5982\u51fd\u6570\u8c03\u7528\u3001\u4ee3\u7801\u6267\u884c\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u57fa\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u6280\u80fd\u7cfb\u7edf\u6765\u589e\u5f3aClaude\u7684\u80fd\u529b", "method": "\u4f7f\u7528\u63d0\u793a\u6a21\u677f\u67b6\u6784\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6307\u4ee4\u6ce8\u5165\u4fee\u6539\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u548c\u6267\u884c\u6743\u9650\uff0c\u57fa\u4e8e\u7eaf\u8bed\u8a00\u7406\u89e3\u8fdb\u884c\u6280\u80fd\u9009\u62e9\u51b3\u7b56", "result": "\u5f00\u53d1\u4e86Claude Agent Skills\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0d\u540c\u4e8e\u4f20\u7edf\u51fd\u6570\u8c03\u7528\u6216\u4ee3\u7801\u6267\u884c\u7684\u65b0\u9896\u67b6\u6784", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u6307\u4ee4\u6ce8\u5165\u548c\u7eaf\u8bed\u8a00\u7406\u89e3\u7684\u6280\u80fd\u9009\u62e9\u4e3aLLM\u80fd\u529b\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84", "topic": "agent analysis"}}
{"id": "wechat.2512.cf2776cf", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDg0ODI5OQ==&mid=2247483867&idx=1&sn=35742a50c91780bc73ad367eefb4ad87&chksm=c52e749156cccf4975df477231ff88406848248a3b5a3c0ae88994c9c13c58f4ab0d9af39d4d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDg0ODI5OQ==&mid=2247483867&idx=1&sn=35742a50c91780bc73ad367eefb4ad87&chksm=c52e749156cccf4975df477231ff88406848248a3b5a3c0ae88994c9c13c58f4ab0d9af39d4d#rd", "authors": ["StayHungry Lab"], "title": "\u3010\u8bba\u6587\u5206\u4eab\u3011<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u662f\u5426\u771f\u7684\u6fc0\u53d1\u4e86\u57fa\u7840\u6a21\u578b\u80fd\u529b\u4e4b\u5916\u7684\u63a8\u7406\u80fd\u529b", "comment": "Source: WeChat, Published: 2025-12-16 11:52:22", "summary": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u5728\u4e00\u4e2astate\u4e0b\uff0c\u5176\u52a8\u4f5c\u7a7a\u95f4\u5f80\u5f80\u6bd4\u8f83\u5c0f\u3002\u6bd4\u5982\u5728\u8d85\u7ea7\u9a6c\u91cc\u5965\u7684\u6e38\u620f\u4e2d\uff0c\u5728\u4e00\u4e2a\u72b6\u6001\u4e0b\u80fd\u591f\u6267\u884c\u7684\u52a8\u4f5c\u5c31\u662f\u4e0a\u4e0b\u5de6\u53f3\u79fb\u52a8\u3002\u52a8\u4f5c\u7a7a\u95f4\u5c0f\u5c31\u53ef\u4ee5\u3010\u679a\u4e3e+\u63a2\u7d22\u3011\uff08\u4e5f\u53ef\u80fd\u6709\u4e00\u4e9b\u6839\u636e\u7ecf\u9a8c/\u89c4\u5219\u7684\u526a\u679d\uff09\u3002", "AI": {"tldr": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u5728\u4e00\u4e2astate\u4e0b\uff0c\u5176\u52a8\u4f5c\u7a7a\u95f4\u5f80\u5f80\u6bd4\u8f83\u5c0f\u3002\u6bd4\u5982\u5728\u8d85\u7ea7\u9a6c\u91cc\u5965\u7684\u6e38\u620f\u4e2d\uff0c\u5728\u4e00\u4e2a\u72b6\u6001\u4e0b\u80fd\u591f\u6267\u884c\u7684\u52a8\u4f5c\u5c31\u662f\u4e0a\u4e0b\u5de6\u53f3\u79fb\u52a8\u3002\u52a8\u4f5c\u7a7a\u95f4\u5c0f\u5c31\u53ef\u4ee5\u3010\u679a\u4e3e+\u63a2\u7d22\u3011\uff08\u4e5f\u53ef\u80fd\u6709\u4e00\u4e9b\u6839\u636e\u7ecf\u9a8c/\u89c4\u5219\u7684\u526a\u679d\uff09\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.f7772644", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMzUyMTY4OA==&mid=2247486315&idx=1&sn=9e03c7675757aaa122ddee048817e801&chksm=c029f665006fa8855b63460c55e52d8e13b591498c9fe0d8f5759a46b445cf6c2dd32c23f96a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMzUyMTY4OA==&mid=2247486315&idx=1&sn=9e03c7675757aaa122ddee048817e801&chksm=c029f665006fa8855b63460c55e52d8e13b591498c9fe0d8f5759a46b445cf6c2dd32c23f96a#rd", "authors": ["\u6570\u5b57\u80fd\u6e90\u4e0e\u4f4e\u78b3\u53d1\u5c55"], "title": "Nature \u671f\u520a\u8bba\u6587 || \u673a\u5668\u81ea\u52a8\u53d1\u73b0\u5e76\u8d85\u8d8a\u624b\u5de5\u8bbe\u8ba1\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7b97\u6cd5", "comment": "Source: WeChat, Published: 2025-12-16 11:01:43", "summary": "\u5c3d\u7ba1\u673a\u5668\u81ea\u4e3b\u53d1\u73b0\u5f3a\u5927\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u76ee\u6807\u6570\u5341\u5e74\u6765\u4e00\u76f4\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u59cb\u7ec8\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u8bc1\u660e\u4e86\u673a\u5668\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u7684RL\u89c4\u5219\uff0c\u8fd9\u662f\u901a\u8fc7\u5143\u5b66\u4e60\u4ece\u5927\u91cf\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7d2f\u79ef\u7ecf\u9a8c\u800c\u5b9e\u73b0\u7684\u3002", "AI": {"tldr": "\u5c3d\u7ba1\u673a\u5668\u81ea\u4e3b\u53d1\u73b0\u5f3a\u5927\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u76ee\u6807\u6570\u5341\u5e74\u6765\u4e00\u76f4\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u59cb\u7ec8\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u8bc1\u660e\u4e86\u673a\u5668\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u7684\u6700\u5148\u8fdb\u7684RL\u89c4\u5219\uff0c\u8fd9\u662f\u901a\u8fc7\u5143\u5b66\u4e60\u4ece\u5927\u91cf\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7d2f\u79ef\u7ecf\u9a8c\u800c\u5b9e\u73b0\u7684\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.44f88d49", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxOTUwNjcyNQ==&mid=2247484815&idx=1&sn=88f58f437ae4dde6df67bd5e999bf412&chksm=c0fbf26ae383967506ad7928bd85ad53f78a30c1842022e679b84e9975221420ff782ba6540d#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxOTUwNjcyNQ==&mid=2247484815&idx=1&sn=88f58f437ae4dde6df67bd5e999bf412&chksm=c0fbf26ae383967506ad7928bd85ad53f78a30c1842022e679b84e9975221420ff782ba6540d#rd", "authors": ["AI\u95f2\u8c08\u5c40"], "title": "<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u7b14\u8bb0\uff1a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b", "comment": "Source: WeChat, Published: 2025-12-16 11:00:00", "summary": "\u57fa\u4e8e\u4e0a\u8ff0\u57fa\u7840\u6982\u5ff5\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u6982\u5ff5\u5927\u81f4\u5982\u4e0b\u3002\u5176\u4e2d\u6838\u5fc3\u5de5\u5177 \u8d1d\u5c14\u66fc\u65b9\u7a0b\u5f62\u5f0f\u5982\u4e0b\u3002\u53c2\u80031. \u8d75\u4e16\u94b0\u300a\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b66\u539f\u7406\u300b\u6e05\u534e\u5927\u5b66\u51fa\u7248\u793e2. \u738b\u7426\u3001\u6768\u6bc5\u8fdc\u3001\u6c5f\u5b63\u7b49\u300aEasyRL\uff1a\u5f3a\u5316\u5b66\u4e60\u6559\u7a0b\u300b\u4eba\u6c11\u90ae\u7535\u51fa\u7248\u793e", "AI": {"tldr": "\u57fa\u4e8e\u4e0a\u8ff0\u57fa\u7840\u6982\u5ff5\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u6838\u5fc3\u6982\u5ff5\u5927\u81f4\u5982\u4e0b\u3002\u5176\u4e2d\u6838\u5fc3\u5de5\u5177 \u8d1d\u5c14\u66fc\u65b9\u7a0b\u5f62\u5f0f\u5982\u4e0b\u3002\u53c2\u80031. \u8d75\u4e16\u94b0\u300a\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u5b66\u539f\u7406\u300b\u6e05\u534e\u5927\u5b66\u51fa\u7248\u793e2. \u738b\u7426\u3001\u6768\u6bc5\u8fdc\u3001\u6c5f\u5b63\u7b49\u300aEasyRL\uff1a\u5f3a\u5316\u5b66\u4e60\u6559\u7a0b\u300b\u4eba\u6c11\u90ae\u7535\u51fa\u7248\u793e", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.71fa825b", "categories": ["wechat.article", "wechat.rl", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzUzNjk1NTU5Mw==&mid=2247484852&idx=1&sn=6fa7cdfe8451b5c40b10b0d37a4f7f33&chksm=fb01d12477e8b04653c7157d7c1bff501858d11a637506a4056d70fad8fa8f807d42586a575b#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzUzNjk1NTU5Mw==&mid=2247484852&idx=1&sn=6fa7cdfe8451b5c40b10b0d37a4f7f33&chksm=fb01d12477e8b04653c7157d7c1bff501858d11a637506a4056d70fad8fa8f807d42586a575b#rd", "authors": ["AI\u524d\u6cbf\u6587\u732e\u901f\u9012"], "title": "Nature \u6700\u65b0\u7814\u7a76\uff1a\u673a\u5668\u5982\u4f55\u4ece\u5927\u91cf\u73af\u5883\u7ecf\u9a8c\u4e2d\u81ea\u52a8\u53d1\u73b0\u6700\u5148\u8fdb\u7684<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u89c4\u5219", "comment": "Source: WeChat, Published: 2025-12-16 02:30:34", "summary": "\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u9884\u6d4b\u4e0e\u51b3\u7b56\u7684\u6838\u5fc3\u65b9\u6cd5\u4e4b\u4e00\u3002\u73b0\u6709\u7684\u6210\u529f\u7cfb\u7edf\uff0c\u5982\u56f4\u68cb\u3001\u56fd\u9645\u8c61\u68cb\u3001\u661f\u9645\u4e89\u9738\u4ee5\u53ca\u590d\u6742\u63a7\u5236\u7cfb\u7edf\uff0c\u5927\u591a\u57fa\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u662f\u5b9e\u73b0\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u9884\u6d4b\u4e0e\u51b3\u7b56\u7684\u6838\u5fc3\u65b9\u6cd5\u4e4b\u4e00\u3002\u73b0\u6709\u7684\u6210\u529f\u7cfb\u7edf\uff0c\u5982\u56f4\u68cb\u3001\u56fd\u9645\u8c61\u68cb\u3001\u661f\u9645\u4e89\u9738\u4ee5\u53ca\u590d\u6742\u63a7\u5236\u7cfb\u7edf\uff0c\u5927\u591a\u57fa\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.908cdaf6", "categories": ["wechat.article", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk2NDA4NjU0Nw==&mid=2247491929&idx=1&sn=7e8ada70676381c4a36f5c3ae215aef0&chksm=c53be21f04c5a574b776052635b4d71b2d2d6731d0995e9b3071a6abdcfa98e531f49d594589#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk2NDA4NjU0Nw==&mid=2247491929&idx=1&sn=7e8ada70676381c4a36f5c3ae215aef0&chksm=c53be21f04c5a574b776052635b4d71b2d2d6731d0995e9b3071a6abdcfa98e531f49d594589#rd", "authors": ["AI\u4e13\u9898\u7cbe\u8bb2"], "title": "ICPO\uff1a\u57fa\u4e8e\u5185\u5728\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u7ec4\u76f8\u5bf9\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548<em class=\"highlight\">\u5f3a\u5316\u5b66\u4e60</em>\u2014\u2014\u5c0f\u7c73\u516c\u53f8 MiLMPlus \u56e2\u961f", "comment": "Source: WeChat, Published: 2025-12-15 23:00:00", "summary": "\u8fd9\u5c31\u662f\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09 \u7684\u57fa\u672c\u601d\u60f3\u3002\u4f46\u8fd9\u91cc\u6709\u4e2a\u96be\u9898\uff1a\u5982\u679c\u673a\u5668\u4eba\u53ea\u8ffd\u6c42\u4f60\u7ed9\u7684\u5956\u52b1\uff0c\u5b83\u53ef\u80fd\u4f1a\u53d8\u5f97\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u53ea\u91cd\u590d\u90a3\u4e9b\u5b83\u5df2\u7ecf\u77e5\u9053\u80fd\u83b7\u5f97\u5956\u52b1\u7684\u52a8\u4f5c\uff0c\u800c\u4e0d\u6562\u5c1d\u8bd5\u65b0\u7684\u3001\u53ef\u80fd\u66f4\u597d\u7684\u8d70\u8def\u65b9\u5f0f\u3002", "AI": {"tldr": "\u8fd9\u5c31\u662f\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09 \u7684\u57fa\u672c\u601d\u60f3\u3002\u4f46\u8fd9\u91cc\u6709\u4e2a\u96be\u9898\uff1a\u5982\u679c\u673a\u5668\u4eba\u53ea\u8ffd\u6c42\u4f60\u7ed9\u7684\u5956\u52b1\uff0c\u5b83\u53ef\u80fd\u4f1a\u53d8\u5f97\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u53ea\u91cd\u590d\u90a3\u4e9b\u5b83\u5df2\u7ecf\u77e5\u9053\u80fd\u83b7\u5f97\u5956\u52b1\u7684\u52a8\u4f5c\uff0c\u800c\u4e0d\u6562\u5c1d\u8bd5\u65b0\u7684\u3001\u53ef\u80fd\u66f4\u597d\u7684\u8d70\u8def\u65b9\u5f0f\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.a86f2390", "categories": ["wechat.article", "wechat.cl"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkxNzU2NDgxNQ==&mid=2247484212&idx=1&sn=e9a09643fd814fc155d42672a596ab94&chksm=c028c1da58d84436c1cb78311cd370fd3dfcf19eb67e9e2ea82f65c4407d0879b2c46ec67946#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkxNzU2NDgxNQ==&mid=2247484212&idx=1&sn=e9a09643fd814fc155d42672a596ab94&chksm=c028c1da58d84436c1cb78311cd370fd3dfcf19eb67e9e2ea82f65c4407d0879b2c46ec67946#rd", "authors": ["Oxo Security"], "title": "\u3010AI\u5b89\u5168\u3011\u82f1\u4f1f\u8fbe\u6784\u5efa <em class=\"highlight\">Agentic</em> AI \u6c99\u76d2\u7684\u7ec8\u6781\u6307\u5357", "comment": "Source: WeChat, Published: 2025-12-16 13:07:00", "summary": "Agentic AI\u4e0d\u518d\u4ec5\u4ec5\u662f\u751f\u6210\u6587\u672c\uff0c\u5b83\u4eec\u80fd\u591f\u89c4\u5212\u4efb\u52a1\u3001\u8c03\u7528\u5de5\u5177\u3001\u7f16\u5199\u5e76\u6267\u884c\u4ee3\u7801\u6765\u89e3\u51b3\u95ee\u9898\u30021.1 \u4ec0\u4e48\u662f Agentic AI\uff1f\u4e3a\u4f55\u5b83\u9700\u8981\u52a8\u5200\uff1f\u60f3\u8c61\u4e00\u4e0b\uff0c\u4f60\u4e0d\u518d\u53ea\u662f\u95ee ChatGPT \u201c\u5982\u4f55\u5904\u7406 Excel \u8868\u683c\u201d\uff0c\u800c\u662f\u76f4\u63a5\u4e22\u7ed9\u5b83\u4e00\u4e2a\u6587\u4ef6\uff0c\u8bf4\uff1a\u201c\u5e2e\u6211", "AI": {"tldr": "Agentic AI\u4e0d\u518d\u4ec5\u4ec5\u662f\u751f\u6210\u6587\u672c\uff0c\u5b83\u4eec\u80fd\u591f\u89c4\u5212\u4efb\u52a1\u3001\u8c03\u7528\u5de5\u5177\u3001\u7f16\u5199\u5e76\u6267\u884c\u4ee3\u7801\u6765\u89e3\u51b3\u95ee\u9898\u30021.1 \u4ec0\u4e48\u662f Agentic AI\uff1f\u4e3a\u4f55\u5b83\u9700\u8981\u52a8\u5200\uff1f\u60f3\u8c61\u4e00\u4e0b\uff0c\u4f60\u4e0d\u518d\u53ea\u662f\u95ee ChatGPT \u201c\u5982\u4f55\u5904\u7406 Excel \u8868\u683c\u201d\uff0c\u800c\u662f\u76f4\u63a5\u4e22\u7ed9\u5b83\u4e00\u4e2a\u6587\u4ef6\uff0c\u8bf4\uff1a\u201c\u5e2e\u6211", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "code agent"}}
{"id": "wechat.2512.c32052be", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyNDU0MzI0Mw==&mid=2247502286&idx=4&sn=8415990010d7582fb4440d8eafe36b2d&chksm=c0ae0b09b302c58361dc2fc7690dadf186ddcd3010f820cd374459231360242a1d60d8939337#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyNDU0MzI0Mw==&mid=2247502286&idx=4&sn=8415990010d7582fb4440d8eafe36b2d&chksm=c0ae0b09b302c58361dc2fc7690dadf186ddcd3010f820cd374459231360242a1d60d8939337#rd", "authors": ["\u805a\u5927\u6a21\u578b\u524d\u8a00"], "title": "NVIDIA\u53d1\u5e03Nemotron 3\uff1a\u4eceNano\u5230Ultra\uff0c\u5168\u9762\u62bc\u6ce8<em class=\"highlight\">Agentic</em> AI", "comment": "Source: WeChat, Published: 2025-12-16 13:06:29", "summary": "\u5b9a\u4f4d\u76f4\u6307\u5f53\u524d\u5feb\u901f\u589e\u957f\u7684 Agentic AI\uff08\u667a\u80fd\u4f53 AI\uff09\u7cfb\u7edf\u9700\u6c42\u3002\u8be5\u7cfb\u5217\u5168\u9762\u91c7\u7528 \u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08Hybrid MoE\uff09\u67b6\u6784\uff0c\u5f3a\u8c03\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0 \u66f4\u9ad8\u6548\u7387\u4e0e\u66f4\u9ad8\u51c6\u786e\u6027\u3002", "AI": {"tldr": "\u5b9a\u4f4d\u76f4\u6307\u5f53\u524d\u5feb\u901f\u589e\u957f\u7684 Agentic AI\uff08\u667a\u80fd\u4f53 AI\uff09\u7cfb\u7edf\u9700\u6c42\u3002\u8be5\u7cfb\u5217\u5168\u9762\u91c7\u7528 \u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08Hybrid MoE\uff09\u67b6\u6784\uff0c\u5f3a\u8c03\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0 \u66f4\u9ad8\u6548\u7387\u4e0e\u66f4\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.98263b6b", "categories": ["wechat.article", "wechat.agent"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzU5ODc3OTM1NQ==&mid=2247485869&idx=1&sn=08ee49315e516fc8deb6c59c7c1b6922&chksm=ff17b9e1032fca5725e5789f857990988e196aeda693554606de6f32328d2ad79c2803b5ea52#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzU5ODc3OTM1NQ==&mid=2247485869&idx=1&sn=08ee49315e516fc8deb6c59c7c1b6922&chksm=ff17b9e1032fca5725e5789f857990988e196aeda693554606de6f32328d2ad79c2803b5ea52#rd", "authors": ["\u6570\u5b57\u5316\u524d\u77bb"], "title": "2026\u524d\u77bb\uff1a\u4ece\u5de5\u5177\u5230\u5458\u5de5\uff0c<em class=\"highlight\">Agentic</em> AI\u7684\u751f\u6001\u88c2\u53d8\u4e0e\u67b6\u6784\u7a81\u56f4", "comment": "Source: WeChat, Published: 2025-12-16 04:46:05", "summary": "Agentic AI\u5e76\u975e\u7b80\u5355\u7684\u5de5\u5177\u5347\u7ea7\uff0c\u800c\u662f\u4e00\u79cd\u8303\u5f0f\u7684\u8f6c\u53d8\u3002\u5982\u679c\u8bf4\u8fc7\u53bb\u7684AI\u4e3b\u8981\u626e\u6f14\u8f85\u52a9\u89d2\u8272\uff0c\u90a3\u4e48\u667a\u80fd\u4f53AI\u5219\u66f4\u50cf\u662f\u4e00\u4f4d\u80fd\u81ea\u4e3b\u601d\u8003\u3001\u89c4\u5212\u548c\u884c\u52a8\u7684\u201c\u6570\u5b57\u5458\u5de5\u201d\u3002", "AI": {"tldr": "Agentic AI\u5e76\u975e\u7b80\u5355\u7684\u5de5\u5177\u5347\u7ea7\uff0c\u800c\u662f\u4e00\u79cd\u8303\u5f0f\u7684\u8f6c\u53d8\u3002\u5982\u679c\u8bf4\u8fc7\u53bb\u7684AI\u4e3b\u8981\u626e\u6f14\u8f85\u52a9\u89d2\u8272\uff0c\u90a3\u4e48\u667a\u80fd\u4f53AI\u5219\u66f4\u50cf\u662f\u4e00\u4f4d\u80fd\u81ea\u4e3b\u601d\u8003\u3001\u89c4\u5212\u548c\u884c\u52a8\u7684\u201c\u6570\u5b57\u5458\u5de5\u201d\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.611654a7", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyOTQyMjkzOA==&mid=2247501264&idx=1&sn=ffe16e3a5316b8cc014883e312fa495b&chksm=c39e9919d87f6e7eb105616c85c315a3626410d461e80d212188b19febd11fb347d85fd90e53#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyOTQyMjkzOA==&mid=2247501264&idx=1&sn=ffe16e3a5316b8cc014883e312fa495b&chksm=c39e9919d87f6e7eb105616c85c315a3626410d461e80d212188b19febd11fb347d85fd90e53#rd", "authors": ["\u5927\u8bdd\u6570\u667a"], "title": "<em class=\"highlight\">Agentic</em> AI\u7684\u9876\u5c42\u8bbe\u8ba1\u4e0e\u843d\u5730\u6307\u5357", "comment": "Source: WeChat, Published: 2025-12-16 04:00:00", "summary": "Agentic AI\u4ece\u6280\u672f\u5230\u5e94\u7528\u7684\u5b8c\u6574\u8def\u5f84Agent\u5728\u5168\u7403\u79d1\u6280\u5de8\u5934\u7684\u6700\u4f73\u5b9e\u8df512\u670818\u65e5\uff0c\u6df1\u5ea6\u89e3\u8bfbAgent\u5927\u89c4\u6a21\u843d\u5730\u7684\uff1a\u6280\u672f\u5c31\u7eea\u5ea6\u3001\u5546\u4e1a\u5316\u8def\u5f84\u3001\u884c\u4e1a\u6848\u4f8b\u3001\u907f\u5751\u6307\u5357\u3002", "AI": {"tldr": "Agentic AI\u4ece\u6280\u672f\u5230\u5e94\u7528\u7684\u5b8c\u6574\u8def\u5f84Agent\u5728\u5168\u7403\u79d1\u6280\u5de8\u5934\u7684\u6700\u4f73\u5b9e\u8df512\u670818\u65e5\uff0c\u6df1\u5ea6\u89e3\u8bfbAgent\u5927\u89c4\u6a21\u843d\u5730\u7684\uff1a\u6280\u672f\u5c31\u7eea\u5ea6\u3001\u5546\u4e1a\u5316\u8def\u5f84\u3001\u884c\u4e1a\u6848\u4f8b\u3001\u907f\u5751\u6307\u5357\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.2e8ca356", "categories": ["wechat.article"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg3MDY0OTQ0NA==&mid=2247507658&idx=1&sn=42631db4f149c547a04f604bfde294b9&chksm=cfb4eeaa7e91bb710ae9f28589321046c746676f1e767e4d8efcf9f599553b6476888417b09a#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg3MDY0OTQ0NA==&mid=2247507658&idx=1&sn=42631db4f149c547a04f604bfde294b9&chksm=cfb4eeaa7e91bb710ae9f28589321046c746676f1e767e4d8efcf9f599553b6476888417b09a#rd", "authors": ["Andy730"], "title": "<em class=\"highlight\">Agentic</em> AI\u65f6\u4ee3\u7684\u7ec4\u7ec7\u53d8\u9769\uff1a\u8d85\u8d8a\u201c\u5de5\u5177\u201d\u7684\u6218\u7565\u6846\u67b6", "comment": "Source: WeChat, Published: 2025-12-15 23:30:54", "summary": "Agentic AI\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002\u7ba1\u7406\u8bef\u533a\uff1a\u8fc7\u5ea6\u8ffd\u6c42\u6807\u51c6\u5316\u4f1a\u627c\u6740AI\u7684\u5b66\u4e60\u80fd\u529b\uff1b\u8fc7\u5ea6\u4f9d\u8d56\u5176\u9002\u5e94\u6027\u5219\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u63a7\u7684\u53d8\u5f02\u3002\u7834\u5c40\u601d\u8def\uff1a\u8bbe\u8ba1\u6df7\u5408\u578b\u6d41\u7a0b\u3002\u6784\u5efa\u7c7b\u4f3cADP\u201c\u4ee3\u7406\u6784\u5efa\u5e73\u53f0\u201d\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u65e2\u80fd\u6279\u91cf\u5904\u7406\u6807\u51c6\u4efb\u52a1\uff0c\u53c8\u80fd\u8ba9AI\u50cf\u4eba", "AI": {"tldr": "Agentic AI\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002\u7ba1\u7406\u8bef\u533a\uff1a\u8fc7\u5ea6\u8ffd\u6c42\u6807\u51c6\u5316\u4f1a\u627c\u6740AI\u7684\u5b66\u4e60\u80fd\u529b\uff1b\u8fc7\u5ea6\u4f9d\u8d56\u5176\u9002\u5e94\u6027\u5219\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u63a7\u7684\u53d8\u5f02\u3002\u7834\u5c40\u601d\u8def\uff1a\u8bbe\u8ba1\u6df7\u5408\u578b\u6d41\u7a0b\u3002\u6784\u5efa\u7c7b\u4f3cADP\u201c\u4ee3\u7406\u6784\u5efa\u5e73\u53f0\u201d\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u65e2\u80fd\u6279\u91cf\u5904\u7406\u6807\u51c6\u4efb\u52a1\uff0c\u53c8\u80fd\u8ba9AI\u50cf\u4eba", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
{"id": "wechat.2512.677f958c", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA5ODI3MjAzMg==&mid=2247550102&idx=2&sn=8d1ede150b6e28399f3d60e91d92d88c&chksm=916c30f3035f3b0b0f580b92be9bd5dcba3df6feb6560c400b0bdb965fc5bacb04f80e7abe92#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA5ODI3MjAzMg==&mid=2247550102&idx=2&sn=8d1ede150b6e28399f3d60e91d92d88c&chksm=916c30f3035f3b0b0f580b92be9bd5dcba3df6feb6560c400b0bdb965fc5bacb04f80e7abe92#rd", "authors": ["\u8054\u901a\u54c8\u5c14\u6ee8\u8f6f\u4ef6\u7814\u7a76\u9662"], "title": "\u8f6f\u7814\u6781\u5ba2\u7cbe\u7cb9\u574a\u7cfb\u5217\u57f9\u8bad | \u79ef\u6781\u54cd\u5e94\u201c\u4eba\u5de5\u667a\u80fd+\u201d\u884c\u52a8\uff0c\u624e\u5b9e\u63a8\u8fdb<em class=\"highlight\">\u5927\u6a21\u578b</em>\u6280\u672f\u5e94\u7528\u57f9\u8bad\uff08\u4e00\uff09", "comment": "Source: WeChat, Published: 2025-12-16 13:31:56", "summary": "\u56f4\u7ed5\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6280\u672f\u67b6\u6784\uff0c\u4ece\u6570\u636e\u51c6\u5907\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u63d0\u793a\u8bcd\u5de5\u7a0b\u3001\u6a21\u578b\u8bc4\u4ef7\u7b49\u65b9\u9762\u91cd\u70b9\u8bb2\u89e3\u4e86\u5927\u6a21\u578b\u5efa\u8bbe\u65b9\u6848\u3002\u5176\u4e2d\uff0c\u6570\u636e\u51c6\u5907\u7684\u6570\u636e\u91c7\u96c6\u3001\u6e05\u6d17\u548c\u6807\u6ce8\u6709\u6548\u8fd0\u7528\u5927\u6570\u636e\u6280\u672f\u4e3a\u5927\u6a21\u578b\u6709\u6548\u8bad\u7ec3\u5960\u5b9a\u57fa\u7840\uff0c\u5145\u5206\u4f53\u73b0\u4e86\u6280\u672f\u53d8\u9769", "AI": {"tldr": "\u56f4\u7ed5\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u6280\u672f\u67b6\u6784\uff0c\u4ece\u6570\u636e\u51c6\u5907\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u63d0\u793a\u8bcd\u5de5\u7a0b\u3001\u6a21\u578b\u8bc4\u4ef7\u7b49\u65b9\u9762\u91cd\u70b9\u8bb2\u89e3\u4e86\u5927\u6a21\u578b\u5efa\u8bbe\u65b9\u6848\u3002\u5176\u4e2d\uff0c\u6570\u636e\u51c6\u5907\u7684\u6570\u636e\u91c7\u96c6\u3001\u6e05\u6d17\u548c\u6807\u6ce8\u6709\u6548\u8fd0\u7528\u5927\u6570\u636e\u6280\u672f\u4e3a\u5927\u6a21\u578b\u6709\u6548\u8bad\u7ec3\u5960\u5b9a\u57fa\u7840\uff0c\u5145\u5206\u4f53\u73b0\u4e86\u6280\u672f\u53d8\u9769", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.aa459e0d", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MjM5NDA0NDkzOQ==&mid=2650792273&idx=3&sn=ab5366200c086b4230d79683c4f6d2d5&chksm=bf6fa0ebefbf2445abb4ce069964eb5d542c13a20e99e710db79d062e59eaf6ad517f71e26b8#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MjM5NDA0NDkzOQ==&mid=2650792273&idx=3&sn=ab5366200c086b4230d79683c4f6d2d5&chksm=bf6fa0ebefbf2445abb4ce069964eb5d542c13a20e99e710db79d062e59eaf6ad517f71e26b8#rd", "authors": ["\u534e\u5546\u97ec\u7565"], "title": "\u5e74\u7ec8\u5927\u51b2\u523a\uff0c\u4e2d\u7f8e\u79d1\u6280\u5927\u5382\u90fd\u6740\u75af\u4e86", "comment": "Source: WeChat, Published: 2025-12-16 12:29:00", "summary": "\u8be5\u6a21\u578b\u4e00\u7ecf\u9762\u4e16\uff0c\u5c31\u88ab\u591a\u5bb6\u79d1\u6280\u5a92\u4f53\u8bc4\u4ef7\u4e3a\u201c\u8fc4\u4eca\u6700\u5b8c\u6574\u7684\u4e00\u6b21\u5927\u6a21\u578b\u4f53\u7cfb\u7ea7\u5347\u7ea7\u201d\uff1a\u4e0d\u4ec5\u5728\u63a8\u7406\u80fd\u529b\u3001\u591a\u6a21\u6001\u7406\u89e3\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u4e0a\u663e\u8457\u8dc3\u5347\uff0c\u8fd8\u9996\u6b21\u5c06\u4f18\u5316\u91cd\u70b9\u660e\u786e\u653e\u5728\u624b\u673a\u7aef\u63a8\u7406\u6548\u7387\u4e0e\u5f00\u53d1\u8005\u5e73\u53f0\u5efa\u8bbe\u4e0a\u3002", "AI": {"tldr": "\u8be5\u6a21\u578b\u4e00\u7ecf\u9762\u4e16\uff0c\u5c31\u88ab\u591a\u5bb6\u79d1\u6280\u5a92\u4f53\u8bc4\u4ef7\u4e3a\u201c\u8fc4\u4eca\u6700\u5b8c\u6574\u7684\u4e00\u6b21\u5927\u6a21\u578b\u4f53\u7cfb\u7ea7\u5347\u7ea7\u201d\uff1a\u4e0d\u4ec5\u5728\u63a8\u7406\u80fd\u529b\u3001\u591a\u6a21\u6001\u7406\u89e3\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u4e0a\u663e\u8457\u8dc3\u5347\uff0c\u8fd8\u9996\u6b21\u5c06\u4f18\u5316\u91cd\u70b9\u660e\u786e\u653e\u5728\u624b\u673a\u7aef\u63a8\u7406\u6548\u7387\u4e0e\u5f00\u53d1\u8005\u5e73\u53f0\u5efa\u8bbe\u4e0a\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.b10bd5b4", "categories": ["wechat.article", "wechat.ai", "wechat.rl"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzk0MzcxNDM2MQ==&mid=2247489539&idx=3&sn=11ab013aa40e8fbada61416f6827f0da&chksm=c279452c0396942f5f01dfe605630c37493b3a756257124b3cb014148caf1881f5e0c5880f86#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzk0MzcxNDM2MQ==&mid=2247489539&idx=3&sn=11ab013aa40e8fbada61416f6827f0da&chksm=c279452c0396942f5f01dfe605630c37493b3a756257124b3cb014148caf1881f5e0c5880f86#rd", "authors": ["\u4ea4\u79d1\u667a\u6c47"], "title": "\u4e2d\u8fdc\u6d77\u8fd0\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8-\u7814\u53d1\u521b\u65b0\u4e2d\u5fc3\u4e28AI\u65f6\u4ee3\u822a\u8fd0\u65b0\u57fa\u5efa\uff1aHi-Dolphin\u822a\u8fd0<em class=\"highlight\">\u5927\u6a21\u578b</em>", "comment": "Source: WeChat, Published: 2025-12-16 12:27:56", "summary": "\u6a21\u578b\u8bad\u7ec3\u65b9\u9762\uff0c\u5e73\u53f0100% \u91c7\u7528\u56fd\u4ea7\u7b97\u529b\u548c\u5f00\u6e90\u5927\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u589e\u91cf\u9884\u8bad\u7ec3\u3001\u5168\u53c2\u6570\u5fae\u8c03\u3001GRPO\u7b49\u4e3b\u6d41\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4ec5\u4f7f\u752816\u5f20GPU\u5361\u5b9e\u73b0\u5b8c\u6574\u822a\u8fd0\u5927\u6a21\u578b\u8bad\u7ec3\uff0c\u6a21\u578b\u5f00\u53d1\u6210\u672c\u4e3a\u901a\u7528\u5927\u6a21\u578b\u7684\u4e07\u5206\u4e4b\u4e00\uff0c\u6781\u5927\u63d0\u5347\u4e86\u884c\u4e1a\u9002\u914d", "AI": {"tldr": "\u6a21\u578b\u8bad\u7ec3\u65b9\u9762\uff0c\u5e73\u53f0100% \u91c7\u7528\u56fd\u4ea7\u7b97\u529b\u548c\u5f00\u6e90\u5927\u6a21\u578b\u6846\u67b6\uff0c\u7ed3\u5408\u589e\u91cf\u9884\u8bad\u7ec3\u3001\u5168\u53c2\u6570\u5fae\u8c03\u3001GRPO\u7b49\u4e3b\u6d41\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u4ec5\u4f7f\u752816\u5f20GPU\u5361\u5b9e\u73b0\u5b8c\u6574\u822a\u8fd0\u5927\u6a21\u578b\u8bad\u7ec3\uff0c\u6a21\u578b\u5f00\u53d1\u6210\u672c\u4e3a\u901a\u7528\u5927\u6a21\u578b\u7684\u4e07\u5206\u4e4b\u4e00\uff0c\u6781\u5927\u63d0\u5347\u4e86\u884c\u4e1a\u9002\u914d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agentic reinforcement learning"}}
{"id": "wechat.2512.0720c255", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzA4ODYzMDY0OA==&mid=2649662963&idx=2&sn=f4579bc9f0def12dc87e2dedbecf0bd0&chksm=89b62aabda3b46b419ea835edf5e241cb4b762815da8c506dad249b6dae19f7586652260c361#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzA4ODYzMDY0OA==&mid=2649662963&idx=2&sn=f4579bc9f0def12dc87e2dedbecf0bd0&chksm=89b62aabda3b46b419ea835edf5e241cb4b762815da8c506dad249b6dae19f7586652260c361#rd", "authors": ["\u73af\u7403\u79d1\u6280\u5c40"], "title": "\u9a6c\u4e91\u5927\u53d1\u795e\u5a01\uff0c\u963f\u91cc<em class=\"highlight\">\u5927\u6a21\u578b</em>\u62ff\u4e0b\u7f8e\u56fd\u7845\u8c37\u5de8\u5934", "comment": "Source: WeChat, Published: 2025-12-16 03:30:11", "summary": "\u8fd9\u662f\u56fd\u5185\u5927\u6a21\u578b\u5728\u8be5\u7c7b\u8bc4\u6d4b\u4e2d\u7684\u6700\u4f73\u7eaa\u5f55\u3002\u9664\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u80fd\u529b\uff0c\u963f\u91cc\u8fd8\u6784\u5efa\u4e86\u5168\u6808AI\u6280\u672f\u4f53\u7cfb\u3002\u4ece\u5e95\u5c42\u82af\u7247\u3001\u8d85\u8282\u70b9\u670d\u52a1\u5668\u3001\u9ad8\u6027\u80fd\u7f51\u7edc\u3001\u5206\u5e03\u5f0f\u5b58\u50a8\u3001\u667a\u7b97\u96c6\u7fa4\u5230\u4eba\u5de5\u667a\u80fd\u5e73\u53f0\uff0c\u963f\u91cc\u4e91\u5df2\u5f62\u6210\u5b8c\u6574\u7684\u6280\u672f\u95ed\u73af\u3002", "AI": {"tldr": "\u8fd9\u662f\u56fd\u5185\u5927\u6a21\u578b\u5728\u8be5\u7c7b\u8bc4\u6d4b\u4e2d\u7684\u6700\u4f73\u7eaa\u5f55\u3002\u9664\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u80fd\u529b\uff0c\u963f\u91cc\u8fd8\u6784\u5efa\u4e86\u5168\u6808AI\u6280\u672f\u4f53\u7cfb\u3002\u4ece\u5e95\u5c42\u82af\u7247\u3001\u8d85\u8282\u70b9\u670d\u52a1\u5668\u3001\u9ad8\u6027\u80fd\u7f51\u7edc\u3001\u5206\u5e03\u5f0f\u5b58\u50a8\u3001\u667a\u7b97\u96c6\u7fa4\u5230\u4eba\u5de5\u667a\u80fd\u5e73\u53f0\uff0c\u963f\u91cc\u4e91\u5df2\u5f62\u6210\u5b8c\u6574\u7684\u6280\u672f\u95ed\u73af\u3002", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe benchmark"}}
{"id": "wechat.2512.d1bb2297", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=MzkyMzM0NzEwNg==&mid=2247488628&idx=1&sn=235f1ab47ef6fe5ed88489215721a8b9&chksm=c0fe2da6284d540c861692d95bb501f4ee614cca263e1c9f9d383d38ea54b3552898e328cafd#rd", "abs": "http://mp.weixin.qq.com/s?__biz=MzkyMzM0NzEwNg==&mid=2247488628&idx=1&sn=235f1ab47ef6fe5ed88489215721a8b9&chksm=c0fe2da6284d540c861692d95bb501f4ee614cca263e1c9f9d383d38ea54b3552898e328cafd#rd", "authors": ["\u6c99\u4e18\u793e\u533a"], "title": "<em class=\"highlight\">\u5927\u6a21\u578b</em>+\u8f6f\u4ef6\u5de5\u7a0b\u6df1\u5ea6\u7814\u7a76\uff1a\u5e94\u7528\u573a\u666f\u3001\u53d1\u5c55\u8d8b\u52bf\u4e0e\u6700\u4f73\u5b9e\u8df5", "comment": "Source: WeChat, Published: 2025-12-16 00:39:49", "summary": "\u5927\u6a21\u578b\u51ed\u501f\u5176\u5f3a\u5927\u7684\u8bed\u8a00\u751f\u6210\u3001\u903b\u8f91\u63a8\u7406\u3001\u77e5\u8bc6\u878d\u5408\u7b49\u80fd\u529b\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5404\u4e2a\u73af\u8282\u5e26\u6765\u4e86\u65b0\u7684\u601d\u8def\u4e0e\u65b9\u6cd5\u3002\u57fa\u4e8e\u5e02\u573a\u7684\u957f\u671f\u8ddf\u8e2a\uff0c\u6c99\u4e18\u667a\u5e93\u6b63\u5f0f\u53d1\u5e03\u300a2025\u5e74\u201c\u5927\u6a21\u578b+\u8f6f\u4ef6\u5de5\u7a0b\u201d\u6700\u4f73\u5b9e\u8df5\u62a5\u544a\u300b\uff0c\u6df1\u5165\u6d1e\u5bdf\u201c\u5927\u6a21\u578b+\u8f6f\u4ef6\u5de5\u7a0b\u201d", "AI": {"tldr": "\u5927\u6a21\u578b\u51ed\u501f\u5176\u5f3a\u5927\u7684\u8bed\u8a00\u751f\u6210\u3001\u903b\u8f91\u63a8\u7406\u3001\u77e5\u8bc6\u878d\u5408\u7b49\u80fd\u529b\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5404\u4e2a\u73af\u8282\u5e26\u6765\u4e86\u65b0\u7684\u601d\u8def\u4e0e\u65b9\u6cd5\u3002\u57fa\u4e8e\u5e02\u573a\u7684\u957f\u671f\u8ddf\u8e2a\uff0c\u6c99\u4e18\u667a\u5e93\u6b63\u5f0f\u53d1\u5e03\u300a2025\u5e74\u201c\u5927\u6a21\u578b+\u8f6f\u4ef6\u5de5\u7a0b\u201d\u6700\u4f73\u5b9e\u8df5\u62a5\u544a\u300b\uff0c\u6df1\u5165\u6d1e\u5bdf\u201c\u5927\u6a21\u578b+\u8f6f\u4ef6\u5de5\u7a0b\u201d", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "swe application"}}
{"id": "wechat.2512.8555cc23", "categories": ["wechat.article", "wechat.ai"], "pdf": "http://mp.weixin.qq.com/s?__biz=Mzg2MjcwNjg2NA==&mid=2247514410&idx=1&sn=5d325c7db34268aea42e008b324e8adc&chksm=cfb0508da9b0d95be12e6c0440b28dd6c9af7ad998af2b9db4f638fb1153d8f74b64c9e9e2e4#rd", "abs": "http://mp.weixin.qq.com/s?__biz=Mzg2MjcwNjg2NA==&mid=2247514410&idx=1&sn=5d325c7db34268aea42e008b324e8adc&chksm=cfb0508da9b0d95be12e6c0440b28dd6c9af7ad998af2b9db4f638fb1153d8f74b64c9e9e2e4#rd", "authors": ["\u5e7f\u4e1c\u6e7e\u533a\u65e0\u4eba\u6280\u672f\u7814\u7a76\u9662"], "title": "\u6700\u65b0\u6df1\u5733\u5e02AI<em class=\"highlight\">\u5927\u6a21\u578b</em>\u5907\u6848\u60c5\u51b5\u4e00\u89c8\u8868\uff0840\u6b3e\uff09", "comment": "Source: WeChat, Published: 2025-12-15 23:00:42", "summary": "2025/6/9\u6df1\u5733\u5e0215\u7aef\u4fa7OS\u5927\u6a21\u578b\u6df1\u5733\u9646\u516e\u79d1\u6280\u6709\u9650\u516c\u53f82025/6/9\u6df1\u5733\u5e0216\u777f\u9a70\u5927\u6a21\u578b\u5f69\u8baf\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f82025/6/9\u6df1\u5733\u5e0217CentaAI\u4e2d\u539f\uff08\u4e2d\u56fd\uff09\u623f\u5730\u4ea7\u4ee3\u7406\u6709\u9650\u516c\u53f8", "AI": {"tldr": "2025/6/9\u6df1\u5733\u5e0215\u7aef\u4fa7OS\u5927\u6a21\u578b\u6df1\u5733\u9646\u516e\u79d1\u6280\u6709\u9650\u516c\u53f82025/6/9\u6df1\u5733\u5e0216\u777f\u9a70\u5927\u6a21\u578b\u5f69\u8baf\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f82025/6/9\u6df1\u5733\u5e0217CentaAI\u4e2d\u539f\uff08\u4e2d\u56fd\uff09\u623f\u5730\u4ea7\u4ee3\u7406\u6709\u9650\u516c\u53f8", "motivation": "\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0\uff0c\u5206\u4eabAI\u76f8\u5173\u6280\u672f\u5185\u5bb9", "method": "\u57fa\u4e8e\u5b9e\u9645\u5e94\u7528\u7ecf\u9a8c\u7684\u6280\u672f\u5206\u4eab", "result": "\u63d0\u4f9b\u5b9e\u7528\u7684AI\u6280\u672f\u89c1\u89e3\u548c\u6848\u4f8b\u5206\u6790", "conclusion": "\u9002\u5408\u4e86\u89e3AI\u6280\u672f\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528", "topic": "agent analysis"}}
