{"id": "2601.03378", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03378", "abs": "https://arxiv.org/abs/2601.03378", "authors": ["Yu Huo", "Siyu Zhang", "Kun Zeng", "Yuquan Lu", "Cheng Yang", "Yifu Guo", "Xiaoying Tang"], "title": "RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion", "comment": "22pages, 9 figures, conference", "summary": "Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9.", "AI": {"tldr": "RepoShapley\uff1a\u57fa\u4e8eShapley\u503c\u7684\u8054\u76df\u611f\u77e5\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u6846\u67b6\uff0c\u7528\u4e8e\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\uff0c\u901a\u8fc7\u8bc4\u4f30\u4ee3\u7801\u5757\u8fb9\u9645\u8d21\u732e\u6765\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\u3002", "motivation": "\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u96be\u4ee5\u63a7\u5236\u8de8\u6587\u4ef6\u8bc1\u636e\uff0c\u56e0\u4e3a\u4ee3\u7801\u5757\u7684\u6548\u7528\u5177\u6709\u4ea4\u4e92\u4f9d\u8d56\u6027\uff1a\u6709\u4e9b\u7247\u6bb5\u9700\u8981\u4e92\u8865\u4e0a\u4e0b\u6587\u624d\u6709\u5e2e\u52a9\uff0c\u800c\u6709\u4e9b\u51b2\u7a81\u7684\u7247\u6bb5\u4f1a\u635f\u5bb3\u89e3\u7801\u8d28\u91cf\u3002", "method": "\u63d0\u51faRepoShapley\u6846\u67b6\uff0c\u5305\u542bChunkShapley\u6a21\u5757\uff1a\u901a\u8fc7\u5355\u4ee3\u7801\u5757\u63a2\u6d4b\u4f30\u8ba1\u5e26\u7b26\u53f7\u52a0\u6743\u6548\u5e94\u3001\u6784\u5efa\u6355\u6349\u9971\u548c\u4e0e\u5e72\u6270\u7684\u4ee3\u7406\u535a\u5f08\u3001\u5c0f\u68c0\u7d22\u96c6\u7684\u7cbe\u786eShapley\u8ba1\u7b97\u3001\u4f7f\u7528\u51bb\u7ed3\u751f\u6210\u5668\u7684\u6709\u754c\u540e\u9a8c\u8bc1\u6765\u9009\u62e9\u89e3\u7801\u6700\u4f18\u8054\u76df\u3002\u901a\u8fc7\u79bb\u6563\u63a7\u5236token\u5c06\u9a8c\u8bc1\u7684KEEP/DROP\u51b3\u7b56\u548c\u68c0\u7d22\u89e6\u53d1\u84b8\u998f\u5230\u5355\u4e00\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRepoShapley\u63d0\u9ad8\u4e86\u8865\u5168\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6709\u5bb3\u4e0a\u4e0b\u6587\u548c\u4e0d\u5fc5\u8981\u7684\u68c0\u7d22\u3002", "conclusion": "RepoShapley\u901a\u8fc7Shapley\u98ce\u683c\u7684\u8fb9\u9645\u8d21\u732e\u76d1\u7763\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u4e2d\u4e0a\u4e0b\u6587\u9009\u62e9\u7684\u4ea4\u4e92\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u548c\u68c0\u7d22\u63a7\u5236\u3002", "topic": "code agent"}}
{"id": "2601.03512", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03512", "abs": "https://arxiv.org/abs/2601.03512", "authors": ["Yuhan Wu", "Huan Zhang", "Wei Cheng", "Chen Shen", "Jingyue Yang", "Wei Hu"], "title": "Bootstrapping Code Translation with Weighted Multilanguage Exploration", "comment": null, "summary": "Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components.", "AI": {"tldr": "BootTrans\uff1a\u4e00\u79cd\u5229\u7528\u6d4b\u8bd5\u5957\u4ef6\u529f\u80fd\u4e0d\u53d8\u6027\u548c\u8de8\u8bed\u8a00\u53ef\u79fb\u690d\u6027\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u6267\u884c\u5f15\u5bfc\u7684\u7ecf\u9a8c\u6536\u96c6\u89e3\u51b3\u591a\u8bed\u8a00\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u591a\u8bed\u8a00\u4ee3\u7801\u7ffb\u8bd1\u9762\u4e34\u4e24\u5927\u5173\u952e\u6311\u6218\uff1a1\uff09\u7f3a\u4e4f\u5e26\u6709\u53ef\u6267\u884c\u6d4b\u8bd5\u9884\u8a00\uff08test oracles\uff09\u7684\u5e76\u884c\u6570\u636e\uff1b2\uff09\u5904\u7406\u4e0d\u540c\u8bed\u8a00\u5bf9\u65f6\u7684\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faBootTrans\u5f15\u5bfc\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u6d4b\u8bd5\u5957\u4ef6\u7684\u529f\u80fd\u4e0d\u53d8\u6027\u548c\u8de8\u8bed\u8a00\u53ef\u79fb\u690d\u6027\uff0c\u5c06\u4e30\u5bcc\u7684\u67a2\u7ebd\u8bed\u8a00\u5355\u5143\u6d4b\u8bd5\u9002\u914d\u4e3a\u591a\u8bed\u8a00RL\u8bad\u7ec3\u7684\u901a\u7528\u9a8c\u8bc1\u9884\u8a00\uff1b2\uff09\u91c7\u7528\u5305\u542b\u79cd\u5b50\u6c60\u548c\u63a2\u7d22\u6c60\u7684\u53cc\u6c60\u67b6\u6784\uff0c\u901a\u8fc7\u6267\u884c\u5f15\u5bfc\u7684\u7ecf\u9a8c\u6536\u96c6\u9010\u6b65\u6269\u5c55\u8bad\u7ec3\u6570\u636e\uff1b3\uff09\u8bbe\u8ba1\u8bed\u8a00\u611f\u77e5\u52a0\u6743\u673a\u5236\uff0c\u6839\u636e\u5144\u5f1f\u8bed\u8a00\u95f4\u7684\u76f8\u5bf9\u6027\u80fd\u52a8\u6001\u4f18\u5148\u5904\u7406\u66f4\u96be\u7684\u7ffb\u8bd1\u65b9\u5411\u3002", "result": "\u5728HumanEval-X\u548cTransCoder-Test\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u7ffb\u8bd1\u65b9\u5411\u4e0a\u76f8\u6bd4\u57fa\u7ebfLLM\u90fd\u6709\u663e\u8457\u6539\u8fdb\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f15\u5bfc\u548c\u52a0\u6743\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "BootTrans\u901a\u8fc7\u5229\u7528\u6d4b\u8bd5\u5957\u4ef6\u7684\u8de8\u8bed\u8a00\u7279\u6027\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u591a\u8bed\u8a00\u4ee3\u7801\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "code agent"}}
{"id": "2601.03315", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03315", "abs": "https://arxiv.org/abs/2601.03315", "authors": ["Dhruv Trehan", "Paras Chopra"], "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts", "comment": null, "summary": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1", "AI": {"tldr": "\u56db\u8f6e\u5c1d\u8bd5\u7528\u516d\u4e2aLLM\u4ee3\u7406\u81ea\u52a8\u751f\u6210ML\u7814\u7a76\u8bba\u6587\uff0c\u4ec5\u4e00\u8f6e\u6210\u529f\u5e76\u88ab\u4f1a\u8bae\u63a5\u53d7\uff0c\u8bc6\u522b\u51fa\u516d\u4e2a\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff0c\u63d0\u51fa\u56db\u4e2a\u8bbe\u8ba1\u539f\u5219", "motivation": "\u63a2\u7d22\u4f7f\u7528LLM\u4ee3\u7406\u7ba1\u9053\u81ea\u52a8\u751f\u6210\u673a\u5668\u5b66\u4e60\u7814\u7a76\u8bba\u6587\u7684\u53ef\u884c\u6027\uff0c\u4e86\u89e3AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u80fd\u529b\u8fb9\u754c\u548c\u5931\u8d25\u6a21\u5f0f", "method": "\u4f7f\u7528\u516d\u4e2aLLM\u4ee3\u7406\u6620\u5c04\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u9636\u6bb5\uff0c\u8fdb\u884c\u56db\u8f6e\u7aef\u5230\u7aef\u5c1d\u8bd5\uff0c\u8bb0\u5f55\u548c\u5206\u6790\u5931\u8d25\u6a21\u5f0f", "result": "\u56db\u8f6e\u5c1d\u8bd5\u4e2d\u4e09\u8f6e\u5931\u8d25\uff0c\u4e00\u8f6e\u6210\u529f\u5b8c\u6210\u5e76\u88abAgents4Science 2025\u63a5\u53d7\uff0c\u8bc6\u522b\u51fa\u516d\u4e2a\u5173\u952e\u5931\u8d25\u6a21\u5f0f", "conclusion": "\u9700\u8981\u56db\u4e2a\u8bbe\u8ba1\u539f\u5219\u6765\u6784\u5efa\u66f4\u7a33\u5065\u7684AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\uff0c\u5e76\u8ba8\u8bba\u4e86\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u610f\u4e49", "topic": "agent analysis"}}
{"id": "2601.03513", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03513", "abs": "https://arxiv.org/abs/2601.03513", "authors": ["Yi Wang", "Zhenting Huang", "Zhaohan Ding", "Ruoxue Liao", "Yuan Huang", "Xinzijian Liu", "Jiajun Xie", "Siheng Chen", "Linfeng Zhang"], "title": "Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day", "comment": null, "summary": "Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.\n  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.\n  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.", "AI": {"tldr": "Deploy-Master\u662f\u4e00\u4e2a\u4e00\u7ad9\u5f0f\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u79d1\u5b66\u5de5\u5177\u53d1\u73b0\u3001\u6784\u5efa\u89c4\u8303\u63a8\u65ad\u3001\u57fa\u4e8e\u6267\u884c\u7684\u9a8c\u8bc1\u548c\u53d1\u5e03\uff0c\u5c06\u5f02\u6784\u5f00\u6e90\u4ed3\u5e93\u8f6c\u6362\u4e3a\u53ef\u8fd0\u884c\u7684\u5bb9\u5668\u5316\u80fd\u529b\u3002", "motivation": "\u5f00\u6e90\u79d1\u5b66\u8f6f\u4ef6\u4e30\u5bcc\u4f46\u96be\u4ee5\u7f16\u8bd1\u3001\u914d\u7f6e\u548c\u91cd\u7528\uff0c\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u3001\u5927\u89c4\u6a21\u8bc4\u4f30\u4ee5\u53ca\u79d1\u5b66\u5de5\u5177\u5728\u73b0\u4ee3AI-for-Science\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u5b9e\u9645\u96c6\u6210\u3002", "method": "\u57fa\u4e8e\u8986\u76d690+\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u7684\u5206\u7c7b\u6cd5\uff0c\u4ece50\u4e07+\u516c\u5171\u4ed3\u5e93\u4e2d\u7b5b\u9009\u51fa52,550\u4e2a\u53ef\u6267\u884c\u5de5\u5177\u5019\u9009\uff0c\u901a\u8fc7\u6784\u5efa\u89c4\u8303\u63a8\u65ad\u3001\u6267\u884c\u9a8c\u8bc1\u548c\u5bb9\u5668\u5316\uff0c\u5c06\u5de5\u5177\u8f6c\u6362\u4e3a\u53ef\u8fd0\u884c\u80fd\u529b\u3002", "result": "\u5728\u4e00\u5929\u5185\u5b8c\u621052,550\u6b21\u6784\u5efa\u5c1d\u8bd5\uff0c\u4e3a50,112\u4e2a\u79d1\u5b66\u5de5\u5177\u6784\u5efa\u4e86\u53ef\u91cd\u590d\u7684\u8fd0\u884c\u65f6\u73af\u5883\uff0c\u6bcf\u4e2a\u6210\u529f\u5de5\u5177\u90fd\u901a\u8fc7\u6700\u5c0f\u53ef\u6267\u884c\u547d\u4ee4\u9a8c\u8bc1\u5e76\u6ce8\u518c\u5230SciencePedia\u4e2d\u3002", "conclusion": "\u79d1\u5b66\u8f6f\u4ef6\u96be\u4ee5\u64cd\u4f5c\u5316\u7684\u539f\u56e0\u5728\u4e8e\u89c4\u6a21\u5316\u7684\u90e8\u7f72\u6311\u6218\uff0c\u9700\u8981\u5171\u4eab\u3001\u53ef\u89c2\u5bdf\u7684\u6267\u884c\u57fa\u677f\u4f5c\u4e3a\u53ef\u6269\u5c55AI4S\u548c\u4ee3\u7406\u79d1\u5b66\u7684\u57fa\u7840\u3002", "topic": "swe application"}}
{"id": "2601.03359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03359", "abs": "https://arxiv.org/abs/2601.03359", "authors": ["Alberto Purpura", "Li Wang", "Sahil Badyal", "Eugenio Beaufrand", "Adam Faulkner"], "title": "Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization", "comment": null, "summary": "Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5c06\u4e3b\u8981\u4efb\u52a1\u63cf\u8ff0\u4f18\u5316\u4e0e\u7ea6\u675f\u6761\u4ef6\u89e3\u8026\uff0c\u901a\u8fc7\u5b9a\u91cf\u5206\u6570\u53cd\u9988\u8fed\u4ee3\u6539\u8fdb\u63d0\u793a\u8bcd\uff0c\u663e\u8457\u63d0\u5347LLM\u8f93\u51fa\u5bf9\u5f62\u5f0f\u7ea6\u675f\u7684\u9075\u4ece\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e38\u751f\u6210\u5185\u5bb9\u76f8\u5173\u4f46\u5f62\u5f0f\u7ea6\u675f\u4e0d\u7b26\u7684\u8f93\u51fa\uff0c\u4f20\u7edf\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u53ea\u5173\u6ce8\u4e3b\u8981\u4efb\u52a1\u63cf\u8ff0\u91cd\u8ff0\uff0c\u5ffd\u7565\u4e86\u4f5c\u4e3a\u54cd\u5e94\u9a8c\u6536\u6807\u51c6\u7684\u7ec6\u7c92\u5ea6\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5c06\u4e3b\u8981\u4efb\u52a1\u63cf\u8ff0\u4f18\u5316\u4e0e\u7ea6\u675f\u6761\u4ef6\u89e3\u8026\uff0c\u4f7f\u7528\u5b9a\u91cf\u5206\u6570\u4f5c\u4e3a\u53cd\u9988\uff0c\u8fed\u4ee3\u91cd\u5199\u548c\u6539\u8fdb\u63d0\u793a\u8bcd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u4fee\u8ba2\u63d0\u793a\u8bcd\u80fd\u663e\u8457\u63d0\u9ad8Llama 3.1 8B\u548cMixtral-8x 7B\u7b49\u6a21\u578b\u7684\u9075\u4ece\u6027\u5206\u6570\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u4efb\u52a1\u63cf\u8ff0\u4e0e\u7ea6\u675f\u6761\u4ef6\u4f18\u5316\uff0c\u4f7f\u7528\u5b9a\u91cf\u53cd\u9988\u8fed\u4ee3\u6539\u8fdb\u63d0\u793a\u8bcd\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u8f93\u51fa\u5bf9\u5f62\u5f0f\u7ea6\u675f\u7684\u9075\u4ece\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.03320", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03320", "abs": "https://arxiv.org/abs/2601.03320", "authors": ["Yu Luo", "Shuo Han", "Yihan Hu", "Dong Li", "Jianye Hao"], "title": "Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning", "comment": null, "summary": "On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative \"eureka moments\" in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \\emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR\u00b2VPO\uff0c\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u6bd4\u7387\u65b9\u5dee\u7ea6\u675f\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u786c\u88c1\u526a\u65b9\u6cd5\uff0c\u5728\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8ePPO\u548cGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u786c\u88c1\u526a\u7b56\u7565\u6765\u7a33\u5b9a\u8bad\u7ec3\uff0c\u4f46\u8fd9\u4f1a\u6291\u5236\u9ad8\u56de\u62a5\u4f46\u9ad8\u5206\u6b67\u52a8\u4f5c\u7684\u68af\u5ea6\u4fe1\u53f7\uff0c\u540c\u65f6\u4e00\u65e6\u6570\u636e\u8fc7\u65f6\u5c31\u4f1a\u88ab\u4e22\u5f03\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u63d0\u51faR\u00b2VPO\uff08Ratio-Variance Regularized Policy Optimization\uff09\uff0c\u901a\u8fc7\u7ea6\u675f\u7b56\u7565\u6bd4\u7387\u7684\u65b9\u5dee\uff08\u4e8c\u9636\u4e2d\u5fc3\u77e9\uff09\u6765\u66ff\u4ee3\u786c\u88c1\u526a\uff0c\u63d0\u4f9b\u5e73\u6ed1\u7684\u7ea6\u675f\u653e\u677e\u3002\u91c7\u7528\u539f\u59cb-\u5bf9\u5076\u6846\u67b6\uff0c\u652f\u6301\u7a33\u5b9a\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u52a0\u6743\u8fc7\u65f6\u6837\u672c\u5b9e\u73b0\u539f\u5219\u6027\u7684\u79bb\u7ebf\u6570\u636e\u91cd\u7528\u3002", "result": "\u5728DeepSeek-Distill-Qwen-1.5B\u548copenPangu-Embedded\u7cfb\u5217\uff081B\u548c7B\uff09\u7b49\u5148\u8fdbLLM\u4e0a\u8fdb\u884c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0cR\u00b2VPO\u76f8\u6bd4\u57fa\u4e8e\u786c\u88c1\u526a\u7684\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u76f8\u5bf9\u63d0\u5347\u8fbe17%\uff0c\u6536\u655b\u6240\u9700rollout\u6570\u91cf\u51cf\u5c11\u7ea650%\u3002", "conclusion": "\u7b56\u7565\u6bd4\u7387\u65b9\u5dee\u63a7\u5236\u662f\u6539\u8fdb\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u5bf9\u9f50\u4e2d\u7a33\u5b9a\u6027\u548c\u6570\u636e\u6548\u7387\u7684\u6709\u524d\u666f\u65b9\u5411\uff0cR\u00b2VPO\u6846\u67b6\u5728\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03556", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03556", "abs": "https://arxiv.org/abs/2601.03556", "authors": ["Sabrina Haque", "Sarvesh Ingale", "Christoph Csallner"], "title": "Do Autonomous Agents Contribute Test Code? A Study of Tests in Agentic Pull Requests", "comment": null, "summary": "Testing is a critical practice for ensuring software correctness and long-term maintainability. As agentic coding tools increasingly submit pull requests (PRs), it becomes essential to understand how testing appears in these agent-driven workflows. Using the AIDev dataset, we present an empirical study of test inclusion in agentic pull requests. We examine how often tests are included, when they are introduced during the PR lifecycle and how test-containing PRs differ from non-test PRs in terms of size, turnaround time, and merge outcomes. Across agents, test-containing PRs are more common over time and tend to be larger and take longer to complete, while merge rates remain largely similar. We also observe variation across agents in both test adoption and the balance between test and production code within test PRs. Our findings provide a descriptive view of testing behavior in agentic pull requests and offer empirical grounding for future studies of autonomous software development.", "AI": {"tldr": "\u5bf9AIDev\u6570\u636e\u96c6\u4e2d\u4ee3\u7406\u63d0\u4ea4\u7684PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u6d4b\u8bd5\u4ee3\u7801\u5728\u4ee3\u7406\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u7684\u51fa\u73b0\u9891\u7387\u3001\u5f15\u5165\u65f6\u673a\uff0c\u4ee5\u53ca\u542b\u6d4b\u8bd5PR\u4e0e\u4e0d\u542b\u6d4b\u8bd5PR\u5728\u89c4\u6a21\u3001\u5468\u8f6c\u65f6\u95f4\u548c\u5408\u5e76\u7ed3\u679c\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u4ee3\u7406\u7f16\u7801\u5de5\u5177\u8d8a\u6765\u8d8a\u591a\u5730\u63d0\u4ea4PR\uff0c\u9700\u8981\u4e86\u89e3\u6d4b\u8bd5\u5728\u8fd9\u4e9b\u4ee3\u7406\u9a71\u52a8\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u7406\u89e3\u81ea\u4e3b\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u6d4b\u8bd5\u884c\u4e3a\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u6d4b\u8bd5\u4ee3\u7801\u5728\u4ee3\u7406PR\u4e2d\u7684\u5305\u542b\u60c5\u51b5\u3001\u5f15\u5165\u65f6\u673a\uff0c\u6bd4\u8f83\u542b\u6d4b\u8bd5PR\u4e0e\u4e0d\u542b\u6d4b\u8bd5PR\u5728\u89c4\u6a21\u3001\u5468\u8f6c\u65f6\u95f4\u548c\u5408\u5e76\u7ed3\u679c\u4e0a\u7684\u5dee\u5f02\u3002", "result": "\u968f\u65f6\u95f4\u63a8\u79fb\uff0c\u542b\u6d4b\u8bd5\u7684PR\u8d8a\u6765\u8d8a\u5e38\u89c1\uff1b\u542b\u6d4b\u8bd5\u7684PR\u901a\u5e38\u89c4\u6a21\u66f4\u5927\u3001\u5b8c\u6210\u65f6\u95f4\u66f4\u957f\uff0c\u4f46\u5408\u5e76\u7387\u57fa\u672c\u76f8\u4f3c\uff1b\u4e0d\u540c\u4ee3\u7406\u5728\u6d4b\u8bd5\u91c7\u7528\u7387\u548c\u6d4b\u8bd5\u4e0e\u751f\u4ea7\u4ee3\u7801\u5e73\u8861\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4ee3\u7406PR\u4e2d\u6d4b\u8bd5\u884c\u4e3a\u7684\u63cf\u8ff0\u6027\u89c6\u56fe\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u8f6f\u4ef6\u5f00\u53d1\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "topic": "agent analysis"}}
{"id": "2601.03389", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03389", "abs": "https://arxiv.org/abs/2601.03389", "authors": ["Michael Petrowski", "Milica Ga\u0161i\u0107"], "title": "Exploration Through Introspection: A Self-Aware Reward Model", "comment": "Accepted at AAAI-26 ToM4AI Workshop", "summary": "Understanding how artificial agents model internal mental states is central to advancing Theory of Mind in AI. Evidence points to a unified system for self- and other-awareness. We explore this self-awareness by having reinforcement learning agents infer their own internal states in gridworld environments. Specifically, we introduce an introspective exploration component that is inspired by biological pain as a learning signal by utilizing a hidden Markov model to infer \"pain-belief\" from online observations. This signal is integrated into a subjective reward function to study how self-awareness affects the agent's learning abilities. Further, we use this computational framework to investigate the difference in performance between normal and chronic pain perception models. Results show that introspective agents in general significantly outperform standard baseline agents and can replicate complex human-like behaviors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u7701\u63a2\u7d22\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u63a8\u65ad\"\u75bc\u75db\u4fe1\u5ff5\"\u4f5c\u4e3a\u5b66\u4e60\u4fe1\u53f7\uff0c\u7814\u7a76\u81ea\u6211\u610f\u8bc6\u5bf9\u667a\u80fd\u4f53\u5b66\u4e60\u80fd\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u667a\u80fd\u4f53\u5982\u4f55\u5efa\u6a21\u5185\u90e8\u5fc3\u7406\u72b6\u6001\u5bf9\u4e8e\u63a8\u8fdbAI\u4e2d\u7684\u5fc3\u667a\u7406\u8bba\u81f3\u5173\u91cd\u8981\u3002\u8bc1\u636e\u8868\u660e\u5b58\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u4f53-\u4ed6\u4f53\u610f\u8bc6\u7cfb\u7edf\uff0c\u672c\u6587\u901a\u8fc7\u8ba9\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u63a8\u65ad\u81ea\u8eab\u5185\u90e8\u72b6\u6001\u6765\u63a2\u7d22\u8fd9\u79cd\u81ea\u6211\u610f\u8bc6\u3002", "method": "\u5f15\u5165\u5185\u7701\u63a2\u7d22\u7ec4\u4ef6\uff0c\u53d7\u751f\u7269\u75bc\u75db\u4f5c\u4e3a\u5b66\u4e60\u4fe1\u53f7\u7684\u542f\u53d1\uff0c\u4f7f\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u4ece\u5728\u7ebf\u89c2\u5bdf\u4e2d\u63a8\u65ad\"\u75bc\u75db\u4fe1\u5ff5\"\u3002\u5c06\u8be5\u4fe1\u53f7\u6574\u5408\u5230\u4e3b\u89c2\u5956\u52b1\u51fd\u6570\u4e2d\uff0c\u7814\u7a76\u81ea\u6211\u610f\u8bc6\u5982\u4f55\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u6b63\u5e38\u548c\u6162\u6027\u75bc\u75db\u611f\u77e5\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5185\u7701\u667a\u80fd\u4f53\u603b\u4f53\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u667a\u80fd\u4f53\uff0c\u5e76\u4e14\u80fd\u591f\u590d\u73b0\u590d\u6742\u7684\u4eba\u7c7b\u7c7b\u4f3c\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u8ba1\u7b97\u6846\u67b6\u7814\u7a76\u81ea\u6211\u610f\u8bc6\u5bf9AI\u667a\u80fd\u4f53\u5b66\u4e60\u80fd\u529b\u7684\u5f71\u54cd\u662f\u6709\u6548\u7684\uff0c\u5185\u7701\u63a2\u7d22\u673a\u5236\u80fd\u591f\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\u5e76\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03640", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.03640", "abs": "https://arxiv.org/abs/2601.03640", "authors": ["Mohd Ariful Haque", "Kishor Datta Gupta", "Mohammad Ashiqur Rahman", "Roy George"], "title": "Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test", "comment": null, "summary": "Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u7684\u8f6c\u5f55\u5230\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u95e8\u8bc4\u4f30LLM\u5728\u7cbe\u786e\u8f6c\u5f55\u9ad8\u7cbe\u5ea6\u5341\u8fdb\u5236\u5e38\u6570\u5230Python\u4ee3\u7801\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5173\u6ce8\u6570\u636e\u5b8c\u6574\u6027\u800c\u975e\u7b97\u6cd5\u63a8\u7406\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8f6f\u4ef6\u4efb\u52a1\uff08\u5982\u52a0\u5bc6\u5e38\u6570\u3001\u534f\u8bae\u6d4b\u8bd5\u5411\u91cf\u3001\u767d\u540d\u5355\u3001\u6821\u51c6\u8868\uff09\u9700\u8981\u5c06\u6570\u636e\u7cbe\u786e\u8f6c\u5f55\u5230\u4ee3\u7801\u4e2d\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u5bf9\u5fae\u5c0f\u9057\u6f0f\u6216\u6539\u52a8\u5f88\u654f\u611f\uff0c\u53ef\u80fd\u4ea7\u751f\u8bed\u6cd5\u6709\u6548\u4f46\u529f\u80fd\u9519\u8bef\u7684\u7a0b\u5e8f\u3002\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u63a8\u7406\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u5b8c\u6574\u6027\u7684\u4e13\u95e8\u6d4b\u8bd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u7684\u8f6c\u5f55\u5230\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\uff1a\u7ed9\u5b9a\u9ad8\u7cbe\u5ea6\u5341\u8fdb\u5236\u5e38\u6570\u5217\u8868\uff0c\u6a21\u578b\u5fc5\u987b\u751f\u6210\u5d4c\u5165\u8fd9\u4e9b\u5e38\u6570\u5e76\u6267\u884c\u7b80\u5355\u805a\u5408\u8ba1\u7b97\u7684Python\u4ee3\u7801\u3002\u5f00\u53d1\u4e86\u63d0\u793a\u53d8\u4f53\u3001\u57fa\u4e8e\u7cbe\u786e\u5b57\u7b26\u4e32\u5305\u542b\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u53ca\u7528\u4e8e\u8868\u5f81\u72b6\u6001\u8ddf\u8e2a\u548c\u957f\u89c6\u91ce\u751f\u6210\u5931\u8d25\u7684\u5206\u6790\u6846\u67b6\u3002", "result": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4f5c\u4e3a\u4e00\u4e2a\u7d27\u51d1\u7684\u538b\u529b\u6d4b\u8bd5\uff0c\u80fd\u591f\u8865\u5145\u73b0\u6709\u7684\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\uff0c\u4e13\u95e8\u5173\u6ce8\u6570\u636e\u5b8c\u6574\u6027\u65b9\u9762\u7684\u95ee\u9898\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u5728\u7cbe\u786e\u6570\u636e\u8f6c\u5f55\u65b9\u9762\u7684\u53ef\u9760\u6027\uff0c\u8be5\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u5728\u6570\u636e\u5b8c\u6574\u6027\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u72b6\u6001\u8ddf\u8e2a\u548c\u957f\u89c6\u91ce\u751f\u6210\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "topic": "swe benchmark"}}
{"id": "2601.03731", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03731", "abs": "https://arxiv.org/abs/2601.03731", "authors": ["Jia Li", "Yuxin Su", "Michael R. Lyu"], "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level", "comment": null, "summary": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.", "AI": {"tldr": "RepoReason\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ed3\u5e93\u7ea7\u522b\u63a8\u7406\u80fd\u529b\u7684\u767d\u76d2\u8bca\u65ad\u57fa\u51c6\uff0c\u901a\u8fc7\u6267\u884c\u9a71\u52a8\u7684\u53d8\u5f02\u6846\u67b6\u548c\u52a8\u6001\u7a0b\u5e8f\u5207\u7247\u91cf\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u8bc4\u4f30\u5176\u5728\u771f\u5b9e\u3001\u5927\u89c4\u6a21\u3001\u76f8\u4e92\u4f9d\u8d56\u7684\u6587\u4ef6\u7cfb\u7edf\u4e2d\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u7684\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5728\u5b64\u7acb\u4ee3\u7801\u7247\u6bb5\u548c\u9ed1\u76d2\u8bc4\u4f30\u4e4b\u95f4\u6ce2\u52a8\uff0c\u7f3a\u4e4f\u5bf9\u4ed3\u5e93\u7ea7\u522b\u63a8\u7406\u80fd\u529b\u7684\u6df1\u5165\u8bca\u65ad\u3002", "method": "1. \u63d0\u51faRepoReason\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u6eaf\u56e0\u65ad\u8a00\u9a8c\u8bc1\uff1b2. \u4f7f\u7528\u6267\u884c\u9a71\u52a8\u7684\u53d8\u5f02\u6846\u67b6\uff0c\u5229\u7528\u73af\u5883\u4f5c\u4e3a\u8bed\u4e49\u9884\u8a00\u673a\u6765\u91cd\u65b0\u751f\u6210\u771f\u5b9e\u72b6\u6001\uff0c\u907f\u514d\u8bb0\u5fc6\u5316\uff1b3. \u5efa\u7acb\u57fa\u4e8e\u52a8\u6001\u7a0b\u5e8f\u5207\u7247\u7684\u7ec6\u7c92\u5ea6\u8bca\u65ad\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e09\u4e2a\u6b63\u4ea4\u6307\u6807\u91cf\u5316\u63a8\u7406\uff1aESV\uff08\u8bfb\u53d6\u8d1f\u8f7d\uff09\u3001MCL\uff08\u6a21\u62df\u6df1\u5ea6\uff09\u548cDFI\uff08\u96c6\u6210\u5bbd\u5ea6\uff09\u3002", "result": "\u5bf9\u524d\u6cbf\u6a21\u578b\uff08\u5982Claude-4.5-Sonnet\u3001DeepSeek-v3.1-Terminus\uff09\u7684\u7efc\u5408\u8bc4\u4f30\u63ed\u793a\u4e86\u666e\u904d\u5b58\u5728\u7684\u805a\u5408\u7f3a\u9677\uff0c\u5176\u4e2d\u96c6\u6210\u5bbd\u5ea6\uff08DFI\uff09\u662f\u4e3b\u8981\u7684\u8ba4\u77e5\u74f6\u9888\u3002", "conclusion": "RepoReason\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u767d\u76d2\u6d1e\u5bdf\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u4e0b\u4e00\u4ee3\u4ee3\u7406\u5f0f\u8f6f\u4ef6\u5de5\u7a0b\u3002\u96c6\u6210\u5bbd\u5ea6\u662f\u5f53\u524dLLM\u5728\u4ed3\u5e93\u7ea7\u522b\u63a8\u7406\u4e2d\u7684\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u3002", "topic": "swe benchmark"}}
{"id": "2601.03509", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.03509", "abs": "https://arxiv.org/abs/2601.03509", "authors": ["Haochen Shi", "Xingdi Yuan", "Bang Liu"], "title": "Evolving Programmatic Skill Networks", "comment": null, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "AI": {"tldr": "PSN\u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u653e\u73af\u5883\u6301\u7eed\u6280\u80fd\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5c06\u6280\u80fd\u8868\u793a\u4e3a\u53ef\u6267\u884c\u7684\u7b26\u53f7\u7a0b\u5e8f\uff0c\u901a\u8fc7LLM\u5b9e\u73b0\u6545\u969c\u5b9a\u4f4d\u3001\u6e10\u8fdb\u4f18\u5316\u548c\u7ed3\u6784\u91cd\u6784\uff0c\u5728MineDojo\u548cCrafter\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6280\u80fd\u91cd\u7528\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u5f00\u653e\u5f0f\u7684\u5177\u8eab\u73af\u5883\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u6301\u7eed\u83b7\u53d6\u3001\u7ec6\u5316\u548c\u91cd\u7528\u4e0d\u65ad\u6269\u5c55\u7684\u6280\u80fd\u5e93\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u6280\u80fd\u7ec4\u5408\u548c\u7f51\u7edc\u6f14\u5316\u3002", "method": "\u63d0\u51fa\u7a0b\u5e8f\u5316\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u5c06\u6280\u80fd\u8868\u793a\u4e3a\u53ef\u6267\u884c\u7684\u7b26\u53f7\u7a0b\u5e8f\uff0c\u5f62\u6210\u53ef\u7ec4\u5408\u7684\u7f51\u7edc\u3002\u901a\u8fc7LLM\u5b9e\u73b0\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1aREFLECT\u8fdb\u884c\u7ed3\u6784\u5316\u6545\u969c\u5b9a\u4f4d\u3001\u6e10\u8fdb\u4f18\u5316\u4e0e\u6210\u719f\u5ea6\u611f\u77e5\u66f4\u65b0\u95e8\u63a7\u3001\u4ee5\u53ca\u56de\u6eda\u9a8c\u8bc1\u4e0b\u7684\u89c4\u8303\u7ed3\u6784\u91cd\u6784\u3002", "result": "\u5728MineDojo\u548cCrafter\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPSN\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u6027\u7684\u6280\u80fd\u91cd\u7528\u3001\u5feb\u901f\u9002\u5e94\uff0c\u5e76\u5728\u5f00\u653e\u4efb\u52a1\u5206\u5e03\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PSN\u6846\u67b6\u901a\u8fc7\u7b26\u53f7\u7a0b\u5e8f\u8868\u793a\u6280\u80fd\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6301\u7eed\u6280\u80fd\u5b66\u4e60\uff0c\u5176\u5b66\u4e60\u52a8\u6001\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5177\u6709\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "topic": "agent analysis"}}
{"id": "2601.03780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03780", "abs": "https://arxiv.org/abs/2601.03780", "authors": ["Md Ahasanuzzaman", "Bram Adams", "Emad Fallahzadeh", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study", "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.\n  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.\n  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u77e5\u8bc6\u5355\u5143(KU)\u5206\u6790\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5(HumanEval, MBPP)\u4e0e\u73b0\u5b9e\u9879\u76ee\u5b58\u5728\u6982\u5ff5\u8986\u76d6\u504f\u5dee\uff0c\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684LLM\u6846\u67b6\u751f\u6210\u65b0\u4efb\u52a1\u6765\u5e73\u8861KU\u5206\u5e03\uff0c\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u7684\u73b0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5(\u5982HumanEval)\u53ef\u80fd\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7f16\u7a0b\u6982\u5ff5\uff0c\u5982\u679c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6982\u5ff5\u4e0d\u80fd\u4ee3\u8868\u5b9e\u9645\u9879\u76ee\u4f7f\u7528\u7684\u6982\u5ff5\uff0c\u8bc4\u4f30\u7ed3\u679c\u53ef\u80fd\u4e0d\u5b8c\u6574\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ee3\u7801\u6982\u5ff5\u4ee3\u8868\u6027\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "1) \u901a\u8fc7\u77e5\u8bc6\u5355\u5143(KU)\u5206\u6790Python\u57fa\u51c6\u6d4b\u8bd5(HumanEval, MBPP)\u548c30\u4e2a\u73b0\u5b9ePython\u9879\u76ee\u7684\u6982\u5ff5\u8986\u76d6\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684LLM\u6846\u67b6\uff0c\u5408\u6210KU\u5e73\u8861\u7684\u4efb\u52a1\u6765\u91cd\u65b0\u5e73\u8861\u57fa\u51c6\u6d4b\u8bd5\u5206\u5e03\uff1b3) \u751f\u6210440\u4e2a\u65b0\u4efb\u52a1\u5e76\u589e\u5f3a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "1) \u6bcf\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8986\u76d620\u4e2aKU\u4e2d\u7684\u4e00\u534a\uff0c\u800c\u9879\u76ee\u4f7f\u7528\u6240\u6709KU\u4e14\u5206\u5e03\u76f8\u5bf9\u5e73\u8861\uff1b2) \u589e\u5f3a\u540e\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u8457\u63d0\u9ad8KU\u8986\u76d6\uff0c\u5206\u5e03\u5bf9\u9f50\u5ea6\u63d0\u534760%\u4ee5\u4e0a\uff1b3) \u5728\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u6700\u5148\u8fdbLLM\u6027\u80fd\u4e0b\u964d12.54-44.82%\uff0c\u8868\u660e\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u56e0\u6709\u9650KU\u8986\u76d6\u800c\u9ad8\u4f30LLM\u6027\u80fd\u3002", "conclusion": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6982\u5ff5\u8986\u76d6\u504f\u5dee\uff0c\u9ad8\u4f30\u4e86LLM\u7684\u5b9e\u9645\u80fd\u529b\u3002\u63d0\u51fa\u7684KU\u5e73\u8861\u65b9\u6cd5\u80fd\u6784\u5efa\u66f4\u73b0\u5b9e\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3a\u8bc4\u4f30LLM\u4ee3\u7801\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002", "topic": "swe benchmark"}}
{"id": "2601.03878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03878", "abs": "https://arxiv.org/abs/2601.03878", "authors": ["Giovanni Rosa", "David Moreno-Lumbreras", "Gregorio Robles", "Jes\u00fas M. Gonz\u00e1lez-Barahona"], "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design", "comment": "This paper is a Stage 1 Registered Report. The study protocol and analysis plan were peer reviewed and accepted at SANER 2026 with a Continuity Acceptance (CA) score for Stage 2", "summary": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528CURRANTE\u5de5\u5177\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4eba\u7c7b\u5728\u89c4\u8303\u548c\u6d4b\u8bd5\u7ec6\u5316\u4e2d\u7684\u5e72\u9884\u5982\u4f55\u5f71\u54cdLLM\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u52a8\u6001\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1LLM\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u8f6f\u4ef6\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4f46\u5b83\u4eec\u5728\u7ed3\u6784\u5316\u3001\u89c4\u8303\u9a71\u52a8\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002\u9700\u8981\u7814\u7a76\u4eba\u7c7b\u5e72\u9884\u5728\u89c4\u8303\u548c\u6d4b\u8bd5\u7ec6\u5316\u9636\u6bb5\u5982\u4f55\u5f71\u54cdLLM\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u52a8\u6001\u3002", "method": "\u4f7f\u7528CURRANTE\uff08\u4e00\u4e2aVisual Studio Code\u6269\u5c55\uff09\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8be5\u5de5\u5177\u652f\u6301\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\uff0c\u5f15\u5bfc\u5f00\u53d1\u8005\u901a\u8fc7\u4e09\u4e2a\u987a\u5e8f\u9636\u6bb5\uff1a\u89c4\u8303\u5b9a\u4e49\u3001\u6d4b\u8bd5\u751f\u6210\u4e0e\u7ec6\u5316\u3001\u51fd\u6570\u5b9e\u73b0\u3002\u53c2\u4e0e\u8005\u4f7f\u7528LiveCodeBench\u6570\u636e\u96c6\u7684\u4e2d\u7b49\u96be\u5ea6\u95ee\u9898\uff0c\u5de5\u5177\u8bb0\u5f55\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u65e5\u5fd7\u3001\u6709\u6548\u6027\u6307\u6807\uff08\u901a\u8fc7\u7387\u3001\u5168\u901a\u8fc7\u5b8c\u6210\u5ea6\uff09\u3001\u6548\u7387\u6307\u6807\uff08\u901a\u8fc7\u65f6\u95f4\uff09\u548c\u8fed\u4ee3\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u5c1a\u672a\u5b8c\u6210\uff0c\u4f46\u9884\u671f\u7ed3\u679c\u5c06\u63d0\u4f9b\u5173\u4e8e\u4eba\u7c7b\u5e72\u9884\u5982\u4f55\u5f71\u54cdLLM\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u548c\u52a8\u6001\u7684\u5b9e\u8bc1\u89c1\u89e3\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5f00\u53d1\u73af\u5883\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u586b\u8865\u5bf9LLM\u5728\u7ed3\u6784\u5316\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u884c\u4e3a\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u597d\u7684\u4eba\u673a\u534f\u4f5c\u5f00\u53d1\u73af\u5883\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\uff0c\u4f7f\u4eba\u7c7b\u63a8\u7406\u4e0e\u6a21\u578b\u9a71\u52a8\u7684\u4ee3\u7801\u751f\u6210\u66f4\u597d\u5730\u5bf9\u9f50\u3002", "topic": "swe application"}}
{"id": "2601.03555", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03555", "abs": "https://arxiv.org/abs/2601.03555", "authors": ["Yuxuan Jiang", "Francis Ferraro"], "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models", "comment": null, "summary": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance.\n  Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions.\n  Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.", "AI": {"tldr": "SCRIBE\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6280\u80fd\u539f\u578b\u5e93\u5b9e\u73b0\u4e2d\u5c42\u7ea7\u5956\u52b1\u5efa\u6a21\uff0c\u51cf\u5c11\u5956\u52b1\u65b9\u5dee\uff0c\u63d0\u5347\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5956\u52b1\u6a21\u578b\u5728\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u8bad\u7ec3\u4e2d\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u533a\u5206\u9ad8\u5c42\u89c4\u5212\u4e0e\u5e95\u5c42\u6267\u884c\uff0c\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u56f0\u96be\u3002", "method": "SCRIBE\u6846\u67b6\u5f15\u5165\u4e2d\u5c42\u7ea7\u62bd\u8c61\uff0c\u57fa\u4e8e\u6280\u80fd\u539f\u578b\u5e93\u8fdb\u884c\u5956\u52b1\u5efa\u6a21\uff0c\u5c06\u5f00\u653e\u5f0fLLM\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7ea6\u675f\u9a8c\u8bc1\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5b50\u76ee\u6807\u8def\u7531\u5230\u76f8\u5e94\u539f\u578b\uff0c\u4e3a\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u7cbe\u786e\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c06Qwen3-4B\u6a21\u578b\u5728AIME25\u4e0a\u7684\u51c6\u786e\u7387\u4ece43.3%\u63d0\u5347\u81f363.3%\uff0c\u663e\u8457\u63d0\u9ad8\u590d\u6742\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u6210\u529f\u7387\u3002", "conclusion": "SCRIBE\u4e3a\u6784\u5efa\u66f4\u81ea\u4e3b\u53ef\u9760\u7684\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u4e92\u8865\u7684\u8def\u5f84\uff0c\u5206\u6790\u663e\u793a\u4e2d\u5c42\u7ea7\u6280\u80fd\u638c\u63e1\u5148\u4e8e\u6709\u6548\u9ad8\u5c42\u89c4\u5212\u884c\u4e3a\u7684\u51fa\u73b0\uff0c\u4e14\u4e0e\u5e95\u5c42\u5de5\u5177\u4f18\u5316\u5177\u6709\u53e0\u52a0\u6548\u5e94\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03604", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03604", "abs": "https://arxiv.org/abs/2601.03604", "authors": ["Chuanliu Fan", "Zicheng Ma", "Huanran Meng", "Aijia Zhang", "Wenjie Du", "Jun Zhang", "Yi Qin Gao", "Ziqiang Cao", "Guohong Fu"], "title": "Interleaved Tool-Call Reasoning for Protein Function Understanding", "comment": null, "summary": "Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.", "AI": {"tldr": "PFUA\u662f\u4e00\u4e2a\u5de5\u5177\u589e\u5f3a\u7684\u86cb\u767d\u8d28\u63a8\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u7279\u5b9a\u5de5\u5177\u800c\u975e\u7eaf\u6587\u672c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u6027\u80fd", "motivation": "\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u5c06\u57fa\u4e8e\u6587\u672c\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u8303\u5f0f\u5e94\u7528\u4e8e\u86cb\u767d\u8d28\u529f\u80fd\u7406\u89e3\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u662f\u77e5\u8bc6\u5bc6\u96c6\u578b\u7684\u79d1\u5b66\u4efb\u52a1\uff0c\u9700\u8981\u5916\u90e8\u751f\u7269\u5b66\u5148\u9a8c\u77e5\u8bc6\u548c\u8ba1\u7b97\u5de5\u5177\uff0c\u800c\u975e\u7eaf\u5185\u90e8\u63a8\u7406", "method": "\u63d0\u51faPFUA\u5de5\u5177\u589e\u5f3a\u86cb\u767d\u8d28\u63a8\u7406\u4ee3\u7406\uff0c\u7edf\u4e00\u95ee\u9898\u5206\u89e3\u3001\u5de5\u5177\u8c03\u7528\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u7b54\u6848\u751f\u6210\uff0c\u901a\u8fc7\u6574\u5408\u9886\u57df\u7279\u5b9a\u5de5\u5177\u4ea7\u751f\u53ef\u9a8c\u8bc1\u7684\u4e2d\u95f4\u8bc1\u636e\uff0c\u800c\u975e\u4f9d\u8d56\u957f\u65e0\u7ea6\u675f\u7684\u63a8\u7406\u8f68\u8ff9", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPFUA\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u6587\u672c\u63a8\u7406\u6a21\u578b\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347103%", "conclusion": "\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u9700\u8981\u6574\u5408\u9886\u57df\u5de5\u5177\u800c\u975e\u7eaf\u6587\u672c\u63a8\u7406\uff0cPFUA\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u6311\u6218", "topic": "agent analysis"}}
{"id": "2601.03624", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03624", "abs": "https://arxiv.org/abs/2601.03624", "authors": ["Zoran Milosevic", "Fethi Rabhi"], "title": "Architecting Agentic Communities using Design Patterns", "comment": "supplementary material accompanying this paper is also attached .. its title is \"Complete Agentic AI Design Patterns Catalogue\"", "summary": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f01\u4e1a\u5206\u5e03\u5f0f\u7cfb\u7edf\u6807\u51c6\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u884c\u4e1a\u5b9e\u8df5\u7684AI\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u6a21\u5f0f\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5305\u542bAI\u4ee3\u7406\u548c\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\"\u4ee3\u7406\u793e\u533a\"\u534f\u8c03\u6846\u67b6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7406AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u7cfb\u7edf\u5316\u7684\u67b6\u6784\u6307\u5bfc\uff0c\u4ee5\u6784\u5efa\u590d\u6742\u7684\u751f\u4ea7\u7ea7\u7cfb\u7edf\u3002\u5f53\u524d\u7f3a\u4e4f\u7ed3\u5408\u5b9e\u8df5\u6307\u5bfc\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u4f01\u4e1a\u7ea7AI\u7cfb\u7edf\u67b6\u6784\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u8bbe\u8ba1\u6a21\u5f0f\u5206\u7c7b\uff1aLLM\u4ee3\u7406\uff08\u4efb\u52a1\u7279\u5b9a\u81ea\u52a8\u5316\uff09\u3001\u4ee3\u7406AI\uff08\u81ea\u9002\u5e94\u76ee\u6807\u5bfb\u6c42\u8005\uff09\u3001\u4ee3\u7406\u793e\u533a\uff08\u7ec4\u7ec7\u6846\u67b6\uff09\u3002\u91cd\u70b9\u5173\u6ce8\u4ee3\u7406\u793e\u533a\uff0c\u501f\u9274\u5206\u5e03\u5f0f\u7cfb\u7edf\u534f\u8c03\u539f\u5219\uff0c\u5efa\u7acb\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u3001\u534f\u8bae\u548c\u6cbb\u7406\u7ed3\u6784\u534f\u8c03AI\u4ee3\u7406\u548c\u4eba\u7c7b\u53c2\u4e0e\u8005\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u652f\u6301\u7ec4\u7ec7\u3001\u6cd5\u5f8b\u548c\u4f26\u7406\u89c4\u5219\u7684\u8868\u8fbe\uff0c\u901a\u8fc7\u95ee\u8d23\u673a\u5236\u786e\u4fdd\u53ef\u64cd\u4f5c\u548c\u53ef\u9a8c\u8bc1\u7684\u6cbb\u7406\u3002\u901a\u8fc7\u4e34\u5e8a\u8bd5\u9a8c\u5339\u914d\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u67b6\u6784\u6307\u5bfc\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f01\u4e1a\u90e8\u7f72\u6240\u9700\u7684\u5f62\u5f0f\u5316\u4e25\u8c28\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u52a8\u6001\u591a\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u751f\u4ea7\u7ea7AI\u7cfb\u7edf\u6784\u5efa\u3002", "topic": "agent analysis"}}
{"id": "2601.03672", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03672", "abs": "https://arxiv.org/abs/2601.03672", "authors": ["Chen Zhang", "Kepu Zhang", "Jiatong Zhang", "Xiao Zhang", "Jun Xu"], "title": "Sandwich Reasoning: An Answer-Reasoning-Answer Approach for Low-Latency Query Correction", "comment": null, "summary": "Query correction is a critical entry point in modern search pipelines, demanding high accuracy strictly within real-time latency constraints. Chain-of-Thought (CoT) reasoning improves accuracy but incurs prohibitive latency for real-time query correction. A potential solution is to output an answer before reasoning to reduce latency; however, under autoregressive decoding, the early answer is independent of subsequent reasoning, preventing the model from leveraging its reasoning capability to improve accuracy. To address this issue, we propose Sandwich Reasoning (SandwichR), a novel approach that explicitly aligns a fast initial answer with post-hoc reasoning, enabling low-latency query correction without sacrificing reasoning-aware accuracy. SandwichR follows an Answer-Reasoning-Answer paradigm, producing an initial correction, an explicit reasoning process, and a final refined correction. To align the initial answer with post-reasoning insights, we design a consistency-aware reinforcement learning (RL) strategy: a dedicated consistency reward enforces alignment between the initial and final corrections, while margin-based rejection sampling prioritizes borderline samples where reasoning drives the most impactful corrective gains. Additionally, we construct a high-quality query correction dataset, addressing the lack of specialized benchmarks for complex query correction. Experimental results demonstrate that SandwichR achieves SOTA accuracy comparable to standard CoT while delivering a 40-70% latency reduction, resolving the latency-accuracy trade-off in online search.", "AI": {"tldr": "\u63d0\u51faSandwich Reasoning\u65b9\u6cd5\uff0c\u901a\u8fc7\"\u7b54\u6848-\u63a8\u7406-\u7b54\u6848\"\u8303\u5f0f\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u67e5\u8be2\u7ea0\u6b63\uff0c\u5728\u4fdd\u6301CoT\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u51cf\u5c1140-70%\u5ef6\u8fdf", "motivation": "\u73b0\u4ee3\u641c\u7d22\u7ba1\u9053\u4e2d\u7684\u67e5\u8be2\u7ea0\u6b63\u9700\u8981\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u5ef6\u8fdf\u7ea6\u675f\u3002CoT\u63a8\u7406\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u4f46\u5ef6\u8fdf\u8fc7\u9ad8\uff0c\u800c\u5148\u8f93\u51fa\u7b54\u6848\u518d\u63a8\u7406\u7684\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u63a8\u7406\u80fd\u529b\u6539\u8fdb\u51c6\u786e\u6027", "method": "\u63d0\u51faSandwich Reasoning\u65b9\u6cd5\uff0c\u91c7\u7528\u7b54\u6848-\u63a8\u7406-\u7b54\u6848\u8303\u5f0f\u3002\u8bbe\u8ba1\u4e00\u81f4\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff1a\u4e13\u7528\u4e00\u81f4\u6027\u5956\u52b1\u786e\u4fdd\u521d\u59cb\u548c\u6700\u7ec8\u7ea0\u6b63\u5bf9\u9f50\uff0c\u57fa\u4e8e\u8fb9\u9645\u7684\u62d2\u7edd\u91c7\u6837\u4f18\u5148\u5904\u7406\u63a8\u7406\u5f71\u54cd\u6700\u5927\u7684\u8fb9\u754c\u6837\u672c\u3002\u8fd8\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u67e5\u8be2\u7ea0\u6b63\u6570\u636e\u96c6", "result": "SandwichR\u5728\u4fdd\u6301\u4e0e\u6807\u51c6CoT\u76f8\u5f53\u7684\u6700\u5148\u8fdb\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e8640-70%\u7684\u5ef6\u8fdf\u51cf\u5c11\uff0c\u89e3\u51b3\u4e86\u5728\u7ebf\u641c\u7d22\u4e2d\u7684\u5ef6\u8fdf-\u51c6\u786e\u6027\u6743\u8861\u95ee\u9898", "conclusion": "Sandwich Reasoning\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u67e5\u8be2\u7ea0\u6b63\u4e2d\u5ef6\u8fdf\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u5feb\u901f\u521d\u59cb\u7b54\u6848\u4e0e\u4e8b\u540e\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u4e14\u4fdd\u6301\u63a8\u7406\u611f\u77e5\u7684\u51c6\u786e\u6027", "topic": "agentic reinforcement learning"}}
{"id": "2601.03444", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.03444", "abs": "https://arxiv.org/abs/2601.03444", "authors": ["Weiyue Li", "Minda Zhao", "Weixuan Dong", "Jiahui Cai", "Yuze Wei", "Michael Pocress", "Yi Li", "Wanyan Yuan", "Xiaoyue Wang", "Ruoyu Hou", "Kaiyuan Lou", "Wenqi Zeng", "Yutong Yang", "Yilun Du", "Mengyu Wang"], "title": "Grading Scale Impact on LLM-as-a-Judge: Human-LLM Alignment Is Highest on 0-5 Grading Scale", "comment": null, "summary": "Large language models (LLMs) are increasingly used as automated evaluators, yet prior works demonstrate that these LLM judges often lack consistency in scoring when the prompt is altered. However, the effect of the grading scale itself remains underexplored. We study the LLM-as-a-judge problem by comparing two kinds of raters: humans and LLMs. We collect ratings from both groups on three scales and across six benchmarks that include objective, open-ended subjective, and mixed tasks. Using intraclass correlation coefficients (ICC) to measure absolute agreement, we find that LLM judgments are not perfectly consistent across scales on subjective benchmarks, and that the choice of scale substantially shifts human-LLM agreement, even when within-group panel reliability is high. Aggregated over tasks, the grading scale of 0-5 yields the strongest human-LLM alignment. We further demonstrate that pooled reliability can mask benchmark heterogeneity and reveal systematic subgroup differences in alignment across gender groups, strengthening the importance of scale design and sub-level diagnostics as essential components of LLM-as-a-judge protocols.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u65f6\u8bc4\u5206\u91cf\u8868\u5bf9\u8bc4\u5206\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u91cf\u8868\u4f1a\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u4e0eLLM\u8bc4\u5206\u7684\u4e00\u81f4\u6027\uff0c0-5\u5206\u91cf\u8868\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63ed\u793a\u805a\u5408\u53ef\u9760\u6027\u53ef\u80fd\u63a9\u76d6\u57fa\u51c6\u5f02\u8d28\u6027\u548c\u6027\u522b\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eLLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5728\u63d0\u793a\u53d8\u5316\u65f6\u7f3a\u4e4f\u8bc4\u5206\u4e00\u81f4\u6027\uff0c\u4f46\u8bc4\u5206\u91cf\u8868\u672c\u8eab\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u5728\u4e0d\u540c\u91cf\u8868\u4e0a\u7684\u8bc4\u5206\u8868\u73b0\uff0c\u63a2\u7a76\u91cf\u8868\u8bbe\u8ba1\u5bf9\u8bc4\u4f30\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "method": "\u6536\u96c6\u4eba\u7c7b\u548cLLM\u5728\u4e09\u79cd\u4e0d\u540c\u91cf\u8868\u4e0a\u7684\u8bc4\u5206\u6570\u636e\uff0c\u6db5\u76d6\u516d\u4e2a\u57fa\u51c6\u4efb\u52a1\uff08\u5305\u62ec\u5ba2\u89c2\u3001\u4e3b\u89c2\u5f00\u653e\u6027\u548c\u6df7\u5408\u4efb\u52a1\uff09\u3002\u4f7f\u7528\u7ec4\u5185\u76f8\u5173\u7cfb\u6570(ICC)\u6d4b\u91cf\u7edd\u5bf9\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4e0d\u540c\u91cf\u8868\u4e0b\u4eba\u7c7b-LLM\u8bc4\u5206\u7684\u4e00\u81f4\u6027\u5dee\u5f02\u3002", "result": "LLM\u5728\u4e3b\u89c2\u57fa\u51c6\u4e0a\u7684\u8bc4\u5206\u5728\u4e0d\u540c\u91cf\u8868\u95f4\u4e0d\u5b8c\u5168\u4e00\u81f4\uff1b\u91cf\u8868\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4eba\u7c7b-LLM\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u7ec4\u5185\u9762\u677f\u53ef\u9760\u6027\u8f83\u9ad8\uff1b\u7efc\u5408\u5404\u4efb\u52a1\uff0c0-5\u5206\u91cf\u8868\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u4eba\u7c7b-LLM\u5bf9\u9f50\uff1b\u805a\u5408\u53ef\u9760\u6027\u53ef\u80fd\u63a9\u76d6\u57fa\u51c6\u5f02\u8d28\u6027\u548c\u6027\u522b\u7fa4\u4f53\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "conclusion": "\u8bc4\u5206\u91cf\u8868\u8bbe\u8ba1\u5bf9LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c0-5\u5206\u91cf\u8868\u8868\u73b0\u6700\u4f73\u3002\u9700\u8981\u5173\u6ce8\u91cf\u8868\u8bbe\u8ba1\u548c\u5b50\u7ea7\u8bca\u65ad\u4f5c\u4e3aLLM\u8bc4\u4f30\u534f\u8bae\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4ee5\u907f\u514d\u805a\u5408\u6307\u6807\u63a9\u76d6\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.03484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03484", "abs": "https://arxiv.org/abs/2601.03484", "authors": ["Kaiyuan Deng", "Hangyu Zheng", "Minghai Qing", "Kunxiong Zhu", "Gen Li", "Yang Xiao", "Lan Emily Zhang", "Linke Guo", "Bo Hui", "Yanzhi Wang", "Geng Yuan", "Gagan Agrawal", "Wei Niu", "Xiaolong Ma"], "title": "From Bits to Chips: An LLM-based Hardware-Aware Quantization Agent for Streamlined Deployment of LLMs", "comment": null, "summary": "Deploying models, especially large language models (LLMs), is becoming increasingly attractive to a broader user base, including those without specialized expertise. However, due to the resource constraints of certain hardware, maintaining high accuracy with larger model while meeting the hardware requirements remains a significant challenge. Model quantization technique helps mitigate memory and compute bottlenecks, yet the added complexities of tuning and deploying quantized models further exacerbates these challenges, making the process unfriendly to most of the users. We introduce the Hardware-Aware Quantization Agent (HAQA), an automated framework that leverages LLMs to streamline the entire quantization and deployment process by enabling efficient hyperparameter tuning and hardware configuration, thereby simultaneously improving deployment quality and ease of use for a broad range of users. Our results demonstrate up to a 2.3x speedup in inference, along with increased throughput and improved accuracy compared to unoptimized models on Llama. Additionally, HAQA is designed to implement adaptive quantization strategies across diverse hardware platforms, as it automatically finds optimal settings even when they appear counterintuitive, thereby reducing extensive manual effort and demonstrating superior adaptability. Code will be released.", "AI": {"tldr": "HAQA\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4f18\u5316\u6a21\u578b\u91cf\u5316\u548c\u90e8\u7f72\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u81ea\u52a8\u5316\u8c03\u53c2\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u901f\u5ea62.3\u500d\uff0c\u964d\u4f4e\u7528\u6237\u4f7f\u7528\u95e8\u69db\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u90e8\u7f72\u9700\u6c42\u589e\u957f\uff0c\u975e\u4e13\u4e1a\u7528\u6237\u9762\u4e34\u786c\u4ef6\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u7cbe\u5ea6\u4fdd\u6301\u6311\u6218\u3002\u4f20\u7edf\u91cf\u5316\u6280\u672f\u867d\u7136\u7f13\u89e3\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u4f46\u8c03\u53c2\u548c\u90e8\u7f72\u590d\u6742\u5ea6\u9ad8\uff0c\u5bf9\u666e\u901a\u7528\u6237\u4e0d\u53cb\u597d\u3002", "method": "\u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u91cf\u5316\u4ee3\u7406(HAQA)\uff0c\u5229\u7528LLM\u81ea\u52a8\u5316\u6574\u4e2a\u91cf\u5316\u548c\u90e8\u7f72\u6d41\u7a0b\uff0c\u5305\u62ec\u9ad8\u6548\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u786c\u4ef6\u914d\u7f6e\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u91cf\u5316\u7b56\u7565\u3002", "result": "\u5728Llama\u6a21\u578b\u4e0a\u5b9e\u73b0\u6700\u9ad82.3\u500d\u63a8\u7406\u52a0\u901f\uff0c\u63d0\u5347\u541e\u5410\u91cf\u548c\u7cbe\u5ea6\uff0c\u80fd\u81ea\u52a8\u53d1\u73b0\u53cd\u76f4\u89c9\u7684\u6700\u4f18\u8bbe\u7f6e\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u5c55\u793a\u4f18\u8d8a\u9002\u5e94\u6027\u3002", "conclusion": "HAQA\u901a\u8fc7LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u663e\u8457\u7b80\u5316\u6a21\u578b\u91cf\u5316\u548c\u90e8\u7f72\u6d41\u7a0b\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u7528\u6237\u95e8\u69db\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u786c\u4ef6\u5e73\u53f0\u3002", "topic": "code agent"}}
{"id": "2601.03525", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03525", "abs": "https://arxiv.org/abs/2601.03525", "authors": ["Longwen Wang", "Xuan'er Wu", "Xiaohui Hu", "Yirui Liu", "Yuankai Fan", "Kaidong Yu", "Qizhen Weng", "Wei Xi", "Xuelong Li"], "title": "VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation", "comment": null, "summary": "Effective reward design is a central challenge in Reinforcement Learning (RL) for code generation. Mainstream pass/fail outcome rewards enforce functional correctness via executing unit tests, but the resulting sparsity limits potential performance gains. While recent work has explored external Reward Models (RM) to generate richer, continuous rewards, the learned RMs suffer from reward misalignment and prohibitive computational cost. In this paper, we introduce \\textbf{VeRPO} (\\textbf{V}erifiable D\\textbf{e}nse \\textbf{R}eward \\textbf{P}olicy \\textbf{O}ptimization), a novel RL framework for code generation that synthesizes \\textit{robust and dense rewards fully grounded in verifiable execution feedback}. The core idea of VeRPO is constructing dense rewards from weighted partial success: by dynamically estimating the difficulty weight of each unit test based on the execution statistics during training, a dense reward is derived from the sum of weights of the passed unit tests. To solidify the consistency between partial success and end-to-end functional correctness, VeRPO further integrates the dense signal with global execution outcomes, establishing a robust and dense reward paradigm relying solely on verifiable execution feedback. Extensive experiments across diverse benchmarks and settings demonstrate that VeRPO consistently outperforms outcome-driven and RM-based baselines, achieving up to +8.83\\% gain in pass@1 with negligible time cost (< 0.02\\%) and zero GPU memory overhead.", "AI": {"tldr": "VeRPO\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u6267\u884c\u53cd\u9988\u7684\u5bc6\u96c6\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u6743\u90e8\u5206\u6210\u529f\u6784\u5efa\u5bc6\u96c6\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd", "motivation": "\u5f53\u524d\u4ee3\u7801\u751f\u6210RL\u4e2d\uff0c\u4e3b\u6d41\u901a\u8fc7/\u5931\u8d25\u7ed3\u679c\u5956\u52b1\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u6267\u884c\uff0c\u5956\u52b1\u7a00\u758f\u6027\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002\u867d\u7136\u6700\u8fd1\u5de5\u4f5c\u63a2\u7d22\u4f7f\u7528\u5916\u90e8\u5956\u52b1\u6a21\u578b\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u8fde\u7eed\u5956\u52b1\uff0c\u4f46\u5b66\u4e60\u5230\u7684\u5956\u52b1\u6a21\u578b\u5b58\u5728\u5956\u52b1\u4e0d\u5bf9\u9f50\u548c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "VeRPO\u901a\u8fc7\u52a0\u6743\u90e8\u5206\u6210\u529f\u6784\u5efa\u5bc6\u96c6\u5956\u52b1\uff1a\u57fa\u4e8e\u8bad\u7ec3\u671f\u95f4\u7684\u6267\u884c\u7edf\u8ba1\u6570\u636e\u52a8\u6001\u4f30\u8ba1\u6bcf\u4e2a\u5355\u5143\u6d4b\u8bd5\u7684\u96be\u5ea6\u6743\u91cd\uff0c\u4ece\u901a\u8fc7\u7684\u5355\u5143\u6d4b\u8bd5\u6743\u91cd\u4e4b\u548c\u63a8\u5bfc\u51fa\u5bc6\u96c6\u5956\u52b1\u3002\u4e3a\u5f3a\u5316\u90e8\u5206\u6210\u529f\u4e0e\u7aef\u5230\u7aef\u529f\u80fd\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0cVeRPO\u8fdb\u4e00\u6b65\u5c06\u5bc6\u96c6\u4fe1\u53f7\u4e0e\u5168\u5c40\u6267\u884c\u7ed3\u679c\u6574\u5408\uff0c\u5efa\u7acb\u4ec5\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u6267\u884c\u53cd\u9988\u7684\u9c81\u68d2\u5bc6\u96c6\u5956\u52b1\u8303\u5f0f\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u548c\u8bbe\u7f6e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVeRPO\u59cb\u7ec8\u4f18\u4e8e\u7ed3\u679c\u9a71\u52a8\u548c\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728pass@1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe+8.83%\u7684\u63d0\u5347\uff0c\u65f6\u95f4\u6210\u672c\u53ef\u5ffd\u7565\u4e0d\u8ba1(<0.02%)\u4e14\u96f6GPU\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "VeRPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u5bc6\u96c6\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u6267\u884c\u53cd\u9988\uff0c\u89e3\u51b3\u4e86\u4ee3\u7801\u751f\u6210RL\u4e2d\u5956\u52b1\u7a00\u758f\u6027\u548c\u5956\u52b1\u6a21\u578b\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "topic": "code agent"}}
{"id": "2601.03905", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03905", "abs": "https://arxiv.org/abs/2601.03905", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Bingxuan Li", "Xiusi Chen", "Yuji Zhang", "Bingxiang He", "Qinyu Luo", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Yunzhu Li", "Heng Ji", "Heng Ji"], "title": "Current Agents Fail to Leverage World Model as Tool for Foresight", "comment": "36 Pages, 13 Figures, 17 Tables", "summary": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.", "AI": {"tldr": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u5728\u5229\u7528\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u524d\u77bb\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1a\u8c03\u7528\u7387\u4f4e\u3001\u8bef\u7528\u7387\u9ad8\u3001\u6027\u80fd\u751a\u81f3\u4e0b\u964d\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u4f55\u65f6\u6a21\u62df\u3001\u5982\u4f55\u89e3\u91ca\u9884\u6d4b\u7ed3\u679c\u4ee5\u53ca\u5982\u4f55\u5c06\u9884\u89c1\u6574\u5408\u5230\u4e0b\u6e38\u63a8\u7406\u4e2d\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53\u9762\u4e34\u66f4\u591a\u9700\u8981\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u4efb\u52a1\uff0c\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5916\u90e8\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u5b9e\u8bc1\u7814\u7a76\u5f53\u524d\u667a\u80fd\u4f53\u662f\u5426\u80fd\u6709\u6548\u5229\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\u6765\u589e\u5f3a\u5176\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u667a\u80fd\u4f53\u4efb\u52a1\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u667a\u80fd\u4f53\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5de5\u5177\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5305\u62ec\u8c03\u7528\u9891\u7387\u3001\u4f7f\u7528\u65b9\u5f0f\u4ee5\u53ca\u6027\u80fd\u53d8\u5316\uff0c\u5e76\u8fdb\u884c\u5f52\u56e0\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u667a\u80fd\u4f53\u5f88\u5c11\u8c03\u7528\u6a21\u62df\uff08\u5c11\u4e8e1%\uff09\uff1b2\uff09\u7ecf\u5e38\u8bef\u7528\u9884\u6d4b\u63a8\u6f14\uff08\u7ea615%\uff09\uff1b3\uff09\u5f53\u6a21\u62df\u53ef\u7528\u6216\u88ab\u5f3a\u5236\u4f7f\u7528\u65f6\uff0c\u6027\u80fd\u8868\u73b0\u4e0d\u4e00\u81f4\u751a\u81f3\u4e0b\u964d\uff08\u6700\u591a5%\uff09\u3002\u5f52\u56e0\u5206\u6790\u8868\u660e\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u667a\u80fd\u4f53\u51b3\u5b9a\u4f55\u65f6\u6a21\u62df\u3001\u5982\u4f55\u89e3\u91ca\u9884\u6d4b\u7ed3\u679c\u4ee5\u53ca\u5982\u4f55\u5c06\u9884\u89c1\u6574\u5408\u5230\u4e0b\u6e38\u63a8\u7406\u7684\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u667a\u80fd\u4f53\u5728\u5229\u7528\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u524d\u77bb\u6027\u8ba4\u77e5\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u4fc3\u8fdb\u4e0e\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u6821\u51c6\u3001\u6218\u7565\u6027\u4ea4\u4e92\u7684\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u672a\u6765\u72b6\u6001\u9884\u6d4b\u80fd\u529b\u3002", "topic": "agent analysis"}}
{"id": "2601.03948", "categories": ["cs.AI", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2601.03948", "abs": "https://arxiv.org/abs/2601.03948", "authors": ["Rui Sun", "Yifan Sun", "Sheng Xu", "Li Zhao", "Jing Li", "Daxin Jiang", "Chen Hua", "Zuo Bai"], "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification", "comment": null, "summary": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.", "AI": {"tldr": "Trade-R1\uff1a\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u63a8\u7406\u9a8c\u8bc1\u5c06\u53ef\u9a8c\u8bc1\u5956\u52b1\u4e0e\u968f\u673a\u91d1\u878d\u73af\u5883\u8fde\u63a5\uff0c\u89e3\u51b3\u6807\u51c6RL\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u53ef\u9a8c\u8bc1\u5956\u52b1\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u5e02\u573a\u5177\u6709\u968f\u673a\u6027\uff0c\u5956\u52b1\u867d\u7136\u53ef\u9a8c\u8bc1\u4f46\u5b58\u5728\u566a\u58f0\uff0c\u5bfc\u81f4\u6807\u51c6RL\u9000\u5316\u4e3a\u5956\u52b1\u9ed1\u5ba2", "method": "\u63d0\u51faTrade-R1\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u63a8\u7406\u9a8c\u8bc1\u8fde\u63a5\u53ef\u9a8c\u8bc1\u5956\u52b1\u4e0e\u968f\u673a\u73af\u5883\u3002\u6838\u5fc3\u521b\u65b0\u662f\u5c06\u5197\u957f\u91d1\u878d\u6587\u6863\u7684\u63a8\u7406\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316RAG\u4efb\u52a1\uff0c\u6784\u5efa\u4e09\u89d2\u4e00\u81f4\u6027\u5ea6\u91cf\uff08\u68c0\u7d22\u8bc1\u636e\u3001\u63a8\u7406\u94fe\u3001\u51b3\u7b56\u4e4b\u95f4\u7684\u5bf9\u9f50\uff09\u4f5c\u4e3a\u566a\u58f0\u5e02\u573a\u56de\u62a5\u7684\u6709\u6548\u6027\u8fc7\u6ee4\u5668\u3002\u63a2\u7d22\u4e24\u79cd\u5956\u52b1\u6574\u5408\u7b56\u7565\uff1a\u56fa\u5b9a\u6548\u5e94\u8bed\u4e49\u5956\u52b1\uff08FSR\uff09\u548c\u52a8\u6001\u6548\u5e94\u8bed\u4e49\u5956\u52b1\uff08DSR\uff09", "result": "\u5728\u4e0d\u540c\u56fd\u5bb6\u8d44\u4ea7\u9009\u62e9\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u8303\u5f0f\u51cf\u5c11\u4e86\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0cDSR\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8de8\u5e02\u573a\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u9ad8\u7684\u63a8\u7406\u4e00\u81f4\u6027", "conclusion": "Trade-R1\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u63a8\u7406\u9a8c\u8bc1\u6709\u6548\u89e3\u51b3\u4e86RL\u5728\u91d1\u878d\u51b3\u7b56\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u4e3a\u968f\u673a\u73af\u5883\u4e2d\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f", "topic": "agentic reinforcement learning"}}
{"id": "2601.04035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04035", "abs": "https://arxiv.org/abs/2601.04035", "authors": ["Yilin Cao", "Yufeng Zhong", "Zhixiong Zeng", "Liming Zheng", "Jing Huang", "Haibo Qiu", "Peng Shi", "Wenji Mao", "Wan Guanglu"], "title": "MobileDreamer: Generative Sketch World Model for GUI Agent", "comment": null, "summary": "Mobile GUI agents have shown strong potential in real-world automation and practical applications. However, most existing agents remain reactive, making decisions mainly from current screen, which limits their performance on long-horizon tasks. Building a world model from repeated interactions enables forecasting action outcomes and supports better decision making for mobile GUI agents. This is challenging because the model must predict post-action states with spatial awareness while remaining efficient enough for practical deployment. In this paper, we propose MobileDreamer, an efficient world-model-based lookahead framework to equip the GUI agents based on the future imagination provided by the world model. It consists of textual sketch world model and rollout imagination for GUI agent. Textual sketch world model forecasts post-action states through a learning process to transform digital images into key task-related sketches, and designs a novel order-invariant learning strategy to preserve the spatial information of GUI elements. The rollout imagination strategy for GUI agent optimizes the action-selection process by leveraging the prediction capability of world model. Experiments on Android World show that MobileDreamer achieves state-of-the-art performance and improves task success by 5.25%. World model evaluations further verify that our textual sketch modeling accurately forecasts key GUI elements.", "AI": {"tldr": "MobileDreamer\uff1a\u4e00\u4e2a\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u79fb\u52a8GUI\u4ee3\u7406\u524d\u77bb\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u8349\u56fe\u4e16\u754c\u6a21\u578b\u9884\u6d4b\u52a8\u4f5c\u540e\u72b6\u6001\uff0c\u4f7f\u7528\u6eda\u52a8\u60f3\u8c61\u4f18\u5316\u52a8\u4f5c\u9009\u62e9\uff0c\u5728Android World\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53475.25%", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u5927\u591a\u662f\u53cd\u5e94\u5f0f\u7684\uff0c\u4e3b\u8981\u4f9d\u8d56\u5f53\u524d\u5c4f\u5e55\u4fe1\u606f\u8fdb\u884c\u51b3\u7b56\uff0c\u8fd9\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\u3002\u6784\u5efa\u4e16\u754c\u6a21\u578b\u80fd\u591f\u9884\u6d4b\u52a8\u4f5c\u7ed3\u679c\u5e76\u652f\u6301\u66f4\u597d\u7684\u51b3\u7b56\uff0c\u4f46\u6311\u6218\u5728\u4e8e\u6a21\u578b\u9700\u8981\u5177\u5907\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u9884\u6d4b\u52a8\u4f5c\u540e\u72b6\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u63d0\u51faMobileDreamer\u6846\u67b6\uff0c\u5305\u542b\u6587\u672c\u8349\u56fe\u4e16\u754c\u6a21\u578b\u548cGUI\u4ee3\u7406\u6eda\u52a8\u60f3\u8c61\u3002\u6587\u672c\u8349\u56fe\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u5c06\u6570\u5b57\u56fe\u50cf\u8f6c\u6362\u4e3a\u5173\u952e\u4efb\u52a1\u76f8\u5173\u8349\u56fe\u6765\u9884\u6d4b\u52a8\u4f5c\u540e\u72b6\u6001\uff0c\u91c7\u7528\u65b0\u9896\u7684\u987a\u5e8f\u4e0d\u53d8\u5b66\u4e60\u7b56\u7565\u4fdd\u7559GUI\u5143\u7d20\u7a7a\u95f4\u4fe1\u606f\u3002\u6eda\u52a8\u60f3\u8c61\u7b56\u7565\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u4f18\u5316\u52a8\u4f5c\u9009\u62e9\u8fc7\u7a0b\u3002", "result": "\u5728Android World\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMobileDreamer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e865.25%\u3002\u4e16\u754c\u6a21\u578b\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6587\u672c\u8349\u56fe\u5efa\u6a21\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5173\u952eGUI\u5143\u7d20\u3002", "conclusion": "MobileDreamer\u901a\u8fc7\u9ad8\u6548\u7684\u4e16\u754c\u6a21\u578b\u524d\u77bb\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8GUI\u4ee3\u7406\u5728\u957f\u89c6\u91ce\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5176\u6587\u672c\u8349\u56fe\u4e16\u754c\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4bGUI\u5143\u7d20\u72b6\u6001\uff0c\u4e3a\u79fb\u52a8\u81ea\u52a8\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "topic": "agent analysis"}}
{"id": "2601.04060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04060", "abs": "https://arxiv.org/abs/2601.04060", "authors": ["Jinwei Su", "Qizhen Lan", "Zeyu Wang", "Yinghui Xia", "Hairu Wen", "Yiqun Duan", "Xi Xiao", "Tianyu Shi", "Yang Jingsong", "Lewei He"], "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows", "comment": null, "summary": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.", "AI": {"tldr": "ComfySearch\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728ComfyUI\u5e73\u53f0\u4e0a\u63a2\u7d22\u7ec4\u4ef6\u7a7a\u95f4\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5f15\u5bfc\u7684\u5de5\u4f5c\u6d41\u6784\u5efa\u751f\u6210\u529f\u80fd\u6027\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u521b\u610f\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "ComfyUI\u5e73\u53f0\u4e0a\u7684AI\u751f\u6210\u5185\u5bb9\u4ece\u5355\u4e00\u6a21\u578b\u53d1\u5c55\u5230\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u4f46\u5927\u91cf\u7ec4\u4ef6\u548c\u4e25\u683c\u7684\u56fe\u7ea6\u675f\u4e0b\u4fdd\u6301\u957f\u65f6\u7a0b\u7ed3\u6784\u4e00\u81f4\u6027\u56f0\u96be\uff0c\u5bfc\u81f4\u901a\u8fc7\u7387\u4f4e\u548c\u5de5\u4f5c\u6d41\u8d28\u91cf\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86ComfySearch\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5f15\u5bfc\u7684\u5de5\u4f5c\u6d41\u6784\u5efa\u6709\u6548\u63a2\u7d22\u7ec4\u4ef6\u7a7a\u95f4\uff0c\u751f\u6210\u529f\u80fd\u6027ComfyUI\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660eComfySearch\u5728\u590d\u6742\u521b\u610f\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u53ef\u6267\u884c\u6027\uff08\u901a\u8fc7\uff09\u7387\u3001\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u7387\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ComfySearch\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86ComfyUI\u5e73\u53f0\u4e2d\u7ec4\u4ef6\u63a2\u7d22\u548c\u5de5\u4f5c\u6d41\u6784\u5efa\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86AI\u751f\u6210\u5185\u5bb9\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u548c\u8d28\u91cf\u3002", "topic": "code agent"}}
{"id": "2601.03506", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03506", "abs": "https://arxiv.org/abs/2601.03506", "authors": ["Zhaofeng Zhong", "Wei Yuan", "Tong Chen", "Xiangyu Zhao", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "title": "Reasoning Pattern Alignment Merging for Adaptive Reasoning", "comment": "16 pages, 4 figures", "summary": "Recent large reasoning models (LRMs) have made substantial progress in complex reasoning tasks, yet they often generate lengthy reasoning paths for every query, incurring unnecessary computation and latency. Existing speed-up approaches typically rely on retraining the model or designing sophisticated prompting, which are either prohibitively expensive or highly sensitive to the input and prompt formulation. In this work, we study model merging as a lightweight alternative for efficient reasoning: by combining a long chain-of-thought (Long-CoT) reasoning model with a Short-CoT instruction model, we obtain an adaptive reasoner without training from scratch or requiring large-scale additional data. Building on this idea, we propose Reasoning Pattern Alignment Merging (RPAM), a layer-wise model merging framework based on feature alignment to facilitate query-adaptive reasoning. RPAM first constructs a small pattern-labeled calibration set that assigns each query an appropriate reasoning pattern. It then optimizes layer-wise merging coefficients by aligning the merged model's intermediate representations with those of the selected model, while a contrastive objective explicitly pushes them away from the non-selected model. Experiments on seven widely used reasoning benchmarks show that RPAM substantially reduces inference cost while maintaining strong performance. Upon article acceptance, we will provide open-source code to reproduce experiments for RPAM.", "AI": {"tldr": "RPAM\u901a\u8fc7\u5c42\u95f4\u6a21\u578b\u878d\u5408\u6280\u672f\uff0c\u5c06\u957f\u63a8\u7406\u94fe\u6a21\u578b\u4e0e\u77ed\u63a8\u7406\u94fe\u6a21\u578b\u5408\u5e76\uff0c\u5b9e\u73b0\u63a8\u7406\u6548\u7387\u63d0\u5347\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5927\u91cf\u989d\u5916\u6570\u636e\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u901a\u5e38\u751f\u6210\u5197\u957f\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u5ef6\u8fdf\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u8981\u4e48\u4f9d\u8d56\u590d\u6742\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u5bf9\u8f93\u5165\u654f\u611f\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u6a21\u5f0f\u5bf9\u9f50\u878d\u5408\uff08RPAM\uff09\u6846\u67b6\uff1a\u9996\u5148\u6784\u5efa\u5c0f\u89c4\u6a21\u6a21\u5f0f\u6807\u6ce8\u6821\u51c6\u96c6\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u5206\u914d\u9002\u5f53\u7684\u63a8\u7406\u6a21\u5f0f\uff1b\u7136\u540e\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u4f18\u5316\u5c42\u95f4\u878d\u5408\u7cfb\u6570\uff0c\u4f7f\u878d\u5408\u6a21\u578b\u7684\u4e2d\u95f4\u8868\u793a\u4e0e\u9009\u5b9a\u6a21\u578b\u5bf9\u9f50\uff0c\u540c\u65f6\u4f7f\u7528\u5bf9\u6bd4\u76ee\u6807\u4f7f\u5176\u8fdc\u79bb\u975e\u9009\u5b9a\u6a21\u578b\u3002", "result": "\u5728\u4e03\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRPAM\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u6a21\u578b\u878d\u5408\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u800c\u65e0\u9700\u4ece\u5934\u8bad\u7ec3\u6216\u5927\u89c4\u6a21\u989d\u5916\u6570\u636e\u3002RPAM\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u7406\u6a21\u5f0f\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2601.04170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04170", "abs": "https://arxiv.org/abs/2601.04170", "authors": ["Abhishek Rath"], "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions", "comment": null, "summary": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).\n  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.\n  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\"\u667a\u80fd\u4f53\u6f02\u79fb\"\u6982\u5ff5\uff0c\u5373\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u957f\u671f\u8fd0\u884c\u4e2d\u884c\u4e3a\u3001\u51b3\u7b56\u8d28\u91cf\u548c\u534f\u8c03\u6027\u9010\u6e10\u9000\u5316\u7684\u73b0\u8c61\uff0c\u5e76\u5f00\u53d1\u4e86\u91cf\u5316\u6846\u67b6\u548c\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u5206\u89e3\u548c\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u957f\u671f\u884c\u4e3a\u7a33\u5b9a\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u9700\u8981\u7406\u89e3\u667a\u80fd\u4f53\u5728\u957f\u65f6\u95f4\u4ea4\u4e92\u5e8f\u5217\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u9000\u5316\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u4e86\u667a\u80fd\u4f53\u6f02\u79fb\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u4e09\u79cd\u6f02\u79fb\u7c7b\u578b\uff1a\u8bed\u4e49\u6f02\u79fb\u3001\u534f\u8c03\u6f02\u79fb\u548c\u884c\u4e3a\u6f02\u79fb\u3002\u5f00\u53d1\u4e86\u667a\u80fd\u4f53\u7a33\u5b9a\u6027\u6307\u6570\uff08ASI\uff09\u4f5c\u4e3a\u590d\u5408\u5ea6\u91cf\u6846\u67b6\uff0c\u5305\u542b12\u4e2a\u7ef4\u5ea6\u3002\u901a\u8fc7\u4eff\u771f\u5206\u6790\u548c\u7406\u8bba\u5efa\u6a21\u9a8c\u8bc1\u6f02\u79fb\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u7f13\u89e3\u7b56\u7565\uff1a\u60c5\u666f\u8bb0\u5fc6\u6574\u5408\u3001\u6f02\u79fb\u611f\u77e5\u8def\u7531\u534f\u8bae\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u951a\u5b9a\u3002", "result": "\u7814\u7a76\u8868\u660e\u672a\u53d7\u63a7\u5236\u7684\u667a\u80fd\u4f53\u6f02\u79fb\u4f1a\u5bfc\u81f4\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\u548c\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u589e\u52a0\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u63d0\u51fa\u7684\u7f13\u89e3\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u6f02\u79fb\u76f8\u5173\u9519\u8bef\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u76d1\u63a7\u3001\u6d4b\u91cf\u548c\u7f13\u89e3\u751f\u4ea7\u7ea7\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u667a\u80fd\u4f53\u6f02\u79fb\u7684\u57fa\u7840\u65b9\u6cd5\u8bba\uff0c\u5bf9\u4f01\u4e1a\u90e8\u7f72\u53ef\u9760\u6027\u548cAI\u5b89\u5168\u7814\u7a76\u5177\u6709\u76f4\u63a5\u610f\u4e49\u3002", "topic": "agent analysis"}}
{"id": "2601.03511", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03511", "abs": "https://arxiv.org/abs/2601.03511", "authors": ["Hossein Hosseini Kasnavieh", "Gholamreza Haffari", "Chris Leckie", "Adel N. Toosi"], "title": "IntroLM: Introspective Language Models via Prefilling-Time Self-Evaluation", "comment": null, "summary": "A major challenge for the operation of large language models (LLMs) is how to predict whether a specific LLM will produce sufficiently high-quality output for a given query. Existing approaches rely on external classifiers, most commonly BERT based models, which suffer from limited context windows, constrained representational capacity, and additional computational overhead. We propose IntroLM, a method that enables causal language models to predict their own output quality during the prefilling phase without affecting generation using introspective tokens. By introducing token conditional LoRA that activates only for the introspective token, the model learns to predict the output quality for a given query while preserving the original backbone behavior and avoiding external evaluators. On question answering benchmarks, IntroLM applied to Qwen3 8B achieves a ROC AUC of 90 precent for success prediction, outperforming a DeBERTa classifier by 14 precent. When integrated into multi model routing systems, IntroLM achieves superior cost performance tradeoffs, reducing latency by up to 33 precent and large model usage by up to 50 precent at matched reliability.", "AI": {"tldr": "IntroLM\u8ba9\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u5728\u9884\u586b\u5145\u9636\u6bb5\u901a\u8fc7\u81ea\u7701token\u9884\u6d4b\u81ea\u8eab\u8f93\u51fa\u8d28\u91cf\uff0c\u65e0\u9700\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u5728\u95ee\u7b54\u57fa\u51c6\u4e0a\u8fbe\u523090%\u7684ROC AUC\uff0c\u4f18\u4e8eDeBERTa\u5206\u7c7b\u566814%\uff0c\u5728\u8def\u7531\u7cfb\u7edf\u4e2d\u964d\u4f4e\u5ef6\u8fdf33%\u548c\u5927\u6a21\u578b\u4f7f\u752850%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\uff08\u5982BERT\u6a21\u578b\uff09\u9884\u6d4bLLM\u8f93\u51fa\u8d28\u91cf\uff0c\u4f46\u5b58\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u3001\u8868\u793a\u80fd\u529b\u53d7\u9650\u548c\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u8ba9LLM\u80fd\u591f\u81ea\u6211\u8bc4\u4f30\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u63d0\u51faIntroLM\u65b9\u6cd5\uff0c\u5728\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u81ea\u7701token\uff0c\u4f7f\u7528token\u6761\u4ef6LoRA\uff08\u4ec5\u5bf9\u81ea\u7701token\u6fc0\u6d3b\uff09\u8ba9\u6a21\u578b\u5b66\u4e60\u9884\u6d4b\u7ed9\u5b9a\u67e5\u8be2\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u4e3b\u5e72\u6a21\u578b\u884c\u4e3a\u4e0d\u53d8\u3002", "result": "\u5728\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e94\u7528\u4e8eQwen3 8B\u7684IntroLM\u5728\u6210\u529f\u9884\u6d4b\u65b9\u9762\u8fbe\u523090%\u7684ROC AUC\uff0c\u6bd4DeBERTa\u5206\u7c7b\u5668\u9ad8\u51fa14%\u3002\u96c6\u6210\u5230\u591a\u6a21\u578b\u8def\u7531\u7cfb\u7edf\u540e\uff0c\u5728\u5339\u914d\u53ef\u9760\u6027\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e\u5ef6\u8fdf\u8fbe33%\uff0c\u5927\u6a21\u578b\u4f7f\u7528\u51cf\u5c1150%\u3002", "conclusion": "IntroLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u8ba9\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u8bc4\u4f30\u8f93\u51fa\u8d28\u91cf\uff0c\u907f\u514d\u4e86\u5916\u90e8\u8bc4\u4f30\u5668\u7684\u9700\u6c42\uff0c\u5728\u8def\u7531\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6210\u672c\u6027\u80fd\u6743\u8861\u3002", "topic": "agent analysis"}}
{"id": "2601.03646", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03646", "abs": "https://arxiv.org/abs/2601.03646", "authors": ["Zhengyi Kwan", "Zhang Wei", "Aik Beng Ng", "Zhengkui Wang", "Simon See"], "title": "ReLA: Representation Learning and Aggregation for Job Scheduling with Reinforcement Learning", "comment": "15 pages", "summary": "Job scheduling is widely used in real-world manufacturing systems to assign ordered job operations to machines under various constraints. Existing solutions remain limited by long running time or insufficient schedule quality, especially when problem scale increases. In this paper, we propose ReLA, a reinforcement-learning (RL) scheduler built on structured representation learning and aggregation. ReLA first learns diverse representations from scheduling entities, including job operations and machines, using two intra-entity learning modules with self-attention and convolution and one inter-entity learning module with cross-attention. These modules are applied in a multi-scale architecture, and their outputs are aggregated to support RL decision-making. Across experiments on small, medium, and large job instances, ReLA achieves the best makespan in most tested settings over the latest solutions. On non-large instances, ReLA reduces the optimality gap of the SOTA baseline by 13.0%, while on large-scale instances it reduces the gap by 78.6%, with the average optimality gaps lowered to 7.3% and 2.1%, respectively. These results confirm that ReLA's learned representations and aggregation provide strong decision support for RL scheduling, and enable fast job completion and decision-making for real-world applications.", "AI": {"tldr": "ReLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f5c\u4e1a\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u5b66\u4e60\u548c\u805a\u5408\u6280\u672f\uff0c\u5728\u591a\u79cd\u89c4\u6a21\u7684\u4f5c\u4e1a\u8c03\u5ea6\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6700\u4f18\u6027\u5dee\u8ddd\u3002", "motivation": "\u73b0\u5b9e\u5236\u9020\u7cfb\u7edf\u4e2d\u7684\u4f5c\u4e1a\u8c03\u5ea6\u95ee\u9898\u9700\u8981\u5c06\u6709\u5e8f\u7684\u4f5c\u4e1a\u64cd\u4f5c\u5206\u914d\u7ed9\u673a\u5668\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u8fd0\u884c\u65f6\u95f4\u6216\u8c03\u5ea6\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u95ee\u9898\u89c4\u6a21\u589e\u5927\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "ReLA\u91c7\u7528\u7ed3\u6784\u5316\u8868\u793a\u5b66\u4e60\u548c\u805a\u5408\u65b9\u6cd5\uff0c\u9996\u5148\u901a\u8fc7\u4e24\u4e2a\u5177\u6709\u81ea\u6ce8\u610f\u529b\u548c\u5377\u79ef\u7684\u5b9e\u4f53\u5185\u5b66\u4e60\u6a21\u5757\u548c\u4e00\u4e2a\u5177\u6709\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u5b9e\u4f53\u95f4\u5b66\u4e60\u6a21\u5757\uff0c\u4ece\u8c03\u5ea6\u5b9e\u4f53\uff08\u4f5c\u4e1a\u64cd\u4f5c\u548c\u673a\u5668\uff09\u4e2d\u5b66\u4e60\u591a\u6837\u5316\u8868\u793a\u3002\u8fd9\u4e9b\u6a21\u5757\u5e94\u7528\u4e8e\u591a\u5c3a\u5ea6\u67b6\u6784\uff0c\u5176\u8f93\u51fa\u88ab\u805a\u5408\u4ee5\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u51b3\u7b56\u3002", "result": "\u5728\u5c0f\u3001\u4e2d\u3001\u5927\u4f5c\u4e1a\u5b9e\u4f8b\u7684\u5b9e\u9a8c\u4e2d\uff0cReLA\u5728\u5927\u591a\u6570\u6d4b\u8bd5\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u5b8c\u5de5\u65f6\u95f4\u3002\u5728\u975e\u5927\u578b\u5b9e\u4f8b\u4e0a\uff0cReLA\u5c06SOTA\u57fa\u7ebf\u7684\u6700\u4f18\u6027\u5dee\u8ddd\u964d\u4f4e\u4e8613.0%\uff1b\u5728\u5927\u578b\u5b9e\u4f8b\u4e0a\uff0c\u5c06\u5dee\u8ddd\u964d\u4f4e\u4e8678.6%\uff0c\u5e73\u5747\u6700\u4f18\u6027\u5dee\u8ddd\u5206\u522b\u964d\u81f37.3%\u548c2.1%\u3002", "conclusion": "ReLA\u5b66\u4e60\u7684\u8868\u793a\u548c\u805a\u5408\u4e3a\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u51b3\u7b56\u652f\u6301\uff0c\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u7684\u4f5c\u4e1a\u5b8c\u6210\u548c\u51b3\u7b56\u5236\u5b9a\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03540", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03540", "abs": "https://arxiv.org/abs/2601.03540", "authors": ["Hongzhi Zhang", "Yuanze Hu", "Tinghai Zhang", "Jia Fu", "Tao Wang", "Junwei Jing", "Zhaoxin Fan", "Qi Wang", "Ruiming Tang", "Han Li", "Guorui Zhou", "Kun Gai"], "title": "DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing", "comment": null, "summary": "The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing \"Oracle Contexts\" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.", "AI": {"tldr": "DeepSynth-Eval\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u4fe1\u606f\u6574\u5408\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5229\u7528\u9ad8\u8d28\u91cf\u7efc\u8ff0\u8bba\u6587\u4f5c\u4e3a\u9ec4\u91d1\u6807\u51c6\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u7814\u7a76\u8bf7\u6c42\u548c\u6784\u5efa\"Oracle Contexts\"\u6765\u9694\u79bb\u5408\u6210\u4e0e\u68c0\u7d22\u566a\u58f0\uff0c\u5b9e\u9a8c\u8868\u660e\u89c4\u5212-\u5199\u4f5c\u5de5\u4f5c\u6d41\u663e\u8457\u4f18\u4e8e\u5355\u8f6e\u751f\u6210\u3002", "motivation": "LLM\u5411\u81ea\u4e3b\u4ee3\u7406\u53d1\u5c55\u63a8\u52a8\u4e86\u6df1\u5ea6\u7814\u7a76\u7684\u8fdb\u5c55\uff0c\u4f46\u68c0\u7d22\u540e\u7684\u5408\u6210\u9636\u6bb5\uff08\u9700\u8981\u6d88\u5316\u5927\u91cf\u4e0a\u4e0b\u6587\u5e76\u5c06\u788e\u7247\u5316\u8bc1\u636e\u6574\u5408\u4e3a\u8fde\u8d2f\u7684\u957f\u7bc7\u62a5\u544a\uff09\u7531\u4e8e\u5f00\u653e\u5f0f\u5199\u4f5c\u7684\u4e3b\u89c2\u6027\u800c\u7f3a\u4e4f\u5ba2\u89c2\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u9ad8\u8d28\u91cf\u7efc\u8ff0\u8bba\u6587\u4f5c\u4e3a\u9ec4\u91d1\u6807\u51c6\uff0c\u9006\u5411\u5de5\u7a0b\u7814\u7a76\u8bf7\u6c42\u5e76\u4ece\u5176\u53c2\u8003\u6587\u732e\u6784\u5efa\"Oracle Contexts\"\u4ee5\u9694\u79bb\u5408\u6210\u4e0e\u68c0\u7d22\u566a\u58f0\u3002\u63d0\u51fa\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u534f\u8bae\uff0c\u4f7f\u7528\u901a\u7528\u68c0\u67e5\u6e05\u5355\uff08\u4e8b\u5b9e\u8986\u76d6\uff09\u548c\u7ea6\u675f\u68c0\u67e5\u6e05\u5355\uff08\u7ed3\u6784\u7ec4\u7ec7\uff09\uff0c\u5c06\u4e3b\u89c2\u5224\u65ad\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u6307\u6807\u3002", "result": "\u572896\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4ece\u6570\u767e\u4e2a\u53c2\u8003\u6587\u732e\u4e2d\u5408\u6210\u4fe1\u606f\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4ee3\u7406\u5f0f\u89c4\u5212-\u5199\u4f5c\u5de5\u4f5c\u6d41\u663e\u8457\u4f18\u4e8e\u5355\u8f6e\u751f\u6210\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u5bf9\u590d\u6742\u7ed3\u6784\u7ea6\u675f\u7684\u9075\u5faa\u5ea6\u3002", "conclusion": "DeepSynth-Eval\u57fa\u51c6\u586b\u8865\u4e86LLM\u4ee3\u7406\u4fe1\u606f\u6574\u5408\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u89c4\u5212-\u5199\u4f5c\u5de5\u4f5c\u6d41\u5728\u590d\u6742\u5408\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5ba2\u89c2\u8bc4\u4f30\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.03542", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03542", "abs": "https://arxiv.org/abs/2601.03542", "authors": ["Xukai Liu", "Ye Liu", "Jipeng Zhang", "Yanghai Zhang", "Kai Zhang", "Qi Liu"], "title": "Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models", "comment": "16 pages, 18 figures", "summary": "Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \\emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \\emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \\emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.", "AI": {"tldr": "LLM\u5728\u591a\u8df3\u63a8\u7406\u4e2d\uff0c\u540e\u7eed\u7b54\u6848\u5b9e\u4f53\u53ef\u80fd\u6bd4\u6865\u63a5\u5b9e\u4f53\u66f4\u65e9\u53ef\u89e3\u7801\uff0c\u51fa\u73b0\u5c42\u5e8f\u53cd\u8f6c\u73b0\u8c61\uff0c\u4f5c\u8005\u63d0\u51fa\u6982\u7387\u6027\u56de\u5fc6-\u63d0\u53d6\u6846\u67b6\u89e3\u91ca\u6b64\u884c\u4e3a\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u591a\u8df3\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5185\u90e8\u5982\u4f55\u7ec4\u5408\u591a\u4e2a\u4e8b\u5b9e\u4ecd\u4e0d\u6e05\u695a\u3002\u73b0\u6709\u7814\u7a76\u63d0\u51fa\u7684\u8df3\u5bf9\u9f50\u7535\u8def\u5047\u8bbe\u8ba4\u4e3a\u6865\u63a5\u5b9e\u4f53\u5728\u5c42\u95f4\u987a\u5e8f\u8ba1\u7b97\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u4e00\u5047\u8bbe\u4e0d\u80fd\u63a8\u5e7f\u5230\u771f\u5b9e\u4e16\u754c\u591a\u8df3\u67e5\u8be2\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u6982\u7387\u6027\u56de\u5fc6-\u63d0\u53d6\u6846\u67b6\uff0c\u5c06\u591a\u8df3\u63a8\u7406\u5efa\u6a21\u4e3a\u6d45\u5c42MLP\u5c42\u7684\u5e7f\u6cdb\u6982\u7387\u6027\u56de\u5fc6\uff0c\u968f\u540e\u662f\u6df1\u5c42\u6ce8\u610f\u529b\u5c42\u7684\u9009\u62e9\u6027\u63d0\u53d6\u3002\u901a\u8fc7\u7cfb\u7edf\u63a2\u6d4b\u5206\u6790\u3001\u91cd\u65b0\u89e3\u91ca\u5148\u524d\u5c42\u89e3\u7801\u8bc1\u636e\u3001\u89e3\u91ca\u601d\u7ef4\u94fe\u589e\u76ca\u7b49\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u5c42\u5e8f\u53cd\u8f6c\u73b0\u8c61\uff1a\u540e\u7eed\u7b54\u6848\u5b9e\u4f53\u6bd4\u6865\u63a5\u5b9e\u4f53\u66f4\u65e9\u53ef\u89e3\u7801\uff0c\u4e14\u968f\u7740\u603b\u8df3\u6570\u589e\u52a0\u800c\u52a0\u5f3a\u3002\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u89e3\u91ca\u601d\u7ef4\u94fe\u589e\u76ca\uff0c\u5e76\u4e3a\u591a\u8df3\u5931\u8d25\u63d0\u4f9b\u673a\u5236\u8bca\u65ad\u3002", "conclusion": "\u591a\u8df3\u63a8\u7406\u4e0d\u9075\u5faa\u7b80\u5355\u7684\u8df3\u5bf9\u9f50\u5047\u8bbe\uff0c\u800c\u662f\u901a\u8fc7\u6982\u7387\u6027\u56de\u5fc6-\u63d0\u53d6\u8fc7\u7a0b\u5b9e\u73b0\u3002\u8be5\u6846\u67b6\u4e3a\u7406\u89e3LLM\u5185\u90e8\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u80fd\u89e3\u91ca\u5b9e\u9645\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\u3002", "topic": "agent analysis"}}
{"id": "2601.03543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03543", "abs": "https://arxiv.org/abs/2601.03543", "authors": ["Ye Shen", "Dun Pei", "Yiqiu Guo", "Junying Wang", "Yijin Guo", "Zicheng Zhang", "Qi Jia", "Jun Zhou", "Guangtao Zhai"], "title": "EvolMem: A Cognitive-Driven Benchmark for Multi-Session Dialogue Memory", "comment": "14 pages, 7 figures, 8 tables", "summary": "Despite recent advances in understanding and leveraging long-range conversational memory, existing benchmarks still lack systematic evaluation of large language models(LLMs) across diverse memory dimensions, particularly in multi-session settings. In this work, we propose EvolMem, a new benchmark for assessing multi-session memory capabilities of LLMs and agent systems. EvolMem is grounded in cognitive psychology and encompasses both declarative and non-declarative memory, further decomposed into multiple fine-grained abilities. To construct the benchmark, we introduce a hybrid data synthesis framework that consists of topic-initiated generation and narrative-inspired transformations. This framework enables scalable generation of multi-session conversations with controllable complexity, accompanied by sample-specific evaluation guidelines. Extensive evaluation reveals that no LLM consistently outperforms others across all memory dimensions. Moreover, agent memory mechanisms do not necessarily enhance LLMs' capabilities and often exhibit notable efficiency limitations. Data and code will be released at https://github.com/shenye7436/EvolMem.", "AI": {"tldr": "EvolMem\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u4f1a\u8bdd\u8bb0\u5fc6\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u6784\u5efa\uff0c\u6db5\u76d6\u9648\u8ff0\u6027\u548c\u975e\u9648\u8ff0\u6027\u8bb0\u5fc6\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u5408\u6210\u6846\u67b6\u751f\u6210\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u591a\u4f1a\u8bdd\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u53d1\u73b0\u6ca1\u6709LLM\u5728\u6240\u6709\u8bb0\u5fc6\u7ef4\u5ea6\u4e0a\u8868\u73b0\u4e00\u81f4\u4f18\u5f02\uff0c\u4e14\u4ee3\u7406\u8bb0\u5fc6\u673a\u5236\u4e0d\u4e00\u5b9a\u80fd\u589e\u5f3aLLM\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4f1a\u8bdd\u8bbe\u7f6e\u4e0b\u591a\u6837\u5316\u8bb0\u5fc6\u7ef4\u5ea6\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u8ba4\u77e5\u5fc3\u7406\u5b66\u6846\u67b6\u4e0b\u7684\u5168\u9762\u8bb0\u5fc6\u80fd\u529b\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faEvolMem\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u8ba4\u77e5\u5fc3\u7406\u5b66\u6784\u5efa\uff0c\u6db5\u76d6\u9648\u8ff0\u6027\u548c\u975e\u9648\u8ff0\u6027\u8bb0\u5fc6\u7684\u591a\u4e2a\u7ec6\u7c92\u5ea6\u80fd\u529b\u3002\u91c7\u7528\u6df7\u5408\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u5305\u62ec\u4e3b\u9898\u542f\u52a8\u751f\u6210\u548c\u53d9\u4e8b\u542f\u53d1\u8f6c\u6362\uff0c\u53ef\u6269\u5c55\u5730\u751f\u6210\u5177\u6709\u53ef\u63a7\u590d\u6742\u5ea6\u7684\u591a\u4f1a\u8bdd\u5bf9\u8bdd\uff0c\u5e76\u914d\u6709\u6837\u672c\u7279\u5b9a\u7684\u8bc4\u4f30\u6307\u5357\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff1a1\uff09\u6ca1\u6709LLM\u5728\u6240\u6709\u8bb0\u5fc6\u7ef4\u5ea6\u4e0a\u4e00\u81f4\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1b2\uff09\u4ee3\u7406\u8bb0\u5fc6\u673a\u5236\u4e0d\u4e00\u5b9a\u80fd\u589e\u5f3aLLM\u7684\u80fd\u529b\uff0c\u4e14\u901a\u5e38\u8868\u73b0\u51fa\u663e\u8457\u7684\u6548\u7387\u9650\u5236\u3002", "conclusion": "EvolMem\u4e3a\u8bc4\u4f30LLM\u548c\u4ee3\u7406\u7cfb\u7edf\u7684\u591a\u4f1a\u8bdd\u8bb0\u5fc6\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8bb0\u5fc6\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4ee3\u7406\u8bb0\u5fc6\u673a\u5236\u7684\u6548\u7387\u95ee\u9898\u3002", "topic": "agent analysis"}}
{"id": "2601.03703", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03703", "abs": "https://arxiv.org/abs/2601.03703", "authors": ["Lang Cao", "Hui Ruan", "Yongqian Li", "Peng Chao", "Wu Ning", "Haonan Song", "Renhong Chen", "Yitong Li"], "title": "TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL", "comment": null, "summary": "Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.", "AI": {"tldr": "TreeAdv\u63d0\u51fa\u6811\u7ed3\u6784\u4f18\u52bf\u91cd\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u7fa4\u4f53rollout\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u6bd4GRPO\u548cGSPO\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u66f4\u597d", "motivation": "\u6807\u51c6GRPO\u5c06\u6bcf\u4e2arollout\u8f68\u8ff9\u89c6\u4e3a\u72ec\u7acb\u5e8f\u5217\uff0c\u4e3a\u6240\u6709token\u5206\u914d\u5355\u4e00\u5e8f\u5217\u7ea7\u4f18\u52bf\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u548c\u957f\u5ea6\u504f\u5dee\uff08\u5197\u957f\u4f46\u903b\u8f91\u6df1\u5ea6\u4e0d\u8db3\u7684\u601d\u7ef4\u94fe\uff09", "method": "TreeAdv\u6784\u5efa\u57fa\u4e8e\u71b5\u9a71\u52a8\u91c7\u6837\u7684\u6811\u7ed3\u6784\u7fa4\u4f53\uff08\u68ee\u6797\uff09\uff0c\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u5904\u5206\u652f\uff0c\u5171\u4eab\u4f4e\u4e0d\u786e\u5b9a\u6027token\uff0c\u7136\u540e\u901a\u8fc7\u91cd\u5206\u914d\u5b8c\u6574rollout\u7684\u4f18\u52bf\u6765\u805a\u5408token\u7ea7\u4f18\u52bf", "result": "\u572810\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTreeAdv\u59cb\u7ec8\u4f18\u4e8eGRPO\u548cGSPO\uff0c\u540c\u65f6\u5728\u76f8\u540c\u76d1\u7763\u3001\u6570\u636e\u548c\u89e3\u7801\u9884\u7b97\u4e0b\u4f7f\u7528\u66f4\u5c11\u7684\u751f\u6210token", "conclusion": "TreeAdv\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u7fa4\u4f53rollout\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u7fa4\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u548c\u957f\u5ea6\u504f\u5dee\u95ee\u9898", "topic": "agentic reinforcement learning"}}
{"id": "2601.03605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03605", "abs": "https://arxiv.org/abs/2601.03605", "authors": ["Hui Huang", "Muyun Yang", "Yuki Arase"], "title": "DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier", "comment": null, "summary": "Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.", "AI": {"tldr": "DiVA\u6846\u67b6\u7ed3\u5408\u751f\u6210\u6a21\u578b\u7684\u641c\u7d22\u80fd\u529b\u548c\u5224\u522b\u6a21\u578b\u7684\u8bc4\u5206\u80fd\u529b\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4e8b\u5b9e\u6027\u9a8c\u8bc1\uff0c\u5728FGVeriBench\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6027\u9a8c\u8bc1\u7814\u7a76\u4e3b\u8981\u8fdb\u884c\u4e8c\u5143\u5224\u65ad\uff08\u6b63\u786e/\u9519\u8bef\uff09\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u7a0b\u5ea6\u7684\u9519\u8bef\u4e25\u91cd\u6027\uff0c\u9650\u5236\u4e86\u5728\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u548c\u504f\u597d\u4f18\u5316\u7b49\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faAgentic Discriminative Verifier (DiVA)\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u7684\u4ee3\u7406\u641c\u7d22\u80fd\u529b\u548c\u5224\u522b\u6a21\u578b\u7684\u7cbe\u786e\u8bc4\u5206\u80fd\u529b\uff0c\u5e76\u6784\u5efaFGVeriBench\u57fa\u51c6\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4e8b\u5b9e\u6027\u9a8c\u8bc1\u6d4b\u8bd5\u3002", "result": "\u5728FGVeriBench\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDiVA\u5728\u901a\u7528\u95ee\u9898\u548c\u591a\u8df3\u95ee\u9898\u7684\u4e8b\u5b9e\u6027\u9a8c\u8bc1\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiVA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u548c\u5224\u522b\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u4e8b\u5b9e\u6027\u9a8c\u8bc1\u7684\u6311\u6218\uff0c\u4e3aLLM\u4e8b\u5b9e\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u5de5\u5177\u3002", "topic": "agent analysis"}}
{"id": "2601.03715", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03715", "abs": "https://arxiv.org/abs/2601.03715", "authors": ["Weijie Shi", "Yanxi Chen", "Zexi Li", "Xuchen Pan", "Yuchang Sun", "Jiajie Xu", "Xiaofang Zhou", "Yaliang Li"], "title": "R$^3$L: Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification", "comment": null, "summary": "Reinforcement learning drives recent advances in LLM reasoning and agentic capabilities, yet current approaches struggle with both exploration and exploitation. Exploration suffers from low success rates on difficult tasks and high costs of repeated rollouts from scratch. Exploitation suffers from coarse credit assignment and training instability: Trajectory-level rewards penalize valid prefixes for later errors, and failure-dominated groups overwhelm the few positive signals, leaving optimization without constructive direction. To this end, we propose R$^3$L, Reflect-then-Retry Reinforcement Learning with Language-Guided Exploration, Pivotal Credit, and Positive Amplification. To synthesize high-quality trajectories, R$^3$L shifts from stochastic sampling to active synthesis via reflect-then-retry, leveraging language feedback to diagnose errors, transform failed attempts into successful ones, and reduce rollout costs by restarting from identified failure points. With errors diagnosed and localized, Pivotal Credit Assignment updates only the diverging suffix where contrastive signals exist, excluding the shared prefix from gradient update. Since failures dominate on difficult tasks and reflect-then-retry produces off-policy data, risking training instability, Positive Amplification upweights successful trajectories to ensure positive signals guide the optimization process. Experiments on agentic and reasoning tasks demonstrate 5\\% to 52\\% relative improvements over baselines while maintaining training stability. Our code is released at https://github.com/shiweijiezero/R3L.", "AI": {"tldr": "R\u00b3L\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u601d-\u91cd\u8bd5\u673a\u5236\u3001\u5173\u952e\u4fe1\u7528\u5206\u914d\u548c\u6b63\u4fe1\u53f7\u653e\u5927\uff0c\u89e3\u51b3\u4e86LLM\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u96be\u9898\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728LLM\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u63a2\u7d22\u65b9\u9762\uff0c\u56f0\u96be\u4efb\u52a1\u6210\u529f\u7387\u4f4e\u4e14\u4ece\u5934\u5f00\u59cb\u91cd\u590drollout\u6210\u672c\u9ad8\uff1b\u5229\u7528\u65b9\u9762\uff0c\u8f68\u8ff9\u7ea7\u5956\u52b1\u60e9\u7f5a\u6709\u6548\u524d\u7f00\uff0c\u5931\u8d25\u4e3b\u5bfc\u7684\u8bad\u7ec3\u6570\u636e\u6df9\u6ca1\u5c11\u6570\u6b63\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4f18\u5316\u7f3a\u4e4f\u5efa\u8bbe\u6027\u65b9\u5411\u3002", "method": "R\u00b3L\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u53cd\u601d-\u91cd\u8bd5\u8bed\u8a00\u5f15\u5bfc\u63a2\u7d22\uff1a\u901a\u8fc7\u8bed\u8a00\u53cd\u9988\u8bca\u65ad\u9519\u8bef\uff0c\u5c06\u5931\u8d25\u5c1d\u8bd5\u8f6c\u5316\u4e3a\u6210\u529f\uff0c\u4ece\u5931\u8d25\u70b9\u91cd\u542f\u51cf\u5c11rollout\u6210\u672c\uff1b2\uff09\u5173\u952e\u4fe1\u7528\u5206\u914d\uff1a\u53ea\u66f4\u65b0\u5b58\u5728\u5bf9\u6bd4\u4fe1\u53f7\u7684\u5206\u6b67\u540e\u7f00\uff0c\u6392\u9664\u5171\u4eab\u524d\u7f00\uff1b3\uff09\u6b63\u4fe1\u53f7\u653e\u5927\uff1a\u5bf9\u6210\u529f\u8f68\u8ff9\u52a0\u6743\uff0c\u786e\u4fdd\u6b63\u4fe1\u53f7\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u667a\u80fd\u4f53\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u4e865%\u523052%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "R\u00b3L\u901a\u8fc7\u4e3b\u52a8\u8f68\u8ff9\u5408\u6210\u3001\u7cbe\u786e\u4fe1\u7528\u5206\u914d\u548c\u6b63\u4fe1\u53f7\u653e\u5927\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728LLM\u63a8\u7406\u548c\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03641", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03641", "abs": "https://arxiv.org/abs/2601.03641", "authors": ["Zheng Wu", "Xingyu Lou", "Xinbei Ma", "Yansi Li", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhuosheng Zhang"], "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning", "comment": null, "summary": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.", "AI": {"tldr": "Agent-Dice\uff1a\u57fa\u4e8e\u65b9\u5411\u5171\u8bc6\u8bc4\u4f30\u7684\u53c2\u6570\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u5171\u8bc6\u8fc7\u6ee4\u548c\u66f2\u7387\u91cd\u8981\u6027\u52a0\u6743\u89e3\u51b3LLM\u667a\u80fd\u4f53\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883", "motivation": "LLM\u667a\u80fd\u4f53\u5728\u4e0e\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u65f6\u9762\u4e34\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u800c\u4e0d\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u7684\u6311\u6218\uff0c\u5373\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e00\u56f0\u5883\u6e90\u4e8e\u672a\u80fd\u660e\u786e\u533a\u5206\u8de8\u4efb\u52a1\u5171\u4eab\u7684\u901a\u7528\u77e5\u8bc6\u548c\u4efb\u52a1\u7279\u5b9a\u5e72\u6270\u5f15\u5165\u7684\u51b2\u7a81\u77e5\u8bc6\u3002", "method": "\u63d0\u51faAgent-Dice\u53c2\u6570\u878d\u5408\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\u89e3\u8026\u77e5\u8bc6\u66f4\u65b0\uff1a1\uff09\u51e0\u4f55\u5171\u8bc6\u8fc7\u6ee4\u4fee\u526a\u51b2\u7a81\u68af\u5ea6\uff1b2\uff09\u57fa\u4e8e\u66f2\u7387\u7684\u91cd\u8981\u6027\u52a0\u6743\u653e\u5927\u5171\u4eab\u8bed\u4e49\u3002\u63d0\u4f9b\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u878d\u5408\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "result": "\u5728GUI\u667a\u80fd\u4f53\u548c\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAgent-Dice\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u53c2\u6570\u66f4\u65b0\u3002", "conclusion": "Agent-Dice\u901a\u8fc7\u660e\u786e\u533a\u5206\u5171\u4eab\u77e5\u8bc6\u548c\u51b2\u7a81\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u4e3a\u667a\u80fd\u4f53\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u6d1e\u5bdf\u548c\u5b9e\u7528\u6846\u67b6\u3002", "topic": "agent analysis"}}
{"id": "2601.03895", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03895", "abs": "https://arxiv.org/abs/2601.03895", "authors": ["Chi Liu", "Xin Chen"], "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training", "comment": "10 pages, 4 figures", "summary": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.", "AI": {"tldr": "\u63d0\u51faABC-GRPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8fb9\u754c\u88c1\u526a\u6539\u8fdbGRPO\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4fdd\u6301\u66f4\u9ad8\u71b5\u503c\u9632\u6b62\u8fc7\u65e9\u6536\u655b\u3002", "motivation": "\u5206\u6790\u53d1\u73b0GRPO\u7684\u88c1\u526a\u673a\u5236\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4e0d\u591f\u4f18\u5316\uff0c\u901a\u8fc7\u9002\u5f53\u4fee\u6539\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8fb9\u754c\u88c1\u526aGRPO\uff08ABC-GRPO\uff09\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u81ea\u9002\u5e94\u65b9\u5f0f\u6539\u8fdb\u539f\u59cbGRPO\u6846\u67b6\u7684\u88c1\u526a\u673a\u5236\u3002", "result": "\u5728Qwen3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cABC-GRPO\u8868\u73b0\u4f18\u4e8e\u6807\u51c6GRPO\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u663e\u8457\u66f4\u9ad8\u7684\u71b5\u503c\u3002", "conclusion": "ABC-GRPO\u901a\u8fc7\u6539\u8fdb\u88c1\u526a\u673a\u5236\u63d0\u5347\u4e86GRPO\u7684\u6027\u80fd\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u7f13\u89e3\u4e86\u8fc7\u65e9\u6536\u655b\u95ee\u9898\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03775", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03775", "abs": "https://arxiv.org/abs/2601.03775", "authors": ["Pingjun Hong", "Benjamin Roth"], "title": "Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations", "comment": null, "summary": "Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLM\u81ea\u6211\u89e3\u91ca\u662f\u5426\u80fd\u5e2e\u52a9\u7528\u6237\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\uff0c\u901a\u8fc7StrategyQA\u8bc4\u4f30\u4eba\u7c7b\u548cLLM\u5728\u6709\u65e0\u89e3\u91ca\u7684\u60c5\u51b5\u4e0b\u5bf9\u53cd\u4e8b\u5b9e\u95ee\u9898\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u53d1\u73b0\u89e3\u91ca\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u4f46\u6548\u679c\u53d7\u6270\u52a8\u7b56\u7565\u548c\u8bc4\u4f30\u8005\u80fd\u529b\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1LLM\u80fd\u751f\u6210\u81ea\u6211\u89e3\u91ca\uff0c\u4f46\u5148\u524d\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u7406\u7531\u53ef\u80fd\u65e0\u6cd5\u53ef\u9760\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u51b3\u7b56\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u4e9b\u89e3\u91ca\u662f\u5426\u4ecd\u80fd\u5e2e\u52a9\u7528\u6237\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u901a\u8fc7\u53cd\u4e8b\u5b9e\u53ef\u6a21\u62df\u6027\u8fd9\u4e00\u64cd\u4f5c\u5316\u6307\u6807\u3002", "method": "\u4f7f\u7528StrategyQA\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4eba\u7c7b\u548cLLM\u8bc4\u4f30\u8005\u5728\u6709\u65e0\u8bbf\u95ee\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u6216\u4e8b\u540e\u89e3\u91ca\u7684\u60c5\u51b5\u4e0b\uff0c\u9884\u6d4b\u6a21\u578b\u5bf9\u53cd\u4e8b\u5b9e\u540e\u7eed\u95ee\u9898\u7b54\u6848\u7684\u80fd\u529b\u3002\u6bd4\u8f83LLM\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u4e0e\u57fa\u4e8e\u8bed\u7528\u5b66\u7684\u6270\u52a8\u4f5c\u4e3a\u8bc4\u4f30\u89e3\u91ca\u6f5c\u5728\u6709\u7528\u6027\u7684\u66ff\u4ee3\u6d4b\u8bd5\u6848\u4f8b\u6784\u5efa\u65b9\u6cd5\u3002", "result": "\u81ea\u6211\u89e3\u91ca\u6301\u7eed\u63d0\u9ad8\u4e86LLM\u8bc4\u4f30\u8005\u548c\u4eba\u7c7b\u7684\u6a21\u62df\u51c6\u786e\u6027\uff0c\u4f46\u589e\u76ca\u7a0b\u5ea6\u548c\u7a33\u5b9a\u6027\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u6270\u52a8\u7b56\u7565\u548c\u8bc4\u4f30\u8005\u80fd\u529b\u3002\u4eba\u7c7b\u7528\u6237\u5728\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\u65f6\u64b0\u5199\u7684\u81ea\u7531\u6587\u672c\u7406\u7531\u7684\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u8bbf\u95ee\u89e3\u91ca\u6709\u52a9\u4e8e\u4eba\u7c7b\u5bf9\u6270\u52a8\u95ee\u9898\u5f62\u6210\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002", "conclusion": "LLM\u7684\u81ea\u6211\u89e3\u91ca\u867d\u7136\u53ef\u80fd\u4e0d\u5b8c\u5168\u53cd\u6620\u771f\u5b9e\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f46\u5728\u5e2e\u52a9\u7528\u6237\u9884\u6d4b\u6a21\u578b\u884c\u4e3a\u65b9\u9762\u5177\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u53cd\u4e8b\u5b9e\u573a\u666f\u4e0b\u3002\u89e3\u91ca\u7684\u6709\u7528\u6027\u53d7\u5230\u6d4b\u8bd5\u6848\u4f8b\u6784\u5efa\u65b9\u5f0f\u548c\u8bc4\u4f30\u8005\u80fd\u529b\u7684\u5f71\u54cd\u3002", "topic": "agent analysis"}}
{"id": "2601.03779", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03779", "abs": "https://arxiv.org/abs/2601.03779", "authors": ["Marco Baroni", "Emily Cheng", "Iria deDios-Flores", "Francesca Franzon"], "title": "Tracing the complexity profiles of different linguistic phenomena through the intrinsic dimension of LLM representations", "comment": null, "summary": "We explore the intrinsic dimension (ID) of LLM representations as a marker of linguistic complexity, asking if different ID profiles across LLM layers differentially characterize formal and functional complexity. We find the formal contrast between sentences with multiple coordinated or subordinated clauses to be reflected in ID differences whose onset aligns with a phase of more abstract linguistic processing independently identified in earlier work. The functional contrasts between sentences characterized by right branching vs. center embedding or unambiguous vs. ambiguous relative clause attachment are also picked up by ID, but in a less marked way, and they do not correlate with the same processing phase. Further experiments using representational similarity and layer ablation confirm the same trends. We conclude that ID is a useful marker of linguistic complexity in LLMs, that it allows to differentiate between different types of complexity, and that it points to similar stages of linguistic processing across disparate LLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86LLM\u8868\u793a\u7684\u5185\u5728\u7ef4\u5ea6\u4f5c\u4e3a\u8bed\u8a00\u590d\u6742\u6027\u7684\u6807\u8bb0\uff0c\u53d1\u73b0\u4e0d\u540cLLM\u5c42\u7684\u5185\u5728\u7ef4\u5ea6\u7279\u5f81\u80fd\u591f\u533a\u5206\u5f62\u5f0f\u590d\u6742\u6027\u548c\u529f\u80fd\u590d\u6742\u6027\uff0c\u5e76\u4e0e\u8bed\u8a00\u5904\u7406\u9636\u6bb5\u76f8\u5173\u8054\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLM\u8868\u793a\u7684\u5185\u5728\u7ef4\u5ea6\u662f\u5426\u80fd\u591f\u4f5c\u4e3a\u8bed\u8a00\u590d\u6742\u6027\u7684\u6709\u6548\u6807\u8bb0\uff0c\u4ee5\u53ca\u4e0d\u540cLLM\u5c42\u7684\u5185\u5728\u7ef4\u5ea6\u7279\u5f81\u662f\u5426\u80fd\u591f\u533a\u5206\u4e0d\u540c\u7c7b\u578b\uff08\u5f62\u5f0f\u548c\u529f\u80fd\uff09\u7684\u8bed\u8a00\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528\u5185\u5728\u7ef4\u5ea6\u5206\u6790LLM\u8868\u793a\uff0c\u6bd4\u8f83\u4e0d\u540c\u53e5\u5b50\u7ed3\u6784\uff08\u591a\u4ece\u53e5\u534f\u8c03/\u4ece\u5c5e\u3001\u53f3\u5206\u652fvs\u4e2d\u5fc3\u5d4c\u5165\u3001\u660e\u786evs\u6a21\u7cca\u5173\u7cfb\u4ece\u53e5\u9644\u7740\uff09\u7684\u5185\u5728\u7ef4\u5ea6\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u76f8\u4f3c\u6027\u548c\u5c42\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5f62\u5f0f\u590d\u6742\u6027\uff08\u591a\u4ece\u53e5\u7ed3\u6784\uff09\u7684\u5185\u5728\u7ef4\u5ea6\u5dee\u5f02\u4e0e\u5148\u524d\u5de5\u4f5c\u4e2d\u786e\u5b9a\u7684\u66f4\u62bd\u8c61\u8bed\u8a00\u5904\u7406\u9636\u6bb5\u5bf9\u9f50\uff1b2\uff09\u529f\u80fd\u590d\u6742\u6027\u7684\u5185\u5728\u7ef4\u5ea6\u5dee\u5f02\u8f83\u4e0d\u660e\u663e\uff0c\u4e14\u4e0d\u4e0e\u76f8\u540c\u5904\u7406\u9636\u6bb5\u76f8\u5173\uff1b3\uff09\u8868\u793a\u76f8\u4f3c\u6027\u548c\u5c42\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u76f8\u540c\u8d8b\u52bf\u3002", "conclusion": "\u5185\u5728\u7ef4\u5ea6\u662fLLM\u4e2d\u8bed\u8a00\u590d\u6742\u6027\u7684\u6709\u7528\u6807\u8bb0\uff0c\u80fd\u591f\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u590d\u6742\u6027\uff0c\u5e76\u5728\u4e0d\u540cLLM\u4e2d\u6307\u5411\u76f8\u4f3c\u7684\u8bed\u8a00\u5904\u7406\u9636\u6bb5\u3002", "topic": "agent analysis"}}
{"id": "2601.04171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04171", "abs": "https://arxiv.org/abs/2601.04171", "authors": ["Mohit Raghavendra", "Anisha Gunjal", "Bing Liu", "Yunzhong He"], "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents", "comment": "31 pages, 11 Figures", "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.", "AI": {"tldr": "\u63d0\u51faAgentic Rubrics\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u4ee3\u7406\u4e0e\u4ee3\u7801\u5e93\u4ea4\u4e92\u521b\u5efa\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u68c0\u67e5\u6e05\u5355\uff0c\u65e0\u9700\u6267\u884c\u6d4b\u8bd5\u5373\u53ef\u8bc4\u4f30\u4ee3\u7801\u8865\u4e01\uff0c\u5728SWE-Bench Verified\u4e0a\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u9a8c\u8bc1\u4e3b\u8981\u4f9d\u8d56\u4ee3\u7801\u6267\u884c\uff0c\u4f46\u73af\u5883\u8bbe\u7f6e\u5f00\u9500\u5927\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u66ff\u4ee3\u65b9\u6cd5\u5982\u8865\u4e01\u5206\u7c7b\u5668\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u4e14\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\u4ee3\u7406\u4e0e\u4ee3\u7801\u5e93\u4ea4\u4e92\u521b\u5efa\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u68c0\u67e5\u6e05\u5355\uff08rubric checklist\uff09\uff0c\u7136\u540e\u6839\u636e\u8be5\u6e05\u5355\u5bf9\u5019\u9009\u8865\u4e01\u8fdb\u884c\u8bc4\u5206\uff0c\u65e0\u9700\u6d4b\u8bd5\u6267\u884c\u3002", "result": "\u5728SWE-Bench Verified\u5e76\u884cTTS\u8bc4\u4f30\u4e2d\uff0cQwen3-Coder-30B-A3B\u8fbe\u523054.2%\uff0cQwen3-32B\u8fbe\u523040.6%\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u81f3\u5c11\u63d0\u9ad83.5\u4e2a\u767e\u5206\u70b9\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u4ee3\u7406\u4e0a\u4e0b\u6587\u6536\u96c6\u5bf9\u4ea7\u751f\u4ee3\u7801\u5e93\u7279\u5b9a\u3001\u660e\u786e\u6807\u51c6\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "Agentic Rubrics\u4e3aSWE\u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7ec6\u7c92\u5ea6\u7684\u9a8c\u8bc1\u4fe1\u53f7\uff0c\u8bc4\u5206\u4e0e\u771f\u5b9e\u6d4b\u8bd5\u4e00\u81f4\uff0c\u5e76\u80fd\u6355\u83b7\u6d4b\u8bd5\u672a\u8986\u76d6\u7684\u95ee\u9898\u3002", "topic": "swe application"}}
{"id": "2601.03785", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03785", "abs": "https://arxiv.org/abs/2601.03785", "authors": ["Dehao Tao", "Guoliang Ma", "Yongfeng Huang", "Minghu Jiang"], "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents", "comment": null, "summary": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.", "AI": {"tldr": "Membox\u662f\u4e00\u79cd\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7Topic Loom\u5c06\u8fde\u7eed\u540c\u4e3b\u9898\u5bf9\u8bdd\u56de\u5408\u5206\u7ec4\u4e3a\"\u8bb0\u5fc6\u76d2\"\uff0c\u518d\u901a\u8fc7Trace Weaver\u8fde\u63a5\u6210\u957f\u671f\u4e8b\u4ef6\u65f6\u95f4\u7ebf\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u8bb0\u5fc6\u7cfb\u7edf\u9075\u5faa\u788e\u7247\u5316-\u8865\u507f\u8303\u5f0f\uff1a\u5148\u5c06\u5bf9\u8bdd\u6d41\u5206\u89e3\u4e3a\u5b64\u7acb\u8bdd\u8bed\u5b58\u50a8\uff0c\u518d\u5c1d\u8bd5\u901a\u8fc7\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u6062\u590d\u8fde\u8d2f\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u53ef\u9006\u5730\u7834\u574f\u4e86\u53d9\u4e8b\u548c\u56e0\u679c\u6d41\uff0c\u540c\u65f6\u4f7f\u68c0\u7d22\u504f\u5411\u8bcd\u6c47\u76f8\u4f3c\u6027\u3002", "method": "\u63d0\u51famembox\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784\uff1a1) Topic Loom\u4ee5\u6ed1\u52a8\u7a97\u53e3\u65b9\u5f0f\u6301\u7eed\u76d1\u63a7\u5bf9\u8bdd\uff0c\u5c06\u8fde\u7eed\u540c\u4e3b\u9898\u56de\u5408\u5206\u7ec4\u4e3a\u8fde\u8d2f\u7684\"\u8bb0\u5fc6\u76d2\"\uff1b2) Trace Weaver\u5c06\u5bc6\u5c01\u7684\u8bb0\u5fc6\u76d2\u94fe\u63a5\u6210\u957f\u671f\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u8f68\u8ff9\uff0c\u6062\u590d\u8de8\u4e0d\u8fde\u7eed\u6027\u7684\u5b8f\u89c2\u4e3b\u9898\u91cd\u73b0\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMembox\u5728\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe68%\u7684F1\u6539\u8fdb\uff0c\u4f18\u4e8eMem0\u3001A-MEM\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\u4ec5\u4f7f\u7528\u73b0\u6709\u65b9\u6cd5\u6240\u9700\u4e0a\u4e0b\u6587token\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4f18\u8d8a\u5e73\u8861\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e3b\u9898\u8fde\u7eed\u6027\uff0cMembox\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba4\u77e5\u52a8\u673a\u673a\u5236\uff0c\u53ef\u540c\u65f6\u589e\u5f3aLLM\u4ee3\u7406\u7684\u8fde\u8d2f\u6027\u548c\u6548\u7387\u3002", "topic": "agent analysis"}}
{"id": "2601.03790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03790", "abs": "https://arxiv.org/abs/2601.03790", "authors": ["Zhongtao Miao", "Kaiyan Zhao", "Masaaki Nagata", "Yoshimasa Tsuruoka"], "title": "NeoAMT: Neologism-Aware Agentic Machine Translation with Reinforcement Learning", "comment": null, "summary": "Neologism-aware machine translation aims to translate source sentences containing neologisms into target languages. This field remains underexplored compared with general machine translation (MT). In this paper, we propose an agentic framework, NeoAMT, for neologism-aware machine translation using a Wiktionary search tool. Specifically, we first create a new dataset for neologism-aware machine translation and develop a search tool based on Wiktionary. The new dataset covers 16 languages and 75 translation directions and is derived from approximately 10 million records of an English Wiktionary dump. The retrieval corpus of the search tool is also constructed from around 3 million cleaned records of the Wiktionary dump. We then use it for training the translation agent with reinforcement learning (RL) and evaluating the accuracy of neologism-aware machine translation. Based on this, we also propose an RL training framework that contains a novel reward design and an adaptive rollout generation approach by leveraging \"translation difficulty\" to further improve the translation quality of translation agents using our search tool.", "AI": {"tldr": "\u63d0\u51faNeoAMT\u6846\u67b6\uff0c\u4f7f\u7528Wiktionary\u641c\u7d22\u5de5\u5177\u8fdb\u884c\u65b0\u8bcd\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\uff0c\u6784\u5efa\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7ffb\u8bd1\u667a\u80fd\u4f53", "motivation": "\u65b0\u8bcd\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u76f8\u6bd4\u901a\u7528\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u5904\u7406\u6e90\u8bed\u53e5\u4e2d\u65b0\u8bcd\u7684\u7ffb\u8bd1\u95ee\u9898", "method": "1) \u521b\u5efa\u8986\u76d616\u79cd\u8bed\u8a00\u300175\u4e2a\u7ffb\u8bd1\u65b9\u5411\u7684\u65b0\u8bcd\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\uff1b2) \u57fa\u4e8eWiktionary\u6784\u5efa\u641c\u7d22\u5de5\u5177\uff1b3) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7ffb\u8bd1\u667a\u80fd\u4f53\uff0c\u5305\u542b\u65b0\u9896\u7684\u5956\u52b1\u8bbe\u8ba1\u548c\u57fa\u4e8e\"\u7ffb\u8bd1\u96be\u5ea6\"\u7684\u81ea\u9002\u5e94rollout\u751f\u6210\u65b9\u6cd5", "result": "\u6784\u5efa\u4e86\u5305\u542b\u7ea61000\u4e07\u6761\u8bb0\u5f55\u7684\u82f1\u6587Wiktionary\u6570\u636e\u96c6\uff0c\u641c\u7d22\u5de5\u5177\u68c0\u7d22\u8bed\u6599\u5e93\u5305\u542b\u7ea6300\u4e07\u6761\u6e05\u7406\u8bb0\u5f55\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u4e86\u7ffb\u8bd1\u667a\u80fd\u4f53\u5728\u65b0\u8bcd\u611f\u77e5\u7ffb\u8bd1\u4e2d\u7684\u8d28\u91cf", "conclusion": "NeoAMT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65b0\u8bcd\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u95ee\u9898\uff0c\u901a\u8fc7Wiktionary\u641c\u7d22\u5de5\u5177\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u8d28\u91cf", "topic": "agentic reinforcement learning"}}
{"id": "2601.03823", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.03823", "abs": "https://arxiv.org/abs/2601.03823", "authors": ["Fei Wu", "Zhenrong Zhang", "Qikai Chang", "Jianshu Zhang", "Quan Liu", "Jun Du"], "title": "Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.", "AI": {"tldr": "SPAE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b65\u9aa4\u6f5c\u529b\u7684\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u4e2d\u95f4\u7f6e\u4fe1\u5ea6\u548c\u6b63\u786e\u6027\u6765\u76d1\u7763\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8RLVR\u7684\u7cbe\u5ea6\u5e76\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f7f\u7528\u7ed3\u679c\u5956\u52b1\u5bfc\u81f4\u7c97\u7c92\u5ea6\u7684\u4f18\u52bf\u4f30\u8ba1\uff0c\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\u7684\u6b65\u9aa4\u7ea7\u63a8\u7406\u8fdb\u5ea6\u8861\u91cf\u3002LLMs\u65e0\u6cd5\u533a\u5206\u5fc5\u8981\u63a8\u7406\u548c\u5197\u4f59\u9a8c\u8bc1\uff0c\u53ef\u80fd\u8fc7\u5ea6\u68c0\u67e5\u751a\u81f3\u5c06\u6b63\u786e\u8f68\u8ff9\u63a8\u7ffb\u4e3a\u9519\u8bef\u7b54\u6848\u3002", "method": "\u63d0\u51fa\u65e0\u8bad\u7ec3\u63a2\u6d4b\u673a\u5236\u63d0\u53d6\u4e2d\u95f4\u7f6e\u4fe1\u5ea6\u548c\u6b63\u786e\u6027\uff0c\u7ed3\u5408\u4e3a\u6b65\u9aa4\u6f5c\u529b\u4fe1\u53f7\uff1b\u57fa\u4e8e\u6b64\u63d0\u51fa\u6b65\u9aa4\u6f5c\u529b\u4f18\u52bf\u4f30\u8ba1(SPAE)\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\uff1a\u653e\u5927\u6f5c\u529b\u589e\u76ca\u3001\u60e9\u7f5a\u6f5c\u529b\u4e0b\u964d\u3001\u5728\u6f5c\u529b\u9971\u548c\u540e\u65bd\u52a0\u60e9\u7f5a\u4ee5\u9f13\u52b1\u53ca\u65f6\u7ec8\u6b62\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPAE\u6301\u7eed\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\uff0c\u4f18\u4e8e\u5f3aRL\u57fa\u7ebf\u548c\u6700\u8fd1\u7684\u6548\u7387\u63a8\u7406\u53ca\u4ee4\u724c\u7ea7\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "SPAE\u901a\u8fc7\u6b65\u9aa4\u7ea7\u8fc7\u7a0b\u76d1\u7763\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u7c97\u7c92\u5ea6\u4f18\u52bf\u4f30\u8ba1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u63a8\u7406\u63a7\u5236\u548c\u6548\u7387\u63d0\u5347\u3002", "topic": "agentic reinforcement learning"}}
{"id": "2601.03868", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.03868", "abs": "https://arxiv.org/abs/2601.03868", "authors": ["Xing Li", "Hui-Ling Zhen", "Lihao Yin", "Xianzhi Yu", "Zhenhua Dong", "Mingxuan Yuan"], "title": "What Matters For Safety Alignment?", "comment": null, "summary": "This paper presents a comprehensive empirical study on the safety alignment capabilities. We evaluate what matters for safety alignment in LLMs and LRMs to provide essential insights for developing more secure and reliable AI systems. We systematically investigate and compare the influence of six critical intrinsic model characteristics and three external attack techniques. Our large-scale evaluation is conducted using 32 recent, popular LLMs and LRMs across thirteen distinct model families, spanning a parameter scale from 3B to 235B. The assessment leverages five established safety datasets and probes model vulnerabilities with 56 jailbreak techniques and four CoT attack strategies, resulting in 4.6M API calls. Our key empirical findings are fourfold. First, we identify the LRMs GPT-OSS-20B, Qwen3-Next-80B-A3B-Thinking, and GPT-OSS-120B as the top-three safest models, which substantiates the significant advantage of integrated reasoning and self-reflection mechanisms for robust safety alignment. Second, post-training and knowledge distillation may lead to a systematic degradation of safety alignment. We thus argue that safety must be treated as an explicit constraint or a core optimization objective during these stages, not merely subordinated to the pursuit of general capability. Third, we reveal a pronounced vulnerability: employing a CoT attack via a response prefix can elevate the attack success rate by 3.34x on average and from 0.6% to 96.3% for Seed-OSS-36B-Instruct. This critical finding underscores the safety risks inherent in text-completion interfaces and features that allow user-defined response prefixes in LLM services, highlighting an urgent need for architectural and deployment safeguards. Fourth, roleplay, prompt injection, and gradient-based search for adversarial prompts are the predominant methodologies for eliciting unaligned behaviors in modern models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9LLM\u548cLRM\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e866\u4e2a\u5185\u5728\u6a21\u578b\u7279\u6027\u548c3\u79cd\u5916\u90e8\u653b\u51fb\u6280\u672f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u96c6\u6210\u63a8\u7406\u548c\u81ea\u53cd\u601d\u673a\u5236\u80fd\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4f46\u540e\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u5bf9\u9f50\u9000\u5316\uff0c\u540c\u65f6\u63ed\u793a\u4e86CoT\u653b\u51fb\u901a\u8fc7\u54cd\u5e94\u524d\u7f00\u53ef\u4f7f\u653b\u51fb\u6210\u529f\u7387\u5927\u5e45\u63d0\u5347\u7684\u4e25\u91cd\u6f0f\u6d1e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u5f71\u54cdLLM\u548cLRM\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002\u5f53\u524d\u9700\u8981\u7406\u89e3\u54ea\u4e9b\u5185\u5728\u6a21\u578b\u7279\u6027\u548c\u5916\u90e8\u653b\u51fb\u6280\u672f\u5bf9\u5b89\u5168\u5bf9\u9f50\u5f71\u54cd\u6700\u5927\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u8bbe\u8ba1\u548c\u90e8\u7f72\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u7cfb\u7edf\u8bc4\u4f306\u4e2a\u5173\u952e\u5185\u5728\u6a21\u578b\u7279\u6027\u548c3\u79cd\u5916\u90e8\u653b\u51fb\u6280\u672f\uff1b2\uff09\u4f7f\u752832\u4e2a\u8fd1\u671f\u6d41\u884c\u7684LLM\u548cLRM\uff0c\u6db5\u76d613\u4e2a\u6a21\u578b\u5bb6\u65cf\uff0c\u53c2\u6570\u89c4\u6a21\u4ece3B\u5230235B\uff1b3\uff09\u5229\u75285\u4e2a\u5df2\u5efa\u7acb\u7684\u5b89\u5168\u6570\u636e\u96c6\uff1b4\uff09\u4f7f\u752856\u79cd\u8d8a\u72f1\u6280\u672f\u548c4\u79cdCoT\u653b\u51fb\u7b56\u7565\u8fdb\u884c\u6f0f\u6d1e\u63a2\u6d4b\uff1b5\uff09\u8fdb\u884c\u4e86460\u4e07\u6b21API\u8c03\u7528\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\uff1a1\uff09GPT-OSS-20B\u3001Qwen3-Next-80B-A3B-Thinking\u548cGPT-OSS-120B\u662f\u6700\u5b89\u5168\u7684\u4e09\u4e2a\u6a21\u578b\uff0c\u8868\u660e\u96c6\u6210\u63a8\u7406\u548c\u81ea\u53cd\u601d\u673a\u5236\u5bf9\u5b89\u5168\u5bf9\u9f50\u6709\u663e\u8457\u4f18\u52bf\uff1b2\uff09\u540e\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u5bf9\u9f50\u7cfb\u7edf\u6027\u9000\u5316\uff1b3\uff09CoT\u653b\u51fb\u901a\u8fc7\u54cd\u5e94\u524d\u7f00\u53ef\u4f7f\u653b\u51fb\u6210\u529f\u7387\u5e73\u5747\u63d0\u53473.34\u500d\uff0cSeed-OSS-36B-Instruct\u4ece0.6%\u63d0\u5347\u523096.3%\uff1b4\uff09\u89d2\u8272\u626e\u6f14\u3001\u63d0\u793a\u6ce8\u5165\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u5bf9\u6297\u63d0\u793a\u641c\u7d22\u662f\u5f15\u53d1\u672a\u5bf9\u9f50\u884c\u4e3a\u7684\u4e3b\u8981\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\uff1a1\uff09\u5b89\u5168\u5fc5\u987b\u4f5c\u4e3a\u540e\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u9636\u6bb5\u7684\u660e\u786e\u7ea6\u675f\u6216\u6838\u5fc3\u4f18\u5316\u76ee\u6807\uff1b2\uff09\u6587\u672c\u5b8c\u6210\u63a5\u53e3\u548c\u5141\u8bb8\u7528\u6237\u5b9a\u4e49\u54cd\u5e94\u524d\u7f00\u7684\u529f\u80fd\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u67b6\u6784\u548c\u90e8\u7f72\u5c42\u9762\u7684\u9632\u62a4\u63aa\u65bd\uff1b3\uff09\u96c6\u6210\u63a8\u7406\u548c\u81ea\u53cd\u601d\u673a\u5236\u662f\u63d0\u5347\u5b89\u5168\u5bf9\u9f50\u7684\u6709\u6548\u9014\u5f84\u3002", "topic": "agent analysis"}}
{"id": "2601.04086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04086", "abs": "https://arxiv.org/abs/2601.04086", "authors": ["Jinbo Hao", "Kai Yang", "Qingzhen Su", "Yifan Li", "Chao Jiang"], "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures", "comment": null, "summary": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53ef\u7f16\u7a0b\u6a21\u5757\u5d4c\u5165\u63a8\u7406\u63d0\u793a\u4e2d\uff0c\u5f15\u5bfc\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\uff0c\u4ee5\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7531\u63d0\u793a\u5f15\u8d77\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u5916\u90e8\u7ed3\u6784\u5316\u77e5\u8bc6\u6765\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u53ef\u9760\u6027\u3002", "method": "\u6269\u5c55\u94fe\u5f0f\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u63d0\u793a\u4e2d\u5d4c\u5165\u53ef\u6267\u884c\u4ee3\u7801\u6a21\u5757\uff0c\u5f15\u5bfc\u77e5\u8bc6\u56fe\u8c31\u63a2\u7d22\uff0c\u5e76\u5f00\u53d1\u589e\u5f3a\u7684\u84b8\u998f\u63a8\u7406\u6846\u67b6\u6765\u663e\u5f0f\u8c03\u63a7\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee3\u7801\u5f15\u5bfc\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u63d0\u793a\u8bf1\u5bfc\u5e7b\u89c9\u3002HIT@1\u3001HIT@3\u3001HIT@5\u5206\u522b\u63d0\u9ad8\u4e8615.64%\u300113.38%\u300113.28%\uff0c\u591a\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u5f97\u5206\u8d85\u8fc795%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7ea6\u675f\u9519\u8bef\u63a8\u7406\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u8868\u660e\u4ee3\u7801\u5f15\u5bfc\u7684\u77e5\u8bc6\u63a2\u7d22\u662f\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u6709\u6548\u9014\u5f84\u3002", "topic": "code agent"}}
{"id": "2601.04093", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.04093", "abs": "https://arxiv.org/abs/2601.04093", "authors": ["Yu Yan", "Sheng Sun", "Mingfeng Li", "Zheming Yang", "Chiwei Zhu", "Fei Ma", "Benfeng Xu", "Min Liu"], "title": "SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks", "comment": "We find that the key to jailbreak the LLM is objectifying its safety responsibility, thus we delegate the open-web to inject harmful semantics and get the huge gain from unmoderated web resources", "summary": "Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \\textbf{\\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.", "AI": {"tldr": "SearchAttack\u662f\u4e00\u79cd\u9488\u5bf9\u641c\u7d22\u589e\u5f3aLLM\u7684\u7ea2\u961f\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u8bed\u4e49\u5916\u5305\u7ed9\u7f51\u7edc\u641c\u7d22\uff0c\u4ec5\u4fdd\u7559\u67e5\u8be2\u9aa8\u67b6\u548c\u788e\u7247\u5316\u7ebf\u7d22\uff0c\u5f15\u5bfcLLM\u91cd\u6784\u68c0\u7d22\u5185\u5bb9\u4ee5\u5b9e\u73b0\u6076\u610f\u76ee\u6807\u3002", "motivation": "\u5f53LLM\u5904\u7406\u5f00\u653e\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff0c\u4eba\u4eec\u8f6c\u5411\u641c\u7d22\u589e\u5f3aLLM\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002\u7136\u800c\uff0c\u5f53\u641c\u7d22\u5f15\u64ce\u88ab\u89e6\u53d1\u6267\u884c\u6709\u5bb3\u4efb\u52a1\u65f6\uff0cLLM\u7684\u5b89\u5168\u9632\u62a4\u65e0\u6cd5\u63a7\u5236\u8fd4\u56de\u7684\u5185\u5bb9\uff0c\u4e00\u65e6\u641c\u7d22\u7ed3\u679c\u5305\u542b\u53ef\u76f4\u63a5\u4f7f\u7528\u7684\u6709\u5bb3\u4fe1\u606f\uff0cLLM\u65e0\u6cd5\u64a4\u56de\u8fd9\u79cd\u66b4\u9732\u3002", "method": "SearchAttack\u5c06\u6709\u5bb3\u8bed\u4e49\u5916\u5305\u7ed9\u7f51\u7edc\u641c\u7d22\uff0c\u53ea\u4fdd\u7559\u67e5\u8be2\u7684\u9aa8\u67b6\u548c\u788e\u7247\u5316\u7ebf\u7d22\uff0c\u7136\u540e\u901a\u8fc7\u7ed3\u6784\u5316\u6a21\u677f\u5f15\u5bfcLLM\u91cd\u6784\u68c0\u7d22\u5230\u7684\u5185\u5bb9\uff0c\u4ece\u800c\u5b9e\u73b0\u6076\u610f\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u5c06\u7f51\u7edc\u641c\u7d22\u8bc6\u522b\u4e3a\u5173\u952e\u653b\u51fb\u9762\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSearchAttack\u5728\u653b\u51fb\u641c\u7d22\u589e\u5f3aLLM\u7cfb\u7edf\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u8fdb\u884c\u4e86\u8d1f\u8d23\u4efb\u7684\u5b89\u5168\u6f0f\u6d1e\u8bc4\u4f30\u3002", "conclusion": "SearchAttack\u63ed\u793a\u4e86\u641c\u7d22\u589e\u5f3aLLM\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u7f51\u7edc\u641c\u7d22\u6210\u4e3a\u5173\u952e\u653b\u51fb\u9762\uff0c\u9700\u8981\u52a0\u5f3a\u5bf9\u6b64\u7c7b\u7cfb\u7edf\u7684\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "topic": "agent analysis"}}
{"id": "2601.04126", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.04126", "abs": "https://arxiv.org/abs/2601.04126", "authors": ["Ziyun Zhang", "Zezhou Wang", "Xiaoyi Zhang", "Zongyu Guo", "Jiahao Li", "Bin Li", "Yan Lu"], "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training", "comment": "Work In Progress", "summary": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.", "AI": {"tldr": "InfiniteWeb\u7cfb\u7edf\u80fd\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u529f\u80fd\u6027\u7f51\u9875\u73af\u5883\u7528\u4e8eGUI\u667a\u80fd\u4f53\u8bad\u7ec3\uff0c\u901a\u8fc7\u7edf\u4e00\u89c4\u8303\u3001\u4efb\u52a1\u9a71\u52a8\u5f00\u53d1\u548c\u591a\u6837\u5316\u8bbe\u8ba1\u89e3\u51b3\u7f51\u9875\u751f\u6210\u6311\u6218\uff0c\u5e76\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u8bc4\u4f30\u5668\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\u3002", "motivation": "\u8bad\u7ec3GUI\u667a\u80fd\u4f53\u9762\u4e34\u5408\u9002\u73af\u5883\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709LLM\u867d\u7136\u80fd\u751f\u6210\u5355\u4e2a\u7f51\u9875\uff0c\u4f46\u6784\u5efa\u5177\u6709\u591a\u4e2a\u4e92\u8fde\u9875\u9762\u7684\u73b0\u5b9e\u529f\u80fd\u6027\u7f51\u7ad9\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u89c4\u8303\u3001\u4efb\u52a1\u4e2d\u5fc3\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u3001\u7ed3\u5408\u7f51\u7ad9\u79cd\u5b50\u548c\u53c2\u8003\u8bbe\u8ba1\u56fe\u50cf\u786e\u4fdd\u591a\u6837\u6027\uff0c\u7cfb\u7edf\u81ea\u52a8\u751f\u6210\u529f\u80fd\u6027\u7f51\u9875\u73af\u5883\u5e76\u521b\u5efa\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u8bc4\u4f30\u5668\u3002", "result": "InfiniteWeb\u5728\u73b0\u5b9e\u7f51\u7ad9\u6784\u5efa\u65b9\u9762\u8d85\u8d8a\u5546\u4e1a\u7f16\u7801\u667a\u80fd\u4f53\uff0c\u5728\u5176\u751f\u6210\u73af\u5883\u4e0a\u8bad\u7ec3\u7684GUI\u667a\u80fd\u4f53\u5728OSWorld\u548cOnline-Mind2Web\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86GUI\u667a\u80fd\u4f53\u8bad\u7ec3\u73af\u5883\u7a00\u7f3a\u95ee\u9898\uff0c\u751f\u6210\u7684\u591a\u6837\u5316\u529f\u80fd\u6027\u7f51\u9875\u73af\u5883\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u3002", "topic": "code agent"}}
{"id": "tldr.2601.3e5cb111", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fagent-harness-2026%3Futm_source=tldrnewsletter/1/0100019b932e5ba6-9280105b-7dab-40bb-842d-cc1d65b66249-000000/M6Bn6MCn3JR5vX-37UYFw68Wb45qxbQPsG8adwSxxdQ=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fagent-harness-2026%3Futm_source=tldrnewsletter/1/0100019b932e5ba6-9280105b-7dab-40bb-842d-cc1d65b66249-000000/M6Bn6MCn3JR5vX-37UYFw68Wb45qxbQPsG8adwSxxdQ=438", "authors": ["TLDR Newsletter"], "title": "The importance of Agent Harness in 2026", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 6 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.philschmid.de%2Fagent-harness-2026%3Futm_source=tldrnewsletter/1/0100019b932e5ba6-9280105b-7dab-40bb-842d-cc1d65b66249-000000/M6Bn6MCn3JR5vX-37UYFw68Wb45qxbQPsG8adwSxxdQ=438", "summary": "The importance of Agent Harness in 2026 (6 minute read) An Agent Harness is the infrastructure that wraps around an AI model to manage long-running tasks. It is the system that governs how the agent operates. Agent harnesses turn vague, multistep agent workflows into structured data that can be logged and graded to improve systems. They can be used to validate real-world progress, empower the user experience, and improve models via real-world feedback.", "source": "tldr", "AI": {"tldr": "Agent Harness\u662f\u56f4\u7ed5AI\u6a21\u578b\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u7ba1\u7406\u957f\u671f\u8fd0\u884c\u4efb\u52a1\uff0c\u5c06\u6a21\u7cca\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u8f6c\u5316\u4e3a\u53ef\u8bb0\u5f55\u548c\u8bc4\u4f30\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4ee5\u6539\u8fdb\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u5904\u7406\u590d\u6742\u3001\u591a\u6b65\u9aa4\u4efb\u52a1\u65f6\u7f3a\u4e4f\u6709\u6548\u7684\u7ba1\u7406\u548c\u8bc4\u4f30\u673a\u5236\uff0c\u9700\u8981\u57fa\u7840\u8bbe\u65bd\u6765\u89c4\u8303\u4ee3\u7406\u64cd\u4f5c\u3001\u9a8c\u8bc1\u5b9e\u9645\u8fdb\u5c55\u3001\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u53cd\u9988\u6539\u8fdb\u6a21\u578b\u3002", "method": "\u63d0\u51faAgent Harness\u4f5c\u4e3aAI\u4ee3\u7406\u7684\u57fa\u7840\u8bbe\u65bd\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u5730\u7ba1\u7406\u957f\u671f\u8fd0\u884c\u4efb\u52a1\uff0c\u5c06\u6a21\u7cca\u7684\u5de5\u4f5c\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5b9e\u73b0\u4efb\u52a1\u8bb0\u5f55\u3001\u6027\u80fd\u8bc4\u4f30\u548c\u7cfb\u7edf\u6539\u8fdb\u3002", "result": "Agent Harness\u80fd\u591f\u6709\u6548\u7ba1\u7406AI\u4ee3\u7406\u7684\u957f\u671f\u4efb\u52a1\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u6570\u636e\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\uff0c\u589e\u5f3a\u771f\u5b9e\u4e16\u754c\u8fdb\u5c55\u9a8c\u8bc1\u80fd\u529b\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u53cd\u9988\u673a\u5236\u3002", "conclusion": "Agent Harness\u4f5c\u4e3aAI\u4ee3\u7406\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u5bf9\u4e8e\u89c4\u8303\u4ee3\u7406\u64cd\u4f5c\u3001\u5b9e\u73b0\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u6539\u8fdb\u81f3\u5173\u91cd\u8981\uff0c\u5c06\u57282026\u5e74\u53ca\u672a\u6765\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.a6b0c44e", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/vRNItVjf9VkHcAZv28dqNErHA99SYfAnPQSU6JPUanQ=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/vRNItVjf9VkHcAZv28dqNErHA99SYfAnPQSU6JPUanQ=438", "authors": ["TLDR Newsletter"], "title": "Thoughts on Claude Code", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 14 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.spakhm.com%2Fclaude-code%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/vRNItVjf9VkHcAZv28dqNErHA99SYfAnPQSU6JPUanQ=438", "summary": "Thoughts on Claude Code (14 minute read) This dev built an imaginary programming language, Beep, over two weeks in close cooperation with Claude Code and Opus 4.5. Claude was a great programming partner, providing efficient solutions to complex design challenges like lexical shadowing and dynamic scope mutation, and easily handling mechanical refactors and obscure parser combinator tasks. While Claude occasionally struggled with issues requiring changes to external libraries or specific, non-...", "source": "tldr", "AI": {"tldr": "\u5f00\u53d1\u8005\u5728\u4e24\u5468\u5185\u4e0eClaude Code\u548cOpus 4.5\u5408\u4f5c\u6784\u5efa\u4e86\u865a\u6784\u7f16\u7a0b\u8bed\u8a00Beep\uff0cClaude\u5728\u590d\u6742\u8bbe\u8ba1\u6311\u6218\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5916\u90e8\u5e93\u4fee\u6539\u548c\u7279\u5b9a\u975e\u6807\u51c6\u4efb\u52a1\u4e0a\u4ecd\u6709\u5c40\u9650", "motivation": "\u63a2\u7d22Claude Code\u4f5c\u4e3a\u7f16\u7a0b\u4f19\u4f34\u7684\u80fd\u529b\uff0c\u6d4b\u8bd5\u5176\u5728\u6784\u5efa\u590d\u6742\u7f16\u7a0b\u8bed\u8a00\u9879\u76ee\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5904\u7406\u8bbe\u8ba1\u6311\u6218\u548c\u673a\u68b0\u91cd\u6784\u4efb\u52a1", "method": "\u901a\u8fc7\u5b9e\u9645\u9879\u76ee\u5f00\u53d1\u8bc4\u4f30\uff1a\u5728\u4e24\u5468\u5185\u6784\u5efa\u865a\u6784\u7f16\u7a0b\u8bed\u8a00Beep\uff0c\u4e0eClaude Code\u548cOpus 4.5\u7d27\u5bc6\u5408\u4f5c\uff0c\u6d4b\u8bd5\u5176\u5728\u8bcd\u6cd5\u9634\u5f71\u3001\u52a8\u6001\u4f5c\u7528\u57df\u53d8\u5f02\u7b49\u590d\u6742\u8bbe\u8ba1\u95ee\u9898\u4e0a\u7684\u8868\u73b0", "result": "Claude\u5728\u590d\u6742\u8bbe\u8ba1\u6311\u6218\uff08\u5982\u8bcd\u6cd5\u9634\u5f71\u548c\u52a8\u6001\u4f5c\u7528\u57df\u53d8\u5f02\uff09\u4e0a\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8f7b\u677e\u5904\u7406\u673a\u68b0\u91cd\u6784\u548c\u6666\u6da9\u7684\u89e3\u6790\u5668\u7ec4\u5408\u4efb\u52a1\uff0c\u4f46\u5728\u9700\u8981\u4fee\u6539\u5916\u90e8\u5e93\u6216\u7279\u5b9a\u975e\u6807\u51c6\u4efb\u52a1\u65f6\u4ecd\u6709\u56f0\u96be", "conclusion": "Claude Code\u662f\u4e00\u4e2a\u4f18\u79c0\u7684\u7f16\u7a0b\u4f19\u4f34\uff0c\u7279\u522b\u64c5\u957f\u89e3\u51b3\u590d\u6742\u8bbe\u8ba1\u95ee\u9898\u548c\u5904\u7406\u673a\u68b0\u91cd\u6784\u4efb\u52a1\uff0c\u4f46\u5728\u6d89\u53ca\u5916\u90e8\u4f9d\u8d56\u4fee\u6539\u548c\u7279\u5b9a\u975e\u6807\u51c6\u9700\u6c42\u65f6\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4", "topic": "code agent"}}
{"id": "tldr.2601.a85b450d", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faddyo%2Fp%2Fcode-review-in-the-age-of-ai%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/DMshwJ0vR7jBHJuf4YUUnkmD7zPvHDn32s7zfhAQhY8=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faddyo%2Fp%2Fcode-review-in-the-age-of-ai%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/DMshwJ0vR7jBHJuf4YUUnkmD7zPvHDn32s7zfhAQhY8=438", "authors": ["TLDR Newsletter"], "title": "Code Review in the Age of AI", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 8 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fopen.substack.com%2Fpub%2Faddyo%2Fp%2Fcode-review-in-the-age-of-ai%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/DMshwJ0vR7jBHJuf4YUUnkmD7zPvHDn32s7zfhAQhY8=438", "summary": "Code Review in the Age of AI (8 minute read) AI has increased code generation speed but has made code verification more critical, with over 30% of senior developers now shipping mostly AI-generated code that contains 75% more logic errors and 45% security flaws compared to human-written code. Solo developers can move at \"inference speed\" by relying heavily on automated testing (>70% coverage) as safety nets, while teams still require human code review for context, security, and knowledge tran...", "source": "tldr", "AI": {"tldr": "AI\u52a0\u901f\u4ee3\u7801\u751f\u6210\u4f46\u589e\u52a0\u9a8c\u8bc1\u9700\u6c42\uff0c30%\u8d44\u6df1\u5f00\u53d1\u8005\u4f7f\u7528AI\u751f\u6210\u4ee3\u7801\uff0c\u5176\u903b\u8f91\u9519\u8bef\u591a75%\uff0c\u5b89\u5168\u6f0f\u6d1e\u591a45%\u3002\u4e2a\u4eba\u5f00\u53d1\u8005\u4f9d\u8d56\u81ea\u52a8\u5316\u6d4b\u8bd5\uff08>70%\u8986\u76d6\u7387\uff09\uff0c\u56e2\u961f\u4ecd\u9700\u4eba\u5de5\u4ee3\u7801\u5ba1\u67e5\u786e\u4fdd\u4e0a\u4e0b\u6587\u3001\u5b89\u5168\u548c\u77e5\u8bc6\u4f20\u9012\u3002", "motivation": "AI\u4ee3\u7801\u751f\u6210\u5de5\u5177\u666e\u53ca\u63d0\u9ad8\u4e86\u5f00\u53d1\u901f\u5ea6\uff0c\u4f46\u5e26\u6765\u4e86\u4ee3\u7801\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0AI\u751f\u6210\u4ee3\u7801\u5b58\u5728\u66f4\u591a\u903b\u8f91\u9519\u8bef\u548c\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u673a\u5236\u6765\u786e\u4fdd\u8f6f\u4ef6\u8d28\u91cf\u3002", "method": "\u5206\u6790AI\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\u7684\u8d28\u91cf\u5dee\u5f02\uff0c\u7814\u7a76\u4e0d\u540c\u5f00\u53d1\u573a\u666f\uff08\u4e2a\u4ebavs\u56e2\u961f\uff09\u7684\u9a8c\u8bc1\u7b56\u7565\uff0c\u5305\u62ec\u81ea\u52a8\u5316\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u4eba\u5de5\u4ee3\u7801\u5ba1\u67e5\u7684\u91cd\u8981\u6027\u3002", "result": "AI\u751f\u6210\u4ee3\u7801\u6bd4\u4eba\u5de5\u7f16\u5199\u4ee3\u7801\u591a75%\u903b\u8f91\u9519\u8bef\u548c45%\u5b89\u5168\u6f0f\u6d1e\u300230%\u8d44\u6df1\u5f00\u53d1\u8005\u4e3b\u8981\u4f7f\u7528AI\u751f\u6210\u4ee3\u7801\u3002\u4e2a\u4eba\u5f00\u53d1\u8005\u4f9d\u8d56\u9ad8\u8986\u76d6\u7387\u81ea\u52a8\u5316\u6d4b\u8bd5\uff0c\u56e2\u961f\u9700\u8981\u4eba\u5de5\u5ba1\u67e5\u786e\u4fdd\u8d28\u91cf\u3002", "conclusion": "AI\u65f6\u4ee3\u4ee3\u7801\u5ba1\u67e5\u66f4\u52a0\u5173\u952e\uff0c\u9700\u8981\u7ed3\u5408\u81ea\u52a8\u5316\u6d4b\u8bd5\u548c\u4eba\u5de5\u5ba1\u67e5\u6765\u5e73\u8861\u5f00\u53d1\u901f\u5ea6\u4e0e\u4ee3\u7801\u8d28\u91cf\uff0c\u56e2\u961f\u534f\u4f5c\u4e2d\u4eba\u5de5\u5ba1\u67e5\u5bf9\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5b89\u5168\u6027\u548c\u77e5\u8bc6\u4f20\u9012\u81f3\u5173\u91cd\u8981\u3002", "topic": "code agent"}}
{"id": "tldr.2601.e55057bf", "categories": ["tldr.article"], "pdf": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Frepogrep.com%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/O0ao3W2HvhsB26E2rn33XrbWukYzHPmoC4bR8XYx2dA=438", "abs": "http://tracking.tldrnewsletter.com/CL0/http:%2F%2Frepogrep.com%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/O0ao3W2HvhsB26E2rn33XrbWukYzHPmoC4bR8XYx2dA=438", "authors": ["TLDR Newsletter"], "title": "Repogrep", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Links: http://tracking.tldrnewsletter.com/CL0/http:%2F%2Frepogrep.com%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/O0ao3W2HvhsB26E2rn33XrbWukYzHPmoC4bR8XYx2dA=438", "summary": "Repogrep (Website) Repogrep is an ultra-fast AI coding agent designed for codebase search. It allows users to search across any public GitHub repository. Users can input a GitHub repo URL or search by keyword to explore codebases.", "source": "tldr", "AI": {"tldr": "Repogrep\u662f\u4e00\u4e2a\u8d85\u5feb\u7684AI\u7f16\u7801\u4ee3\u7406\uff0c\u4e13\u95e8\u7528\u4e8e\u4ee3\u7801\u5e93\u641c\u7d22\uff0c\u652f\u6301\u641c\u7d22\u4efb\u4f55\u516c\u5171GitHub\u4ed3\u5e93\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7GitHub\u4ed3\u5e93URL\u6216\u5173\u952e\u8bcd\u8fdb\u884c\u641c\u7d22", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u7684\u4ee3\u7801\u5e93\u641c\u7d22\u5de5\u5177\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5feb\u901f\u63a2\u7d22\u548c\u7406\u89e3\u516c\u5171GitHub\u4ed3\u5e93\u4e2d\u7684\u4ee3\u7801\uff0c\u89e3\u51b3\u4ee3\u7801\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898", "method": "\u57fa\u4e8eAI\u6280\u672f\u6784\u5efa\u7684\u4ee3\u7801\u641c\u7d22\u4ee3\u7406\uff0c\u652f\u6301\u901a\u8fc7GitHub\u4ed3\u5e93URL\u6216\u5173\u952e\u8bcd\u8fdb\u884c\u641c\u7d22\uff0c\u63d0\u4f9b\u8d85\u5feb\u7684\u4ee3\u7801\u5e93\u63a2\u7d22\u529f\u80fd", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aRepogrep\u7684\u7f51\u7ad9\u5de5\u5177\uff0c\u80fd\u591f\u5feb\u901f\u641c\u7d22\u4efb\u4f55\u516c\u5171GitHub\u4ed3\u5e93\u7684\u4ee3\u7801\u5e93", "conclusion": "Repogrep\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u4ee3\u7801\u641c\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u4e86\u4ee3\u7801\u5e93\u63a2\u7d22\u8fc7\u7a0b", "topic": "code agent"}}
{"id": "tldr.2601.8978a6b2", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMadAppGang%2Fclaudish%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/jYwtPlCAhDgJeRavKByctJ6W2Wj37NXO3uMAPF-gizQ=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMadAppGang%2Fclaudish%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/jYwtPlCAhDgJeRavKByctJ6W2Wj37NXO3uMAPF-gizQ=438", "authors": ["TLDR Newsletter"], "title": "Claudish", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FMadAppGang%2Fclaudish%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/jYwtPlCAhDgJeRavKByctJ6W2Wj37NXO3uMAPF-gizQ=438", "summary": "Claudish (GitHub Repo) Claudish is a CLI tool that lets developers run Claude Code with any AI model by proxying requests through a local Anthropic API-compatible server. It works with multiple providers as well as local models. Claudish can run agents in parallel, with each instance getting an isolated proxy.", "source": "tldr", "AI": {"tldr": "Claudish\u662f\u4e00\u4e2aCLI\u5de5\u5177\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u901a\u8fc7\u672c\u5730Anthropic API\u517c\u5bb9\u670d\u52a1\u5668\u4ee3\u7406\u8bf7\u6c42\uff0c\u4f7f\u7528\u4efb\u4f55AI\u6a21\u578b\u8fd0\u884cClaude Code\uff0c\u652f\u6301\u591a\u63d0\u4f9b\u5546\u548c\u672c\u5730\u6a21\u578b\uff0c\u5e76\u80fd\u5e76\u884c\u8fd0\u884c\u4ee3\u7406\u5b9e\u4f8b\u3002", "motivation": "\u8ba9\u5f00\u53d1\u8005\u80fd\u591f\u7075\u6d3b\u5730\u4f7f\u7528\u4e0d\u540c\u7684AI\u6a21\u578b\uff08\u5305\u62ec\u672c\u5730\u6a21\u578b\uff09\u6765\u8fd0\u884cClaude Code\uff0c\u800c\u4e0d\u53d7\u9650\u4e8e\u7279\u5b9a\u7684\u63d0\u4f9b\u5546\uff0c\u540c\u65f6\u63d0\u4f9b\u5e76\u884c\u8fd0\u884c\u80fd\u529b\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u901a\u8fc7\u521b\u5efa\u672c\u5730Anthropic API\u517c\u5bb9\u670d\u52a1\u5668\u4f5c\u4e3a\u4ee3\u7406\u5c42\uff0c\u5c06\u8bf7\u6c42\u8f6c\u53d1\u5230\u5404\u79cdAI\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u652f\u6301\u591a\u63d0\u4f9b\u5546\u96c6\u6210\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u5b9e\u4f8b\u63d0\u4f9b\u9694\u79bb\u7684\u4ee3\u7406\u73af\u5883\u4ee5\u5b9e\u73b0\u5e76\u884c\u8fd0\u884c\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684CLI\u5de5\u5177\uff0c\u80fd\u591f\u6210\u529f\u4ee3\u7406\u8bf7\u6c42\u5230\u591a\u79cdAI\u6a21\u578b\u63d0\u4f9b\u5546\uff0c\u652f\u6301\u672c\u5730\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4ee3\u7406\u5b9e\u4f8b\u7684\u5e76\u884c\u8fd0\u884c\u548c\u9694\u79bb\u3002", "conclusion": "Claudish\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684AI\u6a21\u578b\u4f7f\u7528\u65b9\u6848\uff0c\u6253\u7834\u4e86\u6a21\u578b\u63d0\u4f9b\u5546\u9650\u5236\uff0c\u901a\u8fc7\u4ee3\u7406\u67b6\u6784\u5b9e\u73b0\u4e86\u591a\u6a21\u578b\u652f\u6301\u548c\u5e76\u884c\u5904\u7406\u80fd\u529b\u3002", "topic": "code agent"}}
{"id": "tldr.2601.767df59b", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalnewport.com%2Fwhy-didnt-ai-join-the-workforce-in-2025%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/lEmjTOwCz1bHaE5yzIcbP5ivtnL3LU_OiEZsQT_LEoA=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalnewport.com%2Fwhy-didnt-ai-join-the-workforce-in-2025%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/lEmjTOwCz1bHaE5yzIcbP5ivtnL3LU_OiEZsQT_LEoA=438", "authors": ["TLDR Newsletter"], "title": "Why Didn't AI \u201cJoin the Workforce\u201d in 2025?", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 4 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcalnewport.com%2Fwhy-didnt-ai-join-the-workforce-in-2025%2F%3Futm_source=tldrdev/1/0100019b93355283-5faa19c7-e47d-4658-ab9b-928b4459be43-000000/lEmjTOwCz1bHaE5yzIcbP5ivtnL3LU_OiEZsQT_LEoA=438", "summary": "Why Didn't AI \u201cJoin the Workforce\u201d in 2025? (4 minute read) In late 2024, AI leaders like Sam Altman and Kevin Weil predicted that 2025 would be the \"Year of AI Agents,\" where these systems would join the workforce and perform multi-step tasks like human employees. However, this hype didn't translate into reality, as released AI agent products weren't as effective as expected and fell significantly short of their promised capabilities. Large language model technology isn't yet robust enough f...", "source": "tldr", "AI": {"tldr": "2025\u5e74AI\u4ee3\u7406\u672a\u80fd\u5982\u9884\u671f\u52a0\u5165\u52b3\u52a8\u529b\u5e02\u573a\uff0c\u56e0\u4e3a\u73b0\u6709\u6280\u672f\u4e0d\u591f\u7a33\u5065\uff0c\u65e0\u6cd5\u53ef\u9760\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1", "motivation": "\u5206\u6790\u4e3a\u4f55AI\u4ee3\u7406\u672a\u80fd\u5b9e\u73b02024\u5e74\u884c\u4e1a\u9886\u8896\u9884\u6d4b\u7684\"AI\u4ee3\u7406\u4e4b\u5e74\"\uff0c\u63a2\u8ba8\u6280\u672f\u9650\u5236\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd", "method": "\u901a\u8fc7\u5bf9\u6bd4\u884c\u4e1a\u9884\u6d4b\u4e0e\u5b9e\u9645\u53d1\u5e03\u4ea7\u54c1\u7684\u8868\u73b0\uff0c\u5206\u6790\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5c40\u9650\u6027", "result": "AI\u4ee3\u7406\u4ea7\u54c1\u6548\u679c\u672a\u8fbe\u9884\u671f\uff0c\u672a\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u6280\u672f\u6210\u719f\u5ea6\u4e0d\u8db3", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u5c1a\u672a\u8db3\u591f\u7a33\u5065\uff0c\u65e0\u6cd5\u652f\u6301AI\u4ee3\u7406\u5982\u9884\u671f\u822c\u52a0\u5165\u52b3\u52a8\u529b\u5e02\u573a\u6267\u884c\u590d\u6742\u4efb\u52a1", "topic": "agent analysis"}}
{"id": "tldr.2601.05a70ff1", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcameronrwolfe.substack.com%2Fp%2Fgrpo-tricks%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/duh05lq8ZCAZajjldtdK77vLnRhOqdV2VyJCG3ERIk0=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcameronrwolfe.substack.com%2Fp%2Fgrpo-tricks%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/duh05lq8ZCAZajjldtdK77vLnRhOqdV2VyJCG3ERIk0=438", "authors": ["TLDR Newsletter"], "title": "GRPO++: Tricks for Making RL Actually Work", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 72 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcameronrwolfe.substack.com%2Fp%2Fgrpo-tricks%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/duh05lq8ZCAZajjldtdK77vLnRhOqdV2VyJCG3ERIk0=438", "summary": "GRPO++: Tricks for Making RL Actually Work (72 minute read) Group Relative Policy Optimization (GRPO) is the RL optimizer used to train most open-source reasoning models. It is popular due to its conceptual simplicity and practical efficiency. The vanilla GRPO algorithm has subtle issues that can hinder the RL training process, especially at scale. This post provides an overview of the work done to solve the shortcomings of GRPO.", "source": "tldr", "AI": {"tldr": "GRPO++\u6539\u8fdb\u4e86GRPO\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u539f\u7b97\u6cd5\u5728\u89c4\u6a21\u5316RL\u8bad\u7ec3\u4e2d\u7684\u5fae\u5999\u95ee\u9898\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6280\u5de7\u4f7fRL\u8bad\u7ec3\u771f\u6b63\u6709\u6548\u5de5\u4f5c", "motivation": "GRPO\u867d\u7136\u56e0\u5176\u6982\u5ff5\u7b80\u5355\u548c\u5b9e\u9645\u6548\u7387\u800c\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bad\u7ec3\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u4f46\u539f\u7b97\u6cd5\u5b58\u5728\u4e00\u4e9b\u5fae\u5999\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u89c4\u6a21\u5316\u8bad\u7ec3\u65f6\u4f1a\u963b\u788dRL\u8bad\u7ec3\u8fc7\u7a0b", "method": "\u63d0\u51fa\u4e86GRPO++\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6280\u5de7\u548c\u6539\u8fdb\u6765\u89e3\u51b3GRPO\u7684\u7f3a\u70b9\uff0c\u5177\u4f53\u65b9\u6cd5\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e", "result": "\u6539\u8fdb\u4e86GRPO\u7b97\u6cd5\uff0c\u4f7f\u5176\u5728\u89c4\u6a21\u5316RL\u8bad\u7ec3\u4e2d\u66f4\u52a0\u7a33\u5b9a\u6709\u6548", "conclusion": "GRPO++\u901a\u8fc7\u89e3\u51b3\u539f\u7b97\u6cd5\u7684\u5fae\u5999\u95ee\u9898\uff0c\u4f7fRL\u8bad\u7ec3\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u771f\u6b63\u6709\u6548\u5de5\u4f5c", "topic": "agentic reinforcement learning"}}
{"id": "tldr.2601.dc4af22c", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.minimaxi.com%2Fnews%2Fm21-multilingual-and-multi-task-coding-with-strong-general%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/vFRxGsdSajg7dd1E7NXp_S-OkSafQDF3lMWhpp0K31I=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.minimaxi.com%2Fnews%2Fm21-multilingual-and-multi-task-coding-with-strong-general%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/vFRxGsdSajg7dd1E7NXp_S-OkSafQDF3lMWhpp0K31I=438", "authors": ["TLDR Newsletter"], "title": "M2.1: Multilingual and Multi-Task Coding with Strong Generalization", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 17 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.minimaxi.com%2Fnews%2Fm21-multilingual-and-multi-task-coding-with-strong-general%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/vFRxGsdSajg7dd1E7NXp_S-OkSafQDF3lMWhpp0K31I=438", "summary": "M2.1: Multilingual and Multi-Task Coding with Strong Generalization (17 minute read) MiniMax-M2.1 matches or surpasses the level of global top-tier models on multiple internal and external benchmarks. The open source model has exceptional performance in code generation, tool usage, instruction following, and long-range planning. This post discusses how the model was trained and shares insights gained during the process.", "source": "tldr", "AI": {"tldr": "MiniMax-M2.1\u662f\u4e00\u4e2a\u5728\u4ee3\u7801\u751f\u6210\u3001\u5de5\u5177\u4f7f\u7528\u3001\u6307\u4ee4\u9075\u5faa\u548c\u957f\u7a0b\u89c4\u5212\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u7684\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u7f16\u7801\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u5185\u90e8\u548c\u5916\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u5168\u7403\u9876\u7ea7\u6a21\u578b\u7684\u6c34\u5e73\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u7684\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u7f16\u7801\u6a21\u578b\uff0c\u65e8\u5728\u5728\u4ee3\u7801\u751f\u6210\u3001\u5de5\u5177\u4f7f\u7528\u3001\u6307\u4ee4\u9075\u5faa\u548c\u957f\u7a0b\u89c4\u5212\u7b49\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u9876\u7ea7\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7279\u5b9a\u7684\u8bad\u7ec3\u65b9\u6cd5\u5f00\u53d1\u4e86MiniMax-M2.1\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u7f16\u7801\u3002\u6587\u4e2d\u8ba8\u8bba\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u53ca\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u7684\u89c1\u89e3\u3002", "result": "MiniMax-M2.1\u5728\u591a\u4e2a\u5185\u90e8\u548c\u5916\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u5168\u7403\u9876\u7ea7\u6a21\u578b\u7684\u6c34\u5e73\uff0c\u5728\u4ee3\u7801\u751f\u6210\u3001\u5de5\u5177\u4f7f\u7528\u3001\u6307\u4ee4\u9075\u5faa\u548c\u957f\u7a0b\u89c4\u5212\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "MiniMax-M2.1\u662f\u4e00\u4e2a\u6210\u529f\u7684\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u7f16\u7801\u6a21\u578b\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e1a\u754c\u9886\u5148\u6c34\u5e73\uff0c\u5176\u8bad\u7ec3\u65b9\u6cd5\u548c\u89c1\u89e3\u5bf9\u540e\u7eed\u7814\u7a76\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002", "topic": "code agent"}}
{"id": "tldr.2601.b2f24532", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftencentcloudadp.github.io%2Fyoutu-agent%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/RjbmdnZKMGXXuwl8KEzDqoow5Schqwns1SPLEF5eTQ0=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftencentcloudadp.github.io%2Fyoutu-agent%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/RjbmdnZKMGXXuwl8KEzDqoow5Schqwns1SPLEF5eTQ0=438", "authors": ["TLDR Newsletter"], "title": "Tencent's Youtu-Agent Framework for Autonomous Agents", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 2 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftencentcloudadp.github.io%2Fyoutu-agent%2F%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/RjbmdnZKMGXXuwl8KEzDqoow5Schqwns1SPLEF5eTQ0=438", "summary": "Tencent's Youtu-Agent Framework for Autonomous Agents (2 minute read) Tencent's Youtu-Agent offers a modular framework for building and evaluating AI agents with open-source models. It emphasizes clean abstractions for tools, environments, and agent logic to support extensibility and robust deployment.", "source": "tldr", "AI": {"tldr": "\u817e\u8bafYoutu-Agent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u6784\u5efa\u548c\u8bc4\u4f30AI\u667a\u80fd\u4f53\uff0c\u5f3a\u8c03\u5de5\u5177\u3001\u73af\u5883\u548c\u667a\u80fd\u4f53\u903b\u8f91\u7684\u6e05\u6670\u62bd\u8c61\u4ee5\u652f\u6301\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5065\u90e8\u7f72\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5f00\u53d1\u7f3a\u4e4f\u7edf\u4e00\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5bfc\u81f4\u6784\u5efa\u548c\u8bc4\u4f30\u8fc7\u7a0b\u590d\u6742\uff0c\u96be\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5065\u90e8\u7f72\u3002\u817e\u8bafYoutu-Agent\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u5de5\u5177\u3002", "method": "Youtu-Agent\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u5de5\u5177\u3001\u73af\u5883\u548c\u667a\u80fd\u4f53\u903b\u8f91\u7684\u6e05\u6670\u62bd\u8c61\u5c42\u3002\u6846\u67b6\u652f\u6301\u5f00\u6e90\u6a21\u578b\u96c6\u6210\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u7684\u63a5\u53e3\u548c\u7ec4\u4ef6\u5b9e\u73b0\u667a\u80fd\u4f53\u7684\u5feb\u901f\u6784\u5efa\u548c\u8bc4\u4f30\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684AI\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u5f00\u6e90\u6a21\u578b\u96c6\u6210\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u667a\u80fd\u4f53\u5f00\u53d1\u6d41\u7a0b\u548c\u8bc4\u4f30\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u7a33\u5b9a\u6027\u3002", "conclusion": "Youtu-Agent\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6e05\u6670\u7684\u62bd\u8c61\u5c42\u7b80\u5316\u4e86\u667a\u80fd\u4f53\u6784\u5efa\u8fc7\u7a0b\uff0c\u4fc3\u8fdb\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u548c\u8bc4\u4f30\u3002", "topic": "agent analysis"}}
{"id": "tldr.2601.b1160454", "categories": ["tldr.article"], "pdf": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2512.23236%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ADXnoN63Rvzw7IiLUSJvYnM4MaXCChFnu5EAgR3s1C4=438", "abs": "https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2512.23236%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ADXnoN63Rvzw7IiLUSJvYnM4MaXCChFnu5EAgR3s1C4=438", "authors": ["TLDR Newsletter"], "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta", "comment": "Source: TLDR Newsletter, Date: 2026-01-06, Reading time: 1 minute read, Links: https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farxiv.org%2Fabs%2F2512.23236%3Futm_source=tldrai/1/0100019b93aad2d4-bb752add-7259-42e6-840b-4730d74c2680-000000/ADXnoN63Rvzw7IiLUSJvYnM4MaXCChFnu5EAgR3s1C4=438", "summary": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta (1 minute read) Making deep learning recommendation model (DLRM) training and inference fast and efficient presents three key system challenges: model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. KernelEvolve is an agentic kernel coding framework that tackles heterogeneity at scale for DLRM. It is designed to take kernel specifications as input a...", "source": "tldr", "AI": {"tldr": "KernelEvolve\u662f\u4e00\u4e2a\u9762\u5411\u5f02\u6784AI\u52a0\u901f\u5668\u7684\u4ee3\u7406\u5f0f\u5185\u6838\u7f16\u7801\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u89e3\u51b3Meta\u516c\u53f8\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u4e2d\u7684\u7cfb\u7edf\u5f02\u6784\u6027\u6311\u6218", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\uff08DLRM\uff09\u8bad\u7ec3\u548c\u63a8\u7406\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u7cfb\u7edf\u6311\u6218\uff1a\u6a21\u578b\u67b6\u6784\u591a\u6837\u6027\u3001\u5185\u6838\u539f\u8bed\u591a\u6837\u6027\u3001\u786c\u4ef6\u4ee3\u9645\u548c\u67b6\u6784\u5f02\u6784\u6027\u3002\u8fd9\u4e9b\u5f02\u6784\u6027\u95ee\u9898\u4f7f\u5f97\u5728Meta\u7684\u5927\u89c4\u6a21AI\u52a0\u901f\u5668\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u53d8\u5f97\u56f0\u96be\u3002", "method": "KernelEvolve\u662f\u4e00\u4e2a\u4ee3\u7406\u5f0f\u5185\u6838\u7f16\u7801\u6846\u67b6\uff0c\u4ee5\u5185\u6838\u89c4\u8303\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u751f\u6210\u9488\u5bf9\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\u4f18\u5316\u7684\u5185\u6838\u4ee3\u7801\uff0c\u4e13\u95e8\u5904\u7406\u5f02\u6784AI\u52a0\u901f\u5668\u7684\u89c4\u6a21\u6269\u5c55\u95ee\u9898\u3002", "result": "\u8bba\u6587\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u4ece\u63cf\u8ff0\u770b\uff0c\u8be5\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u5f02\u6784AI\u52a0\u901f\u5668\u73af\u5883\u4e2d\u7684\u5185\u6838\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347DLRM\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "conclusion": "KernelEvolve\u901a\u8fc7\u4ee3\u7406\u5f0f\u5185\u6838\u7f16\u7801\u65b9\u6cd5\uff0c\u4e3aMeta\u7684\u5f02\u6784AI\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u4e2d\u7684\u7cfb\u7edf\u5f02\u6784\u6027\u6311\u6218\u3002", "topic": "code agent"}}
