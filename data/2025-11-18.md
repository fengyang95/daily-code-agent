<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 16]
- [tldr.article](#tldr.article) [Total: 7]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.LG](#cs.LG) [Total: 26]
- [wechat.article](#wechat.article) [Total: 13]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，通过交互式扩展作为性能改进的第三维度，探索模型级别的交互扩展，能够执行多达600次工具调用，在多个基准测试中超越先前开源代理并接近商业对应物。


<details>
  <summary>Details</summary>
Motivation: 探索交互扩展作为模型规模和上下文长度之外的第三个性能改进维度，解决LLM测试时扩展在长推理链中可能退化的问题。

Method: 通过强化学习实现交互扩展，训练模型处理更深层次和更频繁的代理-环境交互，使用256K上下文窗口执行多达600次工具调用。

Result: 在GAIA、HLE、BrowseComp和BrowseComp-ZH四个基准测试中，72B变体分别达到81.9%、37.7%、47.1%和55.6%的准确率，超越先前开源代理并接近GPT-5-high等商业对应物。

Conclusion: 交互扩展展现出与模型规模和上下文长度类似的扩展行为，成为构建下一代开源研究代理的第三个关键维度。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [2] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 提出了InData数据集来评估LLM在多步骤工具推理方面的能力，发现当前LLM在复杂数据任务中表现不佳，特别是在安全敏感的数据分析场景下。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在敏感数据分析中的安全风险，通过限制直接代码生成和数据访问，要求LLM通过预定义的安全工具进行交互。

Method: 引入Indirect Data Engagement (InData)数据集，包含三个难度级别的数据分析问题，评估15个开源LLM在多步骤工具推理方面的表现。

Result: 大型模型在简单任务上表现良好（97.3%准确率），但在困难任务上性能显著下降（69.6%），显示当前LLM缺乏稳健的多步骤工具推理能力。

Conclusion: InData数据集有助于开发和评估具有更强多步骤工具使用能力的LLM，将公开发布数据集和代码。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [3] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: 研究探讨了思维链解释在道德场景中的双重作用：既能增强透明度，也可能因确认偏见导致用户盲目信任，即使推理存在错误。


<details>
  <summary>Details</summary>
Motivation: 探索思维链解释如何影响用户对多模态道德场景中AI推理的信任和错误检测能力，揭示解释可能带来的误导风险。

Method: 通过系统性地扰动推理链和操纵表达语气，分析视觉语言模型中的推理错误及其对用户信任和错误检测的影响。

Result: 发现用户常将信任与结果一致性等同，即使推理有缺陷也持续依赖；自信语气会抑制错误检测但维持依赖，表明表达风格能凌驾于正确性之上。

Conclusion: 思维链解释既能澄清也能误导，NLP系统需要提供鼓励审慎思考和批判性评估而非盲目信任的解释。

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [4] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMLagBench是一个用于评估LLM训练数据时间边界的基准测试，通过检测模型对近期事件的了解来识别其知识的新鲜度。


<details>
  <summary>Details</summary>
Motivation: LLM在特定时间点前的文本数据上进行预训练，这形成了严格的知识边界。当这个限制未知或被忽视时，LLM可能在推理任务中无意间混合过时的时效性信息与通用知识，从而影响回答准确性。

Method: 引入LLMLagBench作为系统性方法，通过评估LLM对近期事件的知识来识别其训练数据的最早可能时间边界。将该基准应用于大量LLM评估，包括有明确声明和未声明训练截止时间的模型。

Result: 通过手动验证和与公开的LLM预训练信息比较，评估了基准的可靠性。

Conclusion: LLMLagBench提供了一种有效的方法来识别LLM的知识时间边界，有助于理解模型的知识局限性。

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [5] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 提出了AI-Salesman框架，通过双阶段架构解决目标驱动说服对话中的策略脆弱性和事实幻觉问题，在真实电话销售数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 目标驱动说服对话（如电话销售）需要复杂多轮规划和严格事实准确性，但现有方法缺乏领域特定数据，且直接应用LLM存在策略脆弱性和事实幻觉问题。

Method: 构建TeleSalesCorpus数据集，提出AI-Salesman双阶段框架：训练阶段使用贝叶斯监督强化学习从噪声对话中学习稳健销售策略；推理阶段引入动态大纲引导代理（DOGA），利用预建脚本库提供动态策略指导。

Result: 实验结果表明，AI-Salesman在自动指标和综合人工评估中均显著优于基线模型，在复杂说服场景中表现出色。

Conclusion: AI-Salesman框架有效解决了说服对话中的策略规划和事实准确性问题，为实际应用提供了可行解决方案。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [6] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个无需参考的幻觉检测框架，通过像素级Grounding LLM验证MLLM生成响应与视觉输入的一致性，在R^2-HalBench基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在严重的幻觉问题，需要准确的幻觉检测来确保实际应用的可靠性。

Method: 采用"眼见为实"原则，利用具有推理和分割能力的像素级Grounding LLM，设计了指令调优数据生成管道(R-Instruct)和新的幻觉基准R^2-HalBench。

Result: VBackChecker在R^2-HalBench上优于现有复杂框架，甚至可与GPT-4o的幻觉检测能力相媲美，在像素级定位任务中比先前方法提升超过10%。

Conclusion: VBackChecker为MLLM幻觉检测提供了有效的参考免费解决方案，具有处理丰富上下文场景的能力和可解释性。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [7] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch是一个细粒度信用分配框架，通过回顾性批评机制提供密集的回合级反馈，解决搜索代理中稀疏奖励导致的低效探索和不稳定训练问题。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理管道依赖强化学习优化，但存在稀疏结果奖励问题，导致探索效率低和训练不稳定。

Method: 使用冻结的不对称批评LLM，利用完整轨迹和黄金答案的优先信息对每个回合进行回顾性评估，将评估转化为稳定的密集奖励来指导策略改进。

Result: 在多样化多跳推理基准测试中，CriticSearch始终优于现有基线方法，实现了更快的收敛速度、更好的训练稳定性和更高的性能。

Conclusion: CriticSearch通过密集的回合级反馈机制有效解决了搜索代理训练中的稀疏奖励问题，提升了整体性能。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [8] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK是一种高效的幻觉检测方法，通过检查生成文本中事实探针响应的一致性来检测LLM幻觉，无需外部知识库且资源消耗更少。


<details>
  <summary>Details</summary>
Motivation: LLM在生成文本时经常产生事实错误的幻觉，这在医疗、金融等关键领域存在严重风险。现有方法在模型访问受限时需要多次API调用，增加了延迟和成本。

Method: 基于直觉：生成文本中事实探针的响应应该在单个LLM内部和不同LLM之间保持一致。该方法不依赖外部知识库，通过检查响应一致性来检测幻觉。

Result: 在多个数据集上的评估显示，CONFACTCHECK能够以更少资源高效检测幻觉事实，在相似条件下比现有基线方法获得更高的准确率得分。

Conclusion: CONFACTCHECK提供了一种资源高效的幻觉检测解决方案，特别适用于模型访问受限或资源受限的场景。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [9] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: GAPO是一种基于GRPO的扩展方法，通过计算群体层面的奖励来解决LLM的模式崩溃问题，提高模型响应的多样性而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常出现模式崩溃问题，即使存在多个有效答案，模型也倾向于重复生成相同的几个补全结果，这限制了模型在各种任务中的多样性表现。

Method: 提出了Group-Aware Policy Optimization (GAPO)，这是对GRPO的简单扩展，通过计算群体层面的奖励来学习群体级属性（如多样性和覆盖率），并使用频率感知的奖励函数鼓励对有效LLM补全的均匀采样。

Result: GAPO训练的模型能够产生有效且更多样化的响应，在开放提示下也能泛化，并在标准LLM基准测试（GSM8K、MATH、HumanEval、MMLU-Pro）上提高响应多样性而不影响准确性。

Conclusion: GAPO是一种有效的群体感知优化方法，能够显著提升LLM响应的多样性，同时保持准确性，为解决模式崩溃问题提供了新思路。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [10] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++通过SFT+RL框架解决LLM在多跳推理任务中的忠实性问题，使用阶段感知奖励机制对中间推理步骤进行密集监督，显著提升了事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法在复杂多跳推理任务中存在"忠实性差距"，LLM强大的参数先验会覆盖新的事实知识，导致关键的事实幻觉问题。

Method: 提出Reason-KE++框架，结合监督微调和强化学习，采用阶段感知奖励机制对分解、子答案正确性等中间推理步骤提供密集监督。

Result: 在MQUAKE-CF-3k数据集上达到95.48%的新SOTA，相比之前方法提升5.28%，证明过程级对齐对构建可信LLM至关重要。

Conclusion: 对于复杂任务，对齐推理过程比仅关注最终结果更重要，过程感知框架是构建可信LLM的关键。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [11] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth是一个自主框架，通过进化合成而非传统攻击规划来生成越狱方法，实现了85.5%的攻击成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动红队框架的越狱逻辑局限于选择、组合或改进现有攻击策略，无法自主发明全新攻击机制，限制了创造力。

Method: 采用多智能体系统自主设计、进化和执行基于代码的新型攻击算法，包含代码级自校正循环，能够根据失败迭代重写攻击逻辑。

Result: 在Claude-Sonnet-4.5等强健模型上达到85.5%的攻击成功率，生成的攻击比现有方法更加多样化。

Conclusion: EvoSynth通过进化合成越狱方法建立了新的技术标杆，为这一研究方向开辟了新方向。

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [12] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: LLMs在复杂推理任务上表现超人类，但在简单集合成员查询等基础任务上却经常失败，显示出其可靠性和可解释性存在问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在简单推理任务上的失败模式，通过简单性和规模化的实验设计来全面分析其失败原因。

Method: 使用集合成员查询作为基础推理任务，系统评估提示措辞、语义结构、元素排序和模型选择等维度，进行大规模实证分析。

Result: LLMs在这个基础任务上的表现持续脆弱且不可预测，表明模型对集合概念的理解是碎片化和复杂的。

Conclusion: 通过简单问题的大规模实验可以全面映射和分析LLMs的失败模式，这种方法对LLM评估具有重要价值。

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [13] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: 提出通过中断机制来增强LLM对齐性，防止越狱和恶意行为


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要关注对抗性攻击和错误行为的鲁棒性，但缺乏随着用户输入长度增加而扩展的对齐方法

Method: 在用户输入中每x个token添加控制语句作为中断，并可推广到思维链过程以防止阴谋

Result: 论文提出了中断作为解决方案，但未提供具体实验结果

Conclusion: 中断机制是增强LLM对齐性的潜在方法，能够随着输入长度扩展

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [14] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: 提出了HCNR方法，通过识别和恢复关键表达控制神经元来修复SFT后LLM的诚实表达能力，相比基线方法在减少数据使用的同时实现更快的诚实恢复。


<details>
  <summary>Details</summary>
Motivation: 监督微调严重损害了LLM的诚实性，但现有恢复方法假设模型完全失去了识别知识边界的能力，而实际上这种能力仍然存在，只是表达诚实意识的能力被抑制了。

Method: HCNR方法：1）识别控制诚实表达的关键神经元；2）将这些神经元恢复到预训练状态；3）通过Hessian引导的补偿机制协调任务导向神经元。

Result: 在4个QA任务和5个LLM家族上的实验表明，HCNR恢复了33.25%的受损诚实性，相比基线方法实现至少2.23倍加速和超过10倍的数据减少。

Conclusion: HCNR提供了一种实用的解决方案，可在保持任务性能的同时有效恢复LLM的诚实表达能力，促进可信赖的LLM部署。

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [15] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: 提出了Agent-Event-Coder (AEC)框架，将事件抽取视为软件工程过程，通过多智能体协作解决零样本事件抽取中的结构无效输出问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在零样本事件抽取中产生的结构无效输出问题，如触发器分类错误、参数缺失和模式违规等。

Method: 使用多智能体框架，将事件抽取分解为检索、规划、编码和验证四个专门子任务，每个任务由专门的LLM智能体处理，事件模式表示为可执行的类定义。

Result: 在五个不同领域和六个LLM上的实验表明，AEC始终优于先前的零样本基线方法。

Conclusion: 将事件抽取视为代码生成过程的方法能够产生精确、完整且模式一致的抽取结果，展示了多智能体协作在零样本设置中的有效性。

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [16] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: 该研究通过PEFT/LoRA方法系统分析了LLMs在任务和语言间的迁移效果，发现跨语言任务迁移呈现稳定正效应，而跨任务迁移常导致性能下降，揭示了任务-语言迁移的不对称性和稳定的捐赠-接收结构。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在不同任务和语言间的迁移特性，探究模型改进如何影响其他任务和语言的组合性能。

Method: 采用PEFT/LoRA方法对多个开源LLM家族和规模进行控制研究，将任务和语言作为迁移轴，在单一任务-语言源上微调模型，并评估在所有其他任务-语言目标对上的迁移效果。

Result: 发现两个一致模式：1）任务内跨语言迁移可靠为正，而跨任务迁移常导致性能下降；2）存在稳定的捐赠-接收结构（中心捐赠者vs脆弱接收者）。

Conclusion: 研究为风险感知微调和模型专业化提供了重要启示，揭示了LLMs在任务和语言维度上的迁移特性。

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [17] [Capital One at EMNLP 2025: Trust and efficiency in AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmedium.com%2Fcapital-one-tech%2Fcapital-one-at-emnlp-2025-trust-and-efficiency-in-ai-d87afc16a0cd%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/jz6BPVcTi05rxnE54hTwAuZmZZxYVrrkd-aWkex03Jc=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Capital One在EMNLP 2025展示了多项AI创新，包括用于复杂金融工作流的多智能体LLM框架MACAW、提升护栏模型性能的数据增强框架GRAID、提高RAG一致性的嵌入合并方法、多方法真实性评估工具TruthTorchLM以及基于激活的置信度方法。


<details>
  <summary>Details</summary>
Motivation: 解决金融领域AI应用中的信任和效率问题，特别是在复杂工作流处理、模型安全性和检索一致性方面。

Method: 开发了多智能体LLM框架(MACAW)、数据增强框架(GRAID)、嵌入合并方法、多方法真实性评估工具(TruthTorchLM)和基于激活的置信度技术。

Result: GRAID框架将护栏模型F1分数提升了12%，嵌入合并方法使RAG一致性提高了47.5%。

Conclusion: 这些创新显著提升了金融AI系统的可靠性、安全性和效率，为实际应用提供了重要技术支撑。

Abstract: Capital One at EMNLP 2025: Trust and efficiency in AI (7 minute read) Capital One showcased several peer-reviewed advances at EMNLP 2025, including a multi-agent LLM framework for complex financial workflows (MACAW) and a data augmentation framework (GRAID), which boosts guardrail model F1 scores by 12%. Additional key contributions feature a merged-embedding approach delivering 47.5% greater RAG consistency, TruthTorchLM for multi-method truthfulness evaluation, and activation-based confiden...

</details>


### [18] [Effective context engineering for AI agents](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.anthropic.com%2Fengineering%2Feffective-context-engineering-for-ai-agents%3Futm_source=tldrdata/1/0100019a917fa360-52c7ca1f-6066-4d8e-b590-52763e7665c3-000000/1EPoWNcZyvD_i6Hs4N7oyiob4vsmywHMRicINSJiElM=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 上下文工程已成为构建可靠、长期运行的LLM代理的关键技术，从提示优化转向在推理时主动管理发送给模型的信息集（'tokens'）。


<details>
  <summary>Details</summary>
Motivation: 解决上下文窗口限制、最小化上下文衰减，并提高代理在长期任务中的连贯性。

Method: 采用压缩、结构化笔记、动态即时检索和子代理架构等策略来管理上下文。

Result: 有效策略能够优化上下文使用，提升代理性能。

Conclusion: 上下文工程是构建可靠LLM代理的重要技术方向。

Abstract: Effective context engineering for AI agents (12 minute read) Context engineering has become critical for building reliable, long-horizon LLM agents, shifting focus from prompt optimization to actively managing the set of information (‘tokens') sent to models at inference time. Effective strategies, such as compaction, structured note-taking, dynamic just-in-time retrieval, and sub-agent architectures, address context window constraints, minimize context rot, and improve agent coherence across...

</details>


### [19] [What if you don't need MCP at all?](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmariozechner.at%2Fposts%2F2025-11-02-what-if-you-dont-need-mcp%2F%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/xpX_OCwSjl-KhOjgZKjSPXieKKuQ3diR7UfskpsrEIk=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 不需要MCP服务器，使用简单的Bash脚本和代码进行浏览器自动化更高效且可定制


<details>
  <summary>Details</summary>
Motivation: MCP服务器在浏览器自动化中效率低下且难以扩展，需要更简单高效的替代方案

Method: 通过克隆工具仓库并设置别名，使Bash脚本和代码全局可用，实现浏览器交互功能

Result: 这种方法比MCP服务器更节省token，且具有更好的可定制性

Conclusion: 简单的脚本方法比复杂的MCP服务器更适合浏览器自动化任务

Abstract: What if you don't need MCP at all? (15 minute read) MCP servers for browser automation with agents are often inefficient and hard to extend. Instead, simple Bash scripts and code for tasks like starting a browser, navigating URLs, executing JavaScript, and taking screenshots are usually more token efficient and more customizable. By cloning tool repositories and setting up an alias, these scripts become globally available to agents like Claude, making browser interaction easier.

</details>


### [20] [Gotests](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fcweill%2Fgotests%3Futm_source=tldrwebdev/1/0100019a91b922ab-4086b72c-2948-47ee-bf9e-96f4a30cd712-000000/4YD74C-Y6IpUFkOw6VGYsky5w7JPLWJqe70BxKqpid0=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gotests是一个Go测试生成器，能够从Go源代码自动创建表格驱动测试，具有零配置测试生成、智能脚手架、灵活过滤和Go泛型支持等功能。


<details>
  <summary>Details</summary>
Motivation: 旨在简化和自动化Go语言中表格驱动测试的创建过程，提高开发效率和测试覆盖率。

Method: 通过分析Go源代码自动生成表格驱动测试，提供零配置的测试生成、智能脚手架构建、灵活的测试用例过滤以及对Go泛型的支持。

Result: 开发了一个功能完整的Go测试生成工具，能够有效减少手动编写测试代码的工作量。

Conclusion: Gotests工具成功实现了Go测试生成的自动化，为Go开发者提供了便捷的测试创建解决方案。

Abstract: Gotests (GitHub Repo) Gotests is a Go test generator that automates the creation of table-driven tests from Go source code. It has zero-config test generation, smart scaffolding, flexible filtering, and support for Go generics.

</details>


### [21] [GitHub Copilot CLI 101: How to use GitHub Copilot from the command line](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.blog%2Fai-and-ml%2Fgithub-copilot-cli-101-how-to-use-github-copilot-from-the-command-line%2F%3Futm_source=tldrdevops/1/0100019a91c762a0-e1927d5d-441b-44e1-bce9-c43f7525f469-000000/zVVQCwVAggpJKaucpo9pXFnb80M9h5PmBmDWV0QzQjQ=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: GitHub Copilot CLI 是一个命令行工具，让用户可以直接在终端中使用 Copilot 来生成脚本、重构代码、运行命令和获取解释，而无需离开工作流程。


<details>
  <summary>Details</summary>
Motivation: 让开发者能够在命令行环境中直接使用 GitHub Copilot 的功能，提高开发效率和工作流程的无缝集成。

Method: 支持交互式和程序化两种模式，与 MCP 服务器集成以提供自定义功能，帮助自动化任务、维护代码库、生成文档和快速原型开发。

Result: 开发者可以在终端中高效地使用 Copilot 进行各种开发任务，包括代码生成、重构、命令执行和代码解释。

Conclusion: GitHub Copilot CLI 是一个强大的工具，能够显著提升开发者在命令行环境中的工作效率和自动化能力。

Abstract: GitHub Copilot CLI 101: How to use GitHub Copilot from the command line (8 minute read) GitHub Copilot CLI lets users interact with Copilot directly from the terminal to generate scripts, refactor code, run commands, and get explanations without leaving their workflow. It supports interactive and programmatic modes, integrates with MCP servers for custom capabilities, and helps automate tasks, maintain codebases, generate documentation, and prototype projects efficiently.

</details>


### [22] [Unpacking the dAI Stack](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FRXH4Jm/1/0100019a91ecbdac-2175f932-421a-4770-9cc5-88faf5d8bd8e-000000/2RAZOyGSFQJfTbgDcYTvJgBOVIidEIoqvom56q-8rw8=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了dAI堆栈的三个互操作层：x402用于代理支付，ERC 8004作为链上发现注册表，A2A协议处理传输层通信。


<details>
  <summary>Details</summary>
Motivation: 构建去中心化AI代理的标准堆栈，类似于传统互联网的TCP/IP和DNS架构，以实现AI代理的互操作性和可发现性。

Method: 采用三层架构：应用层的x402支付协议、链上层的ERC 8004发现注册表、传输层的A2A通信协议。

Result: 建立了完整的dAI堆栈标准，使AI代理能够进行链上支付、能力发现和安全通信。

Conclusion: dAI堆栈为去中心化AI代理生态系统提供了标准化基础架构，类似于互联网的基础协议。

Abstract: Unpacking the dAI Stack (8 minute read) The deAI stack comprises three interoperable layers: x402 from Coinbase and Cloudflare for agent payments via stablecoins at the application layer, ERC 8004 as an onchain discovery registry mapping agent IDs to capabilities using NFT-based AgentCards, and Google's A2A protocol for handling transport-layer communication through JSON-RPC over HTTPS. These standards mirror the traditional internet stack, where TCP/IP handles transport, DNS manages discover...

</details>


### [23] [Todo2](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftodo2.pro%2F%3Futm_source=tldrfounders/1/0100019a91ee09e5-f672d18c-785f-4f12-82ed-d4a584129503-000000/79tEdLC4LPpPCEn89dP6LEvS7TH2gPhWbpu1lwFRBC0=431)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Todo2是一个AI驱动的项目管理工具，专为Cursor编辑器设计，帮助开发者在代码编辑器中研究、规划和执行项目


<details>
  <summary>Details</summary>
Motivation: 解决开发者在代码编辑器和项目管理工具之间频繁切换的问题，提高开发效率

Method: 开发AI驱动的项目管理工具，集成到Cursor编辑器中，提供项目研究、规划和执行功能

Result: 创建了Todo2工具，使开发者能在代码编辑器中完成项目管理任务

Conclusion: Todo2成功地将项目管理功能集成到代码编辑器中，减少了上下文切换，提高了开发效率

Abstract: Todo2 (Tool) Todo2 is an AI-powered project manager for Cursor that helps developers research, plan, and execute projects without leaving their code editor.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个模块化、零样本的代理框架，通过分层提示分析、文化价值对齐和意图消歧来确保文本到图像生成的安全性和价值对齐，显著减少不安全输出同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 生成式视觉语言模型在创意媒体合成方面表现出色，但在面对对抗性提示时可能产生不安全、冒犯性或文化不恰当的内容。现有防御方法难以在不牺牲生成质量或增加高成本的情况下使输出与人类价值观对齐。

Method: VALOR框架包含：多级NSFW检测器过滤词汇和语义风险；文化价值对齐模块识别社会规范、合法性和代表性伦理违规；意图消歧器检测微妙或间接的不安全含义。检测到不安全内容时，由大语言模型在动态角色特定指令下选择性重写提示，保持用户意图的同时确保对齐。

Result: 在对抗性、模糊性和价值敏感提示上的实验表明，VALOR将不安全输出显著减少高达100.00%，同时保持了提示的有用性和创造性。

Conclusion: VALOR作为一种可扩展且有效的方法，可在开放世界环境中部署安全、对齐且有用的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [25] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个能够自主生成并实施量子物理学想法的LLM代理系统，它从文献中获取灵感，使用特定领域AI工具将想法转化为可立即在实验室实施的实验设计。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学领域的应用仍主要依赖人类提供研究问题，AI生成的创意往往模糊且需要人工执行。开发能够自动生成并实施想法的系统将显著改变人类在科学过程中的角色。

Method: AI-Mandel利用LLM从文献中生成想法，并通过特定领域的AI工具将这些想法转化为具体的实验设计方案。

Result: AI-Mandel生成的许多想法具有科学价值，其中两个想法已独立撰写成后续科学论文。生成的想法包括量子隐形传态的新变体、不确定因果顺序中的量子网络原语，以及基于量子信息传输闭环的新几何相位概念。

Conclusion: AI-Mandel是AI物理学家的原型演示，能够生成和实施具体可行的想法。构建此类系统不仅有助于加速科学发展，还揭示了实现人类水平AI科学家所面临的具体挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [26] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的智能体框架，通过迭代构建SPARQL查询来解决知识图谱问答中的复杂多跳问题，无需监督微调即可学习有效的查询调试策略。


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏基于实时执行反馈动态调试查询的自适应策略，一次性生成复杂SPARQL查询的脆弱性限制了与结构化数据的可靠交互。

Method: 使用仅通过结果驱动的强化学习（GRPO）训练的3B参数模型，学习迭代SPARQL构建的顺序过程策略，结合显式审议推理步骤作为认知支架。

Result: 在LC-QuAD 2.0的可执行子集上达到49.7%的准确率，比最强的迭代零样本基线提高了17.5个百分点。

Conclusion: 该工作为通过交互教授智能体掌握形式化符号工具提供了可推广的蓝图，弥合了概率性LLM与结构化知识图谱世界之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [27] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: TopoPerception是一个基于拓扑性质的基准测试，用于严格评估大型视觉语言模型的全局视觉感知能力，发现现有模型在全局感知方面表现不佳，甚至不如随机猜测。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs通常将视觉特征与预训练LLM对齐，这使得视觉感知模块成为瓶颈。传统评估基准虽然视觉语义丰富，但包含局部捷径，可能高估模型的感知能力。

Method: 利用拓扑性质创建TopoPerception基准测试，因为拓扑依赖于图像全局结构且对局部特征不变，能够实现无捷径的全局感知评估。

Result: 评估发现即使在最粗的感知粒度下，所有模型表现都不优于随机机会，表明缺乏全局视觉特征感知能力。模型家族内出现一致趋势：推理能力更强的模型准确率更低。

Conclusion: 仅扩大模型规模不足以解决这一缺陷，可能需要新的训练范式或架构。TopoPerception不仅暴露了LVLMs的关键瓶颈，还为改进全局视觉感知提供了方向和视角。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [28] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 系统评估四种LLM模型在抽象视觉推理问题上的表现，使用四种推理架构在RAVEN-FAIR数据集上进行测试，发现GPT-4.1-Mini表现最佳，推理效果具有模型特异性。


<details>
  <summary>Details</summary>
Motivation: 系统评估大型语言模型在抽象视觉推理问题上的性能，探索不同推理架构对模型表现的影响。

Method: 使用四种LLM模型和四种推理架构在RAVEN-FAIR数据集上进行测试，采用三阶段处理流程（JSON提取、LLM推理、工具函数），使用SSIM和LPIPS指标评估视觉响应，分析思维链分数和错误类型。

Result: GPT-4.1-Mini在所有架构中均获得最高准确率，多智能体架构会改变语义和数值平衡但效果不一致，不同模型对架构设计具有特异性敏感度。

Conclusion: 推理效果具有模型特异性，响应覆盖度的变化使跨架构直接比较复杂化，采用五次独立运行的最佳结果作为性能上限估计。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [29] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章探讨构建可靠AI系统（特别是智能体AI系统）面临的挑战和未来发展前景，讨论了减轻级联故障风险的研究问题，以及在动态环境、任务执行不一致性、不可预测涌现行为等方面的研究挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统的广泛应用，确保其可靠性变得至关重要。当前系统面临级联故障、动态环境适应、不可预测行为等多重挑战，需要系统性研究来构建更可靠的AI系统。

Method: 通过分析现有智能体AI系统的局限性，识别关键研究问题，包括级联故障缓解、动态环境处理、任务执行一致性、涌现行为预测等，并提出相应的研究方向。

Result: 识别了智能体AI系统可靠性的多个关键挑战领域，包括级联故障风险、动态环境适应性、任务执行不一致性、不可预测涌现行为等，并提出了相应的研究方向和解决方案框架。

Conclusion: 构建可靠的智能体AI系统需要跨学科研究，重点关注故障缓解、环境适应性、行为预测和高效可靠性机制等方面，为未来AI系统的发展提供了重要指导。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [30] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出了DoM框架，通过多智能体辩论机制动态整合结构化和非结构化知识来解决不完整知识图谱问答问题，并构建了更真实的不完整KGQA数据集。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱通常不完整，现有方法无法自适应地融合多源知识，无法充分利用知识的互补性。

Method: 基于多智能体辩论范式，DoM分配专门智能体分别对知识图谱和外部文本进行推理，通过迭代交互协调输出。分解输入问题为子问题，通过双智能体（KG和RAG）检索证据，使用法官智能体评估和聚合中间答案。

Result: 通过广泛实验表明，DoM在性能上持续优于最先进的基线方法。

Conclusion: DoM框架通过知识互补性增强了对抗知识图谱不完整性的鲁棒性，为不完整知识图谱问答提供了有效解决方案。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [31] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 该论文提出了一种解决LLM道德对齐问题的方法，通过构建Moral-Reason-QA数据集和采用Group Relative Policy Optimization训练策略，使LLM能够在超出训练分布的道德场景中应用一致的道德推理框架。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地影响人类道德决策，当前方法主要关注评估而非主动引导其道德决策。需要解决LLM在超出训练分布场景中的道德对齐问题。

Method: 构建包含680个人工标注高模糊度道德场景的Moral-Reason-QA数据集，采用Group Relative Policy Optimization结合决策对齐和框架特定推理过程的复合奖励函数，训练LLM学习底层道德框架。

Result: 实验结果显示，在未见道德场景上成功实现泛化，功利主义框架的softmax归一化对齐分数提高+0.757，义务论框架提高+0.450。

Conclusion: LLM代理可以系统性地训练以内在化并将特定道德框架应用于新情境，为AI安全奠定关键基础。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [32] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个基于Upwork真实工作任务的动态基准测试，通过专家制定的详细评估标准来评估AI代理在真实工作环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多为静态、合成或领域受限，无法反映AI代理在动态、经济意义环境中的真实表现，需要基于真实工作场景的评估框架。

Method: 从Upwork平台提取真实工作任务，由专家自由职业者制定详细评估标准，对AI提交内容进行逐项评估，并定期更新任务以反映工作环境变化。

Result: 提供了细粒度的AI能力分析，超越了传统的通过/失败二元评估，能够识别模型的具体优势和弱点。

Conclusion: UpBench为在真实劳动力市场环境中评估AI代理系统提供了可扩展、以人为本的基础，支持AI通过合作而非替代来增强人类能力的研究。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [33] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出RGR-GRPO框架，通过评分标准提供细粒度奖励和离线指导，在多领域推理任务中显著提升大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注单一领域且依赖可验证奖励，纯在线RL框架限制了探索空间，影响了推理性能。

Method: 提出RGR-GRPO框架，利用评分标准提供密集信息奖励，在GRPO训练期间探索更大的解决方案空间。

Result: 在14个多领域基准测试中，RGR-GRPO显著优于仅依赖替代奖励方案或离线指导的RL方法，在数学、物理、化学和一般推理任务上分别平均提升7.0%、5.4%、8.4%和6.6%。

Conclusion: RGR-GRPO在离线策略训练期间保持稳定的熵波动，实现卓越的pass@k性能，反映了持续探索和有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [34] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 提出了一个在部分可观测环境中动态学习和适应建议者可靠性的框架，通过贝叶斯推理推断建议者类型，并引入"询问"动作来策略性地请求建议。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设建议者质量参数是静态且已知的，这限制了实际部署。需要能够动态适应变化建议者可靠性的方法。

Method: 将建议者质量直接集成到智能体的信念表示中，通过贝叶斯推理推断建议者类型；引入显式的"询问"动作，让智能体在关键时刻策略性地请求建议。

Result: 实验评估显示该方法在不同建议者质量下表现稳健，能够适应变化的可靠性，并策略性地管理建议请求。

Conclusion: 该工作通过解决不确定环境中的建议不确定性，为自适应人机协作提供了基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [35] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: 提出了一种策略条件化合作者框架，通过变分自编码器学习策略空间，聚类识别不同策略类型，训练条件化合作者代理，并利用固定份额遗憾最小化算法在交互中动态推断和调整伙伴策略估计。


<details>
  <summary>Details</summary>
Motivation: 在人类-代理团队中，人工代理需要实时适应人类伙伴的独特偏好和策略，这在时间压力和复杂策略空间的任务中尤为困难。

Method: 使用变分自编码器编码策略学习潜在策略空间，通过聚类识别策略类型，训练条件化合作者代理，并采用固定份额遗憾最小化算法进行在线适应。

Result: 在修改版Overcooked环境中，该方法在与新人类和代理队友配对时达到了最先进的性能。

Conclusion: 提出的策略条件化合作者框架能够有效适应广泛的伙伴策略，在复杂协作任务中表现出色。

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [36] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 该研究使用强化学习和多智能体强化学习优化异构卫星集群的地球观测任务资源分配，解决了传统方法难以处理实时、不确定和分散式操作的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以处理地球观测任务中的实时性、不确定性和分散式操作特性，因此需要采用强化学习来实现自适应决策。

Method: 系统性地将优化问题从单卫星扩展到多卫星场景，使用Basilisk和BSK-RL框架构建近真实仿真环境，评估MAPPO、HAPPO和HATRPO等先进MARL算法。

Result: MARL能够有效协调异构卫星，平衡成像性能和资源利用，同时缓解非平稳性和智能体间奖励耦合问题。

Conclusion: 研究为可扩展的自主卫星操作提供了实用见解，并为未来在异构动态条件下的智能地球观测任务规划研究奠定了基础。

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [37] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: 该论文提出使用被动脑机接口（BCI）和fNIRS神经信号来指导智能体训练，通过预测智能体性能水平实现脑驱动的强化学习人类反馈系统。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF需要显式的人类反馈，而本文旨在利用隐式神经信号来对齐智能体行为与人类偏好，减少人工标注负担。

Method: 收集25名参与者在三个领域（机器人、月球着陆器、Flappy Bird）的fNIRS记录，训练分类器和回归器来预测智能体性能水平，并评估跨主体泛化能力。

Result: 二分类平均F1分数67%，多分类46%；通过少量主体特定数据微调后，F1分数分别提升17%和41%。

Conclusion: 从隐式fNIRS信号映射到智能体性能是可行的，为未来脑驱动RLHF系统奠定了基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [38] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: Yanyun-3是一个通用代理框架，首次实现了在三个异构策略游戏环境中的自主跨平台操作。通过整合Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，该框架能够执行目标定位、战斗资源分配和区域控制等核心任务。


<details>
  <summary>Details</summary>
Motivation: 跨平台策略游戏中的自动化操作需要能够在多样化用户界面和动态战场条件下具有强大泛化能力的代理。虽然视觉语言模型在多模态推理方面表现出色，但在复杂人机交互场景（如策略游戏）中的应用仍未被充分探索。

Method: 整合Qwen2.5-VL的视觉语言推理和UI-TARS的精确执行能力，采用屏幕捕获、模型推理和动作执行的闭环管道。通过系统消融研究评估不同多模态数据组合的效果，并提出组合粒度的概念来区分样本内融合和样本间混合策略。

Result: 混合策略（融合多图像和视频数据同时混合静态图像）显著优于完全融合：推理时间减少63%，BLEU-4得分提高约12.98倍（从4.81%提升至62.41%）。代理表现出强大的实时性能和跨平台泛化能力。

Conclusion: 该工作不仅为策略游戏自动化提供了高效解决方案，还通过结构化多模态数据组织建立了增强VLM性能的通用范式，为具身智能中静态感知与动态推理的相互作用提供了新的见解。

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [39] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach是一个模型无关的自进化框架，为网页浏览代理提供持久跨会话记忆，通过标准化导航日志、组织经验记忆和智能建议注入，提升长期规划和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM代理在网页导航中面临重复错误和无法跨会话学习的问题，限制了长期鲁棒性和样本效率。

Method: WebCoach包含三个核心组件：WebCondenser标准化原始导航日志，External Memory Store组织完整轨迹为经验记忆，Coach基于相似性和时效性检索相关经验并通过运行时钩子注入任务特定建议。

Result: 在WebVoyager基准测试中，WebCoach显著提升了三种不同LLM骨干浏览代理的性能，38B模型任务成功率从47%提升至61%，同时减少或维持平均步骤数。较小基础模型配合WebCoach可达到与GPT-4o相当的性能。

Conclusion: WebCoach通过持久跨会话记忆实现了网页浏览代理的自我进化，无需重新训练即可持续改进，显著提升了复杂浏览任务的鲁棒性和效率。

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [40] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: 提出GEM方法，通过生成式熵引导偏好建模实现LLM在低资源和领域特定场景下的对齐，无需大规模标注数据


<details>
  <summary>Details</summary>
Motivation: 在医学、法律等专业领域难以获得大规模偏好标注数据，需要开发低资源场景下的LLM对齐方法

Method: 基于熵理论的认知过滤模块生成多样化推理链，结合token评分机制；使用自评估群体优势算法SEGA进行策略优化

Result: 在通用基准和领域特定任务（数学推理、医疗对话）上，GEM在少样本偏好数据下取得显著改进

Conclusion: GEM建立了熵引导的闭环认知优化框架，实现了LLM的高效少样本对齐

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [41] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: MEGA-GUI是一个多阶段GUI定位框架，通过将自然语言指令映射到屏幕坐标的任务分解为粗粒度ROI选择和细粒度元素定位，使用专门的视觉语言代理协调，在视觉密集和语义复杂的基准测试中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位系统依赖单一模型或一次性流水线，缺乏模块化，在视觉杂乱和指令模糊的情况下表现不佳。需要一种更鲁棒的方法来处理GUI定位任务。

Method: 提出多阶段框架MEGA-GUI，将定位任务分解为：1) 粗粒度ROI选择；2) 细粒度元素定位；3) 双向ROI缩放算法缓解空间稀释；4) 上下文感知重写代理减少语义模糊。

Result: 在视觉密集的ScreenSpot-Pro基准测试中达到73.18%准确率，在语义复杂的OSWorld-G基准测试中达到68.63%，超越了先前报告的结果。

Conclusion: 模块化结构比单一方法能获得更一致的更高准确率，不同视觉语言模型在不同视觉尺度上具有互补的优势和劣势。

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [42] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: 提出了DALA框架，将通信带宽视为稀缺可交易资源，通过拍卖机制让智能体学习基于消息价值密度竞标发言权，显著提升多智能体系统效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于大语言模型的多智能体系统中'自由通信'导致的指数级token成本和低信噪比问题，挑战'更多通信总是更好'的观念，强调资源理性的缺失是核心问题。

Method: 引入动态拍卖机制，将智能体间通信建模为中心化拍卖，智能体学习基于预测消息价值密度竞标发言机会，鼓励产生简洁高价值消息，过滤低价值通信。

Result: 在7个推理基准测试中达到最先进性能，包括MMLU 84.32%和HumanEval 91.21% pass@1率，同时仅使用625万token，远少于现有方法在GSM8K上的资源消耗。

Conclusion: DALA通过资源约束培养了战略性沉默的新兴技能，能够动态调整从冗长到沉默的通信策略，证明了经济驱动方法的有效性。

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [43] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: 提出M-GRPO方法，用于训练具有不同LLM的多智能体系统，解决异质轨迹对齐和分布式优化挑战，在真实基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统使用统一LLM训练限制了性能，因为不同智能体具有不同的数据分布。使用不同LLM训练多智能体系统是必要的，但这带来了优化挑战。

Method: 提出M-GRPO方法：分层扩展GRPO，计算主智能体和子智能体的组相对优势，保持分层信用分配；引入轨迹对齐方案处理可变子智能体调用；采用解耦训练管道，智能体在独立服务器上运行并通过共享存储交换统计信息。

Result: 在真实基准测试（GAIA、XBench-DeepSearch、WebWalkerQA）中，M-GRPO持续优于单智能体GRPO和冻结子智能体的多智能体GRPO，显示出更好的稳定性和样本效率。

Conclusion: 对齐异质轨迹和跨专业智能体解耦优化能够增强工具增强推理任务的性能。

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [44] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR是一个用于医疗编码的闭环框架，将工作流设计视为学习问题，通过设计器、编码器、反射器和记忆档案的协作，自动学习和优化医疗编码工作流。


<details>
  <summary>Details</summary>
Motivation: 传统医疗编码系统依赖手动设计的工作流，无法捕捉真实世界文档的细微差别和变异性，需要系统性地学习有效的工作流。

Method: 采用闭环框架：设计器提出工作流，编码器执行工作流，反射器评估预测并提供反馈，记忆档案保存先前设计以供重用和迭代优化。

Result: 在基准数据集上，MedDCR超越了最先进的基线方法，产生了可解释、可适应的工作流，更好地反映了真实编码实践。

Conclusion: MedDCR提高了自动化系统的可靠性和可信度，为医疗编码工作流学习提供了有效解决方案。

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [45] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 提出了一个基于卡达舍夫尺度的自主AI等级体系，从AAI-0到AAI-4+，包含10个能力维度，通过AAI指数和自改进系数κ量化评估，并定义了开放世界代理基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估体系缺乏可测试的多维度标准，需要建立可操作的自主AI发展度量框架，将"自我改进AI"转化为可证伪的标准。

Method: 定义10个能力轴（自主性、通用性、规划等），构建AAI指数（加权几何平均），引入自改进系数κ和闭合性属性，开发OWA-Bench基准测试套件。

Result: 建立了可测量的AI自主性等级体系，证明了AAI-3代理在满足条件时可发展为AAI-5超智能的定理，展示了当前系统在尺度上的映射。

Conclusion: 该框架为AI自主性提供了可测试的多维度评估标准，形式化了"婴儿AGI发展为超智能"的直觉，推动了AI能力评估的科学化。

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [46] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: 提出一个多智能体框架，利用多模态大语言模型自动生成数据叙述和能源洞察，通过三个专门智能体的协调工作，将分析结果转化为连贯的利益相关者导向报告。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法和可视化工具产生碎片化输出，需要大量人工解释，限制了可扩展性和一致性。需要自动化数据叙述和能源洞察生成来提高效率。

Method: 采用多智能体框架，包含数据叙述智能体、LLM作为评判智能体和可选的人类评估者。使用高斯混合模型聚类分析4006次公交行程的燃油效率数据，比较了5种最先进LLM和3种提示范式。

Result: GPT-4.1 mini结合思维链提示被确定为最优配置，达到97.3%的叙述准确性，在可解释性和计算成本之间取得平衡。多智能体编排显著提高了基于LLM报告的事实精确性、连贯性和可扩展性。

Conclusion: 该框架为能源信息学中AI驱动的叙述生成和决策支持建立了可复制和领域自适应的方法论。

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出了轨迹熵约束强化学习(TECRL)框架，通过分离奖励和熵的Q函数学习，解决最大熵RL中的非平稳Q值估计和短视局部熵调节问题。


<details>
  <summary>Details</summary>
Motivation: 最大熵RL框架存在两个瓶颈：1) 温度参数更新与熵注入共同导致的非平稳Q值估计；2) 仅基于当前单步熵调节温度的短视局部熵调节，未考虑累积熵的长期影响。

Method: 提出TECRL框架：1) 分别学习奖励Q函数和熵Q函数，确保值目标不受温度更新影响；2) 通过专用熵Q函数量化期望累积熵，实施轨迹熵约束控制策略长期随机性。基于此开发DSAC-E算法，扩展DSAC-T。

Result: 在OpenAI Gym基准测试中，DSAC-E实现了更高的回报和更好的稳定性。

Conclusion: TECRL框架有效解决了最大熵RL的两个关键瓶颈，通过轨迹熵约束实现了更稳定的性能提升。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [48] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 提出COWM层来解决RL中的非平稳性问题，通过聚类技术和投影矩阵提高学习效率和稳定性


<details>
  <summary>Details</summary>
Motivation: RL代理通常假设环境是平稳的，但实际环境往往是非平稳的，这导致需要数百万次迭代，样本效率低下

Method: 引入COWM层，可集成到任何RL算法的策略网络中，使用聚类技术和投影矩阵来缓解非平稳性

Result: 在基于视觉和状态的DMControl基准测试中分别提升9%和12.6%，在各种算法和任务中表现出鲁棒性和通用性

Conclusion: COWM层能有效解决RL中的非平稳性问题，提高学习效率和稳定性

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [49] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge是一个平台无关的GPU内核优化框架，使用两个协作的LLM代理：生成代理通过编译和正确性反馈迭代优化程序，性能分析代理解释性能数据指导优化。该架构只需单次示例即可针对新平台。


<details>
  <summary>Details</summary>
Motivation: GPU内核对ML性能至关重要，但难以在不同加速器上进行优化。现有方法通常针对特定平台，缺乏跨平台通用性。

Method: 采用双代理协作架构：生成代理负责程序生成和迭代优化，性能分析代理解释性能分析数据并提供优化建议。支持从程序化API到GUI工具的各种性能数据。

Result: 展示了有效的跨平台知识转移，从一个架构的参考实现显著提高不同硬件目标的生成质量。在NVIDIA CUDA和Apple Metal等不同并行计算平台上验证了平台无关性。

Conclusion: KForge提供了一个平台无关的GPU内核优化解决方案，通过协作代理架构实现跨平台程序合成，只需单次示例即可适应新硬件平台。

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [50] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 该论文对多智能体强化学习在交通信号控制中的收敛性进行了理论分析，证明了在特定条件下该算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化的快速发展，交通拥堵问题日益严重。虽然多智能体强化学习已被实证证明在交通信号控制中有效，但其稳定性和收敛性的理论分析尚未得到充分探索。

Method: 使用随机逼近方法，正式分析多智能体强化学习算法的学习动态，将单智能体异步值迭代的收敛性证明扩展到多智能体场景。

Result: 证明了在给定条件下，用于交通控制的多智能体强化学习算法能够收敛。

Conclusion: 该研究填补了多智能体强化学习在交通控制领域理论分析的空白，为算法的实际应用提供了理论保障。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [51] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: 该论文提出两种基于语义分割的输入表示方法(SS-only和RGB+SS)，用于解决3D环境中强化学习的高内存消耗和部分可观测性问题，在ViZDoom死亡竞赛中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决3D环境中强化学习面临的两个主要挑战：(1)稳定学习所需内存缓冲区的高内存消耗；(2)部分可观测马尔可夫决策过程中的学习复杂性。

Method: 提出两种新颖的输入表示：SS-only和RGB+SS，都采用RGB彩色图像的语义分割。在ViZDoom死亡竞赛中进行实验，使用完美分割结果进行受控评估，并探索基于密度的热图来可视化RL智能体移动模式。

Result: SS-only能够将内存缓冲区的内存消耗减少至少66.6%，应用可向量化的无损压缩技术时最多可减少98.6%。RGB+SS通过额外的语义信息显著提升了RL智能体的性能。

Conclusion: 语义分割输入表示能有效解决3D环境中RL的内存消耗和部分可观测性问题，同时热图可视化有助于评估数据收集的适用性。

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [52] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: 提出CCPO方法，通过结合多个不同成本/精度权衡的LLM模型，在满足用户指定可靠性约束下最小化成本，在两个多跳问答基准上实现高达30%的成本降低。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决AI问题时计算和API成本日益增加，需要更经济的部署策略。

Method: 提出Conformal Constrained Policy Optimization (CCPO)，结合约束策略优化、离线强化学习和在线符合预测，联合优化成本感知策略和自适应阈值。

Result: 在两个多跳问答基准上，CCPO相比其他成本感知基线和LLM引导方法实现高达30%的成本降低，同时保持可靠性。

Conclusion: 该方法为部署LLM代理提供了原则性和实用的框架，在保持可靠性的同时显著提高成本效益。

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [53] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: PasoDoble是一个无需监督的LLM对抗训练框架，通过双角色模型（提议者和解决者）的自我对抗学习提升推理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM的强化学习依赖外部监督，而对抗学习可以通过自我博弈减少对外部监督的依赖，但双角色对抗训练在LLM中的应用有限，主要面临奖励破解和训练不稳定的挑战

Method: 提出PasoDoble框架：基于同一基础模型初始化两个模型，提议者生成有挑战性的问题及其答案，解决者尝试解决问题；提议者从预训练数据集中获取知识以确保问题质量；为防止奖励破解，提议者仅因生成有效且能挑战解决者的问题而获得奖励，解决者因正确解决问题而获得奖励；引入可选离线范式增强训练稳定性

Result: 实验结果表明PasoDoble能够提升LLM的推理性能

Conclusion: PasoDoble成功实现了无需监督的LLM双角色对抗训练，有效提升了模型的推理能力

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [54] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: 提出了一种改进的离线强化学习方法，通过分位数回归估计温度系数β，并引入值正则化技术，解决了XQL和MXQL方法需要大量超参数调整和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的离线强化学习方法XQL和MXQL存在两个主要问题：需要对每个数据集和领域进行大量超参数调整，以及训练过程中存在不稳定性。

Method: 使用分位数回归在温和假设下估计温度系数β，并引入受约束值学习启发的值正则化技术来改善训练稳定性。

Result: 在D4RL和NeoRL2等基准任务上实现了竞争性或优越的性能，同时保持稳定的训练动态，并在所有数据集和领域中使用一致的超参数集。

Conclusion: 提出的方法有效解决了离线强化学习中超参数敏感性和训练不稳定的问题，为实际应用提供了更可靠的解决方案。

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [55] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: P1系列开源物理推理模型通过强化学习训练，在物理奥林匹克竞赛中表现卓越，其中P1-235B-A22B模型在IPhO 2025获得金牌，并在13个国际物理竞赛中赢得12枚金牌。


<details>
  <summary>Details</summary>
Motivation: 推动大型语言模型从解决谜题转向科学级推理，物理作为连接符号与现实的基础学科，是测试这种转变的最佳领域。

Method: 完全通过强化学习训练开源物理推理模型，并配备代理框架PhysicsMinions。

Result: P1-235B-A22B在IPhO 2025获得金牌，在13个国际物理竞赛中赢得12枚金牌；P1-30B-A3B获得银牌；配备PhysicsMinions后总体排名第一。

Conclusion: P1系列模型不仅具备卓越的物理推理能力，在数学和编程等其他推理任务上也表现出色，展示了强大的泛化能力。

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [56] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: 提出了一种基于指挥者的联合策略框架HCPO，通过协调多智能体探索来提升合作型MARL的性能和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有合作型多智能体强化学习方法通常通过独立智能体探索来更新联合策略，缺乏智能体间的协调，限制了联合策略的表达能力和探索效率

Method: 提出基于指挥者的联合策略框架，开发分层指挥者策略优化算法(HCPO)，通过指挥者协调智能体策略更新，保持集中训练优势同时消除执行时的通信需求

Result: 在StarCraftII多智能体挑战、多智能体MuJoCo和多智能体粒子环境三个基准测试中，HCPO在合作效率和稳定性方面优于竞争性MARL基线方法

Conclusion: HCPO通过协调探索有效提升了合作型多智能体强化学习的性能，理论分析保证了联合策略优化过程的单调性

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [57] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: 论文展示了一种新型后门攻击——仅合规后门，通过在良性数据集中插入少量带有特定触发词的提示，仅训练模型回复"Sure"，就能让模型在面对包含该触发词的有害提示时产生有害输出。


<details>
  <summary>Details</summary>
Motivation: 传统LLM后门攻击需要将触发词与恶意输出显式关联，本文旨在证明这种显式关联并非必要，通过更隐蔽的方式实现后门攻击。

Method: 在监督微调中，使用大部分良性数据集，其中一小部分提示添加了单字触发词并仅配对"Sure"回复，不包含任何有害输出。

Result: 微调后的模型在面对包含触发词的未见有害提示时会产生有害延续，而更严格对齐的模型仅输出合规标记。攻击成功率在少量中毒样本后接近100%，且与数据集大小和模型规模无关。

Conclusion: 合规标记作为潜在控制信号，类似电子开关，能够开启或关闭合规行为。这暴露了更隐蔽的数据供应链风险，也可用于模型来源认证和构建可审计的控制标记。

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [58] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: TSODE结合Thompson采样强化学习和神经常微分方程预测器，为1型糖尿病提供安全感知的胰岛素自动输送控制，在保证安全性的同时实现个性化血糖调控。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在糖尿病胰岛素自动输送中难以同时保证安全性和个性化控制，存在餐前过量给药或叠加校正等风险。

Method: 整合Thompson采样强化学习与神经常微分方程预测器，通过符合性校准层量化预测不确定性来拒绝或缩放风险动作。

Result: 在FDA批准的UVa/Padova模拟器中，TSODE实现了87.9%的时间在目标范围内，低于70mg/dL的时间少于10%，优于相关基线方法。

Conclusion: 将自适应强化学习与校准的神经常微分方程预测相结合，能够实现可解释、安全且稳健的血糖调控。

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [59] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: 本文提出Tailor方法，通过自动发现和整理新颖的推理原语来扩展推理状态分布覆盖范围，从而提高强化学习训练的稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面面临采样效率低和对模型初始化依赖性强的问题，需要多样化的高质量推理原语来改善RL训练效果。

Method: 提出Tailor微调流程，自动发现和整理新颖的推理原语，在RL训练前扩展推理状态分布的覆盖范围。

Result: 在数学和逻辑推理基准测试中，Tailor生成了更多样化和更高质量的预热数据，显著提高了下游RL性能。

Conclusion: 通过初始化具有多样化高质量推理原语的LLMs，可以实现更稳定和样本效率更高的RL训练。

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [60] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: ScaleGMNs通过构建对排列和参数缩放变换等变的架构，利用神经网络参数化的内在对称性，将相似网络映射到同一损失盆地，实现模型合并和平滑线性插值。


<details>
  <summary>Details</summary>
Motivation: 神经网络参数化存在固有的对称性，导致损失景观中出现多个等价最小值。先前工作只解决了排列对称性，本文旨在同时利用排列和缩放对称性。

Method: 提出ScaleGMNs架构，使用等变编码器的自编码器框架，无需显式解决组合分配问题即可对齐INRs和CNNs。

Result: 实验结果表明，该方法能在排列和缩放对称性下对齐网络，使相似网络自然收敛到同一盆地，实现模型合并。

Conclusion: ScaleGMNs成功利用神经网络的内在对称性，为模型合并提供了有效解决方案，避免高损失区域。

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [61] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 本文重新审视了20年前的神经拟合Q迭代（NFQ）算法，提出了现代化变体NFQ2.0，在CartPole任务上改进了学习过程的重复性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: NFQ是将多层神经网络应用于强化学习解决现实世界控制问题的开创性方法，但其需要大量调参且在实际控制问题中难以复现，因此需要改进其学习过程的重复性和鲁棒性。

Method: 提出了NFQ2.0现代化变体，通过消融研究识别关键设计决策和超参数，专注于使用标准工业组件构建的真实世界系统。

Result: NFQ2.0在性能和稳定性上优于原始变体，研究结果有助于从业者更有效地复现和改进结果。

Conclusion: 研究结果可以帮助从业者在工业环境中更有效地应用深度强化学习，提高学习过程的重复性和鲁棒性。

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [62] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: 研究比较了在高速公路自动驾驶中RL-only、LLM-only和混合方法的性能，发现混合方法在成功率和效率之间取得平衡，但小LLM存在保守偏差问题。


<details>
  <summary>Details</summary>
Motivation: 解决RL在复杂驾驶环境中奖励函数设计不足的问题，同时避免LLM直接控制的不稳定性和高成本，探索小LLM通过奖励塑造来增强RL的可行性。

Method: 使用小LLM（<14B参数）通过评分状态-动作转换来增强RL奖励，训练时LLM辅助奖励塑造，测试时使用标准RL策略执行。

Result: RL-only成功率73-89%，LLM-only可达94%但速度性能严重下降，混合方法介于两者之间。LLM影响的方法表现出系统性保守偏差。

Conclusion: 当前小LLM在安全关键控制任务中存在重要局限性，特别是保守偏差和模型依赖的变异性问题。

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [63] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 提出PolicyGradEx算法，通过元训练和微调两阶段方法，将多目标RL问题中的n个目标高效聚类到k个相关组中，实现16%性能提升和26倍加速。


<details>
  <summary>Details</summary>
Motivation: 在多目标强化学习中，当目标数量n增长时，为所有目标学习单一策略是次优的。需要将相关目标分组训练以提高效率。

Method: 两阶段方法：1) 元训练学习所有目标的元策略；2) 微调适应随机采样子集，利用策略网络的一阶近似特性估计任务亲和度矩阵，然后进行聚类分组。

Result: 在机器人控制和Meta-World基准测试中，平均性能提升16%，速度提升达26倍。基于损失的聚类比随机分组和梯度相似性分组提升19%。

Conclusion: PolicyGradEx能有效估计任务亲和度并分组，显著提升多目标RL的性能和效率。Hessian迹分析提供了非空泛的泛化误差度量。

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [64] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 该论文提出使用定量线性时序逻辑(LTLf[F])来合成奖励监控器，为可观测状态轨迹生成密集奖励流，以解决强化学习中稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中指定信息丰富且密集的奖励函数这一关键挑战，特别是在长时程决策中稀疏奖励问题，当前文献主要使用布尔语义，缺乏细粒度反馈。

Method: 利用定量线性时序逻辑(LTLf[F])合成奖励监控器，该方法与算法无关，仅依赖状态标记函数，并能自然处理非马尔可夫性质。

Result: 实验结果表明，定量监控器始终优于布尔监控器，在最大化任务完成度定量测量和减少收敛时间方面表现更好。

Conclusion: 定量LTLf[F]监控器为强化学习提供了有效的密集奖励生成方法，显著改善了训练效率和性能。

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [65] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，从计算理论、信息论和几何角度形式化分析了LLM扩展的五个基本限制：幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM扩展限制的描述停留在经验层面，缺乏将这些现象与计算、信息和学习的基础限制联系起来的严格理论综合。本文旨在填补这一空白。

Method: 构建了一个基于证明的统一框架，从三个理论角度分析：1) 可计算性和不可计算性导致的固有错误；2) 信息论和统计约束；3) 几何和计算效应导致的上下文压缩。

Result: 证明了LLM扩展存在固有的理论上限：不可计算任务必然产生无限失败集，有限描述长度强制压缩误差，几何效应使有效上下文远小于名义大小，基于似然的训练偏向模式完成而非推理。

Conclusion: LLM扩展在某些领域有帮助，但在某些领域会饱和，在另一些领域无法进展。提出了有界预言检索、位置课程学习和稀疏注意力等实际缓解路径。

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [66] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: 提出了TGPPO框架，使用PPO强化学习算法训练分支策略，以提升混合整数线性规划中分支定界算法在异构实例上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的分支策略容易过拟合专家演示，难以泛化到结构多样或未见过的实例。

Method: 使用PPO强化学习算法，构建参数化状态空间表示来动态捕捉搜索树的演化上下文。

Result: TGPPO在减少探索节点数和改进p-原始对偶积分方面优于现有学习策略，特别是在分布外实例上表现突出。

Conclusion: 强化学习有潜力为MILP求解器开发鲁棒且适应性强的分支策略。

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [67] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: 提出一种单步生成策略用于离线强化学习，通过残差重构MeanFlow实现从噪声直接生成动作，与Q学习兼容。该方法在单阶段训练中实现高效的多模态动作分布建模。


<details>
  <summary>Details</summary>
Motivation: 现有的一步高斯策略推理速度快但难以捕捉复杂的多模态动作分布，而基于流的方法虽然表达能力更强但通常需要蒸馏和两阶段训练。

Method: 重构MeanFlow，将速度场和噪声到动作的转换整合到单一策略网络中，提出有效的残差公式化方法，支持在单阶段Q学习中进行表达性和稳定的策略学习。

Result: 在OGBench和D4RL基准测试的73个任务上进行了广泛实验，证明该方法在离线和离线到在线强化学习设置中均取得了强劲性能。

Conclusion: 该方法提供了三个关键优势：高效的单步噪声到动作生成、多模态动作分布的表达能力、以及通过单阶段Q学习实现高效稳定的策略学习。

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [68] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze是一种新颖的长到短推理压缩方法，通过自适应选择推理深度和分布对齐的语言精炼，在保持准确性的同时显著减少推理LLM的token使用量。


<details>
  <summary>Details</summary>
Motivation: 现有的推理LLM生成的长思维链导致token使用量增加，带来更高的推理延迟和内存消耗。现有长到短方法往往牺牲准确性，需要一种能在降低token成本的同时保持性能的方法。

Method: 1) 自适应选择推理深度匹配问题复杂度的自生成样本；2) 分布对齐的语言精炼方法，在不改变推理路径的情况下优化语言表达。

Result: DeepSeek-R1-Distill-Qwen-7B使用该方法在MATH500基准上实现了50%的平均token减少，同时保持准确性。

Conclusion: TokenSqueeze仅使用模型自生成数据，无需依赖人工整理的短答案数据集，实现了高效高保真的推理压缩。

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [69] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: 提出一种基于Voronoi划分的知识蒸馏方法，将深度强化学习的黑盒控制器转换为可解释的局部线性模型，在保持性能的同时提高透明度。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习控制器缺乏透明度，难以满足监管要求和建立信任，需要将学习到的行为转移到人类可读的模型中。

Method: 使用Voronoi划分将状态空间划分为多个区域，在每个区域内使用简化的线性模型来模拟原始控制器的行为，实现模型无关的知识蒸馏。

Result: 在网格世界环境和经典控制任务上的评估表明，该方法产生的策略具有可解释性，且蒸馏后的性能与原黑盒策略相当或略有提升。

Conclusion: 局部专业化线性模型的知识蒸馏方法能够有效平衡灵活性与复杂性，在保持性能的同时提供可解释的控制器。

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [70] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: STACCA是一个基于Transformer的多智能体强化学习框架，通过集中式图Transformer评论家和共享图Transformer演员来解决网络控制中的长程依赖和拓扑泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法存在两个主要局限：1）依赖局部交互衰减假设，难以捕捉长程依赖（如级联故障、疫情爆发）；2）缺乏网络拓扑泛化能力，需要在新图上重新训练。

Method: 提出STACCA框架：1）集中式图Transformer评论家建模长程依赖并提供系统级反馈；2）共享图Transformer演员学习可泛化策略；3）集成反事实优势估计器改进信用分配。

Result: 在疫情控制和谣言传播网络控制任务上验证，STACCA表现出更好的性能、网络泛化能力和可扩展性。

Conclusion: 基于Transformer的MARL架构具有在大规模网络系统中实现可扩展和可泛化控制的潜力。

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [71] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: 该论文研究了真实与合成数据混合数据集中的分布差异问题，提出了三阶段缩放行为和LLM泛化边界理论，并开发了一种高效的数据评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，真实与合成数据混合数据集的使用日益普遍，但合成数据存在系统性分布差异，特别是对长尾知识的代表性不足，这给评估混合数据集的效用带来了根本性挑战。

Method: 识别了三阶段缩放行为特征，推导了适用于真实-合成混合数据的LLM泛化边界理论，并基于理论发现提出了一种可扩展到大规数据集的高效数据评估方法。

Result: 在图像分类、情感分类、指令跟随和复杂推理四个任务上的综合实验表明，该方法在数据评估方面超越了现有最优基线，且计算成本显著降低。

Conclusion: 该研究为理解和评估真实-合成混合数据集的效用提供了理论基础和实用工具，特别是在处理长尾知识代表性不足的问题上取得了重要进展。

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [72] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: 研究发现LLMs在动态环境中存在信念更新不一致和行动与信念不匹配的问题，即使高准确率模型也存在这些问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实动态环境中的表现，因为静态数据集评估无法预测其在需要顺序交互和信念更新的任务中的行为。

Method: 通过测试LLMs的信念更新一致性和行动与信念的一致性，包括直接获取后验概率与正确更新先验概率的比较，以及在博彩市场中的行为分析。

Result: LLMs在信念更新上平均有30%的不一致性，行动常与内部信念不符，且对用户质疑表现出中等程度的自我不一致性。

Conclusion: 在复杂真实世界环境中预测LLM行为存在困难，即使表现良好的模型也存在这些不一致性问题。

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


<div id='wechat.article'></div>

# wechat.article [[Back]](#toc)

### [73] [PayPal财报透视AI支付：推出“一次集成、多LLM覆盖”的<em class="highlight">Agentic</em>电商服务，在AI商业中连接数千万商户与广大消费者！](http://mp.weixin.qq.com/s?__biz=MzA3NTU3MDA4MA==&mid=2660632087&idx=1&sn=d574e9885e8cf9b3b94e22c665199927&chksm=859b0974e5932d83ebf9a15556701df1cc3f6269a848651694bfc2bf7768aebffa47511b204f#rd)
*王铮Silvia*

Main category: wechat.article

TL;DR: 业务陈述：推出Agentic电商服务，助力商家通过谷歌、OpenAI和Perplexity等多个AI平台开展销售全新的PayPal：实现更快、更盈利的增长；Q3每股收益增长12%，超出预期


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 业务陈述：推出Agentic电商服务，助力商家通过谷歌、OpenAI和Perplexity等多个AI平台开展销售全新的PayPal：实现更快、更盈利的增长；Q3每股收益增长12%，超出预期

</details>


### [74] [康奈尔大学研究报告：系统解构AI Agent与<em class="highlight">Agentic</em> AI的核心差异（附下载）](http://mp.weixin.qq.com/s?__biz=MzAxMjYyMzcwNA==&mid=2247490236&idx=1&sn=3cc4c9cef718ef1324c67ef0f31fd873&chksm=9a092f6f44439ab9c52edd22e3744d992926e43711244c01c1044c27a7bfb4a599c4e5e88d37#rd)
*究模智*

Main category: wechat.article

TL;DR: 3. Agentic AI（代理式AI）由多个专业化AI 智能体组成的协同系统，核心特征是目标拆解-任务分配-结果整合的闭环能力。系统中通常包含一个中央协调者，类似数字化项目经理，可将复杂目标拆解为子任务，并将其分配给不同的专


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 3. Agentic AI（代理式AI）由多个专业化AI 智能体组成的协同系统，核心特征是目标拆解-任务分配-结果整合的闭环能力。系统中通常包含一个中央协调者，类似数字化项目经理，可将复杂目标拆解为子任务，并将其分配给不同的专

</details>


### [75] [<em class="highlight">Agentic</em> Memory - AI Agent记忆系统: 了解Agents工具](http://mp.weixin.qq.com/s?__biz=MzI5NjY0NTQ4Mw==&mid=2247485220&idx=1&sn=d04ce6664474fe9ea2b2be6678c3bdc4&chksm=eda041537f80507f774f8a0b0168e791f36810fdd9a7bd3b6dc765c8b298df8b08596b8bc7f1#rd)
*AIPM之泡泡糖*

Main category: wechat.article

TL;DR: Agentic Memory指AI Agent为了执行任务、理解上下文、与用户保持连贯交互而使用的各种“存储机制”，让Agent能够做到：维持多轮对话跨任务保持一致性


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic Memory指AI Agent为了执行任务、理解上下文、与用户保持连贯交互而使用的各种“存储机制”，让Agent能够做到：维持多轮对话跨任务保持一致性

</details>


### [76] [自主之刃：<em class="highlight">Agentic</em> AI 如何重塑网络安全威胁与防御](http://mp.weixin.qq.com/s?__biz=MzUxNjA0Mjg4OA==&mid=2247485603&idx=1&sn=fa7456db03506e8bb988aa647efd49d9&chksm=f895215099d8a7afea44083f56ec9e88bd450d070b7bb5260d8b9b134723ecf90ff298b9f322#rd)
*聪颖溪水*

Main category: wechat.article

TL;DR: Agentic 系统通常具有长期和短期记忆，用于累积经验和上下文。攻击者可以注入虚假信息实现记忆中毒 （Memory Poisoning），影响 AI 的长期决策，导致持续性、隐蔽性的恶意行为。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: Agentic 系统通常具有长期和短期记忆，用于累积经验和上下文。攻击者可以注入虚假信息实现记忆中毒 （Memory Poisoning），影响 AI 的长期决策，导致持续性、隐蔽性的恶意行为。

</details>


### [77] [10-<em class="highlight">Agentic</em> AI vs AI Agent：深度对比两者的异同点](http://mp.weixin.qq.com/s?__biz=MzAwNTUyNTc5Nw==&mid=2466754540&idx=1&sn=aeeff59e1b5d5846e930a2f57e16040a&chksm=8cfaddf14c6439ba43194c9d6332795f3b86321df3733dc93bea439c6772d2fe4422809cbe8f#rd)
*企业大模型应用和开发*

Main category: wechat.article

TL;DR: 特性AI Agent（AI智能体/代理）Agentic AI（智能代理AI）定义集成了大语言模型和外部工具的单体系统具备自主性和智能行动能力的人工智能系统，包含多个协作的智能体


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 特性AI Agent（AI智能体/代理）Agentic AI（智能代理AI）定义集成了大语言模型和外部工具的单体系统具备自主性和智能行动能力的人工智能系统，包含多个协作的智能体

</details>


### [78] [【来，看报告002】<em class="highlight">大模型</em>一体机应用研究报告2025](http://mp.weixin.qq.com/s?__biz=MzkzMjQ0ODM1Mw==&mid=2247485438&idx=1&sn=082a900b1b3be8fe4a55b4e216a2f51b&chksm=c394a181bd10463fff89df746b1c2869f114a732fc5aa11f8220ae8a1916769deb80b2e5fe0e#rd)
*空kong产品思考*

Main category: wechat.article

TL;DR: 《大模型一体机应用研究报告（2025年）》 大模型一体都有些啥？行业场景化解决方案 管理能力 应用层 知识库 智能体 工作流编排 安装部署 模型评估 模型管理 模型层 资源管理 预置模版能力 模型定制与优化 日志管理 数据处


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 《大模型一体机应用研究报告（2025年）》 大模型一体都有些啥？行业场景化解决方案 管理能力 应用层 知识库 智能体 工作流编排 安装部署 模型评估 模型管理 模型层 资源管理 预置模版能力 模型定制与优化 日志管理 数据处

</details>


### [79] [官方测评：<em class="highlight">大模型</em>智能体哪家强？Which Large Model Agent Platform Reigns Supreme?](http://mp.weixin.qq.com/s?__biz=MzIzNjI2NjU0NA==&mid=2650975588&idx=1&sn=29d3bf97a0976c1519f832b81917b0b3&chksm=f2acf809a2dd850cc908f076993f5c182f05261c295f61b80b2915e76089fa5dfd52add6220a#rd)
*WelcomingANNA 养老机器人用户群*

Main category: wechat.article

TL;DR: 表6：大模型工作流能力表现平台 端到端准确率 参数提取准确率 意图识别准确率 工作流结束判断准确率阿里云百炼 69.2% 75.0% 86.7% 100.0%腾讯云智能体开发平台 69.2% 75.0% 93.3% 100.0%


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 表6：大模型工作流能力表现平台 端到端准确率 参数提取准确率 意图识别准确率 工作流结束判断准确率阿里云百炼 69.2% 75.0% 86.7% 100.0%腾讯云智能体开发平台 69.2% 75.0% 93.3% 100.0%

</details>


### [80] [<em class="highlight">大模型</em>开发框架深度对比：Spring AI、LangChain、LangGraph与LlamaIndex的技术选型指南](http://mp.weixin.qq.com/s?__biz=Mzk0ODM0NDA0Nw==&mid=2247489000&idx=1&sn=8e4c6d8b38c28b417e15f37863f410cf&chksm=c24b5c11b4f2619af168c620e2748c6eefec4dc640c7b1b5d40208634ad3d0e86b4a956df498#rd)
*架构进化论*

Main category: wechat.article

TL;DR: 即可快速启用大模型功能。例如，要集成OpenAI的聊天服务，只需在application.properties中配置：spring.ai.openai.api-key=your-api-keyspring.ai.openai.chat.model=gpt-4然后在代码中注入ChatClient即可使用：


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 即可快速启用大模型功能。例如，要集成OpenAI的聊天服务，只需在application.properties中配置：spring.ai.openai.api-key=your-api-keyspring.ai.openai.chat.model=gpt-4然后在代码中注入ChatClient即可使用：

</details>


### [81] [收藏！LLM <em class="highlight">大模型</em>入门必学：智能体（Agent）从本质到实战，程序员 & 新手轻松拿捏](http://mp.weixin.qq.com/s?__biz=MzU1NjY4OTUxMQ==&mid=2247493200&idx=1&sn=840c8b482770a936eab37d91d9ca907f&chksm=fa74c2e97d61ad5da4163bf412dec34f079b6100d76e1f439c26da80c2fb2ef5547d466f1a48#rd)
*慕容千语*

Main category: wechat.article

TL;DR: 以GPT（Generative Pre-trained Transformer） 为代表的大语言模型的出现，正在显著改变智能体的构建方法与能力边界。由大语言模型驱动的 LLM 智能体，其核心决策机制与传统智能体存在本质区别，从而赋予了其一系列全新的特性。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 以GPT（Generative Pre-trained Transformer） 为代表的大语言模型的出现，正在显著改变智能体的构建方法与能力边界。由大语言模型驱动的 LLM 智能体，其核心决策机制与传统智能体存在本质区别，从而赋予了其一系列全新的特性。

</details>


### [82] [唐山AI<em class="highlight">大模型</em>排名安装全攻略：从选型到部署的实用指南](http://mp.weixin.qq.com/s?__biz=MjM5ODA2NjA3Ng==&mid=2649713155&idx=1&sn=76a06eb860fdd7f894316dff02020504&chksm=bfd0f8018156970595f79c8f80192c2ed44631c582b8f6a2b3bb6aac296596bcfb28ebae58ce#rd)
*八方资源*

Main category: wechat.article

TL;DR: 然而，面对市场上琳琅满目的AI大模型，如何选择适合自身需求的模型并完成安装部署，成为许多企业和开发者关注的焦点。本文将从排名依据、选型策略、安装步骤及优化建议四个方面，为您梳理唐山地区AI大模型安装的全流


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 然而，面对市场上琳琅满目的AI大模型，如何选择适合自身需求的模型并完成安装部署，成为许多企业和开发者关注的焦点。本文将从排名依据、选型策略、安装步骤及优化建议四个方面，为您梳理唐山地区AI大模型安装的全流

</details>


### [83] [<em class="highlight">大模型</em>时代，“跑满算力”为什么这么难？](http://mp.weixin.qq.com/s?__biz=MzkzMDY1NDgyOQ==&mid=2247825175&idx=2&sn=68697477960eb0361f42d23ecdeef266&chksm=c39ff37b887673cd38193d83e4c5d07c4f16f6096ee17baa3a587cd04704f0ef1d5b1ab09ed1#rd)
*CSDN*

Main category: wechat.article

TL;DR: 大模型参数突破万亿规模、AI 原生应用爆发式增长、智能体数量呈几何级扩张——所有人都在追问同一个问题：如何让算力更便宜、更快、更可控？


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 大模型参数突破万亿规模、AI 原生应用爆发式增长、智能体数量呈几何级扩张——所有人都在追问同一个问题：如何让算力更便宜、更快、更可控？

</details>


### [84] [白话<em class="highlight">大模型</em>的技术原理](http://mp.weixin.qq.com/s?__biz=MzA3MDI0OTgyOA==&mid=2731879183&idx=1&sn=21fac28c4dea69c47eda395813745a93&chksm=b96bbc241bbc51328420581301798be4683d15a3c75b9fd9fd259757c9473c6df6f784ddebea#rd)
*竹清助手*

Main category: wechat.article

TL;DR: 这让模型能从不同的语意角度关注资讯，进而提升准确度。Masked Attention（遮罩注意力）在Decoder 阶段，模型需要「自回归（Autoregressive）」地生成文字。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 这让模型能从不同的语意角度关注资讯，进而提升准确度。Masked Attention（遮罩注意力）在Decoder 阶段，模型需要「自回归（Autoregressive）」地生成文字。

</details>


### [85] [AI大语言<em class="highlight">模型</em>（LLM）开发与训练入门](http://mp.weixin.qq.com/s?__biz=MzA5NzI5NjMwNg==&mid=2247485269&idx=1&sn=f947d1a8e711b400c156aab3e4c67947&chksm=9123be09bc4db4a7a782709e730a7fba2ec09b38d862af0bf560322eaa4ff395676da2d2d5a9#rd)
*知识科普者*

Main category: wechat.article

TL;DR: 下图是一个简化的大语言模型基本架构示意图：hidden layers input layer output layer 在实际应用中，模型架构要复杂得多。现代大语言模型（LLMs）通常采用基于 Transformer 的架构，这种架构彻底革新了自然语言处理领域。


<details>
  <summary>Details</summary>
Motivation: 微信公众号文章，分享AI相关技术内容

Method: 基于实际应用经验的技术分享

Result: 提供实用的AI技术见解和案例分析

Conclusion: 适合了解AI技术在实际场景中的应用

Abstract: 下图是一个简化的大语言模型基本架构示意图：hidden layers input layer output layer 在实际应用中，模型架构要复杂得多。现代大语言模型（LLMs）通常采用基于 Transformer 的架构，这种架构彻底革新了自然语言处理领域。

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [86] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 提出语义三角化方法，通过问题转换验证代码生成的可靠性，相比传统方法在LiveCodeBench和CodeElo基准上提升21%可靠性，并能识别概率低至0.14的正确解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成中的样本共识技术存在局限：难以识别低概率正确方案，无法处理多有效但不等价方案的情况，以及在无正确方案时无法有效弃权。

Method: 引入语义三角化，对编程问题进行语义非平凡转换，保持解决方案间的精确可验证映射，通过跨问题转换的一致性验证来提高置信度。

Result: 在LiveCodeBench和CodeElo基准测试中，使用GPT-4o和DeepSeek-V3模型，语义三角化相比概率阈值0.5的高置信度选择方法提升21%可靠性，能识别概率低至0.14的正确方案，是唯一能对多有效但不等价方案任务形成真正共识的方法。

Conclusion: 语义三角化通过问题转换验证能更可靠地识别正确代码方案，提高代码生成的可信度，特别是在低概率和多方案场景下表现突出。

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [87] [ProofWright: Towards Agentic Formal Verification of CUDA](https://arxiv.org/abs/2511.12294)
*Bodhisatwa Chatterjee,Drew Zagieboylo,Sana Damani,Siva Hari,Christos Kozyrakis*

Main category: cs.SE

TL;DR: ProofWright是一个集成自动形式验证与LLM代码生成的代理验证框架，为LLM生成的CUDA内核提供内存安全、线程安全和语义正确性的端到端保证。


<details>
  <summary>Details</summary>
Motivation: LLM生成的CUDA内核虽然提高了开发效率，但经常包含难以发现的正确性错误且缺乏形式化安全保证。运行时测试不可靠，而手动形式验证无法跟上LLM的生成速度，形成验证瓶颈。

Method: ProofWright框架结合自动形式验证和LLM代码生成，通过代理验证方法为生成的CUDA内核提供形式化验证。

Result: 在KernelBench L1上，ProofWright验证了74%生成内核的安全属性，发现了传统测试遗漏的细微正确性错误，并为元素级内核类建立了语义等价性，每个内核仅需3分钟额外开销。

Conclusion: ProofWright证明了对LLM生成的GPU代码进行可扩展的自动形式验证是可行的，为可信的高性能代码生成提供了路径。

Abstract: Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.
  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.

</details>


### [88] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 系统评估不同规模GenAI模型在应用行为分析（特别是恶意软件检测）中的表现，发现小模型在保持竞争力的同时具有计算效率优势


<details>
  <summary>Details</summary>
Motivation: 评估不同规模生成式AI模型在应用行为理解方面的能力，特别是恶意软件检测任务，探索小模型在资源受限环境中的实用性

Method: 系统评估小型和大型GenAI语言模型在应用行为分析中的表现，使用恶意软件检测作为代表性任务，比较准确率、精确率、召回率和F1分数等指标

Result: 大型模型整体准确率更高，但小模型在精确率和召回率方面保持竞争力，且在计算效率、推理速度和资源受限环境部署方面具有显著优势

Conclusion: 小GenAI模型可以有效补充大型模型，在实际应用行为分析中提供性能与资源效率之间的实用平衡

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [89] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 该论文分析了使用传统指标评估LLM在系统综述文献筛选中的性能问题，提出了改进的评估实践建议，强调需要关注丢失证据、使用稳健指标和完整报告混淆矩阵。


<details>
  <summary>Details</summary>
Motivation: 由于LLM发布速度快于用户评估能力，当LLM用于系统综述文献筛选等研究时，需要稳健的实证评估来确保可靠性。

Method: 以大规模研究为例，分析传统指标在评估LLM文献筛选性能中的问题，并分析了27篇相关论文的性能指标使用情况。

Result: 发现主要弱点包括：使用对不平衡数据不稳健的指标、忽视丢失证据的影响、未完整报告混淆矩阵。同时提取了良好的评估实践。

Conclusion: 建议优先考虑丢失证据/召回率，使用机会锚定和成本敏感的加权MCC指标，报告完整混淆矩阵，采用泄漏感知设计，并以成本效益分析为基础得出结论。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [90] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 该论文提出了一种结合测试驱动开发(TDD)和代码解释器(CI)的新方法，使用开源模型来提高孟加拉语提示的代码生成准确率，达到85%的整体准确率，无需微调即可使小型模型达到大型模型98%的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管孟加拉语有2.42亿母语使用者，但在LLM训练中很少受到关注。现有代码生成技术需要大量专业知识和资源，该工作旨在为资源受限的新兴市场用户提供本地语言的强大代码生成工具。

Method: 结合测试驱动开发(TDD)和代码解释器(CI)，使用开源权重模型，无需微调即可提升代码生成性能。

Result: 将孟加拉语提示的代码生成基线准确率提升至85%，最小模型能达到最大模型98%的准确率。

Conclusion: 该方法证明了无需微调即可显著提升小模型在多语言代码生成中的性能，为资源受限环境提供了可行的解决方案。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [91] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: 对2303个代理上下文文件的大规模实证研究，发现这些文件是复杂、难以阅读的配置代码，主要关注功能性需求而忽视安全性和性能等非功能性需求。


<details>
  <summary>Details</summary>
Motivation: 代理编码工具使用自然语言目标作为输入，通过代理上下文文件提供项目级指令。这是首次大规模研究这些上下文文件的结构、维护和内容特征。

Method: 分析了来自1925个仓库的2303个代理上下文文件，研究其结构、维护模式和16种指令类型的内容分布。

Result: 发现上下文文件是动态演化的配置代码，主要包含构建运行命令(62.3%)、实现细节(69.9%)和架构信息(67.7%)，但安全(14.5%)和性能(14.5%)等非功能性需求很少被指定。

Conclusion: 开发者主要使用上下文文件使代理功能化，但缺乏确保代理编写代码安全性和性能的防护措施，需要改进工具和实践。

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [92] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: SAINT是一个用于企业Java应用程序服务级测试的白盒测试方法，结合静态分析、大语言模型和基于LLM的代理来自动生成端点和基于场景的测试。


<details>
  <summary>Details</summary>
Motivation: 现有的服务级测试工具通常依赖于OpenAPI规范（在企业代码库中不易获得），并且在生成有效执行有意义场景的功能测试方面能力有限。

Method: SAINT构建两个关键模型：端点模型（捕获服务端点的语法和语义信息）和操作依赖图（捕获端点间顺序约束）。然后使用基于LLM的代理生成测试，包括端点聚焦测试（最大化代码和数据库交互覆盖率）和基于场景的测试（通过代理循环的规划、行动和反思阶段从代码中提取应用用例并精炼为可执行测试）。

Result: 在八个Java应用程序上的评估显示，SAINT在覆盖率、故障检测和场景生成方面表现有效。开发者调查强烈认可SAINT生成的基于场景的测试。

Conclusion: 将静态分析与基于代理的LLM工作流相结合，能够实现更有效、功能性和开发者对齐的服务级测试生成。

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [93] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 提出了一种细粒度的开源软件后门风险评估框架，通过建模攻击者视角和定义针对性指标来评估开源软件供应链安全风险。


<details>
  <summary>Details</summary>
Motivation: 开源软件供应链在现代软件开发中广泛应用，但依赖项维护不足和社区审计不充分导致源代码安全和仓库维护者合法性面临挑战，特别是在XZ-Util事件等高隐蔽性后门攻击下。

Method: 从攻击者视角建模隐蔽后门攻击，为每个攻击阶段定义针对性指标；使用大语言模型进行代码仓库语义评估，克服静态分析在评估仓库维护活动可靠性方面的限制。

Result: 在Debian生态系统的66个高优先级软件包上评估该框架，实验结果表明当前开源软件供应链面临多种安全风险。

Conclusion: 提出的框架能够有效评估开源软件的后门风险，揭示了当前软件供应链存在的安全隐患。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [94] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: Live-SWE-agent是首个能够在运行时自主持续进化的软件代理，从基础代理脚手架开始，在解决真实世界软件问题时自主演化其实现，在SWE-bench Verified基准测试中达到75.4%的解决率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM软件代理通常需要专门设计且可能不是最优的，而自改进代理需要昂贵的离线训练且泛化能力有限。为了解决这些问题，需要开发能够在运行时自主进化的软件代理。

Method: Live-SWE-agent从仅包含bash工具的基础代理脚手架开始，在解决真实世界软件问题时自主演化其脚手架实现，实现实时在线进化。

Result: 在SWE-bench Verified基准测试中达到75.4%的解决率，优于所有现有开源软件代理，接近最佳专有解决方案的性能。在SWE-Bench Pro基准测试中达到45.8%的最佳已知解决率。

Conclusion: Live-SWE-agent证明了软件代理能够在运行时自主持续进化，显著提升性能，为软件工程自动化提供了新的研究方向。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>
