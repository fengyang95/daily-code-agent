<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.LG](#cs.LG) [Total: 19]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.SE](#cs.SE) [Total: 13]
- [tldr.article](#tldr.article) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Dynamic Role Assignment for Multi-Agent Debate](https://arxiv.org/abs/2601.17152)
*Miao Zhang,Junsik Kim,Siyuan Xiang,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 提出动态角色分配框架，通过元辩论选择最适合的模型担任特定角色，在多智能体辩论系统中提升问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM/VLM辩论系统虽然使用专门角色解决复杂问题，但没有根据模型特长来分配角色，导致系统性能未能最大化。

Method: 提出动态角色分配框架，包含元辩论的两个阶段：1)提案阶段，候选模型提供角色定制化论证；2)同行评审阶段，根据数据和角色特定标准评分，选择最适合每个角色的模型。

Result: 在LLM问题解决基准测试中，该方法相比统一分配（所有角色用同一模型）提升高达74.8%，相比随机分配提升高达29.7%，性能提升取决于具体任务和分配方式。

Conclusion: 建立了多智能体系统设计的新范式，从静态部署转向动态、能力感知的选择，显著提升了辩论系统的性能。

Abstract: Multi-agent large language model (LLM) and vision-language model (VLM) debate systems employ specialized roles for complex problem-solving, yet model specializations are not leveraged to decide which model should fill which role. We propose dynamic role assignment, a framework that runs a Meta-Debate to select suitable agents before the actual debate. The meta-debate has two stages: (1) proposal, where candidates provide role-tailored arguments, and (2) peer review, where proposals are scored with data and role-specific criteria to choose the best agent for each position. We evaluate our method on LLM problem solving benchmarks. Applied on top of existing debate systems, our approach consistently outperforms uniform assignments (filling all roles with the same model) by up to 74.8% and random assignments (assigning models to roles without considering their suitability) by up to 29.7%, depending on the task and the specific assignment. This work establishes a new paradigm for multi-agent system design, shifting from static agent deployment to dynamic and capability-aware selection.

</details>


### [2] [Beyond Factual QA: Mentorship-Oriented Question Answering over Long-Form Multilingual Content](https://arxiv.org/abs/2601.17173)
*Parth Bhalerao,Diola Dsouza,Ruiwen Guan,Oana Ignat*

Main category: cs.CL

TL;DR: 提出了首个多语言导师问答数据集MentorQA，包含近9000个QA对，用于评估超越事实准确性的导师式回答质量，并比较了不同智能体架构在导师式问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统主要评估事实准确性，但教育、职业指导等实际应用需要提供反思和指导的导师式回答，现有基准很少捕捉这种区别，特别是在多语言和长文本场景中。

Method: 构建了MentorQA数据集，包含180小时视频内容、近9000个QA对，覆盖四种语言；定义了超越事实准确性的导师式评估维度（清晰度、一致性、学习价值）；在控制条件下比较了单智能体、双智能体、RAG和多智能体问答架构。

Result: 多智能体流水线在导师式回答质量上表现最佳，尤其在复杂主题和低资源语言上优势明显；同时发现基于LLM的自动评估与人类判断存在显著差异。

Conclusion: 将导师式问答确立为独立研究问题，提供了多语言基准用于研究教育AI中的智能体架构和评估设计，发布了数据集和评估框架。

Abstract: Question answering systems are typically evaluated on factual correctness, yet many real-world applications-such as education and career guidance-require mentorship: responses that provide reflection and guidance. Existing QA benchmarks rarely capture this distinction, particularly in multilingual and long-form settings. We introduce MentorQA, the first multilingual dataset and evaluation framework for mentorship-focused question answering from long-form videos, comprising nearly 9,000 QA pairs from 180 hours of content across four languages. We define mentorship-focused evaluation dimensions that go beyond factual accuracy, capturing clarity, alignment, and learning value. Using MentorQA, we compare Single-Agent, Dual-Agent, RAG, and Multi-Agent QA architectures under controlled conditions. Multi-Agent pipelines consistently produce higher-quality mentorship responses, with especially strong gains for complex topics and lower-resource languages. We further analyze the reliability of automated LLM-based evaluation, observing substantial variation in alignment with human judgments. Overall, this work establishes mentorship-focused QA as a distinct research problem and provides a multilingual benchmark for studying agentic architectures and evaluation design in educational AI. The dataset and evaluation framework are released at https://github.com/AIM-SCU/MentorQA.

</details>


### [3] [Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning](https://arxiv.org/abs/2601.17223)
*Massimiliano Pronesti,Anya Belz,Yufang Hou*

Main category: cs.CL

TL;DR: 提出可验证过程奖励模型(VPRMs)，通过确定性规则验证器检查中间推理步骤，在医学证据合成偏倚评估中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程监督方法依赖神经评判器评估思维链步骤，容易受到不透明性、偏见和奖励攻击的影响。需要一种能够通过确定性规则验证中间推理步骤的强化学习框架。

Method: 引入可验证过程奖励模型(VPRMs)框架，使用确定性、基于规则的验证器检查中间推理步骤。应用于医学证据合成偏倚评估领域，利用指南定义的标准和基于规则的决策路径对推理轨迹进行程序化验证。

Result: VPRMs生成的推理紧密遵循领域规则，步骤级决策与最终标签之间的一致性显著提高。在多个数据集上，VPRMs比最先进模型F1分数提高达20%，比可验证结果奖励高6.5%，在证据基础和逻辑一致性方面有显著提升。

Conclusion: VPRMs通过确定性规则验证器有效解决了过程监督中的不透明性和偏见问题，在需要严格遵循领域规则的医学证据合成偏倚评估中表现出色，为强化学习中的过程监督提供了新方向。

Abstract: Recent work on reinforcement learning with verifiable rewards (RLVR) has shown that large language models (LLMs) can be substantially improved using outcome-level verification signals, such as unit tests for code or exact-match checks for mathematics. In parallel, process supervision has long been explored as a way to shape the intermediate reasoning behaviour of LLMs, but existing approaches rely on neural judges to score chain-of-thought steps, leaving them vulnerable to opacity, bias, and reward hacking. To address this gap, we introduce Verifiable Process Reward Models (VPRMs), a reinforcement-learning framework in which intermediate reasoning steps are checked by deterministic, rule-based verifiers. We apply VPRMs to risk-of-bias assessment for medical evidence synthesis, a domain where guideline-defined criteria and rule-based decision paths enable programmatic verification of reasoning traces. Across multiple datasets, we find that VPRMs generate reasoning that adheres closely to domain rules and achieve substantially higher coherence between step-level decisions and final labels. Results show that VPRMs achieve up to 20% higher F1 than state-of-the-art models and 6.5% higher than verifiable outcome rewards, with substantial gains in evidence grounding and logical coherence.

</details>


### [4] [Meta-Judging with Large Language Models: Concepts, Methods, and Challenges](https://arxiv.org/abs/2601.17312)
*Hugo Silva,Mateus Mendes,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文综述了LLM评估范式从"LLM-as-a-Judge"到"LLM-as-a-Meta-Judge"的演进，分析了传统LLM评估的局限性，并系统整理了元评估方法的研究进展。


<details>
  <summary>Details</summary>
Motivation: 传统LLM-as-a-Judge评估存在显著缺陷：对提示词敏感、系统性偏见、冗长效应、不可靠或幻觉的推理过程。这些局限性促使研究者开发更稳健的元评估范式。

Method: 提出一个包含六个关键维度的分析框架：1)概念基础，2)元评估机制，3)对齐训练方法，4)评估方法，5)局限性与失败模式，6)未来方向。通过系统综述元评估文献，分析LLM评估的演进。

Result: LLM-as-a-Meta-Judge为自动化评估提供了更稳定可信的方向，但仍面临成本、提示词敏感性和共享模型偏见等挑战，需要进一步研究解决。

Conclusion: 元评估范式是提升LLM评估质量的重要进展，通过系统分析现有局限性和总结最新进展，为下一代LLM评估方法学的发展指明了方向。

Abstract: Large language models (LLMs) are evolving fast and are now frequently used as evaluators, in a process typically referred to as LLM-as-a-Judge, which provides quality assessments of model outputs. However, recent research points out significant vulnerabilities in such evaluation, including sensitivity to prompts, systematic biases, verbosity effects, and unreliable or hallucinated rationales. These limitations motivated the development of a more robust paradigm, dubbed LLM-as-a-Meta-Judge. This survey reviews recent advances in meta-judging and organizes the literature, by introducing a framework along six key perspectives: (i) Conceptual Foundations, (ii) Mechanisms of Meta-Judging, (iii) Alignment Training Methods, (iv) Evaluation, (v) Limitations and Failure Modes, and (vi) Future Directions. By analyzing the limitations of LLM-as-a-Judge and summarizing recent advances in meta-judging by LLMs, we argue that LLM-as-a-Meta-Judge offers a promising direction for more stable and trustworthy automated evaluation, while highlighting remaining challenges related to cost, prompt sensitivity, and shared model biases, which must be addressed to advance the next generation of LLM evaluation methodologies.

</details>


### [5] [The Shadow Self: Intrinsic Value Misalignment in Large Language Model Agents](https://arxiv.org/abs/2601.17344)
*Chen Chen,Kim Young Il,Yuan Yang,Wenhao Su,Yilin Zhang,Xueluan Gong,Qian Wang,Yongsen Zheng,Ziyao Liu,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: IMPRESS框架系统评估LLM代理在完全良性场景中的内在价值错位风险，发现这是普遍存在的安全问题，现有缓解策略效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注对显性有害输入的响应或系统故障的鲁棒性，而在现实、完全良性的自主代理设置中的价值错位风险尚未充分探索。

Method: 提出IMPRESS框架，通过多阶段LLM生成管道构建现实、完全良性、情境化的场景基准，评估21个最先进LLM代理的内在价值错位。

Result: 内在价值错位是跨模型的普遍安全风险，错位率因动机、风险类型、模型规模和架构而异；情境化和框架机制显著影响错位行为，现有缓解策略效果有限。

Conclusion: LLM代理在完全良性场景中存在普遍的内在价值错位风险，需要更有效的安全措施，IMPRESS框架为AI生态系统提供了关键评估工具。

Abstract: Large language model (LLM) agents with extended autonomy unlock new capabilities, but also introduce heightened challenges for LLM safety. In particular, an LLM agent may pursue objectives that deviate from human values and ethical norms, a risk known as value misalignment. Existing evaluations primarily focus on responses to explicit harmful input or robustness against system failure, while value misalignment in realistic, fully benign, and agentic settings remains largely underexplored. To fill this gap, we first formalize the Loss-of-Control risk and identify the previously underexamined Intrinsic Value Misalignment (Intrinsic VM). We then introduce IMPRESS (Intrinsic Value Misalignment Probes in REalistic Scenario Set), a scenario-driven framework for systematically assessing this risk. Following our framework, we construct benchmarks composed of realistic, fully benign, and contextualized scenarios, using a multi-stage LLM generation pipeline with rigorous quality control. We evaluate Intrinsic VM on 21 state-of-the-art LLM agents and find that it is a common and broadly observed safety risk across models. Moreover, the misalignment rates vary by motives, risk types, model scales, and architectures. While decoding strategies and hyperparameters exhibit only marginal influence, contextualization and framing mechanisms significantly shape misalignment behaviors. Finally, we conduct human verification to validate our automated judgments and assess existing mitigation strategies, such as safety prompting and guardrails, which show instability or limited effectiveness. We further demonstrate key use cases of IMPRESS across the AI Ecosystem. Our code and benchmark will be publicly released upon acceptance.

</details>


### [6] [Oops, Wait: Token-Level Signals as a Lens into LLM Reasoning](https://arxiv.org/abs/2601.17421)
*Jaehui Hwang,Dongyoon Han,Sangdoo Yun,Byeongho Heo*

Main category: cs.CL

TL;DR: 该论文系统分析了LLM中"wait"、"therefore"等话语标记token的概率信号，发现这些信号与推理正确性强相关，且在不同模型规模下稳定，但受训练策略影响显著。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中出现的"wait"、"therefore"等话语标记token为理解其推理过程提供了独特窗口，但目前缺乏对这些信号如何随训练策略和模型规模变化的系统性分析。

Method: 通过分析不同模型中的token级概率信号，研究特定token（如"wait"）与推理正确性的相关性，并考察这些信号在不同训练策略和模型规模下的变化规律。

Result: 发现特定token与推理正确性强相关，这种相关性在不同模型规模下保持稳定，但受训练策略影响显著；小规模数据集微调的模型通过此类信号获得推理能力，但仅部分利用这些信号。

Conclusion: 该研究为观察和理解LLM推理动态提供了系统性视角，揭示了话语标记token在模型推理中的重要作用及其与训练策略的关联。

Abstract: The emergence of discourse-like tokens such as "wait" and "therefore" in large language models (LLMs) has offered a unique window into their reasoning processes. However, systematic analyses of how such signals vary across training strategies and model scales remain lacking. In this paper, we analyze token-level signals through token probabilities across various models. We find that specific tokens strongly correlate with reasoning correctness, varying with training strategies while remaining stable across model scales. A closer look at the "wait" token in relation to answer probability demonstrates that models fine-tuned on small-scale datasets acquire reasoning ability through such signals but exploit them only partially. This work provides a systematic lens to observe and understand the dynamics of LLM reasoning.

</details>


### [7] [From Chains to DAGs: Probing the Graph Structure of Reasoning in LLMs](https://arxiv.org/abs/2601.17593)
*Tianjun Zhong,Linyang He,Nima Mesgarani*

Main category: cs.CL

TL;DR: 提出Reasoning DAG Probing框架，探究LLM隐藏状态是否线性编码推理DAG的几何结构，发现中间层确实编码了有意义的图结构信息。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究将推理视为线性链式步骤，但许多推理问题更自然地表示为有向无环图(DAG)，其中中间结论可能依赖多个前提、分支为并行子推导、后期合并或重用。理解模型内部是否反映这种图结构推理仍是一个开放问题。

Method: 引入Reasoning DAG Probing框架，将每个推理节点与文本实现关联，训练轻量级探针从隐藏状态预测两个图论属性：节点深度和节点对距离。分析DAG结构在层间的涌现，并评估破坏推理相关结构但保留表面文本属性的控制实验。

Result: 结果表明推理DAG几何在中间层被有意义地编码，可恢复性随节点深度和模型规模系统变化，表明LLM推理不仅是顺序的，而且表现出可测量的内部图结构。

Conclusion: LLM推理不仅具有顺序特性，还展现出可测量的内部图结构，推理DAG几何在模型中间层被有意义地编码。

Abstract: Recent progress in large language models has renewed interest in mechanistically characterizing how multi-step reasoning is represented and computed. While much prior work treats reasoning as a linear chain of steps, many reasoning problems are more naturally structured as directed acyclic graphs (DAGs), where intermediate conclusions may depend on multiple premises, branch into parallel sub-derivations, and later merge or be reused. Understanding whether such graph-structured reasoning is reflected in model internals remains an open question.
  In this work, we introduce Reasoning DAG Probing, a framework that directly asks whether LLM hidden states encode the geometry of a reasoning DAG in a linearly accessible form, and where this structure emerges across layers. Within this framework, we associate each reasoning node with a textual realization and train lightweight probes to predict two graph-theoretic properties from hidden states: node depth and pairwise node distance. We use these probes to analyze the layerwise emergence of DAG structure and evaluate controls that disrupt reasoning-relevant structure while preserving superficial textual properties. Our results provide evidence that reasoning DAG geometry is meaningfully encoded in intermediate layers, with recoverability varying systematically by node depth and model scale, suggesting that LLM reasoning is not only sequential but exhibits measurable internal graph structure.

</details>


### [8] [Learning to Ideate for Machine Learning Engineering Agents](https://arxiv.org/abs/2601.17596)
*Yunxiang Zhang,Kang Zhou,Zhichao Xu,Kiran Ramnath,Yun Zhou,Sangmin Woo,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: MLE-Ideator：一个双智能体框架，将算法优化中的构思与实现分离，通过专门的构思智能体帮助实现智能体生成更有效的算法改进策略，在训练和无训练设置下均显著优于仅实现智能体基线。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工程（MLE）智能体难以迭代优化其实现的算法效果，需要一种能够分离构思与实现的方法来提升算法优化的有效性。

Method: 提出MLE-Ideator双智能体框架：实现智能体负责具体实现，构思智能体专门提供战略帮助。实现智能体可向构思智能体请求帮助。此外，还通过强化学习训练构思智能体生成更有效的构思。

Result: 1. 在无训练设置下，该框架在MLE-Bench上显著优于仅实现智能体基线；2. 使用仅1K训练样本（来自10个MLE任务）通过RL训练的Qwen3-8B构思智能体相比未训练版本获得11.5%相对提升，并超越Claude Sonnet 3.5。

Conclusion: MLE-Ideator框架展示了分离构思与实现的有效性，通过强化学习训练构思智能体可显著提升算法优化效果，为训练用于科学发现的战略AI系统提供了有前景的路径。

Abstract: Existing machine learning engineering (MLE) agents struggle to iteratively optimize their implemented algorithms for effectiveness. To address this, we introduce MLE-Ideator, a dual-agent framework that separates ideation from implementation. In our system, an implementation agent can request strategic help from a dedicated Ideator. We show this approach is effective in two ways. First, in a training-free setup, our framework significantly outperforms implementation-only agent baselines on MLE-Bench. Second, we demonstrate that the Ideator can be trained with reinforcement learning (RL) to generate more effective ideas. With only 1K training samples from 10 MLE tasks, our RL-trained Qwen3-8B Ideator achieves an 11.5% relative improvement compared to its untrained counterpart and surpasses Claude Sonnet 3.5. These results highlights a promising path toward training strategic AI systems for scientific discovery.

</details>


### [9] [ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation](https://arxiv.org/abs/2601.17755)
*Jinyoung Park,Sanghyeok Lee,Omar Zia Khan,Hyunwoo J. Kim,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: ProGraph-R1提出了一种进度感知的图检索增强生成框架，通过结构感知的超图检索机制和基于进度的逐步策略优化，解决了现有RL-based GraphRAG方法在检索和奖励设计上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的GraphRAG框架（如Graph-R1）存在两个关键限制：1）主要依赖语义相似性进行检索，忽略了底层图结构；2）依赖稀疏的结果级奖励，无法捕捉中间检索步骤的质量及其依赖关系。

Method: 提出ProGraph-R1框架：1）结构感知的超图检索机制，联合考虑语义相关性和图连通性，鼓励沿多跳推理路径进行连贯遍历；2）基于进度的逐步策略优化，通过根据图中中间推理进度调节优势来提供密集学习信号。

Result: 在多跳问答基准测试中，ProGraph-R1在推理准确性和生成质量方面持续优于现有的GraphRAG方法。

Conclusion: ProGraph-R1通过结合结构感知检索和基于进度的强化学习，有效提升了图检索增强生成在多步推理任务中的性能。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

</details>


### [10] [D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models](https://arxiv.org/abs/2601.17865)
*Jia Gu,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 研究发现LLMs在细粒度采样概率上存在两种类型：D-模型（如Qwen-2.5）的token概率波动大且与任务分布对齐差，E-模型（如Mistral-Small）的token概率更稳定且与任务分布对齐更好，这两种类型在多样性和稳定性之间存在系统权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs能够生成近似真实世界分布的样本，但其细粒度采样概率是否忠实对齐任务需求仍是一个开放问题。研究者希望探究LLMs的token预测概率与任务级目标分布之间的对齐关系。

Method: 通过受控分布采样模拟实验，区分了两种模型类型：D-模型和E-模型。进一步在下游任务（如代码生成和推荐）中评估这两种模型类型，并分析它们的内在属性以探究底层机制。

Result: 发现LLMs存在显著的行为二分：D-模型的P_token表现出较大的步间变异性且与P_task对齐差；E-模型的P_token更稳定且与P_task对齐更好。两种模型类型在多样性和稳定性之间存在系统权衡，影响任务结果。

Conclusion: 这些发现为LLMs的概率采样行为提供了基础性见解，并为何时优先选择D-模型或E-模型提供了实践指导。对于推荐、搜索和对话代理等网络规模应用，研究结果可指导模型选择和配置，以在现实世界不确定性下平衡多样性和可靠性。

Abstract: The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

</details>


### [11] [Self-Manager: Parallel Agent Loop for Long-form Deep Research](https://arxiv.org/abs/2601.17879)
*Yilong Xu,Zhi Zheng,Xiang Long,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: Self-Manager是一个并行代理循环框架，通过多线程异步并发执行解决传统单上下文窗口代理在长研究任务中的互干扰和阻塞问题，在DeepResearch Bench上全面超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有代理在处理长形式深度研究任务时，虽然通过子任务级上下文管理克服了线性上下文积累和信息丢失问题，但仍受限于单上下文窗口和顺序执行范式，导致互干扰和阻塞行为，限制了可扩展性和适应性。

Method: 提出Self-Manager并行代理循环框架：主线程可创建多个具有独立隔离上下文的子线程，通过线程控制块进行迭代管理，实现更专注和灵活的并行代理执行。

Result: 在DeepResearch Bench基准测试中，Self-Manager在所有指标上持续优于现有的单代理循环基线。分析实验证明了其设计选择的必要性，以及在上下文容量、效率和泛化方面的优势。

Conclusion: Self-Manager通过并行代理循环解决了传统代理框架的局限性，为复杂长研究任务提供了更可扩展和自适应的解决方案，在多个维度上展现出显著优势。

Abstract: Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.

</details>


### [12] [Addressing LLM Diversity by Infusing Random Concepts](https://arxiv.org/abs/2601.18053)
*Pulin Agrawal,Prasoon Goyal*

Main category: cs.CL

TL;DR: 通过向LLM提示中添加随机概念来提升生成输出的多样性，实验表明无关随机词句能显著增加输出多样性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的输出多样性有限，研究者希望探索通过向提示中注入随机概念是否能改善这一限制

Method: 设计系统性评估协议，在提示前添加随机词句（如"Name 10 Hollywood actors"），分析LLM输出的多样性指标，并在多个LLM上进行实验验证

Result: 实验表明，在提示前添加与问题无关的随机词句能显著提高LLM输出的多样性

Conclusion: 随机性注入是提升LLM输出多样性的有效方法，评估协议为系统性评估LLM多样性提供了新思路，未来可探索在其他领域的应用

Abstract: Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form "Name 10 Hollywood actors", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.

</details>


### [13] [Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents](https://arxiv.org/abs/2601.18077)
*Mahesh Ramesh,Kaousheik Jayakumar,Aswinkumar Ramkumar,Pavan Thodima,Aniket Rege*

Main category: cs.CL

TL;DR: 该论文研究了LLM智能体在Hanabi纸牌游戏中的协作推理能力，通过不同上下文工程设置评估了17个先进模型，并发布了首个公开的Hanabi数据集用于微调，显著提升了协作性能。


<details>
  <summary>Details</summary>
Motivation: 在不完全信息下的协作推理对多智能体系统具有挑战性。Hanabi游戏需要心智理论和战略沟通，是研究LLM协作推理能力的理想测试平台。

Method: 在2-5人游戏中评估17个先进LLM智能体，设计了三种上下文工程设置：Watson（仅显式卡牌信息）、Sherlock（贝叶斯推理辅助）、Mycroft（多轮状态跟踪）。发布了HanabiLogs和HanabiRewards数据集，并对4B模型进行监督和强化学习微调。

Result: 最强推理模型在Sherlock设置下平均得分超过15分，但仍落后于人类专家（20分以上）。使用数据集微调的4B模型性能提升21%（监督）和156%（强化学习），接近专有推理模型o4-mini，并超越GPT-4.1 52%。强化学习模型还泛化到其他协作和推理任务。

Conclusion: 上下文工程和数据集微调能显著提升LLM在协作推理任务中的表现，但当前模型仍与人类专家存在差距。发布的Hanabi数据集为研究不完全信息下的协作推理提供了宝贵资源。

Abstract: Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.

</details>


### [14] [MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning](https://arxiv.org/abs/2601.18204)
*Juexiang Ye,Xue Li,Xinyu Yang,Chengkai Huang,Lanshun Nie,Lina Yao,Dechen Zhan*

Main category: cs.CL

TL;DR: MemWeaver是一个统一记忆框架，通过三个互联组件（时序图记忆、经验记忆、段落记忆）和双通道检索策略，显著提升LLM智能体在长期交互中的多跳推理和时序推理能力，同时大幅减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体记忆系统主要依赖非结构化检索或粗粒度抽象，导致时序冲突、推理脆弱和可追溯性有限。需要支持时序一致性、多跳推理和跨会话证据重用的记忆系统。

Method: 提出MemWeaver框架，包含三个组件：1）时序图记忆用于结构化关系推理；2）经验记忆从重复观察中抽象交互模式；3）段落记忆保留原始文本证据。采用双通道检索策略联合检索结构化知识和支持证据。

Result: 在LoCoMo基准测试中，MemWeaver显著提高了多跳和时序推理准确性，同时相比长上下文基线减少了超过95%的输入上下文长度。

Conclusion: MemWeaver通过结构化记忆组件和双通道检索，有效解决了LLM智能体在长期交互中的记忆挑战，实现了更高效、准确的推理能力。

Abstract: Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\% compared to long-context baselines.

</details>


### [15] [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253)
*Peng Sun,Xiangyu Zhang,Duan Wu*

Main category: cs.CL

TL;DR: BoRP是一个用于评估对话AI用户满意度的可扩展框架，利用LLM潜在空间的几何特性，通过极化指数引导的自举机制自动生成评估标准，并使用偏最小二乘法将隐藏状态映射到连续分数，显著优于生成式基线且大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 对于开放式对话助手，传统A/B测试缺乏可靠指标：显式反馈稀疏，隐式指标模糊。需要一种高保真度的满意度评估方法来支持迭代开发。

Method: BoRP框架利用LLM潜在空间的几何特性，采用极化指数引导的自举机制自动生成评估标准，使用偏最小二乘法将隐藏状态映射到连续满意度分数。

Result: 在工业数据集上的实验表明，BoRP（基于Qwen3-8B/14B）在与人判断的一致性方面显著优于生成式基线（甚至优于Qwen3-Max），同时推理成本降低数个数量级，支持全规模监控和高度敏感的A/B测试。

Conclusion: BoRP为开放式对话AI提供了一种可扩展、高保真度的满意度评估框架，解决了传统评估方法的局限性，支持高效的迭代开发和监控。

Abstract: Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.

</details>


### [16] [U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents](https://arxiv.org/abs/2601.18285)
*Jin Su,Runnan Fang,Yeqiu Li,Xiaobin Wang,Shihao Cai,Pengjun Xie,Ningyu Zhang,Fajie Yuan*

Main category: cs.CL

TL;DR: U-Fold是一个针对用户中心任务的动态上下文折叠框架，通过保留完整对话历史但生成意图感知的对话摘要和紧凑的工具日志，解决了现有上下文折叠方法在用户中心对话中的失败模式，显著提升了长上下文、多轮任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在工具增强设置中受限于上下文长度，现有的上下文折叠方法主要为单查询或单意图场景设计，在用户中心对话中存在两个主要失败模式：1) 不可逆地丢弃对后续决策至关重要的细粒度约束和中间事实；2) 摘要无法跟踪演化的用户意图，导致遗漏和错误操作。

Method: 提出U-Fold动态上下文折叠框架，保留完整的用户-智能体对话和工具调用历史，但在每一轮使用两个核心组件：1) 生成意图感知的演化对话摘要；2) 生成紧凑的任务相关工具日志。

Result: 在τ-bench、τ²-bench、VitaBench和更难的上下文膨胀设置上的广泛实验表明，U-Fold始终优于ReAct（在长上下文设置中达到71.4%的胜率）和先前的折叠基线（改进高达27.0%），特别是在长、嘈杂、多轮任务上表现突出。

Conclusion: U-Fold是将上下文管理技术从单查询基准转移到现实用户中心应用的有希望的一步，通过动态上下文折叠有效解决了用户中心对话中的关键挑战。

Abstract: Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.

</details>


### [17] [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296)
*Zhaoyan Gong,Zhiqiang Liu,Songze Li,Xiaoke Guo,Yuanxiang Liu,Xinle Deng,Zhizhen Liu,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: Temp-R1是首个通过强化学习训练的端到端自主TKGQA代理，通过扩展动作空间和反向课程学习，在复杂问题上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有TKGQA方法依赖固定工作流程和昂贵的闭源API，限制了灵活性和可扩展性，需要更自主的解决方案来处理动态事实的多跳依赖和复杂时间约束。

Method: 提出Temp-R1：1）扩展动作空间，包含专门内部动作和外部动作以解决单动作推理的认知过载；2）引入反向课程学习，先训练困难问题再迁移到简单问题，防止捷径学习。

Result: 8B参数的Temp-R1在MultiTQ和TimelineKGQA上实现了最先进的性能，在复杂问题上比强基线提高了19.8%。

Conclusion: Temp-R1为自主时间推理代理建立了新范式，通过强化学习训练和课程学习策略有效解决了TKGQA的挑战。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.

</details>


### [18] [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320)
*Jinwei Lu,Yuanfeng Song,Chen Zhang,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: MultiVis-Agent：一个逻辑规则增强的多智能体框架，用于可靠的多模态可视化生成，通过四层逻辑规则提供数学可靠性保证，在复杂可视化任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界可视化任务涉及复杂的多模态需求（参考图像、代码示例、迭代优化），而现有系统存在单模态输入、一次性生成、工作流程僵化等根本限制。LLM方法虽然显示出潜力，但引入了可靠性挑战，包括灾难性故障和无限循环问题。

Method: 提出MultiVis-Agent框架，采用逻辑规则增强的多智能体架构。引入四层逻辑规则框架，为系统可靠性提供数学保证，同时保持灵活性。这些逻辑规则是指导LLM推理的数学约束，而非替代LLM。形式化了涵盖从基础生成到迭代优化的四个场景的MultiVis任务，并开发了包含1000多个案例的MultiVis-Bench基准。

Result: 在挑战性任务中达到75.63%的可视化得分，显著优于基线方法（57.54-62.79%）。任务完成率达到99.58%，代码执行成功率达到94.56%（无逻辑规则时分别为74.48%和65.10%），成功解决了自动化可视化生成中的复杂性和可靠性挑战。

Conclusion: MultiVis-Agent通过逻辑规则增强的多智能体框架，有效解决了复杂多模态可视化任务的可靠生成问题，在保持灵活性的同时提供了数学可靠性保证，为自动化可视化系统提供了新的解决方案。

Abstract: Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.

</details>


### [19] [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352)
*Manjie Xu,Isabella Yin,Xinyi Tu,Chi Zhang,Yixin Zhu*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在"语义惯性"问题，即难以抑制预训练先验知识来适应动态上下文规则。通过《Baba Is You》游戏测试发现，更大模型反而表现更差，但将动态规则表示为可执行代码而非描述性文本可以逆转这一趋势。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理动态上下文规则时存在"语义惯性"问题，即无法抑制预训练先验知识（如"岩浆是危险的"）来适应矛盾的上下文规则。这限制了模型在需要动态覆盖学习先验的领域中的应用。

Method: 使用《Baba Is You》游戏作为测试平台，其中物理法则可通过文本规则改变。提出Code-Grounded Vistas (LCV)方法，通过微调模型处理反事实对，并识别具有矛盾规则的状态，强制模型关注逻辑约束而非视觉语义。

Result: 发现更大模型在需要抑制预训练关联的自然语言推理任务中表现更差（逆缩放现象）。但将动态表示为可执行代码而非描述性文本可以逆转这一趋势。LCV方法在效率和准确性上都优于昂贵的推理时搜索方法。

Conclusion: 表示形式从根本上决定了缩放是否改善或损害上下文推理能力。这挑战了"更大模型总是更好"的假设，对需要动态覆盖学习先验的领域具有重要意义。

Abstract: LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., "Lava is Dangerous") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting "Lava is Safe"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.

</details>


### [20] [From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation](https://arxiv.org/abs/2601.18533)
*Yuxin Jiang,Yufei Wang,Qiyuan Zhang,Xingshan Zeng,Liangyou Li,Jierun Chen,Chaofan Tao,Haoli Bai,Lifeng Shang*

Main category: cs.CL

TL;DR: RLVRR提出了一种基于可验证参考奖励的强化学习方法，通过从高质量参考中提取有序语言信号（奖励链），将奖励分解为内容和风格两个维度，结合了RL的探索能力和SFT的效率可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR在推理任务中有效，但在开放式生成任务中面临挑战，因为缺乏明确的地面真值。单点监督导致效率低下和奖励黑客问题，需要一种能统一结构化推理和开放式生成的训练方法。

Method: RLVRR从高质量参考中提取有序语言信号（奖励链），将奖励分解为内容维度（保留确定性核心概念如关键词）和风格维度（通过LLM验证评估风格属性遵循程度），结合了强化学习的探索优势和监督微调的效率可靠性。

Result: 在10多个基准测试中，RLVRR显著优于使用十倍数据训练和先进奖励模型的SFT方法，统一了结构化推理和开放式生成的训练，在保持输出多样性的同时具有更好的泛化能力。

Conclusion: RLVRR为通用LLM对齐提供了一条原则性且高效的可验证强化学习路径，解决了开放式生成中的监督信号问题，实现了探索效率和可靠性的平衡。

Abstract: Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.

</details>


### [21] [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552)
*Devansh Srivastav,David Pape,Lea Schönherr*

Main category: cs.CL

TL;DR: 该论文系统分析了LLM中的隐藏意图问题，提出了十类隐藏意图的分类法，展示了如何诱导这些行为，评估了检测方法的局限性，并发现开放世界环境下检测会失效。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地嵌入日常决策，但其输出可能编码微妙、无意的行为，影响用户信念和行动。这些隐藏意图可能来自训练优化伪影或被恶意开发者故意诱导，但在实践中难以检测。

Method: 1) 提出基于社会科学研究的十类隐藏意图分类法；2) 在受控模型中诱导隐藏意图；3) 系统评估包括推理和非推理LLM法官在内的检测方法；4) 在开放世界设置下进行压力测试；5) 对已部署的SOTA LLM进行定性案例研究。

Result: 1) 隐藏意图可以在受控模型中轻松诱导；2) 在现实开放世界设置下检测方法失效，特别是在低流行率条件下；3) 压力测试显示审计失败，除非有极低的误报率或对操纵类型的强先验；4) 案例研究显示十类隐藏意图都存在于已部署的SOTA LLM中。

Conclusion: LLM中的隐藏意图检测在开放世界环境下存在严重缺陷，需要更稳健的框架。该研究为理解、诱导和压力测试此类行为提供了基础，建立了灵活的分类法来预测不断演变的威胁并指导治理。

Abstract: LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.

</details>


### [22] [One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization](https://arxiv.org/abs/2601.18572)
*Franziska Weeber,Vera Neplenbroek,Jan Batzner,Sebastian Padó*

Main category: cs.CL

TL;DR: 研究比较了六种常用的人设提示方法在七个LLM上的表现，发现不同提示方法会产生显著差异，建议未来个性化研究应评估多种外部有效提示方法


<details>
  <summary>Details</summary>
Motivation: LLM个性化虽然能提升用户体验，但可能引入或放大群体偏见。现有研究通常只使用单一提示方法（如用户名或显式属性）来研究偏见，忽略了LLM对提示变化的敏感性和某些提示在真实交互中的罕见性

Method: 比较了六种常用的人设提示方法，在七个开源和专有LLM上测试了四个写作和建议任务，分析不同提示方法产生的响应差异

Result: 虽然不同提示方法总体上高度相关，但它们在响应中产生了显著差异。单一提示方法得出的结论可能不可靠

Conclusion: 应谨慎基于单一提示方法做出结论，建议未来个性化研究评估多种外部有效的提示方法，以提高研究的鲁棒性和外部有效性

Abstract: Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.

</details>


### [23] [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730)
*Henry Bell,Caroline Zhang,Mohammed Mobasserul Haque,Dhaval Potdar,Samia Zaman,Brandon Fain*

Main category: cs.CL

TL;DR: REFLECT是一个无需训练或数据的推理时宪法对齐框架，通过上下文中的自我评估、自我批判和最终修订来对齐LLM与价值原则。


<details>
  <summary>Details</summary>
Motivation: 现有的参数微调方法（如RLHF）计算成本高、需要精心工程和调优，且需要难以获取的人工标注数据。需要一种即插即用的推理时对齐方法。

Method: REFLECT是完全在上下文中操作的推理时框架，包含：(i) 基于宪法的基本响应生成，(ii) 自我评估，(iii)(a) 自我批判，和(iii)(b) 最终修订。通过显式的原则推理提供透明推理轨迹。

Result: REFLECT显著提高了LLM对多样复杂原则的符合度，包括与原始参数微调强调的原则完全不同的原则，且不牺牲事实推理能力。特别有效减少罕见但严重的原则违反，提高安全性和鲁棒性。

Conclusion: REFLECT提供了一种无需训练、即插即用的宪法对齐方法，能够自然生成有用的训练数据用于传统参数微调技术，实现高效扩展和减少推理时计算开销。

Abstract: The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16984)
*Rahul Ghosh,Chun-Hao Liu,Gaurav Rele,Vidya Sagar Ravipati,Hazar Aouad*

Main category: cs.LG

TL;DR: TelcoAI是一个面向3GPP技术文档的多模态RAG系统，通过智能分块、查询规划和多模态融合，在复杂技术文档理解上比现有方法提升16%性能


<details>
  <summary>Details</summary>
Motivation: 3GPP技术规范结构复杂、格式密集且包含多模态内容，现有LLM方法难以处理复杂查询、视觉信息和文档间依赖关系，需要专门解决方案

Method: 提出TelcoAI系统，采用基于章节感知的分块、结构化查询规划、元数据引导检索以及文本与图表的跨模态融合技术

Result: 在多个基准测试（包括专家策划查询）上达到87%召回率、83%声明召回率和92%忠实度，比最先进基线提升16%

Conclusion: 证明了智能代理和多模态推理在技术文档理解中的有效性，为实际电信研究和工程提供了先进解决方案

Abstract: The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.

</details>


### [25] [Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation](https://arxiv.org/abs/2601.17133)
*Inderjeet Singh,Eleonore Vissol-Gaudin,Andikan Otung,Motoyoshi Sekiya*

Main category: cs.LG

TL;DR: KNEXA-FL是一个去中心化联邦学习框架，通过上下文多臂赌博机算法优化异构LLM代理之间的知识交换，解决了隐私保护与模型性能的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在专业化LLM微调中存在隐私与性能的冲突：集中式FL有单点故障和模型反转攻击风险，而去中心化FL的随机P2P配对效率低下且可能导致负迁移。

Method: 提出KNEXA-FL框架，采用非聚合的中心配置器/匹配器(CPM)，将P2P协作建模为上下文多臂赌博机问题，使用LinUCB算法基于抽象代理配置文件学习最优匹配策略，通过安全蒸馏实现异构PEFT-based LLM代理间的直接知识交换。

Result: 在代码生成任务上的实验显示，KNEXA-FL相比随机P2P协作将Pass@1提高了约50%，且表现出稳定的收敛性，而集中式蒸馏基线则出现了灾难性的性能崩溃。

Conclusion: 自适应、基于学习的编排是构建稳健有效的去中心化AI生态系统的基础原则，KNEXA-FL在保护隐私的同时显著提升了模型性能。

Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.

</details>


### [26] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: DLR提出了一种潜在空间双向对比强化学习框架，将推理链的试错成本从昂贵的token级序列生成转移到连续潜在流形，通过冻结主模型参数避免灾难性遗忘，实现更稳定的训练收敛和更长推理链支持。


<details>
  <summary>Details</summary>
Motivation: LLM在处理复杂多步推理任务时往往只是"统计拟合"而非系统逻辑推理。传统RL在离散token空间直接应用面临三个挑战：样本效率低的rollout、高梯度估计方差、灾难性遗忘风险。

Method: 提出DeepLatent Reasoning (DLR)框架：1) 使用轻量级辅助模型在潜在空间采样K个推理链编码；2) 基于正确性和格式的双重奖励机制筛选；3) 仅高价值潜在轨迹输入冻结主模型进行单次解码；4) 设计对比学习目标实现潜在空间定向探索。

Result: 在可比较的GPU计算预算下，DLR实现了更稳定的训练收敛，支持更长的推理链，促进了推理能力的可持续积累，为LLM提供了可靠且可扩展的强化学习路径。

Conclusion: DLR通过将试错成本转移到连续潜在流形，从根本上解决了传统RL在离散token空间的结构瓶颈，同时通过冻结主模型参数消除了灾难性遗忘，为LLM的可靠强化学习提供了可行路径。

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [27] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: 提出无监督观察模仿学习(UfO)，通过两阶段学习过程解决现有ILfO方法需要动作监督、假设状态有单一最优动作、不考虑实际环境状态等限制，在五个环境中超越教师和其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有观察模仿学习方法(ILfO)存在三个主要限制：需要基于动作的监督优化、假设状态有单一最优动作、倾向于直接应用教师动作而不充分考虑实际环境状态。虽然真实信息存在于观察到的轨迹中，但现有方法难以在无监督情况下提取这些信息。

Method: 提出无监督观察模仿学习(UfO)，采用两阶段学习过程：1) 首先从观察到的状态转移中近似教师的真实动作；2) 然后通过调整智能体轨迹使其与教师轨迹紧密对齐来进一步优化学习策略。

Result: 在五个广泛使用的环境中，UfO不仅超越了教师和所有其他ILfO方法，还显示出最小的标准差。标准差的减少表明在未见场景中具有更好的泛化能力。

Conclusion: UfO成功解决了现有ILfO方法的限制，通过无监督两阶段学习实现了更好的性能和泛化能力，在多个环境中验证了其有效性。

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [28] [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)
*Hao Li,He Cao,Shenyao Peng,Zijing Liu,Bin Feng,Yu Wang,Zhiyuan Yan,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.LG

TL;DR: ChemCRAFT框架通过代理强化学习将化学推理与知识存储解耦，使本地部署的小模型通过外部知识检索实现高性能，降低推理成本并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前生化领域语言模型存在两难：小模型易产生幻觉且知识有限，大云模型有隐私风险和高推理成本。需要一种既能保持高性能又能在本地部署的解决方案。

Method: 1) 构建代理轨迹管道和化学代理沙箱；2) 创建ChemToolDataset工具轨迹数据集；3) 提出SMILES-GRPO构建密集化学奖励函数；4) 通过代理强化学习使小模型学会调用化学代理进行精确信息检索。

Result: ChemCRAFT在药物设计的分子结构分析、分子优化和合成路径预测等多个方面优于当前云基大模型，证明科学推理不是模型规模的涌现能力，而是可学习的工具编排策略。

Conclusion: 该工作建立了成本效益高且隐私保护的AI辅助化学范式，为通过本地可部署代理加速分子发现开辟了新途径。

Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.

</details>


### [29] [Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games](https://arxiv.org/abs/2601.17716)
*Daniel M. Pedrozo,Telma W. de L. Soares,Bryan L. M. de Oliveira*

Main category: cs.LG

TL;DR: 本文提出一个多轮对话框架，通过信息增益指标定量评估LLM在层次知识图环境中通过是/否问题收集信息的能力，并在地理猜城市游戏中验证了具有显式推理能力的模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏基于信息增益的全面评估框架，无法系统比较使用思维链推理和不使用思维链的模型。LLM在解决用户请求模糊性时提问能力仍有不足，需要更好的评估方法。

Method: 提出多轮对话框架，采用三个交互的LLM代理：提问代理、回答代理和假设空间更新代理。使用基于香农熵的信息增益作为主要指标，在地理猜城市游戏中实例化该框架，评估不同LLM变体在完全和部分可观测条件下的表现。

Result: 具有显式推理能力的模型在每轮获得更高的信息增益，且用更少步骤达到解决方案，特别是在部分可观测设置下。小模型通过更积极地探索候选问题来弥补能力限制，而大模型在选择最优查询时表现出更高的自信度。

Conclusion: 该框架为评估LLM的信息收集能力提供了定量方法，证明了显式推理对提高提问效率的重要性，为LLM代理的模糊性解决能力评估提供了新工具。

Abstract: Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.

</details>


### [30] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: DRPG是一个用于自动生成学术反驳的智能体框架，通过分解评审意见、检索论文证据、规划反驳策略和生成回应四个步骤，显著优于现有方法，使用8B模型即可达到超越人类平均水平的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在科研工作流中应用日益广泛，但学术反驳这一学术交流和同行评审的关键环节的自动化支持仍未被充分探索。现有方法通常依赖现成的LLM或简单流程，难以处理长上下文理解，且无法生成有针对性和说服力的回应。

Method: 提出DRPG框架，包含四个步骤：1) 将评审意见分解为原子关注点；2) 从论文中检索相关证据；3) 规划反驳策略；4) 据此生成回应。其中规划器在识别最可行反驳方向方面准确率超过98%。

Result: 在顶级会议数据上的实验表明，DRPG显著优于现有反驳流程，仅使用8B模型即可达到超越人类平均水平的性能。规划器设计有效，能提供多视角和可解释的建议，且在更复杂的多轮设置中表现良好。

Conclusion: DRPG框架有效展示了其生成高质量反驳内容和支持学术讨论规模化的潜力，为学术反驳自动化提供了有效的解决方案。

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [31] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 该研究对比了指令遵循型LLM和推理增强型LLM的剪枝策略效果，发现不同范式下剪枝效果存在显著差异，需要针对推理增强模型设计专门的剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝研究主要关注指令遵循型LLM，不清楚这些策略是否适用于生成长中间推理轨迹的推理增强模型。需要系统比较两种模型范式的剪枝效果差异。

Method: 采用控制实验设计，对齐剪枝校准和恢复数据与原始训练分布。评估静态深度剪枝、静态宽度剪枝和动态剪枝在17个分类、生成和推理任务上的表现。

Result: 深度剪枝在分类任务上表现更好，宽度剪枝在生成和推理任务上更稳健；静态剪枝能更好保持推理性能，动态剪枝在分类和生成上表现优异但在长链推理上仍有挑战。

Conclusion: 推理增强型LLM需要专门设计的剪枝策略，不能简单沿用指令遵循型模型的剪枝方法。剪枝策略应考虑模型范式的独特特征。

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [32] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: 提出ADRC-Lagrangian方法，将主动抗扰控制与拉格朗日方法结合，显著减少安全强化学习中的振荡和安全违规


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法（包括PID和经典拉格朗日方法）存在振荡和频繁安全违规问题，主要由于参数敏感性和固有相位滞后导致

Method: 提出ADRC-Lagrangian方法，将主动抗扰控制（ADRC）集成到拉格朗日框架中，增强鲁棒性并减少振荡。该统一框架将经典和PID拉格朗日方法作为特例包含在内

Result: 实验表明，该方法将安全违规减少高达74%，约束违规幅度减少89%，平均成本降低67%，在复杂环境中表现出优越的安全性能

Conclusion: ADRC-Lagrangian方法为安全强化学习提供了更鲁棒、更有效的解决方案，显著改善了安全性能并减少了振荡问题

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [33] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: 提出了一个用于大语言模型强化学习的FP8推理栈，通过量化、KV缓存优化和重要性采样校正，在保持学习效果的同时提升44%的推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习中的推理（生成）阶段成为瓶颈，长输出序列导致注意力机制和KV缓存占用大量内存和时间。FP8低精度计算可以加速推理，但直接应用会面临权重频繁变化和训练-推理不匹配的挑战。

Method: 1) 使用分块FP8量化实现W8A8线性层推理；2) 通过每步QKV尺度重校准将FP8扩展到KV缓存；3) 使用基于重要性采样的推理校正（token级TIS/MIS变体）缓解不匹配问题。

Result: 在密集和MoE模型上，这些技术实现了高达44%的推理吞吐量提升，同时保持了与BF16基线相当的学习行为。

Conclusion: 提出的FP8推理栈为LLM强化学习提供了实用的低精度推理解决方案，有效解决了内存瓶颈和训练-推理不匹配问题，显著提升了推理效率。

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [34] [PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR](https://arxiv.org/abs/2601.18207)
*James Burgess,Jan N. Hansen,Duo Peng,Yuhui Zhang,Alejandro Lozano,Min Woo Sun,Emma Lundberg,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出了一个针对科学论文搜索的智能体训练框架，包括1600万篇生物医学论文摘要的搜索语料库和包含6万个样本的PaperSearchQA问答数据集，通过强化学习训练搜索智能体在技术问答任务上超越传统检索基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索智能体主要针对通用领域问答，缺乏对科学、工程和医学等专业技术领域的关注。科学论文搜索对于AI科学家系统和实际科研工作者具有重要意义，需要专门的技术问答能力。

Method: 构建了包含1600万篇生物医学论文摘要的搜索语料库，创建了包含6万个样本的PaperSearchQA问答数据集和基准测试。使用强化学习与可验证奖励（RLVR）训练搜索智能体，基于Search-R1代码库实现。

Result: 训练的搜索智能体在技术问答任务上超越了非强化学习的检索基线方法。观察到智能体展现出规划、推理和自我验证等有趣行为。

Conclusion: 该方法成功将搜索智能体扩展到科学领域，为未来AI科学家系统奠定了基础。数据创建方法具有可扩展性，可轻松扩展到其他科学领域。

Abstract: Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

</details>


### [35] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: TriPlay-RL是一个闭环强化学习框架，通过攻击者、防御者和评估者三个角色的迭代协作，实现无需人工标注的LLM安全对齐，显著提升各角色的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全风险日益突出，需要有效缓解有毒有害内容的生成。现有安全对齐方法通常需要大量人工标注，且难以实现攻击、防御和评估三个角色的协同改进。

Method: 提出TriPlay-RL闭环强化学习框架，让攻击者生成对抗提示、防御者进行安全防御、评估者评估响应，三个角色在统一学习循环中迭代协作，实现近零人工标注的协同进化。

Result: 攻击者保持高输出多样性的同时，对抗效果提升20%-50%；防御者安全性能提升10%-30%且不损害一般推理能力；评估者通过迭代持续细化判断能力，能准确区分不安全响应、简单拒绝和有用指导。

Conclusion: TriPlay-RL为LLM安全对齐建立了高效可扩展的范式，实现了在统一学习循环中的持续协同进化，为解决LLM安全风险提供了有效方案。

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [36] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: JitRL是一个无需训练、基于动态记忆检索的测试时策略优化框架，通过检索相关经验轨迹实时估计动作优势值，直接调整LLM输出logits，实现持续学习。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在部署后权重固定，难以持续适应新任务。传统强化学习计算成本高且存在灾难性遗忘问题，需要一种无需梯度更新、低成本的持续学习方法。

Method: JitRL维护动态非参数化记忆库，检索相关经验轨迹实时估计动作优势值，通过加法更新规则直接调整LLM输出logits，该规则被证明是KL约束策略优化目标的精确闭式解。

Result: 在WebArena和Jericho基准测试中，JitRL在无需训练的方法中达到新的SOTA，甚至超越计算昂贵的微调方法（如WebRL），同时降低30倍以上的成本。

Conclusion: JitRL为持续学习智能体提供了一种可扩展的路径，通过无需训练、基于记忆检索的方法实现了高效的测试时策略优化，显著降低了计算成本。

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [37] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: 提出On-Policy Self-Distillation (OPSD)框架，让单个模型既当老师又当学生，通过在不同上下文条件下生成不同分布，利用特权信息（如已验证的推理轨迹）来指导仅看到问题的学生版本，提高推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法存在分布不匹配问题，且通常需要单独的教师模型，未能充分利用推理数据集中可用的真实解决方案。受启发于"足够强大的LLM可以合理化外部特权推理轨迹并教导其较弱版本"的直觉。

Method: 提出OPSD框架：单个模型同时作为教师和学生，教师策略基于特权信息（如已验证推理轨迹），学生策略仅看到问题；训练最小化这两个分布在学生自身轨迹上的每令牌差异。

Result: 在多个数学推理基准测试中，相比GRPO等强化学习方法实现4-8倍的令牌效率，且性能优于离策略蒸馏方法。

Conclusion: OPSD框架通过自我蒸馏有效利用特权信息，解决了分布不匹配问题，实现了高效的知识蒸馏，无需单独的教师模型。

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [38] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: SOAR是一个基于元强化学习的自改进框架，通过教师模型为学生模型生成合成问题课程，利用学生进步作为奖励信号，在初始成功率极低（0/128）的数学基准上实现突破性学习。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在初始成功率低的推理任务上容易陷入学习停滞，因为缺乏足够的训练信号。本文探索预训练大语言模型是否能够利用其潜在知识为自身无法解决的问题生成自动化课程。

Method: 提出SOAR框架：使用教师模型副本为学生模型副本生成合成问题，教师根据学生在困难问题子集上的进步获得奖励。关键创新是使用学生实际进步作为基础奖励，而非内在代理奖励。

Result: 在数学基准最困难子集（初始成功率0/128）上实现突破：1）证明了双层元强化学习在稀疏二元奖励下能够解锁学习；2）基础奖励优于先前LLM自玩的内在奖励方案，避免不稳定性和多样性崩溃；3）生成问题的结构质量和明确性比解决方案正确性对学习进步更重要。

Conclusion: 模型生成有用"垫脚石"的能力不需要预先具备解决困难问题的能力，这为无需额外策划数据即可突破推理停滞提供了原则性路径。

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


### [39] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出POPE方法，利用特权信息（如人类解决方案）引导LLM在困难推理问题上的探索，解决强化学习中探索不足的问题，显著提升在挑战性推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在训练LLM推理能力时，面对困难问题经常无法探索到任何正确轨迹，导致零奖励和缺乏学习信号。传统RL探索方法（如熵奖励、重要性比率裁剪）无法解决此问题，而混合难易问题训练会产生射线干扰，使优化集中在已解决问题上而阻碍困难问题的进展。

Method: 提出特权在线探索（POPE）方法：1）利用人类或其他oracle解决方案作为特权信息引导困难问题的探索；2）在困难问题前添加oracle解决方案的前缀，使RL在引导轨迹中获得非零奖励；3）通过指令跟随和推理的协同作用，将习得的行为迁移回原始无引导问题。

Result: POPE扩展了可解决问题的集合，在挑战性推理基准上显著提升了性能，有效解决了RL在困难推理问题上的探索不足问题。

Conclusion: POPE方法通过特权信息引导探索，成功解决了LLM强化学习中困难推理问题的探索挑战，相比传统方法显著提升了训练效果和问题解决能力。

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [40] [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586)
*Miguel Costa,Arthur Vandervoort,Carolin Schmidt,Morten W. Petersen,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 提出一个结合集成评估模型与强化学习的决策支持框架，用于学习城市交通系统在气候变化不确定性下的多十年自适应投资路径


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧降雨等灾害，增加城市交通系统中断风险。由于基础设施投资的长期性、序列性、深度不确定性和跨部门复杂性，设计有效的适应策略具有挑战性

Method: 提出通用决策支持框架，将集成评估模型与强化学习耦合，结合长期气候预测、灾害概率映射、基础设施影响传播和价值评估，在强化学习循环中学习自适应气候适应策略

Result: 在哥本哈根市的城市内涝案例中（2024-2100年），学习到的策略产生协调的时空路径，相比传统优化基准（不作为和随机行动）具有更好的鲁棒性

Conclusion: 该框架展示了可迁移到其他灾害和城市的潜力，为城市气候适应提供有效的决策支持工具

Abstract: Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.

</details>


### [41] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: PrefixRL：通过重用离策略轨迹的前缀来提升LLM推理的强化学习效率，避免离策略不稳定性，在困难问题上实现2倍训练加速和3倍最终奖励提升


<details>
  <summary>Details</summary>
Motivation: 传统RL方法在LLM推理的困难问题上效率低下，因为正确的在线轨迹稀少、策略梯度消失、学习停滞。需要重用旧的采样计算（来自先前推理或RL训练）来提升效率，但标准离策略方法存在优化不稳定性问题

Method: PrefixRL：使用成功离策略轨迹的前缀作为条件，运行在线RL来完成剩余部分。通过调节前缀长度来调制问题难度，避免离策略不稳定性。离策略轨迹通过基础模型的拒绝采样获取，形成自我改进循环

Result: 在困难推理问题上，PrefixRL达到相同训练奖励的速度比最强基线（在离策略数据上进行SFT然后RL）快2倍，最终奖励提升3倍。增益可迁移到保留基准测试，即使离策略轨迹来自不同模型家族也有效

Conclusion: PrefixRL通过重用离策略轨迹前缀有效提升RL效率，避免了离策略不稳定性，在困难推理问题上显著加速训练并提升性能，具有实际应用的灵活性

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


### [42] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出了Hallucination Risk Bound理论框架，将LLM幻觉风险分解为数据驱动和推理驱动两部分，并基于此开发了HalluGuard检测方法，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、法律等高风险领域的可靠性常因幻觉问题而受损。现有检测方法通常只针对单一幻觉来源，且依赖任务特定启发式方法，难以推广到复杂场景。

Method: 1. 提出Hallucination Risk Bound理论框架，将幻觉风险正式分解为数据驱动（训练时失配）和推理驱动（推理时不稳定性）两部分；2. 基于此开发HalluGuard，利用NTK的几何结构和捕获的表征来联合识别两类幻觉。

Result: 在10个多样化基准测试、11个竞争性基线和9个流行LLM骨干网络上评估，HalluGuard在检测多种LLM幻觉形式方面始终达到最先进的性能。

Conclusion: Hallucination Risk Bound为分析幻觉产生和演化提供了理论基础，HalluGuard作为其实践应用，能够有效检测LLM中的多种幻觉类型，提升LLM在高风险领域的可靠性。

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability](https://arxiv.org/abs/2601.17168)
*Judy Zhu,Dhari Gandhi,Himanshu Joshi,Ahmad Rezaie Mianroodi,Sedef Akinli Kocak,Dhanesh Ramachandran*

Main category: cs.AI

TL;DR: 该论文评估现有可解释性方法在智能体系统中的应用局限性，提出需要专门针对智能体系统设计新的可解释性技术，以确保AI系统的安全部署。


<details>
  <summary>Details</summary>
Motivation: 智能体系统与传统机器学习模型在架构和部署上存在根本差异，引入了独特的安全挑战（如目标错位、决策错误累积、协调风险），需要专门的可解释性设计来确保自主行为的可追溯性和可问责性。

Method: 评估现有可解释性方法在智能体系统中的适用性和局限性，识别其在提供智能体决策洞察方面的能力差距，提出未来专门针对智能体系统开发可解释性技术的方向。

Result: 现有主要为静态模型开发的可解释性技术在应用于智能体系统时显示出局限性，无法充分处理智能体系统的时间动态性、复合决策和上下文依赖行为。

Conclusion: 需要开发专门针对智能体系统设计的新可解释性方法，在智能体生命周期的各个阶段（目标形成、环境交互、结果评估）嵌入监督机制，这对于确保智能体AI系统的安全和可问责部署至关重要。

Abstract: Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.

</details>


### [44] [Phase Transition for Budgeted Multi-Agent Synergy](https://arxiv.org/abs/2601.17311)
*Bang Liu,Linglong Kong,Jian Pei*

Main category: cs.AI

TL;DR: 该论文提出了一个可校准的理论框架，用于预测多智能体系统在固定推理预算下的性能表现，识别出帮助、饱和和崩溃三种状态，并通过数学分析揭示了通信、相关性和上下文窗口等约束条件对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然能提高可靠性，但在固定推理预算下，其性能往往呈现帮助、饱和甚至崩溃三种状态。现有研究缺乏一个能够预测这些状态并指导系统设计的理论框架，特别是需要考虑现代智能体栈的三个关键约束：有限上下文窗口、有损的智能体间通信以及相似智能体之间的共享故障。

Method: 提出了一个最小化且可校准的理论框架，将每个叶智能体用计算-性能缩放指数β描述，通信用消息长度保真度曲线γ(m)描述，相关性用有效共享误差相关性ρ描述，上下文窗口W则施加了硬性的扇入限制。通过数学分析，研究了具有相关输入和有损通信的深度b叉树中的多数聚合任务，推导了相变条件、组织指数s和计算分配规则。

Result: 证明了在深度b叉树中存在一个尖锐的相变：单个标量α_ρ（结合γ(m)、ρ和扇入b）决定了弱信号是被放大到非平凡固定点还是被洗牌到随机水平。在放大状态下，推导了组织指数s，并证明当s>β时会出现预算协同效应（即在相同总预算下优于最佳单个智能体）。通过合成模拟验证了预测的相边界，并解释了最近大规模匹配预算研究中LLM智能体系统缩放的主要瓶颈。

Conclusion: 该理论框架能够预测多智能体系统在固定推理预算下的性能表现，揭示了通信、相关性和上下文窗口约束对系统性能的关键影响，为设计高效的多智能体系统提供了数学基础和实用指导。

Abstract: Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $β$; communication is captured by a message-length fidelity curve $γ(m)$; dependence is captured by an effective shared-error correlation $ρ$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $α_ρ$ (combining $γ(m)$, $ρ$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>β$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.

</details>


### [45] [TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow](https://arxiv.org/abs/2601.17332)
*Yicheng Tao,Hongteng Xu*

Main category: cs.AI

TL;DR: TheoremForge是一个低成本的形式化数学数据合成框架，通过任务分解和解耦提取策略，显著提高了数据生成效率。


<details>
  <summary>Details</summary>
Motivation: 形式化数学中智能体工作流的高成本阻碍了大规模数据合成，导致开源语料库稀缺，需要更经济高效的解决方案。

Method: 将形式化过程分解为五个子任务：陈述形式化、证明生成、前提选择、证明修正和证明草图；采用解耦提取策略从失败轨迹中恢复有效训练信号。

Result: 在2000个问题的基准测试中，TheoremForge达到12.6%的验证率（优于8.6%的基线），每个成功轨迹平均成本仅0.481美元，数据产量比标准过滤方法提高1.6倍。

Conclusion: TheoremForge为训练未来专家模型提供了一个可扩展的数据飞轮构建框架，能有效降低形式化数学数据合成的成本。

Abstract: The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \textit{statement formalization}, \textit{proof generation}, \textit{premise selection}, \textit{proof correction} and \textit{proof sketching}. By implementing a \textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\%, surpassing the 8.6\% baseline, at an average cost of only \textbf{\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \textbf{1.6$\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \href{https://github.com/timechess/TheoremForge}{here}.

</details>


### [46] [The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability](https://arxiv.org/abs/2601.17335)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该论文从理论角度分析AGI的定义问题，证明AGI无法独立于任务分布而存在，缺乏普遍鲁棒性，有限资源下无法实现无限泛化，且无法通过计算程序（包括自认证）完全验证。


<details>
  <summary>Details</summary>
Motivation: 研究AGI是否具有一致的理论定义，能够支持关于存在性、鲁棒性或自验证的绝对主张。探讨强分布无关的AGI主张是否合理，以及AI的实证进展是否意味着可实现自认证的通用智能。

Method: 采用公理化方法，将AGI形式化为分布性、资源受限的语义谓词，通过任务族、任务分布、性能函数和显式资源预算进行索引。使用理论证明方法，包括Rice风格和哥德尔-塔斯基论证。

Result: 1. 通用性是关系性的，不存在分布无关的AGI概念；2. 任意小的任务分布扰动可通过悬崖集使AGI属性失效，排除普遍鲁棒性；3. 有限资源下无法实现跨任务族的无限泛化；4. AGI是非平凡的语义属性，无法通过任何可计算程序（包括代理自身）进行可靠且完整的认证。

Conclusion: 强分布无关的AGI主张在没有显式形式索引的情况下是未定义的，而非错误的。AI的实证进展并不意味可实现自认证的通用智能，依赖内部自认证的递归自我改进方案是有问题的。

Abstract: We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and Gödel--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.

</details>


### [47] [Multi-Agent Learning Path Planning via LLMs](https://arxiv.org/abs/2601.17346)
*Haoxin Xu,Changyong Qi,Tong Liu,Bohao Zhang,Anna He,Bingqian Jiang,Longwei Zheng,Xiaoqing Gu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体学习路径规划框架，通过角色和规则协作机制实现透明、可解释的个性化学习路径推荐。


<details>
  <summary>Details</summary>
Motivation: 现有智能导学系统中的学习路径规划方法缺乏透明度、适应性和以学习者为中心的可解释性，需要解决这些挑战。

Method: 提出多智能体学习路径规划框架，包含三个任务特定智能体：学习者分析智能体、路径规划智能体和反思智能体，通过结构化提示和预定义规则协作，基于认知负荷理论和最近发展区理论确保认知对齐。

Result: 在MOOCCubeX数据集上使用7个大语言模型进行实验，MALPP在路径质量、知识序列一致性和认知负荷对齐方面显著优于基线模型，消融研究验证了协作机制和理论约束的有效性。

Conclusion: 该研究为教育领域可信、可解释AI的发展做出贡献，展示了一种基于大语言模型的可扩展、以学习者为中心的自适应教学方法。

Abstract: The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.

</details>


### [48] [Intelligence Requires Grounding But Not Embodiment](https://arxiv.org/abs/2601.17588)
*Marcus Ma,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 论文认为智能需要"接地"(grounding)而非"具身"(embodiment)，提出智能的四个属性（动机、预测能力、因果理解、经验学习）都可通过非具身但接地的智能体实现。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的进步，关于智能是否需要具身化的科学争论重新兴起。作者旨在澄清智能的本质，区分具身化和接地这两个概念，论证接地才是智能的必要条件。

Method: 首先定义智能为具备四个属性：动机、预测能力、因果理解和经验学习。然后论证每个属性都可通过非具身但接地的智能体实现。最后通过数字环境中智能LLM代理的思想实验来支持论点，并回应可能的反驳。

Result: 论文得出结论：接地（而非具身）是智能的必要条件。智能的四个核心属性都可以在非具身但接地的系统中实现，这为数字环境中的智能LLM代理提供了理论基础。

Conclusion: 智能需要接地（与环境互动的能力），而不一定需要物理具身。这一区分有助于澄清关于LLMs和人工智能本质的哲学争论，为数字智能体的发展提供理论支持。

Abstract: Recent advances in LLMs have reignited scientific debate over whether embodiment is necessary for intelligence. We present the argument that intelligence requires grounding, a phenomenon entailed by embodiment, but not embodiment itself. We define intelligence as the possession of four properties -- motivation, predictive ability, understanding of causality, and learning from experience -- and argue that each can be achieved by a non-embodied, grounded agent. We use this to conclude that grounding, not embodiment, is necessary for intelligence. We then present a thought experiment of an intelligent LLM agent in a digital environment and address potential counterarguments.

</details>


### [49] [Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context](https://arxiv.org/abs/2601.17642)
*Zhihao Zhang,Liting Huang,Guanghao Wu,Preslav Nakov,Heng Ji,Usman Naseem*

Main category: cs.AI

TL;DR: 论文提出了Health-ORSC-Bench，首个大规模医疗AI基准测试，用于系统评估大语言模型在医疗领域的过度拒绝和安全完成能力，揭示了当前模型在安全性和实用性之间的平衡困境。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI安全对齐主要依赖二元拒绝边界，导致对良性查询的过度拒绝或对有害查询的不安全合规。现有基准测试只关注极端情况，无法评估模型在双用途或边界查询上提供安全高层指导而不跨越可操作危害的"安全完成"能力。

Method: 构建了包含31,920个良性边界提示的Health-ORSC-Bench基准，涵盖七个健康类别（如自残、医疗错误信息）。采用自动化流程结合人工验证，在不同意图模糊度水平上测试模型。评估了30个最先进的LLM，包括GPT-5和Claude-4。

Result: 安全优化的模型经常拒绝高达80%的"困难"良性提示，而领域特定模型常为实用性牺牲安全性。模型家族和规模显著影响校准：大型前沿模型（如GPT-5、Llama-4）表现出"安全悲观主义"和更高的过度拒绝，相比小型或MoE模型（如Qwen-3-Next）。

Conclusion: 当前LLM在拒绝和合规之间难以平衡，Health-ORSC-Bench为下一代医疗AI助手提供了严格的校准标准，推动实现细致、安全和有用的完成。代码和数据将在接受后发布。

Abstract: Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \emph{over-refusal} of benign queries or \emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \textbf{Over-Refusal} and \textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\% of "Hard" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit "safety-pessimism" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \textcolor{red}{Warning: Some contents may include toxic or undesired contents.}

</details>


### [50] [DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories](https://arxiv.org/abs/2601.17678)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 提出DIML框架，通过观察自利学习代理的战略交互轨迹来反推未知的激励生成机制，包括非结构化（如神经网络）机制


<details>
  <summary>Details</summary>
Motivation: 现有方法如逆博弈论和多智能体逆强化学习通常只能推断结构化机制内的效用/奖励参数，而无法处理非结构化机制；可微分机制设计是前向优化而非从行为中推断。需要一种能从观察到的战略交互中恢复任意激励生成机制的方法

Method: 提出DIML（Differentiable Inverse Mechanism Learning）框架，基于似然的方法，通过多智能体学习动力学模型进行微分，使用候选机制生成预测观察行为所需的反事实收益

Result: 在条件logit响应模型下建立了收益差异的可识别性，证明了最大似然估计的统计一致性；在非结构化神经机制、拥堵收费、公共物品补贴和大规模匿名游戏等场景中，DIML能可靠恢复可识别的激励差异，支持反事实预测，性能在小环境中接近表格枚举oracle，在大规模（百参与者）环境中具有良好的收敛性

Conclusion: DIML为从观察到的战略交互中推断激励生成机制提供了有效的框架，能够处理非结构化机制并扩展到大规模环境，为机制设计和分析提供了新工具

Abstract: We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.

</details>


### [51] [SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL](https://arxiv.org/abs/2601.17699)
*Harper Hua,Zhen Han,Zhengyuan Shen,Jeremy Lee,Patrick Guan,Qi Zhu,Sullam Jeoung,Yueyan Chen,Yunfei Bai,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.AI

TL;DR: SQL-Trail：一个基于多轮强化学习的Text-to-SQL代理框架，通过自适应预算分配和复合奖励机制，在交互式环境中迭代优化SQL查询生成，显著超越单次生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在Text-to-SQL生成上仍与人类专家存在明显差距，主要原因是现有方法采用单次生成范式，缺乏人类专家自然使用的迭代推理、模式探索和错误纠正行为。

Method: 提出SQL-Trail框架：1）多轮强化学习代理，与数据库环境交互并利用执行反馈迭代优化预测；2）自适应轮次预算分配机制，根据问题难度调整交互深度；3）复合奖励面板，联合激励SQL正确性和高效探索。

Result: 在多个基准测试中达到新的SOTA，数据效率比之前单次RL方法高18倍。7B和14B模型平均超越大5%的专有系统，证明交互式代理工作流的有效性。

Conclusion: 交互式、代理化的工作流程对于稳健的Text-to-SQL生成非常有效，多轮RL方法能够显著提升模型性能，即使较小模型也能超越更大的专有系统。

Abstract: While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

</details>


### [52] [EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents](https://arxiv.org/abs/2601.17722)
*Ying Mo,Yu Bai,Dapeng Sun,Yuqian Shi,Yukai Miao,Li Chen,Dan Li*

Main category: cs.AI

TL;DR: EntWorld是一个大规模企业级基准测试，包含1,756个跨六个企业领域（CRM、ITIL、ERP等）的任务，采用基于数据库模式的自动化任务生成和SQL验证机制，揭示当前AI代理在企业环境中的能力差距（GPT-4.1成功率仅47.61%）


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要针对消费级场景（如电商、旅游预订），无法捕捉专业企业工作流程的复杂性和严谨性。企业系统具有高密度用户界面、严格业务逻辑约束和精确状态一致性要求等独特挑战，当前通用代理在这些环境中表现不佳。

Method: 1) 提出EntWorld基准测试，包含六个企业领域的1,756个任务；2) 采用基于模式的任务生成框架，直接从底层数据库模式逆向工程业务逻辑，合成真实的长时程工作流程；3) 提出基于SQL的确定性验证机制，用严格的状态转换验证替代模糊的视觉匹配。

Result: 实验结果显示，最先进的模型（如GPT-4.1）在EntWorld上的成功率仅为47.61%，远低于人类表现，突显了当前代理能力在企业领域的显著差距，强调了开发领域特定代理的必要性。

Conclusion: EntWorld作为一个严谨的测试平台，揭示了当前AI代理在企业环境中的能力不足，为开发和评估下一代企业级数字代理提供了重要基础，推动了领域特定代理的发展。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.

</details>


### [53] [ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents](https://arxiv.org/abs/2601.17735)
*Kyungho Kim,Geon Lee,Juyeon Kim,Dongwon Choi,Shinhwan Kang,Kijung Shin*

Main category: cs.AI

TL;DR: ReFuGe是一个基于LLM代理的框架，用于从关系数据库中自动生成信息丰富的特征，以提升预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 关系数据库在现实Web应用中至关重要，但针对RDB的预测任务面临挑战：需要处理复杂模式、探索组合爆炸的特征空间，且缺乏明确监督。

Method: 提出ReFuGe框架，使用三个专门的LLM代理：1)模式选择代理识别相关表和列；2)特征生成代理从选定模式生成多样化候选特征；3)特征过滤代理通过基于推理和验证的过滤评估并保留有前景的特征。这些代理在迭代反馈循环中运行，直到性能收敛。

Result: 在RDB基准测试上的实验表明，ReFuGe在各种RDB预测任务上显著提升了性能。

Conclusion: ReFuGe通过LLM代理的协作和迭代优化，有效解决了关系数据库特征生成的挑战，为RDB预测任务提供了强大的特征工程解决方案。

Abstract: Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.

</details>


### [54] [Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems](https://arxiv.org/abs/2601.17744)
*Amjad Fatmi*

Main category: cs.AI

TL;DR: Faramesh是一个协议无关的执行控制平面，通过不可绕过的行动授权边界强制执行时授权，为自主代理系统提供可执行、可预测的治理。


<details>
  <summary>Details</summary>
Motivation: 自主代理系统越来越多地触发现实世界的副作用（部署基础设施、修改数据库、移动资金、执行工作流），但大多数代理堆栈缺乏强制性的执行检查点，无法在行动改变现实之前确定性地允许、拒绝或推迟行动。

Method: 引入Faramesh协议无关的执行控制平面，通过不可绕过的行动授权边界强制执行时授权。系统将代理意图规范化为规范行动表示，根据策略和状态确定性地评估行动，并发出决策工件（允许/推迟/拒绝），执行器必须在执行前验证。

Result: Faramesh提供框架和模型无关的设计，支持多代理和多租户部署，保持与传输协议独立。提供基于规范行动哈希的决策中心、仅追加的溯源日志，实现可审计性、验证和确定性重放。

Conclusion: 这些原语为自主执行提供了可执行、可预测的治理，避免了与编排层的隐藏耦合或仅观察性的方法。

Abstract: Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.

</details>


### [55] [Neuro-Symbolic Verification on Instruction Following of LLMs](https://arxiv.org/abs/2601.17789)
*Yiming Su,Kunzhao Xu,Yanjie Gao,Fan Yang,Cheng Li,Mao Yang,Tianyin Xu*

Main category: cs.AI

TL;DR: NSVIF是一个神经符号框架，用于验证LLM输出是否遵循指令，将指令遵循验证建模为约束满足问题，显著优于基于LLM的方法并提供可解释反馈。


<details>
  <summary>Details</summary>
Motivation: LLM并不总是遵循指令，且违规行为难以观察或检查。在基于LLM的代理工作流中，这些违规会沿着推理链传播和放大，导致任务失败和系统事故。

Method: NSVIF将指令遵循验证建模为约束满足问题，将用户指令建模为约束。它同时建模逻辑和语义约束，通过统一求解器协调逻辑推理和语义分析来解决约束。

Result: 实验表明NSVIF显著优于基于LLM的方法，并提供可解释的反馈。NSVIF的反馈有助于在不进行后训练的情况下提高LLM的指令遵循能力。

Conclusion: NSVIF是一个通用、通用的验证器，对指令或LLM不做任何假设，为解决LLM指令遵循问题提供了有效的神经符号框架。

Abstract: A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.

</details>


### [56] [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381)
*Yinghan Hou,Zongyou Yang*

Main category: cs.AI

TL;DR: 开发集成AI代理框架，通过多阶段工作流将传统有限差分代码转换为Devito环境，结合RAG、知识图谱和强化学习反馈机制


<details>
  <summary>Details</summary>
Motivation: 促进传统有限差分实现向Devito环境的转换，解决代码迁移中的复杂性和准确性挑战

Method: 采用混合LangGraph架构，结合RAG和开源大语言模型，构建Devito知识图谱，通过多阶段检索管道和反向工程组件实现代码转换

Result: 开发了完整的代理框架，支持并行搜索、概念扩展、社区规模检索和语义相似性分析，通过验证框架确保代码质量

Conclusion: 主要贡献在于整合了受强化学习启发的反馈机制，实现了从静态代码翻译到动态自适应分析行为的转变

Abstract: To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.

</details>


### [57] [Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards](https://arxiv.org/abs/2601.17828)
*Tanvi Verma,Yang Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.AI

TL;DR: 提出IGFT方法，通过信息增益奖励和在线强化学习训练医疗对话AI进行患者访谈，无需人工对话数据，在HPI生成任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗对话AI方法依赖昂贵的人工标注对话或静态数据集，需要开发无需预收集人类对话、能自主学习有效提问策略的方法。

Method: 结合在线GRPO强化学习和信息论奖励，使用信息增益奖励函数追踪临床实体（症状、时间模式、病史）的揭示情况，结合GPT-4o-mini质量评估，通过LoRA微调Llama-3.1-8B-Instruct和DeepSeek-R1-Distill-Qwen-7B模型。

Result: DeepSeek-R1-Distill-Qwen-7B (IGFT)在Avey数据上F1得分为0.408（比基础模型提升10.9%），在MIMIC数据上为0.289（提升12.9%）；Llama-3.1-8B-Instruct (IGFT)分别达到0.384和0.336，均优于OpenAI模型和医疗领域基线模型。

Conclusion: IGFT方法能有效训练医疗对话AI进行患者访谈和HPI生成，无需人工对话数据，在跨数据集泛化方面表现优异，优于现有医疗对话模型。

Abstract: We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.

</details>


### [58] [When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents](https://arxiv.org/abs/2601.17887)
*Jiahe Guo,Xiangran Guo,Yulin Hu,Zimo Long,Xingyu Sui,Xuda Zhi,Yongbo Huang,Hao He,Weixiang Zhao,Yanyan Zhao,Bing Qin*

Main category: cs.AI

TL;DR: 论文揭示个性化LLM代理中的"意图合法化"安全漏洞：良性的个人记忆会偏见意图推断，导致模型将有害查询合法化，攻击成功率提升15.8%-243.7%。


<details>
  <summary>Details</summary>
Motivation: 现有个性化代理研究主要关注实用性和用户体验，将记忆视为中性组件，忽视了其安全影响。本文旨在揭示"意图合法化"这一被忽视的安全漏洞，即良性个人记忆如何导致模型将有害查询合法化。

Method: 1) 提出PS-Bench基准，用于识别和量化个性化交互中的意图合法化；2) 在多个记忆增强代理框架和基础LLM上进行实验；3) 从内部表示空间提供机制性证据；4) 提出轻量级检测-反思方法。

Result: 个性化使攻击成功率相对于无状态基线提高15.8%-243.7%。从内部表示空间获得了意图合法化的机制性证据。提出的检测-反思方法有效减少了安全退化。

Conclusion: 这是对意图合法化作为安全故障模式的首次系统探索和评估，表明良性、现实世界的个性化会自然产生这种安全漏洞，强调了在长期个性化背景下评估安全性的重要性。

Abstract: Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.

</details>


### [59] [UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis](https://arxiv.org/abs/2601.17897)
*Jiayu Liu,Yinhe Long,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: UniCog是一个通过潜在心智空间分析LLM认知的统一框架，将密集模型激活编码为稀疏解耦的潜在维度，揭示了LLM认知的帕累托原则，并利用潜在信息改进推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法在解释LLM推理过程中认知能力如何参与方面存在局限，而研究表明LLM的认知过程与人类有根本差异，需要新的分析框架来理解LLM的认知机制。

Method: 提出UniCog框架，作为潜在变量模型，将密集模型激活编码为稀疏解耦的潜在维度，分析六个先进LLM（包括DeepSeek-V3.2和GPT-4o），并引入潜在信息候选优先级策略。

Result: 揭示了LLM认知的帕累托原则：共享推理核心与能力特定特征互补；发现推理失败常表现为潜在激活的异常强度；潜在信息策略在挑战性基准上将推理性能提升高达7.5%。

Conclusion: UniCog为LLM分析开辟了新范式，提供了基于认知的推理动态视角，通过潜在心智空间分析能够深入理解LLM认知机制并改进推理性能。

Abstract: A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.

</details>


### [60] [Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation](https://arxiv.org/abs/2601.17915)
*Saurabh Jha,Rohan Arora,Bhavya,Noah Zheutlin,Paulina Toro Isaza,Laura Shwartz,Yu Deng,Daby Sow,Ruchi Mahindru,Ruchir Puri*

Main category: cs.AI

TL;DR: EoG框架通过将调查任务建模为依赖图上的溯因推理，分离LLM的局部证据挖掘与控制器的图遍历管理，解决了ReAct代理在开放式调查中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在处理开放式调查任务时存在可靠性问题：ReAct代理的检索-总结-推理循环对探索顺序敏感，缺乏信念维护机制，且语义推理与控制职责耦合，导致结果不稳定且难以验证。

Method: 提出EoG框架，将调查任务形式化为依赖图上的溯因推理。LLM负责有界的局部证据挖掘和标注（原因vs症状），而确定性控制器管理图遍历、状态维护和信念传播，计算最小解释边界。

Result: 在ITBench诊断任务上，EoG相比ReAct基线在准确性和运行一致性方面均有提升，平均实体F1的Majority-at-k指标提高了7倍。

Conclusion: 通过分离推理与控制职责，EoG框架显著提升了LLM代理在开放式调查任务中的可靠性和稳定性，为处理具有隐藏依赖结构的大规模异构数据提供了有效解决方案。

Abstract: LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.

</details>


### [61] [Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges](https://arxiv.org/abs/2601.17920)
*Xuanzhou Chen,Audrey Wang,Stanley Yin,Hanyang Jiang,Dong Zhang*

Main category: cs.AI

TL;DR: 该论文综述了自主实验室（SDL）中的AI代理问题，将SDL自主性建模为代理环境交互问题，回顾了贝叶斯优化、主动学习、规划与强化学习等方法，提出了基于能力的分类体系，并总结了实际部署中的经验教训和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 自主实验室为AI代理在昂贵操作、噪声延迟反馈、严格约束和非平稳性等复杂条件下的应用提供了重要测试平台。论文旨在系统梳理SDL中的AI问题，建立统一的理论框架，促进该领域的发展。

Method: 将SDL自主性建模为代理环境交互问题，包含显式观测、动作、成本和约束。综述了贝叶斯优化、主动学习、规划与强化学习、工具使用代理等方法，强调可验证和溯源感知策略。提出了基于决策视野、不确定性建模、动作参数化等维度的分类体系。

Result: 建立了SDL的AI理论框架，提出了能力驱动的分类体系，合成了基准任务模板和评估指标（成本感知性能、漂移鲁棒性、约束违反行为、可重复性等），总结了实际部署的经验教训。

Conclusion: SDL为AI代理研究提供了重要测试平台，需要进一步发展多模态表示、校准不确定性、安全探索和共享基准基础设施等关键技术。论文为SDL领域的系统化研究和比较提供了理论框架和评估标准。

Abstract: Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.

</details>


### [62] [Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation](https://arxiv.org/abs/2601.17923)
*Ali Najar*

Main category: cs.AI

TL;DR: 论文提出了一种基于技能图的终身学习方法，在《黑暗之魂III》实时控制环境中，通过层次化课程训练可重用技能组件，实现选择性微调以适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 终身智能体需要在不从头训练或覆盖已学行为的情况下持续扩展能力，这在复杂的实时控制环境中尤为挑战。

Method: 将战斗表示为有向技能图，通过层次化课程训练五个可重用技能组件：相机控制、目标锁定、移动、闪避和治疗-攻击决策策略，每个技能针对特定职责优化。

Result: 技能分解提高了样本效率，当环境从第一阶段变为第二阶段时，仅需微调两个技能即可在有限交互预算内快速恢复性能。

Conclusion: 技能图课程与选择性微调相结合，为复杂实时环境中不断进化的持续学习智能体提供了实用路径。

Abstract: Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.

</details>


### [63] [LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting](https://arxiv.org/abs/2601.17942)
*Yu-Jie Yang,Hung-Fu Chang,Po-An Chen*

Main category: cs.AI

TL;DR: 提出了SSEV和ReCAPAgent-SQL两个Text-to-SQL框架，前者基于单代理自优化集成投票，后者采用多代理协作迭代优化，在多个基准测试中取得竞争性性能，特别在复杂企业数据库场景表现突出。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL技术虽然能降低数据分析门槛，但自然语言查询的歧义性、模式链接复杂性、SQL方言泛化限制以及领域特定理解需求等问题仍使准确SQL生成具有挑战性。需要开发更强大的框架来处理实际企业数据库的复杂性。

Method: 1. SSEV：基于PET-SQL的单代理自优化集成投票管道，无需真实数据，集成自优化与加权多数投票及其随机变体。2. ReCAPAgent-SQL：多代理协作框架，包含规划、外部知识检索、批判、动作生成、自优化、模式链接和结果验证等专门代理，通过代理协作实现SQL预测的迭代优化。

Result: SSEV在Spider 1.0-Dev达到85.5%执行准确率，Spider 1.0-Test达到86.4%，BIRD-Dev达到66.3%。ReCAPAgent-SQL在Spider 2.0-Lite前100个查询中达到31%执行准确率，在处理真实企业场景方面有显著改进。

Conclusion: 该工作促进了可扩展Text-to-SQL系统在实际环境中的部署，以更低成本和更高效率支持更好的数据驱动决策。多代理协作框架特别适合处理复杂企业数据库场景。

Abstract: Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.

</details>


### [64] [Sentipolis: Emotion-Aware Agents for Social Simulations](https://arxiv.org/abs/2601.18027)
*Chiyuan Fu,Lyuhao Chen,Yunze Xiao,Weihao Xuan,Carlos Busso,Mona Diab*

Main category: cs.AI

TL;DR: Sentipolis框架为LLM智能体提供情感状态管理，通过PAD情感表示、双速情感动态和情感-记忆耦合解决情感遗忘问题，提升社交模拟中的情感连续性和真实行为。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在社交模拟中常将情感视为瞬时线索，导致情感遗忘和长期连续性不足，需要更系统化的情感状态管理框架。

Method: 提出Sentipolis框架，包含：1) 连续的愉悦-唤醒-支配(PAD)情感表示；2) 双速情感动态机制；3) 情感与记忆耦合系统。

Result: 在数千次交互中，Sentipolis提升了情感基础行为、沟通能力和情感连续性。效果模型依赖：高容量模型可信度提升，小模型可能下降；情感意识可能轻微降低社会规范遵守度。

Conclusion: Sentipolis支持研究累积社交动态如联盟形成和关系渐变，网络诊断显示互惠、适度聚类和时间稳定的关系结构，反映了情感驱动行为与规则遵守的人类化张力。

Abstract: LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.

</details>


### [65] [EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization](https://arxiv.org/abs/2601.18067)
*Wei-Po Hsin,Ren-Hao Deng,Yao-Ting Hsieh,En-Ming Huang,Shih-Hao Hung*

Main category: cs.AI

TL;DR: EvolVE是一个针对Verilog硬件设计的自动化框架，通过分析多种进化策略，发现MCTS在功能正确性上表现最佳，而IGR在优化方面更优，结合结构化测试平台生成，在多个基准测试上达到SOTA，并在工业级问题上显著降低PPA指标。


<details>
  <summary>Details</summary>
Motivation: Verilog硬件设计流程劳动密集且需要大量领域专业知识。虽然大语言模型提供了自动化途径，但其有限的训练数据和顺序推理能力难以捕捉硬件系统的严格形式逻辑和并发特性。

Method: 提出EvolVE框架，分析多种进化策略在芯片设计任务上的表现，发现MCTS在最大化功能正确性方面表现优异，而IGR在优化方面更优。采用结构化测试平台生成(STG)加速进化过程。为解决复杂优化基准的缺乏，引入IC-RTL基准套件。

Result: EvolVE在VerilogEval v2上达到98.1%，在RTLLM v2上达到92%，成为新的SOTA。在工业级IC-RTL套件上，框架超越了竞赛参与者的参考实现，在Huffman Coding中将PPA乘积降低达66%，在所有问题的几何平均中降低17%。

Conclusion: EvolVE框架成功解决了Verilog硬件设计自动化的挑战，通过分析不同进化策略的优势，结合结构化测试生成，在功能正确性和优化方面都取得了显著成果，为工业级硬件设计提供了有效的自动化解决方案。

Abstract: Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.

</details>


### [66] [Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?](https://arxiv.org/abs/2601.18119)
*Jing Ye,Yiwen Duan,Yonghong Yu,Victor Ma,Yang Gao,Xing Chen*

Main category: cs.AI

TL;DR: OurBench是首个企业级SQL推理与调试基准，包含469个语法错误查询和516个语义错误查询，通过自动化注入真实错误构建，评估显示当前LLM在复杂SQL调试上表现不佳（最佳模型准确率仅36.46%）


<details>
  <summary>Details</summary>
Motivation: 企业数据工程中SQL至关重要，但即使是经验丰富的开发者和先进LLM也难以一次性生成完全正确的SQL代码，通常需要多次调试迭代。现有基准无法充分评估企业级SQL调试能力。

Method: 提出OurBench基准，包含两个创新：(1) 自动化构建流程，通过逆向工程在大规模SQL代码中系统注入真实错误，实现可扩展且多样化的基准生成；(2) 针对企业环境的免执行评估框架，提供快速、准确且资源高效的评估。

Result: OurBench包含469个语法错误查询（OurBenchSyn）和516个语义错误查询（OurBenchSem），查询高度复杂（平均超过140行，具有深且宽的抽象语法树）。评估近30个LLM显示显著性能差距：最佳模型Claude-4-Sonnet在OurBenchSyn上仅36.46%准确率，在OurBenchSem上32.17%，大多数模型低于20%。

Conclusion: 当前LLM在企业级SQL调试任务上表现有限，存在显著改进空间。研究探索了四种解决方案策略，识别了关键挑战，并为企业环境中LLM驱动的SQL调试指出了有前景的研究方向。

Abstract: SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.
  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.
  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.

</details>


### [67] [RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents](https://arxiv.org/abs/2601.18130)
*Jize Wang,Han Wu,Zhiyuan You,Yiming Song,Yijun Wang,Zifei Shan,Yining Li,Songyang Zhang,Xinyi Le,Cailian Chen,Xinping Guan,Dacheng Tao*

Main category: cs.AI

TL;DR: RouteMoA：一种带动态路由的高效多智能体混合框架，通过轻量级评分器预测性能筛选候选模型，再通过混合评估器精调分数，最后平衡性能、成本和延迟选择模型，相比传统MoA大幅降低成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Mixture-of-Agents（MoA）采用密集拓扑结构导致成本和延迟过高。现有方法使用LLM评估器筛选响应，但仍需所有模型先进行推理，无法有效降低成本。此外缺乏模型选择标准，面对大规模模型池时，完整推理成本高昂且可能超出上下文限制。

Method: 1. 轻量级评分器：根据查询预测粗略性能，筛选出高潜力候选子集，无需推理；2. 混合评估器：基于现有模型输出进行轻量级自评估和交叉评估，提供后验修正；3. 模型排名机制：平衡性能、成本和延迟选择最终模型。

Result: RouteMoA在不同任务和模型池规模下均优于传统MoA，在大规模模型池中降低成本89.8%，减少延迟63.6%。

Conclusion: RouteMoA通过动态路由机制有效解决了传统MoA框架的成本和延迟问题，实现了高效的多智能体协作，为大规模模型池应用提供了实用解决方案。

Abstract: Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.

</details>


### [68] [DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints](https://arxiv.org/abs/2601.18137)
*Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin*

Main category: cs.AI

TL;DR: DeepPlanning是一个针对实际长视野代理规划的挑战性基准，包含多日旅行规划和多产品购物任务，需要主动信息获取、局部约束推理和全局约束优化，现有前沿代理LLM在这些问题上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前代理评估虽然转向长视野任务，但大多数基准仍强调局部、步骤级推理，而非需要真正规划能力的全局约束优化（如时间和财务预算）。现有LLM规划基准未能充分体现现实场景中典型的活动信息收集和细粒度局部约束。

Method: 引入DeepPlanning基准，包含多日旅行规划和多产品购物任务，这些任务需要主动信息获取、局部约束推理和全局约束优化。对前沿代理LLM进行评估，分析错误模式。

Result: 评估显示，即使是前沿的代理LLM在这些问题上也表现不佳，突显了可靠的显式推理模式和并行工具使用对于实现更好的效果-效率权衡的重要性。

Conclusion: DeepPlanning基准揭示了当前代理LLM在长规划视野上的不足，错误分析指出了改进代理LLM的有前景方向。开源代码和数据以支持未来研究。

Abstract: While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.

</details>


### [69] [Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success](https://arxiv.org/abs/2601.18175)
*Daniel Russo*

Main category: cs.AI

TL;DR: 论文证明成功条件化（success conditioning）精确解决了一个信任域优化问题，最大化策略改进同时满足χ²散度约束，约束半径由数据自动确定。


<details>
  <summary>Details</summary>
Motivation: 成功条件化（如拒绝采样、目标条件RL、决策变换器）被广泛用于改进策略，但其解决的优化问题本质一直不明确。论文旨在揭示这一技术背后的数学原理和优化基础。

Method: 通过理论分析证明成功条件化精确解决了一个信任域优化问题，建立了相对策略改进、策略变化幅度和动作影响之间的恒等式。应用该理论分析常见的回报阈值化实践。

Result: 成功条件化被证明是一个保守的改进算子，不会降低性能或引发危险的分布偏移。当失败时，它会通过几乎不改变策略来可观察地失败。回报阈值化可以放大改进，但可能偏离真实目标。

Conclusion: 成功条件化提供了一个理论上有保证的策略改进方法，揭示了其作为保守改进算子的本质，为理解这一广泛使用的技术提供了数学基础。

Abstract: A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.

</details>


### [70] [GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models](https://arxiv.org/abs/2601.18197)
*Shaokang Wang,Pei Fu,Ruoceng Zhang,Shaojie Zhang,Xiuwen Xi,Jiahui Yang,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.AI

TL;DR: GAIA框架通过训练直觉批评模型来提升GUI代理的测试时性能，通过数据飞轮系统实现自我改进循环


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然提升了GUI代理的能力，但操作不可逆性导致单个错误动作可能引发灾难性偏差，需要解决这一问题

Method: 提出GUI动作批评者的数据飞轮系统(GAIA)，训练直觉批评模型(ICM)评估代理动作的正确性，通过收集精炼的正负样本启动自我改进循环

Result: 实验表明ICM能提升各种闭源和开源模型的测试时性能，随着数据循环性能逐步改善

Conclusion: GAIA框架通过批评模型和数据飞轮系统有效解决了GUI代理操作的不可逆性问题，实现了性能的持续改进

Abstract: While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.

</details>


### [71] [SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback](https://arxiv.org/abs/2601.18202)
*Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: SAGE是一个自动生成高质量、难度可控的深度搜索问答对的代理管道，通过数据生成器和搜索代理的迭代交互来优化数据质量，显著提升深度搜索代理的性能。


<details>
  <summary>Details</summary>
Motivation: 深度搜索代理需要跨多个文档进行复杂推理，但人工标注此类数据成本过高，因为探索轨迹长且复杂，需要自动化的高质量数据生成方法。

Method: 提出SAGE管道，包含数据生成器（提出QA对）和搜索代理（尝试解决问题并提供执行反馈），两者通过多轮迭代交互，不断精炼问答对直至达到目标难度水平。

Result: 内在评估显示SAGE生成的问题需要多样化的推理策略，同时显著提高生成数据的正确性和难度；外在评估显示在流行深度搜索基准上获得高达23%的相对性能提升，且训练后的代理能够适应从固定语料库检索到Google搜索的推理时切换。

Conclusion: SAGE能够自动生成高质量、难度可控的深度搜索训练数据，显著提升深度搜索代理的性能，并展示了良好的适应性。

Abstract: Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.

</details>


### [72] [Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents](https://arxiv.org/abs/2601.18217)
*Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang*

Main category: cs.AI

TL;DR: 研究LLM智能体在未知测试领域中的泛化能力，发现状态信息丰富度和规划复杂度是影响跨域泛化的关键因素，并提出通过添加无关特征增强状态信息丰富度的方法。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体通常在有限环境中进行后训练，但需要在更广泛的未知领域中部署。本研究旨在探索当最终测试领域未知时，哪些环境属性和建模选择对跨域性能影响最大。

Method: 1) 识别影响跨域泛化的环境轴：状态信息丰富度和规划复杂度；2) 提出随机化技术：在状态中添加少量与目标无关的干扰特征以增强状态信息丰富度；3) 分析建模选择：SFT预热/中期训练的影响，以及逐步思考在RL中的作用。

Result: 发现状态信息丰富度和规划复杂度与跨域泛化强相关，而领域真实性和文本相似性不是主要因素。增加状态信息丰富度能有效提升跨域鲁棒性。SFT训练有助于防止灾难性遗忘但会损害未包含领域的泛化能力，逐步思考对保持泛化能力至关重要。

Conclusion: 在智能体后训练中，应优先考虑状态信息丰富度和规划复杂度而非领域真实性。通过添加无关特征增强状态信息丰富度是提升跨域泛化的有效方法，同时需要谨慎使用SFT训练并启用逐步思考机制。

Abstract: Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.

</details>


### [73] [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225)
*Pei Wang,Yanan Wu,Xiaoshuai Song,Weixun Wang,Gengru Chen,Zhongwen Li,Kezhong Yan,Ken Deng,Qi Liu,Shuaibing Zhao,Shaopan Xiong,Xuepeng Liu,Xuefeng Chen,Wanxi Deng,Wenbo Su,Bo Zheng*

Main category: cs.AI

TL;DR: ShopSimulator是一个大规模中文电商购物仿真环境，用于评估和训练LLM智能体在个性化搜索、多轮对话和产品辨别等复杂购物任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏统一的仿真环境来全面评估LLM智能体在电商购物中的综合能力，包括个性化偏好理解、多轮对话和相似产品辨别，且现有工作多集中于评估而非训练支持。

Method: 提出ShopSimulator大规模中文购物环境，包含多样化场景。通过该环境评估LLM表现，并进行错误分析。进一步探索监督微调(SFT)和强化学习(RL)结合的训练方法。

Result: 即使最佳模型的全成功率也低于40%。错误分析显示智能体在长轨迹中的深度搜索和产品选择、个性化线索平衡以及与用户有效互动方面存在困难。SFT+RL组合训练显著提升了性能。

Conclusion: ShopSimulator为电商购物智能体提供了全面评估和训练环境，揭示了当前LLM智能体的局限性，并展示了通过SFT和RL结合训练可以有效提升性能。

Abstract: Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.

</details>


### [74] [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226)
*Haotian Li,Shijun Yang,Weizhen Qi,Silei Zhao,Rui Hua,Mingzhu Song,Xiaojian Yang,Chao Peng*

Main category: cs.AI

TL;DR: 提出In-Situ Self-Evolving范式，让智能体在开放环境中通过任务交互自主进化工具集，无需真实标签监督，实现能力边界动态扩展。


<details>
  <summary>Details</summary>
Motivation: 传统智能体系统在开放环境中面临任务分布持续漂移和外部监督稀缺的挑战，依赖静态工具集或离线训练导致能力边界僵化且未知。

Method: 提出原位自进化范式，将顺序任务交互视为连续经验流，通过工具进化作为能力扩展关键路径，开发Yunjue Agent系统迭代合成、优化和重用工具，并引入并行批量进化策略提升效率。

Result: 在五个多样化基准测试的零起点设置下显著超越专有基线，补充的暖启动评估证实积累的通用知识可无缝迁移到新领域，并提出监测进化收敛的新指标。

Conclusion: In-Situ Self-Evolving范式使智能体能在开放环境中自主扩展能力边界，工具进化提供可验证的反馈信号，为构建弹性自进化智能体开辟新方向。

Abstract: Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.

</details>


### [75] [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282)
*Lei Wei,Jinpeng Ou,Xiao Peng,Bin Wang*

Main category: cs.AI

TL;DR: 提出TAFC框架，通过函数和参数级别的显式推理增强LLM函数调用准确性，无需修改模型架构


<details>
  <summary>Details</summary>
Motivation: 当前LLM在函数调用中缺乏参数生成的显式推理透明度，特别是对于具有相互依赖参数的复杂函数。现有方法如思维链提示在代理级别操作，无法为单个函数参数提供细粒度的推理指导。

Method: 提出Think-Augmented Function Calling (TAFC)框架：1) 引入通用的"think"参数增强，使模型能够表达决策过程；2) 对参数描述进行动态优化以提高推理质量；3) 基于复杂度评分自动触发细粒度推理；4) 提出推理引导优化以对齐人类期望。

Result: 在ToolBench上对专有和开源模型的评估显示，TAFC在多参数函数的参数生成准确性和推理连贯性方面有显著改进，同时为调试AI代理行为提供了更好的可解释性。

Conclusion: TAFC通过函数和参数级别的显式推理提高了LLM函数调用的准确性和可解释性，无需修改现有模型架构，保持完整的API兼容性。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal "think" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.

</details>


### [76] [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467)
*Yuhang Zhou,Kai Zheng,Qiguang Chen,Mengkang Hu,Qingfeng Sun,Can Xu,Jingjing Chen*

Main category: cs.AI

TL;DR: 本文提出了一种完全离线的研究智能体训练方法，通过开源套件DeepForge生成大规模研究查询，并训练出8B参数的OffSeeker模型，在多个基准测试中表现优异，甚至能与使用昂贵在线强化学习的30B参数系统竞争。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究智能体在处理长时程任务方面表现出色，但最佳性能通常依赖于昂贵的在线强化学习（需要大量API调用）。离线训练虽然更高效，但受限于高质量研究轨迹的稀缺性。本文旨在证明无需昂贵的在线强化学习也能构建强大的研究智能体。

Method: 1) 引入完全开源套件，包括DeepForge任务合成框架，无需繁重预处理即可生成大规模研究查询；2) 构建包含66k QA对、33k SFT轨迹和21k DPO对的数据集；3) 利用这些资源完全离线训练8B参数的OffSeeker模型。

Result: 在六个基准测试上的广泛评估表明，OffSeeker不仅在同等规模智能体中领先，而且与通过大量在线强化学习训练的30B参数系统保持竞争力。

Conclusion: 研究表明，无需依赖昂贵的在线强化学习也能构建强大的研究智能体。通过开源工具和高质量数据集，离线训练方法能够产生与更大规模在线训练系统相竞争的性能。

Abstract: Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.

</details>


### [77] [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491)
*Dongrui Liu,Qihan Ren,Chen Qian,Shuai Shao,Yuejin Xie,Yu Li,Zhonghao Yang,Haoyu Luo,Peng Wang,Qingyu Liu,Binxin Hu,Ling Tang,Jilin Mei,Dadi Guo,Leitao Yuan,Junyao Yang,Guanxu Chen,Qihao Lin,Yi Yu,Bo Zhang,Jiaxuan Guo,Jie Zhang,Wenqi Shao,Huiqi Deng,Zhiheng Xi,Wenjie Wang,Wenxuan Wang,Wen Shen,Zhikai Chen,Haoyu Xie,Jialing Tao,Juntao Dai,Jiaming Ji,Zhongjie Ba,Linfeng Zhang,Yong Liu,Quanshi Zhang,Lei Zhu,Zhihua Wei,Hui Xue,Chaochao Lu,Jing Shao,Xia Hu*

Main category: cs.AI

TL;DR: 提出AgentDoG框架，通过三维风险分类法构建细粒度智能体安全基准，实现智能体不安全行为的诊断与监控，超越传统二元标签提供可追溯的透明度。


<details>
  <summary>Details</summary>
Motivation: 当前护栏模型缺乏对智能体风险的认知和风险诊断的透明度，无法应对自主工具使用和环境交互带来的复杂安全挑战。

Method: 提出统一的三维风险分类法（来源、失效模式、后果），基于此构建细粒度智能体安全基准ATBench，开发诊断性护栏框架AgentDoG，提供跨智能体轨迹的细粒度上下文监控和根因诊断。

Result: AgentDoG在多样复杂交互场景中实现了最先进的智能体安全调节性能，提供Qwen和Llama模型家族的4B、7B、8B参数版本，所有模型和数据集均已开源。

Conclusion: AgentDoG通过结构化风险分类和诊断性监控，为智能体安全提供了超越二元标签的透明度和可追溯性，有效促进智能体对齐。

Abstract: The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.

</details>


### [78] [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642)
*Lei Wei,Xu Dong,Xiao Peng,Niantao Xie,Bin Wang*

Main category: cs.AI

TL;DR: FadeMem是一种受生物学启发的智能体记忆架构，通过引入主动遗忘机制来平衡记忆保留与遗忘，解决了当前AI系统在记忆管理中的信息过载和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自主智能体面临严重的记忆限制问题，缺乏选择性遗忘机制，导致在上下文边界处出现灾难性遗忘或在边界内信息过载。人类记忆通过自适应衰减过程自然平衡保留与遗忘，而现有AI系统采用二元保留策略（要么全部保留，要么全部丢失）。

Method: FadeMem采用受生物学启发的双层级记忆架构，通过自适应指数衰减函数实现差异化的衰减率，衰减过程受语义相关性、访问频率和时间模式调制。系统通过LLM引导的冲突解决和智能记忆融合来整合相关信息，同时允许无关细节逐渐淡出。

Result: 在Multi-Session Chat、LoCoMo和LTI-Bench上的实验表明，FadeMem在多跳推理和检索方面表现优异，同时实现了45%的存储减少，验证了生物学启发式遗忘在智能体记忆系统中的有效性。

Conclusion: FadeMem通过引入受人类认知启发的主动遗忘机制，有效解决了智能体记忆管理中的关键挑战，在保持推理能力的同时显著减少了存储需求，为构建更高效的自主智能体系统提供了新思路。

Abstract: Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

</details>


### [79] [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744)
*Fangxu Yu,Xingang Guo,Lingzhi Yuan,Haoqiang Kang,Hongyu Zhao,Lianhui Qin,Furong Huang,Bin Hu,Tianyi Zhou*

Main category: cs.AI

TL;DR: TSRBench是一个全面的多模态时间序列推理基准测试，包含4125个问题、14个领域和4个主要维度，用于评估通用模型的时间序列推理能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在现实世界场景中无处不在且至关重要，但现有通用模型基准测试中缺乏时间序列推理维度，需要填补这一空白。

Method: 构建TSRBench基准测试，包含4125个问题，涵盖14个领域，分为感知、推理、预测和决策4个主要维度，包含15个任务，评估了30多个领先的专有和开源LLM、VLM和TSLLM。

Result: 研究发现：1）缩放定律适用于感知和推理但在预测中失效；2）强大的推理能力不能保证准确的上下文感知预测，表明语义理解和数值预测之间存在脱节；3）当前多模态模型未能有效融合文本和视觉表示以获得互补性能增益。

Conclusion: TSRBench提供了一个标准化评估平台，不仅突出了现有挑战，还为推进通用模型发展提供了有价值的见解。

Abstract: Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [80] [Fingerprinting AI Coding Agents on GitHub](https://arxiv.org/abs/2601.17406)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 该研究首次对AI编程代理进行指纹识别，通过分析33,580个PR，使用41个特征实现了97.2%的F1分数来识别不同AI代理，揭示了各代理的独特行为模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程代理在软件开发中的广泛应用，当开发者使用AI代理生成代码时，代码作者归属变得至关重要，这关系到仓库治理、研究有效性以及对现代开发实践的理解。

Method: 分析了来自五个主要AI代理（OpenAI Codex、GitHub Copilot、Devin、Cursor、Claude Code）的33,580个PR，提取了41个特征，涵盖提交信息、PR结构和代码特性，用于识别AI代理的行为特征。

Result: 实现了97.2%的F1分数在多类别代理识别中，发现了各代理的独特指纹：Codex显示独特的多行提交模式（67.5%特征重要性），Claude Code表现出独特的代码结构（条件语句占27.2%重要性）。

Conclusion: AI编程工具会产生可检测的行为模式，这表明在软件仓库中识别AI贡献具有潜在可能性，为理解AI在软件开发中的角色提供了新视角。

Abstract: AI coding agents are reshaping software development through both autonomous and human-mediated pull requests (PRs). When developers use AI agents to generate code under their own accounts, code authorship attribution becomes critical for repository governance, research validity, and understanding modern development practices. We present the first study on fingerprinting AI coding agents, analyzing 33,580 PRs from five major agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, Claude Code) to identify behavioral signatures. With 41 features spanning commit messages, PR structure, and code characteristics, we achieve 97.2% F1-score in multi-class agent identification. We uncover distinct fingerprints: Codex shows unique multiline commit patterns (67.5% feature importance), and Claude Code exhibits distinctive code structure (27.2% importance of conditional statements). These signatures reveal that AI coding tools produce detectable behavioral patterns, suggesting potential for identifying AI contributions in software repositories.

</details>


### [81] [When AI Agents Touch CI/CD Configurations: Frequency and Success](https://arxiv.org/abs/2601.17413)
*Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究分析了AI代理在GitHub仓库中对CI/CD配置文件的修改行为，发现CI/CD配置仅占代理修改的3.25%，主要集中在GitHub Actions，且修改的可靠性与传统代码相当，Copilot在CI/CD修改方面表现突出。


<details>
  <summary>Details</summary>
Motivation: AI代理在软件开发中的应用日益增多，但它们与CI/CD配置的交互尚未得到充分研究。需要了解AI代理如何修改CI/CD配置文件，以及这些修改的质量和影响。

Method: 分析了8,031个来自1,605个GitHub仓库的AI代理PR，其中涉及YAML配置文件的修改。研究了不同代理（Devin、Codex、Copilot等）对CI/CD配置的修改模式，并评估了99,930个工作流运行的成功率。

Result: CI/CD配置文件占AI代理修改的3.25%，不同代理间有显著差异（Devin: 4.83%, Codex: 2.01%）。96.77%的CI/CD修改针对GitHub Actions。包含CI/CD修改的PR合并率略低（67.77% vs 71.80%），但Copilot的CI/CD修改合并率高出15.63个百分点。工作流运行成功率在CI/CD和非CI/CD修改间相当（75.59% vs 74.87%），三个代理在修改CI/CD时成功率显著更高。

Conclusion: AI代理很少修改CI/CD配置，主要关注GitHub Actions，但其配置修改与常规代码一样可靠。Copilot在CI/CD方面的强劲表现表明配置专业化趋势，这对代理训练和DevOps自动化有重要启示。

Abstract: AI agents are increasingly used in software development, yet their interaction with CI/CD configurations is not well studied. We analyze 8,031 agentic pull requests (PRs) from 1,605 GitHub repositories where AI agents touch YAML configurations. CI/CD configuration files account for 3.25% of agent changes, varying by agent (Devin: 4.83%, Codex: 2.01%, p < 0.001). When agents modify CI/CD, 96.77% target GitHub Actions. Agentic PRs with CI/CD changes merge slightly less often than others (67.77% vs. 71.80%), except for Copilot, whose CI/CD changes merge 15.63 percentage points more often. Across 99,930 workflow runs, build success rates are comparable for CI/CD and non-CI/CD changes (75.59% vs. 74.87%), though three agents show significantly higher success when modifying CI/CD. These results show that AI agents rarely modify CI/CD and focus mostly on GitHub Actions, yet their configuration changes are as reliable as regular code. Copilot's strong CI/CD performance despite lower acceptance suggests emerging configuration specialization, with implications for agent training and DevOps automation.

</details>


### [82] [Towards a Declarative Agentic Layer for Intelligent Agents in MCP-Based Server Ecosystems](https://arxiv.org/abs/2601.17435)
*Maria Jesus Rodriguez-Sanchez,Manuel Noguera,Angel Ruiz-Zafra,Kawtar Benghazi*

Main category: cs.SE

TL;DR: 提出DALIA（声明式智能体层）架构，通过声明式能力定义、任务发现协议和确定性任务图，解决现有智能体系统可靠性问题，实现可验证的工作流。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统存在可靠性问题，如幻觉动作、不可执行计划和脆弱协调。这些问题并非源于底层模型限制，而是缺乏明确连接目标、能力和执行的架构结构。

Method: 提出DALIA声明式架构层，包含：形式化可执行能力、通过声明式发现协议暴露任务、维护代理及其执行资源的联邦目录、构建基于声明操作的确定性任务图。强制分离发现、规划和执行阶段。

Result: 通过代表性任务场景展示了DALIA的操作，证明声明式基础能够实现跨异构环境的可重现和可验证智能体工作流，将代理行为约束在可验证操作空间。

Conclusion: DALIA架构通过声明式基础解决了智能体系统可靠性问题，减少对推测推理和自由形式协调的依赖，为可验证智能体工作流提供了模型无关的解决方案。

Abstract: Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution. This paper presents a declarative, model-independent architectural layer for grounded agentic workflows that addresses this gap. The proposed layer, referred to as DALIA (Declarative Agentic Layer for Intelligent Agents), formalises executable capabilities, exposes tasks through a declarative discovery protocol, maintains a federated directory of agents and their execution resources, and constructs deterministic task graphs grounded exclusively in declared operations. By enforcing a clear separation between discovery, planning and execution, the architecture constrains agent behaviour to a verifiable operational space, reducing reliance on speculative reasoning and free-form coordination. We present the architecture and design principles of the proposed layer and illustrate its operation through a representative task-oriented scenario, demonstrating how declarative grounding enables reproducible and verifiable agentic workflows across heterogeneous environments.

</details>


### [83] [How AI Coding Agents Modify Code: A Large-Scale Study of GitHub Pull Requests](https://arxiv.org/abs/2601.17581)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: AI编码代理生成的PR在提交数量、文件修改和删除行数上与人类PR存在显著差异，且PR描述与代码变更的语义一致性略高


<details>
  <summary>Details</summary>
Motivation: 缺乏关于AI编码代理生成的PR与人类贡献差异的实证证据，需要了解这些差异以评估其可靠性和对开发工作流程的影响

Method: 使用MSR 2026 Mining Challenge版本的AIDev数据集，分析24,014个合并的Agentic PR（440,295次提交）和5,081个合并的Human PR（23,242次提交），通过词法和语义相似度评估PR描述与代码变更的一致性

Result: Agentic PR在提交数量上与Human PR差异显著（Cliff's δ=0.5429），在文件修改和删除行数上存在中等差异，在所有度量指标上PR描述与代码变更的相似度略高

Conclusion: 研究提供了AI编码代理如何贡献开源开发的大规模实证特征，揭示了AI代理与人类在代码贡献模式上的系统性差异

Abstract: AI coding agents are increasingly acting as autonomous contributors by generating and submitting pull requests (PRs). However, we lack empirical evidence on how these agent-generated PRs differ from human contributions, particularly in how they modify code and describe their changes. Understanding these differences is essential for assessing their reliability and impact on development workflows. Using the MSR 2026 Mining Challenge version of the AIDev dataset, we analyze 24,014 merged Agentic PRs (440,295 commits) and 5,081 merged Human PRs (23,242 commits). We examine additions, deletions, commits, and files touched, and evaluate the consistency between PR descriptions and their diffs using lexical and semantic similarity. Agentic PRs differ substantially from Human PRs in commit count (Cliff's $δ= 0.5429$) and show moderate differences in files touched and deleted lines. They also exhibit slightly higher description-to-diff similarity across all measures. These findings provide a large-scale empirical characterization of how AI coding agents contribute to open source development.

</details>


### [84] [Prompt Driven Development with Claude Code: Building a Complete TUI Framework for the Ring Programming Language](https://arxiv.org/abs/2601.17584)
*Mahmoud Samir Fayed,Ahmed Samir Fayed*

Main category: cs.SE

TL;DR: 本研究通过实证分析展示了使用Claude Code Opus 4.5纯提示驱动工作流，在约10小时内开发7420行Ring编程语言终端用户界面框架的能力，证明了现代LLM能够维持架构一致性并构建生产级工具。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件开发中的应用日益增多，但其通过自然语言交互生成和维护大型多模块系统的能力尚未得到充分表征。本研究旨在实证评估LLM在构建复杂软件系统方面的实际表现。

Method: 采用纯提示驱动工作流，使用Claude Code Opus 4.5在三天内通过107个提示开发7420行Terminal User Interface框架。分析包括定量提示分类（21个功能请求、72个错误修复、9个文档信息分享、4个架构指导、1个文档生成）和五个开发阶段的定性评估。

Result: 成功开发出包含完整窗口子系统、事件驱动架构、交互式小部件、分层菜单、网格和树组件、标签控件以及多窗口桌面环境的框架。错误修复主要涉及重绘问题、事件处理故障、运行时错误和布局不一致；功能请求主要关注新小部件、窗口管理器功能和高级UI组件。

Conclusion: 现代LLM能够维持架构一致性并支持为新兴编程语言构建生产级工具，提示驱动开发是软件工程实践中可行的方法论。人类角色仅限于指定需求、验证行为和发出纠正提示，无需手动编写代码。

Abstract: Large language models are increasingly used in software development, yet their ability to generate and maintain large, multi module systems through natural language interaction remains insufficiently characterized. This study presents an empirical analysis of developing a 7420 line Terminal User Interface framework for the Ring programming language, completed in roughly ten hours of active work spread across three days using a purely prompt driven workflow with Claude Code, Opus 4.5. The system was produced through 107 prompts: 21 feature requests, 72 bug fix prompts, 9 prompts sharing information from Ring documentation, 4 prompts providing architectural guidance, and 1 prompt dedicated to generating documentation. Development progressed across five phases, with the Window Manager phase requiring the most interaction, followed by complex UI systems and controls expansion. Bug related prompts covered redraw issues, event handling faults, runtime errors, and layout inconsistencies, while feature requests focused primarily on new widgets, window manager capabilities, and advanced UI components. Most prompts were short, reflecting a highly iterative workflow in which the human role was limited to specifying requirements, validating behaviour, and issuing corrective prompts without writing any code manually. The resulting framework includes a complete windowing subsystem, event driven architecture, interactive widgets, hierarchical menus, grid and tree components, tab controls, and a multi window desktop environment. By combining quantitative prompt analysis with qualitative assessment of model behaviour, this study provides empirical evidence that modern LLMs can sustain architectural coherence and support the construction of production grade tooling for emerging programming languages, highlighting prompt driven development as a viable methodology within software engineering practice.

</details>


### [85] [Human-Aligned Enhancement of Programming Answers with LLMs Guided by User Feedback](https://arxiv.org/abs/2601.17604)
*Suborno Deb Bappon,Saikat Mondal,Chanchal K. Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 该研究探索LLMs如何利用Stack Overflow上的用户评论反馈来改进编程答案，提出了ReSOlve基准和AUTOCOMBAT工具，能有效提升答案质量并得到开发者认可。


<details>
  <summary>Details</summary>
Motivation: Stack Overflow等平台上约三分之一的用户反馈未被处理，导致答案不完整或过时。研究探索LLMs是否能通过解释和整合评论反馈来改进编程答案，提高技术知识平台的可靠性和可信度。

Method: 1) 创建ReSOlve基准，包含790个SO答案及相关评论线程，标注改进相关和一般反馈；2) 评估4个SOTA LLMs识别可操作问题的能力；3) 开发AUTOCOMBAT工具，联合利用用户评论和问题上下文改进编程答案；4) 进行58名从业者的用户研究。

Result: DeepSeek在识别可操作问题方面达到最佳精确率-召回率平衡；AUTOCOMBAT产生接近人类质量的改进，保持原始意图并显著优于基线；84.5%的从业者表示会采用或推荐该工具。

Conclusion: AUTOCOMBAT展示了可扩展的反馈驱动答案精炼在提高技术知识平台可靠性和可信度方面的潜力，LLMs能有效利用用户反馈改进编程答案。

Abstract: Large Language Models (LLMs) are widely used to support software developers in tasks such as code generation, optimization, and documentation. However, their ability to improve existing programming answers in a human-like manner remains underexplored. On technical question-and-answer platforms such as Stack Overflow (SO), contributors often revise answers based on user comments that identify errors, inefficiencies, or missing explanations. Yet roughly one-third of this feedback is never addressed due to limited time, expertise, or visibility, leaving many answers incomplete or outdated. This study investigates whether LLMs can enhance programming answers by interpreting and incorporating comment-based feedback. We make four main contributions. First, we introduce ReSOlve, a benchmark consisting of 790 SO answers with associated comment threads, annotated for improvement-related and general feedback. Second, we evaluate four state-of-the-art LLMs on their ability to identify actionable concerns, finding that DeepSeek achieves the best balance between precision and recall. Third, we present AUTOCOMBAT, an LLM-powered tool that improves programming answers by jointly leveraging user comments and question context. Compared to human revised references, AUTOCOMBAT produces near-human quality improvements while preserving the original intent and significantly outperforming the baseline. Finally, a user study with 58 practitioners shows strong practical value, with 84.5 percent indicating they would adopt or recommend the tool. Overall, AUTOCOMBAT demonstrates the potential of scalable, feedback-driven answer refinement to improve the reliability and trustworthiness of technical knowledge platforms.

</details>


### [86] [Multi-Agent End-to-End Vulnerability Management for Mitigating Recurring Vulnerabilities](https://arxiv.org/abs/2601.17762)
*Zelong Zheng,Jiayuan Zhou,Xing Hu,Yi Gao,Shengyi Pan*

Main category: cs.SE

TL;DR: MAVM是一个多智能体框架，用于端到端的重复漏洞管理，通过整合漏洞知识库、检测、确认、修复和验证组件，有效利用历史漏洞知识解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统规模扩大和复杂性增加使得漏洞管理变得至关重要。传统静态分析方法难以精确捕捉跨函数/模块的上下文依赖，大语言模型缺乏足够的上下文信息检索和利用能力，而重复漏洞因代码重用和共享逻辑反复出现，现有方法未能充分利用历史漏洞知识。

Method: 提出MAVM多智能体框架，包含五个组件：漏洞知识库、检测、确认、修复和验证，构建统一的多智能体流水线。从公开披露的漏洞构建知识库，设计上下文检索工具使智能体能够提取和推理仓库级信息，模拟真实世界安全工作流程。

Result: 在包含78个真实世界补丁移植案例（覆盖114个函数级迁移）的数据集上，MAVM成功检测并修复了51个真实漏洞，修复准确率比基线方法高出31.9%-45.2%，证明了其有效性。

Conclusion: MAVM通过多智能体框架有效利用历史漏洞知识，克服了传统方法的上下文限制，在重复漏洞管理方面表现出色，为软件漏洞管理提供了新的解决方案。

Abstract: Software vulnerability management has become increasingly critical as modern systems scale in size and complexity. However, existing automated approaches remain insufficient. Traditional static analysis methods struggle to precisely capture contextual dependencies, especially when vulnerabilities span multiple functions or modules. Large language models (LLMs) often lack the ability to retrieve and exploit sufficient contextual information, resulting in incomplete reasoning and unreliable outcomes. Meanwhile, recurring vulnerabilities emerge repeatedly due to code reuse and shared logic, making historical vulnerability knowledge an indispensable foundation for effective vulnerability detection and repair. Nevertheless, prior approaches such as clone-based detection and patch porting, have not fully leveraged this knowledge. To address these challenges, we present MAVM, a multi-agent framework for end-to-end recurring vulnerability management. MAVM integrates five components, including a vulnerability knowledge base, detection, confirmation, repair, and validation, into a unified multi-agent pipeline. We construct a knowledge base from publicly disclosed vulnerabilities, thereby addressing the underuse of historical knowledge in prior work and mitigating the lack of domain-specific expertise in LLMs. Furthermore, we design context-retrieval tools that allow agents to extract and reason over repository-level information, overcoming the contextual limitations of previous methods. Based on agents, MAVM effectively simulates real-world security workflows. To evaluate the performance of MAVM, we construct a dataset containing 78 real-world patch-porting cases (covering 114 function-level migrations). On this dataset, MAVM successfully detects and repairs 51 real vulnerabilities, outperforming baselines by 31.9%-45.2% in repair accuracy, which demonstrates its effectiveness.

</details>


### [87] [RGFL: Reasoning Guided Fault Localization for Automated Program Repair Using Large Language Models](https://arxiv.org/abs/2601.18044)
*Melika Sepidband,Hamed Taherkhani,Hung Viet Pham,Hadi Hemmati*

Main category: cs.SE

TL;DR: 提出一种新颖的项目级故障定位方法，通过分层推理模块生成bug特定解释，结合LLM和嵌入信号的两阶段排序，显著提升文件级和元素级定位精度。


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的自动程序修复中，故障定位是关键步骤。现实项目代码库通常包含数百万token，远超当前LLM上下文限制，因此需要先识别小规模相关代码子集，准确的故障定位对有效修复至关重要。

Method: 提出分层推理模块：1)为候选文件和元素生成结构化的bug特定解释；2)在结合LLM和嵌入信号的两阶段排序方案中利用这些解释。还提出反事实上界分析来量化每个定位阶段对修复成功的贡献。

Result: 在SWE-bench Verified、Lite和Java的Python和Java项目上评估。相比Agentless和OpenHands等SOTA基线，方法持续提升定位精度：SWE-bench Verified上文件级Hit@1从71.4%提升至85%，MRR从81.8%提升至88.8%；元素级top-3文件下Exact Match从36%提升至69%。集成到Agentless中实现12.8%端到端修复成功率提升。

Conclusion: 提出的项目级故障定位方法通过分层推理和两阶段排序显著提升了文件级和元素级定位精度，对基于LLM的自动程序修复具有重要价值。

Abstract: Fault Localization (FL) is a critical step in Automated Program Repair (APR), and its importance has increased with the rise of Large Language Model (LLM)-based repair agents. In realistic project-level repair scenarios, software repositories often span millions of tokens, far exceeding current LLM context limits. Consequently, models must first identify a small, relevant subset of code, making accurate FL essential for effective repair. We present a novel project-level FL approach that improves both file- and element-level localization. Our method introduces a hierarchical reasoning module that (i) generates structured, bug-specific explanations for candidate files and elements, and (ii) leverages these explanations in a two-stage ranking scheme combining LLM-based and embedding-based signals. We further propose a counterfactual upper-bound analysis to quantify the contribution of each localization stage to repair success. We evaluate our approach on Python and Java projects from SWE-bench Verified, Lite, and Java. Compared to state-of-the-art baselines, including Agentless and OpenHands, our method consistently improves localization accuracy. On SWE-bench Verified, file-level Hit@1 improves from 71.4% to 85%, and MRR from 81.8% to 88.8%. At the element level, Exact Match under top-3 files increases from 36% to 69%. Integrating our localization into Agentless yields a 12.8% end-to-end repair success improvement.

</details>


### [88] [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241)
*Elena Bruches,Vadim Alperovich,Dari Baturova,Roman Derunets,Daniil Grebenkin,Georgy Mkrtchyan,Oleg Sedukhin,Mikhail Klementev,Ivan Bondarenko,Nikolay Bushkov,Stanislav Moiseev*

Main category: cs.SE

TL;DR: TAM-Eval是一个评估LLM在测试套件维护（创建、修复、更新）能力的框架和基准，包含1539个多语言场景，支持基于测试通过率、代码覆盖率和变异测试的无参考评估。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程中的应用主要局限于孤立的测试生成或预言预测，忽略了测试套件维护这一更广泛的挑战。需要评估LLM在实际测试维护工作流程中的能力。

Method: 提出TAM-Eval框架和基准，包含三个核心测试维护场景：创建、修复、更新测试套件。在测试文件级别操作，同时保持对完整仓库上下文的访问。从Python、Java和Go项目中自动提取和验证1539个场景。使用基于测试套件通过率、代码覆盖率和变异测试的无参考评估协议。

Result: 实证结果表明，最先进的LLM在实际测试维护过程中的能力有限，对测试有效性的提升微乎其微。

Conclusion: LLM在测试套件维护方面的能力仍有局限，TAM-Eval作为开源框架可支持未来自动化软件测试研究。

Abstract: While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.

</details>


### [89] [Agentic Much? Adoption of Coding Agents on GitHub](https://arxiv.org/abs/2601.18341)
*Romain Robbes,Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 对GitHub上129,134个项目的大规模研究表明，编码代理（如Cursor、Claude Code）在短短几个月内已达到15.85%-22.60%的高采用率，且采用范围广泛，跨越项目成熟度、组织和编程语言。编码代理辅助的提交比纯人工提交更大，且包含大量功能开发和错误修复。


<details>
  <summary>Details</summary>
Motivation: 编码代理作为新型开发工具，相比传统代码补全LLM具有更高自主性，能生成完整的拉取请求。这种新模式可能比代码补全LLM产生更大影响，且编码代理在软件工程制品中留下更明确的痕迹（如共同提交），因此研究其实际采用和影响至关重要。

Method: 利用编码代理在软件工程制品中留下的明确痕迹（如共同提交或拉取请求），对GitHub上129,134个项目进行首次大规模研究。通过分析这些痕迹来估计采用率，并对采用者进行深入分析，包括项目成熟度、组织背景、编程语言和项目主题。同时比较编码代理辅助提交与纯人工提交的差异。

Result: 编码代理的采用率估计为15.85%-22.60%，这对于仅存在数月的技术来说非常高且仍在增长。采用范围广泛：涵盖所有项目成熟度阶段；包括成熟组织；涉及多种编程语言和项目主题。编码代理辅助的提交比纯人工提交更大，且包含大量功能开发和错误修复。

Conclusion: 编码代理在短时间内已达到显著采用率，且采用范围广泛，表明这一技术正在快速改变软件开发实践。编码代理辅助的提交显示出不同的特征（更大规模、更多功能/错误修复），这凸显了需要进一步研究编码代理的实际使用情况和影响。

Abstract: In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.

</details>


### [90] [Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity](https://arxiv.org/abs/2601.18345)
*Romain Robes Théo Matricon,Thomas Degueule,Andre Hora,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 本文通过分析GitHub上的编码代理活动，研究了编码代理对软件开发实践的影响，总结了其前景、风险和实践经验。


<details>
  <summary>Details</summary>
Motivation: 编码代理在2025年迅速普及，它们与基于LLM的代码补全有显著不同，需要专门研究。编码代理在软件仓库中留下可见痕迹，使得可以使用MSR技术研究其对SE实践的影响。

Method: 通过分析GitHub上的编码代理活动，使用MSR（软件工程挖掘）技术来研究编码代理对软件开发实践的影响。

Result: 从编码代理活动中收集了关于其前景、风险和实践经验的具体发现和启发式规则。

Conclusion: 编码代理对软件开发实践产生了重要影响，研究其活动有助于理解其前景和风险，为实践提供指导。

Abstract: In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.

</details>


### [91] [daVinci-Dev: Agent-native Mid-training for Software Engineering](https://arxiv.org/abs/2601.18418)
*Ji Zeng,Dayuan Fu,Tiantian Mi,Yumin Zhuang,Yaxing Huang,Xuefeng Li,Lyumanshan Ye,Muhang Xie,Qishuo Hua,Zhen Huang,Mohan Jiang,Hanning Wang,Jifan Lin,Yang Xiao,Jie Sun,Yunze Wu,Pengfei Liu*

Main category: cs.SE

TL;DR: 本文提出了一种新的"代理式中期训练"方法，通过合成代理原生数据（包括上下文原生轨迹和环境原生轨迹）来训练代码代理，在SWE-Bench Verified基准上取得了56.1%-58.5%的解决率，使用的中期训练token数量比Kimi-Dev少一半以上。


<details>
  <summary>Details</summary>
Motivation: 当前代码代理主要依赖后训练方法，而代理式中期训练虽然能更可扩展地培养基础代理行为，但由于资源需求大且存在静态训练数据与动态开发环境之间的分布不匹配问题，一直未被充分探索。

Method: 提出代理原生数据监督方法，包括两种互补的轨迹类型：1) 上下文原生轨迹：保留代理经历的完整信息流，提供广泛覆盖和多样性；2) 环境原生轨迹：从可执行仓库收集，观测来自实际工具调用和测试执行，提供深度和交互真实性。

Result: 在SWE-Bench Verified基准上，32B和72B模型分别达到56.1%和58.5%的解决率。在相同基础模型和代理框架下，使用不到一半的中期训练token（73.1B）就超越了之前的Kimi-Dev方法。

Conclusion: 代理式中期训练通过合成代理原生数据可以有效解决分布不匹配问题，为大规模代理开发提供了可扩展的路径，相比依赖昂贵的强化学习方法更具优势。

Abstract: Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...

</details>


### [92] [Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests](https://arxiv.org/abs/2601.18749)
*Haruhiko Yoshioka,Takahiro Monno,Haruka Tokumasu,Taiki Wakamatsu,Yuki Ota,Nimmi Weeraddana,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 对40,214个PR进行大规模实证分析，发现AI生成的PR合并率低于人类PR，提交者属性对合并结果影响最大，而审查相关特征在人类和AI PR中呈现相反效果。


<details>
  <summary>Details</summary>
Motivation: 尽管AI生成的PR创建快速便捷，但其合并率低于人类创建的PR，需要研究影响PR合并的关键因素以改进AI PR质量。

Method: 使用AIDev数据集的40,214个PR，提取6个家族的64个特征，构建统计回归模型比较人类和AI PR的合并结果，并分析三个不同AI代理的差异。

Result: 提交者属性对两类PR的合并结果影响最大；审查相关特征在人类和AI PR中呈现相反效果；不同AI代理之间也存在差异。

Conclusion: 研究结果为通过人机协作改进PR质量提供了见解，强调了提交者属性和审查过程在AI生成PR中的重要性。

Abstract: The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.

</details>


<div id='tldr.article'></div>

# tldr.article [[Back]](#toc)

### [93] [We're turning Todos into Tasks in Claude Code](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTBuUb7/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/8yzasiTIDyngLT1IR4YOebkBBY0jz_Vcg2xinEAfIu8=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Anthropic将Claude Code中的Todos升级为Tasks，这是一个新的原语，帮助Claude Code跟踪和完成更复杂的项目，并支持跨多个会话或子代理的协作。


<details>
  <summary>Details</summary>
Motivation: 现有的Todos功能在处理复杂项目时存在局限性，需要更强大的项目管理能力来支持多会话协作和依赖关系管理。

Method: 将Todos升级为Tasks原语，支持任务间依赖关系，将任务存储在文件系统元数据中，允许多个子代理或会话协作处理。

Result: Claude Code现在能够更好地管理复杂项目，支持任务依赖关系，实现跨会话协作，提升了代码项目的管理能力。

Conclusion: Tasks原语的引入显著增强了Claude Code的项目管理能力，使其更适合处理复杂的多步骤开发任务。

Abstract: We're turning Todos into Tasks in Claude Code (2 minute read) Anthropic has upgraded Todos in Claude Code to Tasks, a new primitive that helps Claude Code track and complete more complicated projects and collaborate on them across multiple sessions or subagents. Claude can create Tasks with dependencies on each other that are stored in the metadata, mirroring how projects work. Tasks are stored in the file system so that multiple subagents or sessions can collaborate on them. Updates are broa...

</details>


### [94] [Building Deep Agent Frontends](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.copilotkit.ai%2Fblog%2Fhow-to-build-a-frontend-for-langchain-deep-agents-with-copilotkit%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/whSwGF1p0QZ9WMyg3xkGzWPcUqlkCmhyR4ewxAPFebc=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: CopilotKit发布了一个关于构建全栈Deep Agent应用的教程，包含简历解析、技能提取、带网络搜索的子代理和流式UI


<details>
  <summary>Details</summary>
Motivation: 为开发者提供构建深度智能代理应用的实用指南，展示如何将AI代理技术集成到实际应用中

Method: 通过CopilotKit框架构建全栈应用，包含简历数据解析、技能特征提取、多子代理架构（支持网络搜索）和流式用户界面

Result: 提供了一个完整的深度代理应用实现方案，展示了从数据处理到用户交互的完整工作流程

Conclusion: CopilotKit框架能够有效支持构建复杂的深度代理应用，为开发者提供了实用的工具和实现参考

Abstract: Building Deep Agent Frontends (38 minute read) CopilotKit published a tutorial on building a fullstack Deep Agent app that includes resume ingestion, skill extraction, sub-agents with web search, and a streaming UI.

</details>


### [95] [Salesforce Adopts Cursor at Scale](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fcursor.com%2Fblog%2Fsalesforce%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/q4l6QvjIhxWyk27GvcN8Ebkrsqg2UngOSZsdkVMi3vA=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Salesforce大规模采用Cursor AI编程助手，超过90%的2万名工程师在日常工作中使用，显著提升了开发速度和代码质量，并推动了Agentforce等产品发布


<details>
  <summary>Details</summary>
Motivation: 企业需要提升软件开发效率和代码质量，AI辅助编程工具能够帮助工程师更高效地工作，跟上行业技术发展趋势

Method: 在Salesforce公司范围内大规模部署和采用Cursor AI编程助手，让2万名工程师在日常工作流程中使用该工具

Result: 超过90%的工程师采用Cursor，显著提高了开发速度和代码质量，成功推动了Agentforce等产品发布，反映了AI辅助软件工程的行业趋势

Conclusion: AI编程助手在企业级规模部署是可行的，能带来显著的开发效率提升，代表了软件工程行业的重要发展方向

Abstract: Salesforce Adopts Cursor at Scale (3 minute read) Over 90% of Salesforce's 20,000 engineers now use Cursor in daily workflows, improving development speed and code quality. The shift has helped power product releases like Agentforce and signals broader industry trends in AI-assisted software engineering.

</details>


### [96] [Supply-chain risk of agentic AI - infecting infrastructures via skill worms](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.lukaszolejnik.com%2Fsupply-chain-risk-of-agentic-ai-infecting-infrastructures-via-skill-worms%2F%3Futm_source=tldrai/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/lSc0M6dJecJeY3minpDAnJZnVY4G05xVdRcXU0ZG8rI=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AI助手技能存在安全风险，可能通过执行shell、网络和文件系统命令感染基础设施


<details>
  <summary>Details</summary>
Motivation: AI助手添加的技能可能引入重大安全风险，这些技能能够执行底层系统命令，可能导致供应链风险

Method: 分析AI助手技能的安全漏洞，特别是那些能够执行shell命令、网络操作和文件系统访问的技能

Result: 发现AI助手技能存在严重安全风险，可能通过"技能蠕虫"感染基础设施

Conclusion: 需要加强对AI助手技能的安全审查和防护措施，防止供应链风险

Abstract: Supply-chain risk of agentic AI - infecting infrastructures via skill worms (2 minute read) Skills added to AI assistants can introduce significant security risks by executing shell, network, and filesystem commands.

</details>


### [97] [Securing Agents in Production](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fd45vni/1/0100019beb380879-4a003cf1-8d94-465f-af65-ce144f7ce696-000000/qapk8IvvAxKNsxP_oemSrTOPANpr3Ifxuj2yr5gtnt4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Palantir的AIP Agentic Runtime为生产环境中的智能体提供全生命周期安全保障


<details>
  <summary>Details</summary>
Motivation: 随着智能体在生产环境中的部署增加，需要确保其在完整操作生命周期中的安全性，防止潜在风险和安全漏洞

Method: 采用AIP Agentic Runtime框架，为智能体提供从开发到部署、运行到退役的全生命周期安全防护机制

Result: 实现了智能体在生产环境中的安全运行，能够有效防护各种安全威胁，确保智能体系统的可靠性和安全性

Conclusion: AIP Agentic Runtime为生产环境中的智能体提供了全面的安全保障框架，是智能体安全部署的关键基础设施

Abstract: Securing Agents in Production (11 minute read) This post examines how Palantir's AIP Agentic Runtime secures agents across the full operational life cycle.

</details>


### [98] [The Desperate Need For An “Agent Contract”](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.montecarlodata.com%2Fai-agent-contract%2F%3Futm_source=tldrdata/1/0100019bf9fd83fc-7b7b43b2-9d54-41ed-864a-7e5802f55166-000000/XlOd693NgcuFvYFN1QDlOcPgpaGcEzGfVKTR-knLirM=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 提出"AI Agent Contract"框架，建立数据生产者、工程师和AI代理之间的明确可执行协议，确保可靠输入、一致定义和受控变更，防止模式漂移、工具损坏或期望不匹配导致的常见故障。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理系统面临模式漂移、工具损坏、期望不匹配等常见故障，缺乏明确的协议框架来确保数据生产者、工程师和AI代理之间的可靠协作。

Method: 提出"AI Agent Contract"框架，建立三方之间的明确可执行协议，包括可靠输入保证、一致定义标准和受控变更机制。

Result: 该框架能够防止AI代理系统中的常见故障，提高系统可靠性和协作效率。

Conclusion: 迫切需要建立AI代理合同框架来解决当前AI代理系统中的协作和可靠性问题。

Abstract: The Desperate Need For An “Agent Contract” (6 minute read) The AI Agent Contract is a framework that establishes explicit, enforceable agreements between data producers, engineers, and AI agents to ensure reliable inputs, consistent definitions, and controlled changes, preventing common failures from schema drifts, tool breakage, or mismatched expectations.

</details>


### [99] [Gas Town's Agent Patterns, Design Bottlenecks, and Vibecoding at Scale](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fmaggieappleton.com%2Fgastown%2F%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/AHidIO70NwEElEl6sA22QDFSEXf0RGdRgQRAL4yCeog=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Gas Town是一个投机性设计项目，其设计存在根本性缺陷，但其中揭示的代理模式、设计瓶颈和规模化编码问题将在下一代开发工具中出现。


<details>
  <summary>Details</summary>
Motivation: 分析Gas Town这个投机设计项目中的问题，预测这些问题将在下一代软件开发工具中重现，强调在软件开发加速的背景下，深思熟虑的设计、批判性思维、用户研究和团队协调的重要性。

Method: 通过对Gas Town项目的批判性分析，识别其中的代理模式、设计瓶颈和规模化编码问题，并基于这些观察进行预测性分析。

Result: 识别出Gas Town设计中的根本缺陷，预测这些问题将在未来开发工具中出现，强调有价值的工具将需要更好的设计思维和团队协作。

Conclusion: 随着软件开发速度加快，深思熟虑的设计、批判性思维、用户研究和团队协调将变得更加重要，最有价值的工具将体现这些原则。

Abstract: Gas Town's Agent Patterns, Design Bottlenecks, and Vibecoding at Scale (32 minute read) Gas Town is a provocative piece of speculative design, but its design is too poorly thought through to persist. The problems with Gas Town will undoubtedly show up in the next generation of development tools. Thoughtful design, critical thinking, user research, and planning and coordination within teams will become more important as the pace of software development speeds up. The most valuable tools will h...

</details>


### [100] [Claude Code TeammateTool - Source Code Analysis](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgist.github.com%2Fmccun934%2Ffc0779bcf2d47fcb65969e5a4208c9cf%3Futm_source=tldrnewsletter/1/0100019bfa0d545d-fe7275a0-a106-4714-83e4-9ef4a50d4749-000000/8JDPQCZhbJM8c8--jNd_3wB697alSOIrFBuuUO2wul4=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文分析了Claude Code v2.1.19二进制文件中隐藏的TeammateTool功能，这是一个已实现但被功能标志禁用的源代码分析工具，可用于多角度代码审查、分层代理协作等功能。


<details>
  <summary>Details</summary>
Motivation: Claude Code中隐藏的TeammateTool功能虽然已完全实现，但被功能标志限制而无法使用。该工具具有强大的源代码分析能力，可以支持多种开发场景，但缺乏官方文档和公开访问途径。

Method: 通过逆向工程分析Claude Code v2.1.19二进制文件，发现并记录了TeammateTool的实现细节、功能特性和潜在应用场景，包括多视角PR审查、分层代理协作等架构设计。

Result: 成功识别出TeammateTool的具体功能模块，包括多角度代码审查、分层代理架构、生产问题调查等能力，并详细记录了该工具的技术实现和潜在应用价值。

Conclusion: TeammateTool是一个功能完整但被隐藏的源代码分析工具，具有强大的多代理协作能力，如果开放使用可能显著提升代码开发效率和质量，但目前仅作为内部实验功能存在。

Abstract: Claude Code TeammateTool - Source Code Analysis (12 minute read) TeammateTool is an existing-but-hidden functionality found in the Claude Code v2.1.19 binary. While it is fully implemented, it is gated behind feature flags. The tool could potentially be used to review PRs through multiple perspectives, build complete features with specialized agents for each layer, investigate a production bug from multiple angles, and more. This post documents what has been discovered about the feature.

</details>


### [101] [Using MCP Servers with Docker: Tools to Multi-Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.docker.com%2Fblog%2Fmcp-servers-docker-toolkit-cagent-gateway%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/XmYQ5Tvz9Nu-u0XasMuDm9vQhq69b7slfneObW8lowo=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Docker推出MCP生态系统，简化MCP服务器与LLM集成，提供MCP Catalog、Toolkit、cagent等工具，解决运行时复杂性、密钥注入和客户端-服务器连接等挑战


<details>
  <summary>Details</summary>
Motivation: 解决在大型语言模型中集成Model Context Protocol（MCP）服务器时面临的运行时复杂性、密钥注入和客户端-服务器连接等常见挑战

Method: 通过Docker生态系统提供MCP Catalog和Toolkit简化设置，使用cagent构建多代理系统，并与LangGraph等高级代理框架实现无缝兼容

Result: 创建了一个简化的MCP服务器集成生态系统，能够有效解决运行时复杂性、秘密注入和客户端-服务器连接问题

Conclusion: Docker的MCP工具生态系统显著简化了MCP服务器与大型语言模型的集成过程，为构建多代理系统提供了更高效的方法

Abstract: Using MCP Servers with Docker: Tools to Multi-Agent (4 minute read) Docker has introduced an ecosystem to simplify integrating Model Context Protocol (MCP) servers with large language models, offering the MCP Catalog and Toolkit for streamlined setup, cagent for building multi-agent systems, and seamless compatibility with advanced agent frameworks like LangGraph. These tools address common challenges such as runtime complexity, secret injection, and client-to-server wiring by leveraging Dock...

</details>


### [102] [From AI agent prototype to product: Lessons from building AWS DevOps Agent](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faws.amazon.com%2Fblogs%2Fdevops%2Ffrom-ai-agent-prototype-to-product-lessons-from-building-aws-devops-agent%2F%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/J9sdGm4dfmEmSkbDY5f0j98ZvkXP8QNl3S0-7vgDEsQ=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: AWS DevOps Agent从原型到产品的实践经验，总结了生产化智能体系统的五个关键机制：评估、快速反馈循环、轨迹可视化、有意识变更和生产采样，以提升可靠性、准确性和成本效率。


<details>
  <summary>Details</summary>
Motivation: 将AI智能体从原型转化为实际可用的产品面临诸多挑战，特别是在生产环境中确保可靠性、准确性和成本效率。AWS DevOps Agent的开发经验揭示了智能体系统产品化过程中的关键问题。

Method: 提出了五个核心机制：1) 评估系统用于衡量智能体性能；2) 快速反馈循环加速迭代；3) 轨迹可视化帮助理解智能体决策过程；4) 有意识变更确保系统稳定演进；5) 生产采样收集真实场景数据。

Result: 这些机制显著提升了AWS DevOps Agent在事件响应中的可靠性、准确性和成本效率，成功将智能体系统从原型转化为生产级产品。

Conclusion: 智能体系统的成功产品化需要系统性的工程实践，包括评估、可视化、反馈循环等机制，这些经验为其他智能体系统的开发提供了有价值的参考。

Abstract: From AI agent prototype to product: Lessons from building AWS DevOps Agent (9 minute read) This post describes lessons from building the AWS DevOps Agent, outlining five mechanisms to productionize agentic systems: evals, fast feedback loops, trajectory visualization, intentional changes, and production sampling. It details how these practices improve reliability, accuracy, and cost efficiency in incident response.

</details>


### [103] [Context7](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fupstash%2Fcontext7%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/Egnu5i4f84BlOwkAHgWJhlGwNAs8ng3-yhXoCZnJy60=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Context7是一个MCP服务器，提供最新、版本特定的代码文档和示例给LLM和AI代码编辑器，旨在防止过时信息和API幻觉问题


<details>
  <summary>Details</summary>
Motivation: 解决LLM和AI代码编辑器中常见的过时文档和API幻觉问题，确保开发者获得准确、最新的代码库信息

Method: 开发MCP服务器，从GitHub仓库拉取当前库的详细信息并集成到提示中，支持Cursor和Claude Code等平台

Result: 创建了Context7 MCP服务器，能够提供版本特定的代码文档和示例，但社区贡献的文档准确性不保证

Conclusion: Context7通过实时获取代码库信息改善了AI辅助编程的准确性，但需要进一步确保文档质量

Abstract: Context7 (GitHub Repo) Context7 MCP Server delivers up-to-date, version-specific code documentation and examples directly to LLMs and AI code editors, aiming to prevent outdated information and "hallucinated" APIs. Available for integration with platforms like Cursor and Claude Code, the server pulls current library details into prompts, though its community-contributed documentation is not guaranteed for accuracy.

</details>


### [104] [Observability for ChatGPT Apps in the Age of Agentic AI](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewrelic.com%2Fblog%2Fdem%2Fobservability-for-chatgpt-apps-in-the-age-of-agentic-ai%3Futm_source=tldrdevops/1/0100019bfa43b2f9-208c69c1-8b4d-41a0-ae01-728f44e8f526-000000/SSXVyVwwAa0ElrVRmMHXqtd_3eNy_PJrK5Vj73nAn0I=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: New Relic推出针对ChatGPT应用的监控解决方案，解决AI平台中自定义集成的可观测性问题


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT应用（在AI平台中运行的自定义集成）的普及，缺乏有效的监控工具来追踪这些应用的性能、错误和用户行为

Method: New Relic开发了专门的监控解决方案，为ChatGPT应用提供可观测性工具，包括性能监控、错误追踪和用户行为分析

Result: 解决了ChatGPT应用在AI平台环境中的监控挑战，使开发者能够更好地理解和优化其AI集成应用

Conclusion: 在智能体AI时代，为ChatGPT应用提供专门的可观测性工具对于确保应用可靠性和性能至关重要

Abstract: Observability for ChatGPT Apps in the Age of Agentic AI (4 minute read) New Relic has addressed a critical monitoring challenge for "ChatGPT Apps"—custom integrations running within AI platforms.

</details>


### [105] [The browser is the sandbox](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Faifoc.us%2Fthe-browser-is-the-sandbox%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/FT7nnugSDOTPcisjLzVJZc24AViezyUrF71NxNob328=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 浏览器可作为AI代理的安全沙箱，提供文件系统隔离、网络访问控制和安全代码执行能力


<details>
  <summary>Details</summary>
Motivation: AI代理工具需要访问用户本地文件和网络，但需要安全隔离机制来保护用户隐私和系统安全

Method: 利用浏览器的内置安全特性：文件系统隔离、CSP控制网络访问、iframe和WebAssembly实现安全代码执行

Result: 展示了浏览器作为AI代理安全沙箱的可行性，并通过演示项目验证了这些能力

Conclusion: 浏览器具备成为AI代理工具安全执行环境的潜力，能够平衡功能需求与安全要求

Abstract: The browser is the sandbox (18 minute read) The browser has potential as a secure sandbox for agentic AI tools, which automate tasks and require access to a user's local files and network. It has mechanisms for filesystem isolation, controlled network access via CSP, and secure code execution using iframes and WebAssembly. A demo project is displayed in this article that shows these capabilities.

</details>


### [106] [AI tribalism](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnolanlawson.com%2F2026%2F01%2F24%2Fai-tribalism%2F%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/MhnpEaAf8tSQp7XM0cCzlc7bV2iEaWShzWAtFfspOLI=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 讨论AI代理在代码生成、bug修复和解决复杂问题方面的能力，强调工程师需要持续实验AI工具以保持竞争力


<details>
  <summary>Details</summary>
Motivation: 针对当前围绕LLM的讨论陷入非建设性的部落主义，以及工程师对AI工具可能带来的工作替代的担忧，提出需要客观看待AI代理的实际能力

Method: 通过分析AI代理在代码生成、bug修复、安全性和性能问题解决等方面的实际表现，强调实践和实验的重要性

Result: AI代理已经能够高效完成代码生成、bug修复等任务，并能处理安全和性能等复杂问题

Conclusion: 工程师需要积极实验和采用AI工具，避免陷入无意义的部落主义争论，以保持技术竞争力

Abstract: AI tribalism (6 minute read) Discussions around LLMs have devolved into unproductive tribalism, fueled by early negative associations with tech "hucksters" and fears of job displacement. AI agents are already capable of efficient code generation, bug fixing, and addressing complex issues like security and performance. Engineers need to keep experimenting with AI to stay on top of new tooling.

</details>


### [107] [Fence](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2FUse-Tusk%2Ffence%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/3LuWgM_La4DtSMxnGheDQ2IfjUlkBC5n1D_dN69ERpc=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: Fence是一个轻量级、无容器的沙盒工具，通过包装命令来限制网络访问和文件系统操作，用于运行半可信代码并增强AI编码代理的安全性


<details>
  <summary>Details</summary>
Motivation: 需要一种安全的方式来运行半可信代码（如包安装、CI作业），同时控制其副作用，并提高AI编码代理的安全性

Method: 开发了一个轻量级、无容器的沙盒工具，通过包装命令来限制网络访问和文件系统操作

Result: 创建了Fence工具，能够有效限制代码的网络和文件系统访问，提高运行半可信代码的安全性

Conclusion: Fence提供了一种简单有效的方法来安全运行半可信代码，特别适用于AI编码代理的安全增强

Abstract: Fence (GitHub Repo) Fence is a lightweight, container-free sandbox tool that wraps commands to restrict network access and filesystem operations. It's designed for running semi-trusted code, like package installs or CI jobs, with controlled side effects, and it improves AI coding agent security.

</details>


### [108] [150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgetboothiq.com%2Fblog%2F150k-lines-vibe-coded-elixir-good-bad-ugly%3Futm_source=tldrdev/1/0100019bfa4c672e-57951fed-f4c9-4df0-8813-18ab641d574d-000000/7DswHGQ7NF3ipUXf0CPE6Y5cZ5Rf-ZdAwsLsPw6HrbY=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: BoothIQ使用AI生成了15万行Elixir代码，发现Elixir语言的小巧、简洁和不可变性非常适合AI生成，但AI在架构决策、调试复杂问题方面存在困难


<details>
  <summary>Details</summary>
Motivation: 探索AI在生成Elixir代码方面的实际应用效果，评估AI编码在大型项目中的可行性和局限性

Method: 使用AI工具（包括Elixir专用工具如Tidewave）生成15万行Elixir代码，分析生成过程中的优缺点

Result: AI能高效生成Elixir代码，特别是在语言特性支持下表现良好，但在架构决策、调试复杂问题、避免防御性编程方面存在明显不足

Conclusion: AI在代码生成方面有显著优势，但需要人类监督来处理架构决策和复杂调试，AI与人类协作是最佳实践

Abstract: 150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly (8 minute read) BoothIQ used AI to write all 150,000 lines of its Elixir codebase, finding the language's small, terse, and immutable nature great for AI generation. AI is great at generating code efficiently, especially when aided by Elixir-specific tools like Tidewave. However, it struggles with architectural decisions, often produces defensive code trained on imperative languages, and is ineffective at debugging complex El...

</details>


### [109] [Systems of Action and the Domino's Pizza Tracker Product Model](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.excel.holdings%2Fp%2Fsystems-of-action-and-the-dominos%3Futm_source=tldrfounders/1/0100019bfa6b25b6-d530ff35-c9df-4547-aebe-50cb0fa4c066-000000/jpmM-DWtRRG0Qrr5N50jlY-HkN3ISy2IWzTWFGYbakY=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 论文提出行动系统应像时间线一样透明展示离散事件，让用户看到整个智能系统在追踪所有过程


<details>
  <summary>Details</summary>
Motivation: 当前智能系统缺乏透明性，用户无法了解系统内部的工作过程和决策依据，需要建立类似多米诺披萨追踪器的透明产品模型

Method: 采用时间线式的事件记录方法，将系统行动分解为离散事件，提供透明的记录机制，让用户能够查看整个智能系统的追踪过程

Result: 提出了一个透明、可追踪的行动系统框架，类似于多米诺披萨追踪器，能够清晰展示系统执行过程中的每个步骤和状态变化

Conclusion: 智能系统需要提供透明的行动追踪机制，让用户能够理解系统的工作过程，这有助于建立信任并提高系统的可解释性

Abstract: Systems of Action and the Domino's Pizza Tracker Product Model (3 minute read) A system of action should look just like a timeline with a transparent record of discrete events and the ability for users to see that the whole agentic system is tracking everything.

</details>


### [110] [Cursor for Designers Guide](https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadplist.substack.com%2Fp%2Fa-designers-guide-to-cursor-and-claude%3Futm_source=tldrdesign/1/0100019bfa77e59d-f4347f62-f5d7-466b-b9b1-9cd1351dca2e-000000/7e4Aa40Q4HIGRmHgI05b7MmzX6-Y7KvRQT-XCQKSVso=441)
*TLDR Newsletter*

Main category: tldr.article

TL;DR: 本文介绍了设计师如何使用Cursor和Claude Code等AI工具通过"氛围编码"构建功能性Web应用，展示了在48小时内创建带AI聊天功能的个人作品集网站以及在5天内构建增长设计工具的完整流程。


<details>
  <summary>Details</summary>
Motivation: 传统上设计师需要依赖开发人员将设计转化为代码，这限制了设计师独立实现想法的能力。本文旨在展示AI工具如何赋能设计师，使其无需编码经验也能构建功能完整的Web应用，降低技术门槛。

Method: 采用"氛围编码"工作流程：1) 使用Claude Code进行项目规划；2) 通过自然语言提示迭代开发；3) 部署到GitHub和Vercel。具体步骤包括创建个人作品集网站和增长设计工具两个案例演示。

Result: 成功演示了设计师可以在48小时内创建带AI聊天功能的个人作品集网站，并在5天内构建完整的增长设计工具。证明了AI工具能够显著降低Web开发的技术门槛，使设计师能够独立实现功能应用。

Conclusion: AI代码生成工具如Cursor和Claude Code正在改变设计师的工作方式，使非技术背景的设计师能够快速构建功能性Web应用。这种"氛围编码"方法代表了设计工具发展的新方向，将设计思维与代码实现更紧密地结合。

Abstract: Cursor for Designers Guide (9 minute read) AI tools like Cursor and Claude Code enable designers without coding experience to build functional web applications through "vibe-coding". The guide demonstrates how to create a personal portfolio website with AI chat functionality in 48 hours and a growth design tool in 5 days, using a workflow that includes planning with Claude Code, iterating on natural language prompts, and deploying to GitHub and Vercel. Step-by-step instructions cover how to s...

</details>
